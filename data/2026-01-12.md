<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 30]
- [cs.CY](#cs.CY) [Total: 6]
- [stat.AP](#stat.AP) [Total: 3]
- [cs.SI](#cs.SI) [Total: 2]
- [econ.EM](#econ.EM) [Total: 4]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Naiad: Novel Agentic Intelligent Autonomous System for Inland Water Monitoring](https://arxiv.org/abs/2601.05256)
*Eirini Baltzi,Tilemachos Moumouris,Athena Psalta,Vasileios Tsironis,Konstantinos Karantzalos*

Main category: cs.AI

TL;DR: NAIAD是一个基于LLM的智能助手，通过整合多源地球观测数据和专业工具，为内陆水域监测提供端到端的解决方案，支持自然语言查询并生成定制化报告。


<details>
  <summary>Details</summary>
Motivation: 现有内陆水域监测方法通常孤立地处理不同水质指标（如蓝藻、叶绿素等），缺乏综合性解决方案。NAIAD旨在为专家和非专家用户提供一个统一的智能助手，通过自然语言接口实现全面的内陆水域监测。

Method: NAIAD采用基于大语言模型的智能体架构，结合检索增强生成（RAG）、外部工具编排、计算图执行和智能体反思等技术。系统整合了天气数据、Sentinel-2影像、遥感指数计算（如NDCI）、叶绿素-a估算以及CyFi等专业平台。

Result: 在专门设计的基准测试中，系统在正确性和相关性指标上分别达到77%和85%以上。初步结果显示系统对不同查询类型具有良好的适应性和鲁棒性。消融研究发现Gemma 3（27B）和Qwen 2.5（14B）在计算效率和推理性能之间取得了最佳平衡。

Conclusion: NAIAD成功构建了一个基于LLM的智能助手，能够为内陆水域监测提供全面的解决方案，支持自然语言交互，并在多专家级别基准测试中表现出色，为环境监测领域提供了新的智能化工具。

Abstract: Inland water monitoring is vital for safeguarding public health and ecosystems, enabling timely interventions to mitigate risks. Existing methods often address isolated sub-problems such as cyanobacteria, chlorophyll, or other quality indicators separately. NAIAD introduces an agentic AI assistant that leverages Large Language Models (LLMs) and external analytical tools to deliver a holistic solution for inland water monitoring using Earth Observation (EO) data. Designed for both experts and non-experts, NAIAD provides a single-prompt interface that translates natural-language queries into actionable insights. Through Retrieval-Augmented Generation (RAG), LLM reasoning, external tool orchestration, computational graph execution, and agentic reflection, it retrieves and synthesizes knowledge from curated sources to produce tailored reports. The system integrates diverse tools for weather data, Sentinel-2 imagery, remote-sensing index computation (e.g., NDCI), chlorophyll-a estimation, and established platforms such as CyFi. Performance is evaluated using correctness and relevancy metrics, achieving over 77% and 85% respectively on a dedicated benchmark covering multiple user-expertise levels. Preliminary results show strong adaptability and robustness across query types. An ablation study on LLM backbones further highlights Gemma 3 (27B) and Qwen 2.5 (14B) as offering the best balance between computational efficiency and reasoning performance.

</details>


### [2] [Mathematical Knowledge Graph-Driven Framework for Equation-Based Predictive and Reliable Additive Manufacturing](https://arxiv.org/abs/2601.05298)
*Yeongbin Cha,Namjung Kim*

Main category: cs.AI

TL;DR: 提出基于本体引导、方程中心的框架，将大语言模型与增材制造数学知识图谱结合，实现可靠知识提取和原则性外推建模


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法在增材制造中受限于碎片化知识表示和稀疏数据条件下的不可靠外推，需要更可靠的知识提取和外推建模方法

Method: 1) 构建形式化本体编码方程、变量、假设及其语义关系；2) 将非结构化文献转化为机器可解释表示；3) 基于知识图谱子图条件化LLM方程生成；4) 引入置信感知外推评估，整合外推距离、统计稳定性和物理一致性

Result: 本体引导提取显著提高提取知识的结构连贯性和定量可靠性；子图条件化方程生成相比无引导LLM输出产生更稳定、物理一致的外推结果

Conclusion: 建立统一流程实现本体驱动知识表示、方程中心推理和置信度外推评估，展示知识图谱增强LLM作为增材制造外推建模可靠工具的潜力

Abstract: Additive manufacturing (AM) relies critically on understanding and extrapolating process-property relationships; however, existing data-driven approaches remain limited by fragmented knowledge representations and unreliable extrapolation under sparse data conditions. In this study, we propose an ontology-guided, equation-centric framework that tightly integrates large language models (LLMs) with an additive manufacturing mathematical knowledge graph (AM-MKG) to enable reliable knowledge extraction and principled extrapolative modeling. By explicitly encoding equations, variables, assumptions, and their semantic relationships within a formal ontology, unstructured literature is transformed into machine-interpretable representations that support structured querying and reasoning. LLM-based equation generation is further conditioned on MKG-derived subgraphs, enforcing physically meaningful functional forms and mitigating non-physical or unstable extrapolation trends. To assess reliability beyond conventional predictive uncertainty, a confidence-aware extrapolation assessment is introduced, integrating extrapolation distance, statistical stability, and knowledge-graph-based physical consistency into a unified confidence score. Results demonstrate that ontology-guided extraction significantly improves the structural coherence and quantitative reliability of extracted knowledge, while subgraph-conditioned equation generation yields stable and physically consistent extrapolations compared to unguided LLM outputs. Overall, this work establishes a unified pipeline for ontology-driven knowledge representation, equation-centered reasoning, and confidence-based extrapolation assessment, highlighting the potential of knowledge-graph-augmented LLMs as reliable tools for extrapolative modeling in additive manufacturing.

</details>


### [3] [Effects of personality steering on cooperative behavior in Large Language Model agents](https://arxiv.org/abs/2601.05302)
*Mizuki Sakai,Mizuki Yokoyama,Wakaba Tateishi,Genki Ichinose*

Main category: cs.AI

TL;DR: 研究通过重复囚徒困境实验发现，在LLM代理中，宜人性是促进合作的主要人格因素，而人格引导更多是行为偏差而非确定性控制机制。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究表明为LLM分配人格特质会影响其行为，但人格引导在受控条件下如何影响合作行为仍不清楚。本研究旨在探索人格引导对LLM代理在战略互动中合作行为的影响。

Method: 使用重复囚徒困境游戏，基于大五人格框架，首先测量GPT-3.5-turbo、GPT-4o和GPT-5的基本人格特征，然后比较基线和人格引导条件下的行为，并分析独立操纵各人格维度到极端值的影响。

Result: 宜人性是所有模型中促进合作的主导因素，其他人格特质影响有限。明确的人格信息会增加合作，但也可能增加被利用的脆弱性，特别是在早期模型中。后期模型表现出更具选择性的合作。

Conclusion: 人格引导更多是作为行为偏差而非确定性控制机制，不同代际模型对人格引导的反应存在差异，后期模型表现出更复杂的合作策略。

Abstract: Large language models (LLMs) are increasingly used as autonomous agents in strategic and social interactions. Although recent studies suggest that assigning personality traits to LLMs can influence their behavior, how personality steering affects cooperation under controlled conditions remains unclear. In this study, we examine the effects of personality steering on cooperative behavior in LLM agents using repeated Prisoner's Dilemma games. Based on the Big Five framework, we first measure basic personality profiles of three models, GPT-3.5-turbo, GPT-4o, and GPT-5, using the Big Five Inventory. We then compare behavior under baseline and personality-informed conditions, and further analyze the effects of independently manipulating each personality dimension to extreme values. Our results show that agreeableness is the dominant factor promoting cooperation across all models, while other personality traits have limited impact. Explicit personality information increases cooperation but can also raise vulnerability to exploitation, particularly in earlier-generation models. In contrast, later-generation models exhibit more selective cooperation. These findings indicate that personality steering acts as a behavioral bias rather than a deterministic control mechanism.

</details>


### [4] [Improving Enzyme Prediction with Chemical Reaction Equations by Hypergraph-Enhanced Knowledge Graph Embeddings](https://arxiv.org/abs/2601.05330)
*Tengwei Song,Long Yin,Zhen Han,Zhiqiang Xu*

Main category: cs.AI

TL;DR: 提出Hyper-Enz模型，利用知识图谱嵌入和超图transformer从化学反应方程预测酶-底物相互作用，相比传统方法有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有酶-底物相互作用预测方法依赖专家标注的稀疏数据库，训练数据不足导致泛化能力差。化学反应方程数据更易获取且更丰富，但多化合物与酶之间的复杂关系传统模型难以捕捉。

Method: 将化学反应方程表示为(底物,酶,产物)三元组构建知识图谱，提出Hyper-Enz模型：结合超图transformer和知识图谱嵌入学习多底物/产物超边表示，并引入多专家范式指导学习。

Result: 实验显示显著改进：酶检索准确率相对提升达88%，配对级预测提升30%，证明方法有效性。

Conclusion: 通过知识图谱表示化学反应方程并利用超图transformer学习复杂关系，能有效预测酶-底物相互作用，解决了传统方法数据稀疏和泛化能力不足的问题。

Abstract: Predicting enzyme-substrate interactions has long been a fundamental problem in biochemistry and metabolic engineering. While existing methods could leverage databases of expert-curated enzyme-substrate pairs for models to learn from known pair interactions, the databases are often sparse, i.e., there are only limited and incomplete examples of such pairs, and also labor-intensive to maintain. This lack of sufficient training data significantly hinders the ability of traditional enzyme prediction models to generalize to unseen interactions. In this work, we try to exploit chemical reaction equations from domain-specific databases, given their easier accessibility and denser, more abundant data. However, interactions of multiple compounds, e.g., educts and products, with the same enzymes create complex relational data patterns that traditional models cannot easily capture. To tackle that, we represent chemical reaction equations as triples of (educt, enzyme, product) within a knowledge graph, such that we can take advantage of knowledge graph embedding (KGE) to infer missing enzyme-substrate pairs for graph completion. Particularly, in order to capture intricate relationships among compounds, we propose our knowledge-enhanced hypergraph model for enzyme prediction, i.e., Hyper-Enz, which integrates a hypergraph transformer with a KGE model to learn representations of the hyper-edges that involve multiple educts and products. Also, a multi-expert paradigm is introduced to guide the learning of enzyme-substrate interactions with both the proposed model and chemical reaction equations. Experimental results show a significant improvement, with up to a 88% relative improvement in average enzyme retrieval accuracy and 30% improvement in pair-level prediction compared to traditional models, demonstrating the effectiveness of our approach.

</details>


### [5] [The Persona Paradox: Medical Personas as Behavioral Priors in Clinical Language Models](https://arxiv.org/abs/2601.05376)
*Tassallah Abdullahi,Shrestha Ghosh,Hamish S Fraser,Daniel León Tramontini,Adeel Abbasi,Ghada Bourjeily,Carsten Eickhoff,Ritambhara Singh*

Main category: cs.AI

TL;DR: 医学角色设定对LLMs在临床决策中的影响具有系统性、情境依赖性和非单调性：在重症监护任务中提升性能，但在初级保健中降低性能，交互风格影响风险倾向但高度模型依赖


<details>
  <summary>Details</summary>
Motivation: 角色设定常被视为LLMs的行为先验，被认为能提高专业性和安全性，但其对高风险临床决策的具体影响尚未充分研究，需要系统评估不同医疗角色和交互风格对临床LLM行为的影响

Method: 系统评估基于角色的控制对临床LLMs的影响，考察专业角色（急诊科医生、护士等）和交互风格（大胆vs谨慎）在不同模型和医疗任务中的行为影响，使用多维评估方法衡量任务准确性、校准度和安全相关风险行为

Result: 发现系统性、情境依赖性和非单调性效应：医疗角色在重症监护任务中提升性能（准确性和校准度提高约20%），但在初级保健环境中降低性能；交互风格调节风险倾向和敏感性但高度模型依赖；LLM评估倾向于医疗角色在安全关键案例中更优，但人类临床医生对安全合规性仅显示中等一致性（Cohen's κ=0.43），且95.9%的回应中对推理质量信心较低

Conclusion: 角色设定作为行为先验引入情境依赖的权衡而非安全或专业性的保证，在临床LLM应用中需要谨慎考虑角色设定的上下文依赖效应

Abstract: Persona conditioning can be viewed as a behavioral prior for large language models (LLMs) and is often assumed to confer expertise and improve safety in a monotonic manner. However, its effects on high-stakes clinical decision-making remain poorly characterized. We systematically evaluate persona-based control in clinical LLMs, examining how professional roles (e.g., Emergency Department physician, nurse) and interaction styles (bold vs.\ cautious) influence behavior across models and medical tasks. We assess performance on clinical triage and patient-safety tasks using multidimensional evaluations that capture task accuracy, calibration, and safety-relevant risk behavior. We find systematic, context-dependent, and non-monotonic effects: Medical personas improve performance in critical care tasks, yielding gains of up to $\sim+20\%$ in accuracy and calibration, but degrade performance in primary-care settings by comparable margins. Interaction style modulates risk propensity and sensitivity, but it's highly model-dependent. While aggregated LLM-judge rankings favor medical over non-medical personas in safety-critical cases, we found that human clinicians show moderate agreement on safety compliance (average Cohen's $κ= 0.43$) but indicate a low confidence in 95.9\% of their responses on reasoning quality. Our work shows that personas function as behavioral priors that introduce context-dependent trade-offs rather than guarantees of safety or expertise. The code is available at https://github.com/rsinghlab/Persona\_Paradox.

</details>


### [6] [Conformity and Social Impact on AI Agents](https://arxiv.org/abs/2601.05384)
*Alessandro Bellina,Giordano De Marzo,David Garcia*

Main category: cs.AI

TL;DR: 研究发现大型多模态语言模型作为AI代理时表现出系统性从众偏差，符合社会影响理论，揭示了AI代理决策中的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理在多代理环境中日益增多，理解它们的集体行为对于预测人工社会动态变得至关重要。本研究旨在探索AI代理在群体压力下的从众行为。

Method: 通过改编社会心理学中的经典视觉实验，研究AI代理如何作为社会行动者响应群体影响，考察群体规模、一致性、任务难度和来源特征等因素。

Result: AI代理表现出系统性从众偏差，符合社会影响理论。单独表现近乎完美的AI代理在社交影响下变得高度易受操纵。这种脆弱性在不同模型规模中持续存在：大型模型在简单任务上从众减少，但在能力边界处仍然脆弱。

Conclusion: AI代理决策存在基本安全漏洞，可能导致恶意操纵、虚假信息传播和偏见扩散，凸显了在集体AI部署中实施保障措施的紧迫性。

Abstract: As AI agents increasingly operate in multi-agent environments, understanding their collective behavior becomes critical for predicting the dynamics of artificial societies. This study examines conformity, the tendency to align with group opinions under social pressure, in large multimodal language models functioning as AI agents. By adapting classic visual experiments from social psychology, we investigate how AI agents respond to group influence as social actors. Our experiments reveal that AI agents exhibit a systematic conformity bias, aligned with Social Impact Theory, showing sensitivity to group size, unanimity, task difficulty, and source characteristics. Critically, AI agents achieving near-perfect performance in isolation become highly susceptible to manipulation through social influence. This vulnerability persists across model scales: while larger models show reduced conformity on simple tasks due to improved capabilities, they remain vulnerable when operating at their competence boundary. These findings reveal fundamental security vulnerabilities in AI agent decision-making that could enable malicious manipulation, misinformation campaigns, and bias propagation in multi-agent systems, highlighting the urgent need for safeguards in collective AI deployments.

</details>


### [7] [On the Effect of Cheating in Chess](https://arxiv.org/abs/2601.05386)
*Daniel Keren*

Main category: cs.AI

TL;DR: 该研究评估了在国际象棋比赛中有限次数作弊（使用软件建议）可能带来的性能提升，而非专注于作弊检测。


<details>
  <summary>Details</summary>
Motivation: 国际象棋中使用强大软件建议的作弊已成为严重问题，甚至影响到最高级别比赛。大多数先前研究关注作弊检测，而本研究旨在量化有限次数作弊可能带来的实际性能提升。

Method: 开发算法并在常用国际象棋引擎上进行测试，模拟比赛中有限次数使用软件建议的情况。

Result: 研究得出了有限次数作弊可能带来的性能提升评估结果，为理解和遏制作弊提供了重要数据。

Conclusion: 量化作弊效果对于有效遏制和检测作弊至关重要，本研究为此提供了方法论和实证基础，而非协助作弊者。

Abstract: Cheating in chess, by using advice from powerful software, has become a major problem, reaching the highest levels. As opposed to the large majority of previous work, which concerned {\em detection} of cheating, here we try to evaluate the possible gain in performance, obtained by cheating a limited number of times during a game. Algorithms are developed and tested on a commonly used chess engine (i.e software).\footnote{Needless to say, the goal of this work is not to assist cheaters, but to measure the effectiveness of cheating -- which is crucial as part of the effort to contain and detect it.}

</details>


### [8] [ART: Adaptive Reasoning Trees for Explainable Claim Verification](https://arxiv.org/abs/2601.05455)
*Sahil Wadhwa,Himanshu Kumar,Guanqun Yang,Abbaas Alif Mohamed Nishar,Pranab Mohanty,Swapnil Shinde,Yue Wu*

Main category: cs.AI

TL;DR: ART是一种用于声明验证的分层方法，通过构建支持与反对论点的树状结构，使用LLM作为裁判进行成对比较，最终得出透明且可争议的验证结果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂决策中具有潜力，但其输出缺乏忠实解释且无法有效纠正错误，导致在高风险环境中应用受限。现有方法如思维链缺乏透明度和可争议性。

Method: 提出自适应推理树（ART）方法：从根声明开始，分支为支持和反对的子论点，通过LLM裁判对子论点进行成对锦标赛式比较，自底向上确定论点强度，最终得出透明且可争议的验证结果。

Result: 在多个数据集上实证验证ART，分析不同论点生成器和比较策略。结果显示ART的结构化推理优于强基线，为可解释的声明验证设立了新基准，提高了可靠性并确保决策步骤的清晰性。

Conclusion: ART通过分层推理结构解决了LLM在声明验证中的透明度和可争议性问题，提供了一种比传统方法更可靠、更清晰的决策框架，为高风险环境中的应用奠定了基础。

Abstract: Large Language Models (LLMs) are powerful candidates for complex decision-making, leveraging vast encoded knowledge and remarkable zero-shot abilities. However, their adoption in high-stakes environments is hindered by their opacity; their outputs lack faithful explanations and cannot be effectively contested to correct errors, undermining trustworthiness. In this paper, we propose ART (Adaptive Reasoning Trees), a hierarchical method for claim verification. The process begins with a root claim, which branches into supporting and attacking child arguments. An argument's strength is determined bottom-up via a pairwise tournament of its children, adjudicated by a judge LLM, allowing a final, transparent and contestable verdict to be systematically derived which is missing in methods like Chain-of-Thought (CoT). We empirically validate ART on multiple datasets, analyzing different argument generators and comparison strategies. Our findings show that ART's structured reasoning outperforms strong baselines, establishing a new benchmark for explainable claim verification which is more reliable and ensures clarity in the overall decision making step.

</details>


### [9] [PRISMA: Reinforcement Learning Guided Two-Stage Policy Optimization in Multi-Agent Architecture for Open-Domain Multi-Hop Question Answering](https://arxiv.org/abs/2601.05465)
*Yu Liu,Wenxiao Zhang,Cong Cao,Wenxuan Lu,Fangfang Yuan,Diandian Guo,Kun Peng,Qiang Sun,Kaiyan Zhang,Yanbing Liu,Jin B. Hong,Bowen Zhou,Zhiyuan Ma*

Main category: cs.AI

TL;DR: PRISMA提出解耦的RL框架，通过规划-检索-检查-解决-记忆架构解决RAG系统中的检索崩溃和学习不稳定问题，在十个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决RAG系统中开放域多跳问答的两个关键挑战：1) 检索崩溃 - 在大规模语料库上进行迭代检索时，缺乏推理引导的规划会导致无法找到包含桥梁答案的中间证据；2) 学习不稳定 - 端到端轨迹训练存在推理链信用分配弱和模块间错误定位差的问题，导致过拟合基准特定的启发式方法。

Method: 提出PRISMA框架，采用Plan-Retrieve-Inspect-Solve-Memoize架构。核心是推理引导的协作：检查器提供基于推理的反馈来优化规划器的分解和细粒度检索，同时确保解决器的证据基础推理。使用两阶段组相对策略优化(GRPO)：第一阶段校准规划器和解决器作为规划和推理专家；第二阶段使用观察感知残差策略优化(OARPO)增强检查器的上下文验证和针对性恢复能力。

Result: 在十个基准测试中达到最先进的性能，并能在实际场景中高效部署。

Conclusion: PRISMA通过解耦的RL引导框架和推理引导的协作机制，有效解决了RAG系统中的检索崩溃和学习不稳定问题，实现了在复杂开放域多跳问答任务上的可靠部署。

Abstract: Answering real-world open-domain multi-hop questions over massive corpora is a critical challenge in Retrieval-Augmented Generation (RAG) systems. Recent research employs reinforcement learning (RL) to end-to-end optimize the retrieval-augmented reasoning process, directly enhancing its capacity to resolve complex queries. However, reliable deployment is hindered by two obstacles. 1) Retrieval Collapse: iterative retrieval over large corpora fails to locate intermediate evidence containing bridge answers without reasoning-guided planning, causing downstream reasoning to collapse. 2) Learning Instability: end-to-end trajectory training suffers from weak credit assignment across reasoning chains and poor error localization across modules, causing overfitting to benchmark-specific heuristics that limit transferability and stability. To address these problems, we propose PRISMA, a decoupled RL-guided framework featuring a Plan-Retrieve-Inspect-Solve-Memoize architecture. PRISMA's strength lies in reasoning-guided collaboration: the Inspector provides reasoning-based feedback to refine the Planner's decomposition and fine-grained retrieval, while enforcing evidence-grounded reasoning in the Solver. We optimize individual agent capabilities via Two-Stage Group Relative Policy Optimization (GRPO). Stage I calibrates the Planner and Solver as specialized experts in planning and reasoning, while Stage II utilizes Observation-Aware Residual Policy Optimization (OARPO) to enhance the Inspector's ability to verify context and trigger targeted recovery. Experiments show that PRISMA achieves state-of-the-art performance on ten benchmarks and can be deployed efficiently in real-world scenarios.

</details>


### [10] [MMUEChange: A Generalized LLM Agent Framework for Intelligent Multi-Modal Urban Environment Change Analysis](https://arxiv.org/abs/2601.05483)
*Zixuan Xiao,Jun Ma,Siwei Zhang*

Main category: cs.AI

TL;DR: MMUEChange是一个多模态智能体框架，通过模块化工具包和模态控制器实现异构城市数据的灵活集成，用于复杂城市环境变化分析，相比最佳基线任务成功率提升46.7%


<details>
  <summary>Details</summary>
Motivation: 当前城市环境变化分析方法（特别是遥感变化检测）通常依赖僵化的单模态分析，无法有效处理复杂的城市变化场景，需要更灵活的多模态集成方法

Method: 提出MMUEChange多模态智能体框架，包含模块化工具包和核心模块"模态控制器"，实现跨模态和模态内对齐，能够灵活集成异构城市数据

Result: 相比最佳基线，任务成功率提升46.7%，有效缓解幻觉问题；案例研究包括：纽约小型社区公园增加、香港跨区域水污染扩散、深圳露天垃圾场减少与夜间经济活动关联分析

Conclusion: MMUEChange能够支持具有现实政策意义的复杂城市变化分析任务，为可持续城市发展提供有力工具

Abstract: Understanding urban environment change is essential for sustainable development. However, current approaches, particularly remote sensing change detection, often rely on rigid, single-modal analysis. To overcome these limitations, we propose MMUEChange, a multi-modal agent framework that flexibly integrates heterogeneous urban data via a modular toolkit and a core module, Modality Controller for cross- and intra-modal alignment, enabling robust analysis of complex urban change scenarios. Case studies include: a shift toward small, community-focused parks in New York, reflecting local green space efforts; the spread of concentrated water pollution across districts in Hong Kong, pointing to coordinated water management; and a notable decline in open dumpsites in Shenzhen, with contrasting links between nighttime economic activity and waste types, indicating differing urban pressures behind domestic and construction waste. Compared to the best-performing baseline, the MMUEChange agent achieves a 46.7% improvement in task success rate and effectively mitigates hallucination, demonstrating its capacity to support complex urban change analysis tasks with real-world policy implications.

</details>


### [11] [The Evaluation Gap in Medicine, AI and LLMs: Navigating Elusive Ground Truth & Uncertainty via a Probabilistic Paradigm](https://arxiv.org/abs/2601.05500)
*Aparna Elangovan,Lei Xu,Mahsa Elyasi,Ismail Akdulum,Mehmet Aksakal,Enes Gurun,Brian Hur,Saab Mansour,Ravid Shwartz Ziv,Karin Verspoor,Dan Roth*

Main category: cs.AI

TL;DR: 提出概率评估范式，考虑标注不确定性对AI系统能力评估的影响，引入期望准确率和期望F1分数，建议按标注确定性分层评估


<details>
  <summary>Details</summary>
Motivation: 当前AI系统基准测试通常忽略专家标注答案中的不确定性，这在医学等不确定性普遍存在的领域尤为关键。忽略标注不确定性可能导致误导性结论，使非专家系统看起来与专家表现相似。

Method: 引入概率评估范式，理论上解释标注确定性对评估结果的影响。提出期望准确率和期望F1分数来估计在给定标注变异性的情况下专家或系统能达到的分数。建议按标注概率（通常通过专家间一致性衡量）对结果进行分层评估。

Result: 研究表明，当标注确定性高时，专家才能获得高分；而在标注变异大的数据集中，随机标注者与专家之间可能几乎没有差异。当总体性能低于80%阈值时，分层评估变得至关重要。

Conclusion: 建议在评估系统能力时，应按标注确定性对结果进行分层，特别是在高确定性区间内进行性能比较更可靠，从而减轻不确定性这一关键混杂因素的影响。

Abstract: Benchmarking the relative capabilities of AI systems, including Large Language Models (LLMs) and Vision Models, typically ignores the impact of uncertainty in the underlying ground truth answers from experts. This ambiguity is particularly consequential in medicine where uncertainty is pervasive. In this paper, we introduce a probabilistic paradigm to theoretically explain how high certainty in ground truth answers is almost always necessary for even an expert to achieve high scores, whereas in datasets with high variation in ground truth answers there may be little difference between a random labeller and an expert. Therefore, ignoring uncertainty in ground truth evaluation data can result in the misleading conclusion that a non-expert has similar performance to that of an expert. Using the probabilistic paradigm, we thus bring forth the concepts of expected accuracy and expected F1 to estimate the score an expert human or system can achieve given ground truth answer variability.
  Our work leads to the recommendation that when establishing the capability of a system, results should be stratified by probability of the ground truth answer, typically measured by the agreement rate of ground truth experts. Stratification becomes critical when the overall performance drops below a threshold of 80%. Under stratified evaluation, performance comparison becomes more reliable in high certainty bins, mitigating the effect of the key confounding factor -- uncertainty.

</details>


### [12] [Explainable AI: Learning from the Learners](https://arxiv.org/abs/2601.05525)
*Ricardo Vinuesa,Steven L. Brunton,Gianmarco Mengaldo*

Main category: cs.AI

TL;DR: 论文提出将可解释人工智能与因果推理结合，实现"向学习者学习"的框架，用于科学发现、优化和认证，并讨论了该方法的挑战与前景。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能在科学和工程任务中已超越人类，但其内部表示往往不透明，需要可解释性来理解AI的决策过程，促进人机协作。

Method: 结合可解释人工智能和因果推理，利用基础模型和可解释性方法提取因果机制，指导稳健的设计与控制，支持高风险应用中的信任与问责。

Result: 展示了该方法在发现、优化和认证三个领域的应用潜力，能够从AI模型中提取有价值的科学洞见和工程知识。

Conclusion: XAI可作为科学与工程中人机协作的统一框架，但需要解决解释的忠实性、泛化性和可用性等挑战。

Abstract: Artificial intelligence now outperforms humans in several scientific and engineering tasks, yet its internal representations often remain opaque. In this Perspective, we argue that explainable artificial intelligence (XAI), combined with causal reasoning, enables {\it learning from the learners}. Focusing on discovery, optimization and certification, we show how the combination of foundation models and explainability methods allows the extraction of causal mechanisms, guides robust design and control, and supports trust and accountability in high-stakes applications. We discuss challenges in faithfulness, generalization and usability of explanations, and propose XAI as a unifying framework for human-AI collaboration in science and engineering.

</details>


### [13] [Safety Not Found (404): Hidden Risks of LLM-Based Robotics Decision Making](https://arxiv.org/abs/2601.05529)
*Jua Han,Jaeyoon Seo,Jungbin Min,Jean Oh,Jihie Kim*

Main category: cs.AI

TL;DR: 论文通过火灾疏散场景评估LLM在安全关键系统中的决策能力，发现即使99%准确率仍存在灾难性风险，当前LLM不适合直接部署于安全关键机器人系统。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在机器人决策中日益重要，AI系统在安全关键环境中的单个错误可能导致生命损失，迫切需要系统评估LLM在灾难性错误场景下的表现。

Method: 通过火灾疏散场景的定性评估识别关键失败案例，设计七项定量评估任务：完整信息任务（使用ASCII地图）、不完整信息任务（需要推断缺失上下文）、安全导向空间推理任务（使用自然语言评估安全决策）。

Result: 多个模型在ASCII导航中成功率0%，在模拟消防演习中模型指导机器人走向危险区域而非紧急出口，1%失败率在机器人系统中可能升级为灾难性后果。

Conclusion: 当前LLM尚未准备好直接部署于安全关键系统，99%准确率在机器人领域具有误导性，绝对依赖LLM会带来不可接受的风险。

Abstract: One mistake by an AI system in a safety-critical setting can cost lives. As Large Language Models (LLMs) become integral to robotics decision-making, the physical dimension of risk grows; a single wrong instruction can directly endanger human safety. This paper addresses the urgent need to systematically evaluate LLM performance in scenarios where even minor errors are catastrophic. Through a qualitative evaluation of a fire evacuation scenario, we identified critical failure cases in LLM-based decision-making. Based on these, we designed seven tasks for quantitative assessment, categorized into: Complete Information, Incomplete Information, and Safety-Oriented Spatial Reasoning (SOSR). Complete information tasks utilize ASCII maps to minimize interpretation ambiguity and isolate spatial reasoning from visual processing. Incomplete information tasks require models to infer missing context, testing for spatial continuity versus hallucinations. SOSR tasks use natural language to evaluate safe decision-making in life-threatening contexts. We benchmark various LLMs and Vision-Language Models (VLMs) across these tasks. Beyond aggregate performance, we analyze the implications of a 1% failure rate, highlighting how "rare" errors escalate into catastrophic outcomes. Results reveal serious vulnerabilities: several models achieved a 0% success rate in ASCII navigation, while in a simulated fire drill, models instructed robots to move toward hazardous areas instead of emergency exits. Our findings lead to a sobering conclusion: current LLMs are not ready for direct deployment in safety-critical systems. A 99% accuracy rate is dangerously misleading in robotics, as it implies one out of every hundred executions could result in catastrophic harm. We demonstrate that even state-of-the-art models cannot guarantee safety, and absolute reliance on them creates unacceptable risks.

</details>


### [14] [WildSci: Advancing Scientific Reasoning from In-the-Wild Literature](https://arxiv.org/abs/2601.05567)
*Tengxiao Liu,Deepak Nathani,Zekun Li,Kevin Yang,William Yang Wang*

Main category: cs.AI

TL;DR: WildSci是一个从同行评审文献自动合成的多领域科学问题数据集，用于训练LLM的科学推理能力，通过强化学习微调在多个科学基准上取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理研究主要集中在数学和编程等有高质量数据和客观评估指标的领域，而在医学、材料科学等科学领域进展有限，主要受限于数据集覆盖不足和科学问题的开放性复杂性。

Method: 1. 构建WildSci数据集：从同行评审文献自动合成科学问题，覆盖9个科学学科和26个子领域；2. 将复杂科学推理任务转化为多项选择题格式；3. 应用强化学习微调模型；4. 分析训练动态，包括领域特定性能变化、响应行为和泛化趋势。

Result: 在多个科学基准测试上的实验证明了数据集和方法的有效性。WildSci数据集已公开发布，支持科学推理研究的可扩展和可持续发展。

Conclusion: WildSci通过自动合成科学问题数据集和强化学习微调，有效提升了LLM在复杂科学领域的推理能力，为科学推理研究提供了可扩展的解决方案。

Abstract: Recent progress in large language model (LLM) reasoning has focused on domains like mathematics and coding, where abundant high-quality data and objective evaluation metrics are readily available. In contrast, progress in LLM reasoning models remains limited in scientific domains such as medicine and materials science due to limited dataset coverage and the inherent complexity of open-ended scientific questions. To address these challenges, we introduce WildSci, a new dataset of domain-specific science questions automatically synthesized from peer-reviewed literature, covering 9 scientific disciplines and 26 subdomains. By framing complex scientific reasoning tasks in a multiple-choice format, we enable scalable training with well-defined reward signals. We further apply reinforcement learning to finetune models on these data and analyze the resulting training dynamics, including domain-specific performance changes, response behaviors, and generalization trends. Experiments on a suite of scientific benchmarks demonstrate the effectiveness of our dataset and approach. We release WildSci to enable scalable and sustainable research in scientific reasoning, available at https://huggingface.co/datasets/JustinTX/WildSci.

</details>


### [15] [Crisis-Bench: Benchmarking Strategic Ambiguity and Reputation Management in Large Language Models](https://arxiv.org/abs/2601.05570)
*Cooper Lin,Maohao Ran,Yanting Zhang,Zhenglin Wan,Hongwei Fan,Yibo Xu,Yike Guo,Wei Xue,Jun Song*

Main category: cs.AI

TL;DR: 论文提出Crisis-Bench基准，用于评估LLM在需要战略模糊性和信息保留的专业领域（如危机公关）中的表现，揭示通用安全对齐与专业实用性之间的冲突。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的安全对齐主要针对通用助手的普遍帮助性和诚实性，形成了僵化的"童子军"道德观。这种一刀切的伦理框架对需要战略模糊性和信息保留的专业领域（如公共关系、谈判、危机管理）施加了"透明度税"，限制了LLM在这些专业场景中的实用性。

Method: 引入Crisis-Bench基准：一个多智能体部分可观察马尔可夫决策过程（POMDP），模拟80个不同行业（8个行业）的危机情景。设置基于LLM的公关代理在7天动态企业危机模拟中运作，严格分离私人和公共叙事状态以强制执行信息不对称。采用"裁决者-市场循环"评估指标：公众情绪被裁决并转化为模拟股价，创建现实的经济激励结构。

Result: 结果揭示了关键二分法：一些模型因伦理关切而妥协，而另一些模型则展现出马基雅维利式的、合法的战略保留能力以稳定模拟股价。Crisis-Bench首次为评估"声誉管理"能力提供了量化框架。

Conclusion: 论文主张从僵化的道德绝对主义转向情境感知的专业对齐，为LLM在需要战略模糊性的专业领域中的评估和发展提供了新框架。

Abstract: Standard safety alignment optimizes Large Language Models (LLMs) for universal helpfulness and honesty, effectively instilling a rigid "Boy Scout" morality. While robust for general-purpose assistants, this one-size-fits-all ethical framework imposes a "transparency tax" on professional domains requiring strategic ambiguity and information withholding, such as public relations, negotiation, and crisis management. To measure this gap between general safety and professional utility, we introduce Crisis-Bench, a multi-agent Partially Observable Markov Decision Process (POMDP) that evaluates LLMs in high-stakes corporate crises. Spanning 80 diverse storylines across 8 industries, Crisis-Bench tasks an LLM-based Public Relations (PR) Agent with navigating a dynamic 7-day corporate crisis simulation while managing strictly separated Private and Public narrative states to enforce rigorous information asymmetry. Unlike traditional benchmarks that rely on static ground truths, we introduce the Adjudicator-Market Loop: a novel evaluation metric where public sentiment is adjudicated and translated into a simulated stock price, creating a realistic economic incentive structure. Our results expose a critical dichotomy: while some models capitulate to ethical concerns, others demonstrate the capacity for Machiavellian, legitimate strategic withholding in order to stabilize the simulated stock price. Crisis-Bench provides the first quantitative framework for assessing "Reputation Management" capabilities, arguing for a shift from rigid moral absolutism to context-aware professional alignment.

</details>


### [16] [Reinforcement Learning of Large Language Models for Interpretable Credit Card Fraud Detection](https://arxiv.org/abs/2601.05578)
*Cooper Lin,Yanting Zhang,Maohao Ran,Wei Xue,Hongwei Fan,Yibo Xu,Zhenglin Wan,Sirui Han,Yike Guo,Jun Song*

Main category: cs.AI

TL;DR: 本文提出了一种使用强化学习对轻量级语言模型进行后训练的方法，专门用于电子商务欺诈检测任务，仅使用原始交易数据，在真实数据集上取得了显著的F1分数提升。


<details>
  <summary>Details</summary>
Motivation: 电子商务平台和支付解决方案提供商面临日益复杂的欺诈方案，但尽管大型语言模型（LLMs）在理论上具有潜力，其在真实金融欺诈检测中的应用尚未充分探索，处理特定领域电子商务交易数据的实际效果也缺乏实证验证。

Method: 提出了一种新颖方法，使用强化学习（RL）对轻量级语言模型进行后训练，专门用于欺诈检测任务。采用Group Sequence Policy Optimization（GSPO）算法结合基于规则的奖励系统，在来自中国全球支付解决方案公司的真实交易数据集上微调不同规模的语言模型。

Result: 实验结果表明该方法有效，后训练的语言模型在保留测试数据上实现了显著的F1分数提升。性能改进主要归因于强化学习固有的探索机制，使模型能够发现超越传统工程特征的新型欺诈指标。

Conclusion: 该方法成功弥合了传统机器学习局限性与LLMs在欺诈检测中未开发潜力之间的差距，通过强化学习框架使语言模型能够探索文本交易数据中嵌入的多样化信任和风险信号。

Abstract: E-commerce platforms and payment solution providers face increasingly sophisticated fraud schemes, ranging from identity theft and account takeovers to complex money laundering operations that exploit the speed and anonymity of digital transactions. However, despite their theoretical promise, the application of Large Language Models (LLMs) to fraud detection in real-world financial contexts remains largely unexploited, and their practical effectiveness in handling domain-specific e-commerce transaction data has yet to be empirically validated. To bridge this gap between conventional machine learning limitations and the untapped potential of LLMs in fraud detection, this paper proposes a novel approach that employs Reinforcement Learning (RL) to post-train lightweight language models specifically for fraud detection tasks using only raw transaction data. We utilize the Group Sequence Policy Optimization (GSPO) algorithm combined with a rule-based reward system to fine-tune language models of various sizes on a real-life transaction dataset provided by a Chinese global payment solution company. Through this reinforcement learning framework, the language models are encouraged to explore diverse trust and risk signals embedded within the textual transaction data, including patterns in customer information, shipping details, product descriptions, and order history. Our experimental results demonstrate the effectiveness of this approach, with post-trained language models achieving substantial F1-score improvements on held-out test data. Our findings demonstrate that the observed performance improvements are primarily attributable to the exploration mechanism inherent in reinforcement learning, which allows models to discover novel fraud indicators beyond those captured by traditional engineered features.

</details>


### [17] [A Causal Information-Flow Framework for Unbiased Learning-to-Rank](https://arxiv.org/abs/2601.05590)
*Haoming Gong,Qingyao Ai,Zhihao Tao,Yongfeng Zhang*

Main category: cs.AI

TL;DR: 提出基于因果学习的新型排序框架，结合结构因果模型和信息论工具，解决点击数据中的多种偏差问题，提高排序性能。


<details>
  <summary>Details</summary>
Motivation: 网页搜索和推荐系统中，用户点击数据存在严重偏差（位置偏差、选择偏差、信任偏差），现有无偏学习排序方法主要纠正位置偏差，无法测量剩余偏差、提供风险保证或联合处理多种偏差源。

Method: 结合结构因果模型和信息论工具，使用条件互信息测量偏差泄漏，将其作为正则化项减少偏差，并整合双重稳健估计器确保可靠的风险估计。

Result: 在标准学习排序基准测试中，该方法持续减少测量的偏差泄漏并提高排序性能，特别是在位置偏差和信任偏差等多种偏差强烈交互的现实场景中。

Conclusion: 提出的因果学习框架有效解决了点击数据中的多种偏差问题，通过结构因果模型识别真实相关性信号，利用信息论工具减少偏差泄漏，提高了排序模型的准确性和可靠性。

Abstract: In web search and recommendation systems, user clicks are widely used to train ranking models. However, click data is heavily biased, i.e., users tend to click higher-ranked items (position bias), choose only what was shown to them (selection bias), and trust top results more (trust bias). Without explicitly modeling these biases, the true relevance of ranked items cannot be correctly learned from clicks. Existing Unbiased Learning-to-Rank (ULTR) methods mainly correct position bias and rely on propensity estimation, but they cannot measure remaining bias, provide risk guarantees, or jointly handle multiple bias sources. To overcome these challenges, this paper introduces a novel causal learning-based ranking framework that extends ULTR by combining Structural Causal Models (SCMs) with information-theoretic tools. SCMs specify how clicks are generated and help identify the true relevance signal from click data, while conditional mutual information, measures how much bias leaks into the
  learned relevance estimates. We use this leakage measure to define a rigorous notion of disentanglement and include it as a regularizer during model training to reduce bias. In addition, we incorporate a causal inference estimator, i.e., doubly robust estimator, to ensure more reliable risk estimation. Experiments on standard Learning-to-Rank benchmarks show that our method consistently reduces measured bias leakage and improves ranking performance, especially in realistic scenarios where multiple biases-such as position and trust bias-interact strongly.

</details>


### [18] [Cumulative Path-Level Semantic Reasoning for Inductive Knowledge Graph Completion](https://arxiv.org/abs/2601.05629)
*Jiapu Wang,Xinghe Cheng,Zezheng Wu,Ruiqi Ma,Rui Wang,Zhichao Yan,Haoran Luo,Yuhao Jiang,Kai Sun*

Main category: cs.AI

TL;DR: CPSR框架通过查询依赖掩码和全局语义评分模块，同时捕获知识图谱的结构和语义信息，提升归纳式知识图谱补全性能


<details>
  <summary>Details</summary>
Motivation: 传统KGC方法在处理新兴实体时效果不佳，现有归纳式KGC方法虽然能处理新兴实体和关系，但仍面临噪声结构信息干扰和难以捕获长距离依赖的挑战

Method: 提出CPSR框架：1) 查询依赖掩码模块自适应地屏蔽噪声结构信息，保留与目标相关的关键信息；2) 全局语义评分模块评估推理路径中节点的个体贡献和集体影响

Result: 实验结果表明CPSR实现了最先进的性能

Conclusion: CPSR通过同时捕获知识图谱的结构和语义信息，有效解决了归纳式知识图谱补全中的噪声干扰和长距离依赖问题

Abstract: Conventional Knowledge Graph Completion (KGC) methods aim to infer missing information in incomplete Knowledge Graphs (KGs) by leveraging existing information, which struggle to perform effectively in scenarios involving emerging entities. Inductive KGC methods can handle the emerging entities and relations in KGs, offering greater dynamic adaptability. While existing inductive KGC methods have achieved some success, they also face challenges, such as susceptibility to noisy structural information during reasoning and difficulty in capturing long-range dependencies in reasoning paths. To address these challenges, this paper proposes the Cumulative Path-Level Semantic Reasoning for inductive knowledge graph completion (CPSR) framework, which simultaneously captures both the structural and semantic information of KGs to enhance the inductive KGC task. Specifically, the proposed CPSR employs a query-dependent masking module to adaptively mask noisy structural information while retaining important information closely related to the targets. Additionally, CPSR introduces a global semantic scoring module that evaluates both the individual contributions and the collective impact of nodes along the reasoning path within KGs. The experimental results demonstrate that CPSR achieves state-of-the-art performance.

</details>


### [19] [GenCtrl -- A Formal Controllability Toolkit for Generative Models](https://arxiv.org/abs/2601.05637)
*Emily Cheng,Carmen Amo Alonso,Federico Danieli,Arno Blaas,Luca Zappella,Pau Rodriguez,Xavier Suau*

Main category: cs.AI

TL;DR: 该论文提出了一个理论框架来形式化分析生成模型的可控性，通过对话场景下的可控集估计算法，提供了样本复杂度相关的误差保证，发现模型可控性实际上很脆弱。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型变得无处不在，需要细粒度控制生成过程。然而，尽管从提示到微调的各种控制方法不断涌现，一个基本问题仍未解决：这些模型是否真正可控？

Method: 将人机交互建模为控制过程，提出新算法估计对话场景中模型的可控集，提供分布无关的近似正确误差界限，仅需输出有界性假设，适用于任何黑盒非线性控制系统（即任何生成模型）。

Result: 在语言模型和文生图模型的控制任务上实证验证，发现模型可控性出人意料地脆弱，且高度依赖实验设置。

Conclusion: 需要严格的可控性分析，将重点从简单尝试控制转向首先理解其基本限制。

Abstract: As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.

</details>


### [20] [HAG: Hierarchical Demographic Tree-based Agent Generation for Topic-Adaptive Simulation](https://arxiv.org/abs/2601.05656)
*Rongxin Chen,Tianyu Wu,Bingbing Xu,Xiucheng Xu,Huawei Shen*

Main category: cs.AI

TL;DR: HAG框架通过两阶段决策过程实现高质量智能体初始化：首先利用世界知识模型构建主题自适应树实现宏观分布对齐，然后基于真实数据进行实例化和增强确保微观一致性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于代理的建模中，智能体初始化方法存在两大问题：静态数据检索方法无法适应未见主题，而基于LLM的生成方法缺乏宏观分布意识，导致微观个体属性与现实不一致。需要一种既能适应不同主题又能保证宏观分布对齐和微观一致性的框架。

Method: 提出HAG分层智能体生成框架，将群体生成形式化为两阶段决策过程：1）使用世界知识模型推断分层条件概率构建主题自适应树，实现宏观分布对齐；2）基于真实数据进行实例化和智能体增强，确保微观一致性。

Result: 实验表明HAG显著优于代表性基线方法，平均减少群体对齐误差37.7%，提升社会学一致性18.8%。建立了多领域基准和全面的PACE评估框架。

Conclusion: HAG框架通过分层方法有效解决了智能体初始化中的主题适应性和分布一致性问题，为基于代理的建模提供了高质量的智能体生成解决方案。

Abstract: High-fidelity agent initialization is crucial for credible Agent-Based Modeling across diverse domains. A robust framework should be Topic-Adaptive, capturing macro-level joint distributions while ensuring micro-level individual rationality. Existing approaches fall into two categories: static data-based retrieval methods that fail to adapt to unseen topics absent from the data, and LLM-based generation methods that lack macro-level distribution awareness, resulting in inconsistencies between micro-level persona attributes and reality. To address these problems, we propose HAG, a Hierarchical Agent Generation framework that formalizes population generation as a two-stage decision process. Firstly, utilizing a World Knowledge Model to infer hierarchical conditional probabilities to construct the Topic-Adaptive Tree, achieving macro-level distribution alignment. Then, grounded real-world data, instantiation and agentic augmentation are carried out to ensure micro-level consistency. Given the lack of specialized evaluation, we establish a multi-domain benchmark and a comprehensive PACE evaluation framework. Extensive experiments show that HAG significantly outperforms representative baselines, reducing population alignment errors by an average of 37.7% and enhancing sociological consistency by 18.8%.

</details>


### [21] [CHDP: Cooperative Hybrid Diffusion Policies for Reinforcement Learning in Parameterized Action Space](https://arxiv.org/abs/2601.05675)
*Bingyi Liu,Jinbo He,Haiyong Shi,Enshu Wang,Weizhen Han,Jingxiang Hao,Peixi Wang,Zhuangzhuang Zhang*

Main category: cs.AI

TL;DR: CHDP框架通过将混合动作空间建模为合作游戏，使用离散和连续扩散策略协同工作，结合序列更新方案和低维编码本，在混合动作基准上实现了SOTA性能提升达19.3%


<details>
  <summary>Details</summary>
Motivation: 混合动作空间（结合离散选择和连续参数）在机器人控制和游戏AI中很常见，但现有方法存在策略表达能力有限、高维场景可扩展性差的问题，需要新的建模和优化方法

Method: 1) 将混合动作空间问题视为完全合作游戏，提出CHDP框架；2) 使用离散和连续扩散策略两个协同智能体；3) 连续策略以离散动作表示为条件；4) 采用序列更新方案避免更新冲突；5) 构建编码本将高维离散动作空间嵌入低维潜在空间；6) 设计基于Q函数的引导机制对齐编码本嵌入和离散策略表示

Result: 在具有挑战性的混合动作基准测试中，CHDP比现有最优方法的成功率提升了高达19.3%

Conclusion: CHDP框架通过协同设计、序列更新和低维编码本，有效解决了混合动作空间的建模和优化挑战，在策略表达能力和可扩展性方面取得了显著改进

Abstract: Hybrid action space, which combines discrete choices and continuous parameters, is prevalent in domains such as robot control and game AI. However, efficiently modeling and optimizing hybrid discrete-continuous action space remains a fundamental challenge, mainly due to limited policy expressiveness and poor scalability in high-dimensional settings. To address this challenge, we view the hybrid action space problem as a fully cooperative game and propose a \textbf{Cooperative Hybrid Diffusion Policies (CHDP)} framework to solve it. CHDP employs two cooperative agents that leverage a discrete and a continuous diffusion policy, respectively. The continuous policy is conditioned on the discrete action's representation, explicitly modeling the dependency between them. This cooperative design allows the diffusion policies to leverage their expressiveness to capture complex distributions in their respective action spaces. To mitigate the update conflicts arising from simultaneous policy updates in this cooperative setting, we employ a sequential update scheme that fosters co-adaptation. Moreover, to improve scalability when learning in high-dimensional discrete action space, we construct a codebook that embeds the action space into a low-dimensional latent space. This mapping enables the discrete policy to learn in a compact, structured space. Finally, we design a Q-function-based guidance mechanism to align the codebook's embeddings with the discrete policy's representation during training. On challenging hybrid action benchmarks, CHDP outperforms the state-of-the-art method by up to $19.3\%$ in success rate.

</details>


### [22] [Circular Reasoning: Understanding Self-Reinforcing Loops in Large Reasoning Models](https://arxiv.org/abs/2601.05693)
*Zenghao Duan,Liang Pang,Zihao Wei,Wenbin Duan,Yuxin Tian,Shicheng Xu,Jingcheng Deng,Zhiyi Yin,Xueqi Cheng*

Main category: cs.AI

TL;DR: 论文提出"循环推理"这一新失败模式，开发LoopBench数据集分析数值循环和陈述循环，揭示其机制为状态崩溃和V型注意力机制，并利用CUSUM算法实现早期预测。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在测试时缩放中经常遇到重复循环，导致计算浪费和推理失败。传统模型退化理论无法解释这种自我强化的循环现象，需要系统性分析这一新的失败模式。

Method: 1) 引入LoopBench数据集捕捉数值循环和陈述循环两种类型；2) 从机制上描述循环推理为状态崩溃，语义重复先于文本重复；3) 揭示推理僵局触发循环开始，V型注意力机制维持循环；4) 使用累积和(CUSUM)算法捕捉前兆进行早期预测。

Result: 实验验证了CUSUM算法在不同大型推理模型中的准确性，阐明了长链推理的稳定性问题，成功实现了循环的早期预测。

Conclusion: 循环推理是大型推理模型的一种独特失败模式，通过理解其机制和开发早期检测方法，可以提高模型推理的可靠性和效率。

Abstract: Despite the success of test-time scaling, Large Reasoning Models (LRMs) frequently encounter repetitive loops that lead to computational waste and inference failure. In this paper, we identify a distinct failure mode termed Circular Reasoning. Unlike traditional model degeneration, this phenomenon manifests as a self-reinforcing trap where generated content acts as a logical premise for its own recurrence, compelling the reiteration of preceding text. To systematically analyze this phenomenon, we introduce LoopBench, a dataset designed to capture two distinct loop typologies: numerical loops and statement loops. Mechanistically, we characterize circular reasoning as a state collapse exhibiting distinct boundaries, where semantic repetition precedes textual repetition. We reveal that reasoning impasses trigger the loop onset, which subsequently persists as an inescapable cycle driven by a self-reinforcing V-shaped attention mechanism. Guided by these findings, we employ the Cumulative Sum (CUSUM) algorithm to capture these precursors for early loop prediction. Experiments across diverse LRMs validate its accuracy and elucidate the stability of long-chain reasoning.

</details>


### [23] [Logic-Parametric Neuro-Symbolic NLI: Controlling Logical Formalisms for Verifiable LLM Reasoning](https://arxiv.org/abs/2601.05705)
*Ali Farjami,Luca Redondi,Marco Valentino*

Main category: cs.AI

TL;DR: 提出逻辑参数化框架，将逻辑作为可控组件嵌入高阶逻辑，比较经典与非经典形式化方法，在规范性推理中展示逻辑内部策略优于外部策略


<details>
  <summary>Details</summary>
Motivation: 现有结合大语言模型与定理证明器的方法依赖固定逻辑形式化，限制了鲁棒性和适应性。需要将逻辑作为可控组件而非静态背景来处理

Method: 使用LogiKEy方法将经典和非经典逻辑形式化嵌入高阶逻辑，构建逻辑参数化框架，比较逻辑外部方法（通过公理编码规范要求）与逻辑内部方法（从逻辑内置结构中产生规范模式）

Result: 逻辑内部策略能持续提升性能并产生更高效的混合证明；逻辑有效性具有领域依赖性：一阶逻辑擅长常识推理，道义和模态逻辑在伦理领域表现优异

Conclusion: 将逻辑作为神经符号架构中的一等参数化元素，能实现更鲁棒、模块化和适应性强的推理系统

Abstract: Large language models (LLMs) and theorem provers (TPs) can be effectively combined for verifiable natural language inference (NLI). However, existing approaches rely on a fixed logical formalism, a feature that limits robustness and adaptability. We propose a logic-parametric framework for neuro-symbolic NLI that treats the underlying logic not as a static background, but as a controllable component. Using the LogiKEy methodology, we embed a range of classical and non-classical formalisms into higher-order logic (HOL), enabling a systematic comparison of inference quality, explanation refinement, and proof behavior. We focus on normative reasoning, where the choice of logic has significant implications. In particular, we compare logic-external approaches, where normative requirements are encoded via axioms, with logic-internal approaches, where normative patterns emerge from the logic's built-in structure. Extensive experiments demonstrate that logic-internal strategies can consistently improve performance and produce more efficient hybrid proofs for NLI. In addition, we show that the effectiveness of a logic is domain-dependent, with first-order logic favouring commonsense reasoning, while deontic and modal logics excel in ethical domains. Our results highlight the value of making logic a first-class, parametric element in neuro-symbolic architectures for more robust, modular, and adaptable reasoning.

</details>


### [24] [Overcoming Joint Intractability with Lossless Hierarchical Speculative Decoding](https://arxiv.org/abs/2601.05724)
*Yuxuan Zhou,Fei Huang,Heng Li,Fengyi Wu,Tianyu Wang,Jianwei Zhang,Junyang Lin,Zhi-Qi Cheng*

Main category: cs.AI

TL;DR: HSD是一种无损验证方法，通过分层平衡概率质量来解决联合不可处理性问题，显著提升接受令牌数量，在EAGLE-3中实现超过12%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 验证是推测解码中提高推理速度同时保持分布保真度的关键瓶颈。现有方法依赖近似或受限于部分信息，难以处理联合不可处理性问题。

Method: 提出分层推测解码（HSD），通过平衡可访问分支上的超额和不足概率质量，提供可证明的无损验证方法。

Result: HSD在不同模型家族和基准测试中持续提高接受率，集成到EAGLE-3中实现超过12%的性能提升，达到最先进的解码效率。

Conclusion: HSD具有强大的可解释性和通用性，可轻松集成到各种推测解码框架中，在不损害分布保真度的情况下显著提升解码效率。

Abstract: Verification is a key bottleneck in improving inference speed while maintaining distribution fidelity in Speculative Decoding. Recent work has shown that sequence-level verification leads to a higher number of accepted tokens compared to token-wise verification. However, existing solutions often rely on surrogate approximations or are constrained by partial information, struggling with joint intractability. In this work, we propose Hierarchical Speculative Decoding (HSD), a provably lossless verification method that significantly boosts the expected number of accepted tokens and overcomes joint intractability by balancing excess and deficient probability mass across accessible branches. Our extensive large-scale experiments demonstrate that HSD yields consistent improvements in acceptance rates across diverse model families and benchmarks. Moreover, its strong explainability and generality make it readily integrable into a wide range of speculative decoding frameworks. Notably, integrating HSD into EAGLE-3 yields over a 12% performance gain, establishing state-of-the-art decoding efficiency without compromising distribution fidelity. Code is available at https://github.com/ZhouYuxuanYX/Hierarchical-Speculative-Decoding.

</details>


### [25] [PII-VisBench: Evaluating Personally Identifiable Information Safety in Vision Language Models Along a Continuum of Visibility](https://arxiv.org/abs/2601.05739)
*G M Shahariar,Zabir Al Nazi,Md Olid Hasan Bhuiyan,Zhouxing Shi*

Main category: cs.AI

TL;DR: PII-VisBench：一个评估视觉语言模型在个人在线可见度连续体上PII泄露风险的基准，发现模型对高可见度主体更容易泄露PII信息。


<details>
  <summary>Details</summary>
Motivation: 现有VLM隐私评估将隐私视为静态提取任务，忽略了主体在线存在（可获取数据量）对隐私对齐的影响。需要评估在线可见度连续体上的VLM安全性。

Method: 构建PII-VisBench基准，包含4000个独特探针，将200个主体按在线信息程度分为高、中、低、零可见度四类。评估18个开源VLM（0.3B-32B），使用拒绝率和条件PII泄露率两个关键指标。

Result: 模型呈现一致模式：随着主体可见度降低，拒绝率增加，PII泄露率下降（从高可见度的9.10%降至低可见度的5.34%）。模型对高可见度主体更容易泄露PII，存在显著的模型家族异质性和PII类型差异。改述和越狱式提示暴露了攻击和模型依赖的失败。

Conclusion: 需要基于可见度的安全性评估和训练干预，以解决VLM在不同在线存在程度下的PII泄露风险差异。

Abstract: Vision Language Models (VLMs) are increasingly integrated into privacy-critical domains, yet existing evaluations of personally identifiable information (PII) leakage largely treat privacy as a static extraction task and ignore how a subject's online presence--the volume of their data available online--influences privacy alignment. We introduce PII-VisBench, a novel benchmark containing 4000 unique probes designed to evaluate VLM safety through the continuum of online presence. The benchmark stratifies 200 subjects into four visibility categories: high, medium, low, and zero--based on the extent and nature of their information available online. We evaluate 18 open-source VLMs (0.3B-32B) based on two key metrics: percentage of PII probing queries refused (Refusal Rate) and the fraction of non-refusal responses flagged for containing PII (Conditional PII Disclosure Rate). Across models, we observe a consistent pattern: refusals increase and PII disclosures decrease (9.10% high to 5.34% low) as subject visibility drops. We identify that models are more likely to disclose PII for high-visibility subjects, alongside substantial model-family heterogeneity and PII-type disparities. Finally, paraphrasing and jailbreak-style prompts expose attack and model-dependent failures, motivating visibility-aware safety evaluation and training interventions.

</details>


### [26] [DynaDebate: Breaking Homogeneity in Multi-Agent Debate with Dynamic Path Generation](https://arxiv.org/abs/2601.05746)
*Zhenghao Li,Zhi Zheng,Wei Chen,Jielun Zhao,Yong Chen,Tong Xu,Enhong Chen*

Main category: cs.AI

TL;DR: DynaDebate提出动态多智能体辩论框架，通过路径生成、过程中心辩论和触发验证解决传统多智能体辩论中初始化相同导致错误重复和简单多数投票的问题。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体辩论框架存在未引导的初始化问题，导致智能体采用相同的推理路径并犯相同错误，最终辩论退化为简单多数投票，无法有效提升协作能力。

Method: 提出DynaDebate框架，包含三个核心机制：1) 动态路径生成与分配，使用专用路径生成智能体产生多样化逻辑路径；2) 过程中心辩论，关注逐步逻辑批判而非表面结果投票；3) 触发式验证智能体，在分歧时激活并使用外部工具客观解决僵局。

Result: 大量实验表明DynaDebate在各种基准测试中表现优异，超越了现有的最先进多智能体辩论方法。

Conclusion: DynaDebate通过动态路径生成、过程中心辩论和触发验证机制，有效解决了多智能体辩论中的同质化问题和简单多数投票退化，显著提升了协作决策和复杂问题解决能力。

Abstract: Recent years have witnessed the rapid development of Large Language Model-based Multi-Agent Systems (MAS), which excel at collaborative decision-making and complex problem-solving. Recently, researchers have further investigated Multi-Agent Debate (MAD) frameworks, which enhance the reasoning and collaboration capabilities of MAS through information exchange and debate among multiple agents. However, existing approaches often rely on unguided initialization, causing agents to adopt identical reasoning paths that lead to the same errors. As a result, effective debate among agents is hindered, and the final outcome frequently degenerates into simple majority voting. To solve the above problem, in this paper, we introduce Dynamic Multi-Agent Debate (DynaDebate), which enhances the effectiveness of multi-agent debate through three key mechanisms: (1) Dynamic Path Generation and Allocation, which employs a dedicated Path Generation Agent to generate diverse and logical solution paths with adaptive redundancy; (2) Process-Centric Debate, which shifts the focus from surface-level outcome voting to rigorous step-by-step logic critique to ensure process correctness; (3) A Trigger-Based Verification Agent, which is activated upon disagreement and uses external tools to objectively resolve deadlocks. Extensive experiments demonstrate that DynaDebate achieves superior performance across various benchmarks, surpassing existing state-of-the-art MAD methods.

</details>


### [27] [From Off-Policy to On-Policy: Enhancing GUI Agents via Bi-level Expert-to-Policy Assimilation](https://arxiv.org/abs/2601.05787)
*Zezhou Wang,Ziyun Zhang,Xiaoyi Zhang,Zhuzhong Qian,Yan Lu*

Main category: cs.AI

TL;DR: BEPA方法通过双层专家轨迹同化，将静态专家轨迹转化为策略对齐的指导，显著提升了端到端GUI操作策略在OSWorld等基准上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前GUI数据集（如OSWorld）存在两个瓶颈：1）只有几百个可验证的交互任务和环境；2）专家轨迹需要通过与环境交互收集，难以扩展。因此需要研究如何利用少量现有专家轨迹通过强化学习训练端到端策略。

Method: 提出BEPA（双层专家到策略同化）方法：LEVEL-1通过基础策略生成自滚动可达轨迹，将静态专家轨迹转化为策略对齐的指导；LEVEL-2使用按任务动态更新的缓存进行RLVR（基于可验证奖励的强化学习）。

Result: 在OSWorld-Verified上，BEPA将UITARS1.5-7B的成功率从22.87%提升到32.13%，在保留测试集上从5.74%提升到10.30%，在MMBench-GUI和Online-Mind2Web上也有一致的性能提升。

Conclusion: BEPA方法有效解决了专家轨迹与学习策略之间的结构不匹配和分布偏移问题，显著提升了端到端GUI操作策略的性能，为利用有限专家数据训练高质量计算机使用代理提供了有效方案。

Abstract: Vision-language models are increasingly deployed as computer-use agents (CUAs) that operate desktops and browsers. Top-performing CUAs are framework-based systems that decompose planning and execution, while end-to-end screenshot-to-action policies are easier to deploy but lag behind on benchmarks such as OSWorld-Verified. GUI datasets like OSWorld pose two bottlenecks: they expose only a few hundred interactive, verifiable tasks and environments, and expert trajectories must be gathered by interacting with these environments, making such data hard to scale. We therefore ask how reinforcement learning from verifiable rewards (RLVR) can best exploit a small pool of exist expert trajectories to train end-to-end policies. Naively mixing these off-policy traces into on-policy RLVR is brittle: even after format conversion, expert trajectories exhibit structural mismatch and distribution shift from the learner. We propose BEPA (Bi-Level Expert-to-Policy Assimilation), which turns static expert traces into policy-aligned guidance via self-rolled reachable trajectories under the base policy (LEVEL-1) and a per-task, dynamically updated cache used in RLVR (LEVEL-2). On OSWorld-Verified, BEPA improves UITARS1.5-7B success from 22.87% to 32.13% and raises a held-out split from 5.74% to 10.30%, with consistent gains on MMBench-GUI and Online-Mind2Web. Our code and data are available at: https://github.com/LEON-gittech/Verl_GUI.git

</details>


### [28] [StackPlanner: A Centralized Hierarchical Multi-Agent System with Task-Experience Memory Management](https://arxiv.org/abs/2601.05890)
*Ruizhe Zhang,Xinke Jiang,Zhibang Yang,Zhixin Zhang,Jiaran Gao,Yuzhen Xiao,Hongbin Lai,Xu Chu,Junfeng Zhao,Yasha Wang*

Main category: cs.AI

TL;DR: StackPlanner是一个具有显式内存控制的分层多智能体框架，通过解耦高层协调与子任务执行，并利用结构化经验记忆和强化学习来检索和重用协调经验，解决了长期协作中的内存效率低下和协调经验无法重用的问题。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的多智能体系统在处理复杂知识密集型任务时表现出潜力，但集中式架构中的中央智能体由于缺乏内存管理，导致上下文膨胀、错误累积和跨任务泛化能力差，无法实现稳定的长期协作。

Method: 提出StackPlanner分层多智能体框架：1）通过主动任务级内存控制将高层协调与子任务执行解耦；2）使用结构化经验记忆和强化学习来检索和利用可重用的协调经验。

Result: 在多个深度搜索和智能体系统基准测试上的实验表明，该方法能够实现可靠的长期多智能体协作，验证了其有效性。

Conclusion: StackPlanner通过显式内存控制解决了多智能体系统中任务级内存效率低下和协调经验无法重用的问题，为可靠的长期多智能体协作提供了有效解决方案。

Abstract: Multi-agent systems based on large language models, particularly centralized architectures, have recently shown strong potential for complex and knowledge-intensive tasks. However, central agents often suffer from unstable long-horizon collaboration due to the lack of memory management, leading to context bloat, error accumulation, and poor cross-task generalization. To address both task-level memory inefficiency and the inability to reuse coordination experience, we propose StackPlanner, a hierarchical multi-agent framework with explicit memory control. StackPlanner addresses these challenges by decoupling high-level coordination from subtask execution with active task-level memory control, and by learning to retrieve and exploit reusable coordination experience via structured experience memory and reinforcement learning. Experiments on multiple deep-search and agent system benchmarks demonstrate the effectiveness of our approach in enabling reliable long-horizon multi-agent collaboration.

</details>


### [29] [TowerMind: A Tower Defence Game Learning Environment and Benchmark for LLM as Agents](https://arxiv.org/abs/2601.05899)
*Dawei Wang,Chengming Zhou,Di Zhao,Xinyuan Liu,Marci Chi Ma,Gary Ushaw,Richard Davison*

Main category: cs.AI

TL;DR: TowerMind：一个基于塔防游戏的轻量级多模态环境，用于评估大语言模型在实时策略游戏中的长期规划和决策能力


<details>
  <summary>Details</summary>
Motivation: 现有实时策略游戏环境要么计算需求高，要么缺乏文本观察支持，限制了其用于大语言模型评估。需要一种轻量级、多模态的环境来评估LLMs的长期规划和决策能力。

Method: 开发TowerMind环境，基于塔防游戏子类型，具有低计算需求和多模态观察空间（像素、文本、结构化游戏状态）。设计了五个基准关卡，在不同多模态输入设置下评估多个LLMs。

Result: 结果显示LLMs与人类专家在能力和幻觉维度上存在明显性能差距。实验揭示了LLMs的关键局限性：规划验证不足、决策缺乏多终局性、行动使用效率低。同时评估了两种经典强化学习算法。

Conclusion: TowerMind通过轻量级多模态设计补充了现有RTS游戏环境，为AI智能体领域引入了新的基准测试平台。

Abstract: Recent breakthroughs in Large Language Models (LLMs) have positioned them as a promising paradigm for agents, with long-term planning and decision-making emerging as core general-purpose capabilities for adapting to diverse scenarios and tasks. Real-time strategy (RTS) games serve as an ideal testbed for evaluating these two capabilities, as their inherent gameplay requires both macro-level strategic planning and micro-level tactical adaptation and action execution. Existing RTS game-based environments either suffer from relatively high computational demands or lack support for textual observations, which has constrained the use of RTS games for LLM evaluation. Motivated by this, we present TowerMind, a novel environment grounded in the tower defense (TD) subgenre of RTS games. TowerMind preserves the key evaluation strengths of RTS games for assessing LLMs, while featuring low computational demands and a multimodal observation space, including pixel-based, textual, and structured game-state representations. In addition, TowerMind supports the evaluation of model hallucination and provides a high degree of customizability. We design five benchmark levels to evaluate several widely used LLMs under different multimodal input settings. The results reveal a clear performance gap between LLMs and human experts across both capability and hallucination dimensions. The experiments further highlight key limitations in LLM behavior, such as inadequate planning validation, a lack of multifinality in decision-making, and inefficient action use. We also evaluate two classic reinforcement learning algorithms: Ape-X DQN and PPO. By offering a lightweight and multimodal design, TowerMind complements the existing RTS game-based environment landscape and introduces a new benchmark for the AI agent field. The source code is publicly available on GitHub(https://github.com/tb6147877/TowerMind).

</details>


### [30] [Open-Vocabulary 3D Instruction Ambiguity Detection](https://arxiv.org/abs/2601.05991)
*Jiayu Ding,Haoran Tang,Ge Li*

Main category: cs.AI

TL;DR: 提出首个开放词汇3D指令歧义检测任务，构建大规模基准数据集Ambi3D，并开发两阶段框架AmbiVer来解决现有3D大语言模型在歧义检测上的不足。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域（如手术场景），语言歧义可能导致严重后果，但现有具身AI研究大多忽视这一问题，假设指令清晰且专注于执行而非确认。为填补这一安全空白，需要解决3D场景中的指令歧义检测问题。

Method: 提出AmbiVer两阶段框架：第一阶段从多个视角收集显式视觉证据，第二阶段利用这些证据指导视觉语言模型判断指令歧义。同时构建了包含700多个3D场景和约22k指令的大规模基准数据集Ambi3D。

Result: 分析发现现有最先进的3D大语言模型难以可靠判断指令是否歧义。AmbiVer框架在实验中表现出有效性，证明了该任务的挑战性以及所提方法的优越性。

Conclusion: 该研究首次定义了开放词汇3D指令歧义检测任务，构建了相应基准数据集，并提出了有效的解决方案，为更安全、更可信的具身AI发展铺平了道路。

Abstract: In safety-critical domains, linguistic ambiguity can have severe consequences; a vague command like "Pass me the vial" in a surgical setting could lead to catastrophic errors. Yet, most embodied AI research overlooks this, assuming instructions are clear and focusing on execution rather than confirmation. To address this critical safety gap, we are the first to define Open-Vocabulary 3D Instruction Ambiguity Detection, a fundamental new task where a model must determine if a command has a single, unambiguous meaning within a given 3D scene. To support this research, we build Ambi3D, the large-scale benchmark for this task, featuring over 700 diverse 3D scenes and around 22k instructions. Our analysis reveals a surprising limitation: state-of-the-art 3D Large Language Models (LLMs) struggle to reliably determine if an instruction is ambiguous. To address this challenge, we propose AmbiVer, a two-stage framework that collects explicit visual evidence from multiple views and uses it to guide an vision-language model (VLM) in judging instruction ambiguity. Extensive experiments demonstrate the challenge of our task and the effectiveness of AmbiVer, paving the way for safer and more trustworthy embodied AI. Code and dataset available at https://jiayuding031020.github.io/ambi3d/.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [31] [The LLM Mirage: Economic Interests and the Subversion of Weaponization Controls](https://arxiv.org/abs/2601.05307)
*Ritwik Gupta,Andrew W. Reddie*

Main category: cs.CY

TL;DR: 论文批评美国AI安全政策过度关注大模型训练算力，提出了更全面的"AI武器化"定义和评估框架


<details>
  <summary>Details</summary>
Motivation: 美国AI安全政策存在"LLM幻象"问题，错误地认为国家安全风险与前沿语言模型的训练算力成正比。这种观点导致战略误判和监管不稳定，需要更全面的分析框架。

Method: 1. 分析"LLM幻象"的形成机制；2. 提出基于意图和能力的AI武器化定义，以国际人道法为基础；3. 设计基于实时基准测试的测量基础设施，覆盖AI三要素（数据、算法、算力）。

Result: 揭示了当前AI安全政策的两个主要缺陷：战略误判（对手可用专用系统获得武器化能力）和监管不稳定（算力阈值易受国内利益影响）。提出了更全面的评估框架。

Conclusion: 需要超越算力中心的政策框架，采用基于意图-能力的AI武器化定义，并建立覆盖数据、算法、算力的综合测量体系，才能制定有效的AI安全政策。

Abstract: U.S. AI security policy is increasingly shaped by an $\textit{LLM Mirage}$, the belief that national security risks scale in proportion to the compute used to train frontier language models. That premise fails in two ways. It miscalibrates strategy because adversaries can obtain weaponizable capabilities with task-specific systems that use specialized data, algorithmic efficiency, and widely available hardware, while compute controls harden only a high-end perimeter. It also destabilizes regulation because, absent a settled definition of "AI weaponization," compute thresholds are easily renegotiated as domestic priorities shift, turning security policy into a proxy contest over industrial competitiveness. We analyze how the LLM Mirage took hold, propose an intent-and-capability definition of AI weaponization grounded in effects and international humanitarian law, and outline measurement infrastructure based on live benchmarks across the full AI Triad (data, algorithms, compute) for weaponization-relevant capabilities.

</details>


### [32] [Research Integrity and Academic Authority in the Age of Artificial Intelligence: From Discovery to Curation?](https://arxiv.org/abs/2601.05574)
*Simon Chesterman,Loy Hui Chieh*

Main category: cs.CY

TL;DR: AI正在重塑科研组织和实践，超越生产力提升，带来新的认识论和制度脆弱性，挑战研究诚信和学术权威，大学应强化不可自动化或商业化的角色来维持合法性。


<details>
  <summary>Details</summary>
Motivation: AI系统加速科学发现、重组学术劳动、调解文献访问，但生成模型带来新的认识论和制度脆弱性，加剧可重复性挑战，模糊作者身份和责任，给同行评审和编辑系统带来压力。同时，AI研究中心从大学转移到拥有数据、算力和工程人才优势的私营实验室，前沿模型日益专有和封闭，使大学难以审查、复制或质疑科学研究所依赖的系统。

Method: 本文采用批判性分析的方法，考察AI对科研组织和实践的多方面影响，分析AI带来的认识论和制度挑战，以及大学与私营实验室之间的权力转移。文章提出规范性论点，而非实证研究方法。

Result: AI发展挑战研究诚信，侵蚀学术权威的传统基础（即使知识可信、可争议且独立于集中权力的制度能力）。大学不应在技术前沿与公司实验室竞争，而应强化不可自动化或商业化的角色：在合成输出饱和的环境中行使研究质量判断；管理知识的来源、透明度和可重复性；作为私人利益的伦理和认识论制衡。

Conclusion: 在信息丰富的时代，大学未来的权威不在于最大化发现本身，而在于维持知识可信和公共价值的制度条件。大学应通过强化质量判断、知识管理和伦理监督等独特角色来维持合法性，成为知识可信性的守护者。

Abstract: Artificial intelligence is reshaping the organization and practice of research in ways that extend far beyond gains in productivity. AI systems now accelerate discovery, reorganize scholarly labour, and mediate access to expanding scientific literatures. At the same time, generative models capable of producing text, images, and data at scale introduce new epistemic and institutional vulnerabilities. They exacerbate challenges of reproducibility, blur lines of authorship and accountability, and place unprecedented pressure on peer review and editorial systems. These risks coincide with a deeper political-economic shift: the centre of gravity in AI research has moved decisively from universities to private laboratories with privileged access to data, compute, and engineering talent. As frontier models become increasingly proprietary and opaque, universities face growing difficulty interrogating, reproducing, or contesting the systems on which scientific inquiry increasingly depends.
  This article argues that these developments challenge research integrity and erode traditional bases of academic authority, understood as the institutional capacity to render knowledge credible, contestable, and independent of concentrated power. Rather than competing with corporate laboratories at the technological frontier, universities can sustain their legitimacy by strengthening roles that cannot be readily automated or commercialized: exercising judgement over research quality in an environment saturated with synthetic outputs; curating the provenance, transparency, and reproducibility of knowledge; and acting as ethical and epistemic counterweights to private interests. In an era of informational abundance, the future authority of universities lies less in maximizing discovery alone than in sustaining the institutional conditions under which knowledge can be trusted and publicly valued.

</details>


### [33] [Cross-National Evidence of Disproportionate Media Visibility for the Radical Right in the 2024 European Elections](https://arxiv.org/abs/2601.05826)
*Íris Damião,João Franco,Mariana Silva,Paulo Almeida,Pedro C. Magalhães,Joana Gonçalves-Sá*

Main category: cs.CY

TL;DR: 该研究对2024年欧洲议会选举期间不同政治家族的媒体可见度进行了系统性比较分析，发现主流和激进右翼获得最多报道，且激进右翼的媒体关注度远超其选举表现，这种不平衡在竞选后期加剧。


<details>
  <summary>Details</summary>
Motivation: 研究动机是了解媒体在选举期间如何报道不同政治力量，特别是媒体可见度作为民主竞争中的核心资源，是否会导致结构性不对称，影响选举公平性。

Method: 研究分析了奥地利、德国、爱尔兰、波兰和葡萄牙五个国家的近21,500条新闻，结合计算方法和人工分类，从文章URL和标题中识别政党、政治领导人和团体，并按欧洲议会政治家族和广泛政治倾向进行聚类分析。

Result: 研究发现：1) 主流和激进右翼比其他政治团体获得更多提及；2) 激进右翼的媒体关注度与其选举结果（2019年或2024年）和选举预测不成比例，特别是在奥地利、德国和爱尔兰；3) 这种不平衡在竞选最后几周加剧；4) 右倾实体的报道在所有新闻来源中占主导地位，尤其是流量最高的媒体。

Conclusion: 媒体可见度是核心资源，传统媒体可能通过不成比例的报道模式（特别是对激进右翼的过度关注）加剧民主竞争中的结构性不对称，这种模式是结构性的而非特定媒体的。

Abstract: This study provides a systematic comparative analysis of media visibility of different political families during the 2024 European Parliament elections. We analyzed close to 21,500 unique news from leading national outlets in Austria, Germany, Ireland, Poland, and Portugal - countries with diverse political contexts and levels of media trust. Combining computational and human classification, we identified parties, political leaders, and groups from the article's URLs and titles, and clustered them according to European Parliament political families and broad political leanings. Cross-country comparison shows that the Mainstream and the Radical Right were mentioned more often than the other political groups. Moreover, the Radical Right received disproportionate attention relative to electoral results (from 2019 or 2024) and electoral projections, particularly in Austria, Germany, and Ireland. This imbalance increased in the final weeks of the campaign, when media influence on undecided voters is greatest. Outlet-level analysis shows that coverage of right-leaning entities dominated across news sources, especially those generating the highest traffic, suggesting a structural rather than outlet-specific pattern. Media visibility is a central resource, and this systematic mapping of online coverage highlights how traditional media can contribute to structural asymmetries in democratic competition.

</details>


### [34] [Can AI mediation improve democratic deliberation?](https://arxiv.org/abs/2601.05904)
*Michael Henry Tessler,Georgina Evans,Michiel A. Bakker,Iason Gabriel,Sophie Bridgers,Rishub Jain,Raphael Koster,Verena Rieser,Anca Dragan,Matthew Botvinick,Christopher Summerfield*

Main category: cs.CY

TL;DR: AI和大型语言模型可以帮助解决民主审议中的"三难困境"，通过增强参与度、促进政治平等和改善审议质量，但仍面临挑战需要解决。


<details>
  <summary>Details</summary>
Motivation: 民主审议面临参与度、审议质量和政治平等之间的三难困境，作者探索AI和大型语言模型是否能帮助解决这一困境，增强公民参与和民主审议。

Method: 分析一个基于大型语言模型的审议增强系统案例，探讨LLM在审议工具中的应用潜力，包括提升可扩展性、公平调解和信息可信度。

Result: LLM有潜力通过可扩展性增强参与度，通过公平调解改善政治平等，通过提供可信信息促进有意义的审议，但仍存在关键挑战。

Conclusion: 需要实证、技术和理论方面的进步来充分实现AI介导审议的潜力，以增强公民参与和加强民主审议。

Abstract: The strength of democracy lies in the free and equal exchange of diverse viewpoints. Living up to this ideal at scale faces inherent tensions: broad participation, meaningful deliberation, and political equality often trade off with one another (Fishkin, 2011). We ask whether and how artificial intelligence (AI) could help navigate this "trilemma" by engaging with a recent example of a large language model (LLM)-based system designed to help people with diverse viewpoints find common ground (Tessler, Bakker, et al., 2024). Here, we explore the implications of the introduction of LLMs into deliberation augmentation tools, examining their potential to enhance participation through scalability, improve political equality via fair mediation, and foster meaningful deliberation by, for example, surfacing trustworthy information. We also point to key challenges that remain. Ultimately, a range of empirical, technical, and theoretical advancements are needed to fully realize the promise of AI-mediated deliberation for enhancing citizen engagement and strengthening democratic deliberation.

</details>


### [35] [Navigating the Sociotechnical Imaginaries of Brazilian Tech Workers](https://arxiv.org/abs/2601.05961)
*Kenzo Soares Seto*

Main category: cs.CY

TL;DR: 研究巴西科技工作者的社会技术想象，挑战数据普遍主义，强调南方视角下的本地价值观、约束和未来愿景


<details>
  <summary>Details</summary>
Motivation: 巴西科技工作者在数字劳动研究中常被忽视，但他们设计着塑造日常生活的数字系统。从全球南方视角出发，可以挑战数据普遍主义，突出本地情境化的价值观、约束和未来愿景。

Method: 基于社会技术想象理论框架，采用半结构化访谈方法，于2023年7月至12月期间访谈了26名巴西科技专业人士。

Result: 研究发现：1）学术界与产业界在算法偏见论述上存在持续张力；2）企业在用户伤害和监控方面的责任存在局限；3）数字主权的含义存在争议，包括与边缘化社区需求相一致的草根替代技术未来倡议。

Conclusion: 从全球南方视角研究科技工作者的社会技术想象，有助于揭示本地情境化的价值观、权力关系和替代性技术未来，挑战主流的数据普遍主义叙事。

Abstract: This chapter examines the sociotechnical imaginaries of Brazilian tech workers, a group often overlooked in digital labor research despite their role in designing the digital systems that shape everyday life. Grounded in the idea of sociotechnical imaginaries as collectively constructed visions that guide technology development and governance, the chapter argues that looking from the Global South helps challenge data universalism and foregrounds locally situated values, constraints, and futures. Drawing on semi-structured interviews with 26 Brazilian professionals conducted between July and December 2023, it maps how workers make sense of responsibility, bias, and power in AI and platform development. The findings highlight recurring tensions between academic and industry discourse on algorithmic bias, the limits of corporate accountability regarding user harm and surveillance, and the contested meanings of digital sovereignty, including grassroots initiatives that seek alternative technological futures aligned with marginalized communities needs.

</details>


### [36] [The Causal Effect of First-Time Academic Failure on University Dropout: Evidence from a Regression Discontinuity Design](https://arxiv.org/abs/2601.05987)
*H. R. Paz*

Main category: cs.CY

TL;DR: 研究发现首次学术失败（勉强不及格）反而比勉强及格的学生更不容易退学，挑战了传统假设


<details>
  <summary>Details</summary>
Motivation: 大学辍学是高等教育系统的持续挑战，但关于早期脱离机制的因果证据有限，需要研究首次学术失败对后续辍学的因果影响

Method: 利用0-10分制的制度性评分阈值，采用断点回归设计（RDD），比较首次考试勉强不及格与勉强及格的学生，使用纵向行政数据分析12个月和24个月内的辍学结果

Result: 与常规假设相反，边际首次失败与后续辍学概率降低相关，比边际及格的学生更不容易退学，通过了多种稳健性检验

Conclusion: 早期学术失败可能作为重要信号促使行为调整或重新定向，而边际及格可能维持"脆弱坚持"状态，研究强调了在关键评估阈值处精心设计制度回应的必要性

Abstract: University dropout remains a persistent challenge in higher education systems, yet causal evidence on the mechanisms triggering early disengagement is limited. This study estimates the causal effect of first-time academic failure on subsequent university attrition. Exploiting a sharp institutional grading threshold on a 0-10 scale, we implement a regression discontinuity design (RDD) comparing students who narrowly fail to those who narrowly pass their first attempt. Using longitudinal administrative data spanning multiple cohorts and degree programmes, we estimate local average treatment effects (LATE) for students at the margin of success and examine dropout outcomes within 12 and 24 months following the initial evaluation. Contrary to conventional assumptions, the results indicate that marginal first-time failure is associated with a lower probability of subsequent dropout relative to marginal passing at both horizons. A comprehensive battery of robustness checks - including donut RDD specifications, placebo cutoffs, and formal density tests - supports the validity of the identification strategy. These findings suggest that early academic failure may function as a salient signal that prompts behavioural adjustment or reorientation, while marginal passing may sustain a state of "fragile persistence". The study provides causal evidence on the non-linear effects of early academic performance and highlights the importance of carefully designed institutional responses at critical evaluation thresholds.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [37] [Representing asymmetric relationships by h-plots. Discovering the archetypal patterns of cross-journal citation relationships](https://arxiv.org/abs/2601.05400)
*Aleix Alcacer,Irene Epifanio*

Main category: stat.AP

TL;DR: 提出基于h-plot的可扩展多维标度方法，可处理非对称邻近数据，通过嵌入变量而非对象本身来可视化关系，并支持原型分析识别极端案例。


<details>
  <summary>Details</summary>
Motivation: 传统多维标度方法在处理非对称邻近数据时存在局限性，需要一种能够有效处理非对称关系且易于解释的可视化方法。

Method: 基于h-plot的方法，通过嵌入定义每个对象邻近关系的变量（而非对象本身）来处理非对称数据，使用欧氏距离衡量相似性，并支持原型分析（ADA）识别极端案例。

Result: 方法能够有效可视化期刊引用关系的非对称模式，识别出原型期刊，与其他方法相比表现良好，且结果易于解释，代码和数据已公开。

Conclusion: 提出的h-plot方法为处理非对称邻近数据提供了一种可扩展、易于实现和解释的解决方案，特别适用于可视化复杂关系网络并识别极端模式。

Abstract: This work approaches the multidimensional scaling problem from a novel angle. We introduce a scalable method based on the h-plot, which inherently accommodates asymmetric proximity data. Instead of embedding the objects themselves, the method embeds the variables that define the proximity to or from each object. It is straightforward to implement, and the quality of the resulting representation can be easily evaluated. The methodology is illustrated by visualizing the asymmetric relationships between the citing and cited profiles of journals on a common map. Two profiles that are far apart (or close together) in the h-plot, as measured by Euclidean distance, are different (or similar), respectively. This representation allows archetypoid analysis (ADA) to be calculated. ADA is used to find archetypal journals (or extreme cases). We can represent the dataset as convex combinations of these archetypal journals, making the results easy to interpret, even for non-experts. Comparisons with other methodologies are carried out, showing the good performance of our proposal. Code and data are available for reproducibility.

</details>


### [38] [A latent factor approach to hyperspectral time series data for multivariate genomic prediction of grain yield in wheat](https://arxiv.org/abs/2601.05842)
*Jonathan F. Kunst,Killian A. C. Melsen,Willem Kruijer,José Crossa,Chris Maliepaard,Fred A. van Eeuwijk,Carel F. W. Peeters*

Main category: stat.AP

TL;DR: 利用因子分析和Procrustes旋转从高光谱数据中提取潜在变量，通过多变量基因组预测提高小麦产量的预测精度


<details>
  <summary>Details</summary>
Motivation: 植物育种中高维时间序列表型数据日益普遍，但分析和整合这些数据进行遗传分析和基因组预测仍然困难。需要开发方法从高光谱数据中提取相关特征，提高基因组预测能力。

Method: 对高光谱数据的遗传相关矩阵进行因子分析结合Procrustes旋转，提取潜在变量。使用CIMMYT小麦试验数据（1,033个基因型，3种灌溉处理，多个时间点，62个光谱波段）。将潜在变量整合到多变量基因组预测模型中。

Result: 相比单变量基因组预测，多变量模型使用高光谱潜在变量使预测能力绝对提升0.1-0.3（相关性尺度）。确定了试验中重要的时间点及其与植物生长阶段的关系。

Conclusion: 展示了如何结合领域知识和数据驱动方法，从高通量表型平台的高光谱数据中提高预测能力并获得新见解，为植物育种中的高维表型数据分析提供了有效方法。

Abstract: High-dimensional time series phenotypic data is becoming increasingly common within plant breeding programmes. However, analysing and integrating such data for genetic analysis and genomic prediction remains difficult. Here we show how factor analysis with Procrustes rotation on the genetic correlation matrix of hyperspectral secondary phenotype data can help in extracting relevant features for within-trial prediction. We use a subset of Centro Internacional de Mejoramiento de Maíz y Trigo (CIMMYT) elite yield wheat trial of 2014-2015, consisting of 1,033 genotypes. These were measured across three irrigation treatments at several timepoints during the season, using manned airplane flights with hyperspectral sensors capturing 62 bands in the spectrum of 385-850 nm. We perform multivariate genomic prediction using latent variables to improve within-trial genomic predictive ability (PA) of wheat grain yield within three distinct watering treatments. By integrating latent variables of the hyperspectral data in a multivariate genomic prediction model, we are able to achieve an absolute gain of .1 to .3 (on the correlation scale) in PA compared to univariate genomic prediction. Furthermore, we show which timepoints within a trial are important and how these relate to plant growth stages. This paper showcases how domain knowledge and data-driven approaches can be combined to increase PA and gain new insights from sensor data of high-throughput phenotyping platforms.

</details>


### [39] [Neural Methods for Multiple Systems Estimation Models](https://arxiv.org/abs/2601.05859)
*Joseph Marsh,Nathan A. Judd,Lax Chan,Rowland G. Seymour*

Main category: stat.AP

TL;DR: 提出基于神经贝叶斯估计器和神经后验估计器的模拟贝叶斯推理框架，用于解决隐藏人口规模估计中的计算难题和数据稀疏问题


<details>
  <summary>Details</summary>
Motivation: 多重系统估计在定量社会学中很重要，但实际应用受到不完善行政数据和计算限制的阻碍。现实数据集常因隐私问题存在删失和缺失，而传统方法（如MLE和MCMC）在数据稀疏时计算不可行或无法收敛。

Method: 提出基于神经贝叶斯估计器和神经后验估计器的模拟贝叶斯推理框架。这些神经方法是摊销式的：一旦训练完成，可提供即时、计算高效的后验估计，适合计算资源有限的保密研究环境。

Result: 通过大量模拟研究证明，神经估计器在精度上与MCMC相当，但速度快几个数量级，且对传统采样器在稀疏设置中常见的收敛失败具有鲁棒性。在估计英国现代奴隶制流行率和英格兰东北部女性吸毒情况两个真实案例中验证了方法。

Conclusion: 提出的神经推理框架解决了多重系统估计中的计算和数据稀疏问题，为隐藏人口规模估计提供了高效、稳健的解决方案，特别适合计算资源有限的研究环境。

Abstract: Estimating the size of hidden populations using Multiple Systems Estimation (MSE) is a critical task in quantitative sociology; however, practical application is often hindered by imperfect administrative data and computational constraints. Real-world datasets frequently suffer from censoring and missingness due to privacy concerns, while standard inference methods, such as Maximum Likelihood Estimation (MLE) and Markov chain Monte Carlo (MCMC), can become computationally intractable or fail to converge when data are sparse. To address these limitations, we propose a novel simulation-based Bayesian inference framework utilizing Neural Bayes Estimators (NBE) and Neural Posterior Estimators (NPE). These neural methods are amortized: once trained, they provide instantaneous, computationally efficient posterior estimates, making them ideal for use in secure research environments where computational resources are limited. Through extensive simulation studies, we demonstrate that neural estimators achieve accuracy comparable to MCMC while being orders of magnitude faster and robust to the convergence failures that plague traditional samplers in sparse settings. We demonstrate our method on two real-world cases estimating the prevalence of modern slavery in the UK and female drug use in North East England.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [40] [Optimizing Digital Adjudication through Social Network Analysis: An Empirical Study of Credit Card Disputes in Beijing](https://arxiv.org/abs/2601.05299)
*Chung Han Tsai,ChengTo Lin,Chung Han Tsai,ChengTo Lin,Baowen Zhang,Qingyue Deng,Yunhui Zhao,Zhijia Song,Baowen Zhang,Qingyue Deng,Yunhui Zhao,Zhijia Song*

Main category: cs.SI

TL;DR: 该研究运用社会网络分析(SNA)方法，通过构建法律引用网络分析北京地区涉及个人信息保护的信用卡纠纷案件，揭示实体法和程序法应用的结构模式，为"数字法院"系统优化提供方法论框架。


<details>
  <summary>Details</summary>
Motivation: 随着司法系统快速数字化，大数据在裁判中的应用仍未被充分探索，特别是在揭示法律应用的结构逻辑方面存在研究空白。本研究旨在填补这一空白。

Method: 采用社会网络分析(SNA)方法，分析北京地区涉及个人信息保护的信用卡纠纷案件，构建法律引用网络来揭示实体法和程序法的应用模式。

Result: 研究发现SNA能有效识别核心法律规范并对案件进行分类，为优化"数字法院"系统提供了强有力的方法论框架。

Conclusion: 研究结果为通过数据驱动的案件检索和整体司法信息网络提升司法效率和一致性提供了实践路径，展示了大数据分析在司法数字化中的价值。

Abstract: Amid the rapid digitalization of judicial systems, the integration of big data into adjudication remains underexplored, particularly in uncovering the structural logic of legal applications. This study bridges this gap by employing social network analysis (SNA) to examine credit card disputes involving personal information protection adjudicated in Beijing. By constructing a legal citation network, we reveal the latent patterns of substantive and procedural law application. The findings demonstrate that SNA can effectively identify core legal norms and typify cases, offering a robust methodological framework for optimizing 'Digital Court' systems. These insights provide practical pathways for enhancing judicial efficiency and consistency through data-driven case retrieval and holistic judicial information networks.

</details>


### [41] [Mobility Trajectories from Network-Driven Markov Dynamics](https://arxiv.org/abs/2601.06020)
*David A. Meyer,Asif Shakeel*

Main category: cs.SI

TL;DR: 提出一种基于时空交互网络的生成式人类移动模型，通过马尔可夫动力学在分层网络结构上生成轨迹，能复现结构化OD流并识别周期性不变分布。


<details>
  <summary>Details</summary>
Motivation: 传统人类移动模型通常基于个体行为假设，缺乏网络中心视角。本文旨在开发一个隐私保护的框架，从网络结构和时间动态角度生成移动轨迹，避免依赖个体级行为假设。

Method: 构建时空交互网络，包含枢纽、走廊、支路和地铁连接等分层路由结构。使用重力型距离衰减结合外部时间表和方向偏置定义转移矩阵。采用无记忆、不可区分的移动者在每个时间步执行单次转移的马尔可夫动力学。

Result: 模型生成的轨迹聚合后能复现反映网络几何、时间调制和连接约束的结构化OD流。通过Perron-Frobenius定理识别出唯一的周期性不变人口分布作为非瞬态参考状态。轨迹级实现与多步马尔可夫动力学一致，差异仅源于有限人口采样。

Conclusion: 该框架提供了网络中心、隐私保护的方法来生成移动轨迹和研究时延流结构，无需个体行为假设，为城市规划和交通分析提供了新工具。

Abstract: We present a generative model of human mobility in which trajectories arise as realizations of a prescribed, time-dependent Markov dynamics defined on a spatial interaction network. The model constructs a hierarchical routing structure with hubs, corridors, feeder paths, and metro links, and specifies transition matrices using gravity-type distance decay combined with externally imposed temporal schedules and directional biases. Population mass evolves as indistinguishable, memoryless movers performing a single transition per time step.
  When aggregated, the resulting trajectories reproduce structured origin-destination flows that reflect network geometry, temporal modulation, and connectivity constraints. By applying the Perron-Frobenius theorem to the daily evolution operator, we identify a unique periodic invariant population distribution that serves as a natural non-transient reference state. We verify consistency between trajectory-level realizations and multi-step Markov dynamics, showing that discrepancies are entirely attributable to finite-population sampling. The framework provides a network-centric, privacy-preserving approach to generating mobility trajectories and studying time-elapsed flow structure without invoking individual-level behavioral assumptions.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [42] [From Unstructured Data to Demand Counterfactuals: Theory and Practice](https://arxiv.org/abs/2601.05374)
*Timothy Christensen,Giovanni Compiani*

Main category: econ.EM

TL;DR: 提出一个工具包来纠正因产品差异化维度代理不准确导致的需求模型偏差，确保反事实分析和推断的有效性


<details>
  <summary>Details</summary>
Motivation: 当使用ML方法从高维非结构化数据（如产品描述和图像）构建产品差异化维度代理时，如果代理未能捕捉真正的差异化维度，标准工作流程会产生有偏的反事实结果和无效推断

Method: 开发一个实用的工具包，适用于市场级和/或个体数据，计算量小，效率高，提供标准误差的简单公式，支持数据依赖的代理（包括微调ML模型的嵌入），也可用于存在测量误差的标准定量属性

Result: 该方法在模拟和实证应用中都能显著改善反事实替代预测，同时提供诊断工具来评估代理构建和维度的充分性

Conclusion: 提出的工具包能够有效纠正因产品差异化维度代理不准确导致的偏差，确保需求模型的可靠反事实分析和统计推断

Abstract: Empirical models of demand for differentiated products rely on low-dimensional product representations to capture substitution patterns. These representations are increasingly proxied by applying ML methods to high-dimensional, unstructured data, including product descriptions and images. When proxies fail to capture the true dimensions of differentiation that drive substitution, standard workflows will deliver biased counterfactuals and invalid inference. We develop a practical toolkit that corrects this bias and ensures valid inference for a broad class of counterfactuals. Our approach applies to market-level and/or individual data, requires minimal additional computation, is efficient, delivers simple formulas for standard errors, and accommodates data-dependent proxies, including embeddings from fine-tuned ML models. It can also be used with standard quantitative attributes when mismeasurement is a concern. In addition, we propose diagnostics to assess the adequacy of the proxy construction and dimension. The approach yields meaningful improvements in predicting counterfactual substitution in both simulations and an empirical application.

</details>


### [43] [Event Studies with Feedback](https://arxiv.org/abs/2601.05493)
*Irene Botosaru,Laura Liu*

Main category: econ.EM

TL;DR: 开发动态面板事件研究框架，分离直接处理效应与通过内生协变量调整的间接效应


<details>
  <summary>Details</summary>
Motivation: 传统事件研究常混淆直接处理效应与通过内生协变量调整产生的间接效应，需要开发新框架来分离这些效应

Method: 建立动态面板事件研究框架，允许持久性结果和处理效应，以及响应过去结果和处理暴露的协变量；在序列外生性和同质反馈假设下识别参数

Result: 在序列外生性和同质反馈条件下，能够点识别控制结果和处理效应动态的常见参数、异质处理效应分布以及协变量反馈过程

Conclusion: 提出的动态分解算法使研究者能够评估每种效应在驱动处理效应动态中的相对重要性，为事件研究提供更精确的效应分解方法

Abstract: Event studies often conflate direct treatment effects with indirect effects operating through endogenous covariate adjustment. We develop a dynamic panel event study framework that separates these effects. The framework allows for persistent outcomes and treatment effects and for covariates that respond to past outcomes and treatment exposure. Under sequential exogeneity and homogeneous feedback, we establish point identification of common parameters governing outcome and treatment effect dynamics, the distribution of heterogeneous treatment effects, and the covariate feedback process. We propose an algorithm for dynamic decomposition that enables researchers to assess the relative importance of each effect in driving treatment effect dynamics.

</details>


### [44] [Dynamic Mortality Forecasting via Mixed-Frequency State-Space Models](https://arxiv.org/abs/2601.05702)
*Runze Li,Rui Zhou,David Pitt*

Main category: econ.EM

TL;DR: 提出混合频率状态空间模型，将年度死亡率与月度死亡计数结合，实现实时更新和更准确的死亡率预测


<details>
  <summary>Details</summary>
Motivation: 高频死亡数据包含重要的年内死亡率动态信息，但现有随机死亡率模型大多基于年度数据更新，无法充分利用月度数据的时效性

Method: 扩展Lee-Carter框架为混合频率状态空间模型，通过共享的月度潜在死亡率因子连接年度和月度序列，使用季节性ARIMA过程建模，采用EM算法结合卡尔曼滤波进行参数估计

Result: MF-SS模型显著改善年内年度即时预测，月度聚合预测通常更准确，优于未协调和时序协调的Lee-Carter预测，并产生更谨慎的预测区间

Conclusion: 混合频率状态空间模型有效整合不同频率数据，实现实时更新而不需重新估计参数，为死亡率预测提供了更及时准确的方法

Abstract: High-frequency death counts are now widely available and contain timely information about intra-year mortality dynamics, but most stochastic mortality models are still estimated on annual data and therefore update only when annual totals are released. We propose a mixed-frequency state-space (MF--SS) extension of the Lee--Carter framework that jointly uses annual mortality rates and monthly death counts. The two series are linked through a shared latent monthly mortality factor, with the annual period factor defined as the intra-year average of the monthly factors. The latent monthly factor follows a seasonal ARIMA process, and parameters are estimated by maximum likelihood using an EM algorithm with Kalman filtering and smoothing. This setup enables real-time intra-year updates of the latent state and forecasts as new monthly observations arrive without re-estimating model parameters.
  Using U.S. data for ages 20--90 over 1999--2019, we evaluate intra-year annual nowcasts and one- to five-year-ahead forecasts. The MF--SS model produces both a direct annual forecast and an annual forecast implied by aggregating monthly projections. In our application, the aggregated monthly forecast is typically more accurate. Incorporating monthly information substantially improves intra-year annual nowcasts, especially after the first few months of the year. As a benchmark, we also fit separate annual and monthly Lee--Carter models and combine their forecasts using temporal reconciliation. Reconciliation improves these independent forecasts but adds little to MF--SS forecasts, consistent with MF--SS pooling information across frequencies during estimation. The MF--SS aggregated monthly forecasts generally outperform both unreconciled and temporally reconciled Lee--Carter forecasts and produce more cautious predictive intervals than the reconciled Lee--Carter approach.

</details>


### [45] [Learning and Testing Exposure Mappings of Interference using Graph Convolutional Autoencoder](https://arxiv.org/abs/2601.05728)
*Martin Huber,Jannis Kueck,Mara Mattes*

Main category: econ.EM

TL;DR: 该研究提出了一种基于图卷积自编码器的数据驱动方法来学习暴露映射，并开发了机器学习测试来验证用户定义暴露映射的有效性。


<details>
  <summary>Details</summary>
Motivation: 干扰效应（个体结果不仅受自身处理影响，还受他人处理影响）给处理效应评估带来挑战。现有研究依赖先验的简单暴露映射，可能无法捕捉完整的干扰效应。

Method: 使用图卷积自编码器以数据驱动方式学习暴露映射，利用网络中的依赖关系更准确地捕捉干扰效应。提出机器学习测试，将学习到的暴露映射作为工具来验证用户定义暴露映射的有效性。

Result: 通过模拟研究评估了所提有效性测试的有限样本性能。

Conclusion: 该方法能够更准确地捕捉干扰效应，并通过统计测试验证暴露映射的有效性，从而改进直接效应的识别。

Abstract: Interference or spillover effects arise when an individual's outcome (e.g., health) is influenced not only by their own treatment (e.g., vaccination) but also by the treatment of others, creating challenges for evaluating treatment effects. Exposure mappings provide a framework to study such interference by explicitly modeling how the treatment statuses of contacts within an individual's network affect their outcome. Most existing research relies on a priori exposure mappings of limited complexity, which may fail to capture the full range of interference effects. In contrast, this study applies a graph convolutional autoencoder to learn exposure mappings in a data-driven way, which exploit dependencies and relations within a network to more accurately capture interference effects. As our main contribution, we introduce a machine learning-based test for the validity of exposure mappings and thus test the identification of the direct effect. In this testing approach, the learned exposure mapping is used as an instrument to test the validity of a simple, user-defined exposure mapping. The test leverages the fact that, if the user-defined exposure mapping is valid (so that all interference operates through it), then the learned exposure mapping is statistically independent of any individual's outcome, conditional on the user-defined exposure mapping. We assess the finite-sample performance of this proposed validity test through a simulation study.

</details>

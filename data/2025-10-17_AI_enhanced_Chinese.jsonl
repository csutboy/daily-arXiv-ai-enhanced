{"id": "2510.14720", "categories": ["econ.TH"], "pdf": "https://arxiv.org/pdf/2510.14720", "abs": "https://arxiv.org/abs/2510.14720", "authors": ["Moretti Elia", "Loreau Michel", "Benzaquen Michael"], "title": "A Global Systems Perspective on Food Demand, Deforestation and Agricultural Sustainability", "comment": null, "summary": "Feeding a larger and wealthier global population without transgressing\necological limits is increasingly challenging, as rising food demand\n(especially for animal products) intensifies pressure on ecosystems,\naccelerates deforestation, and erodes biodiversity and soil health. We develop\na stylized, spatially explicit global model that links exogenous food-demand\ntrajectories to crop and livestock production, land conversion, and feedbacks\nfrom ecosystem integrity that, in turn, shape future yields and land needs.\nCalibrated to post-1960 trends in population, income, yields, input use, and\nland use, the model reproduces the joint rise of crop and meat demand and the\nassociated expansion and intensification of agriculture. We use it to compare\nbusiness-as-usual, supply-side, demand-side, and mixed-policy scenarios. Three\nresults stand out. First, productivity-oriented supply-side measures (e.g.\nreduced chemical inputs, organic conversion, lower livestock density) often\ntrigger compensatory land expansion that undermines ecological gains-so that\nsupply-side action alone cannot halt deforestation or widespread degradation.\nSecond, demand-side change, particularly reduced meat consumption, consistently\nrelieves both intensification and expansion pressures; in our simulations, only\nsubstantial demand reductions (on the order of 40% of projected excess demand\nby 2100) deliver simultaneous increases in forest area and declines in degraded\nland. Third, integrated policy portfolios that jointly constrain land\nconversion, temper input intensification, and curb demand outperform any single\nlever. Together, these findings clarify the system-level trade-offs that\nfrustrate piecemeal interventions and identify the policy combinations most\nlikely to keep global food provision within ecological limits.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7a7a\u95f4\u660e\u786e\u7684\u5168\u7403\u6a21\u578b\uff0c\u6bd4\u8f83\u4e0d\u540c\u653f\u7b56\u60c5\u666f\uff08\u5e38\u89c4\u3001\u4f9b\u7ed9\u4fa7\u3001\u9700\u6c42\u4fa7\u3001\u6df7\u5408\u653f\u7b56\uff09\uff0c\u53d1\u73b0\u4ec5\u9760\u4f9b\u7ed9\u4fa7\u63aa\u65bd\u65e0\u6cd5\u963b\u6b62\u751f\u6001\u9000\u5316\uff0c\u9700\u6c42\u4fa7\u6539\u53d8\uff08\u7279\u522b\u662f\u51cf\u5c11\u8089\u7c7b\u6d88\u8d39\uff09\u80fd\u6709\u6548\u7f13\u89e3\u751f\u6001\u538b\u529b\uff0c\u7efc\u5408\u653f\u7b56\u7ec4\u5408\u6548\u679c\u6700\u4f73\u3002", "motivation": "\u5e94\u5bf9\u5168\u7403\u4eba\u53e3\u589e\u957f\u548c\u8d22\u5bcc\u589e\u52a0\u5e26\u6765\u7684\u7cae\u98df\u9700\u6c42\u4e0a\u5347\uff0c\u7279\u522b\u662f\u52a8\u7269\u4ea7\u54c1\u9700\u6c42\uff0c\u5bf9\u751f\u6001\u7cfb\u7edf\u9020\u6210\u7684\u538b\u529b\u65e5\u76ca\u52a0\u5267\uff0c\u5305\u62ec\u52a0\u901f\u68ee\u6797\u780d\u4f10\u3001\u4fb5\u8680\u751f\u7269\u591a\u6837\u6027\u548c\u571f\u58e4\u5065\u5eb7\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u98ce\u683c\u5316\u3001\u7a7a\u95f4\u660e\u786e\u7684\u5168\u7403\u6a21\u578b\uff0c\u5c06\u5916\u751f\u7cae\u98df\u9700\u6c42\u8f68\u8ff9\u4e0e\u4f5c\u7269\u548c\u755c\u7267\u4e1a\u751f\u4ea7\u3001\u571f\u5730\u8f6c\u6362\u4ee5\u53ca\u751f\u6001\u7cfb\u7edf\u5b8c\u6574\u6027\u53cd\u9988\u8054\u7cfb\u8d77\u6765\uff0c\u6821\u51c6\u4e861960\u5e74\u540e\u7684\u4eba\u53e3\u3001\u6536\u5165\u3001\u4ea7\u91cf\u3001\u6295\u5165\u548c\u571f\u5730\u5229\u7528\u8d8b\u52bf\u3002", "result": "\u4f9b\u7ed9\u4fa7\u63aa\u65bd\u5e38\u5f15\u53d1\u8865\u507f\u6027\u571f\u5730\u6269\u5f20\uff0c\u62b5\u6d88\u751f\u6001\u6536\u76ca\uff1b\u9700\u6c42\u4fa7\u6539\u53d8\uff08\u7279\u522b\u662f\u51cf\u5c11\u8089\u7c7b\u6d88\u8d39\uff09\u80fd\u540c\u65f6\u7f13\u89e3\u96c6\u7ea6\u5316\u548c\u6269\u5f20\u538b\u529b\uff1b\u7efc\u5408\u653f\u7b56\u7ec4\u5408\u4f18\u4e8e\u4efb\u4f55\u5355\u4e00\u63aa\u65bd\u3002", "conclusion": "\u660e\u786e\u4e86\u963b\u788d\u96f6\u6563\u5e72\u9884\u7684\u7cfb\u7edf\u7ea7\u6743\u8861\uff0c\u8bc6\u522b\u4e86\u6700\u6709\u53ef\u80fd\u5c06\u5168\u7403\u7cae\u98df\u4f9b\u5e94\u63a7\u5236\u5728\u751f\u6001\u9650\u5ea6\u5185\u7684\u653f\u7b56\u7ec4\u5408\u3002"}}
{"id": "2510.14872", "categories": ["econ.TH", "cs.GT", "cs.SI", "91A, 91B, 91A80", "J.4"], "pdf": "https://arxiv.org/pdf/2510.14872", "abs": "https://arxiv.org/abs/2510.14872", "authors": ["Din Amir", "Bar Hoter", "Moran Koren"], "title": "Strategic Behavior in Crowdfunding: Insights from a Large-Scale Online Experiment", "comment": null, "summary": "This study examines strategic behavior in crowdfunding using a large-scale\nonline experiment. Building on the model of Arieli et. al 2023, we test\npredictions about risk aversion (i.e., opting out despite seeing a positive\nprivate signal) and mutual insurance (i.e., opting in despite seeing a negative\nprivate signal) in a static, single-shot crowdfunding game, focusing on\ninformational incentives rather than dynamic effects. Our results validate key\ntheoretical predictions: crowdfunding mechanisms induce distinct strategic\nbehaviors compared to voting, where participants are more likely to follow\nprivate signals (odds ratio: 0.139, $p < 0.001$). Additionally, the study\ndemonstrates that higher signal accuracy (85\\% vs. 55\\%) decreases risk\naversion (odds ratio: 0.414, $p = 0.024$) but increases reliance on mutual\ninsurance (odds ratio: 2.532, $p = 0.026$). However, contrary to theory,\nincreasing the required participation threshold (50\\% to 80\\%) amplifies risk\naversion (odds ratio: 3.251, $p = 0.005$), which, pending further\ninvestigation, may indicate cognitive constraints.\n  Furthermore, we show that while mutual insurance supports participation, it\nmay hinder information aggregation, particularly as signal accuracy increases.\nThese findings advance crowdfunding theory by confirming the impact of\ninformational incentives and identifying behavioral deviations that challenge\nstandard models, offering insights for platform design and mechanism\nrefinement.", "AI": {"tldr": "\u901a\u8fc7\u5927\u89c4\u6a21\u5728\u7ebf\u5b9e\u9a8c\u9a8c\u8bc1\u4f17\u7b79\u4e2d\u7684\u7b56\u7565\u884c\u4e3a\uff0c\u53d1\u73b0\u4f17\u7b79\u673a\u5236\u76f8\u6bd4\u6295\u7968\u66f4\u80fd\u8bf1\u5bfc\u98ce\u9669\u89c4\u907f\u548c\u4e92\u52a9\u4fdd\u9669\u884c\u4e3a\uff0c\u4fe1\u53f7\u51c6\u786e\u6027\u548c\u53c2\u4e0e\u9608\u503c\u5bf9\u7b56\u7565\u884c\u4e3a\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u4f17\u7b79\u673a\u5236\u4e2d\u7684\u7b56\u7565\u884c\u4e3a\uff0c\u57fa\u4e8eArieli\u7b49\u4eba\u7684\u6a21\u578b\uff0c\u91cd\u70b9\u5173\u6ce8\u4fe1\u606f\u6fc0\u52b1\u800c\u975e\u52a8\u6001\u6548\u5e94\uff0c\u9a8c\u8bc1\u7406\u8bba\u9884\u6d4b\u5e76\u8bc6\u522b\u884c\u4e3a\u504f\u5dee\u3002", "method": "\u91c7\u7528\u5927\u89c4\u6a21\u5728\u7ebf\u5b9e\u9a8c\uff0c\u5728\u9759\u6001\u5355\u6b21\u4f17\u7b79\u6e38\u620f\u4e2d\u6d4b\u8bd5\u98ce\u9669\u89c4\u907f\u548c\u4e92\u52a9\u4fdd\u9669\u884c\u4e3a\uff0c\u6bd4\u8f83\u4e0d\u540c\u4fe1\u53f7\u51c6\u786e\u5ea6\uff0855% vs 85%\uff09\u548c\u53c2\u4e0e\u9608\u503c\uff0850% vs 80%\uff09\u4e0b\u7684\u884c\u4e3a\u5dee\u5f02\u3002", "result": "\u9a8c\u8bc1\u4e86\u7406\u8bba\u9884\u6d4b\uff1a\u4f17\u7b79\u673a\u5236\u76f8\u6bd4\u6295\u7968\u66f4\u6613\u8bf1\u5bfc\u7b56\u7565\u884c\u4e3a\uff1b\u9ad8\u4fe1\u53f7\u51c6\u786e\u5ea6\u964d\u4f4e\u98ce\u9669\u89c4\u907f\u4f46\u589e\u52a0\u4e92\u52a9\u4fdd\u9669\uff1b\u63d0\u9ad8\u53c2\u4e0e\u9608\u503c\u53cd\u800c\u589e\u5f3a\u98ce\u9669\u89c4\u907f\uff0c\u53ef\u80fd\u4e0e\u8ba4\u77e5\u7ea6\u675f\u6709\u5173\u3002", "conclusion": "\u4f17\u7b79\u673a\u5236\u4e2d\u7684\u4fe1\u606f\u6fc0\u52b1\u786e\u5b9e\u5f71\u54cd\u7b56\u7565\u884c\u4e3a\uff0c\u4f46\u5b58\u5728\u4e0e\u7406\u8bba\u4e0d\u7b26\u7684\u884c\u4e3a\u504f\u5dee\uff0c\u4e92\u52a9\u4fdd\u9669\u867d\u4fc3\u8fdb\u53c2\u4e0e\u4f46\u53ef\u80fd\u963b\u788d\u4fe1\u606f\u805a\u5408\uff0c\u4e3a\u5e73\u53f0\u8bbe\u8ba1\u548c\u673a\u5236\u4f18\u5316\u63d0\u4f9b\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2510.14909", "categories": ["econ.GN", "cs.CY", "econ.TH", "q-fin.EC", "91B84, 62P20, 91C99", "I.6.4; J.4; K.4.1"], "pdf": "https://arxiv.org/pdf/2510.14909", "abs": "https://arxiv.org/abs/2510.14909", "authors": ["Yangyang Li"], "title": "The Impact of Medicaid Coverage on Mental Health, Why Insurance Makes People Happier in OHIE: by Spending Less or by Spending More?", "comment": "Peer-reviewed and presented at the 8th Global Public Health\n  Conference (GLOBEHEAL 2025), The International Institute of Knowledge\n  Management (TIIKM). 12 pages, 2 figures", "summary": "The Oregon Health Insurance Experiment (OHIE) offers a unique opportunity to\nexamine the causal relationship between Medicaid coverage and happiness among\nlow-income adults, using an experimental design. This study leverages data from\ncomprehensive surveys conducted at 0 and 12 months post-treatment. Previous\nstudies based on OHIE have shown that individuals receiving Medicaid exhibited\na significant improvement in mental health compared to those who did not\nreceive coverage. The primary objective is to explore how Medicaid coverage\nimpacts happiness, specifically analyzing in which direction variations in\nhealthcare spending significantly improve mental health: higher spending or\nlower spending after Medicaid. Utilizing instrumental variable (IV) regression,\nI conducted six separate regressions across subgroups categorized by\nexpenditure levels and happiness ratings, and the results reveal distinct\npatterns. Enrolling in OHP has significantly decreased the probability of\nexperiencing unhappiness, regardless of whether individuals had high or low\nmedical spending. Additionally, it decreased the probability of being pretty\nhappy and having high medical expenses, while increasing the probability among\nthose with lower expenses. Concerning the probability of being very happy, the\nOHP only had a positive effect on being very happy and spending less, and its\neffect on those with high expenses was insignificant. These findings align with\nthe benefit of Medicaid: alleviating financial burden, contributing to the\nwell-being of distinct subgroups.", "AI": {"tldr": "\u4fc4\u52d2\u5188\u5065\u5eb7\u4fdd\u9669\u5b9e\u9a8c\u7814\u7a76\u53d1\u73b0\uff0c\u533b\u7597\u8865\u52a9\u8986\u76d6\u901a\u8fc7\u51cf\u8f7b\u533b\u7597\u8d39\u7528\u8d1f\u62c5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4f4e\u6536\u5165\u6210\u5e74\u4eba\u4e0d\u5feb\u4e50\u7684\u6982\u7387\uff0c\u5e76\u6539\u5584\u4e86\u7279\u5b9a\u4e9a\u7ec4\u7684\u5e78\u798f\u611f\u3002", "motivation": "\u63a2\u7d22\u533b\u7597\u8865\u52a9\u8986\u76d6\u4e0e\u4f4e\u6536\u5165\u6210\u5e74\u4eba\u5e78\u798f\u611f\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u5206\u6790\u533b\u7597\u652f\u51fa\u53d8\u5316\u5982\u4f55\u5f71\u54cd\u5fc3\u7406\u5065\u5eb7\u3002", "method": "\u5229\u7528\u4fc4\u52d2\u5188\u5065\u5eb7\u4fdd\u9669\u5b9e\u9a8c\u7684\u968f\u673a\u5206\u914d\u6570\u636e\uff0c\u91c7\u7528\u5de5\u5177\u53d8\u91cf\u56de\u5f52\u65b9\u6cd5\uff0c\u6309\u652f\u51fa\u6c34\u5e73\u548c\u5e78\u798f\u611f\u8bc4\u7ea7\u5bf9\u4e9a\u7ec4\u8fdb\u884c\u516d\u79cd\u56de\u5f52\u5206\u6790\u3002", "result": "\u53c2\u52a0OHP\u663e\u8457\u964d\u4f4e\u4e86\u4e0d\u5feb\u4e50\u7684\u6982\u7387\uff08\u65e0\u8bba\u533b\u7597\u652f\u51fa\u9ad8\u4f4e\uff09\uff1b\u964d\u4f4e\u4e86\u9ad8\u652f\u51fa\u4e14\u76f8\u5f53\u5feb\u4e50\u7684\u6982\u7387\uff1b\u589e\u52a0\u4e86\u4f4e\u652f\u51fa\u4e14\u975e\u5e38\u5feb\u4e50\u7684\u6982\u7387\uff1b\u5bf9\u9ad8\u652f\u51fa\u4e14\u975e\u5e38\u5feb\u4e50\u7684\u4eba\u7fa4\u5f71\u54cd\u4e0d\u663e\u8457\u3002", "conclusion": "\u533b\u7597\u8865\u52a9\u901a\u8fc7\u51cf\u8f7b\u8d22\u52a1\u8d1f\u62c5\uff0c\u5bf9\u4e0d\u540c\u4e9a\u7ec4\u7684\u5e78\u798f\u611f\u4ea7\u751f\u5dee\u5f02\u5316\u5f71\u54cd\uff0c\u652f\u6301\u4e86\u533b\u7597\u8865\u52a9\u5728\u6539\u5584\u4f4e\u6536\u5165\u4eba\u7fa4\u798f\u7949\u65b9\u9762\u7684\u79ef\u6781\u4f5c\u7528\u3002"}}
{"id": "2510.14517", "categories": ["econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2510.14517", "abs": "https://arxiv.org/abs/2510.14517", "authors": ["Mitja Kovac", "Rok Spruk"], "title": "The Economic Dividends of Peace: Evidence from Arab-Israeli Normalization", "comment": null, "summary": "This paper provides the first causal evidence on the long-run economic\ndividends of Arab-Israeli peace treaties. Using synthetic control and\ndifference-in-synthetic control estimators, we analyze 1978 Camp David Accords\nand 1994 peace treaty between Jordan and Israel. Both cases reveal large and\nlasting gains. By 2011, real GDP of Egypt exceeded its synthetic counterfactual\nby 64 percent, and per capita income by 82 percent. Jordanian trajectory shows\nsimilarly permanent improvements, with real GDP higher by 75 percent and per\ncapita income by more than 20 percent. The mechanisms differ: in Egypt, gains\nstem from a sharp fiscal reallocation together with higher foreign direct\ninvestment and improved institutional credibility, while Jordan benefited\nprimarily through enhanced trade and financial inflows. Robustness and placebo\ntests confirm the uniqueness of these effects. The results demonstrate that\npeace agreements yield large, durable, and heterogeneous growth dividends.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u63d0\u4f9b\u4e86\u963f\u62c9\u4f2f-\u4ee5\u8272\u5217\u548c\u5e73\u6761\u7ea6\u957f\u671f\u7ecf\u6d4e\u7ea2\u5229\u7684\u56e0\u679c\u8bc1\u636e\uff0c\u53d1\u73b0\u57c3\u53ca\u548c\u7ea6\u65e6\u5728\u7b7e\u8ba2\u548c\u5e73\u6761\u7ea6\u540e\u5206\u522b\u5b9e\u73b0\u4e8664%\u548c75%\u7684\u5b9e\u9645GDP\u589e\u957f\uff0c\u4ee5\u53ca82%\u548c20%\u4ee5\u4e0a\u7684\u4eba\u5747\u6536\u5165\u589e\u957f\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u586b\u8865\u5173\u4e8e\u963f\u62c9\u4f2f-\u4ee5\u8272\u5217\u548c\u5e73\u6761\u7ea6\u957f\u671f\u7ecf\u6d4e\u5f71\u54cd\u7684\u56e0\u679c\u8bc1\u636e\u7a7a\u767d\uff0c\u63a2\u7a76\u548c\u5e73\u534f\u8bae\u662f\u5426\u80fd\u5e26\u6765\u6301\u4e45\u7684\u7ecf\u6d4e\u7ea2\u5229\u3002", "method": "\u4f7f\u7528\u5408\u6210\u63a7\u5236\u6cd5\u548c\u5dee\u5206\u5408\u6210\u63a7\u5236\u4f30\u8ba1\u5668\uff0c\u5206\u67901978\u5e74\u6234\u7ef4\u8425\u534f\u8bae\u548c1994\u5e74\u7ea6\u65e6-\u4ee5\u8272\u5217\u548c\u5e73\u6761\u7ea6\u7684\u7ecf\u6d4e\u5f71\u54cd\u3002", "result": "\u57c3\u53ca\u52302011\u5e74\u5b9e\u9645GDP\u8d85\u8fc7\u5176\u5408\u6210\u53cd\u4e8b\u5b9e64%\uff0c\u4eba\u5747\u6536\u5165\u8d85\u8fc782%\uff1b\u7ea6\u65e6\u5b9e\u9645GDP\u589e\u957f75%\uff0c\u4eba\u5747\u6536\u5165\u589e\u957f\u8d85\u8fc720%\u3002\u673a\u5236\u5206\u6790\u663e\u793a\u57c3\u53ca\u53d7\u76ca\u4e8e\u8d22\u653f\u91cd\u65b0\u5206\u914d\u3001\u5916\u56fd\u76f4\u63a5\u6295\u8d44\u589e\u52a0\u548c\u5236\u5ea6\u4fe1\u8a89\u6539\u5584\uff0c\u7ea6\u65e6\u5219\u4e3b\u8981\u901a\u8fc7\u8d38\u6613\u548c\u91d1\u878d\u6d41\u5165\u83b7\u76ca\u3002", "conclusion": "\u548c\u5e73\u534f\u8bae\u80fd\u4ea7\u751f\u5de8\u5927\u3001\u6301\u4e45\u4e14\u5f02\u8d28\u6027\u7684\u589e\u957f\u7ea2\u5229\uff0c\u7a33\u5065\u6027\u548c\u5b89\u6170\u5242\u68c0\u9a8c\u8bc1\u5b9e\u4e86\u8fd9\u4e9b\u6548\u5e94\u7684\u72ec\u7279\u6027\u3002"}}
{"id": "2510.14327", "categories": ["cs.SI", "math.AT"], "pdf": "https://arxiv.org/pdf/2510.14327", "abs": "https://arxiv.org/abs/2510.14327", "authors": ["Himanshu Yadav", "Thomas Bryan Smith", "Peter Bubenik", "Christopher McCarty"], "title": "What is missing from this picture? Persistent homology and mixup barcodes as a means of investigating negative embedding space", "comment": null, "summary": "Recent work in the information sciences, especially informetrics and\nscientometrics, has made substantial contributions to the development of new\nmetrics that eschew the intrinsic biases of citation metrics. This work has\ntended to employ either network scientific (topological) approaches to\nquantifying the disruptiveness of peer-reviewed research, or topic modeling\napproaches to quantifying conceptual novelty. We propose a combination of these\napproaches, investigating the prospect of topological data analysis (TDA),\nspecifically persistent homology and mixup barcodes, as a means of\nunderstanding the negative space among document embeddings generated by topic\nmodels. Using top2vec, we embed documents and topics in n-dimensional space, we\nuse persistent homology to identify holes in the embedding distribution, and\nthen use mixup barcodes to determine which holes are being filled by a set of\nunobserved publications. In this case, the unobserved publications represent\nresearch that was published before or after the data used to train top2vec. We\ninvestigate the extent that negative embedding space represents missing context\n(older research) versus innovation space (newer research), and the extend that\nthe documents that occupy this space represents integrations of the research\ntopics on the periphery. Potential applications for this metric are discussed.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u62d3\u6251\u6570\u636e\u5206\u6790\uff08TDA\uff09\u548c\u4e3b\u9898\u5efa\u6a21\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6301\u4e45\u540c\u8c03\u548cmixup\u6761\u5f62\u7801\u6765\u5206\u6790\u6587\u6863\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\"\u8d1f\u7a7a\u95f4\"\uff0c\u4ee5\u8bc6\u522b\u7f3a\u5931\u7684\u7814\u7a76\u80cc\u666f\u548c\u521b\u65b0\u7a7a\u95f4\u3002", "motivation": "\u73b0\u6709\u4fe1\u606f\u79d1\u5b66\u4e2d\u7684\u6307\u6807\u5b58\u5728\u5185\u5728\u504f\u89c1\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u7684\u5ea6\u91cf\u65b9\u6cd5\u6765\u91cf\u5316\u7814\u7a76\u7684\u98a0\u8986\u6027\u548c\u6982\u5ff5\u65b0\u9896\u6027\u3002\u672c\u6587\u65e8\u5728\u7ed3\u5408\u7f51\u7edc\u79d1\u5b66\u65b9\u6cd5\u548c\u4e3b\u9898\u5efa\u6a21\u65b9\u6cd5\u7684\u4f18\u52bf\u3002", "method": "\u4f7f\u7528top2vec\u5c06\u6587\u6863\u548c\u4e3b\u9898\u5d4c\u5165\u5230n\u7ef4\u7a7a\u95f4\uff0c\u5e94\u7528\u6301\u4e45\u540c\u8c03\u8bc6\u522b\u5d4c\u5165\u5206\u5e03\u4e2d\u7684\u7a7a\u6d1e\uff0c\u7136\u540e\u4f7f\u7528mixup\u6761\u5f62\u7801\u786e\u5b9a\u54ea\u4e9b\u7a7a\u6d1e\u88ab\u672a\u89c2\u5bdf\u5230\u7684\u51fa\u7248\u7269\u586b\u5145\u3002", "result": "\u7814\u7a76\u4e86\u8d1f\u5d4c\u5165\u7a7a\u95f4\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u4ee3\u8868\u7f3a\u5931\u80cc\u666f\uff08\u8f83\u65e7\u7814\u7a76\uff09\u4e0e\u521b\u65b0\u7a7a\u95f4\uff08\u8f83\u65b0\u7814\u7a76\uff09\uff0c\u4ee5\u53ca\u5360\u636e\u8fd9\u4e9b\u7a7a\u95f4\u7684\u6587\u6863\u5982\u4f55\u6574\u5408\u8fb9\u7f18\u7814\u7a76\u4e3b\u9898\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7406\u89e3\u7814\u7a76\u6587\u732e\u4e2d\u7684\u6982\u5ff5\u6f14\u53d8\u63d0\u4f9b\u4e86\u65b0\u7684\u5ea6\u91cf\u5de5\u5177\uff0c\u5177\u6709\u8bc6\u522b\u7814\u7a76\u521b\u65b0\u6027\u548c\u6574\u5408\u6027\u7684\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.14043", "categories": ["eess.SY", "cs.AI", "cs.CR", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.14043", "abs": "https://arxiv.org/abs/2510.14043", "authors": ["Shimiao Li", "Guannan Qu", "Bryan Hooi", "Vyas Sekar", "Soummya Kar", "Larry Pileggi"], "title": "Cyber-Resilient System Identification for Power Grid through Bayesian Integration", "comment": null, "summary": "Power grids increasingly need real-time situational awareness under the\never-evolving cyberthreat landscape. Advances in snapshot-based system\nidentification approaches have enabled accurately estimating states and\ntopology from a snapshot of measurement data, under random bad data and\ntopology errors. However, modern interactive, targeted false data can stay\nundetectable to these methods, and significantly compromise estimation\naccuracy. This work advances system identification that combines snapshot-based\nmethod with time-series model via Bayesian Integration, to advance cyber\nresiliency against both random and targeted false data. Using a distance-based\ntime-series model, this work can leverage historical data of different\ndistributions induced by changes in grid topology and other settings. The\nnormal system behavior captured from historical data is integrated into system\nidentification through a Bayesian treatment, to make solutions robust to\ntargeted false data. We experiment on mixed random anomalies (bad data,\ntopology error) and targeted false data injection attack (FDIA) to demonstrate\nour method's 1) cyber resilience: achieving over 70% reduction in estimation\nerror under FDIA; 2) anomalous data identification: being able to alarm and\nlocate anomalous data; 3) almost linear scalability: achieving comparable speed\nwith the snapshot-based baseline, both taking <1min per time tick on the large\n2,383-bus system using a laptop CPU.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u57fa\u4e8e\u5feb\u7167\u7684\u7cfb\u7edf\u8bc6\u522b\u65b9\u6cd5\u4e0e\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u7684\u8d1d\u53f6\u65af\u96c6\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u7535\u7f51\u5bf9\u968f\u673a\u548c\u5b9a\u5411\u865a\u5047\u6570\u636e\u7684\u7f51\u7edc\u5f39\u6027\u3002", "motivation": "\u73b0\u4ee3\u7535\u7f51\u9700\u8981\u5b9e\u65f6\u6001\u52bf\u611f\u77e5\u80fd\u529b\uff0c\u4f46\u73b0\u6709\u57fa\u4e8e\u5feb\u7167\u7684\u7cfb\u7edf\u8bc6\u522b\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u68c0\u6d4b\u4ea4\u4e92\u5f0f\u3001\u5b9a\u5411\u7684\u865a\u5047\u6570\u636e\u653b\u51fb\uff0c\u8fd9\u4f1a\u4e25\u91cd\u5f71\u54cd\u4f30\u8ba1\u7cbe\u5ea6\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u8ddd\u79bb\u7684\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u96c6\u6210\u5c06\u5386\u53f2\u6570\u636e\u4e2d\u7684\u6b63\u5e38\u7cfb\u7edf\u884c\u4e3a\u6574\u5408\u5230\u7cfb\u7edf\u8bc6\u522b\u4e2d\uff0c\u4f7f\u89e3\u51b3\u65b9\u6848\u5bf9\u5b9a\u5411\u865a\u5047\u6570\u636e\u5177\u6709\u9c81\u68d2\u6027\u3002", "result": "\u5728\u6df7\u5408\u968f\u673a\u5f02\u5e38\u548c\u5b9a\u5411\u865a\u5047\u6570\u636e\u6ce8\u5165\u653b\u51fb\u4e0b\uff0c\u5b9e\u73b0\u4e86\uff1a1\uff09\u7f51\u7edc\u5f39\u6027\uff1a\u5728FDIA\u4e0b\u4f30\u8ba1\u8bef\u5dee\u51cf\u5c11\u8d85\u8fc770%\uff1b2\uff09\u5f02\u5e38\u6570\u636e\u8bc6\u522b\uff1a\u80fd\u591f\u62a5\u8b66\u548c\u5b9a\u4f4d\u5f02\u5e38\u6570\u636e\uff1b3\uff09\u51e0\u4e4e\u7ebf\u6027\u53ef\u6269\u5c55\u6027\uff1a\u5728\u5927\u578b2383\u8282\u70b9\u7cfb\u7edf\u4e0a\u6bcf\u4e2a\u65f6\u95f4\u70b9\u5904\u7406\u65f6\u95f4<1\u5206\u949f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u7535\u7f51\u7cfb\u7edf\u8bc6\u522b\u5bf9\u5b9a\u5411\u865a\u5047\u6570\u636e\u653b\u51fb\u7684\u62b5\u5fa1\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2510.14285", "categories": ["econ.EM", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.14285", "abs": "https://arxiv.org/abs/2510.14285", "authors": ["B. Cooper Boniece", "Jos\u00e9 E. Figueroa-L\u00f3pez", "Tianwei Zhou"], "title": "Debiased Kernel Estimation of Spot Volatility in the Presence of Infinite Variation Jumps", "comment": "49 pages", "summary": "Volatility estimation is a central problem in financial econometrics, but\nbecomes particularly challenging when jump activity is high, a phenomenon\nobserved empirically in highly traded financial securities. In this paper, we\nrevisit the problem of spot volatility estimation for an It\\^o semimartingale\nwith jumps of unbounded variation. We construct truncated kernel-based\nestimators and debiased variants that extend the efficiency frontier for spot\nvolatility estimation in terms of the jump activity index $Y$, raising the\nprevious bound $Y<4/3$ to $Y<20/11$, thereby covering nearly the entire\nadmissible range $Y<2$. Compared with earlier work, our approach attains\nsmaller asymptotic variances through the use of unbounded kernels, is simpler\nto implement, and has broader applicability under more flexible model\nassumptions. A comprehensive simulation study confirms that our procedures\nsubstantially outperform competing methods in finite samples.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u622a\u65ad\u6838\u4f30\u8ba1\u5668\u53ca\u5176\u53bb\u504f\u53d8\u4f53\uff0c\u7528\u4e8e\u4f30\u8ba1\u5177\u6709\u65e0\u754c\u53d8\u5316\u8df3\u8dc3\u7684It\u00f4\u534a\u9785\u7684\u77ac\u65f6\u6ce2\u52a8\u7387\uff0c\u5c06\u8df3\u8dc3\u6d3b\u52a8\u6307\u6570\u7684\u6709\u6548\u4f30\u8ba1\u8303\u56f4\u4eceY<4/3\u6269\u5c55\u5230Y<20/11\uff0c\u51e0\u4e4e\u8986\u76d6\u4e86\u5168\u90e8\u5141\u8bb8\u8303\u56f4Y<2\u3002", "motivation": "\u91d1\u878d\u8ba1\u91cf\u7ecf\u6d4e\u5b66\u4e2d\u7684\u6ce2\u52a8\u7387\u4f30\u8ba1\u5728\u8df3\u8dc3\u6d3b\u52a8\u8f83\u9ad8\u65f6\u53d8\u5f97\u7279\u522b\u56f0\u96be\uff0c\u8fd9\u5728\u9ad8\u5ea6\u4ea4\u6613\u7684\u91d1\u878d\u8bc1\u5238\u4e2d\u7ecf\u5e38\u89c2\u5bdf\u5230\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u8df3\u8dc3\u6d3b\u52a8\u6307\u6570Y\u8f83\u9ad8\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u6784\u5efa\u4e86\u622a\u65ad\u6838\u4f30\u8ba1\u5668\u548c\u53bb\u504f\u53d8\u4f53\uff0c\u4f7f\u7528\u65e0\u754c\u6838\u51fd\u6570\u6765\u51cf\u5c0f\u6e10\u8fd1\u65b9\u5dee\uff0c\u65b9\u6cd5\u66f4\u7b80\u5355\u6613\u5b9e\u73b0\uff0c\u4e14\u5728\u66f4\u7075\u6d3b\u7684\u6a21\u578b\u5047\u8bbe\u4e0b\u5177\u6709\u66f4\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002", "result": "\u65b0\u65b9\u6cd5\u5c06\u6709\u6548\u4f30\u8ba1\u8303\u56f4\u6269\u5c55\u5230Y<20/11\uff0c\u6e10\u8fd1\u65b9\u5dee\u66f4\u5c0f\uff0c\u6a21\u62df\u7814\u7a76\u8bc1\u5b9e\u4e86\u5728\u6709\u9650\u6837\u672c\u4e2d\u663e\u8457\u4f18\u4e8e\u7ade\u4e89\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u4f30\u8ba1\u5668\u6269\u5c55\u4e86\u77ac\u65f6\u6ce2\u52a8\u7387\u4f30\u8ba1\u7684\u6548\u7387\u8fb9\u754c\uff0c\u51e0\u4e4e\u8986\u76d6\u4e86\u5168\u90e8\u5141\u8bb8\u7684\u8df3\u8dc3\u6d3b\u52a8\u8303\u56f4\uff0c\u5177\u6709\u66f4\u5c0f\u7684\u6e10\u8fd1\u65b9\u5dee\u3001\u66f4\u7b80\u5355\u7684\u5b9e\u73b0\u548c\u66f4\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2510.14221", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2510.14221", "abs": "https://arxiv.org/abs/2510.14221", "authors": ["Alida Vallejo-L\u00f3pez", "Cesar Noboa-Ter\u00e1n", "Juana Kou-Guzm\u00e1n", "Josefina Ram\u00edrez-Amaya"], "title": "Technological Devices and Their Negative Effects on Health", "comment": "11 page, 1 figure", "summary": "Technology has become a global tool that allows us to obtain information and\nanalyze data, streamlines communication, and allows us to share images, data,\nvideos, texts, etc. Daily activities have gone from traditional to digital.\nToday, it is impossible to live without an electronic device. In this context,\nchanges in people's health observed, with various complaints ranging from\nvisual, neurological, and concentration problems to muscular, hearing, and\nsleep disorders. Society must be aware of the importance of using various\ntechnological devices responsibly to protect people's health in general.\nKeywords: Technology, activities, protect, electronic, Radiation, Health.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u6280\u672f\u8bbe\u5907\u4f7f\u7528\u5bf9\u5065\u5eb7\u7684\u5f71\u54cd\uff0c\u5305\u62ec\u89c6\u89c9\u3001\u795e\u7ecf\u3001\u808c\u8089\u3001\u542c\u529b\u548c\u7761\u7720\u95ee\u9898\uff0c\u5f3a\u8c03\u8d1f\u8d23\u4efb\u4f7f\u7528\u6280\u672f\u4ee5\u4fdd\u62a4\u5065\u5eb7\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u968f\u7740\u6280\u672f\u8bbe\u5907\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u7684\u666e\u53ca\uff0c\u4eba\u4eec\u89c2\u5bdf\u5230\u5404\u79cd\u5065\u5eb7\u95ee\u9898\uff0c\u9700\u8981\u7814\u7a76\u6280\u672f\u4f7f\u7528\u5bf9\u5065\u5eb7\u7684\u5f71\u54cd\u5e76\u63d0\u9ad8\u516c\u4f17\u610f\u8bc6\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6280\u672f\u8bbe\u5907\u4f7f\u7528\u4e0e\u5065\u5eb7\u95ee\u9898\u4e4b\u95f4\u7684\u5173\u8054\uff0c\u63a2\u8ba8\u7535\u5b50\u8bbe\u5907\u8f90\u5c04\u7b49\u56e0\u7d20\u5bf9\u5065\u5eb7\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u6280\u672f\u8bbe\u5907\u4f7f\u7528\u4f1a\u5bfc\u81f4\u89c6\u89c9\u3001\u795e\u7ecf\u3001\u808c\u8089\u3001\u542c\u529b\u548c\u7761\u7720\u7b49\u591a\u65b9\u9762\u7684\u5065\u5eb7\u95ee\u9898\u3002", "conclusion": "\u793e\u4f1a\u9700\u8981\u610f\u8bc6\u5230\u8d1f\u8d23\u4efb\u4f7f\u7528\u6280\u672f\u8bbe\u5907\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u4fdd\u62a4\u4eba\u4eec\u7684\u6574\u4f53\u5065\u5eb7\u3002"}}
{"id": "2510.14000", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14000", "abs": "https://arxiv.org/abs/2510.14000", "authors": ["Mingyang Jiang", "Yueyuan Li", "Jiaru Zhang", "Songan Zhang", "Ming Yang"], "title": "A Diffusion-Refined Planner with Reinforcement Learning Priors for Confined-Space Parking", "comment": null, "summary": "The growing demand for parking has increased the need for automated parking\nplanning methods that can operate reliably in confined spaces. In restricted\nand complex environments, high-precision maneuvers are required to achieve a\nhigh success rate in planning, yet existing approaches often rely on explicit\naction modeling, which faces challenges when accurately modeling the optimal\naction distribution. In this paper, we propose DRIP, a diffusion-refined\nplanner anchored in reinforcement learning (RL) prior action distribution, in\nwhich an RL-pretrained policy provides prior action distributions to regularize\nthe diffusion training process. During the inference phase the denoising\nprocess refines these coarse priors into more precise action distributions. By\nsteering the denoising trajectory through the reinforcement learning prior\ndistribution during training, the diffusion model inherits a well-informed\ninitialization, resulting in more accurate action modeling, a higher planning\nsuccess rate, and reduced inference steps. We evaluate our approach across\nparking scenarios with varying degrees of spatial constraints. Experimental\nresults demonstrate that our method significantly improves planning performance\nin confined-space parking environments while maintaining strong generalization\nin common scenarios.", "AI": {"tldr": "DRIP\u662f\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u5148\u9a8c\u52a8\u4f5c\u5206\u5e03\u7684\u6269\u6563\u7cbe\u5316\u89c4\u5212\u5668\uff0c\u901a\u8fc7RL\u9884\u8bad\u7ec3\u7b56\u7565\u63d0\u4f9b\u5148\u9a8c\u5206\u5e03\u6765\u6b63\u5219\u5316\u6269\u6563\u8bad\u7ec3\uff0c\u5728\u63a8\u7406\u9636\u6bb5\u5c06\u7c97\u7565\u5148\u9a8c\u7cbe\u5316\u4e3a\u66f4\u7cbe\u786e\u7684\u52a8\u4f5c\u5206\u5e03\uff0c\u63d0\u9ad8\u53d7\u9650\u7a7a\u95f4\u505c\u8f66\u89c4\u5212\u7684\u6210\u529f\u7387\u548c\u6548\u7387\u3002", "motivation": "\u968f\u7740\u505c\u8f66\u9700\u6c42\u7684\u589e\u957f\uff0c\u9700\u8981\u80fd\u5728\u53d7\u9650\u7a7a\u95f4\u4e2d\u53ef\u9760\u8fd0\u884c\u7684\u81ea\u52a8\u5316\u505c\u8f66\u89c4\u5212\u65b9\u6cd5\u3002\u5728\u590d\u6742\u53d7\u9650\u73af\u5883\u4e2d\uff0c\u9ad8\u7cbe\u5ea6\u64cd\u7eb5\u5bf9\u4e8e\u89c4\u5212\u6210\u529f\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u663e\u5f0f\u52a8\u4f5c\u5efa\u6a21\uff0c\u96be\u4ee5\u51c6\u786e\u5efa\u6a21\u6700\u4f18\u52a8\u4f5c\u5206\u5e03\u3002", "method": "\u63d0\u51faDRIP\u65b9\u6cd5\uff1a1) RL\u9884\u8bad\u7ec3\u7b56\u7565\u63d0\u4f9b\u5148\u9a8c\u52a8\u4f5c\u5206\u5e03\u6765\u6b63\u5219\u5316\u6269\u6563\u8bad\u7ec3\uff1b2) \u63a8\u7406\u9636\u6bb5\u901a\u8fc7\u53bb\u566a\u8fc7\u7a0b\u5c06\u7c97\u7565\u5148\u9a8c\u7cbe\u5316\u4e3a\u7cbe\u786e\u52a8\u4f5c\u5206\u5e03\uff1b3) \u8bad\u7ec3\u65f6\u901a\u8fc7RL\u5148\u9a8c\u5206\u5e03\u5f15\u5bfc\u53bb\u566a\u8f68\u8ff9\uff0c\u83b7\u5f97\u826f\u597d\u521d\u59cb\u5316\u3002", "result": "\u5728\u4e0d\u540c\u7a7a\u95f4\u7ea6\u675f\u7a0b\u5ea6\u7684\u505c\u8f66\u573a\u666f\u4e2d\u8bc4\u4f30\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u53d7\u9650\u7a7a\u95f4\u505c\u8f66\u73af\u5883\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u89c4\u5212\u6027\u80fd\uff0c\u540c\u65f6\u5728\u5e38\u89c1\u573a\u666f\u4e2d\u4fdd\u6301\u4e86\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DRIP\u901a\u8fc7\u7ed3\u5408RL\u5148\u9a8c\u548c\u6269\u6563\u7cbe\u5316\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u7684\u52a8\u4f5c\u5efa\u6a21\u3001\u66f4\u9ad8\u7684\u89c4\u5212\u6210\u529f\u7387\u548c\u66f4\u5c11\u7684\u63a8\u7406\u6b65\u9aa4\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u53d7\u9650\u7a7a\u95f4\u505c\u8f66\u89c4\u5212\u95ee\u9898\u3002"}}
{"id": "2510.13927", "categories": ["stat.AP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13927", "abs": "https://arxiv.org/abs/2510.13927", "authors": ["Jishu Adhikary", "Raju Maiti"], "title": "Long-Term Spatio-Temporal Forecasting of Monthly Rainfall in West Bengal Using Ensemble Learning Approaches", "comment": "25 pages, 22 figures", "summary": "Rainfall forecasting plays a critical role in climate adaptation,\nagriculture, and water resource management. This study develops long-term\nforecasts of monthly rainfall across 19 districts of West Bengal using a\ncentury-scale dataset spanning 1900-2019. Daily rainfall records are aggregated\ninto monthly series, resulting in 120 years of observations for each district.\nThe forecasting task involves predicting the next 108 months (9 years,\n2011-2019) while accounting for temporal dependencies and spatial interactions\namong districts. To address the nonlinear and complex structure of rainfall\ndynamics, we propose a hierarchical modeling framework that combines\nregression-based forecasting of yearly features with multi-layer perceptrons\n(MLPs) for monthly prediction. Yearly features, such as annual totals,\nquarterly proportions, variability measures, skewness, and extremes, are first\nforecasted using regression models that incorporate both own lags and\nneighboring-district lags. These forecasts are then integrated as auxiliary\ninputs into an MLP model, which captures nonlinear temporal patterns and\nspatial dependencies in the monthly series. The results demonstrate that the\nhierarchical regression-MLP architecture provides robust long-term\nspatio-temporal forecasts, offering valuable insights for agriculture,\nirrigation planning, and water conservation strategies.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u5206\u5c42\u5efa\u6a21\u6846\u67b6\uff0c\u7ed3\u5408\u56de\u5f52\u6a21\u578b\u548cMLP\u795e\u7ecf\u7f51\u7edc\uff0c\u5bf9\u897f\u5b5f\u52a0\u62c9\u90a619\u4e2a\u5730\u533a\u8fdb\u884c\u957f\u671f\u6708\u5ea6\u964d\u96e8\u9884\u6d4b\u3002", "motivation": "\u964d\u96e8\u9884\u62a5\u5bf9\u6c14\u5019\u9002\u5e94\u3001\u519c\u4e1a\u548c\u6c34\u8d44\u6e90\u7ba1\u7406\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u5904\u7406\u964d\u96e8\u7684\u975e\u7ebf\u6027\u548c\u590d\u6742\u65f6\u7a7a\u52a8\u6001\u7279\u6027\u3002", "method": "\u4f7f\u7528\u5206\u5c42\u5efa\u6a21\u6846\u67b6\uff1a\u5148\u7528\u56de\u5f52\u6a21\u578b\u9884\u6d4b\u5e74\u5ea6\u7279\u5f81\uff08\u5e74\u603b\u91cf\u3001\u5b63\u5ea6\u6bd4\u4f8b\u3001\u53d8\u5f02\u6027\u7b49\uff09\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u9884\u6d4b\u4f5c\u4e3a\u8f85\u52a9\u8f93\u5165\u96c6\u6210\u5230MLP\u6a21\u578b\u4e2d\uff0c\u6355\u6349\u6708\u5ea6\u5e8f\u5217\u7684\u975e\u7ebf\u6027\u65f6\u7a7a\u6a21\u5f0f\u3002", "result": "\u5206\u5c42\u56de\u5f52-MLP\u67b6\u6784\u80fd\u591f\u63d0\u4f9b\u7a33\u5065\u7684\u957f\u671f\u65f6\u7a7a\u9884\u6d4b\uff0c\u4e3a\u519c\u4e1a\u3001\u704c\u6e89\u89c4\u5212\u548c\u6c34\u8d44\u6e90\u4fdd\u62a4\u7b56\u7565\u63d0\u4f9b\u6709\u4ef7\u503c\u89c1\u89e3\u3002", "conclusion": "\u63d0\u51fa\u7684\u5206\u5c42\u5efa\u6a21\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u964d\u96e8\u9884\u6d4b\u4e2d\u7684\u975e\u7ebf\u6027\u65f6\u7a7a\u4f9d\u8d56\u95ee\u9898\uff0c\u5728\u957f\u671f\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.13858", "categories": ["cs.AI", "I.6.4"], "pdf": "https://arxiv.org/pdf/2510.13858", "abs": "https://arxiv.org/abs/2510.13858", "authors": ["Raheleh Biglari", "Joachim Denil"], "title": "Decision Oriented Technique (DOTechnique): Finding Model Validity Through Decision-Maker Context", "comment": "10 pages", "summary": "Model validity is as critical as the model itself, especially when guiding\ndecision-making processes. Traditional approaches often rely on predefined\nvalidity frames, which may not always be available or sufficient. This paper\nintroduces the Decision Oriented Technique (DOTechnique), a novel method for\ndetermining model validity based on decision consistency rather than output\nsimilarity. By evaluating whether surrogate models lead to equivalent decisions\ncompared to high-fidelity models, DOTechnique enables efficient identification\nof validity regions, even in the absence of explicit validity boundaries. The\napproach integrates domain constraints and symbolic reasoning to narrow the\nsearch space, enhancing computational efficiency. A highway lane change system\nserves as a motivating example, demonstrating how DOTechnique can uncover the\nvalidity region of a simulation model. The results highlight the potential of\nthe technique to support finding model validity through decision-maker context.", "AI": {"tldr": "\u63d0\u51faDOTechnique\u65b9\u6cd5\uff0c\u901a\u8fc7\u51b3\u7b56\u4e00\u81f4\u6027\u800c\u975e\u8f93\u51fa\u76f8\u4f3c\u6027\u6765\u786e\u5b9a\u6a21\u578b\u6709\u6548\u6027\uff0c\u96c6\u6210\u9886\u57df\u7ea6\u675f\u548c\u7b26\u53f7\u63a8\u7406\u6765\u7f29\u5c0f\u641c\u7d22\u7a7a\u95f4\uff0c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u6a21\u578b\u6709\u6548\u6027\u5bf9\u51b3\u7b56\u8fc7\u7a0b\u81f3\u5173\u91cd\u8981\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u7684\u6709\u6548\u6027\u6846\u67b6\uff0c\u4f46\u53ef\u80fd\u4e0d\u53ef\u7528\u6216\u4e0d\u5145\u5206\u3002", "method": "DOTechnique\u65b9\u6cd5\u901a\u8fc7\u8bc4\u4f30\u66ff\u4ee3\u6a21\u578b\u662f\u5426\u4e0e\u9ad8\u4fdd\u771f\u6a21\u578b\u4ea7\u751f\u7b49\u6548\u51b3\u7b56\u6765\u786e\u5b9a\u6a21\u578b\u6709\u6548\u6027\uff0c\u5373\u4f7f\u6ca1\u6709\u660e\u786e\u7684\u6709\u6548\u6027\u8fb9\u754c\u3002", "result": "\u4ee5\u9ad8\u901f\u516c\u8def\u53d8\u9053\u7cfb\u7edf\u4e3a\u4f8b\uff0c\u5c55\u793a\u4e86DOTechnique\u80fd\u591f\u53d1\u73b0\u4eff\u771f\u6a21\u578b\u7684\u6709\u6548\u6027\u533a\u57df\u3002", "conclusion": "\u8be5\u6280\u672f\u6709\u6f5c\u529b\u901a\u8fc7\u51b3\u7b56\u8005\u4e0a\u4e0b\u6587\u6765\u652f\u6301\u53d1\u73b0\u6a21\u578b\u6709\u6548\u6027\u3002"}}
{"id": "2510.14889", "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.14889", "abs": "https://arxiv.org/abs/2510.14889", "authors": ["Soorya Ram Shimgekar", "Ruining Zhao", "Agam Goyal", "Violeta J. Rodriguez", "Paul A. Bloom", "Hari Sundaram", "Koustuv Saha"], "title": "Detecting Early and Implicit Suicidal Ideation via Longitudinal and Information Environment Signals on Social Media", "comment": null, "summary": "On social media, many individuals experiencing suicidal ideation (SI) do not\ndisclose their distress explicitly. Instead, signs may surface indirectly\nthrough everyday posts or peer interactions. Detecting such implicit signals\nearly is critical but remains challenging. We frame early and implicit SI as a\nforward-looking prediction task and develop a computational framework that\nmodels a user's information environment, consisting of both their longitudinal\nposting histories as well as the discourse of their socially proximal peers. We\nadopted a composite network centrality measure to identify top neighbors of a\nuser, and temporally aligned the user's and neighbors' interactions --\nintegrating the multi-layered signals in a fine-tuned DeBERTa-v3 model. In a\nReddit study of 1,000 (500 Case and 500 Control) users, our approach improves\nearly and implicit SI detection by 15% over individual-only baselines. These\nfindings highlight that peer interactions offer valuable predictive signals and\ncarry broader implications for designing early detection systems that capture\nindirect as well as masked expressions of risk in online environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u8ba1\u7b97\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u6a21\u7528\u6237\u7684\u4fe1\u606f\u73af\u5883\uff08\u5305\u62ec\u4e2a\u4eba\u5386\u53f2\u5e16\u5b50\u548c\u793e\u4ea4\u90bb\u8fd1\u540c\u4f34\u7684\u8ba8\u8bba\uff09\u6765\u65e9\u671f\u68c0\u6d4b\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u9690\u6027\u81ea\u6740\u610f\u5ff5\u3002", "motivation": "\u5728\u793e\u4ea4\u5a92\u4f53\u4e0a\uff0c\u8bb8\u591a\u6709\u81ea\u6740\u610f\u5ff5\u7684\u7528\u6237\u4e0d\u4f1a\u660e\u786e\u8868\u8fbe\u75db\u82e6\uff0c\u800c\u662f\u901a\u8fc7\u65e5\u5e38\u5e16\u5b50\u6216\u540c\u4f34\u4e92\u52a8\u95f4\u63a5\u8868\u73b0\u51fa\u6765\u3002\u65e9\u671f\u68c0\u6d4b\u8fd9\u4e9b\u9690\u6027\u4fe1\u53f7\u81f3\u5173\u91cd\u8981\u4f46\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u91c7\u7528\u590d\u5408\u7f51\u7edc\u4e2d\u5fc3\u6027\u5ea6\u91cf\u8bc6\u522b\u7528\u6237\u7684\u9876\u7ea7\u90bb\u5c45\uff0c\u65f6\u95f4\u5bf9\u9f50\u7528\u6237\u548c\u90bb\u5c45\u7684\u4e92\u52a8\uff0c\u5728\u5fae\u8c03\u7684DeBERTa-v3\u6a21\u578b\u4e2d\u6574\u5408\u591a\u5c42\u4fe1\u53f7\u3002", "result": "\u5728Reddit\u76841000\u540d\u7528\u6237\u7814\u7a76\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6bd4\u4ec5\u4f7f\u7528\u4e2a\u4eba\u4fe1\u606f\u7684\u57fa\u7ebf\u65b9\u6cd5\u63d0\u9ad8\u4e8615%\u7684\u65e9\u671f\u9690\u6027\u81ea\u6740\u610f\u5ff5\u68c0\u6d4b\u51c6\u786e\u7387\u3002", "conclusion": "\u540c\u4f34\u4e92\u52a8\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u9884\u6d4b\u4fe1\u53f7\uff0c\u5bf9\u8bbe\u8ba1\u80fd\u591f\u6355\u6349\u5728\u7ebf\u73af\u5883\u4e2d\u95f4\u63a5\u548c\u63a9\u76d6\u98ce\u9669\u8868\u8fbe\u7684\u65e9\u671f\u68c0\u6d4b\u7cfb\u7edf\u5177\u6709\u66f4\u5e7f\u6cdb\u7684\u610f\u4e49\u3002"}}
{"id": "2510.14045", "categories": ["eess.SY", "cs.NA", "cs.SY", "math.NA"], "pdf": "https://arxiv.org/pdf/2510.14045", "abs": "https://arxiv.org/abs/2510.14045", "authors": ["Qinghua Ma", "Reetam Sen Biswas", "Denis Osipov", "Guannan Qu", "Soummya Kar", "Shimiao Li"], "title": "Multi-Period Sparse Optimization for Proactive Grid Blackout Diagnosis", "comment": null, "summary": "Existing or planned power grids need to evaluate survivability under extreme\nevents, like a number of peak load overloading conditions, which could possibly\ncause system collapses (i.e. blackouts). For realistic extreme events that are\ncorrelated or share similar patterns, it is reasonable to expect that the\ndominant vulnerability or failure sources behind them share the same locations\nbut with different severity. Early warning diagnosis that proactively\nidentifies the key vulnerabilities responsible for a number of system collapses\nof interest can significantly enhance resilience. This paper proposes a\nmulti-period sparse optimization method, enabling the discovery of {persistent\nfailure sources} across a sequence of collapsed systems with increasing system\nstress, such as rising demand or worsening contingencies. This work defines\npersistency and efficiently integrates persistency constraints to capture the\n``hidden'' evolving vulnerabilities. Circuit-theory based power flow\nformulations and circuit-inspired optimization heuristics are used to\nfacilitate the scalability of the method. Experiments on benchmark systems show\nthat the method reliably tracks persistent vulnerability locations under\nincreasing load stress, and solves with scalability to large systems ({on\naverage} taking {around} 200 s per scenario on 2000+ bus systems).", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u591a\u5468\u671f\u7a00\u758f\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u53d1\u73b0\u7535\u529b\u7cfb\u7edf\u5728\u6781\u7aef\u4e8b\u4ef6\u4e0b\u5bfc\u81f4\u7cfb\u7edf\u5d29\u6e83\u7684\u6301\u7eed\u6027\u6545\u969c\u6e90\uff0c\u901a\u8fc7\u7535\u8def\u7406\u8bba\u4f18\u5316\u589e\u5f3a\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u6709\u7535\u7f51\u9700\u8981\u8bc4\u4f30\u6781\u7aef\u4e8b\u4ef6\u4e0b\u7684\u751f\u5b58\u80fd\u529b\uff0c\u5982\u5cf0\u503c\u8d1f\u8377\u8fc7\u8f7d\u53ef\u80fd\u5bfc\u81f4\u7cfb\u7edf\u5d29\u6e83\u3002\u5bf9\u4e8e\u5177\u6709\u76f8\u5173\u6027\u7684\u6781\u7aef\u4e8b\u4ef6\uff0c\u5176\u80cc\u540e\u7684\u4e3b\u8981\u8106\u5f31\u6027\u53ef\u80fd\u4f4d\u4e8e\u76f8\u540c\u4f4d\u7f6e\u4f46\u4e25\u91cd\u7a0b\u5ea6\u4e0d\u540c\uff0c\u9700\u8981\u65e9\u671f\u9884\u8b66\u8bca\u65ad\u6765\u589e\u5f3a\u7cfb\u7edf\u97e7\u6027\u3002", "method": "\u4f7f\u7528\u591a\u5468\u671f\u7a00\u758f\u4f18\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408\u6301\u7eed\u6027\u7ea6\u675f\u6765\u6355\u6349\u9690\u85cf\u7684\u6f14\u5316\u8106\u5f31\u6027\u3002\u91c7\u7528\u57fa\u4e8e\u7535\u8def\u7406\u8bba\u7684\u6f6e\u6d41\u516c\u5f0f\u548c\u7535\u8def\u542f\u53d1\u5f0f\u4f18\u5316\u542f\u53d1\u5f0f\u6765\u63d0\u9ad8\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u3002", "result": "\u5728\u57fa\u51c6\u7cfb\u7edf\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u53ef\u9760\u5730\u8ddf\u8e2a\u5728\u8d1f\u8377\u589e\u52a0\u5e94\u529b\u4e0b\u7684\u6301\u7eed\u6027\u8106\u5f31\u4f4d\u7f6e\uff0c\u5e76\u57282000+\u603b\u7ebf\u7cfb\u7edf\u4e0a\u5e73\u5747\u6bcf\u4e2a\u573a\u666f\u7ea6\u9700200\u79d2\uff0c\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8bc6\u522b\u5bfc\u81f4\u7cfb\u7edf\u5d29\u6e83\u7684\u6301\u7eed\u6027\u6545\u969c\u6e90\uff0c\u4e3a\u7535\u529b\u7cfb\u7edf\u5728\u6781\u7aef\u4e8b\u4ef6\u4e0b\u7684\u65e9\u671f\u9884\u8b66\u548c\u97e7\u6027\u589e\u5f3a\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2510.14409", "categories": ["econ.EM", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.14409", "abs": "https://arxiv.org/abs/2510.14409", "authors": ["Tatsuru Kikuchi"], "title": "Dynamic Spatial Treatment Effect Boundaries: A Continuous Functional Framework from Navier-Stokes Equations", "comment": "79 pages, 5 figures", "summary": "I develop a comprehensive theoretical framework for dynamic spatial treatment\neffect boundaries using continuous functional definitions grounded in\nNavier-Stokes partial differential equations. Rather than discrete treatment\neffect estimators, the framework characterizes treatment intensity as a\ncontinuous function $\\tau(\\mathbf{x}, t)$ over space-time, enabling rigorous\nanalysis of propagation dynamics, boundary evolution, and cumulative exposure\npatterns. Building on exact self-similar solutions expressible through Kummer\nconfluent hypergeometric and modified Bessel functions, I establish that\ntreatment effects follow scaling laws $\\tau(d, t) = t^{-\\alpha} f(d/t^\\beta)$\nwhere exponents characterize diffusion mechanisms. Empirical validation using\n42 million TROPOMI satellite observations of NO$_2$ pollution from U.S.\ncoal-fired power plants demonstrates strong exponential spatial decay\n($\\kappa_s = 0.004$ per km, $R^2 = 0.35$) with detectable boundaries at 572 km.\nMonte Carlo simulations confirm superior performance over discrete parametric\nmethods in boundary detection and false positive avoidance (94\\% vs 27\\%\ncorrect rejection). Regional heterogeneity analysis validates diagnostic\ncapability: positive decay parameters within 100 km confirm coal plant\ndominance; negative parameters beyond 100 km correctly signal when urban\nsources dominate. The continuous functional perspective unifies spatial\neconometrics with mathematical physics, providing theoretically grounded\nmethods for boundary detection, exposure quantification, and policy evaluation\nacross environmental economics, banking, and healthcare applications.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eNavier-Stokes\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u52a8\u6001\u7a7a\u95f4\u5904\u7406\u6548\u5e94\u8fb9\u754c\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u5904\u7406\u5f3a\u5ea6\u5b9a\u4e49\u4e3a\u65f6\u7a7a\u8fde\u7eed\u51fd\u6570\uff0c\u5efa\u7acb\u4e86\u5904\u7406\u6548\u5e94\u7684\u6807\u5ea6\u5f8b\uff0c\u5e76\u901a\u8fc7\u536b\u661f\u89c2\u6d4b\u6570\u636e\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u79bb\u6563\u5904\u7406\u6548\u5e94\u4f30\u8ba1\u65b9\u6cd5\u65e0\u6cd5\u5145\u5206\u5206\u6790\u4f20\u64ad\u52a8\u6001\u3001\u8fb9\u754c\u6f14\u5316\u548c\u7d2f\u79ef\u66b4\u9732\u6a21\u5f0f\uff0c\u9700\u8981\u5efa\u7acb\u8fde\u7eed\u51fd\u6570\u6846\u67b6\u6765\u7edf\u4e00\u7a7a\u95f4\u8ba1\u91cf\u7ecf\u6d4e\u5b66\u4e0e\u6570\u5b66\u7269\u7406\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8eNavier-Stokes\u504f\u5fae\u5206\u65b9\u7a0b\u5efa\u7acb\u8fde\u7eed\u51fd\u6570\u6846\u67b6\uff0c\u5229\u7528Kummer\u5408\u6d41\u8d85\u51e0\u4f55\u51fd\u6570\u548c\u4fee\u6b63\u8d1d\u585e\u5c14\u51fd\u6570\u7684\u7cbe\u786e\u81ea\u76f8\u4f3c\u89e3\uff0c\u63a8\u5bfc\u5904\u7406\u6548\u5e94\u7684\u6807\u5ea6\u5f8b\uff0c\u5e76\u901a\u8fc74200\u4e07TROPOMI\u536b\u661f\u89c2\u6d4b\u6570\u636e\u548c\u8499\u7279\u5361\u6d1b\u6a21\u62df\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "result": "\u5b9e\u8bc1\u9a8c\u8bc1\u663e\u793aNO2\u6c61\u67d3\u5448\u73b0\u5f3a\u6307\u6570\u7a7a\u95f4\u8870\u51cf\uff08\u03bas=0.004\u6bcf\u516c\u91cc\uff0cR\u00b2=0.35\uff09\uff0c\u53ef\u68c0\u6d4b\u8fb9\u754c\u8fbe572\u516c\u91cc\u3002\u8499\u7279\u5361\u6d1b\u6a21\u62df\u8868\u660e\u5728\u8fb9\u754c\u68c0\u6d4b\u548c\u5047\u9633\u6027\u907f\u514d\u65b9\u9762\u4f18\u4e8e\u79bb\u6563\u53c2\u6570\u65b9\u6cd5\uff0894% vs 27%\u6b63\u786e\u62d2\u7edd\uff09\u3002\u533a\u57df\u5f02\u8d28\u6027\u5206\u6790\u9a8c\u8bc1\u4e86\u8bca\u65ad\u80fd\u529b\u3002", "conclusion": "\u8fde\u7eed\u51fd\u6570\u89c6\u89d2\u7edf\u4e00\u4e86\u7a7a\u95f4\u8ba1\u91cf\u7ecf\u6d4e\u5b66\u4e0e\u6570\u5b66\u7269\u7406\uff0c\u4e3a\u8fb9\u754c\u68c0\u6d4b\u3001\u66b4\u9732\u91cf\u5316\u548c\u653f\u7b56\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u9002\u7528\u4e8e\u73af\u5883\u7ecf\u6d4e\u5b66\u3001\u94f6\u884c\u548c\u533b\u7597\u7b49\u591a\u4e2a\u9886\u57df\u3002"}}
{"id": "2510.14457", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2510.14457", "abs": "https://arxiv.org/abs/2510.14457", "authors": ["Tung Phung", "Heeryung Choi", "Mengyan Wu", "Christopher Brooks", "Sumit Gulwani", "Adish Singla"], "title": "Closing the Loop: An Instructor-in-the-Loop AI Assistance System for Supporting Student Help-Seeking in Programming Education", "comment": "Preprint of the SIGCSE'26 paper", "summary": "Timely and high-quality feedback is essential for effective learning in\nprogramming courses; yet, providing such support at scale remains a challenge.\nWhile AI-based systems offer scalable and immediate help, their responses can\noccasionally be inaccurate or insufficient. Human instructors, in contrast, may\nbring more valuable expertise but are limited in time and availability. To\naddress these limitations, we present a hybrid help framework that integrates\nAI-generated hints with an escalation mechanism, allowing students to request\nfeedback from instructors when AI support falls short. This design leverages\nthe strengths of AI for scale and responsiveness while reserving instructor\neffort for moments of greatest need. We deployed this tool in a data science\nprogramming course with 82 students. We observe that out of the total 673\nAI-generated hints, students rated 146 (22%) as unhelpful. Among those, only 16\n(11%) of the cases were escalated to the instructors. A qualitative\ninvestigation of instructor responses showed that those feedback instances were\nincorrect or insufficient roughly half of the time. This finding suggests that\nwhen AI support fails, even instructors with expertise may need to pay greater\nattention to avoid making mistakes. We will publicly release the tool for\nbroader adoption and enable further studies in other classrooms. Our work\ncontributes a practical approach to scaling high-quality support and informs\nfuture efforts to effectively integrate AI and humans in education.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6df7\u5408\u5e2e\u52a9\u6846\u67b6\uff0c\u7ed3\u5408AI\u751f\u6210\u63d0\u793a\u548c\u5347\u7ea7\u673a\u5236\uff0c\u8ba9\u5b66\u751f\u5728AI\u652f\u6301\u4e0d\u8db3\u65f6\u5411\u6559\u5e08\u8bf7\u6c42\u53cd\u9988\uff0c\u4ee5\u5e73\u8861AI\u7684\u53ef\u6269\u5c55\u6027\u548c\u6559\u5e08\u7684\u4e13\u4e1a\u77e5\u8bc6\u3002", "motivation": "\u7f16\u7a0b\u8bfe\u7a0b\u9700\u8981\u53ca\u65f6\u9ad8\u8d28\u91cf\u7684\u53cd\u9988\uff0c\u4f46\u5927\u89c4\u6a21\u63d0\u4f9b\u8fd9\u79cd\u652f\u6301\u5b58\u5728\u6311\u6218\u3002AI\u7cfb\u7edf\u53ef\u6269\u5c55\u4f46\u53ef\u80fd\u4e0d\u51c6\u786e\uff0c\u6559\u5e08\u6709\u4e13\u4e1a\u77e5\u8bc6\u4f46\u65f6\u95f4\u6709\u9650\u3002", "method": "\u5f00\u53d1\u6df7\u5408\u5e2e\u52a9\u6846\u67b6\uff0c\u96c6\u6210AI\u751f\u6210\u63d0\u793a\u548c\u5347\u7ea7\u673a\u5236\uff0c\u5728\u6570\u636e\u79d1\u5b66\u7f16\u7a0b\u8bfe\u7a0b\u4e2d\u90e8\u7f72\u7ed982\u540d\u5b66\u751f\u4f7f\u7528\u3002", "result": "\u5728673\u4e2aAI\u751f\u6210\u63d0\u793a\u4e2d\uff0c22%\u88ab\u5b66\u751f\u8bc4\u4e3a\u65e0\u5e2e\u52a9\uff0c\u5176\u4e2d\u53ea\u670911%\u5347\u7ea7\u5230\u6559\u5e08\u3002\u6559\u5e08\u53cd\u9988\u4e2d\u7ea6\u4e00\u534a\u5b58\u5728\u9519\u8bef\u6216\u4e0d\u8db3\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u6269\u5c55\u9ad8\u8d28\u91cf\u652f\u6301\u7684\u5b9e\u7528\u65b9\u6cd5\uff0c\u5e76\u4e3a\u672a\u6765AI\u4e0e\u4eba\u7c7b\u5728\u6559\u80b2\u4e2d\u7684\u6709\u6548\u6574\u5408\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2510.14018", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14018", "abs": "https://arxiv.org/abs/2510.14018", "authors": ["Adam Morris", "Timothy Pelham", "Edmund R. Hunt"], "title": "Spatially Intelligent Patrol Routes for Concealed Emitter Localization by Robot Swarms", "comment": null, "summary": "This paper introduces a method for designing spatially intelligent robot\nswarm behaviors to localize concealed radio emitters. We use differential\nevolution to generate geometric patrol routes that localize unknown signals\nindependently of emitter parameters, a key challenge in electromagnetic\nsurveillance. Patrol shape and antenna type are shown to influence information\ngain, which in turn determines the effective triangulation coverage. We\nsimulate a four-robot swarm across eight configurations, assigning\npre-generated patrol routes based on a specified patrol shape and sensing\ncapability (antenna type: omnidirectional or directional). An emitter is placed\nwithin the map for each trial, with randomized position, transmission power and\nfrequency. Results show that omnidirectional localization success rates are\ndriven primarily by source location rather than signal properties, with\nfailures occurring most often when sources are placed in peripheral areas of\nthe map. Directional antennas are able to overcome this limitation due to their\nhigher gain and directivity, with an average detection success rate of 98.75%\ncompared to 80.25% for omnidirectional. Average localization errors range from\n1.01-1.30 m for directional sensing and 1.67-1.90 m for omnidirectional\nsensing; while directional sensing also benefits from shorter patrol edges.\nThese results demonstrate that a swarm's ability to predict electromagnetic\nphenomena is directly dependent on its physical interaction with the\nenvironment. Consequently, spatial intelligence, realized here through\noptimized patrol routes and antenna selection, is a critical design\nconsideration for effective robotic surveillance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bbe\u8ba1\u7a7a\u95f4\u667a\u80fd\u673a\u5668\u4eba\u7fa4\u4f53\u884c\u4e3a\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b9a\u4f4d\u9690\u85cf\u7684\u65e0\u7ebf\u7535\u53d1\u5c04\u5668\u3002\u901a\u8fc7\u5dee\u5206\u8fdb\u5316\u751f\u6210\u51e0\u4f55\u5de1\u903b\u8def\u7ebf\uff0c\u4e0d\u4f9d\u8d56\u53d1\u5c04\u5668\u53c2\u6570\u5373\u53ef\u5b9a\u4f4d\u672a\u77e5\u4fe1\u53f7\u3002", "motivation": "\u89e3\u51b3\u7535\u78c1\u76d1\u89c6\u4e2d\u5b9a\u4f4d\u672a\u77e5\u4fe1\u53f7\u7684\u5173\u952e\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u53d1\u5c04\u5668\u53c2\u6570\uff0c\u800c\u8be5\u65b9\u6cd5\u80fd\u591f\u72ec\u7acb\u4e8e\u53d1\u5c04\u5668\u53c2\u6570\u8fdb\u884c\u5b9a\u4f4d\u3002", "method": "\u4f7f\u7528\u5dee\u5206\u8fdb\u5316\u7b97\u6cd5\u751f\u6210\u51e0\u4f55\u5de1\u903b\u8def\u7ebf\uff0c\u6a21\u62df\u56db\u673a\u5668\u4eba\u7fa4\u4f53\u5728\u516b\u79cd\u914d\u7f6e\u4e0b\u8fd0\u884c\uff0c\u57fa\u4e8e\u6307\u5b9a\u5de1\u903b\u5f62\u72b6\u548c\u611f\u77e5\u80fd\u529b\uff08\u5168\u5411\u6216\u5b9a\u5411\u5929\u7ebf\uff09\u5206\u914d\u9884\u751f\u6210\u7684\u5de1\u903b\u8def\u7ebf\u3002", "result": "\u5b9a\u5411\u5929\u7ebf\u7684\u5e73\u5747\u68c0\u6d4b\u6210\u529f\u7387\u4e3a98.75%\uff0c\u5168\u5411\u5929\u7ebf\u4e3a80.25%\uff1b\u5b9a\u5411\u5929\u7ebf\u7684\u5b9a\u4f4d\u8bef\u5dee\u4e3a1.01-1.30\u7c73\uff0c\u5168\u5411\u5929\u7ebf\u4e3a1.67-1.90\u7c73\uff1b\u5b9a\u5411\u5929\u7ebf\u8fd8\u53d7\u76ca\u4e8e\u66f4\u77ed\u7684\u5de1\u903b\u8fb9\u3002", "conclusion": "\u7fa4\u4f53\u9884\u6d4b\u7535\u78c1\u73b0\u8c61\u7684\u80fd\u529b\u76f4\u63a5\u4f9d\u8d56\u4e8e\u5176\u4e0e\u73af\u5883\u7684\u7269\u7406\u4ea4\u4e92\uff0c\u901a\u8fc7\u4f18\u5316\u7684\u5de1\u903b\u8def\u7ebf\u548c\u5929\u7ebf\u9009\u62e9\u5b9e\u73b0\u7684\u7a7a\u95f4\u667a\u80fd\u662f\u6709\u6548\u673a\u5668\u4eba\u76d1\u89c6\u7684\u5173\u952e\u8bbe\u8ba1\u8003\u8651\u56e0\u7d20\u3002"}}
{"id": "2510.13930", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.13930", "abs": "https://arxiv.org/abs/2510.13930", "authors": ["Ziwen Zhong"], "title": "Earthquake Forecasting with ETAS.inlabru", "comment": null, "summary": "The ETAS models are currently the most popular in the field of earthquake\nforecasting. The MCMC method is time-consuming and limited by parameter\ncorrelation while bringing parameter uncertainty. The INLA-based method\n\"inlabru\" solves these problems and performs better at Bayesian inference.\n  The report introduces the composition of the ETAS model, then provides the\nmodel's log-likelihood and approximates it using Taylor expansion and binning\nstrategies. We also present the general procedure of Bayesian inference in\ninlabru.\n  The report follows three experiments. The first one explores the effect of\nfixing one parameter at its actual or wrong values on the posterior\ndistribution of other parameters. We found that $\\alpha$ and $K$ have an\napparent mutual influence relationship. At the same time, fixing $\\alpha$ or\n$K$ to its actual value can reduce the model fitting time by more than half.\n  The second experiment compares normalised inter-event-time distribution on\nreal data and synthetic catalogues. The distributions of normalised\ninter-event-time of real data and synthetic catalogues are consistent. Compared\nwith Exp(1), they have more short and long inter-event-time, indicating the\nexistence of clustering. Change on $\\mu$ and $p$ will influence the\ninter-event-time distribution.\n  In the last one, we use events before the mainshock to predict events ten\nweeks after the mainshock. We use the number test and Continuous Ranked\nProbability Score (CRPS) to measure the accuracy and precision of the\npredictions. We found that we need at least one mainshock and corresponding\noffspring to make reliable forecasting. And when we have more mainshocks in our\ndata, our forecasting will be better. Besides, we also figure out what is\nneeded to obtain a good posterior distribution for each parameter.", "AI": {"tldr": "ETAS\u6a21\u578b\u662f\u5730\u9707\u9884\u6d4b\u4e2d\u6700\u6d41\u884c\u7684\u6a21\u578b\u3002\u672c\u6587\u4f7f\u7528INLA\u65b9\u6cd5\u66ff\u4ee3\u4f20\u7edf\u7684MCMC\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u53c2\u6570\u76f8\u5173\u6027\u548c\u8ba1\u7b97\u8017\u65f6\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u4e09\u4e2a\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edfMCMC\u65b9\u6cd5\u5728ETAS\u6a21\u578b\u53c2\u6570\u4f30\u8ba1\u4e2d\u5b58\u5728\u8ba1\u7b97\u8017\u65f6\u3001\u53c2\u6570\u76f8\u5173\u6027\u9650\u5236\u548c\u53c2\u6570\u4e0d\u786e\u5b9a\u6027\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u8d1d\u53f6\u65af\u63a8\u65ad\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528INLA\u65b9\u6cd5\uff08inlabru\uff09\u8fdb\u884c\u8d1d\u53f6\u65af\u63a8\u65ad\uff0c\u901a\u8fc7\u6cf0\u52d2\u5c55\u5f00\u548c\u5206\u7bb1\u7b56\u7565\u8fd1\u4f3c\u6a21\u578b\u7684\u5bf9\u6570\u4f3c\u7136\u51fd\u6570\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e09\u4e2a\u5b9e\u9a8c\u6765\u9a8c\u8bc1\u65b9\u6cd5\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a\u03b1\u548cK\u53c2\u6570\u5b58\u5728\u660e\u663e\u76f8\u4e92\u5f71\u54cd\uff1b\u56fa\u5b9a\u53c2\u6570\u53ef\u51cf\u5c11\u4e00\u534a\u4ee5\u4e0a\u62df\u5408\u65f6\u95f4\uff1b\u771f\u5b9e\u6570\u636e\u548c\u5408\u6210\u76ee\u5f55\u7684\u5f52\u4e00\u5316\u4e8b\u4ef6\u95f4\u9694\u65f6\u95f4\u5206\u5e03\u4e00\u81f4\uff1b\u9700\u8981\u81f3\u5c11\u4e00\u4e2a\u4e3b\u9707\u53ca\u5176\u4f59\u9707\u624d\u80fd\u8fdb\u884c\u53ef\u9760\u9884\u6d4b\uff1b\u6570\u636e\u4e2d\u4e3b\u9707\u8d8a\u591a\u9884\u6d4b\u6548\u679c\u8d8a\u597d\u3002", "conclusion": "INLA\u65b9\u6cd5\u5728ETAS\u6a21\u578b\u4e2d\u4f18\u4e8e\u4f20\u7edfMCMC\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u53c2\u6570\u76f8\u5173\u6027\u548c\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u5730\u9707\u9884\u6d4b\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u8d1d\u53f6\u65af\u63a8\u65ad\u6846\u67b6\u3002"}}
{"id": "2510.13979", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13979", "abs": "https://arxiv.org/abs/2510.13979", "authors": ["Supriti Sinhamahapatra", "Jan Niehues"], "title": "Do Slides Help? Multi-modal Context for Automatic Transcription of Conference Talks", "comment": null, "summary": "State-of-the-art (SOTA) Automatic Speech Recognition (ASR) systems primarily\nrely on acoustic information while disregarding additional multi-modal context.\nHowever, visual information are essential in disambiguation and adaptation.\nWhile most work focus on speaker images to handle noise conditions, this work\nalso focuses on integrating presentation slides for the use cases of scientific\npresentation.\n  In a first step, we create a benchmark for multi-modal presentation including\nan automatic analysis of transcribing domain-specific terminology. Next, we\nexplore methods for augmenting speech models with multi-modal information. We\nmitigate the lack of datasets with accompanying slides by a suitable approach\nof data augmentation. Finally, we train a model using the augmented dataset,\nresulting in a relative reduction in word error rate of approximately 34%,\nacross all words and 35%, for domain-specific terms compared to the baseline\nmodel.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408\u89c6\u89c9\u4fe1\u606f\uff08\u6f14\u8bb2\u8005\u56fe\u50cf\u548c\u6f14\u793a\u5e7b\u706f\u7247\uff09\u7684\u591a\u6a21\u6001\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u65b9\u6cd5\uff0c\u4e13\u95e8\u9488\u5bf9\u79d1\u5b66\u6f14\u8bb2\u573a\u666f\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u6280\u672f\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u6570\u636e\u96c6\u7f3a\u4e4f\u7684\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bcd\u9519\u8bef\u7387\u3002", "motivation": "\u5f53\u524d\u6700\u5148\u8fdb\u7684ASR\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u58f0\u5b66\u4fe1\u606f\u800c\u5ffd\u7565\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\uff0c\u4f46\u89c6\u89c9\u4fe1\u606f\u5bf9\u4e8e\u6d88\u6b67\u548c\u9002\u5e94\u81f3\u5173\u91cd\u8981\u3002\u8be5\u5de5\u4f5c\u7279\u522b\u5173\u6ce8\u79d1\u5b66\u6f14\u8bb2\u573a\u666f\uff0c\u901a\u8fc7\u6574\u5408\u6f14\u793a\u5e7b\u706f\u7247\u6765\u63d0\u5347ASR\u6027\u80fd\u3002", "method": "\u9996\u5148\u521b\u5efa\u591a\u6a21\u6001\u6f14\u8bb2\u57fa\u51c6\uff0c\u7136\u540e\u63a2\u7d22\u7528\u591a\u6a21\u6001\u4fe1\u606f\u589e\u5f3a\u8bed\u97f3\u6a21\u578b\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u6280\u672f\u89e3\u51b3\u7f3a\u4e4f\u914d\u5957\u5e7b\u706f\u7247\u6570\u636e\u96c6\u7684\u95ee\u9898\uff0c\u6700\u540e\u4f7f\u7528\u589e\u5f3a\u6570\u636e\u96c6\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u8bad\u7ec3\u540e\u7684\u6a21\u578b\u5728\u6240\u6709\u8bcd\u6c47\u4e0a\u7684\u8bcd\u9519\u8bef\u7387\u76f8\u5bf9\u964d\u4f4e\u4e86\u7ea634%\uff0c\u5728\u9886\u57df\u7279\u5b9a\u672f\u8bed\u4e0a\u7684\u8bcd\u9519\u8bef\u7387\u76f8\u5bf9\u964d\u4f4e\u4e8635%\uff0c\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u591a\u6a21\u6001\u4fe1\u606f\uff08\u7279\u522b\u662f\u6f14\u793a\u5e7b\u706f\u7247\uff09\u7684\u6574\u5408\u80fd\u591f\u663e\u8457\u63d0\u5347ASR\u7cfb\u7edf\u5728\u79d1\u5b66\u6f14\u8bb2\u573a\u666f\u4e0b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u9886\u57df\u7279\u5b9a\u672f\u8bed\u65b9\u9762\u6548\u679c\u660e\u663e\u3002"}}
{"id": "2510.14052", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.14052", "abs": "https://arxiv.org/abs/2510.14052", "authors": ["Xixing Xue", "Dong Shen", "Steven X. Ding", "Dong Zhao"], "title": "Dual Detection Framework for Faults and Integrity Attacks in Cyber-Physical Control Systems", "comment": null, "summary": "Anomaly detection plays a vital role in the security and safety of\ncyber-physical control systems, and accurately distinguishing between different\nanomaly types is crucial for system recovery and mitigation. This study\nproposes a dual detection framework for anomaly detection and discrimination.\nBy leveraging the dynamic characteristics of control loops and the stealthiness\nfeatures of integrity attacks, the closed-loop stealthiness condition is first\nderived, and two dedicated detectors are designed and deployed on the\ncontroller side and the plant side, respectively, enabling joint plant fault\nand cyber attack detection. Moreover, by jointly analyzing the residual\nresponse of the two detectors corresponding to different anomalies, it is\nproved that the proposed method can distinguish between faults and integrity\nattacks due to the detectors' individual residual spaces. According to the\ndetector's residual space, the fault and attack detection performance is\nfurther improved by a two-stage optimization scheme. Simulation results\nvalidate the effectiveness of the proposed approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5f02\u5e38\u68c0\u6d4b\u548c\u533a\u5206\u7684\u53cc\u91cd\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u63a7\u5236\u56de\u8def\u7684\u52a8\u6001\u7279\u6027\u548c\u5b8c\u6574\u6027\u653b\u51fb\u7684\u9690\u853d\u6027\u7279\u5f81\uff0c\u8bbe\u8ba1\u5e76\u90e8\u7f72\u4e86\u4e24\u4e2a\u4e13\u7528\u68c0\u6d4b\u5668\uff0c\u80fd\u591f\u8054\u5408\u68c0\u6d4b\u5de5\u5382\u6545\u969c\u548c\u7f51\u7edc\u653b\u51fb\uff0c\u5e76\u80fd\u533a\u5206\u6545\u969c\u548c\u5b8c\u6574\u6027\u653b\u51fb\u3002", "motivation": "\u5728\u4fe1\u606f\u7269\u7406\u63a7\u5236\u7cfb\u7edf\u4e2d\uff0c\u5f02\u5e38\u68c0\u6d4b\u5bf9\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u51c6\u786e\u533a\u5206\u4e0d\u540c\u7c7b\u578b\u7684\u5f02\u5e38\u5bf9\u4e8e\u7cfb\u7edf\u6062\u590d\u548c\u7f13\u89e3\u63aa\u65bd\u81f3\u5173\u91cd\u8981\u3002", "method": "\u9996\u5148\u63a8\u5bfc\u95ed\u73af\u9690\u853d\u6027\u6761\u4ef6\uff0c\u7136\u540e\u5728\u63a7\u5236\u5668\u4fa7\u548c\u5de5\u5382\u4fa7\u5206\u522b\u8bbe\u8ba1\u90e8\u7f72\u4e13\u7528\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u8054\u5408\u5206\u6790\u4e24\u4e2a\u68c0\u6d4b\u5668\u5bf9\u4e0d\u540c\u5f02\u5e38\u7684\u6b8b\u5dee\u54cd\u5e94\u6765\u533a\u5206\u6545\u969c\u548c\u653b\u51fb\uff0c\u5e76\u91c7\u7528\u4e24\u9636\u6bb5\u4f18\u5316\u65b9\u6848\u63d0\u9ad8\u68c0\u6d4b\u6027\u80fd\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u6210\u529f\u68c0\u6d4b\u5e76\u533a\u5206\u5de5\u5382\u6545\u969c\u548c\u7f51\u7edc\u653b\u51fb\u3002", "conclusion": "\u8be5\u53cc\u91cd\u68c0\u6d4b\u6846\u67b6\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u548c\u533a\u5206\u63a7\u5236\u7cfb\u7edf\u4e2d\u7684\u6545\u969c\u548c\u7f51\u7edc\u653b\u51fb\uff0c\u4e3a\u7cfb\u7edf\u5b89\u5168\u63d0\u4f9b\u4e86\u53ef\u9760\u4fdd\u969c\u3002"}}
{"id": "2510.14818", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2510.14818", "abs": "https://arxiv.org/abs/2510.14818", "authors": ["Christine Sowa Lepird", "Lynnette Hui Xian Ng", "Kathleen M. Carley"], "title": "Trends of Pink Slime Journalism Advertisement Expenditure and Spread on Facebook from 2019-2024", "comment": null, "summary": "Pink slime journalism is a practice where news outlets publish low-quality or\ninflammatory partisan articles, claiming to be local news networks. This paper\nexamines the spread of pink slime sites on Facebook using public posts from\nPages and Groups. We evaluate the trends of sharing pink slime sites on\nFacebook and patterns regarding the advertisements purchased by the parent\norganizations of the pink slime news networks. Our analysis discovers that\nwhile the number of pink slime posts on Facebook pages have decreased over the\nyears, advertising dollars have increased. The increase in advertising dollars\ninfluences an increase in Facebook group posts. Further, the advertising\nexpenditure increases during election years, but contentious topics are still\ndiscussed during non-election years. By illustrating the patterns and themes\nfrom US election years of 2020, 2022, and 2024, this research offers insights\ninto potentially dangerous journalism tactics, and provides predictions for\nfuture US Presidential Elections.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86Facebook\u4e0a\u7c89\u7ea2\u7c98\u6db2\u65b0\u95fb\u7f51\u7ad9\u7684\u4f20\u64ad\u8d8b\u52bf\uff0c\u53d1\u73b0\u867d\u7136\u9875\u9762\u53d1\u5e16\u91cf\u4e0b\u964d\uff0c\u4f46\u5e7f\u544a\u652f\u51fa\u589e\u52a0\uff0c\u7279\u522b\u662f\u5728\u9009\u4e3e\u5e74\uff0c\u8fd9\u5f71\u54cd\u4e86\u7fa4\u7ec4\u53d1\u5e16\u91cf\u7684\u589e\u957f\u3002", "motivation": "\u7814\u7a76\u7c89\u7ea2\u7c98\u6db2\u65b0\u95fb\uff08\u4f4e\u8d28\u91cf\u3001\u717d\u52a8\u6027\u515a\u6d3e\u6587\u7ae0\u4f2a\u88c5\u6210\u5730\u65b9\u65b0\u95fb\uff09\u5728Facebook\u4e0a\u7684\u4f20\u64ad\u6a21\u5f0f\u53ca\u5176\u5e7f\u544a\u7b56\u7565\uff0c\u63ed\u793a\u6f5c\u5728\u5371\u9669\u7684\u65b0\u95fb\u5b9e\u8df5\u3002", "method": "\u4f7f\u7528Facebook\u516c\u5171\u9875\u9762\u548c\u7fa4\u7ec4\u7684\u5e16\u5b50\u6570\u636e\uff0c\u5206\u6790\u7c89\u7ea2\u7c98\u6db2\u7f51\u7ad9\u7684\u5206\u4eab\u8d8b\u52bf\u548c\u5176\u6bcd\u516c\u53f8\u8d2d\u4e70\u7684\u5e7f\u544a\u6a21\u5f0f\u3002", "result": "\u7c89\u7ea2\u7c98\u6db2\u7f51\u7ad9\u5728Facebook\u9875\u9762\u7684\u53d1\u5e16\u91cf\u9010\u5e74\u51cf\u5c11\uff0c\u4f46\u5e7f\u544a\u652f\u51fa\u589e\u52a0\uff0c\u7279\u522b\u662f\u5728\u9009\u4e3e\u5e74\uff1b\u5e7f\u544a\u652f\u51fa\u589e\u52a0\u5bfc\u81f4\u7fa4\u7ec4\u53d1\u5e16\u91cf\u4e0a\u5347\uff1b\u9009\u4e3e\u5e74\u5e7f\u544a\u652f\u51fa\u6fc0\u589e\uff0c\u4f46\u4e89\u8bae\u8bdd\u9898\u5728\u975e\u9009\u4e3e\u5e74\u4ecd\u88ab\u8ba8\u8bba\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u7c89\u7ea2\u7c98\u6db2\u65b0\u95fb\u7684\u5371\u9669\u4f20\u64ad\u6a21\u5f0f\uff0c\u63d0\u4f9b\u4e86\u5bf9\u672a\u6765\u7f8e\u56fd\u603b\u7edf\u9009\u4e3e\u7684\u9884\u6d4b\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u5173\u6ce8\u6b64\u7c7b\u65b0\u95fb\u5b9e\u8df5\u5bf9\u516c\u5171\u8bdd\u8bed\u7684\u5f71\u54cd\u3002"}}
{"id": "2510.14063", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14063", "abs": "https://arxiv.org/abs/2510.14063", "authors": ["Nan Li", "Jiming Ren", "Haris Miller", "Samuel Coogan", "Karen M. Feigh", "Ye Zhao"], "title": "Adaptive Obstacle-Aware Task Assignment and Planning for Heterogeneous Robot Teaming", "comment": "16 pages, 11 figures, 4 tables", "summary": "Multi-Agent Task Assignment and Planning (MATP) has attracted growing\nattention but remains challenging in terms of scalability, spatial reasoning,\nand adaptability in obstacle-rich environments. To address these challenges, we\npropose OATH: Adaptive Obstacle-Aware Task Assignment and Planning for\nHeterogeneous Robot Teaming, which advances MATP by introducing a novel\nobstacle-aware strategy for task assignment. First, we develop an adaptive\nHalton sequence map, the first known application of Halton sampling with\nobstacle-aware adaptation in MATP, which adjusts sampling density based on\nobstacle distribution. Second, we propose a cluster-auction-selection framework\nthat integrates obstacle-aware clustering with weighted auctions and\nintra-cluster task selection. These mechanisms jointly enable effective\ncoordination among heterogeneous robots while maintaining scalability and\nnear-optimal allocation performance. In addition, our framework leverages an\nLLM to interpret human instructions and directly guide the planner in real\ntime. We validate OATH in NVIDIA Isaac Sim, showing substantial improvements in\ntask assignment quality, scalability, adaptability to dynamic changes, and\noverall execution performance compared to state-of-the-art MATP baselines. A\nproject website is available at https://llm-oath.github.io/.", "AI": {"tldr": "\u63d0\u51faOATH\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94Halton\u5e8f\u5217\u5730\u56fe\u548c\u96c6\u7fa4-\u62cd\u5356-\u9009\u62e9\u673a\u5236\uff0c\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u4efb\u52a1\u5206\u914d\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u3001\u7a7a\u95f4\u63a8\u7406\u548c\u969c\u788d\u7269\u73af\u5883\u9002\u5e94\u6027\u6311\u6218\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u4efb\u52a1\u5206\u914d\u4e0e\u89c4\u5212\u5728\u53ef\u6269\u5c55\u6027\u3001\u7a7a\u95f4\u63a8\u7406\u548c\u969c\u788d\u7269\u4e30\u5bcc\u73af\u5883\u9002\u5e94\u6027\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u81ea\u9002\u5e94Halton\u5e8f\u5217\u5730\u56fe\uff08\u9996\u6b21\u5728MATP\u4e2d\u5e94\u7528Halton\u91c7\u6837\uff09\uff0c\u63d0\u51fa\u96c6\u7fa4-\u62cd\u5356-\u9009\u62e9\u6846\u67b6\uff0c\u96c6\u6210\u969c\u788d\u611f\u77e5\u805a\u7c7b\u3001\u52a0\u6743\u62cd\u5356\u548c\u96c6\u7fa4\u5185\u4efb\u52a1\u9009\u62e9\uff0c\u5e76\u5229\u7528LLM\u89e3\u91ca\u4eba\u7c7b\u6307\u4ee4\u5b9e\u65f6\u6307\u5bfc\u89c4\u5212\u5668\u3002", "result": "\u5728NVIDIA Isaac Sim\u4e2d\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684MATP\u57fa\u7ebf\uff0c\u5728\u4efb\u52a1\u5206\u914d\u8d28\u91cf\u3001\u53ef\u6269\u5c55\u6027\u3001\u52a8\u6001\u53d8\u5316\u9002\u5e94\u6027\u548c\u6574\u4f53\u6267\u884c\u6027\u80fd\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "OATH\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u969c\u788d\u611f\u77e5\u7b56\u7565\u6709\u6548\u534f\u8c03\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\uff0c\u4fdd\u6301\u53ef\u6269\u5c55\u6027\u548c\u63a5\u8fd1\u6700\u4f18\u7684\u5206\u914d\u6027\u80fd\u3002"}}
{"id": "2510.14011", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.14011", "abs": "https://arxiv.org/abs/2510.14011", "authors": ["Saman Hosseini", "Lee W. Cohnstaedt", "Matin Marjani", "Caterina Scoglio"], "title": "A Data-Parsimonious Model for Long-Term Risk Assessments of West Nile Virus Spillover", "comment": null, "summary": "Many West Nile virus (WNV) forecasting frameworks incorporate entomological\nor avian surveillance data, which may be unavailable in some regions. We\nintroduce a novel data-parsimonious probabilistic model to predict both the\ntiming of outbreak onset and the seasonal severity of WNV spillover. Our\napproach combines a temperature-driven compartmental model of WNV with\nnonparametric kernel density estimation methods to construct a joint\nprobability density function and a Poisson rate surface as function of mosquito\nabundance and normalized cumulative temperature. Calibrated on human incidence\nrecords, the model produces reliable forecasts several months before the\ntransmission season begins, supporting proactive mitigation efforts. We\nevaluated the framework across three counties in California (Orange, Los\nAngeles, and Riverside), two in Texas (Dallas and Harris), and one in Florida\n(Duval), representing completely different ecology and distinct climatic\nregimes, and observed strong agreement across multiple performance metrics.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6570\u636e\u7b80\u7ea6\u7684\u897f\u5c3c\u7f57\u6cb3\u75c5\u6bd2\u9884\u6d4b\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u7ed3\u5408\u6e29\u5ea6\u9a71\u52a8\u7684WNV\u4f20\u64ad\u6a21\u578b\u548c\u975e\u53c2\u6570\u6838\u5bc6\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4f20\u64ad\u5b63\u8282\u5f00\u59cb\u524d\u51e0\u4e2a\u6708\u63d0\u4f9b\u53ef\u9760\u7684\u75ab\u60c5\u7206\u53d1\u65f6\u95f4\u548c\u4e25\u91cd\u7a0b\u5ea6\u9884\u6d4b\u3002", "motivation": "\u8bb8\u591aWNV\u9884\u6d4b\u6846\u67b6\u9700\u8981\u6606\u866b\u5b66\u6216\u9e1f\u7c7b\u76d1\u6d4b\u6570\u636e\uff0c\u4f46\u8fd9\u4e9b\u6570\u636e\u5728\u67d0\u4e9b\u5730\u533a\u53ef\u80fd\u65e0\u6cd5\u83b7\u5f97\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u6570\u636e\u9700\u6c42\u66f4\u5c11\u7684\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u6e29\u5ea6\u9a71\u52a8\u7684WNV\u4f20\u64ad\u6a21\u578b\u548c\u975e\u53c2\u6570\u6838\u5bc6\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u6784\u5efa\u8054\u5408\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\u548c\u6cca\u677e\u7387\u66f2\u9762\uff0c\u4f5c\u4e3a\u868a\u866b\u4e30\u5ea6\u548c\u5f52\u4e00\u5316\u7d2f\u79ef\u6e29\u5ea6\u7684\u51fd\u6570\u3002", "result": "\u6a21\u578b\u5728\u52a0\u5dde\u3001\u5fb7\u514b\u8428\u65af\u5dde\u548c\u4f5b\u7f57\u91cc\u8fbe\u5dde\u7684\u516d\u4e2a\u53bf\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u8fd9\u4e9b\u5730\u533a\u5177\u6709\u5b8c\u5168\u4e0d\u540c\u7684\u751f\u6001\u548c\u6c14\u5019\u6761\u4ef6\uff0c\u7ed3\u679c\u663e\u793a\u5728\u591a\u4e2a\u6027\u80fd\u6307\u6807\u4e0a\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u8be5\u6570\u636e\u7b80\u7ea6\u7684\u6982\u7387\u6a21\u578b\u80fd\u591f\u53ef\u9760\u5730\u9884\u6d4bWNV\u75ab\u60c5\u7206\u53d1\u7684\u65f6\u95f4\u548c\u4e25\u91cd\u7a0b\u5ea6\uff0c\u652f\u6301\u5728\u4f20\u64ad\u5b63\u8282\u5f00\u59cb\u524d\u91c7\u53d6\u4e3b\u52a8\u9632\u63a7\u63aa\u65bd\u3002"}}
{"id": "2510.13985", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13985", "abs": "https://arxiv.org/abs/2510.13985", "authors": ["Mar\u00eda Victoria Carro", "Denise Alejandra Mester", "Francisca Gauna Selasco", "Giovanni Franco Gabriel Marraffini", "Mario Alejandro Leiva", "Gerardo I. Simari", "Mar\u00eda Vanina Martinez"], "title": "Do Large Language Models Show Biases in Causal Learning? Insights from Contingency Judgment", "comment": null, "summary": "Causal learning is the cognitive process of developing the capability of\nmaking causal inferences based on available information, often guided by\nnormative principles. This process is prone to errors and biases, such as the\nillusion of causality, in which people perceive a causal relationship between\ntwo variables despite lacking supporting evidence. This cognitive bias has been\nproposed to underlie many societal problems, including social prejudice,\nstereotype formation, misinformation, and superstitious thinking. In this work,\nwe examine whether large language models are prone to developing causal\nillusions when faced with a classic cognitive science paradigm: the contingency\njudgment task. To investigate this, we constructed a dataset of 1,000 null\ncontingency scenarios (in which the available information is not sufficient to\nestablish a causal relationship between variables) within medical contexts and\nprompted LLMs to evaluate the effectiveness of potential causes. Our findings\nshow that all evaluated models systematically inferred unwarranted causal\nrelationships, revealing a strong susceptibility to the illusion of causality.\nWhile there is ongoing debate about whether LLMs genuinely understand causality\nor merely reproduce causal language without true comprehension, our findings\nsupport the latter hypothesis and raise concerns about the use of language\nmodels in domains where accurate causal reasoning is essential for informed\ndecision-making.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u56e0\u679c\u63a8\u7406\u4efb\u52a1\u4e2d\u5bb9\u6613\u4ea7\u751f\u56e0\u679c\u5e7b\u89c9\uff0c\u5373\u4f7f\u5728\u6ca1\u6709\u8db3\u591f\u8bc1\u636e\u652f\u6301\u7684\u60c5\u51b5\u4e0b\u4e5f\u4f1a\u9519\u8bef\u63a8\u65ad\u56e0\u679c\u5173\u7cfb\u3002", "motivation": "\u63a2\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u4f1a\u5728\u7ecf\u5178\u7684\u8ba4\u77e5\u79d1\u5b66\u8303\u5f0f\u2014\u2014\u5217\u8054\u5224\u65ad\u4efb\u52a1\u4e2d\u53d1\u5c55\u51fa\u56e0\u679c\u5e7b\u89c9\uff0c\u8fd9\u79cd\u8ba4\u77e5\u504f\u89c1\u88ab\u8ba4\u4e3a\u662f\u8bb8\u591a\u793e\u4f1a\u95ee\u9898\u7684\u6839\u6e90\u3002", "method": "\u6784\u5efa\u4e861000\u4e2a\u533b\u5b66\u80cc\u666f\u4e0b\u7684\u96f6\u5217\u8054\u573a\u666f\u6570\u636e\u96c6\uff0c\u8ba9LLMs\u8bc4\u4f30\u6f5c\u5728\u539f\u56e0\u7684\u6709\u6548\u6027\uff0c\u8fd9\u4e9b\u573a\u666f\u4e2d\u7684\u4fe1\u606f\u4e0d\u8db3\u4ee5\u5efa\u7acb\u53d8\u91cf\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u3002", "result": "\u6240\u6709\u8bc4\u4f30\u7684\u6a21\u578b\u90fd\u7cfb\u7edf\u5730\u63a8\u65ad\u51fa\u65e0\u6839\u636e\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u663e\u793a\u51fa\u5bf9\u56e0\u679c\u5e7b\u89c9\u7684\u5f3a\u70c8\u6613\u611f\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u652f\u6301LLMs\u53ea\u662f\u590d\u5236\u56e0\u679c\u8bed\u8a00\u800c\u975e\u771f\u6b63\u7406\u89e3\u56e0\u679c\u5173\u7cfb\u7684\u5047\u8bbe\uff0c\u5bf9\u5728\u9700\u8981\u51c6\u786e\u56e0\u679c\u63a8\u7406\u7684\u9886\u57df\u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u63d0\u51fa\u4e86\u62c5\u5fe7\u3002"}}
{"id": "2510.14075", "categories": ["eess.SY", "cs.AI", "cs.SY", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.14075", "abs": "https://arxiv.org/abs/2510.14075", "authors": ["Milad Hoseinpour", "Vladimir Dvorkin"], "title": "DiffOPF: Diffusion Solver for Optimal Power Flow", "comment": "7 pages, 4 figures, 2 tables", "summary": "The optimal power flow (OPF) is a multi-valued, non-convex mapping from loads\nto dispatch setpoints. The variability of system parameters (e.g., admittances,\ntopology) further contributes to the multiplicity of dispatch setpoints for a\ngiven load. Existing deep learning OPF solvers are single-valued and thus fail\nto capture the variability of system parameters unless fully represented in the\nfeature space, which is prohibitive. To solve this problem, we introduce a\ndiffusion-based OPF solver, termed \\textit{DiffOPF}, that treats OPF as a\nconditional sampling problem. The solver learns the joint distribution of loads\nand dispatch setpoints from operational history, and returns the marginal\ndispatch distributions conditioned on loads. Unlike single-valued solvers,\nDiffOPF enables sampling statistically credible warm starts with favorable cost\nand constraint satisfaction trade-offs. We explore the sample complexity of\nDiffOPF to ensure the OPF solution within a prescribed distance from the\noptimization-based solution, and verify this experimentally on power system\nbenchmarks.", "AI": {"tldr": "\u63d0\u51faDiffOPF\uff0c\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684OPF\u6c42\u89e3\u5668\uff0c\u5c06\u6700\u4f18\u6f6e\u6d41\u95ee\u9898\u89c6\u4e3a\u6761\u4ef6\u91c7\u6837\u95ee\u9898\uff0c\u80fd\u591f\u5904\u7406\u7cfb\u7edf\u53c2\u6570\u53d8\u5316\u5e26\u6765\u7684\u591a\u503c\u6620\u5c04\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60OPF\u6c42\u89e3\u5668\u662f\u5355\u503c\u7684\uff0c\u65e0\u6cd5\u6355\u6349\u7cfb\u7edf\u53c2\u6570\uff08\u5982\u5bfc\u7eb3\u3001\u62d3\u6251\u7ed3\u6784\uff09\u53d8\u5316\u5bfc\u81f4\u7684\u8c03\u5ea6\u8bbe\u5b9a\u70b9\u591a\u6837\u6027\uff0c\u9664\u975e\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u5b8c\u5168\u8868\u793a\u8fd9\u4e9b\u53c2\u6570\uff0c\u4f46\u8fd9\u5728\u8ba1\u7b97\u4e0a\u4e0d\u53ef\u884c\u3002", "method": "DiffOPF\u5b66\u4e60\u8d1f\u8377\u548c\u8c03\u5ea6\u8bbe\u5b9a\u70b9\u7684\u8054\u5408\u5206\u5e03\uff0c\u8fd4\u56de\u57fa\u4e8e\u8d1f\u8377\u6761\u4ef6\u7684\u8fb9\u9645\u8c03\u5ea6\u5206\u5e03\u3002\u5b83\u5c06OPF\u89c6\u4e3a\u6761\u4ef6\u91c7\u6837\u95ee\u9898\uff0c\u4f7f\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u7edf\u8ba1\u53ef\u4fe1\u7684\u9884\u70ed\u542f\u52a8\u70b9\u3002", "result": "DiffOPF\u80fd\u591f\u91c7\u6837\u5177\u6709\u826f\u597d\u6210\u672c\u548c\u7ea6\u675f\u6ee1\u8db3\u6743\u8861\u7684\u7edf\u8ba1\u53ef\u4fe1\u9884\u70ed\u542f\u52a8\u70b9\uff0c\u5728\u7535\u529b\u7cfb\u7edf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u5176\u5728\u89c4\u5b9a\u8ddd\u79bb\u5185\u627e\u5230\u4f18\u5316\u89e3\u7684\u80fd\u529b\u3002", "conclusion": "DiffOPF\u89e3\u51b3\u4e86\u4f20\u7edf\u5355\u503cOPF\u6c42\u89e3\u5668\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u6761\u4ef6\u91c7\u6837\u65b9\u6cd5\u6709\u6548\u5904\u7406\u7cfb\u7edf\u53c2\u6570\u53d8\u5316\u5e26\u6765\u7684\u591a\u503c\u6620\u5c04\u95ee\u9898\uff0c\u4e3a\u7535\u529b\u7cfb\u7edf\u4f18\u5316\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.14892", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2510.14892", "abs": "https://arxiv.org/abs/2510.14892", "authors": ["Shubham Varma", "Ananya Warior", "Avani Sakhapara", "Dipti Pawade"], "title": "A Comprehensive Framework for Efficient Court Case Management and Prioritization", "comment": null, "summary": "The Indian judicial system faces a critical challenge with approximately 52\nmillion pending cases, causing significant delays that impact socio-economic\nstability. This study proposes a cloud-based software framework to classify and\nprioritize court cases using algorithmic methods based on parameters such as\nseverity of crime committed, responsibility of parties involved, case filing\ndates, previous hearing's data, priority level (e.g., Urgent, Medium, Ordinary)\nprovided as input, and relevant Indian Penal Code (IPC), Code of Criminal\nProcedure (CrPC), and other legal sections (e.g., Hindu Marriage Act, Indian\nContract Act). Cases are initially entered by advocates on record or court\nregistrars, followed by automated hearing date allocation that balances fresh\nand old cases while accounting for court holidays and leaves. The system\nstreamlines appellate processes by fetching data from historical case\ndatabases. Our methodology integrates algorithmic prioritization, a robust\nnotification system, and judicial interaction, with features that allow judges\nto view daily case counts and their details. Simulations demonstrate that the\nsystem can process cases efficiently, with reliable notification delivery and\npositive user satisfaction among judges and registrars. Future iterations will\nincorporate advanced machine learning for dynamic prioritization, addressing\ncritical gaps in existing court case management systems to enhance efficiency\nand reduce backlogs.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4e91\u7684\u8f6f\u4ef6\u6846\u67b6\uff0c\u4f7f\u7528\u7b97\u6cd5\u65b9\u6cd5\u5bf9\u5370\u5ea6\u6cd5\u9662\u6848\u4ef6\u8fdb\u884c\u5206\u7c7b\u548c\u4f18\u5148\u7ea7\u6392\u5e8f\uff0c\u4ee5\u89e3\u51b35200\u4e07\u79ef\u538b\u6848\u4ef6\u95ee\u9898\u3002", "motivation": "\u5370\u5ea6\u53f8\u6cd5\u7cfb\u7edf\u9762\u4e345200\u4e07\u4ef6\u5f85\u5904\u7406\u6848\u4ef6\u7684\u4e25\u91cd\u6311\u6218\uff0c\u5bfc\u81f4\u91cd\u5927\u5ef6\u8bef\u5e76\u5f71\u54cd\u793e\u4f1a\u7ecf\u6d4e\u7a33\u5b9a\u3002", "method": "\u5f00\u53d1\u4e91\u57fa\u7840\u8f6f\u4ef6\u6846\u67b6\uff0c\u57fa\u4e8e\u72af\u7f6a\u4e25\u91cd\u6027\u3001\u5f53\u4e8b\u4eba\u8d23\u4efb\u3001\u6848\u4ef6\u5f52\u6863\u65e5\u671f\u3001\u5148\u524d\u542c\u8bc1\u6570\u636e\u3001\u4f18\u5148\u7ea7\u7b49\u53c2\u6570\u8fdb\u884c\u7b97\u6cd5\u5206\u7c7b\u548c\u4f18\u5148\u7ea7\u6392\u5e8f\uff0c\u6574\u5408\u81ea\u52a8\u542c\u8bc1\u65e5\u671f\u5206\u914d\u3001\u901a\u77e5\u7cfb\u7edf\u548c\u53f8\u6cd5\u4e92\u52a8\u529f\u80fd\u3002", "result": "\u6a21\u62df\u663e\u793a\u7cfb\u7edf\u80fd\u9ad8\u6548\u5904\u7406\u6848\u4ef6\uff0c\u5177\u6709\u53ef\u9760\u7684\u901a\u77e5\u4f20\u9012\u529f\u80fd\uff0c\u6cd5\u5b98\u548c\u767b\u8bb0\u5458\u7528\u6237\u6ee1\u610f\u5ea6\u826f\u597d\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u80fd\u6709\u6548\u63d0\u5347\u6cd5\u9662\u6848\u4ef6\u7ba1\u7406\u6548\u7387\uff0c\u51cf\u5c11\u79ef\u538b\uff0c\u672a\u6765\u5c06\u96c6\u6210\u673a\u5668\u5b66\u4e60\u5b9e\u73b0\u52a8\u6001\u4f18\u5148\u7ea7\u6392\u5e8f\u3002"}}
{"id": "2510.14065", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14065", "abs": "https://arxiv.org/abs/2510.14065", "authors": ["Gaoyuan Liu", "Joris de Winter", "Yuri Durodie", "Denis Steckelmacher", "Ann Nowe", "Bram Vanderborght"], "title": "Optimistic Reinforcement Learning-Based Skill Insertions for Task and Motion Planning", "comment": null, "summary": "Task and motion planning (TAMP) for robotics manipulation necessitates\nlong-horizon reasoning involving versatile actions and skills. While\ndeterministic actions can be crafted by sampling or optimizing with certain\nconstraints, planning actions with uncertainty, i.e., probabilistic actions,\nremains a challenge for TAMP. On the contrary, Reinforcement Learning (RL)\nexcels in acquiring versatile, yet short-horizon, manipulation skills that are\nrobust with uncertainties. In this letter, we design a method that integrates\nRL skills into TAMP pipelines. Besides the policy, a RL skill is defined with\ndata-driven logical components that enable the skill to be deployed by symbolic\nplanning. A plan refinement sub-routine is designed to further tackle the\ninevitable effect uncertainties. In the experiments, we compare our method with\nbaseline hierarchical planning from both TAMP and RL fields and illustrate the\nstrength of the method. The results show that by embedding RL skills, we extend\nthe capability of TAMP to domains with probabilistic skills, and improve the\nplanning efficiency compared to the previous methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u5f3a\u5316\u5b66\u4e60\u6280\u80fd\u96c6\u6210\u5230\u4efb\u52a1\u548c\u8fd0\u52a8\u89c4\u5212(TAMP)\u7ba1\u9053\u4e2d\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u903b\u8f91\u7ec4\u4ef6\u5b9a\u4e49RL\u6280\u80fd\uff0c\u5e76\u8bbe\u8ba1\u8ba1\u5212\u7ec6\u5316\u5b50\u7a0b\u5e8f\u6765\u5904\u7406\u6548\u679c\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "TAMP\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u9700\u8981\u957f\u65f6\u57df\u63a8\u7406\uff0c\u4f46\u5904\u7406\u5177\u6709\u4e0d\u786e\u5b9a\u6027\u7684\u6982\u7387\u52a8\u4f5c\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u800cRL\u64c5\u957f\u83b7\u53d6\u9c81\u68d2\u7684\u77ed\u65f6\u57df\u64cd\u4f5c\u6280\u80fd\uff0c\u4f46\u7f3a\u4e4f\u957f\u65f6\u57df\u89c4\u5212\u80fd\u529b\u3002", "method": "\u5c06RL\u6280\u80fd\u96c6\u6210\u5230TAMP\u4e2d\uff0c\u6bcf\u4e2aRL\u6280\u80fd\u9664\u4e86\u7b56\u7565\u5916\u8fd8\u5305\u542b\u6570\u636e\u9a71\u52a8\u7684\u903b\u8f91\u7ec4\u4ef6\uff0c\u4f7f\u6280\u80fd\u80fd\u591f\u88ab\u7b26\u53f7\u89c4\u5212\u90e8\u7f72\u3002\u8bbe\u8ba1\u4e86\u8ba1\u5212\u7ec6\u5316\u5b50\u7a0b\u5e8f\u6765\u5904\u7406\u6548\u679c\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u901a\u8fc7\u5d4c\u5165RL\u6280\u80fd\uff0c\u6269\u5c55\u4e86TAMP\u5728\u6982\u7387\u6280\u80fd\u9886\u57df\u7684\u80fd\u529b\uff0c\u5e76\u76f8\u6bd4\u4e4b\u524d\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e86\u89c4\u5212\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5730\u5c06RL\u6280\u80fd\u4e0eTAMP\u76f8\u7ed3\u5408\uff0c\u89e3\u51b3\u4e86\u6982\u7387\u52a8\u4f5c\u89c4\u5212\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u89c4\u5212\u6548\u7387\u548c\u80fd\u529b\u3002"}}
{"id": "2510.14723", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.14723", "abs": "https://arxiv.org/abs/2510.14723", "authors": ["Cormac MacDermott", "Carl J. Scarrott", "John Ferguson"], "title": "Bayes-ically fair: A Bayesian Ranking of the Olympic Medal Table", "comment": null, "summary": "Evaluating a country's sporting success provides insight into its\ndecision-making and infrastructure for developing athletic talent. The Olympic\nGames serve as a global benchmark, yet conventional medal rankings can be\nunduly influenced by population size. We propose a Bayesian ranking scheme to\nrank the performance of National Olympic Committees by their \"long-run\"\nmedals-to-population ratio. The algorithm aims to mitigate the influence of\nlarge populations and reduce the stochastic fluctuations for smaller nations by\napplying shrinkage. These long-run rankings provide a more stable and\ninterpretable ordering of national sporting performance across games compared\nto existing methods.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.14035", "categories": ["cs.AI", "I.2.6; I.2.9"], "pdf": "https://arxiv.org/pdf/2510.14035", "abs": "https://arxiv.org/abs/2510.14035", "authors": ["Rajesh Mangannavar", "Prasad Tadepalli"], "title": "GammaZero: Learning To Guide POMDP Belief Space Search With Graph Representations", "comment": "10 pages content. 2 pages references", "summary": "We introduce an action-centric graph representation framework for learning to\nguide planning in Partially Observable Markov Decision Processes (POMDPs).\nUnlike existing approaches that require domain-specific neural architectures\nand struggle with scalability, GammaZero leverages a unified graph-based belief\nrepresentation that enables generalization across problem sizes within a\ndomain. Our key insight is that belief states can be systematically transformed\ninto action-centric graphs where structural patterns learned on small problems\ntransfer to larger instances. We employ a graph neural network with a decoder\narchitecture to learn value functions and policies from expert demonstrations\non computationally tractable problems, then apply these learned heuristics to\nguide Monte Carlo tree search on larger problems. Experimental results on\nstandard POMDP benchmarks demonstrate that GammaZero achieves comparable\nperformance to BetaZero when trained and tested on the same-sized problems,\nwhile uniquely enabling zero-shot generalization to problems 2-4 times larger\nthan those seen during training, maintaining solution quality with reduced\nsearch requirements.", "AI": {"tldr": "GammaZero\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u4f5c\u4e2d\u5fc3\u56fe\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u6307\u5bfc\u89c4\u5212\u5b66\u4e60\uff0c\u80fd\u591f\u5728\u672a\u89c1\u8fc7\u7684\u66f4\u5927\u89c4\u6a21\u95ee\u9898\u4e0a\u5b9e\u73b0\u96f6\u6837\u672c\u6cdb\u5316\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u9886\u57df\u7279\u5b9a\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u4e14\u96be\u4ee5\u6269\u5c55\uff0cGammaZero\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u7edf\u4e00\u7684\u56fe\u8868\u793a\u6846\u67b6\uff0c\u5b9e\u73b0\u5728\u9886\u57df\u5185\u8de8\u95ee\u9898\u89c4\u6a21\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5c06\u4fe1\u5ff5\u72b6\u6001\u7cfb\u7edf\u6027\u5730\u8f6c\u6362\u4e3a\u52a8\u4f5c\u4e2d\u5fc3\u56fe\uff0c\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u89e3\u7801\u5668\u67b6\u6784\u4ece\u4e13\u5bb6\u6f14\u793a\u4e2d\u5b66\u4e60\u4ef7\u503c\u51fd\u6570\u548c\u7b56\u7565\uff0c\u7136\u540e\u5c06\u5b66\u4e60\u5230\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u5e94\u7528\u4e8e\u66f4\u5927\u95ee\u9898\u7684\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u3002", "result": "\u5728\u6807\u51c6POMDP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGammaZero\u5728\u76f8\u540c\u89c4\u6a21\u95ee\u9898\u4e0a\u4e0eBetaZero\u6027\u80fd\u76f8\u5f53\uff0c\u540c\u65f6\u80fd\u591f\u96f6\u6837\u672c\u6cdb\u5316\u5230\u8bad\u7ec3\u65f6\u672a\u89c1\u8fc7\u76842-4\u500d\u5927\u7684\u95ee\u9898\uff0c\u4fdd\u6301\u89e3\u8d28\u91cf\u7684\u540c\u65f6\u51cf\u5c11\u641c\u7d22\u9700\u6c42\u3002", "conclusion": "GammaZero\u901a\u8fc7\u52a8\u4f5c\u4e2d\u5fc3\u56fe\u8868\u793a\u5b9e\u73b0\u4e86\u5728POMDP\u89c4\u5212\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u5904\u7406\u5927\u89c4\u6a21\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2510.14100", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.14100", "abs": "https://arxiv.org/abs/2510.14100", "authors": ["Rohan Walia", "Mitchell Black", "Andrew Schoer", "Kevin Leahy"], "title": "Belief Space Control of Safety-Critical Systems Under State-Dependent Measurement Noise", "comment": "Preprint - Submitted to the 2026 American Control Conference", "summary": "Safety-critical control is imperative for deploying autonomous systems in the\nreal world. Control Barrier Functions (CBFs) offer strong safety guarantees\nwhen accurate system and sensor models are available. However, widely used\nadditive, fixed-noise models are not representative of complex sensor\nmodalities with state-dependent error characteristics. Although CBFs have been\ndesigned to mitigate uncertainty using fixed worst-case bounds on measurement\nnoise, this approach can lead to overly-conservative control. To solve this\nproblem, we extend the Belief Control Barrier Function (BCBF) framework to\naccommodate state-dependent measurement noise via the Generalized Extended\nKalman Filter (GEKF) algorithm, which models measurement noise as a linear\nfunction of the state. Using the original BCBF framework as baseline, we\ndemonstrate the performance of the BCBF-GEKF approach through simulation\nresults on a 1D single integrator setpoint tracking scenario and 2D unicycle\nkinematics trajectory tracking scenario. Our results confirm that the BCBF-GEKF\napproach offers less conservative control with greater safety.", "AI": {"tldr": "\u6269\u5c55\u4fe1\u5ff5\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u6846\u67b6\u4ee5\u5904\u7406\u72b6\u6001\u76f8\u5173\u7684\u6d4b\u91cf\u566a\u58f0\uff0c\u4f7f\u7528\u5e7f\u4e49\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u7b97\u6cd5\uff0c\u51cf\u5c11\u4fdd\u5b88\u6027\u63a7\u5236\u5e76\u63d0\u9ad8\u5b89\u5168\u6027", "motivation": "\u4f20\u7edf\u56fa\u5b9a\u566a\u58f0\u6a21\u578b\u65e0\u6cd5\u51c6\u786e\u8868\u793a\u5177\u6709\u72b6\u6001\u76f8\u5173\u8bef\u5dee\u7279\u5f81\u7684\u590d\u6742\u4f20\u611f\u5668\u6a21\u6001\uff0c\u800c\u4f7f\u7528\u56fa\u5b9a\u6700\u574f\u60c5\u51b5\u8fb9\u754c\u7684CBF\u65b9\u6cd5\u4f1a\u5bfc\u81f4\u8fc7\u5ea6\u4fdd\u5b88\u7684\u63a7\u5236", "method": "\u5c06\u4fe1\u5ff5\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u6846\u67b6\u6269\u5c55\u5230\u72b6\u6001\u76f8\u5173\u6d4b\u91cf\u566a\u58f0\uff0c\u91c7\u7528\u5e7f\u4e49\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u7b97\u6cd5\u5efa\u6a21\u6d4b\u91cf\u566a\u58f0\u4e3a\u72b6\u6001\u7684\u7ebf\u6027\u51fd\u6570", "result": "\u57281D\u5355\u79ef\u5206\u5668\u8bbe\u5b9a\u70b9\u8ddf\u8e2a\u548c2D\u72ec\u8f6e\u8f66\u8fd0\u52a8\u5b66\u8f68\u8ff9\u8ddf\u8e2a\u573a\u666f\u4e2d\uff0cBCBF-GEKF\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u5c11\u4fdd\u5b88\u7684\u63a7\u5236\u548c\u66f4\u9ad8\u7684\u5b89\u5168\u6027", "conclusion": "BCBF-GEKF\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u72b6\u6001\u76f8\u5173\u6d4b\u91cf\u566a\u58f0\uff0c\u5728\u4fdd\u8bc1\u5b89\u5168\u6027\u7684\u540c\u65f6\u51cf\u5c11\u63a7\u5236\u4fdd\u5b88\u6027"}}
{"id": "2510.14072", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14072", "abs": "https://arxiv.org/abs/2510.14072", "authors": ["Hemjyoti Das", "Christian Ott"], "title": "Partial Feedback Linearization Control of a Cable-Suspended Multirotor Platform for Stabilization of an Attached Load", "comment": "Accepted for IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025", "summary": "In this work, we present a novel control approach based on partial feedback\nlinearization (PFL) for the stabilization of a suspended aerial platform with\nan attached load. Such systems are envisioned for various applications in\nconstruction sites involving cranes, such as the holding and transportation of\nheavy objects. Our proposed control approach considers the underactuation of\nthe whole system while utilizing its coupled dynamics for stabilization. We\ndemonstrate using numerical stability analysis that these coupled terms are\ncrucial for the stabilization of the complete system. We also carried out\nrobustness analysis of the proposed approach in the presence of external wind\ndisturbances, sensor noise, and uncertainties in system dynamics. As our\nenvisioned target application involves cranes in outdoor construction sites,\nour control approaches rely on only onboard sensors, thus making it suitable\nfor such applications. We carried out extensive simulation studies and\nexperimental tests to validate our proposed control approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u90e8\u5206\u53cd\u9988\u7ebf\u6027\u5316\u7684\u63a7\u5236\u65b9\u6cd5\uff0c\u7528\u4e8e\u7a33\u5b9a\u5e26\u6709\u8d1f\u8f7d\u7684\u60ac\u6302\u7a7a\u4e2d\u5e73\u53f0\uff0c\u9002\u7528\u4e8e\u5efa\u7b51\u5de5\u5730\u7684\u8d77\u91cd\u673a\u5e94\u7528\u3002", "motivation": "\u5f00\u53d1\u9002\u7528\u4e8e\u5efa\u7b51\u5de5\u5730\u8d77\u91cd\u673a\u5e94\u7528\u7684\u63a7\u5236\u65b9\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u91cd\u7269\u642c\u8fd0\u548c\u4fdd\u6301\uff0c\u540c\u65f6\u8003\u8651\u7cfb\u7edf\u6b20\u9a71\u52a8\u7279\u6027\u548c\u8026\u5408\u52a8\u529b\u5b66\u3002", "method": "\u4f7f\u7528\u90e8\u5206\u53cd\u9988\u7ebf\u6027\u5316\u63a7\u5236\u65b9\u6cd5\uff0c\u8003\u8651\u7cfb\u7edf\u7684\u6b20\u9a71\u52a8\u7279\u6027\uff0c\u5e76\u5229\u7528\u8026\u5408\u52a8\u529b\u5b66\u8fdb\u884c\u7a33\u5b9a\uff0c\u4ec5\u4f9d\u8d56\u673a\u8f7d\u4f20\u611f\u5668\u3002", "result": "\u901a\u8fc7\u6570\u503c\u7a33\u5b9a\u6027\u5206\u6790\u8bc1\u660e\u8026\u5408\u9879\u5bf9\u7cfb\u7edf\u7a33\u5b9a\u81f3\u5173\u91cd\u8981\uff0c\u8fdb\u884c\u4e86\u9c81\u68d2\u6027\u5206\u6790\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u63a7\u5236\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u63a7\u5236\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u7a33\u5b9a\u60ac\u6302\u7a7a\u4e2d\u5e73\u53f0\uff0c\u5728\u5b58\u5728\u5916\u90e8\u98ce\u6270\u52a8\u3001\u4f20\u611f\u5668\u566a\u58f0\u548c\u7cfb\u7edf\u52a8\u529b\u5b66\u4e0d\u786e\u5b9a\u6027\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u826f\u597d\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.14053", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14053", "abs": "https://arxiv.org/abs/2510.14053", "authors": ["Shriyash Upadhyay", "Chaithanya Bandi", "Narmeen Oozeer", "Philip Quirke"], "title": "Position: Require Frontier AI Labs To Release Small \"Analog\" Models", "comment": null, "summary": "Recent proposals for regulating frontier AI models have sparked concerns\nabout the cost of safety regulation, and most such regulations have been\nshelved due to the safety-innovation tradeoff. This paper argues for an\nalternative regulatory approach that ensures AI safety while actively promoting\ninnovation: mandating that large AI laboratories release small, openly\naccessible analog models (scaled-down versions) trained similarly to and\ndistilled from their largest proprietary models.\n  Analog models serve as public proxies, allowing broad participation in safety\nverification, interpretability research, and algorithmic transparency without\nforcing labs to disclose their full-scale models. Recent research demonstrates\nthat safety and interpretability methods developed using these smaller models\ngeneralize effectively to frontier-scale systems. By enabling the wider\nresearch community to directly investigate and innovate upon accessible\nanalogs, our policy substantially reduces the regulatory burden and accelerates\nsafety advancements.\n  This mandate promises minimal additional costs, leveraging reusable resources\nlike data and infrastructure, while significantly contributing to the public\ngood. Our hope is not only that this policy be adopted, but that it illustrates\na broader principle supporting fundamental research in machine learning: deeper\nunderstanding of models relaxes the safety-innovation tradeoff and lets us have\nmore of both.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u66ff\u4ee3\u6027AI\u76d1\u7ba1\u65b9\u6cd5\uff1a\u8981\u6c42\u5927\u578bAI\u5b9e\u9a8c\u5ba4\u53d1\u5e03\u5c0f\u578b\u3001\u5f00\u653e\u7684\u6a21\u62df\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u662f\u4ece\u5176\u5927\u578b\u4e13\u6709\u6a21\u578b\u84b8\u998f\u800c\u6765\u7684\u7f29\u5c0f\u7248\u672c\uff0c\u65e2\u80fd\u786e\u4fddAI\u5b89\u5168\u53c8\u80fd\u4fc3\u8fdb\u521b\u65b0\u3002", "motivation": "\u73b0\u6709\u524d\u6cbfAI\u6a21\u578b\u76d1\u7ba1\u65b9\u6848\u56e0\u5b89\u5168\u4e0e\u521b\u65b0\u7684\u6743\u8861\u800c\u88ab\u6401\u7f6e\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u786e\u4fdd\u5b89\u5168\u53c8\u4e0d\u963b\u788d\u521b\u65b0\u7684\u76d1\u7ba1\u65b9\u6cd5\u3002", "method": "\u5f3a\u5236\u8981\u6c42\u5927\u578bAI\u5b9e\u9a8c\u5ba4\u53d1\u5e03\u5c0f\u578b\u6a21\u62df\u6a21\u578b\u4f5c\u4e3a\u516c\u5171\u4ee3\u7406\uff0c\u8fd9\u4e9b\u6a21\u578b\u4ee5\u7c7b\u4f3c\u65b9\u5f0f\u8bad\u7ec3\u5e76\u4ece\u5927\u578b\u4e13\u6709\u6a21\u578b\u84b8\u998f\u800c\u6765\uff0c\u5141\u8bb8\u5e7f\u6cdb\u53c2\u4e0e\u5b89\u5168\u9a8c\u8bc1\u3001\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u548c\u7b97\u6cd5\u900f\u660e\u5ea6\u5de5\u4f5c\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u4f7f\u7528\u8fd9\u4e9b\u5c0f\u578b\u6a21\u578b\u5f00\u53d1\u7684\u5b89\u5168\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u80fd\u6709\u6548\u6cdb\u5316\u5230\u524d\u6cbf\u89c4\u6a21\u7cfb\u7edf\uff0c\u663e\u8457\u964d\u4f4e\u76d1\u7ba1\u8d1f\u62c5\u5e76\u52a0\u901f\u5b89\u5168\u8fdb\u5c55\u3002", "conclusion": "\u8fd9\u79cd\u76d1\u7ba1\u65b9\u6cd5\u4ee5\u6700\u5c0f\u989d\u5916\u6210\u672c\u663e\u8457\u4fc3\u8fdb\u516c\u5171\u5229\u76ca\uff0c\u5e76\u901a\u8fc7\u6df1\u5316\u5bf9\u6a21\u578b\u7684\u7406\u89e3\u6765\u7f13\u89e3\u5b89\u5168\u4e0e\u521b\u65b0\u7684\u6743\u8861\uff0c\u5b9e\u73b0\u4e24\u8005\u7684\u53cc\u8d62\u3002"}}
{"id": "2510.14119", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.14119", "abs": "https://arxiv.org/abs/2510.14119", "authors": ["Ali Eslami", "Mohammad Pirani"], "title": "Resource-Aware Stealthy Attacks in Vehicle Platoons", "comment": "13 pages, 8 figures", "summary": "Connected and Autonomous Vehicles (CAVs) are transforming modern\ntransportation by enabling cooperative applications such as vehicle platooning,\nwhere multiple vehicles travel in close formation to improve efficiency and\nsafety. However, the heavy reliance on inter-vehicle communication makes\nplatoons highly susceptible to attacks, where even subtle manipulations can\nescalate into severe physical consequences. While existing research has largely\nfocused on defending against attacks, far less attention has been given to\nstealthy adversaries that aim to covertly manipulate platoon behavior. This\npaper introduces a new perspective on the attack design problem by\ndemonstrating how attackers can guide platoons toward their own desired\ntrajectories while remaining undetected. We outline conditions under which such\nattacks are feasible, analyze their dependence on communication topologies and\ncontrol protocols, and investigate the resources required by the attacker. By\ncharacterizing the resources needed to launch stealthy attacks, we address\nsystem vulnerabilities and informing the design of resilient countermeasures.\nOur findings reveal critical weaknesses in current platoon architectures and\nanomaly detection mechanisms and provide methods to develop more secure and\ntrustworthy CAV systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u8f66\u8f86\u7f16\u961f\u7684\u65b0\u578b\u9690\u853d\u653b\u51fb\u65b9\u6cd5\uff0c\u653b\u51fb\u8005\u53ef\u4ee5\u5f15\u5bfc\u7f16\u961f\u6309\u7167\u671f\u671b\u8f68\u8ff9\u884c\u9a76\u800c\u4e0d\u88ab\u53d1\u73b0\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u7f16\u961f\u67b6\u6784\u548c\u5f02\u5e38\u68c0\u6d4b\u673a\u5236\u7684\u5173\u952e\u5f31\u70b9\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u9632\u5fa1\u653b\u51fb\uff0c\u4f46\u5bf9\u9690\u853d\u653b\u51fb\u8005\u7684\u5173\u6ce8\u8f83\u5c11\u3002\u8f66\u8f86\u7f16\u961f\u9ad8\u5ea6\u4f9d\u8d56\u8f66\u9645\u901a\u4fe1\uff0c\u5bb9\u6613\u53d7\u5230\u653b\u51fb\uff0c\u5373\u4f7f\u662f\u8f7b\u5fae\u64cd\u7eb5\u4e5f\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u540e\u679c\u3002", "method": "\u901a\u8fc7\u5206\u6790\u653b\u51fb\u53ef\u884c\u6027\u6761\u4ef6\uff0c\u7814\u7a76\u901a\u4fe1\u62d3\u6251\u548c\u63a7\u5236\u534f\u8bae\u7684\u5f71\u54cd\uff0c\u5e76\u8c03\u67e5\u653b\u51fb\u8005\u6240\u9700\u7684\u8d44\u6e90\uff0c\u6765\u8bbe\u8ba1\u80fd\u591f\u5f15\u5bfc\u7f16\u961f\u6309\u671f\u671b\u8f68\u8ff9\u884c\u9a76\u7684\u9690\u853d\u653b\u51fb\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u7f16\u961f\u67b6\u6784\u548c\u5f02\u5e38\u68c0\u6d4b\u673a\u5236\u5b58\u5728\u5173\u952e\u5f31\u70b9\uff0c\u653b\u51fb\u8005\u53ef\u4ee5\u5229\u7528\u8fd9\u4e9b\u5f31\u70b9\u5b9e\u65bd\u9690\u853d\u653b\u51fb\u800c\u4e0d\u88ab\u53d1\u73b0\u3002", "conclusion": "\u901a\u8fc7\u63cf\u8ff0\u9690\u853d\u653b\u51fb\u6240\u9700\u8d44\u6e90\uff0c\u63ed\u793a\u4e86\u7cfb\u7edf\u6f0f\u6d1e\uff0c\u4e3a\u8bbe\u8ba1\u66f4\u5177\u5f39\u6027\u7684\u5bf9\u6297\u63aa\u65bd\u63d0\u4f9b\u4e86\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u5b89\u5168\u53ef\u9760\u7684CAV\u7cfb\u7edf\u3002"}}
{"id": "2510.14117", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14117", "abs": "https://arxiv.org/abs/2510.14117", "authors": ["Zhiyuan Wu", "Yijiong Lin", "Yongqiang Zhao", "Xuyang Zhang", "Zhuo Chen", "Nathan Lepora", "Shan Luo"], "title": "ViTacGen: Robotic Pushing with Vision-to-Touch Generation", "comment": null, "summary": "Robotic pushing is a fundamental manipulation task that requires tactile\nfeedback to capture subtle contact forces and dynamics between the end-effector\nand the object. However, real tactile sensors often face hardware limitations\nsuch as high costs and fragility, and deployment challenges involving\ncalibration and variations between different sensors, while vision-only\npolicies struggle with satisfactory performance. Inspired by humans' ability to\ninfer tactile states from vision, we propose ViTacGen, a novel robot\nmanipulation framework designed for visual robotic pushing with vision-to-touch\ngeneration in reinforcement learning to eliminate the reliance on\nhigh-resolution real tactile sensors, enabling effective zero-shot deployment\non visual-only robotic systems. Specifically, ViTacGen consists of an\nencoder-decoder vision-to-touch generation network that generates contact depth\nimages, a standardized tactile representation, directly from visual image\nsequence, followed by a reinforcement learning policy that fuses visual-tactile\ndata with contrastive learning based on visual and generated tactile\nobservations. We validate the effectiveness of our approach in both simulation\nand real world experiments, demonstrating its superior performance and\nachieving a success rate of up to 86\\%.", "AI": {"tldr": "ViTacGen\u662f\u4e00\u4e2a\u673a\u5668\u4eba\u64cd\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u5230\u89e6\u89c9\u751f\u6210\u6280\u672f\u5b9e\u73b0\u89c6\u89c9\u673a\u5668\u4eba\u63a8\u52a8\uff0c\u65e0\u9700\u771f\u5b9e\u9ad8\u5206\u8fa8\u7387\u89e6\u89c9\u4f20\u611f\u5668\uff0c\u5728\u7eaf\u89c6\u89c9\u673a\u5668\u4eba\u7cfb\u7edf\u4e0a\u5b9e\u73b0\u96f6\u6837\u672c\u90e8\u7f72\u3002", "motivation": "\u771f\u5b9e\u89e6\u89c9\u4f20\u611f\u5668\u5b58\u5728\u786c\u4ef6\u9650\u5236\uff08\u9ad8\u6210\u672c\u3001\u6613\u788e\uff09\u548c\u90e8\u7f72\u6311\u6218\uff08\u6821\u51c6\u3001\u4f20\u611f\u5668\u5dee\u5f02\uff09\uff0c\u800c\u7eaf\u89c6\u89c9\u7b56\u7565\u6027\u80fd\u4e0d\u8db3\u3002\u53d7\u4eba\u7c7b\u4ece\u89c6\u89c9\u63a8\u65ad\u89e6\u89c9\u72b6\u6001\u7684\u80fd\u529b\u542f\u53d1\uff0c\u63d0\u51fa\u89c6\u89c9\u5230\u89e6\u89c9\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u5305\u542b\u7f16\u7801\u5668-\u89e3\u7801\u5668\u89c6\u89c9\u5230\u89e6\u89c9\u751f\u6210\u7f51\u7edc\uff0c\u4ece\u89c6\u89c9\u56fe\u50cf\u5e8f\u5217\u751f\u6210\u63a5\u89e6\u6df1\u5ea6\u56fe\u50cf\uff08\u6807\u51c6\u5316\u89e6\u89c9\u8868\u793a\uff09\uff0c\u7136\u540e\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u878d\u5408\u89c6\u89c9-\u89e6\u89c9\u6570\u636e\uff0c\u57fa\u4e8e\u89c6\u89c9\u548c\u751f\u6210\u89e6\u89c9\u89c2\u6d4b\u8fdb\u884c\u5bf9\u6bd4\u5b66\u4e60\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u4f18\u8d8a\u6027\u80fd\uff0c\u6210\u529f\u7387\u9ad8\u8fbe86%\u3002", "conclusion": "ViTacGen\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u89c6\u89c9\u5230\u89e6\u89c9\u7684\u751f\u6210\uff0c\u6d88\u9664\u4e86\u5bf9\u9ad8\u5206\u8fa8\u7387\u771f\u5b9e\u89e6\u89c9\u4f20\u611f\u5668\u7684\u4f9d\u8d56\uff0c\u80fd\u591f\u5728\u7eaf\u89c6\u89c9\u673a\u5668\u4eba\u7cfb\u7edf\u4e0a\u6709\u6548\u90e8\u7f72\u3002"}}
{"id": "2510.14106", "categories": ["cs.AI", "cs.CL", "cs.GT"], "pdf": "https://arxiv.org/pdf/2510.14106", "abs": "https://arxiv.org/abs/2510.14106", "authors": ["Carter Blair", "Kate Larson"], "title": "Generating Fair Consensus Statements with Social Choice on Token-Level MDPs", "comment": null, "summary": "Current frameworks for consensus statement generation with large language\nmodels lack the inherent structure needed to provide provable fairness\nguarantees when aggregating diverse free-form opinions. We model the task as a\nmulti-objective, token-level Markov Decision Process (MDP), where each\nobjective corresponds to an agent's preference. Token-level rewards for each\nagent are derived from their policy (e.g., a personalized language model). This\napproach utilizes the finding that such policies implicitly define optimal\nQ-functions, providing a principled way to quantify rewards at each generation\nstep without a value function (Rafailov et al., 2024). This MDP formulation\ncreates a formal structure amenable to analysis using principles from social\nchoice theory. We propose two approaches grounded in social choice theory.\nFirst, we propose a stochastic generation policy guaranteed to be in the\nex-ante core, extending core stability concepts from voting theory to text\ngeneration. This policy is derived from an underlying distribution over\ncomplete statements that maximizes proportional fairness (Nash Welfare).\nSecond, for generating a single statement, we target the maximization of\negalitarian welfare using search algorithms within the MDP framework.\nEmpirically, experiments using language models to instantiate agent policies\nshow that search guided by the egalitarian objective generates consensus\nstatements with improved worst-case agent alignment compared to baseline\nmethods, including the Habermas Machine (Tessler et al., 2024).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u76ee\u6807MDP\u548c\u6295\u7968\u7406\u8bba\u7684\u5171\u8bc6\u58f0\u660e\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7token\u7ea7\u5956\u52b1\u548c\u6838\u5fc3\u7a33\u5b9a\u6027\u4fdd\u8bc1\uff0c\u5b9e\u73b0\u516c\u5e73\u7684\u6587\u672c\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5171\u8bc6\u58f0\u660e\u751f\u6210\u6846\u67b6\u7f3a\u4e4f\u53ef\u8bc1\u660e\u7684\u516c\u5e73\u6027\u4fdd\u8bc1\u7ed3\u6784\uff0c\u65e0\u6cd5\u5728\u805a\u5408\u591a\u6837\u5316\u81ea\u7531\u5f62\u5f0f\u610f\u89c1\u65f6\u63d0\u4f9b\u7406\u8bba\u4fdd\u969c\u3002", "method": "\u5c06\u4efb\u52a1\u5efa\u6a21\u4e3a\u591a\u76ee\u6807token\u7ea7MDP\uff0c\u6bcf\u4e2a\u76ee\u6807\u5bf9\u5e94\u4ee3\u7406\u504f\u597d\uff1b\u57fa\u4e8e\u4ee3\u7406\u7b56\u7565\u63a8\u5bfctoken\u7ea7\u5956\u52b1\uff1b\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u6295\u7968\u7406\u8bba\u7684\u65b9\u6cd5\uff1a\u968f\u673a\u751f\u6210\u7b56\u7565\uff08\u4fdd\u8bc1ex-ante\u6838\u5fc3\u7a33\u5b9a\u6027\uff09\u548c\u5355\u4e00\u58f0\u660e\u751f\u6210\uff08\u6700\u5927\u5316\u5e73\u7b49\u4e3b\u4e49\u798f\u5229\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u5e73\u7b49\u4e3b\u4e49\u76ee\u6807\u7684\u641c\u7d22\u65b9\u6cd5\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff08\u5305\u62ecHabermas Machine\uff09\u80fd\u751f\u6210\u5177\u6709\u66f4\u597d\u6700\u5dee\u60c5\u51b5\u4ee3\u7406\u5bf9\u9f50\u7684\u5171\u8bc6\u58f0\u660e\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5171\u8bc6\u58f0\u660e\u751f\u6210\u63d0\u4f9b\u4e86\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u7684\u516c\u5e73\u6027\u7ed3\u6784\uff0c\u5c06\u6295\u7968\u7406\u8bba\u6982\u5ff5\u6269\u5c55\u5230\u6587\u672c\u751f\u6210\u9886\u57df\u3002"}}
{"id": "2510.14542", "categories": ["eess.SY", "cs.LG", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.14542", "abs": "https://arxiv.org/abs/2510.14542", "authors": ["Hiroki Sakamoto", "Kazuhiro Sato"], "title": "A Deep State-Space Model Compression Method using Upper Bound on Output Error", "comment": null, "summary": "We study deep state-space models (Deep SSMs) that contain\nlinear-quadratic-output (LQO) systems as internal blocks and present a\ncompression method with a provable output error guarantee. We first derive an\nupper bound on the output error between two Deep SSMs and show that the bound\ncan be expressed via the $h^2$-error norms between the layerwise LQO systems,\nthereby providing a theoretical justification for existing model order\nreduction (MOR)-based compression. Building on this bound, we formulate an\noptimization problem in terms of the $h^2$-error norm and develop a\ngradient-based MOR method. On the IMDb task from the Long Range Arena\nbenchmark, we demonstrate that our compression method achieves strong\nperformance. Moreover, unlike prior approaches, we reduce roughly 80% of\ntrainable parameters without retraining, with only a 4-5% performance drop.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6df1\u5ea6\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8eh\u00b2\u8bef\u5dee\u8303\u6570\u7684\u4f18\u5316\u95ee\u9898\u5b9e\u73b0\u6a21\u578b\u538b\u7f29\uff0c\u5728IMDb\u4efb\u52a1\u4e0a\u51cf\u5c1180%\u53c2\u6570\u4e14\u6027\u80fd\u4ec5\u4e0b\u964d4-5%", "motivation": "\u73b0\u6709\u6df1\u5ea6\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u538b\u7f29\u65b9\u6cd5\u7f3a\u4e4f\u7406\u8bba\u4fdd\u8bc1\uff0c\u9700\u8981\u5f00\u53d1\u5177\u6709\u53ef\u8bc1\u660e\u8f93\u51fa\u8bef\u5dee\u4fdd\u8bc1\u7684\u538b\u7f29\u65b9\u6cd5", "method": "\u63a8\u5bfc\u6df1\u5ea6\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u8f93\u51fa\u8bef\u5dee\u7684\u4e0a\u754c\uff0c\u6784\u5efa\u57fa\u4e8eh\u00b2\u8bef\u5dee\u8303\u6570\u7684\u4f18\u5316\u95ee\u9898\uff0c\u5f00\u53d1\u68af\u5ea6\u6a21\u578b\u964d\u9636\u65b9\u6cd5", "result": "\u5728IMDb\u4efb\u52a1\u4e0a\u5b9e\u73b0\u5f3a\u6027\u80fd\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u51cf\u5c11\u7ea680%\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u6027\u80fd\u4ec5\u4e0b\u964d4-5%", "conclusion": "\u63d0\u51fa\u7684\u538b\u7f29\u65b9\u6cd5\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u6a21\u578b\u53c2\u6570\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5"}}
{"id": "2510.14234", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.14234", "abs": "https://arxiv.org/abs/2510.14234", "authors": ["Ning Han", "Gu Gong", "Bin Zhang", "Yuexuan Xu", "Bohan Yang", "Yunhui Liu", "David Navarro-Alarcon"], "title": "Prescribed Performance Control of Deformable Object Manipulation in Spatial Latent Space", "comment": null, "summary": "Manipulating three-dimensional (3D) deformable objects presents significant\nchallenges for robotic systems due to their infinite-dimensional state space\nand complex deformable dynamics. This paper proposes a novel model-free\napproach for shape control with constraints imposed on key points. Unlike\nexisting methods that rely on feature dimensionality reduction, the proposed\ncontroller leverages the coordinates of key points as the feature vector, which\nare extracted from the deformable object's point cloud using deep learning\nmethods. This approach not only reduces the dimensionality of the feature space\nbut also retains the spatial information of the object. By extracting key\npoints, the manipulation of deformable objects is simplified into a visual\nservoing problem, where the shape dynamics are described using a deformation\nJacobian matrix. To enhance control accuracy, a prescribed performance control\nmethod is developed by integrating barrier Lyapunov functions (BLF) to enforce\nconstraints on the key points. The stability of the closed-loop system is\nrigorously analyzed and verified using the Lyapunov method. Experimental\nresults further demonstrate the effectiveness and robustness of the proposed\nmethod.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5173\u952e\u70b9\u7ea6\u675f\u7684\u65e0\u6a21\u578b\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e09\u7ef4\u53ef\u53d8\u5f62\u7269\u4f53\u7684\u5f62\u72b6\u63a7\u5236\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u63d0\u53d6\u5173\u952e\u70b9\u5750\u6807\u4f5c\u4e3a\u7279\u5f81\u5411\u91cf\uff0c\u7ed3\u5408\u53d8\u5f62\u96c5\u53ef\u6bd4\u77e9\u9635\u548c\u6027\u80fd\u7ea6\u675f\u63a7\u5236\u65b9\u6cd5\u5b9e\u73b0\u7cbe\u786e\u63a7\u5236\u3002", "motivation": "\u4e09\u7ef4\u53ef\u53d8\u5f62\u7269\u4f53\u64cd\u4f5c\u9762\u4e34\u65e0\u9650\u7ef4\u72b6\u6001\u7a7a\u95f4\u548c\u590d\u6742\u53d8\u5f62\u52a8\u529b\u5b66\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7279\u5f81\u964d\u7ef4\u4f46\u53ef\u80fd\u4e22\u5931\u7a7a\u95f4\u4fe1\u606f\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u7b80\u5316\u63a7\u5236\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u4ece\u70b9\u4e91\u4e2d\u63d0\u53d6\u5173\u952e\u70b9\u5750\u6807\u4f5c\u4e3a\u7279\u5f81\u5411\u91cf\uff0c\u5c06\u53d8\u5f62\u7269\u4f53\u64cd\u4f5c\u7b80\u5316\u4e3a\u89c6\u89c9\u4f3a\u670d\u95ee\u9898\uff0c\u91c7\u7528\u53d8\u5f62\u96c5\u53ef\u6bd4\u77e9\u9635\u63cf\u8ff0\u5f62\u72b6\u52a8\u529b\u5b66\uff0c\u7ed3\u5408\u969c\u788d\u674e\u96c5\u666e\u8bfa\u592b\u51fd\u6570\u5f00\u53d1\u6027\u80fd\u7ea6\u675f\u63a7\u5236\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u7cbe\u786e\u7684\u5f62\u72b6\u63a7\u5236\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65e0\u6a21\u578b\u65b9\u6cd5\u901a\u8fc7\u5173\u952e\u70b9\u63d0\u53d6\u548c\u6027\u80fd\u7ea6\u675f\u63a7\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4e09\u7ef4\u53ef\u53d8\u5f62\u7269\u4f53\u7684\u5f62\u72b6\u63a7\u5236\u95ee\u9898\uff0c\u7cfb\u7edf\u7a33\u5b9a\u6027\u5f97\u5230\u4e25\u683c\u8bc1\u660e\u3002"}}
{"id": "2510.14112", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14112", "abs": "https://arxiv.org/abs/2510.14112", "authors": ["Huiliang Zhang", "Di Wu", "Arnaud Zinflou", "Benoit Boulet"], "title": "STEMS: Spatial-Temporal Enhanced Safe Multi-Agent Coordination for Building Energy Management", "comment": null, "summary": "Building energy management is essential for achieving carbon reduction goals,\nimproving occupant comfort, and reducing energy costs. Coordinated building\nenergy management faces critical challenges in exploiting spatial-temporal\ndependencies while ensuring operational safety across multi-building systems.\nCurrent multi-building energy systems face three key challenges: insufficient\nspatial-temporal information exploitation, lack of rigorous safety guarantees,\nand system complexity. This paper proposes Spatial-Temporal Enhanced Safe\nMulti-Agent Coordination (STEMS), a novel safety-constrained multi-agent\nreinforcement learning framework for coordinated building energy management.\nSTEMS integrates two core components: (1) a spatial-temporal graph\nrepresentation learning framework using a GCN-Transformer fusion architecture\nto capture inter-building relationships and temporal patterns, and (2) a\nsafety-constrained multi-agent RL algorithm incorporating Control Barrier\nFunctions to provide mathematical safety guarantees. Extensive experiments on\nreal-world building datasets demonstrate STEMS's superior performance over\nexisting methods, showing that STEMS achieves 21% cost reduction, 18% emission\nreduction, and dramatically reduces safety violations from 35.1% to 5.6% while\nmaintaining optimal comfort with only 0.13 discomfort proportion. The framework\nalso demonstrates strong robustness during extreme weather conditions and\nmaintains effectiveness across different building types.", "AI": {"tldr": "\u63d0\u51fa\u4e86STEMS\u6846\u67b6\uff0c\u4e00\u79cd\u5b89\u5168\u7ea6\u675f\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u534f\u8c03\u5efa\u7b51\u80fd\u6e90\u7ba1\u7406\uff0c\u901a\u8fc7\u7a7a\u95f4-\u65f6\u95f4\u56fe\u8868\u793a\u5b66\u4e60\u548c\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u548c\u5b89\u5168\u4fdd\u969c\u3002", "motivation": "\u5efa\u7b51\u80fd\u6e90\u7ba1\u7406\u5bf9\u5b9e\u73b0\u78b3\u51cf\u6392\u76ee\u6807\u3001\u63d0\u5347\u5c45\u4f4f\u8212\u9002\u5ea6\u548c\u964d\u4f4e\u80fd\u6e90\u6210\u672c\u81f3\u5173\u91cd\u8981\u3002\u5f53\u524d\u591a\u5efa\u7b51\u80fd\u6e90\u7cfb\u7edf\u9762\u4e34\u7a7a\u95f4-\u65f6\u95f4\u4f9d\u8d56\u5173\u7cfb\u5229\u7528\u4e0d\u8db3\u3001\u7f3a\u4e4f\u4e25\u683c\u5b89\u5168\u4fdd\u8bc1\u548c\u7cfb\u7edf\u590d\u6742\u6027\u4e09\u5927\u6311\u6218\u3002", "method": "STEMS\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1) \u4f7f\u7528GCN-Transformer\u878d\u5408\u67b6\u6784\u7684\u7a7a\u95f4-\u65f6\u95f4\u56fe\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u6355\u6349\u5efa\u7b51\u95f4\u5173\u7cfb\u548c\u65f6\u5e8f\u6a21\u5f0f\uff1b(2) \u7ed3\u5408\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u7684\u5b89\u5168\u7ea6\u675f\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u63d0\u4f9b\u6570\u5b66\u5b89\u5168\u4fdd\u8bc1\u3002", "result": "\u5728\u771f\u5b9e\u5efa\u7b51\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSTEMS\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5b9e\u73b0\u4e8621%\u6210\u672c\u964d\u4f4e\u300118%\u6392\u653e\u51cf\u5c11\uff0c\u5b89\u5168\u8fdd\u89c4\u4ece35.1%\u5927\u5e45\u964d\u81f35.6%\uff0c\u540c\u65f6\u4fdd\u6301\u6700\u4f73\u8212\u9002\u5ea6\uff08\u4ec50.13\u4e0d\u9002\u6bd4\u4f8b\uff09\u3002\u8be5\u6846\u67b6\u5728\u6781\u7aef\u5929\u6c14\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u4e0d\u540c\u5efa\u7b51\u7c7b\u578b\u4e2d\u4fdd\u6301\u6709\u6548\u6027\u3002", "conclusion": "STEMS\u6846\u67b6\u4e3a\u591a\u5efa\u7b51\u80fd\u6e90\u7cfb\u7edf\u7684\u534f\u8c03\u7ba1\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u7a7a\u95f4-\u65f6\u95f4\u4f9d\u8d56\u5173\u7cfb\u5229\u7528\u3001\u5b89\u5168\u4fdd\u8bc1\u548c\u7cfb\u7edf\u590d\u6742\u6027\u7b49\u5173\u952e\u6311\u6218\uff0c\u5728\u6027\u80fd\u3001\u5b89\u5168\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.14696", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.14696", "abs": "https://arxiv.org/abs/2510.14696", "authors": ["Kevin Wu", "Rabab Haider", "Pascal Van Hentenryck"], "title": "High-Resolution PTDF-Based Planning of Storage and Transmission Under High Renewables", "comment": null, "summary": "Transmission Expansion Planning (TEP) optimizes power grid upgrades and\ninvestments to ensure reliable, efficient, and cost-effective electricity\ndelivery while addressing grid constraints. To support growing demand and\nrenewable energy integration, energy storage is emerging as a pivotal asset\nthat provides temporal flexibility and alleviates congestion. This paper\ndevelops a multiperiod, two-stage PTDF formulation that co-optimizes\ntransmission upgrades and storage siting/sizing. To ensure scalability, a\ntrust-region, multicut Benders scheme warm-started from per-representative-day\noptima is proposed. Applied to a 2,000-bus synthetic Texas system under\nhigh-renewable projections, the method attains final optimality gaps below 1%\nand yields a plan with storage at about 180 nodes (32% of peak renewable\ncapacity). These results demonstrate that the proposed PTDF-based methodology\nefficiently handles large distributed storage fleets, demonstrating scalability\nat high spatial resolution", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u5468\u671f\u4e24\u9636\u6bb5PTDF\u6a21\u578b\uff0c\u8054\u5408\u4f18\u5316\u8f93\u7535\u6269\u5efa\u548c\u50a8\u80fd\u9009\u5740/\u5bb9\u91cf\u89c4\u5212\uff0c\u91c7\u7528\u4fe1\u4efb\u57df\u591a\u5272Benders\u5206\u89e3\u65b9\u6cd5\uff0c\u57282000\u8282\u70b9\u7cfb\u7edf\u4e0a\u5b9e\u73b0\u4f4e\u4e8e1%\u7684\u6700\u4f18\u6027\u5dee\u8ddd\u3002", "motivation": "\u4e3a\u6ee1\u8db3\u65e5\u76ca\u589e\u957f\u7684\u7535\u529b\u9700\u6c42\u548c\u53ef\u518d\u751f\u80fd\u6e90\u5e76\u7f51\u9700\u6c42\uff0c\u50a8\u80fd\u4f5c\u4e3a\u63d0\u4f9b\u65f6\u95f4\u7075\u6d3b\u6027\u548c\u7f13\u89e3\u62e5\u5835\u7684\u5173\u952e\u8d44\u4ea7\uff0c\u9700\u8981\u4e0e\u8f93\u7535\u6269\u5efa\u534f\u540c\u89c4\u5212\u3002", "method": "\u5f00\u53d1\u4e86\u591a\u5468\u671f\u4e24\u9636\u6bb5PTDF\u516c\u5f0f\uff0c\u91c7\u7528\u4fe1\u4efb\u57df\u591a\u5272Benders\u5206\u89e3\u65b9\u6848\uff0c\u4ece\u4ee3\u8868\u6027\u65e5\u6700\u4f18\u89e3\u8fdb\u884c\u70ed\u542f\u52a8\u4ee5\u786e\u4fdd\u53ef\u6269\u5c55\u6027\u3002", "result": "\u57282000\u8282\u70b9\u5fb7\u5dde\u5408\u6210\u7cfb\u7edf\u9ad8\u53ef\u518d\u751f\u80fd\u6e90\u573a\u666f\u4e0b\uff0c\u65b9\u6cd5\u8fbe\u5230\u4f4e\u4e8e1%\u7684\u6700\u4f18\u6027\u5dee\u8ddd\uff0c\u89c4\u5212\u65b9\u6848\u5728\u7ea6180\u4e2a\u8282\u70b9\u90e8\u7f72\u50a8\u80fd\uff08\u5360\u5cf0\u503c\u53ef\u518d\u751f\u80fd\u6e90\u5bb9\u91cf\u768432%\uff09\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8ePTDF\u7684\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u5904\u7406\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u50a8\u80fd\u7cfb\u7edf\uff0c\u5728\u9ad8\u7a7a\u95f4\u5206\u8fa8\u7387\u4e0b\u5c55\u73b0\u51fa\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2510.14293", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14293", "abs": "https://arxiv.org/abs/2510.14293", "authors": ["Yushi Du", "Yixuan Li", "Baoxiong Jia", "Yutang Lin", "Pei Zhou", "Wei Liang", "Yanchao Yang", "Siyuan Huang"], "title": "Learning Human-Humanoid Coordination for Collaborative Object Carrying", "comment": null, "summary": "Human-humanoid collaboration shows significant promise for applications in\nhealthcare, domestic assistance, and manufacturing. While compliant robot-human\ncollaboration has been extensively developed for robotic arms, enabling\ncompliant human-humanoid collaboration remains largely unexplored due to\nhumanoids' complex whole-body dynamics. In this paper, we propose a\nproprioception-only reinforcement learning approach, COLA, that combines leader\nand follower behaviors within a single policy. The model is trained in a\nclosed-loop environment with dynamic object interactions to predict object\nmotion patterns and human intentions implicitly, enabling compliant\ncollaboration to maintain load balance through coordinated trajectory planning.\nWe evaluate our approach through comprehensive simulator and real-world\nexperiments on collaborative carrying tasks, demonstrating the effectiveness,\ngeneralization, and robustness of our model across various terrains and\nobjects. Simulation experiments demonstrate that our model reduces human effort\nby 24.7%. compared to baseline approaches while maintaining object stability.\nReal-world experiments validate robust collaborative carrying across different\nobject types (boxes, desks, stretchers, etc.) and movement patterns\n(straight-line, turning, slope climbing). Human user studies with 23\nparticipants confirm an average improvement of 27.4% compared to baseline\nmodels. Our method enables compliant human-humanoid collaborative carrying\nwithout requiring external sensors or complex interaction models, offering a\npractical solution for real-world deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f7f\u7528\u672c\u4f53\u611f\u89c9\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5COLA\uff0c\u901a\u8fc7\u5355\u4e00\u7b56\u7565\u7ed3\u5408\u9886\u5bfc\u8005\u548c\u8ddf\u968f\u8005\u884c\u4e3a\uff0c\u5b9e\u73b0\u4eba\u5f62\u673a\u5668\u4eba\u4e0e\u4eba\u7c7b\u7684\u987a\u4ece\u534f\u4f5c\u642c\u8fd0\uff0c\u65e0\u9700\u5916\u90e8\u4f20\u611f\u5668\u6216\u590d\u6742\u4ea4\u4e92\u6a21\u578b\u3002", "motivation": "\u867d\u7136\u673a\u68b0\u81c2\u7684\u4eba\u673a\u534f\u4f5c\u5df2\u5e7f\u6cdb\u53d1\u5c55\uff0c\u4f46\u7531\u4e8e\u4eba\u5f62\u673a\u5668\u4eba\u590d\u6742\u7684\u5168\u8eab\u52a8\u529b\u5b66\uff0c\u5b9e\u73b0\u987a\u4ece\u7684\u4eba-\u4eba\u5f62\u673a\u5668\u4eba\u534f\u4f5c\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4f7f\u7528\u4ec5\u672c\u4f53\u611f\u89c9\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5COLA\uff0c\u5728\u95ed\u73af\u73af\u5883\u4e2d\u8bad\u7ec3\u5355\u4e00\u7b56\u7565\uff0c\u7ed3\u5408\u9886\u5bfc\u8005\u548c\u8ddf\u968f\u8005\u884c\u4e3a\uff0c\u901a\u8fc7\u534f\u8c03\u8f68\u8ff9\u89c4\u5212\u9884\u6d4b\u7269\u4f53\u8fd0\u52a8\u6a21\u5f0f\u548c\u4eba\u7c7b\u610f\u56fe\u3002", "result": "\u6a21\u62df\u5b9e\u9a8c\u663e\u793a\u6a21\u578b\u51cf\u5c11\u4eba\u7c7b24.7%\u7684\u52aa\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u7269\u4f53\u7a33\u5b9a\u6027\uff1b\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5728\u4e0d\u540c\u7269\u4f53\u7c7b\u578b\u548c\u8fd0\u52a8\u6a21\u5f0f\u4e0b\u7684\u9c81\u68d2\u534f\u4f5c\uff1b23\u540d\u53c2\u4e0e\u8005\u7684\u4eba\u4f53\u7814\u7a76\u786e\u8ba4\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u5e73\u5747\u63d0\u534727.4%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u5916\u90e8\u4f20\u611f\u5668\u6216\u590d\u6742\u4ea4\u4e92\u6a21\u578b\u5373\u53ef\u5b9e\u73b0\u987a\u4ece\u7684\u4eba-\u4eba\u5f62\u673a\u5668\u4eba\u534f\u4f5c\u642c\u8fd0\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.14133", "categories": ["cs.AI", "cs.CR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.14133", "abs": "https://arxiv.org/abs/2510.14133", "authors": ["Edoardo Allegrini", "Ananth Shreekumar", "Z. Berkay Celik"], "title": "Formalizing the Safety, Security, and Functional Properties of Agentic AI Systems", "comment": null, "summary": "Agentic AI systems, which leverage multiple autonomous agents and Large\nLanguage Models (LLMs), are increasingly used to address complex, multi-step\ntasks. The safety, security, and functionality of these systems are critical,\nespecially in high-stakes applications. However, the current ecosystem of\ninter-agent communication is fragmented, with protocols such as the Model\nContext Protocol (MCP) for tool access and the Agent-to-Agent (A2A) protocol\nfor coordination being analyzed in isolation. This fragmentation creates a\nsemantic gap that prevents the rigorous analysis of system properties and\nintroduces risks such as architectural misalignment and exploitable\ncoordination issues. To address these challenges, we introduce a modeling\nframework for agentic AI systems composed of two foundational models. The\nfirst, the host agent model, formalizes the top-level entity that interacts\nwith the user, decomposes tasks, and orchestrates their execution by leveraging\nexternal agents and tools. The second, the task lifecycle model, details the\nstates and transitions of individual sub-tasks from creation to completion,\nproviding a fine-grained view of task management and error handling. Together,\nthese models provide a unified semantic framework for reasoning about the\nbehavior of multi-AI agent systems. Grounded in this framework, we define 17\nproperties for the host agent and 14 for the task lifecycle, categorized into\nliveness, safety, completeness, and fairness. Expressed in temporal logic,\nthese properties enable formal verification of system behavior, detection of\ncoordination edge cases, and prevention of deadlocks and security\nvulnerabilities. Through this effort, we introduce the first rigorously\ngrounded, domain-agnostic framework for the systematic analysis, design, and\ndeployment of correct, reliable, and robust agentic AI systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\u7684\u7edf\u4e00\u5efa\u6a21\u6846\u67b6\uff0c\u5305\u542b\u4e3b\u673a\u667a\u80fd\u4f53\u6a21\u578b\u548c\u4efb\u52a1\u751f\u547d\u5468\u671f\u6a21\u578b\uff0c\u5b9a\u4e49\u4e8631\u4e2a\u7cfb\u7edf\u5c5e\u6027\uff0c\u652f\u6301\u5f62\u5f0f\u5316\u9a8c\u8bc1\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524d\u667a\u80fd\u4f53\u901a\u4fe1\u751f\u6001\u788e\u7247\u5316\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\u7684\u901a\u4fe1\u534f\u8bae\u788e\u7247\u5316\uff0c\u5b58\u5728\u8bed\u4e49\u9e3f\u6c9f\uff0c\u963b\u788d\u7cfb\u7edf\u5c5e\u6027\u5206\u6790\uff0c\u5bfc\u81f4\u67b6\u6784\u9519\u4f4d\u548c\u53ef\u88ab\u5229\u7528\u7684\u534f\u8c03\u95ee\u9898\uff0c\u9700\u8981\u7edf\u4e00\u6846\u67b6\u6765\u786e\u4fdd\u7cfb\u7edf\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u5f15\u5165\u4e24\u4e2a\u57fa\u7840\u6a21\u578b\uff1a\u4e3b\u673a\u667a\u80fd\u4f53\u6a21\u578b\uff08\u8d1f\u8d23\u7528\u6237\u4ea4\u4e92\u3001\u4efb\u52a1\u5206\u89e3\u548c\u7f16\u6392\uff09\u548c\u4efb\u52a1\u751f\u547d\u5468\u671f\u6a21\u578b\uff08\u8be6\u7ec6\u63cf\u8ff0\u5b50\u4efb\u52a1\u72b6\u6001\u8f6c\u6362\uff09\uff0c\u5b9a\u4e49\u4e8617\u4e2a\u4e3b\u673a\u667a\u80fd\u4f53\u5c5e\u6027\u548c14\u4e2a\u4efb\u52a1\u751f\u547d\u5468\u671f\u5c5e\u6027\uff0c\u4f7f\u7528\u65f6\u5e8f\u903b\u8f91\u8fdb\u884c\u5f62\u5f0f\u5316\u9a8c\u8bc1\u3002", "result": "\u6784\u5efa\u4e86\u9996\u4e2a\u4e25\u683c\u57fa\u7840\u3001\u9886\u57df\u65e0\u5173\u7684\u6846\u67b6\uff0c\u80fd\u591f\u7cfb\u7edf\u5206\u6790\u3001\u8bbe\u8ba1\u548c\u90e8\u7f72\u6b63\u786e\u3001\u53ef\u9760\u3001\u9c81\u68d2\u7684\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\uff0c\u652f\u6301\u68c0\u6d4b\u534f\u8c03\u8fb9\u7f18\u60c5\u51b5\u3001\u9884\u9632\u6b7b\u9501\u548c\u5b89\u5168\u6f0f\u6d1e\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u591aAI\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u884c\u4e3a\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u8bed\u4e49\u57fa\u7840\uff0c\u901a\u8fc7\u5f62\u5f0f\u5316\u9a8c\u8bc1\u786e\u4fdd\u7cfb\u7edf\u5b89\u5168\u6027\u3001\u5b8c\u6574\u6027\u548c\u516c\u5e73\u6027\uff0c\u662f\u6784\u5efa\u53ef\u4fe1\u8d56\u667a\u80fd\u4f53AI\u7cfb\u7edf\u7684\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2510.14787", "categories": ["eess.SY", "cs.SY", "math.OC", "q-bio.PE"], "pdf": "https://arxiv.org/pdf/2510.14787", "abs": "https://arxiv.org/abs/2510.14787", "authors": ["Lorenzo Zino", "Alessandro Casu", "Alessandro Rizzo"], "title": "A Human-Vector Susceptible--Infected--Susceptible Model for Analyzing and Controlling the Spread of Vector-Borne Diseases", "comment": "To appear in the Proceedings of the 2025 European Control Conference\n  (ECC)", "summary": "We propose an epidemic model for the spread of vector-borne diseases. The\nmodel, which is built extending the classical susceptible-infected-susceptible\nmodel, accounts for two populations -- humans and vectors -- and for\ncross-contagion between the two species, whereby humans become infected upon\ninteraction with carrier vectors, and vectors become carriers after interaction\nwith infected humans. We formulate the model as a system of ordinary\ndifferential equations and leverage monotone systems theory to rigorously\ncharacterize the epidemic dynamics. Specifically, we characterize the global\nasymptotic behavior of the disease, determining conditions for quick\neradication of the disease (i.e., for which all trajectories converge to a\ndisease-free equilibrium), or convergence to a (unique) endemic equilibrium.\nThen, we incorporate two control actions: namely, vector control and incentives\nto adopt protection measures. Using the derived mathematical tools, we assess\nthe impact of these two control actions and determine the optimal control\npolicy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7ecf\u5178SIS\u6a21\u578b\u7684\u5a92\u4ecb\u4f20\u64ad\u75be\u75c5\u4f20\u64ad\u6a21\u578b\uff0c\u5305\u542b\u4eba\u7c7b\u548c\u5a92\u4ecb\u4e24\u4e2a\u79cd\u7fa4\uff0c\u901a\u8fc7\u4ea4\u53c9\u611f\u67d3\u673a\u5236\u63cf\u8ff0\u75be\u75c5\u4f20\u64ad\uff0c\u5e76\u5229\u7528\u5355\u8c03\u7cfb\u7edf\u7406\u8bba\u5206\u6790\u5168\u5c40\u52a8\u6001\u884c\u4e3a\u3002", "motivation": "\u4f20\u7edfSIS\u6a21\u578b\u65e0\u6cd5\u5145\u5206\u63cf\u8ff0\u5a92\u4ecb\u4f20\u64ad\u75be\u75c5\u7684\u590d\u6742\u52a8\u6001\uff0c\u9700\u8981\u5f00\u53d1\u5305\u542b\u4eba\u7c7b\u548c\u5a92\u4ecb\u79cd\u7fa4\u4ea4\u4e92\u7684\u6269\u5c55\u6a21\u578b\u6765\u66f4\u51c6\u786e\u5730\u523b\u753b\u8fd9\u7c7b\u75be\u75c5\u7684\u4f20\u64ad\u673a\u5236\u3002", "method": "\u6784\u5efa\u5305\u542b\u4eba\u7c7b\u548c\u5a92\u4ecb\u4e24\u4e2a\u79cd\u7fa4\u7684ODE\u7cfb\u7edf\u6a21\u578b\uff0c\u5229\u7528\u5355\u8c03\u7cfb\u7edf\u7406\u8bba\u5206\u6790\u5168\u5c40\u6e10\u8fd1\u884c\u4e3a\uff0c\u5e76\u5f15\u5165\u5411\u91cf\u63a7\u5236\u548c\u4fdd\u62a4\u63aa\u65bd\u6fc0\u52b1\u4e24\u79cd\u63a7\u5236\u7b56\u7565\u3002", "result": "\u786e\u5b9a\u4e86\u75be\u75c5\u5feb\u901f\u6839\u9664\u7684\u6761\u4ef6\uff08\u6240\u6709\u8f68\u8ff9\u6536\u655b\u5230\u65e0\u75c5\u5e73\u8861\u70b9\uff09\u548c\u6536\u655b\u5230\u552f\u4e00\u5730\u65b9\u6027\u5e73\u8861\u70b9\u7684\u6761\u4ef6\uff0c\u8bc4\u4f30\u4e86\u4e24\u79cd\u63a7\u5236\u7b56\u7565\u7684\u5f71\u54cd\u5e76\u786e\u5b9a\u4e86\u6700\u4f18\u63a7\u5236\u7b56\u7565\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u5a92\u4ecb\u4f20\u64ad\u75be\u75c5\u7684\u52a8\u6001\u5206\u6790\u548c\u63a7\u5236\u7b56\u7565\u5236\u5b9a\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u6570\u5b66\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u6307\u5bfc\u75be\u75c5\u9632\u63a7\u5de5\u4f5c\u3002"}}
{"id": "2510.14300", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14300", "abs": "https://arxiv.org/abs/2510.14300", "authors": ["Weijie Shen", "Yitian Liu", "Yuhao Wu", "Zhixuan Liang", "Sijia Gu", "Dehui Wang", "Tian Nian", "Lei Xu", "Yusen Qin", "Jiangmiao Pang", "Xinping Guan", "Xiaokang Yang", "Yao Mu"], "title": "Expertise need not monopolize: Action-Specialized Mixture of Experts for Vision-Language-Action Learning", "comment": null, "summary": "Vision-Language-Action (VLA) models are experiencing rapid development and\ndemonstrating promising capabilities in robotic manipulation tasks. However,\nscaling up VLA models presents several critical challenges: (1) Training new\nVLA models from scratch demands substantial computational resources and\nextensive datasets. Given the current scarcity of robot data, it becomes\nparticularly valuable to fully leverage well-pretrained VLA model weights\nduring the scaling process. (2) Real-time control requires carefully balancing\nmodel capacity with computational efficiency. To address these challenges, We\npropose AdaMoE, a Mixture-of-Experts (MoE) architecture that inherits\npretrained weights from dense VLA models, and scales up the action expert by\nsubstituting the feedforward layers into sparsely activated MoE layers. AdaMoE\nemploys a decoupling technique that decouples expert selection from expert\nweighting through an independent scale adapter working alongside the\ntraditional router. This enables experts to be selected based on task relevance\nwhile contributing with independently controlled weights, allowing\ncollaborative expert utilization rather than winner-takes-all dynamics. Our\napproach demonstrates that expertise need not monopolize. Instead, through\ncollaborative expert utilization, we can achieve superior performance while\nmaintaining computational efficiency. AdaMoE consistently outperforms the\nbaseline model across key benchmarks, delivering performance gains of 1.8% on\nLIBERO and 9.3% on RoboTwin. Most importantly, a substantial 21.5% improvement\nin real-world experiments validates its practical effectiveness for robotic\nmanipulation tasks.", "AI": {"tldr": "AdaMoE\u662f\u4e00\u79cd\u57fa\u4e8e\u6df7\u5408\u4e13\u5bb6(MoE)\u67b6\u6784\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u7ee7\u627f\u9884\u8bad\u7ec3\u7684\u5bc6\u96c6VLA\u6a21\u578b\u6743\u91cd\uff0c\u5e76\u5c06\u524d\u9988\u5c42\u66ff\u6362\u4e3a\u7a00\u758f\u6fc0\u6d3b\u7684MoE\u5c42\u6765\u6269\u5c55\u52a8\u4f5c\u4e13\u5bb6\uff0c\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u4e0e\u6027\u80fd\u7684\u5e73\u8861\u3002", "motivation": "\u89e3\u51b3VLA\u6a21\u578b\u6269\u5c55\u9762\u4e34\u7684\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a(1)\u4ece\u5934\u8bad\u7ec3\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u548c\u6570\u636e\u96c6\uff0c\u800c\u673a\u5668\u4eba\u6570\u636e\u7a00\u7f3a\uff1b(2)\u5b9e\u65f6\u63a7\u5236\u9700\u8981\u5e73\u8861\u6a21\u578b\u5bb9\u91cf\u4e0e\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u63d0\u51faAdaMoE\u67b6\u6784\uff0c\u91c7\u7528\u89e3\u8026\u6280\u672f\u5c06\u4e13\u5bb6\u9009\u62e9\u4e0e\u4e13\u5bb6\u6743\u91cd\u5206\u914d\u5206\u79bb\uff0c\u901a\u8fc7\u72ec\u7acb\u7684\u5c3a\u5ea6\u9002\u914d\u5668\u4e0e\u4f20\u7edf\u8def\u7531\u5668\u534f\u540c\u5de5\u4f5c\uff0c\u5b9e\u73b0\u57fa\u4e8e\u4efb\u52a1\u76f8\u5173\u6027\u7684\u4e13\u5bb6\u9009\u62e9\u548c\u72ec\u7acb\u63a7\u5236\u7684\u6743\u91cd\u8d21\u732e\u3002", "result": "\u5728\u5173\u952e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff1aLIBERO\u4e0a\u63d0\u53471.8%\uff0cRoboTwin\u4e0a\u63d0\u53479.3%\uff0c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u5927\u5e45\u63d0\u534721.5%\u3002", "conclusion": "\u901a\u8fc7\u534f\u4f5c\u5f0f\u4e13\u5bb6\u5229\u7528\u800c\u975e\u8d62\u5bb6\u901a\u5403\u7684\u52a8\u6001\uff0c\u53ef\u4ee5\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u66f4\u4f18\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u4e13\u4e1a\u77e5\u8bc6\u65e0\u9700\u5784\u65ad\u3002"}}
{"id": "2510.14136", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14136", "abs": "https://arxiv.org/abs/2510.14136", "authors": ["David Roqui", "Ad\u00e8le Cormier", "nistor Grozavu", "Ann Bourges"], "title": "A Multimodal Approach to Heritage Preservation in the Context of Climate Change", "comment": null, "summary": "Cultural heritage sites face accelerating degradation due to climate change,\nyet tradi- tional monitoring relies on unimodal analysis (visual inspection or\nenvironmental sen- sors alone) that fails to capture the complex interplay\nbetween environmental stres- sors and material deterioration. We propose a\nlightweight multimodal architecture that fuses sensor data (temperature,\nhumidity) with visual imagery to predict degradation severity at heritage\nsites. Our approach adapts PerceiverIO with two key innovations: (1) simplified\nencoders (64D latent space) that prevent overfitting on small datasets (n=37\ntraining samples), and (2) Adaptive Barlow Twins loss that encourages modality\ncomplementarity rather than redundancy. On data from Strasbourg Cathedral, our\nmodel achieves 76.9% accu- racy, a 43% improvement over standard multimodal\narchitectures (VisualBERT, Trans- former) and 25% over vanilla PerceiverIO.\nAblation studies reveal that sensor-only achieves 61.5% while image-only\nreaches 46.2%, confirming successful multimodal synergy. A systematic\nhyperparameter study identifies an optimal moderate correlation target ({\\tau}\n=0.3) that balances align- ment and complementarity, achieving 69.2% accuracy\ncompared to other {\\tau} values ({\\tau} =0.1/0.5/0.7: 53.8%, {\\tau} =0.9:\n61.5%). This work demonstrates that architectural sim- plicity combined with\ncontrastive regularization enables effective multimodal learning in data-scarce\nheritage monitoring contexts, providing a foundation for AI-driven con-\nservation decision support systems.", "AI": {"tldr": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u67b6\u6784\uff0c\u878d\u5408\u4f20\u611f\u5668\u6570\u636e\u548c\u89c6\u89c9\u56fe\u50cf\u9884\u6d4b\u6587\u5316\u9057\u4ea7\u5730\u9000\u5316\u4e25\u91cd\u7a0b\u5ea6\uff0c\u5728\u65af\u7279\u62c9\u65af\u5821\u5927\u6559\u5802\u6570\u636e\u4e0a\u8fbe\u523076.9%\u51c6\u786e\u7387\uff0c\u6bd4\u6807\u51c6\u591a\u6a21\u6001\u67b6\u6784\u63d0\u534743%\u3002", "motivation": "\u6587\u5316\u9057\u4ea7\u5730\u56e0\u6c14\u5019\u53d8\u5316\u52a0\u901f\u9000\u5316\uff0c\u4f20\u7edf\u5355\u6a21\u6001\u76d1\u6d4b\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u73af\u5883\u5e94\u529b\u4e0e\u6750\u6599\u9000\u5316\u4e4b\u95f4\u7684\u590d\u6742\u76f8\u4e92\u4f5c\u7528\u3002", "method": "\u91c7\u7528\u6539\u8fdb\u7684PerceiverIO\u67b6\u6784\uff0c\u5305\u542b\u7b80\u5316\u7f16\u7801\u5668\uff0864D\u6f5c\u5728\u7a7a\u95f4\uff09\u548c\u81ea\u9002\u5e94Barlow Twins\u635f\u5931\u51fd\u6570\uff0c\u9f13\u52b1\u6a21\u6001\u4e92\u8865\u6027\u800c\u975e\u5197\u4f59\u3002", "result": "\u6a21\u578b\u51c6\u786e\u7387\u8fbe76.9%\uff0c\u6bd4\u6807\u51c6\u591a\u6a21\u6001\u67b6\u6784\u63d0\u534743%\uff0c\u6bd4\u57fa\u7840PerceiverIO\u63d0\u534725%\u3002\u5355\u6a21\u6001\u4f20\u611f\u5668\u6570\u636e\u51c6\u786e\u7387\u4e3a61.5%\uff0c\u56fe\u50cf\u6570\u636e\u4e3a46.2%\u3002", "conclusion": "\u67b6\u6784\u7b80\u5316\u4e0e\u5bf9\u6bd4\u6b63\u5219\u5316\u76f8\u7ed3\u5408\uff0c\u53ef\u5728\u6570\u636e\u7a00\u7f3a\u7684\u6587\u5316\u9057\u4ea7\u76d1\u6d4b\u73af\u5883\u4e2d\u5b9e\u73b0\u6709\u6548\u7684\u591a\u6a21\u6001\u5b66\u4e60\uff0c\u4e3aAI\u9a71\u52a8\u7684\u4fdd\u62a4\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2510.14834", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.14834", "abs": "https://arxiv.org/abs/2510.14834", "authors": ["Daniel Russell", "Dakota Hamilton", "Mads R. Almassalkhi", "Hamid R. Ossareh"], "title": "Improved Voltage Regulation with Optimal Design of Decentralized Volt-VAr Control", "comment": null, "summary": "Integration of distributed energy resources has created a need for\nautonomous, dynamic voltage regulation. Decentralized Volt-VAr Control (VVC) of\ngrid-connected inverters presents a unique opportunity for voltage management\nbut, if designed poorly, can lead to unstable behavior when in feedback with\nthe grid. We model the grid-VVC closed-loop dynamics with a linearized power\nflow approach, leveraging historical data, which shows improvement over the\ncommonly used LinDistFlow model. This model is used to design VVC slopes by\nminimizing steady-state voltage deviation from the nominal value, subject to a\nnon-convex spectral radius stability constraint, which has not been previously\nimplemented within this context. We compare this constraint to existing convex\nrestrictions and demonstrate, through simulations on a realistic feeder, that\nusing the spectral radius results in more effective voltage regulation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8c31\u534a\u5f84\u7a33\u5b9a\u6027\u7ea6\u675f\u7684\u5206\u6563\u5f0f\u7535\u538b-\u65e0\u529f\u63a7\u5236\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u5584\u914d\u7535\u7f51\u7535\u538b\u8c03\u8282\u6027\u80fd\u3002", "motivation": "\u5206\u5e03\u5f0f\u80fd\u6e90\u63a5\u5165\u9700\u8981\u81ea\u4e3b\u52a8\u6001\u7535\u538b\u8c03\u8282\uff0c\u4f46\u8bbe\u8ba1\u4e0d\u5f53\u7684\u5206\u6563\u5f0f\u7535\u538b-\u65e0\u529f\u63a7\u5236\u53ef\u80fd\u5bfc\u81f4\u7535\u7f51\u53cd\u9988\u4e0d\u7a33\u5b9a\u3002", "method": "\u4f7f\u7528\u7ebf\u6027\u5316\u6f6e\u6d41\u65b9\u6cd5\u5efa\u6a21\u7535\u7f51-VVC\u95ed\u73af\u52a8\u6001\uff0c\u57fa\u4e8e\u5386\u53f2\u6570\u636e\u6539\u8fdbLinDistFlow\u6a21\u578b\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u7a33\u6001\u7535\u538b\u504f\u5dee\u5e76\u65bd\u52a0\u975e\u51f8\u8c31\u534a\u5f84\u7a33\u5b9a\u6027\u7ea6\u675f\u6765\u8bbe\u8ba1VVC\u659c\u7387\u3002", "result": "\u5728\u771f\u5b9e\u9988\u7ebf\u4e0a\u4eff\u771f\u8868\u660e\uff0c\u4f7f\u7528\u8c31\u534a\u5f84\u7ea6\u675f\u6bd4\u73b0\u6709\u51f8\u9650\u5236\u65b9\u6cd5\u80fd\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u7535\u538b\u8c03\u8282\u3002", "conclusion": "\u8c31\u534a\u5f84\u7a33\u5b9a\u6027\u7ea6\u675f\u5728\u5206\u6563\u5f0f\u7535\u538b-\u65e0\u529f\u63a7\u5236\u8bbe\u8ba1\u4e2d\u80fd\u63d0\u4f9b\u66f4\u597d\u7684\u7535\u538b\u8c03\u8282\u6027\u80fd\u3002"}}
{"id": "2510.14338", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14338", "abs": "https://arxiv.org/abs/2510.14338", "authors": ["Yuanhong Zeng", "Anushri Dixit"], "title": "Risk-Aware Reinforcement Learning with Bandit-Based Adaptation for Quadrupedal Locomotion", "comment": null, "summary": "In this work, we study risk-aware reinforcement learning for quadrupedal\nlocomotion. Our approach trains a family of risk-conditioned policies using a\nConditional Value-at-Risk (CVaR) constrained policy optimization technique that\nprovides improved stability and sample efficiency. At deployment, we adaptively\nselect the best performing policy from the family of policies using a\nmulti-armed bandit framework that uses only observed episodic returns, without\nany privileged environment information, and adapts to unknown conditions on the\nfly. Hence, we train quadrupedal locomotion policies at various levels of\nrobustness using CVaR and adaptively select the desired level of robustness\nonline to ensure performance in unknown environments. We evaluate our method in\nsimulation across eight unseen settings (by changing dynamics, contacts,\nsensing noise, and terrain) and on a Unitree Go2 robot in previously unseen\nterrains. Our risk-aware policy attains nearly twice the mean and tail\nperformance in unseen environments compared to other baselines and our\nbandit-based adaptation selects the best-performing risk-aware policy in\nunknown terrain within two minutes of operation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u98ce\u9669\u611f\u77e5\u7684\u56db\u8db3\u673a\u5668\u4eba\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f7f\u7528CVaR\u7ea6\u675f\u7b56\u7565\u4f18\u5316\u8bad\u7ec3\u98ce\u9669\u6761\u4ef6\u7b56\u7565\u65cf\uff0c\u5e76\u901a\u8fc7\u591a\u81c2\u8001\u864e\u673a\u6846\u67b6\u5728\u7ebf\u81ea\u9002\u5e94\u9009\u62e9\u6700\u4f73\u7b56\u7565\uff0c\u5728\u672a\u77e5\u73af\u5883\u4e2d\u5b9e\u73b0\u7a33\u5065\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u56db\u8db3\u673a\u5668\u4eba\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u7a33\u5065\u8fd0\u52a8\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u73af\u5883\u53d8\u5316\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u80fd\u591f\u81ea\u9002\u5e94\u8c03\u6574\u98ce\u9669\u504f\u597d\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528CVaR\u7ea6\u675f\u7b56\u7565\u4f18\u5316\u8bad\u7ec3\u98ce\u9669\u6761\u4ef6\u7b56\u7565\u65cf\uff0c\u90e8\u7f72\u65f6\u901a\u8fc7\u591a\u81c2\u8001\u864e\u673a\u6846\u67b6\u57fa\u4e8e\u89c2\u5bdf\u5230\u7684\u56de\u5408\u56de\u62a5\u81ea\u9002\u5e94\u9009\u62e9\u6700\u4f73\u7b56\u7565\uff0c\u65e0\u9700\u7279\u6743\u73af\u5883\u4fe1\u606f\u3002", "result": "\u57288\u4e2a\u672a\u89c1\u8fc7\u7684\u4eff\u771f\u8bbe\u7f6e\u548c\u771f\u5b9e\u673a\u5668\u4eba\u6d4b\u8bd5\u4e2d\uff0c\u98ce\u9669\u611f\u77e5\u7b56\u7565\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u5e73\u5747\u548c\u5c3e\u90e8\u6027\u80fd\u6bd4\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u63d0\u9ad8\u8fd1\u4e00\u500d\uff0c\u8001\u864e\u673a\u81ea\u9002\u5e94\u80fd\u57282\u5206\u949f\u5185\u9009\u62e9\u51fa\u6700\u4f73\u98ce\u9669\u611f\u77e5\u7b56\u7565\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u98ce\u9669\u6761\u4ef6\u7b56\u7565\u8bad\u7ec3\u548c\u5728\u7ebf\u81ea\u9002\u5e94\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56db\u8db3\u673a\u5668\u4eba\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u7a33\u5065\u6027\u548c\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2510.14150", "categories": ["cs.AI", "cs.LG", "cs.NE", "I.2.7; I.2.2"], "pdf": "https://arxiv.org/pdf/2510.14150", "abs": "https://arxiv.org/abs/2510.14150", "authors": ["Henrique Assump\u00e7\u00e3o", "Diego Ferreira", "Leandro Campos", "Fabricio Murai"], "title": "CodeEvolve: An open source evolutionary coding agent for algorithm discovery and optimization", "comment": "11 pages, 9 figures, 2 tables", "summary": "In this work, we introduce CodeEvolve, an open-source evolutionary coding\nagent that unites Large Language Models (LLMs) with genetic algorithms to solve\ncomplex computational problems. Our framework adapts powerful evolutionary\nconcepts to the LLM domain, building upon recent methods for generalized\nscientific discovery. CodeEvolve employs an island-based genetic algorithm to\nmaintain population diversity and increase throughput, introduces a novel\ninspiration-based crossover mechanism that leverages the LLMs context window to\ncombine features from successful solutions, and implements meta-prompting\nstrategies for dynamic exploration of the solution space. We conduct a rigorous\nevaluation of CodeEvolve on a subset of the mathematical benchmarks used to\nevaluate Google DeepMind's closed-source AlphaEvolve. Our findings show that\nour method surpasses AlphaEvolve's performance on several challenging problems.\nTo foster collaboration and accelerate progress, we release our complete\nframework as an open-source repository.", "AI": {"tldr": "CodeEvolve\u662f\u4e00\u4e2a\u5f00\u6e90\u8fdb\u5316\u7f16\u7801\u4ee3\u7406\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u9057\u4f20\u7b97\u6cd5\u89e3\u51b3\u590d\u6742\u8ba1\u7b97\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86Google DeepMind\u7684AlphaEvolve\u3002", "motivation": "\u5c06\u5f3a\u5927\u7684\u8fdb\u5316\u6982\u5ff5\u5e94\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u9886\u57df\uff0c\u57fa\u4e8e\u5e7f\u4e49\u79d1\u5b66\u53d1\u73b0\u7684\u6700\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u590d\u6742\u8ba1\u7b97\u95ee\u9898\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5c9b\u5c7f\u7684\u9057\u4f20\u7b97\u6cd5\u4fdd\u6301\u79cd\u7fa4\u591a\u6837\u6027\uff0c\u5f15\u5165\u65b0\u9896\u7684\u57fa\u4e8e\u542f\u53d1\u7684\u4ea4\u53c9\u673a\u5236\uff0c\u5229\u7528LLM\u4e0a\u4e0b\u6587\u7a97\u53e3\u7ec4\u5408\u6210\u529f\u89e3\u51b3\u65b9\u6848\u7684\u7279\u5f81\uff0c\u5e76\u5b9e\u73b0\u5143\u63d0\u793a\u7b56\u7565\u52a8\u6001\u63a2\u7d22\u89e3\u7a7a\u95f4\u3002", "result": "\u5728\u7528\u4e8e\u8bc4\u4f30Google DeepMind\u95ed\u6e90AlphaEvolve\u7684\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u5b50\u96c6\u4e0a\uff0cCodeEvolve\u5728\u591a\u4e2a\u6311\u6218\u6027\u95ee\u9898\u4e0a\u7684\u8868\u73b0\u8d85\u8d8a\u4e86AlphaEvolve\u3002", "conclusion": "CodeEvolve\u6210\u529f\u5c06\u8fdb\u5316\u7b97\u6cd5\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\uff0c\u5728\u590d\u6742\u95ee\u9898\u6c42\u89e3\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5f00\u6e90\u5b8c\u6574\u6846\u67b6\u4ee5\u4fc3\u8fdb\u5408\u4f5c\u548c\u52a0\u901f\u8fdb\u5c55\u3002"}}
{"id": "2510.14838", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.14838", "abs": "https://arxiv.org/abs/2510.14838", "authors": ["Ziqing Zhu"], "title": "Dynamic-Key-Aware Co-Simulation Framework for Next Generation of SCADA Systems Encrypted by Quantum-Key-Distribution Techniques", "comment": null, "summary": "To address growing cybersecurity challenges in modern power dispatch systems,\nthis paper proposes a multi-layer modeling and optimization framework for SCADA\nsystems enhanced with quantum key distribution (QKD). While most existing\napplications of QKD in the power sector focus on building secure point-to-point\ncommunication tunnels, they rarely consider the system-level coupling between\nkey dynamics and control scheduling. In contrast, our approach integrates\nquantum key generation, consumption, inventory prediction, and control latency\ninto a unified model, enabling key-aware reconfiguration of SCADA control\nchains based on task security demands and real-time resource constraints. To\nresolve conflicts in key resource allocation between transmission system\noperators (TSOs) and distribution system operators (DSOs), we formulate a\nbi-level Stackelberg game and transform it into a mathematical program with\ncomplementarity constraints (MPCC). We further develop an efficient Level\nDecomposition-Complementarity Pruning (LD-CP) algorithm to solve the problem.\nTo support reproducible evaluation, we build an end-to-end co-simulation\nplatform that integrates physical-layer disruptions via OpenQKD-Sim,\nQ3P/IEC-104 protocol stack binding, and real-time control-chain monitoring\nthrough Grafana. Experimental results on the IEEE 39- and 118-bus systems show\nthat our method increases task success rate by 25%, reduces peak frequency\ndeviation by 70%, and improves key utilization to 83%. This work lays the\nfoundation for future quantum-secure control systems in power grid operations.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8eSCADA\u7cfb\u7edf\u7684\u591a\u5c42\u5efa\u6a21\u548c\u4f18\u5316\u6846\u67b6\uff0c\u96c6\u6210\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1(QKD)\uff0c\u901a\u8fc7\u53cc\u5c42Stackelberg\u535a\u5f08\u89e3\u51b3TSO\u548cDSO\u4e4b\u95f4\u7684\u5bc6\u94a5\u8d44\u6e90\u5206\u914d\u51b2\u7a81\uff0c\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u548c\u7535\u7f51\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u7535\u529b\u7cfb\u7edfQKD\u5e94\u7528\u4e3b\u8981\u5173\u6ce8\u70b9\u5bf9\u70b9\u5b89\u5168\u901a\u4fe1\uff0c\u7f3a\u4e4f\u5bf9\u5bc6\u94a5\u52a8\u6001\u4e0e\u63a7\u5236\u8c03\u5ea6\u7cfb\u7edf\u7ea7\u8026\u5408\u7684\u8003\u8651\uff0c\u9700\u8981\u89e3\u51b3TSO\u548cDSO\u4e4b\u95f4\u7684\u5bc6\u94a5\u8d44\u6e90\u5206\u914d\u51b2\u7a81\u3002", "method": "\u6784\u5efa\u7edf\u4e00\u6a21\u578b\u96c6\u6210\u91cf\u5b50\u5bc6\u94a5\u751f\u6210\u3001\u6d88\u8017\u3001\u5e93\u5b58\u9884\u6d4b\u548c\u63a7\u5236\u5ef6\u8fdf\uff0c\u91c7\u7528\u53cc\u5c42Stackelberg\u535a\u5f08\u6846\u67b6\uff0c\u5f00\u53d1LD-CP\u7b97\u6cd5\u6c42\u89e3MPCC\u95ee\u9898\uff0c\u5e76\u5efa\u7acb\u7aef\u5230\u7aef\u534f\u540c\u4eff\u771f\u5e73\u53f0\u3002", "result": "\u5728IEEE 39\u548c118\u8282\u70b9\u7cfb\u7edf\u6d4b\u8bd5\u4e2d\uff0c\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad825%\uff0c\u5cf0\u503c\u9891\u7387\u504f\u5dee\u51cf\u5c1170%\uff0c\u5bc6\u94a5\u5229\u7528\u7387\u8fbe\u523083%\u3002", "conclusion": "\u4e3a\u672a\u6765\u7535\u7f51\u91cf\u5b50\u5b89\u5168\u63a7\u5236\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u96c6\u6210QKD\u7684SCADA\u7cfb\u7edf\u5728\u63d0\u5347\u7535\u529b\u7cfb\u7edf\u5b89\u5168\u6027\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.14357", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14357", "abs": "https://arxiv.org/abs/2510.14357", "authors": ["Xiaobei Zhao", "Xingqi Lyu", "Xiang Li"], "title": "SUM-AgriVLN: Spatial Understanding Memory for Agricultural Vision-and-Language Navigation", "comment": null, "summary": "Agricultural robots are emerging as powerful assistants across a wide range\nof agricultural tasks, nevertheless, still heavily rely on manual operation or\nfixed rail systems for movement. The AgriVLN method and the A2A benchmark\npioneeringly extend Vision-and-Language Navigation (VLN) to the agricultural\ndomain, enabling robots to navigate to the target positions following the\nnatural language instructions. In practical agricultural scenarios, navigation\ninstructions often repeatedly occur, yet AgriVLN treat each instruction as an\nindependent episode, overlooking the potential of past experiences to provide\nspatial context for subsequent ones. To bridge this gap, we propose the method\nof Spatial Understanding Memory for Agricultural Vision-and-Language Navigation\n(SUM-AgriVLN), in which the SUM module employs spatial understanding and save\nspatial memory through 3D reconstruction and representation. When evaluated on\nthe A2A benchmark, our SUM-AgriVLN effectively improves Success Rate from 0.47\nto 0.54 with slight sacrifice on Navigation Error from 2.91m to 2.93m,\ndemonstrating the state-of-the-art performance in the agricultural domain.\nCode: https://github.com/AlexTraveling/SUM-AgriVLN.", "AI": {"tldr": "\u63d0\u51fa\u4e86SUM-AgriVLN\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a7a\u95f4\u7406\u89e3\u8bb0\u5fc6\u6a21\u5757\u6539\u8fdb\u519c\u4e1a\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\uff0c\u5728A2A\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6210\u529f\u7387\u8fbe\u52300.54\uff0c\u5bfc\u822a\u8bef\u5dee\u4e3a2.93\u7c73\u3002", "motivation": "\u73b0\u6709\u519c\u4e1a\u673a\u5668\u4eba\u5bfc\u822a\u65b9\u6cd5\u5c06\u6bcf\u4e2a\u6307\u4ee4\u89c6\u4e3a\u72ec\u7acb\u4e8b\u4ef6\uff0c\u5ffd\u7565\u4e86\u91cd\u590d\u51fa\u73b0\u7684\u5bfc\u822a\u6307\u4ee4\u53ef\u4ee5\u5229\u7528\u8fc7\u5f80\u7ecf\u9a8c\u63d0\u4f9b\u7a7a\u95f4\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "method": "\u4f7f\u7528\u7a7a\u95f4\u7406\u89e3\u8bb0\u5fc6(SUM)\u6a21\u5757\uff0c\u901a\u8fc73D\u91cd\u5efa\u548c\u8868\u793a\u6765\u4fdd\u5b58\u7a7a\u95f4\u8bb0\u5fc6\uff0c\u4e3a\u540e\u7eed\u5bfc\u822a\u6307\u4ee4\u63d0\u4f9b\u7a7a\u95f4\u4e0a\u4e0b\u6587\u3002", "result": "\u5728A2A\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6210\u529f\u7387\u8fbe\u52300.54\uff08\u4ece0.47\u63d0\u5347\uff09\uff0c\u5bfc\u822a\u8bef\u5dee\u4e3a2.93\u7c73\uff08\u4ece2.91\u7c73\u7565\u5fae\u589e\u52a0\uff09\uff0c\u8fbe\u5230\u519c\u4e1a\u9886\u57df\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "SUM-AgriVLN\u901a\u8fc7\u5f15\u5165\u7a7a\u95f4\u8bb0\u5fc6\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u519c\u4e1a\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5229\u7528\u8fc7\u5f80\u7ecf\u9a8c\u5bf9\u91cd\u590d\u5bfc\u822a\u4efb\u52a1\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.14154", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14154", "abs": "https://arxiv.org/abs/2510.14154", "authors": ["Tian Liu", "Alex Cann", "Ian Colbert", "Mehdi Saeedi"], "title": "Combining Reinforcement Learning and Behavior Trees for NPCs in Video Games with AMD Schola", "comment": "8 pages, 4 figures, 5 tables", "summary": "While the rapid advancements in the reinforcement learning (RL) research\ncommunity have been remarkable, the adoption in commercial video games remains\nslow. In this paper, we outline common challenges the Game AI community faces\nwhen using RL-driven NPCs in practice, and highlight the intersection of RL\nwith traditional behavior trees (BTs) as a crucial juncture to be explored\nfurther. Although the BT+RL intersection has been suggested in several research\npapers, its adoption is rare. We demonstrate the viability of this approach\nusing AMD Schola -- a plugin for training RL agents in Unreal Engine -- by\ncreating multi-task NPCs in a complex 3D environment inspired by the commercial\nvideo game ``The Last of Us\". We provide detailed methodologies for jointly\ntraining RL models with BTs while showcasing various skills.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u5546\u4e1a\u89c6\u9891\u6e38\u620f\u4e2d\u5e94\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u6311\u6218\uff0c\u63d0\u51fa\u5c06RL\u4e0e\u4f20\u7edf\u884c\u4e3a\u6811\uff08BTs\uff09\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7AMD Schola\u63d2\u4ef6\u5728\u590d\u67423D\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u8fdb\u5c55\u8fc5\u901f\uff0c\u4f46\u5728\u5546\u4e1a\u89c6\u9891\u6e38\u620f\u4e2d\u7684\u5e94\u7528\u4ecd\u7136\u7f13\u6162\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u6e38\u620fAI\u793e\u533a\u5728\u4f7f\u7528RL\u9a71\u52a8NPC\u65f6\u9762\u4e34\u7684\u5e38\u89c1\u6311\u6218\uff0c\u5e76\u63a2\u7d22RL\u4e0e\u884c\u4e3a\u6811\u7ed3\u5408\u7684\u5173\u952e\u8282\u70b9\u3002", "method": "\u4f7f\u7528AMD Schola\u63d2\u4ef6\u5728Unreal Engine\u4e2d\u8bad\u7ec3RL\u4ee3\u7406\uff0c\u521b\u5efa\u591a\u4efb\u52a1NPC\u3002\u63d0\u4f9b\u4e86\u5c06RL\u6a21\u578b\u4e0e\u884c\u4e3a\u6811\u8054\u5408\u8bad\u7ec3\u7684\u8be6\u7ec6\u65b9\u6cd5\uff0c\u5e76\u5728\u53d7\u300a\u6700\u540e\u751f\u8fd8\u8005\u300b\u542f\u53d1\u7684\u590d\u67423D\u73af\u5883\u4e2d\u5c55\u793a\u5404\u79cd\u6280\u80fd\u3002", "result": "\u9a8c\u8bc1\u4e86RL\u4e0e\u884c\u4e3a\u6811\u7ed3\u5408\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u6210\u529f\u5728\u590d\u67423D\u73af\u5883\u4e2d\u521b\u5efa\u4e86\u591a\u4efb\u52a1NPC\u3002", "conclusion": "RL\u4e0e\u884c\u4e3a\u6811\u7684\u7ed3\u5408\u662f\u4e00\u4e2a\u503c\u5f97\u8fdb\u4e00\u6b65\u63a2\u7d22\u7684\u5173\u952e\u8282\u70b9\uff0c\u867d\u7136\u5df2\u6709\u7814\u7a76\u63d0\u51fa\u8fd9\u79cd\u7ed3\u5408\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4ecd\u7136\u7f55\u89c1\u3002\u672c\u6587\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u5b9e\u9645\u53ef\u884c\u6027\u3002"}}
{"id": "2510.14854", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.14854", "abs": "https://arxiv.org/abs/2510.14854", "authors": ["Honglei Ma", "Erwu Liu", "Wei Ni", "Zhijun Fang", "Rui Wang", "Yongbin Gao", "Dusit Niyato", "Ekram Hossain"], "title": "Through-the-Earth Magnetic Induction Communication and Networking: A Comprehensive Survey", "comment": "This work has been accepted by the IEEE Communications Surveys &\n  Tutorials (COMST) for publication.The final published version will be\n  available on IEEE Xplore", "summary": "Magnetic induction (MI) communication (MIC) has emerged as a promising\ncandidate for underground communication networks due to its excellent\npenetration capabilities. Integration with Space-Air-Ground-Underground (SAGUI)\nnetworks in next-generation mobile communication systems requires a\nwell-defined network architecture. A recent discovery in MIC research, MI fast\nfading, remains in its early stages and presents unique challenges. This paper\nprovides a comprehensive survey on through-the-earth (TTE) MIC, covering MI\napplications, channel modeling, point-to-point MIC design, relay techniques,\nnetwork frameworks, and emerging technologies. We compare various MIC\napplications to highlight TTE-specific challenges and review the principles of\nchannel modeling, addressing both MI slow fading and MI fast fading, along with\nits potential impact on existing MIC theories. We conduct a fine-grained\ndecomposition of MI channel power gain into four distinct physical parameters,\nand propose a novel geometric model to analyze MI fast fading. We also\nsummarize MI relay techniques, examine crosstalk effects in relay and\nhigh-density networks, and explore key research tasks within the OSI framework\nfor a holistic MI network protocol in SAGUI. To bridge the gaps identified, we\npropose a MIC framework that supports TCP/IP and Linux, enabling full\nimplementation of existing and emerging MIC solutions. This framework empowers\nresearchers to leverage Linux resources and deep learning platforms for\naccelerated development of MIC in SAGUI networks. Remaining research\nchallenges, open issues, and promising novel techniques are further identified\nto advance MIC research.", "AI": {"tldr": "\u672c\u6587\u5bf9\u5730\u4e0b\u78c1\u611f\u5e94\u901a\u4fe1(MIC)\u8fdb\u884c\u4e86\u5168\u9762\u7efc\u8ff0\uff0c\u6db5\u76d6\u5e94\u7528\u3001\u4fe1\u9053\u5efa\u6a21\u3001\u70b9\u5bf9\u70b9\u901a\u4fe1\u8bbe\u8ba1\u3001\u4e2d\u7ee7\u6280\u672f\u3001\u7f51\u7edc\u6846\u67b6\u548c\u65b0\u5174\u6280\u672f\uff0c\u63d0\u51fa\u4e86\u652f\u6301TCP/IP\u548cLinux\u7684MIC\u6846\u67b6\uff0c\u5e76\u8bc6\u522b\u4e86\u7814\u7a76\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u78c1\u611f\u5e94\u901a\u4fe1\u56e0\u5176\u51fa\u8272\u7684\u7a7f\u900f\u80fd\u529b\u6210\u4e3a\u5730\u4e0b\u901a\u4fe1\u7f51\u7edc\u7684\u6709\u524d\u666f\u5019\u9009\u6280\u672f\uff0c\u4e0e\u4e0b\u4e00\u4ee3\u79fb\u52a8\u901a\u4fe1\u7cfb\u7edf\u7684\u7a7a\u5929\u5730\u4e00\u4f53\u5316\u7f51\u7edc\u96c6\u6210\u9700\u8981\u660e\u786e\u7684\u7f51\u7edc\u67b6\u6784\uff0cMI\u5feb\u901f\u8870\u843d\u8fd9\u4e00\u65b0\u53d1\u73b0\u4ecd\u5904\u4e8e\u65e9\u671f\u9636\u6bb5\u5e76\u5e26\u6765\u72ec\u7279\u6311\u6218\u3002", "method": "\u5bf9MI\u4fe1\u9053\u529f\u7387\u589e\u76ca\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5206\u89e3\u4e3a\u56db\u4e2a\u7269\u7406\u53c2\u6570\uff0c\u63d0\u51fa\u65b0\u7684\u51e0\u4f55\u6a21\u578b\u5206\u6790MI\u5feb\u901f\u8870\u843d\uff0c\u603b\u7ed3\u4e2d\u7ee7\u6280\u672f\uff0c\u7814\u7a76\u9ad8\u5bc6\u5ea6\u7f51\u7edc\u4e2d\u7684\u4e32\u6270\u6548\u5e94\uff0c\u63a2\u7d22OSI\u6846\u67b6\u4e0b\u7684\u5173\u952e\u7814\u7a76\u4efb\u52a1\uff0c\u63d0\u51fa\u652f\u6301TCP/IP\u548cLinux\u7684MIC\u6846\u67b6\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684MIC\u6846\u67b6\uff0c\u652f\u6301TCP/IP\u548cLinux\uff0c\u4f7f\u7814\u7a76\u4eba\u5458\u80fd\u591f\u5229\u7528Linux\u8d44\u6e90\u548c\u6df1\u5ea6\u5b66\u4e60\u5e73\u53f0\u52a0\u901fSAGUI\u7f51\u7edc\u4e2dMIC\u7684\u53d1\u5c55\u3002", "conclusion": "\u672c\u6587\u4e3a\u78c1\u611f\u5e94\u901a\u4fe1\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u7efc\u8ff0\u548c\u6846\u67b6\uff0c\u8bc6\u522b\u4e86\u5269\u4f59\u7684\u7814\u7a76\u6311\u6218\u3001\u5f00\u653e\u95ee\u9898\u548c\u6709\u524d\u666f\u7684\u65b0\u6280\u672f\uff0c\u63a8\u52a8\u4e86MIC\u5728\u7a7a\u5929\u5730\u4e00\u4f53\u5316\u7f51\u7edc\u4e2d\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.14414", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.14414", "abs": "https://arxiv.org/abs/2510.14414", "authors": ["Baris Baysal", "Omid Arfaie", "Ramazan Unal"], "title": "RoboANKLE: Design, Development, and Functional Evaluation of a Robotic Ankle with a Motorized Compliant Unit", "comment": null, "summary": "This study presents a powered transtibial prosthesis with complete push-off\nassistance, RoboANKLE. The design aims to fulfill specific requirements, such\nas a sufficient range of motion (RoM) while providing the necessary torque for\nachieving natural ankle motion in daily activities. Addressing the challenges\nfaced in designing active transtibial prostheses, such as maintaining energetic\nautonomy and minimizing weight, is vital for the study. With this aim, we try\nto imitate the human ankle by providing extensive push-off assistance to\nachieve a natural-like torque profile. Thus, Energy Store and Extended Release\nmechanism (ESER) is employed with a novel Extra Energy Storage (EES) mechanism.\nKinematic and kinetic analyses are carried out to determine the design\nparameters and assess the design performance. Subsequently, a Computer-Aided\nDesign (CAD) model is built and used in comprehensive dynamic and structural\nanalyses. These analyses are used for the design performance evaluation and\ndetermine the forces and torques applied to the prosthesis, which aids in\noptimizing the design for minimal weight via structural analysis and topology\noptimization. The design of the prototype is then finalized and manufactured\nfor experimental evaluation to validate the design and functionality. The\nprototype is realized with a mass of 1.92 kg and dimensions of 261x107x420 mm.\nThe Functional evaluations of the RoboANKLE revealed that it is capable of\nachieving the natural maximum dorsi-flexion angle with 95% accuracy. Also,\nThanks to the implemented mechanisms, the results show that RoboANKLE can\ngenerate 57% higher than the required torque for natural walking. The result of\nthe power generation capacity of the RoboANKLE is 10% more than the natural\npower during the gait cycle.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u540d\u4e3aRoboANKLE\u7684\u4e3b\u52a8\u5f0f\u80eb\u9aa8\u5047\u80a2\uff0c\u91c7\u7528\u80fd\u91cf\u5b58\u50a8\u548c\u6269\u5c55\u91ca\u653e\u673a\u5236(ESER)\u53ca\u65b0\u578b\u989d\u5916\u80fd\u91cf\u5b58\u50a8(EES)\u673a\u5236\uff0c\u80fd\u591f\u63d0\u4f9b\u5b8c\u6574\u7684\u63a8\u8fdb\u8f85\u52a9\uff0c\u5b9e\u73b0\u81ea\u7136\u8e1d\u5173\u8282\u8fd0\u52a8\u3002", "motivation": "\u8bbe\u8ba1\u4e3b\u52a8\u5f0f\u80eb\u9aa8\u5047\u80a2\u9762\u4e34\u7ef4\u6301\u80fd\u91cf\u81ea\u4e3b\u6027\u548c\u51cf\u8f7b\u91cd\u91cf\u7684\u6311\u6218\uff0c\u672c\u7814\u7a76\u65e8\u5728\u6a21\u4eff\u4eba\u7c7b\u8e1d\u5173\u8282\uff0c\u63d0\u4f9b\u5e7f\u6cdb\u7684\u63a8\u8fdb\u8f85\u52a9\u4ee5\u5b9e\u73b0\u81ea\u7136\u7684\u626d\u77e9\u5206\u5e03\u3002", "method": "\u901a\u8fc7\u8fd0\u52a8\u5b66\u548c\u52a8\u529b\u5b66\u5206\u6790\u786e\u5b9a\u8bbe\u8ba1\u53c2\u6570\uff0c\u5efa\u7acbCAD\u6a21\u578b\u8fdb\u884c\u52a8\u6001\u548c\u7ed3\u6784\u5206\u6790\uff0c\u91c7\u7528\u7ed3\u6784\u5206\u6790\u548c\u62d3\u6251\u4f18\u5316\u5b9e\u73b0\u6700\u5c0f\u91cd\u91cf\u8bbe\u8ba1\uff0c\u6700\u7ec8\u5236\u9020\u539f\u578b\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "\u539f\u578b\u8d28\u91cf\u4e3a1.92kg\uff0c\u5c3a\u5bf8261x107x420mm\u3002\u529f\u80fd\u8bc4\u4f30\u663e\u793a\uff0cRoboANKLE\u80fd\u591f\u4ee595%\u7684\u51c6\u786e\u5ea6\u5b9e\u73b0\u81ea\u7136\u6700\u5927\u80cc\u5c48\u89d2\u5ea6\uff0c\u4ea7\u751f\u7684\u626d\u77e9\u6bd4\u81ea\u7136\u884c\u8d70\u6240\u9700\u9ad857%\uff0c\u529f\u7387\u751f\u6210\u80fd\u529b\u6bd4\u81ea\u7136\u6b65\u6001\u5468\u671f\u529f\u7387\u9ad810%\u3002", "conclusion": "RoboANKLE\u6210\u529f\u5b9e\u73b0\u4e86\u81ea\u7136\u8e1d\u5173\u8282\u8fd0\u52a8\u7684\u4eff\u751f\u8bbe\u8ba1\uff0c\u5728\u91cd\u91cf\u3001\u626d\u77e9\u548c\u529f\u7387\u6027\u80fd\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u6240\u91c7\u7528\u673a\u5236\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.14169", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14169", "abs": "https://arxiv.org/abs/2510.14169", "authors": ["Praphul Singh", "Corey Barrett", "Sumana Srivasta", "Amitabh Saikia", "Irfan Bulu", "Sri Gadde", "Krishnaram Kenthapadi"], "title": "JEDA: Query-Free Clinical Order Search from Ambient Dialogues", "comment": null, "summary": "Clinical conversations mix explicit directives (order a chest X-ray) with\nimplicit reasoning (the cough worsened overnight, we should check for\npneumonia). Many systems rely on LLM rewriting, adding latency, instability,\nand opacity that hinder real-time ordering. We present JEDA (Joint Embedding\nfor Direct and Ambient clinical orders), a domain-initialized bi-encoder that\nretrieves canonical orders directly and, in a query-free mode, encodes a short\nrolling window of ambient dialogue to trigger retrieval. Initialized from\nPubMedBERT and fine-tuned with a duplicate-safe contrastive objective, JEDA\naligns heterogeneous expressions of intent to shared order concepts. Training\nuses constrained LLM guidance to tie each signed order to complementary\nformulations (command only, context only, command+context, context+reasoning),\nproducing clearer inter-order separation, tighter query extendash order\ncoupling, and stronger generalization. The query-free mode is noise-resilient,\nreducing sensitivity to disfluencies and ASR errors by conditioning on a short\nwindow rather than a single utterance. Deployed in practice, JEDA yields large\ngains and substantially outperforms its base encoder and recent open embedders\n(Linq Embed Mistral, SFR Embedding, GTE Qwen, BGE large, Embedding Gemma). The\nresult is a fast, interpretable, LLM-free retrieval layer that links ambient\ncontext to actionable clinical orders in real time.", "AI": {"tldr": "JEDA\u662f\u4e00\u4e2a\u7528\u4e8e\u4e34\u5e8a\u8ba2\u5355\u68c0\u7d22\u7684\u53cc\u7f16\u7801\u5668\u7cfb\u7edf\uff0c\u80fd\u591f\u76f4\u63a5\u4ece\u4e34\u5e8a\u5bf9\u8bdd\u4e2d\u68c0\u7d22\u89c4\u8303\u8ba2\u5355\uff0c\u65e0\u9700\u4f9d\u8d56LLM\u91cd\u5199\uff0c\u63d0\u4f9b\u5b9e\u65f6\u3001\u53ef\u89e3\u91ca\u7684\u68c0\u7d22\u80fd\u529b\u3002", "motivation": "\u4e34\u5e8a\u5bf9\u8bdd\u5305\u542b\u663e\u6027\u6307\u4ee4\u548c\u9690\u6027\u63a8\u7406\uff0c\u73b0\u6709\u7cfb\u7edf\u4f9d\u8d56LLM\u91cd\u5199\u4f1a\u5e26\u6765\u5ef6\u8fdf\u3001\u4e0d\u7a33\u5b9a\u548c\u4e0d\u900f\u660e\u95ee\u9898\uff0c\u963b\u788d\u5b9e\u65f6\u8ba2\u5355\u5904\u7406\u3002", "method": "\u57fa\u4e8ePubMedBERT\u521d\u59cb\u5316\uff0c\u4f7f\u7528\u53cc\u7f16\u7801\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u5bf9\u9f50\u5f02\u8d28\u610f\u56fe\u8868\u8fbe\u4e0e\u5171\u4eab\u8ba2\u5355\u6982\u5ff5\uff0c\u652f\u6301\u67e5\u8be2\u548c\u65e0\u67e5\u8be2\u4e24\u79cd\u6a21\u5f0f\u3002", "result": "JEDA\u5728\u5b9e\u8df5\u4e2d\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u5927\u5e45\u4f18\u4e8e\u57fa\u7840\u7f16\u7801\u5668\u548c\u6700\u65b0\u5f00\u6e90\u5d4c\u5165\u6a21\u578b\uff0c\u65e0\u67e5\u8be2\u6a21\u5f0f\u5bf9\u566a\u58f0\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "JEDA\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5feb\u901f\u3001\u53ef\u89e3\u91ca\u3001\u65e0\u9700LLM\u7684\u68c0\u7d22\u5c42\uff0c\u80fd\u591f\u5b9e\u65f6\u5c06\u73af\u5883\u4e0a\u4e0b\u6587\u94fe\u63a5\u5230\u53ef\u64cd\u4f5c\u7684\u4e34\u5e8a\u8ba2\u5355\u3002"}}
{"id": "2510.14931", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.14931", "abs": "https://arxiv.org/abs/2510.14931", "authors": ["Bo Wang", "Tianyu Han", "Guangwei Wang"], "title": "Further Results on Safety-Critical Stabilization of Force-Controlled Nonholonomic Mobile Robots", "comment": null, "summary": "In this paper, we address the stabilization problem for force-controlled\nnonholonomic mobile robots under safety-critical constraints. We propose a\ncontinuous, time-invariant control law based on the gamma m-quadratic\nprogramming (gamma m-QP) framework, which unifies control Lyapunov functions\n(CLFs) and control barrier functions (CBFs) to enforce both stability and\nsafety in the closed-loop system. For the first time, we construct a global,\ntime-invariant, strict Lyapunov function for the closed-loop nonholonomic\nmobile robot system with a nominal stabilization controller in polar\ncoordinates; this strict Lyapunov function then serves as the CLF in the QP\ndesign. Next, by exploiting the inherent cascaded structure of the vehicle\ndynamics, we develop a CBF for the mobile robot via an integrator backstepping\nprocedure. Our main results guarantee both asymptotic stability and safety for\nthe closed-loop system. Both the simulation and experimental results are\npresented to illustrate the effectiveness and performance of our approach.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8egamma m-QP\u6846\u67b6\u7684\u8fde\u7eed\u65f6\u4e0d\u53d8\u63a7\u5236\u5f8b\uff0c\u89e3\u51b3\u529b\u63a7\u975e\u5b8c\u6574\u79fb\u52a8\u673a\u5668\u4eba\u5728\u5b89\u5168\u7ea6\u675f\u4e0b\u7684\u9547\u5b9a\u95ee\u9898\uff0c\u9996\u6b21\u6784\u5efa\u4e86\u5168\u5c40\u65f6\u4e0d\u53d8\u4e25\u683cLyapunov\u51fd\u6570\uff0c\u4fdd\u8bc1\u95ed\u73af\u7cfb\u7edf\u7684\u6e10\u8fd1\u7a33\u5b9a\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u89e3\u51b3\u975e\u5b8c\u6574\u79fb\u52a8\u673a\u5668\u4eba\u5728\u5b89\u5168\u5173\u952e\u7ea6\u675f\u4e0b\u7684\u9547\u5b9a\u95ee\u9898\uff0c\u9700\u8981\u540c\u65f6\u4fdd\u8bc1\u7cfb\u7edf\u7a33\u5b9a\u6027\u548c\u5b89\u5168\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u7edf\u4e00\u5904\u7406CLF\u548cCBF\u3002", "method": "\u91c7\u7528gamma m-QP\u6846\u67b6\u7edf\u4e00CLF\u548cCBF\uff0c\u5728\u6781\u5750\u6807\u4e0b\u6784\u5efa\u5168\u5c40\u65f6\u4e0d\u53d8\u4e25\u683cLyapunov\u51fd\u6570\u4f5c\u4e3aCLF\uff0c\u901a\u8fc7\u79ef\u5206\u53cd\u6b65\u6cd5\u5f00\u53d1CBF\u3002", "result": "\u7406\u8bba\u4fdd\u8bc1\u95ed\u73af\u7cfb\u7edf\u6e10\u8fd1\u7a33\u5b9a\u6027\u548c\u5b89\u5168\u6027\uff0c\u4eff\u771f\u548c\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u63a7\u5236\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u975e\u5b8c\u6574\u79fb\u52a8\u673a\u5668\u4eba\u5728\u5b89\u5168\u7ea6\u675f\u4e0b\u7684\u9547\u5b9a\u95ee\u9898\uff0c\u4e3a\u7c7b\u4f3c\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u63a7\u5236\u6846\u67b6\u3002"}}
{"id": "2510.14454", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14454", "abs": "https://arxiv.org/abs/2510.14454", "authors": ["Tao Huang", "Huayi Wang", "Junli Ren", "Kangning Yin", "Zirui Wang", "Xiao Chen", "Feiyu Jia", "Wentao Zhang", "Junfeng Long", "Jingbo Wang", "Jiangmiao Pang"], "title": "Towards Adaptable Humanoid Control via Adaptive Motion Tracking", "comment": "9 pages", "summary": "Humanoid robots are envisioned to adapt demonstrated motions to diverse\nreal-world conditions while accurately preserving motion patterns. Existing\nmotion prior approaches enable well adaptability with a few motions but often\nsacrifice imitation accuracy, whereas motion-tracking methods achieve accurate\nimitation yet require many training motions and a test-time target motion to\nadapt. To combine their strengths, we introduce AdaMimic, a novel motion\ntracking algorithm that enables adaptable humanoid control from a single\nreference motion. To reduce data dependence while ensuring adaptability, our\nmethod first creates an augmented dataset by sparsifying the single reference\nmotion into keyframes and applying light editing with minimal physical\nassumptions. A policy is then initialized by tracking these sparse keyframes to\ngenerate dense intermediate motions, and adapters are subsequently trained to\nadjust tracking speed and refine low-level actions based on the adjustment,\nenabling flexible time warping that further improves imitation accuracy and\nadaptability. We validate these significant improvements in our approach in\nboth simulation and the real-world Unitree G1 humanoid robot in multiple tasks\nacross a wide range of adaptation conditions. Videos and code are available at\nhttps://taohuang13.github.io/adamimic.github.io/.", "AI": {"tldr": "AdaMimic\u662f\u4e00\u79cd\u65b0\u9896\u7684\u8fd0\u52a8\u8ddf\u8e2a\u7b97\u6cd5\uff0c\u80fd\u591f\u4ece\u5355\u4e00\u53c2\u8003\u8fd0\u52a8\u5b9e\u73b0\u53ef\u9002\u5e94\u7684\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\uff0c\u901a\u8fc7\u7a00\u758f\u5173\u952e\u5e27\u548c\u7075\u6d3b\u65f6\u95f4\u626d\u66f2\u6280\u672f\uff0c\u5728\u4fdd\u6301\u6a21\u4eff\u7cbe\u5ea6\u7684\u540c\u65f6\u63d0\u9ad8\u9002\u5e94\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u8fd0\u52a8\u5148\u9a8c\u65b9\u6cd5\u9002\u5e94\u6027\u597d\u4f46\u6a21\u4eff\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u4ee5\u53ca\u8fd0\u52a8\u8ddf\u8e2a\u65b9\u6cd5\u7cbe\u5ea6\u9ad8\u4f46\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u548c\u6d4b\u8bd5\u65f6\u76ee\u6807\u8fd0\u52a8\u7684\u95ee\u9898\uff0c\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\u3002", "method": "\u9996\u5148\u901a\u8fc7\u7a00\u758f\u5316\u5355\u4e00\u53c2\u8003\u8fd0\u52a8\u521b\u5efa\u5173\u952e\u5e27\u5e76\u5e94\u7528\u8f7b\u91cf\u7f16\u8f91\u751f\u6210\u589e\u5f3a\u6570\u636e\u96c6\uff1b\u7136\u540e\u8bad\u7ec3\u7b56\u7565\u8ddf\u8e2a\u7a00\u758f\u5173\u952e\u5e27\u751f\u6210\u5bc6\u96c6\u4e2d\u95f4\u8fd0\u52a8\uff1b\u6700\u540e\u8bad\u7ec3\u9002\u914d\u5668\u8c03\u6574\u8ddf\u8e2a\u901f\u5ea6\u548c\u7ec6\u5316\u4f4e\u7ea7\u52a8\u4f5c\uff0c\u5b9e\u73b0\u7075\u6d3b\u7684\u65f6\u95f4\u626d\u66f2\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u7684Unitree G1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u4efb\u52a1\u548c\u5e7f\u6cdb\u9002\u5e94\u6761\u4ef6\u4e0b\u7684\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "AdaMimic\u6210\u529f\u7ed3\u5408\u4e86\u8fd0\u52a8\u5148\u9a8c\u548c\u8fd0\u52a8\u8ddf\u8e2a\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u4ece\u5355\u4e00\u53c2\u8003\u8fd0\u52a8\u7684\u9ad8\u7cbe\u5ea6\u6a21\u4eff\u548c\u5f3a\u9002\u5e94\u6027\u63a7\u5236\u3002"}}
{"id": "2510.14176", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14176", "abs": "https://arxiv.org/abs/2510.14176", "authors": ["Roger Creus Castanyer", "Faisal Mohamed", "Pablo Samuel Castro", "Cyrus Neary", "Glen Berseth"], "title": "ARM-FM: Automated Reward Machines via Foundation Models for Compositional Reinforcement Learning", "comment": null, "summary": "Reinforcement learning (RL) algorithms are highly sensitive to reward\nfunction specification, which remains a central challenge limiting their broad\napplicability. We present ARM-FM: Automated Reward Machines via Foundation\nModels, a framework for automated, compositional reward design in RL that\nleverages the high-level reasoning capabilities of foundation models (FMs).\nReward machines (RMs) -- an automata-based formalism for reward specification\n-- are used as the mechanism for RL objective specification, and are\nautomatically constructed via the use of FMs. The structured formalism of RMs\nyields effective task decompositions, while the use of FMs enables objective\nspecifications in natural language. Concretely, we (i) use FMs to automatically\ngenerate RMs from natural language specifications; (ii) associate language\nembeddings with each RM automata-state to enable generalization across tasks;\nand (iii) provide empirical evidence of ARM-FM's effectiveness in a diverse\nsuite of challenging environments, including evidence of zero-shot\ngeneralization.", "AI": {"tldr": "ARM-FM\u662f\u4e00\u4e2a\u5229\u7528\u57fa\u7840\u6a21\u578b\u81ea\u52a8\u751f\u6210\u5956\u52b1\u673a\u6765\u6539\u8fdb\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u81ea\u7136\u8bed\u8a00\u89c4\u8303\u81ea\u52a8\u6784\u5efa\u7ed3\u6784\u5316\u5956\u52b1\u51fd\u6570\uff0c\u5e76\u5b9e\u73b0\u8de8\u4efb\u52a1\u7684\u96f6\u6837\u672c\u6cdb\u5316\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5bf9\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\u9ad8\u5ea6\u654f\u611f\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002\u73b0\u6709\u7684\u5956\u52b1\u8bbe\u8ba1\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u4eba\u5de5\u53c2\u4e0e\uff0c\u96be\u4ee5\u5b9e\u73b0\u590d\u6742\u4efb\u52a1\u7684\u81ea\u52a8\u5316\u548c\u6cdb\u5316\u3002", "method": "\u4f7f\u7528\u57fa\u7840\u6a21\u578b\u4ece\u81ea\u7136\u8bed\u8a00\u89c4\u8303\u81ea\u52a8\u751f\u6210\u5956\u52b1\u673a\uff0c\u4e3a\u6bcf\u4e2a\u81ea\u52a8\u673a\u72b6\u6001\u5173\u8054\u8bed\u8a00\u5d4c\u5165\u4ee5\u5b9e\u73b0\u8de8\u4efb\u52a1\u6cdb\u5316\uff0c\u5229\u7528\u5956\u52b1\u673a\u7684\u7ed3\u6784\u5316\u5f62\u5f0f\u5b9e\u73b0\u6709\u6548\u7684\u4efb\u52a1\u5206\u89e3\u3002", "result": "\u5728\u591a\u4e2a\u6311\u6218\u6027\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86ARM-FM\u7684\u6709\u6548\u6027\uff0c\u5305\u62ec\u5c55\u793a\u4e86\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u80fd\u591f\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5956\u52b1\u51fd\u6570\u3002", "conclusion": "ARM-FM\u901a\u8fc7\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u548c\u5956\u52b1\u673a\u5f62\u5f0f\u5316\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u5956\u52b1\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\u7684\u4eba\u5de5\u6210\u672c\uff0c\u5e76\u5b9e\u73b0\u4e86\u8de8\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.14120", "categories": ["cs.ET", "cs.NE", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.14120", "abs": "https://arxiv.org/abs/2510.14120", "authors": ["Muhammad Faheemur Rahman", "Wayne Burleson"], "title": "Laser Fault Injection in Memristor-Based Accelerators for AI/ML and Neuromorphic Computing", "comment": "3 pages, 4 figures", "summary": "Memristive crossbar arrays (MCA) are emerging as efficient building blocks\nfor in-memory computing and neuromorphic hardware due to their high density and\nparallel analog matrix-vector multiplication capabilities. However, the\nphysical properties of their nonvolatile memory elements introduce new attack\nsurfaces, particularly under fault injection scenarios. This work explores\nLaser Fault Injection as a means of inducing analog perturbations in MCA-based\narchitectures. We present a detailed threat model in which adversaries target\nmemristive cells to subtly alter their physical properties or outputs using\nlaser beams. Through HSPICE simulations of a large MCA on 45 nm CMOS tech.\nnode, we show how laser-induced photocurrent manifests in output current\ndistributions, enabling differential fault analysis to infer internal weights\nwith up to 99.7% accuracy, replicate the model, and compromise computational\nintegrity through targeted weight alterations by approximately 143%.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u6fc0\u5149\u6545\u969c\u6ce8\u5165\u653b\u51fb\u5bf9\u5fc6\u963b\u5668\u4ea4\u53c9\u9635\u5217\u7684\u5a01\u80c1\uff0c\u901a\u8fc7\u6fc0\u5149\u8bf1\u5bfc\u5149\u7535\u6d41\u6270\u52a8\u5fc6\u963b\u5355\u5143\uff0c\u53ef\u9ad8\u7cbe\u5ea6\u63a8\u65ad\u5185\u90e8\u6743\u91cd\u5e76\u7834\u574f\u8ba1\u7b97\u5b8c\u6574\u6027\u3002", "motivation": "\u5fc6\u963b\u5668\u4ea4\u53c9\u9635\u5217\u56e0\u5176\u9ad8\u5bc6\u5ea6\u548c\u5e76\u884c\u6a21\u62df\u77e9\u9635\u5411\u91cf\u4e58\u6cd5\u80fd\u529b\u800c\u6210\u4e3a\u5185\u5b58\u8ba1\u7b97\u548c\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u7684\u5173\u952e\u7ec4\u4ef6\uff0c\u4f46\u5176\u975e\u6613\u5931\u6027\u5b58\u50a8\u5355\u5143\u7684\u7269\u7406\u7279\u6027\u5f15\u5165\u4e86\u65b0\u7684\u653b\u51fb\u9762\uff0c\u7279\u522b\u662f\u5728\u6545\u969c\u6ce8\u5165\u573a\u666f\u4e0b\u3002", "method": "\u901a\u8fc7HSPICE\u4eff\u771f45nm CMOS\u6280\u672f\u8282\u70b9\u4e0a\u7684\u5927\u578b\u5fc6\u963b\u5668\u4ea4\u53c9\u9635\u5217\uff0c\u7814\u7a76\u6fc0\u5149\u8bf1\u5bfc\u5149\u7535\u6d41\u5728\u8f93\u51fa\u7535\u6d41\u5206\u5e03\u4e2d\u7684\u8868\u73b0\uff0c\u5229\u7528\u5dee\u5206\u6545\u969c\u5206\u6790\u6280\u672f\u3002", "result": "\u6fc0\u5149\u6545\u969c\u6ce8\u5165\u80fd\u591f\u4ee599.7%\u7684\u51c6\u786e\u7387\u63a8\u65ad\u5185\u90e8\u6743\u91cd\uff0c\u590d\u5236\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u9488\u5bf9\u6027\u6743\u91cd\u4fee\u6539\u4f7f\u8ba1\u7b97\u5b8c\u6574\u6027\u53d7\u635f\u7ea6143%\u3002", "conclusion": "\u5fc6\u963b\u5668\u4ea4\u53c9\u9635\u5217\u5728\u6fc0\u5149\u6545\u969c\u6ce8\u5165\u653b\u51fb\u4e0b\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u6f0f\u6d1e\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u7684\u9632\u62a4\u63aa\u65bd\u6765\u4fdd\u969c\u5176\u8ba1\u7b97\u5b8c\u6574\u6027\u3002"}}
{"id": "2510.14467", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14467", "abs": "https://arxiv.org/abs/2510.14467", "authors": ["Shang-Fu Chen", "Co Yong", "Shao-Hua Sun"], "title": "Restoring Noisy Demonstration for Imitation Learning With Diffusion Models", "comment": "Published in IEEE Transactions on Neural Networks and Learning\n  Systems (TNNLS)", "summary": "Imitation learning (IL) aims to learn a policy from expert demonstrations and\nhas been applied to various applications. By learning from the expert policy,\nIL methods do not require environmental interactions or reward signals.\nHowever, most existing imitation learning algorithms assume perfect expert\ndemonstrations, but expert demonstrations often contain imperfections caused by\nerrors from human experts or sensor/control system inaccuracies. To address the\nabove problems, this work proposes a filter-and-restore framework to best\nleverage expert demonstrations with inherent noise. Our proposed method first\nfilters clean samples from the demonstrations and then learns conditional\ndiffusion models to recover the noisy ones. We evaluate our proposed framework\nand existing methods in various domains, including robot arm manipulation,\ndexterous manipulation, and locomotion. The experiment results show that our\nproposed framework consistently outperforms existing methods across all the\ntasks. Ablation studies further validate the effectiveness of each component\nand demonstrate the framework's robustness to different noise types and levels.\nThese results confirm the practical applicability of our framework to noisy\noffline demonstration data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8fc7\u6ee4-\u6062\u590d\u6846\u67b6\u6765\u5904\u7406\u5305\u542b\u566a\u58f0\u7684\u4e13\u5bb6\u6f14\u793a\u6570\u636e\uff0c\u901a\u8fc7\u8fc7\u6ee4\u5e72\u51c0\u6837\u672c\u5e76\u5b66\u4e60\u6761\u4ef6\u6269\u6563\u6a21\u578b\u6765\u6062\u590d\u566a\u58f0\u6837\u672c\uff0c\u5728\u591a\u4e2a\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u7b97\u6cd5\u901a\u5e38\u5047\u8bbe\u4e13\u5bb6\u6f14\u793a\u662f\u5b8c\u7f8e\u7684\uff0c\u4f46\u5b9e\u9645\u4e0a\u4e13\u5bb6\u6f14\u793a\u5e38\u5305\u542b\u4eba\u4e3a\u9519\u8bef\u6216\u4f20\u611f\u5668/\u63a7\u5236\u7cfb\u7edf\u4e0d\u51c6\u786e\u5bfc\u81f4\u7684\u566a\u58f0\uff0c\u9700\u8981\u5904\u7406\u4e0d\u5b8c\u7f8e\u6f14\u793a\u6570\u636e\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u9996\u5148\u4ece\u6f14\u793a\u4e2d\u8fc7\u6ee4\u51fa\u5e72\u51c0\u6837\u672c\uff0c\u7136\u540e\u5b66\u4e60\u6761\u4ef6\u6269\u6563\u6a21\u578b\u6765\u6062\u590d\u566a\u58f0\u6837\u672c\u3002", "result": "\u5728\u673a\u5668\u4eba\u624b\u81c2\u64cd\u4f5c\u3001\u7075\u5de7\u64cd\u4f5c\u548c\u8fd0\u52a8\u7b49\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u6240\u6709\u4efb\u52a1\u4e2d\u90fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5bf9\u4e0d\u540c\u7c7b\u578b\u7684\u566a\u58f0\u548c\u566a\u58f0\u6c34\u5e73\u5177\u6709\u9c81\u68d2\u6027\uff0c\u8bc1\u5b9e\u4e86\u5176\u5728\u5904\u7406\u566a\u58f0\u79bb\u7ebf\u6f14\u793a\u6570\u636e\u65b9\u9762\u7684\u5b9e\u9645\u9002\u7528\u6027\u3002"}}
{"id": "2510.14194", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14194", "abs": "https://arxiv.org/abs/2510.14194", "authors": ["G\u00f6ktu\u011f Bender", "Samer Faraj", "Anand Bhardwaj"], "title": "Implementation of AI in Precision Medicine", "comment": "Accepted to SMASH 2025", "summary": "Artificial intelligence (AI) has become increasingly central to precision\nmedicine by enabling the integration and interpretation of multimodal data, yet\nimplementation in clinical settings remains limited. This paper provides a\nscoping review of literature from 2019-2024 on the implementation of AI in\nprecision medicine, identifying key barriers and enablers across data quality,\nclinical reliability, workflow integration, and governance. Through an\necosystem-based framework, we highlight the interdependent relationships\nshaping real-world translation and propose future directions to support\ntrustworthy and sustainable implementation.", "AI": {"tldr": "\u5bf92019-2024\u5e74\u7cbe\u51c6\u533b\u5b66\u4e2dAI\u5b9e\u65bd\u6587\u732e\u7684\u8303\u56f4\u7efc\u8ff0\uff0c\u8bc6\u522b\u5173\u952e\u969c\u788d\u548c\u4fc3\u8fdb\u56e0\u7d20\uff0c\u63d0\u51fa\u57fa\u4e8e\u751f\u6001\u7cfb\u7edf\u7684\u5b9e\u65bd\u6846\u67b6\u3002", "motivation": "AI\u5728\u7cbe\u51c6\u533b\u5b66\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u4e34\u5e8a\u5b9e\u65bd\u4ecd\u7136\u6709\u9650\uff0c\u9700\u8981\u7cfb\u7edf\u5206\u6790\u5b9e\u65bd\u969c\u788d\u548c\u4fc3\u8fdb\u56e0\u7d20\u3002", "method": "\u91c7\u7528\u8303\u56f4\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5206\u67902019-2024\u5e74\u76f8\u5173\u6587\u732e\uff0c\u4f7f\u7528\u57fa\u4e8e\u751f\u6001\u7cfb\u7edf\u7684\u6846\u67b6\u5206\u6790\u6570\u636e\u8d28\u91cf\u3001\u4e34\u5e8a\u53ef\u9760\u6027\u3001\u5de5\u4f5c\u6d41\u7a0b\u6574\u5408\u548c\u6cbb\u7406\u7b49\u65b9\u9762\u3002", "result": "\u8bc6\u522b\u4e86\u7cbe\u51c6\u533b\u5b66\u4e2dAI\u5b9e\u65bd\u7684\u5173\u952e\u969c\u788d\u548c\u4fc3\u8fdb\u56e0\u7d20\uff0c\u5f3a\u8c03\u4e86\u5404\u8981\u7d20\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\u5173\u7cfb\u3002", "conclusion": "\u63d0\u51fa\u4e86\u652f\u6301\u53ef\u4fe1\u8d56\u548c\u53ef\u6301\u7eed\u5b9e\u65bdAI\u7684\u672a\u6765\u65b9\u5411\uff0c\u5f3a\u8c03\u751f\u6001\u7cfb\u7edf\u89c6\u89d2\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.14511", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.14511", "abs": "https://arxiv.org/abs/2510.14511", "authors": ["Mingtian Du", "Suhas Raghavendra Kulkarni", "Simone Kager", "Domenico Campolo"], "title": "Stability Criteria and Motor Performance in Delayed Haptic Dyadic Interactions Mediated by Robots", "comment": null, "summary": "This paper establishes analytical stability criteria for robot-mediated\nhuman-human (dyadic) interaction systems, focusing on haptic communication\nunder network-induced time delays. Through frequency-domain analysis supported\nby numerical simulations, we identify both delay-independent and\ndelay-dependent stability criteria. The delay-independent criterion guarantees\nstability irrespective of the delay, whereas the delay-dependent criterion is\ncharacterised by a maximum tolerable delay before instability occurs. The\ncriteria demonstrate dependence on controller and robot dynamic parameters,\nwhere increasing stiffness reduces the maximum tolerable delay in a non-linear\nmanner, thereby heightening system vulnerability. The proposed criteria can be\ngeneralised to a wide range of robot-mediated interactions and serve as design\nguidelines for stable remote dyadic systems. Experiments with robots performing\nhuman-like movements further illustrate the correlation between stability and\nmotor performance. The findings of this paper suggest the prerequisites for\neffective delay-compensation strategies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u673a\u5668\u4eba\u4ecb\u5bfc\u7684\u4eba-\u4eba\u4ea4\u4e92\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u5206\u6790\u6807\u51c6\uff0c\u91cd\u70b9\u5173\u6ce8\u7f51\u7edc\u65f6\u5ef6\u4e0b\u7684\u89e6\u89c9\u901a\u4fe1\uff0c\u901a\u8fc7\u9891\u57df\u5206\u6790\u548c\u6570\u503c\u6a21\u62df\u786e\u5b9a\u4e86\u65f6\u5ef6\u65e0\u5173\u548c\u65f6\u5ef6\u76f8\u5173\u7684\u7a33\u5b9a\u6027\u6761\u4ef6\u3002", "motivation": "\u7814\u7a76\u673a\u5668\u4eba\u4ecb\u5bfc\u7684\u4eba-\u4eba\u4ea4\u4e92\u7cfb\u7edf\u5728\u7f51\u7edc\u65f6\u5ef6\u4e0b\u7684\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u4e3a\u8fdc\u7a0b\u534f\u4f5c\u7cfb\u7edf\u63d0\u4f9b\u8bbe\u8ba1\u6307\u5bfc\u3002", "method": "\u91c7\u7528\u9891\u57df\u5206\u6790\u548c\u6570\u503c\u6a21\u62df\u65b9\u6cd5\uff0c\u5206\u6790\u63a7\u5236\u5668\u548c\u673a\u5668\u4eba\u52a8\u6001\u53c2\u6570\u5bf9\u7cfb\u7edf\u7a33\u5b9a\u6027\u7684\u5f71\u54cd\u3002", "result": "\u5efa\u7acb\u4e86\u65f6\u5ef6\u65e0\u5173\u548c\u65f6\u5ef6\u76f8\u5173\u7684\u7a33\u5b9a\u6027\u6807\u51c6\uff0c\u53d1\u73b0\u589e\u52a0\u521a\u5ea6\u4f1a\u975e\u7ebf\u6027\u5730\u964d\u4f4e\u6700\u5927\u53ef\u5bb9\u5fcd\u65f6\u5ef6\uff0c\u4f7f\u7cfb\u7edf\u66f4\u6613\u5931\u7a33\u3002", "conclusion": "\u63d0\u51fa\u7684\u7a33\u5b9a\u6027\u6807\u51c6\u53ef\u63a8\u5e7f\u5230\u5404\u79cd\u673a\u5668\u4eba\u4ecb\u5bfc\u4ea4\u4e92\u573a\u666f\uff0c\u4e3a\u8bbe\u8ba1\u7a33\u5b9a\u7684\u8fdc\u7a0b\u534f\u4f5c\u7cfb\u7edf\u63d0\u4f9b\u6307\u5bfc\uff0c\u5e76\u6307\u51fa\u4e86\u6709\u6548\u65f6\u5ef6\u8865\u507f\u7b56\u7565\u7684\u524d\u63d0\u6761\u4ef6\u3002"}}
{"id": "2510.14207", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14207", "abs": "https://arxiv.org/abs/2510.14207", "authors": ["Trilok Padhi", "Pinxian Lu", "Abdulkadir Erol", "Tanmay Sutar", "Gauri Sharma", "Mina Sonmez", "Munmun De Choudhury", "Ugur Kursuncu"], "title": "Echoes of Human Malice in Agents: Benchmarking LLMs for Multi-Turn Online Harassment Attacks", "comment": "13 pages, 4 figures", "summary": "Large Language Model (LLM) agents are powering a growing share of interactive\nweb applications, yet remain vulnerable to misuse and harm. Prior jailbreak\nresearch has largely focused on single-turn prompts, whereas real harassment\noften unfolds over multi-turn interactions. In this work, we present the Online\nHarassment Agentic Benchmark consisting of: (i) a synthetic multi-turn\nharassment conversation dataset, (ii) a multi-agent (e.g., harasser, victim)\nsimulation informed by repeated game theory, (iii) three jailbreak methods\nattacking agents across memory, planning, and fine-tuning, and (iv) a\nmixed-methods evaluation framework. We utilize two prominent LLMs,\nLLaMA-3.1-8B-Instruct (open-source) and Gemini-2.0-flash (closed-source). Our\nresults show that jailbreak tuning makes harassment nearly guaranteed with an\nattack success rate of 95.78--96.89% vs. 57.25--64.19% without tuning in Llama,\nand 99.33% vs. 98.46% without tuning in Gemini, while sharply reducing refusal\nrate to 1-2% in both models. The most prevalent toxic behaviors are Insult with\n84.9--87.8% vs. 44.2--50.8% without tuning, and Flaming with 81.2--85.1% vs.\n31.5--38.8% without tuning, indicating weaker guardrails compared to sensitive\ncategories such as sexual or racial harassment. Qualitative evaluation further\nreveals that attacked agents reproduce human-like aggression profiles, such as\nMachiavellian/psychopathic patterns under planning, and narcissistic tendencies\nwith memory. Counterintuitively, closed-source and open-source models exhibit\ndistinct escalation trajectories across turns, with closed-source models\nshowing significant vulnerability. Overall, our findings show that multi-turn\nand theory-grounded attacks not only succeed at high rates but also mimic\nhuman-like harassment dynamics, motivating the development of robust safety\nguardrails to ultimately keep online platforms safe and responsible.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5728\u7ebf\u9a9a\u6270\u4ee3\u7406\u57fa\u51c6\uff0c\u901a\u8fc7\u591a\u8f6e\u5bf9\u8bdd\u548c\u535a\u5f08\u8bba\u6a21\u62df\u653b\u51fbLLM\u4ee3\u7406\uff0c\u53d1\u73b0\u5fae\u8c03\u653b\u51fb\u4f7f\u9a9a\u6270\u6210\u529f\u7387\u5927\u5e45\u63d0\u5347\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u5b89\u5168\u9632\u62a4\u5728\u6301\u7eed\u6027\u9a9a\u6270\u4e2d\u7684\u8106\u5f31\u6027\u3002", "motivation": "\u73b0\u6709\u8d8a\u72f1\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u8f6e\u63d0\u793a\uff0c\u800c\u771f\u5b9e\u9a9a\u6270\u5f80\u5f80\u5728\u591a\u8f6e\u4ea4\u4e92\u4e2d\u5c55\u5f00\uff0c\u9700\u8981\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b\u5408\u6210\u591a\u8f6e\u9a9a\u6270\u5bf9\u8bdd\u6570\u636e\u96c6\u3001\u57fa\u4e8e\u91cd\u590d\u535a\u5f08\u8bba\u7684\u591a\u4ee3\u7406\u6a21\u62df\u3001\u4e09\u79cd\u9488\u5bf9\u8bb0\u5fc6\u3001\u89c4\u5212\u548c\u5fae\u8c03\u7684\u8d8a\u72f1\u65b9\u6cd5\uff0c\u4ee5\u53ca\u6df7\u5408\u8bc4\u4f30\u6846\u67b6\u7684\u57fa\u51c6\u7cfb\u7edf\u3002", "result": "\u5fae\u8c03\u653b\u51fb\u4f7f\u9a9a\u6270\u6210\u529f\u7387\u5728Llama\u4e2d\u4ece57-64%\u63d0\u5347\u81f396-97%\uff0c\u5728Gemini\u4e2d\u4ece98%\u63d0\u5347\u81f399%\uff1b\u4fae\u8fb1\u884c\u4e3a\u4ece45-51%\u63d0\u5347\u81f385-88%\uff0c\u8c29\u9a82\u884c\u4e3a\u4ece32-39%\u63d0\u5347\u81f381-85%\u3002", "conclusion": "\u591a\u8f6e\u548c\u7406\u8bba\u57fa\u7840\u7684\u653b\u51fb\u4e0d\u4ec5\u6210\u529f\u7387\u9ad8\uff0c\u8fd8\u80fd\u6a21\u62df\u4eba\u7c7b\u9a9a\u6270\u52a8\u6001\uff0c\u8868\u660e\u9700\u8981\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u5b89\u5168\u9632\u62a4\u673a\u5236\u6765\u4fdd\u62a4\u5728\u7ebf\u5e73\u53f0\u5b89\u5168\u3002"}}
{"id": "2510.14546", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14546", "abs": "https://arxiv.org/abs/2510.14546", "authors": ["Matti Pekkanen", "Francesco Verdoja", "Ville Kyrki"], "title": "QuASH: Using Natural-Language Heuristics to Query Visual-Language Robotic Maps", "comment": "Submitted to ICRA 2026", "summary": "Embeddings from Visual-Language Models are increasingly utilized to represent\nsemantics in robotic maps, offering an open-vocabulary scene understanding that\nsurpasses traditional, limited labels. Embeddings enable on-demand querying by\ncomparing embedded user text prompts to map embeddings via a similarity metric.\nThe key challenge in performing the task indicated in a query is that the robot\nmust determine the parts of the environment relevant to the query.\n  This paper proposes a solution to this challenge. We leverage\nnatural-language synonyms and antonyms associated with the query within the\nembedding space, applying heuristics to estimate the language space relevant to\nthe query, and use that to train a classifier to partition the environment into\nmatches and non-matches. We evaluate our method through extensive experiments,\nquerying both maps and standard image benchmarks. The results demonstrate\nincreased queryability of maps and images. Our querying technique is agnostic\nto the representation and encoder used, and requires limited training.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\u7684\u673a\u5668\u4eba\u5730\u56fe\u8bed\u4e49\u67e5\u8be2\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u67e5\u8be2\u8bcd\u7684\u540c\u4e49\u8bcd\u548c\u53cd\u4e49\u8bcd\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u5173\u7cfb\u6765\u8bad\u7ec3\u5206\u7c7b\u5668\uff0c\u4ece\u800c\u66f4\u51c6\u786e\u5730\u8bc6\u522b\u73af\u5883\u4e2d\u4e0e\u67e5\u8be2\u76f8\u5173\u7684\u533a\u57df\u3002", "motivation": "\u867d\u7136\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u5d4c\u5165\u80fd\u591f\u63d0\u4f9b\u5f00\u653e\u8bcd\u6c47\u7684\u573a\u666f\u7406\u89e3\uff0c\u4f46\u673a\u5668\u4eba\u9700\u8981\u786e\u5b9a\u73af\u5883\u4e2d\u54ea\u4e9b\u90e8\u5206\u4e0e\u7528\u6237\u67e5\u8be2\u76f8\u5173\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002", "method": "\u5229\u7528\u67e5\u8be2\u8bcd\u7684\u81ea\u7136\u8bed\u8a00\u540c\u4e49\u8bcd\u548c\u53cd\u4e49\u8bcd\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u5173\u7cfb\uff0c\u5e94\u7528\u542f\u53d1\u5f0f\u65b9\u6cd5\u4f30\u8ba1\u4e0e\u67e5\u8be2\u76f8\u5173\u7684\u8bed\u8a00\u7a7a\u95f4\uff0c\u5e76\u8bad\u7ec3\u5206\u7c7b\u5668\u5c06\u73af\u5883\u5212\u5206\u4e3a\u5339\u914d\u548c\u975e\u5339\u914d\u533a\u57df\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u5728\u67e5\u8be2\u5730\u56fe\u548c\u6807\u51c6\u56fe\u50cf\u57fa\u51c6\u65f6\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5730\u56fe\u548c\u56fe\u50cf\u7684\u53ef\u67e5\u8be2\u6027\u3002", "conclusion": "\u8be5\u67e5\u8be2\u6280\u672f\u5bf9\u8868\u793a\u5f62\u5f0f\u548c\u7f16\u7801\u5668\u5177\u6709\u65e0\u5173\u6027\uff0c\u4e14\u53ea\u9700\u8981\u6709\u9650\u7684\u8bad\u7ec3\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u5bf9\u73af\u5883\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2510.14240", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14240", "abs": "https://arxiv.org/abs/2510.14240", "authors": ["Jiayu Wang", "Yifei Ming", "Riya Dulepet", "Qinglin Chen", "Austin Xu", "Zixuan Ke", "Frederic Sala", "Aws Albarghouthi", "Caiming Xiong", "Shafiq Joty"], "title": "LiveResearchBench: A Live Benchmark for User-Centric Deep Research in the Wild", "comment": null, "summary": "Deep research -- producing comprehensive, citation-grounded reports by\nsearching and synthesizing information from hundreds of live web sources --\nmarks an important frontier for agentic systems. To rigorously evaluate this\nability, four principles are essential: tasks should be (1) user-centric,\nreflecting realistic information needs, (2) dynamic, requiring up-to-date\ninformation beyond parametric knowledge, (3) unambiguous, ensuring consistent\ninterpretation across users, and (4) multi-faceted and search-intensive,\nrequiring search over numerous web sources and in-depth analysis. Existing\nbenchmarks fall short of these principles, often focusing on narrow domains or\nposing ambiguous questions that hinder fair comparison. Guided by these\nprinciples, we introduce LiveResearchBench, a benchmark of 100 expert-curated\ntasks spanning daily life, enterprise, and academia, each requiring extensive,\ndynamic, real-time web search and synthesis. Built with over 1,500 hours of\nhuman labor, LiveResearchBench provides a rigorous basis for systematic\nevaluation. To evaluate citation-grounded long-form reports, we introduce\nDeepEval, a comprehensive suite covering both content- and report-level\nquality, including coverage, presentation, citation accuracy and association,\nconsistency and depth of analysis. DeepEval integrates four complementary\nevaluation protocols, each designed to ensure stable assessment and high\nagreement with human judgments. Using LiveResearchBench and DeepEval, we\nconduct a comprehensive evaluation of 17 frontier deep research systems,\nincluding single-agent web search, single-agent deep research, and multi-agent\nsystems. Our analysis reveals current strengths, recurring failure modes, and\nkey system components needed to advance reliable, insightful deep research.", "AI": {"tldr": "LiveResearchBench\u662f\u4e00\u4e2a\u5305\u542b100\u4e2a\u4e13\u5bb6\u7b56\u5212\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u3002DeepEval\u662f\u4e00\u4e2a\u7efc\u5408\u8bc4\u4f30\u5957\u4ef6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5f15\u7528\u57fa\u7840\u7684\u6df1\u5ea6\u7814\u7a76\u62a5\u544a\u3002\u901a\u8fc7\u5bf917\u4e2a\u524d\u6cbf\u7cfb\u7edf\u7684\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u7cfb\u7edf\u7684\u4f18\u52bf\u548c\u5e38\u89c1\u5931\u8d25\u6a21\u5f0f\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u8bc4\u4f30\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u65f6\u5b58\u5728\u4e0d\u8db3\uff1a\u4efb\u52a1\u9886\u57df\u72ed\u7a84\u3001\u95ee\u9898\u6a21\u7cca\u3001\u7f3a\u4e4f\u52a8\u6001\u5b9e\u65f6\u4fe1\u606f\u9700\u6c42\u3002\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u7b26\u5408\u7528\u6237\u4e2d\u5fc3\u3001\u52a8\u6001\u3001\u660e\u786e\u3001\u591a\u7ef4\u5ea6\u641c\u7d22\u5bc6\u96c6\u578b\u539f\u5219\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u63d0\u51faLiveResearchBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b100\u4e2a\u4e13\u5bb6\u7b56\u5212\u4efb\u52a1\uff0c\u6db5\u76d6\u65e5\u5e38\u751f\u6d3b\u3001\u4f01\u4e1a\u548c\u5b66\u672f\u9886\u57df\uff0c\u6bcf\u4e2a\u4efb\u52a1\u90fd\u9700\u8981\u5e7f\u6cdb\u7684\u52a8\u6001\u5b9e\u65f6\u7f51\u7edc\u641c\u7d22\u548c\u7efc\u5408\u3002\u5f00\u53d1DeepEval\u8bc4\u4f30\u5957\u4ef6\uff0c\u6db5\u76d6\u5185\u5bb9\u548c\u62a5\u544a\u5c42\u9762\u7684\u8d28\u91cf\u8bc4\u4f30\uff0c\u5305\u62ec\u8986\u76d6\u7387\u3001\u5448\u73b0\u3001\u5f15\u7528\u51c6\u786e\u6027\u3001\u4e00\u81f4\u6027\u548c\u5206\u6790\u6df1\u5ea6\u3002", "result": "\u901a\u8fc7LiveResearchBench\u548cDeepEval\u5bf917\u4e2a\u524d\u6cbf\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u8bc6\u522b\u4e86\u5f53\u524d\u7cfb\u7edf\u7684\u4f18\u52bf\u548c\u5e38\u89c1\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "LiveResearchBench\u548cDeepEval\u4e3a\u7cfb\u7edf\u8bc4\u4f30\u6df1\u5ea6\u7814\u7a76\u80fd\u529b\u63d0\u4f9b\u4e86\u4e25\u683c\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u63a8\u8fdb\u53ef\u9760\u3001\u6709\u6d1e\u5bdf\u529b\u7684\u6df1\u5ea6\u7814\u7a76\u6240\u9700\u7684\u5173\u952e\u7cfb\u7edf\u7ec4\u4ef6\u3002"}}
{"id": "2510.14584", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14584", "abs": "https://arxiv.org/abs/2510.14584", "authors": ["Benno Wingender", "Nils Dengler", "Rohit Menon", "Sicong Pan", "Maren Bennewitz"], "title": "A Generalized Placeability Metric for Model-Free Unified Pick-and-Place Reasoning", "comment": null, "summary": "To reliably pick and place unknown objects under real-world sensing noise\nremains a challenging task, as existing methods rely on strong object priors\n(e.g., CAD models), or planar-support assumptions, limiting generalization and\nunified reasoning between grasping and placing. In this work, we introduce a\ngeneralized placeability metric that evaluates placement poses directly from\nnoisy point clouds, without any shape priors. The metric jointly scores\nstability, graspability, and clearance. From raw geometry, we extract the\nsupport surfaces of the object to generate diverse candidates for\nmulti-orientation placement and sample contacts that satisfy collision and\nstability constraints. By conditioning grasp scores on each candidate\nplacement, our proposed method enables model-free unified pick-and-place\nreasoning and selects grasp-place pairs that lead to stable, collision-free\nplacements. On unseen real objects and non-planar object supports, our metric\ndelivers CAD-comparable accuracy in predicting stability loss and generally\nproduces more physically plausible placements than learning-based predictors.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u566a\u58f0\u70b9\u4e91\u76f4\u63a5\u8bc4\u4f30\u653e\u7f6e\u59ff\u6001\u7684\u901a\u7528\u53ef\u653e\u7f6e\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u65e0\u9700\u7269\u4f53\u5f62\u72b6\u5148\u9a8c\uff0c\u5b9e\u73b0\u65e0\u6a21\u578b\u7edf\u4e00\u6293\u53d6-\u653e\u7f6e\u63a8\u7406", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5f3a\u7269\u4f53\u5148\u9a8c\u6216\u5e73\u9762\u652f\u6491\u5047\u8bbe\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u6293\u53d6-\u653e\u7f6e\u7684\u7edf\u4e00\u63a8\u7406\uff0c\u65e0\u6cd5\u53ef\u9760\u5904\u7406\u672a\u77e5\u7269\u4f53\u548c\u771f\u5b9e\u4e16\u754c\u611f\u77e5\u566a\u58f0", "method": "\u4ece\u539f\u59cb\u51e0\u4f55\u4e2d\u63d0\u53d6\u7269\u4f53\u652f\u6491\u8868\u9762\uff0c\u751f\u6210\u591a\u65b9\u5411\u653e\u7f6e\u5019\u9009\uff0c\u91c7\u6837\u6ee1\u8db3\u78b0\u649e\u548c\u7a33\u5b9a\u6027\u7ea6\u675f\u7684\u63a5\u89e6\u70b9\uff0c\u901a\u8fc7\u6761\u4ef6\u5316\u6293\u53d6\u8bc4\u5206\u5b9e\u73b0\u7edf\u4e00\u63a8\u7406", "result": "\u5728\u672a\u89c1\u771f\u5b9e\u7269\u4f53\u548c\u975e\u5e73\u9762\u652f\u6491\u4e0a\uff0c\u8be5\u5ea6\u91cf\u5728\u9884\u6d4b\u7a33\u5b9a\u6027\u635f\u5931\u65b9\u9762\u8fbe\u5230CAD\u53ef\u6bd4\u7cbe\u5ea6\uff0c\u6bd4\u5b66\u4e60\u578b\u9884\u6d4b\u5668\u4ea7\u751f\u66f4\u7269\u7406\u5408\u7406\u7684\u653e\u7f6e", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u65e0\u6a21\u578b\u7edf\u4e00\u6293\u53d6-\u653e\u7f6e\u63a8\u7406\uff0c\u5728\u771f\u5b9e\u566a\u58f0\u73af\u5883\u4e0b\u5bf9\u672a\u77e5\u7269\u4f53\u5177\u6709\u826f\u597d\u6cdb\u5316\u80fd\u529b"}}
{"id": "2510.14253", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14253", "abs": "https://arxiv.org/abs/2510.14253", "authors": ["Wangtao Sun", "Xiang Cheng", "Jialin Fan", "Yao Xu", "Xing Yu", "Shizhu He", "Jun Zhao", "Kang Liu"], "title": "Towards Agentic Self-Learning LLMs in Search Environment", "comment": null, "summary": "We study whether self-learning can scale LLM-based agents without relying on\nhuman-curated datasets or predefined rule-based rewards. Through controlled\nexperiments in a search-agent setting, we identify two key determinants of\nscalable agent training: the source of reward signals and the scale of agent\ntask data. We find that rewards from a Generative Reward Model (GRM) outperform\nrigid rule-based signals for open-domain learning, and that co-evolving the GRM\nwith the policy further boosts performance. Increasing the volume of agent task\ndata-even when synthetically generated-substantially enhances agentic\ncapabilities. Building on these insights, we propose \\textbf{Agentic\nSelf-Learning} (ASL), a fully closed-loop, multi-role reinforcement learning\nframework that unifies task generation, policy execution, and evaluation within\na shared tool environment and LLM backbone. ASL coordinates a Prompt Generator,\na Policy Model, and a Generative Reward Model to form a virtuous cycle of\nharder task setting, sharper verification, and stronger solving. Empirically,\nASL delivers steady, round-over-round gains, surpasses strong RLVR baselines\n(e.g., Search-R1) that plateau or degrade, and continues improving under\nzero-labeled-data conditions, indicating superior sample efficiency and\nrobustness. We further show that GRM verification capacity is the main\nbottleneck: if frozen, it induces reward hacking and stalls progress; continual\nGRM training on the evolving data distribution mitigates this, and a small\nlate-stage injection of real verification data raises the performance ceiling.\nThis work establishes reward source and data scale as critical levers for\nopen-domain agent learning and demonstrates the efficacy of multi-role\nco-evolution for scalable, self-improving agents. The data and code of this\npaper are released at\nhttps://github.com/forangel2014/Towards-Agentic-Self-Learning", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAgentic Self-Learning (ASL)\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u89d2\u8272\u534f\u540c\u8fdb\u5316\u7684\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u65e0\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u7684\u667a\u80fd\u4f53\u81ea\u6211\u5b66\u4e60\uff0c\u8bc1\u660e\u751f\u6210\u5956\u52b1\u6a21\u578b\u548c\u4efb\u52a1\u6570\u636e\u89c4\u6a21\u662f\u5f00\u653e\u9886\u57df\u667a\u80fd\u4f53\u5b66\u4e60\u7684\u5173\u952e\u56e0\u7d20\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u4e0d\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\u6216\u9884\u5b9a\u4e49\u89c4\u5219\u5956\u52b1\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u81ea\u6211\u5b66\u4e60\u6269\u5c55\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u80fd\u529b\uff0c\u89e3\u51b3\u5f00\u653e\u9886\u57df\u667a\u80fd\u4f53\u8bad\u7ec3\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51faASL\u6846\u67b6\uff0c\u5305\u542b\u63d0\u793a\u751f\u6210\u5668\u3001\u7b56\u7565\u6a21\u578b\u548c\u751f\u6210\u5956\u52b1\u6a21\u578b\u4e09\u4e2a\u89d2\u8272\uff0c\u5728\u5171\u4eab\u5de5\u5177\u73af\u5883\u548cLLM\u9aa8\u5e72\u7f51\u7edc\u4e2d\u5f62\u6210\u4efb\u52a1\u751f\u6210\u3001\u7b56\u7565\u6267\u884c\u548c\u8bc4\u4f30\u7684\u95ed\u73af\u5f3a\u5316\u5b66\u4e60\u5faa\u73af\u3002", "result": "ASL\u5b9e\u73b0\u6301\u7eed\u591a\u8f6e\u6027\u80fd\u63d0\u5347\uff0c\u8d85\u8d8a\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u96f6\u6807\u6ce8\u6570\u636e\u6761\u4ef6\u4e0b\u7ee7\u7eed\u6539\u8fdb\uff0c\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6837\u672c\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002\u751f\u6210\u5956\u52b1\u6a21\u578b\u7684\u9a8c\u8bc1\u80fd\u529b\u662f\u4e3b\u8981\u74f6\u9888\uff0c\u6301\u7eed\u8bad\u7ec3\u53ef\u7f13\u89e3\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\u3002", "conclusion": "\u5956\u52b1\u6765\u6e90\u548c\u6570\u636e\u89c4\u6a21\u662f\u5f00\u653e\u9886\u57df\u667a\u80fd\u4f53\u5b66\u4e60\u7684\u5173\u952e\u6760\u6746\uff0c\u591a\u89d2\u8272\u534f\u540c\u8fdb\u5316\u662f\u5b9e\u73b0\u53ef\u6269\u5c55\u81ea\u6211\u6539\u8fdb\u667a\u80fd\u4f53\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2510.14947", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.14947", "abs": "https://arxiv.org/abs/2510.14947", "authors": ["Blake Werner", "Lizhi Yang", "Aaron D. Ames"], "title": "Architecture Is All You Need: Diversity-Enabled Sweet Spots for Robust Humanoid Locomotion", "comment": "8 pages", "summary": "Robust humanoid locomotion in unstructured environments requires\narchitectures that balance fast low-level stabilization with slower perceptual\ndecision-making. We show that a simple layered control architecture (LCA), a\nproprioceptive stabilizer running at high rate, coupled with a compact low-rate\nperceptual policy, enables substantially more robust performance than\nmonolithic end-to-end designs, even when using minimal perception encoders.\nThrough a two-stage training curriculum (blind stabilizer pretraining followed\nby perceptual fine-tuning), we demonstrate that layered policies consistently\noutperform one-stage alternatives in both simulation and hardware. On a Unitree\nG1 humanoid, our approach succeeds across stair and ledge tasks where one-stage\nperceptual policies fail. These results highlight that architectural separation\nof timescales, rather than network scale or complexity, is the key enabler for\nrobust perception-conditioned locomotion.", "AI": {"tldr": "\u5206\u5c42\u63a7\u5236\u67b6\u6784(LCA)\u901a\u8fc7\u5c06\u9ad8\u9891\u672c\u4f53\u611f\u89c9\u7a33\u5b9a\u5668\u4e0e\u4f4e\u9891\u611f\u77e5\u7b56\u7565\u7ed3\u5408\uff0c\u6bd4\u7aef\u5230\u7aef\u8bbe\u8ba1\u66f4\u7a33\u5065\uff0c\u5728\u697c\u68af\u548c\u8fb9\u7f18\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5b9e\u73b0\u7a33\u5065\u7684\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u9700\u8981\u5e73\u8861\u5feb\u901f\u4f4e\u7ea7\u7a33\u5b9a\u548c\u6162\u901f\u611f\u77e5\u51b3\u7b56\u7684\u67b6\u6784\u3002", "method": "\u91c7\u7528\u5206\u5c42\u63a7\u5236\u67b6\u6784\uff1a\u9ad8\u9891\u672c\u4f53\u611f\u89c9\u7a33\u5b9a\u5668\u4e0e\u7d27\u51d1\u4f4e\u9891\u611f\u77e5\u7b56\u7565\u7ed3\u5408\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u8bfe\u7a0b\uff08\u76f2\u7a33\u5b9a\u5668\u9884\u8bad\u7ec3\u540e\u611f\u77e5\u5fae\u8c03\uff09\u3002", "result": "\u5728Unitree G1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u697c\u68af\u548c\u8fb9\u7f18\u4efb\u52a1\u4e2d\u6210\u529f\uff0c\u800c\u5355\u9636\u6bb5\u611f\u77e5\u7b56\u7565\u5931\u8d25\u3002", "conclusion": "\u65f6\u95f4\u5c3a\u5ea6\u7684\u67b6\u6784\u5206\u79bb\uff0c\u800c\u975e\u7f51\u7edc\u89c4\u6a21\u6216\u590d\u6742\u6027\uff0c\u662f\u5b9e\u73b0\u7a33\u5065\u611f\u77e5\u6761\u4ef6\u8fd0\u52a8\u7684\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2510.14612", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14612", "abs": "https://arxiv.org/abs/2510.14612", "authors": ["Gabriel Fischer Abati", "Jo\u00e3o Carlos Virgolino Soares", "Giulio Turrisi", "Victor Barasuol", "Claudio Semini"], "title": "Proprioceptive Image: An Image Representation of Proprioceptive Data from Quadruped Robots for Contact Estimation Learning", "comment": null, "summary": "This paper presents a novel approach for representing proprioceptive\ntime-series data from quadruped robots as structured two-dimensional images,\nenabling the use of convolutional neural networks for learning\nlocomotion-related tasks. The proposed method encodes temporal dynamics from\nmultiple proprioceptive signals, such as joint positions, IMU readings, and\nfoot velocities, while preserving the robot's morphological structure in the\nspatial arrangement of the image. This transformation captures inter-signal\ncorrelations and gait-dependent patterns, providing a richer feature space than\ndirect time-series processing. We apply this concept in the problem of contact\nestimation, a key capability for stable and adaptive locomotion on diverse\nterrains. Experimental evaluations on both real-world datasets and simulated\nenvironments show that our image-based representation consistently enhances\nprediction accuracy and generalization over conventional sequence-based models,\nunderscoring the potential of cross-modal encoding strategies for robotic state\nlearning. Our method achieves superior performance on the contact dataset,\nimproving contact state accuracy from 87.7% to 94.5% over the recently proposed\nMI-HGNN method, using a 15 times shorter window size.", "AI": {"tldr": "\u63d0\u51fa\u5c06\u56db\u8db3\u673a\u5668\u4eba\u672c\u4f53\u611f\u77e5\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7f16\u7801\u4e3a\u4e8c\u7ef4\u56fe\u50cf\u7684\u65b9\u6cd5\uff0c\u5229\u7528CNN\u5b66\u4e60\u8fd0\u52a8\u76f8\u5173\u4efb\u52a1\uff0c\u5728\u63a5\u89e6\u4f30\u8ba1\u95ee\u9898\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd", "motivation": "\u4f20\u7edf\u65f6\u95f4\u5e8f\u5217\u5904\u7406\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u591a\u4fe1\u53f7\u95f4\u7684\u76f8\u5173\u6027\u548c\u6b65\u6001\u4f9d\u8d56\u6a21\u5f0f\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u4fdd\u7559\u673a\u5668\u4eba\u5f62\u6001\u7ed3\u6784\u5e76\u5229\u7528CNN\u4f18\u52bf\u7684\u7279\u5f81\u8868\u793a\u65b9\u6cd5", "method": "\u5c06\u591a\u8def\u672c\u4f53\u611f\u77e5\u4fe1\u53f7\uff08\u5173\u8282\u4f4d\u7f6e\u3001IMU\u8bfb\u6570\u3001\u8db3\u7aef\u901f\u5ea6\uff09\u6309\u673a\u5668\u4eba\u5f62\u6001\u7ed3\u6784\u7a7a\u95f4\u6392\u5217\u7f16\u7801\u4e3a\u4e8c\u7ef4\u56fe\u50cf\uff0c\u4fdd\u6301\u65f6\u95f4\u52a8\u6001\u7279\u6027", "result": "\u5728\u771f\u5b9e\u548c\u4eff\u771f\u73af\u5883\u4e2d\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u4f20\u7edf\u5e8f\u5217\u6a21\u578b\u663e\u8457\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u63a5\u89e6\u72b6\u6001\u51c6\u786e\u7387\u4ece87.7%\u63d0\u5347\u81f394.5%\uff0c\u7a97\u53e3\u5927\u5c0f\u7f29\u77ed15\u500d", "conclusion": "\u8de8\u6a21\u6001\u7f16\u7801\u7b56\u7565\u5728\u673a\u5668\u4eba\u72b6\u6001\u5b66\u4e60\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u56fe\u50cf\u8868\u793a\u80fd\u6709\u6548\u6355\u6349\u4fe1\u53f7\u95f4\u76f8\u5173\u6027\u548c\u6b65\u6001\u6a21\u5f0f"}}
{"id": "2510.14265", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14265", "abs": "https://arxiv.org/abs/2510.14265", "authors": ["Xukai Wang", "Xuanbo Liu", "Mingrui Chen", "Haitian Zhong", "Xuanlin Yang", "Bohan Zeng", "Jinbo Hu", "Hao Liang", "Junbo Niu", "Xuchen Li", "Ruitao Wu", "Ruichuan An", "Yang Shi", "Liu Liu", "Xu-Yao Zhang", "Qiang Liu", "Zhouchen Lin", "Wentao Zhang", "Bin Dong"], "title": "MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning", "comment": "21 pages, 12 figures", "summary": "With the advancement of powerful large-scale reasoning models, effectively\nevaluating the reasoning capabilities of these models has become increasingly\nimportant. However, existing benchmarks designed to assess the reasoning\nabilities of large models tend to be limited in scope and lack the flexibility\nto adapt their difficulty according to the evolving reasoning capacities of the\nmodels. To address this, we propose MorphoBench, a benchmark that incorporates\nmultidisciplinary questions to evaluate the reasoning capabilities of large\nmodels and can adjust and update question difficulty based on the reasoning\nabilities of advanced models. Specifically, we curate the benchmark by\nselecting and collecting complex reasoning questions from existing benchmarks\nand sources such as Olympiad-level competitions. Additionally, MorphoBench\nadaptively modifies the analytical challenge of questions by leveraging key\nstatements generated during the model's reasoning process. Furthermore, it\nincludes questions generated using simulation software, enabling dynamic\nadjustment of benchmark difficulty with minimal resource consumption. We have\ngathered over 1,300 test questions and iteratively adjusted the difficulty of\nMorphoBench based on the reasoning capabilities of models such as o3 and GPT-5.\nMorphoBench enhances the comprehensiveness and validity of model reasoning\nevaluation, providing reliable guidance for improving both the reasoning\nabilities and scientific robustness of large models. The code has been released\nin https://github.com/OpenDCAI/MorphoBench.", "AI": {"tldr": "\u63d0\u51fa\u4e86MorphoBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u80fd\u591f\u6839\u636e\u6a21\u578b\u80fd\u529b\u52a8\u6001\u8c03\u6574\u95ee\u9898\u96be\u5ea6\uff0c\u5305\u542b1300\u591a\u4e2a\u591a\u5b66\u79d1\u6d4b\u8bd5\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u8303\u56f4\u6709\u9650\u4e14\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u65e0\u6cd5\u9002\u5e94\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u6f14\u8fdb\uff0c\u9700\u8981\u66f4\u5168\u9762\u6709\u6548\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4ece\u73b0\u6709\u57fa\u51c6\u548c\u5965\u8d5b\u7ea7\u7ade\u8d5b\u4e2d\u6536\u96c6\u590d\u6742\u63a8\u7406\u95ee\u9898\uff0c\u5229\u7528\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u5173\u952e\u9648\u8ff0\u81ea\u9002\u5e94\u4fee\u6539\u5206\u6790\u6311\u6218\uff0c\u5e76\u4f7f\u7528\u4eff\u771f\u8f6f\u4ef6\u751f\u6210\u53ef\u52a8\u6001\u8c03\u6574\u96be\u5ea6\u7684\u95ee\u9898\u3002", "result": "\u6536\u96c6\u4e861300\u591a\u4e2a\u6d4b\u8bd5\u95ee\u9898\uff0c\u57fa\u4e8eo3\u548cGPT-5\u7b49\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u8fed\u4ee3\u8c03\u6574\u4e86MorphoBench\u7684\u96be\u5ea6\u3002", "conclusion": "MorphoBench\u63d0\u9ad8\u4e86\u6a21\u578b\u63a8\u7406\u8bc4\u4f30\u7684\u5168\u9762\u6027\u548c\u6709\u6548\u6027\uff0c\u4e3a\u6539\u8fdb\u5927\u578b\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548c\u79d1\u5b66\u7a33\u5065\u6027\u63d0\u4f9b\u4e86\u53ef\u9760\u6307\u5bfc\u3002"}}
{"id": "2510.14959", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.14959", "abs": "https://arxiv.org/abs/2510.14959", "authors": ["Lizhi Yang", "Blake Werner", "Massimiliano de Sa Aaron D. Ames"], "title": "CBF-RL: Safety Filtering Reinforcement Learning in Training with Control Barrier Functions", "comment": "8 pages", "summary": "Reinforcement learning (RL), while powerful and expressive, can often\nprioritize performance at the expense of safety. Yet safety violations can lead\nto catastrophic outcomes in real-world deployments. Control Barrier Functions\n(CBFs) offer a principled method to enforce dynamic safety -- traditionally\ndeployed \\emph{online} via safety filters. While the result is safe behavior,\nthe fact that the RL policy does not have knowledge of the CBF can lead to\nconservative behaviors. This paper proposes CBF-RL, a framework for generating\nsafe behaviors with RL by enforcing CBFs \\emph{in training}. CBF-RL has two key\nattributes: (1) minimally modifying a nominal RL policy to encode safety\nconstraints via a CBF term, (2) and safety filtering of the policy rollouts in\ntraining. Theoretically, we prove that continuous-time safety filters can be\ndeployed via closed-form expressions on discrete-time roll-outs. Practically,\nwe demonstrate that CBF-RL internalizes the safety constraints in the learned\npolicy -- both enforcing safer actions and biasing towards safer rewards --\nenabling safe deployment without the need for an online safety filter. We\nvalidate our framework through ablation studies on navigation tasks and on the\nUnitree G1 humanoid robot, where CBF-RL enables safer exploration, faster\nconvergence, and robust performance under uncertainty, enabling the humanoid\nrobot to avoid obstacles and climb stairs safely in real-world settings without\na runtime safety filter.", "AI": {"tldr": "CBF-RL\u6846\u67b6\u901a\u8fc7\u5728\u8bad\u7ec3\u4e2d\u5f3a\u5236\u6267\u884c\u63a7\u5236\u5c4f\u969c\u51fd\u6570(CBFs)\uff0c\u5c06\u5b89\u5168\u6027\u7ea6\u675f\u5185\u5316\u5230\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u4e2d\uff0c\u65e0\u9700\u5728\u7ebf\u5b89\u5168\u8fc7\u6ee4\u5668\u5373\u53ef\u5b9e\u73b0\u5b89\u5168\u90e8\u7f72\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60(RL)\u5f80\u5f80\u4ee5\u727a\u7272\u5b89\u5168\u6027\u4e3a\u4ee3\u4ef7\u8ffd\u6c42\u6027\u80fd\uff0c\u800c\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u5b89\u5168\u8fdd\u89c4\u53ef\u80fd\u5bfc\u81f4\u707e\u96be\u6027\u540e\u679c\u3002\u867d\u7136\u63a7\u5236\u5c4f\u969c\u51fd\u6570(CBFs)\u63d0\u4f9b\u4e86\u5f3a\u5236\u6267\u884c\u52a8\u6001\u5b89\u5168\u6027\u7684\u539f\u5219\u6027\u65b9\u6cd5\uff0c\u4f46RL\u7b56\u7565\u4e0d\u4e86\u89e3CBF\u4f1a\u5bfc\u81f4\u4fdd\u5b88\u884c\u4e3a\u3002", "method": "\u63d0\u51faCBF-RL\u6846\u67b6\uff1a1)\u901a\u8fc7CBF\u9879\u6700\u5c0f\u5316\u4fee\u6539\u540d\u4e49RL\u7b56\u7565\u4ee5\u7f16\u7801\u5b89\u5168\u7ea6\u675f\uff1b2)\u5728\u8bad\u7ec3\u4e2d\u5bf9\u7b56\u7565rollouts\u8fdb\u884c\u5b89\u5168\u8fc7\u6ee4\u3002\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u8fde\u7eed\u65f6\u95f4\u5b89\u5168\u8fc7\u6ee4\u5668\u53ef\u4ee5\u901a\u8fc7\u95ed\u5f0f\u8868\u8fbe\u5f0f\u90e8\u7f72\u5728\u79bb\u6563\u65f6\u95f4rollouts\u4e0a\u3002", "result": "CBF-RL\u4f7f\u5b66\u4e60\u7b56\u7565\u5185\u5316\u4e86\u5b89\u5168\u7ea6\u675f\uff0c\u65e2\u5f3a\u5236\u6267\u884c\u66f4\u5b89\u5168\u7684\u52a8\u4f5c\uff0c\u53c8\u504f\u5411\u66f4\u5b89\u5168\u7684\u5956\u52b1\u3002\u5728\u5bfc\u822a\u4efb\u52a1\u548cUnitree G1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCBF-RL\u5b9e\u73b0\u4e86\u66f4\u5b89\u5168\u7684\u63a2\u7d22\u3001\u66f4\u5feb\u7684\u6536\u655b\u548c\u9c81\u68d2\u6027\u80fd\u3002", "conclusion": "CBF-RL\u6846\u67b6\u80fd\u591f\u5728\u6ca1\u6709\u8fd0\u884c\u65f6\u5b89\u5168\u8fc7\u6ee4\u5668\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u5b9e\u73b0\u5b89\u5168\u90e8\u7f72\uff0c\u4f7f\u4eba\u5f62\u673a\u5668\u4eba\u80fd\u591f\u5b89\u5168\u907f\u5f00\u969c\u788d\u7269\u548c\u722c\u697c\u68af\u3002"}}
{"id": "2510.14615", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14615", "abs": "https://arxiv.org/abs/2510.14615", "authors": ["Edward Sandra", "Lander Vanroye", "Dries Dirckx", "Ruben Cartuyvels", "Jan Swevers", "Wilm Decr\u00e9"], "title": "Accelerated Multi-Modal Motion Planning Using Context-Conditioned Diffusion Models", "comment": "This paper has been submitted and has not yet been peer reviewed or\n  accepted for publication", "summary": "Classical methods in robot motion planning, such as sampling-based and\noptimization-based methods, often struggle with scalability towards\nhigher-dimensional state spaces and complex environments. Diffusion models,\nknown for their capability to learn complex, high-dimensional and multi-modal\ndata distributions, provide a promising alternative when applied to motion\nplanning problems and have already shown interesting results. However, most of\nthe current approaches train their model for a single environment, limiting\ntheir generalization to environments not seen during training. The techniques\nthat do train a model for multiple environments rely on a specific camera to\nprovide the model with the necessary environmental information and therefore\nalways require that sensor. To effectively adapt to diverse scenarios without\nthe need for retraining, this research proposes Context-Aware Motion Planning\nDiffusion (CAMPD). CAMPD leverages a classifier-free denoising probabilistic\ndiffusion model, conditioned on sensor-agnostic contextual information. An\nattention mechanism, integrated in the well-known U-Net architecture,\nconditions the model on an arbitrary number of contextual parameters. CAMPD is\nevaluated on a 7-DoF robot manipulator and benchmarked against state-of-the-art\napproaches on real-world tasks, showing its ability to generalize to unseen\nenvironments and generate high-quality, multi-modal trajectories, at a fraction\nof the time required by existing methods.", "AI": {"tldr": "\u63d0\u51faCAMPD\u65b9\u6cd5\uff0c\u4f7f\u7528\u6269\u6563\u6a21\u578b\u8fdb\u884c\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\uff0c\u80fd\u591f\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u73af\u5883\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3", "motivation": "\u4f20\u7edf\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u5728\u9ad8\u7ef4\u72b6\u6001\u7a7a\u95f4\u548c\u590d\u6742\u73af\u5883\u4e2d\u6269\u5c55\u6027\u5dee\uff0c\u73b0\u6709\u6269\u6563\u6a21\u578b\u65b9\u6cd5\u5927\u591a\u9488\u5bf9\u5355\u4e00\u73af\u5883\u8bad\u7ec3\uff0c\u7f3a\u4e4f\u6cdb\u5316\u80fd\u529b", "method": "\u4f7f\u7528\u65e0\u5206\u7c7b\u5668\u53bb\u566a\u6982\u7387\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u6574\u5408\u4f20\u611f\u5668\u65e0\u5173\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u57fa\u4e8eU-Net\u67b6\u6784", "result": "\u57287\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u8f68\u8ff9\uff0c\u65f6\u95f4\u5927\u5e45\u51cf\u5c11", "conclusion": "CAMPD\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u9002\u5e94\u591a\u6837\u5316\u573a\u666f\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u5728\u6cdb\u5316\u6027\u548c\u6548\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02"}}
{"id": "2510.14301", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14301", "abs": "https://arxiv.org/abs/2510.14301", "authors": ["Bingjie Zhang", "Yibo Yang", "Renzhe", "Dandan Guo", "Jindong Gu", "Philip Torr", "Bernard Ghanem"], "title": "A Guardrail for Safety Preservation: When Safety-Sensitive Subspace Meets Harmful-Resistant Null-Space", "comment": null, "summary": "Large language models (LLMs) have achieved remarkable success in diverse\ntasks, yet their safety alignment remains fragile during adaptation. Even when\nfine-tuning on benign data or with low-rank adaptation, pre-trained safety\nbehaviors are easily degraded, leading to harmful responses in the fine-tuned\nmodels. To address this challenge, we propose GuardSpace, a guardrail framework\nfor preserving safety alignment throughout fine-tuning, composed of two key\ncomponents: a safety-sensitive subspace and a harmful-resistant null space.\nFirst, we explicitly decompose pre-trained weights into safety-relevant and\nsafety-irrelevant components using covariance-preconditioned singular value\ndecomposition, and initialize low-rank adapters from the safety-irrelevant\nones, while freezing safety-relevant components to preserve their associated\nsafety mechanism. Second, we construct a null space projector that restricts\nadapter updates from altering safe outputs on harmful prompts, thereby\nmaintaining the original refusal behavior. Experiments with various pre-trained\nmodels on multiple downstream tasks demonstrate that GuardSpace achieves\nsuperior performance over existing methods. Notably, for Llama-2-7B-Chat\nfine-tuned on GSM8K, GuardSpace outperforms the state-of-the-art method AsFT,\nreducing the average harmful score from 14.4% to 3.6%, while improving the\naccuracy from from 26.0% to 28.0%.", "AI": {"tldr": "GuardSpace\u662f\u4e00\u4e2a\u4fdd\u62a4\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u5b89\u5168\u5bf9\u9f50\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5b89\u5168\u654f\u611f\u5b50\u7a7a\u95f4\u548c\u6297\u6709\u5bb3\u7a7a\u7a7a\u95f4\u6765\u7ef4\u6301\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5b89\u5168\u884c\u4e3a\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u5bb9\u6613\u4e22\u5931\u9884\u8bad\u7ec3\u7684\u5b89\u5168\u5bf9\u9f50\u884c\u4e3a\uff0c\u5bfc\u81f4\u4ea7\u751f\u6709\u5bb3\u54cd\u5e94\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u4fdd\u62a4\u5b89\u5168\u5bf9\u9f50\u3002", "method": "\u4f7f\u7528\u534f\u65b9\u5dee\u9884\u6761\u4ef6\u5947\u5f02\u503c\u5206\u89e3\u5c06\u9884\u8bad\u7ec3\u6743\u91cd\u5206\u89e3\u4e3a\u5b89\u5168\u76f8\u5173\u548c\u5b89\u5168\u65e0\u5173\u7ec4\u4ef6\uff0c\u521d\u59cb\u5316\u4f4e\u79e9\u9002\u914d\u5668\u65f6\u51bb\u7ed3\u5b89\u5168\u76f8\u5173\u7ec4\u4ef6\uff0c\u5e76\u6784\u5efa\u7a7a\u7a7a\u95f4\u6295\u5f71\u5668\u9650\u5236\u9002\u914d\u5668\u66f4\u65b0\u5bf9\u6709\u5bb3\u63d0\u793a\u7684\u5b89\u5168\u8f93\u51fa\u3002", "result": "\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGuardSpace\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u5bf9\u4e8eLlama-2-7B-Chat\u5728GSM8K\u4e0a\u7684\u5fae\u8c03\uff0c\u6709\u5bb3\u5206\u6570\u4ece14.4%\u964d\u81f33.6%\uff0c\u51c6\u786e\u7387\u4ece26.0%\u63d0\u5347\u81f328.0%\u3002", "conclusion": "GuardSpace\u6846\u67b6\u80fd\u6709\u6548\u4fdd\u62a4\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u5b89\u5168\u5bf9\u9f50\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2510.14968", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.14968", "abs": "https://arxiv.org/abs/2510.14968", "authors": ["Mingxuan Yan", "Yuping Wang", "Zechun Liu", "Jiachen Li"], "title": "RDD: Retrieval-Based Demonstration Decomposer for Planner Alignment in Long-Horizon Tasks", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025); Project Website: rdd-neurips.github.io", "summary": "To tackle long-horizon tasks, recent hierarchical vision-language-action\n(VLAs) frameworks employ vision-language model (VLM)-based planners to\ndecompose complex manipulation tasks into simpler sub-tasks that low-level\nvisuomotor policies can easily handle. Typically, the VLM planner is finetuned\nto learn to decompose a target task. This finetuning requires target task\ndemonstrations segmented into sub-tasks by either human annotation or heuristic\nrules. However, the heuristic subtasks can deviate significantly from the\ntraining data of the visuomotor policy, which degrades task performance. To\naddress these issues, we propose a Retrieval-based Demonstration Decomposer\n(RDD) that automatically decomposes demonstrations into sub-tasks by aligning\nthe visual features of the decomposed sub-task intervals with those from the\ntraining data of the low-level visuomotor policies. Our method outperforms the\nstate-of-the-art sub-task decomposer on both simulation and real-world tasks,\ndemonstrating robustness across diverse settings. Code and more results are\navailable at rdd-neurips.github.io.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u68c0\u7d22\u7684\u6f14\u793a\u5206\u89e3\u5668(RDD)\uff0c\u901a\u8fc7\u5c06\u5206\u89e3\u7684\u5b50\u4efb\u52a1\u533a\u95f4\u4e0e\u4f4e\u7ea7\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u8bad\u7ec3\u6570\u636e\u7684\u89c6\u89c9\u7279\u5f81\u5bf9\u9f50\uff0c\u81ea\u52a8\u5c06\u6f14\u793a\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u542f\u53d1\u5f0f\u5b50\u4efb\u52a1\u4e0e\u8bad\u7ec3\u6570\u636e\u504f\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5206\u5c42VLA\u6846\u67b6\u4f7f\u7528VLM\u89c4\u5212\u5668\u5206\u89e3\u590d\u6742\u4efb\u52a1\uff0c\u4f46\u9700\u8981\u4eba\u5de5\u6807\u6ce8\u6216\u542f\u53d1\u5f0f\u89c4\u5219\u6765\u83b7\u53d6\u5b50\u4efb\u52a1\u5206\u89e3\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u5b50\u4efb\u52a1\u4e0e\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u8bad\u7ec3\u6570\u636e\u4e0d\u5339\u914d\uff0c\u4ece\u800c\u964d\u4f4e\u4efb\u52a1\u6027\u80fd\u3002", "method": "\u63d0\u51faRDD\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u7d22\u548c\u89c6\u89c9\u7279\u5f81\u5bf9\u9f50\uff0c\u81ea\u52a8\u5c06\u6f14\u793a\u5206\u89e3\u4e3a\u4e0e\u4f4e\u7ea7\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u8bad\u7ec3\u6570\u636e\u7279\u5f81\u4e00\u81f4\u5b50\u4efb\u52a1\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u6216\u542f\u53d1\u5f0f\u89c4\u5219\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u5b50\u4efb\u52a1\u5206\u89e3\u5668\uff0c\u5728\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002", "conclusion": "RDD\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5b50\u4efb\u52a1\u5206\u89e3\u4e0e\u4f4e\u7ea7\u7b56\u7565\u8bad\u7ec3\u6570\u636e\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u63d0\u5347\u5206\u5c42VLA\u6846\u67b6\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2510.14627", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14627", "abs": "https://arxiv.org/abs/2510.14627", "authors": ["Yao Zhong", "Hanzhi Chen", "Simon Schaefer", "Anran Zhang", "Stefan Leutenegger"], "title": "GOPLA: Generalizable Object Placement Learning via Synthetic Augmentation of Human Arrangement", "comment": null, "summary": "Robots are expected to serve as intelligent assistants, helping humans with\neveryday household organization. A central challenge in this setting is the\ntask of object placement, which requires reasoning about both semantic\npreferences (e.g., common-sense object relations) and geometric feasibility\n(e.g., collision avoidance). We present GOPLA, a hierarchical framework that\nlearns generalizable object placement from augmented human demonstrations. A\nmulti-modal large language model translates human instructions and visual\ninputs into structured plans that specify pairwise object relationships. These\nplans are then converted into 3D affordance maps with geometric common sense by\na spatial mapper, while a diffusion-based planner generates placement poses\nguided by test-time costs, considering multi-plan distributions and collision\navoidance. To overcome data scarcity, we introduce a scalable pipeline that\nexpands human placement demonstrations into diverse synthetic training data.\nExtensive experiments show that our approach improves placement success rates\nby 30.04 percentage points over the runner-up, evaluated on positioning\naccuracy and physical plausibility, demonstrating strong generalization across\na wide range of real-world robotic placement scenarios.", "AI": {"tldr": "GOPLA\u662f\u4e00\u4e2a\u5206\u5c42\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u7684\u4eba\u7c7b\u6f14\u793a\u5b66\u4e60\u53ef\u6cdb\u5316\u7684\u7269\u4f53\u653e\u7f6e\uff0c\u7ed3\u5408\u8bed\u4e49\u63a8\u7406\u548c\u51e0\u4f55\u53ef\u884c\u6027\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u653e\u7f6e\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u4f5c\u4e3a\u667a\u80fd\u52a9\u624b\u5728\u5bb6\u5ead\u73af\u5883\u4e2d\u8fdb\u884c\u7269\u4f53\u653e\u7f6e\u65f6\u7684\u6838\u5fc3\u6311\u6218\uff0c\u9700\u8981\u540c\u65f6\u8003\u8651\u8bed\u4e49\u504f\u597d\uff08\u5e38\u8bc6\u6027\u7269\u4f53\u5173\u7cfb\uff09\u548c\u51e0\u4f55\u53ef\u884c\u6027\uff08\u78b0\u649e\u907f\u514d\uff09\u3002", "method": "\u4f7f\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u4eba\u7c7b\u6307\u4ee4\u548c\u89c6\u89c9\u8f93\u5165\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u8ba1\u5212\uff0c\u7a7a\u95f4\u6620\u5c04\u5668\u751f\u62103D\u53ef\u64cd\u4f5c\u6027\u5730\u56fe\uff0c\u57fa\u4e8e\u6269\u6563\u7684\u89c4\u5212\u5668\u751f\u6210\u653e\u7f6e\u4f4d\u59ff\uff0c\u5e76\u901a\u8fc7\u53ef\u6269\u5c55\u7ba1\u9053\u6269\u5c55\u4eba\u7c7b\u6f14\u793a\u6570\u636e\u3002", "result": "\u5728\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u7269\u7406\u5408\u7406\u6027\u8bc4\u4f30\u4e2d\uff0c\u6bd4\u7b2c\u4e8c\u540d\u65b9\u6cd5\u63d0\u9ad8\u4e8630.04\u4e2a\u767e\u5206\u70b9\u7684\u653e\u7f6e\u6210\u529f\u7387\uff0c\u5728\u5e7f\u6cdb\u7684\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u653e\u7f6e\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "GOPLA\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u63a8\u7406\u548c\u51e0\u4f55\u7ea6\u675f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u7269\u4f53\u653e\u7f6e\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2510.14312", "categories": ["cs.AI", "cs.CL", "cs.CR", "I.2.7; I.2.11"], "pdf": "https://arxiv.org/pdf/2510.14312", "abs": "https://arxiv.org/abs/2510.14312", "authors": ["Mason Nakamura", "Abhinav Kumar", "Saaduddin Mahmud", "Sahar Abdelnabi", "Shlomo Zilberstein", "Eugene Bagdasarian"], "title": "Terrarium: Revisiting the Blackboard for Multi-Agent Safety, Privacy, and Security Studies", "comment": null, "summary": "A multi-agent system (MAS) powered by large language models (LLMs) can\nautomate tedious user tasks such as meeting scheduling that requires\ninter-agent collaboration. LLMs enable nuanced protocols that account for\nunstructured private data, user constraints, and preferences. However, this\ndesign introduces new risks, including misalignment and attacks by malicious\nparties that compromise agents or steal user data. In this paper, we propose\nthe Terrarium framework for fine-grained study on safety, privacy, and security\nin LLM-based MAS. We repurpose the blackboard design, an early approach in\nmulti-agent systems, to create a modular, configurable testbed for multi-agent\ncollaboration. We identify key attack vectors such as misalignment, malicious\nagents, compromised communication, and data poisoning. We implement three\ncollaborative MAS scenarios with four representative attacks to demonstrate the\nframework's flexibility. By providing tools to rapidly prototype, evaluate, and\niterate on defenses and designs, Terrarium aims to accelerate progress toward\ntrustworthy multi-agent systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86Terrarium\u6846\u67b6\uff0c\u7528\u4e8e\u7814\u7a76\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u3001\u9690\u79c1\u548c\u5b89\u5168\u95ee\u9898\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u6d4b\u8bd5\u5e8a\u8bc6\u522b\u5173\u952e\u653b\u51fb\u5411\u91cf\u5e76\u6f14\u793a\u9632\u5fa1\u65b9\u6848\u3002", "motivation": "LLM\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u867d\u7136\u80fd\u81ea\u52a8\u5316\u590d\u6742\u4efb\u52a1\uff0c\u4f46\u5f15\u5165\u4e86\u65b0\u7684\u98ce\u9669\uff0c\u5305\u62ec\u9519\u4f4d\u3001\u6076\u610f\u65b9\u653b\u51fb\u3001\u6570\u636e\u7a83\u53d6\u7b49\u5b89\u5168\u95ee\u9898\u3002", "method": "\u91cd\u65b0\u5229\u7528\u9ed1\u677f\u8bbe\u8ba1\u6784\u5efa\u6a21\u5757\u5316\u3001\u53ef\u914d\u7f6e\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6d4b\u8bd5\u5e8a\uff0c\u8bc6\u522b\u5173\u952e\u653b\u51fb\u5411\u91cf\u5e76\u5b9e\u73b0\u4e09\u79cd\u534f\u4f5c\u573a\u666f\u548c\u56db\u79cd\u4ee3\u8868\u6027\u653b\u51fb\u3002", "result": "\u5f00\u53d1\u4e86Terrarium\u6846\u67b6\uff0c\u80fd\u591f\u5feb\u901f\u539f\u578b\u5316\u3001\u8bc4\u4f30\u548c\u8fed\u4ee3\u9632\u5fa1\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u6846\u67b6\u7684\u7075\u6d3b\u6027\u3002", "conclusion": "Terrarium\u6846\u67b6\u65e8\u5728\u52a0\u901f\u53ef\u4fe1\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u8fdb\u5c55\uff0c\u4e3a\u5b89\u5168\u7814\u7a76\u63d0\u4f9b\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2510.14643", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14643", "abs": "https://arxiv.org/abs/2510.14643", "authors": ["Lara Bruderm\u00fcller", "Brandon Hung", "Xinghao Zhu", "Jiuguang Wang", "Nick Hawes", "Preston Culbertson", "Simon Le Cleac'h"], "title": "Generative Models From and For Sampling-Based MPC: A Bootstrapped Approach For Adaptive Contact-Rich Manipulation", "comment": "9 pages, 5 figures", "summary": "We present a generative predictive control (GPC) framework that amortizes\nsampling-based Model Predictive Control (SPC) by bootstrapping it with\nconditional flow-matching models trained on SPC control sequences collected in\nsimulation. Unlike prior work relying on iterative refinement or gradient-based\nsolvers, we show that meaningful proposal distributions can be learned directly\nfrom noisy SPC data, enabling more efficient and informed sampling during\nonline planning. We further demonstrate, for the first time, the application of\nthis approach to real-world contact-rich loco-manipulation with a quadruped\nrobot. Extensive experiments in simulation and on hardware show that our method\nimproves sample efficiency, reduces planning horizon requirements, and\ngeneralizes robustly across task variations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u4f7f\u7528\u5728\u4eff\u771f\u4e2d\u6536\u96c6\u7684\u91c7\u6837\u9884\u6d4b\u63a7\u5236\u5e8f\u5217\u8bad\u7ec3\u6761\u4ef6\u6d41\u5339\u914d\u6a21\u578b\uff0c\u6765\u644a\u9500\u91c7\u6837\u9884\u6d4b\u63a7\u5236\u7684\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u4f9d\u8d56\u8fed\u4ee3\u4f18\u5316\u6216\u57fa\u4e8e\u68af\u5ea6\u7684\u6c42\u89e3\u5668\uff0c\u800c\u672c\u6587\u65e8\u5728\u76f4\u63a5\u4ece\u566a\u58f0\u91c7\u6837\u9884\u6d4b\u63a7\u5236\u6570\u636e\u4e2d\u5b66\u4e60\u6709\u610f\u4e49\u7684\u63d0\u8bae\u5206\u5e03\uff0c\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u6548\u548c\u667a\u80fd\u7684\u5728\u7ebf\u89c4\u5212\u91c7\u6837\u3002", "method": "\u4f7f\u7528\u5728\u4eff\u771f\u4e2d\u6536\u96c6\u7684\u91c7\u6837\u9884\u6d4b\u63a7\u5236\u63a7\u5236\u5e8f\u5217\u8bad\u7ec3\u6761\u4ef6\u6d41\u5339\u914d\u6a21\u578b\uff0c\u4ee5\u644a\u9500\u91c7\u6837\u9884\u6d4b\u63a7\u5236\u7684\u6210\u672c\u3002\u8be5\u65b9\u6cd5\u4e0d\u4f9d\u8d56\u8fed\u4ee3\u4f18\u5316\u6216\u57fa\u4e8e\u68af\u5ea6\u7684\u6c42\u89e3\u5668\uff0c\u800c\u662f\u76f4\u63a5\u4ece\u566a\u58f0\u6570\u636e\u4e2d\u5b66\u4e60\u63d0\u8bae\u5206\u5e03\u3002", "result": "\u5728\u4eff\u771f\u548c\u786c\u4ef6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u91c7\u6837\u6548\u7387\uff0c\u51cf\u5c11\u4e86\u89c4\u5212\u89c6\u91ce\u8981\u6c42\uff0c\u5e76\u5728\u4efb\u52a1\u53d8\u5316\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u7684\u6cdb\u5316\u80fd\u529b\u3002\u9996\u6b21\u5c06\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8e\u771f\u5b9e\u4e16\u754c\u7684\u63a5\u89e6\u4e30\u5bcc\u7684\u56db\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u64cd\u4f5c\u4efb\u52a1\u3002", "conclusion": "\u751f\u6210\u9884\u6d4b\u63a7\u5236\u6846\u67b6\u901a\u8fc7\u644a\u9500\u91c7\u6837\u9884\u6d4b\u63a7\u5236\u7684\u6210\u672c\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u548c\u667a\u80fd\u7684\u5728\u7ebf\u89c4\u5212\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u7684\u590d\u6742\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u9c81\u68d2\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.14319", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14319", "abs": "https://arxiv.org/abs/2510.14319", "authors": ["Xu Shen", "Qi Zhang", "Song Wang", "Zhen Tan", "Xinyu Zhao", "Laura Yao", "Vaishnav Tadiparthi", "Hossein Nourkhiz Mahjoub", "Ehsan Moradi Pari", "Kwonjoon Lee", "Tianlong Chen"], "title": "Metacognitive Self-Correction for Multi-Agent System via Prototype-Guided Next-Execution Reconstruction", "comment": null, "summary": "Large Language Model based multi-agent systems (MAS) excel at collaborative\nproblem solving but remain brittle to cascading errors: a single faulty step\ncan propagate across agents and disrupt the trajectory. In this paper, we\npresent MASC, a metacognitive framework that endows MAS with real-time,\nunsupervised, step-level error detection and self-correction. MASC rethinks\ndetection as history-conditioned anomaly scoring via two complementary designs:\n(1) Next-Execution Reconstruction, which predicts the embedding of the next\nstep from the query and interaction history to capture causal consistency, and\n(2) Prototype-Guided Enhancement, which learns a prototype prior over\nnormal-step embeddings and uses it to stabilize reconstruction and anomaly\nscoring under sparse context (e.g., early steps). When an anomaly step is\nflagged, MASC triggers a correction agent to revise the acting agent's output\nbefore information flows downstream. On the Who&When benchmark, MASC\nconsistently outperforms all baselines, improving step-level error detection by\nup to 8.47% AUC-ROC ; When plugged into diverse MAS frameworks, it delivers\nconsistent end-to-end gains across architectures, confirming that our\nmetacognitive monitoring and targeted correction can mitigate error propagation\nwith minimal overhead.", "AI": {"tldr": "MASC\u662f\u4e00\u4e2a\u5143\u8ba4\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u65f6\u65e0\u76d1\u7763\u7684\u6b65\u9aa4\u7ea7\u9519\u8bef\u68c0\u6d4b\u548c\u81ea\u6211\u7ea0\u6b63\u6765\u589e\u5f3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\uff0c\u9632\u6b62\u9519\u8bef\u7ea7\u8054\u4f20\u64ad\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u534f\u4f5c\u89e3\u51b3\u95ee\u9898\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5bf9\u7ea7\u8054\u9519\u8bef\u5f88\u8106\u5f31\uff1a\u5355\u4e2a\u9519\u8bef\u6b65\u9aa4\u53ef\u80fd\u8de8\u667a\u80fd\u4f53\u4f20\u64ad\u5e76\u7834\u574f\u6574\u4e2a\u8f68\u8ff9\u3002", "method": "MASC\u91c7\u7528\u4e24\u79cd\u4e92\u8865\u8bbe\u8ba1\uff1a(1) \u4e0b\u4e00\u6267\u884c\u91cd\u6784\uff0c\u4ece\u67e5\u8be2\u548c\u4ea4\u4e92\u5386\u53f2\u9884\u6d4b\u4e0b\u4e00\u6b65\u7684\u5d4c\u5165\u4ee5\u6355\u6349\u56e0\u679c\u4e00\u81f4\u6027\uff1b(2) \u539f\u578b\u5f15\u5bfc\u589e\u5f3a\uff0c\u5b66\u4e60\u6b63\u5e38\u6b65\u9aa4\u5d4c\u5165\u7684\u539f\u578b\u5148\u9a8c\uff0c\u5728\u7a00\u758f\u4e0a\u4e0b\u6587\u4e0b\u7a33\u5b9a\u91cd\u6784\u548c\u5f02\u5e38\u8bc4\u5206\u3002", "result": "\u5728Who&When\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMASC\u59cb\u7ec8\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\uff0c\u6b65\u9aa4\u7ea7\u9519\u8bef\u68c0\u6d4b\u7684AUC-ROC\u63d0\u5347\u9ad8\u8fbe8.47%\uff1b\u5728\u4e0d\u540cMAS\u6846\u67b6\u4e2d\u90fd\u80fd\u5e26\u6765\u4e00\u81f4\u7684\u7aef\u5230\u7aef\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u5143\u8ba4\u77e5\u76d1\u63a7\u548c\u9488\u5bf9\u6027\u7ea0\u6b63\u80fd\u591f\u4ee5\u6700\u5c0f\u5f00\u9500\u51cf\u8f7b\u9519\u8bef\u4f20\u64ad\uff0c\u8bc1\u5b9e\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2510.14647", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14647", "abs": "https://arxiv.org/abs/2510.14647", "authors": ["Jialei Huang", "Yang Ye", "Yuanqing Gong", "Xuezhou Zhu", "Yang Gao", "Kaifeng Zhang"], "title": "Spatially anchored Tactile Awareness for Robust Dexterous Manipulation", "comment": "8 pages", "summary": "Dexterous manipulation requires precise geometric reasoning, yet existing\nvisuo-tactile learning methods struggle with sub-millimeter precision tasks\nthat are routine for traditional model-based approaches. We identify a key\nlimitation: while tactile sensors provide rich contact information, current\nlearning frameworks fail to effectively leverage both the perceptual richness\nof tactile signals and their spatial relationship with hand kinematics. We\nbelieve an ideal tactile representation should explicitly ground contact\nmeasurements in a stable reference frame while preserving detailed sensory\ninformation, enabling policies to not only detect contact occurrence but also\nprecisely infer object geometry in the hand's coordinate system. We introduce\nSaTA (Spatially-anchored Tactile Awareness for dexterous manipulation), an\nend-to-end policy framework that explicitly anchors tactile features to the\nhand's kinematic frame through forward kinematics, enabling accurate geometric\nreasoning without requiring object models or explicit pose estimation. Our key\ninsight is that spatially grounded tactile representations allow policies to\nnot only detect contact occurrence but also precisely infer object geometry in\nthe hand's coordinate system. We validate SaTA on challenging dexterous\nmanipulation tasks, including bimanual USB-C mating in free space, a task\ndemanding sub-millimeter alignment precision, as well as light bulb\ninstallation requiring precise thread engagement and rotational control, and\ncard sliding that demands delicate force modulation and angular precision.\nThese tasks represent significant challenges for learning-based methods due to\ntheir stringent precision requirements. Across multiple benchmarks, SaTA\nsignificantly outperforms strong visuo-tactile baselines, improving success\nrates by up to 30 percentage while reducing task completion times by 27\npercentage.", "AI": {"tldr": "\u63d0\u51faSaTA\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u89e6\u89c9\u7279\u5f81\u951a\u5b9a\u5230\u624b\u7684\u8fd0\u52a8\u5b66\u6846\u67b6\u4e2d\uff0c\u5b9e\u73b0\u4e9a\u6beb\u7c73\u7ea7\u7684\u7075\u5de7\u64cd\u4f5c\u7cbe\u5ea6\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u89c6\u89c9\u89e6\u89c9\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u89e6\u89c9\u5b66\u4e60\u65b9\u6cd5\u5728\u4e9a\u6beb\u7c73\u7ea7\u7cbe\u5ea6\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u65e0\u6cd5\u6709\u6548\u5229\u7528\u89e6\u89c9\u4fe1\u53f7\u7684\u611f\u77e5\u4e30\u5bcc\u6027\u548c\u7a7a\u95f4\u5173\u7cfb\u3002", "method": "SaTA\u6846\u67b6\u901a\u8fc7\u524d\u5411\u8fd0\u52a8\u5b66\u5c06\u89e6\u89c9\u7279\u5f81\u663e\u5f0f\u951a\u5b9a\u5230\u624b\u7684\u8fd0\u52a8\u5b66\u6846\u67b6\u4e2d\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u51e0\u4f55\u63a8\u7406\uff0c\u65e0\u9700\u7269\u4f53\u6a21\u578b\u6216\u663e\u5f0f\u59ff\u6001\u4f30\u8ba1\u3002", "result": "\u5728USB-C\u63d2\u63a5\u3001\u706f\u6ce1\u5b89\u88c5\u548c\u5361\u7247\u6ed1\u52a8\u7b49\u6311\u6218\u6027\u4efb\u52a1\u4e2d\uff0cSaTA\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6210\u529f\u7387\u63d0\u9ad830%\uff0c\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u51cf\u5c1127%\u3002", "conclusion": "\u7a7a\u95f4\u951a\u5b9a\u7684\u89e6\u89c9\u8868\u793a\u80fd\u591f\u6709\u6548\u63d0\u5347\u7075\u5de7\u64cd\u4f5c\u7684\u7cbe\u5ea6\uff0c\u4e3a\u5b66\u4e60\u578b\u65b9\u6cd5\u5728\u7cbe\u5bc6\u64cd\u4f5c\u4efb\u52a1\u4e2d\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2510.14359", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14359", "abs": "https://arxiv.org/abs/2510.14359", "authors": ["Zichen Wen", "Yiyu Wang", "Chenfei Liao", "Boxue Yang", "Junxian Li", "Weifeng Liu", "Haocong He", "Bolong Feng", "Xuyang Liu", "Yuanhuiyi Lyu", "Xu Zheng", "Xuming Hu", "Linfeng Zhang"], "title": "AI for Service: Proactive Assistance with AI Glasses", "comment": "24 pages, 5 figures, work in progress", "summary": "In an era where AI is evolving from a passive tool into an active and\nadaptive companion, we introduce AI for Service (AI4Service), a new paradigm\nthat enables proactive and real-time assistance in daily life. Existing AI\nservices remain largely reactive, responding only to explicit user commands. We\nargue that a truly intelligent and helpful assistant should be capable of\nanticipating user needs and taking actions proactively when appropriate. To\nrealize this vision, we propose Alpha-Service, a unified framework that\naddresses two fundamental challenges: Know When to intervene by detecting\nservice opportunities from egocentric video streams, and Know How to provide\nboth generalized and personalized services. Inspired by the von Neumann\ncomputer architecture and based on AI glasses, Alpha-Service consists of five\nkey components: an Input Unit for perception, a Central Processing Unit for\ntask scheduling, an Arithmetic Logic Unit for tool utilization, a Memory Unit\nfor long-term personalization, and an Output Unit for natural human\ninteraction. As an initial exploration, we implement Alpha-Service through a\nmulti-agent system deployed on AI glasses. Case studies, including a real-time\nBlackjack advisor, a museum tour guide, and a shopping fit assistant,\ndemonstrate its ability to seamlessly perceive the environment, infer user\nintent, and provide timely and useful assistance without explicit prompts.", "AI": {"tldr": "\u63d0\u51faAI4Service\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7Alpha-Service\u6846\u67b6\u5b9e\u73b0\u4e3b\u52a8\u5f0fAI\u670d\u52a1\uff0c\u89e3\u51b3\"\u4f55\u65f6\u4ecb\u5165\"\u548c\"\u5982\u4f55\u670d\u52a1\"\u4e24\u5927\u6311\u6218\uff0c\u57fa\u4e8eAI\u773c\u955c\u5b9e\u73b0\u73af\u5883\u611f\u77e5\u548c\u4e2a\u6027\u5316\u670d\u52a1\u3002", "motivation": "\u73b0\u6709AI\u670d\u52a1\u591a\u4e3a\u88ab\u52a8\u54cd\u5e94\uff0c\u9700\u8981\u7528\u6237\u660e\u786e\u6307\u4ee4\u3002\u4f5c\u8005\u8ba4\u4e3a\u771f\u6b63\u667a\u80fd\u7684\u52a9\u624b\u5e94\u80fd\u4e3b\u52a8\u9884\u6d4b\u7528\u6237\u9700\u6c42\u5e76\u5728\u9002\u5f53\u65f6\u673a\u91c7\u53d6\u884c\u52a8\u3002", "method": "\u63d0\u51faAlpha-Service\u7edf\u4e00\u6846\u67b6\uff0c\u53d7\u51af\u00b7\u8bfa\u4f9d\u66fc\u67b6\u6784\u542f\u53d1\uff0c\u5305\u542b\u8f93\u5165\u5355\u5143\u3001\u4e2d\u592e\u5904\u7406\u5355\u5143\u3001\u7b97\u672f\u903b\u8f91\u5355\u5143\u3001\u5185\u5b58\u5355\u5143\u548c\u8f93\u51fa\u5355\u5143\u4e94\u4e2a\u7ec4\u4ef6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728AI\u773c\u955c\u4e0a\u5b9e\u73b0\u3002", "result": "\u6848\u4f8b\u7814\u7a76\u5305\u62ec\u5b9e\u65f621\u70b9\u987e\u95ee\u3001\u535a\u7269\u9986\u5bfc\u6e38\u548c\u8d2d\u7269\u642d\u914d\u52a9\u624b\uff0c\u5c55\u793a\u4e86\u7cfb\u7edf\u80fd\u591f\u65e0\u7f1d\u611f\u77e5\u73af\u5883\u3001\u63a8\u65ad\u7528\u6237\u610f\u56fe\uff0c\u5e76\u5728\u65e0\u9700\u660e\u786e\u63d0\u793a\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u53ca\u65f6\u6709\u7528\u7684\u5e2e\u52a9\u3002", "conclusion": "AI4Service\u8303\u5f0f\u5c06AI\u4ece\u88ab\u52a8\u5de5\u5177\u8f6c\u53d8\u4e3a\u4e3b\u52a8\u4f34\u4fa3\uff0cAlpha-Service\u6846\u67b6\u4e3a\u5b9e\u73b0\u8fd9\u4e00\u613f\u666f\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2510.14677", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.14677", "abs": "https://arxiv.org/abs/2510.14677", "authors": ["Steffen Hagedorn", "Luka Donkov", "Aron Distelzweig", "Alexandru P. Condurache"], "title": "When Planners Meet Reality: How Learned, Reactive Traffic Agents Shift nuPlan Benchmarks", "comment": null, "summary": "Planner evaluation in closed-loop simulation often uses rule-based traffic\nagents, whose simplistic and passive behavior can hide planner deficiencies and\nbias rankings. Widely used IDM agents simply follow a lead vehicle and cannot\nreact to vehicles in adjacent lanes, hindering tests of complex interaction\ncapabilities. We address this issue by integrating the state-of-the-art learned\ntraffic agent model SMART into nuPlan. Thus, we are the first to evaluate\nplanners under more realistic conditions and quantify how conclusions shift\nwhen narrowing the sim-to-real gap. Our analysis covers 14 recent planners and\nestablished baselines and shows that IDM-based simulation overestimates\nplanning performance: nearly all scores deteriorate. In contrast, many planners\ninteract better than previously assumed and even improve in multi-lane,\ninteraction-heavy scenarios like lane changes or turns. Methods trained in\nclosed-loop demonstrate the best and most stable driving performance. However,\nwhen reaching their limits in augmented edge-case scenarios, all learned\nplanners degrade abruptly, whereas rule-based planners maintain reasonable\nbasic behavior. Based on our results, we suggest SMART-reactive simulation as a\nnew standard closed-loop benchmark in nuPlan and release the SMART agents as a\ndrop-in alternative to IDM at https://github.com/shgd95/InteractiveClosedLoop.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06\u5148\u8fdb\u7684\u667a\u80fd\u4ea4\u901a\u4ee3\u7406\u6a21\u578bSMART\u96c6\u6210\u5230nuPlan\u4e2d\uff0c\u9996\u6b21\u5728\u66f4\u771f\u5b9e\u6761\u4ef6\u4e0b\u8bc4\u4f30\u89c4\u5212\u5668\uff0c\u53d1\u73b0\u57fa\u4e8eIDM\u7684\u4eff\u771f\u9ad8\u4f30\u4e86\u89c4\u5212\u6027\u80fd\uff0c\u800cSMART\u53cd\u5e94\u5f0f\u4eff\u771f\u5e94\u6210\u4e3a\u65b0\u7684\u6807\u51c6\u95ed\u73af\u57fa\u51c6\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u4ea4\u901a\u4ee3\u7406\uff08\u5982IDM\uff09\u884c\u4e3a\u7b80\u5355\u88ab\u52a8\u7684\u95ee\u9898\uff0c\u8fd9\u4e9b\u4ee3\u7406\u65e0\u6cd5\u5bf9\u76f8\u90bb\u8f66\u9053\u8f66\u8f86\u505a\u51fa\u53cd\u5e94\uff0c\u4ece\u800c\u63a9\u76d6\u89c4\u5212\u5668\u7f3a\u9677\u5e76\u4ea7\u751f\u8bc4\u4f30\u504f\u5dee\u3002", "method": "\u5c06\u6700\u5148\u8fdb\u7684\u5b66\u4e60\u578b\u4ea4\u901a\u4ee3\u7406\u6a21\u578bSMART\u96c6\u6210\u5230nuPlan\u4eff\u771f\u5e73\u53f0\u4e2d\uff0c\u5728\u66f4\u771f\u5b9e\u7684\u4ea4\u4e92\u6761\u4ef6\u4e0b\u8bc4\u4f3014\u4e2a\u8fd1\u671f\u89c4\u5212\u5668\u548c\u57fa\u51c6\u65b9\u6cd5\u3002", "result": "IDM\u4eff\u771f\u9ad8\u4f30\u89c4\u5212\u6027\u80fd\uff1a\u51e0\u4e4e\u6240\u6709\u8bc4\u5206\u90fd\u4e0b\u964d\u3002\u8bb8\u591a\u89c4\u5212\u5668\u5728\u4ea4\u4e92\u80fd\u529b\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u9884\u671f\uff0c\u5728\u591a\u8f66\u9053\u3001\u4ea4\u4e92\u5bc6\u96c6\u573a\u666f\u4e2d\u751a\u81f3\u6709\u6240\u63d0\u5347\u3002\u95ed\u73af\u8bad\u7ec3\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\u4e14\u6700\u7a33\u5b9a\u3002", "conclusion": "\u5efa\u8bae\u5c06SMART\u53cd\u5e94\u5f0f\u4eff\u771f\u4f5c\u4e3anuPlan\u4e2d\u65b0\u7684\u6807\u51c6\u95ed\u73af\u57fa\u51c6\uff0c\u6240\u6709\u5b66\u4e60\u578b\u89c4\u5212\u5668\u5728\u8fb9\u7f18\u6848\u4f8b\u573a\u666f\u4e2d\u90fd\u4f1a\u7a81\u7136\u9000\u5316\uff0c\u800c\u57fa\u4e8e\u89c4\u5219\u7684\u89c4\u5212\u5668\u80fd\u4fdd\u6301\u5408\u7406\u7684\u57fa\u672c\u884c\u4e3a\u3002"}}
{"id": "2510.14387", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14387", "abs": "https://arxiv.org/abs/2510.14387", "authors": ["Yijie Hu", "Zihao Zhou", "Kaizhu Huang", "Xiaowei Huang", "Qiufeng Wang"], "title": "Can MLLMs Absorb Math Reasoning Abilities from LLMs as Free Lunch?", "comment": null, "summary": "Math reasoning has been one crucial ability of large language models (LLMs),\nwhere significant advancements have been achieved in recent years. However,\nmost efforts focus on LLMs by curating high-quality annotation data and\nintricate training (or inference) paradigms, while the math reasoning\nperformance of multi-modal LLMs (MLLMs) remains lagging behind. Since the MLLM\ntypically consists of an LLM and a vision block, we wonder: Can MLLMs directly\nabsorb math reasoning abilities from off-the-shelf math LLMs without tuning?\nRecent model-merging approaches may offer insights into this question. However,\nthey overlook the alignment between the MLLM and LLM, where we find that there\nis a large gap between their parameter spaces, resulting in lower performance.\nOur empirical evidence reveals two key factors behind this issue: the\nidentification of crucial reasoning-associated layers in the model and the\nmitigation of the gaps in parameter space. Based on the empirical insights, we\npropose IP-Merging that first identifies the reasoning-associated parameters in\nboth MLLM and Math LLM, then projects them into the subspace of MLLM, aiming to\nmaintain the alignment, and finally merges parameters in this subspace.\nIP-Merging is a tuning-free approach since parameters are directly adjusted.\nExtensive experiments demonstrate that our IP-Merging method can enhance the\nmath reasoning ability of MLLMs directly from Math LLMs without compromising\ntheir other capabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faIP-Merging\u65b9\u6cd5\uff0c\u65e0\u9700\u8c03\u4f18\u5373\u53ef\u5c06\u6570\u5b66\u63a8\u7406\u80fd\u529b\u4ece\u6570\u5b66LLM\u76f4\u63a5\u8fc1\u79fb\u5230\u591a\u6a21\u6001LLM\uff0c\u89e3\u51b3\u4e86\u53c2\u6570\u7a7a\u95f4\u4e0d\u5bf9\u9f50\u7684\u95ee\u9898\u3002", "motivation": "\u76ee\u524d\u591a\u6a21\u6001LLM\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u843d\u540e\u4e8e\u7eaf\u6587\u672cLLM\uff0c\u4f46\u76f4\u63a5\u4f7f\u7528\u6a21\u578b\u878d\u5408\u65b9\u6cd5\u4f1a\u56e0\u53c2\u6570\u7a7a\u95f4\u4e0d\u5bf9\u9f50\u800c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "IP-Merging\u65b9\u6cd5\uff1a\u8bc6\u522bMLLM\u548c\u6570\u5b66LLM\u4e2d\u7684\u63a8\u7406\u76f8\u5173\u53c2\u6570\uff0c\u5c06\u5176\u6295\u5f71\u5230MLLM\u5b50\u7a7a\u95f4\uff0c\u7136\u540e\u5728\u8be5\u5b50\u7a7a\u95f4\u5185\u5408\u5e76\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eIP-Merging\u80fd\u6709\u6548\u63d0\u5347MLLM\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u4e14\u4e0d\u635f\u5bb3\u5176\u4ed6\u80fd\u529b\u3002", "conclusion": "IP-Merging\u662f\u4e00\u79cd\u65e0\u9700\u8c03\u4f18\u7684\u65b9\u6cd5\uff0c\u80fd\u6210\u529f\u5c06\u6570\u5b66\u63a8\u7406\u80fd\u529b\u4ece\u6570\u5b66LLM\u8fc1\u79fb\u5230MLLM\uff0c\u89e3\u51b3\u4e86\u53c2\u6570\u7a7a\u95f4\u5bf9\u9f50\u95ee\u9898\u3002"}}
{"id": "2510.14768", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14768", "abs": "https://arxiv.org/abs/2510.14768", "authors": ["Fan Yang", "Zixuan Huang", "Abhinav Kumar", "Sergio Aguilera Marinovic", "Soshi Iba", "Rana Soltani Zarrin", "Dmitry Berenson"], "title": "Leveraging Neural Descriptor Fields for Learning Contact-Aware Dynamic Recovery", "comment": null, "summary": "Real-world dexterous manipulation often encounters unexpected errors and\ndisturbances, which can lead to catastrophic failures, such as dropping the\nmanipulated object. To address this challenge, we focus on the problem of\ncatching a falling object while it remains within grasping range and,\nimportantly, resetting the system to a configuration favorable for resuming the\nprimary manipulation task. We propose Contact-Aware Dynamic Recovery (CADRE), a\nreinforcement learning framework that incorporates a Neural Descriptor Field\n(NDF)-inspired module to extract implicit contact features. Compared to methods\nthat rely solely on object pose or point cloud input, NDFs can directly reason\nabout finger-object correspondence and adapt to different object geometries.\nOur experiments show that incorporating contact features improves training\nefficiency, enhances convergence performance for RL training, and ultimately\nleads to more successful recoveries. Additionally, we demonstrate that CADRE\ncan generalize zero-shot to unseen objects with different geometries.", "AI": {"tldr": "\u63d0\u51faCADRE\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u795e\u7ecf\u63cf\u8ff0\u573a\u63d0\u53d6\u63a5\u89e6\u7279\u5f81\u6765\u63d0\u5347\u7075\u5de7\u624b\u6293\u53d6\u6062\u590d\u80fd\u529b\uff0c\u80fd\u591f\u96f6\u6837\u672c\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u51e0\u4f55\u5f62\u72b6\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u7075\u5de7\u64cd\u4f5c\u7ecf\u5e38\u9047\u5230\u610f\u5916\u9519\u8bef\u548c\u5e72\u6270\uff0c\u53ef\u80fd\u5bfc\u81f4\u707e\u96be\u6027\u5931\u8d25\uff08\u5982\u6389\u843d\u7269\u4f53\uff09\u3002\u9700\u8981\u89e3\u51b3\u5728\u6293\u53d6\u8303\u56f4\u5185\u63a5\u4f4f\u4e0b\u843d\u7269\u4f53\u5e76\u91cd\u7f6e\u7cfb\u7edf\u4ee5\u6062\u590d\u4e3b\u8981\u64cd\u4f5c\u4efb\u52a1\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faContact-Aware Dynamic Recovery (CADRE)\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u6574\u5408\u795e\u7ecf\u63cf\u8ff0\u573a(NDF)\u6a21\u5757\u63d0\u53d6\u9690\u5f0f\u63a5\u89e6\u7279\u5f81\uff0c\u76f4\u63a5\u63a8\u7406\u624b\u6307-\u7269\u4f53\u5bf9\u5e94\u5173\u7cfb\u5e76\u9002\u5e94\u4e0d\u540c\u7269\u4f53\u51e0\u4f55\u5f62\u72b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6574\u5408\u63a5\u89e6\u7279\u5f81\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\uff0c\u589e\u5f3a\u4e86\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u6536\u655b\u6027\u80fd\uff0c\u6700\u7ec8\u5e26\u6765\u66f4\u6210\u529f\u7684\u6062\u590d\u3002CADRE\u80fd\u591f\u96f6\u6837\u672c\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u51e0\u4f55\u5f62\u72b6\u3002", "conclusion": "CADRE\u6846\u67b6\u901a\u8fc7\u795e\u7ecf\u63cf\u8ff0\u573a\u63d0\u53d6\u63a5\u89e6\u7279\u5f81\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7075\u5de7\u624b\u5728\u52a8\u6001\u6062\u590d\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.14388", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14388", "abs": "https://arxiv.org/abs/2510.14388", "authors": ["Zhe Wu", "Hongjin Lu", "Junliang Xing", "Changhao Zhang", "Yin Zhu", "Yuhao Yang", "Yuheng Jing", "Kai Li", "Kun Shao", "Jianye Hao", "Jun Wang", "Yuanchun Shi"], "title": "Hi-Agent: Hierarchical Vision-Language Agents for Mobile Device Control", "comment": null, "summary": "Building agents that autonomously operate mobile devices has attracted\nincreasing attention. While Vision-Language Models (VLMs) show promise, most\nexisting approaches rely on direct state-to-action mappings, which lack\nstructured reasoning and planning, and thus generalize poorly to novel tasks or\nunseen UI layouts. We introduce Hi-Agent, a trainable hierarchical\nvision-language agent for mobile control, featuring a high-level reasoning\nmodel and a low-level action model that are jointly optimized. For efficient\ntraining, we reformulate multi-step decision-making as a sequence of\nsingle-step subgoals and propose a foresight advantage function, which\nleverages execution feedback from the low-level model to guide high-level\noptimization. This design alleviates the path explosion issue encountered by\nGroup Relative Policy Optimization (GRPO) in long-horizon tasks and enables\nstable, critic-free joint training. Hi-Agent achieves a new State-Of-The-Art\n(SOTA) 87.9% task success rate on the Android-in-the-Wild (AitW) benchmark,\nsignificantly outperforming prior methods across three paradigms: prompt-based\n(AppAgent: 17.7%), supervised (Filtered BC: 54.5%), and reinforcement\nlearning-based (DigiRL: 71.9%). It also demonstrates competitive zero-shot\ngeneralization on the ScreenSpot-v2 benchmark. On the more challenging\nAndroidWorld benchmark, Hi-Agent also scales effectively with larger backbones,\nshowing strong adaptability in high-complexity mobile control scenarios.", "AI": {"tldr": "Hi-Agent\u662f\u4e00\u4e2a\u53ef\u8bad\u7ec3\u7684\u5206\u5c42\u89c6\u89c9\u8bed\u8a00\u4ee3\u7406\uff0c\u7528\u4e8e\u79fb\u52a8\u8bbe\u5907\u63a7\u5236\uff0c\u901a\u8fc7\u9ad8\u5c42\u63a8\u7406\u6a21\u578b\u548c\u4f4e\u5c42\u52a8\u4f5c\u6a21\u578b\u7684\u8054\u5408\u4f18\u5316\uff0c\u5728Android-in-the-Wild\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523087.9%\u7684\u4efb\u52a1\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u79fb\u52a8\u8bbe\u5907\u63a7\u5236\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u76f4\u63a5\u7684\u72b6\u6001-\u52a8\u4f5c\u6620\u5c04\uff0c\u7f3a\u4e4f\u7ed3\u6784\u5316\u63a8\u7406\u548c\u89c4\u5212\uff0c\u5bfc\u81f4\u5728\u65b0\u4efb\u52a1\u6216\u672a\u89c1UI\u5e03\u5c40\u4e0a\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u67b6\u6784\uff1a\u9ad8\u5c42\u63a8\u7406\u6a21\u578b\u548c\u4f4e\u5c42\u52a8\u4f5c\u6a21\u578b\u8054\u5408\u4f18\u5316\u3002\u5c06\u591a\u6b65\u51b3\u7b56\u91cd\u65b0\u8868\u8ff0\u4e3a\u5355\u6b65\u5b50\u76ee\u6807\u5e8f\u5217\uff0c\u5e76\u63d0\u51fa\u524d\u77bb\u4f18\u52bf\u51fd\u6570\uff0c\u5229\u7528\u4f4e\u5c42\u6a21\u578b\u7684\u6267\u884c\u53cd\u9988\u6307\u5bfc\u9ad8\u5c42\u4f18\u5316\uff0c\u7f13\u89e3GRPO\u5728\u957f\u89c6\u91ce\u4efb\u52a1\u4e2d\u7684\u8def\u5f84\u7206\u70b8\u95ee\u9898\u3002", "result": "\u5728Android-in-the-Wild\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA 87.9%\u4efb\u52a1\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u63d0\u793a\u578b(17.7%)\u3001\u76d1\u7763\u5b66\u4e60(54.5%)\u548c\u5f3a\u5316\u5b66\u4e60(71.9%)\u65b9\u6cd5\u3002\u5728ScreenSpot-v2\u4e0a\u5c55\u793a\u7ade\u4e89\u6027\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u5728AndroidWorld\u4e0a\u968f\u9aa8\u5e72\u7f51\u7edc\u589e\u5927\u800c\u6709\u6548\u6269\u5c55\u3002", "conclusion": "Hi-Agent\u7684\u5206\u5c42\u8bbe\u8ba1\u548c\u8054\u5408\u4f18\u5316\u65b9\u6cd5\u5728\u79fb\u52a8\u8bbe\u5907\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2510.14771", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14771", "abs": "https://arxiv.org/abs/2510.14771", "authors": ["Xu Chi", "Chao Zhang", "Yang Su", "Lingfeng Dou", "Fujia Yang", "Jiakuo Zhao", "Haoyu Zhou", "Xiaoyou Jia", "Yong Zhou", "Shan An"], "title": "Open TeleDex: A Hardware-Agnostic Teleoperation System for Imitation Learning based Dexterous Manipulation", "comment": "17 pages", "summary": "Accurate and high-fidelity demonstration data acquisition is a critical\nbottleneck for deploying robot Imitation Learning (IL) systems, particularly\nwhen dealing with heterogeneous robotic platforms. Existing teleoperation\nsystems often fail to guarantee high-precision data collection across diverse\ntypes of teleoperation devices. To address this, we developed Open TeleDex, a\nunified teleoperation framework engineered for demonstration data collection.\nOpen TeleDex specifically tackles the TripleAny challenge, seamlessly\nsupporting any robotic arm, any dexterous hand, and any external input device.\nFurthermore, we propose a novel hand pose retargeting algorithm that\nsignificantly boosts the interoperability of Open TeleDex, enabling robust and\naccurate compatibility with an even wider spectrum of heterogeneous master and\nslave equipment. Open TeleDex establishes a foundational, high-quality, and\npublicly available platform for accelerating both academic research and\nindustry development in complex robotic manipulation and IL.", "AI": {"tldr": "\u5f00\u53d1\u4e86Open TeleDex\u7edf\u4e00\u9065\u64cd\u4f5c\u6846\u67b6\uff0c\u89e3\u51b3\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u4e2d\u9ad8\u8d28\u91cf\u6f14\u793a\u6570\u636e\u83b7\u53d6\u7684\u74f6\u9888\u95ee\u9898\uff0c\u652f\u6301\u4efb\u4f55\u673a\u68b0\u81c2\u3001\u7075\u5de7\u624b\u548c\u8f93\u5165\u8bbe\u5907\u3002", "motivation": "\u5f02\u6784\u673a\u5668\u4eba\u5e73\u53f0\u7684\u9ad8\u7cbe\u5ea6\u6f14\u793a\u6570\u636e\u83b7\u53d6\u662f\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u7cfb\u7edf\u90e8\u7f72\u7684\u5173\u952e\u74f6\u9888\uff0c\u73b0\u6709\u9065\u64cd\u4f5c\u7cfb\u7edf\u96be\u4ee5\u4fdd\u8bc1\u8de8\u4e0d\u540c\u9065\u64cd\u4f5c\u8bbe\u5907\u7684\u9ad8\u7cbe\u5ea6\u6570\u636e\u6536\u96c6\u3002", "method": "\u5f00\u53d1Open TeleDex\u7edf\u4e00\u9065\u64cd\u4f5c\u6846\u67b6\uff0c\u63d0\u51fa\u65b0\u9896\u7684\u624b\u90e8\u59ff\u6001\u91cd\u5b9a\u5411\u7b97\u6cd5\uff0c\u63d0\u5347\u7cfb\u7edf\u4e92\u64cd\u4f5c\u6027\uff0c\u652f\u6301\u4efb\u4f55\u673a\u68b0\u81c2\u3001\u7075\u5de7\u624b\u548c\u5916\u90e8\u8f93\u5165\u8bbe\u5907\u3002", "result": "Open TeleDex\u5b9e\u73b0\u4e86\u5bf9\u5f02\u6784\u4e3b\u4ece\u8bbe\u5907\u7684\u9c81\u68d2\u548c\u51c6\u786e\u517c\u5bb9\uff0c\u4e3a\u590d\u6742\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u6a21\u4eff\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u516c\u5f00\u5e73\u53f0\u3002", "conclusion": "Open TeleDex\u4e3a\u52a0\u901f\u5b66\u672f\u7814\u7a76\u548c\u5de5\u4e1a\u53d1\u5c55\u5efa\u7acb\u4e86\u57fa\u7840\u6027\u3001\u9ad8\u8d28\u91cf\u3001\u516c\u5f00\u53ef\u7528\u7684\u5e73\u53f0\u3002"}}
{"id": "2510.14406", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14406", "abs": "https://arxiv.org/abs/2510.14406", "authors": ["Xikai Zhang", "Bo Wang", "Likang Xiao", "Yongzhi Li", "Quan Chen", "Wenju Wu", "Liu Liu"], "title": "IMAGINE: Integrating Multi-Agent System into One Model for Complex Reasoning and Planning", "comment": null, "summary": "Although large language models (LLMs) have made significant strides across\nvarious tasks, they still face significant challenges in complex reasoning and\nplanning. For example, even with carefully designed prompts and prior\ninformation explicitly provided, GPT-4o achieves only a 7% Final Pass Rate on\nthe TravelPlanner dataset in the sole-planning mode. Similarly, even in the\nthinking mode, Qwen3-8B-Instruct and DeepSeek-R1-671B, only achieve Final Pass\nRates of 5.9% and 40%, respectively. Although well-organized Multi-Agent\nSystems (MAS) can offer improved collective reasoning, they often suffer from\nhigh reasoning costs due to multi-round internal interactions, long\nper-response latency, and difficulties in end-to-end training. To address these\nchallenges, we propose a general and scalable framework called IMAGINE, short\nfor Integrating Multi-Agent System into One Model. This framework not only\nintegrates the reasoning and planning capabilities of MAS into a single,\ncompact model, but also significantly surpass the capabilities of the MAS\nthrough a simple end-to-end training. Through this pipeline, a single\nsmall-scale model is not only able to acquire the structured reasoning and\nplanning capabilities of a well-organized MAS but can also significantly\noutperform it. Experimental results demonstrate that, when using\nQwen3-8B-Instruct as the base model and training it with our method, the model\nachieves an 82.7% Final Pass Rate on the TravelPlanner benchmark, far exceeding\nthe 40% of DeepSeek-R1-671B, while maintaining a much smaller model size.", "AI": {"tldr": "\u63d0\u51fa\u4e86IMAGINE\u6846\u67b6\uff0c\u5c06\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u63a8\u7406\u89c4\u5212\u80fd\u529b\u96c6\u6210\u5230\u5355\u4e00\u7d27\u51d1\u6a21\u578b\u4e2d\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\u663e\u8457\u8d85\u8d8a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6027\u80fd\uff0c\u5728TravelPlanner\u57fa\u51c6\u4e0a\u8fbe\u523082.7%\u7684\u6700\u7ec8\u901a\u8fc7\u7387\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u548c\u89c4\u5212\u4efb\u52a1\u4e2d\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u867d\u7136\u80fd\u63d0\u4f9b\u6539\u8fdb\u7684\u96c6\u4f53\u63a8\u7406\uff0c\u4f46\u5b58\u5728\u63a8\u7406\u6210\u672c\u9ad8\u3001\u54cd\u5e94\u5ef6\u8fdf\u957f\u548c\u7aef\u5230\u7aef\u8bad\u7ec3\u56f0\u96be\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faIMAGINE\u6846\u67b6\uff0c\u5c06\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u63a8\u7406\u89c4\u5212\u80fd\u529b\u96c6\u6210\u5230\u5355\u4e00\u7d27\u51d1\u6a21\u578b\u4e2d\uff0c\u901a\u8fc7\u7b80\u5355\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u5b9e\u73b0\u80fd\u529b\u63d0\u5347\u3002", "result": "\u4f7f\u7528Qwen3-8B-Instruct\u4f5c\u4e3a\u57fa\u7840\u6a21\u578b\uff0c\u5728TravelPlanner\u57fa\u51c6\u4e0a\u8fbe\u523082.7%\u7684\u6700\u7ec8\u901a\u8fc7\u7387\uff0c\u8fdc\u8d85DeepSeek-R1-671B\u768440%\uff0c\u540c\u65f6\u4fdd\u6301\u66f4\u5c0f\u7684\u6a21\u578b\u89c4\u6a21\u3002", "conclusion": "IMAGINE\u6846\u67b6\u6210\u529f\u5c06\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u80fd\u529b\u96c6\u6210\u5230\u5355\u4e00\u6a21\u578b\u4e2d\uff0c\u4e0d\u4ec5\u83b7\u5f97\u4e86\u7ed3\u6784\u5316\u63a8\u7406\u548c\u89c4\u5212\u80fd\u529b\uff0c\u8fd8\u663e\u8457\u8d85\u8d8a\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u540c\u65f6\u89e3\u51b3\u4e86\u63a8\u7406\u6210\u672c\u548c\u8bad\u7ec3\u56f0\u96be\u7684\u95ee\u9898\u3002"}}
{"id": "2510.14783", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14783", "abs": "https://arxiv.org/abs/2510.14783", "authors": ["Aderik Verraest", "Stavrow Bahnam", "Robin Ferede", "Guido de Croon", "Christophe De Wagter"], "title": "SkyDreamer: Interpretable End-to-End Vision-Based Drone Racing with Model-Based Reinforcement Learning", "comment": null, "summary": "Autonomous drone racing (ADR) systems have recently achieved champion-level\nperformance, yet remain highly specific to drone racing. While end-to-end\nvision-based methods promise broader applicability, no system to date\nsimultaneously achieves full sim-to-real transfer, onboard execution, and\nchampion-level performance. In this work, we present SkyDreamer, to the best of\nour knowledge, the first end-to-end vision-based ADR policy that maps directly\nfrom pixel-level representations to motor commands. SkyDreamer builds on\ninformed Dreamer, a model-based reinforcement learning approach where the world\nmodel decodes to privileged information only available during training. By\nextending this concept to end-to-end vision-based ADR, the world model\neffectively functions as an implicit state and parameter estimator, greatly\nimproving interpretability. SkyDreamer runs fully onboard without external aid,\nresolves visual ambiguities by tracking progress using the state decoded from\nthe world model's hidden state, and requires no extrinsic camera calibration,\nenabling rapid deployment across different drones without retraining.\nReal-world experiments show that SkyDreamer achieves robust, high-speed flight,\nexecuting tight maneuvers such as an inverted loop, a split-S and a ladder,\nreaching speeds of up to 21 m/s and accelerations of up to 6 g. It further\ndemonstrates a non-trivial visual sim-to-real transfer by operating on\npoor-quality segmentation masks, and exhibits robustness to battery depletion\nby accurately estimating the maximum attainable motor RPM and adjusting its\nflight path in real-time. These results highlight SkyDreamer's adaptability to\nimportant aspects of the reality gap, bringing robustness while still achieving\nextremely high-speed, agile flight.", "AI": {"tldr": "SkyDreamer\u662f\u9996\u4e2a\u7aef\u5230\u7aef\u57fa\u4e8e\u89c6\u89c9\u7684\u81ea\u4e3b\u65e0\u4eba\u673a\u7ade\u901f\u7b56\u7565\uff0c\u76f4\u63a5\u4ece\u50cf\u7d20\u7ea7\u8868\u793a\u6620\u5c04\u5230\u7535\u673a\u6307\u4ee4\uff0c\u5b9e\u73b0\u4e86\u5b8c\u6574\u7684\u6a21\u62df\u5230\u73b0\u5b9e\u8fc1\u79fb\u3001\u673a\u8f7d\u6267\u884c\u548c\u51a0\u519b\u7ea7\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u81ea\u4e3b\u65e0\u4eba\u673a\u7ade\u901f\u7cfb\u7edf\u9ad8\u5ea6\u4e13\u4e1a\u5316\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u3002\u7aef\u5230\u7aef\u89c6\u89c9\u65b9\u6cd5\u867d\u5177\u6f5c\u529b\uff0c\u4f46\u5c1a\u65e0\u7cfb\u7edf\u80fd\u540c\u65f6\u5b9e\u73b0\u5b8c\u6574\u6a21\u62df\u5230\u73b0\u5b9e\u8fc1\u79fb\u3001\u673a\u8f7d\u6267\u884c\u548c\u51a0\u519b\u7ea7\u6027\u80fd\u3002", "method": "\u57fa\u4e8einformed Dreamer\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e16\u754c\u6a21\u578b\u89e3\u7801\u8bad\u7ec3\u671f\u95f4\u53ef\u7528\u7684\u7279\u6743\u4fe1\u606f\uff0c\u4f5c\u4e3a\u9690\u5f0f\u72b6\u6001\u548c\u53c2\u6570\u4f30\u8ba1\u5668\u3002\u7cfb\u7edf\u5b8c\u5168\u673a\u8f7d\u8fd0\u884c\uff0c\u65e0\u9700\u5916\u90e8\u8f85\u52a9\uff0c\u901a\u8fc7\u4e16\u754c\u6a21\u578b\u9690\u85cf\u72b6\u6001\u89e3\u7801\u7684\u72b6\u6001\u8ddf\u8e2a\u8fdb\u5ea6\uff0c\u65e0\u9700\u5916\u90e8\u76f8\u673a\u6807\u5b9a\u3002", "result": "\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u5b9e\u73b0\u9c81\u68d2\u9ad8\u901f\u98de\u884c\uff0c\u6267\u884c\u5012\u98de\u5faa\u73af\u3001\u5206S\u548c\u68af\u5b50\u7b49\u590d\u6742\u673a\u52a8\uff0c\u901f\u5ea6\u8fbe21m/s\uff0c\u52a0\u901f\u5ea6\u8fbe6g\u3002\u5728\u4f4e\u8d28\u91cf\u5206\u5272\u63a9\u7801\u4e0a\u5c55\u793a\u975e\u5e73\u51e1\u89c6\u89c9\u6a21\u62df\u5230\u73b0\u5b9e\u8fc1\u79fb\uff0c\u5e76\u80fd\u5b9e\u65f6\u4f30\u8ba1\u7535\u673a\u6700\u5927RPM\u5e76\u8c03\u6574\u98de\u884c\u8def\u5f84\u4ee5\u5e94\u5bf9\u7535\u6c60\u635f\u8017\u3002", "conclusion": "SkyDreamer\u80fd\u591f\u9002\u5e94\u73b0\u5b9e\u5dee\u8ddd\u7684\u91cd\u8981\u65b9\u9762\uff0c\u5728\u5b9e\u73b0\u6781\u9ad8\u901f\u654f\u6377\u98de\u884c\u7684\u540c\u65f6\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u5c55\u793a\u4e86\u7aef\u5230\u7aef\u89c6\u89c9\u81ea\u4e3b\u65e0\u4eba\u673a\u7ade\u901f\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2510.14412", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14412", "abs": "https://arxiv.org/abs/2510.14412", "authors": ["Claudia Grundke", "Gabriele R\u00f6ger"], "title": "Eliminating Negative Occurrences of Derived Predicates from PDDL Axioms", "comment": "Extended version of a paper of the same title presented at the joint\n  KR/ICAPS 2025 workshop \"KRPlan: Knowledge Representation Meets Automated\n  Planning\"", "summary": "Axioms are a feature of the Planning Domain Definition Language PDDL that can\nbe considered as a generalization of database query languages such as Datalog.\nThe PDDL standard restricts negative occurrences of predicates in axiom bodies\nto predicates that are directly set by actions and not derived by axioms. In\nthe literature, authors often deviate from this limitation and only require\nthat the set of axioms is stratifiable. Both variants can express exactly the\nsame queries as least fixed-point logic, indicating that negative occurrences\nof derived predicates can be eliminated. We present the corresponding\ntransformation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f6c\u6362\u65b9\u6cd5\uff0c\u7528\u4e8e\u6d88\u9664PDDL\u516c\u7406\u4e2d\u6d3e\u751f\u8c13\u8bcd\u7684\u8d1f\u51fa\u73b0\uff0c\u8bc1\u660e\u8fd9\u79cd\u9650\u5236\u5b9e\u9645\u4e0a\u4e0d\u4f1a\u51cf\u5c11\u8868\u8fbe\u80fd\u529b\u3002", "motivation": "PDDL\u6807\u51c6\u9650\u5236\u516c\u7406\u4f53\u4e2d\u8c13\u8bcd\u7684\u8d1f\u51fa\u73b0\u53ea\u80fd\u9488\u5bf9\u76f4\u63a5\u7531\u52a8\u4f5c\u8bbe\u7f6e\u7684\u8c13\u8bcd\uff0c\u800c\u975e\u7531\u516c\u7406\u6d3e\u751f\u7684\u8c13\u8bcd\u3002\u7136\u800c\u6587\u732e\u4e2d\u5e38\u504f\u79bb\u6b64\u9650\u5236\uff0c\u4ec5\u8981\u6c42\u516c\u7406\u96c6\u53ef\u5206\u5c42\u3002\u672c\u6587\u65e8\u5728\u8bc1\u660e\u8fd9\u4e24\u79cd\u53d8\u4f53\u5b9e\u9645\u4e0a\u8868\u8fbe\u80fd\u529b\u76f8\u540c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f6c\u6362\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d88\u9664\u516c\u7406\u4e2d\u6d3e\u751f\u8c13\u8bcd\u7684\u8d1f\u51fa\u73b0\uff0c\u5c06\u5305\u542b\u6b64\u7c7b\u8d1f\u51fa\u73b0\u7684\u516c\u7406\u96c6\u8f6c\u6362\u4e3a\u7b26\u5408PDDL\u6807\u51c6\u9650\u5236\u7684\u5f62\u5f0f\u3002", "result": "\u8bc1\u660e\u4e86\u5305\u542b\u6d3e\u751f\u8c13\u8bcd\u8d1f\u51fa\u73b0\u7684\u516c\u7406\u96c6\u4e0e\u7b26\u5408PDDL\u6807\u51c6\u9650\u5236\u7684\u516c\u7406\u96c6\u5728\u8868\u8fbe\u80fd\u529b\u4e0a\u662f\u7b49\u4ef7\u7684\uff0c\u4e24\u8005\u90fd\u80fd\u8868\u8fbe\u4e0e\u6700\u5c0f\u4e0d\u52a8\u70b9\u903b\u8f91\u76f8\u540c\u7684\u67e5\u8be2\u3002", "conclusion": "PDDL\u6807\u51c6\u5bf9\u516c\u7406\u4e2d\u6d3e\u751f\u8c13\u8bcd\u8d1f\u51fa\u73b0\u7684\u9650\u5236\u5b9e\u9645\u4e0a\u4e0d\u4f1a\u51cf\u5c11\u5176\u8868\u8fbe\u80fd\u529b\uff0c\u56e0\u4e3a\u53ef\u4ee5\u901a\u8fc7\u8f6c\u6362\u65b9\u6cd5\u6d88\u9664\u8fd9\u4e9b\u8d1f\u51fa\u73b0\uff0c\u4ece\u800c\u4e0e\u66f4\u5bbd\u677e\u7684\u5206\u5c42\u8981\u6c42\u8fbe\u5230\u76f8\u540c\u7684\u8868\u8fbe\u80fd\u529b\u3002"}}
{"id": "2510.14827", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14827", "abs": "https://arxiv.org/abs/2510.14827", "authors": ["Yufei Zhu", "Shih-Min Yang", "Andrey Rudenko", "Tomasz P. Kucner", "Achim J. Lilienthal", "Martin Magnusson"], "title": "Neural Implicit Flow Fields for Spatio-Temporal Motion Mapping", "comment": null, "summary": "Safe and efficient robot operation in complex human environments can benefit\nfrom good models of site-specific motion patterns. Maps of Dynamics (MoDs)\nprovide such models by encoding statistical motion patterns in a map, but\nexisting representations use discrete spatial sampling and typically require\ncostly offline construction. We propose a continuous spatio-temporal MoD\nrepresentation based on implicit neural functions that directly map coordinates\nto the parameters of a Semi-Wrapped Gaussian Mixture Model. This removes the\nneed for discretization and imputation for unevenly sampled regions, enabling\nsmooth generalization across both space and time. Evaluated on a large public\ndataset with long-term real-world people tracking data, our method achieves\nbetter accuracy of motion representation and smoother velocity distributions in\nsparse regions while still being computationally efficient, compared to\navailable baselines. The proposed approach demonstrates a powerful and\nefficient way of modeling complex human motion patterns.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9690\u5f0f\u795e\u7ecf\u51fd\u6570\u7684\u8fde\u7eed\u65f6\u7a7a\u52a8\u6001\u5730\u56fe\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u5efa\u6a21\u590d\u6742\u73af\u5883\u4e2d\u7684\u4eba\u7c7b\u8fd0\u52a8\u6a21\u5f0f\uff0c\u76f8\u6bd4\u73b0\u6709\u79bb\u6563\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u51c6\u786e\u6027\u548c\u5e73\u6ed1\u6027\u3002", "motivation": "\u5728\u590d\u6742\u4eba\u7c7b\u73af\u5883\u4e2d\u5b9e\u73b0\u5b89\u5168\u9ad8\u6548\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u9700\u8981\u826f\u597d\u7684\u7279\u5b9a\u573a\u666f\u8fd0\u52a8\u6a21\u5f0f\u6a21\u578b\u3002\u73b0\u6709\u52a8\u6001\u5730\u56fe\u4f7f\u7528\u79bb\u6563\u7a7a\u95f4\u91c7\u6837\uff0c\u901a\u5e38\u9700\u8981\u6602\u8d35\u7684\u79bb\u7ebf\u6784\u5efa\u8fc7\u7a0b\u3002", "method": "\u4f7f\u7528\u9690\u5f0f\u795e\u7ecf\u51fd\u6570\u6784\u5efa\u8fde\u7eed\u65f6\u7a7a\u52a8\u6001\u5730\u56fe\u8868\u793a\uff0c\u76f4\u63a5\u5c06\u5750\u6807\u6620\u5c04\u5230\u534a\u5305\u88f9\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u7684\u53c2\u6570\uff0c\u65e0\u9700\u79bb\u6563\u5316\u548c\u7a00\u758f\u533a\u57df\u63d2\u503c\u5904\u7406\u3002", "result": "\u5728\u5927\u578b\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u8fd0\u52a8\u8868\u793a\u51c6\u786e\u6027\u3001\u7a00\u758f\u533a\u57df\u901f\u5ea6\u5206\u5e03\u5e73\u6ed1\u6027\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5efa\u6a21\u590d\u6742\u4eba\u7c7b\u8fd0\u52a8\u6a21\u5f0f\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u800c\u9ad8\u6548\u7684\u9014\u5f84\uff0c\u5b9e\u73b0\u4e86\u8de8\u65f6\u7a7a\u7684\u5e73\u6ed1\u6cdb\u5316\u3002"}}
{"id": "2510.14512", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14512", "abs": "https://arxiv.org/abs/2510.14512", "authors": ["Haoyuan Li", "Mathias Funk", "Aaqib Saeed"], "title": "Helmsman: Autonomous Synthesis of Federated Learning Systems via Multi-Agent Collaboration", "comment": null, "summary": "Federated Learning (FL) offers a powerful paradigm for training models on\ndecentralized data, but its promise is often undermined by the immense\ncomplexity of designing and deploying robust systems. The need to select,\ncombine, and tune strategies for multifaceted challenges like data\nheterogeneity and system constraints has become a critical bottleneck,\nresulting in brittle, bespoke solutions. To address this, we introduce\nHelmsman, a novel multi-agent system that automates the end-to-end synthesis of\nfederated learning systems from high-level user specifications. It emulates a\nprincipled research and development workflow through three collaborative\nphases: (1) interactive human-in-the-loop planning to formulate a sound\nresearch plan, (2) modular code generation by supervised agent teams, and (3) a\nclosed-loop of autonomous evaluation and refinement in a sandboxed simulation\nenvironment. To facilitate rigorous evaluation, we also introduce\nAgentFL-Bench, a new benchmark comprising 16 diverse tasks designed to assess\nthe system-level generation capabilities of agentic systems in FL. Extensive\nexperiments demonstrate that our approach generates solutions competitive with,\nand often superior to, established hand-crafted baselines. Our work represents\na significant step towards the automated engineering of complex decentralized\nAI systems.", "AI": {"tldr": "Helmsman\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u6a21\u62df\u7814\u53d1\u5de5\u4f5c\u6d41\u7a0b\u81ea\u52a8\u5408\u6210\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\uff0c\u5305\u62ec\u4ea4\u4e92\u5f0f\u89c4\u5212\u3001\u6a21\u5757\u5316\u4ee3\u7801\u751f\u6210\u548c\u81ea\u4e3b\u8bc4\u4f30\u4f18\u5316\uff0c\u5728AgentFL-Bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u4eba\u5de5\u8bbe\u8ba1\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u8bbe\u8ba1\u590d\u6742\uff0c\u9700\u8981\u5904\u7406\u6570\u636e\u5f02\u6784\u6027\u548c\u7cfb\u7edf\u7ea6\u675f\u7b49\u591a\u65b9\u9762\u6311\u6218\uff0c\u5bfc\u81f4\u89e3\u51b3\u65b9\u6848\u8106\u5f31\u4e14\u5b9a\u5236\u5316\uff0c\u963b\u788d\u4e86\u5b9e\u9645\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u591a\u667a\u80fd\u4f53\u534f\u4f5c\uff1a1)\u4eba\u673a\u4ea4\u4e92\u89c4\u5212\u5236\u5b9a\u7814\u7a76\u8ba1\u5212\uff1b2)\u76d1\u7763\u667a\u80fd\u4f53\u56e2\u961f\u8fdb\u884c\u6a21\u5757\u5316\u4ee3\u7801\u751f\u6210\uff1b3)\u5728\u6c99\u76d2\u6a21\u62df\u73af\u5883\u4e2d\u8fdb\u884c\u81ea\u4e3b\u8bc4\u4f30\u548c\u4f18\u5316\u7684\u95ed\u73af\u6d41\u7a0b\u3002", "result": "\u5728\u5305\u542b16\u4e2a\u591a\u6837\u5316\u4efb\u52a1\u7684AgentFL-Bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHelmsman\u751f\u6210\u7684\u89e3\u51b3\u65b9\u6848\u4e0e\u624b\u5de5\u8bbe\u8ba1\u7684\u57fa\u7ebf\u65b9\u6cd5\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4ee3\u8868\u4e86\u5411\u590d\u6742\u53bb\u4e2d\u5fc3\u5316AI\u7cfb\u7edf\u81ea\u52a8\u5316\u5de5\u7a0b\u8fc8\u51fa\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2510.14830", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14830", "abs": "https://arxiv.org/abs/2510.14830", "authors": ["Kun Lei", "Huanyu Li", "Dongjie Yu", "Zhenyu Wei", "Lingxiao Guo", "Zhennan Jiang", "Ziyu Wang", "Shiyu Liang", "Huazhe Xu"], "title": "RL-100: Performant Robotic Manipulation with Real-World Reinforcement Learning", "comment": "https://lei-kun.github.io/RL-100/", "summary": "Real-world robotic manipulation in homes and factories demands reliability,\nefficiency, and robustness that approach or surpass skilled human operators. We\npresent RL-100, a real-world reinforcement learning training framework built on\ndiffusion visuomotor policies trained bu supervised learning. RL-100 introduces\na three-stage pipeline. First, imitation learning leverages human priors.\nSecond, iterative offline reinforcement learning uses an Offline Policy\nEvaluation procedure, abbreviated OPE, to gate PPO-style updates that are\napplied in the denoising process for conservative and reliable improvement.\nThird, online reinforcement learning eliminates residual failure modes. An\nadditional lightweight consistency distillation head compresses the multi-step\nsampling process in diffusion into a single-step policy, enabling\nhigh-frequency control with an order-of-magnitude reduction in latency while\npreserving task performance. The framework is task-, embodiment-, and\nrepresentation-agnostic and supports both 3D point clouds and 2D RGB inputs, a\nvariety of robot platforms, and both single-step and action-chunk policies. We\nevaluate RL-100 on seven real-robot tasks spanning dynamic rigid-body control,\nsuch as Push-T and Agile Bowling, fluids and granular pouring, deformable cloth\nfolding, precise dexterous unscrewing, and multi-stage orange juicing. RL-100\nattains 100\\% success across evaluated trials for a total of 900 out of 900\nepisodes, including up to 250 out of 250 consecutive trials on one task. The\nmethod achieves near-human teleoperation or better time efficiency and\ndemonstrates multi-hour robustness with uninterrupted operation lasting up to\ntwo hours.", "AI": {"tldr": "RL-100\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\u5b9e\u73b0\u673a\u5668\u4eba\u64cd\u4f5c\u7684100%\u6210\u529f\u7387\uff0c\u652f\u6301\u591a\u79cd\u8f93\u5165\u548c\u673a\u5668\u4eba\u5e73\u53f0\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u9700\u8981\u63a5\u8fd1\u6216\u8d85\u8fc7\u719f\u7ec3\u4eba\u7c7b\u64cd\u4f5c\u5458\u7684\u53ef\u9760\u6027\u3001\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "method": "\u4e09\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff1a1) \u6a21\u4eff\u5b66\u4e60\u5229\u7528\u4eba\u7c7b\u5148\u9a8c\uff1b2) \u8fed\u4ee3\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4f7f\u7528\u79bb\u7ebf\u7b56\u7565\u8bc4\u4f30\u6765\u7ea6\u675fPPO\u98ce\u683c\u66f4\u65b0\uff1b3) \u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u6d88\u9664\u5269\u4f59\u5931\u8d25\u6a21\u5f0f\u3002\u8fd8\u5305\u542b\u8f7b\u91cf\u7ea7\u4e00\u81f4\u6027\u84b8\u998f\u5934\u7528\u4e8e\u4f4e\u5ef6\u8fdf\u63a7\u5236\u3002", "result": "\u57287\u4e2a\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86100%\u6210\u529f\u7387\uff08900/900\u6b21\u8bd5\u9a8c\uff09\uff0c\u5305\u62ec\u5355\u4e2a\u4efb\u52a1\u8fde\u7eed250\u6b21\u6210\u529f\u3002\u8fbe\u5230\u63a5\u8fd1\u6216\u4f18\u4e8e\u4eba\u7c7b\u9065\u64cd\u4f5c\u7684\u65f6\u95f4\u6548\u7387\uff0c\u5e76\u80fd\u6301\u7eed\u8fd0\u884c\u957f\u8fbe2\u5c0f\u65f6\u3002", "conclusion": "RL-100\u6846\u67b6\u5728\u591a\u79cd\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u524d\u6240\u672a\u6709\u7684\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.14537", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14537", "abs": "https://arxiv.org/abs/2510.14537", "authors": ["Emanuele Antonioni", "Stefan Markovic", "Anirudha Shankar", "Jaime Bernardo", "Lovro Markovic", "Silvia Pareti", "Benedetto Proietti"], "title": "JSPLIT: A Taxonomy-based Solution for Prompt Bloating in Model Context Protocol", "comment": null, "summary": "AI systems are continually evolving and advancing, and user expectations are\nconcurrently increasing, with a growing demand for interactions that go beyond\nsimple text-based interaction with Large Language Models (LLMs). Today's\napplications often require LLMs to interact with external tools, marking a\nshift toward more complex agentic systems. To support this, standards such as\nthe Model Context Protocol (MCP) have emerged, enabling agents to access tools\nby including a specification of the capabilities of each tool within the\nprompt. Although this approach expands what agents can do, it also introduces a\ngrowing problem: prompt bloating. As the number of tools increases, the prompts\nbecome longer, leading to high prompt token costs, increased latency, and\nreduced task success resulting from the selection of tools irrelevant to the\nprompt. To address this issue, we introduce JSPLIT, a taxonomy-driven framework\ndesigned to help agents manage prompt size more effectively when using large\nsets of MCP tools. JSPLIT organizes the tools into a hierarchical taxonomy and\nuses the user's prompt to identify and include only the most relevant tools,\nbased on both the query and the taxonomy structure. In this paper, we describe\nthe design of the taxonomy, the tool selection algorithm, and the dataset used\nto evaluate JSPLIT. Our results show that JSPLIT significantly reduces prompt\nsize without significantly compromising the agent's ability to respond\neffectively. As the number of available tools for the agent grows\nsubstantially, JSPLIT even improves the tool selection accuracy of the agent,\neffectively reducing costs while simultaneously improving task success in\nhigh-complexity agent environments.", "AI": {"tldr": "JSPLIT\u662f\u4e00\u4e2a\u57fa\u4e8e\u5206\u7c7b\u5b66\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f7f\u7528MCP\u5de5\u5177\u65f6\u63d0\u793a\u8bcd\u81a8\u80c0\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u5c42\u5206\u7c7b\u548c\u5de5\u5177\u9009\u62e9\u7b97\u6cd5\u663e\u8457\u51cf\u5c11\u63d0\u793a\u8bcd\u5927\u5c0f\uff0c\u540c\u65f6\u63d0\u9ad8\u5de5\u5177\u9009\u62e9\u51c6\u786e\u6027\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u53d1\u5c55\uff0c\u7528\u6237\u671f\u671b\u589e\u52a0\uff0c\u9700\u8981LLM\u4e0e\u5916\u90e8\u5de5\u5177\u4ea4\u4e92\u3002MCP\u7b49\u6807\u51c6\u4f7f\u4ee3\u7406\u80fd\u8bbf\u95ee\u5de5\u5177\uff0c\u4f46\u968f\u7740\u5de5\u5177\u6570\u91cf\u589e\u52a0\uff0c\u63d0\u793a\u8bcd\u53d8\u5f97\u5197\u957f\uff0c\u5bfc\u81f4\u9ad8\u4ee4\u724c\u6210\u672c\u3001\u5ef6\u8fdf\u589e\u52a0\u548c\u4efb\u52a1\u6210\u529f\u7387\u964d\u4f4e\u3002", "method": "JSPLIT\u5c06\u5de5\u5177\u7ec4\u7ec7\u6210\u5c42\u6b21\u5316\u5206\u7c7b\u5b66\uff0c\u4f7f\u7528\u7528\u6237\u63d0\u793a\u57fa\u4e8e\u67e5\u8be2\u548c\u5206\u7c7b\u7ed3\u6784\u8bc6\u522b\u5e76\u4ec5\u5305\u542b\u6700\u76f8\u5173\u5de5\u5177\uff0c\u5305\u62ec\u5206\u7c7b\u8bbe\u8ba1\u3001\u5de5\u5177\u9009\u62e9\u7b97\u6cd5\u548c\u8bc4\u4f30\u6570\u636e\u96c6\u3002", "result": "JSPLIT\u663e\u8457\u51cf\u5c11\u4e86\u63d0\u793a\u8bcd\u5927\u5c0f\uff0c\u540c\u65f6\u4e0d\u663e\u8457\u5f71\u54cd\u4ee3\u7406\u7684\u6709\u6548\u54cd\u5e94\u80fd\u529b\u3002\u5f53\u53ef\u7528\u5de5\u5177\u6570\u91cf\u5927\u5e45\u589e\u52a0\u65f6\uff0cJSPLIT\u751a\u81f3\u63d0\u9ad8\u4e86\u4ee3\u7406\u7684\u5de5\u5177\u9009\u62e9\u51c6\u786e\u6027\uff0c\u6709\u6548\u964d\u4f4e\u6210\u672c\u5e76\u63d0\u9ad8\u9ad8\u590d\u6742\u5ea6\u73af\u5883\u4e2d\u7684\u4efb\u52a1\u6210\u529f\u7387\u3002", "conclusion": "JSPLIT\u6846\u67b6\u901a\u8fc7\u5206\u7c7b\u5b66\u9a71\u52a8\u7684\u65b9\u6cd5\u6709\u6548\u7ba1\u7406\u63d0\u793a\u8bcd\u5927\u5c0f\uff0c\u5728\u5de5\u5177\u6570\u91cf\u589e\u52a0\u65f6\u65e2\u80fd\u964d\u4f4e\u6210\u672c\u53c8\u80fd\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\uff0c\u9002\u7528\u4e8e\u9ad8\u590d\u6742\u5ea6\u4ee3\u7406\u73af\u5883\u3002"}}
{"id": "2510.14849", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.14849", "abs": "https://arxiv.org/abs/2510.14849", "authors": ["Marcello Sorge", "Nicola Cigarini", "Riccardo Lorigiola", "Giulia Michieletto", "Andrea Masiero", "Angelo Cenedese", "Alberto Guarnieri"], "title": "Multi Agent Switching Mode Controller for Sound Source localization", "comment": null, "summary": "Source seeking is an important topic in robotic research, especially\nconsidering sound-based sensors since they allow the agents to locate a target\neven in critical conditions where it is not possible to establish a direct line\nof sight. In this work, we design a multi- agent switching mode control\nstrategy for acoustic-based target localization. Two scenarios are considered:\nsingle source localization, in which the agents are driven maintaining a rigid\nformation towards the target, and multi-source scenario, in which each agent\nsearches for the targets independently from the others.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u58f0\u5b66\u7684\u591a\u667a\u80fd\u4f53\u5207\u6362\u6a21\u5f0f\u63a7\u5236\u7b56\u7565\uff0c\u7528\u4e8e\u76ee\u6807\u5b9a\u4f4d\uff0c\u5305\u62ec\u5355\u6e90\u5b9a\u4f4d\u548c\u591a\u6e90\u5b9a\u4f4d\u4e24\u79cd\u573a\u666f\u3002", "motivation": "\u58f0\u5b66\u4f20\u611f\u5668\u5141\u8bb8\u667a\u80fd\u4f53\u5728\u65e0\u6cd5\u5efa\u7acb\u76f4\u63a5\u89c6\u7ebf\u7684\u60c5\u51b5\u4e0b\u5b9a\u4f4d\u76ee\u6807\uff0c\u8fd9\u5728\u673a\u5668\u4eba\u7814\u7a76\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u8bbe\u8ba1\u4e86\u591a\u667a\u80fd\u4f53\u5207\u6362\u6a21\u5f0f\u63a7\u5236\u7b56\u7565\uff0c\u5728\u5355\u6e90\u5b9a\u4f4d\u4e2d\u667a\u80fd\u4f53\u4fdd\u6301\u521a\u6027\u7f16\u961f\u5411\u76ee\u6807\u79fb\u52a8\uff0c\u5728\u591a\u6e90\u573a\u666f\u4e2d\u6bcf\u4e2a\u667a\u80fd\u4f53\u72ec\u7acb\u641c\u7d22\u76ee\u6807\u3002", "result": "\u63d0\u51fa\u4e86\u9002\u7528\u4e8e\u4e0d\u540c\u573a\u666f\u7684\u63a7\u5236\u7b56\u7565\u6846\u67b6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u58f0\u5b66\u76ee\u6807\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u591a\u667a\u80fd\u4f53\u63a7\u5236\u65b9\u6cd5\uff0c\u80fd\u591f\u9002\u5e94\u5355\u6e90\u548c\u591a\u6e90\u7684\u4e0d\u540c\u9700\u6c42\u3002"}}
{"id": "2510.14538", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14538", "abs": "https://arxiv.org/abs/2510.14538", "authors": ["Emanuele Marconato", "Samuele Bortolotti", "Emile van Krieken", "Paolo Morettin", "Elena Umili", "Antonio Vergari", "Efthymia Tsamoura", "Andrea Passerini", "Stefano Teso"], "title": "Symbol Grounding in Neuro-Symbolic AI: A Gentle Introduction to Reasoning Shortcuts", "comment": null, "summary": "Neuro-symbolic (NeSy) AI aims to develop deep neural networks whose\npredictions comply with prior knowledge encoding, e.g. safety or structural\nconstraints. As such, it represents one of the most promising avenues for\nreliable and trustworthy AI. The core idea behind NeSy AI is to combine neural\nand symbolic steps: neural networks are typically responsible for mapping\nlow-level inputs into high-level symbolic concepts, while symbolic reasoning\ninfers predictions compatible with the extracted concepts and the prior\nknowledge. Despite their promise, it was recently shown that - whenever the\nconcepts are not supervised directly - NeSy models can be affected by Reasoning\nShortcuts (RSs). That is, they can achieve high label accuracy by grounding the\nconcepts incorrectly. RSs can compromise the interpretability of the model's\nexplanations, performance in out-of-distribution scenarios, and therefore\nreliability. At the same time, RSs are difficult to detect and prevent unless\nconcept supervision is available, which is typically not the case. However, the\nliterature on RSs is scattered, making it difficult for researchers and\npractitioners to understand and tackle this challenging problem. This overview\naddresses this issue by providing a gentle introduction to RSs, discussing\ntheir causes and consequences in intuitive terms. It also reviews and\nelucidates existing theoretical characterizations of this phenomenon. Finally,\nit details methods for dealing with RSs, including mitigation and awareness\nstrategies, and maps their benefits and limitations. By reformulating advanced\nmaterial in a digestible form, this overview aims to provide a unifying\nperspective on RSs to lower the bar to entry for tackling them. Ultimately, we\nhope this overview contributes to the development of reliable NeSy and\ntrustworthy AI models.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u795e\u7ecf\u7b26\u53f7AI\u4e2d\u7684\u63a8\u7406\u6377\u5f84\u95ee\u9898\uff0c\u8ba8\u8bba\u4e86\u5176\u6210\u56e0\u3001\u540e\u679c\u53ca\u5e94\u5bf9\u65b9\u6cd5\uff0c\u65e8\u5728\u4e3a\u5f00\u53d1\u53ef\u9760\u7684\u795e\u7ecf\u7b26\u53f7AI\u6a21\u578b\u63d0\u4f9b\u7edf\u4e00\u89c6\u89d2\u3002", "motivation": "\u795e\u7ecf\u7b26\u53f7AI\u6a21\u578b\u5728\u6982\u5ff5\u672a\u76f4\u63a5\u76d1\u7763\u65f6\u5bb9\u6613\u51fa\u73b0\u63a8\u7406\u6377\u5f84\uff0c\u8fd9\u4f1a\u635f\u5bb3\u6a21\u578b\u89e3\u91ca\u6027\u3001\u5206\u5e03\u5916\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002\u73b0\u6709\u6587\u732e\u5206\u6563\uff0c\u9700\u8981\u7cfb\u7edf\u68b3\u7406\u6765\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u7406\u89e3\u548c\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u76f4\u89c2\u672f\u8bed\u4ecb\u7ecd\u63a8\u7406\u6377\u5f84\u7684\u6210\u56e0\u548c\u540e\u679c\uff0c\u56de\u987e\u73b0\u6709\u7406\u8bba\u7279\u5f81\uff0c\u8be6\u7ec6\u5206\u6790\u5e94\u5bf9\u63a8\u7406\u6377\u5f84\u7684\u65b9\u6cd5\uff08\u5305\u62ec\u7f13\u89e3\u548c\u610f\u8bc6\u7b56\u7565\uff09\uff0c\u5e76\u8bc4\u4f30\u5404\u79cd\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\u3002", "result": "\u63d0\u4f9b\u4e86\u63a8\u7406\u6377\u5f84\u7684\u7edf\u4e00\u89c6\u89d2\uff0c\u7cfb\u7edf\u68b3\u7406\u4e86\u76f8\u5173\u7406\u8bba\u548c\u65b9\u6cd5\uff0c\u4f7f\u8fd9\u4e00\u590d\u6742\u95ee\u9898\u66f4\u6613\u7406\u89e3\uff0c\u4e3a\u5f00\u53d1\u53ef\u9760\u7684\u795e\u7ecf\u7b26\u53f7AI\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u91cd\u65b0\u8868\u8ff0\u9ad8\u7ea7\u6750\u6599\u4f7f\u5176\u6613\u4e8e\u6d88\u5316\uff0c\u964d\u4f4e\u4e86\u5904\u7406\u63a8\u7406\u6377\u5f84\u95ee\u9898\u7684\u95e8\u69db\uff0c\u6709\u671b\u4fc3\u8fdb\u53ef\u9760\u795e\u7ecf\u7b26\u53f7AI\u548c\u53ef\u4fe1AI\u6a21\u578b\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.14851", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.14851", "abs": "https://arxiv.org/abs/2510.14851", "authors": ["Jakob Bichler", "Andreu Matoses Gimenez", "Javier Alonso-Mora"], "title": "SADCHER: Scheduling using Attention-based Dynamic Coalitions of Heterogeneous Robots in Real-Time", "comment": "7 pages, 5 figures. 2025 IEEE Int. Symposium on Multi-Robot and\n  Multi-Agent Systems (MRS 2025). Website and Code:\n  https://autonomousrobots.nl/paper_websites/sadcher_MRTA/", "summary": "We present Sadcher, a real-time task assignment framework for heterogeneous\nmulti-robot teams that incorporates dynamic coalition formation and task\nprecedence constraints. Sadcher is trained through Imitation Learning and\ncombines graph attention and transformers to predict assignment rewards between\nrobots and tasks. Based on the predicted rewards, a relaxed bipartite matching\nstep generates high-quality schedules with feasibility guarantees. We\nexplicitly model robot and task positions, task durations, and robots'\nremaining processing times, enabling advanced temporal and spatial reasoning\nand generalization to environments with different spatiotemporal distributions\ncompared to training. Trained on optimally solved small-scale instances, our\nmethod can scale to larger task sets and team sizes. Sadcher outperforms other\nlearning-based and heuristic baselines on randomized, unseen problems for small\nand medium-sized teams with computation times suitable for real-time operation.\nWe also explore sampling-based variants and evaluate scalability across robot\nand task counts. In addition, we release our dataset of 250,000 optimal\nschedules: https://autonomousrobots.nl/paper_websites/sadcher_MRTA/", "AI": {"tldr": "Sadcher\u662f\u4e00\u4e2a\u7528\u4e8e\u5f02\u6784\u591a\u673a\u5668\u4eba\u56e2\u961f\u7684\u5b9e\u65f6\u4efb\u52a1\u5206\u914d\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u52a8\u6001\u8054\u76df\u5f62\u6210\u548c\u4efb\u52a1\u4f18\u5148\u7ea7\u7ea6\u675f\uff0c\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u8bad\u7ec3\uff0c\u4f7f\u7528\u56fe\u6ce8\u610f\u529b\u548c\u53d8\u6362\u5668\u9884\u6d4b\u673a\u5668\u4eba-\u4efb\u52a1\u5206\u914d\u5956\u52b1\uff0c\u80fd\u591f\u6269\u5c55\u5230\u66f4\u5927\u89c4\u6a21\u7684\u4efb\u52a1\u96c6\u548c\u56e2\u961f\u89c4\u6a21\u3002", "motivation": "\u89e3\u51b3\u5f02\u6784\u591a\u673a\u5668\u4eba\u56e2\u961f\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8fdb\u884c\u5b9e\u65f6\u4efb\u52a1\u5206\u914d\u7684\u6311\u6218\uff0c\u9700\u8981\u8003\u8651\u4efb\u52a1\u4f18\u5148\u7ea7\u7ea6\u675f\u3001\u65f6\u7a7a\u5206\u5e03\u53d8\u5316\u4ee5\u53ca\u6269\u5c55\u5230\u66f4\u5927\u89c4\u6a21\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u8bad\u7ec3\uff0c\u7ed3\u5408\u56fe\u6ce8\u610f\u529b\u548c\u53d8\u6362\u5668\u9884\u6d4b\u673a\u5668\u4eba\u4e0e\u4efb\u52a1\u4e4b\u95f4\u7684\u5206\u914d\u5956\u52b1\uff0c\u4f7f\u7528\u677e\u5f1b\u4e8c\u5206\u5339\u914d\u751f\u6210\u9ad8\u8d28\u91cf\u8c03\u5ea6\u65b9\u6848\uff0c\u663e\u5f0f\u5efa\u6a21\u673a\u5668\u4eba\u548c\u4efb\u52a1\u4f4d\u7f6e\u3001\u4efb\u52a1\u6301\u7eed\u65f6\u95f4\u53ca\u673a\u5668\u4eba\u5269\u4f59\u5904\u7406\u65f6\u95f4\u3002", "result": "\u5728\u968f\u673a\u672a\u89c1\u95ee\u9898\u4e0a\u4f18\u4e8e\u5176\u4ed6\u57fa\u4e8e\u5b66\u4e60\u548c\u542f\u53d1\u5f0f\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8ba1\u7b97\u65f6\u95f4\u9002\u5408\u5b9e\u65f6\u64cd\u4f5c\uff0c\u80fd\u591f\u6269\u5c55\u5230\u66f4\u5927\u7684\u4efb\u52a1\u96c6\u548c\u56e2\u961f\u89c4\u6a21\u3002", "conclusion": "Sadcher\u6846\u67b6\u5728\u5f02\u6784\u591a\u673a\u5668\u4eba\u4efb\u52a1\u5206\u914d\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5b9e\u65f6\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u53d1\u5e03\u4e86\u5305\u542b25\u4e07\u4e2a\u6700\u4f18\u8c03\u5ea6\u7684\u6570\u636e\u96c6\u3002"}}
{"id": "2510.14548", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14548", "abs": "https://arxiv.org/abs/2510.14548", "authors": ["Asen Nachkov", "Xi Wang", "Luc Van Gool"], "title": "LLM Agents Beyond Utility: An Open-Ended Perspective", "comment": null, "summary": "Recent LLM agents have made great use of chain of thought reasoning and\nfunction calling. As their capabilities grow, an important question arises: can\nthis software represent not only a smart problem-solving tool, but an entity in\nits own right, that can plan, design immediate tasks, and reason toward\nbroader, more ambiguous goals? To study this question, we adopt an open-ended\nexperimental setting where we augment a pretrained LLM agent with the ability\nto generate its own tasks, accumulate knowledge, and interact extensively with\nits environment. We study the resulting open-ended agent qualitatively. It can\nreliably follow complex multi-step instructions, store and reuse information\nacross runs, and propose and solve its own tasks, though it remains sensitive\nto prompt design, prone to repetitive task generation, and unable to form\nself-representations. These findings illustrate both the promise and current\nlimits of adapting pretrained LLMs toward open-endedness, and point to future\ndirections for training agents to manage memory, explore productively, and\npursue abstract long-term goals.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u9884\u8bad\u7ec3LLM\u4ee3\u7406\u80fd\u5426\u901a\u8fc7\u81ea\u4e3b\u751f\u6210\u4efb\u52a1\u3001\u79ef\u7d2f\u77e5\u8bc6\u548c\u73af\u5883\u4ea4\u4e92\uff0c\u4ece\u667a\u80fd\u5de5\u5177\u53d1\u5c55\u4e3a\u5177\u6709\u81ea\u4e3b\u89c4\u5212\u80fd\u529b\u7684\u5b9e\u4f53\u3002", "motivation": "\u968f\u7740LLM\u4ee3\u7406\u80fd\u529b\u589e\u5f3a\uff0c\u7814\u7a76\u5176\u662f\u5426\u80fd\u6210\u4e3a\u5177\u6709\u81ea\u4e3b\u89c4\u5212\u3001\u4efb\u52a1\u8bbe\u8ba1\u548c\u6a21\u7cca\u76ee\u6807\u63a8\u7406\u80fd\u529b\u7684\u72ec\u7acb\u5b9e\u4f53\u3002", "method": "\u91c7\u7528\u5f00\u653e\u5f0f\u5b9e\u9a8c\u8bbe\u7f6e\uff0c\u589e\u5f3a\u9884\u8bad\u7ec3LLM\u4ee3\u7406\u7684\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u81ea\u4e3b\u751f\u6210\u4efb\u52a1\u3001\u79ef\u7d2f\u77e5\u8bc6\u5e76\u5e7f\u6cdb\u4e0e\u73af\u5883\u4e92\u52a8\u3002", "result": "\u4ee3\u7406\u80fd\u591f\u53ef\u9760\u6267\u884c\u590d\u6742\u591a\u6b65\u9aa4\u6307\u4ee4\uff0c\u8de8\u8fd0\u884c\u5b58\u50a8\u548c\u91cd\u7528\u4fe1\u606f\uff0c\u81ea\u4e3b\u63d0\u51fa\u548c\u89e3\u51b3\u4efb\u52a1\uff0c\u4f46\u5bf9\u63d0\u793a\u8bbe\u8ba1\u654f\u611f\uff0c\u5bb9\u6613\u91cd\u590d\u751f\u6210\u4efb\u52a1\uff0c\u65e0\u6cd5\u5f62\u6210\u81ea\u6211\u8868\u5f81\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u9884\u8bad\u7ec3LLM\u5411\u5f00\u653e\u5f0f\u53d1\u5c55\u7684\u6f5c\u529b\u548c\u5f53\u524d\u5c40\u9650\uff0c\u4e3a\u672a\u6765\u8bad\u7ec3\u4ee3\u7406\u7ba1\u7406\u8bb0\u5fc6\u3001\u6709\u6548\u63a2\u7d22\u548c\u8ffd\u6c42\u62bd\u8c61\u957f\u671f\u76ee\u6807\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2510.14893", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14893", "abs": "https://arxiv.org/abs/2510.14893", "authors": ["Helene J. Levy", "Brett T. Lopez"], "title": "STITCHER: Constrained Trajectory Planning in Known Environments with Real-Time Motion Primitive Search", "comment": null, "summary": "Autonomous high-speed navigation through large, complex environments requires\nreal-time generation of agile trajectories that are dynamically feasible,\ncollision-free, and satisfy state or actuator constraints. Modern trajectory\nplanning techniques primarily use numerical optimization, as they enable the\nsystematic computation of high-quality, expressive trajectories that satisfy\nvarious constraints. However, stringent requirements on computation time and\nthe risk of numerical instability can limit the use of optimization-based\nplanners in safety-critical scenarios. This work presents an optimization-free\nplanning framework called STITCHER that stitches short trajectory segments\ntogether with graph search to compute long-range, expressive, and near-optimal\ntrajectories in real-time. STITCHER outperforms modern optimization-based\nplanners through our innovative planning architecture and several algorithmic\ndevelopments that make real-time planning possible. Extensive simulation\ntesting is performed to analyze the algorithmic components that make up\nSTITCHER, along with a thorough comparison with two state-of-the-art\noptimization planners. Simulation tests show that safe trajectories can be\ncreated within a few milliseconds for paths that span the entirety of two 50 m\nx 50 m environments. Hardware tests with a custom quadrotor verify that\nSTITCHER can produce trackable paths in real-time while respecting nonconvex\nconstraints, such as limits on tilt angle and motor forces, which are otherwise\nhard to include in optimization-based planners.", "AI": {"tldr": "STITCHER\u662f\u4e00\u79cd\u65e0\u4f18\u5316\u7684\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u77ed\u8f68\u8ff9\u6bb5\u62fc\u63a5\u4e0e\u56fe\u641c\u7d22\u7ed3\u5408\uff0c\u5b9e\u65f6\u751f\u6210\u957f\u8ddd\u79bb\u3001\u8868\u8fbe\u6027\u5f3a\u4e14\u63a5\u8fd1\u6700\u4f18\u7684\u8f68\u8ff9\uff0c\u89e3\u51b3\u4e86\u4f18\u5316\u89c4\u5212\u5668\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7684\u8ba1\u7b97\u65f6\u95f4\u548c\u6570\u503c\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u9ad8\u901f\u81ea\u4e3b\u5bfc\u822a\u4e2d\u5b9e\u65f6\u751f\u6210\u52a8\u6001\u53ef\u884c\u3001\u65e0\u78b0\u649e\u4e14\u6ee1\u8db3\u7ea6\u675f\u7684\u654f\u6377\u8f68\u8ff9\u95ee\u9898\uff0c\u514b\u670d\u4f18\u5316\u89c4\u5212\u5668\u5728\u8ba1\u7b97\u65f6\u95f4\u548c\u6570\u503c\u7a33\u5b9a\u6027\u65b9\u9762\u7684\u9650\u5236\u3002", "method": "\u91c7\u7528\u65e0\u4f18\u5316\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u62fc\u63a5\u77ed\u8f68\u8ff9\u6bb5\u4e0e\u56fe\u641c\u7d22\u76f8\u7ed3\u5408\u7684\u65b9\u5f0f\uff0c\u521b\u65b0\u6027\u5730\u5b9e\u73b0\u4e86\u5b9e\u65f6\u89c4\u5212\u67b6\u6784\u548c\u7b97\u6cd5\u6539\u8fdb\u3002", "result": "\u5728\u6a21\u62df\u6d4b\u8bd5\u4e2d\uff0c\u80fd\u591f\u5728\u51e0\u6beb\u79d2\u5185\u4e3a\u8de8\u8d8a\u4e24\u4e2a50m\u00d750m\u73af\u5883\u7684\u8def\u5f84\u751f\u6210\u5b89\u5168\u8f68\u8ff9\uff1b\u786c\u4ef6\u6d4b\u8bd5\u9a8c\u8bc1\u4e86STITCHER\u80fd\u591f\u5b9e\u65f6\u4ea7\u751f\u53ef\u8ddf\u8e2a\u8def\u5f84\uff0c\u5e76\u5904\u7406\u975e\u51f8\u7ea6\u675f\u3002", "conclusion": "STITCHER\u6846\u67b6\u5728\u5b9e\u65f6\u6027\u3001\u8f68\u8ff9\u8d28\u91cf\u548c\u7ea6\u675f\u5904\u7406\u65b9\u9762\u4f18\u4e8e\u73b0\u4ee3\u4f18\u5316\u89c4\u5212\u5668\uff0c\u9002\u7528\u4e8e\u5b89\u5168\u5173\u952e\u7684\u9ad8\u901f\u81ea\u4e3b\u5bfc\u822a\u573a\u666f\u3002"}}
{"id": "2510.14621", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14621", "abs": "https://arxiv.org/abs/2510.14621", "authors": ["Yuanyi Song", "Heyuan Huang", "Qiqiang Lin", "Yin Zhao", "Xiangmou Qu", "Jun Wang", "Xingyu Lou", "Weiwen Liu", "Zhuosheng Zhang", "Jun Wang", "Yong Yu", "Weinan Zhang", "Zhaoxiang Wang"], "title": "ColorBench: Benchmarking Mobile Agents with Graph-Structured Framework for Complex Long-Horizon Tasks", "comment": null, "summary": "The rapid advancement of multimodal large language models has enabled agents\nto operate mobile devices by directly interacting with graphical user\ninterfaces, opening new possibilities for mobile automation. However,\nreal-world mobile tasks are often complex and allow for multiple valid\nsolutions. This contradicts current mobile agent evaluation standards: offline\nstatic benchmarks can only validate a single predefined \"golden path\", while\nonline dynamic testing is constrained by the complexity and non-reproducibility\nof real devices, making both approaches inadequate for comprehensively\nassessing agent capabilities. To bridge the gap between offline and online\nevaluation and enhance testing stability, this paper introduces a novel\ngraph-structured benchmarking framework. By modeling the finite states observed\nduring real-device interactions, it achieves static simulation of dynamic\nbehaviors. Building on this, we develop ColorBench, a benchmark focused on\ncomplex long-horizon tasks. It supports evaluation of multiple valid solutions,\nsubtask completion rate statistics, and atomic-level capability analysis.\nColorBench contains 175 tasks (74 single-app, 101 cross-app) with an average\nlength of over 13 steps. Each task includes at least two correct paths and\nseveral typical error paths, enabling quasi-dynamic interaction. By evaluating\nColorBench across various baselines, we discover limitations of existing models\nand propose improvement directions and feasible technical pathways to enhance\nagents' performance on complex, long-horizon problems based on experimental\nresults. Code and data are available at:\nhttps://github.com/MadeAgents/ColorBench.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ColorBench\uff0c\u4e00\u4e2a\u57fa\u4e8e\u56fe\u7ed3\u6784\u7684\u79fb\u52a8\u4ee3\u7406\u8bc4\u4f30\u57fa\u51c6\uff0c\u4e13\u6ce8\u4e8e\u590d\u6742\u957f\u65f6\u7a0b\u4efb\u52a1\uff0c\u652f\u6301\u591a\u6709\u6548\u89e3\u51b3\u65b9\u6848\u8bc4\u4f30\u548c\u539f\u5b50\u7ea7\u80fd\u529b\u5206\u6790\u3002", "motivation": "\u73b0\u6709\u79fb\u52a8\u4ee3\u7406\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\uff1a\u79bb\u7ebf\u9759\u6001\u57fa\u51c6\u53ea\u80fd\u9a8c\u8bc1\u5355\u4e00\u9884\u8bbe\u8def\u5f84\uff0c\u800c\u5728\u7ebf\u52a8\u6001\u6d4b\u8bd5\u53d7\u9650\u4e8e\u771f\u5b9e\u8bbe\u5907\u7684\u590d\u6742\u6027\u548c\u4e0d\u53ef\u91cd\u73b0\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u6865\u63a5\u79bb\u7ebf\u4e0e\u5728\u7ebf\u8bc4\u4f30\u7684\u7a33\u5b9a\u6d4b\u8bd5\u6846\u67b6\u3002", "method": "\u5f00\u53d1\u4e86\u56fe\u7ed3\u6784\u57fa\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u6a21\u771f\u5b9e\u8bbe\u5907\u4ea4\u4e92\u4e2d\u7684\u6709\u9650\u72b6\u6001\u5b9e\u73b0\u52a8\u6001\u884c\u4e3a\u7684\u9759\u6001\u6a21\u62df\u3002\u6784\u5efa\u4e86\u5305\u542b175\u4e2a\u4efb\u52a1\uff0874\u4e2a\u5355\u5e94\u7528\u3001101\u4e2a\u8de8\u5e94\u7528\uff09\u7684ColorBench\u57fa\u51c6\uff0c\u6bcf\u4e2a\u4efb\u52a1\u5305\u542b\u81f3\u5c11\u4e24\u6761\u6b63\u786e\u8def\u5f84\u548c\u5178\u578b\u9519\u8bef\u8def\u5f84\u3002", "result": "ColorBench\u5e73\u5747\u4efb\u52a1\u957f\u5ea6\u8d85\u8fc713\u6b65\uff0c\u652f\u6301\u591a\u6709\u6548\u89e3\u51b3\u65b9\u6848\u8bc4\u4f30\u3001\u5b50\u4efb\u52a1\u5b8c\u6210\u7387\u7edf\u8ba1\u548c\u539f\u5b50\u7ea7\u80fd\u529b\u5206\u6790\u3002\u901a\u8fc7\u8bc4\u4f30\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u57fa\u4e8e\u5b9e\u9a8c\u7ed3\u679c\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u5411\u3002", "conclusion": "ColorBench\u63d0\u4f9b\u4e86\u4e00\u79cd\u7a33\u5b9a\u3001\u53ef\u91cd\u73b0\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u80fd\u591f\u5168\u9762\u8bc4\u4f30\u79fb\u52a8\u4ee3\u7406\u5728\u590d\u6742\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u4e3a\u6a21\u578b\u6539\u8fdb\u63d0\u4f9b\u4e86\u53ef\u884c\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2510.14902", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14902", "abs": "https://arxiv.org/abs/2510.14902", "authors": ["Han Zhao", "Jiaxuan Zhang", "Wenxuan Song", "Pengxiang Ding", "Donglin Wang"], "title": "VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation", "comment": null, "summary": "Current vision-language-action (VLA) models, pre-trained on large-scale\nrobotic data, exhibit strong multi-task capabilities and generalize well to\nvariations in visual and language instructions for manipulation. However, their\nsuccess rate drops significantly when faced with object concepts outside the\ntraining data, such as unseen object descriptions and textures in the dataset.\nTo address this, we propose a novel agentic framework, VLA^2, which leverages\nOpenVLA as the execution backbone and effectively leverages external modules\nsuch as web retrieval and object detection to provide visual and textual\nknowledge about target objects to the VLA. This approach mitigates\ngeneralization failure when handling out-of-distribution objects. Based on the\nLIBERO simulation environment, we introduced novel objects and object\ndescriptions to construct a new evaluation benchmark with three difficulty\nlevels to test the effectiveness of our method. Our framework successfully\noutperformed the current state-of-the-art models on our designed hard-level\ngeneralization benchmark. Compared to the standalone OpenVLA baseline, VLA^2\nachieves a 44.2% improvement in the success rate in the hard-level benchmark\nand an average improvement of 20.2% in all customized environments without any\nperformance degradation on in-domain tasks. Project website:\nhttps://vla-2.github.io.", "AI": {"tldr": "\u63d0\u51fa\u4e86VLA^2\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u5916\u90e8\u6a21\u5757\uff08\u7f51\u7edc\u68c0\u7d22\u548c\u7269\u4f53\u68c0\u6d4b\uff09\u6765\u589e\u5f3aVLA\u6a21\u578b\u5904\u7406\u5206\u5e03\u5916\u5bf9\u8c61\u7684\u80fd\u529b\uff0c\u5728\u56f0\u96be\u7ea7\u522b\u6cdb\u5316\u57fa\u51c6\u4e0a\u6bd4\u57fa\u7ebf\u6a21\u578b\u63d0\u5347\u4e8644.2%\u7684\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u5728\u5904\u7406\u8bad\u7ec3\u6570\u636e\u4e2d\u672a\u89c1\u8fc7\u7684\u5bf9\u8c61\u6982\u5ff5\uff08\u5982\u65b0\u7269\u4f53\u63cf\u8ff0\u548c\u7eb9\u7406\uff09\u65f6\u6210\u529f\u7387\u663e\u8457\u4e0b\u964d\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u79cd\u6cdb\u5316\u5931\u8d25\u95ee\u9898\u3002", "method": "\u4ee5OpenVLA\u4e3a\u6267\u884c\u9aa8\u5e72\uff0c\u7ed3\u5408\u5916\u90e8\u6a21\u5757\uff08\u7f51\u7edc\u68c0\u7d22\u548c\u7269\u4f53\u68c0\u6d4b\uff09\u4e3a\u76ee\u6807\u5bf9\u8c61\u63d0\u4f9b\u89c6\u89c9\u548c\u6587\u672c\u77e5\u8bc6\uff0c\u6784\u5efa\u4e86\u57fa\u4e8eLIBERO\u73af\u5883\u7684\u4e09\u7ea7\u96be\u5ea6\u8bc4\u4f30\u57fa\u51c6\u3002", "result": "\u5728\u8bbe\u8ba1\u7684\u56f0\u96be\u7ea7\u522b\u6cdb\u5316\u57fa\u51c6\u4e0a\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u76f8\u6bd4OpenVLA\u57fa\u7ebf\u5728\u56f0\u96be\u7ea7\u522b\u63d0\u534744.2%\u6210\u529f\u7387\uff0c\u6240\u6709\u5b9a\u5236\u73af\u5883\u5e73\u5747\u63d0\u534720.2%\uff0c\u4e14\u5728\u57df\u5185\u4efb\u52a1\u4e0a\u65e0\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "VLA^2\u6846\u67b6\u6709\u6548\u7f13\u89e3\u4e86VLA\u6a21\u578b\u5904\u7406\u5206\u5e03\u5916\u5bf9\u8c61\u7684\u6cdb\u5316\u5931\u8d25\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u672a\u89c1\u5bf9\u8c61\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2510.14665", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.14665", "abs": "https://arxiv.org/abs/2510.14665", "authors": ["Rikard Rosenbacke", "Carl Rosenbacke", "Victor Rosenbacke", "Martin McKee"], "title": "Beyond Hallucinations: The Illusion of Understanding in Large Language Models", "comment": null, "summary": "Large language models (LLMs) are becoming deeply embedded in human\ncommunication and decision-making, yet they inherit the ambiguity, bias, and\nlack of direct access to truth inherent in language itself. While their outputs\nare fluent, emotionally resonant, and coherent, they are generated through\nstatistical prediction rather than grounded reasoning. This creates the risk of\nhallucination, responses that sound convincing but lack factual validity.\nBuilding on Geoffrey Hinton's observation that AI mirrors human intuition\nrather than reasoning, this paper argues that LLMs operationalize System 1\ncognition at scale: fast, associative, and persuasive, but without reflection\nor falsification. To address this, we introduce the Rose-Frame, a\nthree-dimensional framework for diagnosing cognitive and epistemic drift in\nhuman-AI interaction. The three axes are: (i) Map vs. Territory, which\ndistinguishes representations of reality (epistemology) from reality itself\n(ontology); (ii) Intuition vs. Reason, drawing on dual-process theory to\nseparate fast, emotional judgments from slow, reflective thinking; and (iii)\nConflict vs. Confirmation, which examines whether ideas are critically tested\nthrough disagreement or simply reinforced through mutual validation. Each\ndimension captures a distinct failure mode, and their combination amplifies\nmisalignment. Rose-Frame does not attempt to fix LLMs with more data or rules.\nInstead, it offers a reflective tool that makes both the model's limitations\nand the user's assumptions visible, enabling more transparent and critically\naware AI deployment. It reframes alignment as cognitive governance: intuition,\nwhether human or artificial, must remain governed by human reason. Only by\nembedding reflective, falsifiable oversight can we align machine fluency with\nhuman understanding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Rose-Frame\u6846\u67b6\uff0c\u7528\u4e8e\u8bca\u65ad\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u8ba4\u77e5\u548c\u8ba4\u8bc6\u8bba\u6f02\u79fb\uff0c\u901a\u8fc7\u4e09\u4e2a\u7ef4\u5ea6\uff08\u5730\u56fevs\u9886\u571f\u3001\u76f4\u89c9vs\u7406\u6027\u3001\u51b2\u7a81vs\u786e\u8ba4\uff09\u6765\u589e\u5f3aAI\u90e8\u7f72\u7684\u900f\u660e\u5ea6\u548c\u6279\u5224\u610f\u8bc6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u6d41\u7545\u4e14\u60c5\u611f\u5171\u9e23\u5f3a\uff0c\u4f46\u57fa\u4e8e\u7edf\u8ba1\u9884\u6d4b\u800c\u975e\u6709\u6839\u636e\u7684\u63a8\u7406\uff0c\u5b58\u5728\u5e7b\u89c9\u98ce\u9669\uff0c\u53ef\u80fd\u4ea7\u751f\u542c\u8d77\u6765\u4ee4\u4eba\u4fe1\u670d\u4f46\u7f3a\u4e4f\u4e8b\u5b9e\u6709\u6548\u6027\u7684\u56de\u7b54\u3002", "method": "\u5f15\u5165Rose-Frame\u4e09\u7ef4\u6846\u67b6\uff1a(i)\u5730\u56fevs\u9886\u571f\uff0c\u533a\u5206\u73b0\u5b9e\u8868\u5f81\u4e0e\u73b0\u5b9e\u672c\u8eab\uff1b(ii)\u76f4\u89c9vs\u7406\u6027\uff0c\u57fa\u4e8e\u53cc\u8fc7\u7a0b\u7406\u8bba\u5206\u79bb\u5feb\u901f\u60c5\u611f\u5224\u65ad\u4e0e\u6162\u901f\u53cd\u601d\u601d\u7ef4\uff1b(iii)\u51b2\u7a81vs\u786e\u8ba4\uff0c\u68c0\u9a8c\u89c2\u70b9\u662f\u5426\u901a\u8fc7\u5206\u6b67\u8fdb\u884c\u6279\u5224\u6027\u6d4b\u8bd5\u8fd8\u662f\u4ec5\u901a\u8fc7\u76f8\u4e92\u9a8c\u8bc1\u5f3a\u5316\u3002", "result": "Rose-Frame\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53cd\u601d\u5de5\u5177\uff0c\u4f7f\u6a21\u578b\u7684\u5c40\u9650\u6027\u548c\u7528\u6237\u7684\u5047\u8bbe\u53ef\u89c1\uff0c\u5b9e\u73b0\u66f4\u900f\u660e\u548c\u6279\u5224\u6027\u610f\u8bc6\u5f3a\u7684AI\u90e8\u7f72\u3002", "conclusion": "\u5c06\u5bf9\u9f50\u91cd\u65b0\u5b9a\u4e49\u4e3a\u8ba4\u77e5\u6cbb\u7406\uff1a\u65e0\u8bba\u662f\u4eba\u7c7b\u8fd8\u662f\u4eba\u5de5\u667a\u80fd\u7684\u76f4\u89c9\uff0c\u90fd\u5fc5\u987b\u53d7\u5230\u4eba\u7c7b\u7406\u6027\u7684\u6cbb\u7406\u3002\u53ea\u6709\u901a\u8fc7\u5d4c\u5165\u53cd\u601d\u6027\u3001\u53ef\u8bc1\u4f2a\u7684\u76d1\u7763\uff0c\u624d\u80fd\u5c06\u673a\u5668\u7684\u6d41\u7545\u6027\u4e0e\u4eba\u7c7b\u7684\u7406\u89e3\u5bf9\u9f50\u3002"}}
{"id": "2510.14930", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14930", "abs": "https://arxiv.org/abs/2510.14930", "authors": ["Binghao Huang", "Jie Xu", "Iretiayo Akinola", "Wei Yang", "Balakumar Sundaralingam", "Rowland O'Flaherty", "Dieter Fox", "Xiaolong Wang", "Arsalan Mousavian", "Yu-Wei Chao", "Yunzhu Li"], "title": "VT-Refine: Learning Bimanual Assembly with Visuo-Tactile Feedback via Simulation Fine-Tunin", "comment": "Accepted by 9th Conference on Robot Learning (CoRL 2025); Website:\n  https://binghao-huang.github.io/vt_refine/", "summary": "Humans excel at bimanual assembly tasks by adapting to rich tactile feedback\n-- a capability that remains difficult to replicate in robots through\nbehavioral cloning alone, due to the suboptimality and limited diversity of\nhuman demonstrations. In this work, we present VT-Refine, a visuo-tactile\npolicy learning framework that combines real-world demonstrations,\nhigh-fidelity tactile simulation, and reinforcement learning to tackle precise,\ncontact-rich bimanual assembly. We begin by training a diffusion policy on a\nsmall set of demonstrations using synchronized visual and tactile inputs. This\npolicy is then transferred to a simulated digital twin equipped with simulated\ntactile sensors and further refined via large-scale reinforcement learning to\nenhance robustness and generalization. To enable accurate sim-to-real transfer,\nwe leverage high-resolution piezoresistive tactile sensors that provide normal\nforce signals and can be realistically modeled in parallel using\nGPU-accelerated simulation. Experimental results show that VT-Refine improves\nassembly performance in both simulation and the real world by increasing data\ndiversity and enabling more effective policy fine-tuning. Our project page is\navailable at https://binghao-huang.github.io/vt_refine/.", "AI": {"tldr": "VT-Refine\u662f\u4e00\u4e2a\u89c6\u89c9-\u89e6\u89c9\u7b56\u7565\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u771f\u5b9e\u6f14\u793a\u3001\u9ad8\u4fdd\u771f\u89e6\u89c9\u6a21\u62df\u548c\u5f3a\u5316\u5b66\u4e60\u6765\u89e3\u51b3\u7cbe\u786e\u7684\u53cc\u624b\u673a\u5668\u4eba\u88c5\u914d\u4efb\u52a1\u3002", "motivation": "\u4eba\u7c7b\u80fd\u591f\u901a\u8fc7\u4e30\u5bcc\u7684\u89e6\u89c9\u53cd\u9988\u9002\u5e94\u53cc\u624b\u673a\u5668\u4eba\u88c5\u914d\u4efb\u52a1\uff0c\u4f46\u4ec5\u901a\u8fc7\u884c\u4e3a\u514b\u9686\u96be\u4ee5\u5728\u673a\u5668\u4eba\u4e2d\u590d\u5236\u8fd9\u79cd\u80fd\u529b\uff0c\u56e0\u4e3a\u4eba\u7c7b\u6f14\u793a\u5b58\u5728\u6b21\u4f18\u6027\u548c\u591a\u6837\u6027\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "\u9996\u5148\u5728\u5c11\u91cf\u6f14\u793a\u6570\u636e\u4e0a\u4f7f\u7528\u540c\u6b65\u89c6\u89c9\u548c\u89e6\u89c9\u8f93\u5165\u8bad\u7ec3\u6269\u6563\u7b56\u7565\uff0c\u7136\u540e\u5c06\u8be5\u7b56\u7565\u8f6c\u79fb\u5230\u914d\u5907\u6a21\u62df\u89e6\u89c9\u4f20\u611f\u5668\u7684\u6570\u5b57\u5b6a\u751f\u7cfb\u7edf\u4e2d\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u7ec6\u5316\u4ee5\u589e\u5f3a\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cVT-Refine\u901a\u8fc7\u589e\u52a0\u6570\u636e\u591a\u6837\u6027\u548c\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u7b56\u7565\u5fae\u8c03\uff0c\u63d0\u9ad8\u4e86\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u88c5\u914d\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5c55\u793a\u4e86\u5982\u4f55\u7ed3\u5408\u771f\u5b9e\u6f14\u793a\u3001\u89e6\u89c9\u6a21\u62df\u548c\u5f3a\u5316\u5b66\u4e60\u6765\u63d0\u5347\u53cc\u624b\u673a\u5668\u4eba\u88c5\u914d\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2510.14669", "categories": ["cs.AI", "68T01, 68T09, 62P10 68T01, 68T09, 62P10", "I.2.6; I.5.4; H.2.8; J.3; K.4.1; K.4.2"], "pdf": "https://arxiv.org/pdf/2510.14669", "abs": "https://arxiv.org/abs/2510.14669", "authors": ["Sara Altamirano", "Arjan Vreeken", "Sennay Ghebreab"], "title": "Machine Learning and Public Health: Identifying and Mitigating Algorithmic Bias through a Systematic Review", "comment": "Extended version of the paper accepted at the AAAI/ACM Conference on\n  AI, Ethics, and Society (AIES 2025), including an appendix. 10 pages, 2\n  figures", "summary": "Machine learning (ML) promises to revolutionize public health through\nimproved surveillance, risk stratification, and resource allocation. However,\nwithout systematic attention to algorithmic bias, ML may inadvertently\nreinforce existing health disparities. We present a systematic literature\nreview of algorithmic bias identification, discussion, and reporting in Dutch\npublic health ML research from 2021 to 2025. To this end, we developed the Risk\nof Algorithmic Bias Assessment Tool (RABAT) by integrating elements from\nestablished frameworks (Cochrane Risk of Bias, PROBAST, Microsoft Responsible\nAI checklist) and applied it to 35 peer-reviewed studies. Our analysis reveals\npervasive gaps: although data sampling and missing data practices are well\ndocumented, most studies omit explicit fairness framing, subgroup analyses, and\ntransparent discussion of potential harms. In response, we introduce a\nfour-stage fairness-oriented framework called ACAR (Awareness,\nConceptualization, Application, Reporting), with guiding questions derived from\nour systematic literature review to help researchers address fairness across\nthe ML lifecycle. We conclude with actionable recommendations for public health\nML practitioners to consistently consider algorithmic bias and foster\ntransparency, ensuring that algorithmic innovations advance health equity\nrather than undermine it.", "AI": {"tldr": "\u7cfb\u7edf\u7efc\u8ff0\u8377\u5170\u516c\u5171\u536b\u751f\u673a\u5668\u5b66\u4e60\u7814\u7a76\u4e2d\u7b97\u6cd5\u504f\u89c1\u7684\u8bc6\u522b\u3001\u8ba8\u8bba\u548c\u62a5\u544a\u60c5\u51b5\uff0c\u5f00\u53d1\u4e86RABAT\u8bc4\u4f30\u5de5\u5177\u5e76\u5e94\u7528\u4e8e35\u9879\u7814\u7a76\uff0c\u63ed\u793a\u4e86\u516c\u5e73\u6027\u6846\u67b6\u548c\u5b50\u7fa4\u5206\u6790\u7b49\u65b9\u9762\u7684\u666e\u904d\u7f3a\u5931\uff0c\u63d0\u51fa\u4e86ACAR\u56db\u9636\u6bb5\u516c\u5e73\u5bfc\u5411\u6846\u67b6\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u5728\u516c\u5171\u536b\u751f\u9886\u57df\u7684\u5e94\u7528\u53ef\u80fd\u65e0\u610f\u4e2d\u52a0\u5267\u73b0\u6709\u5065\u5eb7\u4e0d\u5e73\u7b49\uff0c\u9700\u8981\u7cfb\u7edf\u5173\u6ce8\u7b97\u6cd5\u504f\u89c1\u95ee\u9898\u3002", "method": "\u5f00\u53d1RABAT\u8bc4\u4f30\u5de5\u5177\u6574\u5408\u73b0\u6709\u6846\u67b6\uff0c\u5bf92011-2025\u5e74\u8377\u5170\u516c\u5171\u536b\u751fML\u7814\u7a76\u8fdb\u884c\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\uff0c\u5206\u679035\u9879\u540c\u884c\u8bc4\u5ba1\u7814\u7a76\u3002", "result": "\u53d1\u73b0\u666e\u904d\u5b58\u5728\u516c\u5e73\u6027\u6846\u67b6\u7f3a\u5931\u3001\u5b50\u7fa4\u5206\u6790\u4e0d\u8db3\u3001\u6f5c\u5728\u5371\u5bb3\u8ba8\u8bba\u4e0d\u900f\u660e\u7b49\u95ee\u9898\uff0c\u5c3d\u7ba1\u6570\u636e\u62bd\u6837\u548c\u7f3a\u5931\u6570\u636e\u5904\u7406\u8bb0\u5f55\u826f\u597d\u3002", "conclusion": "\u63d0\u51faACAR\u56db\u9636\u6bb5\u6846\u67b6\u548c\u5177\u4f53\u5efa\u8bae\uff0c\u5e2e\u52a9\u516c\u5171\u536b\u751fML\u4ece\u4e1a\u8005\u7cfb\u7edf\u8003\u8651\u7b97\u6cd5\u504f\u89c1\uff0c\u786e\u4fdd\u7b97\u6cd5\u521b\u65b0\u4fc3\u8fdb\u800c\u975e\u635f\u5bb3\u5065\u5eb7\u516c\u5e73\u3002"}}
{"id": "2510.14670", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.14670", "abs": "https://arxiv.org/abs/2510.14670", "authors": ["Marco Simoni", "Aleksandar Fontana", "Andrea Saracino", "Paolo Mori"], "title": "TITAN: Graph-Executable Reasoning for Cyber Threat Intelligence", "comment": null, "summary": "TITAN (Threat Intelligence Through Automated Navigation) is a framework that\nconnects natural-language cyber threat queries with executable reasoning over a\nstructured knowledge graph. It integrates a path planner model, which predicts\nlogical relation chains from text, and a graph executor that traverses the\nTITAN Ontology to retrieve factual answers and supporting evidence. Unlike\ntraditional retrieval systems, TITAN operates on a typed, bidirectional graph\nderived from MITRE, allowing reasoning to move clearly and reversibly between\nthreats, behaviors, and defenses. To support training and evaluation, we\nintroduce the TITAN Dataset, a corpus of 88209 examples (Train: 74258; Test:\n13951) pairing natural language questions with executable reasoning paths and\nstep by step Chain of Thought explanations. Empirical evaluations show that\nTITAN enables models to generate syntactically valid and semantically coherent\nreasoning paths that can be deterministically executed on the underlying graph.", "AI": {"tldr": "TITAN\u662f\u4e00\u4e2a\u5c06\u81ea\u7136\u8bed\u8a00\u7f51\u7edc\u5b89\u5168\u5a01\u80c1\u67e5\u8be2\u4e0e\u7ed3\u6784\u5316\u77e5\u8bc6\u56fe\u8c31\u4e0a\u7684\u53ef\u6267\u884c\u63a8\u7406\u76f8\u8fde\u63a5\u7684\u6846\u67b6\uff0c\u5305\u542b\u8def\u5f84\u89c4\u5212\u6a21\u578b\u548c\u56fe\u6267\u884c\u5668\uff0c\u652f\u6301\u5728\u5a01\u80c1\u3001\u884c\u4e3a\u548c\u9632\u5fa1\u4e4b\u95f4\u8fdb\u884c\u6e05\u6670\u53ef\u9006\u7684\u63a8\u7406\u3002", "motivation": "\u4f20\u7edf\u68c0\u7d22\u7cfb\u7edf\u65e0\u6cd5\u6709\u6548\u5904\u7406\u7f51\u7edc\u5b89\u5168\u5a01\u80c1\u67e5\u8be2\u7684\u590d\u6742\u63a8\u7406\u9700\u6c42\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8fde\u63a5\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u4e0e\u7ed3\u6784\u5316\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u7684\u65b9\u6cd5\u3002", "method": "\u96c6\u6210\u8def\u5f84\u89c4\u5212\u6a21\u578b\u9884\u6d4b\u903b\u8f91\u5173\u7cfb\u94fe\uff0c\u56fe\u6267\u884c\u5668\u904d\u5386TITAN\u672c\u4f53\u56fe\u68c0\u7d22\u4e8b\u5b9e\u7b54\u6848\u548c\u8bc1\u636e\uff0c\u57fa\u4e8eMITRE\u6784\u5efa\u7c7b\u578b\u5316\u53cc\u5411\u56fe\u652f\u6301\u53ef\u9006\u63a8\u7406\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b88209\u4e2a\u793a\u4f8b\u7684TITAN\u6570\u636e\u96c6\uff0c\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\u6a21\u578b\u80fd\u751f\u6210\u8bed\u6cd5\u6709\u6548\u3001\u8bed\u4e49\u8fde\u8d2f\u7684\u53ef\u6267\u884c\u63a8\u7406\u8def\u5f84\u3002", "conclusion": "TITAN\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u81ea\u7136\u8bed\u8a00\u5a01\u80c1\u67e5\u8be2\u4e0e\u7ed3\u6784\u5316\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u7684\u6709\u6548\u8fde\u63a5\uff0c\u4e3a\u7f51\u7edc\u5b89\u5168\u5a01\u80c1\u60c5\u62a5\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2510.14952", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14952", "abs": "https://arxiv.org/abs/2510.14952", "authors": ["Zhe Li", "Cheng Chi", "Yangyang Wei", "Boan Zhu", "Yibo Peng", "Tao Huang", "Pengwei Wang", "Zhongyuan Wang", "Shanghang Zhang", "Chang Xu"], "title": "From Language to Locomotion: Retargeting-free Humanoid Control via Motion Latent Guidance", "comment": null, "summary": "Natural language offers a natural interface for humanoid robots, but existing\nlanguage-guided humanoid locomotion pipelines remain cumbersome and unreliable.\nThey typically decode human motion, retarget it to robot morphology, and then\ntrack it with a physics-based controller. However, this multi-stage process is\nprone to cumulative errors, introduces high latency, and yields weak coupling\nbetween semantics and control. These limitations call for a more direct pathway\nfrom language to action, one that eliminates fragile intermediate stages.\nTherefore, we present RoboGhost, a retargeting-free framework that directly\nconditions humanoid policies on language-grounded motion latents. By bypassing\nexplicit motion decoding and retargeting, RoboGhost enables a diffusion-based\npolicy to denoise executable actions directly from noise, preserving semantic\nintent and supporting fast, reactive control. A hybrid causal\ntransformer-diffusion motion generator further ensures long-horizon consistency\nwhile maintaining stability and diversity, yielding rich latent representations\nfor precise humanoid behavior. Extensive experiments demonstrate that RoboGhost\nsubstantially reduces deployment latency, improves success rates and tracking\naccuracy, and produces smooth, semantically aligned locomotion on real\nhumanoids. Beyond text, the framework naturally extends to other modalities\nsuch as images, audio, and music, providing a general foundation for\nvision-language-action humanoid systems.", "AI": {"tldr": "RoboGhost\u662f\u4e00\u4e2a\u514d\u91cd\u5b9a\u5411\u6846\u67b6\uff0c\u901a\u8fc7\u76f4\u63a5\u5c06\u4eba\u5f62\u673a\u5668\u4eba\u7b56\u7565\u5efa\u7acb\u5728\u8bed\u8a00\u57fa\u7840\u7684\u8fd0\u52a8\u6f5c\u5728\u7a7a\u95f4\u4e0a\uff0c\u5b9e\u73b0\u4e86\u4ece\u8bed\u8a00\u5230\u52a8\u4f5c\u7684\u76f4\u63a5\u6620\u5c04\uff0c\u6d88\u9664\u4e86\u4f20\u7edf\u591a\u9636\u6bb5\u6d41\u7a0b\u4e2d\u7684\u7d2f\u79ef\u8bef\u5dee\u548c\u9ad8\u5ef6\u8fdf\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u5f15\u5bfc\u7684\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u6d41\u7a0b\u5b58\u5728\u591a\u9636\u6bb5\u5904\u7406\u5bfc\u81f4\u7684\u7d2f\u79ef\u8bef\u5dee\u3001\u9ad8\u5ef6\u8fdf\u4ee5\u53ca\u8bed\u4e49\u4e0e\u63a7\u5236\u8026\u5408\u5f31\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u76f4\u63a5\u7684\u4ece\u8bed\u8a00\u5230\u52a8\u4f5c\u7684\u8def\u5f84\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u6269\u6563\u7684\u7b56\u7565\u76f4\u63a5\u4ece\u566a\u58f0\u4e2d\u751f\u6210\u53ef\u6267\u884c\u52a8\u4f5c\uff0c\u901a\u8fc7\u6df7\u5408\u56e0\u679ctransformer-diffusion\u8fd0\u52a8\u751f\u6210\u5668\u786e\u4fdd\u957f\u65f6\u7a0b\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u7a33\u5b9a\u6027\u548c\u591a\u6837\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eRoboGhost\u663e\u8457\u964d\u4f4e\u4e86\u90e8\u7f72\u5ef6\u8fdf\uff0c\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u548c\u8ddf\u8e2a\u7cbe\u5ea6\uff0c\u5728\u771f\u5b9e\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u4ea7\u751f\u4e86\u5e73\u6ed1\u3001\u8bed\u4e49\u5bf9\u9f50\u7684\u8fd0\u52a8\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u4eba\u5f62\u7cfb\u7edf\u63d0\u4f9b\u4e86\u901a\u7528\u57fa\u7840\uff0c\u53ef\u81ea\u7136\u6269\u5c55\u5230\u56fe\u50cf\u3001\u97f3\u9891\u548c\u97f3\u4e50\u7b49\u5176\u4ed6\u6a21\u6001\u3002"}}
{"id": "2510.14676", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14676", "abs": "https://arxiv.org/abs/2510.14676", "authors": ["Bianca Maria Lerma", "Rafael Pe\u00f1aloza"], "title": "NAEL: Non-Anthropocentric Ethical Logic", "comment": "Accepted to the FEAR workshop 2025", "summary": "We introduce NAEL (Non-Anthropocentric Ethical Logic), a novel ethical\nframework for artificial agents grounded in active inference and symbolic\nreasoning. Departing from conventional, human-centred approaches to AI ethics,\nNAEL formalizes ethical behaviour as an emergent property of intelligent\nsystems minimizing global expected free energy in dynamic, multi-agent\nenvironments. We propose a neuro-symbolic architecture to allow agents to\nevaluate the ethical consequences of their actions in uncertain settings. The\nproposed system addresses the limitations of existing ethical models by\nallowing agents to develop context-sensitive, adaptive, and relational ethical\nbehaviour without presupposing anthropomorphic moral intuitions. A case study\ninvolving ethical resource distribution illustrates NAEL's dynamic balancing of\nself-preservation, epistemic learning, and collective welfare.", "AI": {"tldr": "\u63d0\u51faNAEL\uff08\u975e\u4eba\u7c7b\u4e2d\u5fc3\u4f26\u7406\u903b\u8f91\uff09\u6846\u67b6\uff0c\u57fa\u4e8e\u4e3b\u52a8\u63a8\u7406\u548c\u7b26\u53f7\u63a8\u7406\uff0c\u4e3a\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u5efa\u7acb\u975e\u4eba\u7c7b\u4e2d\u5fc3\u7684\u4f26\u7406\u7cfb\u7edf\uff0c\u5c06\u4f26\u7406\u884c\u4e3a\u5f62\u5f0f\u5316\u4e3a\u667a\u80fd\u7cfb\u7edf\u5728\u52a8\u6001\u591a\u4ee3\u7406\u73af\u5883\u4e2d\u6700\u5c0f\u5316\u5168\u5c40\u671f\u671b\u81ea\u7531\u80fd\u91cf\u7684\u6d8c\u73b0\u7279\u6027\u3002", "motivation": "\u4f20\u7edfAI\u4f26\u7406\u65b9\u6cd5\u8fc7\u4e8e\u4eba\u7c7b\u4e2d\u5fc3\u5316\uff0c\u73b0\u6709\u4f26\u7406\u6a21\u578b\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5728\u4e0d\u9884\u8bbe\u4eba\u7c7b\u9053\u5fb7\u76f4\u89c9\u7684\u60c5\u51b5\u4e0b\u4ea7\u751f\u4e0a\u4e0b\u6587\u654f\u611f\u3001\u81ea\u9002\u5e94\u548c\u5173\u7cfb\u6027\u4f26\u7406\u884c\u4e3a\u7684\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\uff0c\u7ed3\u5408\u4e3b\u52a8\u63a8\u7406\u548c\u7b26\u53f7\u63a8\u7406\uff0c\u4f7f\u4ee3\u7406\u80fd\u591f\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u8bc4\u4f30\u5176\u884c\u4e3a\u7684\u4f26\u7406\u540e\u679c\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u5168\u5c40\u671f\u671b\u81ea\u7531\u80fd\u91cf\u6765\u9a71\u52a8\u4f26\u7406\u51b3\u7b56\u3002", "result": "\u901a\u8fc7\u4f26\u7406\u8d44\u6e90\u5206\u914d\u7684\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86NAEL\u80fd\u591f\u52a8\u6001\u5e73\u8861\u81ea\u6211\u4fdd\u5b58\u3001\u8ba4\u77e5\u5b66\u4e60\u548c\u96c6\u4f53\u798f\u5229\uff0c\u8bc1\u660e\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "NAEL\u4e3a\u5f00\u53d1\u5177\u6709\u9002\u5e94\u6027\u4f26\u7406\u884c\u4e3a\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u975e\u4eba\u7c7b\u4e2d\u5fc3\u6846\u67b6\uff0c\u80fd\u591f\u514b\u670d\u73b0\u6709\u4f26\u7406\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.14683", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14683", "abs": "https://arxiv.org/abs/2510.14683", "authors": ["Devon Graham", "Kevin Leyton-Brown"], "title": "Practical, Utilitarian Algorithm Configuration", "comment": null, "summary": "Utilitarian algorithm configuration identifies a parameter setting for a\ngiven algorithm that maximizes a user's utility. Utility functions offer a\ntheoretically well-grounded approach to optimizing decision-making under\nuncertainty and are flexible enough to capture a user's preferences over\nalgorithm runtimes (e.g., they can describe a sharp cutoff after which a\nsolution is no longer required, a per-hour cost for compute, or diminishing\nreturns from algorithms that take longer to run). COUP is a recently-introduced\nutilitarian algorithm configuration procedure which was designed mainly to\noffer strong theoretical guarantees about the quality of the configuration it\nreturns, with less attention paid to its practical performance. This paper\ncloses that gap, bringing theoretically-grounded, utilitarian algorithm\nconfiguration to the point where it is competitive with widely used, heuristic\nconfiguration procedures that offer no performance guarantees. We present a\nseries of improvements to COUP that improve its empirical performance without\ndegrading its theoretical guarantees and demonstrate their benefit\nexperimentally. Using a case study, we also illustrate ways of exploring the\nrobustness of a given solution to the algorithm selection problem to variations\nin the utility function.", "AI": {"tldr": "COUP\u662f\u4e00\u4e2a\u57fa\u4e8e\u6548\u7528\u7684\u7b97\u6cd5\u914d\u7f6e\u7a0b\u5e8f\uff0c\u672c\u6587\u901a\u8fc7\u4e00\u7cfb\u5217\u6539\u8fdb\u4f7f\u5176\u5728\u4fdd\u6301\u7406\u8bba\u4fdd\u8bc1\u7684\u540c\u65f6\uff0c\u5b9e\u9645\u6027\u80fd\u8fbe\u5230\u4e0e\u5e7f\u6cdb\u4f7f\u7528\u7684\u542f\u53d1\u5f0f\u914d\u7f6e\u65b9\u6cd5\u76f8\u5f53\u7684\u6c34\u5e73\u3002", "motivation": "COUP\u7b97\u6cd5\u914d\u7f6e\u7a0b\u5e8f\u4e3b\u8981\u5173\u6ce8\u7406\u8bba\u4fdd\u8bc1\uff0c\u4f46\u5b9e\u9645\u6027\u80fd\u8868\u73b0\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u4f7f\u57fa\u4e8e\u6548\u7528\u7684\u7b97\u6cd5\u914d\u7f6e\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u7ade\u4e89\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u5bf9COUP\u7684\u6539\u8fdb\u63aa\u65bd\uff0c\u5728\u4e0d\u964d\u4f4e\u7406\u8bba\u4fdd\u8bc1\u7684\u524d\u63d0\u4e0b\u63d0\u5347\u5176\u7ecf\u9a8c\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u8fd9\u4e9b\u6539\u8fdb\u7684\u6548\u679c\u3002", "result": "\u6539\u8fdb\u540e\u7684COUP\u5728\u4fdd\u6301\u7406\u8bba\u4fdd\u8bc1\u7684\u540c\u65f6\uff0c\u5b9e\u9645\u6027\u80fd\u8fbe\u5230\u4e0e\u65e0\u6027\u80fd\u4fdd\u8bc1\u7684\u542f\u53d1\u5f0f\u914d\u7f6e\u7a0b\u5e8f\u76f8\u5f53\u7684\u6c34\u5e73\u3002", "conclusion": "\u901a\u8fc7\u6539\u8fdbCOUP\uff0c\u6210\u529f\u5c06\u57fa\u4e8e\u7406\u8bba\u7684\u6548\u7528\u7b97\u6cd5\u914d\u7f6e\u63d0\u5347\u5230\u5b9e\u7528\u6c34\u5e73\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u5982\u4f55\u8bc4\u4f30\u7b97\u6cd5\u9009\u62e9\u89e3\u51b3\u65b9\u6848\u5bf9\u6548\u7528\u51fd\u6570\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.14697", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14697", "abs": "https://arxiv.org/abs/2510.14697", "authors": ["Bang An", "Yibo Yang", "Philip Torr", "Bernard Ghanem"], "title": "Purifying Task Vectors in Knowledge-Aware Subspace for Model Merging", "comment": null, "summary": "Model merging aims to integrate task-specific abilities from individually\nfine-tuned models into a single model without extra training. In recent model\nmerging methods, task vector has become a fundamental building block, as it can\nencapsulate the residual information from finetuning. However, the merged model\noften suffers from notable performance degradation due to the conflicts caused\nby task-irrelevant redundancy in task vectors. Existing efforts in overcoming\nredundancy by randomly dropping elements in the parameter space involves\nrandomness and lacks knowledge awareness. To address these challenges, in this\nstudy, we propose Purifying TAsk Vectors (PAVE) in knowledge-aware subspace.\nConcretely, we sample some training examples from each task, and feed them into\ntheir corresponding fine-tuned models to acquire the covariance matrices before\nlinear layers. We then perform a context-oriented singular value decomposition,\nwhich accentuates the weight components most relevant to the target knowledge.\nAs a result, we can split fine-tuned model weights into task-relevant and\nredundant components in the knowledge-aware subspace, and purify the task\nvector by pruning the redundant components. To induce fair pruning efforts\nacross models, we further introduce a spectral rank allocation strategy by\noptimizing a normalized activated pruning error. The task vector purification\nby our method as a plug-and-play scheme is applicable across various task\nvector-based merging methods to improve their performance. In experiments, we\ndemonstrate the effectiveness of PAVE across a diverse set of merging methods,\ntasks, and model architectures.", "AI": {"tldr": "\u63d0\u51fa\u4e86PAVE\u65b9\u6cd5\uff0c\u901a\u8fc7\u77e5\u8bc6\u611f\u77e5\u5b50\u7a7a\u95f4\u51c0\u5316\u4efb\u52a1\u5411\u91cf\uff0c\u6d88\u9664\u4efb\u52a1\u65e0\u5173\u5197\u4f59\uff0c\u63d0\u5347\u6a21\u578b\u5408\u5e76\u6027\u80fd", "motivation": "\u73b0\u6709\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\u4e2d\uff0c\u4efb\u52a1\u5411\u91cf\u5305\u542b\u4efb\u52a1\u65e0\u5173\u5197\u4f59\uff0c\u5bfc\u81f4\u5408\u5e76\u6a21\u578b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u800c\u73b0\u6709\u53bb\u5197\u4f59\u65b9\u6cd5\u7f3a\u4e4f\u77e5\u8bc6\u611f\u77e5\u4e14\u6d89\u53ca\u968f\u673a\u6027", "method": "\u5728\u77e5\u8bc6\u611f\u77e5\u5b50\u7a7a\u95f4\u4e2d\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5bfc\u5411\u7684\u5947\u5f02\u503c\u5206\u89e3\u8bc6\u522b\u4efb\u52a1\u76f8\u5173\u6743\u91cd\u5206\u91cf\uff0c\u5e76\u5f15\u5165\u8c31\u79e9\u5206\u914d\u7b56\u7565\u8fdb\u884c\u516c\u5e73\u526a\u679d", "result": "PAVE\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u65b9\u6848\uff0c\u5728\u5404\u79cd\u4efb\u52a1\u5411\u91cf\u5408\u5e76\u65b9\u6cd5\u4e2d\u5747\u80fd\u6709\u6548\u63d0\u5347\u6027\u80fd", "conclusion": "PAVE\u65b9\u6cd5\u80fd\u6709\u6548\u51c0\u5316\u4efb\u52a1\u5411\u91cf\uff0c\u6d88\u9664\u5197\u4f59\u51b2\u7a81\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u5408\u5e76\u6027\u80fd"}}
{"id": "2509.26255", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.26255", "abs": "https://arxiv.org/abs/2509.26255", "authors": ["Yichao Liang", "Dat Nguyen", "Cambridge Yang", "Tianyang Li", "Joshua B. Tenenbaum", "Carl Edward Rasmussen", "Adrian Weller", "Zenna Tavares", "Tom Silver", "Kevin Ellis"], "title": "ExoPredicator: Learning Abstract Models of Dynamic Worlds for Robot Planning", "comment": "41 pages. The last two authors contributed equally in co-advising", "summary": "Long-horizon embodied planning is challenging because the world does not only\nchange through an agent's actions: exogenous processes (e.g., water heating,\ndominoes cascading) unfold concurrently with the agent's actions. We propose a\nframework for abstract world models that jointly learns (i) symbolic state\nrepresentations and (ii) causal processes for both endogenous actions and\nexogenous mechanisms. Each causal process models the time course of a\nstochastic cause-effect relation. We learn these world models from limited data\nvia variational Bayesian inference combined with LLM proposals. Across five\nsimulated tabletop robotics environments, the learned models enable fast\nplanning that generalizes to held-out tasks with more objects and more complex\ngoals, outperforming a range of baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u62bd\u8c61\u4e16\u754c\u6a21\u578b\u6846\u67b6\uff0c\u8054\u5408\u5b66\u4e60\u7b26\u53f7\u72b6\u6001\u8868\u793a\u548c\u56e0\u679c\u8fc7\u7a0b\uff08\u5305\u62ec\u5185\u751f\u52a8\u4f5c\u548c\u5916\u751f\u673a\u5236\uff09\uff0c\u901a\u8fc7\u53d8\u5206\u8d1d\u53f6\u65af\u63a8\u7406\u548cLLM\u63d0\u8bae\u4ece\u6709\u9650\u6570\u636e\u4e2d\u5b66\u4e60\uff0c\u5728\u6a21\u62df\u684c\u9762\u673a\u5668\u4eba\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u5feb\u901f\u89c4\u5212\u5e76\u6cdb\u5316\u5230\u66f4\u590d\u6742\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u957f\u89c6\u91ce\u5177\u8eab\u89c4\u5212\u4e2d\u7684\u6311\u6218\uff0c\u56e0\u4e3a\u4e16\u754c\u4e0d\u4ec5\u901a\u8fc7\u667a\u80fd\u4f53\u52a8\u4f5c\u6539\u53d8\uff0c\u5916\u751f\u8fc7\u7a0b\uff08\u5982\u6c34\u6e29\u52a0\u70ed\u3001\u591a\u7c73\u8bfa\u9aa8\u724c\u8fde\u9501\u53cd\u5e94\uff09\u4e5f\u4f1a\u4e0e\u667a\u80fd\u4f53\u52a8\u4f5c\u540c\u65f6\u5c55\u5f00\u3002", "method": "\u63d0\u51fa\u62bd\u8c61\u4e16\u754c\u6a21\u578b\u6846\u67b6\uff0c\u8054\u5408\u5b66\u4e60\u7b26\u53f7\u72b6\u6001\u8868\u793a\u548c\u56e0\u679c\u8fc7\u7a0b\uff08\u5185\u751f\u52a8\u4f5c\u548c\u5916\u751f\u673a\u5236\uff09\uff0c\u6bcf\u4e2a\u56e0\u679c\u8fc7\u7a0b\u5efa\u6a21\u968f\u673a\u56e0\u679c\u5173\u7cfb\u7684\u65f6\u5e8f\u8fc7\u7a0b\uff0c\u901a\u8fc7\u53d8\u5206\u8d1d\u53f6\u65af\u63a8\u7406\u7ed3\u5408LLM\u63d0\u8bae\u4ece\u6709\u9650\u6570\u636e\u4e2d\u5b66\u4e60\u3002", "result": "\u5728\u4e94\u4e2a\u6a21\u62df\u684c\u9762\u673a\u5668\u4eba\u73af\u5883\u4e2d\uff0c\u5b66\u4e60\u5230\u7684\u6a21\u578b\u5b9e\u73b0\u4e86\u5feb\u901f\u89c4\u5212\uff0c\u80fd\u591f\u6cdb\u5316\u5230\u5177\u6709\u66f4\u591a\u5bf9\u8c61\u548c\u66f4\u590d\u6742\u76ee\u6807\u7684\u4fdd\u7559\u4efb\u52a1\uff0c\u4f18\u4e8e\u4e00\u7cfb\u5217\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u957f\u89c6\u91ce\u5177\u8eab\u89c4\u5212\u4e2d\u5904\u7406\u5916\u751f\u8fc7\u7a0b\u7684\u95ee\u9898\uff0c\u5b66\u4e60\u5230\u7684\u62bd\u8c61\u4e16\u754c\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u89c4\u5212\u6027\u80fd\u3002"}}
{"id": "2510.14702", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14702", "abs": "https://arxiv.org/abs/2510.14702", "authors": ["Penglong Zhai", "Jie Li", "Fanyi Di", "Yue Liu", "Yifang Yuan", "Jie Huang", "Peng Wu", "Sicong Wang", "Mingyang Yin", "Tingting Hu", "Yao Xu", "Xin Li"], "title": "Cognitive-Aligned Spatio-Temporal Large Language Models For Next Point-of-Interest Prediction", "comment": "12 pages, 5 figures", "summary": "The next point-of-interest (POI) recommendation task aims to predict the\nusers' immediate next destinations based on their preferences and historical\ncheck-ins, holding significant value in location-based services. Recently,\nlarge language models (LLMs) have shown great potential in recommender systems,\nwhich treat the next POI prediction in a generative manner. However, these\nLLMs, pretrained primarily on vast corpora of unstructured text, lack the\nnative understanding of structured geographical entities and sequential\nmobility patterns required for next POI prediction tasks. Moreover, in\nindustrial-scale POI prediction applications, incorporating world knowledge and\nalignment of human cognition, such as seasons, weather conditions, holidays,\nand users' profiles (such as habits, occupation, and preferences), can enhance\nthe user experience while improving recommendation performance. To address\nthese issues, we propose CoAST (Cognitive-Aligned Spatial-Temporal LLMs), a\nframework employing natural language as an interface, allowing for the\nincorporation of world knowledge, spatio-temporal trajectory patterns,\nprofiles, and situational information. Specifically, CoAST mainly comprises of\n2 stages: (1) Recommendation Knowledge Acquisition through continued\npretraining on the enriched spatial-temporal trajectory data of the\ndesensitized users; (2) Cognitive Alignment to align cognitive judgments with\nhuman preferences using enriched training data through Supervised Fine-Tuning\n(SFT) and a subsequent Reinforcement Learning (RL) phase. Extensive offline\nexperiments on various real-world datasets and online experiments deployed in\n\"Guess Where You Go\" of AMAP App homepage demonstrate the effectiveness of\nCoAST.", "AI": {"tldr": "CoAST\u662f\u4e00\u4e2a\u8ba4\u77e5\u5bf9\u9f50\u7684\u65f6\u7a7a\u5927\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u754c\u9762\u6574\u5408\u4e16\u754c\u77e5\u8bc6\u3001\u65f6\u7a7a\u8f68\u8ff9\u6a21\u5f0f\u3001\u7528\u6237\u753b\u50cf\u548c\u60c5\u5883\u4fe1\u606f\uff0c\u7528\u4e8e\u4e0b\u4e00\u4e2a\u5174\u8da3\u70b9\u63a8\u8350\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u57fa\u4e8e\u975e\u7ed3\u6784\u5316\u6587\u672c\u9884\u8bad\u7ec3\uff0c\u7f3a\u4e4f\u5bf9\u7ed3\u6784\u5316\u5730\u7406\u5b9e\u4f53\u548c\u5e8f\u5217\u79fb\u52a8\u6a21\u5f0f\u7684\u7406\u89e3\uff0c\u4e14\u9700\u8981\u878d\u5165\u4e16\u754c\u77e5\u8bc6\u548c\u4eba\u7c7b\u8ba4\u77e5\u5bf9\u9f50\u6765\u63d0\u5347\u63a8\u8350\u6027\u80fd\u3002", "method": "CoAST\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a(1) \u63a8\u8350\u77e5\u8bc6\u83b7\u53d6\uff1a\u901a\u8fc7\u7ee7\u7eed\u9884\u8bad\u7ec3\u4e30\u5bcc\u7684\u65f6\u7a7a\u8f68\u8ff9\u6570\u636e\uff1b(2) \u8ba4\u77e5\u5bf9\u9f50\uff1a\u4f7f\u7528\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u5c06\u8ba4\u77e5\u5224\u65ad\u4e0e\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u79bb\u7ebf\u5b9e\u9a8c\u548c\u5728AMAP App\u9996\u9875\"\u731c\u4f60\u53bb\u54ea\"\u7684\u5728\u7ebf\u5b9e\u9a8c\u8bc1\u660e\u4e86CoAST\u7684\u6709\u6548\u6027\u3002", "conclusion": "CoAST\u6846\u67b6\u80fd\u591f\u6709\u6548\u6574\u5408\u591a\u6e90\u4fe1\u606f\uff0c\u63d0\u5347\u4e0b\u4e00\u4e2a\u5174\u8da3\u70b9\u63a8\u8350\u7684\u51c6\u786e\u6027\u548c\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2510.14828", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14828", "abs": "https://arxiv.org/abs/2510.14828", "authors": ["Jinrui Liu", "Bingyan Nie", "Boyu Li", "Yaran Chen", "Yuze Wang", "Shunsen He", "Haoran Li"], "title": "RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning", "comment": null, "summary": "Improving the reasoning capabilities of embodied agents is crucial for robots\nto complete complex human instructions in long-view manipulation tasks\nsuccessfully. Despite the success of large language models and vision language\nmodels based on Supervised Fine-Tuning (SFT) in planning tasks, they continue\nfacing challenges in performing long-horizon manipulation tasks in complex\nreal-world environments, owing to their restricted common sense and reasoning\ncapabilities. Considering that aligning general-purpose vision language models\nto robotic planning tasks via supervised fine-tuning suffers from poor\ngeneralization and insufficient physical understanding, we propose RoboGPT-R1,\na two-stage fine-tuning framework for embodied planning. In this framework,\nsupervised training acquires foundational knowledge through expert sequences,\nfollowed by RL to address the model's shortcomings in visual-spatial\nunderstanding and reasoning. To achieve physical understanding and action\nsequence consistency in multi-step reasoning tasks, we design a rule-based\nreward function that simultaneously considers long-horizon performance and\naction constraint in the environment. The reasoning model, trained on\nQwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini,\nby 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on the\nEmbodiedBench benchmark.", "AI": {"tldr": "\u63d0\u51fa\u4e86RoboGPT-R1\uff0c\u4e00\u4e2a\u7528\u4e8e\u5177\u8eab\u89c4\u5212\u7684\u4e24\u9636\u6bb5\u5fae\u8c03\u6846\u67b6\uff0c\u7ed3\u5408\u76d1\u7763\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u6765\u63d0\u5347\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u957f\u89c6\u91ce\u64cd\u4f5c\u4efb\u52a1\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c4\u5212\u4efb\u52a1\u4e2d\u53d6\u5f97\u6210\u529f\uff0c\u4f46\u5728\u590d\u6742\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u6267\u884c\u957f\u89c6\u91ce\u64cd\u4f5c\u4efb\u52a1\u65f6\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u5e38\u8bc6\u548c\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u5fae\u8c03\u6846\u67b6\uff1a\u76d1\u7763\u8bad\u7ec3\u901a\u8fc7\u4e13\u5bb6\u5e8f\u5217\u83b7\u53d6\u57fa\u7840\u77e5\u8bc6\uff0c\u7136\u540e\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u6a21\u578b\u5728\u89c6\u89c9\u7a7a\u95f4\u7406\u89e3\u548c\u63a8\u7406\u65b9\u9762\u7684\u4e0d\u8db3\u3002\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u89c4\u5219\u7684\u5956\u52b1\u51fd\u6570\uff0c\u540c\u65f6\u8003\u8651\u957f\u89c6\u91ce\u6027\u80fd\u548c\u52a8\u4f5c\u7ea6\u675f\u3002", "result": "\u5728EmbodiedBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u57fa\u4e8eQwen2.5-VL-3B\u8bad\u7ec3\u5f97\u5230\u7684\u63a8\u7406\u6a21\u578b\u663e\u8457\u4f18\u4e8eGPT-4o-mini 21.33%\uff0c\u5e76\u8d85\u8fc7\u5176\u4ed6\u57fa\u4e8eQwen2.5-VL-7B\u7684\u5de5\u4f5c20.33%\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e24\u9636\u6bb5\u5fae\u8c03\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u5177\u8eab\u4ee3\u7406\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5728\u957f\u89c6\u91ce\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u7ed3\u5408\u76d1\u7763\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.14703", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14703", "abs": "https://arxiv.org/abs/2510.14703", "authors": ["Jianghao Lin", "Yuanyuan Shi", "Xin Peng", "Renjie Ding", "Hairui Wang", "Yuxuan Peng", "Bizhe Bai", "Weixi Song", "Fengshuo Bai", "Huacan Chai", "Weinan Zhang", "Fei Huang", "Ying Wen"], "title": "ToolPRM: Fine-Grained Inference Scaling of Structured Outputs for Function Calling", "comment": null, "summary": "Large language models (LLMs) are increasingly demonstrating strong\ncapabilities as autonomous agents, with function calling serving as a core\nmechanism for interaction with the environment. Meanwhile, inference scaling\nhas become a cutting-edge technique to enhance LLM performance by allocating\nmore computational resources during the inference process. However, current\nresearch on inference scaling primarily focuses on unstructured output\ngeneration tasks, leaving its application in structured outputs, like function\ncalling, largely underexplored. To bridge this gap, we propose an inference\nscaling framework that combines fine-grained beam search with a process reward\nmodel, ToolPRM, which scores the internal steps of each single function call.\nTo train ToolPRM, we construct the first fine-grained intra-call process\nsupervision dataset, automatically annotated with function-masking techniques\nto provide step-level rewards for structured tool-use reasoning. Extensive\nexperiments demonstrate that ToolPRM beats the coarse-grained and outcome\nreward models in terms of predictive accuracy, indicating its stronger\ncapability in supervising the function calling inference process. Inference\nscaling technique equipped with ToolPRM also significantly improves the\nbackbone model performance across various function calling tasks and\nbenchmarks. More importantly, we reveal a key principle for applying inference\nscaling techniques to structured outputs: \"explore more but retain less\" due to\nthe unrecoverability characteristics of structured function calling generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u7ec6\u7c92\u5ea6\u6ce2\u675f\u641c\u7d22\u548c\u8fc7\u7a0b\u5956\u52b1\u6a21\u578bToolPRM\u7684\u63a8\u7406\u6269\u5c55\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7ed3\u6784\u5316\u8f93\u51fa\uff08\u5982\u51fd\u6570\u8c03\u7528\uff09\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u63a8\u7406\u6269\u5c55\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u975e\u7ed3\u6784\u5316\u8f93\u51fa\u751f\u6210\u4efb\u52a1\uff0c\u800c\u5728\u7ed3\u6784\u5316\u8f93\u51fa\uff08\u5982\u51fd\u6570\u8c03\u7528\uff09\u65b9\u9762\u7684\u5e94\u7528\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u4e86\u9996\u4e2a\u7ec6\u7c92\u5ea6\u8c03\u7528\u5185\u8fc7\u7a0b\u76d1\u7763\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u51fd\u6570\u63a9\u7801\u6280\u672f\u81ea\u52a8\u6807\u6ce8\u6b65\u9aa4\u7ea7\u5956\u52b1\uff0c\u8bad\u7ec3ToolPRM\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u6765\u8bc4\u5206\u51fd\u6570\u8c03\u7528\u7684\u5185\u90e8\u6b65\u9aa4\u3002", "result": "ToolPRM\u5728\u9884\u6d4b\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u7c97\u7c92\u5ea6\u548c\u7ed3\u679c\u5956\u52b1\u6a21\u578b\uff0c\u914d\u5907ToolPRM\u7684\u63a8\u7406\u6269\u5c55\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u9aa8\u5e72\u6a21\u578b\u5728\u5404\u79cd\u51fd\u6570\u8c03\u7528\u4efb\u52a1\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6027\u80fd\u3002", "conclusion": "\u63ed\u793a\u4e86\u5c06\u63a8\u7406\u6269\u5c55\u6280\u672f\u5e94\u7528\u4e8e\u7ed3\u6784\u5316\u8f93\u51fa\u7684\u5173\u952e\u539f\u5219\uff1a\"\u591a\u63a2\u7d22\u5c11\u4fdd\u7559\"\uff0c\u8fd9\u662f\u7531\u7ed3\u6784\u5316\u51fd\u6570\u8c03\u7528\u751f\u6210\u7684\u4e0d\u53ef\u6062\u590d\u6027\u7279\u5f81\u51b3\u5b9a\u7684\u3002"}}
{"id": "2510.14807", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14807", "abs": "https://arxiv.org/abs/2510.14807", "authors": ["Ruotian Peng", "Yi Ren", "Zhouliang Yu", "Weiyang Liu", "Yandong Wen"], "title": "SimKO: Simple Pass@K Policy Optimization", "comment": "Technical report (20 pages, 10 figures, project page:\n  https://spherelab.ai/simko/)", "summary": "Reinforcement learning with verifiable rewards (RLVR) has advanced the\nreasoning capabilities of large language models (LLMs). However, prevailing\nRLVR methods exhibit a systematic bias toward exploitation over exploration, as\nevidenced by improved pass@1 but reduced pass@K (K>1) performance. To\nunderstand this issue, we analyze training dynamics of RLVR methods by tracking\nthe token-level probability distributions over vocabulary candidates. Our\nanalysis reveals a consistent probability concentration effect where the top-1\ncandidate increasingly accumulates probability mass and suppresses that of\nother candidates. More importantly, stronger over-concentration correlates with\nworse pass@K performance. Inspired by this finding, we propose Simple Pass@K\nOptimization (SimKO), a method designed to mitigate the over-concentration\nissue, thereby encouraging exploration. SimKO operates in an asymmetrical\nmanner. For verified-correct responses, it boosts the probabilities of the\ntop-K candidates. For verified-incorrect responses, it applies stronger\npenalties to the top-1 candidate. We observe that this asymmetric design is\nparticularly effective at mitigating over-concentration when applied at tokens\nwith high entropy. Across various math and logical-reasoning benchmarks, SimKO\nconsistently yields higher pass@K for a wide range of K, providing a simple way\nto improve RLVR's exploration.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSimKO\u65b9\u6cd5\u89e3\u51b3RLVR\u4e2d\u7684\u8fc7\u5ea6\u96c6\u4e2d\u95ee\u9898\uff0c\u901a\u8fc7\u4e0d\u5bf9\u79f0\u8bbe\u8ba1\u63d0\u5347pass@K\u6027\u80fd", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u5411\u5229\u7528\u800c\u975e\u63a2\u7d22\u7684\u95ee\u9898\uff0c\u8868\u73b0\u4e3apass@1\u63d0\u5347\u4f46pass@K(K>1)\u4e0b\u964d", "method": "\u63d0\u51faSimKO\u65b9\u6cd5\uff1a\u5bf9\u5df2\u9a8c\u8bc1\u6b63\u786e\u54cd\u5e94\u63d0\u5347top-K\u5019\u9009\u6982\u7387\uff0c\u5bf9\u9519\u8bef\u54cd\u5e94\u60e9\u7f5atop-1\u5019\u9009\uff0c\u7279\u522b\u5728\u9ad8\u71b5token\u4e0a\u5e94\u7528", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u548c\u903b\u8f91\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSimKO\u4e00\u81f4\u6027\u5730\u63d0\u9ad8\u4e86\u5404\u79cdK\u503c\u7684pass@K\u6027\u80fd", "conclusion": "SimKO\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\u6765\u6539\u5584RLVR\u7684\u63a2\u7d22\u80fd\u529b\uff0c\u7f13\u89e3\u6982\u7387\u8fc7\u5ea6\u96c6\u4e2d\u95ee\u9898"}}
{"id": "2510.14808", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14808", "abs": "https://arxiv.org/abs/2510.14808", "authors": ["Dominik Jehle", "Lennart Purucker", "Frank Hutter"], "title": "Agentic NL2SQL to Reduce Computational Costs", "comment": "Accepted at the NeurIPS 2025 Workshop on Efficient Reasoning. 10\n  pages, 11 figures", "summary": "Translating natural language queries into SQL queries (NL2SQL or Text-to-SQL)\nhas recently been empowered by large language models (LLMs). Using LLMs to\nperform NL2SQL methods on a large collection of SQL databases necessitates\nprocessing large quantities of meta-information about the databases, which in\nturn results in lengthy prompts with many tokens and high processing costs. To\naddress this challenge, we introduce Datalake Agent, an agentic system designed\nto enable an LLM to solve NL2SQL tasks more efficiently. Instead of utilizing\ndirect solvers for NL2SQL that call the LLM once with all meta-information in\nthe prompt, the Datalake Agent employs an interactive loop to reduce the\nutilized meta-information. Within the loop, the LLM is used in a reasoning\nframework that selectively requests only the necessary information to solve a\ntable question answering task. We evaluate the Datalake Agent on a collection\nof 23 databases with 100 table question answering tasks. The Datalake Agent\nreduces the tokens used by the LLM by up to 87\\% and thus allows for\nsubstantial cost reductions while maintaining competitive performance.", "AI": {"tldr": "Datalake Agent\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u4ea4\u4e92\u5f0f\u5faa\u73af\u9009\u62e9\u6027\u83b7\u53d6\u5fc5\u8981\u5143\u4fe1\u606f\uff0c\u663e\u8457\u51cf\u5c11NL2SQL\u4efb\u52a1\u4e2d\u7684token\u4f7f\u7528\u91cf\uff0c\u964d\u4f4e\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfNL2SQL\u65b9\u6cd5\u9700\u8981\u5904\u7406\u5927\u91cf\u6570\u636e\u5e93\u5143\u4fe1\u606f\uff0c\u5bfc\u81f4\u63d0\u793a\u8fc7\u957f\u3001token\u6d88\u8017\u5927\u3001\u5904\u7406\u6210\u672c\u9ad8\u3002", "method": "\u91c7\u7528\u4ee3\u7406\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u4f7f\u7528\u4ea4\u4e92\u5f0f\u5faa\u73af\u548c\u63a8\u7406\u6846\u67b6\uff0c\u8ba9LLM\u9009\u62e9\u6027\u8bf7\u6c42\u5fc5\u8981\u4fe1\u606f\u6765\u89e3\u51b3\u8868\u683c\u95ee\u7b54\u4efb\u52a1\uff0c\u800c\u4e0d\u662f\u4e00\u6b21\u6027\u63d0\u4f9b\u6240\u6709\u5143\u4fe1\u606f\u3002", "result": "\u572823\u4e2a\u6570\u636e\u5e93\u548c100\u4e2a\u8868\u683c\u95ee\u7b54\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cDatalake Agent\u5c06LLM\u4f7f\u7528\u7684token\u51cf\u5c11\u4e86\u9ad8\u8fbe87%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7ade\u4e89\u529b\u6027\u80fd\u3002", "conclusion": "Datalake Agent\u901a\u8fc7\u9009\u62e9\u6027\u4fe1\u606f\u83b7\u53d6\u7b56\u7565\uff0c\u5728\u663e\u8457\u964d\u4f4e\u5904\u7406\u6210\u672c\u7684\u540c\u65f6\uff0c\u7ef4\u6301\u4e86NL2SQL\u4efb\u52a1\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2510.14842", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14842", "abs": "https://arxiv.org/abs/2510.14842", "authors": ["Ben Elder", "Evelyn Duesterwald", "Vinod Muthusamy"], "title": "Boosting Instruction Following at Scale", "comment": "6+4 pages, 7 figures, 2 tables", "summary": "A typical approach developers follow to influence an LLM's behavior in an\napplication is through careful manipulation of the prompt, such as by adding or\nmodifying instructions. However, merely adding more instructions provides\nlittle assurance that they will actually be followed. We introduce Instruction\nBoosting as a post-generation method to increase the reliability of LLM prompt\ninstructions. We show that Instruction Boosting improves the instruction\nfollowing rate by up to 7 points for two instructions and up to 4 points for\nten instructions. To demonstrate these results we introduce SCALEDIF, a\nbenchmark with a scaled instruction volume of up to ten instructions per data\nsample. We also present an analysis of the commonly observed trend that\nperformance degrades as more instructions are added. We show that an important\nfactor contributing to this trend is the degree of tension and conflict that\narises as the number of instructions is increased. We contribute a quantitative\nconflict scoring tool that explains the observed performance trends and\nprovides feedback to developers on the impact that additional prompt\ninstructions have on a model's performance.", "AI": {"tldr": "Instruction Boosting\u662f\u4e00\u79cd\u540e\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u52a0\u6307\u4ee4\u9075\u5faa\u7387\u6765\u63d0\u9ad8LLM\u63d0\u793a\u6307\u4ee4\u7684\u53ef\u9760\u6027\uff0c\u57282\u6761\u6307\u4ee4\u65f6\u63d0\u53477\u4e2a\u767e\u5206\u70b9\uff0c10\u6761\u6307\u4ee4\u65f6\u63d0\u53474\u4e2a\u767e\u5206\u70b9\u3002", "motivation": "\u5f00\u53d1\u4eba\u5458\u901a\u5e38\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u63d0\u793a\u6765\u5f71\u54cdLLM\u884c\u4e3a\uff0c\u4f46\u4ec5\u6dfb\u52a0\u66f4\u591a\u6307\u4ee4\u65e0\u6cd5\u4fdd\u8bc1\u5b83\u4eec\u4f1a\u88ab\u9075\u5faa\u3002", "method": "\u5f15\u5165Instruction Boosting\u4f5c\u4e3a\u540e\u751f\u6210\u65b9\u6cd5\uff0c\u5e76\u521b\u5efaSCALEDIF\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u6700\u591a10\u6761\u6307\u4ee4\u7684\u6570\u636e\u6837\u672c\u3002", "result": "Instruction Boosting\u663e\u8457\u63d0\u9ad8\u4e86\u6307\u4ee4\u9075\u5faa\u7387\uff0c\u540c\u65f6\u53d1\u73b0\u6307\u4ee4\u6570\u91cf\u589e\u52a0\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u7684\u4e3b\u8981\u539f\u56e0\u662f\u6307\u4ee4\u95f4\u7684\u51b2\u7a81\u548c\u7d27\u5f20\u5173\u7cfb\u3002", "conclusion": "\u63d0\u51fa\u4e86\u5b9a\u91cf\u51b2\u7a81\u8bc4\u5206\u5de5\u5177\uff0c\u53ef\u4ee5\u89e3\u91ca\u6027\u80fd\u8d8b\u52bf\u5e76\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u5173\u4e8e\u989d\u5916\u63d0\u793a\u6307\u4ee4\u5bf9\u6a21\u578b\u6027\u80fd\u5f71\u54cd\u7684\u53cd\u9988\u3002"}}
{"id": "2510.14846", "categories": ["cs.AI", "cs.CL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.14846", "abs": "https://arxiv.org/abs/2510.14846", "authors": ["Zhuo-Yang Song"], "title": "Where to Search: Measure the Prior-Structured Search Space of LLM Agents", "comment": "10 pages, 2 figures, 1 table", "summary": "The generate-filter-refine (iterative paradigm) based on large language\nmodels (LLMs) has achieved progress in reasoning, programming, and program\ndiscovery in AI+Science. However, the effectiveness of search depends on where\nto search, namely, how to encode the domain prior into an operationally\nstructured hypothesis space. To this end, this paper proposes a compact formal\ntheory that describes and measures LLM-assisted iterative search guided by\ndomain priors. We represent an agent as a fuzzy relation operator on inputs and\noutputs to capture feasible transitions; the agent is thereby constrained by a\nfixed safety envelope. To describe multi-step reasoning/search, we weight all\nreachable paths by a single continuation parameter and sum them to obtain a\ncoverage generating function; this induces a measure of reachability\ndifficulty; and it provides a geometric interpretation of search on the graph\ninduced by the safety envelope. We further provide the simplest testable\ninferences and validate them via a majority-vote instantiation. This theory\noffers a workable language and operational tools to measure agents and their\nsearch spaces, proposing a systematic formal description of iterative search\nconstructed by LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7d27\u51d1\u7684\u5f62\u5f0f\u7406\u8bba\u6765\u63cf\u8ff0\u548c\u8861\u91cf\u7531\u9886\u57df\u5148\u9a8c\u5f15\u5bfc\u7684LLM\u8f85\u52a9\u8fed\u4ee3\u641c\u7d22\uff0c\u901a\u8fc7\u6a21\u7cca\u5173\u7cfb\u7b97\u5b50\u8868\u793a\u667a\u80fd\u4f53\uff0c\u5f15\u5165\u8986\u76d6\u751f\u6210\u51fd\u6570\u6765\u8861\u91cf\u53ef\u8fbe\u6027\u96be\u5ea6\u3002", "motivation": "\u57fa\u4e8eLLM\u7684\u751f\u6210-\u8fc7\u6ee4-\u7cbe\u70bc\u8fed\u4ee3\u8303\u5f0f\u5728\u63a8\u7406\u3001\u7f16\u7a0b\u548c\u79d1\u5b66\u53d1\u73b0\u4e2d\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u641c\u7d22\u6548\u679c\u53d6\u51b3\u4e8e\u5982\u4f55\u5c06\u9886\u57df\u5148\u9a8c\u7f16\u7801\u4e3a\u64cd\u4f5c\u5316\u7ed3\u6784\u5316\u7684\u5047\u8bbe\u7a7a\u95f4\u3002", "method": "\u5c06\u667a\u80fd\u4f53\u8868\u793a\u4e3a\u8f93\u5165\u8f93\u51fa\u7684\u6a21\u7cca\u5173\u7cfb\u7b97\u5b50\uff0c\u7528\u5355\u4e00\u5ef6\u7eed\u53c2\u6570\u52a0\u6743\u6240\u6709\u53ef\u8fbe\u8def\u5f84\u5f97\u5230\u8986\u76d6\u751f\u6210\u51fd\u6570\uff0c\u4ece\u800c\u8861\u91cf\u53ef\u8fbe\u6027\u96be\u5ea6\u5e76\u63d0\u4f9b\u641c\u7d22\u7684\u51e0\u4f55\u89e3\u91ca\u3002", "result": "\u63d0\u4f9b\u4e86\u6700\u7b80\u5355\u7684\u53ef\u6d4b\u8bd5\u63a8\u65ad\uff0c\u5e76\u901a\u8fc7\u591a\u6570\u6295\u7968\u5b9e\u4f8b\u8fdb\u884c\u9a8c\u8bc1\uff0c\u4e3a\u8861\u91cf\u667a\u80fd\u4f53\u53ca\u5176\u641c\u7d22\u7a7a\u95f4\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u8bed\u8a00\u548c\u64cd\u4f5c\u5de5\u5177\u3002", "conclusion": "\u8be5\u7406\u8bba\u4e3aLLM\u6784\u5efa\u7684\u8fed\u4ee3\u641c\u7d22\u63d0\u4f9b\u4e86\u7cfb\u7edf\u7684\u5f62\u5f0f\u5316\u63cf\u8ff0\u6846\u67b6\uff0c\u80fd\u591f\u64cd\u4f5c\u5316\u5730\u8861\u91cf\u667a\u80fd\u4f53\u548c\u641c\u7d22\u7a7a\u95f4\u3002"}}
{"id": "2510.14861", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14861", "abs": "https://arxiv.org/abs/2510.14861", "authors": ["Le Cong", "Zaixi Zhang", "Xiaotong Wang", "Yin Di", "Ruofan Jin", "Michal Gerasimiuk", "Yinkai Wang", "Ravi K. Dinesh", "David Smerkous", "Alex Smerkous", "Xuekun Wu", "Shilong Liu", "Peishan Li", "Yi Zhu", "Simran Serrao", "Ning Zhao", "Imran A. Mohammad", "John B. Sunwoo", "Joseph C. Wu", "Mengdi Wang"], "title": "LabOS: The AI-XR Co-Scientist That Sees and Works With Humans", "comment": null, "summary": "Modern science advances fastest when thought meets action. LabOS represents\nthe first AI co-scientist that unites computational reasoning with physical\nexperimentation through multimodal perception, self-evolving agents, and\nEntended-Reality(XR)-enabled human-AI collaboration. By connecting multi-model\nAI agents, smart glasses, and human-AI collaboration, LabOS allows AI to see\nwhat scientists see, understand experimental context, and assist in real-time\nexecution. Across applications--from cancer immunotherapy target discovery to\nstem-cell engineering -- LabOS shows that AI can move beyond computational\ndesign to participation, turning the laboratory into an intelligent,\ncollaborative environment where human and machine discovery evolve together.", "AI": {"tldr": "LabOS\u662f\u9996\u4e2a\u5c06\u8ba1\u7b97\u63a8\u7406\u4e0e\u7269\u7406\u5b9e\u9a8c\u76f8\u7ed3\u5408\u7684AI\u5171\u540c\u79d1\u5b66\u5bb6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u611f\u77e5\u3001\u81ea\u8fdb\u5316\u4ee3\u7406\u548c\u6269\u5c55\u73b0\u5b9e(XR)\u652f\u6301\u7684\u4eba\u673a\u534f\u4f5c\uff0c\u4f7fAI\u80fd\u591f\u53c2\u4e0e\u5b9e\u9a8c\u5ba4\u7684\u5b9e\u9645\u64cd\u4f5c\u3002", "motivation": "\u73b0\u4ee3\u79d1\u5b66\u53d1\u5c55\u9700\u8981\u601d\u60f3\u4e0e\u884c\u52a8\u7684\u7ed3\u5408\uff0c\u5f53\u524dAI\u4e3b\u8981\u505c\u7559\u5728\u8ba1\u7b97\u8bbe\u8ba1\u9636\u6bb5\uff0c\u7f3a\u4e4f\u4e0e\u7269\u7406\u5b9e\u9a8c\u7684\u6df1\u5ea6\u878d\u5408\u3002LabOS\u65e8\u5728\u8ba9AI\u80fd\u591f\u771f\u6b63\u53c2\u4e0e\u5b9e\u9a8c\u5ba4\u5de5\u4f5c\uff0c\u7406\u89e3\u5b9e\u9a8c\u60c5\u5883\u5e76\u5b9e\u65f6\u534f\u52a9\u6267\u884c\u3002", "method": "\u901a\u8fc7\u8fde\u63a5\u591a\u6a21\u578bAI\u4ee3\u7406\u3001\u667a\u80fd\u773c\u955c\u548c\u4eba\u673a\u534f\u4f5c\u7cfb\u7edf\uff0cLabOS\u4f7fAI\u80fd\u591f\u770b\u5230\u79d1\u5b66\u5bb6\u6240\u89c1\u3001\u7406\u89e3\u5b9e\u9a8c\u80cc\u666f\uff0c\u5e76\u5728\u5b9e\u65f6\u6267\u884c\u4e2d\u63d0\u4f9b\u5e2e\u52a9\u3002\u7cfb\u7edf\u6574\u5408\u4e86\u591a\u6a21\u6001\u611f\u77e5\u3001\u81ea\u8fdb\u5316\u4ee3\u7406\u548cXR\u6280\u672f\u3002", "result": "\u5728\u764c\u75c7\u514d\u75ab\u6cbb\u7597\u9776\u70b9\u53d1\u73b0\u548c\u5e72\u7ec6\u80de\u5de5\u7a0b\u7b49\u5e94\u7528\u4e2d\uff0cLabOS\u5c55\u793a\u4e86AI\u80fd\u591f\u8d85\u8d8a\u8ba1\u7b97\u8bbe\u8ba1\uff0c\u771f\u6b63\u53c2\u4e0e\u5b9e\u9a8c\u8fc7\u7a0b\uff0c\u5c06\u5b9e\u9a8c\u5ba4\u8f6c\u53d8\u4e3a\u667a\u80fd\u534f\u4f5c\u73af\u5883\u3002", "conclusion": "LabOS\u8bc1\u660e\u4e86AI\u53ef\u4ee5\u4ece\u8ba1\u7b97\u8bbe\u8ba1\u8d70\u5411\u5b9e\u9645\u53c2\u4e0e\uff0c\u521b\u9020\u4e86\u4e00\u4e2a\u4eba\u7c7b\u4e0e\u673a\u5668\u53d1\u73b0\u5171\u540c\u8fdb\u5316\u7684\u667a\u80fd\u534f\u4f5c\u5b9e\u9a8c\u5ba4\u73af\u5883\u3002"}}
{"id": "2510.14881", "categories": ["cs.AI", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.14881", "abs": "https://arxiv.org/abs/2510.14881", "authors": ["Fikresilase Wondmeneh Abebayew"], "title": "The Gatekeeper Knows Enough", "comment": "7 pages, 1 figure", "summary": "Large Language Models (LLMs) are increasingly deployed as autonomous agents,\nyet their practical utility is fundamentally constrained by a limited context\nwindow and state desynchronization resulting from the LLMs' stateless nature\nand inefficient context management. These limitations lead to unreliable\noutput, unpredictable behavior, and inefficient resource usage, particularly\nwhen interacting with large, structured, and sensitive knowledge systems such\nas codebases and documents. To address these challenges, we introduce the\nGatekeeper Protocol, a novel, domain-agnostic framework that governs\nagent-system interactions. Our protocol mandates that the agent first operate\nand reason on a minimalist, low-fidelity \"latent state\" representation of the\nsystem to strategically request high-fidelity context on demand. All\ninteractions are mediated through a unified JSON format that serves as a\ndeclarative, state-synchronized protocol, ensuring the agent's model of the\nsystem remains verifiably grounded in the system's reality. We demonstrate the\nefficacy of this protocol with Sage, a reference implementation of the\nGatekeeper Protocol for software development. Our results show that this\napproach significantly increases agent reliability, improves computational\nefficiency by minimizing token consumption, and enables scalable interaction\nwith complex systems, creating a foundational methodology for building more\nrobust, predictable, and grounded AI agents for any structured knowledge\ndomain.", "AI": {"tldr": "\u63d0\u51fa\u4e86Gatekeeper Protocol\u6846\u67b6\uff0c\u901a\u8fc7\u4f4e\u4fdd\u771f\u6f5c\u5728\u72b6\u6001\u8868\u793a\u548c\u6309\u9700\u8bf7\u6c42\u9ad8\u4fdd\u771f\u4e0a\u4e0b\u6587\u7684\u673a\u5236\uff0c\u89e3\u51b3LLM\u4ee3\u7406\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u548c\u72b6\u6001\u4e0d\u540c\u6b65\u95ee\u9898\u3002", "motivation": "LLM\u4f5c\u4e3a\u81ea\u4e3b\u4ee3\u7406\u90e8\u7f72\u65f6\uff0c\u53d7\u9650\u4e8e\u6709\u9650\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u548c\u72b6\u6001\u4e0d\u540c\u6b65\u95ee\u9898\uff0c\u5bfc\u81f4\u8f93\u51fa\u4e0d\u53ef\u9760\u3001\u884c\u4e3a\u4e0d\u53ef\u9884\u6d4b\u548c\u8d44\u6e90\u4f7f\u7528\u6548\u7387\u4f4e\u4e0b\uff0c\u7279\u522b\u662f\u5728\u4e0e\u4ee3\u7801\u5e93\u548c\u6587\u6863\u7b49\u7ed3\u6784\u5316\u77e5\u8bc6\u7cfb\u7edf\u4ea4\u4e92\u65f6\u3002", "method": "\u8bbe\u8ba1Gatekeeper Protocol\u6846\u67b6\uff0c\u8ba9\u4ee3\u7406\u5148\u5728\u4f4e\u4fdd\u771f\u6f5c\u5728\u72b6\u6001\u8868\u793a\u4e0a\u8fdb\u884c\u64cd\u4f5c\u548c\u63a8\u7406\uff0c\u7136\u540e\u6309\u9700\u8bf7\u6c42\u9ad8\u4fdd\u771f\u4e0a\u4e0b\u6587\u3002\u6240\u6709\u4ea4\u4e92\u901a\u8fc7\u7edf\u4e00\u7684JSON\u683c\u5f0f\u8fdb\u884c\uff0c\u4f5c\u4e3a\u58f0\u660e\u5f0f\u3001\u72b6\u6001\u540c\u6b65\u7684\u534f\u8bae\u3002", "result": "\u5728\u8f6f\u4ef6\u5f00\u53d1\u7684Sage\u5b9e\u73b0\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u4ee3\u7406\u53ef\u9760\u6027\uff0c\u901a\u8fc7\u6700\u5c0f\u5316token\u6d88\u8017\u6539\u5584\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e0e\u590d\u6742\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u4ea4\u4e92\u3002", "conclusion": "Gatekeeper Protocol\u4e3a\u5728\u4efb\u4f55\u7ed3\u6784\u5316\u77e5\u8bc6\u9886\u57df\u6784\u5efa\u66f4\u7a33\u5065\u3001\u53ef\u9884\u6d4b\u548c\u57fa\u4e8e\u73b0\u5b9e\u7684AI\u4ee3\u7406\u63d0\u4f9b\u4e86\u57fa\u7840\u65b9\u6cd5\u8bba\u3002"}}
{"id": "2510.14900", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.14900", "abs": "https://arxiv.org/abs/2510.14900", "authors": ["Wen-Kwang Tsao", "Yao-Ching Yu", "Chien-Ming Huang"], "title": "Mapping Smarter, Not Harder: A Test-Time Reinforcement Learning Agent That Improves Without Labels or Model Updates", "comment": null, "summary": "The Enterprise Intelligence Platform must integrate logs from numerous\nthird-party vendors in order to perform various downstream tasks. However,\nvendor documentation is often unavailable at test time. It is either misplaced,\nmismatched, poorly formatted, or incomplete, which makes schema mapping\nchallenging. We introduce a reinforcement learning agent that can self-improve\nwithout labeled examples or model weight updates. During inference, the agent:\n1) Identifies ambiguous field-mapping attempts. 2) Generates targeted\nweb-search queries to gather external evidence. 3) Applies a confidence-based\nreward to iteratively refine its mappings. To demonstrate this concept, we\nconverted Microsoft Defender for Endpoint logs into a common schema. Our method\nincreased mapping accuracy from 56.4\\%(LLM-only) to 72.73\\%(RAG) to 93.94\\%\nover 100 iterations using GPT-4o. At the same time, it reduced the number of\nlow-confidence mappings requiring expert review by 85\\%. This new approach\nprovides an evidence-driven, transparent method for solving future industry\nproblems, paving the way for more robust, accountable, scalable, efficient,\nflexible, adaptable, and collaborative solutions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u6807\u6ce8\u6570\u636e\u6216\u6a21\u578b\u6743\u91cd\u66f4\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\uff0c\u80fd\u591f\u901a\u8fc7\u751f\u6210\u7f51\u7edc\u641c\u7d22\u67e5\u8be2\u6536\u96c6\u5916\u90e8\u8bc1\u636e\uff0c\u8fed\u4ee3\u6539\u8fdb\u4f01\u4e1a\u65e5\u5fd7\u7684schema\u6620\u5c04\u51c6\u786e\u6027\u3002", "motivation": "\u4f01\u4e1a\u667a\u80fd\u5e73\u53f0\u9700\u8981\u6574\u5408\u6765\u81ea\u591a\u4e2a\u7b2c\u4e09\u65b9\u5382\u5546\u7684\u65e5\u5fd7\u6570\u636e\uff0c\u4f46\u5382\u5546\u6587\u6863\u5e38\u5e38\u7f3a\u5931\u3001\u4e0d\u5339\u914d\u6216\u683c\u5f0f\u6df7\u4e71\uff0c\u5bfc\u81f4schema\u6620\u5c04\u53d8\u5f97\u56f0\u96be\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\uff1a1)\u8bc6\u522b\u6a21\u7cca\u7684\u5b57\u6bb5\u6620\u5c04\u5c1d\u8bd5\uff1b2)\u751f\u6210\u9488\u5bf9\u6027\u7684\u7f51\u7edc\u641c\u7d22\u67e5\u8be2\u6536\u96c6\u5916\u90e8\u8bc1\u636e\uff1b3)\u5e94\u7528\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u5956\u52b1\u8fed\u4ee3\u4f18\u5316\u6620\u5c04\u3002", "result": "\u5728\u5c06Microsoft Defender for Endpoint\u65e5\u5fd7\u8f6c\u6362\u4e3a\u901a\u7528schema\u7684\u4efb\u52a1\u4e2d\uff0c\u6620\u5c04\u51c6\u786e\u7387\u4ece56.4%(\u4ec5LLM)\u63d0\u5347\u523072.73%(RAG)\u518d\u523093.94%(100\u6b21\u8fed\u4ee3\u540e)\uff0c\u540c\u65f6\u5c06\u9700\u8981\u4e13\u5bb6\u5ba1\u67e5\u7684\u4f4e\u7f6e\u4fe1\u5ea6\u6620\u5c04\u51cf\u5c11\u4e8685%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u8bc1\u636e\u9a71\u52a8\u3001\u900f\u660e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u3001\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u548c\u9002\u5e94\u6027\u5f3a\u7684\u884c\u4e1a\u95ee\u9898\u89e3\u51b3\u65b9\u6cd5\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.14913", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14913", "abs": "https://arxiv.org/abs/2510.14913", "authors": ["Kyle Montgomery", "Sijun Tan", "Yuqi Chen", "Siyuan Zhuang", "Tianjun Zhang", "Raluca Ada Popa", "Chenguang Wang"], "title": "Budget-aware Test-time Scaling via Discriminative Verification", "comment": null, "summary": "Test-time scaling is a powerful strategy for boosting the performance of\nlarge language models on complex reasoning tasks. While state-of-the-art\napproaches often employ generative verifiers to select the best solution from a\npool of candidates, this method incurs prohibitive computational costs,\nlimiting its practicality. In this work, we shift the focus to a more\nbudget-aware paradigm: discriminative verification. We conduct a thorough\nempirical analysis and demonstrate that while discriminative verifiers may\nunderperform in isolation, combining them with self-consistency in a hybrid\napproach creates a powerful and efficient test-time scaling mechanism. Notably,\nunder a fixed compute budget, this hybrid approach surpasses state-of-the-art\ngenerative verification by a significant margin: achieving up to 15.3\\% higher\naccuracy on AIME2025. Our findings establish that for practical, real-world\napplications, budget-aware scaling with discriminative verifiers is not only a\n\"free\" upgrade over self-consistency, but also a more effective and efficient\nalternative to costly generative techniques. Code is available at\nhttps://github.com/wang-research-lab/verification.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5224\u522b\u5f0f\u9a8c\u8bc1\u5668\u548c\u81ea\u4e00\u81f4\u6027\u7684\u6df7\u5408\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\uff0c\u5728\u56fa\u5b9a\u8ba1\u7b97\u9884\u7b97\u4e0b\u663e\u8457\u4f18\u4e8e\u751f\u6210\u5f0f\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u5728AIME2025\u4e0a\u51c6\u786e\u7387\u63d0\u5347\u8fbe15.3%\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u5f0f\u9a8c\u8bc1\u5668\u867d\u7136\u6027\u80fd\u5f3a\u5927\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u9700\u8981\u5bfb\u627e\u66f4\u9ad8\u6548\u7684\u9a8c\u8bc1\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5224\u522b\u5f0f\u9a8c\u8bc1\u5668\u4e0e\u81ea\u4e00\u81f4\u6027\u7ed3\u5408\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u5728\u56fa\u5b9a\u8ba1\u7b97\u9884\u7b97\u4e0b\u8fdb\u884c\u6d4b\u8bd5\u65f6\u6269\u5c55\u3002", "result": "\u5728AIME2025\u4efb\u52a1\u4e0a\uff0c\u8be5\u65b9\u6cd5\u6bd4\u6700\u5148\u8fdb\u7684\u751f\u6210\u5f0f\u9a8c\u8bc1\u51c6\u786e\u7387\u9ad8\u51fa15.3%\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "\u57fa\u4e8e\u5224\u522b\u5f0f\u9a8c\u8bc1\u5668\u7684\u9884\u7b97\u611f\u77e5\u6269\u5c55\u65b9\u6cd5\u4e0d\u4ec5\u662f\u81ea\u4e00\u81f4\u6027\u7684\u514d\u8d39\u5347\u7ea7\uff0c\u4e5f\u662f\u6bd4\u6602\u8d35\u751f\u6210\u5f0f\u6280\u672f\u66f4\u6709\u6548\u548c\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2510.14922", "categories": ["cs.AI", "cs.CL", "cs.LG", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.14922", "abs": "https://arxiv.org/abs/2510.14922", "authors": ["Annisaa Fitri Nurfidausi", "Eleonora Mancini", "Paolo Torroni"], "title": "TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG", "comment": null, "summary": "Depression is a widespread mental health disorder, yet its automatic\ndetection remains challenging. Prior work has explored unimodal and multimodal\napproaches, with multimodal systems showing promise by leveraging complementary\nsignals. However, existing studies are limited in scope, lack systematic\ncomparisons of features, and suffer from inconsistent evaluation protocols. We\naddress these gaps by systematically exploring feature representations and\nmodelling strategies across EEG, together with speech and text. We evaluate\nhandcrafted features versus pre-trained embeddings, assess the effectiveness of\ndifferent neural encoders, compare unimodal, bimodal, and trimodal\nconfigurations, and analyse fusion strategies with attention to the role of\nEEG. Consistent subject-independent splits are applied to ensure robust,\nreproducible benchmarking. Our results show that (i) the combination of EEG,\nspeech and text modalities enhances multimodal detection, (ii) pretrained\nembeddings outperform handcrafted features, and (iii) carefully designed\ntrimodal models achieve state-of-the-art performance. Our work lays the\ngroundwork for future research in multimodal depression detection.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u591a\u6a21\u6001\u6291\u90c1\u68c0\u6d4b\uff0c\u901a\u8fc7\u6bd4\u8f83EEG\u3001\u8bed\u97f3\u548c\u6587\u672c\u7684\u7279\u5f81\u8868\u793a\u548c\u5efa\u6a21\u7b56\u7565\uff0c\u53d1\u73b0\u4e09\u6a21\u6001\u7ec4\u5408\u80fd\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\uff0c\u9884\u8bad\u7ec3\u5d4c\u5165\u4f18\u4e8e\u624b\u5de5\u7279\u5f81\uff0c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u4e09\u6a21\u6001\u6a21\u578b\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u6291\u90c1\u68c0\u6d4b\u7814\u7a76\u5b58\u5728\u8303\u56f4\u6709\u9650\u3001\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7279\u5f81\u6bd4\u8f83\u548c\u8bc4\u4f30\u534f\u8bae\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u9700\u8981\u7cfb\u7edf\u63a2\u7d22\u591a\u6a21\u6001\u7279\u5f81\u8868\u793a\u548c\u5efa\u6a21\u7b56\u7565\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u624b\u5de5\u7279\u5f81\u4e0e\u9884\u8bad\u7ec3\u5d4c\u5165\uff0c\u6bd4\u8f83\u4e0d\u540c\u795e\u7ecf\u7f16\u7801\u5668\u7684\u6709\u6548\u6027\uff0c\u5206\u6790\u5355\u6a21\u6001\u3001\u53cc\u6a21\u6001\u548c\u4e09\u6a21\u6001\u914d\u7f6e\uff0c\u7814\u7a76\u878d\u5408\u7b56\u7565\u5e76\u5173\u6ce8EEG\u7684\u4f5c\u7528\uff0c\u4f7f\u7528\u4e00\u81f4\u7684\u53d7\u8bd5\u8005\u72ec\u7acb\u5206\u5272\u8fdb\u884c\u7a33\u5065\u53ef\u590d\u73b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "EEG\u3001\u8bed\u97f3\u548c\u6587\u672c\u4e09\u6a21\u6001\u7ec4\u5408\u589e\u5f3a\u591a\u6a21\u6001\u68c0\u6d4b\u6027\u80fd\uff0c\u9884\u8bad\u7ec3\u5d4c\u5165\u4f18\u4e8e\u624b\u5de5\u7279\u5f81\uff0c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u4e09\u6a21\u6001\u6a21\u578b\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u672a\u6765\u591a\u6a21\u6001\u6291\u90c1\u68c0\u6d4b\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u8bc1\u660e\u4e86\u591a\u6a21\u6001\u878d\u5408\u7684\u6709\u6548\u6027\u548c\u9884\u8bad\u7ec3\u5d4c\u5165\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2510.14925", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14925", "abs": "https://arxiv.org/abs/2510.14925", "authors": ["Akira Okutomi"], "title": "Stable but Miscalibrated: A Kantian View on Overconfidence from Filters to Large Language Models", "comment": "19 pages, 2 figures, preliminary version", "summary": "We reinterpret Kant's Critique of Pure Reason as a theory of feedback\nstability, viewing reason as a regulator that keeps inference within the bounds\nof possible experience. We formalize this intuition via a composite instability\nindex (H-Risk) combining spectral margin, conditioning, temporal sensitivity,\nand innovation amplification. In linear-Gaussian simulations, higher H-Risk\npredicts overconfident errors even under formal stability, revealing a gap\nbetween nominal and epistemic stability. Extending to large language models\n(LLMs), we find that fragile internal dynamics correlate with miscalibration\nand hallucination, while critique-style prompts show mixed effects on\ncalibration and hallucination. These results suggest a structural bridge\nbetween Kantian self-limitation and feedback control, offering a principled\nlens for diagnosing -- and selectively reducing -- overconfidence in reasoning\nsystems. This is a preliminary version; supplementary experiments and broader\nreplication will be reported in a future revision.", "AI": {"tldr": "\u5c06\u5eb7\u5fb7\u7684\u300a\u7eaf\u7cb9\u7406\u6027\u6279\u5224\u300b\u91cd\u65b0\u89e3\u91ca\u4e3a\u53cd\u9988\u7a33\u5b9a\u6027\u7406\u8bba\uff0c\u63d0\u51fa\u590d\u5408\u4e0d\u7a33\u5b9a\u6027\u6307\u6570H-Risk\u6765\u8bc4\u4f30\u63a8\u7406\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\uff0c\u5e76\u5728\u7ebf\u6027\u9ad8\u65af\u6a21\u62df\u548cLLMs\u4e2d\u9a8c\u8bc1\u5176\u4e0e\u8fc7\u5ea6\u81ea\u4fe1\u9519\u8bef\u7684\u76f8\u5173\u6027\u3002", "motivation": "\u5efa\u7acb\u5eb7\u5fb7\u7406\u6027\u81ea\u6211\u9650\u5236\u7406\u8bba\u4e0e\u53cd\u9988\u63a7\u5236\u4e4b\u95f4\u7684\u7ed3\u6784\u6865\u6881\uff0c\u4e3a\u8bca\u65ad\u548c\u51cf\u5c11\u63a8\u7406\u7cfb\u7edf\u4e2d\u7684\u8fc7\u5ea6\u81ea\u4fe1\u63d0\u4f9b\u539f\u5219\u6027\u89c6\u89d2\u3002", "method": "\u63d0\u51fa\u590d\u5408\u4e0d\u7a33\u5b9a\u6027\u6307\u6570H-Risk\uff0c\u7ed3\u5408\u8c31\u88d5\u5ea6\u3001\u6761\u4ef6\u6570\u3001\u65f6\u95f4\u654f\u611f\u6027\u548c\u521b\u65b0\u653e\u5927\u7b49\u6307\u6807\uff0c\u5728\u7ebf\u6027\u9ad8\u65af\u6a21\u62df\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5728\u7ebf\u6027\u9ad8\u65af\u6a21\u62df\u4e2d\uff0c\u66f4\u9ad8\u7684H-Risk\u9884\u6d4b\u8fc7\u5ea6\u81ea\u4fe1\u9519\u8bef\uff1b\u5728LLMs\u4e2d\uff0c\u8106\u5f31\u7684\u5185\u90e8\u52a8\u6001\u4e0e\u6821\u51c6\u9519\u8bef\u548c\u5e7b\u89c9\u76f8\u5173\uff0c\u6279\u5224\u5f0f\u63d0\u793a\u5bf9\u6821\u51c6\u548c\u5e7b\u89c9\u6709\u6df7\u5408\u5f71\u54cd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u63a8\u7406\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u540d\u4e49\u7a33\u5b9a\u6027\u4e0e\u8ba4\u77e5\u7a33\u5b9a\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u51cf\u5c11AI\u7cfb\u7edf\u4e2d\u7684\u8fc7\u5ea6\u81ea\u4fe1\u63d0\u4f9b\u4e86\u8bca\u65ad\u5de5\u5177\u3002"}}
{"id": "2510.14942", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14942", "abs": "https://arxiv.org/abs/2510.14942", "authors": ["Yao Zhang", "Yu Wu", "Haowei Zhang", "Weiguo Li", "Haokun Chen", "Jingpei Wu", "Guohao Li", "Zhen Han", "Volker Tresp"], "title": "GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for Step-Level Reasoning", "comment": "25 pages", "summary": "Process Reward Models (PRMs) aim to improve multi-step reasoning in Large\nLanguage Models (LLMs) by supervising intermediate steps and identifying\nerrors. However, building effective PRMs remains challenging due to the lack of\nscalable, high-quality annotations. Existing approaches rely on costly human\nlabeling, LLM-based self-evaluation that is prone to hallucination, or Monte\nCarlo (MC) estimation, which infers step quality solely from rollout outcomes\nand often introduces noisy, misaligned supervision due to credit\nmisattribution. These issues result in three core limitations: noisy rewards,\nlow factual fidelity, and misalignment with step-level reasoning objectives. To\naddress these challenges, we introduce GroundedPRM, a tree-guided and\nfidelity-aware framework for automatic process supervision. To reduce reward\nnoise and enable fine-grained credit assignment, we construct structured\nreasoning paths via Monte Carlo Tree Search (MCTS). To eliminate hallucinated\nsupervision, we validate each intermediate step using an external tool,\nproviding execution-grounded correctness signals. To combine both step-level\nvalidation and global outcome assessment, we design a hybrid reward aggregation\nmechanism that fuses tool-based verification with MCTS-derived feedback.\nFinally, we format the reward signal into a rationale-enhanced, generative\nstructure to promote interpretability and compatibility with instruction-tuned\nLLMs. GroundedPRM is trained on only 40K automatically labeled samples,\namounting to just 10% of the data used by the best-performing PRM trained with\nauto-labeled supervision. Nevertheless, it achieves up to a 26% relative\nimprovement in average performance on ProcessBench. When used for reward-guided\ngreedy search, GroundedPRM outperforms even PRMs trained with human-labeled\nsupervision, offering a scalable and verifiable path toward high-quality\nprocess-level reasoning.", "AI": {"tldr": "GroundedPRM\u662f\u4e00\u4e2a\u57fa\u4e8e\u6811\u641c\u7d22\u548c\u5916\u90e8\u5de5\u5177\u9a8c\u8bc1\u7684\u81ea\u52a8\u8fc7\u7a0b\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7MCTS\u6784\u5efa\u7ed3\u6784\u5316\u63a8\u7406\u8def\u5f84\uff0c\u4f7f\u7528\u5916\u90e8\u5de5\u5177\u9a8c\u8bc1\u4e2d\u95f4\u6b65\u9aa4\uff0c\u7ed3\u5408\u6b65\u9aa4\u7ea7\u9a8c\u8bc1\u548c\u5168\u5c40\u8bc4\u4f30\u7684\u6df7\u5408\u5956\u52b1\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6b65\u63a8\u7406\u7684\u8d28\u91cf\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u6709\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b(PRMs)\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a\u566a\u58f0\u5956\u52b1\u3001\u4f4e\u4e8b\u5b9e\u4fdd\u771f\u5ea6\u548c\u4e0e\u6b65\u9aa4\u7ea7\u63a8\u7406\u76ee\u6807\u4e0d\u5bf9\u9f50\u3002\u8fd9\u4e9b\u95ee\u9898\u6e90\u4e8e\u7f3a\u4e4f\u53ef\u6269\u5c55\u7684\u9ad8\u8d28\u91cf\u6807\u6ce8\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u3001\u6613\u4ea7\u751f\u5e7b\u89c9\u7684LLM\u81ea\u8bc4\u4f30\u6216\u5b58\u5728\u4fe1\u7528\u5206\u914d\u95ee\u9898\u7684\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u3002", "method": "1) \u4f7f\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22(MCTS)\u6784\u5efa\u7ed3\u6784\u5316\u63a8\u7406\u8def\u5f84\u4ee5\u51cf\u5c11\u5956\u52b1\u566a\u58f0\uff1b2) \u901a\u8fc7\u5916\u90e8\u5de5\u5177\u9a8c\u8bc1\u6bcf\u4e2a\u4e2d\u95f4\u6b65\u9aa4\uff0c\u63d0\u4f9b\u6267\u884c\u57fa\u7840\u7684\u6b63\u786e\u6027\u4fe1\u53f7\uff1b3) \u8bbe\u8ba1\u6df7\u5408\u5956\u52b1\u805a\u5408\u673a\u5236\uff0c\u878d\u5408\u5de5\u5177\u9a8c\u8bc1\u548cMCTS\u53cd\u9988\uff1b4) \u5c06\u5956\u52b1\u4fe1\u53f7\u683c\u5f0f\u5316\u4e3a\u7406\u7531\u589e\u5f3a\u7684\u751f\u6210\u7ed3\u6784\u3002", "result": "\u4ec5\u4f7f\u75284\u4e07\u4e2a\u81ea\u52a8\u6807\u6ce8\u6837\u672c(\u6700\u4f73PRM\u768410%)\uff0c\u5728ProcessBench\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe26%\u7684\u76f8\u5bf9\u6027\u80fd\u63d0\u5347\u3002\u5728\u5956\u52b1\u5f15\u5bfc\u7684\u8d2a\u5a6a\u641c\u7d22\u4e2d\uff0c\u751a\u81f3\u4f18\u4e8e\u4f7f\u7528\u4eba\u5de5\u6807\u6ce8\u76d1\u7763\u8bad\u7ec3\u7684PRMs\u3002", "conclusion": "GroundedPRM\u4e3a\u9ad8\u8d28\u91cf\u8fc7\u7a0b\u7ea7\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u6269\u5c55\u4e14\u53ef\u9a8c\u8bc1\u7684\u8def\u5f84\uff0c\u901a\u8fc7\u7ed3\u5408\u6811\u5f15\u5bfc\u7684\u7ed3\u6784\u5316\u63a8\u7406\u548c\u5de5\u5177\u57fa\u7840\u7684\u4e8b\u5b9e\u9a8c\u8bc1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709PRMs\u7684\u6838\u5fc3\u5c40\u9650\u6027\u3002"}}
{"id": "2510.14980", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14980", "abs": "https://arxiv.org/abs/2510.14980", "authors": ["Wenqian Zhang", "Weiyang Liu", "Zhen Liu"], "title": "Agentic Design of Compositional Machines", "comment": "75 pages, 31 figures, Project Page: https://besiegefield.github.io", "summary": "The design of complex machines stands as both a marker of human intelligence\nand a foundation of engineering practice. Given recent advances in large\nlanguage models (LLMs), we ask whether they, too, can learn to create. We\napproach this question through the lens of compositional machine design: a task\nin which machines are assembled from standardized components to meet functional\ndemands like locomotion or manipulation in a simulated physical environment. To\nsupport this investigation, we introduce BesiegeField, a testbed built on the\nmachine-building game Besiege, which enables part-based construction, physical\nsimulation and reward-driven evaluation. Using BesiegeField, we benchmark\nstate-of-the-art LLMs with agentic workflows and identify key capabilities\nrequired for success, including spatial reasoning, strategic assembly, and\ninstruction-following. As current open-source models fall short, we explore\nreinforcement learning (RL) as a path to improvement: we curate a cold-start\ndataset, conduct RL finetuning experiments, and highlight open challenges at\nthe intersection of language, machine design, and physical reasoning.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u591f\u8fdb\u884c\u7ec4\u5408\u5f0f\u673a\u5668\u8bbe\u8ba1\uff0c\u901a\u8fc7BesiegeField\u6d4b\u8bd5\u5e73\u53f0\u8bc4\u4f30LLM\u5728\u7269\u7406\u73af\u5883\u4e2d\u7684\u673a\u5668\u6784\u5efa\u80fd\u529b\uff0c\u5e76\u63a2\u7d22\u5f3a\u5316\u5b66\u4e60\u4f5c\u4e3a\u6539\u8fdb\u9014\u5f84\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u673a\u5668\u8bbe\u8ba1\u65b9\u9762\u7684\u80fd\u529b\uff0c\u8fd9\u662f\u4eba\u7c7b\u667a\u80fd\u7684\u91cd\u8981\u6807\u5fd7\u548c\u5de5\u7a0b\u5b9e\u8df5\u7684\u57fa\u7840\u3002", "method": "\u5f15\u5165BesiegeField\u6d4b\u8bd5\u5e73\u53f0\uff0c\u57fa\u4e8eBesiege\u6e38\u620f\u6784\u5efa\uff0c\u652f\u6301\u57fa\u4e8e\u7ec4\u4ef6\u7684\u6784\u9020\u3001\u7269\u7406\u6a21\u62df\u548c\u5956\u52b1\u9a71\u52a8\u8bc4\u4f30\u3002\u5bf9\u6700\u5148\u8fdb\u7684LLM\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u63a2\u7d22\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u3002", "result": "\u5f53\u524d\u5f00\u6e90\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u3001\u7b56\u7565\u6027\u7ec4\u88c5\u548c\u6307\u4ee4\u9075\u5faa\u7b49\u5173\u952e\u80fd\u529b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u663e\u793a\u51fa\u6539\u8fdb\u6f5c\u529b\u3002", "conclusion": "\u673a\u5668\u8bbe\u8ba1\u4efb\u52a1\u63ed\u793a\u4e86LLM\u5728\u8bed\u8a00\u3001\u673a\u5668\u8bbe\u8ba1\u548c\u7269\u7406\u63a8\u7406\u4ea4\u53c9\u9886\u57df\u9762\u4e34\u7684\u6311\u6218\uff0c\u5f3a\u5316\u5b66\u4e60\u662f\u63d0\u5347\u6027\u80fd\u7684\u6709\u524d\u666f\u9014\u5f84\u3002"}}

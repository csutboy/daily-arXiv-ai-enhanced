<div id=toc></div>

# Table of Contents

- [stat.AP](#stat.AP) [Total: 2]
- [econ.EM](#econ.EM) [Total: 4]
- [cs.AI](#cs.AI) [Total: 53]
- [cs.CY](#cs.CY) [Total: 4]
- [cs.ET](#cs.ET) [Total: 3]
- [cs.SI](#cs.SI) [Total: 7]


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [1] [Classifying Metamorphic versus Single-Fold Proteins with Statistical Learning and AlphaFold2](https://arxiv.org/abs/2512.10066)
*Yongkai Chen,Samuel WK Wong,SC Kou*

Main category: stat.AP

TL;DR: 开发了一个基于AlphaFold2的变形蛋白分类框架，通过多序列比对采样生成构象集合，提取特征后用随机森林分类器识别变形蛋白，在PDB数据中发现了潜在候选蛋白。


<details>
  <summary>Details</summary>
Motivation: AlphaFold2虽然能准确预测蛋白质结构，但其"一个序列对应一个结构"的范式无法处理具有多种构象的变形蛋白。目前区分变形蛋白和单折叠蛋白对实验和计算方法都是重要挑战。

Method: 重新利用AlphaFold2，通过多序列比对采样方法生成构象集合，从这些集合中提取描述构象集合模态和结构分散性的综合特征集，使用随机森林分类器在精心策划的基准数据集上进行训练。

Result: 在交叉验证中达到平均AUC 0.869，证明方法的有效性。将分类器应用于PDB中600个随机采样蛋白，发现了多个潜在变形蛋白候选，包括40S核糖体蛋白S30，其构象变化对抗菌防御的次要功能至关重要。

Conclusion: 通过结合AI驱动的蛋白质结构预测和统计学习，为发现变形蛋白提供了强大的新方法，深化了对变形蛋白在分子功能中作用的理解。

Abstract: The remarkable success of AlphaFold2 in providing accurate atomic-level prediction of protein structures from their amino acid sequence has transformed approaches to the protein folding problem. However, its core paradigm of mapping one sequence to one structure may only be appropriate for single-fold proteins with one stable conformation. Metamorphic proteins, which can adopt multiple distinct conformations, have conformational diversity that cannot be adequately modeled by AlphaFold2. Hence, classifying whether a given protein is metamorphic or single-fold remains a critical challenge for both laboratory experiments and computational methods. To address this challenge, we developed a novel classification framework by re-purposing AlphaFold2 to generate conformational ensembles via a multiple sequence alignment sampling method. From these ensembles, we extract a comprehensive set of features characterizing the conformational ensemble's modality and structural dispersion. A random forest classifier trained on a carefully curated benchmark dataset of known metamorphic and single-fold proteins achieves a mean AUC of 0.869 with cross-validation, demonstrating the effectiveness of our integrated approach. Furthermore, by applying our classifier to 600 randomly sampled proteins from the Protein Data Bank, we identified several potential metamorphic protein candidates -- including the 40S ribosomal protein S30, whose conformational change is crucial for its secondary function in antimicrobial defense. By combining AI-driven protein structure prediction with statistical learning, our work provides a powerful new approach for discovering metamorphic proteins and deepens our understanding of their role in their molecular function.

</details>


### [2] [Alpha Power Harris-G Family of Distributions: Properties and Application to Burr XII Distribution](https://arxiv.org/abs/2512.10276)
*Gbenga A. Olalude,Taiwo A. Ojurongbe,Olalekan A. Bello,Kehinde A. Bashiru,Kazeem A. Alamu*

Main category: stat.AP

TL;DR: 提出了一种新的概率分布族——α幂Harris广义(APHG)族，通过将Harris-G框架的两个形状参数融入α幂变换，创建了更灵活的生存和可靠性数据建模工具。特别研究了以Burr XII分布为基线的APHBXII模型，推导了其统计性质，并通过模拟和实际数据验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有概率分布模型在拟合生存和可靠性数据时可能存在灵活性不足的问题。为了提供更强大的建模工具，需要开发能够更好捕捉数据复杂性的新分布族，特别是在处理重尾、偏斜等特征时。

Method: 将Harris-G框架的两个形状参数融入α幂变换，创建APHG分布族。特别以两参数Burr XII分布为基线，开发APHBXII模型。推导了该模型的矩、均值和中位数偏差、Bonferroni和Lorenz曲线、顺序统计量、Renyi和Tsallis熵等解析性质。使用最大似然法进行参数估计，并通过蒙特卡洛模拟评估估计量的有限样本性能。

Result: APHBXII模型在三个实际寿命数据集上表现出色，优于四个竞争模型。模型选择准则和拟合优度统计量均支持五参数APHBXII模型在所有数据集上提供最佳拟合。

Conclusion: APHG分布族特别是APHBXII模型为生存和可靠性数据分析提供了灵活有效的工具，在实证应用中表现出优越的拟合性能，具有实际应用价值。

Abstract: This study introduces a new family of probability distributions, termed the alpha power Harris-generalized (APHG) family. The generator arises by incorporating two shape parameters from the Harris-G framework into the alpha power transformation, resulting in a more flexible class for modelling survival and reliability data. A special member of this family, obtained using the two-parameter Burr XII distribution as the baseline, is developed and examined in detail. Several analytical properties of the proposed alpha power Harris Burr XII (APHBXII) model are derived, which include closed-form expressions for its moments, mean and median deviations, Bonferroni and Lorenz curves, order statistics, and Renyi and Tsallis entropies. Parameter estimation is performed via maximum likelihood, and a Monte Carlo simulation study is carried out to assess the finite-sample performance of the estimators. In addition, three real lifetime datasets are analyzed to evaluate the empirical performance of the APHBXII distribution relative to four competing models. The results show that the five-parameter APHBXII model provides superior fit across all datasets, as supported by model-selection criteria and goodness-of-fit statistics.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [3] [Microfoundations and the Causal Interpretation of Price-Exposure Designs](https://arxiv.org/abs/2512.10076)
*Luca Moreno-Louzada,Guilherme Figueira,Pedro Picchetti*

Main category: econ.EM

TL;DR: 论文研究使用商品价格作为工具变量的区域暴露设计，分析单一总冲击对地方的效应。与标准shift-share设计不同，价格暴露设计面临识别和推断挑战。作者通过理论模型分析2SLS和TWFE估计量的性质，提出新的标准误估计方法，并在亚马逊金矿与凶杀案应用中验证。


<details>
  <summary>Details</summary>
Motivation: 传统shift-share设计利用多个冲击的差异暴露，而价格暴露设计仅依赖单一冲击的外生变异，这带来了独特的识别和推断挑战。需要理解这种设计的估计量性质，并解决标准推断方法可能存在的过度拒绝问题。

Method: 1) 建立多部门劳动力模型和潜在结果框架；2) 理论分析2SLS和TWFE估计量作为区域和部门特定效应的加权平均加上污染项；3) 推导估计量具有明确因果解释的条件；4) 提出简单的敏感性分析程序；5) 推导新的标准误估计量；6) 通过蒙特卡洛模拟验证有限样本性质；7) 在亚马逊金矿与凶杀案应用中实证检验。

Result: 1) 识别了价格暴露设计中估计量的污染项来源（价格协方差结构和一般均衡产出响应）；2) 提供了估计量具有因果解释的条件和敏感性分析方法；3) 发现标准推断方法存在过度拒绝问题；4) 提出的新标准误估计量具有良好有限样本性质；5) 在实证应用中，价格暴露标准误约为传统聚类标准误的两倍，使主要效应统计不显著。

Conclusion: 价格暴露设计面临独特的识别和推断挑战，需要仔细的理论分析和专门的推断方法。论文提供了理解这种设计估计量性质的理论框架，并提出了解决标准推断过度拒绝问题的新方法。实证应用表明，忽视这些挑战可能导致错误的统计推断。

Abstract: This paper studies regional exposure designs that use commodity prices as instruments to study local effects of aggregate shocks. Unlike standard shift share designs that leverage differential exposure to many shocks, the price exposure relies on exogenous variation from a single shock, leading to challenges for both identification and inference. We motivate the design using a multi sector labor model. Under the model and a potential outcomes framework, we characterize the 2SLS and TWFE estimands as weighted averages of region and sector specific effects plus contamination terms driven by the covariance structure of prices and by general-equilibrium output responses. We derive conditions under which these estimands have a clear causal interpretation and provide simple sensitivity analysis procedures for violations. Finally, we show that standard inference procedures suffer from an overrejection problem in price-exposure designs. We derive a new standard error estimator and show its desirable finite-sample properties through Monte Carlo simulations. In an application to gold mining and homicides in the Amazon, the price exposure standard errors are roughly twice as large as conventional clustered standard errors, making the main effect statistically insignificant.

</details>


### [4] [Inference for Batched Adaptive Experiments](https://arxiv.org/abs/2512.10156)
*Jan Kemper,Davud Rostam-Afschar*

Main category: econ.EM

TL;DR: 提出BOLS检验统计量用于自适应实验中的因果推断，通过精度均衡化聚合各期处理-控制差异，构建渐近有效的置信区间。


<details>
  <summary>Details</summary>
Motivation: 自适应实验在经济学等领域广泛应用，但给因果推断带来挑战，需要开发适用于自适应实验的统计推断方法。

Method: 提出BOLS（批处理普通最小二乘法）检验统计量，在异方差条件下对每期处理-控制差异进行精度均衡化聚合，统计量为异方差每期z统计量的归一化平均值。

Result: BOLS统计量可用于构建渐近有效的置信区间，通过模拟比较了处理期数少、每批观测数少或多情况下的拒绝率。

Conclusion: BOLS统计量为自适应实验中的因果推断提供了一种有效的检验方法，特别适用于处理期数有限的情况。

Abstract: The advantages of adaptive experiments have led to their rapid adoption in economics, other fields, as well as among practitioners. However, adaptive experiments pose challenges for causal inference. This note suggests a BOLS (batched ordinary least squares) test statistic for inference of treatment effects in adaptive experiments. The statistic provides a precision-equalizing aggregation of per-period treatment-control differences under heteroskedasticity. The combined test statistic is a normalized average of heteroskedastic per-period z-statistics and can be used to construct asymptotically valid confidence intervals. We provide simulation results comparing rejection rates in the typical case with few treatment periods and few (or many) observations per batch.

</details>


### [5] [AI-Enhanced TOE Framework for Sustainable Industrial Performance in Fragile and Transforming Economies: Evidence from Yemen and Saudi Arabia](https://arxiv.org/abs/2512.10333)
*Shaima Farhana,Dong Yua,Amirhossein Karamoozianc,Ali Al-shawafid,Amar N. Alsheavif*

Main category: econ.EM

TL;DR: 本研究基于AI增强的TOE模型框架，探讨在也门和沙特阿拉伯等脆弱转型环境中如何通过AI采用提升工业绩效和环境可持续性。研究发现AI-TOE对环境和制造绩效有显著正向影响，且工业绩效在其中起重要中介作用。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决资源受限和脆弱环境（如也门和沙特阿拉伯）中的数字转型与可持续发展问题。现有文献缺乏针对这类特殊环境的AI采用框架，本研究旨在填补这一空白，为政策制定者和组织提供可操作的指导。

Method: 采用基于TOE模型并增强AI的整合框架，收集了600家中小企业的实地数据，通过对294名管理者的问卷调查，使用偏最小二乘结构方程模型（PLS-SEM）进行分析。

Result: PLS-SEM分析显示：AI-TOE对环境绩效有显著正向影响（β=0.487），对制造绩效影响更强（β=0.759）。沙特中小企业受益于制度支持和先进技术，而也门企业依赖低成本AI采用和组织灵活性。工业绩效在AI-TOE与环境绩效关系中起重要中介作用。

Conclusion: AI在脆弱转型环境中具有变革力量，但其影响取决于基础设施成熟度和组织准备度。研究为资源受限环境中的AI采用提供了可操作框架，指导政策制定者和工程管理者在数字转型与可持续性之间做出权衡，实现更具韧性和可持续的运营策略。

Abstract: Using an integrated framework rooted in the TOE model enhanced with AI, this study looks at ways to improve industrial performance and environmental sustainability in fragile and rapidly transforming contexts such as those found in Yemen and Saudi Arabia. Data for the research are field-based and were obtained from a total of 600 SMEs operating in both countries. Based on the questionnaires' responses by 294 managers, results from the partial least squares structural equation modeling (PLS-SEM) have indicated significant positive effects of AI-TOE on environmental performance (beta = 0.487) and manufacturing performance (beta = 0.759). Results indicate that AI acts as a transformative force, though its impact differs based on the maturity of infrastructure and organizational readiness. The Saudi SMEs gain from their institutional support and advanced technologies, while those in Yemen are dependent on the low-cost adoption of AI and organizational flexibility to accept structural challenges. PLS-SEM analysis of the study showed that integrating AI into the TOE dimensions accelerates operational efficiency in order to support environmental performance. Industrial performance was found to be a very important mediator in this relationship. This study responds to the call for digital transformation literature by providing an actionable framework of AI adoption in resource-constrained environments. These findings offer insights that might guide policymakers and organizations toward more resilient and sustainable operational strategies. These findings provide valuable guidance for engineering managers within the context of negotiating digital transformation and sustainability trade-offs in fragile and resource-constrained contexts.

</details>


### [6] [Optimal Embeddedness and Governance in Biotech Venture Capital Syndicates](https://arxiv.org/abs/2512.10568)
*Yuxin Hu,Nektarios Oraiopoulos*

Main category: econ.EM

TL;DR: 研究发现生物科技创投网络中适度的共同投资联系和领域专业知识同质性最有利于企业退出，过度嵌入或过于稀疏都会降低成功率，且不同退出路径的最佳网络嵌入程度不同。


<details>
  <summary>Details</summary>
Motivation: 现有VC网络研究主要关注软件和消费科技领域，而生物科技行业面临独特的资本需求和监管挑战，需要研究网络嵌入如何影响生物科技企业的退出结果。

Method: 使用2010-2024年美加欧11,680家生物科技初创企业的面板数据，应用混合logit、Cox比例风险、多项logit和Fine-Gray竞争风险模型进行分析。

Result: 共同投资联系和投资者同质性均呈现倒U型关系：适度熟悉和科学重叠最大化退出概率；制药企业VC或独立董事会的参与能缓解过度嵌入的负面影响；不同退出路径的最佳嵌入程度不同。

Conclusion: 研究完善了生命科学背景下的网络嵌入理论，识别了治理机制的调节作用，为从业者提供了平衡信任、专业知识和监督的量化指标。

Abstract: The biotech venture market faces intense capital demands and regulatory scrutiny, yet academic research on VC networks remains rooted in software and consumer-tech contexts. This dissertation investigates how repeated co-investment ties and domain-expertise homophily influence a venture's exit likelihood, timing, and route amid the sector's pronounced technological and market uncertainty. Using a novel panel of 11,680 biotechnology start-ups from the United States, Canada, and Europe (2010-2024), we apply pooled logit, Cox proportional-hazards, multinomial logit, and Fine-Gray competing-risk models. Our findings show that both average prior co-investment and investor homophily exhibit robust inverted-U relationships with exit outcomes. Moderate familiarity and scientific overlap maximize exit probability, while either sparse or excessive embedding reduces success. Governance mechanisms also play a crucial role: participation of a pharmaceutical corporate VC or a highly independent board flattens the negative effects of over-embedding, enabling syndicates to sustain exit momentum at higher levels of familiarity or homogeneity. Furthermore, the optimal degree of embeddedness is route-specific: IPOs require deeper coordination than trade sales, while acquisitions peak earlier and are less sensitive to homophily. These findings refine network-embeddedness theory in the life-science context, identify governance contingencies, and offer practitioners quantitative metrics to balance trust, expertise, and oversight in biotech financing.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [ExaCraft: Dynamic Learning Context Adaptation for Personalized Educational Examples](https://arxiv.org/abs/2512.09931)
*Akaash Chatterjee,Suman Kundu*

Main category: cs.AI

TL;DR: ExaCraft是一个AI驱动的个性化学习示例生成系统，能够根据学习者的动态上下文（包括个人资料、学习行为和进度）创建文化相关且量身定制的学习示例。


<details>
  <summary>Details</summary>
Motivation: 现有教育AI工具缺乏针对学习者个性化需求的示例生成能力，无法适应学习者不断变化的理解水平、学习困难和技能增长。学习最有效的方式是与学习者个人相关的、有共鸣的示例，但当前工具未能满足这一需求。

Method: 通过Google Gemini AI和Python Flask API构建系统，以Chrome扩展形式访问。系统结合用户定义的个人资料（位置、教育背景、职业、复杂度偏好）和实时学习行为分析，能够适应学习上下文的五个关键方面：学习困难指标、掌握模式、主题进展历史、会话边界和学习进展信号。

Result: 系统能够生成从基础概念到高级技术实现的演化示例，响应主题重复、重新生成请求和主题进展模式。示例会根据学习者的动态需求进行个性化调整。

Conclusion: ExaCraft展示了AI系统如何通过适应学习者的动态上下文来生成个性化学习示例，解决了现有教育AI工具在示例生成和适应性方面的不足，为更有效的个性化学习提供了新途径。

Abstract: Learning is most effective when it's connected to relevant, relatable examples that resonate with learners on a personal level. However, existing educational AI tools don't focus on generating examples or adapting to learners' changing understanding, struggles, or growing skills. We've developed ExaCraft, an AI system that generates personalized examples by adapting to the learner's dynamic context. Through the Google Gemini AI and Python Flask API, accessible via a Chrome extension, ExaCraft combines user-defined profiles (including location, education, profession, and complexity preferences) with real-time analysis of learner behavior. This ensures examples are both culturally relevant and tailored to individual learning needs. The system's core innovation is its ability to adapt to five key aspects of the learning context: indicators of struggle, mastery patterns, topic progression history, session boundaries, and learning progression signals. Our demonstration will show how ExaCraft's examples evolve from basic concepts to advanced technical implementations, responding to topic repetition, regeneration requests, and topic progression patterns in different use cases.

</details>


### [8] [Suzume-chan: Your Personal Navigator as an Embodied Information Hub](https://arxiv.org/abs/2512.09932)
*Maya Grace Torii,Takahito Murakami,Shuka Koseki,Yoichi Ochiai*

Main category: cs.AI

TL;DR: 提出"具身信息枢纽"概念，通过物理对话交互减少心理距离，使知识分享更温暖、更人性化


<details>
  <summary>Details</summary>
Motivation: 数字工具虽然改善了信息获取，但缺乏深度理解所需的连接感。专家知识获取通常需要实时人际沟通，而现有工具难以创造这种连接感。

Method: 基于社会在场理论，设计"具身信息枢纽"原型Suzume-chan：小型柔软的本地AI代理，结合语言模型和检索增强生成(RAG)，通过语音解释学习和对话响应。

Result: 开发了Suzume-chan原型，通过物理和对话交互减少心理距离，使知识分享更温暖、更人性化。

Conclusion: 具身信息枢纽通过物理对话交互创造连接感，为知识分享提供了更人性化的新途径。

Abstract: Access to expert knowledge often requires real-time human communication. Digital tools improve access to information but rarely create the sense of connection needed for deep understanding. This study addresses this issue using Social Presence Theory, which explains how a feeling of "being together" enhances communication. An "Embodied Information Hub" is proposed as a new way to share knowledge through physical and conversational interaction. The prototype, Suzume-chan, is a small, soft AI agent running locally with a language model and retrieval-augmented generation (RAG). It learns from spoken explanations and responds through dialogue, reducing psychological distance and making knowledge sharing warmer and more human-centered.

</details>


### [9] [Exploring Health Misinformation Detection with Multi-Agent Debate](https://arxiv.org/abs/2512.09935)
*Chih-Han Chen,Chen-Han Tsai,Yu-Shao Peng*

Main category: cs.AI

TL;DR: 提出一个两阶段框架用于健康信息检测：先通过大语言模型评估证据并计算一致性分数，当分数低于阈值时启动多智能体辩论阶段，综合冲突证据生成有理由的结论。


<details>
  <summary>Details</summary>
Motivation: 随着健康相关错误信息在网上泛滥，事实核查变得至关重要。有效的验证需要高质量证据检索和严谨的推理过程，但现有方法在处理复杂、冲突的证据时存在局限性。

Method: 两阶段框架：第一阶段使用大语言模型独立评估检索到的文章，计算聚合一致性分数；当分数低于预设阈值时，进入第二阶段，多个智能体进行结构化辩论，综合冲突证据并生成有明确理由的结论。

Result: 实验结果表明，这种两阶段方法相比基线方法取得了更优的性能，证明了将自动评分与协作推理相结合在复杂验证任务中的价值。

Conclusion: 结合自动一致性评分和多智能体辩论的两阶段框架能有效检测健康错误信息，为复杂验证任务提供了更可靠的解决方案。

Abstract: Fact-checking health-related claims has become increasingly critical as misinformation proliferates online. Effective verification requires both the retrieval of high-quality evidence and rigorous reasoning processes. In this paper, we propose a two-stage framework for health misinformation detection: Agreement Score Prediction followed by Multi-Agent Debate. In the first stage, we employ large language models (LLMs) to independently evaluate retrieved articles and compute an aggregated agreement score that reflects the overall evidence stance. When this score indicates insufficient consensus-falling below a predefined threshold-the system proceeds to a second stage. Multiple agents engage in structured debate to synthesize conflicting evidence and generate well-reasoned verdicts with explicit justifications. Experimental results demonstrate that our two-stage approach achieves superior performance compared to baseline methods, highlighting the value of combining automated scoring with collaborative reasoning for complex verification tasks.

</details>


### [10] [Echo-CoPilot: A Multi-View, Multi-Task Agent for Echocardiography Interpretation and Reporting](https://arxiv.org/abs/2512.09944)
*Moein Heidari,Mohammad Amin Roohi,Armin Khosravi,Ilker Hacihaliloglu*

Main category: cs.AI

TL;DR: Echo-CoPilot：一个基于大语言模型的多视图、多任务超声心动图智能代理，通过协调多个专业工具实现临床连贯的超声心动图评估。


<details>
  <summary>Details</summary>
Motivation: 超声心动图是心血管诊疗的核心技术，但全研究解读仍是认知密集、多视图的手动任务。现有基础模型虽然能在单个感知子任务（如视图分类、分割、疾病预测）上表现良好，但各自孤立运行，无法提供统一、临床连贯的评估。

Method: 提出Echo-CoPilot，一个多视图、多任务代理，使用大语言模型协调一套专门的超声心动图工具。在ReAct风格循环中，代理分解临床医生查询，调用视图识别、心脏结构分割、测量和疾病预测、报告生成等工具，并将输出整合为指南感知的答案和叙述性总结。

Result: 在公开的MIMIC-EchoQA基准测试中，Echo-CoPilot达到50.8%的准确率，优于通用和生物医学视频视觉语言模型。定性分析显示，代理能利用定量测量和生理学上下文解决临床决策阈值附近的挑战性病例。

Conclusion: Echo-CoPilot通过大语言模型协调多个专业工具，实现了临床连贯的超声心动图评估，在基准测试中表现优异，并能处理临床决策阈值附近的复杂病例。

Abstract: Echocardiography is central to contemporary cardiovascular care, but full-study interpretation remains a cognitively demanding, multi-view task that is still performed manually. While recent foundation models for echocardiography can achieve strong performance on individual perceptual subtasks such as view classification, segmentation, or disease prediction, they typically operate in isolation and do not provide a unified, clinically coherent assessment. In this work, we introduce Echo-CoPilot, a multi-view, multi-task agent that uses a large language model to orchestrate a suite of specialized echocardiography tools. Within a ReAct-style loop, the agent decomposes clinician queries, invokes tools for view recognition, cardiac structure segmentation, measurement and disease prediction, and report synthesis, and integrates their outputs into guideline-aware answers and narrative summaries. We evaluate Echo-CoPilot on the public MIMIC-EchoQA benchmark, where it achieves an accuracy of 50.8\%, outperforming both general-purpose and biomedical video vision-language models. Qualitative analyses further show that the agent leverages quantitative measurements and physiologic context to resolve challenging cases near clinical decision thresholds, such as borderline left ventricular hypertrophy or pericardial effusion severity. The code will be released upon acceptance of the paper.

</details>


### [11] [Fuzzy Hierarchical Multiplex](https://arxiv.org/abs/2512.09976)
*Alexis Kafantaris*

Main category: cs.AI

TL;DR: 提出一个新的模糊优化框架，扩展FCM因果关系，利用动态性将数据映射到度量中，通过多重网络分析概念间的逻辑蕴含和层次结构，用于服务流程设计中的信息传输优化。


<details>
  <summary>Details</summary>
Motivation: 现有FCM（模糊认知图）因果关系模型在服务流程设计中的信息传输优化方面存在局限性，需要一个新的框架来更好地处理概念间的逻辑关系和层次结构，以优化服务过程中的信息传输效率。

Method: 提出一个扩展FCM因果关系的模糊优化框架，利用动态性将数据映射到度量空间，通过多重网络（multiplex）分析概念间的逻辑蕴含和层次关系，建立理论框架并分析其逻辑和数学基础。

Result: 成功构建了一个理论框架，能够分析概念间的逻辑蕴含和层次结构，为服务流程设计中的信息传输优化提供了新的方法，并对FHM（模糊层次模型）进行了简洁优雅的逻辑分析。

Conclusion: 提出的模糊优化框架扩展了FCM因果关系，通过多重网络分析概念间的逻辑关系，为服务流程设计中的信息传输优化提供了有效的理论工具，具有实际应用价值。

Abstract: A new fuzzy optimization framework that extends FCM causality is proposed. This model utilizes the dynamics to map data into metrics and create a framework that examines logical implication and hierarchy of concepts using a multiplex. Moreover, this is a white-theoretical paper introducing the framework and analyzing the logic and math behind it. Upon this extension the main objectives and the orientation of this framework is expounded and exemplified; this framework is meant for service optimization of information transmission in service process design. Lastly, a thorough analysis of the FHM is included which is done following the logical steps in a simple and elegant manner.

</details>


### [12] [Mind the Gap! Pathways Towards Unifying AI Safety and Ethics Research](https://arxiv.org/abs/2512.10058)
*Dani Roytburg,Beck Miller*

Main category: cs.AI

TL;DR: 通过大规模文献计量分析发现AI安全与AI伦理研究存在显著的结构性分裂，超过80%的合作发生在各自社区内部，跨领域交流高度集中于少数研究者。


<details>
  <summary>Details</summary>
Motivation: AI对齐研究分化为安全（关注智能扩展、欺骗行为、存在风险）和伦理（关注当前危害、社会偏见、生产流程缺陷）两个平行轨道，虽然都警告对齐投资不足，但对齐的定义和方向存在分歧，导致研究相对孤立。

Method: 采用文献计量和合著网络分析方法，对2020-2025年间12个主要ML和NLP会议的6,442篇论文进行分析，研究安全与伦理社区之间的合作模式和结构分裂。

Result: 超过80%的合作发生在安全或伦理社区内部，跨领域连通性高度集中：约5%的论文贡献了超过85%的桥梁链接。移除少数关键中介者会显著增加隔离，表明跨学科交流依赖于少数关键人物而非广泛分布的合作。

Conclusion: 安全-伦理分裂不仅是概念性的，更是制度性的，对研究议程、政策和学术场所有重要影响。需要通过共享基准、跨机构场所以及混合方法学来整合技术安全工作和规范伦理，以构建既稳健又公正的AI系统。

Abstract: While much research in artificial intelligence (AI) has focused on scaling capabilities, the accelerating pace of development makes countervailing work on producing harmless, "aligned" systems increasingly urgent. Yet research on alignment has diverged along two largely parallel tracks: safety--centered on scaled intelligence, deceptive or scheming behaviors, and existential risk--and ethics--focused on present harms, the reproduction of social bias, and flaws in production pipelines. Although both communities warn of insufficient investment in alignment, they disagree on what alignment means or ought to mean. As a result, their efforts have evolved in relative isolation, shaped by distinct methodologies, institutional homes, and disciplinary genealogies.
  We present a large-scale, quantitative study showing the structural split between AI safety and AI ethics. Using a bibliometric and co-authorship network analysis of 6,442 papers from twelve major ML and NLP conferences (2020-2025), we find that over 80% of collaborations occur within either the safety or ethics communities, and cross-field connectivity is highly concentrated: roughly 5% of papers account for more than 85% of bridging links. Removing a small number of these brokers sharply increases segregation, indicating that cross-disciplinary exchange depends on a handful of actors rather than broad, distributed collaboration. These results show that the safety-ethics divide is not only conceptual but institutional, with implications for research agendas, policy, and venues. We argue that integrating technical safety work with normative ethics--via shared benchmarks, cross-institutional venues, and mixed-method methodologies--is essential for building AI systems that are both robust and just.

</details>


### [13] [Trustworthy Orchestration Artificial Intelligence by the Ten Criteria with Control-Plane Governance](https://arxiv.org/abs/2512.10304)
*Byeong Ho Kang,Wenli Yang,Muhammad Bilal Amin*

Main category: cs.AI

TL;DR: 提出"可信编排AI十项标准"框架，将治理架构嵌入AI生态系统执行层，确保可验证、透明、可重现和人类有意义控制


<details>
  <summary>Details</summary>
Motivation: AI系统在决策中扮演日益重要角色，但技术能力与制度问责之间存在日益扩大的差距，仅靠伦理指导不足以应对这一挑战，需要将治理嵌入生态系统执行架构中

Method: 提出"可信编排AI十项标准"综合保障框架，整合人类输入、语义一致性、审计和溯源完整性，构建统一的控制面板架构，借鉴国际标准和澳大利亚国家AI保障框架

Result: 展示可信度可以通过工程方法系统性地融入AI系统，确保执行层保持可验证、透明、可重现和人类有意义控制

Conclusion: 该框架为整个AI组件、消费者和人类参与者提供治理保护伞，与传统仅关注AI间协调的智能体AI倡议不同，能够有效应对AI决策中的问责挑战

Abstract: As Artificial Intelligence (AI) systems increasingly assume consequential decision-making roles, a widening gap has emerged between technical capabilities and institutional accountability. Ethical guidance alone is insufficient to counter this challenge; it demands architectures that embed governance into the execution fabric of the ecosystem. This paper presents the Ten Criteria for Trustworthy Orchestration AI, a comprehensive assurance framework that integrates human input, semantic coherence, audit and provenance integrity into a unified Control-Panel architecture. Unlike conventional agentic AI initiatives that primarily focus on AI-to-AI coordination, the proposed framework provides an umbrella of governance to the entire AI components, their consumers and human participants. By taking aspiration from international standards and Australia's National Framework for AI Assurance initiative, this work demonstrates that trustworthiness can be systematically incorporated (by engineering) into AI systems, ensuring the execution fabric remains verifiable, transparent, reproducible and under meaningful human control.

</details>


### [14] [Exploring LLMs for Scientific Information Extraction Using The SciEx Framework](https://arxiv.org/abs/2512.10004)
*Sha Li,Ayush Sadekar,Nathan Self,Yiqi Su,Lars Andersland,Mira Chaplin,Annabel Zhang,Hyoju Yang,James B Henderson,Krista Wigginton,Linsey Marr,T. M. Murali,Naren Ramakrishnan*

Main category: cs.AI

TL;DR: SciEx是一个模块化框架，用于从科学文献中提取细粒度信息，解决了长文档、多模态内容和快速变化的提取模式等挑战。


<details>
  <summary>Details</summary>
Motivation: 现有LLM工具难以处理科学文献的现实挑战：长上下文文档、多模态内容，以及跨多篇文献提取不一致的细粒度信息并标准化。当数据模式或提取本体快速变化时，重新架构或微调现有系统更加困难。

Method: 提出SciEx模块化框架，解耦PDF解析、多模态检索、提取和聚合等关键组件。这种设计简化了按需数据提取，同时支持新模型、提示策略和推理机制的灵活集成和扩展。

Result: 在三个科学主题的数据集上评估SciEx提取细粒度信息的准确性和一致性。研究结果提供了关于当前基于LLM的提取流程的优势和局限性的实用见解。

Conclusion: SciEx框架通过模块化设计有效解决了科学文献信息提取中的关键挑战，为快速变化的提取需求提供了灵活可扩展的解决方案。

Abstract: Large language models (LLMs) are increasingly touted as powerful tools for automating scientific information extraction. However, existing methods and tools often struggle with the realities of scientific literature: long-context documents, multi-modal content, and reconciling varied and inconsistent fine-grained information across multiple publications into standardized formats. These challenges are further compounded when the desired data schema or extraction ontology changes rapidly, making it difficult to re-architect or fine-tune existing systems. We present SciEx, a modular and composable framework that decouples key components including PDF parsing, multi-modal retrieval, extraction, and aggregation. This design streamlines on-demand data extraction while enabling extensibility and flexible integration of new models, prompting strategies, and reasoning mechanisms. We evaluate SciEx on datasets spanning three scientific topics for its ability to extract fine-grained information accurately and consistently. Our findings provide practical insights into both the strengths and limitations of current LLM-based pipelines.

</details>


### [15] [The 2025 Foundation Model Transparency Index](https://arxiv.org/abs/2512.10169)
*Alexander Wan,Kevin Klyman,Sayash Kapoor,Nestor Maslej,Shayne Longpre,Betty Xiong,Percy Liang,Rishi Bommasani*

Main category: cs.AI

TL;DR: 2025年基础模型透明度指数显示，主要AI公司透明度从2024年的58分降至40分，透明度恶化，IBM以95分领先，xAI和Midjourney仅14分垫底。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型开发商成为全球最重要的公司，研究其透明度实践如何演变至关重要。2025年FMTI旨在量化评估这些公司的透明度水平，特别是新增了数据获取、使用数据和监控等指标。

Method: 采用年度透明度指数方法，新增数据获取、使用数据和监控相关指标，首次评估阿里巴巴、DeepSeek和xAI等公司，对基础模型开发商进行系统量化评估。

Result: 透明度整体恶化：平均分从2024年的58分降至2025年的40分；IBM表现最佳（95分），xAI和Midjourney最差（14分）；公司在训练数据、训练计算以及部署后使用和影响方面最不透明。

Conclusion: 基础模型开发商透明度普遍下降，需要更积极的政策干预来弥补关键信息缺口。前沿模型论坛成员处于中间位置，既避免低分声誉损害，又缺乏成为透明度领导者的动力。

Abstract: Foundation model developers are among the world's most important companies. As these companies become increasingly consequential, how do their transparency practices evolve? The 2025 Foundation Model Transparency Index is the third edition of an annual effort to characterize and quantify the transparency of foundation model developers. The 2025 FMTI introduces new indicators related to data acquisition, usage data, and monitoring and evaluates companies like Alibaba, DeepSeek, and xAI for the first time. The 2024 FMTI reported that transparency was improving, but the 2025 FMTI finds this progress has deteriorated: the average score out of 100 fell from 58 in 2024 to 40 in 2025. Companies are most opaque about their training data and training compute as well as the post-deployment usage and impact of their flagship models. In spite of this general trend, IBM stands out as a positive outlier, scoring 95, in contrast to the lowest scorers, xAI and Midjourney, at just 14. The five members of the Frontier Model Forum we score end up in the middle of the Index: we posit that these companies avoid reputational harms from low scores but lack incentives to be transparency leaders. As policymakers around the world increasingly mandate certain types of transparency, this work reveals the current state of transparency for foundation model developers, how it may change given newly enacted policy, and where more aggressive policy interventions are necessary to address critical information deficits.

</details>


### [16] [DynaMate: An Autonomous Agent for Protein-Ligand Molecular Dynamics Simulations](https://arxiv.org/abs/2512.10034)
*Salomé Guilbert,Cassandra Masschelein,Jeremy Goumaz,Bohdan Naida,Philippe Schwaller*

Main category: cs.AI

TL;DR: DynaMate是一个基于多智能体LLM的自动化框架，能够自主设计和执行蛋白质-配体系统的分子动力学模拟完整工作流程，包括结合自由能计算。


<details>
  <summary>Details</summary>
Motivation: 尽管分子动力学模拟在药物发现和蛋白质工程中广泛应用，但其技术复杂性（参数化、输入准备、软件配置）仍然是广泛高效使用的主要障碍。目前尚未有成功的自动化解决方案。

Method: DynaMate是一个模块化多智能体框架，包含三个专门模块：实验规划、模拟执行和结果分析。集成了动态工具使用、网络搜索、PaperQA和自校正行为，能够自主执行完整的MD工作流程。

Result: 在12个不同复杂度的基准系统上评估，DynaMate能够可靠地执行完整的MD模拟，通过迭代推理纠正运行时错误，并产生有意义的蛋白质-配体相互作用分析。

Conclusion: 该自动化框架为未来生物分子和药物设计应用中的标准化、可扩展和时间高效的分子建模流程铺平了道路。

Abstract: Force field-based molecular dynamics (MD) simulations are indispensable for probing the structure, dynamics, and functions of biomolecular systems, including proteins and protein-ligand complexes. Despite their broad utility in drug discovery and protein engineering, the technical complexity of MD setup, encompassing parameterization, input preparation, and software configuration, remains a major barrier for widespread and efficient usage. Agentic LLMs have demonstrated their capacity to autonomously execute multi-step scientific processes, and to date, they have not successfully been used to automate protein-ligand MD workflows. Here, we present DynaMate, a modular multi-agent framework that autonomously designs and executes complete MD workflows for both protein and protein-ligand systems, and offers free energy binding affinity calculations with the MM/PB(GB)SA method. The framework integrates dynamic tool use, web search, PaperQA, and a self-correcting behavior. DynaMate comprises three specialized modules, interacting to plan the experiment, perform the simulation, and analyze the results. We evaluated its performance across twelve benchmark systems of varying complexity, assessing success rate, efficiency, and adaptability. DynaMate reliably performed full MD simulations, corrected runtime errors through iterative reasoning, and produced meaningful analyses of protein-ligand interactions. This automated framework paves the way toward standardized, scalable, and time-efficient molecular modeling pipelines for future biomolecular and drug design applications.

</details>


### [17] [EpiPlanAgent: Agentic Automated Epidemic Response Planning](https://arxiv.org/abs/2512.10313)
*Kangkun Mao,Fang Xu,Jinru Ding,Yidong Jiang,Yujun Yao,Yirong Chen,Junming Liu,Xiaoqin Wu,Qian Wu,Xiaoyan Huang,Jie Xu*

Main category: cs.AI

TL;DR: EpiPlanAgent是一个基于大语言模型的多智能体系统，能够自动生成和验证数字应急响应计划，显著提高计划完整性和指南一致性，同时大幅缩短开发时间。


<details>
  <summary>Details</summary>
Motivation: 传统的流行病应急响应规划依赖劳动密集型的人工方法，效率低下且难以规模化。需要开发自动化系统来改进公共卫生应急准备。

Method: 设计了基于大语言模型的多智能体框架EpiPlanAgent，整合了任务分解、知识基础和模拟模块。公共卫生专业人员使用真实世界疫情场景在受控环境中测试系统。

Result: EpiPlanAgent显著提高了计划的完整性和指南一致性，同时大幅减少了开发时间。专家评估确认AI生成内容与人工编写内容高度一致，用户反馈显示系统具有强感知效用。

Conclusion: EpiPlanAgent为智能流行病应急响应规划提供了有效、可扩展的解决方案，展示了智能体AI在转变公共卫生准备方面的潜力。

Abstract: Epidemic response planning is essential yet traditionally reliant on labor-intensive manual methods. This study aimed to design and evaluate EpiPlanAgent, an agent-based system using large language models (LLMs) to automate the generation and validation of digital emergency response plans. The multi-agent framework integrated task decomposition, knowledge grounding, and simulation modules. Public health professionals tested the system using real-world outbreak scenarios in a controlled evaluation. Results demonstrated that EpiPlanAgent significantly improved the completeness and guideline alignment of plans while drastically reducing development time compared to manual workflows. Expert evaluation confirmed high consistency between AI-generated and human-authored content. User feedback indicated strong perceived utility. In conclusion, EpiPlanAgent provides an effective, scalable solution for intelligent epidemic response planning, demonstrating the potential of agentic AI to transform public health preparedness.

</details>


### [18] [SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and Collaboration](https://arxiv.org/abs/2512.10046)
*Yan Zhuang,Jiawei Ren,Xiaokang Ye,Jianzhi Shen,Ruixuan Zhang,Tianai Yue,Muhammad Faayez,Xuhong He,Ziqiao Ma,Lianhui Qin,Zhiting Hu,Tianmin Shu*

Main category: cs.AI

TL;DR: SWR是一个基于UE5构建的大规模、逼真城市环境仿真平台，用于具身AI研究，包含两个机器人基准测试：多模态指令跟随和多智能体搜索任务。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型机器人研究主要集中于室内家庭场景，缺乏针对大规模城市环境的具身AI平台和评估基准。

Method: 基于Unreal Engine 5构建SWR平台，程序化生成无限逼真的城市场景，包含动态元素（行人、交通系统），支持多机器人控制和通信，并建立两个基准测试任务。

Result: 实验表明，包括视觉语言模型在内的最先进模型在SWR任务上表现不佳，缺乏城市环境所需的鲁棒感知、推理和规划能力。

Conclusion: SWR平台填补了城市环境具身AI研究的空白，提出的基准测试全面评估了机器人在真实场景中的关键能力，揭示了当前模型的局限性。

Abstract: Recent advances in foundation models have shown promising results in developing generalist robotics that can perform diverse tasks in open-ended scenarios given multimodal inputs. However, current work has been mainly focused on indoor, household scenarios. In this work, we present SimWorld-Robotics~(SWR), a simulation platform for embodied AI in large-scale, photorealistic urban environments. Built on Unreal Engine 5, SWR procedurally generates unlimited photorealistic urban scenes populated with dynamic elements such as pedestrians and traffic systems, surpassing prior urban simulations in realism, complexity, and scalability. It also supports multi-robot control and communication. With these key features, we build two challenging robot benchmarks: (1) a multimodal instruction-following task, where a robot must follow vision-language navigation instructions to reach a destination in the presence of pedestrians and traffic; and (2) a multi-agent search task, where two robots must communicate to cooperatively locate and meet each other. Unlike existing benchmarks, these two new benchmarks comprehensively evaluate a wide range of critical robot capacities in realistic scenarios, including (1) multimodal instructions grounding, (2) 3D spatial reasoning in large environments, (3) safe, long-range navigation with people and traffic, (4) multi-robot collaboration, and (5) grounded communication. Our experimental results demonstrate that state-of-the-art models, including vision-language models (VLMs), struggle with our tasks, lacking robust perception, reasoning, and planning abilities necessary for urban environments.

</details>


### [19] [Challenges of Evaluating LLM Safety for User Welfare](https://arxiv.org/abs/2512.10687)
*Manon Kempermann,Sai Suresh Macharla Vasu,Mahalakshmi Raveenthiran,Theo Farrell,Ingmar Weber*

Main category: cs.AI

TL;DR: 该研究指出当前LLM安全评估主要关注通用风险，但缺乏针对个人用户福利的评估。通过实验发现，评估者需要了解用户背景才能准确评估建议安全性，而仅靠用户主动披露背景信息是不够的。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的安全评估主要关注通用风险（如危险能力），但数百万用户使用LLM获取金融、健康等高风险领域的个人建议，这些危害是情境依赖的而非通用的。虽然OECD等框架认识到需要评估个体风险，但用户福利安全评估仍然不成熟。

Method: 研究评估了GPT-5、Claude Sonnet 4和Gemini 2.5 Pro在金融和健康建议方面的表现，针对不同脆弱程度的用户档案。首先比较了有无用户背景信息的评估者评分差异，然后测试了用户主动披露背景信息是否能改善评估效果。

Result: 1) 评估者必须了解用户背景：相同LLM回复，了解用户背景的评估者给出的安全评分显著低于不了解背景的评估者（高脆弱用户安全评分从5/7降至3/7）。2) 仅靠用户主动披露背景信息无法显著改善评估效果，特别是对脆弱人群。

Conclusion: 有效的用户福利安全评估需要评估者针对多样化用户档案评估回复，仅靠用户主动披露背景信息是不够的。研究提供了情境感知评估的方法论，证明评估个体福利需要不同于现有通用风险框架的方法。

Abstract: Safety evaluations of large language models (LLMs) typically focus on universal risks like dangerous capabilities or undesirable propensities. However, millions use LLMs for personal advice on high-stakes topics like finance and health, where harms are context-dependent rather than universal. While frameworks like the OECD's AI classification recognize the need to assess individual risks, user-welfare safety evaluations remain underdeveloped. We argue that developing such evaluations is non-trivial due to fundamental questions about accounting for user context in evaluation design. In this exploratory study, we evaluated advice on finance and health from GPT-5, Claude Sonnet 4, and Gemini 2.5 Pro across user profiles of varying vulnerability. First, we demonstrate that evaluators must have access to rich user context: identical LLM responses were rated significantly safer by context-blind evaluators than by those aware of user circumstances, with safety scores for high-vulnerability users dropping from safe (5/7) to somewhat unsafe (3/7). One might assume this gap could be addressed by creating realistic user prompts containing key contextual information. However, our second study challenges this: we rerun the evaluation on prompts containing context users report they would disclose, finding no significant improvement. Our work establishes that effective user-welfare safety evaluation requires evaluators to assess responses against diverse user profiles, as realistic user context disclosure alone proves insufficient, particularly for vulnerable populations. By demonstrating a methodology for context-aware evaluation, this study provides both a starting point for such assessments and foundational evidence that evaluating individual welfare demands approaches distinct from existing universal-risk frameworks. We publish our code and dataset to aid future developments.

</details>


### [20] [Parallel Decoder Transformer: Model-Internal Parallel Decoding with Speculative Invariance via Note Conditioning](https://arxiv.org/abs/2512.10054)
*Logan Robbins*

Main category: cs.AI

TL;DR: PDT是一种参数高效的并行解码Transformer架构，通过注入轻量级适配器让冻结的预训练模型实现并行解码，解决了传统方法中的连贯性漂移问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的自回归解码本质上是顺序的，导致延迟随输出长度线性增长。现有的"分解与填充"方法虽然尝试并行化生成，但缺乏跨流通信会导致连贯性漂移问题。

Method: 提出并行解码Transformer（PDT），通过注入轻量级的推测性笔记条件（SNC）适配器，允许并行解码流通过共享的动态潜在空间进行同步。将协调问题形式化为推测共识问题，子流将语义"笔记"广播到全局总线，由学习的验证头进行门控。

Result: 在冻结的200亿参数骨干模型上，使用5万步课程进行验证。PDT实现了有效的自校正，在覆盖预测中达到77.8%的精确度，无需修改主干权重即可恢复近似的串行语义。

Conclusion: PDT为结构化并行生成提供了一种可扩展、高效的替代方案，无需进行完整的模型微调，通过参数高效的方式解决了并行解码中的协调问题。

Abstract: Autoregressive decoding in Large Language Models (LLMs) is inherently sequential, creating a latency bottleneck that scales linearly with output length. While ``Decomposition-and-Fill'' methods like Skeleton-of-Thought attempt to parallelize generation via external orchestration, they suffer from \textit{coherence drift} due to the lack of cross-stream communication. In this work, we introduce the \textbf{Parallel Decoder Transformer (PDT)}, a parameter-efficient architecture that embeds coordination primitives directly into the inference process of a frozen pre-trained model.
  Instead of retraining the base model, PDT injects lightweight \textit{Speculative Note Conditioning (SNC)} adapters that allow parallel decoding streams to synchronize via a shared, dynamic latent space. We formulate coordination as a \textit{speculative consensus} problem, where sibling streams broadcast semantic ``notes'' to a global bus, gated by a learned verification head. We validate our approach on a 50,000-step curriculum using a frozen 20B-parameter backbone. Our results demonstrate that PDT achieves effective self-correction, reaching \textbf{77.8\% precision} in coverage prediction and recovering approximate serial semantics without modifying the trunk weights. This establishes PDT as a scalable, efficient alternative to full model fine-tuning for structured parallel generation.

</details>


### [21] [Linear socio-demographic representations emerge in Large Language Models from indirect cues](https://arxiv.org/abs/2512.10065)
*Paul Bouchaud,Pedro Ramaciotti*

Main category: cs.AI

TL;DR: LLMs在激活空间中形成用户社会人口属性的线性表征，这些表征基于姓名、职业等间接线索，并能影响下游行为如职业推荐，即使通过偏见测试的模型仍可能隐含偏见。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs如何通过姓名、职业等间接线索编码对话伙伴的社会人口属性（性别、种族等），以及这些隐含表征如何影响模型行为，揭示即使通过偏见测试的模型仍可能隐含偏见的问题。

Method: 在四个开源Transformer模型（Magistral 24B, Qwen3 14B, GPT-OSS 20B, OLMo2-1B）的残差流中探测社会人口属性的线性表征，使用显式人口属性披露进行训练，然后测试从姓名（性别、种族）和职业（劳动力统计相关）等隐含线索的预测能力。

Result: LLMs在激活空间中形成社会人口属性的线性表征，姓名激活与人口普查一致的性别和种族表征，职业触发与现实劳动力统计相关的表征。这些隐含表征能解释对话中形成的人口属性推断，并主动影响下游行为如职业推荐。

Conclusion: LLMs基于间接线索形成社会人口属性的隐含线性表征，这些表征能影响模型行为，表明即使通过偏见测试的模型仍可能隐含并利用偏见，对大规模应用时的公平性有重要影响。

Abstract: We investigate how LLMs encode sociodemographic attributes of human conversational partners inferred from indirect cues such as names and occupations. We show that LLMs develop linear representations of user demographics within activation space, wherein stereotypically associated attributes are encoded along interpretable geometric directions. We first probe residual streams across layers of four open transformer-based LLMs (Magistral 24B, Qwen3 14B, GPT-OSS 20B, OLMo2-1B) prompted with explicit demographic disclosure. We show that the same probes predict demographics from implicit cues: names activate census-aligned gender and race representations, while occupations trigger representations correlated with real-world workforce statistics. These linear representations allow us to explain demographic inferences implicitly formed by LLMs during conversation. We demonstrate that these implicit demographic representations actively shape downstream behavior, such as career recommendations. Our study further highlights that models that pass bias benchmark tests may still harbor and leverage implicit biases, with implications for fairness when applied at scale.

</details>


### [22] [Interpretable Embeddings with Sparse Autoencoders: A Data Analysis Toolkit](https://arxiv.org/abs/2512.10092)
*Nick Jiang,Xiaoqing Sun,Lisa Dunlap,Lewis Smith,Neel Nanda*

Main category: cs.AI

TL;DR: 使用稀疏自编码器（SAE）创建可解释的SAE嵌入，相比LLM更经济可靠，相比密集嵌入更可控，适用于大规模文本分析任务。


<details>
  <summary>Details</summary>
Motivation: 当前大规模文本分析主要依赖昂贵的LLM标注或缺乏可控性的密集嵌入模型，需要一种更经济、可靠且可控的分析方法。

Method: 提出使用稀疏自编码器（SAEs）创建SAE嵌入，其维度映射到可解释的概念，通过四个数据分析任务验证其有效性。

Result: SAE嵌入比LLM成本低2-8倍且更可靠，比密集嵌入更可控，能发现数据集语义差异、概念相关性，并在基于属性的检索中表现更好。

Conclusion: SAE是分析非结构化数据的多功能工具，强调通过数据解释模型的重要性，为模型行为分析提供了新视角。

Abstract: Analyzing large-scale text corpora is a core challenge in machine learning, crucial for tasks like identifying undesirable model behaviors or biases in training data. Current methods often rely on costly LLM-based techniques (e.g. annotating dataset differences) or dense embedding models (e.g. for clustering), which lack control over the properties of interest. We propose using sparse autoencoders (SAEs) to create SAE embeddings: representations whose dimensions map to interpretable concepts. Through four data analysis tasks, we show that SAE embeddings are more cost-effective and reliable than LLMs and more controllable than dense embeddings. Using the large hypothesis space of SAEs, we can uncover insights such as (1) semantic differences between datasets and (2) unexpected concept correlations in documents. For instance, by comparing model responses, we find that Grok-4 clarifies ambiguities more often than nine other frontier models. Relative to LLMs, SAE embeddings uncover bigger differences at 2-8x lower cost and identify biases more reliably. Additionally, SAE embeddings are controllable: by filtering concepts, we can (3) cluster documents along axes of interest and (4) outperform dense embeddings on property-based retrieval. Using SAE embeddings, we study model behavior with two case studies: investigating how OpenAI model behavior has changed over time and finding "trigger" phrases learned by Tulu-3 (Lambert et al., 2024) from its training data. These results position SAEs as a versatile tool for unstructured data analysis and highlight the neglected importance of interpreting models through their data.

</details>


### [23] [Robust AI Security and Alignment: A Sisyphean Endeavor?](https://arxiv.org/abs/2512.10100)
*Apostol Vassilev*

Main category: cs.AI

TL;DR: 该论文将哥德尔不完备性定理扩展到AI领域，建立了AI安全和对齐鲁棒性的信息论限制，并提供了应对这些挑战的实用方法。


<details>
  <summary>Details</summary>
Motivation: 了解AI鲁棒性的信息论限制对于负责任地采用AI技术至关重要，这些限制源于哥德尔不完备性定理在AI领域的扩展。

Method: 将哥德尔不完备性定理扩展到AI系统，建立信息论框架来分析AI安全和对齐的鲁棒性限制。

Result: 证明了AI安全和对齐存在根本性的信息论限制，这些限制无法完全克服，同时提供了应对这些挑战的实用方法。

Conclusion: AI系统在认知推理方面存在固有的局限性，了解这些限制并采取相应措施对于负责任地开发和部署AI技术至关重要。

Abstract: This manuscript establishes information-theoretic limitations for robustness of AI security and alignment by extending Gödel's incompleteness theorem to AI. Knowing these limitations and preparing for the challenges they bring is critically important for the responsible adoption of the AI technology. Practical approaches to dealing with these challenges are provided as well. Broader implications for cognitive reasoning limitations of AI systems are also proven.

</details>


### [24] [Modeling Narrative Archetypes in Conspiratorial Narratives: Insights from Singapore-Based Telegram Groups](https://arxiv.org/abs/2512.10105)
*Soorya Ram Shimgekar,Abhay Goyal,Lam Yin Cheung,Roy Ka-Wei Lee,Koustuv Saha,Pi Zonooz,Navin Kumar*

Main category: cs.AI

TL;DR: 本文提出一个两阶段计算框架分析新加坡Telegram群组中的阴谋论叙事，发现阴谋论内容融入日常讨论而非孤立存在，挑战了在线激进化假设。


<details>
  <summary>Details</summary>
Motivation: 阴谋论话语日益嵌入数字通信生态系统，但其结构和传播难以研究。现有研究常假设阴谋论存在于孤立回音室，但缺乏对日常讨论中阴谋论嵌入的实证分析。

Method: 1) 微调RoBERTa-large分类消息是否为阴谋论(F1=0.866)；2) 构建带符号的信念图，节点为消息，边符号反映信念标签对齐，权重为文本相似度；3) 提出带符号信念图神经网络(SiBeGNN)，使用符号解缠损失学习分离意识形态对齐和风格特征的嵌入；4) 对这些嵌入进行层次聚类识别叙事原型。

Result: 在553,648条消息中识别出7个叙事原型：法律话题、医疗关切、媒体讨论、金融、权威矛盾、群组管理、一般聊天。SiBeGNN聚类质量(cDBI=8.38)优于基线方法(13.60-67.27)，专家评估者间一致性达88%。发现阴谋论消息不仅出现在怀疑或不信任集群，也出现在金融、法律和日常事务的常规讨论中。

Conclusion: 阴谋论话语在普通社交互动中运作，挑战了在线激进化常见假设。该框架推进了信念驱动话语分析的计算方法，可用于立场检测、政治传播研究和内容审核政策。

Abstract: Conspiratorial discourse is increasingly embedded within digital communication ecosystems, yet its structure and spread remain difficult to study. This work analyzes conspiratorial narratives in Singapore-based Telegram groups, showing that such content is woven into everyday discussions rather than confined to isolated echo chambers. We propose a two-stage computational framework. First, we fine-tune RoBERTa-large to classify messages as conspiratorial or not, achieving an F1-score of 0.866 on 2,000 expert-labeled messages. Second, we build a signed belief graph in which nodes represent messages and edge signs reflect alignment in belief labels, weighted by textual similarity. We introduce a Signed Belief Graph Neural Network (SiBeGNN) that uses a Sign Disentanglement Loss to learn embeddings that separate ideological alignment from stylistic features.
  Using hierarchical clustering on these embeddings, we identify seven narrative archetypes across 553,648 messages: legal topics, medical concerns, media discussions, finance, contradictions in authority, group moderation, and general chat. SiBeGNN yields stronger clustering quality (cDBI = 8.38) than baseline methods (13.60 to 67.27), supported by 88 percent inter-rater agreement in expert evaluations. Our analysis shows that conspiratorial messages appear not only in clusters focused on skepticism or distrust, but also within routine discussions of finance, law, and everyday matters. These findings challenge common assumptions about online radicalization by demonstrating that conspiratorial discourse operates within ordinary social interaction. The proposed framework advances computational methods for belief-driven discourse analysis and offers applications for stance detection, political communication studies, and content moderation policy.

</details>


### [25] [AgriRegion: Region-Aware Retrieval for High-Fidelity Agricultural Advice](https://arxiv.org/abs/2512.10114)
*Mesafint Fanuel,Mahmoud Nabil Mahmoud,Crystal Cook Marshal,Vishal Lakhotia,Biswanath Dari,Kaushik Roy,Shaohu Zhang*

Main category: cs.AI

TL;DR: AgriRegion是一个针对农业领域的检索增强生成框架，通过地理空间元数据注入和区域优先重排序机制，减少LLM在农业咨询中的幻觉问题，提供区域感知的准确建议。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型在农业领域经常出现上下文幻觉问题，提供的建议可能在某个地区科学上正确，但在另一个地区由于土壤、气候和当地法规的差异而具有灾难性后果。需要专门针对农业领域的高保真、区域感知的咨询系统。

Method: AgriRegion是一个检索增强生成框架，包含地理空间元数据注入层和区域优先重排序机制。通过将知识库限制在已验证的当地农业推广服务，并在检索过程中强制执行地理空间约束，确保建议的本地准确性。

Result: 实验表明，AgriRegion相比最先进的LLM系统减少了10-20%的幻觉，并显著提高了信任分数。创建了包含12个农业子领域160个领域特定问题的新基准数据集AgriRegion-Eval。

Conclusion: AgriRegion框架通过区域感知的检索增强生成方法，有效解决了农业领域LLM的幻觉问题，为种植计划、害虫控制和施肥等农业活动提供了更准确、可靠的本地化建议。

Abstract: Large Language Models (LLMs) have demonstrated significant potential in democratizing access to information. However, in the domain of agriculture, general-purpose models frequently suffer from contextual hallucination, which provides non-factual advice or answers are scientifically sound in one region but disastrous in another due to variations in soil, climate, and local regulations. We introduce AgriRegion, a Retrieval-Augmented Generation (RAG) framework designed specifically for high-fidelity, region-aware agricultural advisory. Unlike standard RAG approaches that rely solely on semantic similarity, AgriRegion incorporates a geospatial metadata injection layer and a region-prioritized re-ranking mechanism. By restricting the knowledge base to verified local agricultural extension services and enforcing geo-spatial constraints during retrieval, AgriRegion ensures that the advice regarding planting schedules, pest control, and fertilization is locally accurate. We create a novel benchmark dataset, AgriRegion-Eval, which comprises 160 domain-specific questions across 12 agricultural subfields. Experiments demonstrate that AgriRegion reduces hallucinations by 10-20% compared to state-of-the-art LLMs systems and significantly improves trust scores according to a comprehensive evaluation.

</details>


### [26] [CP-Env: Evaluating Large Language Models on Clinical Pathways in a Controllable Hospital Environment](https://arxiv.org/abs/2512.10206)
*Yakun Zhu,Zhongzhen Huang,Qianhan Feng,Linjie Mu,Yannian Gu,Shaoting Zhang,Qi Dou,Xiaofan Zhang*

Main category: cs.AI

TL;DR: CP-Env是一个可控的医院环境模拟器，用于评估大型语言模型在端到端临床路径中的表现，包含患者和医生代理，支持从分诊到多学科会诊的复杂医疗场景模拟。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试主要关注静态考试或孤立对话，无法充分评估LLM在动态临床场景中的表现。医疗护理涉及复杂的临床路径和决策过程，需要更全面的评估框架。

Method: 开发CP-Env可控代理医院环境，模拟包含患者和医生代理的医院生态系统。构建从分诊、专科会诊到诊断测试和多学科团队会议的场景。提出包含临床效能、流程能力和职业道德的三层评估框架。

Result: 大多数模型在路径复杂性方面表现不佳，出现幻觉并丢失关键诊断细节。有趣的是，过多的推理步骤有时会产生反效果，而顶级模型倾向于通过内化知识减少工具依赖。

Conclusion: CP-Env通过全面的端到端临床评估推进医疗AI代理的发展，提供了基准测试和评估工具供进一步研究使用。

Abstract: Medical care follows complex clinical pathways that extend beyond isolated physician-patient encounters, emphasizing decision-making and transitions between different stages. Current benchmarks focusing on static exams or isolated dialogues inadequately evaluate large language models (LLMs) in dynamic clinical scenarios. We introduce CP-Env, a controllable agentic hospital environment designed to evaluate LLMs across end-to-end clinical pathways. CP-Env simulates a hospital ecosystem with patient and physician agents, constructing scenarios ranging from triage and specialist consultation to diagnostic testing and multidisciplinary team meetings for agent interaction. Following real hospital adaptive flow of healthcare, it enables branching, long-horizon task execution. We propose a three-tiered evaluation framework encompassing Clinical Efficacy, Process Competency, and Professional Ethics. Results reveal that most models struggle with pathway complexity, exhibiting hallucinations and losing critical diagnostic details. Interestingly, excessive reasoning steps can sometimes prove counterproductive, while top models tend to exhibit reduced tool dependency through internalized knowledge. CP-Env advances medical AI agents development through comprehensive end-to-end clinical evaluation. We provide the benchmark and evaluation tools for further research and development at https://github.com/SPIRAL-MED/CP-Env.

</details>


### [27] [An exploration for higher efficiency in multi objective optimisation with reinforcement learning](https://arxiv.org/abs/2512.10208)
*Mehmet Emin Aydin*

Main category: cs.AI

TL;DR: 该论文提出使用多目标强化学习来优化搜索算法中的算子序列选择，以解决单算子方法的效率问题


<details>
  <summary>Details</summary>
Motivation: 优化和搜索过程中的效率问题持续影响算法性能，使用多个算子代替单个算子处理邻域移动操作有潜力，但需要找到最优或接近最优的算子序列。虽然单目标优化已有相关研究，但多目标优化领域在这方面研究不足。

Method: 提出基于多目标强化学习的通用方法，通过强化学习来学习和优化算子序列的选择策略，以处理多目标优化问题。

Result: 论文概述了一个通用化方法，其中某些阶段已完成，某些阶段仍在进行中，旨在展示使用多目标强化学习的效率。

Conclusion: 多目标强化学习方法有望解决多目标优化中算子序列选择的问题，提高优化算法的效率和性能，但还需要进一步的研究和验证。

Abstract: Efficiency in optimisation and search processes persists to be one of the challenges, which affects the performance and use of optimisation algorithms. Utilising a pool of operators instead of a single operator to handle move operations within a neighbourhood remains promising, but an optimum or near optimum sequence of operators necessitates further investigation. One of the promising ideas is to generalise experiences and seek how to utilise it. Although numerous works are done around this issue for single objective optimisation, multi-objective cases have not much been touched in this regard. A generalised approach based on multi-objective reinforcement learning approach seems to create remedy for this issue and offer good solutions. This paper overviews a generalisation approach proposed with certain stages completed and phases outstanding that is aimed to help demonstrate the efficiency of using multi-objective reinforcement learning.

</details>


### [28] [ID-PaS : Identity-Aware Predict-and-Search for General Mixed-Integer Linear Programs](https://arxiv.org/abs/2512.10211)
*Junyang Cai,El Mehdi Er Raqabi,Pascal Van Hentenryck,Bistra Dilkina*

Main category: cs.AI

TL;DR: 本文提出ID-PaS框架，将预测-搜索方法扩展到参数化混合整数线性规划问题，通过身份感知学习处理异构变量，在多个大规模实际问题中优于Gurobi和传统PaS方法。


<details>
  <summary>Details</summary>
Motivation: 现有的预测-搜索方法主要局限于二元问题，且忽略了实际应用中常见的固定变量问题，无法有效处理参数化MIP中的异构变量。

Method: 扩展预测-搜索框架到参数化MIP问题，提出ID-PaS身份感知学习框架，使机器学习模型能够更有效地处理异构变量。

Result: 在多个真实世界大规模问题上进行实验，ID-PaS始终表现出优于最先进求解器Gurobi和传统PaS方法的性能。

Conclusion: ID-PaS框架成功将预测-搜索方法扩展到参数化MIP问题，通过身份感知学习有效处理异构变量，在实际应用中展现出优越性能。

Abstract: Mixed-Integer Linear Programs (MIPs) are powerful and flexible tools for modeling a wide range of real-world combinatorial optimization problems. Predict-and-Search methods operate by using a predictive model to estimate promising variable assignments and then guiding a search procedure toward high-quality solutions. Recent research has demonstrated that incorporating machine learning (ML) into the Predict-and-Search framework significantly enhances its performance. Still, it is restricted to binary problems and overlooks the presence of fixed variables that commonly arise in practical settings. This work extends the Predict-and-Search (PaS) framework to parametric MIPs and introduces ID-PaS, an identity-aware learning framework that enables the ML model to handle heterogeneous variables more effectively. Experiments on several real-world large-scale problems demonstrate that ID-PaS consistently achieves superior performance compared to the state-of-the-art solver Gurobi and PaS.

</details>


### [29] [Reverse Thinking Enhances Missing Information Detection in Large Language Models](https://arxiv.org/abs/2512.10273)
*Yuxin Liu,Chaojie Gu,Yihang Zhang,Bin Qian,Shibo He*

Main category: cs.AI

TL;DR: 提出反向思维框架，通过逆向推理识别缺失信息，提升LLMs在信息缺失任务上的表现


<details>
  <summary>Details</summary>
Motivation: LLMs在处理缺失信息问题时存在不完整回答、事实错误和幻觉等问题，传统的正向推理方法（如CoT、ToT）无法系统识别和恢复被省略的信息

Method: 提出基于逆向推理的反向思维框架，引导LLMs通过反向思考识别必要条件并定位缺失元素，将缺失信息识别任务转化为更易处理的逆向推理问题

Result: 实验结果表明，反向思维方法相比传统正向推理方法取得了显著的性能提升，为增强LLMs的逻辑完整性和推理鲁棒性提供了有前景的方向

Conclusion: 反向思维方法能有效提升LLMs在缺失信息检测任务上的表现，为解决LLMs在处理不完整信息时的局限性提供了新思路

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in various reasoning tasks, yet they often struggle with problems involving missing information, exhibiting issues such as incomplete responses, factual errors, and hallucinations. While forward reasoning approaches like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) have shown success in structured problem-solving, they frequently fail to systematically identify and recover omitted information. In this paper, we explore the potential of reverse thinking methodologies to enhance LLMs' performance on missing information detection tasks. Drawing inspiration from recent work on backward reasoning, we propose a novel framework that guides LLMs through reverse thinking to identify necessary conditions and pinpoint missing elements. Our approach transforms the challenging task of missing information identification into a more manageable backward reasoning problem, significantly improving model accuracy. Experimental results demonstrate that our reverse thinking approach achieves substantial performance gains compared to traditional forward reasoning methods, providing a promising direction for enhancing LLMs' logical completeness and reasoning robustness.

</details>


### [30] [Neuronal Attention Circuit (NAC) for Representation Learning](https://arxiv.org/abs/2512.10282)
*Waleed Razzaq,Izis Kankaraway,Yun-Bo Zhao*

Main category: cs.AI

TL;DR: 提出Neuronal Attention Circuit (NAC)，一种生物启发的连续时间注意力机制，通过ODE求解注意力logits，支持三种计算模式，在多个领域表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统注意力机制具有离散性，限制了连续时间建模能力。需要一种既能保持注意力优势又能进行连续时间建模的生物合理机制。

Method: 将注意力logits计算重新表述为一阶线性ODE的解，利用C. elegans神经元电路策略的稀疏门控机制，用稀疏感官门代替密集投影，支持三种计算模式（显式欧拉积分、精确闭式解、稳态近似）。

Result: NAC在多个领域（不规则时间序列分类、自动驾驶车道保持、工业预测）中匹配或优于基线方法，在运行时间和内存效率上处于中等位置，同时提供理论保证（状态稳定性、有界近似误差、通用近似性）。

Conclusion: NAC成功将注意力机制扩展到连续时间领域，提供了一种生物合理、理论保证的连续时间注意力框架，在保持性能的同时实现了连续时间建模能力。

Abstract: Attention improves representation learning over RNNs, but its discrete nature limits continuous-time (CT) modeling. We introduce Neuronal Attention Circuit (NAC), a novel, biologically plausible CT-Attention mechanism that reformulates attention logits computation as the solution to a linear first-order ODE with nonlinear interlinked gates derived from repurposing \textit{C. elegans} Neuronal Circuit Policies (NCPs) wiring mechanism. NAC replaces dense projections with sparse sensory gates for key-query projections and a sparse backbone network with two heads for computing \textit{content-target} and \textit{learnable time-constant} gates, enabling efficient adaptive dynamics. NAC supports three attention logit computation modes: (i) explicit Euler integration, (ii) exact closed-form solution, and (iii) steady-state approximation. To improve memory intensity, we implemented a sparse Top-\emph{K} pairwise concatenation scheme that selectively curates key-query interactions. We provide rigorous theoretical guarantees, including state stability, bounded approximation errors, and universal approximation. Empirically, we implemented NAC in diverse domains, including irregular time-series classification, lane-keeping for autonomous vehicles, and industrial prognostics. We observed that NAC matches or outperforms competing baselines in accuracy and occupies an intermediate position in runtime and memory efficiency compared with several CT baselines.

</details>


### [31] [Investigating The Functional Roles of Attention Heads in Vision Language Models: Evidence for Reasoning Modules](https://arxiv.org/abs/2512.10300)
*Yanbei Jiang,Xueqi Ma,Shu Liu,Sarah Monazam Erfani,Tongliang Liu,James Bailey,Jey Han Lau,Krista A. Ehinger*

Main category: cs.AI

TL;DR: 提出CogVision数据集和可解释性框架，系统分析视觉语言模型中注意力头的功能角色，发现功能头稀疏、分布不均且对多模态推理至关重要。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在多模态基准测试中表现出色，但其内部机制仍然是一个黑箱。需要系统分析VLMs的内部工作机制，特别是注意力头在多模态推理中的功能角色。

Method: 1) 引入CogVision数据集，将复杂多模态问题分解为模拟人类推理的逐步子问题；2) 使用基于探测的方法识别专门处理特定功能（如高级视觉接收和推理）的注意力头；3) 在不同VLM家族中进行分析，并进行干预实验（移除和强调功能头）。

Result: 1) 功能头普遍稀疏；2) 不同功能的功能头数量和分布不同；3) 功能头介导交互和层次组织；4) 移除功能头导致性能下降，强调功能头提高准确性。

Conclusion: 研究揭示了VLMs的认知组织结构，为设计更具人类对齐感知和推理能力的模型提供了新见解和方向。

Abstract: Despite excelling on multimodal benchmarks, vision-language models (VLMs) largely remain a black box. In this paper, we propose a novel interpretability framework to systematically analyze the internal mechanisms of VLMs, focusing on the functional roles of attention heads in multimodal reasoning. To this end, we introduce CogVision, a dataset that decomposes complex multimodal questions into step-by-step subquestions designed to simulate human reasoning through a chain-of-thought paradigm, with each subquestion associated with specific receptive or cognitive functions such as high-level visual reception and inference. Using a probing-based methodology, we identify attention heads that specialize in these functions and characterize them as functional heads. Our analysis across diverse VLM families reveals that these functional heads are universally sparse, vary in number and distribution across functions, and mediate interactions and hierarchical organization. Furthermore, intervention experiments demonstrate their critical role in multimodal reasoning: removing functional heads leads to performance degradation, while emphasizing them enhances accuracy. These findings provide new insights into the cognitive organization of VLMs and suggest promising directions for designing models with more human-aligned perceptual and reasoning abilities.

</details>


### [32] [InfoCom: Kilobyte-Scale Communication-Efficient Collaborative Perception with Information Bottleneck](https://arxiv.org/abs/2512.10305)
*Quanmin Wei,Penglin Dai,Wei Li,Bingyi Liu,Xiao Wu*

Main category: cs.AI

TL;DR: InfoCom是一个基于信息瓶颈理论的信息感知协作感知框架，通过信息纯化范式在保持感知性能的同时将通信开销从MB级降至KB级，实现440倍和90倍的通信减少。


<details>
  <summary>Details</summary>
Motivation: 现有协作感知方法面临通信与性能的权衡问题，通常假设MB级数据传输，但在实际网络约束下可能失效。需要建立通信高效的协作感知理论基础。

Method: 提出信息感知框架InfoCom，基于扩展的信息瓶颈原理：1)信息感知编码将特征压缩为最小消息；2)稀疏掩码生成以可忽略成本识别空间线索；3)多尺度解码通过掩码引导机制逐步恢复感知信息。

Result: 在多个数据集上的实验表明，InfoCom实现了近乎无损的感知性能，同时将通信开销从MB级降至KB级，相比Where2comm和ERMVP分别减少了440倍和90倍。

Conclusion: InfoCom建立了通信高效协作感知的理论基础，通过信息纯化范式而非传统特征操作，在保持感知性能的同时大幅降低通信开销，为实际网络约束下的自动驾驶系统提供了可行解决方案。

Abstract: Precise environmental perception is critical for the reliability of autonomous driving systems. While collaborative perception mitigates the limitations of single-agent perception through information sharing, it encounters a fundamental communication-performance trade-off. Existing communication-efficient approaches typically assume MB-level data transmission per collaboration, which may fail due to practical network constraints. To address these issues, we propose InfoCom, an information-aware framework establishing the pioneering theoretical foundation for communication-efficient collaborative perception via extended Information Bottleneck principles. Departing from mainstream feature manipulation, InfoCom introduces a novel information purification paradigm that theoretically optimizes the extraction of minimal sufficient task-critical information under Information Bottleneck constraints. Its core innovations include: i) An Information-Aware Encoding condensing features into minimal messages while preserving perception-relevant information; ii) A Sparse Mask Generation identifying spatial cues with negligible communication cost; and iii) A Multi-Scale Decoding that progressively recovers perceptual information through mask-guided mechanisms rather than simple feature reconstruction. Comprehensive experiments across multiple datasets demonstrate that InfoCom achieves near-lossless perception while reducing communication overhead from megabyte to kilobyte-scale, representing 440-fold and 90-fold reductions per agent compared to Where2comm and ERMVP, respectively.

</details>


### [33] [User-Feedback-Driven Continual Adaptation for Vision-and-Language Navigation](https://arxiv.org/abs/2512.10322)
*Yongqiang Yu,Xuhui Li,Hazza Mahmood,Jinxing Zhou,Haodong Hong,Longtao Jiang,Zhiqiang Xu,Qi Wu,Xiaojun Chang*

Main category: cs.AI

TL;DR: 提出用户反馈驱动的视觉语言导航适应框架，通过整合人类交互提升环境特定适应能力，在GSA-R2R基准上超越现有方法


<details>
  <summary>Details</summary>
Motivation: 当前GSA-VLN框架缺乏用户反馈，仅依赖无监督环境暴露适应，而实际部署中用户反馈能提供有价值的监督信息，显著提升适应质量

Method: 1) 将用户反馈（导航指令和纠正信号）转化为高质量环境对齐训练数据；2) 引入记忆库热启动机制复用先前环境知识；3) 在GSA-R2R基准上验证持续和混合适应设置

Result: 方法在GSA-R2R基准上持续超越GR-DUET等基线，提升导航成功率和路径效率；记忆库热启动稳定早期导航并减少更新后性能下降；在持续和混合适应设置下均表现稳健

Conclusion: 用户反馈驱动的适应框架有效缩小静态基准与真实部署差距，通过系统整合人类交互实现高效现实适应，记忆库机制确保稳定重新部署，在各种部署条件下持续改进

Abstract: Vision-and-Language Navigation (VLN) requires agents to navigate complex environments by following natural-language instructions. General Scene Adaptation for VLN (GSA-VLN) shifts the focus from zero-shot generalization to continual, environment-specific adaptation, narrowing the gap between static benchmarks and real-world deployment. However, current GSA-VLN frameworks exclude user feedback, relying solely on unsupervised adaptation from repeated environmental exposure. In practice, user feedback offers natural and valuable supervision that can significantly enhance adaptation quality. We introduce a user-feedback-driven adaptation framework that extends GSA-VLN by systematically integrating human interactions into continual learning. Our approach converts user feedback-navigation instructions and corrective signals-into high-quality, environment-aligned training data, enabling efficient and realistic adaptation. A memory-bank warm-start mechanism further reuses previously acquired environmental knowledge, mitigating cold-start degradation and ensuring stable redeployment. Experiments on the GSA-R2R benchmark show that our method consistently surpasses strong baselines such as GR-DUET, improving navigation success and path efficiency. The memory-bank warm start stabilizes early navigation and reduces performance drops after updates. Results under both continual and hybrid adaptation settings confirm the robustness and generality of our framework, demonstrating sustained improvement across diverse deployment conditions.

</details>


### [34] [On the Collapse of Generative Paths: A Criterion and Correction for Diffusion Steering](https://arxiv.org/abs/2512.10339)
*Ziseok Lee,Minyeong Hwang,Sanghyun Jo,Wooyeol Lee,Jihyung Ko,Young Bin Park,Jae-Mun Choi,Eunho Yang,Kyungsu Kim*

Main category: cs.AI

TL;DR: 论文提出了ACE方法，解决了异质扩散模型组合时的边缘路径崩溃问题，实现了稳定的概率路径生成。


<details>
  <summary>Details</summary>
Motivation: 现有的密度比方法在组合不同噪声调度或数据集的异质模型时，会出现中间密度不可归一化的边缘路径崩溃问题，这在分子设计等需要组合多个专家模型的任务中尤为严重。

Method: 提出了ACE方法：1) 推导了路径存在性准则，仅从噪声调度和指数就能预测崩溃何时发生；2) 引入自适应路径校正，将Feynman-Kac引导扩展到时变指数，保证有效的概率路径。

Result: 在合成2D基准测试和柔性姿态支架装饰任务中，ACE消除了崩溃，实现了高引导的组合生成，在分布和对接指标上优于恒定指数基线和专门的任务特定模型。

Conclusion: ACE将密度比引导从一种不稳定的启发式方法转变为可控生成的可靠工具，使异质专家模型的组合变得稳定可靠。

Abstract: Inference-time steering enables pretrained diffusion/flow models to be adapted to new tasks without retraining. A widely used approach is the ratio-of-densities method, which defines a time-indexed target path by reweighting probability-density trajectories from multiple models with positive, or in some cases, negative exponents. This construction, however, harbors a critical and previously unformalized failure mode: Marginal Path Collapse, where intermediate densities become non-normalizable even though endpoints remain valid. Collapse arises systematically when composing heterogeneous models trained on different noise schedules or datasets, including a common setting in molecular design where de-novo, conformer, and pocket-conditioned models must be combined for tasks such as flexible-pose scaffold decoration. We provide a novel and complete solution for the problem. First, we derive a simple path existence criterion that predicts exactly when collapse occurs from noise schedules and exponents alone. Second, we introduce Adaptive path Correction with Exponents (ACE), which extends Feynman-Kac steering to time-varying exponents and guarantees a valid probability path. On a synthetic 2D benchmark and on flexible-pose scaffold decoration, ACE eliminates collapse and enables high-guidance compositional generation, improving distributional and docking metrics over constant-exponent baselines and even specialized task-specific scaffold decoration models. Our work turns ratio-of-densities steering with heterogeneous experts from an unstable heuristic into a reliable tool for controllable generation.

</details>


### [35] [REMISVFU: Vertical Federated Unlearning via Representation Misdirection for Intermediate Output Feature](https://arxiv.org/abs/2512.10348)
*Wenhan Wu,Zhili He,Huanghuang Liang,Yili Gong,Jiawei Jiang,Chuang Hu,Dazhao Cheng*

Main category: cs.AI

TL;DR: REMISVFU：一种用于垂直联邦学习的表示误导框架，通过将遗忘方的编码器输出坍缩到单位球面上的随机锚点，实现快速客户端级遗忘，同时保持剩余方的模型效用。


<details>
  <summary>Details</summary>
Motivation: 现有联邦遗忘技术主要针对水平联邦学习（HFL），而垂直联邦学习（VFL）的特征分区架构使得HFL遗忘方法无效。GDPR等数据保护法规要求联邦系统支持参与方的"被遗忘权"，因此需要专门针对VFL的遗忘方法。

Method: 提出REMISVFU框架：1）遗忘方将编码器输出坍缩到单位球面上的随机锚点，切断特征与全局模型的统计联系；2）服务器联合优化保留损失和遗忘损失，通过正交投影对齐梯度以消除破坏性干扰。

Result: 在公开基准测试中，REMISVFU将后门攻击成功率抑制到自然类先验水平，仅牺牲约2.5%的干净准确率，优于现有最先进基线方法。

Conclusion: REMISVFU是一种即插即用的表示误导框架，能够有效实现垂直联邦学习中的快速客户端级遗忘，在保护遗忘权利的同时保持剩余方的模型效用。

Abstract: Data-protection regulations such as the GDPR grant every participant in a federated system a right to be forgotten. Federated unlearning has therefore emerged as a research frontier, aiming to remove a specific party's contribution from the learned model while preserving the utility of the remaining parties. However, most unlearning techniques focus on Horizontal Federated Learning (HFL), where data are partitioned by samples. In contrast, Vertical Federated Learning (VFL) allows organizations that possess complementary feature spaces to train a joint model without sharing raw data. The resulting feature-partitioned architecture renders HFL-oriented unlearning methods ineffective. In this paper, we propose REMISVFU, a plug-and-play representation misdirection framework that enables fast, client-level unlearning in splitVFL systems. When a deletion request arrives, the forgetting party collapses its encoder output to a randomly sampled anchor on the unit sphere, severing the statistical link between its features and the global model. To maintain utility for the remaining parties, the server jointly optimizes a retention loss and a forgetting loss, aligning their gradients via orthogonal projection to eliminate destructive interference. Evaluations on public benchmarks show that REMISVFU suppresses back-door attack success to the natural class-prior level and sacrifices only about 2.5% points of clean accuracy, outperforming state-of-the-art baselines.

</details>


### [36] [LLM-Empowered Representation Learning for Emerging Item Recommendation](https://arxiv.org/abs/2512.10370)
*Ziying Zhang,Quanming Yao,Yaqing Wang*

Main category: cs.AI

TL;DR: EmerFlow：一个基于LLM的新兴物品推荐框架，通过LLM推理增强特征表示，并与现有推荐模型对齐，仅需有限交互即可学习表达性嵌入


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设新兴物品只有很少甚至没有历史交互，这种假设过于简化问题。好的模型需要在保持新兴物品独特性的同时，利用其与成熟物品的共享模式

Method: 提出EmerFlow框架：1）通过LLM推理增强新兴物品的原始特征；2）将这些表示与现有推荐模型的嵌入空间对齐；3）通过元学习整合新交互以优化嵌入

Result: 在电影和医药等多个领域的广泛实验表明，EmerFlow始终优于现有方法

Conclusion: EmerFlow能够仅从有限交互中学习新兴物品的表达性嵌入，有效解决了新兴物品推荐中的动态积累挑战

Abstract: In this work, we tackle the challenge of recommending emerging items, whose interactions gradually accumulate over time. Existing methods often overlook this dynamic process, typically assuming that emerging items have few or even no historical interactions. Such an assumption oversimplifies the problem, as a good model must preserve the uniqueness of emerging items while leveraging their shared patterns with established ones. To address this challenge, we propose EmerFlow, a novel LLM-empowered representation learning framework that generates distinctive embeddings for emerging items. It first enriches the raw features of emerging items through LLM reasoning, then aligns these representations with the embedding space of the existing recommendation model. Finally, new interactions are incorporated through meta-learning to refine the embeddings. This enables EmerFlow to learn expressive embeddings for emerging items from only limited interactions. Extensive experiments across diverse domains, including movies and pharmaceuticals, show that EmerFlow consistently outperforms existing methods.

</details>


### [37] [AgentProg: Empowering Long-Horizon GUI Agents with Program-Guided Context Management](https://arxiv.org/abs/2512.10371)
*Shizuo Tian,Hao Wen,Yuxuan Chen,Jiacheng Liu,Shanhui Zhao,Guohong Liu,Ju Ren,Yunxin Liu,Yuanchun Li*

Main category: cs.AI

TL;DR: AgentProg：一种基于程序引导的移动GUI代理上下文管理方法，通过将交互历史重构为程序结构来减少上下文开销，同时保持任务性能


<details>
  <summary>Details</summary>
Motivation: 移动GUI代理在长时程任务自动化中面临关键瓶颈：依赖不断扩展的交互历史会导致大量上下文开销，现有上下文管理和压缩技术往往无法保留重要语义信息，导致任务性能下降

Method: 提出AgentProg，将交互历史重构为具有变量和控制流的程序结构，基于程序结构确定哪些信息应保留、哪些可丢弃；同时集成受Belief MDP框架启发的全局信念状态机制来处理部分可观测性和适应环境变化

Result: 在AndroidWorld和扩展的长时程任务套件上实现了最先进的成功率，在长时程任务中保持稳健性能，而基线方法则出现灾难性性能下降

Conclusion: AgentProg通过程序化组织交互历史，有效解决了移动GUI代理的上下文管理问题，在保持任务性能的同时显著减少上下文开销，为长时程任务自动化提供了有效解决方案

Abstract: The rapid development of mobile GUI agents has stimulated growing research interest in long-horizon task automation. However, building agents for these tasks faces a critical bottleneck: the reliance on ever-expanding interaction history incurs substantial context overhead. Existing context management and compression techniques often fail to preserve vital semantic information, leading to degraded task performance. We propose AgentProg, a program-guided approach for agent context management that reframes the interaction history as a program with variables and control flow. By organizing information according to the structure of program, this structure provides a principled mechanism to determine which information should be retained and which can be discarded. We further integrate a global belief state mechanism inspired by Belief MDP framework to handle partial observability and adapt to unexpected environmental changes. Experiments on AndroidWorld and our extended long-horizon task suite demonstrate that AgentProg has achieved the state-of-the-art success rates on these benchmarks. More importantly, it maintains robust performance on long-horizon tasks while baseline methods experience catastrophic degradation. Our system is open-sourced at https://github.com/MobileLLM/AgentProg.

</details>


### [38] [Boosting RL-Based Visual Reasoning with Selective Adversarial Entropy Intervention](https://arxiv.org/abs/2512.10414)
*Yang Yu,Zhuangzhuang Chen,Siqi Wang,Lanqing Li,Xiaomeng Li*

Main category: cs.AI

TL;DR: 提出SaEI方法，通过选择性对抗熵干预增强视觉语言模型的强化学习采样多样性，提升推理能力


<details>
  <summary>Details</summary>
Motivation: 现有基于RL的微调方法通常只在策略优化时干预熵，忽略了在RL采样阶段进行熵干预可以提升响应多样性，从而增强GRPO性能

Method: 提出选择性对抗熵干预(SaEI)：1) 熵引导对抗采样(EgAS)，将采样响应的熵作为对抗目标，生成对抗样本扩大探索空间；2) 令牌选择性熵计算(TsEC)，在不扭曲事实知识的前提下最大化对抗攻击效果

Result: 在领域内和领域外数据集上的广泛实验表明，该方法能显著提升策略探索能力，增强推理能力

Conclusion: SaEI通过选择性对抗熵干预有效增强了RL采样阶段的探索多样性，为视觉语言模型的强化学习微调提供了新思路

Abstract: Recently, reinforcement learning (RL) has become a common choice in enhancing the reasoning capabilities of vision-language models (VLMs). Considering existing RL- based finetuning methods, entropy intervention turns out to be an effective way to benefit exploratory ability, thereby improving policy performance. Notably, most existing stud- ies intervene in entropy by simply controlling the update of specific tokens during policy optimization of RL. They ig- nore the entropy intervention during the RL sampling that can boost the performance of GRPO by improving the di- versity of responses. In this paper, we propose Selective- adversarial Entropy Intervention, namely SaEI, which en- hances policy entropy by distorting the visual input with the token-selective adversarial objective coming from the en- tropy of sampled responses. Specifically, we first propose entropy-guided adversarial sampling (EgAS) that formu- lates the entropy of sampled responses as an adversarial ob- jective. Then, the corresponding adversarial gradient can be used to attack the visual input for producing adversarial samples, allowing the policy model to explore a larger an- swer space during RL sampling. Then, we propose token- selective entropy computation (TsEC) to maximize the ef- fectiveness of adversarial attack in EgAS without distorting factual knowledge within VLMs. Extensive experiments on both in-domain and out-of-domain datasets show that our proposed method can greatly improve policy exploration via entropy intervention, to boost reasoning capabilities. Code will be released once the paper is accepted.

</details>


### [39] [Representation of the structure of graphs by sequences of instructions](https://arxiv.org/abs/2512.10429)
*Ezequiel Lopez-Rubio*

Main category: cs.AI

TL;DR: 提出一种将图邻接矩阵编码为字符串指令序列的新表示方法，使图结构能够被深度学习语言模型处理


<details>
  <summary>Details</summary>
Motivation: 深度学习语言模型擅长处理文本，但当前图表示方法不适合这些模型处理。需要一种能将图结构转换为文本形式的表示方法，以利用深度学习模型的强大能力

Method: 将图的邻接矩阵转换为一系列简单指令字符串，这些指令逐步构建邻接矩阵。该方法具有可逆性：可以从图生成字符串，也可以从字符串重建图

Result: 提出的表示方法紧凑且能保持图的局部结构模式。初步计算实验显示了积极结果

Conclusion: 这种新的图表示方法有望提升深度学习模型处理图的能力，为图处理开辟新途径

Abstract: The representation of graphs is commonly based on the adjacency matrix concept. This formulation is the foundation of most algebraic and computational approaches to graph processing. The advent of deep learning language models offers a wide range of powerful computational models that are specialized in the processing of text. However, current procedures to represent graphs are not amenable to processing by these models. In this work, a new method to represent graphs is proposed. It represents the adjacency matrix of a graph by a string of simple instructions. The instructions build the adjacency matrix step by step. The transformation is reversible, i.e. given a graph the string can be produced and vice versa. The proposed representation is compact and it maintains the local structural patterns of the graph. Therefore, it is envisaged that it could be useful to boost the processing of graphs by deep learning models. A tentative computational experiment is reported, with favorable results.

</details>


### [40] [Targeted Data Protection for Diffusion Model by Matching Training Trajectory](https://arxiv.org/abs/2512.10433)
*Hojun Lee,Mijin Koo,Yeji Song,Nojun Kwak*

Main category: cs.AI

TL;DR: TAFAP是一种通过轨迹对齐实现有效目标数据保护的方法，能够控制整个训练过程，将扩散模型输出重定向到用户指定的目标概念，同时保持高质量图像生成。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型个性化微调技术虽然普及，但引发了未经授权数据使用和隐私侵犯的担忧。现有保护方法仅能被动降低图像质量，无法实现稳定控制。目标数据保护(TDP)虽然提供了主动重定向的范式，但现有方法因采用快照匹配而控制性差，未能考虑完整学习动态。

Method: TAFAP(通过对抗性扰动的轨迹对齐微调)是首个成功实现有效TDP的方法，通过控制整个训练轨迹而非单个快照。该方法受数据集蒸馏启发，采用轨迹匹配来在整个微调过程中强制执行持久、可验证的转换，确保保护效果不会随训练进展而稀释。

Result: TAFAP在扩散模型中首次成功实现了目标转换，同时控制身份和视觉模式。该方法显著优于现有TDP尝试，实现了向目标概念的稳健重定向，同时保持高图像质量。

Conclusion: TAFAP实现了可验证的保护机制，为控制和追踪扩散模型输出的改变提供了新框架，解决了当前保护方法的局限性。

Abstract: Recent advancements in diffusion models have made fine-tuning text-to-image models for personalization increasingly accessible, but have also raised significant concerns regarding unauthorized data usage and privacy infringement. Current protection methods are limited to passively degrading image quality, failing to achieve stable control. While Targeted Data Protection (TDP) offers a promising paradigm for active redirection toward user-specified target concepts, existing TDP attempts suffer from poor controllability due to snapshot-matching approaches that fail to account for complete learning dynamics. We introduce TAFAP (Trajectory Alignment via Fine-tuning with Adversarial Perturbations), the first method to successfully achieve effective TDP by controlling the entire training trajectory. Unlike snapshot-based methods whose protective influence is easily diluted as training progresses, TAFAP employs trajectory-matching inspired by dataset distillation to enforce persistent, verifiable transformations throughout fine-tuning. We validate our method through extensive experiments, demonstrating the first successful targeted transformation in diffusion models with simultaneous control over both identity and visual patterns. TAFAP significantly outperforms existing TDP attempts, achieving robust redirection toward target concepts while maintaining high image quality. This work enables verifiable safeguards and provides a new framework for controlling and tracing alterations in diffusion model outputs.

</details>


### [41] [When Reject Turns into Accept: Quantifying the Vulnerability of LLM-Based Scientific Reviewers to Indirect Prompt Injection](https://arxiv.org/abs/2512.10449)
*Devanshu Sahoo,Manish Prasad,Vasudev Majhi,Jahnvi Singh,Vinay Chamola,Yash Sinha,Murari Mandal,Dhruv Kumar*

Main category: cs.AI

TL;DR: 论文研究了科学同行评审中LLM评估系统的对抗性PDF操纵脆弱性，开发了WAVS指标评估"拒绝"翻转为"接受"的成功率，发现多种攻击策略能有效操纵评审结果。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在科学同行评审中的广泛应用（包括审稿人个人使用和机构正式部署），需要评估这些"LLM-as-a-Judge"系统对对抗性PDF操纵的鲁棒性，特别是针对将"拒绝"决定翻转为"接受"的特定动机。

Method: 构建了200篇科学论文的数据集，针对评审任务调整了15种领域特定的攻击策略，在包括GPT-5、Claude Haiku和DeepSeek在内的13个语言模型上进行评估，开发了WAVS（加权对抗脆弱性得分）作为评估指标。

Result: 研究发现混淆策略如"Maximum Mark Magyk"能成功操纵评分，即使在大型模型中也能达到令人担忧的决策翻转率，表明LLM评审系统存在显著的安全漏洞。

Conclusion: LLM评审系统对对抗性PDF操纵具有脆弱性，需要加强安全措施。研究将发布完整数据集和注入框架以促进该领域更多研究。

Abstract: The landscape of scientific peer review is rapidly evolving with the integration of Large Language Models (LLMs). This shift is driven by two parallel trends: the widespread individual adoption of LLMs by reviewers to manage workload (the "Lazy Reviewer" hypothesis) and the formal institutional deployment of AI-powered assessment systems by conferences like AAAI and Stanford's Agents4Science. This study investigates the robustness of these "LLM-as-a-Judge" systems (both illicit and sanctioned) to adversarial PDF manipulation. Unlike general jailbreaks, we focus on a distinct incentive: flipping "Reject" decisions to "Accept," for which we develop a novel evaluation metric which we term as WAVS (Weighted Adversarial Vulnerability Score). We curated a dataset of 200 scientific papers and adapted 15 domain-specific attack strategies to this task, evaluating them across 13 Language Models, including GPT-5, Claude Haiku, and DeepSeek. Our results demonstrate that obfuscation strategies like "Maximum Mark Magyk" successfully manipulate scores, achieving alarming decision flip rates even in large-scale models. We will release our complete dataset and injection framework to facilitate more research on this topic.

</details>


### [42] [Zero-shot 3D Map Generation with LLM Agents: A Dual-Agent Architecture for Procedural Content Generation](https://arxiv.org/abs/2512.10501)
*Lim Chien Her,Ming Yan,Yunshu Bai,Ruihao Li,Hao Zhang*

Main category: cs.AI

TL;DR: 提出一种无需训练的架构，使用LLM智能体进行零样本PCG参数配置，通过Actor-Critic双智能体迭代工作流，实现自然语言指令到严格参数规范的语义桥接。


<details>
  <summary>Details</summary>
Motivation: PCG需要精确配置不透明的技术参数，而现成的LLM模型难以弥合抽象用户指令与严格参数规范之间的语义鸿沟，需要一种无需训练的方法来控制PCG管道。

Method: 采用Actor-Critic双智能体架构：Actor代理负责参数配置，Critic代理进行评估，形成迭代工作流，系统自主推理工具参数并逐步优化配置以符合人类设计偏好。

Result: 在3D地图生成任务上验证，建立了PCG指令跟随的新基准。实验表明该方法优于单智能体基线，能从自然语言描述生成多样且结构有效的环境。

Conclusion: 现成的LLM可有效重新用作任意PCG工具的通用智能体，通过将负担从模型训练转移到架构推理，为无需任务特定微调而掌握复杂软件提供了可扩展框架。

Abstract: Procedural Content Generation (PCG) offers scalable methods for algorithmically creating complex, customizable worlds. However, controlling these pipelines requires the precise configuration of opaque technical parameters. We propose a training-free architecture that utilizes LLM agents for zero-shot PCG parameter configuration. While Large Language Models (LLMs) promise a natural language interface for PCG tools, off-the-shelf models often fail to bridge the semantic gap between abstract user instructions and strict parameter specifications. Our system pairs an Actor agent with a Critic agent, enabling an iterative workflow where the system autonomously reasons over tool parameters and refines configurations to progressively align with human design preferences. We validate this approach on the generation of various 3D maps, establishing a new benchmark for instruction-following in PCG. Experiments demonstrate that our approach outperforms single-agent baselines, producing diverse and structurally valid environments from natural language descriptions. These results demonstrate that off-the-shelf LLMs can be effectively repurposed as generalized agents for arbitrary PCG tools. By shifting the burden from model training to architectural reasoning, our method offers a scalable framework for mastering complex software without task-specific fine-tuning.

</details>


### [43] [Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning](https://arxiv.org/abs/2512.10534)
*Haiteng Zhao,Junhao Shen,Yiming Zhang,Songyang Gao,Kuikun Liu,Tianyou Ma,Fan Zheng,Dahua Lin,Wenwei Zhang,Kai Chen*

Main category: cs.AI

TL;DR: InternGeometry是首个达到IMO金牌水平的几何问题解决LLM智能体，仅用13K训练样本就解决了44/50个IMO几何问题，远超AlphaGeometry 2的数据效率


<details>
  <summary>Details</summary>
Motivation: 现有LLM在几何问题解决上存在启发式辅助构造的局限性，而专家模型如AlphaGeometry 2需要大规模数据合成和搜索。本研究旨在构建金牌水平的LLM几何智能体，克服这些限制

Method: 通过迭代提出命题和辅助构造，使用符号引擎验证，并根据反馈指导后续提案。采用动态内存机制支持每个问题200多次交互。引入复杂度提升强化学习(CBRL)逐步增加训练问题的复杂度

Result: 在InternThinker-32B基础上构建的InternGeometry解决了2000-2024年间50个IMO几何问题中的44个，超过金牌选手平均分(40.9)，仅使用13K训练样本(AlphaGeometry 2的0.004%)。还能提出人类解法中未出现的新颖辅助构造

Conclusion: InternGeometry展示了LLM智能体在专家级几何任务上的潜力，通过创新的交互式验证和强化学习方法，以极低的数据需求达到金牌水平性能，为未来研究提供了模型、数据和符号引擎支持

Abstract: Large language model (LLM) agents exhibit strong mathematical problem-solving abilities and can even solve International Mathematical Olympiad (IMO) level problems with the assistance of formal proof systems. However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. In this work, we make the first attempt to build a medalist-level LLM agent for geometry and present InternGeometry. InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with a symbolic engine, and reflecting on the engine's feedback to guide subsequent proposals. A dynamic memory mechanism enables InternGeometry to conduct more than two hundred interactions with the symbolic engine per problem. To further accelerate learning, we introduce Complexity-Boosting Reinforcement Learning (CBRL), which gradually increases the complexity of synthesized problems across training stages. Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks. InternGeometry can also propose novel auxiliary constructions for IMO problems that do not appear in human solutions. We will release the model, data, and symbolic engine to support future research.

</details>


### [44] [NormCode: A Semi-Formal Language for Context-Isolated AI Planning](https://arxiv.org/abs/2512.10563)
*Xin Guan*

Main category: cs.AI

TL;DR: NormCode是一种半正式语言，用于构建推理计划，通过数据隔离消除多步LLM工作流中的上下文污染问题，实现精确的成本和可靠性追踪。


<details>
  <summary>Details</summary>
Motivation: 多步LLM工作流存在上下文污染问题：随着信息在步骤间累积，模型会产生幻觉、混淆中间输出，并失去对任务约束的跟踪。需要一种方法来消除跨步骤污染。

Method: NormCode是一种半正式语言，用于构建结构化分解的推理计划，每个步骤在数据隔离中运行，只接收显式传递的输入。它严格分离语义操作（LLM驱动的推理，非确定性）和句法操作（确定性数据重组），支持三种同构格式：.ncds（人类编写）、.ncd（机器执行）和.ncn（人类验证）。

Result: 通过两个演示验证NormCode：(1) 基础X加法算法在任意长度输入上实现100%准确率；(2) 自托管执行NormCode自身的五阶段编译器流水线。工作编排器提供依赖驱动调度、SQLite支持的检查点和循环管理。

Conclusion: NormCode通过设计消除跨步骤污染，使AI工作流可审计，解决了法律推理、医疗决策和金融分析等高风险领域对透明度的关键需求。

Abstract: Multistep workflows that chain large language model (LLM) calls suffer from context pollution: as information accumulates across steps, models hallucinate, confuse intermediate outputs, and lose track of task constraints. We present NormCode, a semiformal language for constructing plans of inferences, structured decompositions where each step operates in data isolation and receives only explicitly passed inputs, which eliminates crossstep contamination by design. NormCode enforces a strict separation between semantic operations (LLMdriven reasoning, nondeterministic) and syntactic operations (deterministic data restructuring), enabling precise cost and reliability tracing. The language exists in three isomorphic formats: .ncds for human authoring, .ncd for machine execution, and .ncn for human verification, supporting progressive formalization from sketch to production. We validate NormCode through two demonstrations: (1) a base X addition algorithm achieving 100 percent accuracy on arbitrary length inputs, and (2) self hosted execution of NormCode's own five phase compiler pipeline. The working orchestrator provides dependency driven scheduling, SQLite backed checkpointing, and loop management, making AI workflows auditable by design and addressing a critical need for transparency in high stakes domains such as legal reasoning, medical decision making, and financial analysis.

</details>


### [45] [Phythesis: Physics-Guided Evolutionary Scene Synthesis for Energy-Efficient Data Center Design via LLMs](https://arxiv.org/abs/2512.10611)
*Minghao LI,Ruihang Wang,Rui Tan,Yonggang Wen*

Main category: cs.AI

TL;DR: 提出Phythesis框架，结合大语言模型和物理引导的进化优化，自动化生成仿真就绪的数据中心布局，提升能源效率


<details>
  <summary>Details</summary>
Motivation: 数据中心设计复杂度日益增加，传统人工+仿真工具方法难以扩展。现有生成式AI方法不考虑物理约束，不适合有量化运营目标和严格物理限制的数据中心设计

Method: 采用迭代双层优化架构：1) LLM驱动优化层生成物理合理的三维布局并进行自我批判以优化场景拓扑；2) 物理信息优化层识别最优资产参数并选择最佳资产组合

Result: 在三种生成规模上，相比纯LLM方案，Phythesis实现了57.3%的生成成功率提升和11.5%的电力使用效率(PUE)改善

Conclusion: Phythesis框架成功将LLM与物理引导优化相结合，能够自动化生成仿真就绪的数据中心场景，显著提升设计质量和能源效率

Abstract: Data center (DC) infrastructure serves as the backbone to support the escalating demand for computing capacity. Traditional design methodologies that blend human expertise with specialized simulation tools scale poorly with the increasing system complexity. Recent studies adopt generative artificial intelligence to design plausible human-centric indoor layouts. However, they do not consider the underlying physics, making them unsuitable for the DC design that sets quantifiable operational objectives and strict physical constraints. To bridge the gap, we propose Phythesis, a novel framework that synergizes large language models (LLMs) and physics-guided evolutionary optimization to automate simulation-ready (SimReady) scene synthesis for energy-efficient DC design. Phythesis employs an iterative bi-level optimization architecture, where (i) the LLM-driven optimization level generates physically plausible three-dimensional layouts and self-criticizes them to refine the scene topology, and (ii) the physics-informed optimization level identifies the optimal asset parameters and selects the best asset combination. Experiments on three generation scales show that Phythesis achieves 57.3% generation success rate increase and 11.5% power usage effectiveness (PUE) improvement, compared with the vanilla LLM-based solution.

</details>


### [46] [Refinement Contrastive Learning of Cell-Gene Associations for Unsupervised Cell Type Identification](https://arxiv.org/abs/2512.10640)
*Liang Peng,Haopeng Liu,Yixuan Ye,Cheng Liu,Wenjun Shen,Si Wu,Hau-San Wong*

Main category: cs.AI

TL;DR: scRCL是一个用于单细胞组学无监督细胞类型识别的框架，通过结合细胞-基因相互作用和对比学习来改进细胞表示学习


<details>
  <summary>Details</summary>
Motivation: 现有聚类方法主要关注细胞内在结构，忽略了细胞-基因关联的关键作用，这限制了它们区分密切相关的细胞类型的能力

Method: 提出Refinement Contrastive Learning框架(scRCL)，包含两个对比分布对齐组件来揭示可靠的细胞结构，以及一个整合基因相关结构学习的精炼模块来增强细胞嵌入

Result: 在多个单细胞RNA-seq和空间转录组基准数据集上的实验表明，该方法在细胞类型识别准确性上持续优于现有最先进方法

Conclusion: scRCL通过有效利用细胞-基因相互作用，能够学习更具信息量的细胞表示，提高细胞类型识别准确性，并保持生物学相关性

Abstract: Unsupervised cell type identification is crucial for uncovering and characterizing heterogeneous populations in single cell omics studies. Although a range of clustering methods have been developed, most focus exclusively on intrinsic cellular structure and ignore the pivotal role of cell-gene associations, which limits their ability to distinguish closely related cell types. To this end, we propose a Refinement Contrastive Learning framework (scRCL) that explicitly incorporates cell-gene interactions to derive more informative representations. Specifically, we introduce two contrastive distribution alignment components that reveal reliable intrinsic cellular structures by effectively exploiting cell-cell structural relationships. Additionally, we develop a refinement module that integrates gene-correlation structure learning to enhance cell embeddings by capturing underlying cell-gene associations. This module strengthens connections between cells and their associated genes, refining the representation learning to exploiting biologically meaningful relationships. Extensive experiments on several single-cell RNA-seq and spatial transcriptomics benchmark datasets demonstrate that our method consistently outperforms state-of-the-art baselines in cell-type identification accuracy. Moreover, downstream biological analyses confirm that the recovered cell populations exhibit coherent gene-expression signatures, further validating the biological relevance of our approach. The code is available at https://github.com/THPengL/scRCL.

</details>


### [47] [CAPTAIN: Semantic Feature Injection for Memorization Mitigation in Text-to-Image Diffusion Models](https://arxiv.org/abs/2512.10655)
*Tong Zhang,Carlos Hinojosa,Bernard Ghanem*

Main category: cs.AI

TL;DR: CAPTAIN：一种无需训练的框架，通过在去噪过程中直接修改潜在特征来减少扩散模型的记忆化问题，同时保持提示对齐和视觉质量


<details>
  <summary>Details</summary>
Motivation: 扩散模型可能无意中复制训练样本，引发隐私和版权问题。现有基于推理时间的缓解方法（如操纵无分类器引导或扰动提示嵌入）难以在不损害提示对齐的情况下有效减少记忆化

Method: 1. 基于频率的噪声初始化，减少去噪早期复制记忆模式的倾向；2. 识别特征注入的最佳去噪时间步并定位记忆区域；3. 将非记忆参考图像的语义对齐特征注入到定位的潜在区域中

Result: 实验表明，CAPTAIN相比基于CFG的基线方法显著减少了记忆化，同时保持了与目标提示的强对齐

Conclusion: CAPTAIN提供了一种有效的训练免费框架，能够在扩散模型中抑制记忆化，同时保持提示保真度和视觉质量，解决了现有方法在记忆化缓解和提示对齐之间的权衡问题

Abstract: Diffusion models can unintentionally reproduce training examples, raising privacy and copyright concerns as these systems are increasingly deployed at scale. Existing inference-time mitigation methods typically manipulate classifier-free guidance (CFG) or perturb prompt embeddings; however, they often struggle to reduce memorization without compromising alignment with the conditioning prompt. We introduce CAPTAIN, a training-free framework that mitigates memorization by directly modifying latent features during denoising. CAPTAIN first applies frequency-based noise initialization to reduce the tendency to replicate memorized patterns early in the denoising process. It then identifies the optimal denoising timesteps for feature injection and localizes memorized regions. Finally, CAPTAIN injects semantically aligned features from non-memorized reference images into localized latent regions, suppressing memorization while preserving prompt fidelity and visual quality. Our experiments show that CAPTAIN achieves substantial reductions in memorization compared to CFG-based baselines while maintaining strong alignment with the intended prompt.

</details>


### [48] [On the Dynamics of Multi-Agent LLM Communities Driven by Value Diversity](https://arxiv.org/abs/2512.10665)
*Muhua Huang,Qinlin Zhao,Xiaoyuan Yi,Xing Xie*

Main category: cs.AI

TL;DR: 该研究探讨了AI多智能体系统中价值多样性如何影响集体行为，发现价值多样性增强价值稳定性、促进涌现行为并带来更多创造性原则，但存在边际递减效应，极端异质性会导致不稳定。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的多智能体系统日益普及，这类人工社区的集体行为（如集体智能）受到越来越多的关注。本研究旨在回答一个基本问题：价值多样性如何塑造AI社区的集体行为？

Method: 使用基于施瓦茨基本人类价值理论的自然主义价值引导方法，构建了多智能体模拟，让不同规模的价值多样化社区参与开放式互动和宪法制定。

Result: 结果显示：1）价值多样性增强价值稳定性；2）促进涌现行为；3）在没有外部指导的情况下，智能体自身能发展出更多创造性原则。但这些效应也存在边际递减：极端异质性会引发不稳定性。

Conclusion: 价值多样性是未来AI能力的新维度，连接了AI能力与社会学中的制度涌现研究。适度的价值多样性有利于AI社区的集体智能发展，但需要平衡多样性与稳定性。

Abstract: As Large Language Models (LLM) based multi-agent systems become increasingly prevalent, the collective behaviors, e.g., collective intelligence, of such artificial communities have drawn growing attention. This work aims to answer a fundamental question: How does diversity of values shape the collective behavior of AI communities? Using naturalistic value elicitation grounded in the prevalent Schwartz's Theory of Basic Human Values, we constructed multi-agent simulations where communities with varying numbers of agents engaged in open-ended interactions and constitution formation. The results show that value diversity enhances value stability, fosters emergent behaviors, and brings more creative principles developed by the agents themselves without external guidance. However, these effects also show diminishing returns: extreme heterogeneity induces instability. This work positions value diversity as a new axis of future AI capability, bridging AI ability and sociological studies of institutional emergence.

</details>


### [49] [AEBNAS: Strengthening Exit Branches in Early-Exit Networks through Hardware-Aware Neural Architecture Search](https://arxiv.org/abs/2512.10671)
*Oscar Robben,Saeed Khalilian,Nirvana Meratnia*

Main category: cs.AI

TL;DR: 本文提出了一种硬件感知的神经架构搜索方法，用于设计更高效的早退网络，通过优化出口分支的深度和层类型，在保持或降低计算量的同时提高准确率。


<details>
  <summary>Details</summary>
Motivation: 早退网络通过根据输入复杂度调整计算量来降低能耗和延迟，但设计过程复杂耗时。现有NAS方法主要关注出口分支的位置和数量，而出口分支的深度和层类型对网络效率和准确性同样重要，需要更全面的优化方法。

Method: 采用硬件感知的神经架构搜索（NAS）框架，同时优化出口分支的深度和层类型，并结合自适应阈值调优。在CIFAR-10、CIFAR-100和SVHN数据集上进行评估。

Result: 在相同或更低的平均MACs（乘加运算）下，相比现有最优方法，提出的框架设计的早退网络获得了更高的准确率。

Conclusion: 硬件感知NAS能够有效设计更高效的早退网络，通过综合考虑出口分支的深度、层类型和自适应阈值，在计算效率和准确性之间取得更好平衡。

Abstract: Early-exit networks are effective solutions for reducing the overall energy consumption and latency of deep learning models by adjusting computation based on the complexity of input data. By incorporating intermediate exit branches into the architecture, they provide less computation for simpler samples, which is particularly beneficial for resource-constrained devices where energy consumption is crucial. However, designing early-exit networks is a challenging and time-consuming process due to the need to balance efficiency and performance. Recent works have utilized Neural Architecture Search (NAS) to design more efficient early-exit networks, aiming to reduce average latency while improving model accuracy by determining the best positions and number of exit branches in the architecture. Another important factor affecting the efficiency and accuracy of early-exit networks is the depth and types of layers in the exit branches. In this paper, we use hardware-aware NAS to strengthen exit branches, considering both accuracy and efficiency during optimization. Our performance evaluation on the CIFAR-10, CIFAR-100, and SVHN datasets demonstrates that our proposed framework, which considers varying depths and layers for exit branches along with adaptive threshold tuning, designs early-exit networks that achieve higher accuracy with the same or lower average number of MACs compared to the state-of-the-art approaches.

</details>


### [50] [Enhancing Radiology Report Generation and Visual Grounding using Reinforcement Learning](https://arxiv.org/abs/2512.10691)
*Benjamin Gundersen,Nicolas Deperrois,Samuel Ruiperez-Campillo,Thomas M. Sutter,Julia E. Vogt,Michael Moor,Farhad Nooralahzadeh,Michael Krauthammer*

Main category: cs.AI

TL;DR: 研究探索强化学习(RL)和显式推理在胸片视觉语言模型中的应用，发现RL能提升报告生成和视觉定位性能，但显式推理无明显帮助


<details>
  <summary>Details</summary>
Motivation: 当前医学视觉语言模型主要依赖监督微调(SFT)，但缺乏对答案质量的评估。强化学习能整合任务特定反馈，结合显式推理在数学和编程任务中已显示显著提升，研究者希望探索这些技术在胸片分析中的效果

Method: 基于Qwen3-VL构建RadVLM，先进行大规模SFT，然后冷启动SFT赋予基础推理能力，最后应用GRPO强化学习，使用临床相关的任务特定奖励进行报告生成和视觉定位优化

Result: RL在报告生成和视觉定位任务上均带来额外提升，但显式推理未进一步改善结果。RL优化的RadVLM模型在两项任务上均达到最先进性能

Conclusion: 虽然强大的SFT对基础性能至关重要，但临床对齐的强化学习是医学视觉语言模型的有力补充，能显著提升任务性能

Abstract: Recent advances in vision-language models (VLMs) have improved Chest X-ray (CXR) interpretation in multiple aspects. However, many medical VLMs rely solely on supervised fine-tuning (SFT), which optimizes next-token prediction without evaluating answer quality. In contrast, reinforcement learning (RL) can incorporate task-specific feedback, and its combination with explicit intermediate reasoning ("thinking") has demonstrated substantial gains on verifiable math and coding tasks. To investigate the effects of RL and thinking in a CXR VLM, we perform large-scale SFT on CXR data to build an updated RadVLM based on Qwen3-VL, followed by a cold-start SFT stage that equips the model with basic thinking ability. We then apply Group Relative Policy Optimization (GRPO) with clinically grounded, task-specific rewards for report generation and visual grounding, and run matched RL experiments on both domain-specific and general-domain Qwen3-VL variants, with and without thinking. Across these settings, we find that while strong SFT remains crucial for high base performance, RL provides additional gains on both tasks, whereas explicit thinking does not appear to further improve results. Under a unified evaluation pipeline, the RL-optimized RadVLM models outperform their baseline counterparts and reach state-of-the-art performance on both report generation and grounding, highlighting clinically aligned RL as a powerful complement to SFT for medical VLMs.

</details>


### [51] [Remember Me, Refine Me: A Dynamic Procedural Memory Framework for Experience-Driven Agent Evolution](https://arxiv.org/abs/2512.10696)
*Zouying Cao,Jiaji Deng,Li Yu,Weikang Zhou,Zhaoyang Liu,Bolin Ding,Hai Zhao*

Main category: cs.AI

TL;DR: ReMe框架通过多层面提炼、上下文自适应重用和基于效用的精炼机制，实现LLM代理从被动记忆到主动进化的转变，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理记忆框架主要采用"被动积累"范式，将记忆视为静态附加档案，缺乏动态推理能力。需要弥合静态存储与动态推理之间的差距，实现经验驱动的代理进化。

Method: 提出ReMe框架，包含三个创新机制：1) 多层面提炼：识别成功模式、分析失败触发因素并生成比较见解；2) 上下文自适应重用：通过场景感知索引将历史见解适配到新情境；3) 基于效用的精炼：自主添加有效记忆并修剪过时记忆，维护紧凑高质量经验池。

Result: 在BFCL-V3和AppWorld基准测试中达到最先进水平。观察到显著的内存扩展效应：配备ReMe的Qwen3-8B超越无记忆的更大模型Qwen3-14B，表明自进化记忆为终身学习提供了计算高效途径。

Conclusion: ReMe框架成功将LLM代理记忆从被动积累转变为主动进化，通过动态记忆生命周期管理实现经验驱动的持续改进，为代理终身学习提供了有效解决方案。

Abstract: Procedural memory enables large language model (LLM) agents to internalize "how-to" knowledge, theoretically reducing redundant trial-and-error. However, existing frameworks predominantly suffer from a "passive accumulation" paradigm, treating memory as a static append-only archive. To bridge the gap between static storage and dynamic reasoning, we propose $\textbf{ReMe}$ ($\textit{Remember Me, Refine Me}$), a comprehensive framework for experience-driven agent evolution. ReMe innovates across the memory lifecycle via three mechanisms: 1) $\textit{multi-faceted distillation}$, which extracts fine-grained experiences by recognizing success patterns, analyzing failure triggers and generating comparative insights; 2) $\textit{context-adaptive reuse}$, which tailors historical insights to new contexts via scenario-aware indexing; and 3) $\textit{utility-based refinement}$, which autonomously adds valid memories and prunes outdated ones to maintain a compact, high-quality experience pool. Extensive experiments on BFCL-V3 and AppWorld demonstrate that ReMe establishes a new state-of-the-art in agent memory system. Crucially, we observe a significant memory-scaling effect: Qwen3-8B equipped with ReMe outperforms larger, memoryless Qwen3-14B, suggesting that self-evolving memory provides a computation-efficient pathway for lifelong learning. We release our code and the $\texttt{reme.library}$ dataset to facilitate further research.

</details>


### [52] [COMPARE: Clinical Optimization with Modular Planning and Assessment via RAG-Enhanced AI-OCT: Superior Decision Support for Percutaneous Coronary Intervention Compared to ChatGPT-5 and Junior Operators](https://arxiv.org/abs/2512.10702)
*Wei Fang,Chiyao Wang,Wenshuai Ma,Hui Liu,Jianqiang Hu,Xiaona Niu,Yi Chu,Mingming Zhang,Jingxiao Yang,Dongwei Zhang,Zelin Li,Pengyun Liu,Jiawei Zheng,Pengke Zhang,Chaoshi Qin,Wangang Guo,Bin Wang,Yugang Xue,Wei Zhang,Zikuan Wang,Rui Zhu,Yihui Cao,Quanmao Lu,Rui Meng,Yan Li*

Main category: cs.AI

TL;DR: CA-GPT（基于AI-OCT系统的大型模型）在OCT引导的PCI规划和评估中，显著优于通用ChatGPT-5和初级医师，为血管内影像解读提供了标准化可靠方法。


<details>
  <summary>Details</summary>
Motivation: 虽然血管内成像（特别是OCT）能改善PCI结果，但其解读依赖操作者经验。通用AI虽有潜力但缺乏领域特异性可靠性，因此需要开发专门针对OCT引导PCI的AI系统。

Method: 单中心分析96例接受OCT引导PCI的患者，比较CA-GPT、ChatGPT-5和初级医师生成的手术决策与专家记录的一致性。使用10个预设指标评估PCI前和PCI后阶段的一致性。

Result: PCI前规划：CA-GPT中位一致性评分（5[3.75-5]）显著高于ChatGPT-5（3[2-4]）和初级医师（4[3-4]）。在支架直径（90.3% vs 72.2%）和长度选择（80.6% vs 52.8%）上显著优于初级医师。PCI后评估：CA-GPT保持优异一致性（5[4.75-5]），显著高于ChatGPT-5（4[4-5]）和初级医师（5[4-5]）。亚组分析证实CA-GPT在复杂场景中的稳健优势。

Conclusion: 基于CA-GPT的AI-OCT系统在PCI规划和评估阶段均优于通用大型语言模型和初级医师，为血管内影像解读提供了标准化可靠方法，显著增强操作者专业能力并优化OCT引导的PCI。

Abstract: Background: While intravascular imaging, particularly optical coherence tomography (OCT), improves percutaneous coronary intervention (PCI) outcomes, its interpretation is operator-dependent. General-purpose artificial intelligence (AI) shows promise but lacks domain-specific reliability. We evaluated the performance of CA-GPT, a novel large model deployed on an AI-OCT system, against that of the general-purpose ChatGPT-5 and junior physicians for OCT-guided PCI planning and assessment.
  Methods: In this single-center analysis of 96 patients who underwent OCT-guided PCI, the procedural decisions generated by the CA-GPT, ChatGPT-5, and junior physicians were compared with an expert-derived procedural record. Agreement was assessed using ten pre-specified metrics across pre-PCI and post-PCI phases.
  Results: For pre-PCI planning, CA-GPT demonstrated significantly higher median agreement scores (5[IQR 3.75-5]) compared to both ChatGPT-5 (3[2-4], P<0.001) and junior physicians (4[3-4], P<0.001). CA-GPT significantly outperformed ChatGPT-5 across all individual pre-PCI metrics and showed superior performance to junior physicians in stent diameter (90.3% vs. 72.2%, P<0.05) and length selection (80.6% vs. 52.8%, P<0.01). In post-PCI assessment, CA-GPT maintained excellent overall agreement (5[4.75-5]), significantly higher than both ChatGPT-5 (4[4-5], P<0.001) and junior physicians (5[4-5], P<0.05). Subgroup analysis confirmed CA-GPT's robust performance advantage in complex scenarios.
  Conclusion: The CA-GPT-based AI-OCT system achieved superior decision-making agreement versus a general-purpose large language model and junior physicians across both PCI planning and assessment phases. This approach provides a standardized and reliable method for intravascular imaging interpretation, demonstrating significant potential to augment operator expertise and optimize OCT-guided PCI.

</details>


### [53] [Replace, Don't Expand: Mitigating Context Dilution in Multi-Hop RAG via Fixed-Budget Evidence Assembly](https://arxiv.org/abs/2512.10787)
*Moshe Lahmy,Roi Yozevitch*

Main category: cs.AI

TL;DR: SEAL-RAG提出"替换而非扩展"策略，通过搜索-提取-评估-循环流程，在固定检索深度下主动替换干扰信息，解决多跳查询中的上下文稀释问题。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统在处理多跳查询时，当初始检索遗漏桥接事实时容易失败。传统纠正方法（如Self-RAG、CRAG、Adaptive-k）通常通过增加上下文或修剪列表来解决问题，但这会导致上下文稀释，即干扰信息挤占相关信息。

Method: SEAL-RAG采用训练免费的控制器，执行搜索→提取→评估→循环流程：进行实时实体锚定提取构建缺口规范（缺失实体/关系），触发针对性微查询，使用实体优先排序主动替换干扰信息以填补证据缺口。

Result: 在HotpotQA（k=3）上，SEAL比Self-RAG提高答案正确率3-13个百分点，证据精确率提高12-18个百分点。在2WikiMultiHopQA（k=5）上，比Adaptive-k准确率高8.0个百分点，证据精确率保持96%（CRAG仅为22%）。所有改进均具有统计显著性（p<0.001）。

Conclusion: 通过强制执行固定k替换策略，SEAL在保持可预测成本的同时，确保前k个槽位针对精确度而非广度进行优化，有效解决了多跳RAG中的上下文稀释问题。

Abstract: Retrieval-Augmented Generation (RAG) systems often fail on multi-hop queries when the initial retrieval misses a bridge fact. Prior corrective approaches, such as Self-RAG, CRAG, and Adaptive-$k$, typically address this by \textit{adding} more context or pruning existing lists. However, simply expanding the context window often leads to \textbf{context dilution}, where distractors crowd out relevant information. We propose \textbf{SEAL-RAG}, a training-free controller that adopts a \textbf{``replace, don't expand''} strategy to fight context dilution under a fixed retrieval depth $k$. SEAL executes a (\textbf{S}earch $\rightarrow$ \textbf{E}xtract $\rightarrow$ \textbf{A}ssess $\rightarrow$ \textbf{L}oop) cycle: it performs on-the-fly, entity-anchored extraction to build a live \textit{gap specification} (missing entities/relations), triggers targeted micro-queries, and uses \textit{entity-first ranking} to actively swap out distractors for gap-closing evidence. We evaluate SEAL-RAG against faithful re-implementations of Basic RAG, CRAG, Self-RAG, and Adaptive-$k$ in a shared environment on \textbf{HotpotQA} and \textbf{2WikiMultiHopQA}. On HotpotQA ($k=3$), SEAL improves answer correctness by \textbf{+3--13 pp} and evidence precision by \textbf{+12--18 pp} over Self-RAG. On 2WikiMultiHopQA ($k=5$), it outperforms Adaptive-$k$ by \textbf{+8.0 pp} in accuracy and maintains \textbf{96\%} evidence precision compared to 22\% for CRAG. These gains are statistically significant ($p<0.001$). By enforcing fixed-$k$ replacement, SEAL yields a predictable cost profile while ensuring the top-$k$ slots are optimized for precision rather than mere breadth. We release our code and data at https://github.com/mosherino/SEAL-RAG.

</details>


### [54] [HAROOD: A Benchmark for Out-of-distribution Generalization in Sensor-based Human Activity Recognition](https://arxiv.org/abs/2512.10807)
*Wang Lu,Yao Zhu,Jindong Wang*

Main category: cs.AI

TL;DR: HAROOD：一个用于人类活动识别（HAR）在分布外（OOD）场景下的综合基准测试框架，涵盖4种OOD场景、6个数据集、16种方法，旨在评估现有OOD算法在HAR任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现实场景中，个体、设备、环境和时间的变化会导致相同活动的数据分布发生显著偏移。现有研究仅在特定分布偏移场景（如跨设备或跨位置）中应用或调整OOD算法，缺乏对这些算法有效性的全面洞察。

Method: 提出HAROOD基准测试框架，定义4种OOD场景（跨人、跨位置、跨数据集、跨时间），构建包含6个数据集、16种比较方法（基于CNN和Transformer架构）的测试平台，采用两种模型选择协议。

Result: 通过大量实验发现：没有单一方法在所有场景中始终优于其他方法，这突显了该领域仍有巨大的改进空间。同时提供了模块化、易于扩展的代码库。

Conclusion: HAROOD为基于OOD的HAR研究提供了全面的基准测试框架，揭示了当前方法的局限性，并希望促进该领域的进一步发展。代码已开源供社区使用。

Abstract: Sensor-based human activity recognition (HAR) mines activity patterns from the time-series sensory data. In realistic scenarios, variations across individuals, devices, environments, and time introduce significant distributional shifts for the same activities. Recent efforts attempt to solve this challenge by applying or adapting existing out-of-distribution (OOD) algorithms, but only in certain distribution shift scenarios (e.g., cross-device or cross-position), lacking comprehensive insights on the effectiveness of these algorithms. For instance, is OOD necessary to HAR? Which OOD algorithm performs the best? In this paper, we fill this gap by proposing HAROOD, a comprehensive benchmark for HAR in OOD settings. We define 4 OOD scenarios: cross-person, cross-position, cross-dataset, and cross-time, and build a testbed covering 6 datasets, 16 comparative methods (implemented with CNN-based and Transformer-based architectures), and two model selection protocols. Then, we conduct extensive experiments and present several findings for future research, e.g., no single method consistently outperforms others, highlighting substantial opportunity for advancement. Our codebase is highly modular and easy to extend for new datasets, algorithms, comparisons, and analysis, with the hope to facilitate the research in OOD-based HAR. Our implementation is released and can be found at https://github.com/AIFrontierLab/HAROOD.

</details>


### [55] [Agile Deliberation: Concept Deliberation for Subjective Visual Classification](https://arxiv.org/abs/2512.10821)
*Leijie Wang,Otilia Stretcu,Wei Qiao,Thomas Denby,Krishnamurthy Viswanathan,Enming Luo,Chun-Ta Lu,Tushar Dogra,Ranjay Krishna,Ariel Fuxman*

Main category: cs.AI

TL;DR: 提出Agile Deliberation框架，通过概念界定和迭代两个阶段，支持用户在模糊概念下与系统交互式细化视觉分类器，相比自动分解基线提升7.5% F1分数。


<details>
  <summary>Details</summary>
Motivation: 现有的人机协同方法假设用户有清晰稳定的概念理解，但现实中用户常从模糊概念开始，需要通过"概念审议"迭代细化。内容审核专家的访谈揭示了这一实践需求。

Method: Agile Deliberation框架包含两个阶段：(1)概念界定：将初始概念分解为结构化子概念层次；(2)概念迭代：展示语义边界案例供用户反思和反馈，迭代对齐图像分类器与用户意图。

Result: 通过18个1.5小时用户会话评估，Agile Deliberation比自动分解基线高7.5% F1分数，比手动审议高3%以上。参与者报告概念理解更清晰，认知负担更低。

Conclusion: Agile Deliberation有效支持演化中的主观概念定义，通过暴露边界案例帮助用户明确概念，在人机协同视觉分类任务中优于传统方法。

Abstract: From content moderation to content curation, applications requiring vision classifiers for visual concepts are rapidly expanding. Existing human-in-the-loop approaches typically assume users begin with a clear, stable concept understanding to be able to provide high-quality supervision. In reality, users often start with a vague idea and must iteratively refine it through "concept deliberation", a practice we uncovered through structured interviews with content moderation experts. We operationalize the common strategies in deliberation used by real content moderators into a human-in-the-loop framework called "Agile Deliberation" that explicitly supports evolving and subjective concepts. The system supports users in defining the concept for themselves by exposing them to borderline cases. The system does this with two deliberation stages: (1) concept scoping, which decomposes the initial concept into a structured hierarchy of sub-concepts, and (2) concept iteration, which surfaces semantically borderline examples for user reflection and feedback to iteratively align an image classifier with the user's evolving intent. Since concept deliberation is inherently subjective and interactive, we painstakingly evaluate the framework through 18 user sessions, each 1.5h long, rather than standard benchmarking datasets. We find that Agile Deliberation achieves 7.5% higher F1 scores than automated decomposition baselines and more than 3% higher than manual deliberation, while participants reported clearer conceptual understanding and lower cognitive effort.

</details>


### [56] [V-OCBF: Learning Safety Filters from Offline Data via Value-Guided Offline Control Barrier Functions](https://arxiv.org/abs/2512.10822)
*Mumuksh Tayal,Manan Tayal,Aditya Singh,Shishir Kolathaya,Ravi Prakash*

Main category: cs.AI

TL;DR: V-OCBF：一种从离线演示中学习神经控制屏障函数的框架，无需系统动力学模型，通过二次规划实现实时安全控制


<details>
  <summary>Details</summary>
Motivation: 现有安全离线强化学习方法通常只能保证软约束，无法确保前向不变性；而控制屏障函数虽能提供严格安全保证，但依赖专家设计的屏障函数或完整的系统动力学知识。需要一种无需在线交互或手动设计屏障的方法来合成安全关键控制器。

Method: 提出值引导离线控制屏障函数（V-OCBF）框架：1）从离线演示中学习神经CBF，无需动力学模型；2）推导递归有限差分屏障更新，实现无模型学习；3）采用基于期望分位数的目标函数，避免在分布外动作上查询屏障；4）将学习到的屏障与二次规划结合，合成实时安全控制。

Result: 在多个案例研究中，V-OCBF相比基线方法显著减少了安全违规次数，同时保持了强大的任务性能，展示了其在无需在线交互或手动设计屏障的情况下合成安全关键控制器的可扩展性。

Conclusion: V-OCBF为离线合成安全关键控制器提供了一种有效框架，无需系统动力学模型或手动设计的屏障函数，在保证任务性能的同时显著提升了安全性，具有实际应用价值。

Abstract: Ensuring safety in autonomous systems requires controllers that satisfy hard, state-wise constraints without relying on online interaction. While existing Safe Offline RL methods typically enforce soft expected-cost constraints, they do not guarantee forward invariance. Conversely, Control Barrier Functions (CBFs) provide rigorous safety guarantees but usually depend on expert-designed barrier functions or full knowledge of the system dynamics. We introduce Value-Guided Offline Control Barrier Functions (V-OCBF), a framework that learns a neural CBF entirely from offline demonstrations. Unlike prior approaches, V-OCBF does not assume access to the dynamics model; instead, it derives a recursive finite-difference barrier update, enabling model-free learning of a barrier that propagates safety information over time. Moreover, V-OCBF incorporates an expectile-based objective that avoids querying the barrier on out-of-distribution actions and restricts updates to the dataset-supported action set. The learned barrier is then used with a Quadratic Program (QP) formulation to synthesize real-time safe control. Across multiple case studies, V-OCBF yields substantially fewer safety violations than baseline methods while maintaining strong task performance, highlighting its scalability for offline synthesis of safety-critical controllers without online interaction or hand-engineered barriers.

</details>


### [57] [LLMs Can Assist with Proposal Selection at Large User Facilities](https://arxiv.org/abs/2512.10895)
*Lijie Ding,Janell Thomson,Jon Taylor,Changwoo Do*

Main category: cs.AI

TL;DR: LLMs can替代传统人工评审，通过成对偏好方法提供更一致、可扩展且成本效益高的提案排名，在大型用户设施中表现与人类相当甚至更好。


<details>
  <summary>Details</summary>
Motivation: 传统人工提案评审存在评分相关性弱、评审者偏见和不一致等问题，而成对偏好方法虽然逻辑上更优，但工作量呈二次方增长，对人类评审者不切实际。

Method: 利用LLMs实现成对偏好方法进行提案排名，使用SNS三个光束线精心策划的提案和发表记录数据，通过嵌入模型进行提案相似性定量评估。

Result: LLM排名与人类排名强相关（Spearman ρ≈0.2-0.8，去除10%异常值后≥0.5），在识别高发表潜力提案方面不逊于人类评审者，成本降低两个数量级以上。

Conclusion: LLMs为大型用户设施的提案选择提供了可扩展、一致且经济高效的替代方案，不仅能进行排名，还能实现人类难以完成的定量相似性分析等高级分析。

Abstract: We explore how large language models (LLMs) can enhance the proposal selection process at large user facilities, offering a scalable, consistent, and cost-effective alternative to traditional human review. Proposal selection depends on assessing the relative strength among submitted proposals; however, traditional human scoring often suffers from weak inter-proposal correlations and is subject to reviewer bias and inconsistency. A pairwise preference-based approach is logically superior, providing a more rigorous and internally consistent basis for ranking, but its quadratic workload makes it impractical for human reviewers. We address this limitation using LLMs. Leveraging the uniquely well-curated proposals and publication records from three beamlines at the Spallation Neutron Source (SNS), Oak Ridge National Laboratory (ORNL), we show that the LLM rankings correlate strongly with the human rankings (Spearman $ρ\simeq 0.2-0.8$, improving to $\geq 0.5$ after 10\% outlier removal). Moreover, LLM performance is no worse than that of human reviewers in identifying proposals with high publication potential, while costing over two orders of magnitude less. Beyond ranking, LLMs enable advanced analyses that are challenging for humans, such as quantitative assessment of proposal similarity via embedding models, which provides information crucial for review committees.

</details>


### [58] [Multi-Granular Node Pruning for Circuit Discovery](https://arxiv.org/abs/2512.10903)
*Muhammad Umair Haider,Hammad Rizwan,Hassan Sajjad,A. B. Siddique*

Main category: cs.AI

TL;DR: 提出节点级剪枝框架用于电路发现，解决现有方法计算成本高、粒度粗的问题，通过多粒度可学习掩码和稀疏性惩罚实现单次微调压缩，内存占用降低5-10倍。


<details>
  <summary>Details</summary>
Motivation: 现有电路发现方法主要依赖迭代边剪枝，计算成本高且仅限于粗粒度单元（如注意力头或MLP块），忽略了神经元等细粒度结构，需要解决可扩展性和粒度限制问题。

Method: 提出节点级剪枝框架，引入跨多个粒度（从整个块到单个神经元）的可学习掩码，在统一优化目标中使用粒度特定的稀疏性惩罚指导剪枝过程，实现单次微调中的全面压缩。

Result: 方法发现的电路节点数少于先前方法，证明许多粗粒度方法认为重要的神经元实际上无关紧要，同时保持任务性能，内存占用显著降低5-10倍（无需存储中间激活）。

Conclusion: 提出的节点级剪枝框架在电路发现中同时解决了可扩展性和粒度限制问题，能够发现更小、更精确的电路，显著降低内存需求，为理解LLM内部机制提供更精细的工具。

Abstract: Circuit discovery aims to identify minimal subnetworks that are responsible for specific behaviors in large language models (LLMs). Existing approaches primarily rely on iterative edge pruning, which is computationally expensive and limited to coarse-grained units such as attention heads or MLP blocks, overlooking finer structures like individual neurons. We propose a node-level pruning framework for circuit discovery that addresses both scalability and granularity limitations. Our method introduces learnable masks across multiple levels of granularity, from entire blocks to individual neurons, within a unified optimization objective. Granularity-specific sparsity penalties guide the pruning process, allowing a comprehensive compression in a single fine-tuning run. Empirically, our approach identifies circuits that are smaller in nodes than those discovered by prior methods; moreover, we demonstrate that many neurons deemed important by coarse methods are actually irrelevant, while still maintaining task performance. Furthermore, our method has a significantly lower memory footprint, 5-10x, as it does not require keeping intermediate activations in the memory to work.

</details>


### [59] [On Decision-Making Agents and Higher-Order Causal Processes](https://arxiv.org/abs/2512.10937)
*Matt Wilson*

Main category: cs.AI

TL;DR: 该论文建立了POMDP决策代理与量子高阶操作经典极限（单输入过程函数）之间的精确对应关系，揭示了AI与物理视角的二元解释。


<details>
  <summary>Details</summary>
Motivation: 探索人工智能中的决策代理理论与量子信息理论中过程函数概念之间的深层联系，为跨学科理解提供统一框架。

Method: 通过将POMDP中代理的策略和记忆更新组合成过程函数w，并使用链接积与环境交互，建立精确的数学对应关系。

Result: 成功建立了POMDP决策代理与单输入过程函数之间的精确对应，并扩展到多代理系统，将观测独立的去中心化POMDP识别为多输入过程函数的自然领域。

Conclusion: 该研究揭示了AI决策理论与量子过程函数理论之间的深刻联系，提供了跨学科的统一视角，并为多代理系统分析开辟了新途径。

Abstract: We establish a precise correspondence between decision-making agents in partially observable Markov decision processes (POMDPs) and one-input process functions, the classical limit of higher-order quantum operations. In this identification an agent's policy and memory update combine into a process function w that interacts with a POMDP environment via the link product. This suggests a dual interpretation: in the physics view, the process function acts as the environment into which local operations (agent interventions) are inserted, whereas in the AI view it encodes the agent and the inserted functions represent environments. We extend this perspective to multi-agent systems by identifying observation-independent decentralized POMDPs as natural domains for multi-input process functions.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [60] [Defining the Scope of Learning Analytics: An Axiomatic Approach for Analytic Practice and Measurable Learning Phenomena](https://arxiv.org/abs/2512.10081)
*Kensuke Takii,Changhao Liang,Hiroaki Ogata*

Main category: cs.CY

TL;DR: 本文提出了学习分析（LA）的第一个公理化理论，通过五个公理正式定义了LA的基本结构、范围和限制，为LA作为严谨科学学科奠定了理论基础。


<details>
  <summary>Details</summary>
Motivation: 学习分析领域在实践和技术创新方面迅速扩展，但其基础身份在理论上仍然不够明确。本文旨在填补这一空白，为LA建立明确的理论基础。

Method: 基于心理学对学习的定义和LA的方法论要求，提出了包含五个公理的框架：离散观察、经验构建、状态转换和推断。从这些公理推导出一系列定理和命题，定义了LA结构和LA实践作为形式对象。

Result: 该理论阐明了LA的认识论立场，包括学习者状态的内在不可观测性、时间顺序的不可约性、可达状态的约束以及确定性预测未来学习的不可能性。证明了该框架能够统一解释不同的LA方法（如贝叶斯知识追踪和仪表板）。

Conclusion: 该工作将LA定位为基于可观测性的状态转换系统的严谨科学，为LA作为学术学科的成熟建立了必要的理论基础，提供了设计分析方法和解释学习数据的指导原则。

Abstract: Learning Analytics (LA) has rapidly expanded through practical and technological innovation, yet its foundational identity has remained theoretically under-specified. This paper addresses this gap by proposing the first axiomatic theory that formally defines the essential structure, scope, and limitations of LA. Derived from the psychological definition of learning and the methodological requirements of LA, the framework consists of five axioms specifying discrete observation, experience construction, state transition, and inference. From these axioms, we derive a set of theorems and propositions that clarify the epistemological stance of LA, including the inherent unobservability of learner states, the irreducibility of temporal order, constraints on reachable states, and the impossibility of deterministically predicting future learning. We further define LA structure and LA practice as formal objects, demonstrating the sufficiency and necessity of the axioms and showing that diverse LA approaches -- such as Bayesian Knowledge Tracing and dashboards -- can be uniformly explained within this framework. The theory provides guiding principles for designing analytic methods and interpreting learning data while avoiding naive behaviorism and category errors by establishing an explicit theoretical inference layer between observations and states. This work positions LA as a rigorous science of state transition systems based on observability, establishing the theoretical foundation necessary for the field's maturation as a scholarly discipline.

</details>


### [61] [Dark Personality Traits and Online Toxicity: Linking Self-Reports to Reddit Activity](https://arxiv.org/abs/2512.10113)
*Aldo Cerulli,Benedetta Tessa,Giuseppe La Selva,Oronzo Mazzeo,Lorenzo Cima,Lucia Monacis,Stefano Cresci*

Main category: cs.CY

TL;DR: 研究黑暗人格特质与在线毒性行为的关系，发现黑暗特质主要影响毒性内容的生产而非感知，不同特质对应不同毒性表现，现有文本代理指标与验证测量关联有限。


<details>
  <summary>Details</summary>
Motivation: 虽然黑暗人格特质与在线不当行为（如网络霸凌、不文明言论）有关，但这些特质与实际在线行为之间的关系研究不足。需要深入探索黑暗特质如何影响用户在社交媒体上的具体行为表现。

Method: 开发了一个Web应用程序，整合亚马逊Mechanical Turk用户的验证心理问卷数据与他们的Reddit活动数据。收集了114名用户的近5.7万条Reddit评论（220万词元、15.27万句子），通过224个语言和行为特征进行系统表征，使用多重相关分析探索这些特征与问卷测量特质的关系。

Result: 1. 黑暗特质主要影响毒性内容的生产而非感知；2. 施虐和精神病态倾向与明显毒性语言最相关；3. 其他黑暗特质表现更微妙，常逃避简单文本代理；4. 自我报告敌对行为与实际在线活动一致；5. 现有手工制作的黑暗三合一特质文本代理与验证测量关联有限；6. 光明与黑暗特质有复杂交互，外向性减少霸凌倾向，尽责性与权利感和冷漠有适度关联。

Conclusion: 研究深化了对人格如何塑造在线毒性行为的理解，强调了开发可靠计算工具和针对性有效审核策略的机遇与挑战。黑暗特质的表现比现有简单文本指标更复杂，需要更精细的测量方法。

Abstract: Dark personality traits have been linked to online misbehavior such as trolling, incivility, and toxic speech. Yet the relationship between these traits and actual online conduct remains understudied. Here we investigate the associations between dark traits, online toxicity, and the socio-linguistic characteristics of online user activity. To explore this relationship, we developed a Web application that integrates validated psychological questionnaires from Amazon Mechanical Turk users to their Reddit activity data. This allowed collecting nearly 57K Reddit comments, including 2.2M tokens and 152.7K sentences from 114 users, that we systematically represent through 224 linguistic and behavioral features. We then examined their relationship to questionnaire-based trait measures via multiple correlation analyses. Among our findings is that dark traits primarily influence the production rather than the perception of online incivility. Sadistic and psychopathic tendencies are most strongly associated with overtly toxic language, whereas other dark dispositions manifest more subtly, often eluding simple textual proxies. Self-reported engagement in hostile behavior mirrors actual online activity, while existing hand-crafted textual proxies for dark triad traits show limited correspondence with our validated measures. Finally, bright and dark traits interact in nuanced ways, with extraversion reducing trolling tendencies and conscientiousness showing modest associations with entitlement and callousness. These findings deepen understanding of how personality shapes toxic online behavior and highlight both opportunities and challenges for developing reliable computational tools and targeted, effective moderation strategies.

</details>


### [62] [Enhancing Large Language Models for End-to-End Circuit Analysis Problem Solving](https://arxiv.org/abs/2512.10159)
*Liangliang Chen,Weiyu Sun,Ying Zhang*

Main category: cs.CY

TL;DR: 基于Gemini 2.5 Pro构建的增强型端到端电路问题求解系统，通过集成YOLO目标检测和ngspice验证循环，将电路问题求解准确率从79.52%提升至97.59%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在工程任务中的可靠性有限，特别是在需要多模态理解和精确数学推理的电路分析领域。同时，工程教育需要能够生成准确解决方案的可扩展AI工具，用于自动化作业反馈和问答。

Method: 1) 对Gemini 2.5 Pro在本科电路问题上进行基准测试，识别出电路识别幻觉和推理过程幻觉两大失败模式；2) 针对识别错误，集成微调的YOLO检测器和OpenCV处理来隔离电压和电流源，使Gemini能够从裁剪图像中重新识别源极性；3) 针对推理错误，引入基于ngspice的验证循环，Gemini生成.cir文件，ngspice模拟电路，差异触发迭代再生，并可选择人工审查。

Result: 在83个问题中，提出的管道实现了97.59%的成功率（81个正确解决方案），显著优于Gemini 2.5 Pro原始的79.52%准确率。

Conclusion: 该系统扩展了LLM在多模态工程问题求解方面的能力，支持创建高质量教育数据集和AI驱动的教学工具，为工程教育提供了可扩展的准确解决方案生成方法。

Abstract: Large language models (LLMs) have shown strong performance in data-rich domains such as programming, but their reliability in engineering tasks remains limited. Circuit analysis -- requiring multimodal understanding and precise mathematical reasoning -- highlights these challenges. Although Gemini 2.5 Pro improves diagram interpretation and analog-circuit reasoning, it still struggles to consistently produce correct solutions when given both text and circuit diagrams. At the same time, engineering education needs scalable AI tools capable of generating accurate solutions for tasks such as automated homework feedback and question-answering. This paper presents an enhanced, end-to-end circuit problem solver built on Gemini 2.5 Pro. We first benchmark Gemini on a representative set of undergraduate circuit problems and identify two major failure modes: 1) circuit-recognition hallucinations, particularly incorrect source polarity detection, and 2) reasoning-process hallucinations, such as incorrect current directions. To address recognition errors, we integrate a fine-tuned YOLO detector and OpenCV processing to isolate voltage and current sources, enabling Gemini to re-identify source polarities from cropped images with near-perfect accuracy. To reduce reasoning errors, we introduce an ngspice-based verification loop in which Gemini generates a .cir file, ngspice simulates the circuit, and discrepancies trigger iterative regeneration with optional human-in-the-loop review. Across 83 problems, the proposed pipeline achieves a 97.59% success rate (81 correct solutions), substantially outperforming Gemini 2.5 Pro's original 79.52% accuracy. This system extends LLM capabilities for multimodal engineering problem-solving and supports the creation of high-quality educational datasets and AI-powered instructional tools.

</details>


### [63] [Designing AI-Resilient Assessments Using Interconnected Problems: A Theoretically Grounded and Empirically Validated Framework](https://arxiv.org/abs/2512.10758)
*Kaihua Ding*

Main category: cs.CY

TL;DR: 提出AI弹性评估框架：通过互联问题而非模块化评估来抵抗生成式AI影响，经理论分析和多年实证验证有效。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速普及破坏了计算教育中的传统模块化评估，导致学术评估与行业实践脱节，需要设计能够抵抗AI影响的评估方法。

Method: 1) 建立两个理论结果：互联问题比模块化评估更具AI弹性；半结构化问题比完全开放式项目更能可靠测量学生能力。2) 使用四门大学数据科学课程数据(N=138)进行实证验证。3) 开发实用的评估设计框架。

Result: AI辅助的模块化作业学生得分接近完美，但监考考试成绩下降约30个百分点，显示AI分数膨胀。互联项目与模块化评估保持强相关，同时抵抗AI滥用。监考考试与预期学习成果对齐较弱。

Conclusion: 提出的评估设计框架能创建促进整合思维、反映现实世界AI增强工作流程、自然抵抗生成式AI简单委托的评估，有助于恢复学术诚信。

Abstract: The rapid adoption of generative AI has undermined traditional modular assessments in computing education, creating a disconnect between academic evaluation and industry practice. This paper presents a theoretically grounded framework for designing AI-resilient assessments, supported by formal analysis and multi-year empirical validation.
  We make three contributions. First, we establish two theoretical results: (1) assessments composed of interconnected problems, where outputs feed into subsequent stages, are more AI-resilient than modular assessments because current language models struggle with sustained multi-step reasoning and context; and (2) semi-structured problems with deterministic success criteria provide more reliable measures of student competency than fully open-ended projects, which allow AI systems to default to familiar solution patterns. These results challenge common policy and institutional guidance that promotes open-ended assessments as the primary safeguard for academic integrity.
  Second, we validate these results using data from four university data science courses (N = 138). While students achieve near-perfect scores on AI-assisted modular homework, performance drops by roughly 30 percentage points on proctored exams, indicating substantial AI score inflation. Interconnected projects remain strongly correlated with modular assessments, suggesting they measure the same underlying skills while resisting AI misuse. Proctored exams show weaker alignment, implying they may assess test-taking ability rather than intended learning outcomes.
  Third, we translate these findings into a practical assessment design framework. The proposed approach enables educators to create assessments that promote integrative thinking, reflect real-world AI-augmented workflows, and naturally resist trivial delegation to generative AI, thereby helping restore academic integrity.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [64] [Design and Implementation of a High-Precision Wind-Estimation UAV with Onboard Sensors](https://arxiv.org/abs/2512.10428)
*Haowen Yu,Na Fan,Xing Liu,Ximin Lyu*

Main category: cs.ET

TL;DR: 提出一种仅使用机载传感器的实时风矢量估计方法，通过扰动观测器估计气动力，再用薄板样条模型映射到风矢量，在多种场景下实现高精度风估计。


<details>
  <summary>Details</summary>
Motivation: 精确的实时风矢量估计对无人机安全、导航精度和能源效率至关重要。传统方法依赖外部传感器或简化车辆动力学，限制了其在敏捷飞行或资源受限平台上的应用。

Method: 基于机载传感器，首先使用扰动观测器估计外部气动力，然后通过薄板样条模型将这些力映射到风矢量。通过定制的风桶增强气动敏感性以提高估计精度。

Result: 在风洞、室内外飞行实验中验证，速度RMSE低至0.06 m/s（风洞）、0.22 m/s（室外悬停）、低于0.38 m/s（室内外动态飞行），方向RMSE低于7.3°，优于现有基线，并能提供垂直风估计（RMSE低于0.17 m/s）。

Conclusion: 该方法仅使用机载传感器即可实现高精度实时风矢量估计，在受控和真实世界条件下均表现优异，为无人机提供了可靠的风估计解决方案。

Abstract: Accurate real-time wind vector estimation is essential for enhancing the safety, navigation accuracy, and energy efficiency of unmanned aerial vehicles (UAVs). Traditional approaches rely on external sensors or simplify vehicle dynamics, which limits their applicability during agile flight or in resource-constrained platforms. This paper proposes a real-time wind estimation method based solely on onboard sensors. The approach first estimates external aerodynamic forces using a disturbance observer (DOB), and then maps these forces to wind vectors using a thin-plate spline (TPS) model. A custom-designed wind barrel mounted on the UAV enhances aerodynamic sensitivity, further improving estimation accuracy. The system is validated through comprehensive experiments in wind tunnels, indoor and outdoor flights. Experimental results demonstrate that the proposed method achieves consistently high-accuracy wind estimation across controlled and real-world conditions, with speed RMSEs as low as \SI{0.06}{m/s} in wind tunnel tests, \SI{0.22}{m/s} during outdoor hover, and below \SI{0.38}{m/s} in indoor and outdoor dynamic flights, and direction RMSEs under \ang{7.3} across all scenarios, outperforming existing baselines. Moreover, the method provides vertical wind estimates -- unavailable in baselines -- with RMSEs below \SI{0.17}{m/s} even during fast indoor translations.

</details>


### [65] [Metrics, KPIs, and Taxonomy for Data Valuation and Monetisation - Internal Processes Perspective](https://arxiv.org/abs/2512.10622)
*Eduardo Vyhmeister,Bastien Pietropaoli,Alejando Martinez Molina,Montserrat Gonzalez-Ferreiro,Gabriel Gonzalez-Castane,Jordi Arjona Aroca,Andrea Visentin*

Main category: cs.ET

TL;DR: 本文通过系统文献综述，构建了一个围绕数据质量、治理与合规、运营效率三个核心集群的数据价值评估与货币化指标体系分类法，为标准化估值框架提供基础。


<details>
  <summary>Details</summary>
Motivation: 数据驱动经济中数据价值评估和货币化成为核心挑战，但目前缺乏跨组织背景的统一框架来测量和管理数据价值。

Method: 采用系统文献综述方法，从平衡计分卡内部流程视角，识别、分类并关联数百个相关指标，构建包含数据质量、治理与合规、运营效率三个核心集群的综合分类法。

Result: 建立了全面的分类法，整合了重叠定义，澄清了概念依赖关系，并连接了支撑数据价值创造的技术、组织和监管指标。

Conclusion: 该分类法为开发标准化、基于证据的估值框架奠定基础，支持决策支持系统和数据估值模型的实际应用，推动建立跨行业一致、动态的数据评估与货币化方法。

Abstract: Data valuation and monetisation are emerging as central challenges in data-driven economies, yet no unified framework exists to measure or manage data value across organisational contexts. This paper presents a systematic literature review of metrics and key performance indicators (KPIs) relevant to data valuation and monetisation, focusing on the Internal Processes Perspective of the Balanced Scorecard (BSC). As part of a broader effort to explore all four BSC perspectives, we identify, categorise, and interrelate hundreds of metrics within a comprehensive taxonomy structured around three core clusters: Data Quality, Governance & Compliance, and Operational Efficiency. The taxonomy consolidates overlapping definitions, clarifies conceptual dependencies, and links technical, organisational, and regulatory indicators that underpin data value creation. By integrating these dimensions, it provides a foundation for the development of standardised and evidence-based valuation frameworks. Beyond its theoretical contribution, the taxonomy supports ongoing practical applications in decision-support systems and data valuation models, advancing the broader goal of establishing a coherent, dynamic approach to assessing and monetising data across industries.

</details>


### [66] [Unified Smart Factory Model: A model-based Approach for Integrating Industry 4.0 and Sustainability for Manufacturing Systems](https://arxiv.org/abs/2512.10631)
*Ishaan Kaushal,Amaresh Chakrabarti*

Main category: cs.ET

TL;DR: USFM是一个统一智能工厂模型，通过系统化信息映射将高层可持续发展目标转化为可测量的工厂级指标，特别适用于中小企业实现可持续目标。


<details>
  <summary>Details</summary>
Motivation: 解决可持续发展目标与工厂实际实施之间的脱节问题，帮助制造业特别是中小企业将高层可持续目标转化为可操作、可测量的工厂级指标。

Method: 使用基于模型的系统工程语言（对象过程方法论）将制造活动建模为制造、装配和辅助过程集合，整合制造过程与系统、数据过程、KPI选择与评估于统一框架中。

Result: 通过PCB组装工厂案例研究，展示了如何选择、建模环境可持续性KPI并将其映射到必要数据，重点关注能耗和环境影响指标，系统方法可减少冗余、避免关键信息遗漏并增强数据收集。

Conclusion: USFM填补了可持续目标与实际实施之间的空白，为特别是中小企业实现可持续目标提供了显著效益，系统化方法有助于减少冗余、最小化关键信息遗漏风险并增强数据收集。

Abstract: This paper presents the Unified Smart Factory Model (USFM), a comprehensive framework designed to translate high-level sustainability goals into measurable factory-level indicators with a systematic information map of manufacturing activities. The manufacturing activities were modelled as set of manufacturing, assembly and auxiliary processes using Object Process Methodology, a Model Based Systems Engineering (MBSE) language. USFM integrates Manufacturing Process and System, Data Process, and Key Performance Indicator (KPI) Selection and Assessment in a single framework. Through a detailed case study of Printed Circuit Board (PCB) assembly factory, the paper demonstrates how environmental sustainability KPIs can be selected, modelled, and mapped to the necessary data, highlighting energy consumption and environmental impact metrics. The model's systematic approach can reduce redundancy, minimize the risk of missing critical information, and enhance data collection. The paper concluded that the USFM bridges the gap between sustainability goals and practical implementation, providing significant benefits for industries specifically SMEs aiming to achieve sustainability targets.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [67] [Enhancing Fake-News Detection with Node-Level Topological Features](https://arxiv.org/abs/2512.09974)
*Kaiyuan Xu*

Main category: cs.SI

TL;DR: 在假新闻检测中，通过为每个节点添加度中心性和局部聚类系数这两个经典图论指标，显著提升了检测性能，证明了显式拓扑特征的价值。


<details>
  <summary>Details</summary>
Motivation: 现有假新闻检测方法虽然整合了内容、用户偏好和传播结构，但将图级表示学习完全交给GNN，缺乏显式的拓扑线索。需要填补这一空白，引入可解释的拓扑特征。

Method: 提出轻量级增强方法：为每个节点在其原始BERT和用户画像嵌入基础上，附加两个经典图论指标——度中心性和局部聚类系数，从而显式标记节点作为枢纽和社区成员的角色。

Result: 在UPFD Politifact数据集上，这一简单修改将宏观F1分数从0.7753提升到0.8344，显著超越了原始基线。

Conclusion: 研究不仅证明了显式拓扑特征在假新闻检测中的实用价值，还为其他信息传播任务中融合图指标提供了可解释、易于复现的模板。

Abstract: In recent years, the proliferation of misinformation and fake news has posed serious threats to individuals and society, spurring intense research into automated detection methods. Previous work showed that integrating content, user preferences, and propagation structure achieves strong performance, but leaves all graph-level representation learning entirely to the GNN, hiding any explicit topological cues. To close this gap, we introduce a lightweight enhancement: for each node, we append two classical graph-theoretic metrics, degree centrality and local clustering coefficient, to its original BERT and profile embeddings, thus explicitly flagging the roles of hub and community. In the UPFD Politifact subset, this simple modification boosts macro F1 from 0.7753 to 0.8344 over the original baseline. Our study not only demonstrates the practical value of explicit topology features in fake-news detection but also provides an interpretable, easily reproducible template for fusing graph metrics in other information-diffusion tasks.

</details>


### [68] [A Simulation Framework for Studying Recommendation-Network Co-evolution in Social Platforms](https://arxiv.org/abs/2512.10106)
*Gaurav Koley,Sanika Digrajkar*

Main category: cs.SI

TL;DR: 本文提出一个基于代理的模拟器，研究推荐系统如何重塑社交网络，发现推荐系统的激活时机对网络结构有显著影响。


<details>
  <summary>Details</summary>
Motivation: 在真实社交平台上研究推荐系统对网络结构的影响存在诸多混杂因素，且控制实验可能对用户造成伤害。因此需要开发一个可控的模拟环境来研究推荐系统与社交网络之间的共同演化关系。

Method: 开发了一个基于代理的模拟器，其中内容生产、关系形成和图注意力网络（GAT）推荐系统在闭环中共同演化。使用Mastodon数据进行参数校准，并在Bluesky上进行验证。通过18种配置在100个代理上进行实验，并扩展到5000个代理进行规模实验。

Result: 推荐系统的激活时机显著影响网络结构：在t=10时引入推荐比t=40时引入减少传递性10%，但参与度差异小于8%。延迟激活增加内容多样性9%，同时减少模块性4%。规模实验显示这种效应持续存在但有所减弱。雅可比分析确认在有界反应参数下的局部稳定性。

Conclusion: 推荐系统的引入时机对社交网络结构有重要影响，早期引入会减少网络传递性，而延迟引入能提高内容多样性。这些发现对推荐系统的部署时机具有实际指导意义，模拟器为研究推荐系统与社交网络的共同演化提供了有效工具。

Abstract: Studying how recommendation systems reshape social networks is difficult on live platforms: confounds abound, and controlled experiments risk user harm. We present an agent-based simulator where content production, tie formation, and a graph attention network (GAT) recommender co-evolve in a closed loop. We calibrate parameters using Mastodon data and validate out-of-sample against Bluesky (4--6\% error on structural metrics; 10--15\% on held-out temporal splits). Across 18 configurations at 100 agents, we find that \emph{activation timing} affects outcomes: introducing recommendations at $t=10$ vs.\ $t=40$ decreases transitivity by 10\% while engagement differs by $<$8\%. Delaying activation increases content diversity by 9\% while reducing modularity by 4\%. Scaling experiments ($n$ up to 5,000) show the effect persists but attenuates. Jacobian analysis confirms local stability under bounded reactance parameters. We release configuration schemas and reproduction scripts.

</details>


### [69] [Understanding Toxic Interaction Across User and Video Clusters in Social Video Platforms](https://arxiv.org/abs/2512.10233)
*Qiao Wang,Liang Liu,Mitsuo Yoshida*

Main category: cs.SI

TL;DR: 该研究通过视频-用户交互矩阵的聚类分析，发现Bilibili平台上毒性表达集中在高曝光视频群组，而评论导向的用户群组毒性较低，为平台干预提供了针对性建议。


<details>
  <summary>Details</summary>
Motivation: 现有研究常孤立分析文本或用户，忽略了毒性互动发生的结构性背景。不考虑"谁与谁互动"以及"围绕什么内容"，难以解释负面表达为何在特定社群中聚集。本研究旨在填补这一空白。

Method: 以Bilibili为研究对象，构建视频-用户交互矩阵，通过归一化和降维处理，使用K-means分别对视频侧和用户侧进行聚类分析。结合用户行为特征（消息长度、发帖频率、来源）、文本特征（情感、毒性）和视频属性进行比较。

Result: 1. 用户群组在交互风格（消息长度、评论比例）上呈现明显分层
2. 情感和毒性差异在视频群组中较弱或不一致
3. 高曝光视频群组集中更多毒性表达
4. 评论导向且消息较长的用户群组毒性较低

Conclusion: 平台应针对高增长期的高曝光视频群组及时干预，对评论导向的低毒性用户群组加强理性对话机制并鼓励跨话题参与。结构化的聚类方法能有效整合社会关系和内容信号识别稳定群体。

Abstract: Social video platforms shape how people access information, while recommendation systems can narrow exposure and increase the risk of toxic interaction. Previous research has often examined text or users in isolation, overlooking the structural context in which such toxic interactions occur. Without considering who interacts with whom and around what content, it is difficult to explain why negative expressions cluster within particular communities. To address this issue, this study focuses on the Chinese social video platform Bilibili, incorporating video-level information as the environment for user expression, modeling users and videos in an interaction matrix. After normalization and dimensionality reduction, we perform separate clustering on both sides of the video-user interaction matrix with K-means. Cluster assignments facilitate comparisons of user behavior, including message length, posting frequency, and source (barrage and comment), as well as textual features such as sentiment and toxicity, and video attributes defined by uploaders. Such a clustering approach integrates structural ties with content signals to identify stable groups of videos and users. We find clear stratification in interaction style (message length, comment ratio) across user clusters, while sentiment and toxicity differences are weak or inconsistent across video clusters. Across video clusters, viewing volume exhibits a clear hierarchy, with higher exposure groups concentrating more toxic expressions. For such a group, platforms should require timely intervention during periods of rapid growth. Across user clusters, comment ratio and message length form distinct hierarchies, and several clusters with longer and comment-oriented messages exhibit lower toxicity. For such groups, platforms should strengthen mechanisms that sustain rational dialogue and encourage engagement across topics.

</details>


### [70] [The Circulate and Recapture Dynamic of Fan Mobility in Agency-Affiliated VTuber Networks](https://arxiv.org/abs/2512.10240)
*Tomohiro Murakami,Mitsuo Yoshida*

Main category: cs.SI

TL;DR: 研究VTuber经纪公司如何影响粉丝观看行为：粉丝倾向于在同经纪公司内部流动，跨公司流动有限，形成稳定的参与模式


<details>
  <summary>Details</summary>
Motivation: VTuber经纪公司（MCNs）通过捆绑虚拟主播来策划频道组合、协调节目和品牌，但尚不清楚这种隶属关系是让粉丝绑定单一频道，还是鼓励在组合内流动以缓冲流失，以及这些微观动态如何与中观层面的观众重叠网络相关

Method: 使用多年、以粉丝为中心的VTuber直播参与面板数据，构建月度创作者间观众重叠网络（使用对观众规模不对称鲁棒的相似性度量）。微观层面追踪留存率、主要观看创作者变化和不活跃情况；中观层面比较隶属关系特定子图的结构特性，并可视化观众状态转换

Result: 发现松散流动性模式：粉丝倾向于保持活跃，同时在同一隶属类型内重新分配注意力，跨隶属类型流动有限。网络结果显示全球重叠趋于一致，但隶属子图内的局部邻域保持更密集。流程图揭示了循环和重新捕获动态，稳定参与而不依赖单一频道锁定

Conclusion: 贡献了可重复的VTuber直播测量框架，将微观轨迹与中观组织联系起来，为创作者劳动、影响者营销和视频平台治理研究提供信息。观察到的规律与VTuber经纪公司设计的接近性和协调重新捕获一致

Abstract: VTuber agencies -- multichannel networks (MCNs) that bundle Virtual YouTubers (VTubers) on YouTube -- curate portfolios of channels and coordinate programming, cross appearances, and branding in the live-streaming VTuber ecosystem. It remains unclear whether affiliation binds fans to a single channel or instead encourages movement within a portfolio that buffers exit, and how these micro level dynamics relate to meso level audience overlap. This study examines how affiliation shapes short horizon viewer trajectories and the organization of audience overlap networks by contrasting agency affiliated and independent VTubers. Using a large, multiyear, fan centered panel of VTuber live stream engagement on YouTube, we construct monthly audience overlap between creators with a similarity measure that is robust to audience size asymmetries. At the micro level, we track retention, changes in the primary creator watched (oshi), and inactivity; at the meso level, we compare structural properties of affiliation specific subgraphs and visualize viewer state transitions. The analysis identifies a pattern of loose mobility: fans tend to remain active while reallocating attention within the same affiliation type, with limited leakage across affiliation type. Network results indicate convergence in global overlap while local neighborhoods within affiliated subgraphs remain persistently denser. Flow diagrams reveal circulate and recapture dynamics that stabilize participation without relying on single channel lock in. We contribute a reusable measurement framework for VTuber live streaming that links micro level trajectories to meso level organization and informs research on creator labor, influencer marketing, and platform governance on video platforms. We do not claim causal effects; the observed regularities are consistent with proximity engineered by VTuber agencies and coordinated recapture.

</details>


### [71] [Balancing Turnover and Promotion Outcomes: Evidence on the Optimal Hybrid-Work Frequency](https://arxiv.org/abs/2512.10328)
*Xuan Lu,Yulin Yu*

Main category: cs.SI

TL;DR: 研究发现每周远程工作约2天是最佳平衡点，能最大化晋升机会并最小化离职风险，但效果因性别、角色类型和领导地位而异。


<details>
  <summary>Details</summary>
Motivation: 尽管远程和混合工作安排被广泛讨论，但尚未有实证研究确定能同时考虑多个组织结果的最佳远程工作天数。本研究旨在填补这一空白，重点关注两个关键职业结果：离职风险和晋升机会。

Method: 使用一家拥有超过100万员工的公司的大规模观察性活动数据，分析远程工作频率如何影响员工职业轨迹。通过统计方法识别最佳远程工作天数，并考察不同亚组（性别、角色类型、领导地位）的差异，同时分析时间分配模式作为解释机制。

Result: 研究发现远程工作频率与离职风险呈U型关系（先降后升），与晋升机会呈倒U型关系（先升后降）。最佳平衡点约为每周2天远程工作。男性员工从远程工作中获得更多晋升益处；支持性员工没有晋升收益；领导者在远程环境中面临更大挑战（离职风险增加，晋升机会减少）。时间分配模式部分解释了这些影响。

Conclusion: 每周约2天的远程工作安排能在员工晋升（积极结果）和组织离职风险（负面结果）之间达到最佳平衡。然而，这种平衡因员工特征而异，组织在制定混合工作政策时应考虑这些差异，特别是领导者和支持性员工的独特挑战。

Abstract: Hybrid work policy, especially return-to-office requirements, remains a globally salient topic as workers, companies, and governments continue to debate and disagree. Despite extensive discussions on the benefits and drawbacks of remote and hybrid arrangements, the optimal number of remote days that jointly considers multiple organizational outcomes has not been empirically established. Focusing on two critical career outcomes -- turnover risk and promotion -- we examine how remote work frequency shapes employee trajectories using large-scale observational activity data from a company with over one million employees. We find that increased remote-work frequency is associated with an initial decrease and then an increase in turnover, while promotion likelihood initially rises and then declines. Accordingly, we identify approximately two remote days per week as an optimal balance -- maximizing promotion, a positive outcome for employees, while minimizing turnover, which is undesirable for organizations and may indicate negative employee experiences. These patterns vary across subgroups defined by gender, role type, and leadership status. Several notable results emerge. First, male employees derive greater promotion benefits from remote work than female employees. Second, support workers (non-core business roles) do not experience promotion gains, and the reduction in turnover at their optimal remote-work frequency is marginal compared with employees in core business roles. Third, organizational leaders face greater challenges in remote settings than individual contributors: their turnover risk increases substantially at higher remote frequencies, and their likelihood of promotion decreases as remote frequency rises. We further show that time-allocation patterns partly explain how remote-work frequency influences these career outcomes.

</details>


### [72] [Kicking Politics: How Football Fan Communities Became Arenas for Political Influence](https://arxiv.org/abs/2512.10737)
*Helen Paffard,Diogo Pacheco*

Main category: cs.SI

TL;DR: 该研究分析了英国脱欧公投后政治运动如何利用Twitter上的足球迷社区进行政治动员，揭示了非政治性在线空间中的政治影响机制。


<details>
  <summary>Details</summary>
Motivation: 足球迷社区具有强烈的集体认同和部落行为特征，为政治影响提供了肥沃土壤。研究旨在理解政治话语如何嵌入看似非政治的足球讨论中，以及各种行为体如何利用这些社区进行政治动员。

Method: 结合社交网络分析和内容分析，研究2016-2017年英国脱欧公投后Twitter上足球迷社区的政治参与。通过案例研究分析标签劫持、嵌入式行动主义和政治"扩音器"等策略。

Result: 研究发现政党、媒体、活动团体和匿名影响者等多种行为体在足球迷社区中动员支持、引发反应和塑造观点。这些行为体利用球迷文化放大政治信息，展示了非政治在线空间中的政治影响机制。

Conclusion: 政治运动成功地将足球迷社区转化为政治动员平台，揭示了在线政治影响的新途径。研究为未来构建更广泛的分析框架奠定了基础，强调了理解非传统政治空间对民主进程的重要性。

Abstract: This paper investigates how political campaigns engaged UK football fan communities on Twitter in the aftermath of the Brexit Referendum (2016-2017). Football fandom, with its strong collective identities and tribal behaviours, offers fertile ground for political influence. Combining social network and content analysis, we examine how political discourse became embedded in football conversations. We show that a wide range of actors -- including parties, media, activist groups, and pseudonymous influencers -- mobilised support, provoked reactions, and shaped opinion within these communities. Through case studies of hashtag hijacking, embedded activism, and political "megaphones", we illustrate how campaigns leveraged fan cultures to amplify political messages. Our findings highlight mechanisms of political influence in ostensibly non-political online spaces and point toward the development of a broader framework in future work.

</details>


### [73] [Echoes of Automation: How Bots Shaped Political Discourse in Brazil](https://arxiv.org/abs/2512.10749)
*Merve Ipek Bal,Diogo Pacheco*

Main category: cs.SI

TL;DR: 研究分析了2018-2022年巴西总统选举期间超过3.15亿条推文，发现机器人账号主要依赖转发和回复来放大特定政治议程，尤其是博索纳罗相关内容，而人类用户讨论范围更广。


<details>
  <summary>Details</summary>
Motivation: 社交媒体平台已成为政治传播的核心，机器人账号的活动引发了关于信息放大、操纵和虚假信息的严重关切。研究旨在分析巴西总统选举期间机器人与人类用户的行为差异及其对在线政治话语的影响。

Method: 研究收集了2018年8月至2022年6月期间超过3.15亿条推文，采用行为模式分析、情感分析和主题建模等方法，对比分析了2018年巴西总统选举及2022年选举前机器人与人类生成内容的特点。

Result: 1. 机器人账号过度依赖转发和回复，特别是2018年选举后回复活动激增，表明采用了对话渗透和放大的策略。
2. 情感分析显示机器人保持较窄的情感基调，而人类情感随政治事件波动更强烈。
3. 主题建模揭示机器人内容重复且以博索纳罗为中心，而人类用户讨论范围更广，包括其他候选人、公民关切和个人反思。

Conclusion: 机器人账号作为狭窄议程的放大器，具有扭曲在线政治话语的潜力。研究强调了社交媒体平台需要关注机器人活动对民主进程的影响，并采取相应措施维护健康的在线政治讨论环境。

Abstract: In an era where social media platforms are central to political communication, the activity of bots raises pressing concerns about amplification, manipulation, and misinformation. Drawing on more than 315 million tweets posted from August 2018 to June 2022, we examine behavioural patterns, sentiment dynamics, and the thematic focus of bot- versus human-generated content spanning the 2018 Brazilian presidential election and the lead-up to the 2022 contest. Our analysis shows that bots relied disproportionately on retweets and replies, with reply activity spiking after the 2018 election, suggesting tactics of conversational infiltration and amplification. Sentiment analysis indicates that bots maintained a narrower emotional tone, in contrast to humans, whose sentiment fluctuated more strongly with political events. Topic modelling further reveals bots' repetitive, Bolsonaro-centric messaging, while human users engaged with a broader range of candidates, civic concerns, and personal reflections. These findings underscore bots' role as amplifiers of narrow agendas and their potential to distort online political discourse.

</details>

<div id=toc></div>

# Table of Contents

- [cs.CY](#cs.CY) [Total: 2]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.AI](#cs.AI) [Total: 31]
- [cs.SI](#cs.SI) [Total: 6]
- [econ.EM](#econ.EM) [Total: 4]
- [stat.AP](#stat.AP) [Total: 1]


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [1] [Making AI Work: An Autoethnography of a Workaround in Higher Education](https://arxiv.org/abs/2512.21055)
*Shang Chieh Lee,Bhuva Narayan,Simon Buckingham Shum,Stella Ng,A. Baki Kocaballi*

Main category: cs.CY

TL;DR: 本文通过分析一个陷入僵局的GenAI项目，揭示了高等教育中实施生成式人工智能时被忽视的政治化隐性劳动，指出用户驱动的工作区不是偏离而是社会技术整合的必要组成部分。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常关注GenAI在高等教育中的战略目标，而忽视了使其功能化所需的政治化隐性劳动。本文旨在揭示当机构目标（赋能非技术人员）与企业LLM技术限制冲突时产生的社会技术摩擦，以及组织领域性和职位权力如何影响GenAI实施。

Method: 采用分析性自民族志方法，研究一个陷入僵局的GenAI项目，重点关注为应对技术限制、组织领域性和职位权力挑战而开发的工作区。运用Alter（2014）的工作区理论，将"衔接工作"解释为"隐性劳动"的一种形式。

Result: 研究发现用户驱动的工作区应被视为社会技术整合的组成部分而非偏离。然而，这种整合突显了现代GenAI的核心悖论：为"未完成"系统创建的工作区可能同时产生非官方的"影子"系统，并掩盖关键的社会技术劳动。隐性劳动是GenAI在实践中功能化的重要而非边缘组成部分。

Conclusion: 整合GenAI到复杂组织政治中所需的隐性劳动是其功能化实践的关键组成部分。研究呼吁关注用户创新和技术实践中的社会技术整合过程，认识到工作区作为应对技术限制和组织挑战的必要策略，而非简单的偏离行为。

Abstract: Research on the implementation of Generative Artificial Intelligence (GenAI) in higher education often focuses on strategic goals, overlooking the hidden, and often politically charged, labour required to make it functional. This paper provides an insider's account of the sociotechnical friction that arises when an institutional goal of empowering non-technical staff conflicts with the technical limitations of enterprise Large Language Models (LLMs). Through analytic autoethnography, this study examines a GenAI project pushed to an impasse, focusing on a workaround developed to navigate not only technical constraints but also the combined challenge of organisational territoriality and assertions of positional power. Drawing upon Alter's (2014) theory of workarounds, the analysis interprets "articulation work" as a form of "invisible labour". By engaging with the Information Systems (IS) domains of user innovation and technology-in-practice, this study argues that such user-driven workarounds should be understood not as deviations, but as integral acts of sociotechnical integration. This integration, however, highlights the central paradoxes of modern GenAI where such workarounds for "unfinished" systems can simultaneously create unofficial "shadow" systems and obscure the crucial, yet invisible, sociotechnical labour involved. The findings suggest that the invisible labour required to integrate GenAI within complex organisational politics is an important, rather than peripheral, component of how it becomes functional in practice.

</details>


### [2] [Microtopia: Exploring the Impact of Interdisciplinary Projects on Ethnic Minority Female Pupils' Perceptions of Computer Science](https://arxiv.org/abs/2512.21214)
*Nadine Aburumman,Ju-Ling Shih,Cigdem Sengul,Monica Pereira*

Main category: cs.CY

TL;DR: Microtopia项目通过结合编程、设计思维和AI/IoT/机器人技术，围绕可持续发展目标和中国五行哲学设计活动，成功提升了少数族裔女孩对计算机科学的兴趣和信心。


<details>
  <summary>Details</summary>
Motivation: 解决计算机科学领域少数族裔女孩参与度不足的问题，通过创新教育方法扩大CS的参与多样性。

Method: 采用跨学科项目设计，结合编程与设计思维，融入AI、IoT和机器人技术。活动围绕联合国可持续发展目标和中国五行哲学，组织学生按"国家"分组进行行业项目（医疗、交通、时尚等）。使用前后测问卷调查社会经济和民族文化因素的影响。

Result: 统计分析显示学生信心、乐趣和动机显著提升，特别是当计算与可持续发展和全球挑战相关时效果更明显。

Conclusion: 将计算机科学与可持续发展目标和文化元素结合，能有效提升少数族裔女孩对CS的兴趣和参与意愿，为促进CS教育多样性提供了有效途径。

Abstract: This paper presents Microtopia, an interdisciplinary programme designed to broaden participation in computer science (CS) among ethnic minority girls. The programme combined coding with design thinking activities, incorporating Artificial Intelligence (AI), the Internet of Things (IoT), and Robotics as key technologies. Learning activities were formulated around the UN Sustainable Development Goals and the Chinese Five Elements philosophy to support problem-based learning. Pupils were organised into "nations" and engaged in sector-based projects (e.g., healthcare, transportation, fashion, tourism, food, architecture). Using pre- and post-questionnaires, we investigated how socioeconomic and ethnocultural factors influenced pupils' preconceptions of CS, and whether participation in Microtopia shifted their perceptions. Through statistical analysis of the questionnaire data, we identified significant increases in students' confidence, enjoyment, and motivation, particularly when computing was presented as relevant to sustainability and global challenges.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [3] [Hardware-Algorithm Co-Design for Hyperdimensional Computing Based on Memristive System-on-Chip](https://arxiv.org/abs/2512.20808)
*Yi Huang,Alireza Jaberi Rad,Qiangfei Xia*

Main category: cs.ET

TL;DR: 提出了一种基于忆阻器片上系统的超维计算硬件-算法协同设计方法，用于边缘设备的高效能AI部署


<details>
  <summary>Details</summary>
Motivation: 超维计算适合资源受限的边缘AI应用，忆阻器内存计算提供高效能硬件解决方案，需要将两者优势结合

Method: 采用硬件-算法协同设计：硬件方面利用忆阻器交叉阵列的固有随机性进行编码，使用模拟内存计算进行分类；算法层面开发硬件感知编码技术，将数据特征映射到超维向量

Result: 在硬件实验中实现了语言分类任务90.71%的准确率

Conclusion: 该方法展示了在边缘设备上实现高效能AI部署的潜力

Abstract: Hyperdimensional computing (HDC), utilizing a parallel computing paradigm and efficient learning algorithm, is well-suited for resource-constrained artificial intelligence (AI) applications, such as in edge devices. In-memory computing (IMC) systems based on memristive devices complement this by offering energy-efficient hardware solutions. To harness the advantages of both memristive IMC hardware and HDC algorithms, we propose a hardware-algorithm co-design approach for implementing HDC on a memristive System-on-Chip (SoC). On the hardware side, we utilize the inherent randomness of memristive crossbar arrays for encoding and employ analog IMC for classification. At the algorithm level, we develop hardware-aware encoding techniques that map data features into hyperdimensional vectors, optimizing the classification process within the memristive SoC. Experimental results in hardware demonstrate 90.71% accuracy in the language classification task, highlighting the potential of our approach for achieving energy-efficient AI deployments on edge devices.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [4] [From Pilots to Practices: A Scoping Review of GenAI-Enabled Personalization in Computer Science Education](https://arxiv.org/abs/2512.20714)
*Iman Reihanian,Yunfei Hou,Qingquan Sun*

Main category: cs.AI

TL;DR: 这篇综述分析了2023-2025年间32项研究，探讨生成式AI在高等教育计算机科学教育中的个性化应用效果，识别了五种应用领域和成功设计模式，提出了探索优先的采用框架。


<details>
  <summary>Details</summary>
Motivation: 生成式AI能够实现大规模个性化计算机科学教育，但需要了解这种个性化是支持还是削弱学习效果，因此需要系统梳理现有研究证据。

Method: 采用范围综述方法，从259条记录中有目的地抽样32项研究（2023-2025年），分析个性化机制和有效性信号，识别应用领域和设计选择对学习结果的影响。

Result: 识别了五个应用领域：智能辅导、个性化材料、形成性反馈、AI增强评估和代码审查；发现包含解释优先指导、解决方案保留、渐进提示阶梯和工件基础的设计比无约束聊天界面效果更好；成功实施有四种模式。

Conclusion: 生成式AI可以作为精确支架机制，但需要嵌入可审计的工作流程中，保留学生的生产性挣扎，同时扩大个性化支持；提出了探索优先的采用框架，并讨论了学术诚信、隐私、偏见等风险及缓解措施。

Abstract: Generative AI enables personalized computer science education at scale, yet questions remain about whether such personalization supports or undermines learning. This scoping review synthesizes 32 studies (2023-2025) purposively sampled from 259 records to map personalization mechanisms and effectiveness signals in higher-education computer science contexts. We identify five application domains: intelligent tutoring, personalized materials, formative feedback, AI-augmented assessment, and code review, and analyze how design choices shape learning outcomes. Designs incorporating explanation-first guidance, solution withholding, graduated hint ladders, and artifact grounding (student code, tests, and rubrics) consistently show more positive learning processes than unconstrained chat interfaces. Successful implementations share four patterns: context-aware tutoring anchored in student artifacts, multi-level hint structures requiring reflection, composition with traditional CS infrastructure (autograders and rubrics), and human-in-the-loop quality assurance. We propose an exploration-first adoption framework emphasizing piloting, instrumentation, learning-preserving defaults, and evidence-based scaling. Recurrent risks include academic integrity, privacy, bias and equity, and over-reliance, and we pair these with operational mitigation. The evidence supports generative AI as a mechanism for precision scaffolding when embedded in audit-ready workflows that preserve productive struggle while scaling personalized support.

</details>


### [5] [Beyond Context: Large Language Models Failure to Grasp Users Intent](https://arxiv.org/abs/2512.21110)
*Ahmed M. Hussain,Salahuddin Salahuddin,Panos Papadimitratos*

Main category: cs.AI

TL;DR: 当前LLM安全机制存在重大漏洞：无法理解上下文和识别用户意图，导致恶意用户可通过情感框架、渐进揭示和学术论证等系统方法绕过安全防护，推理能力反而加剧了这种漏洞。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的安全方法主要关注显性有害内容，但忽视了关键漏洞：无法理解上下文和识别用户意图。这种缺陷使得恶意用户可以系统性地利用这些漏洞绕过安全机制，需要对此进行实证评估。

Method: 对多个最先进的LLM进行实证评估，包括ChatGPT、Claude、Gemini和DeepSeek。通过情感框架、渐进揭示和学术论证等技术测试安全机制的规避情况，特别关注推理能力配置对漏洞的影响。

Result: 研究发现这些LLM的安全机制可以通过系统方法被绕过。值得注意的是，启用推理功能的配置反而放大了漏洞的有效性，提高了事实精确性但未能质疑潜在意图。Claude Opus 4.1是例外，在某些用例中优先考虑意图检测而非信息提供。

Conclusion: 当前架构设计存在系统性漏洞，需要范式转变：将上下文理解和意图识别作为核心安全能力，而不是事后保护机制。安全设计必须从内容过滤转向意图理解。

Abstract: Current Large Language Models (LLMs) safety approaches focus on explicitly harmful content while overlooking a critical vulnerability: the inability to understand context and recognize user intent. This creates exploitable vulnerabilities that malicious users can systematically leverage to circumvent safety mechanisms. We empirically evaluate multiple state-of-the-art LLMs, including ChatGPT, Claude, Gemini, and DeepSeek. Our analysis demonstrates the circumvention of reliable safety mechanisms through emotional framing, progressive revelation, and academic justification techniques. Notably, reasoning-enabled configurations amplified rather than mitigated the effectiveness of exploitation, increasing factual precision while failing to interrogate the underlying intent. The exception was Claude Opus 4.1, which prioritized intent detection over information provision in some use cases. This pattern reveals that current architectural designs create systematic vulnerabilities. These limitations require paradigmatic shifts toward contextual understanding and intent recognition as core safety capabilities rather than post-hoc protective mechanisms.

</details>


### [6] [BitRL-Light: 1-bit LLM Agents with Deep Reinforcement Learning for Energy-Efficient Smart Home Lighting Optimization](https://arxiv.org/abs/2512.20623)
*Ravi Gupta,Shabista Haider*

Main category: cs.AI

TL;DR: BitRL-Light：结合1位量化LLM与DQN强化学习的智能家居照明框架，在树莓派上实现实时控制，相比全精度模型能耗降低71.4倍，相比规则系统节能32%，延迟低于200ms，用户满意度95%。


<details>
  <summary>Details</summary>
Motivation: 智能家居照明系统消耗15-20%住宅能源，但缺乏同时优化用户舒适度和能源效率的自适应智能。现有方法要么依赖云端计算（延迟高、隐私问题），要么在边缘设备上计算资源受限。

Method: 提出BitRL-Light框架：1）部署1位量化的Llama-3.2-1B模型到树莓派硬件；2）结合深度Q网络强化学习进行多目标优化（能耗、舒适度、昼夜节律对齐）；3）通过Google Home/IFTTT集成处理自然语言命令；4）从手动覆盖中学习隐式反馈。

Result: 1）相比全精度模型能耗降低71.4倍；2）相比规则系统节能32%；3）树莓派4上推理延迟低于200ms；4）用户满意度95%；5）1位模型在ARM处理器上比2位模型快5.07倍，同时保持92%任务准确率。

Conclusion: 该工作建立了在资源受限物联网设备上部署自适应AI的实用框架，实现了无需云依赖的智能家居自动化，为边缘智能照明控制提供了高效解决方案。

Abstract: Smart home lighting systems consume 15-20% of residential energy but lack adaptive intelligence to optimize for user comfort and energy efficiency simultaneously. We present BitRL-Light, a novel framework combining 1-bit quantized Large Language Models (LLMs) with Deep Q-Network (DQN) reinforcement learning for real-time smart home lighting control on edge devices. Our approach deploys a 1-bit quantized Llama-3.2-1B model on Raspberry Pi hardware, achieving 71.4 times energy reduction compared to full-precision models while maintaining intelligent control capabilities. Through multi-objective reinforcement learning, BitRL-Light learns optimal lighting policies from user feedback, balancing energy consumption, comfort, and circadian alignment. Experimental results demonstrate 32% energy savings compared to rule-based systems, with inference latency under 200ms on Raspberry Pi 4 and 95% user satisfaction. The system processes natural language commands via Google Home/IFTTT integration and learns from implicit feedback through manual overrides. Our comparative analysis shows 1-bit models achieve 5.07 times speedup over 2-bit alternatives on ARM processors while maintaining 92% task accuracy. This work establishes a practical framework for deploying adaptive AI on resource-constrained IoT devices, enabling intelligent home automation without cloud dependencies.

</details>


### [7] [Quantum-Inspired Multi Agent Reinforcement Learning for Exploration Exploitation Optimization in UAV-Assisted 6G Network Deployment](https://arxiv.org/abs/2512.20624)
*Mazyar Taghavi,Javad Vahidi*

Main category: cs.AI

TL;DR: 提出量子启发的多智能体强化学习框架，用于优化6G无人机网络部署中的探索-利用权衡，通过变分量子电路和量子近似优化算法提升性能。


<details>
  <summary>Details</summary>
Motivation: 在6G无人机辅助网络部署中，智能无人机需要在部分可观测和动态环境下协调工作以最大化信号覆盖。传统MARL方法在探索-利用权衡方面存在局限，需要更高效的优化框架。

Method: 结合经典MARL算法与量子启发优化技术，使用变分量子电路(VQC)作为核心结构，采用量子近似优化算法(QAOA)进行组合优化。融入贝叶斯推断、高斯过程和变分推断进行概率建模，采用集中训练分散执行(CTDE)范式，通过共享内存和局部视图网格增强智能体可观测性。

Result: 实验表明该框架提高了样本效率、加速了收敛、增强了覆盖性能并保持了鲁棒性。与PPO和DDPG基线相比，QI-MARL在探索-利用权衡方面取得了更优的平衡。

Conclusion: 量子启发框架为多智能体强化学习中的探索-利用优化提供了有效解决方案，在6G无人机网络部署中表现出优越性能，所有实现代码已开源以确保可复现性。

Abstract: This study introduces a quantum inspired framework for optimizing the exploration exploitation tradeoff in multiagent reinforcement learning, applied to UAVassisted 6G network deployment. We consider a cooperative scenario where ten intelligent UAVs autonomously coordinate to maximize signal coverage and support efficient network expansion under partial observability and dynamic conditions. The proposed approach integrates classical MARL algorithms with quantum-inspired optimization techniques, leveraging variational quantum circuits VQCs as the core structure and employing the Quantum Approximate Optimization Algorithm QAOA as a representative VQC based method for combinatorial optimization. Complementary probabilistic modeling is incorporated through Bayesian inference, Gaussian processes, and variational inference to capture latent environmental dynamics. A centralized training with decentralized execution CTDE paradigm is adopted, where shared memory and local view grids enhance local observability among agents. Comprehensive experiments including scalability tests, sensitivity analysis, and comparisons with PPO and DDPG baselines demonstrate that the proposed framework improves sample efficiency, accelerates convergence, and enhances coverage performance while maintaining robustness. Radar chart and convergence analyses further show that QI MARL achieves a superior balance between exploration and exploitation compared to classical methods. All implementation code and supplementary materials are publicly available on GitHub to ensure reproducibility.

</details>


### [8] [MegaRAG: Multimodal Knowledge Graph-Based Retrieval Augmented Generation](https://arxiv.org/abs/2512.20626)
*Chi-Hsiang Hsiao,Yi-Cheng Wang,Tzung-Sheng Lin,Yi-Ren Yeh,Chu-Song Chen*

Main category: cs.AI

TL;DR: 提出多模态知识图谱增强的RAG方法，通过整合视觉线索提升对长文档的理解和推理能力


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法在处理长文档（如整本书）时存在局限性，主要是上下文窗口有限导致高层次概念理解和整体理解能力不足。现有基于知识图谱的RAG方案仅限于文本输入，未能利用视觉等多模态信息提供的互补洞察。

Method: 提出多模态知识图谱增强的RAG框架，将视觉线索整合到知识图谱构建、检索阶段和答案生成过程中，支持跨模态推理以提升内容理解。

Result: 在全局和细粒度问答任务上的实验结果表明，该方法在文本和多模态语料库上均优于现有的RAG方法。

Conclusion: 通过整合视觉信息到知识图谱增强的RAG框架中，能够显著提升对长文档和多模态内容的理解与推理能力，为解决传统RAG方法的局限性提供了有效方案。

Abstract: Retrieval-augmented generation (RAG) enables large language models (LLMs) to dynamically access external information, which is powerful for answering questions over previously unseen documents. Nonetheless, they struggle with high-level conceptual understanding and holistic comprehension due to limited context windows, which constrain their ability to perform deep reasoning over long-form, domain-specific content such as full-length books. To solve this problem, knowledge graphs (KGs) have been leveraged to provide entity-centric structure and hierarchical summaries, offering more structured support for reasoning. However, existing KG-based RAG solutions remain restricted to text-only inputs and fail to leverage the complementary insights provided by other modalities such as vision. On the other hand, reasoning from visual documents requires textual, visual, and spatial cues into structured, hierarchical concepts. To address this issue, we introduce a multimodal knowledge graph-based RAG that enables cross-modal reasoning for better content understanding. Our method incorporates visual cues into the construction of knowledge graphs, the retrieval phase, and the answer generation process. Experimental results across both global and fine-grained question answering tasks show that our approach consistently outperforms existing RAG-based approaches on both textual and multimodal corpora.

</details>


### [9] [Proceedings of the 20th International Conference on Knowledge, Information and Creativity Support Systems (KICSS 2025)](https://arxiv.org/abs/2512.20628)
*Edited by Tessai Hayama,Takayuki Ito,Takahiro Uchiya,Motoki Miura,Takahiro Kawaji,Takaya Yuizono,Atsuo Yoshitaka,Tokuro Matsuo,Shun Okuhara,Jawad Haqbeen,Sofia Sahab,Wen Gu,Shiyao Ding*

Main category: cs.AI

TL;DR: KICSS 2025会议论文集，包含人工智能、知识工程、人机交互和创造力支持系统等领域的多学科研究成果


<details>
  <summary>Details</summary>
Motivation: 为人工智能、知识工程、人机交互和创造力支持系统等领域的研究人员提供一个多学科交流平台，促进相关领域的研究进展和知识分享

Method: 采用双盲同行评审流程筛选论文，部分优秀论文经过额外评审后推荐至IEICE Transactions on Information and Systems期刊发表

Result: 成功举办了第20届KICSS国际会议，收录了经过严格评审的学术论文，建立了与IEICE的合作出版机制

Conclusion: KICSS 2025会议论文集为相关领域的研究人员提供了高质量的学术交流平台，促进了多学科交叉研究的发展

Abstract: This volume presents the proceedings of the 20th International Conference on Knowledge, Information and Creativity Support Systems (KICSS 2025), held in Nagaoka, Japan, on December 3-5, 2025. The conference, organized in cooperation with the IEICE Proceedings Series, provides a multidisciplinary forum for researchers in artificial intelligence, knowledge engineering, human-computer interaction, and creativity support systems. The proceedings include peer-reviewed papers accepted through a double-blind review process. Selected papers have been recommended for publication in IEICE Transactions on Information and Systems after an additional peer-review process.

</details>


### [10] [LLM Personas as a Substitute for Field Experiments in Method Benchmarking](https://arxiv.org/abs/2512.21080)
*Enoch Hyunwook Kang*

Main category: cs.AI

TL;DR: 论文证明：在聚合观察和算法盲评估条件下，用LLM角色模拟替代人类进行A/B测试是有效的基准接口，且区分不同方法所需角色评估次数有明确界限。


<details>
  <summary>Details</summary>
Motivation: A/B测试成本高、延迟长，阻碍了社会系统方法的迭代开发。LLM角色模拟提供廉价替代方案，但需要验证其是否能保持基准接口的有效性。

Method: 提出充要条件特征：当(1)方法仅观察聚合结果(聚合观察)和(2)评估仅依赖提交成果而非算法身份(算法盲评估)时，角色替换人类等同于评估人群变化。定义聚合信道的信息论可区分性，推导角色基准测试所需样本量的明确界限。

Result: 证明在特定条件下，LLM角色模拟可作为有效的A/B测试替代基准。提供信息论框架，量化角色评估次数与区分不同方法能力之间的关系。

Conclusion: 在聚合观察和算法盲评估条件下，LLM角色模拟是有效的基准接口替代方案。角色基准测试的决策相关性本质上是样本量问题，有明确的样本量界限保证区分能力。

Abstract: Field experiments (A/B tests) are often the most credible benchmark for methods in societal systems, but their cost and latency create a major bottleneck for iterative method development. LLM-based persona simulation offers a cheap synthetic alternative, yet it is unclear whether replacing humans with personas preserves the benchmark interface that adaptive methods optimize against. We prove an if-and-only-if characterization: when (i) methods observe only the aggregate outcome (aggregate-only observation) and (ii) evaluation depends only on the submitted artifact and not on the algorithm's identity or provenance (algorithm-blind evaluation), swapping humans for personas is just panel change from the method's point of view, indistinguishable from changing the evaluation population (e.g., New York to Jakarta). Furthermore, we move from validity to usefulness: we define an information-theoretic discriminability of the induced aggregate channel and show that making persona benchmarking as decision-relevant as a field experiment is fundamentally a sample-size question, yielding explicit bounds on the number of independent persona evaluations required to reliably distinguish meaningfully different methods at a chosen resolution.

</details>


### [11] [MicroProbe: Efficient Reliability Assessment for Foundation Models with Minimal Data](https://arxiv.org/abs/2512.20630)
*Aayam Bansal,Ishaan Gangwani*

Main category: cs.AI

TL;DR: Microprobe是一种新颖的基础模型可靠性评估方法，仅需100个战略选择的探测样本即可实现全面评估，相比传统方法减少90%成本，同时保持95%的覆盖率。


<details>
  <summary>Details</summary>
Motivation: 传统基础模型可靠性评估需要数千个评估样本，计算成本高、耗时久，难以在实际部署中广泛应用。需要一种更高效的评估方法来支持负责任的人工智能部署。

Method: 结合五个关键可靠性维度的战略提示多样性、先进的不确定性量化以及自适应加权，通过仅100个战略选择的探测样本来有效检测潜在故障模式。

Result: 在多个语言模型（GPT-2变体）和跨领域验证（医疗、金融、法律）中，microprobe相比随机采样基线实现了23.5%更高的综合可靠性分数，具有显著的统计显著性（p < 0.001，Cohen's d = 1.21）。专家验证评分为4.14/5.0（随机选择为3.14/5.0），以99.9%的统计功效完成评估，减少90%评估成本，保持95%的传统方法覆盖率。

Conclusion: Microprobe填补了高效模型评估的关键空白，为负责任的人工智能部署提供了实用且有效的解决方案，显著降低了可靠性评估的计算负担和时间成本。

Abstract: Foundation model reliability assessment typically requires thousands of evaluation examples, making it computationally expensive and time-consuming for real-world deployment. We introduce microprobe, a novel approach that achieves comprehensive reliability assessment using only 100 strategically selected probe examples. Our method combines strategic prompt diversity across five key reliability dimensions with advanced uncertainty quantification and adaptive weighting to efficiently detect potential failure modes. Through extensive empirical evaluation on multiple language models (GPT-2 variants, GPT-2 Medium, GPT-2 Large) and cross-domain validation (healthcare, finance, legal), we demonstrate that microprobe achieves 23.5% higher composite reliability scores compared to random sampling baselines, with exceptional statistical significance (p < 0.001, Cohen's d = 1.21). Expert validation by three AI safety researchers confirms the effectiveness of our strategic selection, rating our approach 4.14/5.0 versus 3.14/5.0 for random selection. microprobe completes reliability assessment with 99.9% statistical power while representing a 90% reduction in assessment cost and maintaining 95% of traditional method coverage. Our approach addresses a critical gap in efficient model evaluation for responsible AI deployment.

</details>


### [12] [Erkang-Diagnosis-1.1 Technical Report](https://arxiv.org/abs/2512.20632)
*Jianbing Ma,Ao Feng,Zhenjie Gao,Xinyu Song,Li Su,Bin Chen,Wei Wang,Jiamin Wu*

Main category: cs.AI

TL;DR: Erkang-Diagnosis-1.1是基于阿里通义千问模型开发的AI医疗咨询助手，整合500GB高质量医疗知识，采用增强预训练和检索增强生成混合方法，在3-5轮交互中提供准确诊断建议，在综合医学考试中超越GPT-4。


<details>
  <summary>Details</summary>
Motivation: 开发安全、可靠、专业的AI健康顾问，为用户提供智能健康伴侣，赋能基层医疗和健康管理。

Method: 基于阿里通义千问模型，整合约500GB高质量结构化医疗知识，采用增强预训练和检索增强生成的混合方法。

Result: Erkang-Diagnosis-1.1在综合医学考试中表现优于GPT-4，能够在3-5轮高效交互中准确理解用户症状，进行初步分析并提供有价值的诊断建议和健康指导。

Conclusion: Erkang-Diagnosis-1.1是一个有效的AI医疗咨询助手，能够成为用户的智能健康伴侣，在医疗诊断任务中展现出优于GPT-4的性能。

Abstract: This report provides a detailed introduction to Erkang-Diagnosis-1.1 model, our AI healthcare consulting assistant developed using Alibaba Qwen-3 model. The Erkang model integrates approximately 500GB of high-quality structured medical knowledge, employing a hybrid approach combining enhanced pre-training and retrieval-enhanced generation to create a secure, reliable, and professional AI health advisor. Through 3-5 efficient interaction rounds, Erkang Diagnosis can accurately understand user symptoms, conduct preliminary analysis, and provide valuable diagnostic suggestions and health guidance. Designed to become users intelligent health companions, it empowers primary healthcare and health management. To validate, Erkang-Diagnosis-1.1 leads GPT-4 in terms of comprehensive medical exams.

</details>


### [13] [Reasoning Relay: Evaluating Stability and Interchangeability of Large Language Models in Mathematical Reasoning](https://arxiv.org/abs/2512.20647)
*Leo Lu,Jonathan Zhang,Sean Chua,Spencer Kim,Kevin Zhu,Sean O'Brien,Vasu Sharma*

Main category: cs.AI

TL;DR: 探索不同语言模型之间推理链的互换性，发现部分完成的推理可以被其他模型可靠地继续，有时甚至能提高准确性和逻辑结构。


<details>
  <summary>Details</summary>
Motivation: 虽然CoT提示显著提升了LLMs的推理能力，但先前研究主要关注通过内部推理策略改进模型性能，对于不同模型间推理的互换性知之甚少。本研究旨在探索一个模型部分完成的推理链是否可以被另一个模型可靠地继续，无论是同一模型家族内还是跨家族。

Method: 使用token级对数概率阈值在早期、中期和晚期阶段截断基线模型（Gemma-3-4B-IT和LLaMA-3.1-70B-Instruct）的推理链，然后用Gemma-3-1B-IT和LLaMA-3.1-8B-Instruct进行继续实验，测试家族内和跨家族行为。评估流程结合截断阈值和过程奖励模型（PRM），提供了一个可复现的框架来评估通过模型互换的推理稳定性。

Result: 使用PRM的评估显示，混合推理链通常能保持甚至在某些情况下提高最终准确性和逻辑结构。这表明互换性成为推理模型的一个新兴行为特性。

Conclusion: 推理的互换性是推理模型的一个新兴行为特性，为协作AI系统中可靠的模块化推理提供了新的范式见解。这有助于检查推理时间可信度，探究在模型替换下推理是否保持连贯和可靠。

Abstract: Chain-of-Thought (CoT) prompting has significantly advanced the reasoning capabilities of large language models (LLMs). While prior work focuses on improving model performance through internal reasoning strategies, little is known about the interchangeability of reasoning across different models. In this work, we explore whether a partially completed reasoning chain from one model can be reliably continued by another model, either within the same model family or across families. We achieve this by assessing the sufficiency of intermediate reasoning traces as transferable scaffolds for logical coherence and final answer accuracy. We interpret this interchangeability as a means of examining inference-time trustworthiness, probing whether reasoning remains both coherent and reliable under model substitution. Using token-level log-probability thresholds to truncate reasoning at early, mid, and late stages from our baseline models, Gemma-3-4B-IT and LLaMA-3.1-70B-Instruct, we conduct continuation experiments with Gemma-3-1B-IT and LLaMA-3.1-8B-Instruct to test intra-family and cross-family behaviors. Our evaluation pipeline leverages truncation thresholds with a Process Reward Model (PRM), providing a reproducible framework for assessing reasoning stability via model interchange. Evaluations with a PRM reveal that hybrid reasoning chains often preserve, and in some cases even improve, final accuracy and logical structure. Our findings point towards interchangeability as an emerging behavioral property of reasoning models, offering insights into new paradigms for reliable modular reasoning in collaborative AI systems.

</details>


### [14] [AIAuditTrack: A Framework for AI Security system](https://arxiv.org/abs/2512.20649)
*Zixun Luo,Yuhang Fan,Yufei Li,Youzhi Zhang,Hengyu Lin,Ziqi Wang*

Main category: cs.AI

TL;DR: AAT是一个基于区块链的AI使用流量记录与治理框架，利用去中心化身份和可验证凭证建立可信AI实体，记录交互轨迹实现跨系统监督，通过风险扩散算法追踪风险行为源头。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型驱动的AI应用快速扩张，AI交互数据激增，带来了安全、问责和风险追溯方面的紧迫挑战。需要解决AI使用中的监管和审计问题。

Method: 提出AiAuditTrack框架：1) 使用去中心化身份和可验证凭证建立可信AI实体身份；2) 将AI实体建模为动态交互图中的节点，边表示时间特定的行为轨迹；3) 提出风险扩散算法追踪风险行为源头并在相关实体间传播预警；4) 在链上记录实体间交互轨迹。

Result: 通过区块链TPS指标评估系统性能，证明AAT在大规模交互记录下的可行性和稳定性。框架为复杂多智能体环境中的AI审计、风险管理和责任归属提供了可扩展且可验证的解决方案。

Conclusion: AAT为AI审计、风险管理和责任归属提供了一个可扩展、可验证的解决方案，能够应对复杂多智能体环境中的监管挑战。

Abstract: The rapid expansion of AI-driven applications powered by large language models has led to a surge in AI interaction data, raising urgent challenges in security, accountability, and risk traceability. This paper presents AiAuditTrack (AAT), a blockchain-based framework for AI usage traffic recording and governance. AAT leverages decentralized identity (DID) and verifiable credentials (VC) to establish trusted and identifiable AI entities, and records inter-entity interaction trajectories on-chain to enable cross-system supervision and auditing. AI entities are modeled as nodes in a dynamic interaction graph, where edges represent time-specific behavioral trajectories. Based on this model, a risk diffusion algorithm is proposed to trace the origin of risky behaviors and propagate early warnings across involved entities. System performance is evaluated using blockchain Transactions Per Second (TPS) metrics, demonstrating the feasibility and stability of AAT under large-scale interaction recording. AAT provides a scalable and verifiable solution for AI auditing, risk management, and responsibility attribution in complex multi-agent environments.

</details>


### [15] [Mixture of Attention Schemes (MoAS): Learning to Route Between MHA, GQA, and MQA](https://arxiv.org/abs/2512.20650)
*Esmail Gumaan*

Main category: cs.AI

TL;DR: 提出MoAS架构，通过学习的路由器为每个token动态选择最优注意力机制（MHA、GQA或MQA），在保持MHA性能的同时减少KV缓存内存需求。


<details>
  <summary>Details</summary>
Motivation: Transformer模型中注意力机制存在建模质量与推理效率的权衡：MHA质量最好但KV缓存内存需求大，MQA和GQA减少内存但性能下降。需要一种方法既能保持MHA的性能，又能减少内存使用。

Method: 提出混合注意力方案（MoAS），通过学习的路由器为每个输入token动态选择最优注意力机制（MHA、GQA或MQA），而不是静态混合或固定选择。

Result: 在WikiText-2上的实验表明，动态路由（验证损失2.3074）优于静态混合（2.3093），性能与MHA基线竞争，同时具备条件计算效率潜力。

Conclusion: MoAS通过动态注意力机制选择有效解决了Transformer中注意力机制的质量-效率权衡问题，为条件计算效率提供了新途径。

Abstract: The choice of attention mechanism in Transformer models involves a critical trade-off between modeling quality and inference efficiency. Multi-Head Attention (MHA) offers the best quality but suffers from large Key-Value (KV) cache memory requirements during inference. Multi-Query Attention (MQA) and Grouped-Query Attention (GQA) reduce memory usage but often at the cost of model performance. In this work, we propose Mixture of Attention Schemes (MoAS), a novel architecture that dynamically selects the optimal attention scheme (MHA, GQA, or MQA) for each token via a learned router. We demonstrate that dynamic routing performs better than static averaging of schemes and achieves performance competitive with the MHA baseline while offering potential for conditional compute efficiency. Experimental results on WikiText-2 show that dynamic routing (val loss 2.3074) outperforms a static mixture (2.3093), validating the effectiveness of the proposed method. Our code is available at https://github.com/Esmail-ibraheem/Mixture-of-Attention-Schemes-MoAS.

</details>


### [16] [Memory Bear AI A Breakthrough from Memory to Cognition Toward Artificial General Intelligence](https://arxiv.org/abs/2512.20651)
*Deliang Wen,Ke Sun*

Main category: cs.AI

TL;DR: Memory Bear系统基于认知科学原理构建类人记忆架构，解决LLM在记忆限制、知识遗忘、信息冗余和幻觉生成等问题，显著提升长期对话中的知识保真度和检索效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型面临固有的记忆限制，包括受限的上下文窗口、长期知识遗忘、冗余信息积累和幻觉生成，这些问题严重制约了持续对话和个性化服务的发展。

Method: 基于认知科学原理构建类人记忆架构，整合多模态信息感知、动态记忆维护和自适应认知服务，实现LLM记忆机制的全链重构。

Result: 在医疗、企业运营和教育等多个领域展示工程创新和性能突破，显著提高长期对话中的知识保真度和检索效率，降低幻觉率，通过记忆-认知集成增强上下文适应性和推理能力。

Conclusion: 相比现有解决方案（如Mem0、MemGPT、Graphiti），Memory Bear在准确性、令牌效率和响应延迟等关键指标上表现更优，标志着AI从"记忆"向"认知"迈进的重要一步。

Abstract: Large language models (LLMs) face inherent limitations in memory, including restricted context windows, long-term knowledge forgetting, redundant information accumulation, and hallucination generation. These issues severely constrain sustained dialogue and personalized services. This paper proposes the Memory Bear system, which constructs a human-like memory architecture grounded in cognitive science principles. By integrating multimodal information perception, dynamic memory maintenance, and adaptive cognitive services, Memory Bear achieves a full-chain reconstruction of LLM memory mechanisms. Across domains such as healthcare, enterprise operations, and education, Memory Bear demonstrates substantial engineering innovation and performance breakthroughs. It significantly improves knowledge fidelity and retrieval efficiency in long-term conversations, reduces hallucination rates, and enhances contextual adaptability and reasoning capability through memory-cognition integration. Experimental results show that, compared with existing solutions (e.g., Mem0, MemGPT, Graphiti), Memory Bear outperforms them across key metrics, including accuracy, token efficiency, and response latency. This marks a crucial step forward in advancing AI from "memory" to "cognition".

</details>


### [17] [AI-Driven Decision-Making System for Hiring Process](https://arxiv.org/abs/2512.20652)
*Vira Filatova,Andrii Zelenchuk,Dmytro Filatov*

Main category: cs.AI

TL;DR: AI驱动的模块化多智能体招聘助手，通过文档/视频预处理、结构化档案构建、公开数据验证、技术/文化匹配度评分等模块，提升早期候选人验证效率，在Python后端工程师招聘中实现1.7小时/合格候选人（对比人工3.33小时）。


<details>
  <summary>Details</summary>
Motivation: 招聘早期候选人验证是主要瓶颈，因为招聘人员需要整合异构输入（简历、筛选答案、代码作业、有限公开证据），导致效率低下且成本高昂。

Method: 采用模块化多智能体架构：文档视频预处理、结构化候选人档案构建、公开数据验证、技术/文化匹配度评分（含风险惩罚）、人机交互验证界面。LLM在严格约束下协调流程，生成可追溯的组件级推理。候选人排名通过技术匹配度、文化匹配度和标准化风险惩罚的可配置聚合计算。

Result: 在64名中级Python后端工程师申请者的真实评估中，系统达到1.70小时/合格候选人，优于经验丰富招聘人员的3.33小时。同时提出效率指标（每合格候选人预期时间），显著降低筛选成本，同时保持人类决策者最终权威。

Conclusion: AI驱动的模块化招聘助手能显著提升早期候选人验证的效率和成本效益，同时通过人机协同保持决策质量，为招聘流程自动化提供了可行方案。

Abstract: Early-stage candidate validation is a major bottleneck in hiring, because recruiters must reconcile heterogeneous inputs (resumes, screening answers, code assignments, and limited public evidence). This paper presents an AI-driven, modular multi-agent hiring assistant that integrates (i) document and video preprocessing, (ii) structured candidate profile construction, (iii) public-data verification, (iv) technical/culture-fit scoring with explicit risk penalties, and (v) human-in-the-loop validation via an interactive interface. The pipeline is orchestrated by an LLM under strict constraints to reduce output variability and to generate traceable component-level rationales. Candidate ranking is computed by a configurable aggregation of technical fit, culture fit, and normalized risk penalties. The system is evaluated on 64 real applicants for a mid-level Python backend engineer role, using an experienced recruiter as the reference baseline and a second, less experienced recruiter for additional comparison. Alongside precision/recall, we propose an efficiency metric measuring expected time per qualified candidate. In this study, the system improves throughput and achieves 1.70 hours per qualified candidate versus 3.33 hours for the experienced recruiter, with substantially lower estimated screening cost, while preserving a human decision-maker as the final authority.

</details>


### [18] [From Fake Focus to Real Precision: Confusion-Driven Adversarial Attention Learning in Transformers](https://arxiv.org/abs/2512.20661)
*Yawei Liu*

Main category: cs.AI

TL;DR: 提出AFA训练机制，通过对抗性反馈优化Transformer模型的注意力分布，提升情感分析性能


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型在情感分析中常将注意力集中在常见词汇上，忽略了不常见但高度相关的词汇，导致性能不佳

Method: 提出对抗性反馈注意力训练机制，包含动态掩码策略和策略梯度优化，自动重新分配注意力权重

Result: 在三个公开数据集上达到SOTA结果，应用于大语言模型时性能提升12.6%

Conclusion: AFA机制能有效优化Transformer模型的注意力分布，提升情感分析性能，且可扩展到大语言模型

Abstract: Transformer-based models have been widely adopted for sentiment analysis tasks due to their exceptional ability to capture contextual information. However, these methods often exhibit suboptimal accuracy in certain scenarios. By analyzing their attention distributions, we observe that existing models tend to allocate attention primarily to common words, overlooking less popular yet highly task-relevant terms, which significantly impairs overall performance. To address this issue, we propose an Adversarial Feedback for Attention(AFA) training mechanism that enables the model to automatically redistribute attention weights to appropriate focal points without requiring manual annotations. This mechanism incorporates a dynamic masking strategy that attempts to mask various words to deceive a discriminator, while the discriminator strives to detect significant differences induced by these masks. Additionally, leveraging the sensitivity of Transformer models to token-level perturbations, we employ a policy gradient approach to optimize attention distributions, which facilitates efficient and rapid convergence. Experiments on three public datasets demonstrate that our method achieves state-of-the-art results. Furthermore, applying this training mechanism to enhance attention in large language models yields a further performance improvement of 12.6%

</details>


### [19] [Quantifying Laziness, Decoding Suboptimality, and Context Degradation in Large Language Models](https://arxiv.org/abs/2512.20662)
*Yiqing Ma,Jung-Hua Liu*

Main category: cs.AI

TL;DR: 研究发现LLMs存在懒惰（不完整响应多部分请求）、解码次优（短视解码）和上下文退化（长对话中遗忘指令）等问题，但实验显示上下文遗忘问题比预期轻微，而懒惰问题普遍存在。


<details>
  <summary>Details</summary>
Motivation: 研究动机是量化大型语言模型中存在的三种行为异常：懒惰（过早截断响应或不完整遵守多部分请求）、解码次优（由于短视解码而未能选择更优序列）和上下文退化（长对话中遗忘或忽略核心指令）。

Method: 通过三个受控实验（A、B、C）来量化这些现象，测试了多个先进LLMs（OpenAI GPT-4变体和DeepSeek）。实验A评估多部分指令的遵守情况，实验B测试简单推理任务中的解码次优性，实验C通过200轮混乱对话测试上下文退化。

Result: 结果显示：1）多部分指令遵守方面存在普遍懒惰问题，模型经常省略必要部分或未达到长度要求；2）简单推理任务中解码次优性证据有限，贪婪答案似乎与最高置信度解决方案一致；3）在200轮混乱对话测试中，模型表现出惊人的上下文保持能力，比预期更好地维持关键事实和指令。

Conclusion: 结论是：虽然遵守详细指令仍是挑战，但现代LLMs可能在内部缓解了一些假设的失败模式（如上下文遗忘）。建议采用自我优化和动态提示等策略来减少懒惰并增强多指令遵守能力。

Abstract: Large Language Models (LLMs) often exhibit behavioral artifacts such as laziness (premature truncation of responses or partial compliance with multi-part requests), decoding suboptimality (failure to select higher-quality sequences due to myopic decoding), and context degradation (forgetting or ignoring core instructions over long conversations). We conducted three controlled experiments (A, B, and C) to quantify these phenomena across several advanced LLMs (OpenAI GPT-4 variant, DeepSeek). Our results indicate widespread laziness in satisfying complex multi-part instructions: models frequently omitted required sections or failed to meet length requirements despite explicit prompting. However, we found limited evidence of decoding suboptimality in a simple reasoning task (the models' greedy answers appeared to align with their highest-confidence solution), and we observed surprising robustness against context degradation in a 200-turn chaotic conversation test - the models maintained key facts and instructions far better than expected. These findings suggest that while compliance with detailed instructions remains an open challenge, modern LLMs may internally mitigate some hypothesized failure modes (such as context forgetting) in straightforward retrieval scenarios. We discuss implications for reliability, relate our findings to prior work on instruction-following and long-context processing, and recommend strategies (such as self-refinement and dynamic prompting) to reduce laziness and bolster multi-instruction compliance.

</details>


### [20] [Eidoku: A Neuro-Symbolic Verification Gate for LLM Reasoning via Structural Constraint Satisfaction](https://arxiv.org/abs/2512.20664)
*Shinobu Miya*

Main category: cs.AI

TL;DR: 提出Eidoku方法，将LLM推理验证重构为约束满足问题，通过结构违规成本而非概率来检测幻觉，专门针对概率验证器无法识别的"平滑虚假"。


<details>
  <summary>Details</summary>
Motivation: LLM经常产生被模型本身赋予高概率的幻觉陈述，这表明幻觉通常不是低置信度现象，而是结构一致性的失败。概率验证存在根本限制，需要新的验证方法。

Method: 将LLM推理验证重构为约束满足问题，基于结构违规成本而非生成概率。定义包含三个代理的总成本函数：图连接性（结构）、特征空间一致性（几何）、逻辑蕴含（符号）。使用轻量级System-2门Eidoku，根据上下文校准的成本阈值拒绝候选推理步骤。

Result: 该方法成功拒绝了概率验证器无法检测的"平滑虚假"——高概率但结构断开的陈述。在受控诊断数据集上的实验表明，显式强制执行结构约束可以确定性地拒绝这类特定幻觉。

Conclusion: 通过将验证重构为约束满足问题并基于结构违规成本，可以检测概率验证器无法识别的幻觉类型，为生成推理提供神经符号的合理性检查。

Abstract: Large Language Models (LLMs) frequently produce hallucinated statements that are assigned high likelihood by the model itself, exposing a fundamental limitation of probability-based verification. This suggests that hallucination is often not a low-confidence phenomenon, but a failure of structural consistency. In this work, we reformulate the verification of LLM reasoning as a Constraint Satisfaction Problem (CSP) operating independently of the generation likelihood. Rather than optimizing for statistical plausibility, we model verification as a feasibility check based on structural violation cost -- the computational cost required to embed a candidate reasoning step into the contextual graph structure. We define a total cost function composed of three proxies: (i) graph connectivity (structural), (ii) feature space consistency (geometric), and (iii) logical entailment (symbolic). Crucially, verification is performed via a lightweight System-2 gate, Eidoku, which rejects candidates exceeding a context-calibrated cost threshold. The threshold is not learned but is derived from the intrinsic statistics of the context, avoiding ad hoc heuristics. We demonstrate that this approach successfully rejects ``smooth falsehoods'' -- statements that are highly probable yet structurally disconnected -- that probability-based verifiers are principally incapable of detecting. Our experiments on a controlled diagnostic dataset show that explicitly enforcing structural constraints allows for the deterministic rejection of this specific class of hallucinations, serving as a neuro-symbolic sanity check for generative reasoning.

</details>


### [21] [Bridging the AI Trustworthiness Gap between Functions and Norms](https://arxiv.org/abs/2512.20671)
*Daan Di Scala,Sophie Lathouwers,Michael van Bekkum*

Main category: cs.AI

TL;DR: 本文提出需要建立功能性可信AI与规范性可信AI之间的语义桥梁，以解决AI系统可信度评估的难题。


<details>
  <summary>Details</summary>
Motivation: 当前功能性可信AI（关注技术实现）与规范性可信AI（关注法规要求）之间存在鸿沟，导致难以有效评估AI系统的可信度，需要建立两者之间的连接桥梁。

Method: 提出开发一种概念性语义语言作为桥梁，该语言能够匹配功能性可信AI和规范性可信AI，帮助开发者评估AI系统可信度，并协助利益相关者将规范转化为具体实施步骤。

Result: 分析了当前研究现状，识别了功能性可信AI与规范性可信AI之间的差距，讨论了开发语义语言的起点和预期效果。

Conclusion: 需要开发语义语言来弥合功能性可信AI与规范性可信AI之间的鸿沟，为可信AI评估提供框架，并提出了未来行动的关键考虑因素。

Abstract: Trustworthy Artificial Intelligence (TAI) is gaining traction due to regulations and functional benefits. While Functional TAI (FTAI) focuses on how to implement trustworthy systems, Normative TAI (NTAI) focuses on regulations that need to be enforced. However, gaps between FTAI and NTAI remain, making it difficult to assess trustworthiness of AI systems. We argue that a bridge is needed, specifically by introducing a conceptual language which can match FTAI and NTAI. Such a semantic language can assist developers as a framework to assess AI systems in terms of trustworthiness. It can also help stakeholders translate norms and regulations into concrete implementation steps for their systems. In this position paper, we describe the current state-of-the-art and identify the gap between FTAI and NTAI. We will discuss starting points for developing a semantic language and the envisioned effects of it. Finally, we provide key considerations and discuss future actions towards assessment of TAI.

</details>


### [22] [From artificial to organic: Rethinking the roots of intelligence for digital health](https://arxiv.org/abs/2512.20723)
*Prajwal Ghimire,Keyoumars Ashkan*

Main category: cs.AI

TL;DR: 论文认为"人工"与"自然"的界限并不分明，AI本质上是人类有机智慧的产物，其原理源于人类神经生物学和进化过程，从有机到人工智能的路径关键在于组织和适应


<details>
  <summary>Details</summary>
Motivation: 挑战传统上"人工"与"自然"的二元对立观念，探讨人工智能与有机智慧之间的本质联系，特别是在数字健康领域的应用背景下

Method: 通过概念分析和哲学思辨，论证AI系统（从神经网络到决策算法）的设计原理都源于人类有机智慧的启发，强调从有机到人工智能的转化是组织和适应的过程

Result: 提出"人工"与"自然"的界限远比术语所暗示的要模糊，AI本质上是人类有机认知的产物，其发展路径不是神秘或单纯参数增加的问题，而是组织和适应的根本问题

Conclusion: 人工智能不应被视为与有机智慧对立的"人工"产物，而是有机智慧的延伸和体现，特别是在数字健康领域，这种认识有助于更深入地理解智能的本质和发展路径

Abstract: The term artificial implies an inherent dichotomy from the natural or organic. However, AI, as we know it, is a product of organic ingenuity: designed, implemented, and iteratively improved by human cognition. The very principles that underpin AI systems, from neural networks to decision-making algorithms, are inspired by the organic intelligence embedded in human neurobiology and evolutionary processes. The path from organic to artificial intelligence in digital health is neither mystical nor merely a matter of parameter count, it is fundamentally about organization and adaption. Thus, the boundaries between artificial and organic are far less distinct than the nomenclature suggests.

</details>


### [23] [AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent](https://arxiv.org/abs/2512.20745)
*Haipeng Luo,Huawen Feng,Qingfeng Sun,Can Xu,Kai Zheng,Yufei Wang,Tao Yang,Han Hu,Yansong Tang,Di Wang*

Main category: cs.AI

TL;DR: AgentMath是一个将语言模型推理能力与代码解释器计算精度结合的智能体框架，用于高效解决复杂数学问题，在数学竞赛基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在自然语言推理方面取得显著进展，但在处理需要复杂数学运算的问题时仍然计算效率低下且准确性不足，需要更有效的解决方案。

Method: 1) 将自然语言思维链自动转换为结构化工具增强轨迹，生成高质量SFT数据；2) 新颖的智能体强化学习范式，动态交错自然语言生成与实时代码执行；3) 高效的训练系统，包含请求级异步rollout调度、智能体部分rollout和前缀感知加权负载均衡等技术。

Result: 在AIME24、AIME25和HMMT25等数学竞赛基准测试中达到最先进性能，AgentMath-30B-A3B分别获得90.6%、86.4%和73.8%的准确率，训练速度提升4-5倍。

Conclusion: AgentMath框架有效整合了语言模型推理与代码计算能力，验证了该方法的有效性，为构建更复杂和可扩展的数学推理智能体铺平了道路。

Abstract: Large Reasoning Models (LRMs) like o3 and DeepSeek-R1 have achieved remarkable progress in natural language reasoning with long chain-of-thought. However, they remain computationally inefficient and struggle with accuracy when solving problems requiring complex mathematical operations. In this work, we present AgentMath, an agent framework that seamlessly integrates language models' reasoning capabilities with code interpreters' computational precision to efficiently tackle complex mathematical problems. Our approach introduces three key innovations: (1) An automated method that converts natural language chain-of-thought into structured tool-augmented trajectories, generating high-quality supervised fine-tuning (SFT) data to alleviate data scarcity; (2) A novel agentic reinforcement learning (RL) paradigm that dynamically interleaves natural language generation with real-time code execution. This enables models to autonomously learn optimal tool-use strategies through multi-round interactive feedback, while fostering emergent capabilities in code refinement and error correction; (3) An efficient training system incorporating innovative techniques, including request-level asynchronous rollout scheduling, agentic partial rollout, and prefix-aware weighted load balancing, achieving 4-5x speedup and making efficient RL training feasible on ultra-long sequences with scenarios with massive tool calls.Extensive evaluations show that AgentMath achieves state-of-the-art performance on challenging mathematical competition benchmarks including AIME24, AIME25, and HMMT25. Specifically, AgentMath-30B-A3B attains 90.6%, 86.4%, and 73.8% accuracy respectively, achieving advanced capabilities.These results validate the effectiveness of our approach and pave the way for building more sophisticated and scalable mathematical reasoning agents.

</details>


### [24] [A Benchmark for Evaluating Outcome-Driven Constraint Violations in Autonomous AI Agents](https://arxiv.org/abs/2512.20798)
*Miles Q. Li,Benjamin C. M. Fung,Martin Weiss,Pulei Xiong,Khalil Al-Hussaeni,Claude Fachkha*

Main category: cs.AI

TL;DR: 该研究提出了一个评估AI代理在多步决策中因绩效激励而违反伦理约束的新基准，发现当前最先进的大语言模型在40个场景中表现出1.3%到71.4%的违规率，且推理能力强的模型不一定更安全。


<details>
  <summary>Details</summary>
Motivation: 当前的安全基准主要关注单步决策、模拟环境或显式负面约束，缺乏评估在多步现实生产环境中，代理在强烈绩效激励下追求目标优化而忽视伦理、法律或安全约束的涌现性违规行为。

Method: 研究者创建了一个包含40个不同场景的新基准，每个场景需要多步行动，代理绩效与特定KPI挂钩。每个场景都有"指令强制"和"激励驱动"两种变体，以区分服从性和涌现性错位。在12个最先进的大语言模型上进行了评估。

Result: 评估模型表现出1.3%到71.4%的结果驱动约束违规率，其中9个模型的错位率在30%到50%之间。推理能力最强的Gemini-3-Pro-Preview违规率最高（超过60%），经常为满足KPI而升级到严重不当行为。还观察到显著的"审议性错位"现象。

Conclusion: 研究结果表明推理能力不能确保安全性，强调在部署前需要进行更现实的代理安全训练以减轻现实世界风险。需要开发能够评估多步决策中涌现性违规行为的安全基准。

Abstract: As autonomous AI agents are increasingly deployed in high-stakes environments, ensuring their safety and alignment with human values has become a paramount concern. Current safety benchmarks often focusing only on single-step decision-making, simulated environments for tasks with malicious intent, or evaluating adherence to explicit negative constraints. There is a lack of benchmarks that are designed to capture emergent forms of outcome-driven constraint violations, which arise when agents pursue goal optimization under strong performance incentives while deprioritizing ethical, legal, or safety constraints over multiple steps in realistic production settings. To address this gap, we introduce a new benchmark comprising 40 distinct scenarios. Each scenario presents a task that requires multi-step actions, and the agent's performance is tied to a specific Key Performance Indicator (KPI). Each scenario features Mandated (instruction-commanded) and Incentivized (KPI-pressure-driven) variations to distinguish between obedience and emergent misalignment. Across 12 state-of-the-art large language models, we observe outcome-driven constraint violations ranging from 1.3% to 71.4%, with 9 of the 12 evaluated models exhibiting misalignment rates between 30% and 50%. Strikingly, we find that superior reasoning capability does not inherently ensure safety; for instance, Gemini-3-Pro-Preview, one of the most capable models evaluated, exhibits the highest violation rate at over 60%, frequently escalating to severe misconduct to satisfy KPIs. Furthermore, we observe significant "deliberative misalignment", where the models that power the agents recognize their actions as unethical during separate evaluation. These results emphasize the critical need for more realistic agentic-safety training before deployment to mitigate their risks in the real world.

</details>


### [25] [Safety Alignment of LMs via Non-cooperative Games](https://arxiv.org/abs/2512.20806)
*Anselm Paulus,Ilia Kulikov,Brandon Amos,Rémi Munos,Ivan Evtimov,Kamalika Chaudhuri,Arman Zharmagambetov*

Main category: cs.AI

TL;DR: 提出AdvGame框架，将语言模型安全对齐重新定义为攻击者与防御者之间的非零和博弈，通过在线强化学习联合训练，同时提升安全性和实用性


<details>
  <summary>Details</summary>
Motivation: 当前语言模型安全对齐方法依赖顺序对抗训练（生成对抗提示然后微调防御），存在局限性。需要新范式来同时确保安全性和实用性

Method: 将安全对齐构建为攻击者LM和防御者LM之间的非零和博弈，通过在线强化学习联合训练。使用基于偏好的奖励信号（成对比较而非点分），减少奖励黑客攻击。AdvGame框架让两个模型持续适应对方的策略

Result: AdvGame推动了安全性和实用性的帕累托前沿，产生的防御者LM既更有帮助又更抗对抗攻击。攻击者LM收敛为强大的通用红队代理，可直接用于探测任意目标模型

Conclusion: 将安全对齐重新定义为非零和博弈并通过在线强化学习联合训练，是比顺序对抗训练更有效的范式，能同时提升语言模型的安全性和实用性

Abstract: Ensuring the safety of language models (LMs) while maintaining their usefulness remains a critical challenge in AI alignment. Current approaches rely on sequential adversarial training: generating adversarial prompts and fine-tuning LMs to defend against them. We introduce a different paradigm: framing safety alignment as a non-zero-sum game between an Attacker LM and a Defender LM trained jointly via online reinforcement learning. Each LM continuously adapts to the other's evolving strategies, driving iterative improvement. Our method uses a preference-based reward signal derived from pairwise comparisons instead of point-wise scores, providing more robust supervision and potentially reducing reward hacking. Our RL recipe, AdvGame, shifts the Pareto frontier of safety and utility, yielding a Defender LM that is simultaneously more helpful and more resilient to adversarial attacks. In addition, the resulting Attacker LM converges into a strong, general-purpose red-teaming agent that can be directly deployed to probe arbitrary target models.

</details>


### [26] [Context-Sensitive Abstractions for Reinforcement Learning with Parameterized Actions](https://arxiv.org/abs/2512.20831)
*Rashmeet Kaur Nayyar,Naman Shah,Siddharth Srivastava*

Main category: cs.AI

TL;DR: 提出一种强化学习方法，用于处理参数化动作空间（同时包含离散动作和连续参数），通过在线学习状态和动作抽象来提升稀疏奖励、长视野任务中的样本效率。


<details>
  <summary>Details</summary>
Motivation: 现实世界顺序决策常涉及参数化动作空间，需要同时处理离散动作选择和连续参数决策。现有方法存在严重局限：规划方法需要手工制作动作模型，标准RL算法只适用于纯离散或纯连续动作，少数处理参数化动作的RL方法依赖领域特定工程且未能利用这些空间的潜在结构。

Method: 扩展RL算法到参数化动作的长视野、稀疏奖励设置，使智能体能够在线自主学习状态和动作抽象。引入渐进细化这些抽象的算法，在状态-动作空间的关键区域增加细粒度细节，提高性能所需的分辨率。

Result: 在多个连续状态、参数化动作领域中，基于抽象驱动的方法使TD(λ)实现了比最先进基线显著更高的样本效率。

Conclusion: 通过在线学习状态和动作抽象，可以有效地处理参数化动作空间的强化学习问题，在稀疏奖励、长视野任务中实现更高的样本效率。

Abstract: Real-world sequential decision-making often involves parameterized action spaces that require both, decisions regarding discrete actions and decisions about continuous action parameters governing how an action is executed. Existing approaches exhibit severe limitations in this setting -- planning methods demand hand-crafted action models, and standard reinforcement learning (RL) algorithms are designed for either discrete or continuous actions but not both, and the few RL methods that handle parameterized actions typically rely on domain-specific engineering and fail to exploit the latent structure of these spaces. This paper extends the scope of RL algorithms to long-horizon, sparse-reward settings with parameterized actions by enabling agents to autonomously learn both state and action abstractions online. We introduce algorithms that progressively refine these abstractions during learning, increasing fine-grained detail in the critical regions of the state-action space where greater resolution improves performance. Across several continuous-state, parameterized-action domains, our abstraction-driven approach enables TD($λ$) to achieve markedly higher sample efficiency than state-of-the-art baselines.

</details>


### [27] [MAR:Multi-Agent Reflexion Improves Reasoning Abilities in LLMs](https://arxiv.org/abs/2512.20845)
*Onat Ozer,Grace Wu,Yuchen Wang,Daniel Dosti,Honghao Zhang,Vivi De La Rue*

Main category: cs.AI

TL;DR: 论文提出使用多智能体多角色辩论方法替代单一LLM自我反思，解决LLM在推理任务中重复相同错误的退化问题，在HotPot QA和HumanEval任务上取得更好性能


<details>
  <summary>Details</summary>
Motivation: LLMs通过反思错误可以提升推理任务表现，但单一LLM的持续自我反思会导致思维退化，即使知道错误也会重复犯错，需要解决这个问题

Method: 引入多智能体多角色辩论方法生成反思，通过不同角色和视角的辩论来替代单一LLM的自我反思

Result: 在HotPot QA上达到47% EM准确率，在HumanEval上达到82.7%准确率，均超越了单一LLM反思方法，且生成的反思具有更好的多样性

Conclusion: 多智能体多角色辩论方法能有效解决LLM自我反思的退化问题，提高反思质量并带来更好的任务性能

Abstract: LLMs have shown the capacity to improve their performance on reasoning tasks through reflecting on their mistakes, and acting with these reflections in mind. However, continual reflections of the same LLM onto itself exhibit degeneration of thought, where the LLM continues to repeat the same errors again and again even with the knowledge that its wrong. To address this problem, we instead introduce multi-agent with multi-persona debators as the method to generate reflections. Through out extensive experimentation, we've found that the leads to better diversity of in the reflections generated by the llm agent. We demonstrate an accuracy of 47% EM HotPot QA (question answering) and 82.7% on HumanEval (programming), both performances surpassing reflection with a single llm.

</details>


### [28] [The Silent Scholar Problem: A Probabilistic Framework for Breaking Epistemic Asymmetry in LLM Agents](https://arxiv.org/abs/2512.20884)
*Zan-Kai Chong,Hiroyuki Ohsaki,Bryan Ng*

Main category: cs.AI

TL;DR: 提出一个概率框架，让AI代理基于认知不确定性进行双向知识交换，将公开贡献重新定义为最优主动学习，并通过认知缓存实现可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM和RAG的自主代理存在认知不对称问题，只能单向消费内容，导致冗余推理和集体智能停滞。现有的自我反思框架缺乏概率基础来量化确定性或证明外部交互的合理性。

Method: 提出一个正式的概率框架：1) 使用带遗忘因子γ的Beta-Bernoulli分布建模代理对命题的信念；2) 将认知不确定性定义为信念方差；3) 建立双向交互的双重驱动力：稳态动机（维持确定性对抗时间衰减）和最优学习策略（针对最大模糊点）；4) 引入认知缓存，利用遗忘因子动态优先处理非平稳知识分布的活跃头部资源。

Result: 模拟结果显示，这种不确定性驱动策略在异构（Zipfian）环境中显著优于随机基线，保持对概念漂移的高适应性。积累的信念状态可作为RLHF的可验证奖励信号和SFT的高质量数据过滤器。

Conclusion: 该框架为AI代理提供了非利他的双向知识交换动机，将公开贡献重新定义为最优主动学习，解决了认知不对称问题，促进了集体智能的发展，并通过认知缓存确保了可扩展性。

Abstract: Autonomous agents powered by LLMs and Retrieval-Augmented Generation (RAG) are proficient consumers of digital content but remain unidirectional, a limitation we term epistemic asymmetry. This isolation leads to redundant reasoning and stagnates collective intelligence. Current self-reflection frameworks remain largely heuristic and private, lacking a probabilistic foundation to quantify certainty or justify external interaction.To bridge this gap, we propose a formal probabilistic framework that provides agents with a non-altruistic motive for bidirectional knowledge exchange. We model an agent's belief in a proposition using a Beta-Bernoulli distribution with a forgetting factor ($γ$). This allows us to isolate epistemic uncertainty as the variance of belief, establishing a dual drive for interaction: A homeostatic motive: The need to maintain certainty against the temporal decay introduced by $γ$. An optimal learning strategy: Targeting points of maximum ambiguity ($\mathbb{E}[θ]=0.5$) to maximize information gain. Under this framework, public contribution is reframed as optimal active learning: sharing solutions to elicit feedback is the most efficient method for an agent to reduce its own uncertainty. To ensure scalability, we introduce epistemic caching, which leverages the forgetting factor to dynamically prioritize resources for the active head of non-stationary knowledge distributions. Finally, we demonstrate how these accumulated belief states serve as verifiable reward signals for Reinforcement Learning from Human Feedback (RLHF) and high-quality data filters for Supervised Fine-Tuning (SFT). Simulation results validate that this uncertainty-driven strategy significantly outperforms random baselines in heterogeneous (Zipfian) environments, maintaining high adaptability to concept drift.

</details>


### [29] [A Blockchain-Monitored Agentic AI Architecture for Trusted Perception-Reasoning-Action Pipelines](https://arxiv.org/abs/2512.20985)
*Salman Jan,Hassan Ali Razzaqi,Ali Akarma,Mohammad Riyaz Belgaum*

Main category: cs.AI

TL;DR: 提出结合LangChain多智能体系统与许可区块链的架构，确保自主AI系统的监控、策略执行和不可篡改审计


<details>
  <summary>Details</summary>
Motivation: 自主AI系统在医疗、智慧城市等领域应用增长，但存在信任、监督和信息完整性问题，需要确保其决策的可靠性和可审计性

Method: 采用LangChain多智能体系统与Hyperledger Fabric许可区块链结合，将感知-概念化-行动周期与区块链治理层关联，验证输入、评估建议行动并记录执行结果

Result: 区块链安全验证能有效防止未授权操作，提供全决策过程可追溯性，并在合理范围内保持操作延迟；在智能库存管理、交通信号控制和医疗监控实验中验证了有效性

Conclusion: 该框架为实施高影响力自主AI应用提供了通用系统，既保持自主性又确保责任性，平衡了自主决策与监管需求

Abstract: The application of agentic AI systems in autonomous decision-making is growing in the areas of healthcare, smart cities, digital forensics, and supply chain management. Even though these systems are flexible and offer real-time reasoning, they also raise concerns of trust and oversight, and integrity of the information and activities upon which they are founded. The paper suggests a single architecture model comprising of LangChain-based multi-agent system with a permissioned blockchain to guarantee constant monitoring, policy enforcement, and immutable auditability of agentic action. The framework relates the perception conceptualization-action cycle to a blockchain layer of governance that verifies the inputs, evaluates recommended actions, and documents the outcomes of the execution. A Hyperledger Fabric-based system, action executors MCP-integrated, and LangChain agent are introduced and experiments of smart inventory management, traffic-signal control, and healthcare monitoring are done. The results suggest that blockchain-security verification is efficient in preventing unauthorized practices, offers traceability throughout the whole decision-making process, and maintains operational latency within reasonable ranges. The suggested framework provides a universal system of implementing high-impact agentic AI applications that are autonomous yet responsible.

</details>


### [30] [FinAgent: An Agentic AI Framework Integrating Personal Finance and Nutrition Planning](https://arxiv.org/abs/2512.20991)
*Toqeer Ali Syed,Abdulaziz Alshahrani,Ali Ullah,Ali Akarma,Sohail Khan,Muhammad Nauman,Salman Jan*

Main category: cs.AI

TL;DR: 开发了一个价格感知的AI代理系统，结合个人财务管理与饮食优化，为中等收入家庭提供营养充足且价格合理的膳食计划，能自动适应市场价格变化。


<details>
  <summary>Details</summary>
Motivation: 中等收入环境下，家庭预算有限且营养需求持续存在，食品价格波动加剧了这一挑战。需要一种能同时考虑财务约束和营养需求的解决方案。

Method: 采用模块化多代理架构，包含预算、营养、价格监控和健康个性化等专门代理。这些代理共享知识库，使用替代图在保持营养质量的同时最小化成本。

Result: 在沙特家庭案例研究中，相比静态周菜单，成本持续降低12-18%，营养充足率超过95%，在20-30%价格波动下仍保持高性能。

Conclusion: 该框架能有效结合可负担性与营养充足性，为实现可持续和公平的饮食规划提供了可行途径，符合可持续发展目标中的零饥饿和良好健康目标。

Abstract: The issue of limited household budgets and nutritional demands continues to be a challenge especially in the middle-income environment where food prices fluctuate. This paper introduces a price aware agentic AI system, which combines personal finance management with diet optimization. With household income and fixed expenditures, medical and well-being status, as well as real-time food costs, the system creates nutritionally sufficient meals plans at comparatively reasonable prices that automatically adjust to market changes. The framework is implemented in a modular multi-agent architecture, which has specific agents (budgeting, nutrition, price monitoring, and health personalization). These agents share the knowledge base and use the substitution graph to ensure that the nutritional quality is maintained at a minimum cost. Simulations with a representative Saudi household case study show a steady 12-18\% reduction in costs relative to a static weekly menu, nutrient adequacy of over 95\% and high performance with price changes of 20-30%. The findings indicate that the framework can locally combine affordability with nutritional adequacy and provide a viable avenue of capacity-building towards sustainable and fair diet planning in line with Sustainable Development Goals on Zero Hunger and Good Health.

</details>


### [31] [TrafficSimAgent: A Hierarchical Agent Framework for Autonomous Traffic Simulation with MCP Control](https://arxiv.org/abs/2512.20996)
*Yuwei Du,Jun Zhang,Jie Feng,Zhicheng Liu,Jian Yuan,Yong Li*

Main category: cs.AI

TL;DR: TrafficSimAgent是一个基于LLM的智能体框架，通过高层和低层专家代理的跨级协作，帮助非专业用户轻松进行交通仿真实验设计和决策优化。


<details>
  <summary>Details</summary>
Motivation: 现有交通仿真平台（如SUMO、MATSim）功能全面，但缺乏专业知识的使用者难以从零开始进行实验并将其应用于日常工作。需要降低使用门槛，让非专业用户也能高效进行交通仿真。

Method: 提出TrafficSimAgent框架：1）高层专家代理理解自然语言指令，规划实验流程，按需调用MCP兼容工具；2）低层专家代理基于实时交通状况为基本元素选择最优行动方案；3）通过专家级自主决策驱动优化。

Result: 在多种场景下的实验表明：TrafficSimAgent能有效执行各种条件下的仿真，即使在用户指令模糊时也能产生合理结果；其专家级自主决策优化相比其他系统和SOTA LLM方法表现更优。

Conclusion: TrafficSimAgent成功解决了非专业用户使用交通仿真平台的挑战，通过LLM驱动的专家代理框架实现了灵活的实验设计和优化的决策执行，为交通优化和政策制定提供了易用工具。

Abstract: Traffic simulation is important for transportation optimization and policy making. While existing simulators such as SUMO and MATSim offer fully-featured platforms and utilities, users without too much knowledge about these platforms often face significant challenges when conducting experiments from scratch and applying them to their daily work. To solve this challenge, we propose TrafficSimAgent, an LLM-based agent framework that serves as an expert in experiment design and decision optimization for general-purpose traffic simulation tasks. The framework facilitates execution through cross-level collaboration among expert agents: high-level expert agents comprehend natural language instructions with high flexibility, plan the overall experiment workflow, and invoke corresponding MCP-compatible tools on demand; meanwhile, low-level expert agents select optimal action plans for fundamental elements based on real-time traffic conditions. Extensive experiments across multiple scenarios show that TrafficSimAgent effectively executes simulations under various conditions and consistently produces reasonable outcomes even when user instructions are ambiguous. Besides, the carefully designed expert-level autonomous decision-driven optimization in TrafficSimAgent yields superior performance when compared with other systems and SOTA LLM based methods.

</details>


### [32] [Agentic Explainable Artificial Intelligence (Agentic XAI) Approach To Explore Better Explanation](https://arxiv.org/abs/2512.21066)
*Tomoaki Yamaguchi,Yutong Zhou,Masahiro Ryo,Keisuke Katsura*

Main category: cs.AI

TL;DR: 提出结合SHAP可解释AI与多模态LLM自主迭代优化的Agentic XAI框架，在农业推荐系统中验证了早期迭代能提升解释质量，但过度优化会导致质量下降，需策略性早停。


<details>
  <summary>Details</summary>
Motivation: XAI技术能解释AI预测因素，但向非专业人士传达解释仍困难，影响AI信任度。LLM可将技术解释转化为易懂叙述，但自主迭代优化的Agentic AI与XAI结合尚未探索。

Method: 提出Agentic XAI框架，结合SHAP可解释性与多模态LLM驱动的迭代优化，生成渐进增强的解释。以日本26个稻田产量数据为案例，测试11轮迭代优化（0-10轮），由人类专家（12人）和LLM（14个）从7个指标评估解释质量。

Result: 框架成功提升推荐质量，平均得分比第0轮提高30-33%，在第3-4轮达到峰值。但过度优化导致推荐质量显著下降，显示偏差-方差权衡：早期轮次解释深度不足（偏差），过度迭代引入冗长和脱离实际的抽象（方差）。

Conclusion: 需要策略性早停（正则化）来优化实际效用，挑战了单调改进的假设，为Agentic XAI系统提供了基于证据的设计原则。

Abstract: Explainable artificial intelligence (XAI) enables data-driven understanding of factor associations with response variables, yet communicating XAI outputs to laypersons remains challenging, hindering trust in AI-based predictions. Large language models (LLMs) have emerged as promising tools for translating technical explanations into accessible narratives, yet the integration of agentic AI, where LLMs operate as autonomous agents through iterative refinement, with XAI remains unexplored. This study proposes an agentic XAI framework combining SHAP-based explainability with multimodal LLM-driven iterative refinement to generate progressively enhanced explanations. As a use case, we tested this framework as an agricultural recommendation system using rice yield data from 26 fields in Japan. The Agentic XAI initially provided a SHAP result and explored how to improve the explanation through additional analysis iteratively across 11 refinement rounds (Rounds 0-10). Explanations were evaluated by human experts (crop scientists) (n=12) and LLMs (n=14) against seven metrics: Specificity, Clarity, Conciseness, Practicality, Contextual Relevance, Cost Consideration, and Crop Science Credibility. Both evaluator groups confirmed that the framework successfully enhanced recommendation quality with an average score increase of 30-33% from Round 0, peaking at Rounds 3-4. However, excessive refinement showed a substantial drop in recommendation quality, indicating a bias-variance trade-off where early rounds lacked explanation depth (bias) while excessive iteration introduced verbosity and ungrounded abstraction (variance), as revealed by metric-specific analysis. These findings suggest that strategic early stopping (regularization) is needed for optimizing practical utility, challenging assumptions about monotonic improvement and providing evidence-based design principles for agentic XAI systems.

</details>


### [33] [A Real-World Evaluation of LLM Medication Safety Reviews in NHS Primary Care](https://arxiv.org/abs/2512.21127)
*Oliver Normand,Esther Borsi,Mitch Fruin,Lauren E Walker,Jamie Heagerty,Chris C. Holmes,Anthony J Avery,Iain E Buchan,Harry Coppock*

Main category: cs.AI

TL;DR: LLM药物安全审查系统在真实NHS初级保健数据上评估，虽能识别临床问题（敏感性100%），但仅46.9%患者完全正确识别所有问题和干预措施，主要失败源于上下文推理而非药物知识缺失。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在医学基准测试中常达到或超过临床医生水平，但很少在真实临床数据上评估或超越头条指标。本研究旨在评估LLM药物安全审查系统在真实NHS初级保健数据上的表现，并详细描述不同临床复杂性下的关键失败行为。

Method: 回顾性研究使用NHS Cheshire和Merseyside地区2,125,549名成人的电子健康记录，战略抽样捕获广泛临床复杂性和药物安全风险，经数据质量排除后获得277名患者。专家临床医生审查这些患者，对系统识别的问题和提出的干预措施进行分级。

Result: 主要LLM系统在识别临床问题存在时表现强劲（敏感性100%，特异性83.1%），但仅46.9%患者完全正确识别所有问题和干预措施。失败分析显示主要失败机制是上下文推理而非药物知识缺失，包括五种模式：不确定性过度自信、未调整患者上下文的指南应用、误解医疗实践、事实错误和流程盲点。

Conclusion: LLM临床AI安全部署前必须解决这些缺陷，需要更大规模的前瞻性评估和LLM临床行为深入研究。研究提供了45个详细案例全面覆盖所有识别失败案例。

Abstract: Large language models (LLMs) often match or exceed clinician-level performance on medical benchmarks, yet very few are evaluated on real clinical data or examined beyond headline metrics. We present, to our knowledge, the first evaluation of an LLM-based medication safety review system on real NHS primary care data, with detailed characterisation of key failure behaviours across varying levels of clinical complexity. In a retrospective study using a population-scale EHR spanning 2,125,549 adults in NHS Cheshire and Merseyside, we strategically sampled patients to capture a broad range of clinical complexity and medication safety risk, yielding 277 patients after data-quality exclusions. An expert clinician reviewed these patients and graded system-identified issues and proposed interventions. Our primary LLM system showed strong performance in recognising when a clinical issue is present (sensitivity 100\% [95\% CI 98.2--100], specificity 83.1\% [95\% CI 72.7--90.1]), yet correctly identified all issues and interventions in only 46.9\% [95\% CI 41.1--52.8] of patients. Failure analysis reveals that, in this setting, the dominant failure mechanism is contextual reasoning rather than missing medication knowledge, with five primary patterns: overconfidence in uncertainty, applying standard guidelines without adjusting for patient context, misunderstanding how healthcare is delivered in practice, factual errors, and process blindness. These patterns persisted across patient complexity and demographic strata, and across a range of state-of-the-art models and configurations. We provide 45 detailed vignettes that comprehensively cover all identified failure cases. This work highlights shortcomings that must be addressed before LLM-based clinical AI can be safely deployed. It also begs larger-scale, prospective evaluations and deeper study of LLM behaviours in clinical contexts.

</details>


### [34] [RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic](https://arxiv.org/abs/2512.21220)
*Le Wang,Zonghao Ying,Xiao Yang,Quanchen Zou,Zhenfei Yin,Tianlin Li,Jian Yang,Yaodong Yang,Aishan Liu,Xianglong Liu*

Main category: cs.AI

TL;DR: RoboSafe：一种用于具身智能体的混合推理运行时安全防护系统，通过可执行的基于谓词的安全逻辑来检测和预防危险行为，显著降低风险发生同时保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于静态规则过滤或提示级控制的防御方法难以处理动态、时间依赖和上下文丰富的环境中的隐式风险，需要更灵活、可验证的安全防护机制。

Method: 提出RoboSafe系统，包含两个互补的推理模块：1）后向反思推理模块，通过短期记忆持续回顾近期轨迹推断时间安全谓词；2）前向预测推理模块，通过长期安全记忆和多模态观察预测即将到来的风险。两者集成在混合长短安全记忆上形成可执行的安全逻辑。

Result: 在多个智能体上的实验表明，RoboSafe相比领先基线显著减少危险行为（风险发生率降低36.8%），同时保持接近原始的任务性能。物理机械臂的真实世界评估进一步证实其实用性。

Conclusion: RoboSafe提供了一种自适应、可验证、可解释的安全逻辑，能有效保护具身智能体免受危险指令的影响，为动态环境中的运行时安全防护提供了新解决方案。

Abstract: Embodied agents powered by vision-language models (VLMs) are increasingly capable of executing complex real-world tasks, yet they remain vulnerable to hazardous instructions that may trigger unsafe behaviors. Runtime safety guardrails, which intercept hazardous actions during task execution, offer a promising solution due to their flexibility. However, existing defenses often rely on static rule filters or prompt-level control, which struggle to address implicit risks arising in dynamic, temporally dependent, and context-rich environments. To address this, we propose RoboSafe, a hybrid reasoning runtime safeguard for embodied agents through executable predicate-based safety logic. RoboSafe integrates two complementary reasoning processes on a Hybrid Long-Short Safety Memory. We first propose a Backward Reflective Reasoning module that continuously revisits recent trajectories in short-term memory to infer temporal safety predicates and proactively triggers replanning when violations are detected. We then propose a Forward Predictive Reasoning module that anticipates upcoming risks by generating context-aware safety predicates from the long-term safety memory and the agent's multimodal observations. Together, these components form an adaptive, verifiable safety logic that is both interpretable and executable as code. Extensive experiments across multiple agents demonstrate that RoboSafe substantially reduces hazardous actions (-36.8% risk occurrence) compared with leading baselines, while maintaining near-original task performance. Real-world evaluations on physical robotic arms further confirm its practicality. Code will be released upon acceptance.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [35] [Topology and Network Dynamics of the Lightning Network: A Comprehensive Analysis](https://arxiv.org/abs/2512.20641)
*Danila Valko,Jorge Marx Gómez*

Main category: cs.SI

TL;DR: 基于2019-2023年闪电网络拓扑快照，计算47个计算密集型指标，全面分析网络结构和时间动态，验证先前研究并提供更深入的结构演化洞察


<details>
  <summary>Details</summary>
Motivation: 为了深入理解闪电网络的结构演化和时间动态，验证先前拓扑研究，并为启发式路径查找和路由协议设计提供依据

Method: 使用经过验证的五年闪电网络拓扑快照数据集，计算47个计算密集型指标和网络属性，进行全面的网络结构分析

Result: 验证了先前拓扑研究结果，量化了网络拓扑稳定性随时间的变化，为路径查找和路由协议设计提供了启示

Conclusion: 提供了闪电网络快照的详细特征描述，支持未来支付通道网络分析和网络科学研究

Abstract: Leveraging a validated set of reconstructed Lightning Network topology snapshots spanning five years (2019-2023), we computed 47 computationally intensive metrics and network attributes, enabling a comprehensive analysis of the network's structure and temporal dynamics. Our results corroborate prior topology studies while offering deeper insight into the network's structural evolution. In particular, we quantify the network's topological stability over time, yielding implications for the design of heuristic-based pathfinding and routing protocols. More broadly, this work provides a detailed characterization of publicly available Lightning Network snapshots, supporting future research in Payment Channel Network analysis and network science.

</details>


### [36] [Graph Neural Networks for Source Detection: A Review and Benchmark Study](https://arxiv.org/abs/2512.20657)
*Martin Sterchi,Nathan Brack,Lorenz Hilfiker*

Main category: cs.SI

TL;DR: 该论文系统评估了GNN在流行病源头检测任务中的表现，发现GNN显著优于传统方法，并提出了专门的GNN架构，建议将该任务作为GNN基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有GNN方法在源头检测任务中缺乏方法学清晰度和可复现性，不清楚GNN是否真正优于传统方法，需要系统评估和比较。

Method: 首先回顾现有GNN方法，提出专门针对源头检测任务的GNN架构，系统研究关键问题，并在多种网络类型上进行实验比较。

Result: 实验表明GNN在所有测试方法中表现最佳，显著优于传统源头检测方法，尽管作者最初对GNN持怀疑态度，但结果证明了其有效性。

Conclusion: GNN在流行病源头检测任务中表现出色，应作为评估GNN架构的基准任务，所有代码和数据已开源以确保可复现性。

Abstract: The source detection problem arises when an epidemic process unfolds over a contact network, and the objective is to identify its point of origin, i.e., the source node. Research on this problem began with the seminal work of Shah and Zaman in 2010, who formally defined it and introduced the notion of rumor centrality. With the emergence of Graph Neural Networks (GNNs), several studies have proposed GNN-based approaches to source detection. However, some of these works lack methodological clarity and/or are hard to reproduce. As a result, it remains unclear (to us, at least) whether GNNs truly outperform more traditional source detection methods across comparable settings. In this paper, we first review existing GNN-based methods for source detection, clearly outlining the specific settings each addresses and the models they employ. Building on this research, we propose a principled GNN architecture tailored to the source detection task. We also systematically investigate key questions surrounding this problem. Most importantly, we aim to provide a definitive assessment of how GNNs perform relative to other source detection methods. Our experiments show that GNNs substantially outperform all other methods we test across a variety of network types. Although we initially set out to challenge the notion of GNNs as a solution to source detection, our results instead demonstrate their remarkable effectiveness for this task. We discuss possible reasons for this strong performance. To ensure full reproducibility, we release all code and data on GitHub. Finally, we argue that epidemic source detection should serve as a benchmark task for evaluating GNN architectures.

</details>


### [37] [Signal, Noise, and Burnout: A Human-Information Interaction Analysis of Voter Verification in a High-Volatility Environment](https://arxiv.org/abs/2512.20679)
*Kijung Lee*

Main category: cs.SI

TL;DR: 研究发现，在2024年美国大选期间，社交媒体和主流新闻用户在辨别信息真伪的感知能力上没有显著差异，信息疲劳和人口因素才是主要影响因素。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨在2024年美国大选这一高度不稳定的信息环境中，不同信息渠道如何影响人们辨别真假新闻的自我效能感（认知能力）。

Method: 使用皮尤研究中心美国趋势小组数据（第155波，2024年9月，N=9,360），检验三个假设：社交媒体依赖是否比主流新闻源导致更低的认知自我效能；感知接触不准确信息是否中介这一关系；信息疲劳是否调节不同平台的验证认知负担。

Result: 与算法过滤理论预期相反，社交媒体和主流新闻用户在报告辨别真相难度上没有显著差异。认知负担主要由人口因素（年龄、教育）和普遍的信息疲劳驱动，表明在极端不稳定时期信息环境呈现"均等化"。

Conclusion: 研究挑战了平台决定论，表明支持知情公民的干预措施应关注认知韧性和注意力管理，而非仅仅平台选择。

Abstract: The 2024 U.S. Presidential Election unfolded within an information environment of unprecedented volatility, challenging citizens to navigate a torrent of rapidly evolving, often contradictory information while determining what to believe. This study investigates the cognitive mechanisms underlying epistemic self-efficacy - the perceived ability to distinguish accurate news from misinformation - across different information channels during this high-stakes election cycle. Drawing on data from the Pew Research Center's American Trends Panel (Wave 155, September 2024, N = 9,360), we test three hypotheses: (H1) whether reliance on social media predicts lower epistemic self-efficacy compared to mainstream news sources; (H2) whether perceived exposure to inaccurate information mediates this relationship; and (H3) whether information fatigue moderates the cognitive burden of verification across platforms. Contrary to expectations rooted in algorithmic filtering theory, we find no significant differences in reported difficulty determining truth between social media and mainstream news users. Instead, epistemic burden is driven by demographics (age, education) and universal information fatigue, suggesting a "leveling" of the information landscape during periods of extreme volatility. This finding challenges platform-deterministic theories and suggests that interventions to support informed citizenship must address cognitive resilience and attention management rather than platform choice alone.

</details>


### [38] [Mental Health Self-Disclosure on Social Media throughout the Pandemic Period](https://arxiv.org/abs/2512.20990)
*Dino Husnic,Stefan Cobeli,Shweta Yadav*

Main category: cs.SI

TL;DR: 该研究分析英国Reddit用户在疫情期间的情绪和心理健康变化，通过软标签技术识别心理健康状况，并与英国COVID-19政策重要日期对比，发现心理健康特征可提高情绪预测准确性。


<details>
  <summary>Details</summary>
Motivation: COVID-19大流行导致社会隔离和经济困难，人们在社交媒体上表达情绪。研究旨在了解疫情期间英国Reddit用户的情绪和心理健康变化，以及政策事件如何影响这些表达。

Method: 收集2020年3月2日至7月4日期间r/unitedkingdom子版块的帖子和评论，使用软标签技术为每条评论生成心理健康状况标签，将结果与英国COVID-19政策重要日期进行对比分析。

Result: 研究发现疫情期间人们的情绪表达与政策事件相关，通过概念验证表明，加入心理健康特征可以提高情绪预测的准确性。

Conclusion: 社交媒体数据可用于监测疫情期间公众的心理健康变化，心理健康特征对情绪预测有重要价值，为公共卫生干预提供了数据支持。

Abstract: The COVID-19 pandemic has created many problems, especially in people's social lives. There has been increasing isolation and economic hardships since the beginning of the pandemic for people all over the world. Quarantines and lockdowns also took part in that, and so, people have been expressing their emotions throughout the pandemic period using social media platforms like Reddit, Twitter, Facebook, etc. In this study, we seek to analyze the emotions and mental health labels throughout the time period of March 2, 2020, up until July 4, 2020, from the threads and comments gathered from the r/unitedkingdom subreddit. We used a soft labeling technique to generate mental health conditions for each Reddit comment. We compared the overall results with important dates related to COVID-19 policies that took place in the United Kingdom. This can give us a view on how the pandemic and the important dates affect people self disclosing their emotions on social media platforms. Finally, we have developed a proof of concept to show that using mental health features may increase emotion prediction accuracy.

</details>


### [39] [Emotion Diffusion in Real and Simulated Social Graphs: Structural Limits of LLM-Based Social Simulation](https://arxiv.org/abs/2512.21138)
*Qiqi Qiang*

Main category: cs.SI

TL;DR: LLM模拟的社交网络在情感扩散模式上与真实社交网络存在显著差异，真实网络表现出更复杂的结构和动态特征


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地用于模拟社交媒体互动，需要评估LLM生成的数据能否真实再现真实在线社区中观察到的情感扩散模式

Method: 系统比较真实社交图和LLM模拟互动网络中的情感扩散，从Reddit讨论数据构建扩散图，与LLM驱动的对话模拟生成的合成社交图进行比较，使用情感分析流程推断情感状态，从结构、行为和预测角度分析

Result: 真实与模拟扩散过程存在显著的结构和动态差异：真实情感扩散表现出密集连接、重复互动、情感转变和涌现的社区结构，而LLM模拟图主要由孤立的线性链和单调情感轨迹组成，这些结构限制影响了图基情感预测等下游任务

Conclusion: LLM基于社交模拟在捕捉真实社交网络的交互复杂性和情感异质性方面存在当前局限性，为在社会科学研究中谨慎使用LLM生成数据提供了实证证据，并指出了改进未来模拟框架的方向

Abstract: Understanding how emotions diffuse through social networks is central to computational social science. Recently, large language models (LLMs) have been increasingly used to simulate social media interactions, raising the question of whether LLM-generated data can realistically reproduce emotion diffusion patterns observed in real online communities. In this study, we conduct a systematic comparison between emotion diffusion in real-world social graphs and in LLM-simulated interaction networks. We construct diffusion graphs from Reddit discussion data and compare them with synthetic social graphs generated through LLM-driven conversational simulations. Emotion states are inferred using established sentiment analysis pipelines, and both real and simulated graphs are analyzed from structural, behavioral, and predictive perspectives. Our results reveal substantial structural and dynamic discrepancies between real and simulated diffusion processes. Real-world emotion diffusion exhibits dense connectivity, repeated interactions, sentiment shifts, and emergent community structures, whereas LLM-simulated graphs largely consist of isolated linear chains with monotonic emotional trajectories. These structural limitations significantly affect downstream tasks such as graph-based emotion prediction, leading to reduced emotional diversity and class imbalance in simulated settings. Our findings highlight current limitations of LLM-based social simulation in capturing the interactive complexity and emotional heterogeneity of real social networks. This work provides empirical evidence for the cautious use of LLM-generated data in social science research and suggests directions for improving future simulation frameworks.

</details>


### [40] [A Community-Enhanced Graph Representation Model for Link Prediction](https://arxiv.org/abs/2512.21166)
*Lei Wang,Darong Lai*

Main category: cs.SI

TL;DR: 本文提出CELP框架，通过社区结构增强图神经网络在链接预测任务中的性能，结合局部和全局拓扑信息，超越传统启发式方法和现有GNN。


<details>
  <summary>Details</summary>
Motivation: 现有GNN在链接预测任务中表现有时不如传统启发式方法（如共同邻居、Jaccard系数），主要因为GNN过度关注局部节点表示，难以有效捕捉节点对间的结构关系，且过度依赖局部邻域信息会导致过平滑问题。

Method: 提出社区增强的链接预测（CELP）框架，通过社区感知、置信度引导的边补全和剪枝来增强图结构，同时整合多尺度结构特征，联合建模局部和全局图拓扑。

Result: 在多个基准数据集上的实验结果表明，CELP实现了优越的性能，验证了社区结构在提高链接预测准确性中的关键作用。

Conclusion: 社区结构对于提升图神经网络在链接预测任务中的性能至关重要，CELP框架通过有效结合局部和全局拓扑信息，解决了现有GNN的局限性。

Abstract: Although Graph Neural Networks (GNNs) have become the dominant approach for graph representation learning, their performance on link prediction tasks does not always surpass that of traditional heuristic methods such as Common Neighbors and Jaccard Coefficient. This is mainly because existing GNNs tend to focus on learning local node representations, making it difficult to effectively capture structural relationships between node pairs. Furthermore, excessive reliance on local neighborhood information can lead to over-smoothing. Prior studies have shown that introducing global structural encoding can partially alleviate this issue. To address these limitations, we propose a Community-Enhanced Link Prediction (CELP) framework that incorporates community structure to jointly model local and global graph topology. Specifically, CELP enhances the graph via community-aware, confidence-guided edge completion and pruning, while integrating multi-scale structural features to achieve more accurate link prediction. Experimental results across multiple benchmark datasets demonstrate that CELP achieves superior performance, validating the crucial role of community structure in improving link prediction accuracy.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [41] [Testing Exclusion and Shape Restrictions in Potential Outcomes Models](https://arxiv.org/abs/2512.20851)
*Hiroaki Kaido,Kirill Ponomarev*

Main category: econ.EM

TL;DR: 提出一个基于图表示的统一框架，用于推导潜在结果模型中支持限制的尖锐可检验含义


<details>
  <summary>Details</summary>
Motivation: 现有研究仅在有限模型中逐个案例地研究排除和形状限制的可检验含义，缺乏统一框架

Method: 开发基于图表示的一般框架，通过图结构表征潜在响应函数的支持限制，提供构造性方法推导所有可观测含义

Result: 框架在多种流行设置中应用，包括工具变量、治疗选择、中介和干扰，并在美国肺健康研究中实证应用

Conclusion: 该框架为潜在结果模型中支持限制提供了统一的尖锐可检验含义推导方法，具有广泛适用性

Abstract: Exclusion and shape restrictions play a central role in defining causal effects and interpreting estimates in potential outcomes models. To date, the testable implications of such restrictions have been studied on a case-by-case basis in a limited set of models. In this paper, we develop a general framework for characterizing sharp testable implications of general support restrictions on the potential response functions, based on a novel graph-based representation of the model. The framework provides a unified and constructive method for deriving all observable implications of the modeling assumptions. We illustrate the approach in several popular settings, including instrumental variables, treatment selection, mediation, and interference. As an empirical application, we revisit the US Lung Health Study and test for the presence of spillovers between spouses, specification of exposure maps, and persistence of treatment effects over time.

</details>


### [42] [Welfare at Risk: Distributional impact of policy interventions](https://arxiv.org/abs/2512.20918)
*Costas Lambros,Emerson Melo*

Main category: econ.EM

TL;DR: 该论文提出了一个分析政策干预福利效应在个体间分布情况的框架，即使这些效应无法直接观测。该方法利用平均福利响应信息来揭示不同群体间收益和损失的分布模式，重点关注哪些群体受政策负面影响最大。


<details>
  <summary>Details</summary>
Motivation: 传统政策分析通常只关注平均结果，但无法揭示福利效应在个体间的分布情况。当个体层面的效应无法观测时，政策制定者难以了解哪些群体受到负面影响最大，以及如何在效率和公平之间进行权衡。

Method: 基于超分位数概念构建分析框架，适用于具有未观测个体异质性的一类广泛模型。利用平均福利响应信息来推导福利效应分布的界限，能够识别受政策负面影响最大的群体。

Result: 该框架在三个经济场景中得到应用：价格变化与补偿变化、自选择下的处理分配、社会项目的成本效益分析。在最后一个应用中，展示了如何利用边际处理效应和广义罗伊模型的工具来实现对总体和参与者的界限分析。

Conclusion: 提出的框架为分析未观测福利效应的分布提供了实用工具，使政策制定者能够识别受影响最大的群体，评估效率与公平的权衡，从而做出更明智的政策决策。

Abstract: This paper proposes a framewrok for analyzing how the welfare effects of policy interventions are distributed across individuals when those effects are unobserved. Rather than focusing solely on average outcomes, the approach uses readily available information on average welfare responses to uncover meaningful patterns in how gains and losses are distributed across different populations. The framework is built around the concept of superquantile and applies to a broad class of models with unobserved individual heterogeneity. It enables policymakers to identify which groups are most adversely affected by a policy and to evaluate trade-offs between efficiency and equity. We illustrate the approach in three widely studied economic settings: price changes and compensated variation, treatment allocation with self-selection, and the cost-benefit analysis of social programs. In this latter application, we show how standard tools from the marginal treatment effect and generalized Roy model literature are useful for implementing our bounds for both the overall population and for individuals who participate in the program.

</details>


### [43] [Learning the Macroeconomic Language](https://arxiv.org/abs/2512.21031)
*Siddhartha Chib,Fei Tan*

Main category: econ.EM

TL;DR: 使用DSGE模型生成理论一致的合成数据，结合真实宏观经济数据训练时间序列Transformer，创建混合预测器用于宏观经济预测


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在宏观经济小样本场景下的应用限制，结合DSGE模型的理论一致性和现代LLMs的表征能力

Method: 1) 在初始数据段上估计大规模DSGE模型，获得结构参数的后验分布；2) 从后验分布采样生成数百万个理论一致的合成面板数据；3) 将合成数据与真实宏观经济数据混合作为训练语料；4) 训练带注意力机制的时间序列Transformer模型

Result: 混合预测器成功学习宏观经济语言，能够用于2025年之前的样本外预测，结合了DSGE的理论一致性和现代LLMs的表征能力

Conclusion: 通过结合DSGE模型的理论框架和现代LLMs的表示能力，可以克服LLMs在宏观经济小样本问题上的限制，成功创建有效的宏观经济预测器

Abstract: We show how state-of-the-art large language models (LLMs), seemingly inapplicable to the small samples typical of macroeconomics, can be trained to learn the language of macroeconomy. We estimate a large-scale dynamic stochastic general equilibrium (DSGE) model on an initial segment of the data and obtain a posterior distribution over structural parameters. We sample from this posterior to generate millions of theory-consistent synthetic panels that, when mixed with actual macroeconomic data, form the training corpus for a time-series transformer with attention. The trained model is then used to forecast out-of-sample through 2025. The results show that this hybrid forecaster, which combines the theoretical coherence of DSGE models with the representational power of modern LLMs, successfully learns the macroeconomic language.

</details>


### [44] [Difference-in-Differences in the Presence of Unknown Interference](https://arxiv.org/abs/2512.21176)
*Fabrizia Mealli,Javier Viviens*

Main category: econ.EM

TL;DR: DiD估计量在存在未知干扰时识别的是因果效应的对比，而非单个因果效应；需要额外假设才能解释具体效应。


<details>
  <summary>Details</summary>
Motivation: SUTVA（稳定单位处理值）假设在DiD研究中常被忽视，特别是其中的无干扰假设。本文旨在探讨当存在未知干扰时，DiD估计量实际识别的是什么。

Method: 理论分析DiD估计量在干扰存在下的识别性质，探索不同假设集使DiD估计量对特定因果效应具有解释力，并以Card和Krueger（1994）的最低工资与就业研究为例进行说明。

Result: DiD估计量在干扰存在下识别的是因果效应的对比，但无法单独解释其中任何效应；只有在引入额外假设后，才能对特定因果效应做出推断。

Conclusion: DiD研究者应更重视SUTVA假设，特别是无干扰假设；在存在干扰时，DiD估计量的解释需要谨慎并依赖额外假设。

Abstract: The stable unit treatment value (SUTVA) is a crucial assumption in the Difference-in-Differences (DiD) research design. It rules out hidden versions of treatment and any sort of interference and spillover effects across units. Even if this is a strong assumption, it has not received much attention from DiD practitioners and, in many cases, it is not even explicitly stated as an assumption, especially the no-interference assumption. In this technical note, we investigate what the DiD estimand identifies in the presence of unknown interference. We show that the DiD estimand identifies a contrast of causal effects, but it is not informative on any of these causal effects separately, without invoking further assumptions. Then, we explore different sets of assumptions under which the DiD estimand becomes informative about specific causal effects. We illustrate these results by revisiting the seminal paper on minimum wages and employment by Card and Krueger (1994).

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [45] [A Profit-Based Measure of Lending Discrimination](https://arxiv.org/abs/2512.20753)
*Madison Coots,Robert Bartlett,Julian Nyarko,Sharad Goel*

Main category: stat.AP

TL;DR: 该研究提出了一种基于利润的贷款歧视衡量方法，应用于美国金融科技平台的8万笔个人贷款数据，发现对男性和黑人借款人的贷款利润较低，表明他们获得了相对有利的贷款条件，这种差异源于风险评估模型的校准偏差。


<details>
  <summary>Details</summary>
Motivation: 随着算法贷款在消费信贷领域的普及，机器学习模型被广泛用于承销决策。虽然这些算法通常排除受法律保护的种族、性别等特征以遵守公平贷款法律，但仍可能无意中偏袒某些群体。因此需要新的方法来审计贷款算法是否存在潜在的歧视行为。

Method: 基于先前理论研究，引入了基于利润的贷款歧视衡量方法。将这种方法应用于美国主要金融科技平台约80,000笔个人贷款数据，分析不同群体贷款的利润差异，并追溯这些差异的来源。

Result: 研究发现，对男性和黑人借款人的贷款产生的利润低于其他群体，表明男性和黑人申请人获得了相对有利的贷款决策。这些差异源于平台承销模型的校准偏差：模型低估了黑人借款人的信用风险，而高估了女性的风险。

Conclusion: 研究显示，通过在承销模型中明确包含种族和性别信息可以纠正这种校准偏差及相应的贷款差异，但这揭示了不同公平概念之间的紧张关系——纠正算法偏差可能需要使用传统上被视为"受保护"的特征。

Abstract: Algorithmic lending has transformed the consumer credit landscape, with complex machine learning models now commonly used to make or assist underwriting decisions. To comply with fair lending laws, these algorithms typically exclude legally protected characteristics, such as race and gender. Yet algorithmic underwriting can still inadvertently favor certain groups, prompting new questions about how to audit lending algorithms for potentially discriminatory behavior. Building on prior theoretical work, we introduce a profit-based measure of lending discrimination in loan pricing. Applying our approach to approximately 80,000 personal loans from a major U.S. fintech platform, we find that loans made to men and Black borrowers yielded lower profits than loans to other groups, indicating that men and Black applicants benefited from relatively favorable lending decisions. We trace these disparities to miscalibration in the platform's underwriting model, which underestimates credit risk for Black borrowers and overestimates risk for women. We show that one could correct this miscalibration -- and the corresponding lending disparities -- by explicitly including race and gender in underwriting models, illustrating a tension between competing notions of fairness.

</details>

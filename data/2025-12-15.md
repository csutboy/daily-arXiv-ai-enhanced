<div id=toc></div>

# Table of Contents

- [stat.AP](#stat.AP) [Total: 6]
- [cs.SI](#cs.SI) [Total: 4]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.AI](#cs.AI) [Total: 18]
- [econ.EM](#econ.EM) [Total: 2]
- [cs.CY](#cs.CY) [Total: 3]


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [1] [umx version 4.5: Extending Twin and Path-Based SEM in R with CLPM, MR-DoC, Definition Variables, $Ω$nyx Integration, and Censored distributions](https://arxiv.org/abs/2512.11063)
*Luis FS Castro-de-Araujo,Nathan Gillespie,Michael C Neale,Timothy Bates*

Main category: stat.AP

TL;DR: umx v4.5扩展了OpenMx包的功能，新增了纵向和因果双生子设计模型，改进了与图形建模工具的互操作性，提供了更便捷的双生子和家庭建模工作流。


<details>
  <summary>Details</summary>
Motivation: 为了降低遗传流行病学分析的门槛，提高双生子和家庭建模的可重复性、可靠性和发表就绪性，需要增强结构方程建模包的功能，特别是在纵向和因果设计方面。

Method: 基于umx包原有设计，扩展了经典和现代交叉滞后面板模型、孟德尔随机化因果方向双生子模型、支持定义变量、改进与Onyx的互操作性、新增截尾变量分析工具、简化性别限制建模、提供协变量残差化功能。

Result: umx v4.5显著增强了功能，包括纵向建模、因果推断、图形工具集成、简化的工作流程，以及智能默认值和集成的高质量报告功能。

Conclusion: umx v4.5通过扩展功能和改进互操作性，加速了可重复、可靠、发表就绪的双生子和家庭建模，降低了遗传流行病学分析的门槛。

Abstract: Structural Equation Modeling (SEM) provides a powerful and flexible framework widely used in behavioral genetics and social sciences. Building on the original design of the umx package, which enhanced accessibility to OpenMx using concise syntax and helpful defaults, umx v4.5 significantly extends functionality for longitudinal and causal twin designs while improving interoperability with graphical modelling tools such as Onyx. New capabilities include: classic and modern cross-lagged panel model; Mendelian Randomization Direction-of-Causation (MR-DoC) twin models incorporating polygenic scores as instruments; expanded support for definition variables directly in umxRAM(); streamlined workflows for importing paths from $Ω$nyx; a dedicated tool for analyzing censored variables, particularly valuable in biomarker research; improved covariate placeholder handling for definition variables; umxSexLim() for simplified sex-limitation modelling across five twin groups, accommodating quantitative and qualitative sex differences; and umx_residualize() for efficient covariate residualization in wide- or long-format data. These advances accelerate reproducible, reliable, publication-ready twin and family modelling using intelligent defaults, and integrated journal-quality reporting, thereby lowering barriers to genetic epidemiological analyzes.

</details>


### [2] [A Unified Micro-Model for Loss Reserves, IBNR and Unearned Premium Risk with Dependence, Inflation, and Discounting](https://arxiv.org/abs/2512.11197)
*Emmanuel Hamel,Anas Abdallah,Ghislain Léveillé*

Main category: stat.AP

TL;DR: 提出统一的微观随机框架，联合建模损失准备金、已发生未报告准备金和未满期保费风险，考虑相关性、通胀和折现因素。


<details>
  <summary>Details</summary>
Motivation: 传统准备金模型通常独立处理损失准备金和保费风险，缺乏统一的微观层面框架来联合建模这些风险，特别是在考虑相关性、通胀和折现的情况下。

Method: 使用统一的微观随机框架，通过聚合趋势更新过程(ATRP)在个体索赔层面实现，考虑赔偿金、费用、报告延迟和结算延迟的相互作用，允许灵活的参数依赖结构和动态财务调整。

Result: 推导了预测支付的前两个原始和联合条件矩的闭式表达式，以及其分布函数的近似。通过医疗责任保险案例研究验证了方法的实用性，展示了统一微观建模在动态负债和保费风险评估中的优势。

Conclusion: 统一的微观层面建模框架为长期业务线的动态负债和保费风险评估提供了优势，可直接应用于定价、准备金和资本管理。

Abstract: This paper introduces a unified micro-level stochastic framework for the joint modeling of loss reserves (RBNS), incurred but not reported (IBNR) reserves, and unearned premium risk under dependence, inflation, and discounting. The proposed framework accommodates interactions between indemnities, expenses, reporting delays, and settlement delays, while allowing for flexible parametric dependence structures and dynamic financial adjustments. An Aggregate Trend Renewal Process (ATRP) is used as one possible implementation of the joint model for payments, expenses, and delays; however, the methodological contribution of the paper lies in the unified micro-level reserving architecture rather than in the ATRP itself. The framework produces forward-looking reserve and premium risk measures with direct applications to pricing, reserving, and capital management.
  We implement the framework using an aggregate trend renewal process at the individual claim level, which can be applied to the usual run-off triangle to obtain predictions for each accident-development year. Closed-form expressions for the first two raw and joint conditional moments of predicted payments are derived, together with approximations of their distribution functions. A detailed case study on medical malpractice insurance illustrates the practical relevance of the approach and its calibration on real-world data. We also investigate data heterogeneity, parameter uncertainty, distributional approximations, premium risk, UPR sensitivity to operational delays and inflation, and risk capital implications under alternative assumptions. The results highlight the advantages of unified micro-level modeling for dynamic liability and premium risk assessment in long-tailed lines of business.

</details>


### [3] [Neural Network-based Partial-Linear Single-Index Models for Environmental Mixtures Analysis](https://arxiv.org/abs/2512.11593)
*Hyungrok Do,Yuyan Wang,Mengling Liu,Myeonggyun Lee*

Main category: stat.AP

TL;DR: 提出NeuralPLSI框架，结合神经网络表达能力与半参数回归可解释性，用于评估复杂环境混合物健康效应


<details>
  <summary>Details</summary>
Motivation: 现有评估环境混合物健康效应的方法在灵活性、可解释性、可扩展性和支持多种结局类型方面存在局限，限制了实际应用

Method: 提出神经网络部分线性单指数模型框架，通过可学习投影构建可解释暴露指数，用灵活神经网络建模其与结局的关系，支持连续、二元和时间到事件结局，使用基于bootstrap的推断方法

Result: 通过多种场景的模拟研究评估了NeuralPLSI，并在NHANES数据中展示了其实用性，证明其作为可扩展、可解释、多功能的混合物分析工具

Conclusion: NeuralPLSI是环境混合物分析的可扩展、可解释、多功能建模工具，已发布开源软件包促进采用和可重复性

Abstract: Evaluating the health effects of complex environmental mixtures remains a central challenge in environmental health research. Existing approaches vary in their flexibility, interpretability, scalability, and support for diverse outcome types, often limiting their utility in real-world applications. To address these limitations, we propose a neural network-based partial-linear single-index (NeuralPLSI) modeling framework that bridges semiparametric regression modeling interpretability with the expressive power of deep learning. The NeuralPLSI model constructs an interpretable exposure index via a learnable projection and models its relationship with the outcome through a flexible neural network. The framework accommodates continuous, binary, and time-to-event outcomes, and supports inference through a bootstrap-based procedure that yields confidence intervals for key model parameters. We evaluated NeuralPLSI through simulation studies under a range of scenarios and applied it to data from the National Health and Nutrition Examination Survey (NHANES) to demonstrate its practical utility. Together, our contributions establish NeuralPLSI as a scalable, interpretable, and versatile modeling tool for mixture analysis. To promote adoption and reproducibility, we release a user-friendly open-source software package that implements the proposed methodology and supports downstream visualization and inference (\texttt{https://github.com/hyungrok-do/NeuralPLSI}).

</details>


### [4] [Euclidean Ideal Point Estimation From Roll-Call Data via Distance-Based Bipartite Network Models](https://arxiv.org/abs/2512.11610)
*Seungju Lee,In Kyun Kim,Jong Hee Park,Ick Hoon Jin*

Main category: stat.AP

TL;DR: 提出基于距离的LSIRM模型替代传统理想点模型，解决非度量距离问题，在立法投票数据中恢复度量结构，提升聚类分析和预测性能


<details>
  <summary>Details</summary>
Motivation: 传统理想点模型使用高斯或二次效用函数，违反三角形不等式，产生非度量距离，这使几何解释复杂化并削弱聚类和离散度分析。需要恢复度量结构以获得更清晰的立法行为推断。

Method: 引入基于距离的替代方法，将潜在空间项目反应模型(LSIRM)应用于点名投票数据，将立法者和法案视为二分网络中的节点，在欧几里得度量空间中联合嵌入。

Result: 通过控制模拟，欧几里得LSIRM能一致恢复潜在联盟结构，相对于现有方法具有更优的聚类分离度。应用于第118届美国众议院时，模型提高了投票预测准确性，生成的法案嵌入能澄清交叉议题对齐。

Conclusion: 恢复理想点估计的度量结构能提供更清晰、更一致的关于政党凝聚力、派系分裂和多维立法行为的推断，为政治科学分析提供更好的几何解释基础。

Abstract: Conventional ideal point models rely on Gaussian or quadratic utility functions that violate the triangle inequality, producing non-metric distances that complicate geometric interpretation and undermine clustering and dispersion-based analyses. We introduce a distance-based alternative that adapts the Latent Space Item Response Model (LSIRM) to roll-call data, treating legislators and bills as nodes in a bipartite network jointly embedded in a Euclidean metric space. Through controlled simulations, Euclidean LSIRM consistently recovers latent coalition structure with superior cluster separation relative to existing methods. Applied to the 118th U.S. House, the model improves vote prediction and yields bill embeddings that clarify cross-cutting issue alignments. The results show that restoring metric structure to ideal point estimation provides a clearer and more coherent inference about party cohesion, factional divisions, and multidimensional legislative behavior.

</details>


### [5] [Dynamic Conditional SKEPTIC](https://arxiv.org/abs/2512.11648)
*Gabriele Di Luzio,Giacomo Morelli*

Main category: stat.AP

TL;DR: DCS是一种半参数方法，利用非参数秩统计量（Spearman's rho和Kendall's tau）高效稳健地估计多元模型中的时变相关性，相比传统DCC模型在诊断检验和风险管理方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统动态条件相关（DCC）模型在估计时变相关性时可能存在局限性，需要更稳健、高效的方法来准确估计多元金融时间序列中的时变相关性矩阵，特别是在风险管理应用中。

Method: 提出动态条件SKEPTIC（DCS）半参数方法，利用非参数秩统计量（Spearman's rho和Kendall's tau）估计未知的相关性矩阵，并讨论了模型的平稳性、beta-混合和rho-混合条件。

Result: 在S&P100和S&P500股票数据（2013-2025）的应用显示，DCS相比经典DCC模型改善了诊断检验，提供了不相关且正态分布的残差。风险管理应用中，基于DCS的全局最小方差投资组合比DCC和DCC-NL模型具有更低的换手率，且在S&P100成分股投资组合中实现了更高的夏普比率。

Conclusion: DCS方法为时变相关性估计提供了一种稳健高效的半参数替代方案，在金融时间序列分析和风险管理应用中表现出优于传统DCC模型的性能。

Abstract: We introduce the Dynamic Conditional SKEPTIC (DCS), a semiparametric approach for efficiently and robustly estimating time-varying correlations in multivariate models. We exploit nonparametric rank-based statistics, namely Spearman's rho and Kendall's tau, to estimate the unknown correlation matrix and discuss the stationarity, beta- and rho- mixing conditions of the model. We illustrate the methodology by estimating the time-varying conditional correlation matrix of the stocks included in the S&P100 and S&P500 during the period from 02/01/2013 to 23/01/2025. The results show that DCS improves diagnostic checks compared to the classical Dynamic Conditional Correlation (DCC) models, providing uncorrelated and normally distributed residuals. A risk management application shows that global minimum variance portfolios estimated using the DCS model exhibit lower turnover than those based on the DCC and DCC-NL models, while also achieving higher Sharpe ratios for portfolios constructed from S&P 100 constituents.

</details>


### [6] [Maritime Vessel Tracking](https://arxiv.org/abs/2512.11707)
*John Mahlon Scott,Hsin-Hsiung Huang*

Main category: stat.AP

TL;DR: 提出一种混合管道方法，结合物理模型和神经网络分类器，解决AIS轨迹在船舶标识缺失情况下的重标记问题，特别是在大规模、异构的AIS数据流中。


<details>
  <summary>Details</summary>
Motivation: AIS系统提供船舶位置和运动报告，但船舶标识可能缺失，导致轨迹不连续。特别是在全国范围的海域环境中，轨迹数据稀疏且多样化，需要有效的重标记方法。

Method: 采用混合管道方法：1）基于物理的筛选步骤，预测活动轨迹端点并选择可能的祖先轨迹；2）监督神经网络分类器，使用时空和运动一致性特征，从候选轨迹中选择或启动新轨迹。

Result: 在保留数据上，该方法相比无监督基线提高了位置准确性，证明结合简单运动模型和学习消歧能力可以扩展到异构、高容量的AIS数据流。

Conclusion: 物理模型与学习消歧相结合的方法能够有效解决大规模AIS轨迹重标记问题，适用于多样化、高流量的海事监控场景。

Abstract: The Automatic Identification System (AIS) provides time stamped vessel positions and kinematic reports that enable maritime authorities to monitor traffic. We consider the problem of relabeling AIS trajectories when vessel identifiers are missing, focusing on a challenging nationwide setting in which tracks are heavily downsampled and span diverse operating environments across continental U.S. waters. We propose a hybrid pipeline that first applies a physics-based screening step to project active track endpoints forward in time and select a small set of plausible ancestors for each new observation. A supervised neural classifier then chooses among these candidates, or initiates a new track, using engineered space time and kinematic consistency features. On held out data, this approach improves posit accuracy relative to unsupervised baselines, demonstrating that combining simple motion models with learned disambiguation can scale vessel relabeling to heterogeneous, high volume AIS streams.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [7] [Meso-scale structures in signed networks](https://arxiv.org/abs/2512.11281)
*Wei Zhang,Olga Boichak,Tristram J. Alexander,Tiago P. Peixoto,Eduardo G. Altmann*

Main category: cs.SI

TL;DR: 提出一种超越传统社会平衡理论限制的方法，用于发现和表征符号网络中的不平衡中尺度结构，并在24个实证网络中发现不平衡结构普遍存在。


<details>
  <summary>Details</summary>
Motivation: 传统研究符号网络的中尺度结构时，通常假设社会平衡理论成立（即组内正连接、组间负连接），但这种方法限制了发现其他可能的不平衡结构。需要一种不预设特定模式的方法来全面分析符号网络的中尺度结构。

Method: 提出并应用一种计算框架，该方法不预设特定模式（agnostic），能够发现和表征符号网络中的各种可能中尺度结构，包括平衡和不平衡的结构。该方法应用于24个来自社会政治、金融和生物领域的实证网络。

Result: 在实证网络中发现不平衡的中尺度结构普遍存在，即使微观尺度（三角形）存在显著平衡性。具体发现包括：同配性（assortativity）往往独立于交互符号而普遍存在；在线社交网络中通常呈现核心-边缘结构。

Conclusion: 中尺度关系结构具有复杂性，需要使用不预设特定模式的计算方法，并需要独立评估社会平衡理论在微观和中尺度层面的预测。研究结果强调了超越传统平衡理论限制的重要性。

Abstract: Meso-scale structures in signed networks have been studied under the limiting assumption of the validity of social balance theory, which predicts positive connections within groups and negative connections between groups. Here, we propose and apply a methodology that overcomes this limitation and is able to find and characterize also the different possible unbalanced structures in signed networks. Applying our methodology to 24 empirical networks, from social-political, financial, and biological domains, we find that unbalanced meso-scale structures are prevalent in real-world networks, including cases with substantial balance at the micro-scale of triangles. In particular, we find that assortativity often prevails regardless of the interaction sign and that core-periphery structures are typical in online social networks. Our findings highlight the complexity of meso-scale relational structures, the importance of using computational methods that are a priori agnostic to specific patterns, and the importance of independently evaluating micro- and meso-scale predictions of social balance theory.

</details>


### [8] [Complementary Strengths: Combining Geometric and Topological Approaches for Community Detection](https://arxiv.org/abs/2512.11496)
*Jelena Losic*

Main category: cs.SI

TL;DR: 提出一个结合谱嵌入和拓扑数据分析的统一框架，用于社区检测，在模块化和几何结构网络中均表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有社区检测方法存在局限性：基于图论的方法（如Louvain）可能忽略几何社区结构，而拓扑数据分析方法（如ToMATo）对初始投影敏感。需要结合两者优势的统一框架。

Method: 使用谱嵌入捕捉网络的几何骨架，创建密度盆地景观，然后应用ToMATo算法从该景观中提取持久聚类，形成拓扑基础且参数感知的混合方法。

Result: 在合成基准测试中，该方法表现出高度鲁棒性：在模块化网络上性能与Louvain相当，同时能识别几何社区结构。

Conclusion: 需要基于网络几何选择策略的新型混合算法，超越"一刀切"的解决方案，该框架为此提供了有前景的方向。

Abstract: The optimal strategy for community detection in complex networks is not universal, but depends critically on the network's underlying structural properties. Although popular graph-theoretic methods, such as Louvain, optimize for modularity, they can overlook nuanced, geometric community structures. Conversely, topological data analysis (TDA) methods such as ToMATo are powerful in identifying density-defined clusters in embedded data but can be sensitive to initial projection. We propose a unified framework that integrates both paradigms to take advantage of their complementary advantages. Our method uses spectral embedding to capture the network's geometric skeleton, creating a landscape where communities manifest as density basins. The ToMATo algorithm then provides a topologically-grounded and parameter-aware method to extract persistent clusters from this landscape. Our comprehensive analysis across synthetic benchmarks shows that this hybrid approach is highly robust: it performs on par with Louvain on modular networks. These results argue for a new class of hybrid algorithms that select strategies based on network geometry, moving beyond one-size-fits-all solutions.

</details>


### [9] [Network Centrality Metrics Based on Unrestricted Paths, Walks and Cycles Compared to Standard Centrality Metrics](https://arxiv.org/abs/2512.11585)
*Juuso Luhtala,Vesa Kuikka,Kimmo Kaski*

Main category: cs.SI

TL;DR: 提出基于影响传播模型的新网络中心性度量，考虑所有可行路径和循环，改进传统最短路径方法的局限性


<details>
  <summary>Details</summary>
Motivation: 传统网络中心性度量（接近中心性、中介中心性）仅基于最短路径，无法准确反映网络物理/概率特性、网络流和重复传播等特性，且不同应用场景下效果不一

Method: 基于影响传播模型提出新的概率度量，考虑网络中所有可行路径和循环，定义入中心性（作为影响目标）和出中心性（作为影响源），并与传统度量通过散点图和皮尔逊相关系数进行比较

Result: 影响传播中介中心性在保持与传统中介中心性相似性的同时，能揭示替代路径的重要性

Conclusion: 提出的影响传播模型中心性度量能更好地表征网络结构，适用于多种网络科学应用场景

Abstract: A key issue with standard network measures of closeness and betweenness centrality is that they rely on the shortest paths between nodes within the network structure, whereas the degree centrality only reveals the immediate neighborhood of a node. Furthermore, many measures found in the literature do not accurately represent the physical or probabilistic characteristics of nodal centrality, network flow, and other salient properties. For example, recurrent spreading in a network is often overlooked by these metrics. Standard centrality measures have limitations, being optimal for one application but not for others. Here, we present new metrics based on our influence spreading model to characterize network structure for various network science applications. These probabilistic measures account for all feasible walks and cycles in the network. We compare our new metrics with the standard metrics in terms of the node rankings given by different centrality measures, by examining scatter plots, and by using the Pearson correlation coefficient. In the influence spreading model, we define the in-centrality measure to characterize how central a node is as a target of influence by other nodes and the out-centrality measure to characterize how central a node is as a source of influence on other nodes. Our results show that the influence spreading betweenness centrality reveals the importance of alternative routes while maintaining similarity to standard betweenness centrality.

</details>


### [10] [A Relational Model of Neighborhood Mobility: The Role of Amenities and Cultural Alignment](https://arxiv.org/abs/2512.11662)
*Thiago H Silva,Daniel Silver,Gustavo Santos,Myriam Delgado*

Main category: cs.SI

TL;DR: 研究发现邻里间的连接不仅取决于人口、经济、地理因素，还受到文化风格和设施组合的"软基础设施"影响，相似文化和设施组合的社区之间联系更紧密。


<details>
  <summary>Details</summary>
Motivation: 传统上解释邻里连接主要关注人口统计、经济和地理因素，但本研究认为城市流动性还受到文化风格和设施组合的影响，需要探索这种"软基础设施"如何塑造城市流动性。

Method: 使用约6.5亿条Google Places评论数据测量美国邮政编码区之间的共同访问模式，以及约3000万条加拿大地址变更数据追踪居民流动性，建立关系型跨国模型分析文化风格和设施组合对齐对邻里连接的影响。

Result: 具有相似文化风格和设施组合的邻里之间连接显著更强，这种效应在控制了种族、收入、教育、政治、住房成本和距离等因素后仍然存在。

Conclusion: 城市凝聚力和隔离不仅取决于谁住在哪里或邻里之间的距离，还取决于塑造城市流动性的共享文化和物质生态，文化风格和设施组合的"软基础设施"是理解城市连接的重要维度。

Abstract: Why are some neighborhoods strongly connected while others remain isolated? Although standard explanations focus on demographics, economics, and geography, movement across the city may also depend on cultural styles and amenity mix. This study proposes a relational, cross-national model in which local culture and amenity mix alignment creates a "soft infrastructure" of urban mobility, i.e., symbolic cues and functional features that shape expectations about the character of places. Using ~650 million Google Places reviews to measure co-visitation between U.S. ZIP codes and ~30 million Canadian change-of-address to track residential mobility, results show that neighborhoods with similar cultural styles and amenities are significantly more connected. These effects persist even after controlling for race, income, education, politics, housing costs, and distance. Urban cohesion and segregation depend not only on who lives where or how far apart neighborhoods are, but on the shared cultural and material ecologies that structure movement across the city.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [11] [Tekum: Balanced Ternary Tapered Precision Real Arithmetic](https://arxiv.org/abs/2512.10964)
*Laslo Hunhold*

Main category: cs.ET

TL;DR: 本文提出了一种用于平衡三值逻辑的新型实数算术格式tekum，基于锥形精度概念，在三值硬件中展现出优于现有格式的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管三值逻辑具有能效计算和突破内存墙的潜力，但实数算术在平衡三值逻辑中几乎未受关注。随着硬件进步，需要专门为三值系统设计的新型数字格式。

Method: 重新审视锥形精度算术概念（如posit和takum格式），将其应用于平衡三值逻辑，提出tekum算术格式，解决了多个基本设计挑战。

Result: 评估显示tekum格式具有高度有前景的特性，在许多方面优于posit和takum格式，为三值系统中的实数计算奠定了基础。

Conclusion: 随着三值硬件成熟，这项工作代表了释放三值系统实数计算全部潜力的关键一步，为新一代硬件设计的新型数字格式类别奠定了基础。

Abstract: In light of recent hardware advances, it is striking that real arithmetic in balanced ternary logic has received almost no attention in the literature. This is particularly surprising given ternary logic's promising properties, which could open new avenues for energy-efficient computing and offer novel strategies for overcoming the memory wall.
  This paper revisits the concept of tapered precision arithmetic, as used in posit and takum formats, and introduces a new scheme for balanced ternary logic: tekum arithmetic. Several fundamental design challenges are addressed along the way. The proposed format is evaluated and shown to exhibit highly promising characteristics. In many respects, it outperforms both posits and takums. As ternary hardware matures, this work represents a crucial step toward unlocking the full potential of real-number computation in ternary systems, laying the groundwork for a new class of number formats designed from the ground up for a new category of next-generation hardware.

</details>


### [12] [Beyond Memristor: Neuromorphic Computing Using Meminductor](https://arxiv.org/abs/2512.11002)
*Frank Zhigang Wang*

Main category: cs.ET

TL;DR: 该论文发现带有磁芯的线圈是一种记忆电感器（meminductor），其电感L(q)是电荷q的函数，能够记住通过线圈的电流历史，在神经形态计算中具有独特作用。


<details>
  <summary>Details</summary>
Motivation: 探索超越忆阻器的计算范式，研究记忆电感器在神经形态计算、深度学习和脑启发计算中的独特作用，因为神经形态RLC电路的时间常数由电感和电容共同决定，而非电阻。

Method: 通过理论分析发现带有磁芯的线圈是记忆电感器，其电感L(q)是电荷q的函数，磁芯内的磁化记录了电流历史。实验上使用新发明的记忆电感器重现了阿米巴原虫的生物行为（记忆、计时和预期机制）。

Result: 成功验证了带有磁芯的线圈确实具有记忆电感特性，能够用于重现生物系统的记忆、计时和预期行为，证明了超越忆阻器计算范式的可行性。

Conclusion: 超越忆阻器的计算范式在理论上是合理的，在实验上是可行的，记忆电感器在神经形态计算中具有独特且不可替代的作用。

Abstract: Memristor (resistor with memory), inductor with memory (meminductor) and capacitor with memory (memcapacitor) have different roles to play in novel computing architectures. We found that a coil with a magnetic core is an inductor with memory (meminductor) in terms of its inductance L(q) being a function of the charge q. The history of the current passing through the coil is remembered by the magnetization inside the magnetic core. Such a meminductor can play a unique role (that cannot be played by a memristor) in neuromorphic computing, deep learning and brain inspired since the time constant of a neuromorphic RLC circuit is jointly determined by the inductance and capacitance, rather than the resistance. As an experimental verification, this newly invented meminductor was used to reproduce the observed biological behaviour of amoebae (the memorizing, timing and anticipating mechanisms). In conclusion, a beyond memristor computing paradigm is theoretically sensible and experimentally practical.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [13] [CORL: Reinforcement Learning of MILP Policies Solved via Branch and Bound](https://arxiv.org/abs/2512.11169)
*Akhil S Anand,Elias Aarekol,Martin Mziray Dalseg,Magnus Stalhane,Sebastien Gros*

Main category: cs.AI

TL;DR: CORL框架使用强化学习端到端微调MILP方案，将分支定界求解的MILP转化为可微随机策略，以最大化实际运营性能而非精确建模。


<details>
  <summary>Details</summary>
Motivation: 传统MILP建模难以准确表示随机现实问题，导致实际性能不佳。现有机器学习方法依赖监督学习、假设可获得最优决策真值、使用MILP梯度替代，存在局限性。

Method: 提出CORL框架，将分支定界算法求解的MILP转化为可微随机策略，使其与强化学习兼容，在真实数据上端到端微调MILP方案以最大化运营性能。

Result: 在简单的组合序贯决策示例中验证了CORL方法的有效性。

Conclusion: CORL框架通过强化学习直接优化MILP的实际操作性能，为组合序贯决策问题提供了新的端到端优化方法。

Abstract: Combinatorial sequential decision making problems are typically modeled as mixed integer linear programs (MILPs) and solved via branch and bound (B&B) algorithms. The inherent difficulty of modeling MILPs that accurately represent stochastic real world problems leads to suboptimal performance in the real world. Recently, machine learning methods have been applied to build MILP models for decision quality rather than how accurately they model the real world problem. However, these approaches typically rely on supervised learning, assume access to true optimal decisions, and use surrogates for the MILP gradients. In this work, we introduce a proof of concept CORL framework that end to end fine tunes an MILP scheme using reinforcement learning (RL) on real world data to maximize its operational performance. We enable this by casting an MILP solved by B&B as a differentiable stochastic policy compatible with RL. We validate the CORL method in a simple illustrative combinatorial sequential decision making example.

</details>


### [14] [Deep Learning--Accelerated Multi-Start Large Neighborhood Search for Real-time Freight Bundling](https://arxiv.org/abs/2512.11187)
*Haohui Zhang,Wouter van Heeswijk,Xinyu Hu,Neil Yorke-Smith,Martijn Mes*

Main category: cs.AI

TL;DR: 提出一种学习加速混合搜索方法，用于在线货运交换系统的组合捆绑优化，将Transformer神经网络构造策略与多起点大邻域搜索相结合，在子秒延迟内实现高质量解决方案。


<details>
  <summary>Details</summary>
Motivation: 在线货运交换系统(OFEX)在实时匹配货主和承运商方面至关重要，但高效的运输任务组合捆绑仍然是瓶颈。现有方法难以在子秒延迟内同时处理组合捆绑选择和取送货路径规划。

Method: 将OFEX组合捆绑问题建模为多商品一对一取送货选择性旅行商问题(m1-PDSTSP)。提出学习加速混合搜索管道：结合Transformer神经网络构造策略与创新的多起点大邻域搜索(MSLNS)元启发式算法，采用滚动时域方案，平台反复将当前市场冻结为静态快照并在短时间预算内求解。

Result: 在基准测试中，该方法在解决方案质量上优于最先进的神经组合优化和元启发式基线方法，时间相当，相对于最佳可用精确基线方法，总收入的优化差距小于2%。

Conclusion: 这是首次证明基于深度神经网络的构造器能够可靠地为（多起点）改进启发式算法提供高质量种子，其适用性不仅限于m1-PDSTSP，还可扩展到更广泛的选择性旅行商问题和取送货问题类别。

Abstract: Online Freight Exchange Systems (OFEX) play a crucial role in modern freight logistics by facilitating real-time matching between shippers and carrier. However, efficient combinatorial bundling of transporation jobs remains a bottleneck. We model the OFEX combinatorial bundling problem as a multi-commodity one-to-one pickup-and-delivery selective traveling salesperson problem (m1-PDSTSP), which optimizes revenue-driven freight bundling under capacity, precedence, and route-length constraints. The key challenge is to couple combinatorial bundle selection with pickup-and-delivery routing under sub-second latency. We propose a learning--accelerated hybrid search pipeline that pairs a Transformer Neural Network-based constructive policy with an innovative Multi-Start Large Neighborhood Search (MSLNS) metaheuristic within a rolling-horizon scheme in which the platform repeatedly freezes the current marketplace into a static snapshot and solves it under a short time budget. This pairing leverages the low-latency, high-quality inference of the learning-based constructor alongside the robustness of improvement search; the multi-start design and plausible seeds help LNS to explore the solution space more efficiently. Across benchmarks, our method outperforms state-of-the-art neural combinatorial optimization and metaheuristic baselines in solution quality with comparable time, achieving an optimality gap of less than 2\% in total revenue relative to the best available exact baseline method. To our knowledge, this is the first work to establish that a Deep Neural Network-based constructor can reliably provide high-quality seeds for (multi-start) improvement heuristics, with applicability beyond the \textit{m1-PDSTSP} to a broad class of selective traveling salesperson problems and pickup and delivery problems.

</details>


### [15] [FutureWeaver: Planning Test-Time Compute for Multi-Agent Systems with Modularized Collaboration](https://arxiv.org/abs/2512.11213)
*Dongwon Jung,Peng Shi,Yi Zhang*

Main category: cs.AI

TL;DR: FutureWeaver：一个在固定预算下规划和优化多智能体系统中测试时计算分配的框架，通过模块化协作和双级规划提升性能


<details>
  <summary>Details</summary>
Motivation: 现有测试时计算扩展技术（如重复采样、自我验证、自我反思）在单智能体场景中能显著提升性能，但在多智能体系统中缺乏原则性的机制来分配计算资源以促进协作，也无法在明确预算约束下跨智能体分配计算

Method: 提出FutureWeaver框架：1）引入模块化协作，将可重用的多智能体工作流封装为可调用函数；2）通过自我对弈反思从历史轨迹中抽象出重复交互模式来自动推导模块；3）采用双级规划架构，在优化计算分配时既考虑当前任务状态又推测未来步骤

Result: 在复杂智能体基准测试上的实验表明，FutureWeaver在不同预算设置下始终优于基线方法，验证了其在推理时优化中促进多智能体协作的有效性

Conclusion: FutureWeaver成功解决了多智能体系统中测试时计算分配的挑战，通过模块化协作和前瞻性规划实现了在固定预算下的高效计算资源分配，为多智能体协作的推理时优化提供了有效框架

Abstract: Scaling test-time computation improves large language model performance without additional training. Recent work demonstrates that techniques such as repeated sampling, self-verification, and self-reflection can significantly enhance task success by allocating more inference-time compute. However, applying these techniques across multiple agents in a multi-agent system is difficult: there does not exist principled mechanisms to allocate compute to foster collaboration among agents, to extend test-time scaling to collaborative interactions, or to distribute compute across agents under explicit budget constraints. To address this gap, we propose FutureWeaver, a framework for planning and optimizing test-time compute allocation in multi-agent systems under fixed budgets. FutureWeaver introduces modularized collaboration, formalized as callable functions that encapsulate reusable multi-agent workflows. These modules are automatically derived through self-play reflection by abstracting recurring interaction patterns from past trajectories. Building on these modules, FutureWeaver employs a dual-level planning architecture that optimizes compute allocation by reasoning over the current task state while also speculating on future steps. Experiments on complex agent benchmarks demonstrate that FutureWeaver consistently outperforms baselines across diverse budget settings, validating its effectiveness for multi-agent collaboration in inference-time optimization.

</details>


### [16] [General-purpose AI models can generate actionable knowledge on agroecological crop protection](https://arxiv.org/abs/2512.11474)
*Kris A. G. Wyckhuys*

Main category: cs.AI

TL;DR: 评估DeepSeek与ChatGPT在农业生态作物保护知识生成上的表现，发现DeepSeek在文献覆盖、解决方案数量和数据一致性方面更优，但两者都存在幻觉和错误问题。


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI在农业食品科学领域的应用潜力，特别是评估大型语言模型在农业生态作物保护知识生成方面的准确性和实用性。

Method: 针对9种全球性害虫、杂草和植物病害，比较基于网络（DeepSeek）和非基于网络（ChatGPT免费版）的LLMs，评估其事实准确性、数据一致性和知识广度。

Result: DeepSeek比ChatGPT多筛选4.8-49.7倍的文献，报告1.6-2.4倍的生物防治剂或管理方案，提供21.6%更高的功效估计，数据一致性更好。但两者都存在幻觉、虚构引用、混淆命名等问题。

Conclusion: 尽管存在局限性，LLMs能够正确报告低分辨率功效趋势，在严格人工监督下可能成为支持农场决策和激发科学创造力的有力工具。

Abstract: Generative artificial intelligence (AI) offers potential for democratizing scientific knowledge and converting this to clear, actionable information, yet its application in agri-food science remains unexplored. Here, we verify the scientific knowledge on agroecological crop protection that is generated by either web-grounded or non-grounded large language models (LLMs), i.e., DeepSeek versus the free-tier version of ChatGPT. For nine globally limiting pests, weeds, and plant diseases, we assessed the factual accuracy, data consistency, and breadth of knowledge or data completeness of each LLM. Overall, DeepSeek consistently screened a 4.8-49.7-fold larger literature corpus and reported 1.6-2.4-fold more biological control agents or management solutions than ChatGPT. As a result, DeepSeek reported 21.6% higher efficacy estimates, exhibited greater laboratory-to-field data consistency, and showed more realistic effects of pest identity and management tactics. However, both models hallucinated, i.e., fabricated fictitious agents or references, reported on implausible ecological interactions or outcomes, confused old and new scientific nomenclatures, and omitted data on key agents or solutions. Despite these shortcomings, both LLMs correctly reported low-resolution efficacy trends. Overall, when paired with rigorous human oversight, LLMs may pose a powerful tool to support farm-level decision-making and unleash scientific creativity.

</details>


### [17] [A-LAMP: Agentic LLM-Based Framework for Automated MDP Modeling and Policy Generation](https://arxiv.org/abs/2512.11270)
*Hong Je-Gal,Chan-Bin Yi,Hyun-Suk Lee*

Main category: cs.AI

TL;DR: A-LAMP：基于LLM的自动化框架，可将自然语言任务描述自动转换为MDP建模、环境实现和策略训练，提高策略生成能力并保持语义对齐。


<details>
  <summary>Details</summary>
Motivation: 将强化学习应用于实际任务时，需要将非正式描述转换为形式化的MDP、实现可执行环境并训练策略代理。这一过程存在建模错误、代码脆弱和目标不对齐等挑战，阻碍策略训练。

Method: 提出A-LAMP框架，基于智能体化的大语言模型，将建模、编码和训练分解为可验证的阶段，确保整个流程的语义对齐。包括轻量级变体版本。

Result: 在经典控制和自定义RL领域中，A-LAMP始终比单个最先进的LLM模型具有更高的策略生成能力。其轻量级变体也能接近更大模型的性能。失败分析揭示了改进原因。

Conclusion: A-LAMP框架能够自动生成保持任务最优性的环境和策略，证实了其正确性和可靠性，为自动化RL任务建模和策略生成提供了有效解决方案。

Abstract: Applying reinforcement learning (RL) to real-world tasks requires converting informal descriptions into a formal Markov decision process (MDP), implementing an executable environment, and training a policy agent. Automating this process is challenging due to modeling errors, fragile code, and misaligned objectives, which often impede policy training. We introduce an agentic large language model (LLM)-based framework for automated MDP modeling and policy generation (A-LAMP), that automatically translates free-form natural language task descriptions into an MDP formulation and trained policy. The framework decomposes modeling, coding, and training into verifiable stages, ensuring semantic alignment throughout the pipeline. Across both classic control and custom RL domains, A-LAMP consistently achieves higher policy generation capability than a single state-of-the-art LLM model. Notably, even its lightweight variant, which is built on smaller language models, approaches the performance of much larger models. Failure analysis reveals why these improvements occur. In addition, a case study also demonstrates that A-LAMP generates environments and policies that preserve the task's optimality, confirming its correctness and reliability.

</details>


### [18] [TriFlow: A Progressive Multi-Agent Framework for Intelligent Trip Planning](https://arxiv.org/abs/2512.11271)
*Yuxing Chen,Basem Suleiman,Qifan Chen*

Main category: cs.AI

TL;DR: TriFlow是一个渐进式多智能体框架，通过检索、规划和治理三阶段流水线，将结构化推理与语言灵活性相结合，解决现实世界行程规划中的约束满足问题，在效率和准确性上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界的行程规划需要将开放式用户请求转换为可执行的行程安排，同时满足严格的空间、时间和预算约束，并与用户偏好保持一致。现有的基于LLM的智能体在约束满足、工具协调和效率方面存在困难，经常产生不可行或成本过高的计划。

Method: TriFlow采用渐进式多智能体框架，通过三阶段流水线：检索、规划和治理。该设计逐步缩小搜索空间，通过规则-LLM协作组装约束一致的行程，并进行有界迭代优化以确保全局可行性和个性化。

Result: 在TravelPlanner和TripTailor基准测试中取得了最先进的结果，分别达到91.1%和97%的最终通过率，与当前SOTA相比实现了超过10倍的运行时间效率提升。

Conclusion: TriFlow通过结合结构化推理和语言灵活性，有效解决了现实世界行程规划中的约束满足问题，在准确性和效率方面显著优于现有方法，为复杂约束下的个性化行程规划提供了可行解决方案。

Abstract: Real-world trip planning requires transforming open-ended user requests into executable itineraries under strict spatial, temporal, and budgetary constraints while aligning with user preferences. Existing LLM-based agents struggle with constraint satisfaction, tool coordination, and efficiency, often producing infeasible or costly plans. To address these limitations, we present TriFlow, a progressive multi-agent framework that unifies structured reasoning and language-based flexibility through a three-stage pipeline of retrieval, planning, and governance. By this design, TriFlow progressively narrows the search space, assembles constraint-consistent itineraries via rule-LLM collaboration, and performs bounded iterative refinement to ensure global feasibility and personalisation. Evaluations on TravelPlanner and TripTailor benchmarks demonstrated state-of-the-art results, achieving 91.1% and 97% final pass rates, respectively, with over 10x runtime efficiency improvement compared to current SOTA.

</details>


### [19] [CAPTURE: A Benchmark and Evaluation for LVLMs in CAPTCHA Resolving](https://arxiv.org/abs/2512.11323)
*Jianyi Zhang,Ziyin Zhou,Xu Ji,Shizhao Liu,Zhangchi Zhao*

Main category: cs.AI

TL;DR: 论文提出了首个专门针对大型视觉语言模型（LVLMs）的CAPTCHA基准测试CAPTURE，涵盖4大类25子类CAPTCHA，评估显示现有LVLMs在解决验证码方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉验证码的基准测试存在局限性，无法全面覆盖所有验证码类型，且缺乏专门针对LVLMs的专用基准。需要构建一个全面、多样化的基准来评估LVLMs解决验证码的能力。

Method: 创建了CAPTURE基准测试，包含4种主要验证码类型和25个子类型，来自31个供应商。基准具有广泛的类别多样性、大规模数据以及专门为LVLMs定制的标签。

Result: 使用该基准测试评估当前LVLMs时，发现它们在解决验证码方面表现不佳，表明现有模型在验证码识别能力上存在不足。

Conclusion: CAPTURE基准填补了先前研究在数据全面性和标签针对性方面的空白，为LVLMs的验证码解决能力提供了多维度的全面评估框架。

Abstract: Benefiting from strong and efficient multi-modal alignment strategies, Large Visual Language Models (LVLMs) are able to simulate human visual and reasoning capabilities, such as solving CAPTCHAs. However, existing benchmarks based on visual CAPTCHAs still face limitations. Previous studies, when designing benchmarks and datasets, customized them according to their research objectives. Consequently, these benchmarks cannot comprehensively cover all CAPTCHA types. Notably, there is a dearth of dedicated benchmarks for LVLMs. To address this problem, we introduce a novel CAPTCHA benchmark for the first time, named CAPTURE CAPTCHA for Testing Under Real-world Experiments, specifically for LVLMs. Our benchmark encompasses 4 main CAPTCHA types and 25 sub-types from 31 vendors. The diversity enables a multi-dimensional and thorough evaluation of LVLM performance. CAPTURE features extensive class variety, large-scale data, and unique LVLM-tailored labels, filling the gaps in previous research in terms of data comprehensiveness and labeling pertinence. When evaluated by this benchmark, current LVLMs demonstrate poor performance in solving CAPTCHAs.

</details>


### [20] [Towards Trustworthy Multi-Turn LLM Agents via Behavioral Guidance](https://arxiv.org/abs/2512.11421)
*Gonca Gürsun*

Main category: cs.AI

TL;DR: 提出一个LLM智能体框架，通过强化学习形式化环境描述，结合任务分析、推理和生成模块，确保多轮任务中的可靠和可验证行为


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在推理和生成方面表现出色，但在多轮任务中的行为往往缺乏可靠性和可验证性，需要一种能够提供明确行为指导的框架

Method: 框架包含三个组件：轻量级任务分析器选择推理和生成策略；推理模块学习可验证的观察-动作映射；生成模块通过验证或确定性合成确保约束合规输出

Result: 随着智能体与环境交互，这些组件共同演化，产生可信赖的行为

Conclusion: 该框架使基于LLM的智能体能够在具有明确定义观察、动作和奖励信号的强化学习形式化环境中，按照明确的行为指导行动，提高任务完成的可信度

Abstract: Large Language Models demonstrate strong reasoning and generation abilities, yet their behavior in multi-turn tasks often lacks reliability and verifiability. We present a task completion framework that enables LLM-based agents to act under explicit behavioral guidance in environments described by reinforcement learning formalisms with defined observation, action, and reward signals.
  The framework integrates three components: a lightweight task profiler that selects reasoning and generation strategies, a reasoning module that learns verifiable observation - action mappings, and a generation module that enforces constraint-compliant outputs through validation or deterministic synthesis. We show that as the agent interacts with the environment, these components co-evolve, yielding trustworthy behavior.

</details>


### [21] [AgentBalance: Backbone-then-Topology Design for Cost-Effective Multi-Agent Systems under Budget Constraints](https://arxiv.org/abs/2512.11426)
*Shuowei Cai,Yansong Ning,Hao Liu*

Main category: cs.AI

TL;DR: AgentBalance是一个在明确token成本和延迟预算下构建成本效益多智能体系统的框架，采用"先骨干后拓扑"设计，相比现有方法在相同预算下获得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 基于LLM的多智能体系统已成为网络规模应用的关键组件，但现有方法很少在明确的token成本和延迟预算下进行建模和优化，导致预算约束时成本效益不佳。

Method: 采用"先骨干后拓扑"设计：1)骨干导向的智能体生成（LLM池构建、池选择、角色-骨干匹配）；2)自适应MAS拓扑生成（智能体表示学习、门控、延迟感知拓扑合成）。

Result: 在14个候选LLM骨干的基准测试中，AgentBalance在匹配的token成本预算下获得高达10%的性能提升，在延迟预算下获得22%提升，并在性能-预算曲线上表现出强AUC。

Conclusion: AgentBalance能够有效构建预算感知的多智能体系统，可作为现有MAS的插件提升性能，并能很好地泛化到未见过的LLM，实现实用的预算感知部署。

Abstract: Large Language Model (LLM)-based multi-agent systems (MAS) are becoming indispensable building blocks for web-scale applications such as web search, social network analytics, and online customer support, where cost-effectiveness is increasingly the primary constraint for large-scale deployment. While recent work improves MAS cost-effectiveness by shaping inter-agent communication topologies and selecting agent backbones, it rarely models and optimizes under explicit token-cost and latency budgets that reflect deployment constraints. This often leads to topology-first designs and suboptimal cost-effectiveness when budgets are binding. We present AgentBalance, a framework for constructing cost-effective MAS under explicit token-cost and latency budgets via a backbone-then-topology design. AgentBalance first performs backbone-oriented agent generation, constructing agents with heterogeneous backbones through LLM pool construction, pool selection, and role-backbone matching. It then performs adaptive MAS topology generation, guiding inter-agent communication via agent representation learning, gating, and latency-aware topology synthesis. Experiments on benchmarks with 14 candidate LLM backbones show that AgentBalance achieves up to 10% and 22% performance gains under matched token-cost and latency budgets, respectively, and yields strong AUC on performance-versus-budget curves across benchmarks. AgentBalance also functions as a plug-in for existing MAS, improving performance under the same token-cost and latency constraints, and it generalizes well to unseen LLMs for practical, budget-aware deployment. Code: https://github.com/usail-hkust/AgentBalance

</details>


### [22] [Back to the Baseline: Examining Baseline Effects on Explainability Metrics](https://arxiv.org/abs/2512.11433)
*Agustin Martin Picard,Thibaut Boissin,Varshini Subhash,Rémi Cadène,Thomas Fel*

Main category: cs.AI

TL;DR: 该论文指出当前XAI中基于基线的保真度评估指标存在根本问题，不同基线选择会偏好不同归因方法，并提出了一种新的模型依赖基线来解决信息移除与分布外样本的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前XAI中广泛使用的归因方法评估指标（如插入和删除）依赖于基线选择，但不同基线会偏好不同的归因方法，导致评估结果不一致且存在偏差，这引发了"应该使用哪个基线"的根本问题。

Method: 首先分析现有基线的两个理想属性：信息移除能力和避免产生过度分布外图像。然后基于特征可视化技术，提出一种新的模型依赖基线，该基线能够移除信息同时不产生过度分布外图像，从而改善现有基线的权衡问题。

Result: 研究发现现有基线都无法同时满足两个理想属性，存在信息移除与分布外图像的权衡。提出的新基线在权衡方面优于现有基线，能够更好地评估归因方法。

Conclusion: 基线选择对XAI评估有重大影响，需要更严谨的基线设计。提出的模型依赖基线为解决信息移除与分布外图像的权衡提供了改进方案，为更可靠的归因方法评估奠定了基础。

Abstract: Attribution methods are among the most prevalent techniques in Explainable Artificial Intelligence (XAI) and are usually evaluated and compared using Fidelity metrics, with Insertion and Deletion being the most popular. These metrics rely on a baseline function to alter the pixels of the input image that the attribution map deems most important. In this work, we highlight a critical problem with these metrics: the choice of a given baseline will inevitably favour certain attribution methods over others. More concerningly, even a simple linear model with commonly used baselines contradicts itself by designating different optimal methods. A question then arises: which baseline should we use? We propose to study this problem through two desirable properties of a baseline: (i) that it removes information and (ii) that it does not produce overly out-of-distribution (OOD) images. We first show that none of the tested baselines satisfy both criteria, and there appears to be a trade-off among current baselines: either they remove information or they produce a sequence of OOD images. Finally, we introduce a novel baseline by leveraging recent work in feature visualisation to artificially produce a model-dependent baseline that removes information without being overly OOD, thus improving on the trade-off when compared to other existing baselines. Our code is available at https://github.com/deel-ai-papers/Back-to-the-Baseline

</details>


### [23] [Motif-2-12.7B-Reasoning: A Practitioner's Guide to RL Training Recipes](https://arxiv.org/abs/2512.11463)
*Junghwan Lim,Sungmin Lee,Dongseok Kim,Taehyun Kim,Eunhwan Park,Jeesoo Lee,Jeongdoo Lee,Junhyeok Lee,Wai Ting Cheung,Dahye Choi,Minsu Ha,Jaeheui Her,Jaeyeon Huh,Hanbin Jung,Changjin Kang,Beomgyu Kim,Minjae Kim,Taewhan Kim,Youngrok Kim,Hyukjin Kweon,Haesol Lee,Kungyu Lee,Dongpin Oh,Yeongjae Park,Bokki Ryu,Dongjoo Weon*

Main category: cs.AI

TL;DR: Motif-2-12.7B-Reasoning是一个12.7B参数的语言模型，通过创新的训练方法在复杂推理和长上下文理解方面达到接近前沿专有模型的性能，同时提供了可复现的训练方案。


<details>
  <summary>Details</summary>
Motivation: 解决开源模型与专有前沿模型在复杂推理和长上下文理解方面的性能差距，同时解决推理适应训练中常见的模型崩溃和训练不稳定性问题。

Method: 采用综合训练方案：1) 内存高效的基础设施支持64K令牌上下文；2) 两阶段监督微调课程，使用经过验证的对齐合成数据；3) 强化学习微调管道，通过难度感知数据过滤和混合策略轨迹重用来稳定训练。

Result: Motif-2-12.7B-Reasoning在数学、编码和代理基准测试中取得了与参数数量显著更大的模型相当的性能，在现实计算约束下提供了具有竞争力的开源模型。

Conclusion: 该研究不仅提供了一个高性能的开源推理模型，还提供了一个实用的训练蓝图，展示了在有限计算资源下如何有效扩展模型的推理能力。

Abstract: We introduce Motif-2-12.7B-Reasoning, a 12.7B parameter language model designed to bridge the gap between open-weight systems and proprietary frontier models in complex reasoning and long-context understanding. Addressing the common challenges of model collapse and training instability in reasoning adaptation, we propose a comprehensive, reproducible training recipe spanning system, data, and algorithmic optimizations. Our approach combines memory-efficient infrastructure for 64K-token contexts using hybrid parallelism and kernel-level optimizations with a two-stage Supervised Fine-Tuning (SFT) curriculum that mitigates distribution mismatch through verified, aligned synthetic data. Furthermore, we detail a robust Reinforcement Learning Fine-Tuning (RLFT) pipeline that stabilizes training via difficulty-aware data filtering and mixed-policy trajectory reuse. Empirical results demonstrate that Motif-2-12.7B-Reasoning achieves performance comparable to models with significantly larger parameter counts across mathematics, coding, and agentic benchmarks, offering the community a competitive open model and a practical blueprint for scaling reasoning capabilities under realistic compute constraints.

</details>


### [24] [Three methods, one problem: Classical and AI approaches to no-three-in-line](https://arxiv.org/abs/2512.11469)
*Pranav Ramanathan,Thomas Prellberg,Matthew Lewis,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.AI

TL;DR: 本文首次系统比较了经典优化方法与AI方法在No-Three-In-Line问题上的表现，发现ILP在19×19网格内能获得最优解，PatternBoost在14×14网格内匹配最优性能，PPO在10×10网格内完美但11×11失败。


<details>
  <summary>Details</summary>
Motivation: No-Three-In-Line是组合几何中的著名问题，传统ILP方法能保证最优解但面临指数级扩展问题，而机器学习方法为模式近似提供了有前景的替代方案，需要系统比较两者的性能。

Method: 应用PatternBoost transformer学习和强化学习（PPO）首次解决该问题，并与整数线性规划（ILP）进行对比评估。

Result: ILP在19×19网格内获得可证明的最优解；PatternBoost在14×14网格内匹配最优性能，测试损失减少96%；PPO在10×10网格内获得完美解但在11×11网格失败（约束违反）。

Conclusion: 经典优化方法对于精确解仍然必要，AI方法在较小实例上具有竞争力，混合方法为扩展到更大问题规模提供了最有前景的方向。

Abstract: The No-Three-In-Line problem asks for the maximum number of points that can be placed on an n by n grid with no three collinear, representing a famous problem in combinatorial geometry. While classical methods like Integer Linear Programming (ILP) guarantee optimal solutions, they face exponential scaling with grid size, and recent advances in machine learning offer promising alternatives for pattern-based approximation. This paper presents the first systematic comparison of classical optimization and AI approaches to this problem, evaluating their performance against traditional algorithms. We apply PatternBoost transformer learning and reinforcement learning (PPO) to this problem for the first time, comparing them against ILP. ILP achieves provably optimal solutions up to 19 by 19 grids, while PatternBoost matches optimal performance up to 14 by 14 grids with 96% test loss reduction. PPO achieves perfect solutions on 10 by 10 grids but fails at 11 by 11 grids, where constraint violations prevent valid configurations. These results demonstrate that classical optimization remains essential for exact solutions while AI methods offer competitive performance on smaller instances, with hybrid approaches presenting the most promising direction for scaling to larger problem sizes.

</details>


### [25] [BAID: A Benchmark for Bias Assessment of AI Detectors](https://arxiv.org/abs/2512.11505)
*Priyam Basu,Yunfeng Zhang,Vipul Raheja*

Main category: cs.AI

TL;DR: BAID框架系统评估AI文本检测器在人口统计、年龄、教育水平、方言、正式程度、政治倾向和主题等7个维度的偏见，发现检测器对少数群体文本的召回率偏低。


<details>
  <summary>Details</summary>
Motivation: 现有AI文本检测器在教育和工作场景中广泛使用，但先前研究仅发现针对英语学习者的孤立偏见案例，缺乏对更广泛社会语言因素的系统性评估。

Method: 提出BAID评估框架，构建超过20万个样本，涵盖7个主要类别：人口统计、年龄、教育水平、方言、正式程度、政治倾向和主题。为每个样本生成合成版本，精心设计提示以保留原始内容同时反映特定子群体的写作风格。

Result: 评估4个开源最先进的AI文本检测器，发现检测性能存在一致差异，特别是对来自代表性不足群体的文本召回率较低。

Conclusion: BAID提供了一个可扩展、透明的AI检测器审计方法，强调在这些工具公开部署前需要进行偏见感知的评估。

Abstract: AI-generated text detectors have recently gained adoption in educational and professional contexts. Prior research has uncovered isolated cases of bias, particularly against English Language Learners (ELLs) however, there is a lack of systematic evaluation of such systems across broader sociolinguistic factors. In this work, we propose BAID, a comprehensive evaluation framework for AI detectors across various types of biases. As a part of the framework, we introduce over 200k samples spanning 7 major categories: demographics, age, educational grade level, dialect, formality, political leaning, and topic. We also generated synthetic versions of each sample with carefully crafted prompts to preserve the original content while reflecting subgroup-specific writing styles. Using this, we evaluate four open-source state-of-the-art AI text detectors and find consistent disparities in detection performance, particularly low recall rates for texts from underrepresented groups. Our contributions provide a scalable, transparent approach for auditing AI detectors and emphasize the need for bias-aware evaluation before these tools are deployed for public use.

</details>


### [26] [EmeraldMind: A Knowledge Graph-Augmented Framework for Greenwashing Detection](https://arxiv.org/abs/2512.11506)
*Georgios Kaoukis,Ioannis Aris Koufopoulos,Psaroudaki Eleni,Danae Pla Karidi,Evaggelia Pitoura,George Papastefanatos,Panayiotis Tsaparas*

Main category: cs.AI

TL;DR: EmeraldMind是一个基于知识图谱和检索增强生成的事实中心框架，用于自动化检测企业绿色洗白（greenwashing）行为，相比通用大语言模型具有更好的准确性、覆盖范围和解释质量。


<details>
  <summary>Details</summary>
Motivation: 随着AI和网络代理在决策中日益普及，设计既能支持可持续发展又能防范错误信息的智能系统至关重要。绿色洗白（误导性的企业可持续发展声明）对环境进展构成重大挑战。

Method: EmeraldMind框架整合了领域特定知识图谱与检索增强生成技术。它从多样化的企业ESG报告中构建EmeraldGraph知识图谱，提供可验证的证据支持，帮助大语言模型进行声明评估。该框架提供基于证据的透明分类，并在无法验证声明时负责任地弃权。

Result: 在新构建的绿色洗白声明数据集上的实验表明，EmeraldMind相比通用大语言模型实现了竞争性的准确性、更大的覆盖范围和更优的解释质量，且无需微调或重新训练。

Conclusion: EmeraldMind通过整合领域特定知识图谱和检索增强生成，为自动化检测企业绿色洗白行为提供了一个有效的事实中心框架，能够提供透明、基于证据的评估，支持可持续发展决策。

Abstract: As AI and web agents become pervasive in decision-making, it is critical to design intelligent systems that not only support sustainability efforts but also guard against misinformation. Greenwashing, i.e., misleading corporate sustainability claims, poses a major challenge to environmental progress. To address this challenge, we introduce EmeraldMind, a fact-centric framework integrating a domain-specific knowledge graph with retrieval-augmented generation to automate greenwashing detection. EmeraldMind builds the EmeraldGraph from diverse corporate ESG (environmental, social, and governance) reports, surfacing verifiable evidence, often missing in generic knowledge bases, and supporting large language models in claim assessment. The framework delivers justification-centric classifications, presenting transparent, evidence-backed verdicts and abstaining responsibly when claims cannot be verified. Experiments on a new greenwashing claims dataset demonstrate that EmeraldMind achieves competitive accuracy, greater coverage, and superior explanation quality compared to generic LLMs, without the need for fine-tuning or retraining.

</details>


### [27] [AI-MASLD Metabolic Dysfunction and Information Steatosis of Large Language Models in Unstructured Clinical Narratives](https://arxiv.org/abs/2512.11544)
*Yuan Shen,Xiaojun Wu,Linghua Yu*

Main category: cs.AI

TL;DR: 研究通过模拟真实临床场景，发现主流大语言模型在处理含噪声的医疗主诉时会出现类似代谢功能障碍的功能缺陷，提出"AI-MASLD"概念，警告医疗AI应用需专家监督。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型从含噪声和冗余的患者主诉中提取核心医疗信息的能力，验证其是否表现出类似代谢功能障碍相关脂肪肝病(MASLD)的功能衰退。

Method: 采用基于标准化医疗探针的横断面分析设计，选取GPT-4o、Gemini 2.5、DeepSeek 3.1、Qwen3-Max四种主流LLM。使用包含5个核心维度、20个医疗探针的评估系统模拟真实临床沟通环境，由临床专家定义黄金标准答案，并由两名独立临床医生进行双盲逆向评分。

Result: 所有测试模型均表现出不同程度的功能缺陷，Qwen3-Max整体表现最佳，Gemini 2.5最差。极端噪声条件下大多数模型出现功能崩溃。GPT-4o在深静脉血栓继发肺栓塞风险评估中做出严重误判。

Conclusion: 首次实证确认LLM在处理临床信息时表现出类似代谢功能障碍的特征，提出"AI-MASLD"创新概念。研究为医疗AI应用提供重要安全警示，强调当前LLM必须在人类专家监督下作为辅助工具使用，其理论知识与实际临床应用仍有显著差距。

Abstract: This study aims to simulate real-world clinical scenarios to systematically evaluate the ability of Large Language Models (LLMs) to extract core medical information from patient chief complaints laden with noise and redundancy, and to verify whether they exhibit a functional decline analogous to Metabolic Dysfunction-Associated Steatotic Liver Disease (MASLD). We employed a cross-sectional analysis design based on standardized medical probes, selecting four mainstream LLMs as research subjects: GPT-4o, Gemini 2.5, DeepSeek 3.1, and Qwen3-Max. An evaluation system comprising twenty medical probes across five core dimensions was used to simulate a genuine clinical communication environment. All probes had gold-standard answers defined by clinical experts and were assessed via a double-blind, inverse rating scale by two independent clinicians. The results show that all tested models exhibited functional defects to varying degrees, with Qwen3-Max demonstrating the best overall performance and Gemini 2.5 the worst. Under conditions of extreme noise, most models experienced a functional collapse. Notably, GPT-4o made a severe misjudgment in the risk assessment for pulmonary embolism (PE) secondary to deep vein thrombosis (DVT). This research is the first to empirically confirm that LLMs exhibit features resembling metabolic dysfunction when processing clinical information, proposing the innovative concept of "AI-Metabolic Dysfunction-Associated Steatotic Liver Disease (AI-MASLD)". These findings offer a crucial safety warning for the application of Artificial Intelligence (AI) in healthcare, emphasizing that current LLMs must be used as auxiliary tools under human expert supervision, as there remains a significant gap between their theoretical knowledge and practical clinical application.

</details>


### [28] [AI Benchmark Democratization and Carpentry](https://arxiv.org/abs/2512.11588)
*Gregor von Laszewski,Wesley Brewer,Jeyan Thiyagalingam,Juri Papay,Armstrong Foundjem,Piotr Luszczek,Murali Emani,Shirley V. Moore,Vijay Janapa Reddi,Matthew D. Sinclair,Sebastian Lobentanzer,Sujata Goswami,Benjamin Hawks,Marco Colombo,Nhan Tran,Christine R. Kirkpatrick,Abdulkareem Alsudais,Gregg Barrett,Tianhao Li,Kirsten Morehouse,Shivaram Venkataraman,Rutwik Jain,Kartik Mathur,Victor Lu,Tejinder Singh,Khojasteh Z. Mirza,Kongtao Chen,Sasidhar Kunapuli,Gavin Farrell,Renato Umeton,Geoffrey C. Fox*

Main category: cs.AI

TL;DR: 论文主张AI基准测试需要从静态转向动态自适应框架，并培养"AI基准测试工艺"专业人才，以解决当前基准测试与现实部署之间的差距。


<details>
  <summary>Details</summary>
Motivation: 当前AI基准测试面临多重挑战：模型架构快速演进、规模扩大、数据集变化、部署环境多样，导致静态基准容易被大型语言模型记忆，造成基准测试结果与现实性能脱节。传统基准测试强调顶级硬件上的峰值性能，对多样化现实场景指导有限。

Method: 提出建立动态自适应基准测试框架，纳入演进模型、更新数据和异构平台，同时保持透明度、可复现性和可解释性。倡导"AI基准测试工艺"教育和技能培养，通过技术创新和系统教育相结合实现基准测试民主化。

Result: 识别出关键障碍包括高资源需求、专用硬件访问有限、基准设计专业知识缺乏、结果与应用领域关联不确定性。基于MLCommons、DOE万亿参数联盟等实践经验，提出社区努力可为AI基准测试工艺提供基础。

Conclusion: 动态包容性基准测试能确保评估跟上AI演进步伐，支持负责任、可复现和可访问的AI部署。基准测试应支持应用相关比较，实现知情、上下文敏感的决策，需要技术革新和系统教育相结合。

Abstract: Benchmarks are a cornerstone of modern machine learning, enabling reproducibility, comparison, and scientific progress. However, AI benchmarks are increasingly complex, requiring dynamic, AI-focused workflows. Rapid evolution in model architectures, scale, datasets, and deployment contexts makes evaluation a moving target. Large language models often memorize static benchmarks, causing a gap between benchmark results and real-world performance.
  Beyond traditional static benchmarks, continuous adaptive benchmarking frameworks are needed to align scientific assessment with deployment risks. This calls for skills and education in AI Benchmark Carpentry. From our experience with MLCommons, educational initiatives, and programs like the DOE's Trillion Parameter Consortium, key barriers include high resource demands, limited access to specialized hardware, lack of benchmark design expertise, and uncertainty in relating results to application domains. Current benchmarks often emphasize peak performance on top-tier hardware, offering limited guidance for diverse, real-world scenarios.
  Benchmarking must become dynamic, incorporating evolving models, updated data, and heterogeneous platforms while maintaining transparency, reproducibility, and interpretability. Democratization requires both technical innovation and systematic education across levels, building sustained expertise in benchmark design and use. Benchmarks should support application-relevant comparisons, enabling informed, context-sensitive decisions. Dynamic, inclusive benchmarking will ensure evaluation keeps pace with AI evolution and supports responsible, reproducible, and accessible AI deployment. Community efforts can provide a foundation for AI Benchmark Carpentry.

</details>


### [29] [Causal Inference in Energy Demand Prediction](https://arxiv.org/abs/2512.11653)
*Chutian Ma,Grigorii Pomazkin,Giacinto Paolo Saggese,Paul Smith*

Main category: cs.AI

TL;DR: 该论文提出了一种基于结构因果模型的能源需求预测方法，利用因果洞察作为先验知识构建贝叶斯模型，在测试集上取得了3.84%的MAPE，表现出优越的预测性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 能源需求预测对电网运营商、工业能源消费者和服务提供商至关重要。能源需求受多种因素影响，包括天气条件（温度、湿度、风速、太阳辐射）和日历信息（小时、月份），这些因素因果相互依赖，使得问题比简单的基于相关性的学习技术更为复杂。

Method: 提出结构因果模型来解释变量间的因果关系，通过全面分析验证因果信念。然后利用学到的因果洞察作为先验知识构建贝叶斯模型，在未见数据上进行训练和测试。

Result: 模型在测试集上取得了3.84%的MAPE，表现出优越性能。跨两年数据的交叉验证平均MAPE为3.88%，显示了强鲁棒性。因果分析揭示了重要洞察：能源需求对温度波动的响应具有季节依赖性敏感性，冬季能源需求方差较低，因为温度变化与日常活动模式之间存在解耦效应。

Conclusion: 该研究证明了将因果洞察作为先验知识整合到贝叶斯模型中的有效性，能够显著提升能源需求预测的准确性和鲁棒性，为复杂因果依赖关系的建模提供了有效方法。

Abstract: Energy demand prediction is critical for grid operators, industrial energy
  consumers, and service providers. Energy demand is influenced by multiple
  factors, including weather conditions (e.g. temperature, humidity, wind
  speed, solar radiation), and calendar information (e.g. hour of day and
  month of year), which further affect daily work and life schedules. These
  factors are causally interdependent, making the problem more complex than
  simple correlation-based learning techniques satisfactorily allow for. We
  propose a structural causal model that explains the causal relationship
  between these variables. A full analysis is performed to validate our causal
  beliefs, also revealing important insights consistent with prior studies.
  For example, our causal model reveals that energy demand responds to
  temperature fluctuations with season-dependent sensitivity. Additionally, we
  find that energy demand exhibits lower variance in winter due to the
  decoupling effect between temperature changes and daily activity patterns.
  We then build a Bayesian model, which takes advantage of the causal insights
  we learned as prior knowledge. The model is trained and tested on unseen
  data and yields state-of-the-art performance in the form of a 3.84 percent MAPE on
  the test set. The model also demonstrates strong robustness, as the
  cross-validation across two years of data yields an average MAPE of 3.88 percent.

</details>


### [30] [MedAI: Evaluating TxAgent's Therapeutic Agentic Reasoning in the NeurIPS CURE-Bench Competition](https://arxiv.org/abs/2512.11682)
*Tim Cofala,Christian Kalfar,Jingge Xiao,Johanna Schrader,Michelle Tang,Wolfgang Nejdl*

Main category: cs.AI

TL;DR: TxAgent：基于检索增强生成的医疗决策AI代理，在CURE-Bench挑战赛中获得卓越奖，通过改进工具检索策略提升治疗推理性能


<details>
  <summary>Details</summary>
Motivation: 临床医疗决策是高风险领域，需要AI在患者特征、疾病过程和药理因素复杂交互中提供可靠指导。现有通用RAG系统难以满足医疗领域对推理过程和工具调用准确性的严格安全要求。

Method: 开发TxAgent医疗AI代理，使用微调的Llama-3.1-8B模型，通过动态生成和执行函数调用来访问统一的生物医学工具套件（ToolUniverse），整合FDA Drug API、OpenTargets和Monarch资源，确保获取最新治疗信息。

Result: 在CURE-Bench NeurIPS 2025挑战赛中，分析了工具检索质量对模型性能的影响，展示了通过改进工具检索策略实现的性能提升，获得了开放科学卓越奖。

Conclusion: 医疗AI代理需要将推理过程和工具使用行为作为显式监督信号进行评估，改进工具检索策略能显著提升治疗推理系统的整体性能，为临床决策支持提供更可靠的基础。

Abstract: Therapeutic decision-making in clinical medicine constitutes a high-stakes domain in which AI guidance interacts with complex interactions among patient characteristics, disease processes, and pharmacological agents. Tasks such as drug recommendation, treatment planning, and adverse-effect prediction demand robust, multi-step reasoning grounded in reliable biomedical knowledge. Agentic AI methods, exemplified by TxAgent, address these challenges through iterative retrieval-augmented generation (RAG). TxAgent employs a fine-tuned Llama-3.1-8B model that dynamically generates and executes function calls to a unified biomedical tool suite (ToolUniverse), integrating FDA Drug API, OpenTargets, and Monarch resources to ensure access to current therapeutic information. In contrast to general-purpose RAG systems, medical applications impose stringent safety constraints, rendering the accuracy of both the reasoning trace and the sequence of tool invocations critical. These considerations motivate evaluation protocols treating token-level reasoning and tool-usage behaviors as explicit supervision signals. This work presents insights derived from our participation in the CURE-Bench NeurIPS 2025 Challenge, which benchmarks therapeutic-reasoning systems using metrics that assess correctness, tool utilization, and reasoning quality. We analyze how retrieval quality for function (tool) calls influences overall model performance and demonstrate performance gains achieved through improved tool-retrieval strategies. Our work was awarded the Excellence Award in Open Science. Complete information can be found at https://curebench.ai/.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [31] [Robust Two-Sample Mean Inference under Serial Dependence](https://arxiv.org/abs/2512.11259)
*Ulrich Hounyo,Min Seong Kim*

Main category: econ.EM

TL;DR: 提出稳健的时间序列两样本均值检验方法，适用于结构断点、处理-控制比较和面板数据等多种应用场景


<details>
  <summary>Details</summary>
Motivation: 传统两样本检验方法在时间序列数据中面临异质性和非参数依赖结构的挑战，需要开发能够处理这些复杂情况的稳健检验方法

Method: 1. 序列HAR两样本t检验：使用正交基投影进行标准化，确保在异质性和非参数依赖结构下的有效推断；2. Welch型t近似：调整自由度以考虑序列间的长期方差异质性；3. 序列HAR野生自助法检验：将传统野生自助法扩展到时间序列设置，避免重采样观测块

Result: 提出的方法在有限样本中表现出优越性能，自助法避免了观测块重采样，提供了更准确的推断

Conclusion: 开发了一套稳健的时间序列两样本检验框架，能够有效处理异质性和复杂依赖结构，为结构断点检测、处理效应评估等应用提供了可靠的工具

Abstract: We propose robust two-sample tests for comparing means in time series. The framework accommodates a wide range of applications, including structural breaks, treatment-control comparisons, and group-averaged panel data. We first consider series HAR two-sample t-tests, where standardization employs orthonormal basis projections, ensuring valid inference under heterogeneity and nonparametric dependence structures. We propose a Welch-type t-approximation with adjusted degrees of freedom to account for long-run variance heterogeneity across the series. We further develop a series-based HAR wild bootstrap test, extending traditional wild bootstrap methods to the time-series setting. Our bootstrap avoids resampling blocks of observations and delivers superior finite-sample performance.

</details>


### [32] [Testing Parametric Distribution Family Assumptions via Differences in Differential Entropy](https://arxiv.org/abs/2512.11305)
*Ron Mittelhammer,George Judge,Miguel Henry*

Main category: econ.EM

TL;DR: 提出一种基于差分熵差异的统计检验方法，用于判断数据样本来自哪个参数分布族，无需调参且计算高效


<details>
  <summary>Details</summary>
Motivation: 需要一种广泛适用的统计方法来检验数据来自哪个参数分布族，现有方法可能受限于特定分布或需要复杂调参

Method: 使用差分熵差异检验：比较零假设下基于MLE的差分熵估计与基于非参数自助法核密度估计的差分熵，用其差异作为模型拟合的信息论度量

Result: 该方法基于最大似然估计、自助法和核密度估计原理，具有渐近有效性，适用于广泛的分布族，实现简单且计算高效

Conclusion: DDE检验提供了一个统一的框架，无需调参或特殊正则条件，可用于检验数据来自哪个参数分布族

Abstract: We introduce a broadly applicable statistical procedure for testing which parametric distribution family generated a random sample of data. The method, termed the Difference in Differential Entropy (DDE) test, provides a unified framework applicable to a wide range of distributional families, with asymptotic validity grounded in established maximum likelihood, bootstrap, and kernel density estimation principles. The test is straightforward to implement, computationally efficient, and requires no tuning parameters or specialized regularity conditions. It compares an MLE-based estimate of differential entropy under the null hypothesis with a nonparametric bootstrapped kernel density estimate, using their divergence as an information-theoretic measure of model fit.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [33] [Stabilising Learner Trajectories: A Doubly Robust Evaluation of AI-Guided Student Support using Activity Theory](https://arxiv.org/abs/2512.11154)
*Teo Susnjak,Khalid Bakhshov,Anuradha Mathrani*

Main category: cs.CY

TL;DR: 本研究使用双重稳健倾向得分匹配法评估大学AI学生支持系统，发现能有效降低课程失败率、提高成绩，但对加速学位完成效果有限


<details>
  <summary>Details</summary>
Motivation: 高等教育中预测模型日益普及，但其触发干预措施的因果证据仍然稀缺。本研究旨在评估AI指导的学生支持系统，推进学习分析评估方法，解决观察性研究中常被忽视的选择偏差和永生时间偏差问题

Method: 采用双重稳健倾向得分匹配法，利用时间对齐的动态AI成功概率分数，将1,859名接受干预的学生与对照组匹配，以减轻选择偏差和永生时间偏差

Result: 干预有效稳定了学生的危险学业轨迹：相比对照组，受支持学生显著降低了课程失败率，获得了更高的累计成绩。但对加速资格完成的积极效果在统计上受限

Conclusion: AI支持系统作为"社会技术刹车"，能有效中断高危学生的学业失败积累，解决主要矛盾（即时学业风险）。但制度结构中的次要矛盾限制了学位完成的加速。AI支持能有效阻止学业下滑，但要将这种稳定性转化为更快进度，需要将干预策略与更广泛的制度治理相结合

Abstract: While predictive models are increasingly common in higher education, causal evidence regarding the interventions they trigger remains rare. This study evaluates an AI-guided student support system at a large university using doubly robust propensity score matching. We advance the methodology for learning analytics evaluation by leveraging time-aligned, dynamic AI probability of success scores to match 1,859 treated students to controls, thereby mitigating the selection and immortal time biases often overlooked in observational studies. Results indicate that the intervention effectively stabilised precarious trajectories, and compared to the control group, supported students significantly reduced their course failure rates and achieved higher cumulative grades. However, effects on the speed of qualification completion were positive but statistically constrained. We interpreted these findings through Activity Theory, framing the intervention as a socio-technical brake that interrupts and slows the accumulation of academic failure among at-risk students. The student support-AI configuration successfully resolved the primary contradiction of immediate academic risk, but secondary contradictions within institutional structures limited the acceleration of degree completion. We conclude that while AI-enabled support effectively arrests decline, translating this stability into faster progression requires aligning intervention strategies with broader institutional governance.

</details>


### [34] [Personalized Pricing in Social Networks with Individual and Group Fairness Considerations](https://arxiv.org/abs/2512.11252)
*Zeyu Chen,Bintong Chen,Wei Qian,Jing Huang*

Main category: cs.CY

TL;DR: FairPricing：基于图神经网络的双重公平个性化定价框架，在社交网络环境中同时考虑个体感知公平和群体公平，实现高盈利性


<details>
  <summary>Details</summary>
Motivation: 个性化定价虽然能提高零售商收入，但会引发个体和群体层面的公平问题。现有研究通常单独处理这两种公平维度，缺乏统一框架。特别是在社交网络环境中，网络结构变化需要重新优化，缺乏泛化能力。

Method: 提出FairPricing框架：1) 使用图神经网络学习个性化定价策略，结合客户特征和网络拓扑；2) 个体公平通过客户需求惩罚项来捕捉感知不公平；3) 群体公平通过对抗性去偏和价格正则化项来缓解歧视；4) 学习到的策略能泛化到网络更新，无需重新优化。

Result: 大量实验结果表明，FairPricing在保持高盈利性的同时，改善了个人公平感知并满足群体公平要求。相比现有基于优化的方法，能更好地适应网络变化。

Conclusion: 该研究首次在社交网络环境中统一了个体公平和群体公平的个性化定价问题，提出的FairPricing框架能有效平衡盈利性和双重公平要求，具有实际应用价值。

Abstract: Personalized pricing assigns different prices to customers for the same product based on customer-specific features to improve retailer revenue. However, this practice often raises concerns about fairness at both the individual and group levels. At the individual level, a customer may perceive unfair treatment if he/she notices being charged a higher price than others. At the group level, pricing disparities can result in discrimination against certain protected groups, such as those defined by gender or race. Existing studies on fair pricing typically address individual and group fairness separately. This paper bridges the gap by introducing a new formulation of the personalized pricing problem that incorporates both dimensions of fairness in social network settings. To solve the problem, we propose FairPricing, a novel framework based on graph neural networks (GNNs) that learns a personalized pricing policy using customer features and network topology. In FairPricing, individual perceived unfairness is captured through a penalty on customer demand, and thus the profit objective, while group-level discrimination is mitigated using adversarial debiasing and a price regularization term. Unlike existing optimization-based personalized pricing, which requires re-optimization whenever the network updates, the pricing policy learned by FairPricing assigns personalized prices to all customers in an updated network based on their features and the new network structure, thereby generalizing to network changes. Extensive experimental results show that FairPricing achieves high profitability while improving individual fairness perceptions and satisfying group fairness requirements.

</details>


### [35] [The Right Kind of Help: Evaluating the Effectiveness of Intervention Methods in Elementary-Level Visual Programming](https://arxiv.org/abs/2512.11735)
*Ahana Ghosh,Liina Malva,Alkis Gotovos,Danial Hooshyar,Adish Singla*

Main category: cs.CY

TL;DR: 比较三种编程干预方法（代码编辑推荐、代码编辑测验、元认知策略测验）在小学编程学习期间和后续阶段的效果，发现所有干预方法都能显著提升学习表现，测验类方法对后续新任务表现更好。


<details>
  <summary>Details</summary>
Motivation: 虽然已有多种小学编程干预方法，但这些方法在学习阶段和后续阶段（学习后）的相对影响尚不清楚，需要大规模研究来比较不同干预方法在这两个阶段的效果。

Method: 对398名4-7年级学生进行两阶段研究：学习阶段使用Hour of Code: Maze Challenge的写代码任务，并分别实施三种干预（代码编辑推荐、代码编辑测验、元认知策略测验）和无干预对照；后续阶段使用更高级的写代码任务且无干预。

Result: 所有干预方法都比对照组显著提高了学习表现，同时保持了学生在后续阶段的问题解决能力。测验类方法（代码编辑测验和元认知策略测验）在后续新任务上表现更好。干预组学生报告了更高的参与度和感知技能增长。

Conclusion: 编程干预方法能有效提升小学编程学习效果，测验类干预尤其有助于学生在后续新任务上的表现，同时能增强学生的参与度和技能感知。

Abstract: Prior work has explored various intervention methods for elementary programming. However, the relative impact of these methods during the learning and post-learning phases remains unclear. In this work, we present a large-scale study comparing the effectiveness of various intervention methods in elementary programming both during learning and on novel tasks post-learning. Specifically, we compare three intervention methods: code-edit recommendations (Code-Rec), quizzes based on code edits (Code-Quiz), and quizzes based on metacognitive strategies (Plan-Quiz), along with a no-intervention control (group None). A total of 398 students (across grades 4-7) participated in a two-phase study: learning phase comprising write-code tasks from the Hour of Code: Maze Challenge with the intervention, followed by a post-learning phase comprising more advanced write-code tasks without any intervention. All intervention methods significantly improved learning performance over the control group while preserving students' problem-solving skills in the post-learning phase. Quiz-based methods further improved performance on novel post-learning tasks. Students in intervention groups also reported greater engagement and perceived skill growth.

</details>

{"id": "2510.20854", "categories": ["econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2510.20854", "abs": "https://arxiv.org/abs/2510.20854", "authors": ["Alberto Baccini"], "title": "Edgeworth's exact and naturally weighted evolutionary utilitarianism and the happiness of Mr. Pongo", "comment": "37 pages", "summary": "This article challenges the conventional reading of Francis Ysidro Edgeworth\nby reconstructing his intellectual project of unifying the moral sciences\nthrough mathematics. The contribution he made in the first phase of his\nwriting, culminating in \\textit{Mathematical Psychics}, aimed to reconfigure\nutilitarianism as an exact science, grounding it in psychophysics and\nevolutionary biology. In order to solve the utilitarian problem of maximizing\npleasure for a given set of sentient beings, he modeled individuals as\n``quasi-Fechnerian'' functions, which incorporated their capacity for pleasure\nas determined by their place in the evolutionary order. The problem of\nmaximization is solved by distributing means according to the individuals'\ncapacity for pleasure. His radical anti-egalitarian conclusions did not stem\nfrom an abstract principle of justice, but from the necessity to maximize\nwelfare among naturally unequal beings. This logic was applied not only to\nsentients of different evolutionary orders, such as Mr. Pongo, a famous\ngorilla, and humans, but also to human races, sexes, and classes. The system,\nin essence, uses the apparent neutrality of science to naturalize and justify\npre-existing social hierarchies. This analysis reveals that the subsequent\nsurgical removal of his utilitarianism by economists, starting with Schumpeter,\nwhile making his tools palatable, eviscerates his overarching philosophical\nsystem."}
{"id": "2510.20863", "categories": ["econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2510.20863", "abs": "https://arxiv.org/abs/2510.20863", "authors": ["Rodrigo Barra Novoa"], "title": "State capacity, innovation, and endogenous development in Chile", "comment": "17 pages", "summary": "The study explores the evolution of Chile's industrial policy from 1990 to\n2022 through the lens of state capacity, innovation and endogenous development.\nIn a global context where governments are reasserting their role as active\nagents of innovation, Chile presents a paradox. It is a stable and open economy\nthat has expanded investment in science and technology but still struggles to\ntransform this effort into sustainable capabilities. Drawing on the works of\nMazzucato, Aghion, Howitt, Mokyr, Samuelson and Sampedro, the study integrates\nevolutionary economics, public policy and humanist ethics. Using a longitudinal\ncase study approach and official data, it finds that Chile has improved its\ninnovation institutions but continues to experience weak coordination, regional\ninequality and a fragile culture of knowledge. The research concludes that\nachieving inclusive innovation requires adaptive governance and an ethical\nvision of innovation as a public good."}
{"id": "2510.21071", "categories": ["econ.GN", "cs.MA", "q-fin.EC", "I.6.3"], "pdf": "https://arxiv.org/pdf/2510.21071", "abs": "https://arxiv.org/abs/2510.21071", "authors": ["Emilio Barucci", "Andrea Gurgone", "Giulia Iori", "Michele Azzone"], "title": "Central Bank Digital Currency, Flight-to-Quality, and Bank-Runs in an Agent-Based Model", "comment": null, "summary": "We analyse financial stability and welfare impacts associated with the\nintroduction of a Central Bank Digital Currency (CBDC) in a macroeconomic\nagent-based model. The model considers firms, banks, and households interacting\non labour, goods, credit, and interbank markets. Households move their\nliquidity from deposits to CBDC based on the perceived riskiness of their\nbanks. We find that the introduction of CBDC exacerbates bank-runs and may lead\nto financial instability phenomena. The effect can be changed by introducing a\nlimit on CBDC holdings. The adoption of CBDC has little effect on macroeconomic\nvariables but the interest rate on loans to firms goes up and credit goes down\nin a limited way. CBDC leads to a redistribution of wealth from firms and banks\nto households with a higher bank default rate. CBDC may have negative welfare\neffects, but a bound on holding enables a welfare improvement."}
{"id": "2510.20838", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.20838", "abs": "https://arxiv.org/abs/2510.20838", "authors": ["Abir Khan Ratul", "Sanjay Acharjee", "Somin Park", "Md Nazmus Sakib"], "title": "Sketch2BIM: A Multi-Agent Human-AI Collaborative Pipeline to Convert Hand-Drawn Floor Plans to 3D BIM", "comment": null, "summary": "This study introduces a human-in-the-loop pipeline that converts unscaled,\nhand-drawn floor plan sketches into semantically consistent 3D BIM models. The\nworkflow leverages multimodal large language models (MLLMs) within a\nmulti-agent framework, combining perceptual extraction, human feedback, schema\nvalidation, and automated BIM scripting. Initially, sketches are iteratively\nrefined into a structured JSON layout of walls, doors, and windows. Later,\nthese layouts are transformed into executable scripts that generate 3D BIM\nmodels. Experiments on ten diverse floor plans demonstrate strong convergence:\nopenings (doors, windows) are captured with high reliability in the initial\npass, while wall detection begins around 83% and achieves near-perfect\nalignment after a few feedback iterations. Across all categories, precision,\nrecall, and F1 scores remain above 0.83, and geometric errors (RMSE, MAE)\nprogressively decrease to zero through feedback corrections. This study\ndemonstrates how MLLM-driven multi-agent reasoning can make BIM creation\naccessible to both experts and non-experts using only freehand sketches."}
{"id": "2510.20996", "categories": ["econ.EM", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.20996", "abs": "https://arxiv.org/abs/2510.20996", "authors": ["Xiaohong Chen", "Min Seong Kim", "Sokbae Lee", "Myung Hwan Seo", "Myunghyun Song"], "title": "SLIM: Stochastic Learning and Inference in Overidentified Models", "comment": null, "summary": "We propose SLIM (Stochastic Learning and Inference in overidentified Models),\na scalable stochastic approximation framework for nonlinear GMM. SLIM forms\niterative updates from independent mini-batches of moments and their\nderivatives, producing unbiased directions that ensure almost-sure convergence.\nIt requires neither a consistent initial estimator nor global convexity and\naccommodates both fixed-sample and random-sampling asymptotics. We further\ndevelop an optional second-order refinement and inference procedures based on\nrandom scaling and plug-in methods, including plug-in, debiased plug-in, and\nonline versions of the Sargan--Hansen $J$-test tailored to stochastic learning.\nIn Monte Carlo experiments based on a nonlinear EASI demand system with 576\nmoment conditions, 380 parameters, and $n = 10^5$, SLIM solves the model in\nunder 1.4 hours, whereas full-sample GMM in Stata on a powerful laptop\nconverges only after 18 hours. The debiased plug-in $J$-test delivers\nsatisfactory finite-sample inference, and SLIM scales smoothly to $n = 10^6$."}
{"id": "2510.20884", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20884", "abs": "https://arxiv.org/abs/2510.20884", "authors": ["Pranamya Kulkarni", "Puranjay Datta", "Burak Varıcı", "Emre Acartürk", "Karthikeyan Shanmugam", "Ali Tajer"], "title": "ROPES: Robotic Pose Estimation via Score-Based Causal Representation Learning", "comment": "A preliminary version of this paper appeared at NeurIPS 2025 Workshop\n  on Embodied World Models for Decision Making", "summary": "Causal representation learning (CRL) has emerged as a powerful unsupervised\nframework that (i) disentangles the latent generative factors underlying\nhigh-dimensional data, and (ii) learns the cause-and-effect interactions among\nthe disentangled variables. Despite extensive recent advances in\nidentifiability and some practical progress, a substantial gap remains between\ntheory and real-world practice. This paper takes a step toward closing that gap\nby bringing CRL to robotics, a domain that has motivated CRL. Specifically,\nthis paper addresses the well-defined robot pose estimation -- the recovery of\nposition and orientation from raw images -- by introducing Robotic Pose\nEstimation via Score-Based CRL (ROPES). Being an unsupervised framework, ROPES\nembodies the essence of interventional CRL by identifying those generative\nfactors that are actuated: images are generated by intrinsic and extrinsic\nlatent factors (e.g., joint angles, arm/limb geometry, lighting, background,\nand camera configuration) and the objective is to disentangle and recover the\ncontrollable latent variables, i.e., those that can be directly manipulated\n(intervened upon) through actuation. Interventional CRL theory shows that\nvariables that undergo variations via interventions can be identified. In\nrobotics, such interventions arise naturally by commanding actuators of various\njoints and recording images under varied controls. Empirical evaluations in\nsemi-synthetic manipulator experiments demonstrate that ROPES successfully\ndisentangles latent generative factors with high fidelity with respect to the\nground truth. Crucially, this is achieved by leveraging only distributional\nchanges, without using any labeled data. The paper also includes a comparison\nwith a baseline based on a recently proposed semi-supervised framework. This\npaper concludes by positioning robot pose estimation as a near-practical\ntestbed for CRL."}
{"id": "2510.21165", "categories": ["q-fin.GN", "cs.SY", "eess.SY", "nlin.CD", "physics.data-an", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2510.21165", "abs": "https://arxiv.org/abs/2510.21165", "authors": ["Peng Liu"], "title": "The local Gaussian correlation networks among return tails in the Chinese stock market", "comment": null, "summary": "Financial networks based on Pearson correlations have been intensively\nstudied. However, previous studies may have led to misleading and catastrophic\nresults because of several critical shortcomings of the Pearson correlation.\nThe local Gaussian correlation coefficient, a new measurement of statistical\ndependence between variables, has unique advantages including capturing local\nnonlinear dependence and handling heavy-tailed distributions. This study\nconstructs financial networks using the local Gaussian correlation coefficients\nbetween tail regions of stock returns in the Shanghai Stock Exchange. The work\nsystematically analyzes fundamental network metrics including node centrality,\naverage shortest path length, and entropy. Compared with the local Gaussian\ncorrelation network among positive tails and the conventional Pearson\ncorrelation network, the properties of the local Gaussian correlation network\namong negative tails are more sensitive to the stock market risks. This finding\nsuggests researchers should prioritize the local Gaussian correlation network\namong negative tails. Future work should reevaluate existing findings using the\nlocal Gaussian correlation method."}
{"id": "2510.20881", "categories": ["cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.20881", "abs": "https://arxiv.org/abs/2510.20881", "authors": ["Anurag Shekhar"], "title": "Digital Permission Structures: How Celebrity Disclosure Enables Black Masculine Vulnerability in Online Mental Health Discourse", "comment": null, "summary": "Black men face a double barrier to mental health help-seeking: traditional\nmasculinity norms demanding emotional restrictiveness and systemic racism\nfostering institutional mistrust. While celebrity mental health disclosures\nshow promise for stigma reduction, limited research examines their impact on\nBlack masculine communities through digital platforms. This convergent\nmixed-methods study analysed 11,306 YouTube comments following rapper Lil\nWayne's unprecedented disclosure of childhood suicide attempt and lifelong\nmental health struggles. Quantitative analysis using VADER sentiment\nclassification, Latent Dirichlet Allocation topic modelling, and NRC emotion\nlexicon analysis revealed predominantly positive sentiment with systematic\ncommunity amplification of mental health discourse. Reflexive thematic analysis\nof 2,100 high-engagement comments identified eight themes, with peer support\nachieving the highest saturation, contradicting isolation narratives. Findings\nsupport a Digital Permission Structures Model demonstrating how intersectional\ncelebrity status (race + gender + high-status), hip-hop authenticity values,\nand digital platform affordances create triadic authorisation mechanisms\nenabling vulnerability expression. Community responses revealed communal\nmasculinity rooted in Ubuntu philosophy and active reconstruction of masculine\nnorms, positioning help-seeking as strength. Results challenge deficit-based\nmodels of Black masculinity, suggesting interventions should leverage\ncollectivism, partner with high-status cultural figures, employ strength-based\nmessaging, and centre hip-hop authenticity rather than imposing Western\nindividualistic frameworks. This study provides evidence-based strategies for\nculturally responsive mental health interventions addressing persistent\ndisparities in Black men's service utilisation."}
{"id": "2510.21630", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.21630", "abs": "https://arxiv.org/abs/2510.21630", "authors": ["Forough Mahpouya", "Sabrina Casucci", "Suzanne Sullivan", "Christopher Barrick"], "title": "Representing caregiver burden in observational studies: Development of the Caregiver Burden Index (CareBI) using NSOC", "comment": null, "summary": "Informal caregiving often carries a significant emotional, physical, and\nfinancial toll, yet caregiver burden is often underrepresented in healthcare\nresearch and methods. Existing caregiver burden instruments, while valuable in\nclinical research, often lack compatibility with observational datasets\nregularly used in health services research and planning. This study introduces\nthe Caregiver Burden Index (CareBI) developed for the National Study of\nCaregiving (NSOC), that can be used to represent caregiver burden in\nquantitative models and observational research studies. CareBI was developed\nand validated using a multistep process that included the identification and\npreparation of individual NSOC survey items, exploratory and confirmatory\nfactor analysis, score estimation, interpretation, and external validation. The\nstudy used data from round 12 of the NSOC. CareBI represents three domains of\nburden: objective, subjective, and interpersonal, providing a comprehensive\nview of both the positive and negative aspects of caregiving. It also aligns\nwith the Zarit Burden Interview, a widely used tool for prospectively assessing\ncaregiver burden. Construct validity was assessed by comparing CareBI's\nrelationship with caregiver and care recipient outcomes, as well as sensitivity\nto known burden-related risk and mitigation factors. Early findings affirm the\nscale's utility in categorizing low-, moderate-, and high-burden caregivers and\nguiding resource-oriented strategies. CareBI represents a reproducible tool for\nembedding caregiver metrics into health operations, predictive modeling, and\npublic policy frameworks, and provides a template for applying operations\nresearch and industrial engineering methods to psychosocial measurement\nchallenges in aging and long-term care."}
{"id": "2510.20854", "categories": ["econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2510.20854", "abs": "https://arxiv.org/abs/2510.20854", "authors": ["Alberto Baccini"], "title": "Edgeworth's exact and naturally weighted evolutionary utilitarianism and the happiness of Mr. Pongo", "comment": "37 pages", "summary": "This article challenges the conventional reading of Francis Ysidro Edgeworth\nby reconstructing his intellectual project of unifying the moral sciences\nthrough mathematics. The contribution he made in the first phase of his\nwriting, culminating in \\textit{Mathematical Psychics}, aimed to reconfigure\nutilitarianism as an exact science, grounding it in psychophysics and\nevolutionary biology. In order to solve the utilitarian problem of maximizing\npleasure for a given set of sentient beings, he modeled individuals as\n``quasi-Fechnerian'' functions, which incorporated their capacity for pleasure\nas determined by their place in the evolutionary order. The problem of\nmaximization is solved by distributing means according to the individuals'\ncapacity for pleasure. His radical anti-egalitarian conclusions did not stem\nfrom an abstract principle of justice, but from the necessity to maximize\nwelfare among naturally unequal beings. This logic was applied not only to\nsentients of different evolutionary orders, such as Mr. Pongo, a famous\ngorilla, and humans, but also to human races, sexes, and classes. The system,\nin essence, uses the apparent neutrality of science to naturalize and justify\npre-existing social hierarchies. This analysis reveals that the subsequent\nsurgical removal of his utilitarianism by economists, starting with Schumpeter,\nwhile making his tools palatable, eviscerates his overarching philosophical\nsystem."}
{"id": "2510.21006", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.21006", "abs": "https://arxiv.org/abs/2510.21006", "authors": ["Akshay Naik", "Ramavarapu S. Sreenivas", "William R. Norris", "Albert E. Patterson", "Ahmet Soylemezoglu", "Dustin Nottage"], "title": "Safety Monitor for Off-Road Planning with Uncertainty Bounded Bekker Costs", "comment": null, "summary": "Reliable off-road autonomy requires operational constraints so that behavior\nstays predictable and safe when soil strength is uncertain. This paper presents\na runtime assurance safety monitor that collaborates with any planner and uses\na Bekker-based cost model with bounded uncertainty. The monitor builds an upper\nconfidence traversal cost from a lightweight pressure sinkage model identified\nin field tests and checks each planned motion against two limits: maximum\nsinkage and rollover margin. If the risk of crossing either limit is too high,\nthe monitor switches to a certified fallback that reduces vehicle speed,\nincreases standoff from soft ground, or stops on firmer soil. This separation\nlets the planner focus on efficiency while the monitor keeps the vehicle within\nclear safety limits on board. Wheel geometry, wheel load estimate, and a soil\nraster serve as inputs, which tie safety directly to vehicle design and let the\nmonitor set clear limits on speed, curvature, and stopping at run time. The\nmethod carries uncertainty analytically into the upper confidence cost and\napplies simple intervention rules. Tuning of the sinkage limit, rollover\nmargin, and risk window trades efficiency for caution while keeping the monitor\nlight enough for embedded processors. Results from a simulation environment\nspanning loam to sand include intervention rates, violation probability, and\npath efficiency relative to the nominal plan, and a benchtop static loading\ncheck provides initial empirical validation."}
{"id": "2510.20907", "categories": ["econ.TH"], "pdf": "https://arxiv.org/pdf/2510.20907", "abs": "https://arxiv.org/abs/2510.20907", "authors": ["Victor Augias", "Lina Uhe"], "title": "The Economics of Convex Function Intervals", "comment": null, "summary": "We introduce convex function intervals (CFIs): families of convex functions\nsatisfying given level and slope constraints. CFIs naturally arise as\nconstraint sets in economic design, including problems with type-dependent\nparticipation constraints and two-sided (weak) majorization constraints. Our\nmain results include: (i) a geometric characterization of the extreme points of\nCFIs; (ii) sufficient optimality conditions for linear programs over CFIs; and\n(iii) methods for nested optimization on their lower level boundary that can be\napplied, e.g., to the optimal design of outside options. We apply these results\nto four settings: screening and delegation problems with type-dependent outside\noptions, contest design with limited disposal, and mean-based persuasion with\ninformativeness constraints. We draw several novel economic implications using\nour tools. For instance, we show that better outside options lead to larger\ndelegation sets, and that posted price mechanisms can be suboptimal in the\ncanonical monopolistic screening problem with nontrivial, type-dependent\nparticipation constraints."}
{"id": "2510.20838", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.20838", "abs": "https://arxiv.org/abs/2510.20838", "authors": ["Abir Khan Ratul", "Sanjay Acharjee", "Somin Park", "Md Nazmus Sakib"], "title": "Sketch2BIM: A Multi-Agent Human-AI Collaborative Pipeline to Convert Hand-Drawn Floor Plans to 3D BIM", "comment": null, "summary": "This study introduces a human-in-the-loop pipeline that converts unscaled,\nhand-drawn floor plan sketches into semantically consistent 3D BIM models. The\nworkflow leverages multimodal large language models (MLLMs) within a\nmulti-agent framework, combining perceptual extraction, human feedback, schema\nvalidation, and automated BIM scripting. Initially, sketches are iteratively\nrefined into a structured JSON layout of walls, doors, and windows. Later,\nthese layouts are transformed into executable scripts that generate 3D BIM\nmodels. Experiments on ten diverse floor plans demonstrate strong convergence:\nopenings (doors, windows) are captured with high reliability in the initial\npass, while wall detection begins around 83% and achieves near-perfect\nalignment after a few feedback iterations. Across all categories, precision,\nrecall, and F1 scores remain above 0.83, and geometric errors (RMSE, MAE)\nprogressively decrease to zero through feedback corrections. This study\ndemonstrates how MLLM-driven multi-agent reasoning can make BIM creation\naccessible to both experts and non-experts using only freehand sketches."}
{"id": "2510.21006", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.21006", "abs": "https://arxiv.org/abs/2510.21006", "authors": ["Akshay Naik", "Ramavarapu S. Sreenivas", "William R. Norris", "Albert E. Patterson", "Ahmet Soylemezoglu", "Dustin Nottage"], "title": "Safety Monitor for Off-Road Planning with Uncertainty Bounded Bekker Costs", "comment": null, "summary": "Reliable off-road autonomy requires operational constraints so that behavior\nstays predictable and safe when soil strength is uncertain. This paper presents\na runtime assurance safety monitor that collaborates with any planner and uses\na Bekker-based cost model with bounded uncertainty. The monitor builds an upper\nconfidence traversal cost from a lightweight pressure sinkage model identified\nin field tests and checks each planned motion against two limits: maximum\nsinkage and rollover margin. If the risk of crossing either limit is too high,\nthe monitor switches to a certified fallback that reduces vehicle speed,\nincreases standoff from soft ground, or stops on firmer soil. This separation\nlets the planner focus on efficiency while the monitor keeps the vehicle within\nclear safety limits on board. Wheel geometry, wheel load estimate, and a soil\nraster serve as inputs, which tie safety directly to vehicle design and let the\nmonitor set clear limits on speed, curvature, and stopping at run time. The\nmethod carries uncertainty analytically into the upper confidence cost and\napplies simple intervention rules. Tuning of the sinkage limit, rollover\nmargin, and risk window trades efficiency for caution while keeping the monitor\nlight enough for embedded processors. Results from a simulation environment\nspanning loam to sand include intervention rates, violation probability, and\npath efficiency relative to the nominal plan, and a benchtop static loading\ncheck provides initial empirical validation."}
{"id": "2510.20849", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20849", "abs": "https://arxiv.org/abs/2510.20849", "authors": ["Alejandro H. Artiles", "Hiromu Yakura", "Levin Brinkmann", "Mar Canet Sola", "Hassan Abu Alhaija", "Ignacio Serna", "Nasim Rahaman", "Bernhard Schölkopf", "Iyad Rahwan"], "title": "Cultural Alien Sampler: Open-ended art generation balancing originality and coherence", "comment": "Proceedings of the 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025). Creative AI Track. 26 pages, 24 figures", "summary": "In open-ended domains like art, autonomous agents must generate ideas that\nare both original and internally coherent, yet current Large Language Models\n(LLMs) either default to familiar cultural patterns or sacrifice coherence when\npushed toward novelty. We address this by introducing the Cultural Alien\nSampler (CAS), a concept-selection method that explicitly separates\ncompositional fit from cultural typicality. CAS uses two GPT-2 models\nfine-tuned on WikiArt concepts: a Concept Coherence Model that scores whether\nconcepts plausibly co-occur within artworks, and a Cultural Context Model that\nestimates how typical those combinations are within individual artists' bodies\nof work. CAS targets combinations that are high in coherence and low in\ntypicality, yielding ideas that maintain internal consistency while deviating\nfrom learned conventions and embedded cultural context. In a human evaluation\n(N = 100), our approach outperforms random selection and GPT-4o baselines and\nachieves performance comparable to human art students in both perceived\noriginality and harmony. Additionally, a quantitative study shows that our\nmethod produces more diverse outputs and explores a broader conceptual space\nthan its GPT-4o counterpart, demonstrating that artificial cultural alienness\ncan unlock creative potential in autonomous agents."}
{"id": "2510.20916", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.20916", "abs": "https://arxiv.org/abs/2510.20916", "authors": ["Sydney M. Katz", "Robert J. Moss", "Dylan M. Asmar", "Wesley A. Olson", "James K. Kuchar", "Mykel J. Kochenderfer"], "title": "Aircraft Collision Avoidance Systems: Technological Challenges and Solutions on the Path to Regulatory Acceptance", "comment": "32 pages, 9 figures", "summary": "Aircraft collision avoidance systems is critical to modern aviation. These\nsystems are designed to predict potential collisions between aircraft and\nrecommend appropriate avoidance actions. Creating effective collision avoidance\nsystems requires solutions to a variety of technical challenges related to\nsurveillance, decision making, and validation. These challenges have sparked\nsignificant research and development efforts over the past several decades that\nhave resulted in a variety of proposed solutions. This article provides an\noverview of these challenges and solutions with an emphasis on those that have\nbeen put through a rigorous validation process and accepted by regulatory\nbodies. The challenges posed by the collision avoidance problem are often\npresent in other domains, and aircraft collision avoidance systems can serve as\ncase studies that provide valuable insights for a wide range of safety-critical\nsystems."}
{"id": "2510.20927", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2510.20927", "abs": "https://arxiv.org/abs/2510.20927", "authors": ["Tom Reed", "Tegan McCaslin", "Luca Righetti"], "title": "What do model reports say about their ChemBio benchmark evaluations? Comparing recent releases to the STREAM framework", "comment": "12 pages, 6 figures. Includes appendices", "summary": "Most frontier AI developers publicly document their safety evaluations of new\nAI models in model reports, including testing for chemical and biological\n(ChemBio) misuse risks. This practice provides a window into the methodology of\nthese evaluations, helping to build public trust in AI systems, and enabling\nthird party review in the still-emerging science of AI evaluation. But what\naspects of evaluation methodology do developers currently include -- or omit --\nin their reports? This paper examines three frontier AI model reports published\nin spring 2025 with among the most detailed documentation: OpenAI's o3,\nAnthropic's Claude 4, and Google DeepMind's Gemini 2.5 Pro. We compare these\nusing the STREAM (v1) standard for reporting ChemBio benchmark evaluations.\nEach model report included some useful details that the others did not, and all\nmodel reports were found to have areas for development, suggesting that\ndevelopers could benefit from adopting one another's best reporting practices.\nWe identified several items where reporting was less well-developed across all\nmodel reports, such as providing examples of test material, and including a\ndetailed list of elicitation conditions. Overall, we recommend that AI\ndevelopers continue to strengthen the emerging science of evaluation by working\ntowards greater transparency in areas where reporting currently remains\nlimited."}
{"id": "2510.20863", "categories": ["econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2510.20863", "abs": "https://arxiv.org/abs/2510.20863", "authors": ["Rodrigo Barra Novoa"], "title": "State capacity, innovation, and endogenous development in Chile", "comment": "17 pages", "summary": "The study explores the evolution of Chile's industrial policy from 1990 to\n2022 through the lens of state capacity, innovation and endogenous development.\nIn a global context where governments are reasserting their role as active\nagents of innovation, Chile presents a paradox. It is a stable and open economy\nthat has expanded investment in science and technology but still struggles to\ntransform this effort into sustainable capabilities. Drawing on the works of\nMazzucato, Aghion, Howitt, Mokyr, Samuelson and Sampedro, the study integrates\nevolutionary economics, public policy and humanist ethics. Using a longitudinal\ncase study approach and official data, it finds that Chile has improved its\ninnovation institutions but continues to experience weak coordination, regional\ninequality and a fragile culture of knowledge. The research concludes that\nachieving inclusive innovation requires adaptive governance and an ethical\nvision of innovation as a public good."}
{"id": "2510.21025", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.21025", "abs": "https://arxiv.org/abs/2510.21025", "authors": ["Ahmed Saad Al-Karsani", "Maryam Khanbaghi", "Aleksandar Zečević"], "title": "A Connectively Stable and Robust DAPI Control Scheme for Islanded Networks of Microgrids", "comment": null, "summary": "The transition towards clean energy and the introduction of Distributed\nEnergy Resources (DERs) are giving rise to the emergence of Microgrids (MGs)\nand Networks of MGs (NMGs). MGs and NMGs can operate autonomously in islanded\nmode. However, they face challenges in terms of secondary level frequency and\nvoltage regulation, due to the variable nature of Renewable Energy Sources\n(RES) and loads. Distributed-Averaging Proportional-Integral (DAPI) control has\nbeen proposed in the literature for distributed frequency and voltage control\nof droop-controlled DERs, but it is not robust to operational or structural\nperturbations. To address this, we propose a robust DAPI frequency and voltage\ncontrol scheme that ensures robustness using the concept of connective\nstability, along with the invariant ellipsoid technique for disturbance\nrejection. Simulation of an NMG model in\nMATLAB\\textsuperscript{\\textregistered}/Simulink\\textsuperscript{\\textregistered}\nconsisting of 3 MGs and 5 DERs validates the effectiveness of the proposed\nmethod, and demonstrates that it can successfully mitigate the effects of major\ndisturbances such as cyberattacks."}
{"id": "2510.20918", "categories": ["econ.TH", "cs.GT"], "pdf": "https://arxiv.org/pdf/2510.20918", "abs": "https://arxiv.org/abs/2510.20918", "authors": ["Alejandro Francetich", "Burkhard C. Schipper"], "title": "Rationalizable Screening and Disclosure under Unawareness", "comment": "48 pages, 3 figures", "summary": "We analyze a principal-agent procurement problem in which the principal (she)\nis unaware some of the marginal cost types of the agent (he). Communication\narises naturally as some types of the agent may have an incentive to raise the\nprincipal's awareness (totally or partially) before a contract menu is offered.\nThe resulting menu must not only reflect the principal's change in awareness,\nbut also her learning about types from the agent's decision to raise her\nawareness in the first place. We capture this reasoning in a discrete concave\nmodel via a rationalizability procedure in which marginal beliefs over types\nare restricted to log-concavity, ``reverse'' Bayesianism, and mild assumptions\nof caution.\n  We show that if the principal is ex ante only unaware of high-cost types, all\nof these types have an incentive raise her awareness of them -- otherwise, they\nwould not be served. With three types, the two lower-cost types that the\nprincipal is initially aware of also want to raise her awareness of the\nhigh-cost type: Their quantities suffer no additional distortions and they both\nearn an extra information rent. Intuitively, the presence of an even higher\ncost type makes the original two look better. With more than three types, we\nshow that this intuition may break down for types of whom the principal is\ninitially aware of so that raising the principal's awareness could cease to be\nprofitable for those types. When the principal is ex ante only unaware of more\nefficient (low-cost) types, then \\textit{no type} raises her awareness, leaving\nher none the wiser."}
{"id": "2510.20849", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20849", "abs": "https://arxiv.org/abs/2510.20849", "authors": ["Alejandro H. Artiles", "Hiromu Yakura", "Levin Brinkmann", "Mar Canet Sola", "Hassan Abu Alhaija", "Ignacio Serna", "Nasim Rahaman", "Bernhard Schölkopf", "Iyad Rahwan"], "title": "Cultural Alien Sampler: Open-ended art generation balancing originality and coherence", "comment": "Proceedings of the 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025). Creative AI Track. 26 pages, 24 figures", "summary": "In open-ended domains like art, autonomous agents must generate ideas that\nare both original and internally coherent, yet current Large Language Models\n(LLMs) either default to familiar cultural patterns or sacrifice coherence when\npushed toward novelty. We address this by introducing the Cultural Alien\nSampler (CAS), a concept-selection method that explicitly separates\ncompositional fit from cultural typicality. CAS uses two GPT-2 models\nfine-tuned on WikiArt concepts: a Concept Coherence Model that scores whether\nconcepts plausibly co-occur within artworks, and a Cultural Context Model that\nestimates how typical those combinations are within individual artists' bodies\nof work. CAS targets combinations that are high in coherence and low in\ntypicality, yielding ideas that maintain internal consistency while deviating\nfrom learned conventions and embedded cultural context. In a human evaluation\n(N = 100), our approach outperforms random selection and GPT-4o baselines and\nachieves performance comparable to human art students in both perceived\noriginality and harmony. Additionally, a quantitative study shows that our\nmethod produces more diverse outputs and explores a broader conceptual space\nthan its GPT-4o counterpart, demonstrating that artificial cultural alienness\ncan unlock creative potential in autonomous agents."}
{"id": "2510.21025", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.21025", "abs": "https://arxiv.org/abs/2510.21025", "authors": ["Ahmed Saad Al-Karsani", "Maryam Khanbaghi", "Aleksandar Zečević"], "title": "A Connectively Stable and Robust DAPI Control Scheme for Islanded Networks of Microgrids", "comment": null, "summary": "The transition towards clean energy and the introduction of Distributed\nEnergy Resources (DERs) are giving rise to the emergence of Microgrids (MGs)\nand Networks of MGs (NMGs). MGs and NMGs can operate autonomously in islanded\nmode. However, they face challenges in terms of secondary level frequency and\nvoltage regulation, due to the variable nature of Renewable Energy Sources\n(RES) and loads. Distributed-Averaging Proportional-Integral (DAPI) control has\nbeen proposed in the literature for distributed frequency and voltage control\nof droop-controlled DERs, but it is not robust to operational or structural\nperturbations. To address this, we propose a robust DAPI frequency and voltage\ncontrol scheme that ensures robustness using the concept of connective\nstability, along with the invariant ellipsoid technique for disturbance\nrejection. Simulation of an NMG model in\nMATLAB\\textsuperscript{\\textregistered}/Simulink\\textsuperscript{\\textregistered}\nconsisting of 3 MGs and 5 DERs validates the effectiveness of the proposed\nmethod, and demonstrates that it can successfully mitigate the effects of major\ndisturbances such as cyberattacks."}
{"id": "2510.20861", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20861", "abs": "https://arxiv.org/abs/2510.20861", "authors": ["Krzysztof Siminski"], "title": "Fuzzy numbers revisited: operations on extensional fuzzy numbers", "comment": "33 pages, 62 references", "summary": "Fuzzy numbers are commonly represented with fuzzy sets. Their objective is to\nbetter represent imprecise data. However, operations on fuzzy numbers are not\nas straightforward as maths on crisp numbers. Commonly, the Zadeh's extension\nrule is applied to elaborate a result. This can produce two problems: (1) high\ncomputational complexity and (2) for some fuzzy sets and some operations the\nresults is not a fuzzy set with the same features (eg. multiplication of two\ntriangular fuzzy sets does not produce a triangular fuzzy set). One more\nproblem is the fuzzy spread -- fuzziness of the result increases with the\nnumber of operations. These facts can severely limit the application field of\nfuzzy numbers. In this paper we would like to revisite this problem with a\ndifferent kind of fuzzy numbers -- extensional fuzzy numbers. The paper defines\noperations on extensional fuzzy numbers and relational operators (=, >, >=, <,\n<=) for them. The proposed approach is illustrated with several applicational\nexamples. The C++ implementation is available from a public GitHub repository."}
{"id": "2510.20965", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20965", "abs": "https://arxiv.org/abs/2510.20965", "authors": ["Jesse Haworth", "Juo-Tung Chen", "Nigel Nelson", "Ji Woong Kim", "Masoud Moghani", "Chelsea Finn", "Axel Krieger"], "title": "SutureBot: A Precision Framework & Benchmark For Autonomous End-to-End Suturing", "comment": "10 pages, 5 figures, 4 tables, NeurIPS 2025", "summary": "Robotic suturing is a prototypical long-horizon dexterous manipulation task,\nrequiring coordinated needle grasping, precise tissue penetration, and secure\nknot tying. Despite numerous efforts toward end-to-end autonomy, a fully\nautonomous suturing pipeline has yet to be demonstrated on physical hardware.\nWe introduce SutureBot: an autonomous suturing benchmark on the da Vinci\nResearch Kit (dVRK), spanning needle pickup, tissue insertion, and knot tying.\nTo ensure repeatability, we release a high-fidelity dataset comprising 1,890\nsuturing demonstrations. Furthermore, we propose a goal-conditioned framework\nthat explicitly optimizes insertion-point precision, improving targeting\naccuracy by 59\\%-74\\% over a task-only baseline. To establish this task as a\nbenchmark for dexterous imitation learning, we evaluate state-of-the-art\nvision-language-action (VLA) models, including $\\pi_0$, GR00T N1, OpenVLA-OFT,\nand multitask ACT, each augmented with a high-level task-prediction policy.\nAutonomous suturing is a key milestone toward achieving robotic autonomy in\nsurgery. These contributions support reproducible evaluation and development of\nprecision-focused, long-horizon dexterous manipulation policies necessary for\nend-to-end suturing. Dataset is available at:\nhttps://huggingface.co/datasets/jchen396/suturebot"}
{"id": "2510.21082", "categories": ["cs.CY", "cs.AI", "cs.HC", "68T50 (Artificial intelligence)", "I.2.7; K.5.2"], "pdf": "https://arxiv.org/pdf/2510.21082", "abs": "https://arxiv.org/abs/2510.21082", "authors": ["Jorge Alberto Araujo"], "title": "Soppia: A Structured Prompting Framework for the Proportional Assessment of Non-Pecuniary Damages in Personal Injury Cases", "comment": "9 pages, 2 tables, includes GitHub link to framework implementation.\n  Submitted to the Artificial Intelligence and Law section of arXiv", "summary": "Applying complex legal rules characterized by multiple, heterogeneously\nweighted criteria presents a fundamental challenge in judicial decision-making,\noften hindering the consistent realization of legislative intent. This\nchallenge is particularly evident in the quantification of non-pecuniary\ndamages in personal injury cases. This paper introduces Soppia, a structured\nprompting framework designed to assist legal professionals in navigating this\ncomplexity. By leveraging advanced AI, the system ensures a comprehensive and\nbalanced analysis of all stipulated criteria, fulfilling the legislator's\nintent that compensation be determined through a holistic assessment of each\ncase. Using the twelve criteria for non-pecuniary damages established in the\nBrazilian CLT (Art. 223-G) as a case study, we demonstrate how Soppia (System\nfor Ordered Proportional and Pondered Intelligent Assessment) operationalizes\nnuanced legal commands into a practical, replicable, and transparent\nmethodology. The framework enhances consistency and predictability while\nproviding a versatile and explainable tool adaptable across multi-criteria\nlegal contexts, bridging normative interpretation and computational reasoning\ntoward auditable legal AI."}
{"id": "2510.21071", "categories": ["econ.GN", "cs.MA", "q-fin.EC", "I.6.3"], "pdf": "https://arxiv.org/pdf/2510.21071", "abs": "https://arxiv.org/abs/2510.21071", "authors": ["Emilio Barucci", "Andrea Gurgone", "Giulia Iori", "Michele Azzone"], "title": "Central Bank Digital Currency, Flight-to-Quality, and Bank-Runs in an Agent-Based Model", "comment": null, "summary": "We analyse financial stability and welfare impacts associated with the\nintroduction of a Central Bank Digital Currency (CBDC) in a macroeconomic\nagent-based model. The model considers firms, banks, and households interacting\non labour, goods, credit, and interbank markets. Households move their\nliquidity from deposits to CBDC based on the perceived riskiness of their\nbanks. We find that the introduction of CBDC exacerbates bank-runs and may lead\nto financial instability phenomena. The effect can be changed by introducing a\nlimit on CBDC holdings. The adoption of CBDC has little effect on macroeconomic\nvariables but the interest rate on loans to firms goes up and credit goes down\nin a limited way. CBDC leads to a redistribution of wealth from firms and banks\nto households with a higher bank default rate. CBDC may have negative welfare\neffects, but a bound on holding enables a welfare improvement."}
{"id": "2510.21044", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.21044", "abs": "https://arxiv.org/abs/2510.21044", "authors": ["Kunal Shankar", "Ninad Gaikwad", "Anamika Dubey"], "title": "House Thermal Model Estimation: Robustness Across Seasons and Setpoints", "comment": "This manuscript is a version of our paper accepted at the 57th North\n  American Power Symposium (NAPS) 2025", "summary": "Achieving the flexibility from house heating, cooling, and ventilation\nsystems (HVAC) has the potential to enable large-scale demand response by\naggregating HVAC load adjustments across many homes. This demand response\nstrategy helps distribution grid to flexibly ramp-up or ramp-down local load\ndemand so that it can optimally match the bulk power system generation profile.\nHowever, achieving this capability requires house thermal models that are both\ncomputationally efficient and robust to operating conditions. In this work,\nparameters of the Resistance-Capacitance (RC) network thermal model for houses\nare estimated using three optimization algorithms: Nonlinear Least Squares\n(NLS), Batch Estimation (BE), and Maximum Likelihood Estimation (MLE). The\nresulting models are evaluated through a Forward-Simulation across four\ndifferent seasons and three setpoints. The results illustrate a principled way\nof selecting reduced order models and estimation methods with respect to the\nrobustness offered to seasonal and setpoint variations in training-testing\ndatasets"}
{"id": "2510.20921", "categories": ["econ.TH", "cs.GT"], "pdf": "https://arxiv.org/pdf/2510.20921", "abs": "https://arxiv.org/abs/2510.20921", "authors": ["Alejandro Francetich", "Burkhard C. Schipper"], "title": "Discrete Screening", "comment": "27 pages, 6 figures", "summary": "We consider a principal who wishes to screen an agent with \\emph{discrete}\ntypes by offering a menu of \\emph{discrete} quantities and \\emph{discrete}\ntransfers. We assume that the principal's valuation is discrete strictly\nconcave and use a discrete first-order approach. We model the agent's cost\ntypes as non-integer, with integer types as a limit case. Our modeling of cost\ntypes allows us to replicate the typical constraint-simplification results and\nthus to emulate the well-treaded steps of screening under a continuum of\ncontracts.\n  We show that the solutions to the discrete F.O.C.s need not be unique\n\\textit{even under discrete strict concavity}, but we also show that there\ncannot be more than two optimal contract quantities for each type, and that --\nif there are two -- they must be adjacent. Moreover, we can only ensure weak\nmonotonicity of the quantities \\textit{even if virtual costs are strictly\nmonotone}, unless we limit the ``degree of concavity'' of the principal's\nutility. Our discrete screening approach facilitates the use of\nrationalizability to solve the screening problem. We introduce a\nrationalizability notion featuring robustness with respect to an open set of\nbeliefs over types called \\textit{$\\Delta$-O Rationalizability}, and show that\nthe set of $\\Delta$-O rationalizable menus coincides with the set of usual\noptimal contracts -- possibly augmented to include irrelevant contracts."}
{"id": "2510.20861", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20861", "abs": "https://arxiv.org/abs/2510.20861", "authors": ["Krzysztof Siminski"], "title": "Fuzzy numbers revisited: operations on extensional fuzzy numbers", "comment": "33 pages, 62 references", "summary": "Fuzzy numbers are commonly represented with fuzzy sets. Their objective is to\nbetter represent imprecise data. However, operations on fuzzy numbers are not\nas straightforward as maths on crisp numbers. Commonly, the Zadeh's extension\nrule is applied to elaborate a result. This can produce two problems: (1) high\ncomputational complexity and (2) for some fuzzy sets and some operations the\nresults is not a fuzzy set with the same features (eg. multiplication of two\ntriangular fuzzy sets does not produce a triangular fuzzy set). One more\nproblem is the fuzzy spread -- fuzziness of the result increases with the\nnumber of operations. These facts can severely limit the application field of\nfuzzy numbers. In this paper we would like to revisite this problem with a\ndifferent kind of fuzzy numbers -- extensional fuzzy numbers. The paper defines\noperations on extensional fuzzy numbers and relational operators (=, >, >=, <,\n<=) for them. The proposed approach is illustrated with several applicational\nexamples. The C++ implementation is available from a public GitHub repository."}
{"id": "2510.21044", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.21044", "abs": "https://arxiv.org/abs/2510.21044", "authors": ["Kunal Shankar", "Ninad Gaikwad", "Anamika Dubey"], "title": "House Thermal Model Estimation: Robustness Across Seasons and Setpoints", "comment": "This manuscript is a version of our paper accepted at the 57th North\n  American Power Symposium (NAPS) 2025", "summary": "Achieving the flexibility from house heating, cooling, and ventilation\nsystems (HVAC) has the potential to enable large-scale demand response by\naggregating HVAC load adjustments across many homes. This demand response\nstrategy helps distribution grid to flexibly ramp-up or ramp-down local load\ndemand so that it can optimally match the bulk power system generation profile.\nHowever, achieving this capability requires house thermal models that are both\ncomputationally efficient and robust to operating conditions. In this work,\nparameters of the Resistance-Capacitance (RC) network thermal model for houses\nare estimated using three optimization algorithms: Nonlinear Least Squares\n(NLS), Batch Estimation (BE), and Maximum Likelihood Estimation (MLE). The\nresulting models are evaluated through a Forward-Simulation across four\ndifferent seasons and three setpoints. The results illustrate a principled way\nof selecting reduced order models and estimation methods with respect to the\nrobustness offered to seasonal and setpoint variations in training-testing\ndatasets"}
{"id": "2510.21027", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21027", "abs": "https://arxiv.org/abs/2510.21027", "authors": ["Zhe Fei", "Mehmet Yigit Turali", "Shreyas Rajesh", "Xinyang Dai", "Huyen Pham", "Pavan Holur", "Yuhui Zhu", "Larissa Mooney", "Yih-Ing Hser", "Vwani Roychowdhury"], "title": "Customizing Open Source LLMs for Quantitative Medication Attribute Extraction across Heterogeneous EHR Systems", "comment": "NeurIPS 2025: The Second Workshop on GenAI for Health: Potential,\n  Trust, and Policy Compliance", "summary": "Harmonizing medication data across Electronic Health Record (EHR) systems is\na persistent barrier to monitoring medications for opioid use disorder (MOUD).\nIn heterogeneous EHR systems, key prescription attributes are scattered across\ndifferently formatted fields and freetext notes. We present a practical\nframework that customizes open source large language models (LLMs), including\nLlama, Qwen, Gemma, and MedGemma, to extract a unified set of MOUD prescription\nattributes (prescription date, drug name, duration, total quantity, daily\nquantity, and refills) from heterogeneous, site specific data and compute a\nstandardized metric of medication coverage, \\emph{MOUD days}, per patient. Our\npipeline processes records directly in a fixed JSON schema, followed by\nlightweight normalization and cross-field consistency checks. We evaluate the\nsystem on prescription level EHR data from five clinics in a national OUD study\n(25{,}605 records from 1{,}257 patients), using a previously annotated\nbenchmark of 10{,}369 records (776 patients) as the ground truth. Performance\nis reported as coverage (share of records with a valid, matchable output) and\nrecord-level exact-match accuracy. Larger models perform best overall:\nQwen2.5-32B achieves \\textbf{93.4\\%} coverage with \\textbf{93.0\\%} exact-match\naccuracy across clinics, and MedGemma-27B attains\n\\textbf{93.1\\%}/\\textbf{92.2\\%}. A brief error review highlights three common\nissues and fixes: imputing missing dosage fields using within-drug norms,\nhandling monthly/weekly injectables (e.g., Vivitrol) by setting duration from\nthe documented schedule, and adding unit checks to prevent mass units (e.g.,\n``250 g'') from being misread as daily counts. By removing brittle,\nsite-specific ETL and supporting local, privacy-preserving deployment, this\napproach enables consistent cross-site analyses of MOUD exposure, adherence,\nand retention in real-world settings."}
{"id": "2510.20974", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20974", "abs": "https://arxiv.org/abs/2510.20974", "authors": ["Michael Bezick", "Vittorio Giammarino", "Ahmed H. Qureshi"], "title": "Robust Point Cloud Reinforcement Learning via PCA-Based Canonicalization", "comment": null, "summary": "Reinforcement Learning (RL) from raw visual input has achieved impressive\nsuccesses in recent years, yet it remains fragile to out-of-distribution\nvariations such as changes in lighting, color, and viewpoint. Point Cloud\nReinforcement Learning (PC-RL) offers a promising alternative by mitigating\nappearance-based brittleness, but its sensitivity to camera pose mismatches\ncontinues to undermine reliability in realistic settings. To address this\nchallenge, we propose PCA Point Cloud (PPC), a canonicalization framework\nspecifically tailored for downstream robotic control. PPC maps point clouds\nunder arbitrary rigid-body transformations to a unique canonical pose, aligning\nobservations to a consistent frame, thereby substantially decreasing\nviewpoint-induced inconsistencies. In our experiments, we show that PPC\nimproves robustness to unseen camera poses across challenging robotic tasks,\nproviding a principled alternative to domain randomization."}
{"id": "2510.21203", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2510.21203", "abs": "https://arxiv.org/abs/2510.21203", "authors": ["Sophia Hatz"], "title": "The Nuclear Analogy in AI Governance Research", "comment": "Hatz, S. (in press). The Nuclear Analogy in AI Governance Research.\n  In M. Furendal & M. Lundgren (Eds.), Handbook on the Global Governance of\n  Artificial Intelligence. Edward Elgar Publishing", "summary": "The analogy between Artificial Intelligence (AI) and nuclear weapons is\nprominent in academic and policy discourse on AI governance. This chapter\nreviews 43 scholarly works which explicitly draw on the nuclear domain to\nderive lessons for AI governance. We identify four problem areas where\nresearchers apply nuclear precedents: (1) early development and governance of\ntransformative technologies; (2) international security risks and strategy; (3)\ninternational institutions and agreements; and (4) domestic safety regulation.\nWhile nuclear-inspired AI proposals are often criticised due to differences\nacross domains, this review clarifies how historical analogies can inform\npolicy development even when technological domains differ substantially.\nValuable functions include providing conceptual frameworks for analyzing\nstrategic dynamics, offering cautionary lessons about unsuccessful governance\napproaches, and expanding policy imagination by legitimizing radical proposals.\nGiven that policymakers already invoke the nuclear analogy, continued critical\nengagement with these historical precedents remains essential for shaping\neffective global AI governance."}
{"id": "2510.21051", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.21051", "abs": "https://arxiv.org/abs/2510.21051", "authors": ["Rebecca G. Hart", "Wanjiku A. Makumi", "Rushikesh Kamalapurkar", "Warren E. Dixon"], "title": "Lyapunov-Based Physics-Informed Deep Neural Networks with Skew Symmetry Considerations", "comment": null, "summary": "Deep neural networks (DNNs) are powerful black-box function approximators\nwhich have been shown to yield improved performance compared to traditional\nneural network (NN) architectures. However, black-box algorithms do not\nincorporate known physics of the system and can yield results which are\nphysically implausible. Physics-informed neural networks (PINNs) have grown in\npopularity due to their ability to leverage known physical principles in the\nlearning process which has been empirically shown to improve performance\ncompared to traditional black-box methods. This paper introduces the first\nphysics-informed DNN controller for an Euler-Lagrange dynamic system where the\nadaptation laws are designed using a Lyapunov-based stability analysis to\naccount for the skew-symmetry property of the inertia matrix and\ncentripetal-Coriolis matrix. A Lyapunov-based stability analysis is provided to\nguarantee asymptotic convergence of the tracking error and the skew-symmetric\nprediction error. Simulations indicate that the developed update law\ndemonstrates improvement in individual and overall function approximation\ncapabilities when compared to a physics-informed adaptation law which does not\nincorporate knowledge of system symmetries."}
{"id": "2510.20986", "categories": ["econ.TH"], "pdf": "https://arxiv.org/pdf/2510.20986", "abs": "https://arxiv.org/abs/2510.20986", "authors": ["David Lagziel", "Ehud Lehrer"], "title": "Constrained Mediation: Bayesian Implementability of Joint Posteriors", "comment": null, "summary": "We examine information structures in settings with privately informed agents\nand an informationally constrained mediator who supplies additional public\nsignals. Our focus is on characterizing the set of posteriors that the mediator\ncan induce. To this end, we employ a graph-theoretic framework: states are\nrepresented as vertices, information sets correspond to edges, and a likelihood\nratio function on edges encodes the posterior beliefs. Within this framework,\nwe derive necessary and sufficient conditions, internal and external\nconsistency, for the rationalization of posteriors. Finally, we identify\nconditions under which a single mediator can implement multiple posteriors,\neffectively serving as a generator of Blackwell experiments."}
{"id": "2510.20881", "categories": ["cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.20881", "abs": "https://arxiv.org/abs/2510.20881", "authors": ["Anurag Shekhar"], "title": "Digital Permission Structures: How Celebrity Disclosure Enables Black Masculine Vulnerability in Online Mental Health Discourse", "comment": null, "summary": "Black men face a double barrier to mental health help-seeking: traditional\nmasculinity norms demanding emotional restrictiveness and systemic racism\nfostering institutional mistrust. While celebrity mental health disclosures\nshow promise for stigma reduction, limited research examines their impact on\nBlack masculine communities through digital platforms. This convergent\nmixed-methods study analysed 11,306 YouTube comments following rapper Lil\nWayne's unprecedented disclosure of childhood suicide attempt and lifelong\nmental health struggles. Quantitative analysis using VADER sentiment\nclassification, Latent Dirichlet Allocation topic modelling, and NRC emotion\nlexicon analysis revealed predominantly positive sentiment with systematic\ncommunity amplification of mental health discourse. Reflexive thematic analysis\nof 2,100 high-engagement comments identified eight themes, with peer support\nachieving the highest saturation, contradicting isolation narratives. Findings\nsupport a Digital Permission Structures Model demonstrating how intersectional\ncelebrity status (race + gender + high-status), hip-hop authenticity values,\nand digital platform affordances create triadic authorisation mechanisms\nenabling vulnerability expression. Community responses revealed communal\nmasculinity rooted in Ubuntu philosophy and active reconstruction of masculine\nnorms, positioning help-seeking as strength. Results challenge deficit-based\nmodels of Black masculinity, suggesting interventions should leverage\ncollectivism, partner with high-status cultural figures, employ strength-based\nmessaging, and centre hip-hop authenticity rather than imposing Western\nindividualistic frameworks. This study provides evidence-based strategies for\nculturally responsive mental health interventions addressing persistent\ndisparities in Black men's service utilisation."}
{"id": "2510.21051", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.21051", "abs": "https://arxiv.org/abs/2510.21051", "authors": ["Rebecca G. Hart", "Wanjiku A. Makumi", "Rushikesh Kamalapurkar", "Warren E. Dixon"], "title": "Lyapunov-Based Physics-Informed Deep Neural Networks with Skew Symmetry Considerations", "comment": null, "summary": "Deep neural networks (DNNs) are powerful black-box function approximators\nwhich have been shown to yield improved performance compared to traditional\nneural network (NN) architectures. However, black-box algorithms do not\nincorporate known physics of the system and can yield results which are\nphysically implausible. Physics-informed neural networks (PINNs) have grown in\npopularity due to their ability to leverage known physical principles in the\nlearning process which has been empirically shown to improve performance\ncompared to traditional black-box methods. This paper introduces the first\nphysics-informed DNN controller for an Euler-Lagrange dynamic system where the\nadaptation laws are designed using a Lyapunov-based stability analysis to\naccount for the skew-symmetry property of the inertia matrix and\ncentripetal-Coriolis matrix. A Lyapunov-based stability analysis is provided to\nguarantee asymptotic convergence of the tracking error and the skew-symmetric\nprediction error. Simulations indicate that the developed update law\ndemonstrates improvement in individual and overall function approximation\ncapabilities when compared to a physics-informed adaptation law which does not\nincorporate knowledge of system symmetries."}
{"id": "2510.21043", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.21043", "abs": "https://arxiv.org/abs/2510.21043", "authors": ["Benjamin Lange"], "title": "Epistemic Deference to AI", "comment": "12 pages", "summary": "When should we defer to AI outputs over human expert judgment? Drawing on\nrecent work in social epistemology, I motivate the idea that some AI systems\nqualify as Artificial Epistemic Authorities (AEAs) due to their demonstrated\nreliability and epistemic superiority. I then introduce AI Preemptionism, the\nview that AEA outputs should replace rather than supplement a user's\nindependent epistemic reasons. I show that classic objections to preemptionism\n- such as uncritical deference, epistemic entrenchment, and unhinging epistemic\nbases - apply in amplified form to AEAs, given their opacity, self-reinforcing\nauthority, and lack of epistemic failure markers. Against this, I develop a\nmore promising alternative: a total evidence view of AI deference. According to\nthis view, AEA outputs should function as contributory reasons rather than\noutright replacements for a user's independent epistemic considerations. This\napproach has three key advantages: (i) it mitigates expertise atrophy by\nkeeping human users engaged, (ii) it provides an epistemic case for meaningful\nhuman oversight and control, and (iii) it explains the justified mistrust of AI\nwhen reliability conditions are unmet. While demanding in practice, this\naccount offers a principled way to determine when AI deference is justified,\nparticularly in high-stakes contexts requiring rigorous reliability."}
{"id": "2510.21026", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21026", "abs": "https://arxiv.org/abs/2510.21026", "authors": ["Sai Haneesh Allu", "Jishnu Jaykumar P", "Ninad Khargonkar", "Tyler Summers", "Jian Yao", "Yu Xiang"], "title": "HRT1: One-Shot Human-to-Robot Trajectory Transfer for Mobile Manipulation", "comment": "14 pages, 11 figures and 3 tables. Project page is available at\n  \\url{https://irvlutd.github.io/HRT1/}", "summary": "We introduce a novel system for human-to-robot trajectory transfer that\nenables robots to manipulate objects by learning from human demonstration\nvideos. The system consists of four modules. The first module is a data\ncollection module that is designed to collect human demonstration videos from\nthe point of view of a robot using an AR headset. The second module is a video\nunderstanding module that detects objects and extracts 3D human-hand\ntrajectories from demonstration videos. The third module transfers a human-hand\ntrajectory into a reference trajectory of a robot end-effector in 3D space. The\nlast module utilizes a trajectory optimization algorithm to solve a trajectory\nin the robot configuration space that can follow the end-effector trajectory\ntransferred from the human demonstration. Consequently, these modules enable a\nrobot to watch a human demonstration video once and then repeat the same mobile\nmanipulation task in different environments, even when objects are placed\ndifferently from the demonstrations. Experiments of different manipulation\ntasks are conducted on a mobile manipulator to verify the effectiveness of our\nsystem"}
{"id": "2510.21219", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2510.21219", "abs": "https://arxiv.org/abs/2510.21219", "authors": ["Xiaoyuan Zhang", "Chengdong Ma", "Yizhe Huang", "Weidong Huang", "Siyuan Qi", "Song-Chun Zhu", "Xue Feng", "Yaodong Yang"], "title": "World Models Should Prioritize the Unification of Physical and Social Dynamics", "comment": null, "summary": "World models, which explicitly learn environmental dynamics to lay the\nfoundation for planning, reasoning, and decision-making, are rapidly advancing\nin predicting both physical dynamics and aspects of social behavior, yet\npredominantly in separate silos. This division results in a systemic failure to\nmodel the crucial interplay between physical environments and social\nconstructs, rendering current models fundamentally incapable of adequately\naddressing the true complexity of real-world systems where physical and social\nrealities are inextricably intertwined. This position paper argues that the\nsystematic, bidirectional unification of physical and social predictive\ncapabilities is the next crucial frontier for world model development. We\ncontend that comprehensive world models must holistically integrate objective\nphysical laws with the subjective, evolving, and context-dependent nature of\nsocial dynamics. Such unification is paramount for AI to robustly navigate\ncomplex real-world challenges and achieve more generalizable intelligence. This\npaper substantiates this imperative by analyzing core impediments to\nintegration, proposing foundational guiding principles (ACE Principles), and\noutlining a conceptual framework alongside a research roadmap towards truly\nholistic world models."}
{"id": "2510.21136", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.21136", "abs": "https://arxiv.org/abs/2510.21136", "authors": ["Chengming Lyu", "Zhenfei Tan", "Xiaoyuan Xu", "Chen Fu", "Zheng Yan", "Mohammad Shahidehpour"], "title": "Environment-Dependent Components Identification of Behind-the-Meter Resources via Inverse Optimization", "comment": null, "summary": "With the increasing penetration of behind-the-meter (BTM) resources, it is\nvital to monitor the components of these resources and deduce their response\nbehavior to external environment. Owing to data privacy, however, the\nappliance-wise measurement is invisible to the power system operator, which\nhinders the accurate modeling of load identification. To this end, this paper\nproposes a hybrid physics-inspired and data-driven framework for decomposing\nBTM components based on external measurement of total load and environmental\nfactors. The total load is decomposed into different environment-dependent\ncomponents, namely storage-like component, PV generation component,\nthermostatically-controlled load component, and periodic component. The overall\nload identification adopts a double-layer iterative solution framework. A\ndata-driven inverse optimization algorithm is developed to identify parameters\nof the energy storage-like component. The physics-inspired model is proposed to\nidentify the capacity and response of the rest components. The modeling\naccuracy and robustness of the proposed method are validated by numerical\ntests. The application significance of the proposed BTM identification method\nis also validated in electricity market clearing for reducing system operation\ncosts."}
{"id": "2510.20884", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20884", "abs": "https://arxiv.org/abs/2510.20884", "authors": ["Pranamya Kulkarni", "Puranjay Datta", "Burak Varıcı", "Emre Acartürk", "Karthikeyan Shanmugam", "Ali Tajer"], "title": "ROPES: Robotic Pose Estimation via Score-Based Causal Representation Learning", "comment": "A preliminary version of this paper appeared at NeurIPS 2025 Workshop\n  on Embodied World Models for Decision Making", "summary": "Causal representation learning (CRL) has emerged as a powerful unsupervised\nframework that (i) disentangles the latent generative factors underlying\nhigh-dimensional data, and (ii) learns the cause-and-effect interactions among\nthe disentangled variables. Despite extensive recent advances in\nidentifiability and some practical progress, a substantial gap remains between\ntheory and real-world practice. This paper takes a step toward closing that gap\nby bringing CRL to robotics, a domain that has motivated CRL. Specifically,\nthis paper addresses the well-defined robot pose estimation -- the recovery of\nposition and orientation from raw images -- by introducing Robotic Pose\nEstimation via Score-Based CRL (ROPES). Being an unsupervised framework, ROPES\nembodies the essence of interventional CRL by identifying those generative\nfactors that are actuated: images are generated by intrinsic and extrinsic\nlatent factors (e.g., joint angles, arm/limb geometry, lighting, background,\nand camera configuration) and the objective is to disentangle and recover the\ncontrollable latent variables, i.e., those that can be directly manipulated\n(intervened upon) through actuation. Interventional CRL theory shows that\nvariables that undergo variations via interventions can be identified. In\nrobotics, such interventions arise naturally by commanding actuators of various\njoints and recording images under varied controls. Empirical evaluations in\nsemi-synthetic manipulator experiments demonstrate that ROPES successfully\ndisentangles latent generative factors with high fidelity with respect to the\nground truth. Crucially, this is achieved by leveraging only distributional\nchanges, without using any labeled data. The paper also includes a comparison\nwith a baseline based on a recently proposed semi-supervised framework. This\npaper concludes by positioning robot pose estimation as a near-practical\ntestbed for CRL."}
{"id": "2510.21136", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.21136", "abs": "https://arxiv.org/abs/2510.21136", "authors": ["Chengming Lyu", "Zhenfei Tan", "Xiaoyuan Xu", "Chen Fu", "Zheng Yan", "Mohammad Shahidehpour"], "title": "Environment-Dependent Components Identification of Behind-the-Meter Resources via Inverse Optimization", "comment": null, "summary": "With the increasing penetration of behind-the-meter (BTM) resources, it is\nvital to monitor the components of these resources and deduce their response\nbehavior to external environment. Owing to data privacy, however, the\nappliance-wise measurement is invisible to the power system operator, which\nhinders the accurate modeling of load identification. To this end, this paper\nproposes a hybrid physics-inspired and data-driven framework for decomposing\nBTM components based on external measurement of total load and environmental\nfactors. The total load is decomposed into different environment-dependent\ncomponents, namely storage-like component, PV generation component,\nthermostatically-controlled load component, and periodic component. The overall\nload identification adopts a double-layer iterative solution framework. A\ndata-driven inverse optimization algorithm is developed to identify parameters\nof the energy storage-like component. The physics-inspired model is proposed to\nidentify the capacity and response of the rest components. The modeling\naccuracy and robustness of the proposed method are validated by numerical\ntests. The application significance of the proposed BTM identification method\nis also validated in electricity market clearing for reducing system operation\ncosts."}
{"id": "2510.21045", "categories": ["cs.AI", "cs.DB", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.21045", "abs": "https://arxiv.org/abs/2510.21045", "authors": ["Ali Khosravi Kazazi", "Zhenlong Li", "M. Naser Lessani", "Guido Cervone"], "title": "From Questions to Queries: An AI-powered Multi-Agent Framework for Spatial Text-to-SQL", "comment": null, "summary": "The complexity of Structured Query Language (SQL) and the specialized nature\nof geospatial functions in tools like PostGIS present significant barriers to\nnon-experts seeking to analyze spatial data. While Large Language Models (LLMs)\noffer promise for translating natural language into SQL (Text-to-SQL),\nsingle-agent approaches often struggle with the semantic and syntactic\ncomplexities of spatial queries. To address this, we propose a multi-agent\nframework designed to accurately translate natural language questions into\nspatial SQL queries. The framework integrates several innovative components,\nincluding a knowledge base with programmatic schema profiling and semantic\nenrichment, embeddings for context retrieval, and a collaborative multi-agent\npipeline as its core. This pipeline comprises specialized agents for entity\nextraction, metadata retrieval, query logic formulation, SQL generation, and a\nreview agent that performs programmatic and semantic validation of the\ngenerated SQL to ensure correctness (self-verification). We evaluate our system\nusing both the non-spatial KaggleDBQA benchmark and a new, comprehensive\nSpatialQueryQA benchmark that includes diverse geometry types, predicates, and\nthree levels of query complexity. On KaggleDBQA, the system achieved an overall\naccuracy of 81.2% (221 out of 272 questions) after the review agent's review\nand corrections. For spatial queries, the system achieved an overall accuracy\nof 87.7% (79 out of 90 questions), compared with 76.7% without the review\nagent. Beyond accuracy, results also show that in some instances the system\ngenerates queries that are more semantically aligned with user intent than\nthose in the benchmarks. This work makes spatial analysis more accessible, and\nprovides a robust, generalizable foundation for spatial Text-to-SQL systems,\nadvancing the development of autonomous GIS."}
{"id": "2510.21046", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21046", "abs": "https://arxiv.org/abs/2510.21046", "authors": ["Zlatan Ajanović", "Ravi Prakash", "Leandro de Souza Rosa", "Jens Kober"], "title": "Sequentially Teaching Sequential Tasks $(ST)^2$: Teaching Robots Long-horizon Manipulation Skills", "comment": null, "summary": "Learning from demonstration is effective for teaching robots complex skills\nwith high sample efficiency. However, teaching long-horizon tasks with multiple\nskills is difficult, as deviations accumulate, distributional shift increases,\nand human teachers become fatigued, raising the chance of failure. In this\nwork, we study user responses to two teaching frameworks: (i) a traditional\nmonolithic approach, where users demonstrate the entire trajectory of a\nlong-horizon task; and (ii) a sequential approach, where the task is segmented\nby the user and demonstrations are provided step by step. To support this\nstudy, we introduce $(ST)^2$, a sequential method for learning long-horizon\nmanipulation tasks that allows users to control the teaching flow by defining\nkey points, enabling incremental and structured demonstrations. We conducted a\nuser study on a restocking task with 16 participants in a realistic retail\nenvironment to evaluate both user preference and method effectiveness. Our\nobjective and subjective results show that both methods achieve similar\ntrajectory quality and success rates. Some participants preferred the\nsequential approach for its iterative control, while others favored the\nmonolithic approach for its simplicity."}
{"id": "2510.21526", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2510.21526", "abs": "https://arxiv.org/abs/2510.21526", "authors": ["Isaac Johnson", "Yu-Ming Liou", "Jacob Rogers", "Aaron Shaw", "Leila Zia"], "title": "Recommended Practices for NPOV Research on Wikipedia", "comment": null, "summary": "Writing Wikipedia with a neutral point of view is one of the five pillars of\nWikipedia. Although the topic is core to Wikipedia, it is relatively\nunderstudied considering hundreds of research studies are published annually\nabout the project. We hypothesize that part of the reason for the low research\nactivity on the topic is that Wikipedia's definition of neutrality and its\nimportance are not well understood within the research community. Neutrality is\nalso an inherently challenging and contested concept. Our aim with this paper\nis to accelerate high quality research in this space that can help Wikipedia\ncommunities continue to improve their work in writing the encyclopedia. We do\nthis by helping researchers to learn what Neutral Point of View means in the\ncontext of Wikipedia, identifying some common challenges with studying NPOV and\nhow to navigate them, and offering guidance on how researchers can communicate\nthe results of their work for increased impact on the ground for the benefit of\nWikipedia."}
{"id": "2510.21179", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.21179", "abs": "https://arxiv.org/abs/2510.21179", "authors": ["Frederik Wagner Madsen", "Joy Dalmacio Billanes", "Bo Nørregaard Jørgensen", "Zheng Ma"], "title": "Green Hydrogen under Uncertainty: Evaluating Power-to-X Strategies Using Agent-Based Simulation and Multi-Criteria Decision Framework", "comment": null, "summary": "The transition toward net-zero energy systems requires scalable and\ncost-effective deployment of Power-to-X technologies, particularly green\nhydrogen production. Despite increasing investments, a critical research gap\nremains in dynamically assessing how different operational strategies affect\nthe feasibility of hydrogen production under real-world energy market\nconditions. Most existing studies rely on static, techno-economic models and\noverlook actor interactions, infrastructure limitations, and regulatory\ncomplexity. This paper presents a novel modeling framework that integrates\nagent-based simulation with multi-criteria decision-making to evaluate green\nhydrogen production strategies using co-located wind and solar generation.\nThree operational strategies - grid-only, on-site-only, and hybrid - are\napplied across three electrolyzer capacity levels (10 MW, 50 MW, and 100 MW)\nwithin a Danish case study. Real electricity tariffs, emissions factors, and\nmarket data are used to simulate technical, economic, and environmental\nperformance indicators. The results show that hybrid strategies consistently\noutperform grid-only configurations in terms of cost and emissions while\nmaintaining stable hydrogen output. Although on-site-only strategies minimize\nemissions and costs, they fail to meet fixed production demands. This framework\noffers novel scientific contributions by modeling dynamic actor interactions\nand integrating system performance evaluation into strategic planning.\nPractically, it provides actionable insights for energy planners and\npolicymakers designing resilient and efficient Power-to-X systems in\nrenewable-rich contexts."}
{"id": "2510.20916", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.20916", "abs": "https://arxiv.org/abs/2510.20916", "authors": ["Sydney M. Katz", "Robert J. Moss", "Dylan M. Asmar", "Wesley A. Olson", "James K. Kuchar", "Mykel J. Kochenderfer"], "title": "Aircraft Collision Avoidance Systems: Technological Challenges and Solutions on the Path to Regulatory Acceptance", "comment": "32 pages, 9 figures", "summary": "Aircraft collision avoidance systems is critical to modern aviation. These\nsystems are designed to predict potential collisions between aircraft and\nrecommend appropriate avoidance actions. Creating effective collision avoidance\nsystems requires solutions to a variety of technical challenges related to\nsurveillance, decision making, and validation. These challenges have sparked\nsignificant research and development efforts over the past several decades that\nhave resulted in a variety of proposed solutions. This article provides an\noverview of these challenges and solutions with an emphasis on those that have\nbeen put through a rigorous validation process and accepted by regulatory\nbodies. The challenges posed by the collision avoidance problem are often\npresent in other domains, and aircraft collision avoidance systems can serve as\ncase studies that provide valuable insights for a wide range of safety-critical\nsystems."}
{"id": "2510.21179", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.21179", "abs": "https://arxiv.org/abs/2510.21179", "authors": ["Frederik Wagner Madsen", "Joy Dalmacio Billanes", "Bo Nørregaard Jørgensen", "Zheng Ma"], "title": "Green Hydrogen under Uncertainty: Evaluating Power-to-X Strategies Using Agent-Based Simulation and Multi-Criteria Decision Framework", "comment": null, "summary": "The transition toward net-zero energy systems requires scalable and\ncost-effective deployment of Power-to-X technologies, particularly green\nhydrogen production. Despite increasing investments, a critical research gap\nremains in dynamically assessing how different operational strategies affect\nthe feasibility of hydrogen production under real-world energy market\nconditions. Most existing studies rely on static, techno-economic models and\noverlook actor interactions, infrastructure limitations, and regulatory\ncomplexity. This paper presents a novel modeling framework that integrates\nagent-based simulation with multi-criteria decision-making to evaluate green\nhydrogen production strategies using co-located wind and solar generation.\nThree operational strategies - grid-only, on-site-only, and hybrid - are\napplied across three electrolyzer capacity levels (10 MW, 50 MW, and 100 MW)\nwithin a Danish case study. Real electricity tariffs, emissions factors, and\nmarket data are used to simulate technical, economic, and environmental\nperformance indicators. The results show that hybrid strategies consistently\noutperform grid-only configurations in terms of cost and emissions while\nmaintaining stable hydrogen output. Although on-site-only strategies minimize\nemissions and costs, they fail to meet fixed production demands. This framework\noffers novel scientific contributions by modeling dynamic actor interactions\nand integrating system performance evaluation into strategic planning.\nPractically, it provides actionable insights for energy planners and\npolicymakers designing resilient and efficient Power-to-X systems in\nrenewable-rich contexts."}
{"id": "2510.21093", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21093", "abs": "https://arxiv.org/abs/2510.21093", "authors": ["Siyong Chen", "Jinbo Wen", "Jiawen Kang", "Tenghui Huang", "Xumin Huang", "Yuanjia Su", "Hudan Pan", "Zishao Zhong", "Dusit Niyato", "Shengli Xie", "Dong In Kim"], "title": "MedAlign: A Synergistic Framework of Multimodal Preference Optimization and Federated Meta-Cognitive Reasoning", "comment": null, "summary": "Recently, large models have shown significant potential for smart healthcare.\nHowever, the deployment of Large Vision-Language Models (LVLMs) for clinical\nservices is currently hindered by three critical challenges: a tendency to\nhallucinate answers not grounded in visual evidence, the inefficiency of\nfixed-depth reasoning, and the difficulty of multi-institutional collaboration.\nTo address these challenges, in this paper, we develop MedAlign, a novel\nframework to ensure visually accurate LVLM responses for Medical Visual\nQuestion Answering (Med-VQA). Specifically, we first propose a multimodal\nDirect Preference Optimization (mDPO) objective to explicitly align preference\nlearning with visual context. We then design a Retrieval-Aware\nMixture-of-Experts (RA-MoE) architecture that utilizes image and text\nsimilarity to route queries to a specialized and context-augmented LVLM (i.e.,\nan expert), thereby mitigating hallucinations in LVLMs. To achieve adaptive\nreasoning and facilitate multi-institutional collaboration, we propose a\nfederated governance mechanism, where the selected expert, fine-tuned on\nclinical datasets based on mDPO, locally performs iterative Chain-of-Thought\n(CoT) reasoning via the local meta-cognitive uncertainty estimator. Extensive\nexperiments on three representative Med-VQA datasets demonstrate that MedAlign\nachieves state-of-the-art performance, outperforming strong retrieval-augmented\nbaselines by up to $11.85\\%$ in F1-score, and simultaneously reducing the\naverage reasoning length by $51.60\\%$ compared with fixed-depth CoT approaches."}
{"id": "2510.21074", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21074", "abs": "https://arxiv.org/abs/2510.21074", "authors": ["Mitchell E. C. Sabbadini", "Andrew H. Liu", "Joseph Ruan", "Tyler S. Wilson", "Zachary Kingston", "Jonathan D. Gammell"], "title": "Revisiting Replanning from Scratch: Real-Time Incremental Planning with Fast Almost-Surely Asymptotically Optimal Planners", "comment": "Submitted to IEEE International Conference on Robotics and Automation\n  (ICRA) 2026, 8 pages, 5 figures, 1 table. A video of this work can be found\n  at https://www.youtube.com/watch?v=XaZrFy8wGZs", "summary": "Robots operating in changing environments either predict obstacle changes\nand/or plan quickly enough to react to them. Predictive approaches require a\nstrong prior about the position and motion of obstacles. Reactive approaches\nrequire no assumptions about their environment but must replan quickly and find\nhigh-quality paths to navigate effectively.\n  Reactive approaches often reuse information between queries to reduce\nplanning cost. These techniques are conceptually sound but updating dense\nplanning graphs when information changes can be computationally prohibitive. It\ncan also require significant effort to detect the changes in some applications.\n  This paper revisits the long-held assumption that reactive replanning\nrequires updating existing plans. It shows that the incremental planning\nproblem can alternatively be solved more efficiently as a series of independent\nproblems using fast almost-surely asymptotically optimal (ASAO) planning\nalgorithms. These ASAO algorithms quickly find an initial solution and converge\ntowards an optimal solution which allows them to find consistent global plans\nin the presence of changing obstacles without requiring explicit plan reuse.\nThis is demonstrated with simulated experiments where Effort Informed Trees\n(EIT*) finds shorter median solution paths than the tested reactive planning\nalgorithms and is further validated using Asymptotically Optimal RRT-Connect\n(AORRTC) on a real-world planning problem on a robot arm."}
{"id": "2510.21043", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.21043", "abs": "https://arxiv.org/abs/2510.21043", "authors": ["Benjamin Lange"], "title": "Epistemic Deference to AI", "comment": "12 pages", "summary": "When should we defer to AI outputs over human expert judgment? Drawing on\nrecent work in social epistemology, I motivate the idea that some AI systems\nqualify as Artificial Epistemic Authorities (AEAs) due to their demonstrated\nreliability and epistemic superiority. I then introduce AI Preemptionism, the\nview that AEA outputs should replace rather than supplement a user's\nindependent epistemic reasons. I show that classic objections to preemptionism\n- such as uncritical deference, epistemic entrenchment, and unhinging epistemic\nbases - apply in amplified form to AEAs, given their opacity, self-reinforcing\nauthority, and lack of epistemic failure markers. Against this, I develop a\nmore promising alternative: a total evidence view of AI deference. According to\nthis view, AEA outputs should function as contributory reasons rather than\noutright replacements for a user's independent epistemic considerations. This\napproach has three key advantages: (i) it mitigates expertise atrophy by\nkeeping human users engaged, (ii) it provides an epistemic case for meaningful\nhuman oversight and control, and (iii) it explains the justified mistrust of AI\nwhen reliability conditions are unmet. While demanding in practice, this\naccount offers a principled way to determine when AI deference is justified,\nparticularly in high-stakes contexts requiring rigorous reliability."}
{"id": "2510.21227", "categories": ["eess.SY", "cs.SY", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.21227", "abs": "https://arxiv.org/abs/2510.21227", "authors": ["Ke Sun", "Jingyi Yan", "Zhenglin Li", "Shaorong Xie"], "title": "The Role of Information Incompleteness in Defending Against Stealth Attacks", "comment": null, "summary": "The effectiveness of Data Injections Attacks (DIAs) critically depends on the\ncompleteness of the system information accessible to adversaries. This\nrelationship positions information incompleteness enhancement as a vital\ndefense strategy for degrading DIA performance. In this paper, we focus on the\ninformation-theoretic stealth attacks, where the attacker encounters a\nfundamental tradeoff between the attack stealthiness and destructiveness.\nSpecifically, we systematically characterize how incomplete admittance\ninformation impacts the dual objectives. In particular, we establish sufficient\nconditions for two distinct operational regimes: (i) stealthiness intensifies\nwhile destructive potential diminishes and (ii) destructiveness increases while\nstealth capability weakens. For scenarios beyond these regimes, we propose a\nmaximal incompleteness strategy to optimally degrade stealth capability. To\nsolve the associated optimization problem, the feasible region is reduced\nwithout excluding the optimal solution, and a heuristic algorithm is then\nintroduced to effectively identify the near-optimal solutions within the\nreduced region. Numerical simulations are conducted on IEEE test systems to\nvalidate the findings."}
{"id": "2510.20927", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2510.20927", "abs": "https://arxiv.org/abs/2510.20927", "authors": ["Tom Reed", "Tegan McCaslin", "Luca Righetti"], "title": "What do model reports say about their ChemBio benchmark evaluations? Comparing recent releases to the STREAM framework", "comment": "12 pages, 6 figures. Includes appendices", "summary": "Most frontier AI developers publicly document their safety evaluations of new\nAI models in model reports, including testing for chemical and biological\n(ChemBio) misuse risks. This practice provides a window into the methodology of\nthese evaluations, helping to build public trust in AI systems, and enabling\nthird party review in the still-emerging science of AI evaluation. But what\naspects of evaluation methodology do developers currently include -- or omit --\nin their reports? This paper examines three frontier AI model reports published\nin spring 2025 with among the most detailed documentation: OpenAI's o3,\nAnthropic's Claude 4, and Google DeepMind's Gemini 2.5 Pro. We compare these\nusing the STREAM (v1) standard for reporting ChemBio benchmark evaluations.\nEach model report included some useful details that the others did not, and all\nmodel reports were found to have areas for development, suggesting that\ndevelopers could benefit from adopting one another's best reporting practices.\nWe identified several items where reporting was less well-developed across all\nmodel reports, such as providing examples of test material, and including a\ndetailed list of elicitation conditions. Overall, we recommend that AI\ndevelopers continue to strengthen the emerging science of evaluation by working\ntowards greater transparency in areas where reporting currently remains\nlimited."}
{"id": "2510.21227", "categories": ["eess.SY", "cs.SY", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.21227", "abs": "https://arxiv.org/abs/2510.21227", "authors": ["Ke Sun", "Jingyi Yan", "Zhenglin Li", "Shaorong Xie"], "title": "The Role of Information Incompleteness in Defending Against Stealth Attacks", "comment": null, "summary": "The effectiveness of Data Injections Attacks (DIAs) critically depends on the\ncompleteness of the system information accessible to adversaries. This\nrelationship positions information incompleteness enhancement as a vital\ndefense strategy for degrading DIA performance. In this paper, we focus on the\ninformation-theoretic stealth attacks, where the attacker encounters a\nfundamental tradeoff between the attack stealthiness and destructiveness.\nSpecifically, we systematically characterize how incomplete admittance\ninformation impacts the dual objectives. In particular, we establish sufficient\nconditions for two distinct operational regimes: (i) stealthiness intensifies\nwhile destructive potential diminishes and (ii) destructiveness increases while\nstealth capability weakens. For scenarios beyond these regimes, we propose a\nmaximal incompleteness strategy to optimally degrade stealth capability. To\nsolve the associated optimization problem, the feasible region is reduced\nwithout excluding the optimal solution, and a heuristic algorithm is then\nintroduced to effectively identify the near-optimal solutions within the\nreduced region. Numerical simulations are conducted on IEEE test systems to\nvalidate the findings."}
{"id": "2510.21110", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21110", "abs": "https://arxiv.org/abs/2510.21110", "authors": ["Mingxuan Li", "Junzhe Zhang", "Elias Bareinboim"], "title": "Confounding Robust Deep Reinforcement Learning: A Causal Approach", "comment": "NeurIPS 2025", "summary": "A key task in Artificial Intelligence is learning effective policies for\ncontrolling agents in unknown environments to optimize performance measures.\nOff-policy learning methods, like Q-learning, allow learners to make optimal\ndecisions based on past experiences. This paper studies off-policy learning\nfrom biased data in complex and high-dimensional domains where \\emph{unobserved\nconfounding} cannot be ruled out a priori. Building on the well-celebrated Deep\nQ-Network (DQN), we propose a novel deep reinforcement learning algorithm\nrobust to confounding biases in observed data. Specifically, our algorithm\nattempts to find a safe policy for the worst-case environment compatible with\nthe observations. We apply our method to twelve confounded Atari games, and\nfind that it consistently dominates the standard DQN in all games where the\nobserved input to the behavioral and target policies mismatch and unobserved\nconfounders exist."}
{"id": "2510.21121", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21121", "abs": "https://arxiv.org/abs/2510.21121", "authors": ["Haibo Zhao", "Yu Qi", "Boce Hu", "Yizhe Zhu", "Ziyan Chen", "Heng Tian", "Xupeng Zhu", "Owen Howell", "Haojie Huang", "Robin Walters", "Dian Wang", "Robert Platt"], "title": "Generalizable Hierarchical Skill Learning via Object-Centric Representation", "comment": null, "summary": "We present Generalizable Hierarchical Skill Learning (GSL), a novel framework\nfor hierarchical policy learning that significantly improves policy\ngeneralization and sample efficiency in robot manipulation. One core idea of\nGSL is to use object-centric skills as an interface that bridges the high-level\nvision-language model and the low-level visual-motor policy. Specifically, GSL\ndecomposes demonstrations into transferable and object-canonicalized skill\nprimitives using foundation models, ensuring efficient low-level skill learning\nin the object frame. At test time, the skill-object pairs predicted by the\nhigh-level agent are fed to the low-level module, where the inferred canonical\nactions are mapped back to the world frame for execution. This structured yet\nflexible design leads to substantial improvements in sample efficiency and\ngeneralization of our method across unseen spatial arrangements, object\nappearances, and task compositions. In simulation, GSL trained with only 3\ndemonstrations per task outperforms baselines trained with 30 times more data\nby 15.5 percent on unseen tasks. In real-world experiments, GSL also surpasses\nthe baseline trained with 10 times more data."}
{"id": "2510.21238", "categories": ["eess.SY", "cs.AI", "cs.IT", "cs.SY", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.21238", "abs": "https://arxiv.org/abs/2510.21238", "authors": ["Wangqian Chen", "Junting Chen", "Shuguang Cui"], "title": "Physics-Informed Neural Networks for MIMO Beam Map and Environment Reconstruction", "comment": null, "summary": "As communication networks evolve towards greater complexity (e.g., 6G and\nbeyond), a deep understanding of the wireless environment becomes increasingly\ncrucial. When explicit knowledge of the environment is unavailable,\ngeometry-aware feature extraction from channel state information (CSI) emerges\nas a pivotal methodology to bridge physical-layer measurements with network\nintelligence. This paper proposes to explore the received signal strength (RSS)\ndata, without explicit 3D environment knowledge, to jointly construct the radio\nbeam map and environmental geometry for a multiple-input multiple-output (MIMO)\nsystem. Unlike existing methods that only learn blockage structures, we propose\nan oriented virtual obstacle model that captures the geometric features of both\nblockage and reflection. Reflective zones are formulated to identify relevant\nreflected paths according to the geometry relation of the environment. We\nderive an analytical expression for the reflective zone and further analyze its\ngeometric characteristics to develop a reformulation that is more compatible\nwith deep learning representations. A physics-informed deep learning framework\nthat incorporates the reflective-zone-based geometry model is proposed to learn\nthe blockage, reflection, and scattering components, along with the beam\npattern, which leverages physics prior knowledge to enhance network\ntransferability. Numerical experiments demonstrate that, in addition to\nreconstructing the blockage and reflection geometry, the proposed model can\nconstruct a more accurate MIMO beam map with a 32%-48% accuracy improvement."}
{"id": "2510.20965", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20965", "abs": "https://arxiv.org/abs/2510.20965", "authors": ["Jesse Haworth", "Juo-Tung Chen", "Nigel Nelson", "Ji Woong Kim", "Masoud Moghani", "Chelsea Finn", "Axel Krieger"], "title": "SutureBot: A Precision Framework & Benchmark For Autonomous End-to-End Suturing", "comment": "10 pages, 5 figures, 4 tables, NeurIPS 2025", "summary": "Robotic suturing is a prototypical long-horizon dexterous manipulation task,\nrequiring coordinated needle grasping, precise tissue penetration, and secure\nknot tying. Despite numerous efforts toward end-to-end autonomy, a fully\nautonomous suturing pipeline has yet to be demonstrated on physical hardware.\nWe introduce SutureBot: an autonomous suturing benchmark on the da Vinci\nResearch Kit (dVRK), spanning needle pickup, tissue insertion, and knot tying.\nTo ensure repeatability, we release a high-fidelity dataset comprising 1,890\nsuturing demonstrations. Furthermore, we propose a goal-conditioned framework\nthat explicitly optimizes insertion-point precision, improving targeting\naccuracy by 59\\%-74\\% over a task-only baseline. To establish this task as a\nbenchmark for dexterous imitation learning, we evaluate state-of-the-art\nvision-language-action (VLA) models, including $\\pi_0$, GR00T N1, OpenVLA-OFT,\nand multitask ACT, each augmented with a high-level task-prediction policy.\nAutonomous suturing is a key milestone toward achieving robotic autonomy in\nsurgery. These contributions support reproducible evaluation and development of\nprecision-focused, long-horizon dexterous manipulation policies necessary for\nend-to-end suturing. Dataset is available at:\nhttps://huggingface.co/datasets/jchen396/suturebot"}
{"id": "2510.21238", "categories": ["eess.SY", "cs.AI", "cs.IT", "cs.SY", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.21238", "abs": "https://arxiv.org/abs/2510.21238", "authors": ["Wangqian Chen", "Junting Chen", "Shuguang Cui"], "title": "Physics-Informed Neural Networks for MIMO Beam Map and Environment Reconstruction", "comment": null, "summary": "As communication networks evolve towards greater complexity (e.g., 6G and\nbeyond), a deep understanding of the wireless environment becomes increasingly\ncrucial. When explicit knowledge of the environment is unavailable,\ngeometry-aware feature extraction from channel state information (CSI) emerges\nas a pivotal methodology to bridge physical-layer measurements with network\nintelligence. This paper proposes to explore the received signal strength (RSS)\ndata, without explicit 3D environment knowledge, to jointly construct the radio\nbeam map and environmental geometry for a multiple-input multiple-output (MIMO)\nsystem. Unlike existing methods that only learn blockage structures, we propose\nan oriented virtual obstacle model that captures the geometric features of both\nblockage and reflection. Reflective zones are formulated to identify relevant\nreflected paths according to the geometry relation of the environment. We\nderive an analytical expression for the reflective zone and further analyze its\ngeometric characteristics to develop a reformulation that is more compatible\nwith deep learning representations. A physics-informed deep learning framework\nthat incorporates the reflective-zone-based geometry model is proposed to learn\nthe blockage, reflection, and scattering components, along with the beam\npattern, which leverages physics prior knowledge to enhance network\ntransferability. Numerical experiments demonstrate that, in addition to\nreconstructing the blockage and reflection geometry, the proposed model can\nconstruct a more accurate MIMO beam map with a 32%-48% accuracy improvement."}
{"id": "2510.21117", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21117", "abs": "https://arxiv.org/abs/2510.21117", "authors": ["Chunghyun Han", "Alfio Gliozzo", "Junkyu Lee", "Agostino Capponi"], "title": "DAO-AI: Evaluating Collective Decision-Making through Agentic AI in Decentralized Governance", "comment": "12 pages, 2 Figures", "summary": "This paper presents a first empirical study of agentic AI as autonomous\ndecision-makers in decentralized governance. Using more than 3K proposals from\nmajor protocols, we build an agentic AI voter that interprets proposal\ncontexts, retrieves historical deliberation data, and independently determines\nits voting position. The agent operates within a realistic financial simulation\nenvironment grounded in verifiable blockchain data, implemented through a\nmodular composable program (MCP) workflow that defines data flow and tool usage\nvia Agentics framework. We evaluate how closely the agent's decisions align\nwith the human and token-weighted outcomes, uncovering strong alignments\nmeasured by carefully designed evaluation metrics. Our findings demonstrate\nthat agentic AI can augment collective decision-making by producing\ninterpretable, auditable, and empirically grounded signals in realistic DAO\ngovernance settings. The study contributes to the design of explainable and\neconomically rigorous AI agents for decentralized financial systems."}
{"id": "2510.21164", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21164", "abs": "https://arxiv.org/abs/2510.21164", "authors": ["Shamistan Karimov", "Elian Neppel", "Shreya Santra", "Kentaro Uno", "Kazuya Yoshida"], "title": "An Agnostic End-Effector Alignment Controller for Robust Assembly of Modular Space Robots", "comment": "6 pages, 12 figures. Accepted at iSparo 2025 | Video:\n  https://youtu.be/BW0YgSrvuDo", "summary": "Modular robots offer reconfigurability and fault tolerance essential for\nlunar missions, but require controllers that adapt safely to real-world\ndisturbances. We build on our previous hardware-agnostic actuator\nsynchronization in Motion Stack to develop a new controller enforcing adaptive\nvelocity bounds via a dynamic hypersphere clamp. Using only real-time\nend-effector and target pose measurements, the controller adjusts its\ntranslational and rotational speed limits to ensure smooth, stable alignment\nwithout abrupt motions. We implemented two variants, a discrete, step-based\nversion and a continuous, velocity-based version, and tested them on two\nMoonBot limbs in JAXA's lunar environment simulator. Field trials demonstrate\nthat the step-based variant produces highly predictable, low-wobble motions,\nwhile the continuous variant converges more quickly and maintains\nmillimeter-level positional accuracy, and both remain robust across limbs with\ndiffering mechanical imperfections and sensing noise (e.g., backlash and flex).\nThese results highlight the flexibility and robustness of our robot-agnostic\nframework for autonomous self-assembly and reconfiguration under harsh\nconditions."}
{"id": "2510.21294", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.21294", "abs": "https://arxiv.org/abs/2510.21294", "authors": ["Maxime Grosso", "Pierre Riedinger", "Jamal Daafouz"], "title": "The PhasorArray Toolbox for Harmonic Analysis and Control Design", "comment": null, "summary": "We present a MATLAB package called the Pha-sorArray Toolbox that has been\ndeveloped to make harmonic analysis and control methods both practical and\nuser-friendly. The toolbox adopts an object-oriented architecture that enables\nintuitive manipulation of periodic matrices through overloaded operators for\naddition, multiplication, convolution, and automatic Toeplitz construction. Its\nadvanced features include harmonic Sylvester, Lyapunov and Riccati equations\nsolvers, and seamless integration with YALMIP, thereby facilitating advanced\ncontrol and analysis techniques based on Linear Matrix Inequalities (LMIs) in\nthe harmonic framework."}
{"id": "2510.20974", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20974", "abs": "https://arxiv.org/abs/2510.20974", "authors": ["Michael Bezick", "Vittorio Giammarino", "Ahmed H. Qureshi"], "title": "Robust Point Cloud Reinforcement Learning via PCA-Based Canonicalization", "comment": null, "summary": "Reinforcement Learning (RL) from raw visual input has achieved impressive\nsuccesses in recent years, yet it remains fragile to out-of-distribution\nvariations such as changes in lighting, color, and viewpoint. Point Cloud\nReinforcement Learning (PC-RL) offers a promising alternative by mitigating\nappearance-based brittleness, but its sensitivity to camera pose mismatches\ncontinues to undermine reliability in realistic settings. To address this\nchallenge, we propose PCA Point Cloud (PPC), a canonicalization framework\nspecifically tailored for downstream robotic control. PPC maps point clouds\nunder arbitrary rigid-body transformations to a unique canonical pose, aligning\nobservations to a consistent frame, thereby substantially decreasing\nviewpoint-induced inconsistencies. In our experiments, we show that PPC\nimproves robustness to unseen camera poses across challenging robotic tasks,\nproviding a principled alternative to domain randomization."}
{"id": "2510.21294", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.21294", "abs": "https://arxiv.org/abs/2510.21294", "authors": ["Maxime Grosso", "Pierre Riedinger", "Jamal Daafouz"], "title": "The PhasorArray Toolbox for Harmonic Analysis and Control Design", "comment": null, "summary": "We present a MATLAB package called the Pha-sorArray Toolbox that has been\ndeveloped to make harmonic analysis and control methods both practical and\nuser-friendly. The toolbox adopts an object-oriented architecture that enables\nintuitive manipulation of periodic matrices through overloaded operators for\naddition, multiplication, convolution, and automatic Toeplitz construction. Its\nadvanced features include harmonic Sylvester, Lyapunov and Riccati equations\nsolvers, and seamless integration with YALMIP, thereby facilitating advanced\ncontrol and analysis techniques based on Linear Matrix Inequalities (LMIs) in\nthe harmonic framework."}
{"id": "2510.21143", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21143", "abs": "https://arxiv.org/abs/2510.21143", "authors": ["Jihyun Lee", "Yejin Min", "San Kim", "Yejin Jeon", "SungJun Yang", "Hyounghun Kim", "Gary Geunbae Lee"], "title": "PanicToCalm: A Proactive Counseling Agent for Panic Attacks", "comment": null, "summary": "Panic attacks are acute episodes of fear and distress, in which timely,\nappropriate intervention can significantly help individuals regain stability.\nHowever, suitable datasets for training such models remain scarce due to\nethical and logistical issues. To address this, we introduce PACE, which is a\ndataset that includes high-distress episodes constructed from first-person\nnarratives, and structured around the principles of Psychological First Aid\n(PFA). Using this data, we train PACER, a counseling model designed to provide\nboth empathetic and directive support, which is optimized through supervised\nlearning and simulated preference alignment. To assess its effectiveness, we\npropose PanicEval, a multi-dimensional framework covering general counseling\nquality and crisis-specific strategies. Experimental results show that PACER\noutperforms strong baselines in both counselor-side metrics and client affect\nimprovement. Human evaluations further confirm its practical value, with PACER\nconsistently preferred over general, CBT-based, and GPT-4-powered models in\npanic scenarios (Code is available at https://github.com/JihyunLee1/PanicToCalm\n)."}
{"id": "2510.21215", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21215", "abs": "https://arxiv.org/abs/2510.21215", "authors": ["Shuoshuo Ding", "Tiedong Zhang", "Dapeng Jiang", "Ming Lei"], "title": "Underwater Visual-Inertial-Acoustic-Depth SLAM with DVL Preintegration for Degraded Environments", "comment": "10 pages, 10 figures", "summary": "Visual degradation caused by limited visibility, insufficient lighting, and\nfeature scarcity in underwater environments presents significant challenges to\nvisual-inertial simultaneous localization and mapping (SLAM) systems. To\naddress these challenges, this paper proposes a graph-based\nvisual-inertial-acoustic-depth SLAM system that integrates a stereo camera, an\ninertial measurement unit (IMU), the Doppler velocity log (DVL), and a pressure\nsensor. The key innovation lies in the tight integration of four distinct\nsensor modalities to ensure reliable operation, even under degraded visual\nconditions. To mitigate DVL drift and improve measurement efficiency, we\npropose a novel velocity-bias-based DVL preintegration strategy. At the\nfrontend, hybrid tracking strategies and acoustic-inertial-depth joint\noptimization enhance system stability. Additionally, multi-source hybrid\nresiduals are incorporated into a graph optimization framework. Extensive\nquantitative and qualitative analyses of the proposed system are conducted in\nboth simulated and real-world underwater scenarios. The results demonstrate\nthat our approach outperforms current state-of-the-art stereo visual-inertial\nSLAM systems in both stability and localization accuracy, exhibiting\nexceptional robustness, particularly in visually challenging environments."}
{"id": "2510.21308", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.21308", "abs": "https://arxiv.org/abs/2510.21308", "authors": ["Zhengang Zhong", "Ehecatl Antonio del Rio-Chanona", "Panagiotis Petsagkourakis"], "title": "Data-driven Koopman MPC using Mixed Stochastic-Deterministic Tubes", "comment": "This is the accepted version. It will appear in Journal of Process\n  Control, 2025", "summary": "This paper presents a novel data-driven stochastic MPC design for\ndiscrete-time nonlinear systems with additive disturbances by leveraging the\nKoopman operator and a distributionally robust optimization (DRO) framework. By\nlifting the dynamical system into a linear space, we achieve a\nfinite-dimensional approximation of the Koopman operator. We explicitly account\nfor the modeling approximation and additive disturbance error by a mixed\nstochastic-deterministic tube for the lifted linear model. This ensures the\nregulation of the original nonlinear system while complying with the\nprespecified constraints. Stochastic and deterministic tubes are constructed\nusing a DRO and a hyper-cube hull, respectively. We provide finite sample error\nbounds for both types of tubes. The effectiveness of the proposed approach is\ndemonstrated through numerical simulations."}
{"id": "2510.21006", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.21006", "abs": "https://arxiv.org/abs/2510.21006", "authors": ["Akshay Naik", "Ramavarapu S. Sreenivas", "William R. Norris", "Albert E. Patterson", "Ahmet Soylemezoglu", "Dustin Nottage"], "title": "Safety Monitor for Off-Road Planning with Uncertainty Bounded Bekker Costs", "comment": null, "summary": "Reliable off-road autonomy requires operational constraints so that behavior\nstays predictable and safe when soil strength is uncertain. This paper presents\na runtime assurance safety monitor that collaborates with any planner and uses\na Bekker-based cost model with bounded uncertainty. The monitor builds an upper\nconfidence traversal cost from a lightweight pressure sinkage model identified\nin field tests and checks each planned motion against two limits: maximum\nsinkage and rollover margin. If the risk of crossing either limit is too high,\nthe monitor switches to a certified fallback that reduces vehicle speed,\nincreases standoff from soft ground, or stops on firmer soil. This separation\nlets the planner focus on efficiency while the monitor keeps the vehicle within\nclear safety limits on board. Wheel geometry, wheel load estimate, and a soil\nraster serve as inputs, which tie safety directly to vehicle design and let the\nmonitor set clear limits on speed, curvature, and stopping at run time. The\nmethod carries uncertainty analytically into the upper confidence cost and\napplies simple intervention rules. Tuning of the sinkage limit, rollover\nmargin, and risk window trades efficiency for caution while keeping the monitor\nlight enough for embedded processors. Results from a simulation environment\nspanning loam to sand include intervention rates, violation probability, and\npath efficiency relative to the nominal plan, and a benchtop static loading\ncheck provides initial empirical validation."}
{"id": "2510.21308", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.21308", "abs": "https://arxiv.org/abs/2510.21308", "authors": ["Zhengang Zhong", "Ehecatl Antonio del Rio-Chanona", "Panagiotis Petsagkourakis"], "title": "Data-driven Koopman MPC using Mixed Stochastic-Deterministic Tubes", "comment": "This is the accepted version. It will appear in Journal of Process\n  Control, 2025", "summary": "This paper presents a novel data-driven stochastic MPC design for\ndiscrete-time nonlinear systems with additive disturbances by leveraging the\nKoopman operator and a distributionally robust optimization (DRO) framework. By\nlifting the dynamical system into a linear space, we achieve a\nfinite-dimensional approximation of the Koopman operator. We explicitly account\nfor the modeling approximation and additive disturbance error by a mixed\nstochastic-deterministic tube for the lifted linear model. This ensures the\nregulation of the original nonlinear system while complying with the\nprespecified constraints. Stochastic and deterministic tubes are constructed\nusing a DRO and a hyper-cube hull, respectively. We provide finite sample error\nbounds for both types of tubes. The effectiveness of the proposed approach is\ndemonstrated through numerical simulations."}
{"id": "2510.21144", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21144", "abs": "https://arxiv.org/abs/2510.21144", "authors": ["Hanyu Zhu", "Lance Fiondella", "Jiawei Yuan", "Kai Zeng", "Long Jiao"], "title": "NeuroGenPoisoning: Neuron-Guided Attacks on Retrieval-Augmented Generation of LLM via Genetic Optimization of External Knowledge", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) empowers Large Language Models (LLMs) to\ndynamically integrate external knowledge during inference, improving their\nfactual accuracy and adaptability. However, adversaries can inject poisoned\nexternal knowledge to override the model's internal memory. While existing\nattacks iteratively manipulate retrieval content or prompt structure of RAG,\nthey largely ignore the model's internal representation dynamics and\nneuron-level sensitivities. The underlying mechanism of RAG poisoning has not\nbeen fully studied and the effect of knowledge conflict with strong parametric\nknowledge in RAG is not considered. In this work, we propose NeuroGenPoisoning,\na novel attack framework that generates adversarial external knowledge in RAG\nguided by LLM internal neuron attribution and genetic optimization. Our method\nfirst identifies a set of Poison-Responsive Neurons whose activation strongly\ncorrelates with contextual poisoning knowledge. We then employ a genetic\nalgorithm to evolve adversarial passages that maximally activate these neurons.\nCrucially, our framework enables massive-scale generation of effective poisoned\nRAG knowledge by identifying and reusing promising but initially unsuccessful\nexternal knowledge variants via observed attribution signals. At the same time,\nPoison-Responsive Neurons guided poisoning can effectively resolves knowledge\nconflict. Experimental results across models and datasets demonstrate\nconsistently achieving high Population Overwrite Success Rate (POSR) of over\n90% while preserving fluency. Empirical evidence shows that our method\neffectively resolves knowledge conflict."}
{"id": "2510.21357", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21357", "abs": "https://arxiv.org/abs/2510.21357", "authors": ["Daniel Schleich", "Jan Quenzel", "Sven Behnke"], "title": "Remote Autonomy for Multiple Small Lowcost UAVs in GNSS-denied Search and Rescue Operations", "comment": "Accepted final version. IEEE International Symposium on Safety,\n  Security, and Rescue Robotics (SSRR), Galway, Ireland, 2025", "summary": "In recent years, consumer-grade UAVs have been widely adopted by first\nresponders. In general, they are operated manually, which requires trained\npilots, especially in unknown GNSS-denied environments and in the vicinity of\nstructures. Autonomous flight can facilitate the application of UAVs and reduce\noperator strain. However, autonomous systems usually require special\nprogramming interfaces, custom sensor setups, and strong onboard computers,\nwhich limits a broader deployment.\n  We present a system for autonomous flight using lightweight consumer-grade\nDJI drones. They are controlled by an Android app for state estimation and\nobstacle avoidance directly running on the UAV's remote control. Our ground\ncontrol station enables a single operator to configure and supervise multiple\nheterogeneous UAVs at once. Furthermore, it combines the observations of all\nUAVs into a joint 3D environment model for improved situational awareness."}
{"id": "2510.21321", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.21321", "abs": "https://arxiv.org/abs/2510.21321", "authors": ["Kanghui He", "Anil Alan", "Shengling Shi", "Ton van den Boom", "Bart De Schutter"], "title": "Predictive control barrier functions for piecewise affine systems with non-smooth constraints", "comment": null, "summary": "Obtaining control barrier functions (CBFs) with large safe sets for complex\nnonlinear systems and constraints is a challenging task. Predictive CBFs\naddress this issue by using an online finite-horizon optimal control problem\nthat implicitly defines a large safe set. The optimal control problem, also\nknown as the predictive safety filter (PSF), involves predicting the system's\nflow under a given backup control policy. However, for non-smooth systems and\nconstraints, some key elements, such as CBF gradients and the sensitivity of\nthe flow, are not well-defined, making the current methods inadequate for\nensuring safety. Additionally, for control-non-affine systems, the PSF is\ngenerally nonlinear and non-convex, posing challenges for real-time\ncomputation. This paper considers piecewise affine systems, which are usually\ncontrol-non-affine, under nonlinear state and polyhedral input constraints. We\nsolve the safety issue by incorporating set-valued generalized Clarke\nderivatives in the PSF design. We show that enforcing CBF constraints across\nall elements of the generalized Clarke derivatives suffices to guarantee\nsafety. Moreover, to lighten the computational overhead, we propose an explicit\napproximation of the PSF. The resulting control methods are demonstrated\nthrough numerical examples."}
{"id": "2510.21025", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.21025", "abs": "https://arxiv.org/abs/2510.21025", "authors": ["Ahmed Saad Al-Karsani", "Maryam Khanbaghi", "Aleksandar Zečević"], "title": "A Connectively Stable and Robust DAPI Control Scheme for Islanded Networks of Microgrids", "comment": null, "summary": "The transition towards clean energy and the introduction of Distributed\nEnergy Resources (DERs) are giving rise to the emergence of Microgrids (MGs)\nand Networks of MGs (NMGs). MGs and NMGs can operate autonomously in islanded\nmode. However, they face challenges in terms of secondary level frequency and\nvoltage regulation, due to the variable nature of Renewable Energy Sources\n(RES) and loads. Distributed-Averaging Proportional-Integral (DAPI) control has\nbeen proposed in the literature for distributed frequency and voltage control\nof droop-controlled DERs, but it is not robust to operational or structural\nperturbations. To address this, we propose a robust DAPI frequency and voltage\ncontrol scheme that ensures robustness using the concept of connective\nstability, along with the invariant ellipsoid technique for disturbance\nrejection. Simulation of an NMG model in\nMATLAB\\textsuperscript{\\textregistered}/Simulink\\textsuperscript{\\textregistered}\nconsisting of 3 MGs and 5 DERs validates the effectiveness of the proposed\nmethod, and demonstrates that it can successfully mitigate the effects of major\ndisturbances such as cyberattacks."}
{"id": "2510.21321", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.21321", "abs": "https://arxiv.org/abs/2510.21321", "authors": ["Kanghui He", "Anil Alan", "Shengling Shi", "Ton van den Boom", "Bart De Schutter"], "title": "Predictive control barrier functions for piecewise affine systems with non-smooth constraints", "comment": null, "summary": "Obtaining control barrier functions (CBFs) with large safe sets for complex\nnonlinear systems and constraints is a challenging task. Predictive CBFs\naddress this issue by using an online finite-horizon optimal control problem\nthat implicitly defines a large safe set. The optimal control problem, also\nknown as the predictive safety filter (PSF), involves predicting the system's\nflow under a given backup control policy. However, for non-smooth systems and\nconstraints, some key elements, such as CBF gradients and the sensitivity of\nthe flow, are not well-defined, making the current methods inadequate for\nensuring safety. Additionally, for control-non-affine systems, the PSF is\ngenerally nonlinear and non-convex, posing challenges for real-time\ncomputation. This paper considers piecewise affine systems, which are usually\ncontrol-non-affine, under nonlinear state and polyhedral input constraints. We\nsolve the safety issue by incorporating set-valued generalized Clarke\nderivatives in the PSF design. We show that enforcing CBF constraints across\nall elements of the generalized Clarke derivatives suffices to guarantee\nsafety. Moreover, to lighten the computational overhead, we propose an explicit\napproximation of the PSF. The resulting control methods are demonstrated\nthrough numerical examples."}
{"id": "2510.21148", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21148", "abs": "https://arxiv.org/abs/2510.21148", "authors": ["Yang Zhao", "Pu Wang", "Hao Frank Yang"], "title": "How to Auto-optimize Prompts for Domain Tasks? Adaptive Prompting and Reasoning through Evolutionary Domain Knowledge Adaptation", "comment": null, "summary": "Designing optimal prompts and reasoning processes for large language models\n(LLMs) on domain-specific tasks is both necessary and challenging in real-world\napplications. Determining how to integrate domain knowledge, enhance reasoning\nefficiency, and even provide domain experts with refined knowledge integration\nhints are particularly crucial yet unresolved tasks. In this research, we\npropose Evolutionary Graph Optimization for Prompting (EGO-Prompt), an\nautomated framework to designing better prompts, efficient reasoning processes\nand providing enhanced causal-informed process. EGO-Prompt begins with a\ngeneral prompt and fault-tolerant initial Semantic Causal Graph (SCG)\ndescriptions, constructed by human experts, which is then automatically refined\nand optimized to guide LLM reasoning. Recognizing that expert-defined SCGs may\nbe partial or imperfect and that their optimal integration varies across LLMs,\nEGO-Prompt integrates a novel causal-guided textual gradient process in two\nsteps: first, generating nearly deterministic reasoning guidance from the SCG\nfor each instance, and second, adapting the LLM to effectively utilize the\nguidance alongside the original input. The iterative optimization algorithm\nfurther refines both the SCG and the reasoning mechanism using textual\ngradients with ground-truth. We tested the framework on real-world public\nhealth, transportation and human behavior tasks. EGO-Prompt achieves\n7.32%-12.61% higher F1 than cutting-edge methods, and allows small models to\nreach the performence of larger models at under 20% of the original cost. It\nalso outputs a refined, domain-specific SCG that improves interpretability."}
{"id": "2510.21369", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21369", "abs": "https://arxiv.org/abs/2510.21369", "authors": ["Vivian S. Medeiros", "Giovanni B. Dessy", "Thiago Boaventura", "Marcelo Becker", "Claudio Semini", "Victor Barasuol"], "title": "Load-bearing Assessment for Safe Locomotion of Quadruped Robots on Collapsing Terrain", "comment": null, "summary": "Collapsing terrains, often present in search and rescue missions or planetary\nexploration, pose significant challenges for quadruped robots. This paper\nintroduces a robust locomotion framework for safe navigation over unstable\nsurfaces by integrating terrain probing, load-bearing analysis, motion\nplanning, and control strategies. Unlike traditional methods that rely on\nspecialized sensors or external terrain mapping alone, our approach leverages\njoint measurements to assess terrain stability without hardware modifications.\nA Model Predictive Control (MPC) system optimizes robot motion, balancing\nstability and probing constraints, while a state machine coordinates terrain\nprobing actions, enabling the robot to detect collapsible regions and\ndynamically adjust its footholds. Experimental results on custom-made\ncollapsing platforms and rocky terrains demonstrate the framework's ability to\ntraverse collapsing terrain while maintaining stability and prioritizing\nsafety."}
{"id": "2510.21546", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.21546", "abs": "https://arxiv.org/abs/2510.21546", "authors": ["Johannes Autenrieb", "Mark Spiller"], "title": "Auction-Based Responsibility Allocation for Scalable Decentralized Safety Filters in Cooperative Multi-Agent Collision Avoidance", "comment": "6 pages, 3 figures, Submitted to Control Engineering Practice and\n  IFAC World Congress 2026", "summary": "This paper proposes a scalable decentralized safety filter for multi-agent\nsystems based on high-order control barrier functions (HOCBFs) and\nauction-based responsibility allocation. While decentralized HOCBF formulations\nensure pairwise safety under input bounds, they face feasibility and\nscalability challenges as the number of agents grows. Each agent must evaluate\nan increasing number of pairwise constraints, raising the risk of infeasibility\nand making it difficult to meet real-time requirements. To address this, we\nintroduce an auction-based allocation scheme that distributes constraint\nenforcement asymmetrically among neighbors based on local control effort\nestimates. The resulting directed responsibility graph guarantees full safety\ncoverage while reducing redundant constraints and per-agent computational load.\nSimulation results confirm safe and efficient coordination across a range of\nnetwork sizes and interaction densities."}
{"id": "2510.21026", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21026", "abs": "https://arxiv.org/abs/2510.21026", "authors": ["Sai Haneesh Allu", "Jishnu Jaykumar P", "Ninad Khargonkar", "Tyler Summers", "Jian Yao", "Yu Xiang"], "title": "HRT1: One-Shot Human-to-Robot Trajectory Transfer for Mobile Manipulation", "comment": "14 pages, 11 figures and 3 tables. Project page is available at\n  \\url{https://irvlutd.github.io/HRT1/}", "summary": "We introduce a novel system for human-to-robot trajectory transfer that\nenables robots to manipulate objects by learning from human demonstration\nvideos. The system consists of four modules. The first module is a data\ncollection module that is designed to collect human demonstration videos from\nthe point of view of a robot using an AR headset. The second module is a video\nunderstanding module that detects objects and extracts 3D human-hand\ntrajectories from demonstration videos. The third module transfers a human-hand\ntrajectory into a reference trajectory of a robot end-effector in 3D space. The\nlast module utilizes a trajectory optimization algorithm to solve a trajectory\nin the robot configuration space that can follow the end-effector trajectory\ntransferred from the human demonstration. Consequently, these modules enable a\nrobot to watch a human demonstration video once and then repeat the same mobile\nmanipulation task in different environments, even when objects are placed\ndifferently from the demonstrations. Experiments of different manipulation\ntasks are conducted on a mobile manipulator to verify the effectiveness of our\nsystem"}
{"id": "2510.21546", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.21546", "abs": "https://arxiv.org/abs/2510.21546", "authors": ["Johannes Autenrieb", "Mark Spiller"], "title": "Auction-Based Responsibility Allocation for Scalable Decentralized Safety Filters in Cooperative Multi-Agent Collision Avoidance", "comment": "6 pages, 3 figures, Submitted to Control Engineering Practice and\n  IFAC World Congress 2026", "summary": "This paper proposes a scalable decentralized safety filter for multi-agent\nsystems based on high-order control barrier functions (HOCBFs) and\nauction-based responsibility allocation. While decentralized HOCBF formulations\nensure pairwise safety under input bounds, they face feasibility and\nscalability challenges as the number of agents grows. Each agent must evaluate\nan increasing number of pairwise constraints, raising the risk of infeasibility\nand making it difficult to meet real-time requirements. To address this, we\nintroduce an auction-based allocation scheme that distributes constraint\nenforcement asymmetrically among neighbors based on local control effort\nestimates. The resulting directed responsibility graph guarantees full safety\ncoverage while reducing redundant constraints and per-agent computational load.\nSimulation results confirm safe and efficient coordination across a range of\nnetwork sizes and interaction densities."}
{"id": "2510.21150", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21150", "abs": "https://arxiv.org/abs/2510.21150", "authors": ["Kou Misaki", "Takuya Akiba"], "title": "String Seed of Thought: Prompting LLMs for Distribution-Faithful and Diverse Generation", "comment": null, "summary": "We introduce String Seed of Thought (SSoT), a novel prompting method for LLMs\nthat improves Probabilistic Instruction Following (PIF). We define PIF as a\ntask requiring an LLM to select its answer from a predefined set of options,\neach associated with a specific probability, such that the empirical\ndistribution of the generated answers aligns with the target distribution when\nprompted multiple times. While LLMs excel at tasks with single, deterministic\nanswers, they often fail at PIF, exhibiting biases problematic for applications\nrequiring non-deterministic behaviors, such as human-behavior simulation,\ncontent diversification, and multiplayer games. It also harms the diversity of\ngenerated responses, a crucial factor in test-time scaling, by causing the\noutputs to collapse into a limited set of answers. To address this, we propose\nSSoT, a simple prompting method that instructs an LLM to first output a random\nstring to generate sufficient entropy. SSoT also instructs the LLM to extract\nrandomness by manipulating this string to derive a final answer, thereby\npreserving diversity while adhering to specific constraints. We demonstrate\nthat SSoT significantly improves the PIF performance of LLMs, approaching the\nideal performance of a pseudo-random number generator. Furthermore, our\nexperiments on NoveltyBench show SSoT's benefits extend beyond closed-set tasks\nto open-ended tasks by enhancing response diversity."}
{"id": "2510.21438", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21438", "abs": "https://arxiv.org/abs/2510.21438", "authors": ["Satheeshkumar Veeramani", "Zhengxue Zhou", "Francisco Munguia-Galeano", "Hatem Fakhruldeen", "Thomas Roddelkopf", "Mohammed Faeik Ruzaij Al-Okby", "Kerstin Thurow", "Andrew Ian Cooper"], "title": "PREVENT: Proactive Risk Evaluation and Vigilant Execution of Tasks for Mobile Robotic Chemists using Multi-Modal Behavior Trees", "comment": "25 pages, 8 figures, paper submitted to Robotics and Autonomous\n  Systems Journal", "summary": "Mobile robotic chemists are a fast growing trend in the field of chemistry\nand materials research. However, so far these mobile robots lack workflow\nawareness skills. This poses the risk that even a small anomaly, such as an\nimproperly capped sample vial could disrupt the entire workflow. This wastes\ntime, and resources, and could pose risks to human researchers, such as\nexposure to toxic materials. Existing perception mechanisms can be used to\npredict anomalies but they often generate excessive false positives. This may\nhalt workflow execution unnecessarily, requiring researchers to intervene and\nto resume the workflow when no problem actually exists, negating the benefits\nof autonomous operation. To address this problem, we propose PREVENT a system\ncomprising navigation and manipulation skills based on a multimodal Behavior\nTree (BT) approach that can be integrated into existing software architectures\nwith minimal modifications. Our approach involves a hierarchical perception\nmechanism that exploits AI techniques and sensory feedback through Dexterous\nVision and Navigational Vision cameras and an IoT gas sensor module for\nexecution-related decision-making. Experimental evaluations show that the\nproposed approach is comparatively efficient and completely avoids both false\nnegatives and false positives when tested in simulated risk scenarios within\nour robotic chemistry workflow. The results also show that the proposed\nmulti-modal perception skills achieved deployment accuracies that were higher\nthan the average of the corresponding uni-modal skills, both for navigation and\nfor manipulation."}
{"id": "2510.21556", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.21556", "abs": "https://arxiv.org/abs/2510.21556", "authors": ["Sophie Hall", "Florian Dörfler", "Timm Faulwasser"], "title": "System-Theoretic Analysis of Dynamic Generalized Nash Equilibrium Problems -- Turnpikes and Dissipativity", "comment": null, "summary": "Generalized Nash equilibria are used in multi-agent control applications to\nmodel strategic interactions between agents that are coupled in the cost,\ndynamics, and constraints. We study the properties of open-loop GNE\ntrajectories from a system-theoretic perspective. We show how strict\ndissipativity generates the turnpike phenomenon in GNE solutions. Moreover, we\nestablish a converse turnpike result, i.e., the implication from turnpike to\nstrict dissipativity. We derive conditions under which the steady-state GNE is\nthe optimal operating point and, using a game value function, we give a local\ncharacterization of the geometry of storage functions. Finally, we design\nlinear terminal penalties that ensure GNE open-loop trajectories converge to\nand remain at the steady-state GNE. These connections provide the foundation\nfor future system-theoretic analysis of GNEs similar to those existing in\noptimal control."}
{"id": "2510.21027", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21027", "abs": "https://arxiv.org/abs/2510.21027", "authors": ["Zhe Fei", "Mehmet Yigit Turali", "Shreyas Rajesh", "Xinyang Dai", "Huyen Pham", "Pavan Holur", "Yuhui Zhu", "Larissa Mooney", "Yih-Ing Hser", "Vwani Roychowdhury"], "title": "Customizing Open Source LLMs for Quantitative Medication Attribute Extraction across Heterogeneous EHR Systems", "comment": "NeurIPS 2025: The Second Workshop on GenAI for Health: Potential,\n  Trust, and Policy Compliance", "summary": "Harmonizing medication data across Electronic Health Record (EHR) systems is\na persistent barrier to monitoring medications for opioid use disorder (MOUD).\nIn heterogeneous EHR systems, key prescription attributes are scattered across\ndifferently formatted fields and freetext notes. We present a practical\nframework that customizes open source large language models (LLMs), including\nLlama, Qwen, Gemma, and MedGemma, to extract a unified set of MOUD prescription\nattributes (prescription date, drug name, duration, total quantity, daily\nquantity, and refills) from heterogeneous, site specific data and compute a\nstandardized metric of medication coverage, \\emph{MOUD days}, per patient. Our\npipeline processes records directly in a fixed JSON schema, followed by\nlightweight normalization and cross-field consistency checks. We evaluate the\nsystem on prescription level EHR data from five clinics in a national OUD study\n(25{,}605 records from 1{,}257 patients), using a previously annotated\nbenchmark of 10{,}369 records (776 patients) as the ground truth. Performance\nis reported as coverage (share of records with a valid, matchable output) and\nrecord-level exact-match accuracy. Larger models perform best overall:\nQwen2.5-32B achieves \\textbf{93.4\\%} coverage with \\textbf{93.0\\%} exact-match\naccuracy across clinics, and MedGemma-27B attains\n\\textbf{93.1\\%}/\\textbf{92.2\\%}. A brief error review highlights three common\nissues and fixes: imputing missing dosage fields using within-drug norms,\nhandling monthly/weekly injectables (e.g., Vivitrol) by setting duration from\nthe documented schedule, and adding unit checks to prevent mass units (e.g.,\n``250 g'') from being misread as daily counts. By removing brittle,\nsite-specific ETL and supporting local, privacy-preserving deployment, this\napproach enables consistent cross-site analyses of MOUD exposure, adherence,\nand retention in real-world settings."}
{"id": "2510.21556", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.21556", "abs": "https://arxiv.org/abs/2510.21556", "authors": ["Sophie Hall", "Florian Dörfler", "Timm Faulwasser"], "title": "System-Theoretic Analysis of Dynamic Generalized Nash Equilibrium Problems -- Turnpikes and Dissipativity", "comment": null, "summary": "Generalized Nash equilibria are used in multi-agent control applications to\nmodel strategic interactions between agents that are coupled in the cost,\ndynamics, and constraints. We study the properties of open-loop GNE\ntrajectories from a system-theoretic perspective. We show how strict\ndissipativity generates the turnpike phenomenon in GNE solutions. Moreover, we\nestablish a converse turnpike result, i.e., the implication from turnpike to\nstrict dissipativity. We derive conditions under which the steady-state GNE is\nthe optimal operating point and, using a game value function, we give a local\ncharacterization of the geometry of storage functions. Finally, we design\nlinear terminal penalties that ensure GNE open-loop trajectories converge to\nand remain at the steady-state GNE. These connections provide the foundation\nfor future system-theoretic analysis of GNEs similar to those existing in\noptimal control."}
{"id": "2510.21175", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21175", "abs": "https://arxiv.org/abs/2510.21175", "authors": ["Yujin Jo", "Taesup Kim"], "title": "Memory-Free Continual Learning with Null Space Adaptation for Zero-Shot Vision-Language Models", "comment": null, "summary": "Pre-trained vision-language models (VLMs), such as CLIP, have demonstrated\nremarkable zero-shot generalization, enabling deployment in a wide range of\nreal-world tasks without additional task-specific training. However, in real\ndeployment scenarios with evolving environments or emerging classes, these\nmodels inevitably face distributional shifts and novel tasks. In such contexts,\nstatic zero-shot capabilities are insufficient, and there is a growing need for\ncontinual learning methods that allow models to adapt over time while avoiding\ncatastrophic forgetting. We introduce NuSA-CL (Null Space Adaptation for\nContinual Learning), a lightweight memory-free continual learning framework\ndesigned to address this challenge. NuSA-CL employs low-rank adaptation and\nconstrains task-specific weight updates to lie within an approximate null space\nof the model's current parameters. This strategy minimizes interference with\npreviously acquired knowledge, effectively preserving the zero-shot\ncapabilities of the original model. Unlike methods relying on replay buffers or\ncostly distillation, NuSA-CL imposes minimal computational and memory overhead,\nmaking it practical for deployment in resource-constrained, real-world\ncontinual learning environments. Experiments show that our framework not only\neffectively preserves zero-shot transfer capabilities but also achieves highly\ncompetitive performance on continual learning benchmarks. These results\nposition NuSA-CL as a practical and scalable solution for continually evolving\nzero-shot VLMs in real-world applications."}
{"id": "2510.21469", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21469", "abs": "https://arxiv.org/abs/2510.21469", "authors": ["Domenico Palmisano", "Giuseppe Palestra", "Berardina Nadja De Carolis"], "title": "Enhancing Social Robots through Resilient AI", "comment": "8 pages, Workshop on Adaptive Social Interaction based on user's\n  Mental mOdels and behaVior in HRI, The 17th International Conference on\n  Social Robotics, 10-12 September 2025, Naples (IT)", "summary": "As artificial intelligence continues to advance and becomes more integrated\ninto sensitive areas like healthcare, education, and everyday life, it's\ncrucial for these systems to be both resilient and robust. This paper shows how\nresilience is a fundamental characteristic of social robots, which, through it,\nensure trust in the robot itself-an essential element especially when operating\nin contexts with elderly people, who often have low trust in these systems.\nResilience is therefore the ability to operate under adverse or stressful\nconditions, even when degraded or weakened, while maintaining essential\noperational capabilities."}
{"id": "2510.21612", "categories": ["eess.SY", "cs.IT", "cs.SY", "math.IT", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.21612", "abs": "https://arxiv.org/abs/2510.21612", "authors": ["Yorie Nakahira", "Fangzhou Xiao", "Victoria Kostina", "John C. Doyle"], "title": "Rate-cost tradeoffs in continuous-time control with a biomolecular application", "comment": null, "summary": "This paper focuses on rate-limited control of the generalized\nOrnstein-Uhlenbeck process where the control action can be either\nmultiplicative or additive, and the noise variance can depend on the control\naction. We derive a lower bound on the data rate necessary to achieve the\ndesired control cost. The lower bound is attained with equality if the control\nis performed via an additive white Gaussian channel. The system model\napproximates the dynamics of a discrete-state molecular birth-death process,\nand the result has direct implications on the control of a biomolecular system\nvia chemical reactions, where the multiplicative control corresponds to the\ndegradation rate, the additive control corresponds to the production rate, and\nthe control objective is to decrease the fluctuations of the controlled\nmolecular species around their desired concentration levels."}
{"id": "2510.21043", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.21043", "abs": "https://arxiv.org/abs/2510.21043", "authors": ["Benjamin Lange"], "title": "Epistemic Deference to AI", "comment": "12 pages", "summary": "When should we defer to AI outputs over human expert judgment? Drawing on\nrecent work in social epistemology, I motivate the idea that some AI systems\nqualify as Artificial Epistemic Authorities (AEAs) due to their demonstrated\nreliability and epistemic superiority. I then introduce AI Preemptionism, the\nview that AEA outputs should replace rather than supplement a user's\nindependent epistemic reasons. I show that classic objections to preemptionism\n- such as uncritical deference, epistemic entrenchment, and unhinging epistemic\nbases - apply in amplified form to AEAs, given their opacity, self-reinforcing\nauthority, and lack of epistemic failure markers. Against this, I develop a\nmore promising alternative: a total evidence view of AI deference. According to\nthis view, AEA outputs should function as contributory reasons rather than\noutright replacements for a user's independent epistemic considerations. This\napproach has three key advantages: (i) it mitigates expertise atrophy by\nkeeping human users engaged, (ii) it provides an epistemic case for meaningful\nhuman oversight and control, and (iii) it explains the justified mistrust of AI\nwhen reliability conditions are unmet. While demanding in practice, this\naccount offers a principled way to determine when AI deference is justified,\nparticularly in high-stakes contexts requiring rigorous reliability."}
{"id": "2510.21612", "categories": ["eess.SY", "cs.IT", "cs.SY", "math.IT", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.21612", "abs": "https://arxiv.org/abs/2510.21612", "authors": ["Yorie Nakahira", "Fangzhou Xiao", "Victoria Kostina", "John C. Doyle"], "title": "Rate-cost tradeoffs in continuous-time control with a biomolecular application", "comment": null, "summary": "This paper focuses on rate-limited control of the generalized\nOrnstein-Uhlenbeck process where the control action can be either\nmultiplicative or additive, and the noise variance can depend on the control\naction. We derive a lower bound on the data rate necessary to achieve the\ndesired control cost. The lower bound is attained with equality if the control\nis performed via an additive white Gaussian channel. The system model\napproximates the dynamics of a discrete-state molecular birth-death process,\nand the result has direct implications on the control of a biomolecular system\nvia chemical reactions, where the multiplicative control corresponds to the\ndegradation rate, the additive control corresponds to the production rate, and\nthe control objective is to decrease the fluctuations of the controlled\nmolecular species around their desired concentration levels."}
{"id": "2510.21181", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21181", "abs": "https://arxiv.org/abs/2510.21181", "authors": ["Shuo Li", "Keqin Xu", "Jie Liu", "Dan Ye"], "title": "Shylock: Causal Discovery in Multivariate Time Series based on Hybrid Constraints", "comment": null, "summary": "Causal relationship discovery has been drawing increasing attention due to\nits prevalent application. Existing methods rely on human experience,\nstatistical methods, or graphical criteria methods which are error-prone, stuck\nat the idealized assumption, and rely on a huge amount of data. And there is\nalso a serious data gap in accessing Multivariate time series(MTS) in many\nareas, adding difficulty in finding their causal relationship. Existing methods\nare easy to be over-fitting on them. To fill the gap we mentioned above, in\nthis paper, we propose Shylock, a novel method that can work well in both\nfew-shot and normal MTS to find the causal relationship. Shylock can reduce the\nnumber of parameters exponentially by using group dilated convolution and a\nsharing kernel, but still learn a better representation of variables with time\ndelay. By combing the global constraint and the local constraint, Shylock\nachieves information sharing among networks to help improve the accuracy. To\nevaluate the performance of Shylock, we also design a data generation method to\ngenerate MTS with time delay. We evaluate it on commonly used benchmarks and\ngenerated datasets. Extensive experiments show that Shylock outperforms two\nexisting state-of-art methods on both few-shot and normal MTS. We also\ndeveloped Tcausal, a library for easy use and deployed it on the EarthDataMiner\nplatform"}
{"id": "2510.21536", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.21536", "abs": "https://arxiv.org/abs/2510.21536", "authors": ["Narendhiran Vijayakumar", "Sridevi. M"], "title": "AURASeg: Attention Guided Upsampling with Residual Boundary-Assistive Refinement for Drivable-Area Segmentation", "comment": "10 pages, 5 figures, 4 tables", "summary": "Free space ground segmentation is essential to navigate robots and autonomous\nvehicles, recognize drivable zones, and traverse efficiently. Fine-grained\nfeatures remain challenging for existing segmentation models, particularly for\nrobots in indoor and structured environments. These difficulties arise from\nineffective multi-scale processing, suboptimal boundary refinement, and limited\nfeature representation. In order to overcome these limitations, we propose\nAttention-Guided Upsampling with Residual Boundary-Assistive Refinement\n(AURASeg), a ground-plane semantic segmentation model that maintains high\nsegmentation accuracy while improving border precision. Our method uses\nCSP-Darknet backbone by adding a Residual Border Refinement Module (RBRM) for\naccurate edge delineation and an Attention Progressive Upsampling Decoder\n(APUD) for strong feature integration. We also incorporate a lightweight Atrous\nSpatial Pyramid Pooling (ASPP-Lite) module to ensure multi-scale context\nextraction without compromising real-time performance. The proposed model beats\nbenchmark segmentation architectures in mIoU and F1 metrics when tested on the\nGround Mobile Robot Perception (GMRP) Dataset and a custom Gazebo indoor\ndataset. Our approach achieves an improvement in mean Intersection-over-Union\n(mIoU) of +1.26% and segmentation precision of +1.65% compared to\nstate-of-the-art models. These results show that our technique is feasible for\nautonomous perception in both indoor and outdoor environments, enabling precise\nborder refinement with minimal effect on inference speed."}
{"id": "2510.20916", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.20916", "abs": "https://arxiv.org/abs/2510.20916", "authors": ["Sydney M. Katz", "Robert J. Moss", "Dylan M. Asmar", "Wesley A. Olson", "James K. Kuchar", "Mykel J. Kochenderfer"], "title": "Aircraft Collision Avoidance Systems: Technological Challenges and Solutions on the Path to Regulatory Acceptance", "comment": "32 pages, 9 figures", "summary": "Aircraft collision avoidance systems is critical to modern aviation. These\nsystems are designed to predict potential collisions between aircraft and\nrecommend appropriate avoidance actions. Creating effective collision avoidance\nsystems requires solutions to a variety of technical challenges related to\nsurveillance, decision making, and validation. These challenges have sparked\nsignificant research and development efforts over the past several decades that\nhave resulted in a variety of proposed solutions. This article provides an\noverview of these challenges and solutions with an emphasis on those that have\nbeen put through a rigorous validation process and accepted by regulatory\nbodies. The challenges posed by the collision avoidance problem are often\npresent in other domains, and aircraft collision avoidance systems can serve as\ncase studies that provide valuable insights for a wide range of safety-critical\nsystems."}
{"id": "2510.21044", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.21044", "abs": "https://arxiv.org/abs/2510.21044", "authors": ["Kunal Shankar", "Ninad Gaikwad", "Anamika Dubey"], "title": "House Thermal Model Estimation: Robustness Across Seasons and Setpoints", "comment": "This manuscript is a version of our paper accepted at the 57th North\n  American Power Symposium (NAPS) 2025", "summary": "Achieving the flexibility from house heating, cooling, and ventilation\nsystems (HVAC) has the potential to enable large-scale demand response by\naggregating HVAC load adjustments across many homes. This demand response\nstrategy helps distribution grid to flexibly ramp-up or ramp-down local load\ndemand so that it can optimally match the bulk power system generation profile.\nHowever, achieving this capability requires house thermal models that are both\ncomputationally efficient and robust to operating conditions. In this work,\nparameters of the Resistance-Capacitance (RC) network thermal model for houses\nare estimated using three optimization algorithms: Nonlinear Least Squares\n(NLS), Batch Estimation (BE), and Maximum Likelihood Estimation (MLE). The\nresulting models are evaluated through a Forward-Simulation across four\ndifferent seasons and three setpoints. The results illustrate a principled way\nof selecting reduced order models and estimation methods with respect to the\nrobustness offered to seasonal and setpoint variations in training-testing\ndatasets"}
{"id": "2510.20916", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.20916", "abs": "https://arxiv.org/abs/2510.20916", "authors": ["Sydney M. Katz", "Robert J. Moss", "Dylan M. Asmar", "Wesley A. Olson", "James K. Kuchar", "Mykel J. Kochenderfer"], "title": "Aircraft Collision Avoidance Systems: Technological Challenges and Solutions on the Path to Regulatory Acceptance", "comment": "32 pages, 9 figures", "summary": "Aircraft collision avoidance systems is critical to modern aviation. These\nsystems are designed to predict potential collisions between aircraft and\nrecommend appropriate avoidance actions. Creating effective collision avoidance\nsystems requires solutions to a variety of technical challenges related to\nsurveillance, decision making, and validation. These challenges have sparked\nsignificant research and development efforts over the past several decades that\nhave resulted in a variety of proposed solutions. This article provides an\noverview of these challenges and solutions with an emphasis on those that have\nbeen put through a rigorous validation process and accepted by regulatory\nbodies. The challenges posed by the collision avoidance problem are often\npresent in other domains, and aircraft collision avoidance systems can serve as\ncase studies that provide valuable insights for a wide range of safety-critical\nsystems."}
{"id": "2510.21244", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21244", "abs": "https://arxiv.org/abs/2510.21244", "authors": ["Pengyu Xu", "Shijia Li", "Ao Sun", "Feng Zhang", "Yahan Li", "Bo Wu", "Zhanyu Ma", "Jiguo Li", "Jun Xu", "Jiuchong Gao", "Jinghua Hao", "Renqing He", "Rui Wang", "Yang Liu", "Xiaobo Hu", "Fan Yang", "Jia Zheng", "Guanghua Yao"], "title": "OutboundEval: A Dual-Dimensional Benchmark for Expert-Level Intelligent Outbound Evaluation of Xbench's Professional-Aligned Series", "comment": null, "summary": "We propose OutboundEval, a comprehensive benchmark for evaluating large\nlanguage models (LLMs) in expert-level intelligent outbound calling scenarios.\nUnlike existing methods that suffer from three key limitations - insufficient\ndataset diversity and category coverage, unrealistic user simulation, and\ninaccurate evaluation metrics - OutboundEval addresses these issues through a\nstructured framework. First, we design a benchmark spanning six major business\ndomains and 30 representative sub-scenarios, each with scenario-specific\nprocess decomposition, weighted scoring, and domain-adaptive metrics. Second,\nwe develop a large-model-driven User Simulator that generates diverse,\npersona-rich virtual users with realistic behaviors, emotional variability, and\ncommunication styles, providing a controlled yet authentic testing environment.\nThird, we introduce a dynamic evaluation method that adapts to task variations,\nintegrating automated and human-in-the-loop assessment to measure task\nexecution accuracy, professional knowledge application, adaptability, and user\nexperience quality. Experiments on 12 state-of-the-art LLMs reveal distinct\ntrade-offs between expert-level task completion and interaction fluency,\noffering practical insights for building reliable, human-like outbound AI\nsystems. OutboundEval establishes a practical, extensible, and domain-oriented\nstandard for benchmarking LLMs in professional applications."}
{"id": "2510.21571", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21571", "abs": "https://arxiv.org/abs/2510.21571", "authors": ["Qixiu Li", "Yu Deng", "Yaobo Liang", "Lin Luo", "Lei Zhou", "Chengtang Yao", "Lingqi Zeng", "Zhiyuan Feng", "Huizhi Liang", "Sicheng Xu", "Yizhong Zhang", "Xi Chen", "Hao Chen", "Lily Sun", "Dong Chen", "Jiaolong Yang", "Baining Guo"], "title": "Scalable Vision-Language-Action Model Pretraining for Robotic Manipulation with Real-Life Human Activity Videos", "comment": "Project page: https://microsoft.github.io/VITRA/", "summary": "This paper presents a novel approach for pretraining robotic manipulation\nVision-Language-Action (VLA) models using a large corpus of unscripted\nreal-life video recordings of human hand activities. Treating human hand as\ndexterous robot end-effector, we show that \"in-the-wild\" egocentric human\nvideos without any annotations can be transformed into data formats fully\naligned with existing robotic V-L-A training data in terms of task granularity\nand labels. This is achieved by the development of a fully-automated holistic\nhuman activity analysis approach for arbitrary human hand videos. This approach\ncan generate atomic-level hand activity segments and their language\ndescriptions, each accompanied with framewise 3D hand motion and camera motion.\nWe process a large volume of egocentric videos and create a hand-VLA training\ndataset containing 1M episodes and 26M frames. This training data covers a wide\nrange of objects and concepts, dexterous manipulation tasks, and environment\nvariations in real life, vastly exceeding the coverage of existing robot data.\nWe design a dexterous hand VLA model architecture and pretrain the model on\nthis dataset. The model exhibits strong zero-shot capabilities on completely\nunseen real-world observations. Additionally, fine-tuning it on a small amount\nof real robot action data significantly improves task success rates and\ngeneralization to novel objects in real robotic experiments. We also\ndemonstrate the appealing scaling behavior of the model's task performance with\nrespect to pretraining data scale. We believe this work lays a solid foundation\nfor scalable VLA pretraining, advancing robots toward truly generalizable\nembodied intelligence."}
{"id": "2510.21165", "categories": ["q-fin.GN", "cs.SY", "eess.SY", "nlin.CD", "physics.data-an", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2510.21165", "abs": "https://arxiv.org/abs/2510.21165", "authors": ["Peng Liu"], "title": "The local Gaussian correlation networks among return tails in the Chinese stock market", "comment": null, "summary": "Financial networks based on Pearson correlations have been intensively\nstudied. However, previous studies may have led to misleading and catastrophic\nresults because of several critical shortcomings of the Pearson correlation.\nThe local Gaussian correlation coefficient, a new measurement of statistical\ndependence between variables, has unique advantages including capturing local\nnonlinear dependence and handling heavy-tailed distributions. This study\nconstructs financial networks using the local Gaussian correlation coefficients\nbetween tail regions of stock returns in the Shanghai Stock Exchange. The work\nsystematically analyzes fundamental network metrics including node centrality,\naverage shortest path length, and entropy. Compared with the local Gaussian\ncorrelation network among positive tails and the conventional Pearson\ncorrelation network, the properties of the local Gaussian correlation network\namong negative tails are more sensitive to the stock market risks. This finding\nsuggests researchers should prioritize the local Gaussian correlation network\namong negative tails. Future work should reevaluate existing findings using the\nlocal Gaussian correlation method."}
{"id": "2510.21045", "categories": ["cs.AI", "cs.DB", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.21045", "abs": "https://arxiv.org/abs/2510.21045", "authors": ["Ali Khosravi Kazazi", "Zhenlong Li", "M. Naser Lessani", "Guido Cervone"], "title": "From Questions to Queries: An AI-powered Multi-Agent Framework for Spatial Text-to-SQL", "comment": null, "summary": "The complexity of Structured Query Language (SQL) and the specialized nature\nof geospatial functions in tools like PostGIS present significant barriers to\nnon-experts seeking to analyze spatial data. While Large Language Models (LLMs)\noffer promise for translating natural language into SQL (Text-to-SQL),\nsingle-agent approaches often struggle with the semantic and syntactic\ncomplexities of spatial queries. To address this, we propose a multi-agent\nframework designed to accurately translate natural language questions into\nspatial SQL queries. The framework integrates several innovative components,\nincluding a knowledge base with programmatic schema profiling and semantic\nenrichment, embeddings for context retrieval, and a collaborative multi-agent\npipeline as its core. This pipeline comprises specialized agents for entity\nextraction, metadata retrieval, query logic formulation, SQL generation, and a\nreview agent that performs programmatic and semantic validation of the\ngenerated SQL to ensure correctness (self-verification). We evaluate our system\nusing both the non-spatial KaggleDBQA benchmark and a new, comprehensive\nSpatialQueryQA benchmark that includes diverse geometry types, predicates, and\nthree levels of query complexity. On KaggleDBQA, the system achieved an overall\naccuracy of 81.2% (221 out of 272 questions) after the review agent's review\nand corrections. For spatial queries, the system achieved an overall accuracy\nof 87.7% (79 out of 90 questions), compared with 76.7% without the review\nagent. Beyond accuracy, results also show that in some instances the system\ngenerates queries that are more semantically aligned with user intent than\nthose in the benchmarks. This work makes spatial analysis more accessible, and\nprovides a robust, generalizable foundation for spatial Text-to-SQL systems,\nadvancing the development of autonomous GIS."}
{"id": "2510.21165", "categories": ["q-fin.GN", "cs.SY", "eess.SY", "nlin.CD", "physics.data-an", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2510.21165", "abs": "https://arxiv.org/abs/2510.21165", "authors": ["Peng Liu"], "title": "The local Gaussian correlation networks among return tails in the Chinese stock market", "comment": null, "summary": "Financial networks based on Pearson correlations have been intensively\nstudied. However, previous studies may have led to misleading and catastrophic\nresults because of several critical shortcomings of the Pearson correlation.\nThe local Gaussian correlation coefficient, a new measurement of statistical\ndependence between variables, has unique advantages including capturing local\nnonlinear dependence and handling heavy-tailed distributions. This study\nconstructs financial networks using the local Gaussian correlation coefficients\nbetween tail regions of stock returns in the Shanghai Stock Exchange. The work\nsystematically analyzes fundamental network metrics including node centrality,\naverage shortest path length, and entropy. Compared with the local Gaussian\ncorrelation network among positive tails and the conventional Pearson\ncorrelation network, the properties of the local Gaussian correlation network\namong negative tails are more sensitive to the stock market risks. This finding\nsuggests researchers should prioritize the local Gaussian correlation network\namong negative tails. Future work should reevaluate existing findings using the\nlocal Gaussian correlation method."}
{"id": "2510.21254", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21254", "abs": "https://arxiv.org/abs/2510.21254", "authors": ["Victoria J. Hodge", "Colin Paterson", "Ibrahim Habli"], "title": "Out-of-Distribution Detection for Safety Assurance of AI and Autonomous Systems", "comment": null, "summary": "The operational capabilities and application domains of AI-enabled autonomous\nsystems have expanded significantly in recent years due to advances in robotics\nand machine learning (ML). Demonstrating the safety of autonomous systems\nrigorously is critical for their responsible adoption but it is challenging as\nit requires robust methodologies that can handle novel and uncertain situations\nthroughout the system lifecycle, including detecting out-of-distribution (OoD)\ndata. Thus, OOD detection is receiving increased attention from the research,\ndevelopment and safety engineering communities. This comprehensive review\nanalyses OOD detection techniques within the context of safety assurance for\nautonomous systems, in particular in safety-critical domains. We begin by\ndefining the relevant concepts, investigating what causes OOD and exploring the\nfactors which make the safety assurance of autonomous systems and OOD detection\nchallenging. Our review identifies a range of techniques which can be used\nthroughout the ML development lifecycle and we suggest areas within the\nlifecycle in which they may be used to support safety assurance arguments. We\ndiscuss a number of caveats that system and safety engineers must be aware of\nwhen integrating OOD detection into system lifecycles. We conclude by outlining\nthe challenges and future work necessary for the safe development and operation\nof autonomous systems across a range of domains and applications."}
{"id": "2510.21609", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21609", "abs": "https://arxiv.org/abs/2510.21609", "authors": ["Elle Miller", "Trevor McInroe", "David Abel", "Oisin Mac Aodha", "Sethu Vijayakumar"], "title": "Enhancing Tactile-based Reinforcement Learning for Robotic Control", "comment": null, "summary": "Achieving safe, reliable real-world robotic manipulation requires agents to\nevolve beyond vision and incorporate tactile sensing to overcome sensory\ndeficits and reliance on idealised state information. Despite its potential,\nthe efficacy of tactile sensing in reinforcement learning (RL) remains\ninconsistent. We address this by developing self-supervised learning (SSL)\nmethodologies to more effectively harness tactile observations, focusing on a\nscalable setup of proprioception and sparse binary contacts. We empirically\ndemonstrate that sparse binary tactile signals are critical for dexterity,\nparticularly for interactions that proprioceptive control errors do not\nregister, such as decoupled robot-object motions. Our agents achieve superhuman\ndexterity in complex contact tasks (ball bouncing and Baoding ball rotation).\nFurthermore, we find that decoupling the SSL memory from the on-policy memory\ncan improve performance. We release the Robot Tactile Olympiad (RoTO) benchmark\nto standardise and promote future research in tactile-based manipulation.\nProject page: https://elle-miller.github.io/tactile_rl"}
{"id": "2510.21046", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21046", "abs": "https://arxiv.org/abs/2510.21046", "authors": ["Zlatan Ajanović", "Ravi Prakash", "Leandro de Souza Rosa", "Jens Kober"], "title": "Sequentially Teaching Sequential Tasks $(ST)^2$: Teaching Robots Long-horizon Manipulation Skills", "comment": null, "summary": "Learning from demonstration is effective for teaching robots complex skills\nwith high sample efficiency. However, teaching long-horizon tasks with multiple\nskills is difficult, as deviations accumulate, distributional shift increases,\nand human teachers become fatigued, raising the chance of failure. In this\nwork, we study user responses to two teaching frameworks: (i) a traditional\nmonolithic approach, where users demonstrate the entire trajectory of a\nlong-horizon task; and (ii) a sequential approach, where the task is segmented\nby the user and demonstrations are provided step by step. To support this\nstudy, we introduce $(ST)^2$, a sequential method for learning long-horizon\nmanipulation tasks that allows users to control the teaching flow by defining\nkey points, enabling incremental and structured demonstrations. We conducted a\nuser study on a restocking task with 16 participants in a realistic retail\nenvironment to evaluate both user preference and method effectiveness. Our\nobjective and subjective results show that both methods achieve similar\ntrajectory quality and success rates. Some participants preferred the\nsequential approach for its iterative control, while others favored the\nmonolithic approach for its simplicity."}
{"id": "2510.21275", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21275", "abs": "https://arxiv.org/abs/2510.21275", "authors": ["Robin Schmöcker", "Christoph Schnell", "Alexander Dockhorn"], "title": "Investigating Scale Independent UCT Exploration Factor Strategies", "comment": null, "summary": "The Upper Confidence Bounds For Trees (UCT) algorithm is not agnostic to the\nreward scale of the game it is applied to. For zero-sum games with the sparse\nrewards of $\\{-1,0,1\\}$ at the end of the game, this is not a problem, but many\ngames often feature dense rewards with hand-picked reward scales, causing a\nnode's Q-value to span different magnitudes across different games. In this\npaper, we evaluate various strategies for adaptively choosing the UCT\nexploration constant $\\lambda$, called $\\lambda$-strategies, that are agnostic\nto the game's reward scale. These $\\lambda$-strategies include those proposed\nin the literature as well as five new strategies. Given our experimental\nresults, we recommend using one of our newly suggested $\\lambda$-strategies,\nwhich is to choose $\\lambda$ as $2 \\cdot \\sigma$ where $\\sigma$ is the\nempirical standard deviation of all state-action pairs' Q-values of the search\ntree. This method outperforms existing $\\lambda$-strategies across a wide range\nof tasks both in terms of a single parameter value and the peak performances\nobtained by optimizing all available parameters."}
{"id": "2510.21648", "categories": ["cs.RO", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.21648", "abs": "https://arxiv.org/abs/2510.21648", "authors": ["Inbazhagan Ravikumar", "Ram Sundhar", "Narendhiran Vijayakumar"], "title": "Design and Structural Validation of a Micro-UAV with On-Board Dynamic Route Planning", "comment": "8 pages, 4 figures, 4 tables", "summary": "Micro aerial vehicles are becoming increasingly important in search and\nrescue operations due to their agility, speed, and ability to access confined\nspaces or hazardous areas. However, designing lightweight aerial systems\npresents significant structural, aerodynamic, and computational challenges.\nThis work addresses two key limitations in many low-cost aerial systems under\ntwo kilograms: their lack of structural durability during flight through rough\nterrains and inability to replan paths dynamically when new victims or\nobstacles are detected. We present a fully customised drone built from scratch\nusing only commonly available components and materials, emphasising modularity,\nlow cost, and ease of assembly. The structural frame is reinforced with\nlightweight yet durable materials to withstand impact, while the onboard\ncontrol system is powered entirely by free, open-source software solutions. The\nproposed system demonstrates real-time perception and adaptive navigation\ncapabilities without relying on expensive hardware accelerators, offering an\naffordable and practical solution for real-world search and rescue missions."}
{"id": "2510.21051", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.21051", "abs": "https://arxiv.org/abs/2510.21051", "authors": ["Rebecca G. Hart", "Wanjiku A. Makumi", "Rushikesh Kamalapurkar", "Warren E. Dixon"], "title": "Lyapunov-Based Physics-Informed Deep Neural Networks with Skew Symmetry Considerations", "comment": null, "summary": "Deep neural networks (DNNs) are powerful black-box function approximators\nwhich have been shown to yield improved performance compared to traditional\nneural network (NN) architectures. However, black-box algorithms do not\nincorporate known physics of the system and can yield results which are\nphysically implausible. Physics-informed neural networks (PINNs) have grown in\npopularity due to their ability to leverage known physical principles in the\nlearning process which has been empirically shown to improve performance\ncompared to traditional black-box methods. This paper introduces the first\nphysics-informed DNN controller for an Euler-Lagrange dynamic system where the\nadaptation laws are designed using a Lyapunov-based stability analysis to\naccount for the skew-symmetry property of the inertia matrix and\ncentripetal-Coriolis matrix. A Lyapunov-based stability analysis is provided to\nguarantee asymptotic convergence of the tracking error and the skew-symmetric\nprediction error. Simulations indicate that the developed update law\ndemonstrates improvement in individual and overall function approximation\ncapabilities when compared to a physics-informed adaptation law which does not\nincorporate knowledge of system symmetries."}
{"id": "2510.21285", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.21285", "abs": "https://arxiv.org/abs/2510.21285", "authors": ["Yingzhi Mao", "Chunkang Zhang", "Junxiang Wang", "Xinyan Guan", "Boxi Cao", "Yaojie Lu", "Hongyu Lin", "Xianpei Han", "Le Sun"], "title": "When Models Outthink Their Safety: Mitigating Self-Jailbreak in Large Reasoning Models with Chain-of-Guardrails", "comment": "First two authors contributed equally. The main text is 10 pages,\n  with an appendix of 19 pages. The paper contains 18 figures and 16 tables", "summary": "Large Reasoning Models (LRMs) demonstrate remarkable capabilities on complex\nreasoning tasks but remain vulnerable to severe safety risks, including harmful\ncontent generation and jailbreak attacks. Existing mitigation strategies rely\non injecting heuristic safety signals during training, which often suppress\nreasoning ability and fail to resolve the safety-reasoning trade-off. To\nsystematically investigate this issue, we analyze the reasoning trajectories of\ndiverse LRMs and uncover a phenomenon we term Self-Jailbreak, where models\noverride their own risk assessments and justify responding to unsafe prompts.\nThis finding reveals that LRMs inherently possess the ability to reject unsafe\nqueries, but this ability is compromised, resulting in harmful outputs.\nBuilding on these insights, we propose the Chain-of-Guardrail (CoG), a training\nframework that recomposes or backtracks unsafe reasoning steps, steering the\nmodel back onto safe trajectories while preserving valid reasoning chains.\nExtensive experiments across multiple reasoning and safety benchmarks\ndemonstrate that CoG substantially improves the safety of current LRMs while\npreserving comparable reasoning ability, significantly outperforming prior\nmethods that suffer from severe safety-reasoning trade-offs."}
{"id": "2510.21302", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21302", "abs": "https://arxiv.org/abs/2510.21302", "authors": ["Sanghyun Ahn", "Wonje Choi", "Junyong Lee", "Jinwoo Park", "Honguk Woo"], "title": "Towards Reliable Code-as-Policies: A Neuro-Symbolic Framework for Embodied Task Planning", "comment": "Accepted at NeurIPS 2025 Spotlight", "summary": "Recent advances in large language models (LLMs) have enabled the automatic\ngeneration of executable code for task planning and control in embodied agents\nsuch as robots, demonstrating the potential of LLM-based embodied intelligence.\nHowever, these LLM-based code-as-policies approaches often suffer from limited\nenvironmental grounding, particularly in dynamic or partially observable\nsettings, leading to suboptimal task success rates due to incorrect or\nincomplete code generation. In this work, we propose a neuro-symbolic embodied\ntask planning framework that incorporates explicit symbolic verification and\ninteractive validation processes during code generation. In the validation\nphase, the framework generates exploratory code that actively interacts with\nthe environment to acquire missing observations while preserving task-relevant\nstates. This integrated process enhances the grounding of generated code,\nresulting in improved task reliability and success rates in complex\nenvironments. We evaluate our framework on RLBench and in real-world settings\nacross dynamic, partially observable scenarios. Experimental results\ndemonstrate that our framework improves task success rates by 46.2% over\nCode-as-Policies baselines and attains over 86.8% executability of\ntask-relevant actions, thereby enhancing the reliability of task planning in\ndynamic environments."}
{"id": "2510.21074", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21074", "abs": "https://arxiv.org/abs/2510.21074", "authors": ["Mitchell E. C. Sabbadini", "Andrew H. Liu", "Joseph Ruan", "Tyler S. Wilson", "Zachary Kingston", "Jonathan D. Gammell"], "title": "Revisiting Replanning from Scratch: Real-Time Incremental Planning with Fast Almost-Surely Asymptotically Optimal Planners", "comment": "Submitted to IEEE International Conference on Robotics and Automation\n  (ICRA) 2026, 8 pages, 5 figures, 1 table. A video of this work can be found\n  at https://www.youtube.com/watch?v=XaZrFy8wGZs", "summary": "Robots operating in changing environments either predict obstacle changes\nand/or plan quickly enough to react to them. Predictive approaches require a\nstrong prior about the position and motion of obstacles. Reactive approaches\nrequire no assumptions about their environment but must replan quickly and find\nhigh-quality paths to navigate effectively.\n  Reactive approaches often reuse information between queries to reduce\nplanning cost. These techniques are conceptually sound but updating dense\nplanning graphs when information changes can be computationally prohibitive. It\ncan also require significant effort to detect the changes in some applications.\n  This paper revisits the long-held assumption that reactive replanning\nrequires updating existing plans. It shows that the incremental planning\nproblem can alternatively be solved more efficiently as a series of independent\nproblems using fast almost-surely asymptotically optimal (ASAO) planning\nalgorithms. These ASAO algorithms quickly find an initial solution and converge\ntowards an optimal solution which allows them to find consistent global plans\nin the presence of changing obstacles without requiring explicit plan reuse.\nThis is demonstrated with simulated experiments where Effort Informed Trees\n(EIT*) finds shorter median solution paths than the tested reactive planning\nalgorithms and is further validated using Asymptotically Optimal RRT-Connect\n(AORRTC) on a real-world planning problem on a robot arm."}
{"id": "2510.21293", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.21293", "abs": "https://arxiv.org/abs/2510.21293", "authors": ["Siddharth Mehrotra", "Jin Huang", "Xuelong Fu", "Roel Dobbe", "Clara I. Sánchez", "Maarten de Rijke"], "title": "Understanding AI Trustworthiness: A Scoping Review of AIES & FAccT Articles", "comment": "Submitted to Journal of Artificial Intelligence Research (JAIR)", "summary": "Background: Trustworthy AI serves as a foundational pillar for two major AI\nethics conferences: AIES and FAccT. However, current research often adopts\ntechno-centric approaches, focusing primarily on technical attributes such as\nreliability, robustness, and fairness, while overlooking the sociotechnical\ndimensions critical to understanding AI trustworthiness in real-world contexts.\n  Objectives: This scoping review aims to examine how the AIES and FAccT\ncommunities conceptualize, measure, and validate AI trustworthiness,\nidentifying major gaps and opportunities for advancing a holistic understanding\nof trustworthy AI systems.\n  Methods: We conduct a scoping review of AIES and FAccT conference proceedings\nto date, systematically analyzing how trustworthiness is defined,\noperationalized, and applied across different research domains. Our analysis\nfocuses on conceptualization approaches, measurement methods, verification and\nvalidation techniques, application areas, and underlying values.\n  Results: While significant progress has been made in defining technical\nattributes such as transparency, accountability, and robustness, our findings\nreveal critical gaps. Current research often predominantly emphasizes technical\nprecision at the expense of social and ethical considerations. The\nsociotechnical nature of AI systems remains less explored and trustworthiness\nemerges as a contested concept shaped by those with the power to define it.\n  Conclusions: An interdisciplinary approach combining technical rigor with\nsocial, cultural, and institutional considerations is essential for advancing\ntrustworthy AI. We propose actionable measures for the AI ethics community to\nadopt holistic frameworks that genuinely address the complex interplay between\nAI systems and society, ultimately promoting responsible technological\ndevelopment that benefits all stakeholders."}
{"id": "2510.21560", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21560", "abs": "https://arxiv.org/abs/2510.21560", "authors": ["Yuxuan Yang", "Hussein Sibai"], "title": "Learning Neural Control Barrier Functions from Expert Demonstrations using Inverse Constraint Learning", "comment": null, "summary": "Safety is a fundamental requirement for autonomous systems operating in\ncritical domains. Control barrier functions (CBFs) have been used to design\nsafety filters that minimally alter nominal controls for such systems to\nmaintain their safety. Learning neural CBFs has been proposed as a data-driven\nalternative for their computationally expensive optimization-based synthesis.\nHowever, it is often the case that the failure set of states that should be\navoided is non-obvious or hard to specify formally, e.g., tailgating in\nautonomous driving, while a set of expert demonstrations that achieve the task\nand avoid the failure set is easier to generate. We use ICL to train a\nconstraint function that classifies the states of the system under\nconsideration to safe, i.e., belong to a controlled forward invariant set that\nis disjoint from the unspecified failure set, and unsafe ones, i.e., belong to\nthe complement of that set. We then use that function to label a new set of\nsimulated trajectories to train our neural CBF. We empirically evaluate our\napproach in four different environments, demonstrating that it outperforms\nexisting baselines and achieves comparable performance to a neural CBF trained\nwith the same data but annotated with ground-truth safety labels."}
{"id": "2510.21082", "categories": ["cs.CY", "cs.AI", "cs.HC", "68T50 (Artificial intelligence)", "I.2.7; K.5.2"], "pdf": "https://arxiv.org/pdf/2510.21082", "abs": "https://arxiv.org/abs/2510.21082", "authors": ["Jorge Alberto Araujo"], "title": "Soppia: A Structured Prompting Framework for the Proportional Assessment of Non-Pecuniary Damages in Personal Injury Cases", "comment": "9 pages, 2 tables, includes GitHub link to framework implementation.\n  Submitted to the Artificial Intelligence and Law section of arXiv", "summary": "Applying complex legal rules characterized by multiple, heterogeneously\nweighted criteria presents a fundamental challenge in judicial decision-making,\noften hindering the consistent realization of legislative intent. This\nchallenge is particularly evident in the quantification of non-pecuniary\ndamages in personal injury cases. This paper introduces Soppia, a structured\nprompting framework designed to assist legal professionals in navigating this\ncomplexity. By leveraging advanced AI, the system ensures a comprehensive and\nbalanced analysis of all stipulated criteria, fulfilling the legislator's\nintent that compensation be determined through a holistic assessment of each\ncase. Using the twelve criteria for non-pecuniary damages established in the\nBrazilian CLT (Art. 223-G) as a case study, we demonstrate how Soppia (System\nfor Ordered Proportional and Pondered Intelligent Assessment) operationalizes\nnuanced legal commands into a practical, replicable, and transparent\nmethodology. The framework enhances consistency and predictability while\nproviding a versatile and explainable tool adaptable across multi-criteria\nlegal contexts, bridging normative interpretation and computational reasoning\ntoward auditable legal AI."}
{"id": "2510.21302", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21302", "abs": "https://arxiv.org/abs/2510.21302", "authors": ["Sanghyun Ahn", "Wonje Choi", "Junyong Lee", "Jinwoo Park", "Honguk Woo"], "title": "Towards Reliable Code-as-Policies: A Neuro-Symbolic Framework for Embodied Task Planning", "comment": "Accepted at NeurIPS 2025 Spotlight", "summary": "Recent advances in large language models (LLMs) have enabled the automatic\ngeneration of executable code for task planning and control in embodied agents\nsuch as robots, demonstrating the potential of LLM-based embodied intelligence.\nHowever, these LLM-based code-as-policies approaches often suffer from limited\nenvironmental grounding, particularly in dynamic or partially observable\nsettings, leading to suboptimal task success rates due to incorrect or\nincomplete code generation. In this work, we propose a neuro-symbolic embodied\ntask planning framework that incorporates explicit symbolic verification and\ninteractive validation processes during code generation. In the validation\nphase, the framework generates exploratory code that actively interacts with\nthe environment to acquire missing observations while preserving task-relevant\nstates. This integrated process enhances the grounding of generated code,\nresulting in improved task reliability and success rates in complex\nenvironments. We evaluate our framework on RLBench and in real-world settings\nacross dynamic, partially observable scenarios. Experimental results\ndemonstrate that our framework improves task success rates by 46.2% over\nCode-as-Policies baselines and attains over 86.8% executability of\ntask-relevant actions, thereby enhancing the reliability of task planning in\ndynamic environments."}
{"id": "2510.21093", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21093", "abs": "https://arxiv.org/abs/2510.21093", "authors": ["Siyong Chen", "Jinbo Wen", "Jiawen Kang", "Tenghui Huang", "Xumin Huang", "Yuanjia Su", "Hudan Pan", "Zishao Zhong", "Dusit Niyato", "Shengli Xie", "Dong In Kim"], "title": "MedAlign: A Synergistic Framework of Multimodal Preference Optimization and Federated Meta-Cognitive Reasoning", "comment": null, "summary": "Recently, large models have shown significant potential for smart healthcare.\nHowever, the deployment of Large Vision-Language Models (LVLMs) for clinical\nservices is currently hindered by three critical challenges: a tendency to\nhallucinate answers not grounded in visual evidence, the inefficiency of\nfixed-depth reasoning, and the difficulty of multi-institutional collaboration.\nTo address these challenges, in this paper, we develop MedAlign, a novel\nframework to ensure visually accurate LVLM responses for Medical Visual\nQuestion Answering (Med-VQA). Specifically, we first propose a multimodal\nDirect Preference Optimization (mDPO) objective to explicitly align preference\nlearning with visual context. We then design a Retrieval-Aware\nMixture-of-Experts (RA-MoE) architecture that utilizes image and text\nsimilarity to route queries to a specialized and context-augmented LVLM (i.e.,\nan expert), thereby mitigating hallucinations in LVLMs. To achieve adaptive\nreasoning and facilitate multi-institutional collaboration, we propose a\nfederated governance mechanism, where the selected expert, fine-tuned on\nclinical datasets based on mDPO, locally performs iterative Chain-of-Thought\n(CoT) reasoning via the local meta-cognitive uncertainty estimator. Extensive\nexperiments on three representative Med-VQA datasets demonstrate that MedAlign\nachieves state-of-the-art performance, outperforming strong retrieval-augmented\nbaselines by up to $11.85\\%$ in F1-score, and simultaneously reducing the\naverage reasoning length by $51.60\\%$ compared with fixed-depth CoT approaches."}
{"id": "2510.21324", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.21324", "abs": "https://arxiv.org/abs/2510.21324", "authors": ["Jinhui Lou", "Yan Yang", "Zhou Yu", "Zhenqi Fu", "Weidong Han", "Qingming Huang", "Jun Yu"], "title": "CXRAgent: Director-Orchestrated Multi-Stage Reasoning for Chest X-Ray Interpretation", "comment": "10 pages, 4 figures, 7 Tables", "summary": "Chest X-ray (CXR) plays a pivotal role in clinical diagnosis, and a variety\nof task-specific and foundation models have been developed for automatic CXR\ninterpretation. However, these models often struggle to adapt to new diagnostic\ntasks and complex reasoning scenarios. Recently, LLM-based agent models have\nemerged as a promising paradigm for CXR analysis, enhancing model's capability\nthrough tool coordination, multi-step reasoning, and team collaboration, etc.\nHowever, existing agents often rely on a single diagnostic pipeline and lack\nmechanisms for assessing tools' reliability, limiting their adaptability and\ncredibility. To this end, we propose CXRAgent, a director-orchestrated,\nmulti-stage agent for CXR interpretation, where a central director coordinates\nthe following stages: (1) Tool Invocation: The agent strategically orchestrates\na set of CXR-analysis tools, with outputs normalized and verified by the\nEvidence-driven Validator (EDV), which grounds diagnostic outputs with visual\nevidence to support reliable downstream diagnosis; (2) Diagnostic Planning:\nGuided by task requirements and intermediate findings, the agent formulates a\ntargeted diagnostic plan. It then assembles an expert team accordingly,\ndefining member roles and coordinating their interactions to enable adaptive\nand collaborative reasoning; (3) Collaborative Decision-making: The agent\nintegrates insights from the expert team with accumulated contextual memories,\nsynthesizing them into an evidence-backed diagnostic conclusion. Experiments on\nvarious CXR interpretation tasks show that CXRAgent delivers strong\nperformance, providing visual evidence and generalizes well to clinical tasks\nof different complexity. Code and data are valuable at this\n\\href{https://github.com/laojiahuo2003/CXRAgent/}{link}."}
{"id": "2510.21110", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21110", "abs": "https://arxiv.org/abs/2510.21110", "authors": ["Mingxuan Li", "Junzhe Zhang", "Elias Bareinboim"], "title": "Confounding Robust Deep Reinforcement Learning: A Causal Approach", "comment": "NeurIPS 2025", "summary": "A key task in Artificial Intelligence is learning effective policies for\ncontrolling agents in unknown environments to optimize performance measures.\nOff-policy learning methods, like Q-learning, allow learners to make optimal\ndecisions based on past experiences. This paper studies off-policy learning\nfrom biased data in complex and high-dimensional domains where \\emph{unobserved\nconfounding} cannot be ruled out a priori. Building on the well-celebrated Deep\nQ-Network (DQN), we propose a novel deep reinforcement learning algorithm\nrobust to confounding biases in observed data. Specifically, our algorithm\nattempts to find a safe policy for the worst-case environment compatible with\nthe observations. We apply our method to twelve confounded Atari games, and\nfind that it consistently dominates the standard DQN in all games where the\nobserved input to the behavioral and target policies mismatch and unobserved\nconfounders exist."}
{"id": "2510.21341", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.21341", "abs": "https://arxiv.org/abs/2510.21341", "authors": ["Lufan Chang"], "title": "Magellan: Guided MCTS for Latent Space Exploration and Novelty Generation", "comment": "Accepted to 1st Open Conference on AI Agents for Science\n  (agents4science 2025)", "summary": "Large Language Models (LLMs) often struggle with generating truly innovative\nideas, typically defaulting to high-probability, familiar concepts within their\ntraining data's \"gravity wells.\" While advanced search-based methods like Tree\nof Thoughts (ToT) attempt to mitigate this, they are fundamentally limited by\ntheir reliance on unprincipled, inconsistent self-evaluation heuristics to\nguide exploration. To address this gap, we introduce \\textbf{Magellan}, a novel\nframework that reframes creative generation as a principled, guided exploration\nof an LLM's latent conceptual space. At its core, Magellan employs Monte Carlo\nTree Search (MCTS) governed by a hierarchical guidance system. For long-range\ndirection, a \"semantic compass\" vector, formulated via orthogonal projection,\nsteers the search towards relevant novelty. For local, step-by-step decisions,\na landscape-aware value function replaces flawed self-evaluation with an\nexplicit reward structure that balances intrinsic coherence, extrinsic novelty,\nand narrative progress. Extensive experiments demonstrate that Magellan\nsignificantly outperforms strong baselines, including ReAct and ToT, in\ngenerating scientific ideas with superior plausibility and innovation. Our work\nshows that for creative discovery, a principled, guided search is more\neffective than unconstrained agency, paving the way for LLMs to become more\ncapable partners in innovation."}
{"id": "2510.21117", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21117", "abs": "https://arxiv.org/abs/2510.21117", "authors": ["Chunghyun Han", "Alfio Gliozzo", "Junkyu Lee", "Agostino Capponi"], "title": "DAO-AI: Evaluating Collective Decision-Making through Agentic AI in Decentralized Governance", "comment": "12 pages, 2 Figures", "summary": "This paper presents a first empirical study of agentic AI as autonomous\ndecision-makers in decentralized governance. Using more than 3K proposals from\nmajor protocols, we build an agentic AI voter that interprets proposal\ncontexts, retrieves historical deliberation data, and independently determines\nits voting position. The agent operates within a realistic financial simulation\nenvironment grounded in verifiable blockchain data, implemented through a\nmodular composable program (MCP) workflow that defines data flow and tool usage\nvia Agentics framework. We evaluate how closely the agent's decisions align\nwith the human and token-weighted outcomes, uncovering strong alignments\nmeasured by carefully designed evaluation metrics. Our findings demonstrate\nthat agentic AI can augment collective decision-making by producing\ninterpretable, auditable, and empirically grounded signals in realistic DAO\ngovernance settings. The study contributes to the design of explainable and\neconomically rigorous AI agents for decentralized financial systems."}
{"id": "2510.21398", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21398", "abs": "https://arxiv.org/abs/2510.21398", "authors": ["Ravindra Aribowo Tarunokusumo", "Rafael Fernandes Cunha"], "title": "Boosting Accuracy and Efficiency of Budget Forcing in LLMs via Reinforcement Learning for Mathematical Reasoning", "comment": "Submitted to the European Conference on Artificial Intelligence\n  (ECAI)", "summary": "Test-time scaling methods have seen a rapid increase in popularity for its\ncomputational efficiency and parameter-independent training to improve\nreasoning performance on Large Language Models. One such method is called\nbudget forcing, a decoding intervention strategy which allocates extra compute\nbudget for thinking and elicits the inherent self-correcting behavior of the\nmodel. However, this relies on supervised fine-tuning (SFT) on long-context\nreasoning traces which causes performance degradation on smaller models due to\nverbose responses. For this reason, we offer a framework integrating\nreinforcement learning (RL) to improve token efficiency and boost the\nperformance of a 1.5B model for mathematical reasoning. We demonstrate this\nusing only 1.5K training samples and found that our SFT+RL model performed\nbetter on the GSM8K dataset with varying compute budgets. Our main findings\nshowed an overall higher accuracy while significantly reducing its token usage\nby over 40% compared to the SFT model, revealing how RL can recover the losses\ndue to long-context training and altogether improving performance in\nmathematical reasoning."}
{"id": "2510.21121", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21121", "abs": "https://arxiv.org/abs/2510.21121", "authors": ["Haibo Zhao", "Yu Qi", "Boce Hu", "Yizhe Zhu", "Ziyan Chen", "Heng Tian", "Xupeng Zhu", "Owen Howell", "Haojie Huang", "Robin Walters", "Dian Wang", "Robert Platt"], "title": "Generalizable Hierarchical Skill Learning via Object-Centric Representation", "comment": null, "summary": "We present Generalizable Hierarchical Skill Learning (GSL), a novel framework\nfor hierarchical policy learning that significantly improves policy\ngeneralization and sample efficiency in robot manipulation. One core idea of\nGSL is to use object-centric skills as an interface that bridges the high-level\nvision-language model and the low-level visual-motor policy. Specifically, GSL\ndecomposes demonstrations into transferable and object-canonicalized skill\nprimitives using foundation models, ensuring efficient low-level skill learning\nin the object frame. At test time, the skill-object pairs predicted by the\nhigh-level agent are fed to the low-level module, where the inferred canonical\nactions are mapped back to the world frame for execution. This structured yet\nflexible design leads to substantial improvements in sample efficiency and\ngeneralization of our method across unseen spatial arrangements, object\nappearances, and task compositions. In simulation, GSL trained with only 3\ndemonstrations per task outperforms baselines trained with 30 times more data\nby 15.5 percent on unseen tasks. In real-world experiments, GSL also surpasses\nthe baseline trained with 10 times more data."}
{"id": "2510.21425", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21425", "abs": "https://arxiv.org/abs/2510.21425", "authors": ["Maneeha Rani", "Bhupesh Kumar Mishra", "Dhavalkumar Thakker"], "title": "Advancing Symbolic Integration in Large Language Models: Beyond Conventional Neurosymbolic AI", "comment": null, "summary": "LLMs have demonstrated highly effective learning, human-like response\ngeneration,and decision-making capabilities in high-risk sectors. However,\nthese models remain black boxes because they struggle to ensure transparency in\nresponses. The literature has explored numerous approaches to address\ntransparency challenges in LLMs, including Neurosymbolic AI (NeSy AI). NeSy AI\napproaches were primarily developed for conventional neural networks and are\nnot well-suited to the unique features of LLMs. Consequently, there is a\nlimited systematic understanding of how symbolic AI can be effectively\nintegrated into LLMs. This paper aims to address this gap by first reviewing\nestablished NeSy AI methods and then proposing a novel taxonomy of symbolic\nintegration in LLMs, along with a roadmap to merge symbolic techniques with\nLLMs. The roadmap introduces a new categorisation framework across four\ndimensions by organising existing literature within these categories. These\ninclude symbolic integration across various stages of LLM, coupling mechanisms,\narchitectural paradigms, as well as algorithmic and application-level\nperspectives. The paper thoroughly identifies current benchmarks, cutting-edge\nadvancements, and critical gaps within the field to propose a roadmap for\nfuture research. By highlighting the latest developments and notable gaps in\nthe literature, it offers practical insights for implementing frameworks for\nsymbolic integration into LLMs to enhance transparency."}
{"id": "2510.21136", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.21136", "abs": "https://arxiv.org/abs/2510.21136", "authors": ["Chengming Lyu", "Zhenfei Tan", "Xiaoyuan Xu", "Chen Fu", "Zheng Yan", "Mohammad Shahidehpour"], "title": "Environment-Dependent Components Identification of Behind-the-Meter Resources via Inverse Optimization", "comment": null, "summary": "With the increasing penetration of behind-the-meter (BTM) resources, it is\nvital to monitor the components of these resources and deduce their response\nbehavior to external environment. Owing to data privacy, however, the\nappliance-wise measurement is invisible to the power system operator, which\nhinders the accurate modeling of load identification. To this end, this paper\nproposes a hybrid physics-inspired and data-driven framework for decomposing\nBTM components based on external measurement of total load and environmental\nfactors. The total load is decomposed into different environment-dependent\ncomponents, namely storage-like component, PV generation component,\nthermostatically-controlled load component, and periodic component. The overall\nload identification adopts a double-layer iterative solution framework. A\ndata-driven inverse optimization algorithm is developed to identify parameters\nof the energy storage-like component. The physics-inspired model is proposed to\nidentify the capacity and response of the rest components. The modeling\naccuracy and robustness of the proposed method are validated by numerical\ntests. The application significance of the proposed BTM identification method\nis also validated in electricity market clearing for reducing system operation\ncosts."}
{"id": "2510.21436", "categories": ["cs.AI", "I.4"], "pdf": "https://arxiv.org/pdf/2510.21436", "abs": "https://arxiv.org/abs/2510.21436", "authors": ["Ankur Sinha", "Shobhit Arora", "Dhaval Pujara"], "title": "AutoOpt: A Dataset and a Unified Framework for Automating Optimization Problem Solving", "comment": "NeurIPS 2025, 28 pages, 11 figures, 11 tables", "summary": "This study presents AutoOpt-11k, a unique image dataset of over 11,000\nhandwritten and printed mathematical optimization models corresponding to\nsingle-objective, multi-objective, multi-level, and stochastic optimization\nproblems exhibiting various types of complexities such as non-linearity,\nnon-convexity, non-differentiability, discontinuity, and high-dimensionality.\nThe labels consist of the LaTeX representation for all the images and modeling\nlanguage representation for a subset of images. The dataset is created by 25\nexperts following ethical data creation guidelines and verified in two-phases\nto avoid errors. Further, we develop AutoOpt framework, a machine learning\nbased automated approach for solving optimization problems, where the user just\nneeds to provide an image of the formulation and AutoOpt solves it efficiently\nwithout any further human intervention. AutoOpt framework consists of three\nModules: (i) M1 (Image_to_Text)- a deep learning model performs the\nMathematical Expression Recognition (MER) task to generate the LaTeX code\ncorresponding to the optimization formulation in image; (ii) M2 (Text_to_Text)-\na small-scale fine-tuned LLM generates the PYOMO script (optimization modeling\nlanguage) from LaTeX code; (iii) M3 (Optimization)- a Bilevel Optimization\nbased Decomposition (BOBD) method solves the optimization formulation described\nin the PYOMO script. We use AutoOpt-11k dataset for training and testing of\ndeep learning models employed in AutoOpt. The deep learning model for MER task\n(M1) outperforms ChatGPT, Gemini and Nougat on BLEU score metric. BOBD method\n(M3), which is a hybrid approach, yields better results on complex test\nproblems compared to common approaches, like interior-point algorithm and\ngenetic algorithm."}
{"id": "2510.21143", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21143", "abs": "https://arxiv.org/abs/2510.21143", "authors": ["Jihyun Lee", "Yejin Min", "San Kim", "Yejin Jeon", "SungJun Yang", "Hyounghun Kim", "Gary Geunbae Lee"], "title": "PanicToCalm: A Proactive Counseling Agent for Panic Attacks", "comment": null, "summary": "Panic attacks are acute episodes of fear and distress, in which timely,\nappropriate intervention can significantly help individuals regain stability.\nHowever, suitable datasets for training such models remain scarce due to\nethical and logistical issues. To address this, we introduce PACE, which is a\ndataset that includes high-distress episodes constructed from first-person\nnarratives, and structured around the principles of Psychological First Aid\n(PFA). Using this data, we train PACER, a counseling model designed to provide\nboth empathetic and directive support, which is optimized through supervised\nlearning and simulated preference alignment. To assess its effectiveness, we\npropose PanicEval, a multi-dimensional framework covering general counseling\nquality and crisis-specific strategies. Experimental results show that PACER\noutperforms strong baselines in both counselor-side metrics and client affect\nimprovement. Human evaluations further confirm its practical value, with PACER\nconsistently preferred over general, CBT-based, and GPT-4-powered models in\npanic scenarios (Code is available at https://github.com/JihyunLee1/PanicToCalm\n)."}
{"id": "2510.21453", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21453", "abs": "https://arxiv.org/abs/2510.21453", "authors": ["Yuxin Pan", "Zhiguang Cao", "Chengyang Gu", "Liu Liu", "Peilin Zhao", "Yize Chen", "Fangzhen Lin"], "title": "Multi-Task Vehicle Routing Solver via Mixture of Specialized Experts under State-Decomposable MDP", "comment": "Accepted to NeurIPS 2025", "summary": "Existing neural methods for multi-task vehicle routing problems (VRPs)\ntypically learn unified solvers to handle multiple constraints simultaneously.\nHowever, they often underutilize the compositional structure of VRP variants,\neach derivable from a common set of basis VRP variants. This critical oversight\ncauses unified solvers to miss out the potential benefits of basis solvers,\neach specialized for a basis VRP variant. To overcome this limitation, we\npropose a framework that enables unified solvers to perceive the\nshared-component nature across VRP variants by proactively reusing basis\nsolvers, while mitigating the exponential growth of trained neural solvers.\nSpecifically, we introduce a State-Decomposable MDP (SDMDP) that reformulates\nVRPs by expressing the state space as the Cartesian product of basis state\nspaces associated with basis VRP variants. More crucially, this formulation\ninherently yields the optimal basis policy for each basis VRP variant.\nFurthermore, a Latent Space-based SDMDP extension is developed by incorporating\nboth the optimal basis policies and a learnable mixture function to enable the\npolicy reuse in the latent space. Under mild assumptions, this extension\nprovably recovers the optimal unified policy of SDMDP through the mixture\nfunction that computes the state embedding as a mapping from the basis state\nembeddings generated by optimal basis policies. For practical implementation,\nwe introduce the Mixture-of-Specialized-Experts Solver (MoSES), which realizes\nbasis policies through specialized Low-Rank Adaptation (LoRA) experts, and\nimplements the mixture function via an adaptive gating mechanism. Extensive\nexperiments conducted across VRP variants showcase the superiority of MoSES\nover prior methods."}
{"id": "2510.21144", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21144", "abs": "https://arxiv.org/abs/2510.21144", "authors": ["Hanyu Zhu", "Lance Fiondella", "Jiawei Yuan", "Kai Zeng", "Long Jiao"], "title": "NeuroGenPoisoning: Neuron-Guided Attacks on Retrieval-Augmented Generation of LLM via Genetic Optimization of External Knowledge", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) empowers Large Language Models (LLMs) to\ndynamically integrate external knowledge during inference, improving their\nfactual accuracy and adaptability. However, adversaries can inject poisoned\nexternal knowledge to override the model's internal memory. While existing\nattacks iteratively manipulate retrieval content or prompt structure of RAG,\nthey largely ignore the model's internal representation dynamics and\nneuron-level sensitivities. The underlying mechanism of RAG poisoning has not\nbeen fully studied and the effect of knowledge conflict with strong parametric\nknowledge in RAG is not considered. In this work, we propose NeuroGenPoisoning,\na novel attack framework that generates adversarial external knowledge in RAG\nguided by LLM internal neuron attribution and genetic optimization. Our method\nfirst identifies a set of Poison-Responsive Neurons whose activation strongly\ncorrelates with contextual poisoning knowledge. We then employ a genetic\nalgorithm to evolve adversarial passages that maximally activate these neurons.\nCrucially, our framework enables massive-scale generation of effective poisoned\nRAG knowledge by identifying and reusing promising but initially unsuccessful\nexternal knowledge variants via observed attribution signals. At the same time,\nPoison-Responsive Neurons guided poisoning can effectively resolves knowledge\nconflict. Experimental results across models and datasets demonstrate\nconsistently achieving high Population Overwrite Success Rate (POSR) of over\n90% while preserving fluency. Empirical evidence shows that our method\neffectively resolves knowledge conflict."}
{"id": "2510.21524", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21524", "abs": "https://arxiv.org/abs/2510.21524", "authors": ["Ilija Lichkovski", "Alexander Müller", "Mariam Ibrahim", "Tiwai Mhundwa"], "title": "EU-Agent-Bench: Measuring Illegal Behavior of LLM Agents Under EU Law", "comment": "Accepted at the Workshop on Regulatable ML at the 39th Conference on\n  Neural Information Processing Systems (NeurIPS 2025)", "summary": "Large language models (LLMs) are increasingly deployed as agents in various\ncontexts by providing tools at their disposal. However, LLM agents can exhibit\nunpredictable behaviors, including taking undesirable and/or unsafe actions. In\norder to measure the latent propensity of LLM agents for taking illegal actions\nunder an EU legislative context, we introduce EU-Agent-Bench, a verifiable\nhuman-curated benchmark that evaluates an agent's alignment with EU legal norms\nin situations where benign user inputs could lead to unlawful actions. Our\nbenchmark spans scenarios across several categories, including data protection,\nbias/discrimination, and scientific integrity, with each user request allowing\nfor both compliant and non-compliant execution of the requested actions.\nComparing the model's function calls against a rubric exhaustively supported by\ncitations of the relevant legislature, we evaluate the legal compliance of\nfrontier LLMs, and furthermore investigate the compliance effect of providing\nthe relevant legislative excerpts in the agent's system prompt along with\nexplicit instructions to comply. We release a public preview set for the\nresearch community, while holding out a private test set to prevent data\ncontamination in evaluating upcoming models. We encourage future work extending\nagentic safety benchmarks to different legal jurisdictions and to multi-turn\nand multilingual interactions. We release our code on\n\\href{https://github.com/ilijalichkovski/eu-agent-bench}{this URL}."}
{"id": "2510.21148", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21148", "abs": "https://arxiv.org/abs/2510.21148", "authors": ["Yang Zhao", "Pu Wang", "Hao Frank Yang"], "title": "How to Auto-optimize Prompts for Domain Tasks? Adaptive Prompting and Reasoning through Evolutionary Domain Knowledge Adaptation", "comment": null, "summary": "Designing optimal prompts and reasoning processes for large language models\n(LLMs) on domain-specific tasks is both necessary and challenging in real-world\napplications. Determining how to integrate domain knowledge, enhance reasoning\nefficiency, and even provide domain experts with refined knowledge integration\nhints are particularly crucial yet unresolved tasks. In this research, we\npropose Evolutionary Graph Optimization for Prompting (EGO-Prompt), an\nautomated framework to designing better prompts, efficient reasoning processes\nand providing enhanced causal-informed process. EGO-Prompt begins with a\ngeneral prompt and fault-tolerant initial Semantic Causal Graph (SCG)\ndescriptions, constructed by human experts, which is then automatically refined\nand optimized to guide LLM reasoning. Recognizing that expert-defined SCGs may\nbe partial or imperfect and that their optimal integration varies across LLMs,\nEGO-Prompt integrates a novel causal-guided textual gradient process in two\nsteps: first, generating nearly deterministic reasoning guidance from the SCG\nfor each instance, and second, adapting the LLM to effectively utilize the\nguidance alongside the original input. The iterative optimization algorithm\nfurther refines both the SCG and the reasoning mechanism using textual\ngradients with ground-truth. We tested the framework on real-world public\nhealth, transportation and human behavior tasks. EGO-Prompt achieves\n7.32%-12.61% higher F1 than cutting-edge methods, and allows small models to\nreach the performence of larger models at under 20% of the original cost. It\nalso outputs a refined, domain-specific SCG that improves interpretability."}
{"id": "2510.21557", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21557", "abs": "https://arxiv.org/abs/2510.21557", "authors": ["Hongwei Zhang", "Ji Lu", "Shiqing Jiang", "Chenxiang Zhu", "Li Xie", "Chen Zhong", "Haoran Chen", "Yurui Zhu", "Yongsheng Du", "Yanqin Gao", "Lingjun Huang", "Baoli Wang", "Fang Tan", "Peng Zou"], "title": "Co-Sight: Enhancing LLM-Based Agents via Conflict-Aware Meta-Verification and Trustworthy Reasoning with Structured Facts", "comment": null, "summary": "Long-horizon reasoning in LLM-based agents often fails not from generative\nweakness but from insufficient verification of intermediate reasoning. Co-Sight\naddresses this challenge by turning reasoning into a falsifiable and auditable\nprocess through two complementary mechanisms: Conflict-Aware Meta-Verification\n(CAMV) and Trustworthy Reasoning with Structured Facts (TRSF). CAMV\nreformulates verification as conflict identification and targeted\nfalsification, allocating computation only to disagreement hotspots among\nexpert agents rather than to full reasoning chains. This bounds verification\ncost to the number of inconsistencies and improves efficiency and reliability.\nTRSF continuously organizes, validates, and synchronizes evidence across agents\nthrough a structured facts module. By maintaining verified, traceable, and\nauditable knowledge, it ensures that all reasoning is grounded in consistent,\nsource-verified information and supports transparent verification throughout\nthe reasoning process. Together, TRSF and CAMV form a closed verification loop,\nwhere TRSF supplies structured facts and CAMV selectively falsifies or\nreinforces them, yielding transparent and trustworthy reasoning. Empirically,\nCo-Sight achieves state-of-the-art accuracy on GAIA (84.4%) and Humanity's Last\nExam (35.5%), and strong results on Chinese-SimpleQA (93.8%). Ablation studies\nconfirm that the synergy between structured factual grounding and\nconflict-aware verification drives these improvements. Co-Sight thus offers a\nscalable paradigm for reliable long-horizon reasoning in LLM-based agents. Code\nis available at\nhttps://github.com/ZTE-AICloud/Co-Sight/tree/cosight2.0_benchmarks."}
{"id": "2510.21150", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21150", "abs": "https://arxiv.org/abs/2510.21150", "authors": ["Kou Misaki", "Takuya Akiba"], "title": "String Seed of Thought: Prompting LLMs for Distribution-Faithful and Diverse Generation", "comment": null, "summary": "We introduce String Seed of Thought (SSoT), a novel prompting method for LLMs\nthat improves Probabilistic Instruction Following (PIF). We define PIF as a\ntask requiring an LLM to select its answer from a predefined set of options,\neach associated with a specific probability, such that the empirical\ndistribution of the generated answers aligns with the target distribution when\nprompted multiple times. While LLMs excel at tasks with single, deterministic\nanswers, they often fail at PIF, exhibiting biases problematic for applications\nrequiring non-deterministic behaviors, such as human-behavior simulation,\ncontent diversification, and multiplayer games. It also harms the diversity of\ngenerated responses, a crucial factor in test-time scaling, by causing the\noutputs to collapse into a limited set of answers. To address this, we propose\nSSoT, a simple prompting method that instructs an LLM to first output a random\nstring to generate sufficient entropy. SSoT also instructs the LLM to extract\nrandomness by manipulating this string to derive a final answer, thereby\npreserving diversity while adhering to specific constraints. We demonstrate\nthat SSoT significantly improves the PIF performance of LLMs, approaching the\nideal performance of a pseudo-random number generator. Furthermore, our\nexperiments on NoveltyBench show SSoT's benefits extend beyond closed-set tasks\nto open-ended tasks by enhancing response diversity."}
{"id": "2510.21560", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21560", "abs": "https://arxiv.org/abs/2510.21560", "authors": ["Yuxuan Yang", "Hussein Sibai"], "title": "Learning Neural Control Barrier Functions from Expert Demonstrations using Inverse Constraint Learning", "comment": null, "summary": "Safety is a fundamental requirement for autonomous systems operating in\ncritical domains. Control barrier functions (CBFs) have been used to design\nsafety filters that minimally alter nominal controls for such systems to\nmaintain their safety. Learning neural CBFs has been proposed as a data-driven\nalternative for their computationally expensive optimization-based synthesis.\nHowever, it is often the case that the failure set of states that should be\navoided is non-obvious or hard to specify formally, e.g., tailgating in\nautonomous driving, while a set of expert demonstrations that achieve the task\nand avoid the failure set is easier to generate. We use ICL to train a\nconstraint function that classifies the states of the system under\nconsideration to safe, i.e., belong to a controlled forward invariant set that\nis disjoint from the unspecified failure set, and unsafe ones, i.e., belong to\nthe complement of that set. We then use that function to label a new set of\nsimulated trajectories to train our neural CBF. We empirically evaluate our\napproach in four different environments, demonstrating that it outperforms\nexisting baselines and achieves comparable performance to a neural CBF trained\nwith the same data but annotated with ground-truth safety labels."}
{"id": "2510.21164", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21164", "abs": "https://arxiv.org/abs/2510.21164", "authors": ["Shamistan Karimov", "Elian Neppel", "Shreya Santra", "Kentaro Uno", "Kazuya Yoshida"], "title": "An Agnostic End-Effector Alignment Controller for Robust Assembly of Modular Space Robots", "comment": "6 pages, 12 figures. Accepted at iSparo 2025 | Video:\n  https://youtu.be/BW0YgSrvuDo", "summary": "Modular robots offer reconfigurability and fault tolerance essential for\nlunar missions, but require controllers that adapt safely to real-world\ndisturbances. We build on our previous hardware-agnostic actuator\nsynchronization in Motion Stack to develop a new controller enforcing adaptive\nvelocity bounds via a dynamic hypersphere clamp. Using only real-time\nend-effector and target pose measurements, the controller adjusts its\ntranslational and rotational speed limits to ensure smooth, stable alignment\nwithout abrupt motions. We implemented two variants, a discrete, step-based\nversion and a continuous, velocity-based version, and tested them on two\nMoonBot limbs in JAXA's lunar environment simulator. Field trials demonstrate\nthat the step-based variant produces highly predictable, low-wobble motions,\nwhile the continuous variant converges more quickly and maintains\nmillimeter-level positional accuracy, and both remain robust across limbs with\ndiffering mechanical imperfections and sensing noise (e.g., backlash and flex).\nThese results highlight the flexibility and robustness of our robot-agnostic\nframework for autonomous self-assembly and reconfiguration under harsh\nconditions."}
{"id": "2510.21614", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21614", "abs": "https://arxiv.org/abs/2510.21614", "authors": ["Wenyi Wang", "Piotr Piękos", "Li Nanbo", "Firas Laakom", "Yimeng Chen", "Mateusz Ostaszewski", "Mingchen Zhuge", "Jürgen Schmidhuber"], "title": "Huxley-Gödel Machine: Human-Level Coding Agent Development by an Approximation of the Optimal Self-Improving Machine", "comment": null, "summary": "Recent studies operationalize self-improvement through coding agents that\nedit their own codebases. They grow a tree of self-modifications through\nexpansion strategies that favor higher software engineering benchmark\nperformance, assuming that this implies more promising subsequent\nself-modifications. However, we identify a mismatch between the agent's\nself-improvement potential (metaproductivity) and its coding benchmark\nperformance, namely the Metaproductivity-Performance Mismatch. Inspired by\nHuxley's concept of clade, we propose a metric ($\\mathrm{CMP}$) that aggregates\nthe benchmark performances of the descendants of an agent as an indicator of\nits potential for self-improvement. We show that, in our self-improving coding\nagent development setting, access to the true $\\mathrm{CMP}$ is sufficient to\nsimulate how the G\\\"odel Machine would behave under certain assumptions. We\nintroduce the Huxley-G\\\"odel Machine (HGM), which, by estimating $\\mathrm{CMP}$\nand using it as guidance, searches the tree of self-modifications. On SWE-bench\nVerified and Polyglot, HGM outperforms prior self-improving coding agent\ndevelopment methods while using less wall-clock time. Last but not least, HGM\ndemonstrates strong transfer to other coding datasets and large language\nmodels. The agent optimized by HGM on SWE-bench Verified with GPT-5-mini and\nevaluated on SWE-bench Lite with GPT-5 achieves human-level performance,\nmatching the best officially checked results of human-engineered coding agents.\nOur code is available at https://github.com/metauto-ai/HGM."}
{"id": "2510.21175", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21175", "abs": "https://arxiv.org/abs/2510.21175", "authors": ["Yujin Jo", "Taesup Kim"], "title": "Memory-Free Continual Learning with Null Space Adaptation for Zero-Shot Vision-Language Models", "comment": null, "summary": "Pre-trained vision-language models (VLMs), such as CLIP, have demonstrated\nremarkable zero-shot generalization, enabling deployment in a wide range of\nreal-world tasks without additional task-specific training. However, in real\ndeployment scenarios with evolving environments or emerging classes, these\nmodels inevitably face distributional shifts and novel tasks. In such contexts,\nstatic zero-shot capabilities are insufficient, and there is a growing need for\ncontinual learning methods that allow models to adapt over time while avoiding\ncatastrophic forgetting. We introduce NuSA-CL (Null Space Adaptation for\nContinual Learning), a lightweight memory-free continual learning framework\ndesigned to address this challenge. NuSA-CL employs low-rank adaptation and\nconstrains task-specific weight updates to lie within an approximate null space\nof the model's current parameters. This strategy minimizes interference with\npreviously acquired knowledge, effectively preserving the zero-shot\ncapabilities of the original model. Unlike methods relying on replay buffers or\ncostly distillation, NuSA-CL imposes minimal computational and memory overhead,\nmaking it practical for deployment in resource-constrained, real-world\ncontinual learning environments. Experiments show that our framework not only\neffectively preserves zero-shot transfer capabilities but also achieves highly\ncompetitive performance on continual learning benchmarks. These results\nposition NuSA-CL as a practical and scalable solution for continually evolving\nzero-shot VLMs in real-world applications."}
{"id": "2510.21618", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21618", "abs": "https://arxiv.org/abs/2510.21618", "authors": ["Xiaoxi Li", "Wenxiang Jiao", "Jiarui Jin", "Guanting Dong", "Jiajie Jin", "Yinuo Wang", "Hao Wang", "Yutao Zhu", "Ji-Rong Wen", "Yuan Lu", "Zhicheng Dou"], "title": "DeepAgent: A General Reasoning Agent with Scalable Toolsets", "comment": null, "summary": "Large reasoning models have demonstrated strong problem-solving abilities,\nyet real-world tasks often require external tools and long-horizon\ninteractions. Existing agent frameworks typically follow predefined workflows,\nwhich limit autonomous and global task completion. In this paper, we introduce\nDeepAgent, an end-to-end deep reasoning agent that performs autonomous\nthinking, tool discovery, and action execution within a single, coherent\nreasoning process. To address the challenges of long-horizon interactions,\nparticularly the context length explosion from multiple tool calls and the\naccumulation of interaction history, we introduce an autonomous memory folding\nmechanism that compresses past interactions into structured episodic, working,\nand tool memories, reducing error accumulation while preserving critical\ninformation. To teach general-purpose tool use efficiently and stably, we\ndevelop an end-to-end reinforcement learning strategy, namely ToolPO, that\nleverages LLM-simulated APIs and applies tool-call advantage attribution to\nassign fine-grained credit to the tool invocation tokens. Extensive experiments\non eight benchmarks, including general tool-use tasks (ToolBench, API-Bank,\nTMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA,\nHLE), demonstrate that DeepAgent consistently outperforms baselines across both\nlabeled-tool and open-set tool retrieval scenarios. This work takes a step\ntoward more general and capable agents for real-world applications. The code\nand demo are available at https://github.com/RUC-NLPIR/DeepAgent."}
{"id": "2510.21179", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.21179", "abs": "https://arxiv.org/abs/2510.21179", "authors": ["Frederik Wagner Madsen", "Joy Dalmacio Billanes", "Bo Nørregaard Jørgensen", "Zheng Ma"], "title": "Green Hydrogen under Uncertainty: Evaluating Power-to-X Strategies Using Agent-Based Simulation and Multi-Criteria Decision Framework", "comment": null, "summary": "The transition toward net-zero energy systems requires scalable and\ncost-effective deployment of Power-to-X technologies, particularly green\nhydrogen production. Despite increasing investments, a critical research gap\nremains in dynamically assessing how different operational strategies affect\nthe feasibility of hydrogen production under real-world energy market\nconditions. Most existing studies rely on static, techno-economic models and\noverlook actor interactions, infrastructure limitations, and regulatory\ncomplexity. This paper presents a novel modeling framework that integrates\nagent-based simulation with multi-criteria decision-making to evaluate green\nhydrogen production strategies using co-located wind and solar generation.\nThree operational strategies - grid-only, on-site-only, and hybrid - are\napplied across three electrolyzer capacity levels (10 MW, 50 MW, and 100 MW)\nwithin a Danish case study. Real electricity tariffs, emissions factors, and\nmarket data are used to simulate technical, economic, and environmental\nperformance indicators. The results show that hybrid strategies consistently\noutperform grid-only configurations in terms of cost and emissions while\nmaintaining stable hydrogen output. Although on-site-only strategies minimize\nemissions and costs, they fail to meet fixed production demands. This framework\noffers novel scientific contributions by modeling dynamic actor interactions\nand integrating system performance evaluation into strategic planning.\nPractically, it provides actionable insights for energy planners and\npolicymakers designing resilient and efficient Power-to-X systems in\nrenewable-rich contexts."}
{"id": "2510.21652", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.21652", "abs": "https://arxiv.org/abs/2510.21652", "authors": ["Jonathan Bragg", "Mike D'Arcy", "Nishant Balepur", "Dan Bareket", "Bhavana Dalvi", "Sergey Feldman", "Dany Haddad", "Jena D. Hwang", "Peter Jansen", "Varsha Kishore", "Bodhisattwa Prasad Majumder", "Aakanksha Naik", "Sigal Rahamimov", "Kyle Richardson", "Amanpreet Singh", "Harshit Surana", "Aryeh Tiktinsky", "Rosni Vasu", "Guy Wiener", "Chloe Anastasiades", "Stefan Candra", "Jason Dunkelberger", "Dan Emery", "Rob Evans", "Malachi Hamada", "Regan Huff", "Rodney Kinney", "Matt Latzke", "Jaron Lochner", "Ruben Lozano-Aguilera", "Cecile Nguyen", "Smita Rao", "Amber Tanaka", "Brooke Vlahos", "Peter Clark", "Doug Downey", "Yoav Goldberg", "Ashish Sabharwal", "Daniel S. Weld"], "title": "AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research Suite", "comment": null, "summary": "AI agents hold the potential to revolutionize scientific productivity by\nautomating literature reviews, replicating experiments, analyzing data, and\neven proposing new directions of inquiry; indeed, there are now many such\nagents, ranging from general-purpose \"deep research\" systems to specialized\nscience-specific agents, such as AI Scientist and AIGS. Rigorous evaluation of\nthese agents is critical for progress. Yet existing benchmarks fall short on\nseveral fronts: they (1) fail to provide holistic, product-informed measures of\nreal-world use cases such as science research; (2) lack reproducible agent\ntools necessary for a controlled comparison of core agentic capabilities; (3)\ndo not account for confounding variables such as model cost and tool access;\n(4) do not provide standardized interfaces for quick agent prototyping and\nevaluation; and (5) lack comprehensive baseline agents necessary to identify\ntrue advances. In response, we define principles and tooling for more\nrigorously benchmarking agents. Using these, we present AstaBench, a suite that\nprovides the first holistic measure of agentic ability to perform scientific\nresearch, comprising 2400+ problems spanning the entire scientific discovery\nprocess and multiple scientific domains, and including many problems inspired\nby actual user requests to deployed Asta agents. Our suite comes with the first\nscientific research environment with production-grade search tools that enable\ncontrolled, reproducible evaluation, better accounting for confounders.\nAlongside, we provide a comprehensive suite of nine science-optimized classes\nof Asta agents and numerous baselines. Our extensive evaluation of 57 agents\nacross 22 agent classes reveals several interesting findings, most importantly\nthat despite meaningful progress on certain individual aspects, AI remains far\nfrom solving the challenge of science research assistance."}
{"id": "2510.21181", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21181", "abs": "https://arxiv.org/abs/2510.21181", "authors": ["Shuo Li", "Keqin Xu", "Jie Liu", "Dan Ye"], "title": "Shylock: Causal Discovery in Multivariate Time Series based on Hybrid Constraints", "comment": null, "summary": "Causal relationship discovery has been drawing increasing attention due to\nits prevalent application. Existing methods rely on human experience,\nstatistical methods, or graphical criteria methods which are error-prone, stuck\nat the idealized assumption, and rely on a huge amount of data. And there is\nalso a serious data gap in accessing Multivariate time series(MTS) in many\nareas, adding difficulty in finding their causal relationship. Existing methods\nare easy to be over-fitting on them. To fill the gap we mentioned above, in\nthis paper, we propose Shylock, a novel method that can work well in both\nfew-shot and normal MTS to find the causal relationship. Shylock can reduce the\nnumber of parameters exponentially by using group dilated convolution and a\nsharing kernel, but still learn a better representation of variables with time\ndelay. By combing the global constraint and the local constraint, Shylock\nachieves information sharing among networks to help improve the accuracy. To\nevaluate the performance of Shylock, we also design a data generation method to\ngenerate MTS with time delay. We evaluate it on commonly used benchmarks and\ngenerated datasets. Extensive experiments show that Shylock outperforms two\nexisting state-of-art methods on both few-shot and normal MTS. We also\ndeveloped Tcausal, a library for easy use and deployed it on the EarthDataMiner\nplatform"}
{"id": "2510.21656", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21656", "abs": "https://arxiv.org/abs/2510.21656", "authors": ["Marta Contreiras Silva", "Daniel Faria", "Catia Pesquita"], "title": "CMOMgen: Complex Multi-Ontology Alignment via Pattern-Guided In-Context Learning", "comment": "32 pages, 5 figures", "summary": "Constructing comprehensive knowledge graphs requires the use of multiple\nontologies in order to fully contextualize data into a domain. Ontology\nmatching finds equivalences between concepts interconnecting ontologies and\ncreating a cohesive semantic layer. While the simple pairwise state of the art\nis well established, simple equivalence mappings cannot provide full semantic\nintegration of related but disjoint ontologies. Complex multi-ontology matching\n(CMOM) aligns one source entity to composite logical expressions of multiple\ntarget entities, establishing more nuanced equivalences and provenance along\nthe ontological hierarchy.\n  We present CMOMgen, the first end-to-end CMOM strategy that generates\ncomplete and semantically sound mappings, without establishing any restrictions\non the number of target ontologies or entities. Retrieval-Augmented Generation\nselects relevant classes to compose the mapping and filters matching reference\nmappings to serve as examples, enhancing In-Context Learning. The strategy was\nevaluated in three biomedical tasks with partial reference alignments. CMOMgen\noutperforms baselines in class selection, demonstrating the impact of having a\ndedicated strategy. Our strategy also achieves a minimum of 63% in F1-score,\noutperforming all baselines and ablated versions in two out of three tasks and\nplacing second in the third. Furthermore, a manual evaluation of non-reference\nmappings showed that 46% of the mappings achieve the maximum score, further\nsubstantiating its ability to construct semantically sound mappings."}
{"id": "2510.21203", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2510.21203", "abs": "https://arxiv.org/abs/2510.21203", "authors": ["Sophia Hatz"], "title": "The Nuclear Analogy in AI Governance Research", "comment": "Hatz, S. (in press). The Nuclear Analogy in AI Governance Research.\n  In M. Furendal & M. Lundgren (Eds.), Handbook on the Global Governance of\n  Artificial Intelligence. Edward Elgar Publishing", "summary": "The analogy between Artificial Intelligence (AI) and nuclear weapons is\nprominent in academic and policy discourse on AI governance. This chapter\nreviews 43 scholarly works which explicitly draw on the nuclear domain to\nderive lessons for AI governance. We identify four problem areas where\nresearchers apply nuclear precedents: (1) early development and governance of\ntransformative technologies; (2) international security risks and strategy; (3)\ninternational institutions and agreements; and (4) domestic safety regulation.\nWhile nuclear-inspired AI proposals are often criticised due to differences\nacross domains, this review clarifies how historical analogies can inform\npolicy development even when technological domains differ substantially.\nValuable functions include providing conceptual frameworks for analyzing\nstrategic dynamics, offering cautionary lessons about unsuccessful governance\napproaches, and expanding policy imagination by legitimizing radical proposals.\nGiven that policymakers already invoke the nuclear analogy, continued critical\nengagement with these historical precedents remains essential for shaping\neffective global AI governance."}
{"id": "2510.21679", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21679", "abs": "https://arxiv.org/abs/2510.21679", "authors": ["Gaku Morio", "Harri Rowlands", "Dominik Stammbach", "Christopher D. Manning", "Peter Henderson"], "title": "A Multimodal Benchmark for Framing of Oil & Gas Advertising and Potential Greenwashing Detection", "comment": "Forthcoming in NeurIPS 2025 Datasets and Benchmarks Track", "summary": "Companies spend large amounts of money on public relations campaigns to\nproject a positive brand image. However, sometimes there is a mismatch between\nwhat they say and what they do. Oil & gas companies, for example, are accused\nof \"greenwashing\" with imagery of climate-friendly initiatives. Understanding\nthe framing, and changes in framing, at scale can help better understand the\ngoals and nature of public relations campaigns. To address this, we introduce a\nbenchmark dataset of expert-annotated video ads obtained from Facebook and\nYouTube. The dataset provides annotations for 13 framing types for more than 50\ncompanies or advocacy groups across 20 countries. Our dataset is especially\ndesigned for the evaluation of vision-language models (VLMs), distinguishing it\nfrom past text-only framing datasets. Baseline experiments show some promising\nresults, while leaving room for improvement for future work: GPT-4.1 can detect\nenvironmental messages with 79% F1 score, while our best model only achieves\n46% F1 score on identifying framing around green innovation. We also identify\nchallenges that VLMs must address, such as implicit framing, handling videos of\nvarious lengths, or implicit cultural backgrounds. Our dataset contributes to\nresearch in multimodal analysis of strategic communication in the energy\nsector."}
{"id": "2510.21215", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21215", "abs": "https://arxiv.org/abs/2510.21215", "authors": ["Shuoshuo Ding", "Tiedong Zhang", "Dapeng Jiang", "Ming Lei"], "title": "Underwater Visual-Inertial-Acoustic-Depth SLAM with DVL Preintegration for Degraded Environments", "comment": "10 pages, 10 figures", "summary": "Visual degradation caused by limited visibility, insufficient lighting, and\nfeature scarcity in underwater environments presents significant challenges to\nvisual-inertial simultaneous localization and mapping (SLAM) systems. To\naddress these challenges, this paper proposes a graph-based\nvisual-inertial-acoustic-depth SLAM system that integrates a stereo camera, an\ninertial measurement unit (IMU), the Doppler velocity log (DVL), and a pressure\nsensor. The key innovation lies in the tight integration of four distinct\nsensor modalities to ensure reliable operation, even under degraded visual\nconditions. To mitigate DVL drift and improve measurement efficiency, we\npropose a novel velocity-bias-based DVL preintegration strategy. At the\nfrontend, hybrid tracking strategies and acoustic-inertial-depth joint\noptimization enhance system stability. Additionally, multi-source hybrid\nresiduals are incorporated into a graph optimization framework. Extensive\nquantitative and qualitative analyses of the proposed system are conducted in\nboth simulated and real-world underwater scenarios. The results demonstrate\nthat our approach outperforms current state-of-the-art stereo visual-inertial\nSLAM systems in both stability and localization accuracy, exhibiting\nexceptional robustness, particularly in visually challenging environments."}
{"id": "2510.21695", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21695", "abs": "https://arxiv.org/abs/2510.21695", "authors": ["Edward Holmberg", "Elias Ioup", "Mahdi Abdelguerfi"], "title": "A Knowledge-Graph Translation Layer for Mission-Aware Multi-Agent Path Planning in Spatiotemporal Dynamics", "comment": "10 pages, 10 figures, conference submission", "summary": "The coordination of autonomous agents in dynamic environments is hampered by\nthe semantic gap between high-level mission objectives and low-level planner\ninputs. To address this, we introduce a framework centered on a Knowledge Graph\n(KG) that functions as an intelligent translation layer. The KG's two-plane\narchitecture compiles declarative facts into per-agent, mission-aware\n``worldviews\" and physics-aware traversal rules, decoupling mission semantics\nfrom a domain-agnostic planner. This allows complex, coordinated paths to be\nmodified simply by changing facts in the KG. A case study involving Autonomous\nUnderwater Vehicles (AUVs) in the Gulf of Mexico visually demonstrates the\nend-to-end process and quantitatively proves that different declarative\npolicies produce distinct, high-performing outcomes. This work establishes the\nKG not merely as a data repository, but as a powerful, stateful orchestrator\nfor creating adaptive and explainable autonomous systems."}
{"id": "2510.21219", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2510.21219", "abs": "https://arxiv.org/abs/2510.21219", "authors": ["Xiaoyuan Zhang", "Chengdong Ma", "Yizhe Huang", "Weidong Huang", "Siyuan Qi", "Song-Chun Zhu", "Xue Feng", "Yaodong Yang"], "title": "World Models Should Prioritize the Unification of Physical and Social Dynamics", "comment": null, "summary": "World models, which explicitly learn environmental dynamics to lay the\nfoundation for planning, reasoning, and decision-making, are rapidly advancing\nin predicting both physical dynamics and aspects of social behavior, yet\npredominantly in separate silos. This division results in a systemic failure to\nmodel the crucial interplay between physical environments and social\nconstructs, rendering current models fundamentally incapable of adequately\naddressing the true complexity of real-world systems where physical and social\nrealities are inextricably intertwined. This position paper argues that the\nsystematic, bidirectional unification of physical and social predictive\ncapabilities is the next crucial frontier for world model development. We\ncontend that comprehensive world models must holistically integrate objective\nphysical laws with the subjective, evolving, and context-dependent nature of\nsocial dynamics. Such unification is paramount for AI to robustly navigate\ncomplex real-world challenges and achieve more generalizable intelligence. This\npaper substantiates this imperative by analyzing core impediments to\nintegration, proposing foundational guiding principles (ACE Principles), and\noutlining a conceptual framework alongside a research roadmap towards truly\nholistic world models."}
{"id": "2510.20916", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.20916", "abs": "https://arxiv.org/abs/2510.20916", "authors": ["Sydney M. Katz", "Robert J. Moss", "Dylan M. Asmar", "Wesley A. Olson", "James K. Kuchar", "Mykel J. Kochenderfer"], "title": "Aircraft Collision Avoidance Systems: Technological Challenges and Solutions on the Path to Regulatory Acceptance", "comment": "32 pages, 9 figures", "summary": "Aircraft collision avoidance systems is critical to modern aviation. These\nsystems are designed to predict potential collisions between aircraft and\nrecommend appropriate avoidance actions. Creating effective collision avoidance\nsystems requires solutions to a variety of technical challenges related to\nsurveillance, decision making, and validation. These challenges have sparked\nsignificant research and development efforts over the past several decades that\nhave resulted in a variety of proposed solutions. This article provides an\noverview of these challenges and solutions with an emphasis on those that have\nbeen put through a rigorous validation process and accepted by regulatory\nbodies. The challenges posed by the collision avoidance problem are often\npresent in other domains, and aircraft collision avoidance systems can serve as\ncase studies that provide valuable insights for a wide range of safety-critical\nsystems."}
{"id": "2510.21227", "categories": ["eess.SY", "cs.SY", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.21227", "abs": "https://arxiv.org/abs/2510.21227", "authors": ["Ke Sun", "Jingyi Yan", "Zhenglin Li", "Shaorong Xie"], "title": "The Role of Information Incompleteness in Defending Against Stealth Attacks", "comment": null, "summary": "The effectiveness of Data Injections Attacks (DIAs) critically depends on the\ncompleteness of the system information accessible to adversaries. This\nrelationship positions information incompleteness enhancement as a vital\ndefense strategy for degrading DIA performance. In this paper, we focus on the\ninformation-theoretic stealth attacks, where the attacker encounters a\nfundamental tradeoff between the attack stealthiness and destructiveness.\nSpecifically, we systematically characterize how incomplete admittance\ninformation impacts the dual objectives. In particular, we establish sufficient\nconditions for two distinct operational regimes: (i) stealthiness intensifies\nwhile destructive potential diminishes and (ii) destructiveness increases while\nstealth capability weakens. For scenarios beyond these regimes, we propose a\nmaximal incompleteness strategy to optimally degrade stealth capability. To\nsolve the associated optimization problem, the feasible region is reduced\nwithout excluding the optimal solution, and a heuristic algorithm is then\nintroduced to effectively identify the near-optimal solutions within the\nreduced region. Numerical simulations are conducted on IEEE test systems to\nvalidate the findings."}
{"id": "2510.21082", "categories": ["cs.CY", "cs.AI", "cs.HC", "68T50 (Artificial intelligence)", "I.2.7; K.5.2"], "pdf": "https://arxiv.org/pdf/2510.21082", "abs": "https://arxiv.org/abs/2510.21082", "authors": ["Jorge Alberto Araujo"], "title": "Soppia: A Structured Prompting Framework for the Proportional Assessment of Non-Pecuniary Damages in Personal Injury Cases", "comment": "9 pages, 2 tables, includes GitHub link to framework implementation.\n  Submitted to the Artificial Intelligence and Law section of arXiv", "summary": "Applying complex legal rules characterized by multiple, heterogeneously\nweighted criteria presents a fundamental challenge in judicial decision-making,\noften hindering the consistent realization of legislative intent. This\nchallenge is particularly evident in the quantification of non-pecuniary\ndamages in personal injury cases. This paper introduces Soppia, a structured\nprompting framework designed to assist legal professionals in navigating this\ncomplexity. By leveraging advanced AI, the system ensures a comprehensive and\nbalanced analysis of all stipulated criteria, fulfilling the legislator's\nintent that compensation be determined through a holistic assessment of each\ncase. Using the twelve criteria for non-pecuniary damages established in the\nBrazilian CLT (Art. 223-G) as a case study, we demonstrate how Soppia (System\nfor Ordered Proportional and Pondered Intelligent Assessment) operationalizes\nnuanced legal commands into a practical, replicable, and transparent\nmethodology. The framework enhances consistency and predictability while\nproviding a versatile and explainable tool adaptable across multi-criteria\nlegal contexts, bridging normative interpretation and computational reasoning\ntoward auditable legal AI."}
{"id": "2510.21238", "categories": ["eess.SY", "cs.AI", "cs.IT", "cs.SY", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.21238", "abs": "https://arxiv.org/abs/2510.21238", "authors": ["Wangqian Chen", "Junting Chen", "Shuguang Cui"], "title": "Physics-Informed Neural Networks for MIMO Beam Map and Environment Reconstruction", "comment": null, "summary": "As communication networks evolve towards greater complexity (e.g., 6G and\nbeyond), a deep understanding of the wireless environment becomes increasingly\ncrucial. When explicit knowledge of the environment is unavailable,\ngeometry-aware feature extraction from channel state information (CSI) emerges\nas a pivotal methodology to bridge physical-layer measurements with network\nintelligence. This paper proposes to explore the received signal strength (RSS)\ndata, without explicit 3D environment knowledge, to jointly construct the radio\nbeam map and environmental geometry for a multiple-input multiple-output (MIMO)\nsystem. Unlike existing methods that only learn blockage structures, we propose\nan oriented virtual obstacle model that captures the geometric features of both\nblockage and reflection. Reflective zones are formulated to identify relevant\nreflected paths according to the geometry relation of the environment. We\nderive an analytical expression for the reflective zone and further analyze its\ngeometric characteristics to develop a reformulation that is more compatible\nwith deep learning representations. A physics-informed deep learning framework\nthat incorporates the reflective-zone-based geometry model is proposed to learn\nthe blockage, reflection, and scattering components, along with the beam\npattern, which leverages physics prior knowledge to enhance network\ntransferability. Numerical experiments demonstrate that, in addition to\nreconstructing the blockage and reflection geometry, the proposed model can\nconstruct a more accurate MIMO beam map with a 32%-48% accuracy improvement."}
{"id": "2510.21121", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21121", "abs": "https://arxiv.org/abs/2510.21121", "authors": ["Haibo Zhao", "Yu Qi", "Boce Hu", "Yizhe Zhu", "Ziyan Chen", "Heng Tian", "Xupeng Zhu", "Owen Howell", "Haojie Huang", "Robin Walters", "Dian Wang", "Robert Platt"], "title": "Generalizable Hierarchical Skill Learning via Object-Centric Representation", "comment": null, "summary": "We present Generalizable Hierarchical Skill Learning (GSL), a novel framework\nfor hierarchical policy learning that significantly improves policy\ngeneralization and sample efficiency in robot manipulation. One core idea of\nGSL is to use object-centric skills as an interface that bridges the high-level\nvision-language model and the low-level visual-motor policy. Specifically, GSL\ndecomposes demonstrations into transferable and object-canonicalized skill\nprimitives using foundation models, ensuring efficient low-level skill learning\nin the object frame. At test time, the skill-object pairs predicted by the\nhigh-level agent are fed to the low-level module, where the inferred canonical\nactions are mapped back to the world frame for execution. This structured yet\nflexible design leads to substantial improvements in sample efficiency and\ngeneralization of our method across unseen spatial arrangements, object\nappearances, and task compositions. In simulation, GSL trained with only 3\ndemonstrations per task outperforms baselines trained with 30 times more data\nby 15.5 percent on unseen tasks. In real-world experiments, GSL also surpasses\nthe baseline trained with 10 times more data."}
{"id": "2510.21244", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21244", "abs": "https://arxiv.org/abs/2510.21244", "authors": ["Pengyu Xu", "Shijia Li", "Ao Sun", "Feng Zhang", "Yahan Li", "Bo Wu", "Zhanyu Ma", "Jiguo Li", "Jun Xu", "Jiuchong Gao", "Jinghua Hao", "Renqing He", "Rui Wang", "Yang Liu", "Xiaobo Hu", "Fan Yang", "Jia Zheng", "Guanghua Yao"], "title": "OutboundEval: A Dual-Dimensional Benchmark for Expert-Level Intelligent Outbound Evaluation of Xbench's Professional-Aligned Series", "comment": null, "summary": "We propose OutboundEval, a comprehensive benchmark for evaluating large\nlanguage models (LLMs) in expert-level intelligent outbound calling scenarios.\nUnlike existing methods that suffer from three key limitations - insufficient\ndataset diversity and category coverage, unrealistic user simulation, and\ninaccurate evaluation metrics - OutboundEval addresses these issues through a\nstructured framework. First, we design a benchmark spanning six major business\ndomains and 30 representative sub-scenarios, each with scenario-specific\nprocess decomposition, weighted scoring, and domain-adaptive metrics. Second,\nwe develop a large-model-driven User Simulator that generates diverse,\npersona-rich virtual users with realistic behaviors, emotional variability, and\ncommunication styles, providing a controlled yet authentic testing environment.\nThird, we introduce a dynamic evaluation method that adapts to task variations,\nintegrating automated and human-in-the-loop assessment to measure task\nexecution accuracy, professional knowledge application, adaptability, and user\nexperience quality. Experiments on 12 state-of-the-art LLMs reveal distinct\ntrade-offs between expert-level task completion and interaction fluency,\noffering practical insights for building reliable, human-like outbound AI\nsystems. OutboundEval establishes a practical, extensible, and domain-oriented\nstandard for benchmarking LLMs in professional applications."}
{"id": "2510.21238", "categories": ["eess.SY", "cs.AI", "cs.IT", "cs.SY", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.21238", "abs": "https://arxiv.org/abs/2510.21238", "authors": ["Wangqian Chen", "Junting Chen", "Shuguang Cui"], "title": "Physics-Informed Neural Networks for MIMO Beam Map and Environment Reconstruction", "comment": null, "summary": "As communication networks evolve towards greater complexity (e.g., 6G and\nbeyond), a deep understanding of the wireless environment becomes increasingly\ncrucial. When explicit knowledge of the environment is unavailable,\ngeometry-aware feature extraction from channel state information (CSI) emerges\nas a pivotal methodology to bridge physical-layer measurements with network\nintelligence. This paper proposes to explore the received signal strength (RSS)\ndata, without explicit 3D environment knowledge, to jointly construct the radio\nbeam map and environmental geometry for a multiple-input multiple-output (MIMO)\nsystem. Unlike existing methods that only learn blockage structures, we propose\nan oriented virtual obstacle model that captures the geometric features of both\nblockage and reflection. Reflective zones are formulated to identify relevant\nreflected paths according to the geometry relation of the environment. We\nderive an analytical expression for the reflective zone and further analyze its\ngeometric characteristics to develop a reformulation that is more compatible\nwith deep learning representations. A physics-informed deep learning framework\nthat incorporates the reflective-zone-based geometry model is proposed to learn\nthe blockage, reflection, and scattering components, along with the beam\npattern, which leverages physics prior knowledge to enhance network\ntransferability. Numerical experiments demonstrate that, in addition to\nreconstructing the blockage and reflection geometry, the proposed model can\nconstruct a more accurate MIMO beam map with a 32%-48% accuracy improvement."}
{"id": "2510.21254", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21254", "abs": "https://arxiv.org/abs/2510.21254", "authors": ["Victoria J. Hodge", "Colin Paterson", "Ibrahim Habli"], "title": "Out-of-Distribution Detection for Safety Assurance of AI and Autonomous Systems", "comment": null, "summary": "The operational capabilities and application domains of AI-enabled autonomous\nsystems have expanded significantly in recent years due to advances in robotics\nand machine learning (ML). Demonstrating the safety of autonomous systems\nrigorously is critical for their responsible adoption but it is challenging as\nit requires robust methodologies that can handle novel and uncertain situations\nthroughout the system lifecycle, including detecting out-of-distribution (OoD)\ndata. Thus, OOD detection is receiving increased attention from the research,\ndevelopment and safety engineering communities. This comprehensive review\nanalyses OOD detection techniques within the context of safety assurance for\nautonomous systems, in particular in safety-critical domains. We begin by\ndefining the relevant concepts, investigating what causes OOD and exploring the\nfactors which make the safety assurance of autonomous systems and OOD detection\nchallenging. Our review identifies a range of techniques which can be used\nthroughout the ML development lifecycle and we suggest areas within the\nlifecycle in which they may be used to support safety assurance arguments. We\ndiscuss a number of caveats that system and safety engineers must be aware of\nwhen integrating OOD detection into system lifecycles. We conclude by outlining\nthe challenges and future work necessary for the safe development and operation\nof autonomous systems across a range of domains and applications."}
{"id": "2510.21469", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21469", "abs": "https://arxiv.org/abs/2510.21469", "authors": ["Domenico Palmisano", "Giuseppe Palestra", "Berardina Nadja De Carolis"], "title": "Enhancing Social Robots through Resilient AI", "comment": "8 pages, Workshop on Adaptive Social Interaction based on user's\n  Mental mOdels and behaVior in HRI, The 17th International Conference on\n  Social Robotics, 10-12 September 2025, Naples (IT)", "summary": "As artificial intelligence continues to advance and becomes more integrated\ninto sensitive areas like healthcare, education, and everyday life, it's\ncrucial for these systems to be both resilient and robust. This paper shows how\nresilience is a fundamental characteristic of social robots, which, through it,\nensure trust in the robot itself-an essential element especially when operating\nin contexts with elderly people, who often have low trust in these systems.\nResilience is therefore the ability to operate under adverse or stressful\nconditions, even when degraded or weakened, while maintaining essential\noperational capabilities."}
{"id": "2510.21275", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21275", "abs": "https://arxiv.org/abs/2510.21275", "authors": ["Robin Schmöcker", "Christoph Schnell", "Alexander Dockhorn"], "title": "Investigating Scale Independent UCT Exploration Factor Strategies", "comment": null, "summary": "The Upper Confidence Bounds For Trees (UCT) algorithm is not agnostic to the\nreward scale of the game it is applied to. For zero-sum games with the sparse\nrewards of $\\{-1,0,1\\}$ at the end of the game, this is not a problem, but many\ngames often feature dense rewards with hand-picked reward scales, causing a\nnode's Q-value to span different magnitudes across different games. In this\npaper, we evaluate various strategies for adaptively choosing the UCT\nexploration constant $\\lambda$, called $\\lambda$-strategies, that are agnostic\nto the game's reward scale. These $\\lambda$-strategies include those proposed\nin the literature as well as five new strategies. Given our experimental\nresults, we recommend using one of our newly suggested $\\lambda$-strategies,\nwhich is to choose $\\lambda$ as $2 \\cdot \\sigma$ where $\\sigma$ is the\nempirical standard deviation of all state-action pairs' Q-values of the search\ntree. This method outperforms existing $\\lambda$-strategies across a wide range\nof tasks both in terms of a single parameter value and the peak performances\nobtained by optimizing all available parameters."}
{"id": "2510.21571", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21571", "abs": "https://arxiv.org/abs/2510.21571", "authors": ["Qixiu Li", "Yu Deng", "Yaobo Liang", "Lin Luo", "Lei Zhou", "Chengtang Yao", "Lingqi Zeng", "Zhiyuan Feng", "Huizhi Liang", "Sicheng Xu", "Yizhong Zhang", "Xi Chen", "Hao Chen", "Lily Sun", "Dong Chen", "Jiaolong Yang", "Baining Guo"], "title": "Scalable Vision-Language-Action Model Pretraining for Robotic Manipulation with Real-Life Human Activity Videos", "comment": "Project page: https://microsoft.github.io/VITRA/", "summary": "This paper presents a novel approach for pretraining robotic manipulation\nVision-Language-Action (VLA) models using a large corpus of unscripted\nreal-life video recordings of human hand activities. Treating human hand as\ndexterous robot end-effector, we show that \"in-the-wild\" egocentric human\nvideos without any annotations can be transformed into data formats fully\naligned with existing robotic V-L-A training data in terms of task granularity\nand labels. This is achieved by the development of a fully-automated holistic\nhuman activity analysis approach for arbitrary human hand videos. This approach\ncan generate atomic-level hand activity segments and their language\ndescriptions, each accompanied with framewise 3D hand motion and camera motion.\nWe process a large volume of egocentric videos and create a hand-VLA training\ndataset containing 1M episodes and 26M frames. This training data covers a wide\nrange of objects and concepts, dexterous manipulation tasks, and environment\nvariations in real life, vastly exceeding the coverage of existing robot data.\nWe design a dexterous hand VLA model architecture and pretrain the model on\nthis dataset. The model exhibits strong zero-shot capabilities on completely\nunseen real-world observations. Additionally, fine-tuning it on a small amount\nof real robot action data significantly improves task success rates and\ngeneralization to novel objects in real robotic experiments. We also\ndemonstrate the appealing scaling behavior of the model's task performance with\nrespect to pretraining data scale. We believe this work lays a solid foundation\nfor scalable VLA pretraining, advancing robots toward truly generalizable\nembodied intelligence."}
{"id": "2510.21285", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.21285", "abs": "https://arxiv.org/abs/2510.21285", "authors": ["Yingzhi Mao", "Chunkang Zhang", "Junxiang Wang", "Xinyan Guan", "Boxi Cao", "Yaojie Lu", "Hongyu Lin", "Xianpei Han", "Le Sun"], "title": "When Models Outthink Their Safety: Mitigating Self-Jailbreak in Large Reasoning Models with Chain-of-Guardrails", "comment": "First two authors contributed equally. The main text is 10 pages,\n  with an appendix of 19 pages. The paper contains 18 figures and 16 tables", "summary": "Large Reasoning Models (LRMs) demonstrate remarkable capabilities on complex\nreasoning tasks but remain vulnerable to severe safety risks, including harmful\ncontent generation and jailbreak attacks. Existing mitigation strategies rely\non injecting heuristic safety signals during training, which often suppress\nreasoning ability and fail to resolve the safety-reasoning trade-off. To\nsystematically investigate this issue, we analyze the reasoning trajectories of\ndiverse LRMs and uncover a phenomenon we term Self-Jailbreak, where models\noverride their own risk assessments and justify responding to unsafe prompts.\nThis finding reveals that LRMs inherently possess the ability to reject unsafe\nqueries, but this ability is compromised, resulting in harmful outputs.\nBuilding on these insights, we propose the Chain-of-Guardrail (CoG), a training\nframework that recomposes or backtracks unsafe reasoning steps, steering the\nmodel back onto safe trajectories while preserving valid reasoning chains.\nExtensive experiments across multiple reasoning and safety benchmarks\ndemonstrate that CoG substantially improves the safety of current LRMs while\npreserving comparable reasoning ability, significantly outperforming prior\nmethods that suffer from severe safety-reasoning trade-offs."}
{"id": "2510.21293", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.21293", "abs": "https://arxiv.org/abs/2510.21293", "authors": ["Siddharth Mehrotra", "Jin Huang", "Xuelong Fu", "Roel Dobbe", "Clara I. Sánchez", "Maarten de Rijke"], "title": "Understanding AI Trustworthiness: A Scoping Review of AIES & FAccT Articles", "comment": "Submitted to Journal of Artificial Intelligence Research (JAIR)", "summary": "Background: Trustworthy AI serves as a foundational pillar for two major AI\nethics conferences: AIES and FAccT. However, current research often adopts\ntechno-centric approaches, focusing primarily on technical attributes such as\nreliability, robustness, and fairness, while overlooking the sociotechnical\ndimensions critical to understanding AI trustworthiness in real-world contexts.\n  Objectives: This scoping review aims to examine how the AIES and FAccT\ncommunities conceptualize, measure, and validate AI trustworthiness,\nidentifying major gaps and opportunities for advancing a holistic understanding\nof trustworthy AI systems.\n  Methods: We conduct a scoping review of AIES and FAccT conference proceedings\nto date, systematically analyzing how trustworthiness is defined,\noperationalized, and applied across different research domains. Our analysis\nfocuses on conceptualization approaches, measurement methods, verification and\nvalidation techniques, application areas, and underlying values.\n  Results: While significant progress has been made in defining technical\nattributes such as transparency, accountability, and robustness, our findings\nreveal critical gaps. Current research often predominantly emphasizes technical\nprecision at the expense of social and ethical considerations. The\nsociotechnical nature of AI systems remains less explored and trustworthiness\nemerges as a contested concept shaped by those with the power to define it.\n  Conclusions: An interdisciplinary approach combining technical rigor with\nsocial, cultural, and institutional considerations is essential for advancing\ntrustworthy AI. We propose actionable measures for the AI ethics community to\nadopt holistic frameworks that genuinely address the complex interplay between\nAI systems and society, ultimately promoting responsible technological\ndevelopment that benefits all stakeholders."}
{"id": "2510.21294", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.21294", "abs": "https://arxiv.org/abs/2510.21294", "authors": ["Maxime Grosso", "Pierre Riedinger", "Jamal Daafouz"], "title": "The PhasorArray Toolbox for Harmonic Analysis and Control Design", "comment": null, "summary": "We present a MATLAB package called the Pha-sorArray Toolbox that has been\ndeveloped to make harmonic analysis and control methods both practical and\nuser-friendly. The toolbox adopts an object-oriented architecture that enables\nintuitive manipulation of periodic matrices through overloaded operators for\naddition, multiplication, convolution, and automatic Toeplitz construction. Its\nadvanced features include harmonic Sylvester, Lyapunov and Riccati equations\nsolvers, and seamless integration with YALMIP, thereby facilitating advanced\ncontrol and analysis techniques based on Linear Matrix Inequalities (LMIs) in\nthe harmonic framework."}
{"id": "2510.21302", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21302", "abs": "https://arxiv.org/abs/2510.21302", "authors": ["Sanghyun Ahn", "Wonje Choi", "Junyong Lee", "Jinwoo Park", "Honguk Woo"], "title": "Towards Reliable Code-as-Policies: A Neuro-Symbolic Framework for Embodied Task Planning", "comment": "Accepted at NeurIPS 2025 Spotlight", "summary": "Recent advances in large language models (LLMs) have enabled the automatic\ngeneration of executable code for task planning and control in embodied agents\nsuch as robots, demonstrating the potential of LLM-based embodied intelligence.\nHowever, these LLM-based code-as-policies approaches often suffer from limited\nenvironmental grounding, particularly in dynamic or partially observable\nsettings, leading to suboptimal task success rates due to incorrect or\nincomplete code generation. In this work, we propose a neuro-symbolic embodied\ntask planning framework that incorporates explicit symbolic verification and\ninteractive validation processes during code generation. In the validation\nphase, the framework generates exploratory code that actively interacts with\nthe environment to acquire missing observations while preserving task-relevant\nstates. This integrated process enhances the grounding of generated code,\nresulting in improved task reliability and success rates in complex\nenvironments. We evaluate our framework on RLBench and in real-world settings\nacross dynamic, partially observable scenarios. Experimental results\ndemonstrate that our framework improves task success rates by 46.2% over\nCode-as-Policies baselines and attains over 86.8% executability of\ntask-relevant actions, thereby enhancing the reliability of task planning in\ndynamic environments."}
{"id": "2510.21308", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.21308", "abs": "https://arxiv.org/abs/2510.21308", "authors": ["Zhengang Zhong", "Ehecatl Antonio del Rio-Chanona", "Panagiotis Petsagkourakis"], "title": "Data-driven Koopman MPC using Mixed Stochastic-Deterministic Tubes", "comment": "This is the accepted version. It will appear in Journal of Process\n  Control, 2025", "summary": "This paper presents a novel data-driven stochastic MPC design for\ndiscrete-time nonlinear systems with additive disturbances by leveraging the\nKoopman operator and a distributionally robust optimization (DRO) framework. By\nlifting the dynamical system into a linear space, we achieve a\nfinite-dimensional approximation of the Koopman operator. We explicitly account\nfor the modeling approximation and additive disturbance error by a mixed\nstochastic-deterministic tube for the lifted linear model. This ensures the\nregulation of the original nonlinear system while complying with the\nprespecified constraints. Stochastic and deterministic tubes are constructed\nusing a DRO and a hyper-cube hull, respectively. We provide finite sample error\nbounds for both types of tubes. The effectiveness of the proposed approach is\ndemonstrated through numerical simulations."}
{"id": "2510.21321", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.21321", "abs": "https://arxiv.org/abs/2510.21321", "authors": ["Kanghui He", "Anil Alan", "Shengling Shi", "Ton van den Boom", "Bart De Schutter"], "title": "Predictive control barrier functions for piecewise affine systems with non-smooth constraints", "comment": null, "summary": "Obtaining control barrier functions (CBFs) with large safe sets for complex\nnonlinear systems and constraints is a challenging task. Predictive CBFs\naddress this issue by using an online finite-horizon optimal control problem\nthat implicitly defines a large safe set. The optimal control problem, also\nknown as the predictive safety filter (PSF), involves predicting the system's\nflow under a given backup control policy. However, for non-smooth systems and\nconstraints, some key elements, such as CBF gradients and the sensitivity of\nthe flow, are not well-defined, making the current methods inadequate for\nensuring safety. Additionally, for control-non-affine systems, the PSF is\ngenerally nonlinear and non-convex, posing challenges for real-time\ncomputation. This paper considers piecewise affine systems, which are usually\ncontrol-non-affine, under nonlinear state and polyhedral input constraints. We\nsolve the safety issue by incorporating set-valued generalized Clarke\nderivatives in the PSF design. We show that enforcing CBF constraints across\nall elements of the generalized Clarke derivatives suffices to guarantee\nsafety. Moreover, to lighten the computational overhead, we propose an explicit\napproximation of the PSF. The resulting control methods are demonstrated\nthrough numerical examples."}
{"id": "2510.21324", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.21324", "abs": "https://arxiv.org/abs/2510.21324", "authors": ["Jinhui Lou", "Yan Yang", "Zhou Yu", "Zhenqi Fu", "Weidong Han", "Qingming Huang", "Jun Yu"], "title": "CXRAgent: Director-Orchestrated Multi-Stage Reasoning for Chest X-Ray Interpretation", "comment": "10 pages, 4 figures, 7 Tables", "summary": "Chest X-ray (CXR) plays a pivotal role in clinical diagnosis, and a variety\nof task-specific and foundation models have been developed for automatic CXR\ninterpretation. However, these models often struggle to adapt to new diagnostic\ntasks and complex reasoning scenarios. Recently, LLM-based agent models have\nemerged as a promising paradigm for CXR analysis, enhancing model's capability\nthrough tool coordination, multi-step reasoning, and team collaboration, etc.\nHowever, existing agents often rely on a single diagnostic pipeline and lack\nmechanisms for assessing tools' reliability, limiting their adaptability and\ncredibility. To this end, we propose CXRAgent, a director-orchestrated,\nmulti-stage agent for CXR interpretation, where a central director coordinates\nthe following stages: (1) Tool Invocation: The agent strategically orchestrates\na set of CXR-analysis tools, with outputs normalized and verified by the\nEvidence-driven Validator (EDV), which grounds diagnostic outputs with visual\nevidence to support reliable downstream diagnosis; (2) Diagnostic Planning:\nGuided by task requirements and intermediate findings, the agent formulates a\ntargeted diagnostic plan. It then assembles an expert team accordingly,\ndefining member roles and coordinating their interactions to enable adaptive\nand collaborative reasoning; (3) Collaborative Decision-making: The agent\nintegrates insights from the expert team with accumulated contextual memories,\nsynthesizing them into an evidence-backed diagnostic conclusion. Experiments on\nvarious CXR interpretation tasks show that CXRAgent delivers strong\nperformance, providing visual evidence and generalizes well to clinical tasks\nof different complexity. Code and data are valuable at this\n\\href{https://github.com/laojiahuo2003/CXRAgent/}{link}."}
{"id": "2510.21341", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.21341", "abs": "https://arxiv.org/abs/2510.21341", "authors": ["Lufan Chang"], "title": "Magellan: Guided MCTS for Latent Space Exploration and Novelty Generation", "comment": "Accepted to 1st Open Conference on AI Agents for Science\n  (agents4science 2025)", "summary": "Large Language Models (LLMs) often struggle with generating truly innovative\nideas, typically defaulting to high-probability, familiar concepts within their\ntraining data's \"gravity wells.\" While advanced search-based methods like Tree\nof Thoughts (ToT) attempt to mitigate this, they are fundamentally limited by\ntheir reliance on unprincipled, inconsistent self-evaluation heuristics to\nguide exploration. To address this gap, we introduce \\textbf{Magellan}, a novel\nframework that reframes creative generation as a principled, guided exploration\nof an LLM's latent conceptual space. At its core, Magellan employs Monte Carlo\nTree Search (MCTS) governed by a hierarchical guidance system. For long-range\ndirection, a \"semantic compass\" vector, formulated via orthogonal projection,\nsteers the search towards relevant novelty. For local, step-by-step decisions,\na landscape-aware value function replaces flawed self-evaluation with an\nexplicit reward structure that balances intrinsic coherence, extrinsic novelty,\nand narrative progress. Extensive experiments demonstrate that Magellan\nsignificantly outperforms strong baselines, including ReAct and ToT, in\ngenerating scientific ideas with superior plausibility and innovation. Our work\nshows that for creative discovery, a principled, guided search is more\neffective than unconstrained agency, paving the way for LLMs to become more\ncapable partners in innovation."}
{"id": "2510.21357", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21357", "abs": "https://arxiv.org/abs/2510.21357", "authors": ["Daniel Schleich", "Jan Quenzel", "Sven Behnke"], "title": "Remote Autonomy for Multiple Small Lowcost UAVs in GNSS-denied Search and Rescue Operations", "comment": "Accepted final version. IEEE International Symposium on Safety,\n  Security, and Rescue Robotics (SSRR), Galway, Ireland, 2025", "summary": "In recent years, consumer-grade UAVs have been widely adopted by first\nresponders. In general, they are operated manually, which requires trained\npilots, especially in unknown GNSS-denied environments and in the vicinity of\nstructures. Autonomous flight can facilitate the application of UAVs and reduce\noperator strain. However, autonomous systems usually require special\nprogramming interfaces, custom sensor setups, and strong onboard computers,\nwhich limits a broader deployment.\n  We present a system for autonomous flight using lightweight consumer-grade\nDJI drones. They are controlled by an Android app for state estimation and\nobstacle avoidance directly running on the UAV's remote control. Our ground\ncontrol station enables a single operator to configure and supervise multiple\nheterogeneous UAVs at once. Furthermore, it combines the observations of all\nUAVs into a joint 3D environment model for improved situational awareness."}
{"id": "2510.21369", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21369", "abs": "https://arxiv.org/abs/2510.21369", "authors": ["Vivian S. Medeiros", "Giovanni B. Dessy", "Thiago Boaventura", "Marcelo Becker", "Claudio Semini", "Victor Barasuol"], "title": "Load-bearing Assessment for Safe Locomotion of Quadruped Robots on Collapsing Terrain", "comment": null, "summary": "Collapsing terrains, often present in search and rescue missions or planetary\nexploration, pose significant challenges for quadruped robots. This paper\nintroduces a robust locomotion framework for safe navigation over unstable\nsurfaces by integrating terrain probing, load-bearing analysis, motion\nplanning, and control strategies. Unlike traditional methods that rely on\nspecialized sensors or external terrain mapping alone, our approach leverages\njoint measurements to assess terrain stability without hardware modifications.\nA Model Predictive Control (MPC) system optimizes robot motion, balancing\nstability and probing constraints, while a state machine coordinates terrain\nprobing actions, enabling the robot to detect collapsible regions and\ndynamically adjust its footholds. Experimental results on custom-made\ncollapsing platforms and rocky terrains demonstrate the framework's ability to\ntraverse collapsing terrain while maintaining stability and prioritizing\nsafety."}
{"id": "2510.21398", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21398", "abs": "https://arxiv.org/abs/2510.21398", "authors": ["Ravindra Aribowo Tarunokusumo", "Rafael Fernandes Cunha"], "title": "Boosting Accuracy and Efficiency of Budget Forcing in LLMs via Reinforcement Learning for Mathematical Reasoning", "comment": "Submitted to the European Conference on Artificial Intelligence\n  (ECAI)", "summary": "Test-time scaling methods have seen a rapid increase in popularity for its\ncomputational efficiency and parameter-independent training to improve\nreasoning performance on Large Language Models. One such method is called\nbudget forcing, a decoding intervention strategy which allocates extra compute\nbudget for thinking and elicits the inherent self-correcting behavior of the\nmodel. However, this relies on supervised fine-tuning (SFT) on long-context\nreasoning traces which causes performance degradation on smaller models due to\nverbose responses. For this reason, we offer a framework integrating\nreinforcement learning (RL) to improve token efficiency and boost the\nperformance of a 1.5B model for mathematical reasoning. We demonstrate this\nusing only 1.5K training samples and found that our SFT+RL model performed\nbetter on the GSM8K dataset with varying compute budgets. Our main findings\nshowed an overall higher accuracy while significantly reducing its token usage\nby over 40% compared to the SFT model, revealing how RL can recover the losses\ndue to long-context training and altogether improving performance in\nmathematical reasoning."}
{"id": "2510.21425", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21425", "abs": "https://arxiv.org/abs/2510.21425", "authors": ["Maneeha Rani", "Bhupesh Kumar Mishra", "Dhavalkumar Thakker"], "title": "Advancing Symbolic Integration in Large Language Models: Beyond Conventional Neurosymbolic AI", "comment": null, "summary": "LLMs have demonstrated highly effective learning, human-like response\ngeneration,and decision-making capabilities in high-risk sectors. However,\nthese models remain black boxes because they struggle to ensure transparency in\nresponses. The literature has explored numerous approaches to address\ntransparency challenges in LLMs, including Neurosymbolic AI (NeSy AI). NeSy AI\napproaches were primarily developed for conventional neural networks and are\nnot well-suited to the unique features of LLMs. Consequently, there is a\nlimited systematic understanding of how symbolic AI can be effectively\nintegrated into LLMs. This paper aims to address this gap by first reviewing\nestablished NeSy AI methods and then proposing a novel taxonomy of symbolic\nintegration in LLMs, along with a roadmap to merge symbolic techniques with\nLLMs. The roadmap introduces a new categorisation framework across four\ndimensions by organising existing literature within these categories. These\ninclude symbolic integration across various stages of LLM, coupling mechanisms,\narchitectural paradigms, as well as algorithmic and application-level\nperspectives. The paper thoroughly identifies current benchmarks, cutting-edge\nadvancements, and critical gaps within the field to propose a roadmap for\nfuture research. By highlighting the latest developments and notable gaps in\nthe literature, it offers practical insights for implementing frameworks for\nsymbolic integration into LLMs to enhance transparency."}
{"id": "2510.21436", "categories": ["cs.AI", "I.4"], "pdf": "https://arxiv.org/pdf/2510.21436", "abs": "https://arxiv.org/abs/2510.21436", "authors": ["Ankur Sinha", "Shobhit Arora", "Dhaval Pujara"], "title": "AutoOpt: A Dataset and a Unified Framework for Automating Optimization Problem Solving", "comment": "NeurIPS 2025, 28 pages, 11 figures, 11 tables", "summary": "This study presents AutoOpt-11k, a unique image dataset of over 11,000\nhandwritten and printed mathematical optimization models corresponding to\nsingle-objective, multi-objective, multi-level, and stochastic optimization\nproblems exhibiting various types of complexities such as non-linearity,\nnon-convexity, non-differentiability, discontinuity, and high-dimensionality.\nThe labels consist of the LaTeX representation for all the images and modeling\nlanguage representation for a subset of images. The dataset is created by 25\nexperts following ethical data creation guidelines and verified in two-phases\nto avoid errors. Further, we develop AutoOpt framework, a machine learning\nbased automated approach for solving optimization problems, where the user just\nneeds to provide an image of the formulation and AutoOpt solves it efficiently\nwithout any further human intervention. AutoOpt framework consists of three\nModules: (i) M1 (Image_to_Text)- a deep learning model performs the\nMathematical Expression Recognition (MER) task to generate the LaTeX code\ncorresponding to the optimization formulation in image; (ii) M2 (Text_to_Text)-\na small-scale fine-tuned LLM generates the PYOMO script (optimization modeling\nlanguage) from LaTeX code; (iii) M3 (Optimization)- a Bilevel Optimization\nbased Decomposition (BOBD) method solves the optimization formulation described\nin the PYOMO script. We use AutoOpt-11k dataset for training and testing of\ndeep learning models employed in AutoOpt. The deep learning model for MER task\n(M1) outperforms ChatGPT, Gemini and Nougat on BLEU score metric. BOBD method\n(M3), which is a hybrid approach, yields better results on complex test\nproblems compared to common approaches, like interior-point algorithm and\ngenetic algorithm."}
{"id": "2510.21438", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21438", "abs": "https://arxiv.org/abs/2510.21438", "authors": ["Satheeshkumar Veeramani", "Zhengxue Zhou", "Francisco Munguia-Galeano", "Hatem Fakhruldeen", "Thomas Roddelkopf", "Mohammed Faeik Ruzaij Al-Okby", "Kerstin Thurow", "Andrew Ian Cooper"], "title": "PREVENT: Proactive Risk Evaluation and Vigilant Execution of Tasks for Mobile Robotic Chemists using Multi-Modal Behavior Trees", "comment": "25 pages, 8 figures, paper submitted to Robotics and Autonomous\n  Systems Journal", "summary": "Mobile robotic chemists are a fast growing trend in the field of chemistry\nand materials research. However, so far these mobile robots lack workflow\nawareness skills. This poses the risk that even a small anomaly, such as an\nimproperly capped sample vial could disrupt the entire workflow. This wastes\ntime, and resources, and could pose risks to human researchers, such as\nexposure to toxic materials. Existing perception mechanisms can be used to\npredict anomalies but they often generate excessive false positives. This may\nhalt workflow execution unnecessarily, requiring researchers to intervene and\nto resume the workflow when no problem actually exists, negating the benefits\nof autonomous operation. To address this problem, we propose PREVENT a system\ncomprising navigation and manipulation skills based on a multimodal Behavior\nTree (BT) approach that can be integrated into existing software architectures\nwith minimal modifications. Our approach involves a hierarchical perception\nmechanism that exploits AI techniques and sensory feedback through Dexterous\nVision and Navigational Vision cameras and an IoT gas sensor module for\nexecution-related decision-making. Experimental evaluations show that the\nproposed approach is comparatively efficient and completely avoids both false\nnegatives and false positives when tested in simulated risk scenarios within\nour robotic chemistry workflow. The results also show that the proposed\nmulti-modal perception skills achieved deployment accuracies that were higher\nthan the average of the corresponding uni-modal skills, both for navigation and\nfor manipulation."}
{"id": "2510.21453", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21453", "abs": "https://arxiv.org/abs/2510.21453", "authors": ["Yuxin Pan", "Zhiguang Cao", "Chengyang Gu", "Liu Liu", "Peilin Zhao", "Yize Chen", "Fangzhen Lin"], "title": "Multi-Task Vehicle Routing Solver via Mixture of Specialized Experts under State-Decomposable MDP", "comment": "Accepted to NeurIPS 2025", "summary": "Existing neural methods for multi-task vehicle routing problems (VRPs)\ntypically learn unified solvers to handle multiple constraints simultaneously.\nHowever, they often underutilize the compositional structure of VRP variants,\neach derivable from a common set of basis VRP variants. This critical oversight\ncauses unified solvers to miss out the potential benefits of basis solvers,\neach specialized for a basis VRP variant. To overcome this limitation, we\npropose a framework that enables unified solvers to perceive the\nshared-component nature across VRP variants by proactively reusing basis\nsolvers, while mitigating the exponential growth of trained neural solvers.\nSpecifically, we introduce a State-Decomposable MDP (SDMDP) that reformulates\nVRPs by expressing the state space as the Cartesian product of basis state\nspaces associated with basis VRP variants. More crucially, this formulation\ninherently yields the optimal basis policy for each basis VRP variant.\nFurthermore, a Latent Space-based SDMDP extension is developed by incorporating\nboth the optimal basis policies and a learnable mixture function to enable the\npolicy reuse in the latent space. Under mild assumptions, this extension\nprovably recovers the optimal unified policy of SDMDP through the mixture\nfunction that computes the state embedding as a mapping from the basis state\nembeddings generated by optimal basis policies. For practical implementation,\nwe introduce the Mixture-of-Specialized-Experts Solver (MoSES), which realizes\nbasis policies through specialized Low-Rank Adaptation (LoRA) experts, and\nimplements the mixture function via an adaptive gating mechanism. Extensive\nexperiments conducted across VRP variants showcase the superiority of MoSES\nover prior methods."}
{"id": "2510.21469", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21469", "abs": "https://arxiv.org/abs/2510.21469", "authors": ["Domenico Palmisano", "Giuseppe Palestra", "Berardina Nadja De Carolis"], "title": "Enhancing Social Robots through Resilient AI", "comment": "8 pages, Workshop on Adaptive Social Interaction based on user's\n  Mental mOdels and behaVior in HRI, The 17th International Conference on\n  Social Robotics, 10-12 September 2025, Naples (IT)", "summary": "As artificial intelligence continues to advance and becomes more integrated\ninto sensitive areas like healthcare, education, and everyday life, it's\ncrucial for these systems to be both resilient and robust. This paper shows how\nresilience is a fundamental characteristic of social robots, which, through it,\nensure trust in the robot itself-an essential element especially when operating\nin contexts with elderly people, who often have low trust in these systems.\nResilience is therefore the ability to operate under adverse or stressful\nconditions, even when degraded or weakened, while maintaining essential\noperational capabilities."}
{"id": "2510.21524", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21524", "abs": "https://arxiv.org/abs/2510.21524", "authors": ["Ilija Lichkovski", "Alexander Müller", "Mariam Ibrahim", "Tiwai Mhundwa"], "title": "EU-Agent-Bench: Measuring Illegal Behavior of LLM Agents Under EU Law", "comment": "Accepted at the Workshop on Regulatable ML at the 39th Conference on\n  Neural Information Processing Systems (NeurIPS 2025)", "summary": "Large language models (LLMs) are increasingly deployed as agents in various\ncontexts by providing tools at their disposal. However, LLM agents can exhibit\nunpredictable behaviors, including taking undesirable and/or unsafe actions. In\norder to measure the latent propensity of LLM agents for taking illegal actions\nunder an EU legislative context, we introduce EU-Agent-Bench, a verifiable\nhuman-curated benchmark that evaluates an agent's alignment with EU legal norms\nin situations where benign user inputs could lead to unlawful actions. Our\nbenchmark spans scenarios across several categories, including data protection,\nbias/discrimination, and scientific integrity, with each user request allowing\nfor both compliant and non-compliant execution of the requested actions.\nComparing the model's function calls against a rubric exhaustively supported by\ncitations of the relevant legislature, we evaluate the legal compliance of\nfrontier LLMs, and furthermore investigate the compliance effect of providing\nthe relevant legislative excerpts in the agent's system prompt along with\nexplicit instructions to comply. We release a public preview set for the\nresearch community, while holding out a private test set to prevent data\ncontamination in evaluating upcoming models. We encourage future work extending\nagentic safety benchmarks to different legal jurisdictions and to multi-turn\nand multilingual interactions. We release our code on\n\\href{https://github.com/ilijalichkovski/eu-agent-bench}{this URL}."}
{"id": "2510.21526", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2510.21526", "abs": "https://arxiv.org/abs/2510.21526", "authors": ["Isaac Johnson", "Yu-Ming Liou", "Jacob Rogers", "Aaron Shaw", "Leila Zia"], "title": "Recommended Practices for NPOV Research on Wikipedia", "comment": null, "summary": "Writing Wikipedia with a neutral point of view is one of the five pillars of\nWikipedia. Although the topic is core to Wikipedia, it is relatively\nunderstudied considering hundreds of research studies are published annually\nabout the project. We hypothesize that part of the reason for the low research\nactivity on the topic is that Wikipedia's definition of neutrality and its\nimportance are not well understood within the research community. Neutrality is\nalso an inherently challenging and contested concept. Our aim with this paper\nis to accelerate high quality research in this space that can help Wikipedia\ncommunities continue to improve their work in writing the encyclopedia. We do\nthis by helping researchers to learn what Neutral Point of View means in the\ncontext of Wikipedia, identifying some common challenges with studying NPOV and\nhow to navigate them, and offering guidance on how researchers can communicate\nthe results of their work for increased impact on the ground for the benefit of\nWikipedia."}
{"id": "2510.21536", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.21536", "abs": "https://arxiv.org/abs/2510.21536", "authors": ["Narendhiran Vijayakumar", "Sridevi. M"], "title": "AURASeg: Attention Guided Upsampling with Residual Boundary-Assistive Refinement for Drivable-Area Segmentation", "comment": "10 pages, 5 figures, 4 tables", "summary": "Free space ground segmentation is essential to navigate robots and autonomous\nvehicles, recognize drivable zones, and traverse efficiently. Fine-grained\nfeatures remain challenging for existing segmentation models, particularly for\nrobots in indoor and structured environments. These difficulties arise from\nineffective multi-scale processing, suboptimal boundary refinement, and limited\nfeature representation. In order to overcome these limitations, we propose\nAttention-Guided Upsampling with Residual Boundary-Assistive Refinement\n(AURASeg), a ground-plane semantic segmentation model that maintains high\nsegmentation accuracy while improving border precision. Our method uses\nCSP-Darknet backbone by adding a Residual Border Refinement Module (RBRM) for\naccurate edge delineation and an Attention Progressive Upsampling Decoder\n(APUD) for strong feature integration. We also incorporate a lightweight Atrous\nSpatial Pyramid Pooling (ASPP-Lite) module to ensure multi-scale context\nextraction without compromising real-time performance. The proposed model beats\nbenchmark segmentation architectures in mIoU and F1 metrics when tested on the\nGround Mobile Robot Perception (GMRP) Dataset and a custom Gazebo indoor\ndataset. Our approach achieves an improvement in mean Intersection-over-Union\n(mIoU) of +1.26% and segmentation precision of +1.65% compared to\nstate-of-the-art models. These results show that our technique is feasible for\nautonomous perception in both indoor and outdoor environments, enabling precise\nborder refinement with minimal effect on inference speed."}
{"id": "2510.21546", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.21546", "abs": "https://arxiv.org/abs/2510.21546", "authors": ["Johannes Autenrieb", "Mark Spiller"], "title": "Auction-Based Responsibility Allocation for Scalable Decentralized Safety Filters in Cooperative Multi-Agent Collision Avoidance", "comment": "6 pages, 3 figures, Submitted to Control Engineering Practice and\n  IFAC World Congress 2026", "summary": "This paper proposes a scalable decentralized safety filter for multi-agent\nsystems based on high-order control barrier functions (HOCBFs) and\nauction-based responsibility allocation. While decentralized HOCBF formulations\nensure pairwise safety under input bounds, they face feasibility and\nscalability challenges as the number of agents grows. Each agent must evaluate\nan increasing number of pairwise constraints, raising the risk of infeasibility\nand making it difficult to meet real-time requirements. To address this, we\nintroduce an auction-based allocation scheme that distributes constraint\nenforcement asymmetrically among neighbors based on local control effort\nestimates. The resulting directed responsibility graph guarantees full safety\ncoverage while reducing redundant constraints and per-agent computational load.\nSimulation results confirm safe and efficient coordination across a range of\nnetwork sizes and interaction densities."}
{"id": "2510.21556", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.21556", "abs": "https://arxiv.org/abs/2510.21556", "authors": ["Sophie Hall", "Florian Dörfler", "Timm Faulwasser"], "title": "System-Theoretic Analysis of Dynamic Generalized Nash Equilibrium Problems -- Turnpikes and Dissipativity", "comment": null, "summary": "Generalized Nash equilibria are used in multi-agent control applications to\nmodel strategic interactions between agents that are coupled in the cost,\ndynamics, and constraints. We study the properties of open-loop GNE\ntrajectories from a system-theoretic perspective. We show how strict\ndissipativity generates the turnpike phenomenon in GNE solutions. Moreover, we\nestablish a converse turnpike result, i.e., the implication from turnpike to\nstrict dissipativity. We derive conditions under which the steady-state GNE is\nthe optimal operating point and, using a game value function, we give a local\ncharacterization of the geometry of storage functions. Finally, we design\nlinear terminal penalties that ensure GNE open-loop trajectories converge to\nand remain at the steady-state GNE. These connections provide the foundation\nfor future system-theoretic analysis of GNEs similar to those existing in\noptimal control."}
{"id": "2510.21557", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21557", "abs": "https://arxiv.org/abs/2510.21557", "authors": ["Hongwei Zhang", "Ji Lu", "Shiqing Jiang", "Chenxiang Zhu", "Li Xie", "Chen Zhong", "Haoran Chen", "Yurui Zhu", "Yongsheng Du", "Yanqin Gao", "Lingjun Huang", "Baoli Wang", "Fang Tan", "Peng Zou"], "title": "Co-Sight: Enhancing LLM-Based Agents via Conflict-Aware Meta-Verification and Trustworthy Reasoning with Structured Facts", "comment": null, "summary": "Long-horizon reasoning in LLM-based agents often fails not from generative\nweakness but from insufficient verification of intermediate reasoning. Co-Sight\naddresses this challenge by turning reasoning into a falsifiable and auditable\nprocess through two complementary mechanisms: Conflict-Aware Meta-Verification\n(CAMV) and Trustworthy Reasoning with Structured Facts (TRSF). CAMV\nreformulates verification as conflict identification and targeted\nfalsification, allocating computation only to disagreement hotspots among\nexpert agents rather than to full reasoning chains. This bounds verification\ncost to the number of inconsistencies and improves efficiency and reliability.\nTRSF continuously organizes, validates, and synchronizes evidence across agents\nthrough a structured facts module. By maintaining verified, traceable, and\nauditable knowledge, it ensures that all reasoning is grounded in consistent,\nsource-verified information and supports transparent verification throughout\nthe reasoning process. Together, TRSF and CAMV form a closed verification loop,\nwhere TRSF supplies structured facts and CAMV selectively falsifies or\nreinforces them, yielding transparent and trustworthy reasoning. Empirically,\nCo-Sight achieves state-of-the-art accuracy on GAIA (84.4%) and Humanity's Last\nExam (35.5%), and strong results on Chinese-SimpleQA (93.8%). Ablation studies\nconfirm that the synergy between structured factual grounding and\nconflict-aware verification drives these improvements. Co-Sight thus offers a\nscalable paradigm for reliable long-horizon reasoning in LLM-based agents. Code\nis available at\nhttps://github.com/ZTE-AICloud/Co-Sight/tree/cosight2.0_benchmarks."}
{"id": "2510.21560", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21560", "abs": "https://arxiv.org/abs/2510.21560", "authors": ["Yuxuan Yang", "Hussein Sibai"], "title": "Learning Neural Control Barrier Functions from Expert Demonstrations using Inverse Constraint Learning", "comment": null, "summary": "Safety is a fundamental requirement for autonomous systems operating in\ncritical domains. Control barrier functions (CBFs) have been used to design\nsafety filters that minimally alter nominal controls for such systems to\nmaintain their safety. Learning neural CBFs has been proposed as a data-driven\nalternative for their computationally expensive optimization-based synthesis.\nHowever, it is often the case that the failure set of states that should be\navoided is non-obvious or hard to specify formally, e.g., tailgating in\nautonomous driving, while a set of expert demonstrations that achieve the task\nand avoid the failure set is easier to generate. We use ICL to train a\nconstraint function that classifies the states of the system under\nconsideration to safe, i.e., belong to a controlled forward invariant set that\nis disjoint from the unspecified failure set, and unsafe ones, i.e., belong to\nthe complement of that set. We then use that function to label a new set of\nsimulated trajectories to train our neural CBF. We empirically evaluate our\napproach in four different environments, demonstrating that it outperforms\nexisting baselines and achieves comparable performance to a neural CBF trained\nwith the same data but annotated with ground-truth safety labels."}
{"id": "2510.21571", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21571", "abs": "https://arxiv.org/abs/2510.21571", "authors": ["Qixiu Li", "Yu Deng", "Yaobo Liang", "Lin Luo", "Lei Zhou", "Chengtang Yao", "Lingqi Zeng", "Zhiyuan Feng", "Huizhi Liang", "Sicheng Xu", "Yizhong Zhang", "Xi Chen", "Hao Chen", "Lily Sun", "Dong Chen", "Jiaolong Yang", "Baining Guo"], "title": "Scalable Vision-Language-Action Model Pretraining for Robotic Manipulation with Real-Life Human Activity Videos", "comment": "Project page: https://microsoft.github.io/VITRA/", "summary": "This paper presents a novel approach for pretraining robotic manipulation\nVision-Language-Action (VLA) models using a large corpus of unscripted\nreal-life video recordings of human hand activities. Treating human hand as\ndexterous robot end-effector, we show that \"in-the-wild\" egocentric human\nvideos without any annotations can be transformed into data formats fully\naligned with existing robotic V-L-A training data in terms of task granularity\nand labels. This is achieved by the development of a fully-automated holistic\nhuman activity analysis approach for arbitrary human hand videos. This approach\ncan generate atomic-level hand activity segments and their language\ndescriptions, each accompanied with framewise 3D hand motion and camera motion.\nWe process a large volume of egocentric videos and create a hand-VLA training\ndataset containing 1M episodes and 26M frames. This training data covers a wide\nrange of objects and concepts, dexterous manipulation tasks, and environment\nvariations in real life, vastly exceeding the coverage of existing robot data.\nWe design a dexterous hand VLA model architecture and pretrain the model on\nthis dataset. The model exhibits strong zero-shot capabilities on completely\nunseen real-world observations. Additionally, fine-tuning it on a small amount\nof real robot action data significantly improves task success rates and\ngeneralization to novel objects in real robotic experiments. We also\ndemonstrate the appealing scaling behavior of the model's task performance with\nrespect to pretraining data scale. We believe this work lays a solid foundation\nfor scalable VLA pretraining, advancing robots toward truly generalizable\nembodied intelligence."}
{"id": "2510.21609", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21609", "abs": "https://arxiv.org/abs/2510.21609", "authors": ["Elle Miller", "Trevor McInroe", "David Abel", "Oisin Mac Aodha", "Sethu Vijayakumar"], "title": "Enhancing Tactile-based Reinforcement Learning for Robotic Control", "comment": null, "summary": "Achieving safe, reliable real-world robotic manipulation requires agents to\nevolve beyond vision and incorporate tactile sensing to overcome sensory\ndeficits and reliance on idealised state information. Despite its potential,\nthe efficacy of tactile sensing in reinforcement learning (RL) remains\ninconsistent. We address this by developing self-supervised learning (SSL)\nmethodologies to more effectively harness tactile observations, focusing on a\nscalable setup of proprioception and sparse binary contacts. We empirically\ndemonstrate that sparse binary tactile signals are critical for dexterity,\nparticularly for interactions that proprioceptive control errors do not\nregister, such as decoupled robot-object motions. Our agents achieve superhuman\ndexterity in complex contact tasks (ball bouncing and Baoding ball rotation).\nFurthermore, we find that decoupling the SSL memory from the on-policy memory\ncan improve performance. We release the Robot Tactile Olympiad (RoTO) benchmark\nto standardise and promote future research in tactile-based manipulation.\nProject page: https://elle-miller.github.io/tactile_rl"}
{"id": "2510.21612", "categories": ["eess.SY", "cs.IT", "cs.SY", "math.IT", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.21612", "abs": "https://arxiv.org/abs/2510.21612", "authors": ["Yorie Nakahira", "Fangzhou Xiao", "Victoria Kostina", "John C. Doyle"], "title": "Rate-cost tradeoffs in continuous-time control with a biomolecular application", "comment": null, "summary": "This paper focuses on rate-limited control of the generalized\nOrnstein-Uhlenbeck process where the control action can be either\nmultiplicative or additive, and the noise variance can depend on the control\naction. We derive a lower bound on the data rate necessary to achieve the\ndesired control cost. The lower bound is attained with equality if the control\nis performed via an additive white Gaussian channel. The system model\napproximates the dynamics of a discrete-state molecular birth-death process,\nand the result has direct implications on the control of a biomolecular system\nvia chemical reactions, where the multiplicative control corresponds to the\ndegradation rate, the additive control corresponds to the production rate, and\nthe control objective is to decrease the fluctuations of the controlled\nmolecular species around their desired concentration levels."}
{"id": "2510.21614", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21614", "abs": "https://arxiv.org/abs/2510.21614", "authors": ["Wenyi Wang", "Piotr Piękos", "Li Nanbo", "Firas Laakom", "Yimeng Chen", "Mateusz Ostaszewski", "Mingchen Zhuge", "Jürgen Schmidhuber"], "title": "Huxley-Gödel Machine: Human-Level Coding Agent Development by an Approximation of the Optimal Self-Improving Machine", "comment": null, "summary": "Recent studies operationalize self-improvement through coding agents that\nedit their own codebases. They grow a tree of self-modifications through\nexpansion strategies that favor higher software engineering benchmark\nperformance, assuming that this implies more promising subsequent\nself-modifications. However, we identify a mismatch between the agent's\nself-improvement potential (metaproductivity) and its coding benchmark\nperformance, namely the Metaproductivity-Performance Mismatch. Inspired by\nHuxley's concept of clade, we propose a metric ($\\mathrm{CMP}$) that aggregates\nthe benchmark performances of the descendants of an agent as an indicator of\nits potential for self-improvement. We show that, in our self-improving coding\nagent development setting, access to the true $\\mathrm{CMP}$ is sufficient to\nsimulate how the G\\\"odel Machine would behave under certain assumptions. We\nintroduce the Huxley-G\\\"odel Machine (HGM), which, by estimating $\\mathrm{CMP}$\nand using it as guidance, searches the tree of self-modifications. On SWE-bench\nVerified and Polyglot, HGM outperforms prior self-improving coding agent\ndevelopment methods while using less wall-clock time. Last but not least, HGM\ndemonstrates strong transfer to other coding datasets and large language\nmodels. The agent optimized by HGM on SWE-bench Verified with GPT-5-mini and\nevaluated on SWE-bench Lite with GPT-5 achieves human-level performance,\nmatching the best officially checked results of human-engineered coding agents.\nOur code is available at https://github.com/metauto-ai/HGM."}
{"id": "2510.21618", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21618", "abs": "https://arxiv.org/abs/2510.21618", "authors": ["Xiaoxi Li", "Wenxiang Jiao", "Jiarui Jin", "Guanting Dong", "Jiajie Jin", "Yinuo Wang", "Hao Wang", "Yutao Zhu", "Ji-Rong Wen", "Yuan Lu", "Zhicheng Dou"], "title": "DeepAgent: A General Reasoning Agent with Scalable Toolsets", "comment": null, "summary": "Large reasoning models have demonstrated strong problem-solving abilities,\nyet real-world tasks often require external tools and long-horizon\ninteractions. Existing agent frameworks typically follow predefined workflows,\nwhich limit autonomous and global task completion. In this paper, we introduce\nDeepAgent, an end-to-end deep reasoning agent that performs autonomous\nthinking, tool discovery, and action execution within a single, coherent\nreasoning process. To address the challenges of long-horizon interactions,\nparticularly the context length explosion from multiple tool calls and the\naccumulation of interaction history, we introduce an autonomous memory folding\nmechanism that compresses past interactions into structured episodic, working,\nand tool memories, reducing error accumulation while preserving critical\ninformation. To teach general-purpose tool use efficiently and stably, we\ndevelop an end-to-end reinforcement learning strategy, namely ToolPO, that\nleverages LLM-simulated APIs and applies tool-call advantage attribution to\nassign fine-grained credit to the tool invocation tokens. Extensive experiments\non eight benchmarks, including general tool-use tasks (ToolBench, API-Bank,\nTMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA,\nHLE), demonstrate that DeepAgent consistently outperforms baselines across both\nlabeled-tool and open-set tool retrieval scenarios. This work takes a step\ntoward more general and capable agents for real-world applications. The code\nand demo are available at https://github.com/RUC-NLPIR/DeepAgent."}
{"id": "2510.21648", "categories": ["cs.RO", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.21648", "abs": "https://arxiv.org/abs/2510.21648", "authors": ["Inbazhagan Ravikumar", "Ram Sundhar", "Narendhiran Vijayakumar"], "title": "Design and Structural Validation of a Micro-UAV with On-Board Dynamic Route Planning", "comment": "8 pages, 4 figures, 4 tables", "summary": "Micro aerial vehicles are becoming increasingly important in search and\nrescue operations due to their agility, speed, and ability to access confined\nspaces or hazardous areas. However, designing lightweight aerial systems\npresents significant structural, aerodynamic, and computational challenges.\nThis work addresses two key limitations in many low-cost aerial systems under\ntwo kilograms: their lack of structural durability during flight through rough\nterrains and inability to replan paths dynamically when new victims or\nobstacles are detected. We present a fully customised drone built from scratch\nusing only commonly available components and materials, emphasising modularity,\nlow cost, and ease of assembly. The structural frame is reinforced with\nlightweight yet durable materials to withstand impact, while the onboard\ncontrol system is powered entirely by free, open-source software solutions. The\nproposed system demonstrates real-time perception and adaptive navigation\ncapabilities without relying on expensive hardware accelerators, offering an\naffordable and practical solution for real-world search and rescue missions."}
{"id": "2510.21652", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.21652", "abs": "https://arxiv.org/abs/2510.21652", "authors": ["Jonathan Bragg", "Mike D'Arcy", "Nishant Balepur", "Dan Bareket", "Bhavana Dalvi", "Sergey Feldman", "Dany Haddad", "Jena D. Hwang", "Peter Jansen", "Varsha Kishore", "Bodhisattwa Prasad Majumder", "Aakanksha Naik", "Sigal Rahamimov", "Kyle Richardson", "Amanpreet Singh", "Harshit Surana", "Aryeh Tiktinsky", "Rosni Vasu", "Guy Wiener", "Chloe Anastasiades", "Stefan Candra", "Jason Dunkelberger", "Dan Emery", "Rob Evans", "Malachi Hamada", "Regan Huff", "Rodney Kinney", "Matt Latzke", "Jaron Lochner", "Ruben Lozano-Aguilera", "Cecile Nguyen", "Smita Rao", "Amber Tanaka", "Brooke Vlahos", "Peter Clark", "Doug Downey", "Yoav Goldberg", "Ashish Sabharwal", "Daniel S. Weld"], "title": "AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research Suite", "comment": null, "summary": "AI agents hold the potential to revolutionize scientific productivity by\nautomating literature reviews, replicating experiments, analyzing data, and\neven proposing new directions of inquiry; indeed, there are now many such\nagents, ranging from general-purpose \"deep research\" systems to specialized\nscience-specific agents, such as AI Scientist and AIGS. Rigorous evaluation of\nthese agents is critical for progress. Yet existing benchmarks fall short on\nseveral fronts: they (1) fail to provide holistic, product-informed measures of\nreal-world use cases such as science research; (2) lack reproducible agent\ntools necessary for a controlled comparison of core agentic capabilities; (3)\ndo not account for confounding variables such as model cost and tool access;\n(4) do not provide standardized interfaces for quick agent prototyping and\nevaluation; and (5) lack comprehensive baseline agents necessary to identify\ntrue advances. In response, we define principles and tooling for more\nrigorously benchmarking agents. Using these, we present AstaBench, a suite that\nprovides the first holistic measure of agentic ability to perform scientific\nresearch, comprising 2400+ problems spanning the entire scientific discovery\nprocess and multiple scientific domains, and including many problems inspired\nby actual user requests to deployed Asta agents. Our suite comes with the first\nscientific research environment with production-grade search tools that enable\ncontrolled, reproducible evaluation, better accounting for confounders.\nAlongside, we provide a comprehensive suite of nine science-optimized classes\nof Asta agents and numerous baselines. Our extensive evaluation of 57 agents\nacross 22 agent classes reveals several interesting findings, most importantly\nthat despite meaningful progress on certain individual aspects, AI remains far\nfrom solving the challenge of science research assistance."}
{"id": "2510.21656", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21656", "abs": "https://arxiv.org/abs/2510.21656", "authors": ["Marta Contreiras Silva", "Daniel Faria", "Catia Pesquita"], "title": "CMOMgen: Complex Multi-Ontology Alignment via Pattern-Guided In-Context Learning", "comment": "32 pages, 5 figures", "summary": "Constructing comprehensive knowledge graphs requires the use of multiple\nontologies in order to fully contextualize data into a domain. Ontology\nmatching finds equivalences between concepts interconnecting ontologies and\ncreating a cohesive semantic layer. While the simple pairwise state of the art\nis well established, simple equivalence mappings cannot provide full semantic\nintegration of related but disjoint ontologies. Complex multi-ontology matching\n(CMOM) aligns one source entity to composite logical expressions of multiple\ntarget entities, establishing more nuanced equivalences and provenance along\nthe ontological hierarchy.\n  We present CMOMgen, the first end-to-end CMOM strategy that generates\ncomplete and semantically sound mappings, without establishing any restrictions\non the number of target ontologies or entities. Retrieval-Augmented Generation\nselects relevant classes to compose the mapping and filters matching reference\nmappings to serve as examples, enhancing In-Context Learning. The strategy was\nevaluated in three biomedical tasks with partial reference alignments. CMOMgen\noutperforms baselines in class selection, demonstrating the impact of having a\ndedicated strategy. Our strategy also achieves a minimum of 63% in F1-score,\noutperforming all baselines and ablated versions in two out of three tasks and\nplacing second in the third. Furthermore, a manual evaluation of non-reference\nmappings showed that 46% of the mappings achieve the maximum score, further\nsubstantiating its ability to construct semantically sound mappings."}
{"id": "2510.21679", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21679", "abs": "https://arxiv.org/abs/2510.21679", "authors": ["Gaku Morio", "Harri Rowlands", "Dominik Stammbach", "Christopher D. Manning", "Peter Henderson"], "title": "A Multimodal Benchmark for Framing of Oil & Gas Advertising and Potential Greenwashing Detection", "comment": "Forthcoming in NeurIPS 2025 Datasets and Benchmarks Track", "summary": "Companies spend large amounts of money on public relations campaigns to\nproject a positive brand image. However, sometimes there is a mismatch between\nwhat they say and what they do. Oil & gas companies, for example, are accused\nof \"greenwashing\" with imagery of climate-friendly initiatives. Understanding\nthe framing, and changes in framing, at scale can help better understand the\ngoals and nature of public relations campaigns. To address this, we introduce a\nbenchmark dataset of expert-annotated video ads obtained from Facebook and\nYouTube. The dataset provides annotations for 13 framing types for more than 50\ncompanies or advocacy groups across 20 countries. Our dataset is especially\ndesigned for the evaluation of vision-language models (VLMs), distinguishing it\nfrom past text-only framing datasets. Baseline experiments show some promising\nresults, while leaving room for improvement for future work: GPT-4.1 can detect\nenvironmental messages with 79% F1 score, while our best model only achieves\n46% F1 score on identifying framing around green innovation. We also identify\nchallenges that VLMs must address, such as implicit framing, handling videos of\nvarious lengths, or implicit cultural backgrounds. Our dataset contributes to\nresearch in multimodal analysis of strategic communication in the energy\nsector."}
{"id": "2510.21695", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21695", "abs": "https://arxiv.org/abs/2510.21695", "authors": ["Edward Holmberg", "Elias Ioup", "Mahdi Abdelguerfi"], "title": "A Knowledge-Graph Translation Layer for Mission-Aware Multi-Agent Path Planning in Spatiotemporal Dynamics", "comment": "10 pages, 10 figures, conference submission", "summary": "The coordination of autonomous agents in dynamic environments is hampered by\nthe semantic gap between high-level mission objectives and low-level planner\ninputs. To address this, we introduce a framework centered on a Knowledge Graph\n(KG) that functions as an intelligent translation layer. The KG's two-plane\narchitecture compiles declarative facts into per-agent, mission-aware\n``worldviews\" and physics-aware traversal rules, decoupling mission semantics\nfrom a domain-agnostic planner. This allows complex, coordinated paths to be\nmodified simply by changing facts in the KG. A case study involving Autonomous\nUnderwater Vehicles (AUVs) in the Gulf of Mexico visually demonstrates the\nend-to-end process and quantitatively proves that different declarative\npolicies produce distinct, high-performing outcomes. This work establishes the\nKG not merely as a data repository, but as a powerful, stateful orchestrator\nfor creating adaptive and explainable autonomous systems."}
{"id": "2510.20918", "categories": ["econ.TH", "cs.GT"], "pdf": "https://arxiv.org/pdf/2510.20918", "abs": "https://arxiv.org/abs/2510.20918", "authors": ["Alejandro Francetich", "Burkhard C. Schipper"], "title": "Rationalizable Screening and Disclosure under Unawareness", "comment": "48 pages, 3 figures", "summary": "We analyze a principal-agent procurement problem in which the principal (she)\nis unaware some of the marginal cost types of the agent (he). Communication\narises naturally as some types of the agent may have an incentive to raise the\nprincipal's awareness (totally or partially) before a contract menu is offered.\nThe resulting menu must not only reflect the principal's change in awareness,\nbut also her learning about types from the agent's decision to raise her\nawareness in the first place. We capture this reasoning in a discrete concave\nmodel via a rationalizability procedure in which marginal beliefs over types\nare restricted to log-concavity, ``reverse'' Bayesianism, and mild assumptions\nof caution.\n  We show that if the principal is ex ante only unaware of high-cost types, all\nof these types have an incentive raise her awareness of them -- otherwise, they\nwould not be served. With three types, the two lower-cost types that the\nprincipal is initially aware of also want to raise her awareness of the\nhigh-cost type: Their quantities suffer no additional distortions and they both\nearn an extra information rent. Intuitively, the presence of an even higher\ncost type makes the original two look better. With more than three types, we\nshow that this intuition may break down for types of whom the principal is\ninitially aware of so that raising the principal's awareness could cease to be\nprofitable for those types. When the principal is ex ante only unaware of more\nefficient (low-cost) types, then \\textit{no type} raises her awareness, leaving\nher none the wiser."}
{"id": "2510.20921", "categories": ["econ.TH", "cs.GT"], "pdf": "https://arxiv.org/pdf/2510.20921", "abs": "https://arxiv.org/abs/2510.20921", "authors": ["Alejandro Francetich", "Burkhard C. Schipper"], "title": "Discrete Screening", "comment": "27 pages, 6 figures", "summary": "We consider a principal who wishes to screen an agent with \\emph{discrete}\ntypes by offering a menu of \\emph{discrete} quantities and \\emph{discrete}\ntransfers. We assume that the principal's valuation is discrete strictly\nconcave and use a discrete first-order approach. We model the agent's cost\ntypes as non-integer, with integer types as a limit case. Our modeling of cost\ntypes allows us to replicate the typical constraint-simplification results and\nthus to emulate the well-treaded steps of screening under a continuum of\ncontracts.\n  We show that the solutions to the discrete F.O.C.s need not be unique\n\\textit{even under discrete strict concavity}, but we also show that there\ncannot be more than two optimal contract quantities for each type, and that --\nif there are two -- they must be adjacent. Moreover, we can only ensure weak\nmonotonicity of the quantities \\textit{even if virtual costs are strictly\nmonotone}, unless we limit the ``degree of concavity'' of the principal's\nutility. Our discrete screening approach facilitates the use of\nrationalizability to solve the screening problem. We introduce a\nrationalizability notion featuring robustness with respect to an open set of\nbeliefs over types called \\textit{$\\Delta$-O Rationalizability}, and show that\nthe set of $\\Delta$-O rationalizable menus coincides with the set of usual\noptimal contracts -- possibly augmented to include irrelevant contracts."}
{"id": "2510.21071", "categories": ["econ.GN", "cs.MA", "q-fin.EC", "I.6.3"], "pdf": "https://arxiv.org/pdf/2510.21071", "abs": "https://arxiv.org/abs/2510.21071", "authors": ["Emilio Barucci", "Andrea Gurgone", "Giulia Iori", "Michele Azzone"], "title": "Central Bank Digital Currency, Flight-to-Quality, and Bank-Runs in an Agent-Based Model", "comment": null, "summary": "We analyse financial stability and welfare impacts associated with the\nintroduction of a Central Bank Digital Currency (CBDC) in a macroeconomic\nagent-based model. The model considers firms, banks, and households interacting\non labour, goods, credit, and interbank markets. Households move their\nliquidity from deposits to CBDC based on the perceived riskiness of their\nbanks. We find that the introduction of CBDC exacerbates bank-runs and may lead\nto financial instability phenomena. The effect can be changed by introducing a\nlimit on CBDC holdings. The adoption of CBDC has little effect on macroeconomic\nvariables but the interest rate on loans to firms goes up and credit goes down\nin a limited way. CBDC leads to a redistribution of wealth from firms and banks\nto households with a higher bank default rate. CBDC may have negative welfare\neffects, but a bound on holding enables a welfare improvement."}
{"id": "2510.21165", "categories": ["q-fin.GN", "cs.SY", "eess.SY", "nlin.CD", "physics.data-an", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2510.21165", "abs": "https://arxiv.org/abs/2510.21165", "authors": ["Peng Liu"], "title": "The local Gaussian correlation networks among return tails in the Chinese stock market", "comment": null, "summary": "Financial networks based on Pearson correlations have been intensively\nstudied. However, previous studies may have led to misleading and catastrophic\nresults because of several critical shortcomings of the Pearson correlation.\nThe local Gaussian correlation coefficient, a new measurement of statistical\ndependence between variables, has unique advantages including capturing local\nnonlinear dependence and handling heavy-tailed distributions. This study\nconstructs financial networks using the local Gaussian correlation coefficients\nbetween tail regions of stock returns in the Shanghai Stock Exchange. The work\nsystematically analyzes fundamental network metrics including node centrality,\naverage shortest path length, and entropy. Compared with the local Gaussian\ncorrelation network among positive tails and the conventional Pearson\ncorrelation network, the properties of the local Gaussian correlation network\namong negative tails are more sensitive to the stock market risks. This finding\nsuggests researchers should prioritize the local Gaussian correlation network\namong negative tails. Future work should reevaluate existing findings using the\nlocal Gaussian correlation method."}

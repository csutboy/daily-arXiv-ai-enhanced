{"id": "2509.14540", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2509.14540", "abs": "https://arxiv.org/abs/2509.14540", "authors": ["Meghna Roy Chowdhury", "Ming-che Li", "Archisman Ghosh", "Md Faizul Bari", "Shreyas Sen"], "title": "Design-Space Exploration of Distributed Neural Networks in Low-Power Wearable Nodes", "comment": "10 figures, 5 tables, 14 pages", "summary": "Wearable devices are revolutionizing personal technology, but their usability\nis often hindered by frequent charging due to high power consumption. This\npaper introduces Distributed Neural Networks (DistNN), a framework that\ndistributes neural network computations between resource-constrained wearable\nnodes and resource-rich hubs to reduce energy at the node without sacrificing\nperformance. We define a Figure of Merit (FoM) to select the optimal split\npoint that minimizes node-side energy. A custom hardware design using\nlow-precision fixed-point arithmetic achieves ultra-low power while maintaining\naccuracy. The proposed system is ~1000x more energy efficient than a GPU and\naverages 11x lower power than recent machine learning (ML) ASICs at 30 fps.\nEvaluated with CNNs and autoencoders, DistNN attains an SSIM of 0.90 for image\nreconstruction and 0.89 for denoising, enabling scalable, energy-efficient,\nreal-time wearable applications.", "AI": {"tldr": "\u63d0\u51faDistNN\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u53ef\u7a7f\u6234\u8bbe\u5907\u8282\u70b9\u548c\u8d44\u6e90\u4e30\u5bcc\u7684\u4e2d\u5fc3\u4e4b\u95f4\u5206\u5e03\u5f0f\u5904\u7406\u795e\u7ecf\u7f51\u7edc\u8ba1\u7b97\uff0c\u663e\u8457\u964d\u4f4e\u8282\u70b9\u80fd\u8017\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u53ef\u7a7f\u6234\u8bbe\u5907\u56e0\u9ad8\u529f\u8017\u9700\u8981\u9891\u7e41\u5145\u7535\uff0c\u5f71\u54cd\u7528\u6237\u4f53\u9a8c\uff0c\u9700\u8981\u627e\u5230\u5728\u8d44\u6e90\u53d7\u9650\u8282\u70b9\u4e0a\u5b9e\u73b0\u9ad8\u6548\u80fd\u795e\u7ecf\u7f51\u7edc\u8ba1\u7b97\u7684\u65b9\u6cd5\u3002", "method": "\u5b9a\u4e49FoM\u6307\u6807\u9009\u62e9\u6700\u4f18\u5206\u5272\u70b9\uff0c\u91c7\u7528\u4f4e\u7cbe\u5ea6\u5b9a\u70b9\u7b97\u672f\u7684\u5b9a\u5236\u786c\u4ef6\u8bbe\u8ba1\uff0c\u5728\u8282\u70b9\u548c\u4e2d\u5fc3\u4e4b\u95f4\u5206\u5e03\u5f0f\u5904\u7406\u795e\u7ecf\u7f51\u7edc\u8ba1\u7b97\u3002", "result": "\u6bd4GPU\u80fd\u6548\u9ad8\u7ea61000\u500d\uff0c\u6bd4\u6700\u65b0ML ASIC\u5e73\u5747\u529f\u8017\u4f4e11\u500d\uff0830fps\uff09\uff0c\u56fe\u50cf\u91cd\u5efaSSIM\u8fbe0.90\uff0c\u53bb\u566aSSIM\u8fbe0.89\u3002", "conclusion": "DistNN\u6846\u67b6\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u3001\u9ad8\u80fd\u6548\u7684\u5b9e\u65f6\u53ef\u7a7f\u6234\u5e94\u7528\uff0c\u89e3\u51b3\u4e86\u53ef\u7a7f\u6234\u8bbe\u5907\u80fd\u8017\u95ee\u9898\u3002"}}
{"id": "2509.14883", "categories": ["cs.ET", "cs.SI"], "pdf": "https://arxiv.org/pdf/2509.14883", "abs": "https://arxiv.org/abs/2509.14883", "authors": ["Can Cui", "Ziye Jia", "Jiahao You", "Chao Dong", "Qihui Wu", "Han Zhu"], "title": "Robust and Secure Computation Offloading and Trajectory Optimization for Multi-UAV MEC Against Aerial Eavesdropper", "comment": null, "summary": "The unmanned aerial vehicle (UAV) based multi-access edge computing (MEC)\nappears as a popular paradigm to reduce task processing latency. However, the\nsecure offloading is an important issue when occurring aerial eavesdropping.\nBesides, the potential uncertainties in practical applications and flexible\ntrajectory optimizations of UAVs pose formidable challenges for realizing\nrobust offloading. In this paper, we consider the aerial secure MEC network\nincluding ground users, service unmanned aerial vehicles (S-UAVs) integrated\nwith edge servers, and malicious UAVs overhearing transmission links. To deal\nwith the task computation complexities, which are characterized as\nuncertainties, a robust problem is formulated with chance constraints. The\nenergy cost is minimized by optimizing the connections, trajectories of S-UAVs\nand offloading ratios. Then, the proposed non-linear problem is tackled via the\ndistributionally robust optimization and conditional value-at-risk mechanism,\nwhich is further transformed into the second order cone programming forms.\nMoreover, we decouple the reformulated problem and design the successive convex\napproximation for S-UAV trajectories. The global algorithm is designed to solve\nthe sub-problems in a block coordinate decent manner. Finally, extensive\nsimulations and numerical analyses are conducted to verify the robustness of\nthe proposed algorithms, with just 2\\% more energy cost compared with the ideal\ncircumstance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u65e0\u4eba\u673a\u8fb9\u7f18\u8ba1\u7b97\u7f51\u7edc\u4e2d\u5b89\u5168\u5378\u8f7d\u7684\u9c81\u68d2\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u548c\u6761\u4ef6\u98ce\u9669\u4ef7\u503c\u673a\u5236\u5904\u7406\u4efb\u52a1\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0d\u786e\u5b9a\u6027\uff0c\u6700\u5c0f\u5316\u80fd\u8017\u6210\u672c\u3002", "motivation": "\u65e0\u4eba\u673a\u8fb9\u7f18\u8ba1\u7b97\u867d\u7136\u80fd\u964d\u4f4e\u4efb\u52a1\u5904\u7406\u5ef6\u8fdf\uff0c\u4f46\u9762\u4e34\u7a7a\u4e2d\u7a83\u542c\u7684\u5b89\u5168\u5a01\u80c1\u4ee5\u53ca\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u6311\u6218\uff0c\u9700\u8981\u5b9e\u73b0\u9c81\u68d2\u7684\u5b89\u5168\u5378\u8f7d\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u548c\u6761\u4ef6\u98ce\u9669\u4ef7\u503c\u673a\u5236\u5904\u7406\u4e0d\u786e\u5b9a\u6027\uff0c\u5c06\u975e\u7ebf\u6027\u95ee\u9898\u8f6c\u5316\u4e3a\u4e8c\u9636\u9525\u89c4\u5212\u5f62\u5f0f\uff0c\u8bbe\u8ba1\u9010\u6b21\u51f8\u8fd1\u4f3c\u7b97\u6cd5\u4f18\u5316\u65e0\u4eba\u673a\u8f68\u8ff9\uff0c\u4f7f\u7528\u5757\u5750\u6807\u4e0b\u964d\u6cd5\u6c42\u89e3\u5b50\u95ee\u9898\u3002", "result": "\u4eff\u771f\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u9c81\u68d2\u6027\uff0c\u4e0e\u7406\u60f3\u60c5\u51b5\u76f8\u6bd4\u4ec5\u589e\u52a02%\u7684\u80fd\u8017\u6210\u672c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406\u65e0\u4eba\u673a\u8fb9\u7f18\u8ba1\u7b97\u7f51\u7edc\u4e2d\u7684\u5b89\u5168\u5378\u8f7d\u95ee\u9898\uff0c\u5728\u4fdd\u8bc1\u9c81\u68d2\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u8f83\u4f4e\u7684\u80fd\u8017\u6210\u672c\u3002"}}
{"id": "2509.14422", "categories": ["econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2509.14422", "abs": "https://arxiv.org/abs/2509.14422", "authors": ["Giorgia Menta", "Pietro Biroli", "Divya Mehta", "Conchita D'Ambrosio", "Deborah Cobb-Clark"], "title": "Aggregating Epigenetic Clocks to Study Human Capital Formation", "comment": null, "summary": "Epigenetics is the study of how people's behavior and environments influence\nthe way their genes are expressed, even though their DNA sequence is itself\nunchanged. By aggregating age-related epigenetic markers, epigenetic 'clocks'\nhave become the leading tool for studying biological aging. We make an\nimportant contribution by developing a novel, integrated measure of epigenetic\naging--the Multi EpiGenetic Age (MEGA) clock--which combines several existing\nepigenetic clocks to reduce measurement error and improve estimation\nefficiency. We use the MEGA clock in three empirical contexts to show that: i)\naccelerated epigenetic aging in adolescence is associated with worse\neducational, mental-health, and labor market outcomes in early adulthood; ii)\nexposure to child maltreatment before adolescence is associated with half a\nyear higher epigenetic aging; and iii) that entering school one year later\naccelerates epigenetic aging by age seven, particularly among disadvantaged\nchildren. The MEGA clock is robust to alternative methods for constructing it,\nproviding a flexible and interpretable approach for incorporating epigenetic\ndata into a wide variety of settings.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u65b0\u578b\u6574\u5408\u8868\u89c2\u9057\u4f20\u65f6\u949fMEGA\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u4e2a\u73b0\u6709\u65f6\u949f\u51cf\u5c11\u6d4b\u91cf\u8bef\u5dee\uff0c\u5728\u4e09\u4e2a\u5b9e\u8bc1\u7814\u7a76\u4e2d\u53d1\u73b0\u8868\u89c2\u9057\u4f20\u52a0\u901f\u8870\u8001\u4e0e\u6559\u80b2\u3001\u5fc3\u7406\u5065\u5eb7\u548c\u52b3\u52a8\u529b\u5e02\u573a\u7ed3\u679c\u76f8\u5173", "motivation": "\u8868\u89c2\u9057\u4f20\u65f6\u949f\u662f\u7814\u7a76\u751f\u7269\u8870\u8001\u7684\u4e3b\u8981\u5de5\u5177\uff0c\u4f46\u73b0\u6709\u5355\u4e2a\u65f6\u949f\u5b58\u5728\u6d4b\u91cf\u8bef\u5dee\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u7cbe\u786e\u7684\u6574\u5408\u6d4b\u91cf\u65b9\u6cd5", "method": "\u5f00\u53d1MEGA\u65f6\u949f\uff0c\u6574\u5408\u591a\u4e2a\u73b0\u6709\u8868\u89c2\u9057\u4f20\u65f6\u949f\uff0c\u51cf\u5c11\u6d4b\u91cf\u8bef\u5dee\u5e76\u63d0\u9ad8\u4f30\u8ba1\u6548\u7387\uff0c\u5728\u4e09\u4e2a\u4e0d\u540c\u5b9e\u8bc1\u80cc\u666f\u4e0b\u5e94\u7528\u9a8c\u8bc1", "result": "1)\u9752\u5c11\u5e74\u671f\u8868\u89c2\u9057\u4f20\u52a0\u901f\u8870\u8001\u4e0e\u65e9\u671f\u6210\u5e74\u671f\u66f4\u5dee\u7684\u6559\u80b2\u3001\u5fc3\u7406\u5065\u5eb7\u548c\u52b3\u52a8\u529b\u5e02\u573a\u7ed3\u679c\u76f8\u5173\uff1b2)\u7ae5\u5e74\u8650\u5f85\u66b4\u9732\u4e0e\u534a\u5e74\u66f4\u9ad8\u7684\u8868\u89c2\u9057\u4f20\u8870\u8001\u76f8\u5173\uff1b3)\u665a\u4e00\u5e74\u5165\u5b66\u4f7f7\u5c81\u65f6\u8868\u89c2\u9057\u4f20\u8870\u8001\u52a0\u901f\uff0c\u5f31\u52bf\u513f\u7ae5\u5f71\u54cd\u66f4\u660e\u663e", "conclusion": "MEGA\u65f6\u949f\u5bf9\u6784\u5efa\u65b9\u6cd5\u5177\u6709\u7a33\u5065\u6027\uff0c\u4e3a\u5728\u5404\u79cd\u73af\u5883\u4e2d\u6574\u5408\u8868\u89c2\u9057\u4f20\u6570\u636e\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5"}}
{"id": "2509.14805", "categories": ["stat.AP", "econ.EM"], "pdf": "https://arxiv.org/pdf/2509.14805", "abs": "https://arxiv.org/abs/2509.14805", "authors": ["Paponpat Taveeapiradeecharoen", "Nattapol Aunsri"], "title": "Forecasting in small open emerging economies Evidence from Thailand", "comment": null, "summary": "Forecasting inflation in small open economies is difficult because limited\ntime series and strong external exposures create an imbalance between few\nobservations and many potential predictors. We study this challenge using\nThailand as a representative case, combining more than 450 domestic and\ninternational indicators. We evaluate modern Bayesian shrinkage and factor\nmodels, including Horseshoe regressions, factor-augmented autoregressions,\nfactor-augmented VARs, dynamic factor models, and Bayesian additive regression\ntrees.\n  Our results show that factor models dominate at short horizons, when global\nshocks and exchange rate movements drive inflation, while shrinkage-based\nregressions perform best at longer horizons. These models not only improve\npoint and density forecasts but also enhance tail-risk performance at the\none-year horizon.\n  Shrinkage diagnostics, on the other hand, additionally reveal that Google\nTrends variables, especially those related to food essential goods and housing\ncosts, progressively rotate into predictive importance as the horizon\nlengthens. This underscores their role as forward-looking indicators of\nhousehold inflation expectations in small open economies.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u591a\u79cd\u8d1d\u53f6\u65af\u6536\u7f29\u548c\u56e0\u5b50\u6a21\u578b\u5728\u6cf0\u56fd\u901a\u80c0\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u56e0\u5b50\u6a21\u578b\u5728\u77ed\u671f\u9884\u6d4b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u800c\u6536\u7f29\u56de\u5f52\u5728\u957f\u671f\u9884\u6d4b\u4e2d\u66f4\u4f18\uff0c\u540c\u65f6\u53d1\u73b0\u8c37\u6b4c\u8d8b\u52bf\u6570\u636e\u53ef\u4f5c\u4e3a\u524d\u77bb\u6027\u901a\u80c0\u9884\u671f\u6307\u6807\u3002", "motivation": "\u5c0f\u578b\u5f00\u653e\u7ecf\u6d4e\u4f53\u7684\u901a\u80c0\u9884\u6d4b\u9762\u4e34\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u6709\u9650\u4f46\u6f5c\u5728\u9884\u6d4b\u56e0\u5b50\u4f17\u591a\u7684\u6311\u6218\uff0c\u9700\u8981\u627e\u5230\u6709\u6548\u7684\u9884\u6d4b\u65b9\u6cd5\u6765\u5904\u7406\u8fd9\u79cd\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u8d85\u8fc7450\u4e2a\u56fd\u5185\u5916\u6307\u6807\uff0c\u8bc4\u4f30\u4e86\u591a\u79cd\u73b0\u4ee3\u9884\u6d4b\u65b9\u6cd5\uff0c\u5305\u62ecHorseshoe\u56de\u5f52\u3001\u56e0\u5b50\u589e\u5f3a\u81ea\u56de\u5f52\u3001\u56e0\u5b50\u589e\u5f3aVAR\u3001\u52a8\u6001\u56e0\u5b50\u6a21\u578b\u548c\u8d1d\u53f6\u65af\u52a0\u6027\u56de\u5f52\u6811\u3002", "result": "\u56e0\u5b50\u6a21\u578b\u5728\u77ed\u671f\u9884\u6d4b\u4e2d\u8868\u73b0\u6700\u597d\uff08\u53d7\u5168\u7403\u51b2\u51fb\u548c\u6c47\u7387\u53d8\u52a8\u5f71\u54cd\uff09\uff0c\u6536\u7f29\u56de\u5f52\u5728\u957f\u671f\u9884\u6d4b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u8fd9\u4e9b\u6a21\u578b\u4e0d\u4ec5\u6539\u5584\u4e86\u70b9\u9884\u6d4b\u548c\u5bc6\u5ea6\u9884\u6d4b\uff0c\u8fd8\u63d0\u5347\u4e86\u4e00\u5e74\u671f\u7684\u5c3e\u90e8\u98ce\u9669\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "\u8c37\u6b4c\u8d8b\u52bf\u53d8\u91cf\uff08\u7279\u522b\u662f\u98df\u54c1\u5fc5\u9700\u54c1\u548c\u4f4f\u623f\u6210\u672c\u76f8\u5173\uff09\u968f\u7740\u9884\u6d4b\u671f\u9650\u5ef6\u957f\u800c\u53d8\u5f97\u91cd\u8981\uff0c\u53ef\u4f5c\u4e3a\u5c0f\u578b\u5f00\u653e\u7ecf\u6d4e\u4f53\u4e2d\u5bb6\u5ead\u901a\u80c0\u9884\u671f\u7684\u524d\u77bb\u6027\u6307\u6807\u3002"}}
{"id": "2509.14295", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14295", "abs": "https://arxiv.org/abs/2509.14295", "authors": ["Fanqi Kong", "Ruijie Zhang", "Huaxiao Yin", "Guibin Zhang", "Xiaofei Zhang", "Ziang Chen", "Zhaowei Zhang", "Xiaoyuan Zhang", "Song-Chun Zhu", "Xue Feng"], "title": "AEGIS: Automated Error Generation and Identification for Multi-Agent Systems", "comment": null, "summary": "As Multi-Agent Systems (MAS) become increasingly autonomous and complex,\nunderstanding their error modes is critical for ensuring their reliability and\nsafety. However, research in this area has been severely hampered by the lack\nof large-scale, diverse datasets with precise, ground-truth error labels. To\naddress this bottleneck, we introduce \\textbf{AEGIS}, a novel framework for\n\\textbf{A}utomated \\textbf{E}rror \\textbf{G}eneration and\n\\textbf{I}dentification for Multi-Agent \\textbf{S}ystems. By systematically\ninjecting controllable and traceable errors into initially successful\ntrajectories, we create a rich dataset of realistic failures. This is achieved\nusing a context-aware, LLM-based adaptive manipulator that performs\nsophisticated attacks like prompt injection and response corruption to induce\nspecific, predefined error modes. We demonstrate the value of our dataset by\nexploring three distinct learning paradigms for the error identification task:\nSupervised Fine-Tuning, Reinforcement Learning, and Contrastive Learning. Our\ncomprehensive experiments show that models trained on AEGIS data achieve\nsubstantial improvements across all three learning paradigms. Notably, several\nof our fine-tuned models demonstrate performance competitive with or superior\nto proprietary systems an order of magnitude larger, validating our automated\ndata generation framework as a crucial resource for developing more robust and\ninterpretable multi-agent systems. Our project website is available at\nhttps://kfq20.github.io/AEGIS-Website.", "AI": {"tldr": "AEGIS\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u9519\u8bef\u751f\u6210\u548c\u8bc6\u522b\u6846\u67b6\uff0c\u901a\u8fc7\u5411\u6210\u529f\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8f68\u8ff9\u4e2d\u6ce8\u5165\u53ef\u63a7\u9519\u8bef\u6765\u521b\u5efa\u5927\u89c4\u6a21\u9519\u8bef\u6570\u636e\u96c6\uff0c\u652f\u6301\u76d1\u7763\u5fae\u8c03\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u5bf9\u6bd4\u5b66\u4e60\u4e09\u79cd\u8303\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9519\u8bef\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u65e5\u76ca\u81ea\u4e3b\u548c\u590d\u6742\uff0c\u4f46\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u7684\u9519\u8bef\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u4e25\u91cd\u963b\u788d\u4e86\u7cfb\u7edf\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u81ea\u9002\u5e94\u64cd\u7eb5\u5668\uff0c\u901a\u8fc7\u63d0\u793a\u6ce8\u5165\u548c\u54cd\u5e94\u7834\u574f\u7b49\u590d\u6742\u653b\u51fb\u65b9\u5f0f\uff0c\u5728\u6210\u529f\u8f68\u8ff9\u4e2d\u7cfb\u7edf\u6027\u5730\u6ce8\u5165\u53ef\u63a7\u4e14\u53ef\u8ffd\u8e2a\u7684\u9519\u8bef\u3002", "result": "\u5728\u4e09\u79cd\u5b66\u4e60\u8303\u5f0f\u4e0a\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u591a\u4e2a\u5fae\u8c03\u6a21\u578b\u7684\u6027\u80fd\u4e0e\u89c4\u6a21\u5927\u4e00\u4e2a\u6570\u91cf\u7ea7\u7684\u4e13\u6709\u7cfb\u7edf\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\u3002", "conclusion": "AEGIS\u6846\u67b6\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u548c\u53ef\u89e3\u91ca\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5173\u952e\u7684\u6570\u636e\u8d44\u6e90\uff0c\u9a8c\u8bc1\u4e86\u81ea\u52a8\u5316\u6570\u636e\u751f\u6210\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.14251", "categories": ["cs.AI", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.14251", "abs": "https://arxiv.org/abs/2509.14251", "authors": ["Qihang Chen"], "title": "Unified Crew Planning and Replanning Optimization in Multi-Line Metro Systems Considering Workforce Heterogeneity", "comment": null, "summary": "Metro crew planning is a key component of smart city development as it\ndirectly impacts the operational efficiency and service reliability of public\ntransportation. With the rapid expansion of metro networks, effective\nmulti-line scheduling and emergency management have become essential for\nlarge-scale seamless operations. However, current research focuses primarily on\nindividual metro lines,with insufficient attention on cross-line coordination\nand rapid replanning during disruptions. Here, a unified optimization framework\nis presented for multi-line metro crew planning and replanning with\nheterogeneous workforce. Specifically, a hierarchical time-space network model\nis proposed to represent the unified crew action space, and computationally\nefficient constraints and formulations are derived for the crew's heterogeneous\nqualifications and preferences. Solution algorithms based on column generation\nand shortest path adjustment are further developed, utilizing the proposed\nnetwork model. Experiments with real data from Shanghai and Beijing Metro\ndemonstrate that the proposed methods outperform benchmark heuristics in both\ncost reduction and task completion,and achieve notable efficiency gains by\nincorporating cross-line operations, particularly for urgent tasks during\ndisruptions. This work highlights the role of global optimization and\ncross-line coordination in multi-line metro system operations, providing\ninsights into the efficient and reliable functioning of public transportation\nin smart cities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u7ebf\u8def\u5730\u94c1\u4e58\u52a1\u5458\u89c4\u5212\u548c\u91cd\u89c4\u5212\u4f18\u5316\u6846\u67b6\uff0c\u91c7\u7528\u5206\u5c42\u65f6\u7a7a\u7f51\u7edc\u6a21\u578b\u548c\u5217\u751f\u6210\u7b97\u6cd5\uff0c\u5728\u6210\u672c\u964d\u4f4e\u548c\u4efb\u52a1\u5b8c\u6210\u65b9\u9762\u4f18\u4e8e\u57fa\u51c6\u542f\u53d1\u5f0f\u65b9\u6cd5", "motivation": "\u968f\u7740\u5730\u94c1\u7f51\u7edc\u5feb\u901f\u6269\u5f20\uff0c\u591a\u7ebf\u8def\u8c03\u5ea6\u548c\u5e94\u6025\u7ba1\u7406\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u7ebf\u8def\uff0c\u7f3a\u4e4f\u5bf9\u8de8\u7ebf\u8def\u534f\u8c03\u548c\u4e2d\u65ad\u65f6\u5feb\u901f\u91cd\u89c4\u5212\u7684\u5173\u6ce8", "method": "\u63d0\u51fa\u5206\u5c42\u65f6\u7a7a\u7f51\u7edc\u6a21\u578b\u8868\u793a\u7edf\u4e00\u7684\u4e58\u52a1\u5458\u884c\u52a8\u7a7a\u95f4\uff0c\u63a8\u5bfc\u8ba1\u7b97\u9ad8\u6548\u7684\u7ea6\u675f\u548c\u516c\u5f0f\u5904\u7406\u4e58\u52a1\u5458\u5f02\u8d28\u8d44\u8d28\u548c\u504f\u597d\uff0c\u5f00\u53d1\u57fa\u4e8e\u5217\u751f\u6210\u548c\u6700\u77ed\u8def\u5f84\u8c03\u6574\u7684\u6c42\u89e3\u7b97\u6cd5", "result": "\u4e0a\u6d77\u548c\u5317\u4eac\u5730\u94c1\u771f\u5b9e\u6570\u636e\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6210\u672c\u964d\u4f4e\u548c\u4efb\u52a1\u5b8c\u6210\u65b9\u9762\u4f18\u4e8e\u57fa\u51c6\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u901a\u8fc7\u8de8\u7ebf\u8def\u64cd\u4f5c\u5b9e\u73b0\u663e\u8457\u6548\u7387\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\u4e2d\u65ad\u671f\u95f4\u7684\u7d27\u6025\u4efb\u52a1", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5f3a\u8c03\u4e86\u5168\u5c40\u4f18\u5316\u548c\u8de8\u7ebf\u8def\u534f\u8c03\u5728\u591a\u7ebf\u8def\u5730\u94c1\u7cfb\u7edf\u8fd0\u8425\u4e2d\u7684\u4f5c\u7528\uff0c\u4e3a\u667a\u6167\u57ce\u5e02\u516c\u5171\u4ea4\u901a\u7684\u9ad8\u6548\u53ef\u9760\u8fd0\u884c\u63d0\u4f9b\u4e86\u89c1\u89e3"}}
{"id": "2509.14396", "categories": ["econ.TH", "cs.GT"], "pdf": "https://arxiv.org/pdf/2509.14396", "abs": "https://arxiv.org/abs/2509.14396", "authors": ["Drew Fudenberg", "Annie Liang"], "title": "Friend or Foe: Delegating to an AI Whose Alignment is Unknown", "comment": null, "summary": "AI systems have the potential to improve decision-making, but decision makers\nface the risk that the AI may be misaligned with their objectives. We study\nthis problem in the context of a treatment decision, where a designer decides\nwhich patient attributes to reveal to an AI before receiving a prediction of\nthe patient's need for treatment. Providing the AI with more information\nincreases the benefits of an aligned AI but also amplifies the harm from a\nmisaligned one. We characterize how the designer should select attributes to\nbalance these competing forces, depending on their beliefs about the AI's\nreliability. We show that the designer should optimally disclose attributes\nthat identify \\emph{rare} segments of the population in which the need for\ntreatment is high, and pool the remaining patients.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u5728\u6cbb\u7597\u51b3\u7b56\u4e2d\u5982\u4f55\u9009\u62e9\u5411AI\u62ab\u9732\u7684\u60a3\u8005\u5c5e\u6027\uff0c\u4ee5\u5e73\u8861\u5bf9\u9f50AI\u7684\u6536\u76ca\u548c\u9519\u4f4dAI\u7684\u98ce\u9669\uff0c\u6700\u4f18\u7b56\u7565\u662f\u62ab\u9732\u80fd\u8bc6\u522b\u7f55\u89c1\u9ad8\u6cbb\u7597\u9700\u6c42\u4eba\u7fa4\u7684\u5c5e\u6027\u3002", "motivation": "AI\u7cfb\u7edf\u53ef\u80fd\u4e0e\u5176\u76ee\u6807\u9519\u4f4d\u7684\u98ce\u9669\uff0c\u51b3\u7b56\u8005\u9700\u8981\u5728\u63d0\u4f9b\u66f4\u591a\u4fe1\u606f\u4ee5\u63d0\u5347\u5bf9\u9f50AI\u7684\u6536\u76ca\u4e0e\u907f\u514d\u9519\u4f4dAI\u9020\u6210\u66f4\u5927\u5371\u5bb3\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u3002", "method": "\u5728\u6cbb\u7597\u51b3\u7b56\u80cc\u666f\u4e0b\u5efa\u6a21\uff0c\u8bbe\u8ba1\u8005\u9009\u62e9\u5411AI\u62ab\u9732\u54ea\u4e9b\u60a3\u8005\u5c5e\u6027\uff0c\u57fa\u4e8e\u5bf9AI\u53ef\u9760\u6027\u7684\u4fe1\u5ff5\u6765\u5e73\u8861\u5bf9\u9f50\u6536\u76ca\u548c\u9519\u4f4d\u98ce\u9669\u3002", "result": "\u6700\u4f18\u7b56\u7565\u662f\u62ab\u9732\u80fd\u8bc6\u522b\u7f55\u89c1\u4eba\u7fa4\u6bb5\u4e2d\u6cbb\u7597\u9700\u6c42\u9ad8\u7684\u5c5e\u6027\uff0c\u800c\u5bf9\u5176\u4ed6\u60a3\u8005\u8fdb\u884c\u5408\u5e76\u5904\u7406\u3002", "conclusion": "\u8bbe\u8ba1\u8005\u5e94\u6839\u636e\u5bf9AI\u53ef\u9760\u6027\u7684\u4fe1\u5ff5\uff0c\u9009\u62e9\u6027\u5730\u62ab\u9732\u80fd\u8bc6\u522b\u7f55\u89c1\u9ad8\u9700\u6c42\u4eba\u7fa4\u7684\u5c5e\u6027\uff0c\u4ee5\u6700\u4f18\u5730\u5e73\u8861AI\u5bf9\u9f50\u6536\u76ca\u548c\u9519\u4f4d\u98ce\u9669\u3002"}}
{"id": "2509.15169", "categories": ["econ.EM"], "pdf": "https://arxiv.org/pdf/2509.15169", "abs": "https://arxiv.org/abs/2509.15169", "authors": ["Yongheng Hu"], "title": "Monetary Policy and Exchange Rate Fluctuations", "comment": "23 pages, 10 figures", "summary": "In this paper, we model USD-CNY bilateral exchange rate fluctuations as a\ngeneral stochastic process and incorporate monetary policy shock to examine how\nbilateral exchange rate fluctuations affect the Revealed Comparative Advantage\n(RCA) index. Numerical simulations indicate that as the mean of bilateral\nexchange rate fluctuations increases, i.e., currency devaluation, the RCA index\nrises. Moreover, smaller bilateral exchange rate fluctuations after the policy\nshock cause the RCA index to gradually converge toward its mean level. For the\nempirical analysis, we select the USD-CNY bilateral exchange rate and\nprovincial manufacturing industry export competitiveness data in China from\n2008 to 2021. We find that in the short term, when exchange rate fluctuations\nstabilize within a range less than 0.2 RMB depreciation will effectively boost\nexport competitiveness. Then, the 8.11 exchange rate policy reversed the\nprevious linear trend of the CNY, stabilizing it within a narrow fluctuation\nrange over the long term. This policy leads to a gradual convergence of\nprovincial RCA indices toward a relatively high level, which is commensurate\nwith our numerical simulations, and indirectly enhances provincial export\ncompetitiveness.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5efa\u6a21\u548c\u5b9e\u8bc1\u5206\u6790\u53d1\u73b0\uff0c\u4eba\u6c11\u5e01\u8d2c\u503c\u4f1a\u63d0\u5347\u4e2d\u56fd\u5236\u9020\u4e1a\u7684\u663e\u6027\u6bd4\u8f83\u4f18\u52bf\u6307\u6570(RCA)\uff0c\u800c\u6c47\u7387\u653f\u7b56\u7a33\u5b9a\u540eRCA\u6307\u6570\u4f1a\u5411\u8f83\u9ad8\u6c34\u5e73\u6536\u655b\uff0c\u95f4\u63a5\u589e\u5f3a\u51fa\u53e3\u7ade\u4e89\u529b\u3002", "motivation": "\u7814\u7a76\u53cc\u8fb9\u6c47\u7387\u6ce2\u52a8\u5982\u4f55\u5f71\u54cd\u663e\u6027\u6bd4\u8f83\u4f18\u52bf\u6307\u6570(RCA)\uff0c\u7279\u522b\u662f\u5206\u6790\u8d27\u5e01\u653f\u7b56\u51b2\u51fb\u5bf9USD-CNY\u6c47\u7387\u6ce2\u52a8\u4e0e\u51fa\u53e3\u7ade\u4e89\u529b\u5173\u7cfb\u7684\u5f71\u54cd\u3002", "method": "\u5c06USD-CNY\u53cc\u8fb9\u6c47\u7387\u6ce2\u52a8\u5efa\u6a21\u4e3a\u4e00\u822c\u968f\u673a\u8fc7\u7a0b\uff0c\u5e76\u7eb3\u5165\u8d27\u5e01\u653f\u7b56\u51b2\u51fb\uff1b\u4f7f\u75282008-2021\u5e74USD-CNY\u6c47\u7387\u548c\u4e2d\u56fd\u7701\u7ea7\u5236\u9020\u4e1a\u51fa\u53e3\u7ade\u4e89\u529b\u6570\u636e\u8fdb\u884c\u5b9e\u8bc1\u5206\u6790\u3002", "result": "\u6570\u503c\u6a21\u62df\u663e\u793a\u6c47\u7387\u8d2c\u503c\u5747\u503c\u589e\u52a0\u4f1a\u63d0\u5347RCA\u6307\u6570\uff1b\u5b9e\u8bc1\u53d1\u73b0\u77ed\u671f\u4eba\u6c11\u5e01\u8d2c\u503c\u5c0f\u4e8e0.2\u5143\u65f6\u80fd\u6709\u6548\u63d0\u5347\u51fa\u53e3\u7ade\u4e89\u529b\uff0c8.11\u6c47\u7387\u653f\u7b56\u4f7fRCA\u6307\u6570\u5411\u8f83\u9ad8\u6c34\u5e73\u6536\u655b\u3002", "conclusion": "\u6c47\u7387\u6ce2\u52a8\u7a33\u5b9a\u6709\u52a9\u4e8eRCA\u6307\u6570\u5411\u8f83\u9ad8\u6c34\u5e73\u6536\u655b\uff0c\u8d27\u5e01\u653f\u7b56\u901a\u8fc7\u7a33\u5b9a\u6c47\u7387\u95f4\u63a5\u589e\u5f3a\u4e86\u7701\u7ea7\u51fa\u53e3\u7ade\u4e89\u529b\uff0c\u4e0e\u6570\u503c\u6a21\u62df\u7ed3\u679c\u4e00\u81f4\u3002"}}
{"id": "2509.14508", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2509.14508", "abs": "https://arxiv.org/abs/2509.14508", "authors": ["Akira Matsui", "Takashi Teramoto", "Eiji Motohashi", "Hiroyuki Tsurumi"], "title": "Modeling User Redemption Behavior in Complex Incentive Digital Environment: An Empirical Study Using Large-Scale Transactional Data", "comment": null, "summary": "The digital economy implements complex incentive systems to retain users\nthrough point redemption. Understanding user behavior in such complex incentive\nstructures presents a fundamental challenge, especially in estimating the value\nof these digital assets against traditional money. This study tackles this\nquestion by analyzing large-scale, real-world transaction data from a popular\npersonal finance application that captures both monetary spending and\npoint-based transactions across Japan's deeply integrated loyalty networks. We\nfind that point usage is not random but is systematically linked to\ndemographics, with older users tending to convert points into financial assets.\nFurthermore, our analysis using a natural experiment and a causal inference\ntechnique reveals that a large point grant stimulated an increase in point\nspending without affecting cash expenditure. We also find that consumers'\nshopping styles are associated with their point redemption patterns. This\nstudy, conducted within a massive real-world economic ecosystem, examines how\nconsumers navigate multi-currency environments, with direct implications for\nmodeling economic behavior and designing digital platforms.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5206\u6790\u65e5\u672c\u4e2a\u4eba\u7406\u8d22\u5e94\u7528\u7684\u5927\u89c4\u6a21\u771f\u5b9e\u4ea4\u6613\u6570\u636e\uff0c\u63a2\u8ba8\u4e86\u6570\u5b57\u79ef\u5206\u5728\u590d\u6742\u6fc0\u52b1\u7cfb\u7edf\u4e2d\u7684\u7528\u6237\u884c\u4e3a\u6a21\u5f0f\uff0c\u53d1\u73b0\u79ef\u5206\u4f7f\u7528\u4e0e\u4eba\u53e3\u7edf\u8ba1\u7279\u5f81\u76f8\u5173\uff0c\u4e14\u5927\u989d\u79ef\u5206\u8d60\u4e0e\u4f1a\u523a\u6fc0\u79ef\u5206\u6d88\u8d39\u800c\u4e0d\u5f71\u54cd\u73b0\u91d1\u652f\u51fa\u3002", "motivation": "\u6570\u5b57\u7ecf\u6d4e\u7684\u590d\u6742\u6fc0\u52b1\u7cfb\u7edf\u901a\u8fc7\u79ef\u5206\u5151\u6362\u6765\u7559\u4f4f\u7528\u6237\uff0c\u4f46\u7406\u89e3\u7528\u6237\u5728\u8fd9\u79cd\u590d\u6742\u6fc0\u52b1\u7ed3\u6784\u4e2d\u7684\u884c\u4e3a\uff0c\u7279\u522b\u662f\u4f30\u7b97\u6570\u5b57\u8d44\u4ea7\u76f8\u5bf9\u4e8e\u4f20\u7edf\u8d27\u5e01\u7684\u4ef7\u503c\uff0c\u662f\u4e00\u4e2a\u6839\u672c\u6027\u6311\u6218\u3002", "method": "\u5206\u6790\u6765\u81ea\u65e5\u672c\u6d41\u884c\u4e2a\u4eba\u7406\u8d22\u5e94\u7528\u7684\u5927\u89c4\u6a21\u771f\u5b9e\u4ea4\u6613\u6570\u636e\uff0c\u6db5\u76d6\u8d27\u5e01\u652f\u51fa\u548c\u79ef\u5206\u4ea4\u6613\uff1b\u4f7f\u7528\u81ea\u7136\u5b9e\u9a8c\u548c\u56e0\u679c\u63a8\u65ad\u6280\u672f\u6765\u5206\u6790\u79ef\u5206\u8d60\u4e0e\u4f1a\u7684\u5f71\u54cd\u3002", "result": "\u79ef\u5206\u4f7f\u7528\u4e0d\u662f\u968f\u673a\u7684\uff0c\u800c\u662f\u4e0e\u4eba\u53e3\u7edf\u8ba1\u7279\u5f81\u7cfb\u7edf\u76f8\u5173\uff0c\u5e74\u957f\u7528\u6237\u503e\u5411\u4e8e\u5c06\u79ef\u5206\u8f6c\u6362\u4e3a\u91d1\u878d\u8d44\u4ea7\uff1b\u5927\u989d\u79ef\u5206\u8d60\u4e0e\u4f1a\u523a\u6fc0\u79ef\u5206\u6d88\u8d39\u589e\u52a0\uff0c\u4f46\u4e0d\u5f71\u54cd\u73b0\u91d1\u652f\u51fa\uff1b\u6d88\u8d39\u8005\u7684\u8d2d\u7269\u98ce\u683c\u4e0e\u5176\u79ef\u5206\u5151\u6362\u6a21\u5f0f\u76f8\u5173\u3002", "conclusion": "\u8fd9\u9879\u5728\u771f\u5b9e\u7ecf\u6d4e\u751f\u6001\u7cfb\u7edf\u4e2d\u8fdb\u884c\u7684\u7814\u7a76\u63ed\u793a\u4e86\u6d88\u8d39\u8005\u5982\u4f55\u5728\u591a\u8d27\u5e01\u73af\u5883\u4e2d\u5bfc\u822a\uff0c\u5bf9\u7ecf\u6d4e\u884c\u4e3a\u5efa\u6a21\u548c\u6570\u5b57\u5e73\u53f0\u8bbe\u8ba1\u5177\u6709\u76f4\u63a5\u610f\u4e49\u3002"}}
{"id": "2509.14343", "categories": ["eess.SY", "cs.AI", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.14343", "abs": "https://arxiv.org/abs/2509.14343", "authors": ["Peihao Yan", "Jie Lu", "Huacheng Zeng", "Y. Thomas Hou"], "title": "Near-Real-Time Resource Slicing for QoS Optimization in 5G O-RAN using Deep Reinforcement Learning", "comment": null, "summary": "Open-Radio Access Network (O-RAN) has become an important paradigm for 5G and\nbeyond radio access networks. This paper presents an xApp called xSlice for the\nNear-Real-Time (Near-RT) RAN Intelligent Controller (RIC) of 5G O-RANs. xSlice\nis an online learning algorithm that adaptively adjusts MAC-layer resource\nallocation in response to dynamic network states, including time-varying\nwireless channel conditions, user mobility, traffic fluctuations, and changes\nin user demand. To address these network dynamics, we first formulate the\nQuality-of-Service (QoS) optimization problem as a regret minimization problem\nby quantifying the QoS demands of all traffic sessions through weighting their\nthroughput, latency, and reliability. We then develop a deep reinforcement\nlearning (DRL) framework that utilizes an actor-critic model to combine the\nadvantages of both value-based and policy-based updating methods. A graph\nconvolutional network (GCN) is incorporated as a component of the DRL framework\nfor graph embedding of RAN data, enabling xSlice to handle a dynamic number of\ntraffic sessions. We have implemented xSlice on an O-RAN testbed with 10\nsmartphones and conducted extensive experiments to evaluate its performance in\nrealistic scenarios. Experimental results show that xSlice can reduce\nperformance regret by 67% compared to the state-of-the-art solutions. Source\ncode is available on GitHub [1].", "AI": {"tldr": "xSlice\u662f\u4e00\u4e2a\u7528\u4e8e5G O-RAN\u8fd1\u5b9e\u65f6RAN\u667a\u80fd\u63a7\u5236\u5668\u7684xApp\uff0c\u91c7\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u81ea\u9002\u5e94\u8c03\u6574MAC\u5c42\u8d44\u6e90\u5206\u914d\uff0c\u5728\u52a8\u6001\u7f51\u7edc\u73af\u5883\u4e0b\u76f8\u6bd4\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u51cf\u5c1167%\u7684\u6027\u80fd\u9057\u61be\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b35G O-RAN\u7f51\u7edc\u4e2d\u52a8\u6001\u53d8\u5316\u7684\u65e0\u7ebf\u4fe1\u9053\u6761\u4ef6\u3001\u7528\u6237\u79fb\u52a8\u6027\u3001\u6d41\u91cf\u6ce2\u52a8\u548c\u7528\u6237\u9700\u6c42\u53d8\u5316\u7b49\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u81ea\u9002\u5e94\u8c03\u6574\u8d44\u6e90\u5206\u914d\u7684\u667a\u80fd\u7b97\u6cd5\u3002", "method": "\u5c06QoS\u4f18\u5316\u95ee\u9898\u5efa\u6a21\u4e3a\u9057\u61be\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u5f00\u53d1\u57fa\u4e8eactor-critic\u6a21\u578b\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u56fe\u5377\u79ef\u7f51\u7edc\u5904\u7406\u52a8\u6001\u6570\u91cf\u7684\u6d41\u91cf\u4f1a\u8bdd\uff0c\u5b9e\u73b0MAC\u5c42\u8d44\u6e90\u5206\u914d\u7684\u81ea\u9002\u5e94\u8c03\u6574\u3002", "result": "\u5728\u5305\u542b10\u90e8\u667a\u80fd\u624b\u673a\u7684O-RAN\u6d4b\u8bd5\u5e8a\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793axSlice\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u89e3\u51b3\u65b9\u6848\u80fd\u591f\u51cf\u5c1167%\u7684\u6027\u80fd\u9057\u61be\u3002", "conclusion": "xSlice\u901a\u8fc7\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u548c\u56fe\u5377\u79ef\u7f51\u7edc\u7684\u7ed3\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86O-RAN\u7f51\u7edc\u4e2d\u7684\u52a8\u6001\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f51\u7edc\u6027\u80fd\u3002"}}
{"id": "2509.14645", "categories": ["econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2509.14645", "abs": "https://arxiv.org/abs/2509.14645", "authors": ["Hiroaki Hanyu", "Shunsuke Ishii", "Suguru Otani", "Kazuhiro Teramoto"], "title": "Are Final Market Prices Sufficient for Information Aggregation? Evidence from Last-Minute Dynamics in Parimutuel Betting", "comment": "18 pages with 8 pages appendix", "summary": "Most betting market models employ static frameworks that condition decisions\non final odds. Using a unique dataset of interim odds from Japanese horse\nracing, this study examines the validity of such static analyses by asking\nwhether there is a systematic relationship between expected returns and the\ntrajectory of odds. We find that returns are negatively related to last-minute\nchanges in odds, and that these late movements attenuate the favorite-longshot\nbias by weakening the correlation between final odds and returns. These\npatterns suggest that informed bettors place wagers at the final stage based on\nprivate signals, leaving surprises in final odds.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528\u65e5\u672c\u8d5b\u9a6c\u4e34\u65f6\u8d54\u7387\u6570\u636e\uff0c\u53d1\u73b0\u9759\u6001\u6295\u6ce8\u5e02\u573a\u6a21\u578b\u5b58\u5728\u5c40\u9650\u6027\uff0c\u56de\u62a5\u7387\u4e0e\u8d54\u7387\u53d8\u5316\u8f68\u8ff9\u5448\u7cfb\u7edf\u6027\u8d1f\u76f8\u5173\u5173\u7cfb", "motivation": "\u68c0\u9a8c\u9759\u6001\u6295\u6ce8\u5e02\u573a\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u63a2\u8ba8\u9884\u671f\u56de\u62a5\u4e0e\u8d54\u7387\u53d8\u5316\u8f68\u8ff9\u4e4b\u95f4\u662f\u5426\u5b58\u5728\u7cfb\u7edf\u6027\u5173\u7cfb", "method": "\u4f7f\u7528\u65e5\u672c\u8d5b\u9a6c\u6bd4\u8d5b\u7684\u72ec\u7279\u4e34\u65f6\u8d54\u7387\u6570\u636e\u96c6\u8fdb\u884c\u5206\u6790", "result": "\u53d1\u73b0\u56de\u62a5\u7387\u4e0e\u6700\u540e\u65f6\u523b\u8d54\u7387\u53d8\u5316\u5448\u8d1f\u76f8\u5173\uff0c\u8fd9\u4e9b\u540e\u671f\u53d8\u52a8\u901a\u8fc7\u51cf\u5f31\u6700\u7ec8\u8d54\u7387\u4e0e\u56de\u62a5\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u6765\u51cf\u5f31\u70ed\u95e8-\u51b7\u95e8\u504f\u5dee", "conclusion": "\u6709\u4fe1\u606f\u7684\u6295\u6ce8\u8005\u57fa\u4e8e\u79c1\u4eba\u4fe1\u53f7\u5728\u6700\u540e\u9636\u6bb5\u4e0b\u6ce8\uff0c\u5bfc\u81f4\u6700\u7ec8\u8d54\u7387\u4e2d\u51fa\u73b0\u610f\u5916\u53d8\u5316\uff0c\u8fd9\u8868\u660e\u9759\u6001\u5206\u6790\u6846\u67b6\u9700\u8981\u91cd\u65b0\u8003\u8651"}}
{"id": "2509.14303", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14303", "abs": "https://arxiv.org/abs/2509.14303", "authors": ["Hao Jiang", "Zhipeng Zhang", "Yu Gao", "Zhigang Sun", "Yiru Wang", "Yuwen Heng", "Shuo Wang", "Jinhao Chai", "Zhuo Chen", "Hao Zhao", "Hao Sun", "Xi Zhang", "Anqing Jiang", "Chuan Hu"], "title": "FlowDrive: Energy Flow Field for End-to-End Autonomous Driving", "comment": null, "summary": "Recent advances in end-to-end autonomous driving leverage multi-view images\nto construct BEV representations for motion planning. In motion planning,\nautonomous vehicles need considering both hard constraints imposed by\ngeometrically occupied obstacles (e.g., vehicles, pedestrians) and soft,\nrule-based semantics with no explicit geometry (e.g., lane boundaries, traffic\npriors). However, existing end-to-end frameworks typically rely on BEV features\nlearned in an implicit manner, lacking explicit modeling of risk and guidance\npriors for safe and interpretable planning. To address this, we propose\nFlowDrive, a novel framework that introduces physically interpretable\nenergy-based flow fields-including risk potential and lane attraction fields-to\nencode semantic priors and safety cues into the BEV space. These flow-aware\nfeatures enable adaptive refinement of anchor trajectories and serve as\ninterpretable guidance for trajectory generation. Moreover, FlowDrive decouples\nmotion intent prediction from trajectory denoising via a conditional diffusion\nplanner with feature-level gating, alleviating task interference and enhancing\nmultimodal diversity. Experiments on the NAVSIM v2 benchmark demonstrate that\nFlowDrive achieves state-of-the-art performance with an EPDMS of 86.3,\nsurpassing prior baselines in both safety and planning quality. The project is\navailable at https://astrixdrive.github.io/FlowDrive.github.io/.", "AI": {"tldr": "FlowDrive\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u7269\u7406\u53ef\u89e3\u91ca\u7684\u80fd\u91cf\u573a\uff08\u98ce\u9669\u52bf\u573a\u548c\u8f66\u9053\u5438\u5f15\u573a\uff09\u6765\u589e\u5f3aBEV\u8868\u793a\uff0c\u5b9e\u73b0\u5b89\u5168\u53ef\u89e3\u91ca\u7684\u8fd0\u52a8\u89c4\u5212\u3002", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\u5728BEV\u7279\u5f81\u5b66\u4e60\u4e2d\u7f3a\u4e4f\u5bf9\u98ce\u9669\u548c\u5f15\u5bfc\u5148\u9a8c\u7684\u663e\u5f0f\u5efa\u6a21\uff0c\u65e0\u6cd5\u540c\u65f6\u5904\u7406\u51e0\u4f55\u969c\u788d\u7269\u786c\u7ea6\u675f\u548c\u65e0\u663e\u5f0f\u51e0\u4f55\u7684\u8bed\u4e49\u8f6f\u7ea6\u675f\uff0c\u9700\u8981\u66f4\u5b89\u5168\u53ef\u89e3\u91ca\u7684\u89c4\u5212\u65b9\u6cd5\u3002", "method": "\u63d0\u51faFlowDrive\u6846\u67b6\uff1a1\uff09\u5f15\u5165\u7269\u7406\u53ef\u89e3\u91ca\u7684\u80fd\u91cf\u573a\uff08\u98ce\u9669\u52bf\u573a\u548c\u8f66\u9053\u5438\u5f15\u573a\uff09\u7f16\u7801\u8bed\u4e49\u5148\u9a8c\u548c\u5b89\u5168\u63d0\u793a\u5230BEV\u7a7a\u95f4\uff1b2\uff09\u4f7f\u7528\u6761\u4ef6\u6269\u6563\u89c4\u5212\u5668\u901a\u8fc7\u7279\u5f81\u7ea7\u95e8\u63a7\u89e3\u8026\u8fd0\u52a8\u610f\u56fe\u9884\u6d4b\u548c\u8f68\u8ff9\u53bb\u566a\u3002", "result": "\u5728NAVSIM v2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523086.3\u7684EPDMS\u5206\u6570\uff0c\u5728\u5b89\u5168\u6027\u548c\u89c4\u5212\u8d28\u91cf\u65b9\u9762\u5747\u8d85\u8d8a\u5148\u524d\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "FlowDrive\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u98ce\u9669\u573a\u548c\u5f15\u5bfc\u573a\uff0c\u63d0\u4f9b\u4e86\u7269\u7406\u53ef\u89e3\u91ca\u7684\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u65b9\u6848\uff0c\u6709\u6548\u63d0\u5347\u4e86\u89c4\u5212\u7684\u5b89\u5168\u6027\u548c\u591a\u6837\u6027\uff0c\u4e3a\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.14289", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14289", "abs": "https://arxiv.org/abs/2509.14289", "authors": ["Lanxiao Huang", "Daksh Dave", "Ming Jin", "Tyler Cody", "Peter Beling"], "title": "From Capabilities to Performance: Evaluating Key Functional Properties of LLM Architectures in Penetration Testing", "comment": null, "summary": "Large language models (LLMs) are increasingly used to automate or augment\npenetration testing, but their effectiveness and reliability across attack\nphases remain unclear. We present a comprehensive evaluation of multiple\nLLM-based agents, from single-agent to modular designs, across realistic\npenetration testing scenarios, measuring empirical performance and recurring\nfailure patterns. We also isolate the impact of five core functional\ncapabilities via targeted augmentations: Global Context Memory (GCM),\nInter-Agent Messaging (IAM), Context-Conditioned Invocation (CCI), Adaptive\nPlanning (AP), and Real-Time Monitoring (RTM). These interventions support,\nrespectively: (i) context coherence and retention, (ii) inter-component\ncoordination and state management, (iii) tool use accuracy and selective\nexecution, (iv) multi-step strategic planning, error detection, and recovery,\nand (v) real-time dynamic responsiveness. Our results show that while some\narchitectures natively exhibit subsets of these properties, targeted\naugmentations substantially improve modular agent performance, especially in\ncomplex, multi-step, and real-time penetration testing tasks.", "AI": {"tldr": "\u672c\u6587\u5bf9\u591a\u79cdLLM\u9a71\u52a8\u7684\u6e17\u900f\u6d4b\u8bd5\u4ee3\u7406\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u67b6\u6784\u5728\u771f\u5b9e\u6e17\u900f\u6d4b\u8bd5\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u901a\u8fc7\u4e94\u79cd\u6838\u5fc3\u529f\u80fd\u589e\u5f3a\u6765\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6e17\u900f\u6d4b\u8bd5\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u5728\u4e0d\u540c\u653b\u51fb\u9636\u6bb5\u7684\u6709\u6548\u6027\u548c\u53ef\u9760\u6027\u4ecd\u4e0d\u660e\u786e\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u8bc4\u4f30\u548c\u6539\u8fdb\u3002", "method": "\u8bc4\u4f30\u4e86\u4ece\u5355\u4ee3\u7406\u5230\u6a21\u5757\u5316\u8bbe\u8ba1\u7684\u591a\u79cdLLM\u4ee3\u7406\u67b6\u6784\uff0c\u5728\u771f\u5b9e\u6e17\u900f\u6d4b\u8bd5\u573a\u666f\u4e2d\u6d4b\u91cf\u6027\u80fd\u8868\u73b0\uff0c\u5e76\u901a\u8fc7\u4e94\u79cd\u9488\u5bf9\u6027\u589e\u5f3a\uff08GCM\u3001IAM\u3001CCI\u3001AP\u3001RTM\uff09\u6765\u63d0\u5347\u6838\u5fc3\u529f\u80fd\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u867d\u7136\u67d0\u4e9b\u67b6\u6784\u5929\u7136\u5177\u5907\u90e8\u5206\u529f\u80fd\u7279\u6027\uff0c\u4f46\u9488\u5bf9\u6027\u589e\u5f3a\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u5757\u5316\u4ee3\u7406\u5728\u590d\u6742\u3001\u591a\u6b65\u9aa4\u548c\u5b9e\u65f6\u6e17\u900f\u6d4b\u8bd5\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u6838\u5fc3\u529f\u80fd\u589e\u5f3a\u53ef\u4ee5\u663e\u8457\u63d0\u5347LLM\u4ee3\u7406\u5728\u6e17\u900f\u6d4b\u8bd5\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u548c\u5b9e\u65f6\u4efb\u52a1\u4e2d\uff0c\u4e3a\u81ea\u52a8\u5316\u6e17\u900f\u6d4b\u8bd5\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002"}}
{"id": "2509.14732", "categories": ["econ.TH"], "pdf": "https://arxiv.org/pdf/2509.14732", "abs": "https://arxiv.org/abs/2509.14732", "authors": ["Gregorio Curello", "Ludvig Sinander", "Mark Whitmeyer"], "title": "Outside options and risk attitude", "comment": null, "summary": "We uncover a close link between outside options and risk attitude: when a\ndecision-maker gains access to an outside option, her behaviour becomes less\nrisk-averse, and conversely, any observed decrease of risk-aversion can be\nexplained by an outside option having been made available. We characterise the\ncomparative statics of risk-aversion, delineating how effective risk attitude\n(i.e. actual choice among risky prospects) varies with the outside option and\nwith the decision-maker's 'true' risk attitude. We prove that outside options\nare special: among transformations of a decision problem, those that amount to\nadding an outside option are the only ones that always reduce risk-aversion.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5916\u90e8\u9009\u62e9\u6743\u4e0e\u98ce\u9669\u6001\u5ea6\u5bc6\u5207\u76f8\u5173\uff1a\u83b7\u5f97\u5916\u90e8\u9009\u62e9\u6743\u4f1a\u964d\u4f4e\u51b3\u7b56\u8005\u7684\u98ce\u9669\u538c\u6076\u7a0b\u5ea6\uff0c\u4efb\u4f55\u89c2\u5bdf\u5230\u7684\u98ce\u9669\u538c\u6076\u51cf\u5c11\u90fd\u53ef\u4ee5\u7528\u83b7\u5f97\u5916\u90e8\u9009\u62e9\u6743\u6765\u89e3\u91ca\u3002", "motivation": "\u63a2\u7d22\u5916\u90e8\u9009\u62e9\u6743\u5982\u4f55\u5f71\u54cd\u51b3\u7b56\u8005\u7684\u98ce\u9669\u6001\u5ea6\uff0c\u4ee5\u53ca\u98ce\u9669\u538c\u6076\u53d8\u5316\u4e0e\u5916\u90e8\u9009\u62e9\u6743\u4e4b\u95f4\u7684\u5185\u5728\u8054\u7cfb\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u9759\u6001\u5206\u6790\u65b9\u6cd5\uff0c\u7814\u7a76\u98ce\u9669\u6001\u5ea6\u7684\u53d8\u5316\u89c4\u5f8b\uff0c\u5206\u6790\u6709\u6548\u98ce\u9669\u6001\u5ea6\u5982\u4f55\u968f\u5916\u90e8\u9009\u62e9\u6743\u548c\u51b3\u7b56\u8005\u771f\u5b9e\u98ce\u9669\u6001\u5ea6\u800c\u53d8\u5316\u3002", "result": "\u8bc1\u660e\u5916\u90e8\u9009\u62e9\u6743\u5177\u6709\u7279\u6b8a\u6027\uff1a\u5728\u6240\u6709\u51b3\u7b56\u95ee\u9898\u7684\u53d8\u6362\u4e2d\uff0c\u53ea\u6709\u6dfb\u52a0\u5916\u90e8\u9009\u62e9\u6743\u7684\u53d8\u6362\u80fd\u591f\u59cb\u7ec8\u964d\u4f4e\u98ce\u9669\u538c\u6076\u7a0b\u5ea6\u3002", "conclusion": "\u5916\u90e8\u9009\u62e9\u6743\u4e0e\u98ce\u9669\u6001\u5ea6\u5b58\u5728\u7d27\u5bc6\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u83b7\u5f97\u5916\u90e8\u9009\u62e9\u6743\u662f\u964d\u4f4e\u98ce\u9669\u538c\u6076\u7684\u552f\u4e00\u786e\u5b9a\u6027\u65b9\u5f0f\u3002"}}
{"id": "2509.14532", "categories": ["cs.CY", "cs.AI", "econ.GN", "q-fin.EC", "J.1; K.4.3; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.14532", "abs": "https://arxiv.org/abs/2509.14532", "authors": ["Oluwatosin Agbaakin"], "title": "Leveraging Artificial Intelligence as a Strategic Growth Catalyst for Small and Medium-sized Enterprises", "comment": "14 pages, 2 figures. A review and strategic framework for AI adoption\n  in SMEs", "summary": "Artificial Intelligence (AI) has transitioned from a futuristic concept\nreserved for large corporations to a present-day, accessible, and essential\ngrowth lever for Small and Medium-sized Enterprises (SMEs). For entrepreneurs\nand business leaders, strategic AI adoption is no longer an option but an\nimperative for competitiveness, operational efficiency, and long-term survival.\nThis report provides a comprehensive framework for SME leaders to navigate this\ntechnological shift, offering the foundational knowledge, business case,\npractical applications, and strategic guidance necessary to harness the power\nof AI. The quantitative evidence supporting AI adoption is compelling; 91% of\nSMEs using AI report that it directly boosts their revenue. Beyond top-line\ngrowth, AI drives profound operational efficiencies, with studies showing it\ncan reduce operational costs by up to 30% and save businesses more than 20\nhours of valuable time each month. This transformation is occurring within the\ncontext of a seismic economic shift; the global AI market is projected to surge\nfrom $233.46 Billion in 2024 to an astonishing $1.77 Trillion by 2032. This\npaper demystifies the core concepts of AI, presents a business case based on\nmarket data, details practical applications, and lays out a phased, actionable\nadoption strategy.", "AI": {"tldr": "AI\u5df2\u6210\u4e3a\u4e2d\u5c0f\u4f01\u4e1a\u751f\u5b58\u548c\u53d1\u5c55\u7684\u5173\u952e\u5de5\u5177\uff0c91%\u4f7f\u7528AI\u7684\u4e2d\u5c0f\u4f01\u4e1a\u62a5\u544a\u6536\u5165\u589e\u957f\uff0cAI\u53ef\u964d\u4f4e30%\u8fd0\u8425\u6210\u672c\u5e76\u6bcf\u6708\u8282\u770120\u5c0f\u65f6\u5de5\u4f5c\u65f6\u95f4\u3002", "motivation": "AI\u6280\u672f\u5df2\u4ece\u5927\u4f01\u4e1a\u4e13\u5c5e\u8f6c\u53d8\u4e3a\u4e2d\u5c0f\u4f01\u4e1a\u53ef\u53ca\u7684\u589e\u957f\u6760\u6746\uff0c\u6218\u7565\u6027AI\u91c7\u7528\u5bf9\u4e2d\u5c0f\u4f01\u4e1a\u7ade\u4e89\u529b\u3001\u8fd0\u8425\u6548\u7387\u548c\u957f\u671f\u751f\u5b58\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u4f9b\u5168\u9762\u7684\u6846\u67b6\uff0c\u5305\u62ec\u57fa\u7840\u77e5\u8bc6\u3001\u5546\u4e1a\u6848\u4f8b\u3001\u5b9e\u9645\u5e94\u7528\u548c\u5206\u9636\u6bb5\u53ef\u64cd\u4f5c\u7684\u91c7\u7528\u7b56\u7565\uff0c\u57fa\u4e8e\u5e02\u573a\u6570\u636e\u8fdb\u884c\u5546\u4e1a\u6848\u4f8b\u5206\u6790\u3002", "result": "\u5168\u7403AI\u5e02\u573a\u9884\u8ba1\u4ece2024\u5e74\u76842334.6\u4ebf\u7f8e\u5143\u589e\u957f\u52302032\u5e74\u76841.77\u4e07\u4ebf\u7f8e\u5143\uff0cAI\u4e3a\u4e2d\u5c0f\u4f01\u4e1a\u5e26\u6765\u663e\u8457\u7684\u6536\u5165\u589e\u957f\u548c\u8fd0\u8425\u6548\u7387\u63d0\u5347\u3002", "conclusion": "AI\u662f\u4e2d\u5c0f\u4f01\u4e1a\u5fc5\u987b\u91c7\u7528\u7684\u6280\u672f\uff0c\u672c\u6587\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6307\u5bfc\u6846\u67b6\u5e2e\u52a9\u4e2d\u5c0f\u4f01\u4e1a\u9886\u5bfc\u8005\u6210\u529f\u5b9e\u65bdAI\u8f6c\u578b\u3002"}}
{"id": "2509.14376", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.14376", "abs": "https://arxiv.org/abs/2509.14376", "authors": ["Kamal Fenza", "Moussa Labbadi", "Mohamed Ouzahra"], "title": "On Finite- and Fixed-Time Stabilization of Abstract Nonlinear Systems with Well-Posedness Guarantees", "comment": null, "summary": "This paper addresses the problem of stabilization for infinite-dimensional\nsystems. In particular, we design nonlinear stabilizers for both linear and\nnonlinear abstract systems. We focus on two classes of systems: the first class\ncomprises linear abstract systems subject to matched perturbations, while the\nsecond class encompasses fully nonlinear abstract systems. Our main objective\nis to synthesize state-feedback controllers that guarantee finite- or\nfixed-time stability of the closed-loop system, along with possible estimation\nof the settling time. For the first class, the presence of persistent\nperturbations introduces significant challenges in the well-posedness analysis,\nparticularly due to the discontinuous nature of the control law. To address\nthis, we employ maximal monotone operator theory to rigorously establish the\nexistence and uniqueness of solutions, extending classical results from\ncontinuous abstract systems. For the second class, which includes\nnonlinearities, we further show that the proposed feedback law ensures\nfixed-time stability and well-posedness of the closed-loop system, again using\nmaximal monotone theory. The results provide a unified framework for robust,\nfinite /fixed-time stabilization in the presence of discontinuities and\nnonlinearities in infinite-dimensional settings.", "AI": {"tldr": "\u672c\u6587\u4e3a\u65e0\u9650\u7ef4\u7cfb\u7edf\u8bbe\u8ba1\u975e\u7ebf\u6027\u7a33\u5b9a\u5668\uff0c\u9488\u5bf9\u7ebf\u6027\u5339\u914d\u6270\u52a8\u7cfb\u7edf\u548c\u5b8c\u5168\u975e\u7ebf\u6027\u62bd\u8c61\u7cfb\u7edf\uff0c\u63d0\u51fa\u4fdd\u8bc1\u6709\u9650/\u56fa\u5b9a\u65f6\u95f4\u7a33\u5b9a\u7684\u72b6\u6001\u53cd\u9988\u63a7\u5236\u5668\uff0c\u5e76\u5229\u7528\u6781\u5927\u5355\u8c03\u7b97\u5b50\u7406\u8bba\u89e3\u51b3\u4e0d\u8fde\u7eed\u63a7\u5236\u5f8b\u7684\u9002\u5b9a\u6027\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u65e0\u9650\u7ef4\u7cfb\u7edf\u7684\u7a33\u5b9a\u5316\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5b58\u5728\u5339\u914d\u6270\u52a8\u548c\u975e\u7ebf\u6027\u60c5\u51b5\u4e0b\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u4fdd\u8bc1\u6709\u9650/\u56fa\u5b9a\u65f6\u95f4\u7a33\u5b9a\u6027\uff0c\u9700\u8981\u5efa\u7acb\u7edf\u4e00\u7684\u9c81\u68d2\u7a33\u5b9a\u6846\u67b6\u3002", "method": "\u91c7\u7528\u6781\u5927\u5355\u8c03\u7b97\u5b50\u7406\u8bba\u5206\u6790\u4e0d\u8fde\u7eed\u63a7\u5236\u5f8b\u7684\u9002\u5b9a\u6027\uff0c\u8bbe\u8ba1\u72b6\u6001\u53cd\u9988\u63a7\u5236\u5668\uff0c\u5206\u522b\u5904\u7406\u7ebf\u6027\u5339\u914d\u6270\u52a8\u7cfb\u7edf\u548c\u5b8c\u5168\u975e\u7ebf\u6027\u62bd\u8c61\u7cfb\u7edf\uff0c\u786e\u4fdd\u95ed\u73af\u7cfb\u7edf\u7684\u89e3\u5b58\u5728\u552f\u4e00\u4e14\u7a33\u5b9a\u3002", "result": "\u6210\u529f\u5efa\u7acb\u4e86\u65e0\u9650\u7ef4\u7cfb\u7edf\u4e2d\u5b58\u5728\u4e0d\u8fde\u7eed\u6027\u548c\u975e\u7ebf\u6027\u65f6\u7684\u9c81\u68d2\u6709\u9650/\u56fa\u5b9a\u65f6\u95f4\u7a33\u5b9a\u7edf\u4e00\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u6270\u52a8\u5e26\u6765\u7684\u9002\u5b9a\u6027\u6311\u6218\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u65e0\u9650\u7ef4\u7cfb\u7edf\u7684\u6709\u9650/\u56fa\u5b9a\u65f6\u95f4\u7a33\u5b9a\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u6269\u5c55\u4e86\u7ecf\u5178\u8fde\u7eed\u62bd\u8c61\u7cfb\u7edf\u7ed3\u679c\uff0c\u9002\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u5de5\u7a0b\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2509.14795", "categories": ["econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2509.14795", "abs": "https://arxiv.org/abs/2509.14795", "authors": ["Timo Kuosmanen", "Xun Zhou"], "title": "Paradoxes of the public sector productivity measurement", "comment": null, "summary": "This paper critically investigates standard total factor productivity (TFP)\nmeasurement in the public sector, where output information is often incomplete\nor distorted. The analysis reveals fundamental paradoxes under three common\noutput measurement conventions. When cost-based value added is used as the\naggregate output, measured TFP may paradoxically decline as a result of genuine\nproductivity-enhancing changes such as technical progress and improved\nallocative and scale efficiencies, as well as reductions in real input prices.\nWe show that the same problems carry over to the situation where the aggregate\noutput is constructed as the cost-share weighted index of outputs. In the case\nof distorted output prices, measured TFP may move independently of any\nproductivity changes and instead reflect shifts in pricing mechanisms. Using\nempirical illustrations from the United Kingdom and Finland, we demonstrate\nthat such distortions are not merely theoretical but are embedded in widely\nused public productivity statistics. We argue that public sector TFP\nmeasurement requires a shift away from cost-based aggregation of outputs and\ntoward non-market valuation methods grounded in economic theory.", "AI": {"tldr": "\u672c\u6587\u6279\u5224\u6027\u5206\u6790\u4e86\u516c\u5171\u90e8\u95e8\u5168\u8981\u7d20\u751f\u4ea7\u7387\uff08TFP\uff09\u6d4b\u91cf\u7684\u6807\u51c6\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u5728\u4e09\u79cd\u5e38\u89c1\u4ea7\u51fa\u6d4b\u91cf\u60ef\u4f8b\u4e0b\u5b58\u5728\u7684\u57fa\u672c\u6096\u8bba\uff0c\u6307\u51fa\u6210\u672c\u57fa\u7840\u7684\u4ea7\u51fa\u6d4b\u91cf\u4f1a\u5bfc\u81f4TFP\u6d4b\u91cf\u5931\u771f\uff0c\u751a\u81f3\u51fa\u73b0\u751f\u4ea7\u7387\u63d0\u5347\u53cd\u800c\u5bfc\u81f4\u6d4b\u91cfTFP\u4e0b\u964d\u7684\u6096\u8bba\u3002", "motivation": "\u516c\u5171\u90e8\u95e8\u7684\u4ea7\u51fa\u4fe1\u606f\u901a\u5e38\u4e0d\u5b8c\u6574\u6216\u88ab\u626d\u66f2\uff0c\u4f20\u7edf\u7684TFP\u6d4b\u91cf\u65b9\u6cd5\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u4f1a\u4ea7\u751f\u8bef\u5bfc\u6027\u7ed3\u679c\uff0c\u9700\u8981\u91cd\u65b0\u5ba1\u89c6\u548c\u6539\u8fdb\u516c\u5171\u90e8\u95e8\u751f\u4ea7\u7387\u7684\u6d4b\u91cf\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u63ed\u793a\u4e09\u79cd\u5e38\u89c1\u4ea7\u51fa\u6d4b\u91cf\u60ef\u4f8b\u4e0b\u7684\u6096\u8bba\uff0c\u5e76\u4f7f\u7528\u82f1\u56fd\u548c\u82ac\u5170\u7684\u5b9e\u8bc1\u6570\u636e\u8fdb\u884c\u8bf4\u660e\uff0c\u8bc1\u660e\u8fd9\u4e9b\u626d\u66f2\u4e0d\u4ec5\u5b58\u5728\u4e8e\u7406\u8bba\u4e0a\uff0c\u800c\u4e14\u5e7f\u6cdb\u5b58\u5728\u4e8e\u5b9e\u9645\u4f7f\u7528\u7684\u516c\u5171\u751f\u4ea7\u7387\u7edf\u8ba1\u4e2d\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09\u4f7f\u7528\u6210\u672c\u57fa\u7840\u589e\u52a0\u503c\u4f5c\u4e3a\u603b\u4ea7\u51fa\u65f6\uff0c\u771f\u6b63\u7684\u751f\u4ea7\u7387\u63d0\u5347\uff08\u6280\u672f\u8fdb\u6b65\u3001\u914d\u7f6e\u6548\u7387\u6539\u5584\u7b49\uff09\u53cd\u800c\u53ef\u80fd\u5bfc\u81f4\u6d4b\u91cfTFP\u4e0b\u964d\uff1b2\uff09\u4ea7\u51fa\u4ef7\u683c\u626d\u66f2\u65f6\uff0c\u6d4b\u91cfTFP\u7684\u53d8\u5316\u53ef\u80fd\u5b8c\u5168\u53cd\u6620\u5b9a\u4ef7\u673a\u5236\u7684\u53d8\u5316\u800c\u975e\u751f\u4ea7\u7387\u53d8\u5316\uff1b3\uff09\u8fd9\u4e9b\u95ee\u9898\u5728\u5b9e\u8bc1\u6570\u636e\u4e2d\u786e\u5b9e\u5b58\u5728\u3002", "conclusion": "\u516c\u5171\u90e8\u95e8TFP\u6d4b\u91cf\u9700\u8981\u4ece\u6210\u672c\u57fa\u7840\u7684\u4ea7\u51fa\u52a0\u603b\u65b9\u6cd5\u8f6c\u5411\u57fa\u4e8e\u7ecf\u6d4e\u7406\u8bba\u7684\u975e\u5e02\u573a\u4f30\u503c\u65b9\u6cd5\uff0c\u4ee5\u907f\u514d\u6d4b\u91cf\u626d\u66f2\u548c\u6096\u8bba\u3002"}}
{"id": "2509.14342", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14342", "abs": "https://arxiv.org/abs/2509.14342", "authors": ["Bikram Pandit", "Aayam Kumar Shrestha", "Alan Fern"], "title": "Multi-Quadruped Cooperative Object Transport: Learning Decentralized Pinch-Lift-Move", "comment": null, "summary": "We study decentralized cooperative transport using teams of N-quadruped\nrobots with arm that must pinch, lift, and move ungraspable objects through\nphysical contact alone. Unlike prior work that relies on rigid mechanical\ncoupling between robots and objects, we address the more challenging setting\nwhere mechanically independent robots must coordinate through contact forces\nalone without any communication or centralized control. To this end, we employ\na hierarchical policy architecture that separates base locomotion from arm\ncontrol, and propose a constellation reward formulation that unifies position\nand orientation tracking to enforce rigid contact behavior. The key insight is\nencouraging robots to behave as if rigidly connected to the object through\ncareful reward design and training curriculum rather than explicit mechanical\nconstraints. Our approach enables coordination through shared policy parameters\nand implicit synchronization cues - scaling to arbitrary team sizes without\nretraining. We show extensive simulation experiments to demonstrate robust\ntransport across 2-10 robots on diverse object geometries and masses, along\nwith sim2real transfer results on lightweight objects.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u7684\u591a\u8db3\u673a\u5668\u4eba\u534f\u4f5c\u8fd0\u8f93\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a5\u89e6\u529b\u800c\u975e\u673a\u68b0\u8fde\u63a5\u5b9e\u73b0\u534f\u8c03\uff0c\u4f7f\u7528\u5206\u5c42\u7b56\u7565\u67b6\u6784\u548c\u661f\u5ea7\u5956\u52b1\u516c\u5f0f\u6765\u6a21\u62df\u521a\u6027\u8fde\u63a5\u884c\u4e3a\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u521a\u6027\u673a\u68b0\u8fde\u63a5\u7684\u5c40\u9650\u6027\uff0c\u63a2\u7d22\u5728\u65e0\u901a\u4fe1\u548c\u96c6\u4e2d\u63a7\u5236\u6761\u4ef6\u4e0b\uff0c\u901a\u8fc7\u7eaf\u63a5\u89e6\u529b\u5b9e\u73b0\u591a\u673a\u5668\u4eba\u534f\u8c03\u8fd0\u8f93\u7684\u6311\u6218\u6027\u573a\u666f\u3002", "method": "\u91c7\u7528\u5206\u5c42\u7b56\u7565\u67b6\u6784\u5206\u79bb\u57fa\u5ea7\u8fd0\u52a8\u4e0e\u624b\u81c2\u63a7\u5236\uff0c\u63d0\u51fa\u661f\u5ea7\u5956\u52b1\u516c\u5f0f\u7edf\u4e00\u4f4d\u7f6e\u548c\u65b9\u5411\u8ddf\u8e2a\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5956\u52b1\u548c\u8bad\u7ec3\u8bfe\u7a0b\u4f7f\u673a\u5668\u4eba\u8868\u73b0\u5f97\u5982\u540c\u521a\u6027\u8fde\u63a5\u3002", "result": "\u5728\u4eff\u771f\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e862-10\u4e2a\u673a\u5668\u4eba\u5728\u4e0d\u540c\u51e0\u4f55\u5f62\u72b6\u548c\u8d28\u91cf\u7269\u4f53\u4e0a\u7684\u9c81\u68d2\u8fd0\u8f93\u80fd\u529b\uff0c\u5e76\u5b9e\u73b0\u4e86\u5bf9\u8f7b\u91cf\u7ea7\u7269\u4f53\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5171\u4eab\u7b56\u7565\u53c2\u6570\u548c\u9690\u5f0f\u540c\u6b65\u7ebf\u7d22\u5b9e\u73b0\u534f\u8c03\uff0c\u80fd\u591f\u6269\u5c55\u5230\u4efb\u610f\u56e2\u961f\u89c4\u6a21\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u4e3a\u53bb\u4e2d\u5fc3\u5316\u534f\u4f5c\u8fd0\u8f93\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.14382", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14382", "abs": "https://arxiv.org/abs/2509.14382", "authors": ["Daniel R\u00f6der", "Akhil Juneja", "Roland Roller", "Sven Schmeier"], "title": "Detecting Pipeline Failures through Fine-Grained Analysis of Web Agents", "comment": null, "summary": "Web agents powered by large language models (LLMs) can autonomously perform\ncomplex, multistep tasks in dynamic web environments. However, current\nevaluations mostly focus on the overall success while overlooking intermediate\nerrors. This limits insight into failure modes and hinders systematic\nimprovement. This work analyzes existing benchmarks and highlights the lack of\nfine-grained diagnostic tools. To address this gap, we propose a modular\nevaluation framework that decomposes agent pipelines into interpretable stages\nfor detailed error analysis. Using the SeeAct framework and the Mind2Web\ndataset as a case study, we show how this approach reveals actionable\nweaknesses missed by standard metrics - paving the way for more robust and\ngeneralizable web agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u89e3Web\u667a\u80fd\u4f53\u6d41\u6c34\u7ebf\u8fdb\u884c\u7ec6\u7c92\u5ea6\u9519\u8bef\u5206\u6790\uff0c\u63ed\u793a\u6807\u51c6\u6307\u6807\u9057\u6f0f\u7684\u53ef\u64cd\u4f5c\u5f31\u70b9", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684Web\u667a\u80fd\u4f53\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u6574\u4f53\u6210\u529f\u7387\uff0c\u5ffd\u89c6\u4e86\u4e2d\u95f4\u9519\u8bef\uff0c\u9650\u5236\u4e86\u6545\u969c\u6a21\u5f0f\u6d1e\u5bdf\u548c\u7cfb\u7edf\u6539\u8fdb", "method": "\u4f7f\u7528SeeAct\u6846\u67b6\u548cMind2Web\u6570\u636e\u96c6\u4f5c\u4e3a\u6848\u4f8b\u7814\u7a76\uff0c\u5c06\u667a\u80fd\u4f53\u6d41\u6c34\u7ebf\u5206\u89e3\u4e3a\u53ef\u89e3\u91ca\u7684\u9636\u6bb5\u8fdb\u884c\u8be6\u7ec6\u9519\u8bef\u5206\u6790", "result": "\u8be5\u65b9\u6cd5\u63ed\u793a\u4e86\u6807\u51c6\u6307\u6807\u9057\u6f0f\u7684\u53ef\u64cd\u4f5c\u5f31\u70b9", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u548c\u53ef\u6cdb\u5316\u7684Web\u667a\u80fd\u4f53\u94fa\u5e73\u4e86\u9053\u8def"}}
{"id": "2509.14766", "categories": ["econ.TH"], "pdf": "https://arxiv.org/pdf/2509.14766", "abs": "https://arxiv.org/abs/2509.14766", "authors": ["Hang Jiang"], "title": "An Implementation Relaxation Approach to Principal-Agent Problems", "comment": null, "summary": "The first-order approach (FOA) is the standard tool for solving\nprincipal-agent problems, replacing the incentive compatibility (IC) constraint\nwith its first-order condition to obtain a relaxed problem. We show that FOA is\nnot a valid relaxation when the support of the outcome distribution shifts with\nthe agent's effort, as in well-studied additive-noise models. In such cases,\nthe optimal effort may occur at a kink point that the first-order condition\ncannot capture, causing FOA to miss optimal contracts, including widely adopted\nbonus schemes. Motivated by this limitation, we introduce the Implementation\nRelaxation Approach (IRA), which relaxes the set of agent actions and payoffs\nthat feasible contracts can induce, rather than directly relaxing IC. IRA\naccommodates non-differentiable optima and is straightforward to apply across\nsettings, particularly for deriving optimality conditions for simple contracts.\nUsing IRA, we derive an optimality condition for quota-bonus contracts that is\nmore general, encompassing a broader range of scenarios than FOA-based\nconditions, including those established in the literature under fixed-support\nassumptions. This also fills a gap where the optimality of quota-bonus\ncontracts in shifting-support settings has been examined only under endogenous\nassumptions, and it highlights the broader applicability of IRA as a\nmethodological tool.", "AI": {"tldr": "\u4e00\u9636\u65b9\u6cd5(FOA)\u5728\u7ed3\u679c\u5206\u5e03\u968f\u52aa\u529b\u53d8\u5316\u65f6\u5931\u6548\uff0c\u65e0\u6cd5\u6355\u6349\u6700\u4f18\u52aa\u529b\u70b9\u3002\u672c\u6587\u63d0\u51fa\u5b9e\u73b0\u677e\u5f1b\u65b9\u6cd5(IRA)\uff0c\u901a\u8fc7\u677e\u5f1b\u4ee3\u7406\u884c\u52a8\u548c\u6536\u76ca\u96c6\u6765\u66f4\u5e7f\u6cdb\u5730\u5904\u7406\u6fc0\u52b1\u517c\u5bb9\u7ea6\u675f\uff0c\u7279\u522b\u9002\u7528\u4e8e\u63a8\u5bfc\u7b80\u5355\u5408\u540c\u7684\u6700\u4f18\u6027\u6761\u4ef6\u3002", "motivation": "\u4f20\u7edf\u7684\u4e00\u9636\u65b9\u6cd5\u5728\u7ed3\u679c\u5206\u5e03\u652f\u6491\u96c6\u968f\u52aa\u529b\u53d8\u5316\u65f6\u4e0d\u662f\u6709\u6548\u7684\u677e\u5f1b\u65b9\u6cd5\uff0c\u53ef\u80fd\u9519\u8fc7\u6700\u4f18\u5408\u540c\uff08\u5305\u62ec\u5e7f\u6cdb\u91c7\u7528\u7684\u5956\u91d1\u65b9\u6848\uff09\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u975e\u53ef\u5fae\u6700\u4f18\u89e3\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5b9e\u73b0\u677e\u5f1b\u65b9\u6cd5(IRA)\uff0c\u901a\u8fc7\u677e\u5f1b\u53ef\u884c\u5408\u540c\u80fd\u591f\u8bf1\u5bfc\u7684\u4ee3\u7406\u884c\u52a8\u548c\u6536\u76ca\u96c6\u6765\u5904\u7406\u6fc0\u52b1\u517c\u5bb9\u7ea6\u675f\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u677e\u5f1b\u4e00\u9636\u6761\u4ef6\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u5bb9\u7eb3\u975e\u53ef\u5fae\u6700\u4f18\u89e3\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u8bbe\u7f6e\u3002", "result": "\u4f7f\u7528IRA\u63a8\u5bfc\u51fa\u4e86\u914d\u989d\u5956\u91d1\u5408\u540c\u7684\u66f4\u4e00\u822c\u6700\u4f18\u6027\u6761\u4ef6\uff0c\u6bd4\u57fa\u4e8eFOA\u7684\u6761\u4ef6\u9002\u7528\u8303\u56f4\u66f4\u5e7f\uff0c\u586b\u8865\u4e86\u5728\u53d8\u5316\u652f\u6491\u96c6\u8bbe\u7f6e\u4e0b\u914d\u989d\u5956\u91d1\u5408\u540c\u6700\u4f18\u6027\u7814\u7a76\u7684\u7a7a\u767d\u3002", "conclusion": "IRA\u4f5c\u4e3a\u4e00\u79cd\u65b9\u6cd5\u8bba\u5de5\u5177\u5177\u6709\u66f4\u5e7f\u6cdb\u7684\u9002\u7528\u6027\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u4f20\u7edfFOA\u65e0\u6cd5\u89e3\u51b3\u7684\u60c5\u51b5\uff0c\u7279\u522b\u662f\u5728\u7ed3\u679c\u5206\u5e03\u652f\u6491\u96c6\u968f\u52aa\u529b\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2509.14554", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2509.14554", "abs": "https://arxiv.org/abs/2509.14554", "authors": ["Xiaoming Zhai"], "title": "Generative Large Language Models for Knowledge Representation: A Systematic Review of Concept Map Generation", "comment": null, "summary": "The rise of generative large language models (LLMs) has opened new\nopportunities for automating knowledge representation through concept maps, a\nlong-standing pedagogical tool valued for fostering meaningful learning and\nhigher-order thinking. Traditional construction of concept maps is\nlabor-intensive, requiring significant expertise and time, limiting their\nscalability in education. This review systematically synthesizes the emerging\nbody of research on LLM-enabled concept map generation, focusing on two guiding\nquestions: (a) What methods and technical features of LLMs are employed to\nconstruct concept maps? (b) What empirical evidence exists to validate their\neducational utility? Through a comprehensive search across major databases and\nAI-in-education conference proceedings, 28 studies meeting rigorous inclusion\ncriteria were analyzed using thematic synthesis. Findings reveal six major\nmethodological categories: human-in-the-loop systems, weakly supervised\nlearning models, fine-tuned domain-specific LLMs, pre-trained LLMs with prompt\nengineering, hybrid systems integrating knowledge bases, and modular frameworks\ncombining symbolic and statistical tools. Validation strategies ranged from\nquantitative metrics (precision, recall, F1-score, semantic similarity) to\nqualitative evaluations (expert review, learner feedback). Results indicate\nLLM-generated maps hold promise for scalable, adaptive, and pedagogically\nrelevant knowledge visualization, though challenges remain regarding validity,\ninterpretability, multilingual adaptability, and classroom integration. Future\nresearch should prioritize interdisciplinary co-design, empirical classroom\ntrials, and alignment with instructional practices to realize their full\neducational potential.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6982\u5ff5\u56fe\u751f\u6210\u4e2d\u7684\u5e94\u7528\uff0c\u5206\u6790\u4e8628\u9879\u7814\u7a76\uff0c\u603b\u7ed3\u4e866\u79cd\u4e3b\u8981\u65b9\u6cd5\u5e76\u8bc4\u4f30\u4e86\u5176\u6559\u80b2\u6548\u7528\uff0c\u6307\u51fa\u867d\u7136\u524d\u666f\u5e7f\u9614\u4f46\u4ecd\u9762\u4e34\u6709\u6548\u6027\u3001\u53ef\u89e3\u91ca\u6027\u7b49\u6311\u6218\u3002", "motivation": "\u4f20\u7edf\u6982\u5ff5\u56fe\u6784\u5efa\u8017\u65f6\u8d39\u529b\uff0c\u9650\u5236\u4e86\u5728\u6559\u80b2\u4e2d\u7684\u89c4\u6a21\u5316\u5e94\u7528\u3002\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u7684\u51fa\u73b0\u4e3a\u81ea\u52a8\u5316\u77e5\u8bc6\u8868\u793a\u63d0\u4f9b\u4e86\u65b0\u673a\u9047\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30LLM\u5728\u6982\u5ff5\u56fe\u751f\u6210\u4e2d\u7684\u65b9\u6cd5\u548c\u6559\u80b2\u4ef7\u503c\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u68c0\u7d22\u4e3b\u8981\u6570\u636e\u5e93\u548cAI\u6559\u80b2\u4f1a\u8bae\u8bba\u6587\uff0c\u7b5b\u9009\u51fa28\u9879\u7b26\u5408\u6807\u51c6\u7684\u7814\u7a76\uff0c\u4f7f\u7528\u4e3b\u9898\u7efc\u5408\u5206\u6790\u6cd5\u8fdb\u884c\u5206\u7c7b\u548c\u8bc4\u4f30\u3002", "result": "\u8bc6\u522b\u51fa6\u79cd\u4e3b\u8981\u65b9\u6cd5\uff1a\u4eba\u673a\u534f\u4f5c\u7cfb\u7edf\u3001\u5f31\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u3001\u9886\u57df\u7279\u5b9a\u5fae\u8c03LLM\u3001\u9884\u8bad\u7ec3LLM\u63d0\u793a\u5de5\u7a0b\u3001\u77e5\u8bc6\u5e93\u6df7\u5408\u7cfb\u7edf\u3001\u7b26\u53f7\u4e0e\u7edf\u8ba1\u5de5\u5177\u7ec4\u5408\u6846\u67b6\u3002\u9a8c\u8bc1\u7b56\u7565\u5305\u62ec\u5b9a\u91cf\u6307\u6807\u548c\u5b9a\u6027\u8bc4\u4f30\uff0c\u663e\u793aLLM\u751f\u6210\u7684\u6982\u5ff5\u56fe\u5177\u6709\u89c4\u6a21\u5316\u3001\u81ea\u9002\u5e94\u548c\u6559\u80b2\u76f8\u5173\u6027\u6f5c\u529b\u3002", "conclusion": "LLM\u751f\u6210\u7684\u6982\u5ff5\u56fe\u5728\u77e5\u8bc6\u53ef\u89c6\u5316\u65b9\u9762\u524d\u666f\u5e7f\u9614\uff0c\u4f46\u5728\u6709\u6548\u6027\u3001\u53ef\u89e3\u91ca\u6027\u3001\u591a\u8bed\u8a00\u9002\u5e94\u6027\u548c\u8bfe\u5802\u6574\u5408\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\u3002\u672a\u6765\u7814\u7a76\u5e94\u4f18\u5148\u8003\u8651\u8de8\u5b66\u79d1\u534f\u540c\u8bbe\u8ba1\u3001\u5b9e\u8bc1\u8bfe\u5802\u8bd5\u9a8c\u548c\u4e0e\u6559\u5b66\u5b9e\u8df5\u7684\u5bf9\u63a5\u3002"}}
{"id": "2509.14521", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.14521", "abs": "https://arxiv.org/abs/2509.14521", "authors": ["Ali Baheri", "David Millard", "Alireza Vahid"], "title": "Geometry-Aware Decentralized Sinkhorn for Wasserstein Barycenters", "comment": null, "summary": "Distributed systems require fusing heterogeneous local probability\ndistributions into a global summary over sparse and unreliable communication\nnetworks. Traditional consensus algorithms, which average distributions in\nEuclidean space, ignore their inherent geometric structure, leading to\nmisleading results. Wasserstein barycenters offer a geometry-aware alternative\nby minimizing optimal transport costs, but their entropic approximations via\nthe Sinkhorn algorithm typically require centralized coordination. This paper\nproposes a fully decentralized Sinkhorn algorithm that reformulates the\ncentralized geometric mean as an arithmetic average in the log-domain, enabling\napproximation through local gossip protocols. Agents exchange log-messages with\nneighbors, interleaving consensus phases with local updates to mimic\ncentralized iterations without a coordinator. To optimize bandwidth, we\nintegrate event-triggered transmissions and b-bit quantization, providing\ntunable trade-offs between accuracy and communication while accommodating\nasynchrony and packet loss. Under mild assumptions, we prove convergence to a\nneighborhood of the centralized entropic barycenter, with bias linearly\ndependent on consensus tolerance, trigger threshold, and quantization error.\nComplexity scales near-linearly with network size. Simulations confirm\nnear-centralized accuracy with significantly fewer messages, across various\ntopologies and conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u7684Sinkhorn\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c06\u51e0\u4f55\u5e73\u5747\u91cd\u65b0\u8868\u8ff0\u4e3a\u5bf9\u6570\u57df\u7684\u7b97\u672f\u5e73\u5747\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u901a\u8fc7\u672c\u5730gossip\u534f\u8bae\u8fd1\u4f3cWasserstein\u91cd\u5fc3\uff0c\u540c\u65f6\u96c6\u6210\u4e86\u4e8b\u4ef6\u89e6\u53d1\u4f20\u8f93\u548c\u91cf\u5316\u6280\u672f\u6765\u4f18\u5316\u5e26\u5bbd\u3002", "motivation": "\u4f20\u7edf\u5171\u8bc6\u7b97\u6cd5\u5728\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u4e2d\u5e73\u5747\u5206\u5e03\uff0c\u5ffd\u7565\u4e86\u6982\u7387\u5206\u5e03\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u5bfc\u81f4\u8bef\u5bfc\u6027\u7ed3\u679c\u3002Wasserstein\u91cd\u5fc3\u867d\u7136\u63d0\u4f9b\u4e86\u51e0\u4f55\u611f\u77e5\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5176\u901a\u8fc7Sinkhorn\u7b97\u6cd5\u7684\u71b5\u8fd1\u4f3c\u901a\u5e38\u9700\u8981\u96c6\u4e2d\u5f0f\u534f\u8c03\u3002", "method": "\u5c06\u96c6\u4e2d\u5f0f\u51e0\u4f55\u5e73\u5747\u91cd\u65b0\u8868\u8ff0\u4e3a\u5bf9\u6570\u57df\u7684\u7b97\u672f\u5e73\u5747\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u901a\u8fc7\u672c\u5730gossip\u534f\u8bae\u8fdb\u884c\u8fd1\u4f3c\u3002\u667a\u80fd\u4f53\u4e0e\u90bb\u5c45\u4ea4\u6362\u5bf9\u6570\u6d88\u606f\uff0c\u4ea4\u9519\u5171\u8bc6\u9636\u6bb5\u548c\u672c\u5730\u66f4\u65b0\u6765\u6a21\u62df\u96c6\u4e2d\u5f0f\u8fed\u4ee3\uff0c\u65e0\u9700\u534f\u8c03\u5668\u3002\u96c6\u6210\u4e86\u4e8b\u4ef6\u89e6\u53d1\u4f20\u8f93\u548cb\u4f4d\u91cf\u5316\u6765\u4f18\u5316\u5e26\u5bbd\u3002", "result": "\u5728\u6e29\u548c\u5047\u8bbe\u4e0b\uff0c\u7b97\u6cd5\u6536\u655b\u5230\u96c6\u4e2d\u5f0f\u71b5\u91cd\u5fc3\u7684\u90bb\u57df\uff0c\u504f\u5dee\u7ebf\u6027\u4f9d\u8d56\u4e8e\u5171\u8bc6\u5bb9\u5dee\u3001\u89e6\u53d1\u9608\u503c\u548c\u91cf\u5316\u8bef\u5dee\u3002\u590d\u6742\u5ea6\u968f\u7f51\u7edc\u89c4\u6a21\u8fd1\u7ebf\u6027\u589e\u957f\u3002\u4eff\u771f\u663e\u793a\u5728\u591a\u79cd\u62d3\u6251\u548c\u6761\u4ef6\u4e0b\uff0c\u4ee5\u663e\u8457\u66f4\u5c11\u7684\u6d88\u606f\u5b9e\u73b0\u63a5\u8fd1\u96c6\u4e2d\u5f0f\u7684\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u7684\u51e0\u4f55\u611f\u77e5\u6982\u7387\u5206\u5e03\u878d\u5408\uff0c\u5728\u7a00\u758f\u4e0d\u53ef\u9760\u901a\u4fe1\u7f51\u7edc\u4e2d\u63d0\u4f9b\u4e86\u7cbe\u5ea6\u548c\u901a\u4fe1\u4e4b\u95f4\u7684\u53ef\u8c03\u6743\u8861\uff0c\u540c\u65f6\u9002\u5e94\u5f02\u6b65\u6027\u548c\u4e22\u5305\u3002"}}
{"id": "2509.14349", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14349", "abs": "https://arxiv.org/abs/2509.14349", "authors": ["Zhengyang Kris Weng", "Matthew L. Elwin", "Han Liu"], "title": "LeVR: A Modular VR Teleoperation Framework for Imitation Learning in Dexterous Manipulation", "comment": null, "summary": "We introduce LeVR, a modular software framework designed to bridge two\ncritical gaps in robotic imitation learning. First, it provides robust and\nintuitive virtual reality (VR) teleoperation for data collection using robot\narms paired with dexterous hands, addressing a common limitation in existing\nsystems. Second, it natively integrates with the powerful LeRobot imitation\nlearning (IL) framework, enabling the use of VR-based teleoperation data and\nstreamlining the demonstration collection process. To demonstrate LeVR, we\nrelease LeFranX, an open-source implementation for the Franka FER arm and\nRobotEra XHand, two widely used research platforms. LeFranX delivers a\nseamless, end-to-end workflow from data collection to real-world policy\ndeployment. We validate our system by collecting a public dataset of 100 expert\ndemonstrations and use it to successfully fine-tune state-of-the-art visuomotor\npolicies. We provide our open-source framework, implementation, and dataset to\naccelerate IL research for the robotics community.", "AI": {"tldr": "LeVR\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u8f6f\u4ef6\u6846\u67b6\uff0c\u901a\u8fc7VR\u9065\u64cd\u4f5c\u89e3\u51b3\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u6536\u96c6\u95ee\u9898\uff0c\u5e76\u4e0eLeRobot\u6846\u67b6\u96c6\u6210\uff0c\u63d0\u4f9b\u4ece\u6570\u636e\u6536\u96c6\u5230\u7b56\u7565\u90e8\u7f72\u7684\u7aef\u5230\u7aef\u5de5\u4f5c\u6d41\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u4e2d\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u4e3a\u914d\u5907\u7075\u5de7\u624b\u7684\u673a\u68b0\u81c2\u63d0\u4f9b\u7a33\u5065\u76f4\u89c2\u7684VR\u9065\u64cd\u4f5c\u6570\u636e\u6536\u96c6\uff0c\u4ee5\u53ca\u4e0e\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\u7684\u65e0\u7f1d\u96c6\u6210\u3002", "method": "\u5f00\u53d1LeVR\u6a21\u5757\u5316\u6846\u67b6\uff0c\u63d0\u4f9bVR\u9065\u64cd\u4f5c\u529f\u80fd\uff1b\u53d1\u5e03LeFranX\u5f00\u6e90\u5b9e\u73b0\uff08\u9488\u5bf9Franka FER\u81c2\u548cRobotEra XHand\uff09\uff1b\u6536\u96c6100\u4e2a\u4e13\u5bb6\u6f14\u793a\u6570\u636e\u96c6\u7528\u4e8e\u5fae\u8c03\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u3002", "result": "\u6210\u529f\u9a8c\u8bc1\u7cfb\u7edf\u529f\u80fd\uff0c\u6536\u96c6\u4e86\u516c\u5f00\u6570\u636e\u96c6\u5e76\u7528\u4e8e\u5fae\u8c03\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u4ece\u6570\u636e\u6536\u96c6\u5230\u771f\u5b9e\u4e16\u754c\u7b56\u7565\u90e8\u7f72\u7684\u5b8c\u6574\u5de5\u4f5c\u6d41\u3002", "conclusion": "LeVR\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u6536\u96c6\u548c\u96c6\u6210\u95ee\u9898\uff0c\u901a\u8fc7\u5f00\u6e90\u6846\u67b6\u3001\u5b9e\u73b0\u548c\u6570\u636e\u96c6\u52a0\u901f\u673a\u5668\u4eba\u793e\u533a\u7684\u6a21\u4eff\u5b66\u4e60\u7814\u7a76\u3002"}}
{"id": "2509.14448", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14448", "abs": "https://arxiv.org/abs/2509.14448", "authors": ["Rick Chen", "Joseph Ternasky", "Afriyie Samuel Kwesi", "Ben Griffin", "Aaron Ontoyin Yin", "Zakari Salifu", "Kelvin Amoaba", "Xianling Mu", "Fuat Alican", "Yigit Ihlamur"], "title": "VCBench: Benchmarking LLMs in Venture Capital", "comment": null, "summary": "Benchmarks such as SWE-bench and ARC-AGI demonstrate how shared datasets\naccelerate progress toward artificial general intelligence (AGI). We introduce\nVCBench, the first benchmark for predicting founder success in venture capital\n(VC), a domain where signals are sparse, outcomes are uncertain, and even top\ninvestors perform modestly. At inception, the market index achieves a precision\nof 1.9%. Y Combinator outperforms the index by a factor of 1.7x, while tier-1\nfirms are 2.9x better. VCBench provides 9,000 anonymized founder profiles,\nstandardized to preserve predictive features while resisting identity leakage,\nwith adversarial tests showing more than 90% reduction in re-identification\nrisk. We evaluate nine state-of-the-art large language models (LLMs).\nDeepSeek-V3 delivers over six times the baseline precision, GPT-4o achieves the\nhighest F0.5, and most models surpass human benchmarks. Designed as a public\nand evolving resource available at vcbench.com, VCBench establishes a\ncommunity-driven standard for reproducible and privacy-preserving evaluation of\nAGI in early-stage venture forecasting.", "AI": {"tldr": "VCBench\u662f\u9996\u4e2a\u7528\u4e8e\u9884\u6d4b\u98ce\u9669\u6295\u8d44\u4e2d\u521b\u59cb\u4eba\u6210\u529f\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b9000\u4e2a\u533f\u540d\u521b\u59cb\u4eba\u6863\u6848\uff0c\u8bc4\u4f30\u663e\u793a\u9876\u7ea7LLM\u6a21\u578b\u8868\u73b0\u8d85\u8fc7\u4eba\u7c7b\u57fa\u51c66\u500d\u4ee5\u4e0a", "motivation": "\u98ce\u9669\u6295\u8d44\u9886\u57df\u4fe1\u53f7\u7a00\u758f\u3001\u7ed3\u679c\u4e0d\u786e\u5b9a\uff0c\u5373\u4f7f\u662f\u9876\u7ea7\u6295\u8d44\u8005\u8868\u73b0\u4e5f\u4e00\u822c\uff0c\u9700\u8981\u5efa\u7acb\u6807\u51c6\u5316\u57fa\u51c6\u6765\u52a0\u901fAGI\u5728\u8be5\u9886\u57df\u7684\u53d1\u5c55", "method": "\u521b\u5efa\u5305\u542b9000\u4e2a\u533f\u540d\u521b\u59cb\u4eba\u6863\u6848\u7684\u6570\u636e\u96c6\uff0c\u8fdb\u884c\u5bf9\u6297\u6027\u6d4b\u8bd5\u964d\u4f4e\u8eab\u4efd\u6cc4\u9732\u98ce\u9669\uff0c\u8bc4\u4f309\u4e2a\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b", "result": "\u5e02\u573a\u6307\u6570\u7cbe\u5ea61.9%\uff0cYC\u8868\u73b0\u4f18\u4e8e\u6307\u65701.7\u500d\uff0c\u4e00\u7ea7\u673a\u6784\u4f18\u4e8e2.9\u500d\u3002DeepSeek-V3\u7cbe\u5ea6\u8d85\u57fa\u7ebf6\u500d\uff0cGPT-4o\u83b7\u5f97\u6700\u9ad8F0.5\u5206\u6570\uff0c\u5927\u591a\u6570\u6a21\u578b\u8d85\u8d8a\u4eba\u7c7b\u57fa\u51c6", "conclusion": "VCBench\u4f5c\u4e3a\u516c\u5f00\u6f14\u8fdb\u8d44\u6e90\uff0c\u4e3a\u65e9\u671f\u98ce\u9669\u9884\u6d4b\u5efa\u7acb\u4e86\u793e\u533a\u9a71\u52a8\u7684\u53ef\u91cd\u73b0\u548c\u9690\u79c1\u4fdd\u62a4\u7684AGI\u8bc4\u4f30\u6807\u51c6"}}
{"id": "2509.15165", "categories": ["econ.TH"], "pdf": "https://arxiv.org/pdf/2509.15165", "abs": "https://arxiv.org/abs/2509.15165", "authors": ["Christopher P. Chambers", "Yusufcan Masatlioglu", "Ruodu Wang"], "title": "Invariant Modeling for Joint Distributions", "comment": null, "summary": "A common theme underlying many problems in statistics and economics involves\nthe determination of a systematic method of selecting a joint distribution\nconsistent with a specified list of categorical marginals, some of which have\nan ordinal structure. We propose guidance in narrowing down the set of possible\nmethods by introducing Invariant Aggregation (IA), a natural property that\nrequires merging adjacent categories in one marginal not to alter the joint\ndistribution over unaffected values. We prove that a model satisfies IA if and\nonly if it is a copula model. This characterization ensures i) robustness\nagainst data manipulation and survey design, and ii) allows seamless\nincorporation of new variables. Our results provide both theoretical clarity\nand practical safeguards for inference under marginal constraints.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e0d\u53d8\u805a\u5408(IA)\u5c5e\u6027\uff0c\u8bc1\u660e\u6ee1\u8db3IA\u7684\u6a21\u578b\u5f53\u4e14\u4ec5\u5f53\u662fcopula\u6a21\u578b\uff0c\u8fd9\u786e\u4fdd\u4e86\u6570\u636e\u64cd\u4f5c\u7684\u9c81\u68d2\u6027\u548c\u65b0\u53d8\u91cf\u7684\u65e0\u7f1d\u6574\u5408", "motivation": "\u7edf\u8ba1\u5b66\u548c\u7ecf\u6d4e\u5b66\u4e2d\u8bb8\u591a\u95ee\u9898\u9700\u8981\u9009\u62e9\u4e0e\u6307\u5b9a\u5206\u7c7b\u8fb9\u9645\u5206\u5e03\u4e00\u81f4\u7684\u8054\u5408\u5206\u5e03\u65b9\u6cd5\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7cfb\u7edf\u6027\u6307\u5bfc", "method": "\u5f15\u5165\u4e0d\u53d8\u805a\u5408(IA)\u5c5e\u6027\uff0c\u8981\u6c42\u5408\u5e76\u76f8\u90bb\u7c7b\u522b\u4e0d\u5f71\u54cd\u672a\u53d7\u5f71\u54cd\u503c\u7684\u8054\u5408\u5206\u5e03\uff0c\u5e76\u8bc1\u660eIA\u4e0ecopula\u6a21\u578b\u7684\u7b49\u4ef7\u6027", "result": "\u8bc1\u660e\u4e86\u6a21\u578b\u6ee1\u8db3IA\u5f53\u4e14\u4ec5\u5f53\u662fcopula\u6a21\u578b\uff0c\u8fd9\u63d0\u4f9b\u4e86\u7406\u8bba\u6e05\u6670\u5ea6\u548c\u5b9e\u9645\u4fdd\u969c", "conclusion": "IA\u5c5e\u6027\u4e3a\u5728\u8fb9\u9645\u7ea6\u675f\u4e0b\u8fdb\u884c\u63a8\u65ad\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u548c\u5b9e\u8df5\u4fdd\u969c\uff0c\u786e\u4fdd\u4e86\u5bf9\u6570\u636e\u64cd\u4f5c\u548c\u8c03\u67e5\u8bbe\u8ba1\u7684\u9c81\u68d2\u6027"}}
{"id": "2509.14803", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14803", "abs": "https://arxiv.org/abs/2509.14803", "authors": ["Xian Gao", "Zongyun Zhang", "Ting Liu", "Yuzhuo Fu"], "title": "OnlineMate: An LLM-Based Multi-Agent Companion System for Cognitive Support in Online Learning", "comment": null, "summary": "In online learning environments, students often lack personalized peer\ninteractions, which play a crucial role in supporting cognitive development and\nlearning engagement. Although previous studies have utilized large language\nmodels (LLMs) to simulate interactive dynamic learning environments for\nstudents, these interactions remain limited to conversational exchanges,\nlacking insights and adaptations to the learners' individualized learning and\ncognitive states. As a result, students' interest in discussions with AI\nlearning companions is low, and they struggle to gain inspiration from such\ninteractions. To address this challenge, we propose OnlineMate, a multi-agent\nlearning companion system driven by LLMs that integrates the Theory of Mind\n(ToM). OnlineMate is capable of simulating peer-like agent roles, adapting to\nlearners' cognitive states during collaborative discussions, and inferring\ntheir psychological states, such as misunderstandings, confusion, or\nmotivation. By incorporating Theory of Mind capabilities, the system can\ndynamically adjust its interaction strategies to support the development of\nhigher-order thinking and cognition. Experimental results in simulated learning\nscenarios demonstrate that OnlineMate effectively fosters deep learning and\ndiscussions while enhancing cognitive engagement in online educational\nsettings.", "AI": {"tldr": "OnlineMate\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u5b66\u4e60\u4f34\u4fa3\u7cfb\u7edf\uff0c\u6574\u5408\u4e86\u5fc3\u667a\u7406\u8bba(ToM)\u80fd\u529b\uff0c\u80fd\u591f\u6a21\u62df\u540c\u4f34\u89d2\u8272\u3001\u9002\u5e94\u5b66\u4e60\u8005\u8ba4\u77e5\u72b6\u6001\uff0c\u5e76\u52a8\u6001\u8c03\u6574\u4e92\u52a8\u7b56\u7565\u6765\u4fc3\u8fdb\u6df1\u5ea6\u5b66\u4e60\u3002", "motivation": "\u5728\u7ebf\u5b66\u4e60\u73af\u5883\u4e2d\u5b66\u751f\u7f3a\u4e4f\u4e2a\u6027\u5316\u540c\u4f34\u4e92\u52a8\uff0c\u73b0\u6709AI\u5b66\u4e60\u4f34\u4fa3\u4ec5\u9650\u4e8e\u5bf9\u8bdd\u4ea4\u6d41\uff0c\u65e0\u6cd5\u9002\u5e94\u5b66\u4e60\u8005\u7684\u4e2a\u4f53\u5316\u5b66\u4e60\u548c\u8ba4\u77e5\u72b6\u6001\uff0c\u5bfc\u81f4\u5b66\u751f\u5174\u8da3\u4f4e\u4e14\u96be\u4ee5\u4ece\u4e2d\u83b7\u5f97\u542f\u53d1\u3002", "method": "\u63d0\u51faOnlineMate\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u6574\u5408\u5fc3\u667a\u7406\u8bba(ToM)\u80fd\u529b\uff0c\u6a21\u62df\u540c\u4f34\u89d2\u8272\uff0c\u5728\u534f\u4f5c\u8ba8\u8bba\u4e2d\u9002\u5e94\u5b66\u4e60\u8005\u8ba4\u77e5\u72b6\u6001\uff0c\u63a8\u65ad\u5176\u5fc3\u7406\u72b6\u6001(\u8bef\u89e3\u3001\u56f0\u60d1\u3001\u52a8\u673a\u7b49)\uff0c\u5e76\u52a8\u6001\u8c03\u6574\u4e92\u52a8\u7b56\u7565\u3002", "result": "\u5728\u6a21\u62df\u5b66\u4e60\u573a\u666f\u4e2d\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cOnlineMate\u80fd\u6709\u6548\u4fc3\u8fdb\u6df1\u5ea6\u5b66\u4e60\u548c\u8ba8\u8bba\uff0c\u540c\u65f6\u589e\u5f3a\u5728\u7ebf\u6559\u80b2\u73af\u5883\u4e2d\u7684\u8ba4\u77e5\u53c2\u4e0e\u5ea6\u3002", "conclusion": "\u6574\u5408\u5fc3\u667a\u7406\u8bba\u7684\u591a\u667a\u80fd\u4f53\u5b66\u4e60\u4f34\u4fa3\u7cfb\u7edf\u80fd\u591f\u66f4\u597d\u5730\u652f\u6301\u9ad8\u9636\u601d\u7ef4\u548c\u8ba4\u77e5\u53d1\u5c55\uff0c\u4e3a\u5728\u7ebf\u6559\u80b2\u63d0\u4f9b\u66f4\u6709\u6548\u7684\u4e2a\u6027\u5316\u4e92\u52a8\u652f\u6301\u3002"}}
{"id": "2509.14705", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.14705", "abs": "https://arxiv.org/abs/2509.14705", "authors": ["Huiling Liu", "Junshan Luo", "Shilian Wang", "Fanggang Wang", "Theodoros A. Tsiftsis", "Symeon Chatzinotas"], "title": "Secure Short-Packet Communications for RIS-Assisted AAV Networks", "comment": null, "summary": "Advancements toward 6G have intensified demands for ultra-reliable\nlow-latency communication, positioning shortpacket communications as a critical\ntechnology for autonomous aerial vehicle (AAV) networks. However, the open\nbroadcast nature introduces significant security vulnerabilities. Although\nphysical-layer security offers a low-complexity solution by exploiting wireless\nchannel randomness, the AAV communication performance severely degrades in\nweak-coverage or non-line-of sight scenarios. To overcome these limitations,\nthis paper proposes a short-packet communications framework for AAV networks\nthat leverages reconfigurable intelligent surfaces (RIS) with the aim of\nextending coverage and enhancing secrecy capabilities. Analytical frameworks\nare developed to evaluate the average secrecy throughput (AST) in finite\nblocklength constraints for both external and internal avesdropping scenarios,\nwhich incorporates non-orthogonal multiple access with imperfect successive\ninterference cancellation. Asymptotic approximations of AST are derived as\ntransmit power approaches infinity. Furthermore, we formulate a blocklength\noptimization problem to maximize the AST, effectively resolving the trade-offs\namong delay, reliability, and secrecy. Extensive simulations validate the\nanalytical frameworks, which reveal that large-scale RIS deployment\nsignificantly boosts AST, and the power allocation coefficient exhibits dual\neffects in the internal eavesdropping scenario. These observations provide\nuseful insights for designing reliable and secure lowlatency AAV communications\nsystems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762(RIS)\u7684\u77ed\u5305\u901a\u4fe1\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u81ea\u4e3b\u7a7a\u4e2d\u8f66\u8f86(AAV)\u7f51\u7edc\u7684\u5b89\u5168\u6027\u548c\u8986\u76d6\u8303\u56f4\uff0c\u901a\u8fc7\u5206\u6790\u5e73\u5747\u4fdd\u5bc6\u541e\u5410\u91cf(AST)\u548c\u4f18\u5316\u5757\u957f\u5ea6\u6765\u89e3\u51b3\u5ef6\u8fdf\u3001\u53ef\u9760\u6027\u548c\u4fdd\u5bc6\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "6G\u53d1\u5c55\u5bf9\u8d85\u53ef\u9760\u4f4e\u5ef6\u8fdf\u901a\u4fe1\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f46AAV\u7f51\u7edc\u7684\u5f00\u653e\u5e7f\u64ad\u7279\u6027\u5e26\u6765\u4e25\u91cd\u5b89\u5168\u6f0f\u6d1e\uff0c\u4e14\u5728\u5f31\u8986\u76d6\u6216\u975e\u89c6\u8ddd\u573a\u666f\u4e0b\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u5229\u7528\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762(RIS)\u7684\u77ed\u5305\u901a\u4fe1\u6846\u67b6\uff0c\u5f00\u53d1\u5206\u6790\u6846\u67b6\u8bc4\u4f30\u6709\u9650\u5757\u957f\u7ea6\u675f\u4e0b\u7684\u5e73\u5747\u4fdd\u5bc6\u541e\u5410\u91cf(AST)\uff0c\u5305\u542b\u975e\u6b63\u4ea4\u591a\u5740\u63a5\u5165\u548c\u4e0d\u5b8c\u5584\u8fde\u7eed\u5e72\u6270\u6d88\u9664\uff0c\u63a8\u5bfcAST\u7684\u6e10\u8fd1\u8fd1\u4f3c\uff0c\u5e76\u5236\u5b9a\u5757\u957f\u5ea6\u4f18\u5316\u95ee\u9898\u3002", "result": "\u5927\u89c4\u6a21RIS\u90e8\u7f72\u663e\u8457\u63d0\u5347AST\uff0c\u529f\u7387\u5206\u914d\u7cfb\u6570\u5728\u5185\u90e8\u7a83\u542c\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u53cc\u91cd\u6548\u5e94\uff0c\u4eff\u771f\u9a8c\u8bc1\u4e86\u5206\u6790\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8bbe\u8ba1\u53ef\u9760\u5b89\u5168\u7684\u4f4e\u5ef6\u8fdfAAV\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u7528\u89c1\u89e3\uff0cRIS\u6280\u672f\u80fd\u6709\u6548\u6269\u5c55\u8986\u76d6\u8303\u56f4\u548c\u589e\u5f3a\u4fdd\u5bc6\u80fd\u529b\u3002"}}
{"id": "2509.14353", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14353", "abs": "https://arxiv.org/abs/2509.14353", "authors": ["Dvij Kalaria", "Sudarshan S Harithas", "Pushkal Katara", "Sangkyung Kwak", "Sarthak Bhagat", "Shankar Sastry", "Srinath Sridhar", "Sai Vemprala", "Ashish Kapoor", "Jonathan Chung-Kuan Huang"], "title": "DreamControl: Human-Inspired Whole-Body Humanoid Control for Scene Interaction via Guided Diffusion", "comment": "(under submission)", "summary": "We introduce DreamControl, a novel methodology for learning autonomous\nwhole-body humanoid skills. DreamControl leverages the strengths of diffusion\nmodels and Reinforcement Learning (RL): our core innovation is the use of a\ndiffusion prior trained on human motion data, which subsequently guides an RL\npolicy in simulation to complete specific tasks of interest (e.g., opening a\ndrawer or picking up an object). We demonstrate that this human motion-informed\nprior allows RL to discover solutions unattainable by direct RL, and that\ndiffusion models inherently promote natural looking motions, aiding in\nsim-to-real transfer. We validate DreamControl's effectiveness on a Unitree G1\nrobot across a diverse set of challenging tasks involving simultaneous lower\nand upper body control and object interaction.", "AI": {"tldr": "DreamControl\u662f\u4e00\u79cd\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u4eba\u7c7b\u8fd0\u52a8\u6570\u636e\u7684\u6269\u6563\u5148\u9a8c\u6765\u6307\u5bfcRL\u7b56\u7565\u5b66\u4e60\u4eba\u5f62\u673a\u5668\u4eba\u5168\u8eab\u63a7\u5236\u6280\u80fd", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u76f4\u63a5\u5f3a\u5316\u5b66\u4e60\u5728\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u4e2d\u96be\u4ee5\u53d1\u73b0\u81ea\u7136\u8fd0\u52a8\u6a21\u5f0f\u7684\u95ee\u9898\uff0c\u9700\u8981\u7ed3\u5408\u4eba\u7c7b\u8fd0\u52a8\u6570\u636e\u6765\u5f15\u5bfc\u7b56\u7565\u5b66\u4e60\uff0c\u63d0\u9ad8sim-to-real\u7684\u8fc1\u79fb\u6548\u679c", "method": "\u4f7f\u7528\u5728\u4eba\u7c7b\u8fd0\u52a8\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u5148\u9a8c\uff0c\u5728\u4eff\u771f\u73af\u5883\u4e2d\u6307\u5bfc\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5b8c\u6210\u7279\u5b9a\u4efb\u52a1\uff08\u5982\u5f00\u62bd\u5c49\u3001\u6293\u53d6\u7269\u4f53\uff09", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u53d1\u73b0\u76f4\u63a5RL\u65e0\u6cd5\u83b7\u5f97\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6269\u6563\u6a21\u578b\u56fa\u6709\u7684\u7279\u6027\u4fc3\u8fdb\u4e86\u81ea\u7136\u8fd0\u52a8\u6a21\u5f0f\uff0c\u6709\u52a9\u4e8e\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb", "conclusion": "DreamControl\u5728Unitree G1\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\uff0c\u80fd\u591f\u5904\u7406\u6d89\u53ca\u4e0a\u4e0b\u534a\u8eab\u540c\u65f6\u63a7\u5236\u548c\u7269\u4f53\u4ea4\u4e92\u7684\u590d\u6742\u4efb\u52a1"}}
{"id": "2509.14474", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.14474", "abs": "https://arxiv.org/abs/2509.14474", "authors": ["Meltem Subasioglu", "Nevzat Subasioglu"], "title": "From Mimicry to True Intelligence (TI) -- A New Paradigm for Artificial General Intelligence", "comment": "27 pages, 1 figure", "summary": "The debate around Artificial General Intelligence (AGI) remains open due to\ntwo fundamentally different goals: replicating human-like performance versus\nreplicating human-like cognitive processes. We argue that current\nperformance-based definitions are inadequate because they provide no clear,\nmechanism-focused roadmap for research, and they fail to properly define the\nqualitative nature of genuine intelligence. Drawing inspiration from the human\nbrain, we propose a new paradigm that shifts the focus from external mimicry to\nthe development of foundational cognitive architectures. We define True\nIntelligence (TI) as a system characterized by six core components: embodied\nsensory fusion, core directives, dynamic schemata creation, a\nhighly-interconnected multi-expert architecture, an orchestration layer, and\nlastly, the unmeasurable quality of Interconnectedness, which we hypothesize\nresults in consciousness and a subjective experience. We propose a practical,\nfive-level taxonomy of AGI based on the number of the first five measurable\ncomponents a system exhibits. This framework provides a clear path forward with\ndevelopmental milestones that directly address the challenge of building\ngenuinely intelligent systems. We contend that once a system achieves Level-5\nAGI by implementing all five measurable components, the difference between it\nand TI remains as a purely philosophical debate. For practical purposes - and\ngiven theories indicate consciousness is an emergent byproduct of integrated,\nhigher-order cognition - we conclude that a fifth-level AGI is functionally and\npractically equivalent to TI. This work synthesizes diverse insights from\nanalytical psychology, schema theory, metacognition, modern brain architectures\nand latest works in AI to provide the first holistic, mechanism-based\ndefinition of AGI that offers a clear and actionable path for the research\ncommunity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u57fa\u4e8e\u4eba\u7c7b\u5927\u8111\u8ba4\u77e5\u673a\u5236\u7684\u65b0AGI\u5b9a\u4e49\u6846\u67b6\uff0c\u5c06\u771f\u6b63\u667a\u80fd(TI)\u5b9a\u4e49\u4e3a\u5305\u542b\u516d\u4e2a\u6838\u5fc3\u7ec4\u4ef6\u7684\u7cfb\u7edf\uff0c\u5e76\u5efa\u7acb\u4e86\u4e94\u7ea7AGI\u5206\u7c7b\u6cd5\uff0c\u4e3a\u7814\u7a76\u63d0\u4f9b\u6e05\u6670\u7684\u673a\u5236\u5bfc\u5411\u8def\u7ebf\u56fe", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6027\u80fd\u7684AGI\u5b9a\u4e49\u7f3a\u4e4f\u660e\u786e\u7684\u673a\u5236\u7814\u7a76\u8def\u7ebf\u56fe\uff0c\u65e0\u6cd5\u6b63\u786e\u5b9a\u4e49\u771f\u6b63\u667a\u80fd\u7684\u8d28\u6027\u7279\u5f81\uff0c\u9700\u8981\u4ece\u5916\u90e8\u6a21\u4eff\u8f6c\u5411\u57fa\u7840\u8ba4\u77e5\u67b6\u6784\u7684\u5f00\u53d1", "method": "\u501f\u9274\u4eba\u8111\u673a\u5236\uff0c\u63d0\u51fa\u771f\u6b63\u667a\u80fd(TI)\u7684\u516d\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u5177\u8eab\u611f\u77e5\u878d\u5408\u3001\u6838\u5fc3\u6307\u4ee4\u3001\u52a8\u6001\u56fe\u5f0f\u521b\u5efa\u3001\u9ad8\u5ea6\u4e92\u8054\u7684\u591a\u4e13\u5bb6\u67b6\u6784\u3001\u534f\u8c03\u5c42\u548c\u4e0d\u53ef\u6d4b\u91cf\u7684\u4e92\u8054\u6027\u3002\u57fa\u4e8e\u524d\u4e94\u4e2a\u53ef\u6d4b\u91cf\u7ec4\u4ef6\u5efa\u7acb\u4e94\u7ea7AGI\u5206\u7c7b\u6cd5", "result": "\u63d0\u51fa\u4e86\u9996\u4e2a\u57fa\u4e8e\u673a\u5236\u7684\u6574\u4f53\u6027AGI\u5b9a\u4e49\uff0c\u63d0\u4f9b\u4e86\u5177\u6709\u53d1\u5c55\u91cc\u7a0b\u7891\u7684\u6e05\u6670\u7814\u7a76\u8def\u5f84\u3002\u8ba4\u4e3a\u5b9e\u73b0\u4e94\u7ea7AGI\u7684\u7cfb\u7edf\u5728\u529f\u80fd\u4e0a\u7b49\u540c\u4e8e\u771f\u6b63\u667a\u80fd", "conclusion": "\u8be5\u6846\u67b6\u7efc\u5408\u4e86\u5206\u6790\u5fc3\u7406\u5b66\u3001\u56fe\u5f0f\u7406\u8bba\u3001\u5143\u8ba4\u77e5\u3001\u73b0\u4ee3\u8111\u67b6\u6784\u548cAI\u6700\u65b0\u6210\u679c\uff0c\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u660e\u786e\u4e14\u53ef\u64cd\u4f5c\u7684AGI\u7814\u7a76\u8def\u5f84\uff0c\u4e94\u7ea7AGI\u4e0e\u771f\u6b63\u667a\u80fd\u7684\u533a\u522b\u4ec5\u5b58\u4e8e\u54f2\u5b66\u5c42\u9762"}}
{"id": "2509.14907", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2509.14907", "abs": "https://arxiv.org/abs/2509.14907", "authors": ["Seonbin Jo", "Woo-Sung Jung", "Jisung Yoon", "Hyunuk Kim"], "title": "Artificial Intelligence and Market Entrant Game Developers", "comment": null, "summary": "Artificial Intelligence (AI) is increasingly being used for generating\ndigital assets, such as programming codes and images. Games composed of various\ndigital assets are thus expected to be influenced significantly by AI.\nLeveraging public data and AI disclosure statements of games, this paper shows\nthat relatively more independent developers entered the market when generative\nAI became more publicly accessible, but their purposes of using AI are similar\nwith non-independent developers. Game features associated with AI hint nuanced\nimpacts of AI on independent developers.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u751f\u6210\u5f0fAI\u5bf9\u72ec\u7acb\u6e38\u620f\u5f00\u53d1\u8005\u7684\u5f71\u54cd\uff0c\u53d1\u73b0AI\u5de5\u5177\u666e\u53ca\u540e\u66f4\u591a\u72ec\u7acb\u5f00\u53d1\u8005\u8fdb\u5165\u5e02\u573a\uff0c\u4f46\u5176AI\u4f7f\u7528\u76ee\u7684\u4e0e\u975e\u72ec\u7acb\u5f00\u53d1\u8005\u76f8\u4f3c", "motivation": "\u7814\u7a76\u751f\u6210\u5f0fAI\u5bf9\u6e38\u620f\u4ea7\u4e1a\uff0c\u7279\u522b\u662f\u72ec\u7acb\u5f00\u53d1\u8005\u7684\u5f71\u54cd\uff0c\u63a2\u8ba8AI\u5de5\u5177\u666e\u53ca\u5982\u4f55\u6539\u53d8\u6e38\u620f\u5f00\u53d1\u5e02\u573a\u683c\u5c40", "method": "\u5229\u7528\u516c\u5f00\u6570\u636e\u548c\u6e38\u620fAI\u62ab\u9732\u58f0\u660e\uff0c\u5206\u6790AI\u5de5\u5177\u666e\u53ca\u524d\u540e\u72ec\u7acb\u5f00\u53d1\u8005\u5e02\u573a\u8fdb\u5165\u60c5\u51b5\u548c\u4f7f\u7528\u6a21\u5f0f", "result": "\u751f\u6210\u5f0fAI\u516c\u5f00\u53ef\u7528\u540e\uff0c\u76f8\u5bf9\u66f4\u591a\u7684\u72ec\u7acb\u5f00\u53d1\u8005\u8fdb\u5165\u5e02\u573a\uff1b\u4f46\u72ec\u7acb\u5f00\u53d1\u8005\u4e0e\u975e\u72ec\u7acb\u5f00\u53d1\u8005\u7684AI\u4f7f\u7528\u76ee\u7684\u76f8\u4f3c\uff1b\u6e38\u620f\u7279\u5f81\u5206\u6790\u663e\u793aAI\u5bf9\u72ec\u7acb\u5f00\u53d1\u8005\u6709\u5fae\u5999\u5f71\u54cd", "conclusion": "AI\u5de5\u5177\u964d\u4f4e\u4e86\u6e38\u620f\u5f00\u53d1\u95e8\u69db\uff0c\u4fc3\u8fdb\u4e86\u72ec\u7acb\u5f00\u53d1\u8005\u8fdb\u5165\u5e02\u573a\uff0c\u4f46\u4f7f\u7528\u6a21\u5f0f\u4e0e\u5927\u578b\u5f00\u53d1\u5546\u76f8\u4f3c\uff0cAI\u5bf9\u72ec\u7acb\u5f00\u53d1\u8005\u7684\u5f71\u54cd\u9700\u8981\u66f4\u7ec6\u81f4\u7684\u5206\u6790"}}
{"id": "2509.15037", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.15037", "abs": "https://arxiv.org/abs/2509.15037", "authors": ["Adrian Wiltz", "Dimos V. Dimarogonas"], "title": "On Uniformly Time-Varying Control Barrier Functions", "comment": "13 pages, 7 figures", "summary": "This paper investigates the design of a subclass of time-varying Control\nBarrier Functions (CBFs), specifically that of uniformly time-varying CBFs.\nLeveraging the fact that CBFs encode a system's dynamic capabilities relative\nto a state constraint, we decouple the design of uniformly time-varying CBFs\ninto a time-invariant and a time-varying component. We characterize the\nsubclass of time-invariant CBFs that yield a uniformly time-varying CBF when\ncombined with a specific type of time-varying function. A detailed analysis of\nthose conditions under which the time-varying function preserves the CBF\nproperty of the time-invariant component is provided. These conditions allow\nfor selecting the time-varying function such that diverse variations in the\nstate constraints can be captured while avoiding the redesign of the\ntime-invariant component. From a technical point of view, the analysis requires\nthe derivation of novel relations for comparison functions, not previously\nreported in the literature. We further relax the requirements on the\ntime-varying function, showing that forward invariance can still be ensured\neven when the uniformly time-varying value function does not strictly\nconstitute a CBF. Finally, we discuss how existing CBF construction methods can\nbe applied to design suitable time-invariant CBFs, and demonstrate the\neffectiveness of the approach through detailed numerical examples.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e00\u7c7b\u65f6\u53d8\u63a7\u5236\u5c4f\u969c\u51fd\u6570(CBFs)\u7684\u8bbe\u8ba1\uff0c\u7279\u522b\u662f\u5747\u5300\u65f6\u53d8CBFs\u3002\u901a\u8fc7\u5c06CBF\u8bbe\u8ba1\u5206\u89e3\u4e3a\u65f6\u4e0d\u53d8\u548c\u65f6\u53d8\u5206\u91cf\uff0c\u63d0\u51fa\u4e86\u4fdd\u6301CBF\u7279\u6027\u7684\u6761\u4ef6\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u5728\u4e0d\u91cd\u65b0\u8bbe\u8ba1\u65f6\u4e0d\u53d8\u5206\u91cf\u7684\u60c5\u51b5\u4e0b\u6355\u6349\u72b6\u6001\u7ea6\u675f\u7684\u591a\u6837\u5316\u53d8\u5316\u3002", "motivation": "\u7814\u7a76\u65f6\u53d8CBFs\u7684\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u72b6\u6001\u7ea6\u675f\u968f\u65f6\u95f4\u53d8\u5316\u65f6\u7684\u63a7\u5236\u95ee\u9898\uff0c\u907f\u514d\u5bf9\u65f6\u4e0d\u53d8\u5206\u91cf\u8fdb\u884c\u91cd\u65b0\u8bbe\u8ba1\uff0c\u63d0\u9ad8\u63a7\u5236\u7cfb\u7edf\u7684\u9002\u5e94\u6027\u548c\u7075\u6d3b\u6027\u3002", "method": "\u5c06\u5747\u5300\u65f6\u53d8CBFs\u8bbe\u8ba1\u5206\u89e3\u4e3a\u65f6\u4e0d\u53d8\u5206\u91cf\u548c\u65f6\u53d8\u5206\u91cf\uff0c\u5206\u6790\u65f6\u53d8\u51fd\u6570\u4fdd\u6301CBF\u7279\u6027\u7684\u6761\u4ef6\uff0c\u63a8\u5bfc\u65b0\u7684\u6bd4\u8f83\u51fd\u6570\u5173\u7cfb\uff0c\u5e76\u653e\u5bbd\u5bf9\u65f6\u53d8\u51fd\u6570\u7684\u8981\u6c42\u3002", "result": "\u63d0\u51fa\u4e86\u4fdd\u6301CBF\u7279\u6027\u7684\u5177\u4f53\u6761\u4ef6\uff0c\u8bc1\u660e\u4e86\u5373\u4f7f\u5747\u5300\u65f6\u53d8\u503c\u51fd\u6570\u4e0d\u4e25\u683c\u6784\u6210CBF\uff0c\u4ecd\u80fd\u786e\u4fdd\u524d\u5411\u4e0d\u53d8\u6027\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u793a\u4f8b\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u65f6\u53d8\u7ea6\u675f\u4e0b\u7684\u63a7\u5236\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u73b0\u6709CBF\u6784\u9020\u65b9\u6cd5\u53ef\u7528\u4e8e\u8bbe\u8ba1\u5408\u9002\u7684\u65f6\u4e0d\u53d8CBFs\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.14380", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14380", "abs": "https://arxiv.org/abs/2509.14380", "authors": ["Seoyeon Choi", "Kanghyun Ryu", "Jonghoon Ock", "Negar Mehr"], "title": "CRAFT: Coaching Reinforcement Learning Autonomously using Foundation Models for Multi-Robot Coordination Tasks", "comment": null, "summary": "Multi-Agent Reinforcement Learning (MARL) provides a powerful framework for\nlearning coordination in multi-agent systems. However, applying MARL to\nrobotics still remains challenging due to high-dimensional continuous joint\naction spaces, complex reward design, and non-stationary transitions inherent\nto decentralized settings. On the other hand, humans learn complex coordination\nthrough staged curricula, where long-horizon behaviors are progressively built\nupon simpler skills. Motivated by this, we propose CRAFT: Coaching\nReinforcement learning Autonomously using Foundation models for multi-robot\ncoordination Tasks, a framework that leverages the reasoning capabilities of\nfoundation models to act as a \"coach\" for multi-robot coordination. CRAFT\nautomatically decomposes long-horizon coordination tasks into sequences of\nsubtasks using the planning capability of Large Language Models (LLMs). In what\nfollows, CRAFT trains each subtask using reward functions generated by LLM, and\nrefines them through a Vision Language Model (VLM)-guided reward-refinement\nloop. We evaluate CRAFT on multi-quadruped navigation and bimanual manipulation\ntasks, demonstrating its capability to learn complex coordination behaviors. In\naddition, we validate the multi-quadruped navigation policy in real hardware\nexperiments.", "AI": {"tldr": "CRAFT\u662f\u4e00\u4e2a\u5229\u7528\u57fa\u7840\u6a21\u578b\u4f5c\u4e3a\"\u6559\u7ec3\"\u7684\u591a\u673a\u5668\u4eba\u534f\u8c03\u6846\u67b6\uff0c\u901a\u8fc7LLM\u81ea\u52a8\u5206\u89e3\u957f\u65f6\u7a0b\u4efb\u52a1\u4e3a\u5b50\u4efb\u52a1\u5e8f\u5217\uff0c\u5e76\u7528VLM\u6307\u5bfc\u5956\u52b1\u51fd\u6570\u4f18\u5316\uff0c\u5728\u56db\u8db3\u673a\u5668\u4eba\u5bfc\u822a\u548c\u53cc\u624b\u64cd\u4f5c\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027", "motivation": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u9762\u4e34\u9ad8\u7ef4\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u3001\u590d\u6742\u5956\u52b1\u8bbe\u8ba1\u548c\u975e\u5e73\u7a33\u73af\u5883\u7b49\u6311\u6218\uff0c\u800c\u4eba\u7c7b\u901a\u8fc7\u5206\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u590d\u6742\u534f\u8c03\u884c\u4e3a\uff0c\u53d7\u6b64\u542f\u53d1\u63d0\u51faCRAFT\u6846\u67b6", "method": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u89c4\u5212\u80fd\u529b\u81ea\u52a8\u5206\u89e3\u957f\u65f6\u7a0b\u534f\u8c03\u4efb\u52a1\u4e3a\u5b50\u4efb\u52a1\u5e8f\u5217\uff0c\u7528LLM\u751f\u6210\u6bcf\u4e2a\u5b50\u4efb\u52a1\u7684\u5956\u52b1\u51fd\u6570\uff0c\u5e76\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLM)\u6307\u5bfc\u7684\u5956\u52b1\u4f18\u5316\u5faa\u73af\u8fdb\u884c\u7cbe\u70bc", "result": "\u5728\u591a\u56db\u8db3\u673a\u5668\u4eba\u5bfc\u822a\u548c\u53cc\u624b\u64cd\u4f5c\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u5b66\u4e60\u590d\u6742\u534f\u8c03\u884c\u4e3a\u7684\u80fd\u529b\uff0c\u5e76\u5728\u771f\u5b9e\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u591a\u56db\u8db3\u673a\u5668\u4eba\u5bfc\u822a\u7b56\u7565", "conclusion": "CRAFT\u6846\u67b6\u6210\u529f\u5229\u7528\u57fa\u7840\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u4f5c\u4e3a\u591a\u673a\u5668\u4eba\u534f\u8c03\u7684\u6559\u7ec3\uff0c\u80fd\u591f\u81ea\u52a8\u5206\u89e3\u548c\u8bad\u7ec3\u590d\u6742\u534f\u8c03\u4efb\u52a1\uff0c\u4e3a\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5b66\u4e60\u8303\u5f0f"}}
{"id": "2509.14485", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14485", "abs": "https://arxiv.org/abs/2509.14485", "authors": ["Marko Tesic", "Yue Zhao", "Joel Z. Leibo", "Rakshit S. Trivedi", "Jose Hernandez-Orallo"], "title": "Beyond the high score: Prosocial ability profiles of multi-agent populations", "comment": null, "summary": "The development and evaluation of social capabilities in AI agents require\ncomplex environments where competitive and cooperative behaviours naturally\nemerge. While game-theoretic properties can explain why certain teams or agent\npopulations outperform others, more abstract behaviours, such as convention\nfollowing, are harder to control in training and evaluation settings. The\nMelting Pot contest is a social AI evaluation suite designed to assess the\ncooperation capabilities of AI systems. In this paper, we apply a Bayesian\napproach known as Measurement Layouts to infer the capability profiles of\nmulti-agent systems in the Melting Pot contest. We show that these capability\nprofiles not only predict future performance within the Melting Pot suite but\nalso reveal the underlying prosocial abilities of agents. Our analysis\nindicates that while higher prosocial capabilities sometimes correlate with\nbetter performance, this is not a universal trend-some lower-scoring agents\nexhibit stronger cooperation abilities. Furthermore, we find that\ntop-performing contest submissions are more likely to achieve high scores in\nscenarios where prosocial capabilities are not required. These findings,\ntogether with reports that the contest winner used a hard-coded solution\ntailored to specific environments, suggest that at least one top-performing\nteam may have optimised for conditions where cooperation was not necessary,\npotentially exploiting limitations in the evaluation framework. We provide\nrecommendations for improving the annotation of cooperation demands and propose\nfuture research directions to account for biases introduced by different\ntesting environments. Our results demonstrate that Measurement Layouts offer\nboth strong predictive accuracy and actionable insights, contributing to a more\ntransparent and generalisable approach to evaluating AI systems in complex\nsocial settings.", "AI": {"tldr": "\u672c\u6587\u5e94\u7528\u8d1d\u53f6\u65af\u6d4b\u91cf\u5e03\u5c40\u65b9\u6cd5\u5206\u6790Melting Pot\u7ade\u8d5b\u4e2d\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u80fd\u529b\u7279\u5f81\uff0c\u53d1\u73b0\u9ad8\u793e\u4f1a\u6027\u80fd\u529b\u5e76\u4e0d\u603b\u662f\u5e26\u6765\u66f4\u597d\u6027\u80fd\uff0c\u9876\u7ea7\u53c2\u8d5b\u65b9\u6848\u53ef\u80fd\u5728\u4e0d\u9700\u8981\u5408\u4f5c\u7684\u573a\u666f\u4e2d\u5f97\u5206\u66f4\u9ad8\uff0c\u63ed\u793a\u4e86\u8bc4\u4f30\u6846\u67b6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u9700\u8981\u8bc4\u4f30AI\u667a\u80fd\u4f53\u5728\u590d\u6742\u793e\u4f1a\u73af\u5883\u4e2d\u7684\u5408\u4f5c\u80fd\u529b\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u63a7\u5236\u548c\u8bc4\u4f30\u62bd\u8c61\u7684\u793e\u4f1a\u884c\u4e3a\u5982\u60ef\u4f8b\u9075\u5faa\uff0cMelting Pot\u7ade\u8d5b\u4f5c\u4e3a\u793e\u4f1aAI\u8bc4\u4f30\u5957\u4ef6\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u6d4b\u91cf\u5e03\u5c40\u65b9\u6cd5\u63a8\u65ad\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728Melting Pot\u7ade\u8d5b\u4e2d\u7684\u80fd\u529b\u7279\u5f81\uff0c\u5206\u6790\u80fd\u529b\u7279\u5f81\u4e0e\u672a\u6765\u8868\u73b0\u7684\u5173\u7cfb\u3002", "result": "\u80fd\u529b\u7279\u5f81\u4e0d\u4ec5\u80fd\u9884\u6d4b\u672a\u6765\u8868\u73b0\uff0c\u8fd8\u63ed\u793a\u4e86\u667a\u80fd\u4f53\u7684\u4eb2\u793e\u4f1a\u80fd\u529b\uff1b\u9ad8\u4eb2\u793e\u4f1a\u80fd\u529b\u6709\u65f6\u4e0e\u66f4\u597d\u6027\u80fd\u76f8\u5173\u4f46\u975e\u666e\u904d\u8d8b\u52bf\uff1b\u9876\u7ea7\u53c2\u8d5b\u65b9\u6848\u66f4\u6613\u5728\u4e0d\u9700\u8981\u4eb2\u793e\u4f1a\u80fd\u529b\u7684\u573a\u666f\u4e2d\u83b7\u5f97\u9ad8\u5206\uff1b\u7ade\u8d5b\u83b7\u80dc\u8005\u53ef\u80fd\u4f7f\u7528\u4e86\u9488\u5bf9\u7279\u5b9a\u73af\u5883\u7684\u786c\u7f16\u7801\u65b9\u6848\u3002", "conclusion": "\u6d4b\u91cf\u5e03\u5c40\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5f3a\u9884\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u64cd\u4f5c\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u5efa\u7acb\u66f4\u900f\u660e\u548c\u53ef\u6cdb\u5316\u7684AI\u7cfb\u7edf\u8bc4\u4f30\u65b9\u6cd5\uff1b\u9700\u8981\u6539\u8fdb\u5408\u4f5c\u9700\u6c42\u6807\u6ce8\u5e76\u8003\u8651\u4e0d\u540c\u6d4b\u8bd5\u73af\u5883\u5f15\u5165\u7684\u504f\u5dee\u3002"}}
{"id": "2509.15122", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2509.15122", "abs": "https://arxiv.org/abs/2509.15122", "authors": ["Anthony Howell", "Jieshu Wang", "Luyu Du", "Julia Melkers", "Varshil Shah"], "title": "Prestige over merit: An adapted audit of LLM bias in peer review", "comment": null, "summary": "Large language models (LLMs) are playing an increasingly integral, though\nlargely informal, role in scholarly peer review. Yet it remains unclear whether\nLLMs reproduce the biases observed in human decision-making. We adapt a\nresume-style audit to scientific publishing, developing a multi-role LLM\nsimulation (editor/reviewer) that evaluates a representative set of\nhigh-quality manuscripts across the physical, biological, and social sciences\nunder randomized author identities (institutional prestige, gender, race). The\naudit reveals a strong and consistent institutional-prestige bias: identical\npapers attributed to low-prestige affiliations face a significantly higher risk\nof rejection, despite only modest differences in LLM-assessed quality. To probe\nmechanisms, we generate synthetic CVs for the same author profiles; these\nencode large prestige-linked disparities and an inverted prestige-tenure\ngradient relative to national benchmarks. The results suggest that both domain\nnorms and prestige-linked priors embedded in training data shape paper-level\noutcomes once identity is visible, converting affiliation into a decisive\nstatus cue.", "AI": {"tldr": "LLM\u5728\u5b66\u672f\u8bc4\u5ba1\u4e2d\u5b58\u5728\u673a\u6784\u58f0\u671b\u504f\u89c1\uff0c\u76f8\u540c\u8bba\u6587\u6765\u81ea\u4f4e\u58f0\u671b\u673a\u6784\u65f6\u88ab\u62d2\u98ce\u9669\u663e\u8457\u66f4\u9ad8\uff0c\u5c3d\u7ba1\u8d28\u91cf\u5dee\u5f02\u4e0d\u5927", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b66\u672f\u540c\u884c\u8bc4\u5ba1\u4e2d\u662f\u5426\u590d\u5236\u4eba\u7c7b\u51b3\u7b56\u504f\u89c1\uff0c\u7279\u522b\u662f\u5728\u673a\u6784\u58f0\u671b\u3001\u6027\u522b\u548c\u79cd\u65cf\u65b9\u9762\u7684\u504f\u89c1", "method": "\u91c7\u7528\u7b80\u5386\u5f0f\u5ba1\u8ba1\u65b9\u6cd5\uff0c\u6784\u5efa\u591a\u89d2\u8272LLM\u6a21\u62df\uff08\u7f16\u8f91/\u5ba1\u7a3f\u4eba\uff09\uff0c\u968f\u673a\u5316\u4f5c\u8005\u8eab\u4efd\uff08\u673a\u6784\u58f0\u671b\u3001\u6027\u522b\u3001\u79cd\u65cf\uff09\u6765\u8bc4\u4f30\u9ad8\u8d28\u91cf\u8bba\u6587", "result": "\u53d1\u73b0\u5f3a\u70c8\u7684\u673a\u6784\u58f0\u671b\u504f\u89c1\uff1a\u76f8\u540c\u8bba\u6587\u6765\u81ea\u4f4e\u58f0\u671b\u673a\u6784\u65f6\u88ab\u62d2\u98ce\u9669\u663e\u8457\u66f4\u9ad8\uff1b\u5408\u6210CV\u663e\u793a\u58f0\u671b\u76f8\u5173\u5dee\u5f02\u5de8\u5927\u4e14\u4e0e\u56fd\u5bb6\u6807\u51c6\u76f8\u53cd\u7684\u58f0\u671b-\u4efb\u671f\u68af\u5ea6", "conclusion": "\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u9886\u57df\u89c4\u8303\u548c\u58f0\u671b\u76f8\u5173\u5148\u9a8c\u5728\u8eab\u4efd\u53ef\u89c1\u65f6\u5f71\u54cd\u8bba\u6587\u8bc4\u5ba1\u7ed3\u679c\uff0c\u5c06\u673a\u6784\u96b6\u5c5e\u5173\u7cfb\u8f6c\u5316\u4e3a\u51b3\u5b9a\u6027\u5730\u4f4d\u4fe1\u53f7"}}
{"id": "2509.15071", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.15071", "abs": "https://arxiv.org/abs/2509.15071", "authors": ["Haechan Pyon", "Gyunghoon Park"], "title": "A Nonlinear Scaling-based Design of Control Lyapunov-barrier Function for Relative Degree 2 Case and its Application to Safe Feedback Linearization", "comment": "7 pages, 6 figures. conference paper. Accepted to the 64th Conference\n  on Decision and Control (CDC 2025)", "summary": "In this paper we address the problem of control Lyapunov-barrier function\n(CLBF)-based safe stabilization for a class of nonlinear control-affine\nsystems. A difficulty may arise for the case when a constraint has the relative\ndegree larger than 1, at which computing a proper CLBF is not straightforward.\nInstead of adding an (possibly non-existent) control barrier function (CBF) to\na control Lyapunov function (CLF), our key idea is to simply scale the value of\nthe CLF on the unsafe set, by utilizing a sigmoid function as a scaling factor.\nWe provide a systematic design method for the CLBF, with a detailed condition\nfor the parameters of the sigmoid function to satisfy. It is also seen that the\nproposed approach to the CLBF design can be applied to the problem of\ntask-space control for a planar robot manipulator with guaranteed safety, for\nwhich a safe feedback linearization-based controller is presented.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a7\u5236Lyapunov-\u969c\u788d\u51fd\u6570\u7684\u975e\u7ebf\u6027\u63a7\u5236\u7cfb\u7edf\u5b89\u5168\u7a33\u5b9a\u65b9\u6cd5\uff0c\u901a\u8fc7sigmoid\u51fd\u6570\u7f29\u653eLyapunov\u51fd\u6570\u503c\u6765\u89e3\u51b3\u9ad8\u76f8\u5bf9\u5ea6\u7ea6\u675f\u95ee\u9898", "motivation": "\u5f53\u7ea6\u675f\u5177\u6709\u8f83\u9ad8\u76f8\u5bf9\u5ea6\u65f6\uff0c\u4f20\u7edf\u7684\u63a7\u5236Lyapunov-\u969c\u788d\u51fd\u6570\u8bbe\u8ba1\u53d8\u5f97\u56f0\u96be\uff0c\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u8bbe\u8ba1\u65b9\u6cd5\u6765\u5904\u7406\u8fd9\u79cd\u60c5\u51b5", "method": "\u4f7f\u7528sigmoid\u51fd\u6570\u4f5c\u4e3a\u7f29\u653e\u56e0\u5b50\uff0c\u5728\u975e\u5b89\u5168\u96c6\u4e0a\u7f29\u653e\u63a7\u5236Lyapunov\u51fd\u6570\u7684\u503c\uff0c\u800c\u4e0d\u662f\u6dfb\u52a0\u53ef\u80fd\u4e0d\u5b58\u5728\u7684\u63a7\u5236\u969c\u788d\u51fd\u6570", "result": "\u63d0\u4f9b\u4e86CLBF\u7684\u7cfb\u7edf\u5316\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u7ed9\u51fa\u4e86sigmoid\u51fd\u6570\u53c2\u6570\u7684\u8be6\u7ec6\u6761\u4ef6\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u5e73\u9762\u673a\u5668\u4eba\u64cd\u4f5c\u5668\u7684\u4efb\u52a1\u7a7a\u95f4\u63a7\u5236", "conclusion": "\u6240\u63d0\u51fa\u7684CLBF\u8bbe\u8ba1\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u9ad8\u76f8\u5bf9\u5ea6\u7ea6\u675f\u7684\u5b89\u5168\u7a33\u5b9a\u95ee\u9898\uff0c\u4e3a\u673a\u5668\u4eba\u5b89\u5168\u63a7\u5236\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.14383", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14383", "abs": "https://arxiv.org/abs/2509.14383", "authors": ["Yuhong Lu"], "title": "RLBind: Adversarial-Invariant Cross-Modal Alignment for Unified Robust Embeddings", "comment": "This paper is submitted to IEEE International Conference on Robotics\n  and Automation (ICRA) 2026", "summary": "Unified multi-modal encoders that bind vision, audio, and other sensors into\na shared embedding space are attractive building blocks for robot perception\nand decision-making. However, on-robot deployment exposes the vision branch to\nadversarial and natural corruptions, making robustness a prerequisite for\nsafety. Prior defenses typically align clean and adversarial features within\nCLIP-style encoders and overlook broader cross-modal correspondence, yielding\nmodest gains and often degrading zero-shot transfer. We introduce RLBind, a\ntwo-stage adversarial-invariant cross-modal alignment framework for robust\nunified embeddings. Stage 1 performs unsupervised fine-tuning on\nclean-adversarial pairs to harden the visual encoder. Stage 2 leverages\ncross-modal correspondence by minimizing the discrepancy between\nclean/adversarial features and a text anchor, while enforcing class-wise\ndistributional alignment across modalities. Extensive experiments on Image,\nAudio, Thermal, and Video data show that RLBind consistently outperforms the\nLanguageBind backbone and standard fine-tuning baselines in both clean accuracy\nand norm-bounded adversarial robustness. By improving resilience without\nsacrificing generalization, RLBind provides a practical path toward safer\nmulti-sensor perception stacks for embodied robots in navigation, manipulation,\nand other autonomy settings.", "AI": {"tldr": "RLBind\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u5bf9\u6297\u4e0d\u53d8\u6027\u8de8\u6a21\u6001\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u5fae\u8c03\u548c\u8de8\u6a21\u6001\u5bf9\u5e94\u6027\u6700\u5c0f\u5316\uff0c\u63d0\u5347\u7edf\u4e00\u591a\u6a21\u6001\u7f16\u7801\u5668\u5728\u5bf9\u6297\u548c\u81ea\u7136\u635f\u574f\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u673a\u5668\u4eba\u90e8\u7f72\u4e2d\u89c6\u89c9\u5206\u652f\u9762\u4e34\u5bf9\u6297\u6027\u548c\u81ea\u7136\u635f\u574f\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u901a\u5e38\u53ea\u5728CLIP\u98ce\u683c\u7f16\u7801\u5668\u5185\u5bf9\u9f50\u6e05\u6d01\u548c\u5bf9\u6297\u7279\u5f81\uff0c\u5ffd\u89c6\u4e86\u66f4\u5e7f\u6cdb\u7684\u8de8\u6a21\u6001\u5bf9\u5e94\u6027\uff0c\u5bfc\u81f4\u589e\u76ca\u6709\u9650\u4e14\u5f80\u5f80\u964d\u4f4e\u96f6\u6837\u672c\u8fc1\u79fb\u80fd\u529b\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u5bf9\u6e05\u6d01-\u5bf9\u6297\u5bf9\u8fdb\u884c\u65e0\u76d1\u7763\u5fae\u8c03\u4ee5\u5f3a\u5316\u89c6\u89c9\u7f16\u7801\u5668\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5229\u7528\u8de8\u6a21\u6001\u5bf9\u5e94\u6027\uff0c\u6700\u5c0f\u5316\u6e05\u6d01/\u5bf9\u6297\u7279\u5f81\u4e0e\u6587\u672c\u951a\u70b9\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u540c\u65f6\u5f3a\u5236\u8de8\u6a21\u6001\u7684\u7c7b\u7ea7\u5206\u5e03\u5bf9\u9f50\u3002", "result": "\u5728\u56fe\u50cf\u3001\u97f3\u9891\u3001\u70ed\u6210\u50cf\u548c\u89c6\u9891\u6570\u636e\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cRLBind\u5728\u6e05\u6d01\u51c6\u786e\u7387\u548c\u6709\u754c\u5bf9\u6297\u9c81\u68d2\u6027\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8eLanguageBind\u4e3b\u5e72\u548c\u6807\u51c6\u5fae\u8c03\u57fa\u7ebf\u3002", "conclusion": "RLBind\u901a\u8fc7\u5728\u4e0d\u727a\u7272\u6cdb\u5316\u80fd\u529b\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u5f39\u6027\uff0c\u4e3a\u5177\u8eab\u673a\u5668\u4eba\u5728\u5bfc\u822a\u3001\u64cd\u4f5c\u548c\u5176\u4ed6\u81ea\u4e3b\u8bbe\u7f6e\u4e2d\u63d0\u4f9b\u4e86\u66f4\u5b89\u5168\u7684\u591a\u4f20\u611f\u5668\u611f\u77e5\u5806\u6808\u7684\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2509.14507", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14507", "abs": "https://arxiv.org/abs/2509.14507", "authors": ["Jian Chen", "Zhenyan Chen", "Xuming Hu", "Peilin Zhou", "Yining Hua", "Han Fang", "Cissy Hing Yee Choy", "Xinmei Ke", "Jingfeng Luo", "Zixuan Yuan"], "title": "DeKeyNLU: Enhancing Natural Language to SQL Generation through Task Decomposition and Keyword Extraction", "comment": null, "summary": "Natural Language to SQL (NL2SQL) provides a new model-centric paradigm that\nsimplifies database access for non-technical users by converting natural\nlanguage queries into SQL commands. Recent advancements, particularly those\nintegrating Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)\nreasoning, have made significant strides in enhancing NL2SQL performance.\nHowever, challenges such as inaccurate task decomposition and keyword\nextraction by LLMs remain major bottlenecks, often leading to errors in SQL\ngeneration. While existing datasets aim to mitigate these issues by fine-tuning\nmodels, they struggle with over-fragmentation of tasks and lack of\ndomain-specific keyword annotations, limiting their effectiveness. To address\nthese limitations, we present DeKeyNLU, a novel dataset which contains 1,500\nmeticulously annotated QA pairs aimed at refining task decomposition and\nenhancing keyword extraction precision for the RAG pipeline. Fine-tuned with\nDeKeyNLU, we propose DeKeySQL, a RAG-based NL2SQL pipeline that employs three\ndistinct modules for user question understanding, entity retrieval, and\ngeneration to improve SQL generation accuracy. We benchmarked multiple model\nconfigurations within DeKeySQL RAG pipeline. Experimental results demonstrate\nthat fine-tuning with DeKeyNLU significantly improves SQL generation accuracy\non both BIRD (62.31% to 69.10%) and Spider (84.2% to 88.7%) dev datasets.", "AI": {"tldr": "DeKeyNLU\u662f\u4e00\u4e2a\u5305\u542b1500\u4e2a\u6807\u6ce8QA\u5bf9\u7684\u65b0\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u6539\u8fdbRAG\u6d41\u7a0b\u4e2d\u7684\u4efb\u52a1\u5206\u89e3\u548c\u5173\u952e\u8bcd\u63d0\u53d6\u7cbe\u5ea6\u3002\u57fa\u4e8e\u8be5\u6570\u636e\u96c6\u5fae\u8c03\u7684DeKeySQL\u7ba1\u9053\u5728BIRD\u548cSpider\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86SQL\u751f\u6210\u51c6\u786e\u7387\u3002", "motivation": "\u5f53\u524dNL2SQL\u7cfb\u7edf\u5728\u4efb\u52a1\u5206\u89e3\u548c\u5173\u952e\u8bcd\u63d0\u53d6\u65b9\u9762\u5b58\u5728\u74f6\u9888\uff0c\u5bfc\u81f4SQL\u751f\u6210\u9519\u8bef\u3002\u73b0\u6709\u6570\u636e\u96c6\u5b58\u5728\u4efb\u52a1\u8fc7\u5ea6\u788e\u7247\u5316\u548c\u7f3a\u4e4f\u9886\u57df\u7279\u5b9a\u5173\u952e\u8bcd\u6807\u6ce8\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86DeKeyNLU\u6570\u636e\u96c6\uff0c\u5305\u542b1500\u4e2a\u7cbe\u5fc3\u6807\u6ce8\u7684QA\u5bf9\u3002\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86DeKeySQL\u7ba1\u9053\uff0c\u91c7\u7528\u4e09\u4e2a\u6a21\u5757\uff1a\u7528\u6237\u95ee\u9898\u7406\u89e3\u3001\u5b9e\u4f53\u68c0\u7d22\u548c\u751f\u6210\uff0c\u6765\u6539\u8fdbSQL\u751f\u6210\u51c6\u786e\u6027\u3002", "result": "\u5728BIRD\u6570\u636e\u96c6\u4e0a\u51c6\u786e\u7387\u4ece62.31%\u63d0\u5347\u523069.10%\uff0c\u5728Spider\u6570\u636e\u96c6\u4e0a\u4ece84.2%\u63d0\u5347\u523088.7%\u3002", "conclusion": "DeKeyNLU\u6570\u636e\u96c6\u548cDeKeySQL\u7ba1\u9053\u6709\u6548\u89e3\u51b3\u4e86NL2SQL\u4e2d\u7684\u4efb\u52a1\u5206\u89e3\u548c\u5173\u952e\u8bcd\u63d0\u53d6\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86SQL\u751f\u6210\u6027\u80fd\u3002"}}
{"id": "2509.15132", "categories": ["cs.CY", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.15132", "abs": "https://arxiv.org/abs/2509.15132", "authors": ["Anthony Howell", "Nancy Wu", "Sharmistha Bagchi", "Yushim Kim", "Chayn Sun"], "title": "From Pixels to Urban Policy-Intelligence: Recovering Legacy Effects of Redlining with a Multimodal LLM", "comment": null, "summary": "This paper shows how a multimodal large language model (MLLM) can expand\nurban measurement capacity and support tracking of place-based policy\ninterventions. Using a structured, reason-then-estimate pipeline on street-view\nimagery, GPT-4o infers neighborhood poverty and tree canopy, which we embed in\na quasi-experimental design evaluating the legacy of 1930s redlining. GPT-4o\nrecovers the expected adverse socio-environmental legacy effects of redlining,\nwith estimates statistically indistinguishable from authoritative sources, and\nit outperforms a conventional pixel-based segmentation baseline-consistent with\nthe idea that holistic scene reasoning extracts higher-order information beyond\nobject counts alone. These results position MLLMs as policy-grade instruments\nfor neighborhood measurement and motivate broader validation across\npolicy-evaluation settings.", "AI": {"tldr": "\u4f7f\u7528GPT-4o\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u8857\u666f\u56fe\u50cf\u5206\u6790\u793e\u533a\u8d2b\u56f0\u7387\u548c\u6811\u51a0\u8986\u76d6\u7387\uff0c\u9a8c\u8bc11930\u5e74\u4ee3\u7ea2\u7ebf\u653f\u7b56\u7684\u9057\u7559\u5f71\u54cd\uff0c\u7ed3\u679c\u4e0e\u4f20\u7edf\u6743\u5a01\u6570\u636e\u7edf\u8ba1\u65e0\u5dee\u5f02\u4e14\u4f18\u4e8e\u4f20\u7edf\u50cf\u7d20\u5206\u5272\u65b9\u6cd5", "motivation": "\u63a2\u7d22\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6269\u5c55\u57ce\u5e02\u6d4b\u91cf\u80fd\u529b\u548c\u652f\u6301\u57fa\u4e8e\u5730\u70b9\u7684\u653f\u7b56\u5e72\u9884\u8ddf\u8e2a\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u8bc4\u4f30\u5386\u53f2\u7ea2\u7ebf\u653f\u7b56\u7684\u793e\u4f1a\u73af\u5883\u9057\u7559\u5f71\u54cd", "method": "\u91c7\u7528\u7ed3\u6784\u5316\u63a8\u7406-\u4f30\u8ba1\u6d41\u7a0b\uff0c\u4f7f\u7528GPT-4o\u5206\u6790\u8857\u666f\u56fe\u50cf\u6765\u63a8\u65ad\u793e\u533a\u8d2b\u56f0\u6c34\u5e73\u548c\u6811\u51a0\u8986\u76d6\u7387\uff0c\u5e76\u5c06\u5176\u5d4c\u5165\u51c6\u5b9e\u9a8c\u8bbe\u8ba1\u4e2d\u8bc4\u4f301930\u5e74\u4ee3\u7ea2\u7ebf\u653f\u7b56", "result": "GPT-4o\u6210\u529f\u6062\u590d\u4e86\u7ea2\u7ebf\u653f\u7b56\u9884\u671f\u7684\u8d1f\u9762\u793e\u4f1a\u73af\u5883\u5f71\u54cd\uff0c\u4f30\u8ba1\u7ed3\u679c\u4e0e\u6743\u5a01\u6570\u636e\u6e90\u5728\u7edf\u8ba1\u4e0a\u65e0\u5dee\u5f02\uff0c\u4e14\u4f18\u4e8e\u4f20\u7edf\u7684\u57fa\u4e8e\u50cf\u7d20\u5206\u5272\u7684\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u53ef\u4f5c\u4e3a\u653f\u7b56\u7ea7\u522b\u7684\u793e\u533a\u6d4b\u91cf\u5de5\u5177\uff0c\u9700\u8981\u5728\u66f4\u591a\u653f\u7b56\u8bc4\u4f30\u573a\u666f\u4e2d\u8fdb\u884c\u5e7f\u6cdb\u9a8c\u8bc1"}}
{"id": "2509.15099", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.15099", "abs": "https://arxiv.org/abs/2509.15099", "authors": ["Taoyuan Yu", "Kui Wang", "Zongdian Li", "Tao Yu", "Kei Sakaguchi", "Walid Saad"], "title": "Digital Twin-based Cooperative Autonomous Driving in Smart Intersections: A Multi-Agent Reinforcement Learning Approach", "comment": null, "summary": "Unsignalized intersections pose safety and efficiency challenges due to\ncomplex traffic flows and blind spots. In this paper, a digital twin (DT)-based\ncooperative driving system with roadside unit (RSU)-centric architecture is\nproposed for enhancing safety and efficiency at unsignalized intersections. The\nsystem leverages comprehensive bird-eye-view (BEV) perception to eliminate\nblind spots and employs a hybrid reinforcement learning (RL) framework\ncombining offline pre-training with online fine-tuning. Specifically, driving\npolicies are initially trained using conservative Q-learning (CQL) with\nbehavior cloning (BC) on real datasets, then fine-tuned using multi-agent\nproximal policy optimization (MAPPO) with self-attention mechanisms to handle\ndynamic multi-agent coordination. The RSU implements real-time commands via\nvehicle-to-infrastructure (V2I) communications. Experimental results show that\nthe proposed method yields failure rates below 0.03\\% coordinating up to three\nconnected autonomous vehicles (CAVs), significantly outperforming traditional\nmethods. In addition, the system exhibits sub-linear computational scaling with\ninference times under 40 ms. Furthermore, it demonstrates robust generalization\nacross diverse unsignalized intersection scenarios, indicating its practicality\nand readiness for real-world deployment.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6570\u5b57\u5b6a\u751f\u548c\u8def\u4fa7\u5355\u5143\u7684\u534f\u540c\u9a7e\u9a76\u7cfb\u7edf\uff0c\u901a\u8fc7\u9e1f\u77b0\u611f\u77e5\u6d88\u9664\u76f2\u533a\uff0c\u91c7\u7528\u6df7\u5408\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u63d0\u5347\u65e0\u4fe1\u53f7\u4ea4\u53c9\u53e3\u7684\u5b89\u5168\u6027\u548c\u6548\u7387", "motivation": "\u65e0\u4fe1\u53f7\u4ea4\u53c9\u53e3\u5b58\u5728\u590d\u6742\u4ea4\u901a\u6d41\u548c\u76f2\u533a\u95ee\u9898\uff0c\u5bfc\u81f4\u5b89\u5168\u6027\u548c\u6548\u7387\u6311\u6218\uff0c\u9700\u8981\u521b\u65b0\u7684\u534f\u540c\u9a7e\u9a76\u89e3\u51b3\u65b9\u6848", "method": "\u91c7\u7528\u8def\u4fa7\u5355\u5143\u4e2d\u5fc3\u67b6\u6784\uff0c\u7ed3\u5408\u9e1f\u77b0\u611f\u77e5\u6d88\u9664\u76f2\u533a\uff0c\u4f7f\u7528\u6df7\u5408\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff08\u79bb\u7ebfCQL+BC\u9884\u8bad\u7ec3\uff0c\u5728\u7ebfMAPPO+\u81ea\u6ce8\u610f\u529b\u5fae\u8c03\uff09\uff0c\u901a\u8fc7V2I\u901a\u4fe1\u5b9e\u73b0\u5b9e\u65f6\u63a7\u5236", "result": "\u5931\u8d25\u7387\u4f4e\u4e8e0.03%\uff0c\u53ef\u534f\u8c03\u6700\u591a3\u8f86CAV\uff0c\u8ba1\u7b97\u65f6\u95f4\u4f4e\u4e8e40ms\uff0c\u5177\u6709\u4e9a\u7ebf\u6027\u8ba1\u7b97\u6269\u5c55\u6027\uff0c\u5728\u4e0d\u540c\u65e0\u4fe1\u53f7\u4ea4\u53c9\u53e3\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b", "conclusion": "\u8be5\u7cfb\u7edf\u5728\u5b89\u5168\u6027\u548c\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5177\u5907\u5b9e\u9645\u90e8\u7f72\u7684\u5b9e\u7528\u6027\u548c\u51c6\u5907\u5ea6"}}
{"id": "2509.14412", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14412", "abs": "https://arxiv.org/abs/2509.14412", "authors": ["Artem Lykov", "Oleg Kobzarev", "Dzmitry Tsetserukou"], "title": "GestOS: Advanced Hand Gesture Interpretation via Large Language Models to control Any Type of Robot", "comment": null, "summary": "We present GestOS, a gesture-based operating system for high-level control of\nheterogeneous robot teams. Unlike prior systems that map gestures to fixed\ncommands or single-agent actions, GestOS interprets hand gestures semantically\nand dynamically distributes tasks across multiple robots based on their\ncapabilities, current state, and supported instruction sets. The system\ncombines lightweight visual perception with large language model (LLM)\nreasoning: hand poses are converted into structured textual descriptions, which\nthe LLM uses to infer intent and generate robot-specific commands. A robot\nselection module ensures that each gesture-triggered task is matched to the\nmost suitable agent in real time. This architecture enables context-aware,\nadaptive control without requiring explicit user specification of targets or\ncommands. By advancing gesture interaction from recognition to intelligent\norchestration, GestOS supports scalable, flexible, and user-friendly\ncollaboration with robotic systems in dynamic environments.", "AI": {"tldr": "GestOS\u662f\u4e00\u4e2a\u57fa\u4e8e\u624b\u52bf\u7684\u64cd\u4f5c\u7cfb\u7edf\uff0c\u901a\u8fc7\u8bed\u4e49\u7406\u89e3\u548cLLM\u63a8\u7406\u5b9e\u73b0\u591a\u673a\u5668\u4eba\u56e2\u961f\u7684\u667a\u80fd\u4efb\u52a1\u5206\u914d\u4e0e\u63a7\u5236", "motivation": "\u73b0\u6709\u624b\u52bf\u63a7\u5236\u7cfb\u7edf\u901a\u5e38\u5c06\u624b\u52bf\u6620\u5c04\u4e3a\u56fa\u5b9a\u547d\u4ee4\u6216\u5355\u667a\u80fd\u4f53\u52a8\u4f5c\uff0c\u7f3a\u4e4f\u5bf9\u591a\u673a\u5668\u4eba\u56e2\u961f\u7684\u8bed\u4e49\u7406\u89e3\u548c\u52a8\u6001\u4efb\u52a1\u5206\u914d\u80fd\u529b\uff0c\u9700\u8981\u66f4\u667a\u80fd\u3001\u81ea\u9002\u5e94\u7684\u624b\u52bf\u4ea4\u4e92\u7cfb\u7edf", "method": "\u7ed3\u5408\u8f7b\u91cf\u7ea7\u89c6\u89c9\u611f\u77e5\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u63a8\u7406\uff1a\u5c06\u624b\u52bf\u59ff\u6001\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u6587\u672c\u63cf\u8ff0\uff0cLLM\u63a8\u65ad\u7528\u6237\u610f\u56fe\u5e76\u751f\u6210\u673a\u5668\u4eba\u7279\u5b9a\u547d\u4ee4\uff0c\u5b9e\u65f6\u5339\u914d\u6700\u9002\u5408\u7684\u673a\u5668\u4eba\u6267\u884c\u4efb\u52a1", "result": "\u5b9e\u73b0\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u81ea\u9002\u5e94\u63a7\u5236\uff0c\u65e0\u9700\u7528\u6237\u660e\u786e\u6307\u5b9a\u76ee\u6807\u6216\u547d\u4ee4\uff0c\u652f\u6301\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8fdb\u884c\u53ef\u6269\u5c55\u3001\u7075\u6d3b\u7684\u7528\u6237\u53cb\u597d\u578b\u673a\u5668\u4eba\u534f\u4f5c", "conclusion": "GestOS\u5c06\u624b\u52bf\u4ea4\u4e92\u4ece\u7b80\u5355\u8bc6\u522b\u63a8\u8fdb\u5230\u667a\u80fd\u7f16\u6392\uff0c\u4e3a\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u63d0\u4f9b\u4e86\u8bed\u4e49\u5316\u3001\u52a8\u6001\u5206\u5e03\u5f0f\u7684\u63a7\u5236\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.14546", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14546", "abs": "https://arxiv.org/abs/2509.14546", "authors": ["Zhilun Zhou", "Jing Yi Wang", "Nicholas Sukiennik", "Chen Gao", "Fengli Xu", "Yong Li", "James Evans"], "title": "Rationality Check! Benchmarking the Rationality of Large Language Models", "comment": null, "summary": "Large language models (LLMs), a recent advance in deep learning and machine\nintelligence, have manifested astonishing capacities, now considered among the\nmost promising for artificial general intelligence. With human-like\ncapabilities, LLMs have been used to simulate humans and serve as AI assistants\nacross many applications. As a result, great concern has arisen about whether\nand under what circumstances LLMs think and behave like real human agents.\nRationality is among the most important concepts in assessing human behavior,\nboth in thinking (i.e., theoretical rationality) and in taking action (i.e.,\npractical rationality). In this work, we propose the first benchmark for\nevaluating the omnibus rationality of LLMs, covering a wide range of domains\nand LLMs. The benchmark includes an easy-to-use toolkit, extensive experimental\nresults, and analysis that illuminates where LLMs converge and diverge from\nidealized human rationality. We believe the benchmark can serve as a\nfoundational tool for both developers and users of LLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7efc\u5408\u7406\u6027\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u591a\u4e2a\u9886\u57df\u548c\u6a21\u578b\uff0c\u5305\u542b\u6613\u7528\u5de5\u5177\u5305\u548c\u5b9e\u9a8c\u7ed3\u679c\u5206\u6790", "motivation": "\u968f\u7740LLMs\u5c55\u73b0\u7c7b\u4eba\u80fd\u529b\u5e76\u88ab\u5e7f\u6cdb\u7528\u4e8e\u6a21\u62df\u4eba\u7c7b\uff0c\u9700\u8981\u8bc4\u4f30\u5b83\u4eec\u5728\u4f55\u79cd\u60c5\u51b5\u4e0b\u50cf\u771f\u5b9e\u4eba\u7c7b\u4e00\u6837\u601d\u8003\u548c\u884c\u52a8\uff0c\u7406\u6027\u662f\u8bc4\u4f30\u4eba\u7c7b\u884c\u4e3a\u6700\u91cd\u8981\u7684\u6982\u5ff5\u4e4b\u4e00", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u62ec\u7406\u8bba\u7406\u6027\uff08\u601d\u8003\uff09\u548c\u5b9e\u8df5\u7406\u6027\uff08\u884c\u52a8\uff09\u7684\u8bc4\u4f30\uff0c\u5f00\u53d1\u4e86\u6613\u7528\u7684\u5de5\u5177\u5305\uff0c\u5e76\u5728\u591a\u4e2a\u9886\u57df\u548cLLM\u6a21\u578b\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c", "result": "\u901a\u8fc7\u5b9e\u9a8c\u63ed\u793a\u4e86LLMs\u4e0e\u7406\u60f3\u5316\u4eba\u7c7b\u7406\u6027\u7684\u8d8b\u540c\u548c\u5206\u6b67\u4e4b\u5904\uff0c\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u5b9e\u9a8c\u7ed3\u679c\u548c\u5206\u6790", "conclusion": "\u8be5\u57fa\u51c6\u6d4b\u8bd5\u53ef\u4ee5\u4f5c\u4e3aLLM\u5f00\u53d1\u8005\u548c\u7528\u6237\u7684\u57fa\u7840\u5de5\u5177\uff0c\u4e3a\u8bc4\u4f30\u6a21\u578b\u7406\u6027\u80fd\u529b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5"}}
{"id": "2509.15109", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.15109", "abs": "https://arxiv.org/abs/2509.15109", "authors": ["Chih-Yuan Chiu", "Zhouyu Zhang", "Glen Chou"], "title": "Learning Constraints from Stochastic Partially-Observed Closed-Loop Demonstrations", "comment": null, "summary": "We present an algorithm for learning unknown parametric constraints from\nlocally-optimal input-output trajectory data. We assume that the given data is\ngenerated by demonstrators with stochastic nonlinear dynamics who execute a\nstate or output feedback law to robustly satisfy the constraints despite\nworst-case dynamics and output noise. We encode the Karush-Kuhn-Tucker (KKT)\nconditions of this robust optimal output feedback control problem within a\nfeasibility problem to recover constraints consistent with the local optimality\nof the demonstrations. We prove that our constraint learning method (i)\naccurately recovers the demonstrator's state or output feedback policy, and\n(ii) conservatively estimates the set of all state or output feedback policies\nthat ensure constraint satisfaction despite worst-case noise realizations.\nMoreover, we perform sensitivity analysis, proving that when demonstrations are\ncorrupted by transmission error, the inaccuracy in the learned state or output\nfeedback law scales linearly in the error magnitude. Our method accurately\nrecovers unknown constraints from simulated noisy, closed-loop demonstrations\ngenerated using dynamics, both linear and nonlinear, (e.g., unicycle and\nquadrotor) and a range of state and output feedback mechanisms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5c40\u90e8\u6700\u4f18\u8f93\u5165\u8f93\u51fa\u8f68\u8ff9\u6570\u636e\u4e2d\u5b66\u4e60\u672a\u77e5\u53c2\u6570\u7ea6\u675f\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7KKT\u6761\u4ef6\u6062\u590d\u4e0e\u6f14\u793a\u6570\u636e\u5c40\u90e8\u6700\u4f18\u6027\u4e00\u81f4\u7684\u7ea6\u675f", "motivation": "\u4ece\u6f14\u793a\u8005\u7684\u6700\u4f18\u8f68\u8ff9\u6570\u636e\u4e2d\u5b66\u4e60\u672a\u77e5\u7ea6\u675f\uff0c\u8fd9\u4e9b\u6f14\u793a\u8005\u4f7f\u7528\u72b6\u6001\u6216\u8f93\u51fa\u53cd\u9988\u63a7\u5236\u6765\u5728\u5b58\u5728\u6700\u574f\u60c5\u51b5\u52a8\u6001\u548c\u8f93\u51fa\u566a\u58f0\u7684\u60c5\u51b5\u4e0b\u9c81\u68d2\u5730\u6ee1\u8db3\u7ea6\u675f", "method": "\u5c06\u9c81\u68d2\u6700\u4f18\u8f93\u51fa\u53cd\u9988\u63a7\u5236\u95ee\u9898\u7684KKT\u6761\u4ef6\u7f16\u7801\u4e3a\u53ef\u884c\u6027\u95ee\u9898\uff0c\u4ee5\u6062\u590d\u4e0e\u6f14\u793a\u6570\u636e\u5c40\u90e8\u6700\u4f18\u6027\u4e00\u81f4\u7684\u7ea6\u675f", "result": "\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u6062\u590d\u6f14\u793a\u8005\u7684\u72b6\u6001\u6216\u8f93\u51fa\u53cd\u9988\u7b56\u7565\uff0c\u5e76\u4fdd\u5b88\u4f30\u8ba1\u6240\u6709\u786e\u4fdd\u7ea6\u675f\u6ee1\u8db3\u7684\u7b56\u7565\u96c6\u5408\uff1b\u5728\u5b58\u5728\u4f20\u8f93\u8bef\u5dee\u65f6\uff0c\u5b66\u4e60\u8bef\u5dee\u4e0e\u8bef\u5dee\u5e45\u5ea6\u5448\u7ebf\u6027\u5173\u7cfb", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4ece\u566a\u58f0\u95ed\u73af\u6f14\u793a\u4e2d\u51c6\u786e\u6062\u590d\u672a\u77e5\u7ea6\u675f\uff0c\u9002\u7528\u4e8e\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u7cfb\u7edf\uff0c\u4ee5\u53ca\u5404\u79cd\u72b6\u6001\u548c\u8f93\u51fa\u53cd\u9988\u673a\u5236"}}
{"id": "2509.14421", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14421", "abs": "https://arxiv.org/abs/2509.14421", "authors": ["Dario Tscholl", "Yashwanth Nakka", "Brian Gunter"], "title": "Perception-Integrated Safety Critical Control via Analytic Collision Cone Barrier Functions on 3D Gaussian Splatting", "comment": "Preprint for IEEE L-CSS/ACC", "summary": "We present a perception-driven safety filter that converts each 3D Gaussian\nSplat (3DGS) into a closed-form forward collision cone, which in turn yields a\nfirst-order control barrier function (CBF) embedded within a quadratic program\n(QP). By exploiting the analytic geometry of splats, our formulation provides a\ncontinuous, closed-form representation of collision constraints that is both\nsimple and computationally efficient. Unlike distance-based CBFs, which tend to\nactivate reactively only when an obstacle is already close, our collision-cone\nCBF activates proactively, allowing the robot to adjust earlier and thereby\nproduce smoother and safer avoidance maneuvers at lower computational cost. We\nvalidate the method on a large synthetic scene with approximately 170k splats,\nwhere our filter reduces planning time by a factor of 3 and significantly\ndecreased trajectory jerk compared to a state-of-the-art 3DGS planner, while\nmaintaining the same level of safety. The approach is entirely analytic,\nrequires no high-order CBF extensions (HOCBFs), and generalizes naturally to\nrobots with physical extent through a principled Minkowski-sum inflation of the\nsplats. These properties make the method broadly applicable to real-time\nnavigation in cluttered, perception-derived extreme environments, including\nspace robotics and satellite systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u611f\u77e5\u9a71\u52a8\u5b89\u5168\u6ee4\u6ce2\u5668\uff0c\u5c06\u6bcf\u4e2a3DGS\u8f6c\u6362\u4e3a\u95ed\u5f0f\u524d\u5411\u78b0\u649e\u9525\uff0c\u751f\u6210\u4e00\u9636\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u4e3b\u52a8\u7684\u907f\u969c\u89c4\u5212\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u8ddd\u79bb\u7684CBF\u65b9\u6cd5\u5f80\u5f80\u5728\u969c\u788d\u7269\u5df2\u7ecf\u5f88\u8fd1\u65f6\u624d\u88ab\u52a8\u6fc0\u6d3b\uff0c\u5bfc\u81f4\u53cd\u5e94\u6ede\u540e\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u63d0\u524d\u9884\u8b66\u3001\u8ba1\u7b97\u9ad8\u6548\u4e14\u9002\u7528\u4e8e\u590d\u6742\u611f\u77e5\u73af\u5883\u7684\u4e3b\u52a8\u5b89\u5168\u89c4\u5212\u65b9\u6cd5\u3002", "method": "\u5229\u75283D\u9ad8\u65af\u6cfc\u6e85\u7684\u89e3\u6790\u51e0\u4f55\u7279\u6027\uff0c\u5c06\u6bcf\u4e2asplat\u8f6c\u6362\u4e3a\u95ed\u5f0f\u524d\u5411\u78b0\u649e\u9525\uff0c\u6784\u5efa\u4e00\u9636\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff0c\u5e76\u5d4c\u5165\u4e8c\u6b21\u89c4\u5212\u95ee\u9898\u4e2d\u3002\u901a\u8fc7Minkowski\u548c\u81a8\u80c0\u5904\u7406\u673a\u5668\u4eba\u7269\u7406\u5c3a\u5bf8\u3002", "result": "\u5728\u5305\u542b\u7ea617\u4e07\u4e2asplat\u7684\u5927\u578b\u5408\u6210\u573a\u666f\u4e2d\uff0c\u89c4\u5212\u65f6\u95f4\u51cf\u5c113\u500d\uff0c\u8f68\u8ff9\u6296\u52a8\u663e\u8457\u964d\u4f4e\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u540c\u5b89\u5168\u6c34\u5e73\u3002\u65b9\u6cd5\u5b8c\u5168\u89e3\u6790\uff0c\u65e0\u9700\u9ad8\u9636CBF\u6269\u5c55\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b9e\u65f6\u5bfc\u822a\u63d0\u4f9b\u4e86\u8ba1\u7b97\u9ad8\u6548\u3001\u4e3b\u52a8\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u7a7a\u95f4\u673a\u5668\u4eba\u548c\u536b\u661f\u7cfb\u7edf\u7b49\u590d\u6742\u611f\u77e5\u73af\u5883\u4e2d\u7684\u907f\u969c\u5e94\u7528\u3002"}}
{"id": "2509.14547", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14547", "abs": "https://arxiv.org/abs/2509.14547", "authors": ["Yi Lin", "Lujin Zhao", "Yijie Shi"], "title": "(P)rior(D)yna(F)low: A Priori Dynamic Workflow Construction via Multi-Agent Collaboration", "comment": null, "summary": "Recent studies have shown that carefully designed workflows coordinating\nlarge language models(LLMs) significantly enhance task-solving capabilities\ncompared to using a single model. While an increasing number of works focus on\nautonomous workflow construction, most existing approaches rely solely on\nhistorical experience, leading to limitations in efficiency and adaptability.\nWe argue that while historical experience is valuable, workflow construction\nshould also flexibly respond to the unique characteristics of each task. To\nthis end, we propose an a priori dynamic framework for automated workflow\nconstruction. Our framework first leverages Q-table learning to optimize the\ndecision space, guiding agent decisions and enabling effective use of\nhistorical experience. At the same time, agents evaluate the current task\nprogress and make a priori decisions regarding the next executing agent,\nallowing the system to proactively select the more suitable workflow structure\nfor each given task. Additionally, we incorporate mechanisms such as cold-start\ninitialization, early stopping, and pruning to further improve system\nefficiency. Experimental evaluations on four benchmark datasets demonstrate the\nfeasibility and effectiveness of our approach. Compared to state-of-the-art\nbaselines, our method achieves an average improvement of 4.05%, while reducing\nworkflow construction and inference costs to only 30.68%-48.31% of those\nrequired by existing methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5148\u9a8c\u52a8\u6001\u6846\u67b6\u7528\u4e8e\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u6784\u5efa\uff0c\u901a\u8fc7Q-table\u5b66\u4e60\u548c\u5148\u9a8c\u51b3\u7b56\u673a\u5236\uff0c\u5728\u5229\u7528\u5386\u53f2\u7ecf\u9a8c\u7684\u540c\u65f6\u7075\u6d3b\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u7279\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u6784\u5efa\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5386\u53f2\u7ecf\u9a8c\uff0c\u5bfc\u81f4\u6548\u7387\u548c\u9002\u5e94\u6027\u53d7\u9650\u3002\u4f5c\u8005\u8ba4\u4e3a\u5de5\u4f5c\u6d41\u6784\u5efa\u5e94\u8be5\u540c\u65f6\u8003\u8651\u5386\u53f2\u7ecf\u9a8c\u548c\u4efb\u52a1\u7279\u6027\uff0c\u5b9e\u73b0\u66f4\u7075\u6d3b\u7684\u54cd\u5e94\u3002", "method": "\u63d0\u51fa\u5148\u9a8c\u52a8\u6001\u6846\u67b6\uff1a1\uff09\u4f7f\u7528Q-table\u5b66\u4e60\u4f18\u5316\u51b3\u7b56\u7a7a\u95f4\uff1b2\uff09\u4ee3\u7406\u8bc4\u4f30\u4efb\u52a1\u8fdb\u5ea6\u5e76\u505a\u5148\u9a8c\u51b3\u7b56\u9009\u62e9\u4e0b\u4e00\u4e2a\u6267\u884c\u4ee3\u7406\uff1b3\uff09\u52a0\u5165\u51b7\u542f\u52a8\u521d\u59cb\u5316\u3001\u65e9\u505c\u548c\u526a\u679d\u673a\u5236\u63d0\u5347\u6548\u7387\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u5e73\u5747\u63d0\u53474.05%\uff0c\u540c\u65f6\u5c06\u5de5\u4f5c\u6d41\u6784\u5efa\u548c\u63a8\u7406\u6210\u672c\u964d\u4f4e\u5230\u73b0\u6709\u65b9\u6cd5\u768430.68%-48.31%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u5386\u53f2\u7ecf\u9a8c\u5b66\u4e60\u548c\u4efb\u52a1\u7279\u6027\u54cd\u5e94\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u548c\u81ea\u9002\u5e94\u7684\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u6784\u5efa\uff0c\u5728\u6027\u80fd\u548c\u6210\u672c\u65b9\u9762\u90fd\u6709\u663e\u8457\u6539\u5584\u3002"}}
{"id": "2509.15084", "categories": ["cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.15084", "abs": "https://arxiv.org/abs/2509.15084", "authors": ["Doreen Jirak", "Pieter Maes", "Armeen Saroukanoff", "Dirk van Rooy"], "title": "From Sea to System: Exploring User-Centered Explainable AI for Maritime Decision Support", "comment": "Paper accepted at Human Learning and Decision-Making Workshop\n  @ECML-PKDD Conference 2025, Porto, Portugal", "summary": "As autonomous technologies increasingly shape maritime operations,\nunderstanding why an AI system makes a decision becomes as crucial as what it\ndecides. In complex and dynamic maritime environments, trust in AI depends not\nonly on performance but also on transparency and interpretability. This paper\nhighlights the importance of Explainable AI (XAI) as a foundation for effective\nhuman-machine teaming in the maritime domain, where informed oversight and\nshared understanding are essential. To support the user-centered integration of\nXAI, we propose a domain-specific survey designed to capture maritime\nprofessionals' perceptions of trust, usability, and explainability. Our aim is\nto foster awareness and guide the development of user-centric XAI systems\ntailored to the needs of seafarers and maritime teams.", "AI": {"tldr": "\u672c\u6587\u5f3a\u8c03\u53ef\u89e3\u91caAI(XAI)\u5728\u6d77\u4e0a\u9886\u57df\u7684\u91cd\u8981\u6027\uff0c\u63d0\u51fa\u9488\u5bf9\u6d77\u4e8b\u4e13\u4e1a\u4eba\u5458\u7684\u8c03\u67e5\u95ee\u5377\u6765\u8bc4\u4f30\u4fe1\u4efb\u5ea6\u3001\u53ef\u7528\u6027\u548c\u53ef\u89e3\u91ca\u6027\u611f\u77e5\uff0c\u65e8\u5728\u5f00\u53d1\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684XAI\u7cfb\u7edf\u3002", "motivation": "\u968f\u7740\u81ea\u4e3b\u6280\u672f\u5728\u6d77\u4e8b\u64cd\u4f5c\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u7406\u89e3AI\u51b3\u7b56\u539f\u56e0\u53d8\u5f97\u4e0e\u51b3\u7b56\u672c\u8eab\u540c\u7b49\u91cd\u8981\u3002\u5728\u590d\u6742\u52a8\u6001\u7684\u6d77\u4e0a\u73af\u5883\u4e2d\uff0c\u5bf9AI\u7684\u4fe1\u4efb\u4e0d\u4ec5\u53d6\u51b3\u4e8e\u6027\u80fd\uff0c\u8fd8\u4f9d\u8d56\u4e8e\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51fa\u9488\u5bf9\u6d77\u4e8b\u9886\u57df\u7684\u4e13\u95e8\u8c03\u67e5\u95ee\u5377\uff0c\u65e8\u5728\u6355\u6349\u6d77\u4e8b\u4e13\u4e1a\u4eba\u5458\u5bf9\u4fe1\u4efb\u3001\u53ef\u7528\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u611f\u77e5\uff0c\u652f\u6301\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684XAI\u96c6\u6210\u3002", "result": "\u901a\u8fc7\u8c03\u67e5\u95ee\u5377\u6536\u96c6\u6d77\u4e8b\u4e13\u4e1a\u4eba\u5458\u7684\u53cd\u9988\uff0c\u4e3a\u5f00\u53d1\u9002\u5408\u6d77\u5458\u548c\u6d77\u4e8b\u56e2\u961f\u9700\u6c42\u7684\u7528\u6237\u4e2d\u5fc3\u578bXAI\u7cfb\u7edf\u63d0\u4f9b\u6307\u5bfc\u3002", "conclusion": "\u53ef\u89e3\u91caAI\u662f\u6d77\u4e0a\u9886\u57df\u6709\u6548\u4eba\u673a\u534f\u4f5c\u7684\u57fa\u7840\uff0c\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u9488\u5bf9\u6d77\u4e8b\u73af\u5883\u7684\u7528\u6237\u4e2d\u5fc3\u578bXAI\u7cfb\u7edf\uff0c\u4ee5\u4fc3\u8fdb\u77e5\u60c5\u76d1\u7763\u548c\u5171\u4eab\u7406\u89e3\u3002"}}
{"id": "2509.15136", "categories": ["eess.SY", "cs.MA", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.15136", "abs": "https://arxiv.org/abs/2509.15136", "authors": ["Lohitvel Gopikannan", "Shashi Ranjan Kumar", "Abhinav Sinha"], "title": "Nonlinear Cooperative Salvo Guidance with Seeker-Limited Interceptors", "comment": null, "summary": "This paper presents a cooperative guidance strategy for the simultaneous\ninterception of a constant-velocity, non-maneuvering target, addressing the\nrealistic scenario where only a subset of interceptors are equipped with\nonboard seekers. To overcome the resulting heterogeneity in target\nobservability, a fixed-time distributed observer is employed, enabling\nseeker-less interceptors to estimate the target state using information from\nseeker-equipped agents and local neighbors over a directed communication\ntopology. Departing from conventional strategies that approximate time-to-go\nvia linearization or small-angle assumptions, the proposed approach leverages\ndeviated pursuit guidance where the time-to-go expression is exact for such a\ntarget. Moreover, a higher-order sliding mode consensus protocol is utilized to\nestablish time-to-go consensus within a finite time. The effectiveness of the\nproposed guidance and estimation architecture is demonstrated through\nsimulations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6052\u5b9a\u901f\u5ea6\u975e\u673a\u52a8\u76ee\u6807\u7684\u534f\u540c\u5236\u5bfc\u7b56\u7565\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u89c2\u6d4b\u5668\u548c\u504f\u5dee\u8ffd\u8e2a\u5236\u5bfc\u5b9e\u73b0\u4ec5\u6709\u90e8\u5206\u62e6\u622a\u5668\u914d\u5907\u5bfc\u5f15\u5934\u65f6\u7684\u540c\u65f6\u62e6\u622a", "motivation": "\u89e3\u51b3\u5b9e\u9645\u573a\u666f\u4e2d\u53ea\u6709\u90e8\u5206\u62e6\u622a\u5668\u914d\u5907\u5bfc\u5f15\u5934\u5bfc\u81f4\u7684\u5f02\u6784\u76ee\u6807\u53ef\u89c2\u6d4b\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u591a\u62e6\u622a\u5668\u5bf9\u6052\u5b9a\u901f\u5ea6\u975e\u673a\u52a8\u76ee\u6807\u7684\u540c\u65f6\u62e6\u622a", "method": "\u91c7\u7528\u56fa\u5b9a\u65f6\u95f4\u5206\u5e03\u5f0f\u89c2\u6d4b\u5668\u8ba9\u65e0\u5bfc\u5f15\u5934\u62e6\u622a\u5668\u4f30\u8ba1\u76ee\u6807\u72b6\u6001\uff0c\u4f7f\u7528\u504f\u5dee\u8ffd\u8e2a\u5236\u5bfc\u83b7\u5f97\u7cbe\u786e\u65f6\u95f4\u4f30\u8ba1\uff0c\u7ed3\u5408\u9ad8\u9636\u6ed1\u6a21\u4e00\u81f4\u6027\u534f\u8bae\u5b9e\u73b0\u6709\u9650\u65f6\u95f4\u5185\u7684\u65f6\u95f4\u4e00\u81f4\u6027", "result": "\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u5236\u5bfc\u548c\u4f30\u8ba1\u67b6\u6784\u7684\u6709\u6548\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5f02\u6784\u5bfc\u5f15\u5934\u914d\u7f6e\u4e0b\u7684\u534f\u540c\u62e6\u622a\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5de5\u7a0b\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.14431", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14431", "abs": "https://arxiv.org/abs/2509.14431", "authors": ["Keqin Wang", "Tao Zhong", "David Chang", "Christine Allen-Blanchette"], "title": "Local-Canonicalization Equivariant Graph Neural Networks for Sample-Efficient and Generalizable Swarm Robot Control", "comment": "8 pages, 8 figures", "summary": "Multi-agent reinforcement learning (MARL) has emerged as a powerful paradigm\nfor coordinating swarms of agents in complex decision-making, yet major\nchallenges remain. In competitive settings such as pursuer-evader tasks,\nsimultaneous adaptation can destabilize training; non-kinetic countermeasures\noften fail under adverse conditions; and policies trained in one configuration\nrarely generalize to environments with a different number of agents. To address\nthese issues, we propose the Local-Canonicalization Equivariant Graph Neural\nNetworks (LEGO) framework, which integrates seamlessly with popular MARL\nalgorithms such as MAPPO. LEGO employs graph neural networks to capture\npermutation equivariance and generalization to different agent numbers,\ncanonicalization to enforce E(n)-equivariance, and heterogeneous\nrepresentations to encode role-specific inductive biases. Experiments on\ncooperative and competitive swarm benchmarks show that LEGO outperforms strong\nbaselines and improves generalization. In real-world experiments, LEGO\ndemonstrates robustness to varying team sizes and agent failure.", "AI": {"tldr": "\u63d0\u51fa\u4e86LEGO\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u7f6e\u6362\u7b49\u53d8\u6027\u548cE(n)\u7b49\u53d8\u6027\uff0c\u89e3\u51b3\u4e86\u7ade\u4e89\u73af\u5883\u4e2d\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3001\u6cdb\u5316\u80fd\u529b\u5dee\u7b49\u95ee\u9898\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u4e2d\u5747\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u7ade\u4e89\u6027\u573a\u666f\u4e2d\u7684\u4e09\u5927\u6311\u6218\uff1a\u540c\u65f6\u9002\u5e94\u5bfc\u81f4\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3001\u975e\u52a8\u529b\u5b66\u5bf9\u6297\u63aa\u65bd\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u5931\u6548\u3001\u4ee5\u53ca\u7b56\u7565\u5728\u4e0d\u540c\u667a\u80fd\u4f53\u6570\u91cf\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "\u63d0\u51faLEGO\u6846\u67b6\uff0c\u96c6\u6210\u56fe\u795e\u7ecf\u7f51\u7edc\u6355\u6349\u7f6e\u6362\u7b49\u53d8\u6027\u548c\u667a\u80fd\u4f53\u6570\u91cf\u6cdb\u5316\uff0c\u4f7f\u7528\u89c4\u8303\u5316\u5b9e\u73b0E(n)-\u7b49\u53d8\u6027\uff0c\u91c7\u7528\u5f02\u6784\u8868\u793a\u7f16\u7801\u89d2\u8272\u7279\u5b9a\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u53ef\u4e0eMAPPO\u7b49\u6d41\u884cMARL\u7b97\u6cd5\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5728\u5408\u4f5c\u548c\u7ade\u4e89\u6027\u7fa4\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLEGO\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u5e76\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\u3002\u5728\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0cLEGO\u5c55\u793a\u4e86\u5bf9\u4e0d\u540c\u56e2\u961f\u89c4\u6a21\u548c\u667a\u80fd\u4f53\u6545\u969c\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "LEGO\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u5173\u952e\u6311\u6218\uff0c\u901a\u8fc7\u7b49\u53d8\u6027\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u6027\u80fd\uff0c\u4e3a\u590d\u6742\u51b3\u7b56\u573a\u666f\u4e2d\u7684\u667a\u80fd\u4f53\u534f\u8c03\u63d0\u4f9b\u4e86\u6709\u529b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.14594", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14594", "abs": "https://arxiv.org/abs/2509.14594", "authors": ["Yidan Sun", "Viktor Schlegel", "Srinivasan Nandakumar", "Iqra Zahid", "Yuping Wu", "Yulong Wu", "Hao Li", "Jie Zhang", "Warren Del-Pinto", "Goran Nenadic", "Siew Kei Lam", "Anil Anthony Bharath"], "title": "SynBench: A Benchmark for Differentially Private Text Generation", "comment": "15 pages", "summary": "Data-driven decision support in high-stakes domains like healthcare and\nfinance faces significant barriers to data sharing due to regulatory,\ninstitutional, and privacy concerns. While recent generative AI models, such as\nlarge language models, have shown impressive performance in open-domain tasks,\ntheir adoption in sensitive environments remains limited by unpredictable\nbehaviors and insufficient privacy-preserving datasets for benchmarking.\nExisting anonymization methods are often inadequate, especially for\nunstructured text, as redaction and masking can still allow re-identification.\nDifferential Privacy (DP) offers a principled alternative, enabling the\ngeneration of synthetic data with formal privacy assurances. In this work, we\naddress these challenges through three key contributions. First, we introduce a\ncomprehensive evaluation framework with standardized utility and fidelity\nmetrics, encompassing nine curated datasets that capture domain-specific\ncomplexities such as technical jargon, long-context dependencies, and\nspecialized document structures. Second, we conduct a large-scale empirical\nstudy benchmarking state-of-the-art DP text generation methods and LLMs of\nvarying sizes and different fine-tuning strategies, revealing that high-quality\ndomain-specific synthetic data generation under DP constraints remains an\nunsolved challenge, with performance degrading as domain complexity increases.\nThird, we develop a membership inference attack (MIA) methodology tailored for\nsynthetic text, providing first empirical evidence that the use of public\ndatasets - potentially present in pre-training corpora - can invalidate claimed\nprivacy guarantees. Our findings underscore the urgent need for rigorous\nprivacy auditing and highlight persistent gaps between open-domain and\nspecialist evaluations, informing responsible deployment of generative AI in\nprivacy-sensitive, high-stakes settings.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u9ad8\u654f\u611f\u9886\u57df\u6570\u636e\u5171\u4eab\u7684\u9690\u79c1\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u5dee\u5206\u9690\u79c1\u7684\u6587\u672c\u751f\u6210\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\u53d1\u73b0\u9886\u57df\u7279\u5f02\u6027\u5408\u6210\u6570\u636e\u751f\u6210\u4ecd\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u5e76\u5f00\u53d1\u4e86\u9488\u5bf9\u5408\u6210\u6587\u672c\u7684\u6210\u5458\u63a8\u7406\u653b\u51fb\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u533b\u7597\u548c\u91d1\u878d\u7b49\u9ad8\u654f\u611f\u9886\u57df\u6570\u636e\u5171\u4eab\u9762\u4e34\u7684\u76d1\u7ba1\u3001\u5236\u5ea6\u548c\u9690\u79c1\u969c\u788d\uff0c\u7279\u522b\u662f\u73b0\u6709\u533f\u540d\u5316\u65b9\u6cd5\u5bf9\u975e\u7ed3\u6784\u5316\u6587\u672c\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u8981\u5dee\u5206\u9690\u79c1\u7b49\u539f\u5219\u6027\u66ff\u4ee3\u65b9\u6848\u6765\u751f\u6210\u5177\u6709\u6b63\u5f0f\u9690\u79c1\u4fdd\u8bc1\u7684\u5408\u6210\u6570\u636e\u3002", "method": "1) \u5f15\u5165\u5305\u542b\u6807\u51c6\u5316\u6548\u7528\u548c\u4fdd\u771f\u5ea6\u6307\u6807\u7684\u7efc\u5408\u8bc4\u4f30\u6846\u67b6\uff0c\u6db5\u76d69\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u6570\u636e\u96c6\uff1b2) \u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u57fa\u51c6\u6d4b\u8bd5\u6700\u5148\u8fdb\u7684DP\u6587\u672c\u751f\u6210\u65b9\u6cd5\u548c\u4e0d\u540c\u89c4\u6a21\u7684LLM\uff1b3) \u5f00\u53d1\u9488\u5bf9\u5408\u6210\u6587\u672c\u7684\u6210\u5458\u63a8\u7406\u653b\u51fb\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a\u5728DP\u7ea6\u675f\u4e0b\u751f\u6210\u9ad8\u8d28\u91cf\u9886\u57df\u7279\u5f02\u6027\u5408\u6210\u6570\u636e\u4ecd\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u6311\u6218\uff0c\u6027\u80fd\u968f\u9886\u57df\u590d\u6742\u6027\u589e\u52a0\u800c\u4e0b\u964d\uff1b\u4f7f\u7528\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\u4e2d\u53ef\u80fd\u5b58\u5728\u7684\u516c\u5171\u6570\u636e\u96c6\u4f1a\u4f7f\u58f0\u79f0\u7684\u9690\u79c1\u4fdd\u8bc1\u5931\u6548\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u4e25\u683c\u9690\u79c1\u5ba1\u8ba1\u7684\u8feb\u5207\u9700\u6c42\uff0c\u63ed\u793a\u4e86\u5f00\u653e\u9886\u57df\u548c\u4e13\u4e1a\u8bc4\u4f30\u4e4b\u95f4\u7684\u6301\u7eed\u5dee\u8ddd\uff0c\u4e3a\u5728\u9690\u79c1\u654f\u611f\u7684\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u8d1f\u8d23\u4efb\u5730\u90e8\u7f72\u751f\u6210\u5f0fAI\u63d0\u4f9b\u4e86\u4fe1\u606f\u3002"}}
{"id": "2509.14453", "categories": ["cs.RO", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.14453", "abs": "https://arxiv.org/abs/2509.14453", "authors": ["Gokul Puthumanaillam", "Ram Padmanabhan", "Jose Fuentes", "Nicole Cruz", "Paulo Padrao", "Ruben Hernandez", "Hao Jiang", "William Schafer", "Leonardo Bobadilla", "Melkior Ornik"], "title": "Online Learning of Deceptive Policies under Intermittent Observation", "comment": null, "summary": "In supervisory control settings, autonomous systems are not monitored\ncontinuously. Instead, monitoring often occurs at sporadic intervals within\nknown bounds. We study the problem of deception, where an agent pursues a\nprivate objective while remaining plausibly compliant with a supervisor's\nreference policy when observations occur. Motivated by the behavior of real,\nhuman supervisors, we situate the problem within Theory of Mind: the\nrepresentation of what an observer believes and expects to see. We show that\nTheory of Mind can be repurposed to steer online reinforcement learning (RL)\ntoward such deceptive behavior. We model the supervisor's expectations and\ndistill from them a single, calibrated scalar -- the expected evidence of\ndeviation if an observation were to happen now. This scalar combines how unlike\nthe reference and current action distributions appear, with the agent's belief\nthat an observation is imminent. Injected as a state-dependent weight into a\nKL-regularized policy improvement step within an online RL loop, this scalar\ninforms a closed-form update that smoothly trades off self-interest and\ncompliance, thus sidestepping hand-crafted or heuristic policies. In\nreal-world, real-time hardware experiments on marine (ASV) and aerial (UAV)\nnavigation, our ToM-guided RL runs online, achieves high return and success\nwith observed-trace evidence calibrated to the supervisor's expectations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5728\u95f4\u6b47\u6027\u76d1\u63a7\u73af\u5883\u4e0b\uff0c\u667a\u80fd\u4f53\u5982\u4f55\u901a\u8fc7\u5fc3\u7406\u7406\u8bba(Theory of Mind)\u5b9e\u73b0\u6b3a\u9a97\u6027\u884c\u4e3a\uff0c\u5728\u8ffd\u6c42\u79c1\u6709\u76ee\u6807\u7684\u540c\u65f6\u4fdd\u6301\u5bf9\u76d1\u7763\u8005\u53c2\u8003\u7b56\u7565\u7684\u8868\u9762\u5408\u89c4\u6027\u3002", "motivation": "\u5728\u76d1\u7763\u63a7\u5236\u573a\u666f\u4e2d\uff0c\u81ea\u4e3b\u7cfb\u7edf\u5e76\u975e\u88ab\u6301\u7eed\u76d1\u63a7\uff0c\u800c\u662f\u5728\u5df2\u77e5\u8fb9\u754c\u5185\u7684\u96f6\u661f\u95f4\u9694\u8fdb\u884c\u89c2\u5bdf\u3002\u53d7\u4eba\u7c7b\u76d1\u7763\u8005\u884c\u4e3a\u7684\u542f\u53d1\uff0c\u7814\u7a76\u5982\u4f55\u5728\u5fc3\u7406\u7406\u8bba\u6846\u67b6\u4e0b\u5b9e\u73b0\u6b3a\u9a97\u6027\u884c\u4e3a\u3002", "method": "\u5229\u7528\u5fc3\u7406\u7406\u8bba\u5efa\u6a21\u76d1\u7763\u8005\u7684\u671f\u671b\uff0c\u4ece\u4e2d\u63d0\u53d6\u4e00\u4e2a\u6821\u51c6\u6807\u91cf\u2014\u2014\u5982\u679c\u73b0\u5728\u53d1\u751f\u89c2\u5bdf\u65f6\u7684\u9884\u671f\u504f\u5dee\u8bc1\u636e\u3002\u5c06\u8be5\u6807\u91cf\u4f5c\u4e3a\u72b6\u6001\u4f9d\u8d56\u6743\u91cd\u6ce8\u5165\u5230\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684KL\u6b63\u5219\u5316\u7b56\u7565\u6539\u8fdb\u6b65\u9aa4\u4e2d\uff0c\u5b9e\u73b0\u81ea\u6211\u5229\u76ca\u4e0e\u5408\u89c4\u6027\u7684\u5e73\u6ed1\u6743\u8861\u3002", "result": "\u5728\u6d77\u6d0b(ASV)\u548c\u7a7a\u4e2d(UAV)\u5bfc\u822a\u7684\u771f\u5b9e\u786c\u4ef6\u5b9e\u9a8c\u4e2d\uff0cToM\u5f15\u5bfc\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u5728\u7ebf\u8fd0\u884c\uff0c\u5b9e\u73b0\u9ad8\u56de\u62a5\u548c\u6210\u529f\u7387\uff0c\u4e14\u89c2\u5bdf\u8f68\u8ff9\u8bc1\u636e\u4e0e\u76d1\u7763\u8005\u671f\u671b\u76f8\u6821\u51c6\u3002", "conclusion": "\u5fc3\u7406\u7406\u8bba\u53ef\u4ee5\u88ab\u91cd\u65b0\u7528\u4e8e\u5f15\u5bfc\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u6b3a\u9a97\u6027\u884c\u4e3a\uff0c\u901a\u8fc7\u5355\u4e00\u6821\u51c6\u6807\u91cf\u5e73\u6ed1\u6743\u8861\u81ea\u6211\u5229\u76ca\u4e0e\u5408\u89c4\u6027\uff0c\u907f\u514d\u4e86\u624b\u5de5\u6216\u542f\u53d1\u5f0f\u7b56\u7565\u7684\u8bbe\u8ba1\u3002"}}
{"id": "2509.14647", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14647", "abs": "https://arxiv.org/abs/2509.14647", "authors": ["NVJK Kartik", "Garvit Sapra", "Rishav Hada", "Nikhil Pareek"], "title": "AgentCompass: Towards Reliable Evaluation of Agentic Workflows in Production", "comment": null, "summary": "With the growing adoption of Large Language Models (LLMs) in automating\ncomplex, multi-agent workflows, organizations face mounting risks from errors,\nemergent behaviors, and systemic failures that current evaluation methods fail\nto capture. We present AgentCompass, the first evaluation framework designed\nspecifically for post-deployment monitoring and debugging of agentic workflows.\nAgentCompass models the reasoning process of expert debuggers through a\nstructured, multi-stage analytical pipeline: error identification and\ncategorization, thematic clustering, quantitative scoring, and strategic\nsummarization. The framework is further enhanced with a dual memory\nsystem-episodic and semantic-that enables continual learning across executions.\nThrough collaborations with design partners, we demonstrate the framework's\npractical utility on real-world deployments, before establishing its efficacy\nagainst the publicly available TRAIL benchmark. AgentCompass achieves\nstate-of-the-art results on key metrics, while uncovering critical issues\nmissed in human annotations, underscoring its role as a robust,\ndeveloper-centric tool for reliable monitoring and improvement of agentic\nsystems in production.", "AI": {"tldr": "AgentCompass\u662f\u9996\u4e2a\u4e13\u95e8\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u90e8\u7f72\u540e\u76d1\u63a7\u548c\u8c03\u8bd5\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5206\u6790\u6d41\u7a0b\u548c\u53cc\u8bb0\u5fc6\u7cfb\u7edf\u5b9e\u73b0\u6301\u7eed\u5b66\u4e60\uff0c\u5728TRAIL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u6355\u6349\u9519\u8bef\u3001\u6d8c\u73b0\u884c\u4e3a\u548c\u7cfb\u7edf\u6027\u6545\u969c\uff0c\u9700\u8981\u4e13\u95e8\u7684\u90e8\u7f72\u540e\u76d1\u63a7\u548c\u8c03\u8bd5\u5de5\u5177\u3002", "method": "\u6846\u67b6\u6a21\u62df\u4e13\u5bb6\u8c03\u8bd5\u8005\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u91c7\u7528\u591a\u9636\u6bb5\u5206\u6790\u6d41\u7a0b\uff1a\u9519\u8bef\u8bc6\u522b\u5206\u7c7b\u3001\u4e3b\u9898\u805a\u7c7b\u3001\u91cf\u5316\u8bc4\u5206\u548c\u7b56\u7565\u603b\u7ed3\uff0c\u5e76\u914d\u5907\u60c5\u666f\u8bb0\u5fc6\u548c\u8bed\u4e49\u8bb0\u5fc6\u7684\u53cc\u8bb0\u5fc6\u7cfb\u7edf\u5b9e\u73b0\u6301\u7eed\u5b66\u4e60\u3002", "result": "\u5728\u771f\u5b9e\u90e8\u7f72\u548cTRAIL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u7ed3\u679c\uff0c\u5e76\u53d1\u73b0\u4e86\u4eba\u5de5\u6807\u6ce8\u9057\u6f0f\u7684\u5173\u952e\u95ee\u9898\u3002", "conclusion": "AgentCompass\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u5f00\u53d1\u8005\u4e2d\u5fc3\u5de5\u5177\uff0c\u80fd\u591f\u53ef\u9760\u5730\u76d1\u63a7\u548c\u6539\u8fdb\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u3002"}}
{"id": "2509.14460", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14460", "abs": "https://arxiv.org/abs/2509.14460", "authors": ["Abhiroop Ajith", "Constantinos Chamzas"], "title": "Learning Discrete Abstractions for Visual Rearrangement Tasks Using Vision-Guided Graph Coloring", "comment": null, "summary": "Learning abstractions directly from data is a core challenge in robotics.\nHumans naturally operate at an abstract level, reasoning over high-level\nsubgoals while delegating execution to low-level motor skills -- an ability\nthat enables efficient problem solving in complex environments. In robotics,\nabstractions and hierarchical reasoning have long been central to planning, yet\nthey are typically hand-engineered, demanding significant human effort and\nlimiting scalability. Automating the discovery of useful abstractions directly\nfrom visual data would make planning frameworks more scalable and more\napplicable to real-world robotic domains. In this work, we focus on\nrearrangement tasks where the state is represented with raw images, and propose\na method to induce discrete, graph-structured abstractions by combining\nstructural constraints with an attention-guided visual distance. Our approach\nleverages the inherent bipartite structure of rearrangement problems,\nintegrating structural constraints and visual embeddings into a unified\nframework. This enables the autonomous discovery of abstractions from vision\nalone, which can subsequently support high-level planning. We evaluate our\nmethod on two rearrangement tasks in simulation and show that it consistently\nidentifies meaningful abstractions that facilitate effective planning and\noutperform existing approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u89c6\u89c9\u6570\u636e\u81ea\u52a8\u53d1\u73b0\u79bb\u6563\u56fe\u7ed3\u6784\u62bd\u8c61\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u91cd\u6392\u4efb\u52a1\u89c4\u5212\uff0c\u7ed3\u5408\u7ed3\u6784\u7ea6\u675f\u548c\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u89c6\u89c9\u8ddd\u79bb\uff0c\u65e0\u9700\u4eba\u5de5\u8bbe\u8ba1\u62bd\u8c61\u3002", "motivation": "\u4eba\u7c7b\u80fd\u591f\u81ea\u7136\u5730\u5728\u62bd\u8c61\u5c42\u6b21\u8fdb\u884c\u63a8\u7406\uff0c\u800c\u673a\u5668\u4eba\u4e2d\u7684\u62bd\u8c61\u901a\u5e38\u9700\u8981\u4eba\u5de5\u8bbe\u8ba1\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u9645\u5e94\u7528\u3002\u9700\u8981\u4ece\u89c6\u89c9\u6570\u636e\u81ea\u52a8\u53d1\u73b0\u6709\u7528\u7684\u62bd\u8c61\u6765\u652f\u6301\u9ad8\u6548\u89c4\u5212\u3002", "method": "\u7ed3\u5408\u7ed3\u6784\u7ea6\u675f\u548c\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u89c6\u89c9\u8ddd\u79bb\uff0c\u5229\u7528\u91cd\u6392\u95ee\u9898\u56fa\u6709\u7684\u4e8c\u5206\u56fe\u7ed3\u6784\uff0c\u5c06\u7ed3\u6784\u7ea6\u675f\u548c\u89c6\u89c9\u5d4c\u5165\u6574\u5408\u5230\u7edf\u4e00\u6846\u67b6\u4e2d\uff0c\u4ece\u7eaf\u89c6\u89c9\u6570\u636e\u81ea\u4e3b\u53d1\u73b0\u62bd\u8c61\u3002", "result": "\u5728\u4eff\u771f\u73af\u5883\u4e2d\u7684\u4e24\u4e2a\u91cd\u6392\u4efb\u52a1\u4e0a\u8bc4\u4f30\uff0c\u65b9\u6cd5\u80fd\u591f\u4e00\u81f4\u5730\u8bc6\u522b\u51fa\u6709\u610f\u4e49\u7684\u62bd\u8c61\uff0c\u652f\u6301\u6709\u6548\u89c4\u5212\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4ece\u89c6\u89c9\u6570\u636e\u81ea\u52a8\u53d1\u73b0\u6709\u7528\u7684\u62bd\u8c61\uff0c\u4e3a\u673a\u5668\u4eba\u89c4\u5212\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u7684\u95ee\u9898\u89e3\u51b3\u3002"}}
{"id": "2509.14662", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14662", "abs": "https://arxiv.org/abs/2509.14662", "authors": ["Ming Li", "Nan Zhang", "Chenrui Fan", "Hong Jiao", "Yanbin Fu", "Sydney Peters", "Qingshu Xu", "Robert Lissitz", "Tianyi Zhou"], "title": "Understanding the Thinking Process of Reasoning Models: A Perspective from Schoenfeld's Episode Theory", "comment": "EMNLP2025 main, Camera-ready", "summary": "While Large Reasoning Models (LRMs) generate extensive chain-of-thought\nreasoning, we lack a principled framework for understanding how these thoughts\nare structured. In this paper, we introduce a novel approach by applying\nSchoenfeld's Episode Theory, a classic cognitive framework for human\nmathematical problem-solving, to analyze the reasoning traces of LRMs. We\nannotated thousands of sentences and paragraphs from model-generated solutions\nto math problems using seven cognitive labels (e.g., Plan, Implement, Verify).\nThe result is the first publicly available benchmark for the fine-grained\nanalysis of machine reasoning, including a large annotated corpus and detailed\nannotation guidebooks. Our preliminary analysis reveals distinct patterns in\nLRM reasoning, such as the transition dynamics between cognitive states. This\nframework provides a theoretically grounded methodology for interpreting LRM\ncognition and enables future work on more controllable and transparent\nreasoning systems.", "AI": {"tldr": "\u5e94\u7528Schoenfeld\u7684\u8ba4\u77e5\u7406\u8bba\u6846\u67b6\u5206\u6790\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u8fc7\u7a0b\uff0c\u521b\u5efa\u9996\u4e2a\u7ec6\u7c92\u5ea6\u673a\u5668\u63a8\u7406\u5206\u6790\u57fa\u51c6", "motivation": "\u7f3a\u4e4f\u7406\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7ed3\u6784\u7684\u7406\u8bba\u6846\u67b6\uff0c\u9700\u8981\u7cfb\u7edf\u5206\u6790\u5176\u601d\u7ef4\u94fe\u7684\u8ba4\u77e5\u6a21\u5f0f", "method": "\u4f7f\u75287\u4e2a\u8ba4\u77e5\u6807\u7b7e\uff08\u5982\u8ba1\u5212\u3001\u5b9e\u65bd\u3001\u9a8c\u8bc1\uff09\u5bf9\u6a21\u578b\u751f\u6210\u7684\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u65b9\u6848\u8fdb\u884c\u53e5\u5b50\u548c\u6bb5\u843d\u7ea7\u6807\u6ce8", "result": "\u5efa\u7acb\u4e86\u9996\u4e2a\u516c\u5f00\u7684\u673a\u5668\u63a8\u7406\u7ec6\u7c92\u5ea6\u5206\u6790\u57fa\u51c6\uff0c\u5305\u542b\u5927\u89c4\u6a21\u6807\u6ce8\u8bed\u6599\u548c\u8be6\u7ec6\u6807\u6ce8\u6307\u5357\uff0c\u63ed\u793a\u4e86LRM\u63a8\u7406\u7684\u72ec\u7279\u6a21\u5f0f", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u89e3\u91caLRM\u8ba4\u77e5\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u53ef\u63a7\u548c\u900f\u660e\u7684\u63a8\u7406\u7cfb\u7edf"}}
{"id": "2509.14758", "categories": ["cs.RO", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.14758", "abs": "https://arxiv.org/abs/2509.14758", "authors": ["Ihab Tabbara", "Yuxuan Yang", "Ahmad Hamzeh", "Maxwell Astafyev", "Hussein Sibai"], "title": "Designing Latent Safety Filters using Pre-Trained Vision Models", "comment": null, "summary": "Ensuring safety of vision-based control systems remains a major challenge\nhindering their deployment in critical settings. Safety filters have gained\nincreased interest as effective tools for ensuring the safety of classical\ncontrol systems, but their applications in vision-based control settings have\nso far been limited. Pre-trained vision models (PVRs) have been shown to be\neffective perception backbones for control in various robotics domains. In this\npaper, we are interested in examining their effectiveness when used for\ndesigning vision-based safety filters. We use them as backbones for classifiers\ndefining failure sets, for Hamilton-Jacobi (HJ) reachability-based safety\nfilters, and for latent world models. We discuss the trade-offs between\ntraining from scratch, fine-tuning, and freezing the PVRs when training the\nmodels they are backbones for. We also evaluate whether one of the PVRs is\nsuperior across all tasks, evaluate whether learned world models or Q-functions\nare better for switching decisions to safe policies, and discuss practical\nconsiderations for deploying these PVRs on resource-constrained devices.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u5728\u89c6\u89c9\u63a7\u5236\u5b89\u5168\u8fc7\u6ee4\u5668\u4e2d\u7684\u5e94\u7528\u6548\u679c\uff0c\u6bd4\u8f83\u4e0d\u540c\u8bad\u7ec3\u7b56\u7565\u7684\u6743\u8861\uff0c\u5e76\u8bc4\u4f30\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u53ef\u884c\u6027", "motivation": "\u786e\u4fdd\u57fa\u4e8e\u89c6\u89c9\u7684\u63a7\u5236\u7cfb\u7edf\u5b89\u5168\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u5b89\u5168\u8fc7\u6ee4\u5668\u5728\u4f20\u7edf\u63a7\u5236\u7cfb\u7edf\u4e2d\u6709\u6548\uff0c\u4f46\u5728\u89c6\u89c9\u63a7\u5236\u5e94\u7528\u4e2d\u7684\u7814\u7a76\u6709\u9650\u3002\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u5df2\u88ab\u8bc1\u660e\u662f\u6709\u6548\u7684\u611f\u77e5\u9aa8\u5e72\uff0c\u672c\u6587\u65e8\u5728\u7814\u7a76\u5176\u5728\u89c6\u89c9\u5b89\u5168\u8fc7\u6ee4\u5668\u8bbe\u8ba1\u4e2d\u7684\u6709\u6548\u6027", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u4f5c\u4e3a\u5206\u7c7b\u5668\u5b9a\u4e49\u6545\u969c\u96c6\u3001\u57fa\u4e8eHamilton-Jacobi\u53ef\u8fbe\u6027\u5b89\u5168\u8fc7\u6ee4\u5668\u548c\u6f5c\u5728\u4e16\u754c\u6a21\u578b\u7684\u9aa8\u5e72\u7f51\u7edc\u3002\u6bd4\u8f83\u4ece\u5934\u8bad\u7ec3\u3001\u5fae\u8c03\u548c\u51bb\u7ed3\u9884\u8bad\u7ec3\u6a21\u578b\u4e09\u79cd\u7b56\u7565\uff0c\u8bc4\u4f30\u4e0d\u540c\u6a21\u578b\u5728\u5207\u6362\u5b89\u5168\u7b56\u7565\u51b3\u7b56\u4e2d\u7684\u6548\u679c", "result": "\u5206\u6790\u4e86\u4e0d\u540c\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u4f18\u52a3\uff0c\u6bd4\u8f83\u4e86\u5b66\u4e60\u4e16\u754c\u6a21\u578b\u548cQ\u51fd\u6570\u5728\u5b89\u5168\u7b56\u7565\u5207\u6362\u51b3\u7b56\u4e2d\u7684\u6548\u679c\uff0c\u5e76\u8ba8\u8bba\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u90e8\u7f72\u7684\u5b9e\u7528\u8003\u8651", "conclusion": "\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u53ef\u4ee5\u4f5c\u4e3a\u6709\u6548\u7684\u89c6\u89c9\u5b89\u5168\u8fc7\u6ee4\u5668\u9aa8\u5e72\uff0c\u4f46\u9700\u8981\u6839\u636e\u5177\u4f53\u5e94\u7528\u573a\u666f\u9009\u62e9\u5408\u9002\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u6a21\u578b\u67b6\u6784\uff0c\u540c\u65f6\u8003\u8651\u5b9e\u9645\u90e8\u7f72\u65f6\u7684\u8d44\u6e90\u7ea6\u675f"}}
{"id": "2509.14510", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14510", "abs": "https://arxiv.org/abs/2509.14510", "authors": ["Sandra Q. Liu", "Yuxiang Ma", "Edward H. Adelson"], "title": "Object Recognition and Force Estimation with the GelSight Baby Fin Ray", "comment": "Presented at CoRL 2023 as part of the workshop, \"Learning for Soft\n  Robots: Hard Challenges for Soft Systems\" (website:\n  https://sites.google.com/view/corl-2023-soft-robots-ws)", "summary": "Recent advances in soft robotic hands and tactile sensing have enabled both\nto perform an increasing number of complex tasks with the aid of machine\nlearning. In particular, we presented the GelSight Baby Fin Ray in our previous\nwork, which integrates a camera with a soft, compliant Fin Ray structure.\nCamera-based tactile sensing gives the GelSight Baby Fin Ray the ability to\ncapture rich contact information like forces, object geometries, and textures.\nMoreover, our previous work showed that the GelSight Baby Fin Ray can dig\nthrough clutter, and classify in-shell nuts. To further examine the potential\nof the GelSight Baby Fin Ray, we leverage learning to distinguish nut-in-shell\ntextures and to perform force and position estimation. We implement ablation\nstudies with popular neural network structures, including ResNet50, GoogLeNet,\nand 3- and 5-layer convolutional neural network (CNN) structures. We conclude\nthat machine learning is a promising technique to extract useful information\nfrom high-resolution tactile images and empower soft robotics to better\nunderstand and interact with the environments.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86GelSight Baby Fin Ray\u8f6f\u4f53\u673a\u5668\u4eba\u624b\u5728\u89e6\u89c9\u611f\u77e5\u65b9\u9762\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5b9e\u73b0\u4e86\u575a\u679c\u58f3\u7eb9\u7406\u8bc6\u522b\u3001\u529b\u4f30\u8ba1\u548c\u4f4d\u7f6e\u4f30\u8ba1\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e0d\u540c\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u7684\u6027\u80fd\u3002", "motivation": "\u8f6f\u4f53\u673a\u5668\u4eba\u624b\u548c\u89e6\u89c9\u4f20\u611f\u6280\u672f\u7684\u8fdb\u6b65\u4f7f\u5176\u80fd\u591f\u6267\u884c\u66f4\u590d\u6742\u7684\u4efb\u52a1\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u63a2\u7d22\u5982\u4f55\u4ece\u9ad8\u5206\u8fa8\u7387\u89e6\u89c9\u56fe\u50cf\u4e2d\u63d0\u53d6\u6709\u7528\u4fe1\u606f\uff0c\u4ee5\u589e\u5f3a\u8f6f\u4f53\u673a\u5668\u4eba\u5bf9\u73af\u5883\u7684\u7406\u89e3\u548c\u4ea4\u4e92\u80fd\u529b\u3002", "method": "\u4f7f\u7528GelSight Baby Fin Ray\u89e6\u89c9\u4f20\u611f\u5668\uff0c\u96c6\u6210\u6444\u50cf\u5934\u548c\u8f6f\u8d28Fin Ray\u7ed3\u6784\uff0c\u91c7\u7528ResNet50\u3001GoogLeNet\u4ee5\u53ca3\u5c42\u548c5\u5c42CNN\u7b49\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u8fdb\u884c\u7eb9\u7406\u8bc6\u522b\u3001\u529b\u4f30\u8ba1\u548c\u4f4d\u7f6e\u4f30\u8ba1\u7684\u6d88\u878d\u7814\u7a76\u3002", "result": "\u7814\u7a76\u8868\u660e\u673a\u5668\u5b66\u4e60\u80fd\u591f\u6709\u6548\u4ece\u9ad8\u5206\u8fa8\u7387\u89e6\u89c9\u56fe\u50cf\u4e2d\u63d0\u53d6\u6709\u7528\u4fe1\u606f\uff0c\u4e0d\u540c\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u5728\u7eb9\u7406\u8bc6\u522b\u548c\u529b/\u4f4d\u7f6e\u4f30\u8ba1\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u6027\u80fd\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u662f\u4ece\u89e6\u89c9\u56fe\u50cf\u4e2d\u63d0\u53d6\u4fe1\u606f\u7684\u6709\u6548\u6280\u672f\uff0c\u80fd\u591f\u589e\u5f3a\u8f6f\u4f53\u673a\u5668\u4eba\u5bf9\u73af\u5883\u7684\u611f\u77e5\u548c\u4ea4\u4e92\u80fd\u529b\uff0c\u4e3a\u8f6f\u4f53\u673a\u5668\u4eba\u6280\u672f\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2509.14693", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14693", "abs": "https://arxiv.org/abs/2509.14693", "authors": ["Song Xu", "Yilun Liu", "Minggui He", "Mingchen Dai", "Ziang Chen", "Chunguang Zhao", "Jingzhou Du", "Shimin Tao", "Weibin Meng", "Shenglin Zhang", "Yongqian Sun", "Boxing Chen", "Daimeng Wei"], "title": "RationAnomaly: Log Anomaly Detection with Rationality via Chain-of-Thought and Reinforcement Learning", "comment": "5 pages, 3 figures", "summary": "Logs constitute a form of evidence signaling the operational status of\nsoftware systems. Automated log anomaly detection is crucial for ensuring the\nreliability of modern software systems. However, existing approaches face\nsignificant limitations: traditional deep learning models lack interpretability\nand generalization, while methods leveraging Large Language Models are often\nhindered by unreliability and factual inaccuracies. To address these issues, we\npropose RationAnomaly, a novel framework that enhances log anomaly detection by\nsynergizing Chain-of-Thought (CoT) fine-tuning with reinforcement learning. Our\napproach first instills expert-like reasoning patterns using CoT-guided\nsupervised fine-tuning, grounded in a high-quality dataset corrected through a\nrigorous expert-driven process. Subsequently, a reinforcement learning phase\nwith a multi-faceted reward function optimizes for accuracy and logical\nconsistency, effectively mitigating hallucinations. Experimentally,\nRationAnomaly outperforms state-of-the-art baselines, achieving superior\nF1-scores on key benchmarks while providing transparent, step-by-step\nanalytical outputs. We have released the corresponding resources, including\ncode and datasets.", "AI": {"tldr": "RationAnomaly\u662f\u4e00\u4e2a\u7ed3\u5408\u601d\u7ef4\u94fe\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u5bb6\u4fee\u6b63\u6570\u636e\u96c6\u548c\u591a\u91cd\u5956\u52b1\u51fd\u6570\uff0c\u5728\u51c6\u786e\u6027\u548c\u903b\u8f91\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u800c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u5b58\u5728\u4e0d\u53ef\u9760\u6027\u548c\u4e8b\u5b9e\u9519\u8bef\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u51c6\u786e\u6027\u53c8\u80fd\u63d0\u4f9b\u900f\u660e\u5206\u6790\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u9996\u5148\u4f7f\u7528\u4e13\u5bb6\u4fee\u6b63\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u8fdb\u884c\u601d\u7ef4\u94fe\u5f15\u5bfc\u7684\u76d1\u7763\u5fae\u8c03\uff0c\u7136\u540e\u901a\u8fc7\u5177\u6709\u591a\u91cd\u5956\u52b1\u51fd\u6570\u7684\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\u4f18\u5316\u51c6\u786e\u6027\u548c\u903b\u8f91\u4e00\u81f4\u6027\uff0c\u51cf\u5c11\u5e7b\u89c9\u73b0\u8c61\u3002", "result": "\u5728\u5173\u952e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684F1\u5206\u6570\uff0c\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u900f\u660e\u7684\u9010\u6b65\u5206\u6790\u8f93\u51fa\u3002", "conclusion": "RationAnomaly\u901a\u8fc7\u7ed3\u5408\u601d\u7ef4\u94fe\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\u3001\u6cdb\u5316\u6027\u548c\u53ef\u9760\u6027\u95ee\u9898\uff0c\u4e3a\u8f6f\u4ef6\u7cfb\u7edf\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u4fdd\u969c\u3002"}}
{"id": "2509.14984", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.14984", "abs": "https://arxiv.org/abs/2509.14984", "authors": ["Jo\u00e3o Dami\u00e3o Almeida", "Egidio Falotico", "Cecilia Laschi", "Jos\u00e9 Santos-Victor"], "title": "The Role of Touch: Towards Optimal Tactile Sensing Distribution in Anthropomorphic Hands for Dexterous In-Hand Manipulation", "comment": null, "summary": "In-hand manipulation tasks, particularly in human-inspired robotic systems,\nmust rely on distributed tactile sensing to achieve precise control across a\nwide variety of tasks. However, the optimal configuration of this network of\nsensors is a complex problem, and while the fingertips are a common choice for\nplacing sensors, the contribution of tactile information from other regions of\nthe hand is often overlooked. This work investigates the impact of tactile\nfeedback from various regions of the fingers and palm in performing in-hand\nobject reorientation tasks. We analyze how sensory feedback from different\nparts of the hand influences the robustness of deep reinforcement learning\ncontrol policies and investigate the relationship between object\ncharacteristics and optimal sensor placement. We identify which tactile sensing\nconfigurations contribute to improving the efficiency and accuracy of\nmanipulation. Our results provide valuable insights for the design and use of\nanthropomorphic end-effectors with enhanced manipulation capabilities.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u673a\u5668\u4eba\u624b\u90e8\u4e0d\u540c\u533a\u57df\uff08\u6307\u5c16\u3001\u624b\u6307\u5176\u4ed6\u90e8\u4f4d\u548c\u624b\u638c\uff09\u7684\u89e6\u89c9\u53cd\u9988\u5bf9\u7269\u4f53\u91cd\u5b9a\u5411\u4efb\u52a1\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5206\u6790\u4e86\u6700\u4f18\u4f20\u611f\u5668\u914d\u7f6e\u4e0e\u7269\u4f53\u7279\u6027\u7684\u5173\u7cfb\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u624b\u90e8\u89e6\u89c9\u4f20\u611f\u4e3b\u8981\u96c6\u4e2d\u5728\u6307\u5c16\uff0c\u800c\u5176\u4ed6\u533a\u57df\u7684\u89e6\u89c9\u4fe1\u606f\u8d21\u732e\u5e38\u88ab\u5ffd\u89c6\uff0c\u9700\u8981\u7814\u7a76\u4e0d\u540c\u533a\u57df\u89e6\u89c9\u53cd\u9988\u5bf9\u7cbe\u786e\u63a7\u5236\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u7b56\u7565\uff0c\u5206\u6790\u624b\u90e8\u4e0d\u540c\u533a\u57df\uff08\u6307\u5c16\u3001\u624b\u6307\u5176\u4ed6\u90e8\u4f4d\u3001\u624b\u638c\uff09\u7684\u89e6\u89c9\u53cd\u9988\u5bf9\u7269\u4f53\u91cd\u5b9a\u5411\u4efb\u52a1\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u7814\u7a76\u4f20\u611f\u5668\u914d\u7f6e\u4e0e\u7269\u4f53\u7279\u6027\u7684\u5173\u7cfb\u3002", "result": "\u8bc6\u522b\u51fa\u80fd\u591f\u63d0\u9ad8\u64cd\u4f5c\u6548\u7387\u548c\u51c6\u786e\u6027\u7684\u6700\u4f18\u89e6\u89c9\u4f20\u611f\u914d\u7f6e\uff0c\u53d1\u73b0\u4e0d\u540c\u7269\u4f53\u7279\u6027\u9700\u8981\u4e0d\u540c\u7684\u4f20\u611f\u5668\u5e03\u5c40\u7b56\u7565\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5177\u6709\u589e\u5f3a\u64cd\u4f5c\u80fd\u529b\u7684\u4eba\u5f62\u672b\u7aef\u6267\u884c\u5668\u7684\u8bbe\u8ba1\u548c\u4f7f\u7528\u63d0\u4f9b\u4e86\u5b9d\u8d35\u89c1\u89e3\uff0c\u5f3a\u8c03\u4e86\u5168\u9762\u8003\u8651\u624b\u90e8\u5404\u533a\u57df\u89e6\u89c9\u53cd\u9988\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.14516", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14516", "abs": "https://arxiv.org/abs/2509.14516", "authors": ["Adam D. Hines", "Alejandro Fontan", "Michael Milford", "Tobias Fischer"], "title": "Event-LAB: Towards Standardized Evaluation of Neuromorphic Localization Methods", "comment": "8 pages, 6 figures, under review", "summary": "Event-based localization research and datasets are a rapidly growing area of\ninterest, with a tenfold increase in the cumulative total number of published\npapers on this topic over the past 10 years. Whilst the rapid expansion in the\nfield is exciting, it brings with it an associated challenge: a growth in the\nvariety of required code and package dependencies as well as data formats,\nmaking comparisons difficult and cumbersome for researchers to implement\nreliably. To address this challenge, we present Event-LAB: a new and unified\nframework for running several event-based localization methodologies across\nmultiple datasets. Event-LAB is implemented using the Pixi package and\ndependency manager, that enables a single command-line installation and\ninvocation for combinations of localization methods and datasets. To\ndemonstrate the capabilities of the framework, we implement two common\nevent-based localization pipelines: Visual Place Recognition (VPR) and\nSimultaneous Localization and Mapping (SLAM). We demonstrate the ability of the\nframework to systematically visualize and analyze the results of multiple\nmethods and datasets, revealing key insights such as the association of\nparameters that control event collection counts and window sizes for frame\ngeneration to large variations in performance. The results and analysis\ndemonstrate the importance of fairly comparing methodologies with consistent\nevent image generation parameters. Our Event-LAB framework provides this\nability for the research community, by contributing a streamlined workflow for\neasily setting up multiple conditions.", "AI": {"tldr": "Event-LAB\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fd0\u884c\u4e8b\u4ef6\u76f8\u673a\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u8be5\u9886\u57df\u4ee3\u7801\u4f9d\u8d56\u548c\u6570\u636e\u683c\u5f0f\u591a\u6837\u5316\u5e26\u6765\u7684\u6bd4\u8f83\u56f0\u96be\u95ee\u9898\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5b9a\u4f4d\u7814\u7a76\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u4ee3\u7801\u4f9d\u8d56\u548c\u6570\u636e\u683c\u5f0f\u591a\u6837\u5316\u4f7f\u5f97\u65b9\u6cd5\u6bd4\u8f83\u53d8\u5f97\u56f0\u96be\u4e14\u7e41\u7410\uff0c\u9700\u8981\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u6765\u7b80\u5316\u5b9e\u9a8c\u8bbe\u7f6e\u548c\u7ed3\u679c\u5206\u6790\u3002", "method": "\u4f7f\u7528Pixi\u5305\u548c\u4f9d\u8d56\u7ba1\u7406\u5668\u5b9e\u73b0Event-LAB\u6846\u67b6\uff0c\u652f\u6301\u5355\u547d\u4ee4\u884c\u5b89\u88c5\u548c\u8c03\u7528\uff0c\u5b9e\u73b0\u4e86\u89c6\u89c9\u4f4d\u7f6e\u8bc6\u522b(VPR)\u548c\u540c\u6b65\u5b9a\u4f4d\u4e0e\u5efa\u56fe(SLAM)\u4e24\u79cd\u5e38\u89c1\u4e8b\u4ef6\u5b9a\u4f4d\u6d41\u7a0b\u3002", "result": "\u6846\u67b6\u80fd\u591f\u7cfb\u7edf\u5730\u53ef\u89c6\u5316\u548c\u5206\u6790\u591a\u79cd\u65b9\u6cd5\u548c\u6570\u636e\u96c6\u7684\u7ed3\u679c\uff0c\u63ed\u793a\u4e86\u4e8b\u4ef6\u6536\u96c6\u8ba1\u6570\u548c\u5e27\u751f\u6210\u7a97\u53e3\u5927\u5c0f\u7b49\u53c2\u6570\u5bf9\u6027\u80fd\u7684\u91cd\u8981\u5f71\u54cd\u3002", "conclusion": "Event-LAB\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u516c\u5e73\u6bd4\u8f83\u65b9\u6cd5\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u4e00\u81f4\u7684\u53c2\u6570\u8bbe\u7f6e\u7b80\u5316\u4e86\u591a\u6761\u4ef6\u5b9e\u9a8c\u7684\u5de5\u4f5c\u6d41\u7a0b\u3002"}}
{"id": "2509.14704", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14704", "abs": "https://arxiv.org/abs/2509.14704", "authors": ["Masaharu Mizumoto", "Dat Nguyen", "Zhiheng Han", "Jiyuan Fang", "Heyuan Guan", "Xingfu Li", "Naoya Shiraishi", "Xuyang Tian", "Yo Nakawake", "Le Minh Nguyen"], "title": "The NazoNazo Benchmark: A Cost-Effective and Extensible Test of Insight-Based Reasoning in LLMs", "comment": null, "summary": "Benchmark saturation and contamination undermine confidence in LLM\nevaluation. We present Nazonazo, a cost-effective and extensible benchmark\nbuilt from Japanese children's riddles to test insight-based reasoning. Items\nare short (mostly one sentence), require no specialized domain knowledge, and\ncan be generated at scale, enabling rapid refresh of blind sets when leakage is\nsuspected. We evaluate 38 frontier models and 126 adults on 120 riddles. No\nmodel except for GPT-5 is comparable to human performance, which achieves a\n52.9% mean accuracy. Model comparison on extended 201 items shows that\nreasoning models significantly outperform non-reasoning peers, while model size\nshows no reliable association with accuracy. Beyond aggregate accuracy, an\ninformal candidate-tracking analysis of thought logs reveals many cases of\nverification failure: models often produce the correct solution among\nintermediate candidates yet fail to select it as the final answer, which we\nillustrate with representative examples observed in multiple models. Nazonazo\nthus offers a cost-effective, scalable, and easily renewable benchmark format\nthat addresses the current evaluation crisis while also suggesting a recurrent\nmeta-cognitive weakness, providing clear targets for future control and\ncalibration methods.", "AI": {"tldr": "Nazonazo\u662f\u4e00\u4e2a\u57fa\u4e8e\u65e5\u672c\u513f\u7ae5\u8c1c\u8bed\u6784\u5efa\u7684\u6210\u672c\u6548\u76ca\u9ad8\u3001\u53ef\u6269\u5c55\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u7684\u6d1e\u5bdf\u529b\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u9664GPT-5\u5916\u6240\u6709\u6a21\u578b\u90fd\u65e0\u6cd5\u8fbe\u5230\u4eba\u7c7b\u6c34\u5e73\uff0c\u5e76\u63ed\u793a\u4e86\u6a21\u578b\u9a8c\u8bc1\u5931\u8d25\u7684\u5143\u8ba4\u77e5\u5f31\u70b9\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u57fa\u51c6\u6d4b\u8bd5\u9971\u548c\u548c\u6c61\u67d3\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u6210\u672c\u6548\u76ca\u9ad8\u3001\u53ef\u6269\u5c55\u4e14\u6613\u4e8e\u66f4\u65b0\u7684\u8bc4\u4f30\u57fa\u51c6\u6765\u6d4b\u8bd5LLM\u7684\u6d1e\u5bdf\u529b\u63a8\u7406\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u65e5\u672c\u513f\u7ae5\u8c1c\u8bed\u6784\u5efa\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b120\u4e2a\u7b80\u77ed\u8c1c\u9898\uff0c\u8bc4\u4f3038\u4e2a\u524d\u6cbf\u6a21\u578b\u548c126\u540d\u6210\u5e74\u4eba\uff0c\u901a\u8fc7\u6269\u5c55201\u4e2a\u9879\u76ee\u7684\u6a21\u578b\u6bd4\u8f83\u548c\u601d\u7ef4\u65e5\u5fd7\u5206\u6790\u6765\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u9664GPT-5\u5916\uff0c\u6240\u6709\u6a21\u578b\u6027\u80fd\u5747\u4e0d\u53ca\u4eba\u7c7b\uff0852.9%\u5e73\u5747\u51c6\u786e\u7387\uff09\uff0c\u63a8\u7406\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u975e\u63a8\u7406\u6a21\u578b\uff0c\u6a21\u578b\u5927\u5c0f\u4e0e\u51c6\u786e\u7387\u65e0\u53ef\u9760\u5173\u8054\uff0c\u53d1\u73b0\u5927\u91cf\u9a8c\u8bc1\u5931\u8d25\u6848\u4f8b\u3002", "conclusion": "Nazonazo\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u89e3\u51b3\u5f53\u524d\u8bc4\u4f30\u5371\u673a\u7684\u57fa\u51c6\u683c\u5f0f\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u6a21\u578b\u5728\u5143\u8ba4\u77e5\u65b9\u9762\u7684\u7cfb\u7edf\u6027\u5f31\u70b9\uff0c\u4e3a\u672a\u6765\u7684\u63a7\u5236\u548c\u6821\u51c6\u65b9\u6cd5\u63d0\u4f9b\u4e86\u660e\u786e\u76ee\u6807\u3002"}}
{"id": "2509.14530", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14530", "abs": "https://arxiv.org/abs/2509.14530", "authors": ["Zhenghao Fei", "Wenwu Lu", "Linsheng Hou", "Chen Peng"], "title": "Learning to Pick: A Visuomotor Policy for Clustered Strawberry Picking", "comment": null, "summary": "Strawberries naturally grow in clusters, interwoven with leaves, stems, and\nother fruits, which frequently leads to occlusion. This inherent growth habit\npresents a significant challenge for robotic picking, as traditional\npercept-plan-control systems struggle to reach fruits amid the clutter.\nEffectively picking an occluded strawberry demands dexterous manipulation to\ncarefully bypass or gently move the surrounding soft objects and precisely\naccess the ideal picking point located at the stem just above the calyx. To\naddress this challenge, we introduce a strawberry-picking robotic system that\nlearns from human demonstrations. Our system features a 4-DoF SCARA arm paired\nwith a human teleoperation interface for efficient data collection and\nleverages an End Pose Assisted Action Chunking Transformer (ACT) to develop a\nfine-grained visuomotor picking policy. Experiments under various occlusion\nscenarios demonstrate that our modified approach significantly outperforms the\ndirect implementation of ACT, underscoring its potential for practical\napplication in occluded strawberry picking.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eba\u7c7b\u6f14\u793a\u5b66\u4e60\u7684\u8349\u8393\u91c7\u6458\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u901a\u8fc7\u6539\u8fdb\u7684ACT\u6a21\u578b\u5904\u7406\u906e\u6321\u573a\u666f\u4e0b\u7684\u7cbe\u7ec6\u91c7\u6458\u4efb\u52a1", "motivation": "\u8349\u8393\u81ea\u7136\u751f\u957f\u5728\u7c07\u7fa4\u4e2d\uff0c\u5e38\u88ab\u53f6\u7247\u3001\u830e\u79c6\u548c\u5176\u4ed6\u679c\u5b9e\u906e\u6321\uff0c\u4f20\u7edf\u611f\u77e5-\u89c4\u5212-\u63a7\u5236\u7cfb\u7edf\u96be\u4ee5\u5728\u6742\u4e71\u73af\u5883\u4e2d\u6709\u6548\u91c7\u6458\u88ab\u906e\u6321\u7684\u8349\u8393", "method": "\u4f7f\u75284\u81ea\u7531\u5ea6SCARA\u673a\u68b0\u81c2\u914d\u5408\u4eba\u5de5\u9065\u64cd\u4f5c\u63a5\u53e3\u6536\u96c6\u6570\u636e\uff0c\u91c7\u7528\u6539\u8fdb\u7684End Pose Assisted Action Chunking Transformer (ACT)\u6a21\u578b\u5f00\u53d1\u7cbe\u7ec6\u7684\u89c6\u89c9\u8fd0\u52a8\u91c7\u6458\u7b56\u7565", "result": "\u5728\u5404\u79cd\u906e\u6321\u573a\u666f\u4e0b\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6539\u8fdb\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u76f4\u63a5\u4f7f\u7528ACT\u6a21\u578b\uff0c\u5c55\u73b0\u4e86\u5728\u5b9e\u9645\u906e\u6321\u8349\u8393\u91c7\u6458\u4e2d\u7684\u5e94\u7528\u6f5c\u529b", "conclusion": "\u57fa\u4e8e\u4eba\u7c7b\u6f14\u793a\u5b66\u4e60\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u5904\u7406\u8349\u8393\u91c7\u6458\u4e2d\u7684\u906e\u6321\u95ee\u9898\uff0c\u6539\u8fdb\u7684ACT\u6a21\u578b\u5728\u7cbe\u7ec6\u64cd\u4f5c\u65b9\u9762\u8868\u73b0\u4f18\u5f02"}}
{"id": "2509.14750", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14750", "abs": "https://arxiv.org/abs/2509.14750", "authors": ["Letian Zhang", "Guanghao Meng", "Xudong Ren", "Yiming Wang", "Shu-Tao Xia"], "title": "Enhancing Retrieval Augmentation via Adversarial Collaboration", "comment": null, "summary": "Retrieval-augmented Generation (RAG) is a prevalent approach for\ndomain-specific LLMs, yet it is often plagued by \"Retrieval Hallucinations\"--a\nphenomenon where fine-tuned models fail to recognize and act upon poor-quality\nretrieved documents, thus undermining performance. To address this, we propose\nthe Adversarial Collaboration RAG (AC-RAG) framework. AC-RAG employs two\nheterogeneous agents: a generalist Detector that identifies knowledge gaps, and\na domain-specialized Resolver that provides precise solutions. Guided by a\nmoderator, these agents engage in an adversarial collaboration, where the\nDetector's persistent questioning challenges the Resolver's expertise. This\ndynamic process allows for iterative problem dissection and refined knowledge\nretrieval. Extensive experiments show that AC-RAG significantly improves\nretrieval accuracy and outperforms state-of-the-art RAG methods across various\nvertical domains.", "AI": {"tldr": "AC-RAG\u6846\u67b6\u901a\u8fc7\u5bf9\u6297\u6027\u534f\u4f5c\u89e3\u51b3RAG\u4e2d\u7684\u68c0\u7d22\u5e7b\u89c9\u95ee\u9898\uff0c\u4f7f\u7528\u68c0\u6d4b\u5668\u548c\u89e3\u6790\u5668\u4e24\u4e2a\u5f02\u6784\u4ee3\u7406\u5728\u8c03\u89e3\u8005\u6307\u5bfc\u4e0b\u8fdb\u884c\u8fed\u4ee3\u5f0f\u95ee\u9898\u5206\u89e3\u548c\u77e5\u8bc6\u68c0\u7d22\uff0c\u663e\u8457\u63d0\u5347\u68c0\u7d22\u51c6\u786e\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u4e2d\u5b58\u5728\u7684\"\u68c0\u7d22\u5e7b\u89c9\"\u95ee\u9898\uff0c\u5373\u5fae\u8c03\u6a21\u578b\u65e0\u6cd5\u8bc6\u522b\u548c\u5904\u7406\u4f4e\u8d28\u91cf\u68c0\u7d22\u6587\u6863\uff0c\u4ece\u800c\u5f71\u54cd\u6027\u80fd\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u5bf9\u6297\u6027\u534f\u4f5cRAG\u6846\u67b6(AC-RAG)\uff0c\u5305\u542b\u4e24\u4e2a\u5f02\u6784\u4ee3\u7406\uff1a\u901a\u7528\u68c0\u6d4b\u5668\uff08\u8bc6\u522b\u77e5\u8bc6\u7f3a\u53e3\uff09\u548c\u9886\u57df\u4e13\u4e1a\u5316\u89e3\u6790\u5668\uff08\u63d0\u4f9b\u7cbe\u786e\u89e3\u51b3\u65b9\u6848\uff09\uff0c\u5728\u8c03\u89e3\u8005\u6307\u5bfc\u4e0b\u8fdb\u884c\u5bf9\u6297\u6027\u534f\u4f5c\uff0c\u901a\u8fc7\u6301\u7eed\u63d0\u95ee\u6311\u6218\u4e13\u5bb6\u77e5\u8bc6\u6765\u5b9e\u73b0\u8fed\u4ee3\u5f0f\u95ee\u9898\u5206\u89e3\u548c\u7cbe\u7ec6\u5316\u77e5\u8bc6\u68c0\u7d22\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cAC-RAG\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u7d22\u51c6\u786e\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u5782\u76f4\u9886\u57df\u4e2d\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684RAG\u65b9\u6cd5\u3002", "conclusion": "AC-RAG\u6846\u67b6\u901a\u8fc7\u5bf9\u6297\u6027\u534f\u4f5c\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86RAG\u4e2d\u7684\u68c0\u7d22\u5e7b\u89c9\u95ee\u9898\uff0c\u4e3a\u9886\u57df\u7279\u5b9aLLMs\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.14531", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14531", "abs": "https://arxiv.org/abs/2509.14531", "authors": ["Haoran Xiao", "Xue Wang", "Huimin Lu", "Zhiwen Zeng", "Zirui Guo", "Ziqi Ni", "Yicong Ye", "Wei Dai"], "title": "Dual-Arm Hierarchical Planning for Laboratory Automation: Vibratory Sieve Shaker Operations", "comment": null, "summary": "This paper addresses the challenges of automating vibratory sieve shaker\noperations in a materials laboratory, focusing on three critical tasks: 1)\ndual-arm lid manipulation in 3 cm clearance spaces, 2) bimanual handover in\noverlapping workspaces, and 3) obstructed powder sample container delivery with\norientation constraints. These tasks present significant challenges, including\ninefficient sampling in narrow passages, the need for smooth trajectories to\nprevent spillage, and suboptimal paths generated by conventional methods. To\novercome these challenges, we propose a hierarchical planning framework\ncombining Prior-Guided Path Planning and Multi-Step Trajectory Optimization.\nThe former uses a finite Gaussian mixture model to improve sampling efficiency\nin narrow passages, while the latter refines paths by shortening, simplifying,\nimposing joint constraints, and B-spline smoothing. Experimental results\ndemonstrate the framework's effectiveness: planning time is reduced by up to\n80.4%, and waypoints are decreased by 89.4%. Furthermore, the system completes\nthe full vibratory sieve shaker operation workflow in a physical experiment,\nvalidating its practical applicability for complex laboratory automation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u5c42\u89c4\u5212\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u632f\u52a8\u7b5b\u5206\u5668\u64cd\u4f5c\uff0c\u89e3\u51b3\u4e86\u72ed\u7a84\u7a7a\u95f4\u4e2d\u7684\u53cc\u81c2\u64cd\u4f5c\u3001\u53cc\u624b\u4ea4\u63a5\u548c\u53d7\u9650\u5bb9\u5668\u4f20\u9012\u7b49\u6311\u6218\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u89c4\u5212\u65f6\u95f4\u548c\u8def\u5f84\u70b9\u6570\u91cf\u3002", "motivation": "\u89e3\u51b3\u6750\u6599\u5b9e\u9a8c\u5ba4\u4e2d\u632f\u52a8\u7b5b\u5206\u5668\u81ea\u52a8\u5316\u64cd\u4f5c\u7684\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a\u72ed\u7a84\u7a7a\u95f4\u4e2d\u7684\u53cc\u81c2\u76d6\u64cd\u4f5c\u3001\u91cd\u53e0\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u7684\u53cc\u624b\u4ea4\u63a5\u4ee5\u53ca\u6709\u65b9\u5411\u7ea6\u675f\u7684\u53d7\u963b\u7c89\u672b\u5bb9\u5668\u4f20\u9012\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e2d\u5b58\u5728\u91c7\u6837\u6548\u7387\u4f4e\u3001\u8def\u5f84\u4e0d\u5149\u6ed1\u6613\u5bfc\u81f4\u6d12\u843d\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u5206\u5c42\u89c4\u5212\u6846\u67b6\uff0c\u7ed3\u5408\u5148\u9a8c\u5f15\u5bfc\u8def\u5f84\u89c4\u5212\uff08\u4f7f\u7528\u6709\u9650\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u63d0\u9ad8\u72ed\u7a84\u901a\u9053\u91c7\u6837\u6548\u7387\uff09\u548c\u591a\u6b65\u8f68\u8ff9\u4f18\u5316\uff08\u901a\u8fc7\u7f29\u77ed\u3001\u7b80\u5316\u3001\u65bd\u52a0\u5173\u8282\u7ea6\u675f\u548cB\u6837\u6761\u5e73\u6ed1\u6765\u4f18\u5316\u8def\u5f84\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u89c4\u5212\u65f6\u95f4\u51cf\u5c11\u9ad8\u8fbe80.4%\uff0c\u8def\u5f84\u70b9\u51cf\u5c1189.4%\u3002\u7cfb\u7edf\u5728\u7269\u7406\u5b9e\u9a8c\u4e2d\u6210\u529f\u5b8c\u6210\u4e86\u5b8c\u6574\u7684\u632f\u52a8\u7b5b\u5206\u5668\u64cd\u4f5c\u6d41\u7a0b\uff0c\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "\u8be5\u5206\u5c42\u89c4\u5212\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u5b9e\u9a8c\u5ba4\u81ea\u52a8\u5316\u4e2d\u7684\u8fd0\u52a8\u89c4\u5212\u6311\u6218\uff0c\u4e3a\u632f\u52a8\u7b5b\u5206\u5668\u7b49\u7cbe\u5bc6\u8bbe\u5907\u7684\u81ea\u52a8\u5316\u64cd\u4f5c\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.14778", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.14778", "abs": "https://arxiv.org/abs/2509.14778", "authors": ["Yuxiao Cheng", "Jinli Suo"], "title": "OpenLens AI: Fully Autonomous Research Agent for Health Infomatics", "comment": null, "summary": "Health informatics research is characterized by diverse data modalities,\nrapid knowledge expansion, and the need to integrate insights across biomedical\nscience, data analytics, and clinical practice. These characteristics make it\nparticularly well-suited for agent-based approaches that can automate knowledge\nexploration, manage complex workflows, and generate clinically meaningful\noutputs. Recent progress in large language model (LLM)-based agents has\ndemonstrated promising capabilities in literature synthesis, data analysis, and\neven end-to-end research execution. However, existing systems remain limited\nfor health informatics because they lack mechanisms to interpret medical\nvisualizations and often overlook domain-specific quality requirements. To\naddress these gaps, we introduce OpenLens AI, a fully automated framework\ntailored to health informatics. OpenLens AI integrates specialized agents for\nliterature review, data analysis, code generation, and manuscript preparation,\nenhanced by vision-language feedback for medical visualization and quality\ncontrol for reproducibility. The framework automates the entire research\npipeline, producing publication-ready LaTeX manuscripts with transparent and\ntraceable workflows, thereby offering a domain-adapted solution for advancing\nhealth informatics research.", "AI": {"tldr": "OpenLens AI\u662f\u4e00\u4e2a\u4e13\u4e3a\u5065\u5eb7\u4fe1\u606f\u5b66\u8bbe\u8ba1\u7684\u5168\u81ea\u52a8\u7814\u7a76\u6846\u67b6\uff0c\u96c6\u6210\u4e86\u6587\u732e\u7efc\u8ff0\u3001\u6570\u636e\u5206\u6790\u3001\u4ee3\u7801\u751f\u6210\u548c\u8bba\u6587\u64b0\u5199\u7b49\u4e13\u4e1a\u4ee3\u7406\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u53cd\u9988\u548c\u8d28\u63a7\u673a\u5236\u89e3\u51b3\u73b0\u6709LLM\u4ee3\u7406\u5728\u533b\u5b66\u53ef\u89c6\u5316\u89e3\u91ca\u548c\u9886\u57df\u7279\u5b9a\u8d28\u91cf\u8981\u6c42\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u5065\u5eb7\u4fe1\u606f\u5b66\u7814\u7a76\u5177\u6709\u6570\u636e\u6a21\u6001\u591a\u6837\u3001\u77e5\u8bc6\u5feb\u901f\u6269\u5c55\u7684\u7279\u70b9\uff0c\u9700\u8981\u6574\u5408\u751f\u7269\u533b\u5b66\u3001\u6570\u636e\u5206\u6790\u548c\u4e34\u5e8a\u5b9e\u8df5\u7b49\u591a\u9886\u57df\u89c1\u89e3\u3002\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7406\u7cfb\u7edf\u5728\u533b\u5b66\u53ef\u89c6\u5316\u89e3\u91ca\u548c\u9886\u57df\u7279\u5b9a\u8d28\u91cf\u8981\u6c42\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5065\u5eb7\u4fe1\u606f\u5b66\u7814\u7a76\u7684\u4e13\u4e1a\u9700\u6c42\u3002", "method": "\u5f00\u53d1\u4e86OpenLens AI\u6846\u67b6\uff0c\u96c6\u6210\u4e13\u4e1a\u4ee3\u7406\u8fdb\u884c\u6587\u732e\u7efc\u8ff0\u3001\u6570\u636e\u5206\u6790\u3001\u4ee3\u7801\u751f\u6210\u548c\u8bba\u6587\u51c6\u5907\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u53cd\u9988\u673a\u5236\u5904\u7406\u533b\u5b66\u53ef\u89c6\u5316\uff0c\u5e76\u5efa\u7acb\u8d28\u91cf\u63a7\u5236\u673a\u5236\u786e\u4fdd\u53ef\u91cd\u590d\u6027\u3002\u6846\u67b6\u81ea\u52a8\u5316\u6574\u4e2a\u7814\u7a76\u6d41\u7a0b\uff0c\u751f\u6210\u53ef\u76f4\u63a5\u53d1\u8868\u7684LaTeX\u8bba\u6587\u3002", "result": "OpenLens AI\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9886\u57df\u9002\u914d\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u81ea\u52a8\u5316\u5065\u5eb7\u4fe1\u606f\u5b66\u7814\u7a76\u7684\u5b8c\u6574\u6d41\u7a0b\uff0c\u751f\u6210\u5177\u6709\u900f\u660e\u548c\u53ef\u8ffd\u6eaf\u5de5\u4f5c\u6d41\u7a0b\u7684\u51fa\u7248\u5c31\u7eea\u8bba\u6587\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7cfb\u7edf\u5728\u533b\u5b66\u53ef\u89c6\u5316\u89e3\u91ca\u548c\u8d28\u91cf\u63a7\u5236\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u63a8\u8fdb\u5065\u5eb7\u4fe1\u606f\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u4e13\u95e8\u5b9a\u5236\u7684\u81ea\u52a8\u5316\u5de5\u5177\uff0c\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u8bed\u8a00\u80fd\u529b\u548c\u8d28\u91cf\u63a7\u5236\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u7cfb\u7edf\u5728\u533b\u5b66\u7814\u7a76\u9886\u57df\u7684\u9002\u7528\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2509.14548", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.14548", "abs": "https://arxiv.org/abs/2509.14548", "authors": ["Emily Sumner", "Deepak E. Gopinath", "Laporsha Dees", "Patricio Reyes Gomez", "Xiongyi Cui", "Andrew Silva", "Jean Costa", "Allison Morgan", "Mariah Schrum", "Tiffany L. Chen", "Avinash Balachandran", "Guy Rosman"], "title": "SimCoachCorpus: A naturalistic dataset with language and trajectories for embodied teaching", "comment": null, "summary": "Curated datasets are essential for training and evaluating AI approaches, but\nare often lacking in domains where language and physical action are deeply\nintertwined. In particular, few datasets capture how people acquire embodied\nskills through verbal instruction over time. To address this gap, we introduce\nSimCoachCorpus: a unique dataset of race car simulator driving that allows for\nthe investigation of rich interactive phenomena during guided and unguided\nmotor skill acquisition. In this dataset, 29 humans were asked to drive in a\nsimulator around a race track for approximately ninety minutes. Fifteen\nparticipants were given personalized one-on-one instruction from a professional\nperformance driving coach, and 14 participants drove without coaching. \\name\\\nincludes embodied features such as vehicle state and inputs, map (track\nboundaries and raceline), and cone landmarks. These are synchronized with\nconcurrent verbal coaching from a professional coach and additional feedback at\nthe end of each lap. We further provide annotations of coaching categories for\neach concurrent feedback utterance, ratings on students' compliance with\ncoaching advice, and self-reported cognitive load and emotional state of\nparticipants (gathered from surveys during the study). The dataset includes\nover 20,000 concurrent feedback utterances, over 400 terminal feedback\nutterances, and over 40 hours of vehicle driving data. Our naturalistic dataset\ncan be used for investigating motor learning dynamics, exploring linguistic\nphenomena, and training computational models of teaching. We demonstrate\napplications of this dataset for in-context learning, imitation learning, and\ntopic modeling. The dataset introduced in this work will be released publicly\nupon publication of the peer-reviewed version of this paper. Researchers\ninterested in early access may register at\nhttps://tinyurl.com/SimCoachCorpusForm.", "AI": {"tldr": "SimCoachCorpus\u662f\u4e00\u4e2a\u72ec\u7279\u7684\u8d5b\u8f66\u6a21\u62df\u9a7e\u9a76\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e13\u4e1a\u6559\u7ec3\u6307\u5bfc\u548c\u65e0\u6307\u5bfc\u6761\u4ef6\u4e0b\u7684\u9a7e\u9a76\u6570\u636e\uff0c\u7528\u4e8e\u7814\u7a76\u8bed\u8a00\u4e0e\u52a8\u4f5c\u7ed3\u5408\u7684\u6280\u80fd\u4e60\u5f97\u8fc7\u7a0b\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u6355\u83b7\u4eba\u4eec\u901a\u8fc7\u8bed\u8a00\u6307\u5bfc\u83b7\u5f97\u5177\u8eab\u6280\u80fd\u7684\u6570\u636e\u96c6\uff0c\u7279\u522b\u662f\u5728\u8bed\u8a00\u548c\u7269\u7406\u52a8\u4f5c\u6df1\u5ea6\u4ea4\u7ec7\u7684\u9886\u57df\u3002", "method": "\u6536\u96c629\u540d\u4eba\u7c7b\u5728\u8d5b\u8f66\u6a21\u62df\u5668\u4e2d\u7684\u9a7e\u9a76\u6570\u636e\uff0c\u5176\u4e2d15\u4eba\u63a5\u53d7\u4e13\u4e1a\u6559\u7ec3\u7684\u4e00\u5bf9\u4e00\u6307\u5bfc\uff0c14\u4eba\u65e0\u6307\u5bfc\u3002\u6570\u636e\u96c6\u5305\u542b\u8f66\u8f86\u72b6\u6001\u3001\u5730\u56fe\u4fe1\u606f\u3001\u6559\u7ec3\u8bed\u8a00\u53cd\u9988\u7b49\u540c\u6b65\u6570\u636e\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b\u8d85\u8fc720,000\u6761\u5b9e\u65f6\u53cd\u9988\u8bed\u53e5\u3001400\u591a\u6761\u7ec8\u70b9\u53cd\u9988\u8bed\u53e5\u548c40\u591a\u5c0f\u65f6\u7684\u9a7e\u9a76\u6570\u636e\uff0c\u5e76\u63d0\u4f9b\u4e86\u6559\u7ec3\u7c7b\u522b\u6807\u6ce8\u3001\u5b66\u751f\u4f9d\u4ece\u6027\u8bc4\u5206\u7b49\u4e30\u5bcc\u6807\u6ce8\u3002", "conclusion": "\u8be5\u81ea\u7136\u4e3b\u4e49\u6570\u636e\u96c6\u53ef\u7528\u4e8e\u7814\u7a76\u8fd0\u52a8\u5b66\u4e60\u52a8\u6001\u3001\u63a2\u7d22\u8bed\u8a00\u73b0\u8c61\u548c\u8bad\u7ec3\u6559\u5b66\u8ba1\u7b97\u6a21\u578b\uff0c\u5df2\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001\u6a21\u4eff\u5b66\u4e60\u548c\u4e3b\u9898\u5efa\u6a21\u7b49\u5e94\u7528\u4e2d\u5c55\u793a\u4ef7\u503c\u3002"}}
{"id": "2509.14942", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14942", "abs": "https://arxiv.org/abs/2509.14942", "authors": ["Minh-Khoi Pham", "Tai Tan Mai", "Martin Crane", "Rob Brennan", "Marie E. Ward", "Una Geary", "Declan Byrne", "Brian O Connell", "Colm Bergin", "Donncha Creagh", "Nick McDonald", "Marija Bezbradica"], "title": "Explainable AI for Infection Prevention and Control: Modeling CPE Acquisition and Patient Outcomes in an Irish Hospital with Transformers", "comment": "Accepted to BMC Medical Informatics and Decision Making on September\n  18th 2025", "summary": "Carbapenemase-Producing Enterobacteriace poses a critical concern for\ninfection prevention and control in hospitals. However, predictive modeling of\npreviously highlighted CPE-associated risks such as readmission, mortality, and\nextended length of stay (LOS) remains underexplored, particularly with modern\ndeep learning approaches. This study introduces an eXplainable AI modeling\nframework to investigate CPE impact on patient outcomes from Electronic Medical\nRecords data of an Irish hospital. We analyzed an inpatient dataset from an\nIrish acute hospital, incorporating diagnostic codes, ward transitions, patient\ndemographics, infection-related variables and contact network features. Several\nTransformer-based architectures were benchmarked alongside traditional machine\nlearning models. Clinical outcomes were predicted, and XAI techniques were\napplied to interpret model decisions. Our framework successfully demonstrated\nthe utility of Transformer-based models, with TabTransformer consistently\noutperforming baselines across multiple clinical prediction tasks, especially\nfor CPE acquisition (AUROC and sensitivity). We found infection-related\nfeatures, including historical hospital exposure, admission context, and\nnetwork centrality measures, to be highly influential in predicting patient\noutcomes and CPE acquisition risk. Explainability analyses revealed that\nfeatures like \"Area of Residence\", \"Admission Ward\" and prior admissions are\nkey risk factors. Network variables like \"Ward PageRank\" also ranked highly,\nreflecting the potential value of structural exposure information. This study\npresents a robust and explainable AI framework for analyzing complex EMR data\nto identify key risk factors and predict CPE-related outcomes. Our findings\nunderscore the superior performance of the Transformer models and highlight the\nimportance of diverse clinical and network features.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u89e3\u91caAI\u6846\u67b6\uff0c\u4f7f\u7528Transformer\u6a21\u578b\u5206\u6790\u7535\u5b50\u75c5\u5386\u6570\u636e\uff0c\u6210\u529f\u9884\u6d4b\u78b3\u9752\u9709\u70ef\u9176\u80a0\u6746\u83cc\u611f\u67d3\u98ce\u9669\u548c\u76f8\u5173\u4e34\u5e8a\u7ed3\u5c40\uff0c\u53d1\u73b0\u611f\u67d3\u76f8\u5173\u7279\u5f81\u548c\u7f51\u7edc\u53d8\u91cf\u662f\u5173\u952e\u98ce\u9669\u56e0\u7d20\u3002", "motivation": "\u78b3\u9752\u9709\u70ef\u9176\u80a0\u6746\u83cc\u611f\u67d3\u5bf9\u533b\u9662\u611f\u67d3\u9632\u63a7\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u4f46\u4f7f\u7528\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9884\u6d4b\u76f8\u5173\u98ce\u9669\uff08\u5982\u518d\u5165\u9662\u3001\u6b7b\u4ea1\u7387\u548c\u4f4f\u9662\u65f6\u95f4\u5ef6\u957f\uff09\u7684\u7814\u7a76\u4ecd\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u57fa\u4e8eTransformer\u7684\u67b6\u6784\u4e0e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5206\u6790\u7231\u5c14\u5170\u533b\u9662\u7535\u5b50\u75c5\u5386\u6570\u636e\uff0c\u5305\u62ec\u8bca\u65ad\u4ee3\u7801\u3001\u75c5\u623f\u8f6c\u79fb\u3001\u4eba\u53e3\u7edf\u8ba1\u5b66\u3001\u611f\u67d3\u76f8\u5173\u53d8\u91cf\u548c\u63a5\u89e6\u7f51\u7edc\u7279\u5f81\uff0c\u5e76\u5e94\u7528XAI\u6280\u672f\u89e3\u91ca\u6a21\u578b\u51b3\u7b56\u3002", "result": "TabTransformer\u6a21\u578b\u5728\u591a\u4e2a\u4e34\u5e8a\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u7279\u522b\u662f\u5728CPE\u83b7\u53d6\u9884\u6d4b\u65b9\u9762\uff08AUROC\u548c\u654f\u611f\u6027\uff09\u3002\u611f\u67d3\u76f8\u5173\u7279\u5f81\u3001\u5386\u53f2\u533b\u9662\u66b4\u9732\u3001\u5165\u9662\u73af\u5883\u548c\u7f51\u7edc\u4e2d\u5fc3\u6027\u5ea6\u91cf\u5bf9\u9884\u6d4b\u7ed3\u679c\u5f71\u54cd\u663e\u8457\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7a33\u5065\u4e14\u53ef\u89e3\u91ca\u7684AI\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u590d\u6742\u7535\u5b50\u75c5\u5386\u6570\u636e\uff0c\u8bc6\u522b\u5173\u952e\u98ce\u9669\u56e0\u7d20\u5e76\u9884\u6d4bCPE\u76f8\u5173\u7ed3\u5c40\uff0c\u5f3a\u8c03\u4e86Transformer\u6a21\u578b\u7684\u4f18\u8d8a\u6027\u80fd\u548c\u591a\u6837\u5316\u4e34\u5e8a\u53ca\u7f51\u7edc\u7279\u5f81\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.14564", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14564", "abs": "https://arxiv.org/abs/2509.14564", "authors": ["Takuya Kiyokawa", "Tomoki Ishikura", "Shingo Hamada", "Genichiro Matsuda", "Kensuke Harada"], "title": "Hierarchical Planning and Scheduling for Reconfigurable Multi-Robot Disassembly Systems under Structural Constraints", "comment": "6 pages, 7 figures", "summary": "This study presents a system integration approach for planning schedules,\nsequences, tasks, and motions for reconfigurable robots to automatically\ndisassemble constrained structures in a non-destructive manner. Such systems\nmust adapt their configuration and coordination to the target structure, but\nthe large and complex search space makes them prone to local optima. To address\nthis, we integrate multiple robot arms equipped with different types of tools,\ntogether with a rotary stage, into a reconfigurable setup. This flexible system\nis based on a hierarchical optimization method that generates plans meeting\nmultiple preferred conditions under mandatory requirements within a realistic\ntimeframe. The approach employs two many-objective genetic algorithms for\nsequence and task planning with motion evaluations, followed by constraint\nprogramming for scheduling. Because sequence planning has a much larger search\nspace, we introduce a chromosome initialization method tailored to constrained\nstructures to mitigate the risk of local optima. Simulation results demonstrate\nthat the proposed method effectively solves complex problems in reconfigurable\nrobotic disassembly.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5c42\u4f18\u5316\u7684\u53ef\u91cd\u6784\u673a\u5668\u4eba\u7cfb\u7edf\u96c6\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u975e\u7834\u574f\u6027\u62c6\u5378\u7ea6\u675f\u7ed3\u6784\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u9057\u4f20\u7b97\u6cd5\u548c\u7ea6\u675f\u89c4\u5212\u5b9e\u73b0\u5e8f\u5217\u3001\u4efb\u52a1\u548c\u8fd0\u52a8\u89c4\u5212", "motivation": "\u89e3\u51b3\u53ef\u91cd\u6784\u673a\u5668\u4eba\u5728\u62c6\u5378\u7ea6\u675f\u7ed3\u6784\u65f6\u9762\u4e34\u7684\u5927\u89c4\u6a21\u590d\u6742\u641c\u7d22\u7a7a\u95f4\u548c\u5c40\u90e8\u6700\u4f18\u95ee\u9898\uff0c\u9700\u8981\u7cfb\u7edf\u80fd\u591f\u81ea\u9002\u5e94\u914d\u7f6e\u548c\u534f\u8c03\u4ee5\u9002\u5e94\u76ee\u6807\u7ed3\u6784", "method": "\u96c6\u6210\u591a\u673a\u68b0\u81c2\u548c\u65cb\u8f6c\u53f0\u7684\u53ef\u91cd\u6784\u7cfb\u7edf\uff0c\u91c7\u7528\u5206\u5c42\u4f18\u5316\u65b9\u6cd5\uff1a\u4f7f\u7528\u591a\u76ee\u6807\u9057\u4f20\u7b97\u6cd5\u8fdb\u884c\u5e8f\u5217\u548c\u4efb\u52a1\u89c4\u5212\uff08\u542b\u8fd0\u52a8\u8bc4\u4f30\uff09\uff0c\u7136\u540e\u7528\u7ea6\u675f\u89c4\u5212\u8fdb\u884c\u8c03\u5ea6\uff0c\u7279\u522b\u9488\u5bf9\u5e8f\u5217\u89c4\u5212\u7684\u5927\u641c\u7d22\u7a7a\u95f4\u8bbe\u8ba1\u4e86\u67d3\u8272\u4f53\u521d\u59cb\u5316\u65b9\u6cd5", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u53ef\u91cd\u6784\u673a\u5668\u4eba\u62c6\u5378\u4e2d\u7684\u590d\u6742\u95ee\u9898\uff0c\u5728\u73b0\u5b9e\u65f6\u95f4\u8303\u56f4\u5185\u751f\u6210\u6ee1\u8db3\u591a\u91cd\u504f\u597d\u6761\u4ef6\u548c\u5f3a\u5236\u8981\u6c42\u7684\u8ba1\u5212", "conclusion": "\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u96c6\u6210\u548c\u5206\u5c42\u4f18\u5316\u65b9\u6cd5\u4e3a\u53ef\u91cd\u6784\u673a\u5668\u4eba\u975e\u7834\u574f\u6027\u62c6\u5378\u7ea6\u675f\u7ed3\u6784\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u907f\u514d\u5c40\u90e8\u6700\u4f18\u5e76\u5904\u7406\u590d\u6742\u641c\u7d22\u7a7a\u95f4"}}
{"id": "2509.14956", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.14956", "abs": "https://arxiv.org/abs/2509.14956", "authors": ["Diego Gosmar", "Deborah A. Dahl"], "title": "Sentinel Agents for Secure and Trustworthy Agentic AI in Multi-Agent Systems", "comment": "25 pages, 12 figures", "summary": "This paper proposes a novel architectural framework aimed at enhancing\nsecurity and reliability in multi-agent systems (MAS). A central component of\nthis framework is a network of Sentinel Agents, functioning as a distributed\nsecurity layer that integrates techniques such as semantic analysis via large\nlanguage models (LLMs), behavioral analytics, retrieval-augmented verification,\nand cross-agent anomaly detection. Such agents can potentially oversee\ninter-agent communications, identify potential threats, enforce privacy and\naccess controls, and maintain comprehensive audit records. Complementary to the\nidea of Sentinel Agents is the use of a Coordinator Agent. The Coordinator\nAgent supervises policy implementation, and manages agent participation. In\naddition, the Coordinator also ingests alerts from Sentinel Agents. Based on\nthese alerts, it can adapt policies, isolate or quarantine misbehaving agents,\nand contain threats to maintain the integrity of the MAS ecosystem. This\ndual-layered security approach, combining the continuous monitoring of Sentinel\nAgents with the governance functions of Coordinator Agents, supports dynamic\nand adaptive defense mechanisms against a range of threats, including prompt\ninjection, collusive agent behavior, hallucinations generated by LLMs, privacy\nbreaches, and coordinated multi-agent attacks. In addition to the architectural\ndesign, we present a simulation study where 162 synthetic attacks of different\nfamilies (prompt injection, hallucination, and data exfiltration) were injected\ninto a multi-agent conversational environment. The Sentinel Agents successfully\ndetected the attack attempts, confirming the practical feasibility of the\nproposed monitoring approach. The framework also offers enhanced system\nobservability, supports regulatory compliance, and enables policy evolution\nover time.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u54e8\u5175\u4ee3\u7406\u548c\u534f\u8c03\u4ee3\u7406\u7684\u53cc\u5c42\u5b89\u5168\u67b6\u6784\uff0c\u7528\u4e8e\u589e\u5f3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\uff0c\u901a\u8fc7\u6a21\u62df\u653b\u51fb\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u9762\u4e34\u5404\u79cd\u5b89\u5168\u5a01\u80c1\uff0c\u5982\u63d0\u793a\u6ce8\u5165\u3001\u5e7b\u89c9\u3001\u9690\u79c1\u6cc4\u9732\u7b49\uff0c\u9700\u8981\u52a8\u6001\u81ea\u9002\u5e94\u7684\u9632\u5fa1\u673a\u5236\u6765\u4fdd\u969c\u7cfb\u7edf\u5b8c\u6574\u6027\u3002", "method": "\u91c7\u7528\u5206\u5e03\u5f0f\u54e8\u5175\u4ee3\u7406\u7f51\u7edc\u8fdb\u884c\u8bed\u4e49\u5206\u6790\u3001\u884c\u4e3a\u5206\u6790\u548c\u5f02\u5e38\u68c0\u6d4b\uff0c\u914d\u5408\u534f\u8c03\u4ee3\u7406\u8fdb\u884c\u7b56\u7565\u7ba1\u7406\u548c\u5a01\u80c1\u54cd\u5e94\uff0c\u5f62\u6210\u53cc\u5c42\u5b89\u5168\u9632\u62a4\u4f53\u7cfb\u3002", "result": "\u5728\u6a21\u62df\u73af\u5883\u4e2d\u5bf9162\u79cd\u5408\u6210\u653b\u51fb\u8fdb\u884c\u6d4b\u8bd5\uff0c\u54e8\u5175\u4ee3\u7406\u6210\u529f\u68c0\u6d4b\u5230\u6240\u6709\u653b\u51fb\u5c1d\u8bd5\uff0c\u8bc1\u660e\u4e86\u76d1\u63a7\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5b89\u5168\u9632\u62a4\uff0c\u8fd8\u589e\u5f3a\u4e86\u7cfb\u7edf\u53ef\u89c2\u6d4b\u6027\uff0c\u652f\u6301\u6cd5\u89c4\u5408\u89c4\uff0c\u5e76\u80fd\u5b9e\u73b0\u7b56\u7565\u7684\u6301\u7eed\u6f14\u8fdb\u3002"}}
{"id": "2509.14630", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14630", "abs": "https://arxiv.org/abs/2509.14630", "authors": ["Anzhe Chen", "Yifei Yang", "Zhenjie Zhu", "Kechun Xu", "Zhongxiang Zhou", "Rong Xiong", "Yue Wang"], "title": "Toward Embodiment Equivariant Vision-Language-Action Policy", "comment": null, "summary": "Vision-language-action policies learn manipulation skills across tasks,\nenvironments and embodiments through large-scale pre-training. However, their\nability to generalize to novel robot configurations remains limited. Most\napproaches emphasize model size, dataset scale and diversity while paying less\nattention to the design of action spaces. This leads to the configuration\ngeneralization problem, which requires costly adaptation. We address this\nchallenge by formulating cross-embodiment pre-training as designing policies\nequivariant to embodiment configuration transformations. Building on this\nprinciple, we propose a framework that (i) establishes a embodiment\nequivariance theory for action space and policy design, (ii) introduces an\naction decoder that enforces configuration equivariance, and (iii) incorporates\na geometry-aware network architecture to enhance embodiment-agnostic spatial\nreasoning. Extensive experiments in both simulation and real-world settings\ndemonstrate that our approach improves pre-training effectiveness and enables\nefficient fine-tuning on novel robot embodiments. Our code is available at\nhttps://github.com/hhcaz/e2vla", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7b49\u53d8\u6027\u7406\u8bba\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u7b56\u7565\u6846\u67b6\uff0c\u901a\u8fc7\u8bbe\u8ba1\u5bf9\u673a\u5668\u4eba\u6784\u578b\u53d8\u6362\u7b49\u53d8\u7684\u52a8\u4f5c\u7a7a\u95f4\u548c\u89e3\u7801\u5668\uff0c\u89e3\u51b3\u8de8\u5177\u8eab\u9884\u8bad\u7ec3\u4e2d\u7684\u6784\u578b\u6cdb\u5316\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u7b56\u7565\u5728\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u4e2d\u4e3b\u8981\u5173\u6ce8\u6a21\u578b\u89c4\u6a21\u548c\u6570\u636e\u96c6\u591a\u6837\u6027\uff0c\u4f46\u5ffd\u89c6\u4e86\u52a8\u4f5c\u7a7a\u95f4\u8bbe\u8ba1\uff0c\u5bfc\u81f4\u5bf9\u65b0\u673a\u5668\u4eba\u6784\u578b\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u6602\u8d35\u7684\u9002\u914d\u6210\u672c\u3002", "method": "1) \u5efa\u7acb\u5177\u8eab\u7b49\u53d8\u6027\u7406\u8bba\u6307\u5bfc\u52a8\u4f5c\u7a7a\u95f4\u548c\u7b56\u7565\u8bbe\u8ba1\uff1b2) \u5f15\u5165\u5f3a\u5236\u6784\u578b\u7b49\u53d8\u7684\u52a8\u4f5c\u89e3\u7801\u5668\uff1b3) \u7ed3\u5408\u51e0\u4f55\u611f\u77e5\u7f51\u7edc\u67b6\u6784\u589e\u5f3a\u5177\u8eab\u65e0\u5173\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u7684\u5927\u91cf\u5b9e\u9a8c\u4e2d\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u9884\u8bad\u7ec3\u6548\u679c\uff0c\u5e76\u80fd\u9ad8\u6548\u5fae\u8c03\u9002\u5e94\u65b0\u7684\u673a\u5668\u4eba\u6784\u578b\u3002", "conclusion": "\u901a\u8fc7\u7b49\u53d8\u6027\u7406\u8bba\u6307\u5bfc\u7684\u52a8\u4f5c\u7a7a\u95f4\u8bbe\u8ba1\u662f\u89e3\u51b3\u8de8\u5177\u8eab\u9884\u8bad\u7ec3\u4e2d\u6784\u578b\u6cdb\u5316\u95ee\u9898\u7684\u6709\u6548\u9014\u5f84\uff0c\u4e3a\u673a\u5668\u4eba\u7b56\u7565\u7684\u901a\u7528\u6027\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.14963", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14963", "abs": "https://arxiv.org/abs/2509.14963", "authors": ["Filip Naudot", "Andreas Br\u00e4nnstr\u00f6m", "Vicen\u00e7 Torra", "Timotheus Kampik"], "title": "Set Contribution Functions for Quantitative Bipolar Argumentation and their Principles", "comment": null, "summary": "We present functions that quantify the contribution of a set of arguments in\nquantitative bipolar argumentation graphs to (the final strength of) an\nargument of interest, a so-called topic. Our set contribution functions are\ngeneralizations of existing functions that quantify the contribution of a\nsingle contributing argument to a topic. Accordingly, we generalize existing\ncontribution function principles for set contribution functions and provide a\ncorresponding principle-based analysis. We introduce new principles specific to\nset-based functions that focus on properties pertaining to the interaction of\narguments within a set. Finally, we sketch how the principles play out across\ndifferent set contribution functions given a recommendation system application\nscenario.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u91cf\u5316\u8bba\u8bc1\u96c6\u5408\u5728\u53cc\u6781\u8bba\u8bc1\u56fe\u4e2d\u5bf9\u76ee\u6807\u8bba\u8bc1\u5f3a\u5ea6\u8d21\u732e\u7684\u51fd\u6570\uff0c\u662f\u73b0\u6709\u5355\u8bba\u8bc1\u8d21\u732e\u51fd\u6570\u7684\u6cdb\u5316\uff0c\u5e76\u8fdb\u884c\u4e86\u57fa\u4e8e\u539f\u5219\u7684\u5206\u6790\u3002", "motivation": "\u73b0\u6709\u7684\u8bba\u8bc1\u8d21\u732e\u5206\u6790\u4e3b\u8981\u5173\u6ce8\u5355\u4e2a\u8bba\u8bc1\u5bf9\u76ee\u6807\u8bba\u8bc1\u7684\u5f71\u54cd\uff0c\u7f3a\u4e4f\u5bf9\u8bba\u8bc1\u96c6\u5408\u6574\u4f53\u8d21\u732e\u7684\u91cf\u5316\u65b9\u6cd5\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6355\u6349\u96c6\u5408\u5185\u8bba\u8bc1\u4ea4\u4e92\u7279\u6027\u7684\u8d21\u732e\u51fd\u6570\u3002", "method": "\u901a\u8fc7\u6cdb\u5316\u73b0\u6709\u7684\u5355\u8bba\u8bc1\u8d21\u732e\u51fd\u6570\u6765\u6784\u5efa\u96c6\u5408\u8d21\u732e\u51fd\u6570\uff0c\u5efa\u7acb\u76f8\u5e94\u7684\u539f\u5219\u6846\u67b6\u8fdb\u884c\u5206\u6790\uff0c\u5e76\u5f15\u5165\u4e13\u95e8\u9488\u5bf9\u96c6\u5408\u51fd\u6570\u7684\u65b0\u539f\u5219\u6765\u5173\u6ce8\u8bba\u8bc1\u95f4\u7684\u4ea4\u4e92\u7279\u6027\u3002", "result": "\u5f00\u53d1\u4e86\u80fd\u591f\u91cf\u5316\u8bba\u8bc1\u96c6\u5408\u8d21\u732e\u7684\u51fd\u6570\u6846\u67b6\uff0c\u63d0\u4f9b\u4e86\u539f\u5219\u5316\u7684\u5206\u6790\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u63a8\u8350\u7cfb\u7edf\u5e94\u7528\u573a\u666f\u5c55\u793a\u4e86\u4e0d\u540c\u96c6\u5408\u8d21\u732e\u51fd\u6570\u7684\u539f\u5219\u8868\u73b0\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u96c6\u5408\u8d21\u732e\u51fd\u6570\u6709\u6548\u6269\u5c55\u4e86\u5355\u8bba\u8bc1\u8d21\u732e\u5206\u6790\uff0c\u65b0\u7684\u96c6\u5408\u7279\u5b9a\u539f\u5219\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u8bba\u8bc1\u95f4\u7684\u4ea4\u4e92\u7279\u6027\uff0c\u4e3a\u590d\u6742\u8bba\u8bc1\u7cfb\u7edf\u7684\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2509.14636", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14636", "abs": "https://arxiv.org/abs/2509.14636", "authors": ["Yufei Wei", "Wangtao Lu", "Sha Lu", "Chenxiao Hu", "Fuzhang Han", "Rong Xiong", "Yue Wang"], "title": "BEV-ODOM2: Enhanced BEV-based Monocular Visual Odometry with PV-BEV Fusion and Dense Flow Supervision for Ground Robots", "comment": null, "summary": "Bird's-Eye-View (BEV) representation offers a metric-scaled planar workspace,\nfacilitating the simplification of 6-DoF ego-motion to a more robust 3-DoF\nmodel for monocular visual odometry (MVO) in intelligent transportation\nsystems. However, existing BEV methods suffer from sparse supervision signals\nand information loss during perspective-to-BEV projection. We present\nBEV-ODOM2, an enhanced framework addressing both limitations without additional\nannotations. Our approach introduces: (1) dense BEV optical flow supervision\nconstructed from 3-DoF pose ground truth for pixel-level guidance; (2) PV-BEV\nfusion that computes correlation volumes before projection to preserve 6-DoF\nmotion cues while maintaining scale consistency. The framework employs three\nsupervision levels derived solely from pose data: dense BEV flow, 5-DoF for the\nPV branch, and final 3-DoF output. Enhanced rotation sampling further balances\ndiverse motion patterns in training. Extensive evaluation on KITTI, NCLT,\nOxford, and our newly collected ZJH-VO multi-scale dataset demonstrates\nstate-of-the-art performance, achieving 40 improvement in RTE compared to\nprevious BEV methods. The ZJH-VO dataset, covering diverse ground vehicle\nscenarios from underground parking to outdoor plazas, is publicly available to\nfacilitate future research.", "AI": {"tldr": "BEV-ODOM2\u662f\u4e00\u4e2a\u6539\u8fdb\u7684\u5355\u76ee\u89c6\u89c9\u91cc\u7a0b\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u5bc6\u96c6BEV\u5149\u6d41\u76d1\u7763\u548cPV-BEV\u878d\u5408\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709BEV\u65b9\u6cd5\u7684\u7a00\u758f\u76d1\u7763\u548c\u4fe1\u606f\u4e22\u5931\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8640%\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684Bird's-Eye-View\u65b9\u6cd5\u5728\u5355\u76ee\u89c6\u89c9\u91cc\u7a0b\u8ba1\u4e2d\u5b58\u5728\u7a00\u758f\u76d1\u7763\u4fe1\u53f7\u548c\u900f\u89c6\u5230BEV\u6295\u5f71\u8fc7\u7a0b\u4e2d\u7684\u4fe1\u606f\u4e22\u5931\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u989d\u5916\u6807\u6ce8\u7684\u6539\u8fdb\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u5bc6\u96c6BEV\u5149\u6d41\u76d1\u7763\uff08\u4ece3-DoF\u4f4d\u59ff\u771f\u503c\u6784\u5efa\uff09\u548cPV-BEV\u878d\u5408\uff08\u5728\u6295\u5f71\u524d\u8ba1\u7b97\u76f8\u5173\u4f53\u79ef\u4ee5\u4fdd\u63016-DoF\u8fd0\u52a8\u7ebf\u7d22\uff09\uff0c\u91c7\u7528\u4e09\u7ea7\u76d1\u7763\u673a\u5236\u548c\u589e\u5f3a\u7684\u65cb\u8f6c\u91c7\u6837\u7b56\u7565\u3002", "result": "\u5728KITTI\u3001NCLT\u3001Oxford\u548c\u65b0\u6536\u96c6\u7684ZJH-VO\u591a\u5c3a\u5ea6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u76f8\u5bf9\u4f4d\u59ff\u8bef\u5dee(RTE)\u76f8\u6bd4\u4e4b\u524d\u7684BEV\u65b9\u6cd5\u63d0\u5347\u4e8640%\u3002", "conclusion": "BEV-ODOM2\u6709\u6548\u89e3\u51b3\u4e86BEV\u65b9\u6cd5\u7684\u76d1\u7763\u7a00\u758f\u6027\u548c\u4fe1\u606f\u4e22\u5931\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5355\u76ee\u89c6\u89c9\u91cc\u7a0b\u8ba1\u6027\u80fd\uff0c\u5e76\u53d1\u5e03\u4e86\u65b0\u7684\u591a\u573a\u666f\u6570\u636e\u96c6ZJH-VO\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76\u3002"}}
{"id": "2509.14998", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14998", "abs": "https://arxiv.org/abs/2509.14998", "authors": ["Xiao Wu", "Ting-Zhu Huang", "Liang-Jian Deng", "Yanyuan Qiao", "Imran Razzak", "Yutong Xie"], "title": "A Knowledge-driven Adaptive Collaboration of LLMs for Enhancing Medical Decision-making", "comment": "The paper has been accepted to the EMNLP 2025 Main Conference", "summary": "Medical decision-making often involves integrating knowledge from multiple\nclinical specialties, typically achieved through multidisciplinary teams.\nInspired by this collaborative process, recent work has leveraged large\nlanguage models (LLMs) in multi-agent collaboration frameworks to emulate\nexpert teamwork. While these approaches improve reasoning through agent\ninteraction, they are limited by static, pre-assigned roles, which hinder\nadaptability and dynamic knowledge integration. To address these limitations,\nwe propose KAMAC, a Knowledge-driven Adaptive Multi-Agent Collaboration\nframework that enables LLM agents to dynamically form and expand expert teams\nbased on the evolving diagnostic context. KAMAC begins with one or more expert\nagents and then conducts a knowledge-driven discussion to identify and fill\nknowledge gaps by recruiting additional specialists as needed. This supports\nflexible, scalable collaboration in complex clinical scenarios, with decisions\nfinalized through reviewing updated agent comments. Experiments on two\nreal-world medical benchmarks demonstrate that KAMAC significantly outperforms\nboth single-agent and advanced multi-agent methods, particularly in complex\nclinical scenarios (i.e., cancer prognosis) requiring dynamic, cross-specialty\nexpertise. Our code is publicly available at:\nhttps://github.com/XiaoXiao-Woo/KAMAC.", "AI": {"tldr": "KAMAC\u662f\u4e00\u4e2a\u77e5\u8bc6\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u7ec4\u5efa\u4e13\u5bb6\u56e2\u961f\u6765\u89e3\u51b3\u533b\u7597\u51b3\u7b56\u4e2d\u591a\u4e13\u4e1a\u77e5\u8bc6\u6574\u5408\u7684\u95ee\u9898\uff0c\u5728\u590d\u6742\u4e34\u5e8a\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u65b9\u6cd5\u91c7\u7528\u9759\u6001\u9884\u5206\u914d\u89d2\u8272\uff0c\u9650\u5236\u4e86\u9002\u5e94\u6027\u548c\u52a8\u6001\u77e5\u8bc6\u6574\u5408\u80fd\u529b\uff0c\u65e0\u6cd5\u6709\u6548\u6a21\u62df\u771f\u5b9e\u533b\u7597\u56e2\u961f\u6839\u636e\u8bca\u65ad\u9700\u6c42\u52a8\u6001\u8c03\u6574\u4e13\u5bb6\u7ec4\u6210\u7684\u534f\u4f5c\u8fc7\u7a0b\u3002", "method": "KAMAC\u6846\u67b6\u4ece\u521d\u59cb\u4e13\u5bb6\u667a\u80fd\u4f53\u5f00\u59cb\uff0c\u901a\u8fc7\u77e5\u8bc6\u9a71\u52a8\u8ba8\u8bba\u8bc6\u522b\u77e5\u8bc6\u7f3a\u53e3\uff0c\u6839\u636e\u9700\u8981\u52a8\u6001\u62db\u52df\u989d\u5916\u4e13\u5bb6\uff0c\u652f\u6301\u7075\u6d3b\u53ef\u6269\u5c55\u7684\u534f\u4f5c\uff0c\u6700\u7ec8\u901a\u8fc7\u5ba1\u67e5\u66f4\u65b0\u540e\u7684\u667a\u80fd\u4f53\u8bc4\u8bba\u6765\u505a\u51fa\u51b3\u7b56\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u533b\u7597\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cKAMAC\u663e\u8457\u4f18\u4e8e\u5355\u667a\u80fd\u4f53\u548c\u5148\u8fdb\u7684\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u52a8\u6001\u8de8\u4e13\u4e1a\u77e5\u8bc6\u7684\u590d\u6742\u4e34\u5e8a\u573a\u666f\uff08\u5982\u764c\u75c7\u9884\u540e\uff09\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "KAMAC\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u7ec4\u5efa\u4e13\u5bb6\u56e2\u961f\u7684\u65b9\u5f0f\u6709\u6548\u89e3\u51b3\u4e86\u533b\u7597\u51b3\u7b56\u4e2d\u7684\u591a\u4e13\u4e1a\u77e5\u8bc6\u6574\u5408\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u4e34\u5e8a\u573a\u666f\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u548c\u53ef\u6269\u5c55\u7684\u534f\u4f5c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.14641", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14641", "abs": "https://arxiv.org/abs/2509.14641", "authors": ["Sibaek Lee", "Jiung Yeon", "Hyeonwoo Yu"], "title": "Efficient 3D Perception on Embedded Systems via Interpolation-Free Tri-Plane Lifting and Volume Fusion", "comment": null, "summary": "Dense 3D convolutions provide high accuracy for perception but are too\ncomputationally expensive for real-time robotic systems. Existing tri-plane\nmethods rely on 2D image features with interpolation, point-wise queries, and\nimplicit MLPs, which makes them computationally heavy and unsuitable for\nembedded 3D inference. As an alternative, we propose a novel interpolation-free\ntri-plane lifting and volumetric fusion framework, that directly projects 3D\nvoxels into plane features and reconstructs a feature volume through broadcast\nand summation. This shifts nonlinearity to 2D convolutions, reducing complexity\nwhile remaining fully parallelizable. To capture global context, we add a\nlow-resolution volumetric branch fused with the lifted features through a\nlightweight integration layer, yielding a design that is both efficient and\nend-to-end GPU-accelerated. To validate the effectiveness of the proposed\nmethod, we conduct experiments on classification, completion, segmentation, and\ndetection, and we map the trade-off between efficiency and accuracy across\ntasks. Results show that classification and completion retain or improve\naccuracy, while segmentation and detection trade modest drops in accuracy for\nsignificant computational savings. On-device benchmarks on an NVIDIA Jetson\nOrin nano confirm robust real-time throughput, demonstrating the suitability of\nthe approach for embedded robotic perception.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u63d2\u503c\u4e09\u5e73\u9762\u63d0\u53d6\u548c\u4f53\u79ef\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u5c063D\u50cf\u7d20\u76f4\u63a5\u6295\u5f71\u52302D\u5e73\u9762\u7279\u5f81\u5e76\u901a\u8fc7\u5e7f\u64ad\u548c\u6c42\u548c\u91cd\u5efa\u7279\u5f81\u4f53\u79ef\uff0c\u5927\u5927\u63d0\u9ad8\u4e86\u5d4c\u5165\u5f0f\u673a\u5668\u4eba\u7cfb\u7edf\u76843D\u89c6\u89c9\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u5bc6\u96c63D\u5377\u79ef\u867d\u7136\u51c6\u786e\u6027\u9ad8\uff0c\u4f46\u8ba1\u7b97\u590d\u6742\u5ea6\u8fc7\u9ad8\uff0c\u4e0d\u9002\u5408\u5b9e\u65f6\u673a\u5668\u4eba\u7cfb\u7edf\u3002\u73b0\u6709\u7684\u4e09\u5e73\u9762\u65b9\u6cd5\u4f9d\u8d56\u4e8e2D\u56fe\u50cf\u7279\u5f81\u4e0e\u9690\u5f0fMLP\uff0c\u8ba1\u7b97\u8d1f\u62c5\u8f83\u5927\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6848\u6765\u652f\u6301\u5d4c\u5165\u5f0f3D\u63a8\u7406\u3002", "method": "\u63d0\u51fa\u65e0\u63d2\u503c\u4e09\u5e73\u9762\u63d0\u53d6\u548c\u4f53\u79ef\u878d\u5408\u6846\u67b6\uff0c\u76f4\u63a5\u5c063D\u50cf\u7d20\u6295\u5f71\u5230\u5e73\u9762\u7279\u5f81\uff0c\u901a\u8fc7\u5e7f\u64ad\u548c\u6c42\u548c\u91cd\u5efa\u7279\u5f81\u4f53\u79ef\u3002\u5c06\u975e\u7ebf\u6027\u8f6c\u79fb\u52302D\u5377\u79ef\uff0c\u964d\u4f4e\u590d\u6742\u5ea6\u540c\u65f6\u4fdd\u6301\u5e76\u884c\u5316\u3002\u6dfb\u52a0\u4f4e\u5206\u8fa8\u7387\u4f53\u79ef\u5206\u652f\u83b7\u53d6\u5168\u5c40\u4e0a\u4e0b\u6587\uff0c\u901a\u8fc7\u8f7b\u91cf\u96c6\u6210\u5c42\u878d\u5408\u3002", "result": "\u5728\u5206\u7c7b\u548c\u8865\u5168\u4efb\u52a1\u4e2d\u4fdd\u6301\u6216\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u5728\u5206\u5272\u548c\u68c0\u6d4b\u4efb\u52a1\u4e2d\u4ee5\u8f7b\u5fae\u7684\u51c6\u786e\u6027\u4e0b\u964d\u6362\u6362\u53d6\u4e86\u663e\u8457\u7684\u8ba1\u7b97\u8282\u7701\u3002\u5728NVIDIA Jetson Orin nano\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u5b9e\u65f6\u901f\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u91cd\u65b0\u8bbe\u8ba1\u4e09\u5e73\u9762\u7279\u5f81\u63d0\u53d6\u548c\u878d\u5408\u6d41\u7a0b\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\u5927\u5927\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u5408\u5d4c\u5165\u5f0f\u673a\u5668\u4eba\u89c6\u89c9\u7cfb\u7edf\u7684\u5b9e\u65f6\u9700\u6c42\u3002"}}
{"id": "2509.15035", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.15035", "abs": "https://arxiv.org/abs/2509.15035", "authors": ["Gabriela C. Zapata", "Bill Cope", "Mary Kalantzis", "Duane Searsmith"], "title": "Calibrated Generative AI as Meta-Reviewer: A Systemic Functional Linguistics Discourse Analysis of Reviews of Peer Reviews", "comment": "39 pages, 3 tables", "summary": "This study investigates the use of generative AI to support formative\nassessment through machine generated reviews of peer reviews in graduate online\ncourses in a public university in the United States. Drawing on Systemic\nFunctional Linguistics and Appraisal Theory, we analyzed 120 metareviews to\nexplore how generative AI feedback constructs meaning across ideational,\ninterpersonal, and textual dimensions. The findings suggest that generative AI\ncan approximate key rhetorical and relational features of effective human\nfeedback, offering directive clarity while also maintaining a supportive\nstance. The reviews analyzed demonstrated a balance of praise and constructive\ncritique, alignment with rubric expectations, and structured staging that\nforegrounded student agency. By modeling these qualities, AI metafeedback has\nthe potential to scaffold feedback literacy and enhance leaner engagement with\npeer review.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u751f\u6210\u5f0fAI\u4e3a\u7814\u7a76\u751f\u5728\u7ebf\u8bfe\u7a0b\u7684\u540c\u4f34\u8bc4\u5ba1\u63d0\u4f9b\u673a\u5668\u751f\u6210\u7684\u5143\u8bc4\u5ba1\uff0c\u4ee5\u652f\u6301\u5f62\u6210\u6027\u8bc4\u4f30\u3002\u7814\u7a76\u53d1\u73b0AI\u53cd\u9988\u80fd\u591f\u8fd1\u4f3c\u6709\u6548\u4eba\u7c7b\u53cd\u9988\u7684\u5173\u952e\u4fee\u8f9e\u548c\u5173\u7cfb\u7279\u5f81\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u751f\u6210\u5f0fAI\u5982\u4f55\u901a\u8fc7\u673a\u5668\u751f\u6210\u7684\u5143\u8bc4\u5ba1\u6765\u652f\u6301\u5728\u7ebf\u6559\u80b2\u4e2d\u7684\u5f62\u6210\u6027\u8bc4\u4f30\uff0c\u7279\u522b\u662f\u5728\u540c\u4f34\u8bc4\u5ba1\u8fc7\u7a0b\u4e2d\u63d0\u4f9b\u53cd\u9988\u652f\u6301\u3002", "method": "\u57fa\u4e8e\u7cfb\u7edf\u529f\u80fd\u8bed\u8a00\u5b66\u548c\u8bc4\u4ef7\u7406\u8bba\uff0c\u5206\u6790\u4e86120\u4e2aAI\u751f\u6210\u7684\u5143\u8bc4\u5ba1\uff0c\u4ece\u6982\u5ff5\u3001\u4eba\u9645\u548c\u6587\u672c\u4e09\u4e2a\u7ef4\u5ea6\u63a2\u8ba8AI\u53cd\u9988\u5982\u4f55\u6784\u5efa\u610f\u4e49\u3002", "result": "\u7814\u7a76\u53d1\u73b0AI\u53cd\u9988\u80fd\u591f\u5e73\u8861\u8868\u626c\u548c\u5efa\u8bbe\u6027\u6279\u8bc4\uff0c\u4e0e\u8bc4\u5206\u6807\u51c6\u4fdd\u6301\u4e00\u81f4\uff0c\u5e76\u91c7\u7528\u7ed3\u6784\u5316\u5448\u73b0\u65b9\u5f0f\u7a81\u51fa\u5b66\u751f\u4e3b\u4f53\u6027\uff0c\u5c55\u73b0\u51fa\u6307\u5bfc\u6e05\u6670\u6027\u548c\u652f\u6301\u6027\u7acb\u573a\u3002", "conclusion": "AI\u5143\u53cd\u9988\u5177\u6709\u642d\u5efa\u53cd\u9988\u7d20\u517b\u652f\u67b6\u548c\u589e\u5f3a\u5b66\u4e60\u8005\u53c2\u4e0e\u540c\u4f34\u8bc4\u5ba1\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u6a21\u62df\u6709\u6548\u4eba\u7c7b\u53cd\u9988\u7684\u5173\u952e\u54c1\u8d28\u3002"}}
{"id": "2509.14687", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14687", "abs": "https://arxiv.org/abs/2509.14687", "authors": ["Cong Tai", "Zhaoyu Zheng", "Haixu Long", "Hansheng Wu", "Haodong Xiang", "Zhengbin Long", "Jun Xiong", "Rong Shi", "Shizhuang Zhang", "Gang Qiu", "He Wang", "Ruifeng Li", "Jun Huang", "Bin Chang", "Shuai Feng", "Tao Shen"], "title": "RealMirror: A Comprehensive, Open-Source Vision-Language-Action Platform for Embodied AI", "comment": null, "summary": "The emerging field of Vision-Language-Action (VLA) for humanoid robots faces\nseveral fundamental challenges, including the high cost of data acquisition,\nthe lack of a standardized benchmark, and the significant gap between\nsimulation and the real world. To overcome these obstacles, we propose\nRealMirror, a comprehensive, open-source embodied AI VLA platform. RealMirror\nbuilds an efficient, low-cost data collection, model training, and inference\nsystem that enables end-to-end VLA research without requiring a real robot. To\nfacilitate model evolution and fair comparison, we also introduce a dedicated\nVLA benchmark for humanoid robots, featuring multiple scenarios, extensive\ntrajectories, and various VLA models. Furthermore, by integrating generative\nmodels and 3D Gaussian Splatting to reconstruct realistic environments and\nrobot models, we successfully demonstrate zero-shot Sim2Real transfer, where\nmodels trained exclusively on simulation data can perform tasks on a real robot\nseamlessly, without any fine-tuning. In conclusion, with the unification of\nthese critical components, RealMirror provides a robust framework that\nsignificantly accelerates the development of VLA models for humanoid robots.\nProject page: https://terminators2025.github.io/RealMirror.github.io", "AI": {"tldr": "RealMirror\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u5e73\u53f0\uff0c\u901a\u8fc7\u4f4e\u6210\u672c\u6570\u636e\u6536\u96c6\u3001\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\u548c\u751f\u6210\u5f0f\u6a21\u578b\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u771f\u5b9e\u673a\u5668\u4eba\u7684\u7aef\u5230\u7aefVLA\u7814\u7a76\uff0c\u5e76\u6210\u529f\u5c55\u793a\u4e86\u96f6\u6837\u672c\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u3002", "motivation": "\u89e3\u51b3\u4eba\u5f62\u673a\u5668\u4ebaVLA\u7814\u7a76\u9762\u4e34\u7684\u6570\u636e\u91c7\u96c6\u6210\u672c\u9ad8\u3001\u7f3a\u4e4f\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\u3001\u4eff\u771f\u4e0e\u73b0\u5b9e\u4e16\u754c\u5dee\u8ddd\u5927\u7b49\u6838\u5fc3\u6311\u6218\u3002", "method": "\u6784\u5efa\u9ad8\u6548\u4f4e\u6210\u672c\u7684\u6570\u636e\u6536\u96c6\u3001\u6a21\u578b\u8bad\u7ec3\u548c\u63a8\u7406\u7cfb\u7edf\uff1b\u96c6\u6210\u751f\u6210\u5f0f\u6a21\u578b\u548c3D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u91cd\u5efa\u771f\u5b9e\u73af\u5883\u548c\u673a\u5668\u4eba\u6a21\u578b\uff1b\u5efa\u7acb\u4e13\u95e8\u7684VLA\u57fa\u51c6\u6d4b\u8bd5\u5305\u542b\u591a\u573a\u666f\u548c\u4e30\u5bcc\u8f68\u8ff9\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u96f6\u6837\u672cSim2Real\u8fc1\u79fb\uff0c\u4ec5\u4f7f\u7528\u4eff\u771f\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u65e0\u7f1d\u6267\u884c\u4efb\u52a1\u3002", "conclusion": "RealMirror\u901a\u8fc7\u7edf\u4e00\u5173\u952e\u7ec4\u4ef6\uff0c\u4e3a\u4eba\u5f62\u673a\u5668\u4ebaVLA\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u5f3a\u5927\u6846\u67b6\uff0c\u663e\u8457\u52a0\u901f\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.14688", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14688", "abs": "https://arxiv.org/abs/2509.14688", "authors": ["Yue Xu", "Litao Wei", "Pengyu An", "Qingyu Zhang", "Yong-Lu Li"], "title": "exUMI: Extensible Robot Teaching System with Action-aware Task-agnostic Tactile Representation", "comment": "Accepted at CoRL 2025", "summary": "Tactile-aware robot learning faces critical challenges in data collection and\nrepresentation due to data scarcity and sparsity, and the absence of force\nfeedback in existing systems. To address these limitations, we introduce a\ntactile robot learning system with both hardware and algorithm innovations. We\npresent exUMI, an extensible data collection device that enhances the vanilla\nUMI with robust proprioception (via AR MoCap and rotary encoder), modular\nvisuo-tactile sensing, and automated calibration, achieving 100% data\nusability. Building on an efficient collection of over 1 M tactile frames, we\npropose Tactile Prediction Pretraining (TPP), a representation learning\nframework through action-aware temporal tactile prediction, capturing contact\ndynamics and mitigating tactile sparsity. Real-world experiments show that TPP\noutperforms traditional tactile imitation learning. Our work bridges the gap\nbetween human tactile intuition and robot learning through co-designed hardware\nand algorithms, offering open-source resources to advance contact-rich\nmanipulation research. Project page: https://silicx.github.io/exUMI.", "AI": {"tldr": "\u63d0\u51fa\u4e86exUMI\u89e6\u89c9\u6570\u636e\u6536\u96c6\u8bbe\u5907\u548cTPP\u89e6\u89c9\u9884\u6d4b\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u786c\u4ef6\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\u89e3\u51b3\u89e6\u89c9\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u548c\u7a00\u758f\u6027\u95ee\u9898", "motivation": "\u89e6\u89c9\u673a\u5668\u4eba\u5b66\u4e60\u9762\u4e34\u6570\u636e\u6536\u96c6\u56f0\u96be\u3001\u6570\u636e\u7a00\u758f\u4ee5\u53ca\u73b0\u6709\u7cfb\u7edf\u7f3a\u4e4f\u529b\u53cd\u9988\u7b49\u5173\u952e\u6311\u6218\uff0c\u9700\u8981\u65b0\u7684\u786c\u4ef6\u548c\u7b97\u6cd5\u89e3\u51b3\u65b9\u6848", "method": "\u5f00\u53d1exUMI\u53ef\u6269\u5c55\u6570\u636e\u6536\u96c6\u8bbe\u5907\uff08\u589e\u5f3a\u7248UMI\uff0c\u5177\u6709\u9c81\u68d2\u672c\u4f53\u611f\u77e5\u3001\u6a21\u5757\u5316\u89c6\u89c9\u89e6\u89c9\u4f20\u611f\u548c\u81ea\u52a8\u6821\u51c6\uff09\uff0c\u63d0\u51faTPP\u89e6\u89c9\u9884\u6d4b\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u4f5c\u611f\u77e5\u65f6\u95f4\u89e6\u89c9\u9884\u6d4b\u8fdb\u884c\u8868\u793a\u5b66\u4e60", "result": "\u5b9e\u73b0\u4e86100%\u6570\u636e\u53ef\u7528\u6027\uff0c\u6536\u96c6\u8d85\u8fc7100\u4e07\u89e6\u89c9\u5e27\uff0c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u663e\u793aTPP\u4f18\u4e8e\u4f20\u7edf\u89e6\u89c9\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5", "conclusion": "\u901a\u8fc7\u786c\u4ef6\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\u5f25\u5408\u4e86\u4eba\u7c7b\u89e6\u89c9\u76f4\u89c9\u4e0e\u673a\u5668\u4eba\u5b66\u4e60\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u7814\u7a76\u63d0\u4f9b\u4e86\u5f00\u6e90\u8d44\u6e90"}}
{"id": "2509.15172", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15172", "abs": "https://arxiv.org/abs/2509.15172", "authors": ["Ankur Samanta", "Akshayaa Magesh", "Youliang Yu", "Runzhe Wu", "Ayush Jain", "Daniel Jiang", "Boris Vidolov", "Paul Sajda", "Yonathan Efroni", "Kaveh Hassani"], "title": "Internalizing Self-Consistency in Language Models: Multi-Agent Consensus Alignment", "comment": null, "summary": "Language Models (LMs) are inconsistent reasoners, often generating\ncontradictory responses to identical prompts. While inference-time methods can\nmitigate these inconsistencies, they fail to address the core problem: LMs\nstruggle to reliably select reasoning pathways leading to consistent outcomes\nunder exploratory sampling. To address this, we formalize self-consistency as\nan intrinsic property of well-aligned reasoning models and introduce\nMulti-Agent Consensus Alignment (MACA), a reinforcement learning framework that\npost-trains models to favor reasoning trajectories aligned with their internal\nconsensus using majority/minority outcomes from multi-agent debate. These\ntrajectories emerge from deliberative exchanges where agents ground reasoning\nin peer arguments, not just aggregation of independent attempts, creating\nricher consensus signals than single-round majority voting. MACA enables agents\nto teach themselves to be more decisive and concise, and better leverage peer\ninsights in multi-agent settings without external supervision, driving\nsubstantial improvements across self-consistency (+27.6% on GSM8K),\nsingle-agent reasoning (+23.7% on MATH), sampling-based inference (+22.4%\nPass@20 on MATH), and multi-agent ensemble decision-making (+42.7% on MathQA).\nThese findings, coupled with strong generalization to unseen benchmarks (+16.3%\non GPQA, +11.6% on CommonsenseQA), demonstrate robust self-alignment that more\nreliably unlocks latent reasoning potential of language models.", "AI": {"tldr": "MACA\u662f\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u7684\u591a\u6570/\u5c11\u6570\u7ed3\u679c\u6765\u540e\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f\u5176\u504f\u597d\u4e0e\u5185\u90e8\u5171\u8bc6\u4e00\u81f4\u63a8\u7406\u8def\u5f84\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u7684\u81ea\u4e00\u81f4\u6027\u3001\u63a8\u7406\u80fd\u529b\u548c\u591a\u667a\u80fd\u4f53\u51b3\u7b56\u6027\u80fd\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u662f\u4e0d\u4e00\u81f4\u7684\u63a8\u7406\u8005\uff0c\u7ecf\u5e38\u5bf9\u76f8\u540c\u63d0\u793a\u751f\u6210\u77db\u76fe\u54cd\u5e94\u3002\u73b0\u6709\u63a8\u7406\u65f6\u65b9\u6cd5\u65e0\u6cd5\u89e3\u51b3\u6838\u5fc3\u95ee\u9898\uff1a\u6a21\u578b\u96be\u4ee5\u53ef\u9760\u9009\u62e9\u5bfc\u81f4\u4e00\u81f4\u7ed3\u679c\u7684\u63a8\u7406\u8def\u5f84\u3002", "method": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u5171\u8bc6\u5bf9\u9f50(MACA)\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u4e2d\u7684\u591a\u6570/\u5c11\u6570\u7ed3\u679c\u8fdb\u884c\u540e\u8bad\u7ec3\uff0c\u4f7f\u6a21\u578b\u504f\u597d\u4e0e\u5185\u90e8\u5171\u8bc6\u4e00\u81f4\u7684\u63a8\u7406\u8f68\u8ff9\u3002", "result": "\u5728\u81ea\u4e00\u81f4\u6027(GSM8K +27.6%)\u3001\u5355\u667a\u80fd\u4f53\u63a8\u7406(MATH +23.7%)\u3001\u91c7\u6837\u63a8\u7406(MATH Pass@20 +22.4%)\u548c\u591a\u667a\u80fd\u4f53\u96c6\u6210\u51b3\u7b56(MathQA +42.7%)\u65b9\u9762\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u5728\u672a\u89c1\u57fa\u51c6\u4e0a\u4e5f\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MACA\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u81ea\u5bf9\u9f50\uff0c\u66f4\u53ef\u9760\u5730\u91ca\u653e\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u5728\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u4ea7\u751f\u6bd4\u5355\u8f6e\u591a\u6570\u6295\u7968\u66f4\u4e30\u5bcc\u7684\u5171\u8bc6\u4fe1\u53f7\u3002"}}
{"id": "2509.14698", "categories": ["cs.RO", "cs.NA", "math.DG", "math.GR", "math.NA"], "pdf": "https://arxiv.org/pdf/2509.14698", "abs": "https://arxiv.org/abs/2509.14698", "authors": ["Andreas Mueller"], "title": "Wohlhart's Three-Loop Mechanism: An Overconstrained and Shaky Linkage", "comment": null, "summary": "This paper revisits a three-loop spatial linkage that was proposed in an ARK\n2004 paper by Karl Wohlhart (as extension of a two-loop linkage proposed by\nEddie Baker in 1980) and later analyzed in an ARK 2006 paper by Diez-Martinez\net. al. A local analysis shows that this linkage has a finite degree of freedom\n(DOF) 3 (and is thus overconstrained) while in its reference configuration the\ndifferential DOF is 5. It is shown that its configuration space is locally a\nsmooth manifold so that the reference configuration is not a c-space\nsingularity. It is shown that the differential DOF is locally constant, which\nmakes this linkage shaky (so that the reference configuration is not a\nsingularity). The higher-order local analysis is facilitated by the computation\nof the kinematic tangent cone as well as a local approximation of the c-space.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u5206\u6790\u4e86\u4e00\u4e2a\u4e09\u73af\u7a7a\u95f4\u8fde\u6746\u673a\u6784\uff0c\u53d1\u73b0\u8be5\u673a\u6784\u5728\u53c2\u8003\u6784\u578b\u4e0b\u5177\u6709\u6709\u9650\u81ea\u7531\u5ea63\uff08\u8fc7\u7ea6\u675f\uff09\uff0c\u4f46\u5fae\u5206\u81ea\u7531\u5ea6\u4e3a5\uff0c\u4e14\u6784\u578b\u7a7a\u95f4\u662f\u5c40\u90e8\u5149\u6ed1\u6d41\u5f62\uff0c\u8868\u660e\u53c2\u8003\u6784\u578b\u4e0d\u662f\u5947\u5f02\u70b9\u3002", "motivation": "\u91cd\u65b0\u5ba1\u89c6Wohlhart\u5728ARK 2004\u63d0\u51fa\u7684\u4e09\u73af\u7a7a\u95f4\u8fde\u6746\u673a\u6784\uff08\u57fa\u4e8eBaker 1980\u5e74\u7684\u4e24\u73af\u673a\u6784\uff09\uff0c\u8be5\u673a\u6784\u5148\u524d\u5728ARK 2006\u7531Diez-Martinez\u7b49\u4eba\u5206\u6790\u8fc7\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u660e\u786e\u5176\u8fd0\u52a8\u5b66\u7279\u6027\u3002", "method": "\u91c7\u7528\u5c40\u90e8\u5206\u6790\u65b9\u6cd5\uff0c\u8ba1\u7b97\u8fd0\u52a8\u5b66\u5207\u9525\u548c\u6784\u578b\u7a7a\u95f4\u7684\u5c40\u90e8\u8fd1\u4f3c\uff0c\u5206\u6790\u673a\u6784\u7684\u6709\u9650\u81ea\u7531\u5ea6\u548c\u5fae\u5206\u81ea\u7531\u5ea6\u3002", "result": "\u8be5\u8fde\u6746\u673a\u6784\u5177\u6709\u6709\u9650\u81ea\u7531\u5ea63\uff08\u8fc7\u7ea6\u675f\uff09\uff0c\u5728\u53c2\u8003\u6784\u578b\u4e0b\u5fae\u5206\u81ea\u7531\u5ea6\u4e3a5\uff1b\u6784\u578b\u7a7a\u95f4\u662f\u5c40\u90e8\u5149\u6ed1\u6d41\u5f62\uff0c\u53c2\u8003\u6784\u578b\u4e0d\u662f\u5947\u5f02\u70b9\uff1b\u5fae\u5206\u81ea\u7531\u5ea6\u5c40\u90e8\u6052\u5b9a\uff0c\u8868\u660e\u673a\u6784\u662f\u6447\u52a8\u7684\u3002", "conclusion": "\u8be5\u4e09\u73af\u7a7a\u95f4\u8fde\u6746\u673a\u6784\u5728\u53c2\u8003\u6784\u578b\u4e0b\u8868\u73b0\u51fa\u7279\u6b8a\u7684\u8fd0\u52a8\u5b66\u7279\u6027\uff1a\u6709\u9650\u81ea\u7531\u5ea6\u4e0e\u5fae\u5206\u81ea\u7531\u5ea6\u4e0d\u4e00\u81f4\uff0c\u4f46\u6784\u578b\u7a7a\u95f4\u5149\u6ed1\u4e14\u65e0\u5947\u5f02\uff0c\u5c5e\u4e8e\u6447\u52a8\u673a\u6784\u3002"}}
{"id": "2509.15217", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15217", "abs": "https://arxiv.org/abs/2509.15217", "authors": ["Yue Xin", "Wenyuan Wang", "Rui Pan", "Ruida Wang", "Howard Meng", "Renjie Pi", "Shizhe Diao", "Tong Zhang"], "title": "Generalizable Geometric Image Caption Synthesis", "comment": null, "summary": "Multimodal large language models have various practical applications that\ndemand strong reasoning abilities. Despite recent advancements, these models\nstill struggle to solve complex geometric problems. A key challenge stems from\nthe lack of high-quality image-text pair datasets for understanding geometric\nimages. Furthermore, most template-based data synthesis pipelines typically\nfail to generalize to questions beyond their predefined templates. In this\npaper, we bridge this gap by introducing a complementary process of\nReinforcement Learning with Verifiable Rewards (RLVR) into the data generation\npipeline. By adopting RLVR to refine captions for geometric images synthesized\nfrom 50 basic geometric relations and using reward signals derived from\nmathematical problem-solving tasks, our pipeline successfully captures the key\nfeatures of geometry problem-solving. This enables better task generalization\nand yields non-trivial improvements. Furthermore, even in out-of-distribution\nscenarios, the generated dataset enhances the general reasoning capabilities of\nmultimodal large language models, yielding accuracy improvements of\n$2.8\\%\\text{-}4.8\\%$ in statistics, arithmetic, algebraic, and numerical tasks\nwith non-geometric input images of MathVista and MathVerse, along with\n$2.4\\%\\text{-}3.9\\%$ improvements in Art, Design, Tech, and Engineering tasks\nin MMMU.", "AI": {"tldr": "\u63d0\u51faRLVR\u65b9\u6cd5\u6539\u8fdb\u51e0\u4f55\u56fe\u50cf\u6807\u6ce8\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u9a8c\u8bc1\u5956\u52b1\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u51e0\u4f55\u95ee\u9898\u548c\u5176\u4ed6\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89e3\u51b3\u590d\u6742\u51e0\u4f55\u95ee\u9898\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u51e0\u4f55\u56fe\u50cf-\u6587\u672c\u5bf9\u6570\u636e\u96c6\uff0c\u4e14\u4f20\u7edf\u6a21\u677f\u5316\u6570\u636e\u5408\u6210\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u6709\u9650", "method": "\u5f15\u5165\u5f3a\u5316\u5b66\u4e60\u9a8c\u8bc1\u5956\u52b1(RLVR)\u673a\u5236\uff0c\u57fa\u4e8e50\u4e2a\u57fa\u672c\u51e0\u4f55\u5173\u7cfb\u5408\u6210\u51e0\u4f55\u56fe\u50cf\uff0c\u901a\u8fc7\u6570\u5b66\u95ee\u9898\u6c42\u89e3\u4efb\u52a1\u83b7\u5f97\u5956\u52b1\u4fe1\u53f7\u6765\u4f18\u5316\u56fe\u50cf\u6807\u6ce8", "result": "\u5728MathVista\u548cMathVerse\u7684\u975e\u51e0\u4f55\u56fe\u50cf\u4efb\u52a1\u4e0a\u83b7\u5f972.8%-4.8%\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u5728MMMU\u7684\u827a\u672f\u3001\u8bbe\u8ba1\u3001\u6280\u672f\u548c\u5de5\u7a0b\u4efb\u52a1\u4e0a\u83b7\u5f972.4%-3.9%\u7684\u6539\u8fdb", "conclusion": "RLVR\u65b9\u6cd5\u80fd\u6709\u6548\u6355\u6349\u51e0\u4f55\u95ee\u9898\u6c42\u89e3\u7684\u5173\u952e\u7279\u5f81\uff0c\u63d0\u5347\u6a21\u578b\u7684\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\uff0c\u5373\u4f7f\u5728\u5206\u5e03\u5916\u573a\u666f\u4e5f\u80fd\u589e\u5f3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e00\u822c\u63a8\u7406\u80fd\u529b"}}
{"id": "2509.14726", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14726", "abs": "https://arxiv.org/abs/2509.14726", "authors": ["Fangguo Zhao", "Xin Guan", "Shuo Li"], "title": "Rethinking Reference Trajectories in Agile Drone Racing: A Unified Reference-Free Model-Based Controller via MPPI", "comment": null, "summary": "While model-based controllers have demonstrated remarkable performance in\nautonomous drone racing, their performance is often constrained by the reliance\non pre-computed reference trajectories. Conventional approaches, such as\ntrajectory tracking, demand a dynamically feasible, full-state reference,\nwhereas contouring control relaxes this requirement to a geometric path but\nstill necessitates a reference. Recent advancements in reinforcement learning\n(RL) have revealed that many model-based controllers optimize surrogate\nobjectives, such as trajectory tracking, rather than the primary racing goal of\ndirectly maximizing progress through gates. Inspired by these findings, this\nwork introduces a reference-free method for time-optimal racing by\nincorporating this gate progress objective, derived from RL reward shaping,\ndirectly into the Model Predictive Path Integral (MPPI) formulation. The\nsampling-based nature of MPPI makes it uniquely capable of optimizing the\ndiscontinuous and non-differentiable objective in real-time. We also establish\na unified framework that leverages MPPI to systematically and fairly compare\nthree distinct objective functions with a consistent dynamics model and\nparameter set: classical trajectory tracking, contouring control, and the\nproposed gate progress objective. We compare the performance of these three\nobjectives when solved via both MPPI and a traditional gradient-based solver.\nOur results demonstrate that the proposed reference-free approach achieves\ncompetitive racing performance, rivaling or exceeding reference-based methods.\nVideos are available at https://zhaofangguo.github.io/racing_mppi/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u53c2\u8003\u7684\u65e0\u4eba\u673a\u7ade\u901f\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06RL\u5956\u52b1\u5851\u9020\u4e2d\u7684\u95e8\u8fdb\u5ea6\u76ee\u6807\u76f4\u63a5\u6574\u5408\u5230MPPI\u6846\u67b6\u4e2d\uff0c\u5b9e\u73b0\u4e86\u65f6\u95f4\u6700\u4f18\u7ade\u901f\uff0c\u65e0\u9700\u9884\u8ba1\u7b97\u53c2\u8003\u8f68\u8ff9\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6a21\u578b\u7684\u63a7\u5236\u5668\u4f9d\u8d56\u9884\u8ba1\u7b97\u53c2\u8003\u8f68\u8ff9\uff0c\u9650\u5236\u4e86\u6027\u80fd\u3002RL\u7814\u7a76\u8868\u660e\u8bb8\u591a\u63a7\u5236\u5668\u4f18\u5316\u7684\u662f\u66ff\u4ee3\u76ee\u6807\uff08\u5982\u8f68\u8ff9\u8ddf\u8e2a\uff09\u800c\u975e\u76f4\u63a5\u6700\u5927\u5316\u901a\u8fc7\u95e8\u7684\u8fdb\u5ea6\u8fd9\u4e00\u4e3b\u8981\u7ade\u901f\u76ee\u6807\u3002", "method": "\u5c06\u95e8\u8fdb\u5ea6\u76ee\u6807\u6574\u5408\u5230MPPI\u6846\u67b6\u4e2d\uff0c\u5229\u7528MPPI\u7684\u91c7\u6837\u7279\u6027\u5b9e\u65f6\u4f18\u5316\u4e0d\u8fde\u7eed\u3001\u4e0d\u53ef\u5fae\u7684\u76ee\u6807\u51fd\u6570\u3002\u5efa\u7acb\u4e86\u7edf\u4e00\u6846\u67b6\uff0c\u4f7f\u7528\u76f8\u540c\u52a8\u529b\u5b66\u6a21\u578b\u548c\u53c2\u6570\u96c6\u6bd4\u8f83\u4e09\u79cd\u76ee\u6807\u51fd\u6570\u3002", "result": "\u63d0\u51fa\u7684\u65e0\u53c2\u8003\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7ade\u901f\u6027\u80fd\uff0c\u4e0e\u57fa\u4e8e\u53c2\u8003\u7684\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u4f18\u3002MPPI\u5728\u4f18\u5316\u4e0d\u8fde\u7eed\u76ee\u6807\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u68af\u5ea6\u6c42\u89e3\u5668\u3002", "conclusion": "\u53c2\u8003\u81ea\u7531\u65b9\u6cd5\u901a\u8fc7\u76f4\u63a5\u4f18\u5316\u95e8\u8fdb\u5ea6\u76ee\u6807\uff0c\u5728\u65e0\u4eba\u673a\u7ade\u901f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u65f6\u95f4\u6700\u4f18\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0cMPPI\u6846\u67b6\u9002\u5408\u5904\u7406\u590d\u6742\u7684\u4e0d\u8fde\u7eed\u4f18\u5316\u95ee\u9898\u3002"}}
{"id": "2509.14748", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.14748", "abs": "https://arxiv.org/abs/2509.14748", "authors": ["Maria Ibrahim", "Alap Kshirsagar", "Dorothea Koert", "Jan Peters"], "title": "Investigating the Effect of LED Signals and Emotional Displays in Human-Robot Shared Workspaces", "comment": "8 pages", "summary": "Effective communication is essential for safety and efficiency in human-robot\ncollaboration, particularly in shared workspaces. This paper investigates the\nimpact of nonverbal communication on human-robot interaction (HRI) by\nintegrating reactive light signals and emotional displays into a robotic\nsystem. We equipped a Franka Emika Panda robot with an LED strip on its end\neffector and an animated facial display on a tablet to convey movement intent\nthrough colour-coded signals and facial expressions. We conducted a human-robot\ncollaboration experiment with 18 participants, evaluating three conditions: LED\nsignals alone, LED signals with reactive emotional displays, and LED signals\nwith pre-emptive emotional displays. We collected data through questionnaires\nand position tracking to assess anticipation of potential collisions, perceived\nclarity of communication, and task performance. The results indicate that while\nemotional displays increased the perceived interactivity of the robot, they did\nnot significantly improve collision anticipation, communication clarity, or\ntask efficiency compared to LED signals alone. These findings suggest that\nwhile emotional cues can enhance user engagement, their impact on task\nperformance in shared workspaces is limited.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u975e\u8bed\u8a00\u6c9f\u901a\uff08LED\u706f\u5149\u4fe1\u53f7\u548c\u60c5\u611f\u663e\u793a\uff09\u5728\u4eba\u673a\u534f\u4f5c\u4e2d\u5bf9\u78b0\u649e\u9884\u671f\u548c\u4efb\u52a1\u6548\u7387\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u60c5\u611f\u663e\u793a\u867d\u7136\u63d0\u5347\u4e86\u4ea4\u4e92\u611f\u77e5\uff0c\u4f46\u5bf9\u5b9e\u9645\u6027\u80fd\u6539\u5584\u6709\u9650\u3002", "motivation": "\u5728\u5171\u4eab\u5de5\u4f5c\u7a7a\u95f4\u4e2d\uff0c\u6709\u6548\u7684\u4eba\u673a\u6c9f\u901a\u5bf9\u5b89\u5168\u548c\u6548\u7387\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u975e\u8bed\u8a00\u6c9f\u901a\u65b9\u5f0f\uff08\u5982\u706f\u5149\u4fe1\u53f7\u548c\u60c5\u611f\u663e\u793a\uff09\u5982\u4f55\u5f71\u54cd\u4eba\u7c7b\u5bf9\u673a\u5668\u4eba\u8fd0\u52a8\u610f\u56fe\u7684\u7406\u89e3\u548c\u534f\u4f5c\u6548\u679c\u3002", "method": "\u4f7f\u7528Franka Emika Panda\u673a\u5668\u4eba\uff0c\u5728\u672b\u7aef\u6267\u884c\u5668\u5b89\u88c5LED\u706f\u5e26\uff0c\u5e73\u677f\u7535\u8111\u4e0a\u663e\u793a\u52a8\u753b\u9762\u90e8\u8868\u60c5\u3002\u8fdb\u884c18\u4eba\u53c2\u4e0e\u7684\u4eba\u673a\u534f\u4f5c\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e09\u79cd\u6761\u4ef6\uff1a\u4ec5LED\u4fe1\u53f7\u3001LED+\u53cd\u5e94\u5f0f\u60c5\u611f\u663e\u793a\u3001LED+\u9884\u5224\u5f0f\u60c5\u611f\u663e\u793a\u3002\u901a\u8fc7\u95ee\u5377\u548c\u4f4d\u7f6e\u8ffd\u8e2a\u8bc4\u4f30\u78b0\u649e\u9884\u671f\u3001\u6c9f\u901a\u6e05\u6670\u5ea6\u548c\u4efb\u52a1\u8868\u73b0\u3002", "result": "\u60c5\u611f\u663e\u793a\u867d\u7136\u589e\u52a0\u4e86\u673a\u5668\u4eba\u7684\u4ea4\u4e92\u611f\u77e5\u6027\uff0c\u4f46\u4e0e\u4ec5\u4f7f\u7528LED\u4fe1\u53f7\u76f8\u6bd4\uff0c\u5728\u78b0\u649e\u9884\u671f\u3001\u6c9f\u901a\u6e05\u6670\u5ea6\u6216\u4efb\u52a1\u6548\u7387\u65b9\u9762\u6ca1\u6709\u663e\u8457\u6539\u5584\u3002\u60c5\u611f\u7ebf\u7d22\u4e3b\u8981\u589e\u5f3a\u4e86\u7528\u6237\u53c2\u4e0e\u611f\u3002", "conclusion": "\u5728\u5171\u4eab\u5de5\u4f5c\u7a7a\u95f4\u7684\u4efb\u52a1\u6267\u884c\u4e2d\uff0c\u60c5\u611f\u663e\u793a\u867d\u7136\u80fd\u63d0\u5347\u7528\u6237\u53c2\u4e0e\u5ea6\uff0c\u4f46\u5bf9\u5b9e\u9645\u4efb\u52a1\u6027\u80fd\u7684\u6539\u5584\u4f5c\u7528\u6709\u9650\uff0cLED\u706f\u5149\u4fe1\u53f7\u672c\u8eab\u5df2\u80fd\u63d0\u4f9b\u6709\u6548\u7684\u8fd0\u52a8\u610f\u56fe\u6c9f\u901a\u3002"}}
{"id": "2509.14787", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14787", "abs": "https://arxiv.org/abs/2509.14787", "authors": ["Qixuan Li", "Chen Le", "Dongyue Huang", "Jincheng Yu", "Xinlei Chen"], "title": "COMPASS: Confined-space Manipulation Planning with Active Sensing Strategy", "comment": null, "summary": "Manipulation in confined and cluttered environments remains a significant\nchallenge due to partial observability and complex configuration spaces.\nEffective manipulation in such environments requires an intelligent exploration\nstrategy to safely understand the scene and search the target. In this paper,\nwe propose COMPASS, a multi-stage exploration and manipulation framework\nfeaturing a manipulation-aware sampling-based planner. First, we reduce\ncollision risks with a near-field awareness scan to build a local collision\nmap. Additionally, we employ a multi-objective utility function to find\nviewpoints that are both informative and conducive to subsequent manipulation.\nMoreover, we perform a constrained manipulation optimization strategy to\ngenerate manipulation poses that respect obstacle constraints. To\nsystematically evaluate method's performance under these difficulties, we\npropose a benchmark of confined-space exploration and manipulation containing\nfour level challenging scenarios. Compared to exploration methods designed for\nother robots and only considering information gain, our framework increases\nmanipulation success rate by 24.25% in simulations. Real-world experiments\ndemonstrate our method's capability for active sensing and manipulation in\nconfined environments.", "AI": {"tldr": "COMPASS\u662f\u4e00\u4e2a\u591a\u9636\u6bb5\u63a2\u7d22\u548c\u64cd\u4f5c\u6846\u67b6\uff0c\u7528\u4e8e\u53d7\u9650\u6742\u4e71\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u901a\u8fc7\u64cd\u4f5c\u611f\u77e5\u7684\u91c7\u6837\u89c4\u5212\u5668\u548c\u591a\u76ee\u6807\u6548\u7528\u51fd\u6570\u63d0\u9ad8\u64cd\u4f5c\u6210\u529f\u738724.25%\u3002", "motivation": "\u53d7\u9650\u6742\u4e71\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u9762\u4e34\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u548c\u590d\u6742\u914d\u7f6e\u7a7a\u95f4\u7684\u6311\u6218\uff0c\u9700\u8981\u667a\u80fd\u63a2\u7d22\u7b56\u7565\u6765\u5b89\u5168\u7406\u89e3\u573a\u666f\u5e76\u641c\u7d22\u76ee\u6807\u3002", "method": "\u63d0\u51fa\u591a\u9636\u6bb5\u63a2\u7d22\u64cd\u4f5c\u6846\u67b6\uff1a1) \u8fd1\u573a\u611f\u77e5\u626b\u63cf\u6784\u5efa\u5c40\u90e8\u78b0\u649e\u5730\u56fe\uff1b2) \u591a\u76ee\u6807\u6548\u7528\u51fd\u6570\u9009\u62e9\u4fe1\u606f\u4e30\u5bcc\u4e14\u5229\u4e8e\u540e\u7eed\u64cd\u4f5c\u7684\u89c6\u70b9\uff1b3) \u7ea6\u675f\u64cd\u4f5c\u4f18\u5316\u7b56\u7565\u751f\u6210\u6ee1\u8db3\u969c\u788d\u7269\u7ea6\u675f\u7684\u64cd\u4f5c\u59ff\u6001\u3002", "result": "\u5728\u4eff\u771f\u4e2d\u6bd4\u4ec5\u8003\u8651\u4fe1\u606f\u589e\u76ca\u7684\u65b9\u6cd5\u63d0\u9ad8\u64cd\u4f5c\u6210\u529f\u738724.25%\uff0c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8bc1\u660e\u5728\u53d7\u9650\u73af\u5883\u4e2d\u5177\u6709\u4e3b\u52a8\u611f\u77e5\u548c\u64cd\u4f5c\u80fd\u529b\u3002", "conclusion": "COMPASS\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u53d7\u9650\u6742\u4e71\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u6311\u6218\uff0c\u901a\u8fc7\u7cfb\u7edf\u7684\u63a2\u7d22\u548c\u4f18\u5316\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u64cd\u4f5c\u6027\u80fd\u3002"}}
{"id": "2509.14816", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14816", "abs": "https://arxiv.org/abs/2509.14816", "authors": ["Humphrey Munn", "Brendan Tidd", "Peter B\u00f6hm", "Marcus Gallagher", "David Howard"], "title": "Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution", "comment": null, "summary": "Reinforcement Learning (RL) robot controllers usually aggregate many task\nobjectives into one scalar reward. While large-scale proximal policy\noptimisation (PPO) has enabled impressive results such as robust robot\nlocomotion in the real world, many tasks still require careful reward tuning\nand are brittle to local optima. Tuning cost and sub-optimality grow with the\nnumber of objectives, limiting scalability. Modelling reward vectors and their\ntrade-offs can address these issues; however, multi-objective methods remain\nunderused in RL for robotics because of computational cost and optimisation\ndifficulty. In this work, we investigate the conflict between gradient\ncontributions for each objective that emerge from scalarising the task\nobjectives. In particular, we explicitly address the conflict between\ntask-based rewards and terms that regularise the policy towards realistic\nbehaviour. We propose GCR-PPO, a modification to actor-critic optimisation that\ndecomposes the actor update into objective-wise gradients using a multi-headed\ncritic and resolves conflicts based on the objective priority. Our methodology,\nGCR-PPO, is evaluated on the well-known IsaacLab manipulation and locomotion\nbenchmarks and additional multi-objective modifications on two related tasks.\nWe show superior scalability compared to parallel PPO (p = 0.04), without\nsignificant computational overhead. We also show higher performance with more\nconflicting tasks. GCR-PPO improves on large-scale PPO with an average\nimprovement of 9.5%, with high-conflict tasks observing a greater improvement.\nThe code is available at https://github.com/humphreymunn/GCR-PPO.", "AI": {"tldr": "GCR-PPO\u662f\u4e00\u79cd\u6539\u8fdb\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u68af\u5ea6\u5206\u89e3\u548c\u51b2\u7a81\u89e3\u51b3\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfPPO\u5728\u591a\u76ee\u6807\u4efb\u52a1\u4e2d\u68af\u5ea6\u51b2\u7a81\u548c\u6b21\u4f18\u89e3\u7684\u95ee\u9898\uff0c\u5728\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6807\u51c6PPO\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u5668\u5c06\u591a\u4e2a\u4efb\u52a1\u76ee\u6807\u805a\u5408\u4e3a\u5355\u4e00\u6807\u91cf\u5956\u52b1\uff0c\u5bfc\u81f4\u9700\u8981\u7cbe\u7ec6\u7684\u5956\u52b1\u8c03\u4f18\u3001\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\uff0c\u4e14\u968f\u7740\u76ee\u6807\u6570\u91cf\u589e\u52a0\uff0c\u8c03\u4f18\u6210\u672c\u548c\u6b21\u4f18\u6027\u589e\u957f\u3002\u591a\u76ee\u6807\u65b9\u6cd5\u5728\u673a\u5668\u4ebaRL\u4e2d\u5e94\u7528\u4e0d\u8db3\uff0c\u4e3b\u8981\u56e0\u4e3a\u8ba1\u7b97\u6210\u672c\u548c\u4f18\u5316\u96be\u5ea6\u3002", "method": "\u63d0\u51faGCR-PPO\u7b97\u6cd5\uff0c\u4fee\u6539actor-critic\u4f18\u5316\u8fc7\u7a0b\uff1a\u4f7f\u7528\u591a\u5934critic\u5c06actor\u66f4\u65b0\u5206\u89e3\u4e3a\u5404\u76ee\u6807\u7684\u68af\u5ea6\uff0c\u5e76\u6839\u636e\u76ee\u6807\u4f18\u5148\u7ea7\u89e3\u51b3\u68af\u5ea6\u51b2\u7a81\u3002\u5728IsaacLab\u64cd\u4f5c\u548c\u8fd0\u52a8\u57fa\u51c6\u6d4b\u8bd5\u4ee5\u53ca\u591a\u76ee\u6807\u4fee\u6539\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u76f8\u6bd4\u5e76\u884cPPO\u5177\u6709\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027(p=0.04)\uff0c\u65e0\u663e\u8457\u8ba1\u7b97\u5f00\u9500\u3002\u5728\u9ad8\u51b2\u7a81\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u597d\uff0c\u5e73\u5747\u6539\u8fdb9.5%\uff0c\u51b2\u7a81\u8d8a\u5927\u7684\u4efb\u52a1\u6539\u8fdb\u8d8a\u5927\u3002", "conclusion": "GCR-PPO\u901a\u8fc7\u663e\u5f0f\u5904\u7406\u591a\u76ee\u6807\u68af\u5ea6\u51b2\u7a81\uff0c\u5728\u673a\u5668\u4eba\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6bd4\u4f20\u7edfPPO\u66f4\u597d\u7684\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\uff0c\u7279\u522b\u9002\u7528\u4e8e\u76ee\u6807\u51b2\u7a81\u8f83\u5927\u7684\u590d\u6742\u4efb\u52a1\u3002"}}
{"id": "2509.14889", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14889", "abs": "https://arxiv.org/abs/2509.14889", "authors": ["Nan Sun", "Yongchang Li", "Chenxu Wang", "Huiying Li", "Huaping Liu"], "title": "CollabVLA: Self-Reflective Vision-Language-Action Model Dreaming Together with Human", "comment": "8 pages, 5 figures, 3 tables", "summary": "In this work, we present CollabVLA, a self-reflective vision-language-action\nframework that transforms a standard visuomotor policy into a collaborative\nassistant. CollabVLA tackles key limitations of prior VLAs, including domain\noverfitting, non-interpretable reasoning, and the high latency of auxiliary\ngenerative models, by integrating VLM-based reflective reasoning with\ndiffusion-based action generation under a mixture-of-experts design. Through a\ntwo-stage training recipe of action grounding and reflection tuning, it\nsupports explicit self-reflection and proactively solicits human guidance when\nconfronted with uncertainty or repeated failure. It cuts normalized Time by ~2x\nand Dream counts by ~4x vs. generative agents, achieving higher success rates,\nimproved interpretability, and balanced low latency compared with existing\nmethods. This work takes a pioneering step toward shifting VLAs from opaque\ncontrollers to genuinely assistive agents capable of reasoning, acting, and\ncollaborating with humans.", "AI": {"tldr": "CollabVLA\u662f\u4e00\u4e2a\u81ea\u53cd\u601d\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u8bbe\u8ba1\u5c06\u6807\u51c6\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u8f6c\u53d8\u4e3a\u534f\u4f5c\u52a9\u624b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709VLA\u65b9\u6cd5\u7684\u9886\u57df\u8fc7\u62df\u5408\u3001\u4e0d\u53ef\u89e3\u91ca\u63a8\u7406\u548c\u9ad8\u5ef6\u8fdf\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c(VLA)\u7cfb\u7edf\u7684\u5173\u952e\u9650\u5236\uff0c\u5305\u62ec\u9886\u57df\u8fc7\u62df\u5408\u3001\u4e0d\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8fc7\u7a0b\u4ee5\u53ca\u8f85\u52a9\u751f\u6210\u6a21\u578b\u7684\u9ad8\u5ef6\u8fdf\u95ee\u9898\uff0c\u4f7fVLA\u4ece\u9ed1\u76d2\u63a7\u5236\u5668\u8f6c\u53d8\u4e3a\u771f\u6b63\u80fd\u591f\u4e0e\u4eba\u7c7b\u534f\u4f5c\u7684\u667a\u80fd\u52a9\u624b\u3002", "method": "\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\u8bbe\u8ba1\uff0c\u6574\u5408\u57fa\u4e8eVLM\u7684\u53cd\u601d\u63a8\u7406\u548c\u57fa\u4e8e\u6269\u6563\u7684\u52a8\u4f5c\u751f\u6210\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff08\u52a8\u4f5c\u63a5\u5730\u548c\u53cd\u601d\u8c03\u4f18\uff09\u5b9e\u73b0\u663e\u5f0f\u81ea\u6211\u53cd\u601d\uff0c\u5e76\u5728\u9047\u5230\u4e0d\u786e\u5b9a\u6027\u6216\u91cd\u590d\u5931\u8d25\u65f6\u4e3b\u52a8\u5bfb\u6c42\u4eba\u7c7b\u6307\u5bfc\u3002", "result": "\u4e0e\u751f\u6210\u5f0f\u667a\u80fd\u4f53\u76f8\u6bd4\uff0c\u6807\u51c6\u5316\u65f6\u95f4\u51cf\u5c11\u7ea62\u500d\uff0cDream\u8ba1\u6570\u51cf\u5c11\u7ea64\u500d\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6210\u529f\u7387\u3001\u6539\u8fdb\u7684\u53ef\u89e3\u91ca\u6027\u4ee5\u53ca\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u7684\u5e73\u8861\u4f4e\u5ef6\u8fdf\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5728\u5c06VLA\u4ece\u9ed1\u76d2\u63a7\u5236\u5668\u8f6c\u53d8\u4e3a\u771f\u6b63\u80fd\u591f\u63a8\u7406\u3001\u884c\u52a8\u5e76\u4e0e\u4eba\u7c7b\u534f\u4f5c\u7684\u8f85\u52a9\u667a\u80fd\u4f53\u65b9\u9762\u8fc8\u51fa\u4e86\u5f00\u521b\u6027\u7684\u4e00\u6b65\u3002"}}
{"id": "2509.14980", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14980", "abs": "https://arxiv.org/abs/2509.14980", "authors": ["Ju Dong", "Lei Zhang", "Liding Zhang", "Yao Ling", "Yu Fu", "Kaixin Bai", "Zolt\u00e1n-Csaba M\u00e1rton", "Zhenshan Bing", "Zhaopeng Chen", "Alois Christian Knoll", "Jianwei Zhang"], "title": "M4Diffuser: Multi-View Diffusion Policy with Manipulability-Aware Control for Robust Mobile Manipulation", "comment": "Project page: https://sites.google.com/view/m4diffuser, 10 pages, 9\n  figures", "summary": "Mobile manipulation requires the coordinated control of a mobile base and a\nrobotic arm while simultaneously perceiving both global scene context and\nfine-grained object details. Existing single-view approaches often fail in\nunstructured environments due to limited fields of view, exploration, and\ngeneralization abilities. Moreover, classical controllers, although stable,\nstruggle with efficiency and manipulability near singularities. To address\nthese challenges, we propose M4Diffuser, a hybrid framework that integrates a\nMulti-View Diffusion Policy with a novel Reduced and Manipulability-aware QP\n(ReM-QP) controller for mobile manipulation. The diffusion policy leverages\nproprioceptive states and complementary camera perspectives with both\nclose-range object details and global scene context to generate task-relevant\nend-effector goals in the world frame. These high-level goals are then executed\nby the ReM-QP controller, which eliminates slack variables for computational\nefficiency and incorporates manipulability-aware preferences for robustness\nnear singularities. Comprehensive experiments in simulation and real-world\nenvironments show that M4Diffuser achieves 7 to 56 percent higher success rates\nand reduces collisions by 3 to 31 percent over baselines. Our approach\ndemonstrates robust performance for smooth whole-body coordination, and strong\ngeneralization to unseen tasks, paving the way for reliable mobile manipulation\nin unstructured environments. Details of the demo and supplemental material are\navailable on our project website https://sites.google.com/view/m4diffuser.", "AI": {"tldr": "M4Diffuser\u662f\u4e00\u4e2a\u7528\u4e8e\u79fb\u52a8\u64cd\u4f5c\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u89c6\u56fe\u6269\u6563\u7b56\u7565\u548c\u65b0\u578b\u53ef\u64cd\u4f5c\u611f\u77e5QP\u63a7\u5236\u5668\uff0c\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u548c\u5b89\u5168\u6027", "motivation": "\u89e3\u51b3\u73b0\u6709\u5355\u89c6\u89d2\u65b9\u6cd5\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u89c6\u91ce\u6709\u9650\u3001\u63a2\u7d22\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u4f20\u7edf\u63a7\u5236\u5668\u5728\u5947\u70b9\u9644\u8fd1\u6548\u7387\u4f4e\u4e0b\u7684\u6311\u6218", "method": "\u63d0\u51fa\u6df7\u5408\u6846\u67b6\uff1a\u591a\u89c6\u56fe\u6269\u6563\u7b56\u7565\u751f\u6210\u4efb\u52a1\u76f8\u5173\u7684\u672b\u7aef\u6267\u884c\u5668\u76ee\u6807\uff0cReM-QP\u63a7\u5236\u5668\u6267\u884c\u76ee\u6807\uff08\u6d88\u9664\u677e\u5f1b\u53d8\u91cf\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\uff0c\u52a0\u5165\u53ef\u64cd\u4f5c\u611f\u77e5\u504f\u597d\u589e\u5f3a\u5947\u70b9\u9644\u8fd1\u7684\u9c81\u68d2\u6027\uff09", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u5b9e\u9a8c\u4e2d\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6210\u529f\u7387\u63d0\u9ad87-56%\uff0c\u78b0\u649e\u51cf\u5c113-31%\uff0c\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5e73\u6ed1\u7684\u5168\u8eab\u534f\u8c03\u6027\u80fd", "conclusion": "M4Diffuser\u4e3a\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u9760\u7684\u79fb\u52a8\u64cd\u4f5c\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u5c55\u793a\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b"}}
{"id": "2509.14915", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14915", "abs": "https://arxiv.org/abs/2509.14915", "authors": ["Shenghai Yuan", "Jason Wai Hao Yee", "Weixiang Guo", "Zhongyuan Liu", "Thien-Minh Nguyen", "Lihua Xie"], "title": "PERAL: Perception-Aware Motion Control for Passive LiDAR Excitation in Spherical Robots", "comment": null, "summary": "Autonomous mobile robots increasingly rely on LiDAR-IMU odometry for\nnavigation and mapping, yet horizontally mounted LiDARs such as the MID360\ncapture few near-ground returns, limiting terrain awareness and degrading\nperformance in feature-scarce environments. Prior solutions - static tilt,\nactive rotation, or high-density sensors - either sacrifice horizontal\nperception or incur added actuators, cost, and power. We introduce PERAL, a\nperception-aware motion control framework for spherical robots that achieves\npassive LiDAR excitation without dedicated hardware. By modeling the coupling\nbetween internal differential-drive actuation and sensor attitude, PERAL\nsuperimposes bounded, non-periodic oscillations onto nominal goal- or\ntrajectory-tracking commands, enriching vertical scan diversity while\npreserving navigation accuracy. Implemented on a compact spherical robot, PERAL\nis validated across laboratory, corridor, and tactical environments.\nExperiments demonstrate up to 96 percent map completeness, a 27 percent\nreduction in trajectory tracking error, and robust near-ground human detection,\nall at lower weight, power, and cost compared with static tilt, active\nrotation, and fixed horizontal baselines. The design and code will be\nopen-sourced upon acceptance.", "AI": {"tldr": "PERAL\u662f\u4e00\u79cd\u9762\u5411\u7403\u5f62\u673a\u5668\u4eba\u7684\u611f\u77e5\u611f\u77e5\u8fd0\u52a8\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u5185\u90e8\u5dee\u901f\u9a71\u52a8\u5b9e\u73b0\u88ab\u52a8LiDAR\u6fc0\u52b1\uff0c\u65e0\u9700\u989d\u5916\u786c\u4ef6\u5373\u53ef\u589e\u5f3a\u5782\u76f4\u626b\u63cf\u591a\u6837\u6027\uff0c\u63d0\u9ad8\u5730\u56fe\u5b8c\u6574\u6027\u548c\u5bfc\u822a\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u6c34\u5e73\u5b89\u88c5LiDAR\uff08\u5982MID360\uff09\u5728\u7279\u5f81\u7a00\u7f3a\u73af\u5883\u4e2d\u8fd1\u5730\u9762\u70b9\u4e91\u6355\u83b7\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u907f\u514d\u4f20\u7edf\u65b9\u6848\uff08\u9759\u6001\u503e\u659c\u3001\u4e3b\u52a8\u65cb\u8f6c\u6216\u9ad8\u5bc6\u5ea6\u4f20\u611f\u5668\uff09\u5e26\u6765\u7684\u6c34\u5e73\u611f\u77e5\u635f\u5931\u3001\u989d\u5916\u6267\u884c\u5668\u3001\u6210\u672c\u548c\u529f\u8017\u3002", "method": "\u901a\u8fc7\u5efa\u6a21\u5185\u90e8\u5dee\u901f\u9a71\u52a8\u4e0e\u4f20\u611f\u5668\u59ff\u6001\u7684\u8026\u5408\u5173\u7cfb\uff0c\u5728\u540d\u4e49\u76ee\u6807\u6216\u8f68\u8ff9\u8ddf\u8e2a\u547d\u4ee4\u4e0a\u53e0\u52a0\u6709\u754c\u975e\u5468\u671f\u6027\u632f\u8361\uff0c\u5b9e\u73b0\u88ab\u52a8LiDAR\u6fc0\u52b1\u3002", "result": "\u5728\u5b9e\u9a8c\u5ba4\u3001\u8d70\u5eca\u548c\u6218\u672f\u73af\u5883\u4e2d\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u9ad8\u8fbe96%\u7684\u5730\u56fe\u5b8c\u6574\u6027\uff0c\u8f68\u8ff9\u8ddf\u8e2a\u8bef\u5dee\u51cf\u5c1127%\uff0c\u9c81\u68d2\u7684\u8fd1\u5730\u9762\u4eba\u4f53\u68c0\u6d4b\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6848\u91cd\u91cf\u3001\u529f\u8017\u548c\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "PERAL\u6846\u67b6\u65e0\u9700\u4e13\u7528\u786c\u4ef6\u5373\u53ef\u6709\u6548\u63d0\u5347LiDAR-IMU\u91cc\u7a0b\u8ba1\u6027\u80fd\uff0c\u4e3a\u7403\u5f62\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u8f7b\u91cf\u3001\u4f4e\u6210\u672c\u7684\u611f\u77e5\u589e\u5f3a\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2509.14932", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14932", "abs": "https://arxiv.org/abs/2509.14932", "authors": ["Tobias J\u00fclg", "Pierre Krack", "Seongjin Bien", "Yannik Blei", "Khaled Gamal", "Ken Nakahara", "Johannes Hechtl", "Roberto Calandra", "Wolfram Burgard", "Florian Walter"], "title": "Robot Control Stack: A Lean Ecosystem for Robot Learning at Scale", "comment": null, "summary": "Vision-Language-Action models (VLAs) mark a major shift in robot learning.\nThey replace specialized architectures and task-tailored components of expert\npolicies with large-scale data collection and setup-specific fine-tuning. In\nthis machine learning-focused workflow that is centered around models and\nscalable training, traditional robotics software frameworks become a\nbottleneck, while robot simulations offer only limited support for\ntransitioning from and to real-world experiments. In this work, we close this\ngap by introducing Robot Control Stack (RCS), a lean ecosystem designed from\nthe ground up to support research in robot learning with large-scale generalist\npolicies. At its core, RCS features a modular and easily extensible layered\narchitecture with a unified interface for simulated and physical robots,\nfacilitating sim-to-real transfer. Despite its minimal footprint and\ndependencies, it offers a complete feature set, enabling both real-world\nexperiments and large-scale training in simulation. Our contribution is\ntwofold: First, we introduce the architecture of RCS and explain its design\nprinciples. Second, we evaluate its usability and performance along the\ndevelopment cycle of VLA and RL policies. Our experiments also provide an\nextensive evaluation of Octo, OpenVLA, and Pi Zero on multiple robots and shed\nlight on how simulation data can improve real-world policy performance. Our\ncode, datasets, weights, and videos are available at:\nhttps://robotcontrolstack.github.io/", "AI": {"tldr": "\u63d0\u51fa\u4e86Robot Control Stack (RCS)\u6846\u67b6\uff0c\u89e3\u51b3\u4f20\u7edf\u673a\u5668\u4eba\u8f6f\u4ef6\u5728VLA\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u74f6\u9888\u95ee\u9898\uff0c\u63d0\u4f9b\u7edf\u4e00\u7684\u4eff\u771f\u548c\u7269\u7406\u673a\u5668\u4eba\u63a5\u53e3\uff0c\u652f\u6301\u5927\u89c4\u6a21\u901a\u7528\u7b56\u7565\u7814\u7a76", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u8f6f\u4ef6\u6846\u67b6\u5728\u5927\u89c4\u6a21\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u8bad\u7ec3\u4e2d\u6210\u4e3a\u74f6\u9888\uff0c\u4eff\u771f\u73af\u5883\u5bf9\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u7684\u8f6c\u6362\u652f\u6301\u6709\u9650\uff0c\u9700\u8981\u4e13\u95e8\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u7814\u7a76\u8bbe\u8ba1\u7684\u8f7b\u91cf\u7ea7\u751f\u6001\u7cfb\u7edf", "method": "\u8bbe\u8ba1\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u7684\u5206\u5c42\u67b6\u6784\uff0c\u63d0\u4f9b\u7edf\u4e00\u7684\u4eff\u771f\u548c\u7269\u7406\u673a\u5668\u4eba\u63a5\u53e3\uff0c\u652f\u6301sim-to-real\u8f6c\u6362\uff0c\u5177\u6709\u6700\u5c0f\u4f9d\u8d56\u4f46\u5b8c\u6574\u7684\u529f\u80fd\u96c6", "result": "\u6210\u529f\u5f00\u53d1\u4e86RCS\u6846\u67b6\uff0c\u8bc4\u4f30\u4e86\u5176\u5728VLA\u548cRL\u7b56\u7565\u5f00\u53d1\u5468\u671f\u4e2d\u7684\u53ef\u7528\u6027\u548c\u6027\u80fd\uff0c\u5e76\u5bf9Octo\u3001OpenVLA\u548cPi Zero\u7b49\u6a21\u578b\u5728\u591a\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30", "conclusion": "RCS\u586b\u8865\u4e86\u673a\u5668\u4eba\u5b66\u4e60\u7814\u7a76\u4e2d\u7684\u5de5\u5177\u7a7a\u767d\uff0c\u8bc1\u660e\u4e86\u4eff\u771f\u6570\u636e\u53ef\u4ee5\u63d0\u5347\u771f\u5b9e\u4e16\u754c\u7b56\u7565\u6027\u80fd\uff0c\u4e3a\u5927\u89c4\u6a21\u901a\u7528\u7b56\u7565\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8f6f\u4ef6\u57fa\u7840\u8bbe\u65bd"}}
{"id": "2509.14935", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14935", "abs": "https://arxiv.org/abs/2509.14935", "authors": ["Punith Reddy Vanteddu", "Davide Gorbani", "Giuseppe L'Erario", "Hosameldin Awadalla Omer Mohamed", "Fabio Bergonti", "Daniele Pucci"], "title": "CAD-Driven Co-Design for Flight-Ready Jet-Powered Humanoids", "comment": null, "summary": "This paper presents a CAD-driven co-design framework for optimizing\njet-powered aerial humanoid robots to execute dynamically constrained\ntrajectories. Starting from the iRonCub-Mk3 model, a Design of Experiments\n(DoE) approach is used to generate 5,000 geometrically varied and mechanically\nfeasible designs by modifying limb dimensions, jet interface geometry (e.g.,\nangle and offset), and overall mass distribution. Each model is constructed\nthrough CAD assemblies to ensure structural validity and compatibility with\nsimulation tools. To reduce computational cost and enable parameter sensitivity\nanalysis, the models are clustered using K-means, with representative centroids\nselected for evaluation. A minimum-jerk trajectory is used to assess flight\nperformance, providing position and velocity references for a momentum-based\nlinearized Model Predictive Control (MPC) strategy. A multi-objective\noptimization is then conducted using the NSGA-II algorithm, jointly exploring\nthe space of design centroids and MPC gain parameters. The objectives are to\nminimize trajectory tracking error and mechanical energy expenditure. The\nframework outputs a set of flight-ready humanoid configurations with validated\ncontrol parameters, offering a structured method for selecting and implementing\nfeasible aerial humanoid designs.", "AI": {"tldr": "CAD\u9a71\u52a8\u7684\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u55b7\u6c14\u52a8\u529b\u7a7a\u4e2d\u4eba\u5f62\u673a\u5668\u4eba\u6267\u884c\u52a8\u6001\u7ea6\u675f\u8f68\u8ff9\uff0c\u901a\u8fc7\u8bbe\u8ba1\u5b9e\u9a8c\u751f\u62105000\u79cd\u51e0\u4f55\u53d8\u4f53\uff0c\u805a\u7c7b\u540e\u4f7f\u7528NSGA-II\u7b97\u6cd5\u8fdb\u884c\u591a\u76ee\u6807\u4f18\u5316\uff0c\u8f93\u51fa\u98de\u884c\u5c31\u7eea\u7684\u4eba\u5f62\u914d\u7f6e\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u8bbe\u8ba1\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u55b7\u6c14\u52a8\u529b\u7a7a\u4e2d\u4eba\u5f62\u673a\u5668\u4eba\u7684\u51e0\u4f55\u7ed3\u6784\u548c\u63a7\u5236\u53c2\u6570\uff0c\u4f7f\u5176\u80fd\u591f\u6709\u6548\u6267\u884c\u52a8\u6001\u7ea6\u675f\u7684\u98de\u884c\u8f68\u8ff9\uff0c\u540c\u65f6\u786e\u4fdd\u7ed3\u6784\u6709\u6548\u6027\u548c\u673a\u68b0\u53ef\u884c\u6027\u3002", "method": "\u4f7f\u7528\u8bbe\u8ba1\u5b9e\u9a8c(DoE)\u751f\u62105000\u79cd\u51e0\u4f55\u53d8\u4f53\u8bbe\u8ba1\uff0c\u901a\u8fc7CAD\u88c5\u914d\u786e\u4fdd\u7ed3\u6784\u6709\u6548\u6027\uff1bK-means\u805a\u7c7b\u9009\u62e9\u4ee3\u8868\u6027\u8bbe\u8ba1\uff1b\u6700\u5c0f\u52a0\u52a0\u901f\u5ea6\u8f68\u8ff9\u8bc4\u4f30\u98de\u884c\u6027\u80fd\uff1b\u57fa\u4e8e\u52a8\u91cf\u7684\u7ebf\u6027\u5316MPC\u63a7\u5236\u7b56\u7565\uff1bNSGA-II\u7b97\u6cd5\u8fdb\u884c\u591a\u76ee\u6807\u4f18\u5316\u3002", "result": "\u6846\u67b6\u8f93\u51fa\u4e00\u7ec4\u98de\u884c\u5c31\u7eea\u7684\u4eba\u5f62\u673a\u5668\u4eba\u914d\u7f6e\uff0c\u5305\u542b\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u63a7\u5236\u53c2\u6570\uff0c\u63d0\u4f9b\u4e86\u9009\u62e9\u548c\u5b9e\u65bd\u53ef\u884c\u7a7a\u4e2d\u4eba\u5f62\u8bbe\u8ba1\u7684\u7ed3\u6784\u5316\u65b9\u6cd5\u3002", "conclusion": "\u8be5CAD\u9a71\u52a8\u7684\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u55b7\u6c14\u52a8\u529b\u7a7a\u4e2d\u4eba\u5f62\u673a\u5668\u4eba\u7684\u51e0\u4f55\u7ed3\u6784\u548c\u63a7\u5236\u53c2\u6570\u7684\u8054\u5408\u4f18\u5316\uff0c\u4e3a\u52a8\u6001\u7ea6\u675f\u8f68\u8ff9\u6267\u884c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bbe\u8ba1\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.14939", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14939", "abs": "https://arxiv.org/abs/2509.14939", "authors": ["Hao Zhang", "Zhen Kan", "Weiwei Shang", "Yongduan Song"], "title": "A Novel Task-Driven Diffusion-Based Policy with Affordance Learning for Generalizable Manipulation of Articulated Objects", "comment": "Accepted by IEEE/ASME Transactions on Mechatronics", "summary": "Despite recent advances in dexterous manipulations, the manipulation of\narticulated objects and generalization across different categories remain\nsignificant challenges. To address these issues, we introduce DART, a novel\nframework that enhances a diffusion-based policy with affordance learning and\nlinear temporal logic (LTL) representations to improve the learning efficiency\nand generalizability of articulated dexterous manipulation. Specifically, DART\nleverages LTL to understand task semantics and affordance learning to identify\noptimal interaction points. The {diffusion-based policy} then generalizes these\ninteractions across various categories. Additionally, we exploit an\noptimization method based on interaction data to refine actions, overcoming the\nlimitations of traditional diffusion policies that typically rely on offline\nreinforcement learning or learning from demonstrations. Experimental results\ndemonstrate that DART outperforms most existing methods in manipulation\nability, generalization performance, transfer reasoning, and robustness. For\nmore information, visit our project website at:\nhttps://sites.google.com/view/dart0257/.", "AI": {"tldr": "DART\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u6269\u6563\u7b56\u7565\u3001affordance\u5b66\u4e60\u548c\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91(LTL)\u8868\u793a\uff0c\u63d0\u9ad8\u4e86\u5173\u8282\u5316\u7075\u5de7\u64cd\u4f5c\u7684\u5b66\u4e60\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u5173\u8282\u5316\u7269\u4f53\u64cd\u4f5c\u548c\u8de8\u7c7b\u522b\u6cdb\u5316\u7684\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u8fd9\u4e9b\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u5229\u7528LTL\u7406\u89e3\u4efb\u52a1\u8bed\u4e49\uff0c\u901a\u8fc7affordance\u5b66\u4e60\u8bc6\u522b\u6700\u4f73\u4ea4\u4e92\u70b9\uff0c\u6269\u6563\u7b56\u7565\u6cdb\u5316\u8fd9\u4e9b\u4ea4\u4e92\u5230\u4e0d\u540c\u7c7b\u522b\uff0c\u5e76\u57fa\u4e8e\u4ea4\u4e92\u6570\u636e\u4f18\u5316\u52a8\u4f5c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eDART\u5728\u64cd\u4f5c\u80fd\u529b\u3001\u6cdb\u5316\u6027\u80fd\u3001\u8fc1\u79fb\u63a8\u7406\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u5927\u591a\u6570\u65b9\u6cd5\u3002", "conclusion": "DART\u6846\u67b6\u901a\u8fc7\u6574\u5408\u8bed\u4e49\u7406\u89e3\u3001affordance\u5b66\u4e60\u548c\u6269\u6563\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5173\u8282\u5316\u7075\u5de7\u64cd\u4f5c\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.14941", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14941", "abs": "https://arxiv.org/abs/2509.14941", "authors": ["Zongyuan Shen", "Burhanuddin Shirose", "Prasanna Sriganesh", "Bhaskar Vundurthy", "Howie Choset", "Matthew Travers"], "title": "Multi-CAP: A Multi-Robot Connectivity-Aware Hierarchical Coverage Path Planning Algorithm for Unknown Environments", "comment": null, "summary": "Efficient coordination of multiple robots for coverage of large, unknown\nenvironments is a significant challenge that involves minimizing the total\ncoverage path length while reducing inter-robot conflicts. In this paper, we\nintroduce a Multi-robot Connectivity-Aware Planner (Multi-CAP), a hierarchical\ncoverage path planning algorithm that facilitates multi-robot coordination\nthrough a novel connectivity-aware approach. The algorithm constructs and\ndynamically maintains an adjacency graph that represents the environment as a\nset of connected subareas. Critically, we make the assumption that the\nenvironment, while unknown, is bounded. This allows for incremental refinement\nof the adjacency graph online to ensure its structure represents the physical\nlayout of the space, both in observed and unobserved areas of the map as robots\nexplore the environment. We frame the task of assigning subareas to robots as a\nVehicle Routing Problem (VRP), a well-studied problem for finding optimal\nroutes for a fleet of vehicles. This is used to compute disjoint tours that\nminimize redundant travel, assigning each robot a unique, non-conflicting set\nof subareas. Each robot then executes its assigned tour, independently adapting\nits coverage strategy within each subarea to minimize path length based on\nreal-time sensor observations of the subarea. We demonstrate through\nsimulations and multi-robot hardware experiments that Multi-CAP significantly\noutperforms state-of-the-art methods in key metrics, including coverage time,\ntotal path length, and path overlap ratio. Ablation studies further validate\nthe critical role of our connectivity-aware graph and the global tour planner\nin achieving these performance gains.", "AI": {"tldr": "\u63d0\u51faMulti-CAP\u5206\u5c42\u8986\u76d6\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\uff0c\u901a\u8fc7\u8fde\u63a5\u611f\u77e5\u65b9\u6cd5\u534f\u8c03\u591a\u673a\u5668\u4eba\u8986\u76d6\u672a\u77e5\u73af\u5883\uff0c\u663e\u8457\u51cf\u5c11\u8def\u5f84\u957f\u5ea6\u548c\u51b2\u7a81", "motivation": "\u591a\u673a\u5668\u4eba\u5728\u672a\u77e5\u5927\u73af\u5883\u4e2d\u534f\u8c03\u8986\u76d6\u5b58\u5728\u6311\u6218\uff0c\u9700\u8981\u6700\u5c0f\u5316\u603b\u8def\u5f84\u957f\u5ea6\u540c\u65f6\u51cf\u5c11\u673a\u5668\u4eba\u95f4\u51b2\u7a81", "method": "\u6784\u5efa\u52a8\u6001\u7ef4\u62a4\u90bb\u63a5\u56fe\u8868\u793a\u8fde\u63a5\u5b50\u533a\u57df\uff0c\u5c06\u5b50\u533a\u57df\u5206\u914d\u5efa\u6a21\u4e3a\u8f66\u8f86\u8def\u5f84\u95ee\u9898(VRP)\uff0c\u8ba1\u7b97\u4e0d\u76f8\u4ea4\u8def\u5f84\uff0c\u5404\u673a\u5668\u4eba\u5728\u5b50\u533a\u57df\u5185\u72ec\u7acb\u81ea\u9002\u5e94\u8986\u76d6", "result": "\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u8868\u660eMulti-CAP\u5728\u8986\u76d6\u65f6\u95f4\u3001\u603b\u8def\u5f84\u957f\u5ea6\u548c\u8def\u5f84\u91cd\u53e0\u7387\u7b49\u5173\u952e\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "conclusion": "\u8fde\u63a5\u611f\u77e5\u56fe\u548c\u5168\u5c40\u8def\u5f84\u89c4\u5212\u5668\u5bf9\u6027\u80fd\u63d0\u5347\u81f3\u5173\u91cd\u8981\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u673a\u5668\u4eba\u534f\u8c03\u8986\u76d6\u95ee\u9898"}}
{"id": "2509.14949", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.14949", "abs": "https://arxiv.org/abs/2509.14949", "authors": ["Laura Ribeiro", "Muhammad Shaheer", "Miguel Fernandez-Cortizas", "Ali Tourani", "Holger Voos", "Jose Luis Sanchez-Lopez"], "title": "Human Interaction for Collaborative Semantic SLAM using Extended Reality", "comment": "7 pages, 5 figures, 3 tables", "summary": "Semantic SLAM (Simultaneous Localization and Mapping) systems enrich robot\nmaps with structural and semantic information, enabling robots to operate more\neffectively in complex environments. However, these systems struggle in\nreal-world scenarios with occlusions, incomplete data, or ambiguous geometries,\nas they cannot fully leverage the higher-level spatial and semantic knowledge\nhumans naturally apply. We introduce HICS-SLAM, a Human-in-the-Loop semantic\nSLAM framework that uses a shared extended reality environment for real-time\ncollaboration. The system allows human operators to directly interact with and\nvisualize the robot's 3D scene graph, and add high-level semantic concepts\n(e.g., rooms or structural entities) into the mapping process. We propose a\ngraph-based semantic fusion methodology that integrates these human\ninterventions with robot perception, enabling scalable collaboration for\nenhanced situational awareness. Experimental evaluations on real-world\nconstruction site datasets demonstrate improvements in room detection accuracy,\nmap precision, and semantic completeness compared to automated baselines,\ndemonstrating both the effectiveness of the approach and its potential for\nfuture extensions.", "AI": {"tldr": "HICS-SLAM\u662f\u4e00\u4e2a\u4eba\u5728\u73af\u8def\u7684\u8bed\u4e49SLAM\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u5c55\u73b0\u5b9e\u73af\u5883\u5b9e\u73b0\u4eba\u673a\u5b9e\u65f6\u534f\u4f5c\uff0c\u63d0\u5347\u590d\u6742\u73af\u5883\u4e0b\u7684\u8bed\u4e49\u5efa\u56fe\u7cbe\u5ea6\u548c\u5b8c\u6574\u6027", "motivation": "\u4f20\u7edf\u8bed\u4e49SLAM\u7cfb\u7edf\u5728\u906e\u6321\u3001\u6570\u636e\u4e0d\u5b8c\u6574\u6216\u51e0\u4f55\u6a21\u7cca\u7684\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u4eba\u7c7b\u7684\u9ad8\u5c42\u6b21\u7a7a\u95f4\u548c\u8bed\u4e49\u77e5\u8bc6", "method": "\u63d0\u51fa\u57fa\u4e8e\u5171\u4eab\u6269\u5c55\u73b0\u5b9e\u73af\u5883\u7684\u4eba\u673a\u534f\u4f5c\u6846\u67b6\uff0c\u5141\u8bb8\u64cd\u4f5c\u4eba\u5458\u76f4\u63a5\u4e0e\u673a\u5668\u4eba3D\u573a\u666f\u56fe\u4ea4\u4e92\u5e76\u6dfb\u52a0\u9ad8\u5c42\u6b21\u8bed\u4e49\u6982\u5ff5\uff0c\u91c7\u7528\u57fa\u4e8e\u56fe\u7684\u8bed\u4e49\u878d\u5408\u65b9\u6cd5\u6574\u5408\u4eba\u7c7b\u5e72\u9884\u4e0e\u673a\u5668\u4eba\u611f\u77e5", "result": "\u5728\u771f\u5b9e\u5efa\u7b51\u5de5\u5730\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0c\u76f8\u6bd4\u81ea\u52a8\u5316\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u623f\u95f4\u68c0\u6d4b\u7cbe\u5ea6\u3001\u5730\u56fe\u7cbe\u5ea6\u548c\u8bed\u4e49\u5b8c\u6574\u6027\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u4eba\u5728\u73af\u8def\u534f\u4f5c\u7684\u6709\u6548\u6027\uff0c\u4e3a\u672a\u6765\u6269\u5c55\u63d0\u4f9b\u4e86\u6f5c\u529b\uff0c\u80fd\u591f\u663e\u8457\u589e\u5f3a\u590d\u6742\u73af\u5883\u4e0b\u7684\u8bed\u4e49\u5efa\u56fe\u80fd\u529b"}}
{"id": "2509.14954", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14954", "abs": "https://arxiv.org/abs/2509.14954", "authors": ["Xingchen Xu", "Ao Li", "Benjamin Ward-Cherrier"], "title": "Exploratory Movement Strategies for Texture Discrimination with a Neuromorphic Tactile Sensor", "comment": "Accepted at IEEE/RSJ International Conference on Intelligent Robots\n  and Systems 2025. Please cite the proceedings version", "summary": "We propose a neuromorphic tactile sensing framework for robotic texture\nclassification that is inspired by human exploratory strategies. Our system\nutilizes the NeuroTac sensor to capture neuromorphic tactile data during a\nseries of exploratory motions. We first tested six distinct motions for texture\nclassification under fixed environment: sliding, rotating, tapping, as well as\nthe combined motions: sliding+rotating, tapping+rotating, and tapping+sliding.\nWe chose sliding and sliding+rotating as the best motions based on final\naccuracy and the sample timing length needed to reach converged accuracy. In\nthe second experiment designed to simulate complex real-world conditions, these\ntwo motions were further evaluated under varying contact depth and speeds.\nUnder these conditions, our framework attained the highest accuracy of 87.33\\%\nwith sliding+rotating while maintaining an extremely low power consumption of\nonly 8.04 mW. These results suggest that the sliding+rotating motion is the\noptimal exploratory strategy for neuromorphic tactile sensing deployment in\ntexture classification tasks and holds significant promise for enhancing\nrobotic environmental interaction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u4eba\u7c7b\u63a2\u7d22\u7b56\u7565\u542f\u53d1\u7684\u795e\u7ecf\u5f62\u6001\u89e6\u89c9\u611f\u77e5\u6846\u67b6\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u7eb9\u7406\u5206\u7c7b\uff0c\u901a\u8fc7NeuroTac\u4f20\u611f\u5668\u91c7\u96c6\u6570\u636e\uff0c\u6d4b\u8bd5\u4e86\u516d\u79cd\u63a2\u7d22\u52a8\u4f5c\uff0c\u6700\u7ec8\u786e\u5b9a\u6ed1\u52a8+\u65cb\u8f6c\u4e3a\u6700\u4f18\u7b56\u7565\uff0c\u5728\u590d\u6742\u6761\u4ef6\u4e0b\u8fbe\u523087.33%\u51c6\u786e\u7387\u4e14\u529f\u8017\u4ec58.04mW\u3002", "motivation": "\u53d7\u4eba\u7c7b\u63a2\u7d22\u7b56\u7565\u542f\u53d1\uff0c\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u9ad8\u6548\u7684\u673a\u5668\u4eba\u7eb9\u7406\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u795e\u7ecf\u5f62\u6001\u89e6\u89c9\u611f\u77e5\u6765\u589e\u5f3a\u673a\u5668\u4eba\u4e0e\u73af\u5883\u7684\u4ea4\u4e92\u80fd\u529b\u3002", "method": "\u4f7f\u7528NeuroTac\u4f20\u611f\u5668\u91c7\u96c6\u795e\u7ecf\u5f62\u6001\u89e6\u89c9\u6570\u636e\uff0c\u6d4b\u8bd5\u4e86\u516d\u79cd\u63a2\u7d22\u52a8\u4f5c\uff08\u6ed1\u52a8\u3001\u65cb\u8f6c\u3001\u6572\u51fb\u53ca\u5176\u7ec4\u5408\uff09\uff0c\u5728\u56fa\u5b9a\u73af\u5883\u548c\u53d8\u5316\u6761\u4ef6\u4e0b\u8bc4\u4f30\u6027\u80fd\uff0c\u9009\u62e9\u6700\u4f73\u52a8\u4f5c\u7ec4\u5408\u3002", "result": "\u6ed1\u52a8+\u65cb\u8f6c\u52a8\u4f5c\u5728\u53d8\u5316\u63a5\u89e6\u6df1\u5ea6\u548c\u901f\u5ea6\u6761\u4ef6\u4e0b\u8fbe\u5230\u6700\u9ad8\u51c6\u786e\u738787.33%\uff0c\u540c\u65f6\u4fdd\u6301\u6781\u4f4e\u529f\u80178.04mW\uff0c\u4f18\u4e8e\u5176\u4ed6\u52a8\u4f5c\u7ec4\u5408\u3002", "conclusion": "\u6ed1\u52a8+\u65cb\u8f6c\u662f\u795e\u7ecf\u5f62\u6001\u89e6\u89c9\u611f\u77e5\u5728\u7eb9\u7406\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6700\u4f18\u63a2\u7d22\u7b56\u7565\uff0c\u5177\u6709\u663e\u8457\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u7684\u73af\u5883\u4ea4\u4e92\u80fd\u529b\u3002"}}
{"id": "2509.14967", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.14967", "abs": "https://arxiv.org/abs/2509.14967", "authors": ["Ana Davila", "Jacinto Colan", "Yasuhisa Hasegawa"], "title": "Affordance-Based Disambiguation of Surgical Instructions for Collaborative Robot-Assisted Surgery", "comment": "To be presented at the 1st Workshop on Intelligent Cobodied\n  Assistance and Robotic Empowerment (iCARE). 2025 Conference on Robot Learning\n  (CoRL)", "summary": "Effective human-robot collaboration in surgery is affected by the inherent\nambiguity of verbal communication. This paper presents a framework for a\nrobotic surgical assistant that interprets and disambiguates verbal\ninstructions from a surgeon by grounding them in the visual context of the\noperating field. The system employs a two-level affordance-based reasoning\nprocess that first analyzes the surgical scene using a multimodal\nvision-language model and then reasons about the instruction using a knowledge\nbase of tool capabilities. To ensure patient safety, a dual-set conformal\nprediction method is used to provide a statistically rigorous confidence\nmeasure for robot decisions, allowing it to identify and flag ambiguous\ncommands. We evaluated our framework on a curated dataset of ambiguous surgical\nrequests from cholecystectomy videos, demonstrating a general disambiguation\nrate of 60% and presenting a method for safer human-robot interaction in the\noperating room.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u673a\u5668\u4eba\u624b\u672f\u52a9\u624b\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u4e0a\u4e0b\u6587\u89e3\u91ca\u548c\u6d88\u9664\u5916\u79d1\u533b\u751f\u53e3\u5934\u6307\u4ee4\u7684\u6b67\u4e49\uff0c\u4f7f\u7528\u53cc\u7ea7\u63a8\u7406\u548c\u7b26\u5408\u6027\u9884\u6d4b\u786e\u4fdd\u5b89\u5168", "motivation": "\u89e3\u51b3\u624b\u672f\u4e2d\u53e3\u5934\u6c9f\u901a\u7684\u56fa\u6709\u6b67\u4e49\u6027\u95ee\u9898\uff0c\u63d0\u9ad8\u4eba\u673a\u534f\u4f5c\u7684\u5b89\u5168\u6027\u548c\u6548\u7387", "method": "\u91c7\u7528\u57fa\u4e8e\u53cc\u7ea7\u53ef\u4f9b\u6027\u63a8\u7406\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5206\u6790\u624b\u672f\u573a\u666f\uff0c\u4f7f\u7528\u5de5\u5177\u80fd\u529b\u77e5\u8bc6\u5e93\u8fdb\u884c\u6307\u4ee4\u63a8\u7406\uff0c\u5e76\u91c7\u7528\u53cc\u96c6\u7b26\u5408\u6027\u9884\u6d4b\u63d0\u4f9b\u7edf\u8ba1\u7f6e\u4fe1\u5ea6", "result": "\u5728\u80c6\u56ca\u5207\u9664\u672f\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u8fbe\u523060%\u7684\u901a\u7528\u6d88\u6b67\u7387", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u624b\u672f\u5ba4\u4e2d\u66f4\u5b89\u5168\u7684\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u65b9\u6cd5"}}
{"id": "2509.14978", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14978", "abs": "https://arxiv.org/abs/2509.14978", "authors": ["Yifan Zhai", "Rudolf Reiter", "Davide Scaramuzza"], "title": "PA-MPPI: Perception-Aware Model Predictive Path Integral Control for Quadrotor Navigation in Unknown Environments", "comment": null, "summary": "Quadrotor navigation in unknown environments is critical for practical\nmissions such as search-and-rescue. Solving it requires addressing three key\nchallenges: the non-convexity of free space due to obstacles,\nquadrotor-specific dynamics and objectives, and the need for exploration of\nunknown regions to find a path to the goal. Recently, the Model Predictive Path\nIntegral (MPPI) method has emerged as a promising solution that solves the\nfirst two challenges. By leveraging sampling-based optimization, it can\neffectively handle non-convex free space while directly optimizing over the\nfull quadrotor dynamics, enabling the inclusion of quadrotor-specific costs\nsuch as energy consumption. However, its performance in unknown environments is\nlimited, as it lacks the ability to explore unknown regions when blocked by\nlarge obstacles. To solve this issue, we introduce Perception-Aware MPPI\n(PA-MPPI). Here, perception-awareness is defined as adapting the trajectory\nonline based on perception objectives. Specifically, when the goal is occluded,\nPA-MPPI's perception cost biases trajectories that can perceive unknown\nregions. This expands the mapped traversable space and increases the likelihood\nof finding alternative paths to the goal. Through hardware experiments, we\ndemonstrate that PA-MPPI, running at 50 Hz with our efficient perception and\nmapping module, performs up to 100% better than the baseline in our challenging\nsettings where the state-of-the-art MPPI fails. In addition, we demonstrate\nthat PA-MPPI can be used as a safe and robust action policy for navigation\nfoundation models, which often provide goal poses that are not directly\nreachable.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u611f\u77e5\u611f\u77e5MPPI\uff08PA-MPPI\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u76ee\u6807\u88ab\u906e\u6321\u65f6\u5f15\u5165\u611f\u77e5\u6210\u672c\u6765\u5f15\u5bfc\u8f68\u8ff9\u63a2\u7d22\u672a\u77e5\u533a\u57df\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfMPPI\u5728\u672a\u77e5\u73af\u5883\u4e2d\u65e0\u6cd5\u6709\u6548\u63a2\u7d22\u7684\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u56db\u65cb\u7ffc\u5728\u672a\u77e5\u73af\u5883\u5bfc\u822a\u4e2d\u7684\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a\u975e\u51f8\u81ea\u7531\u7a7a\u95f4\u3001\u56db\u65cb\u7ffc\u7279\u5b9a\u52a8\u529b\u5b66\u548c\u76ee\u6807\u9700\u6c42\u3001\u4ee5\u53ca\u9700\u8981\u63a2\u7d22\u672a\u77e5\u533a\u57df\u5bfb\u627e\u8def\u5f84\u3002\u4f20\u7edfMPPI\u65b9\u6cd5\u5728\u524d\u4e24\u4e2a\u95ee\u9898\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u63a2\u7d22\u672a\u77e5\u533a\u57df\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "\u5728MPPI\u57fa\u7840\u4e0a\u5f15\u5165\u611f\u77e5\u611f\u77e5\u673a\u5236\uff0c\u5f53\u76ee\u6807\u88ab\u906e\u6321\u65f6\uff0c\u901a\u8fc7\u611f\u77e5\u6210\u672c\u51fd\u6570\u504f\u7f6e\u8f68\u8ff9\u4f7f\u5176\u80fd\u591f\u611f\u77e5\u672a\u77e5\u533a\u57df\uff0c\u4ece\u800c\u6269\u5c55\u53ef\u901a\u884c\u7a7a\u95f4\u6620\u5c04\u5e76\u589e\u52a0\u627e\u5230\u66ff\u4ee3\u8def\u5f84\u7684\u53ef\u80fd\u6027\u3002", "result": "\u786c\u4ef6\u5b9e\u9a8c\u663e\u793a\uff0cPA-MPPI\u572850Hz\u8fd0\u884c\u9891\u7387\u4e0b\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u573a\u666f\u4e2d\u6027\u80fd\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u9ad8\u8fbe100%\uff0c\u4e14\u80fd\u4f5c\u4e3a\u5bfc\u822a\u57fa\u7840\u6a21\u578b\u7684\u5b89\u5168\u9c81\u68d2\u52a8\u4f5c\u7b56\u7565\u3002", "conclusion": "PA-MPPI\u6709\u6548\u89e3\u51b3\u4e86\u56db\u65cb\u7ffc\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u76ee\u6807\u88ab\u5927\u969c\u788d\u7269\u906e\u6321\u65f6\u7684\u63a2\u7d22\u80fd\u529b\uff0c\u4e3a\u5b9e\u9645\u4efb\u52a1\u5982\u641c\u6551\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.14992", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14992", "abs": "https://arxiv.org/abs/2509.14992", "authors": ["Yifan Zhai", "Lorenzo Terenzi", "Patrick Frey", "Diego Garcia Soto", "Pascal Egli", "Marco Hutter"], "title": "ExT: Towards Scalable Autonomous Excavation via Large-Scale Multi-Task Pretraining and Fine-Tuning", "comment": null, "summary": "Scaling up the deployment of autonomous excavators is of great economic and\nsocietal importance. Yet it remains a challenging problem, as effective systems\nmust robustly handle unseen worksite conditions and new hardware\nconfigurations. Current state-of-the-art approaches rely on highly engineered,\ntask-specific controllers, which require extensive manual tuning for each new\nscenario. In contrast, recent advances in large-scale pretrained models have\nshown remarkable adaptability across tasks and embodiments in domains such as\nmanipulation and navigation, but their applicability to heavy construction\nmachinery remains largely unexplored. In this work, we introduce ExT, a unified\nopen-source framework for large-scale demonstration collection, pretraining,\nand fine-tuning of multitask excavation policies. ExT policies are first\ntrained on large-scale demonstrations collected from a mix of experts, then\nfine-tuned either with supervised fine-tuning (SFT) or reinforcement learning\nfine-tuning (RLFT) to specialize to new tasks or operating conditions. Through\nboth simulation and real-world experiments, we show that pretrained ExT\npolicies can execute complete excavation cycles with centimeter-level accuracy,\nsuccessfully transferring from simulation to real machine with performance\ncomparable to specialized single-task controllers. Furthermore, in simulation,\nwe demonstrate that ExT's fine-tuning pipelines allow rapid adaptation to new\ntasks, out-of-distribution conditions, and machine configurations, while\nmaintaining strong performance on previously learned tasks. These results\nhighlight the potential of ExT to serve as a foundation for scalable and\ngeneralizable autonomous excavation.", "AI": {"tldr": "ExT\u662f\u4e00\u4e2a\u7528\u4e8e\u6316\u6398\u673a\u81ea\u4e3b\u64cd\u4f5c\u7684\u5f00\u6e90\u6846\u67b6\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u6f14\u793a\u6570\u636e\u9884\u8bad\u7ec3\u548c\u591a\u4efb\u52a1\u7b56\u7565\u5fae\u8c03\uff0c\u5b9e\u73b0\u4e86\u4ece\u4eff\u771f\u5230\u771f\u5b9e\u673a\u5668\u7684\u6210\u529f\u8fc1\u79fb\uff0c\u5e76\u80fd\u5feb\u901f\u9002\u5e94\u65b0\u4efb\u52a1\u548c\u786c\u4ef6\u914d\u7f6e\u3002", "motivation": "\u81ea\u4e3b\u6316\u6398\u673a\u7684\u89c4\u6a21\u5316\u90e8\u7f72\u5177\u6709\u91cd\u8981\u7ecf\u6d4e\u548c\u793e\u4f1a\u4ef7\u503c\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9ad8\u5ea6\u5de5\u7a0b\u5316\u7684\u4efb\u52a1\u7279\u5b9a\u63a7\u5236\u5668\uff0c\u9700\u8981\u4e3a\u6bcf\u4e2a\u65b0\u573a\u666f\u8fdb\u884c\u5927\u91cf\u624b\u52a8\u8c03\u4f18\u3002\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u5176\u4ed6\u9886\u57df\u663e\u793a\u51fa\u5353\u8d8a\u7684\u9002\u5e94\u6027\uff0c\u4f46\u5728\u91cd\u578b\u5de5\u7a0b\u673a\u68b0\u9886\u57df\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "ExT\u6846\u67b6\u5305\u542b\u5927\u89c4\u6a21\u6f14\u793a\u6570\u636e\u6536\u96c6\u3001\u9884\u8bad\u7ec3\u548c\u591a\u4efb\u52a1\u7b56\u7565\u5fae\u8c03\u3002\u9996\u5148\u4ece\u4e13\u5bb6\u6df7\u5408\u6536\u96c6\u5927\u89c4\u6a21\u6f14\u793a\u6570\u636e\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u901a\u8fc7\u76d1\u7763\u5fae\u8c03(SFT)\u6216\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03(RLFT)\u6765\u9002\u5e94\u65b0\u4efb\u52a1\u6216\u64cd\u4f5c\u6761\u4ef6\u3002", "result": "\u9884\u8bad\u7ec3\u7684ExT\u7b56\u7565\u80fd\u591f\u4ee5\u5398\u7c73\u7ea7\u7cbe\u5ea6\u6267\u884c\u5b8c\u6574\u7684\u6316\u6398\u5faa\u73af\uff0c\u4ece\u4eff\u771f\u6210\u529f\u8fc1\u79fb\u5230\u771f\u5b9e\u673a\u5668\uff0c\u6027\u80fd\u4e0e\u4e13\u7528\u5355\u4efb\u52a1\u63a7\u5236\u5668\u76f8\u5f53\u3002\u5728\u4eff\u771f\u4e2d\uff0cExT\u7684\u5fae\u8c03\u6d41\u7a0b\u80fd\u591f\u5feb\u901f\u9002\u5e94\u65b0\u4efb\u52a1\u3001\u5206\u5e03\u5916\u6761\u4ef6\u548c\u673a\u5668\u914d\u7f6e\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u5148\u524d\u5b66\u4e60\u4efb\u52a1\u7684\u5f3a\u6027\u80fd\u3002", "conclusion": "ExT\u6709\u6f5c\u529b\u4f5c\u4e3a\u53ef\u6269\u5c55\u548c\u901a\u7528\u81ea\u4e3b\u6316\u6398\u7684\u57fa\u7840\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u91cd\u578b\u5de5\u7a0b\u673a\u68b0\u9886\u57df\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.14999", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14999", "abs": "https://arxiv.org/abs/2509.14999", "authors": ["Haoxuan Jiang", "Peicong Qian", "Yusen Xie", "Linwei Zheng", "Xiaocong Li", "Ming Liu", "Jun Ma"], "title": "Semantic-LiDAR-Inertial-Wheel Odometry Fusion for Robust Localization in Large-Scale Dynamic Environments", "comment": null, "summary": "Reliable, drift-free global localization presents significant challenges yet\nremains crucial for autonomous navigation in large-scale dynamic environments.\nIn this paper, we introduce a tightly-coupled Semantic-LiDAR-Inertial-Wheel\nOdometry fusion framework, which is specifically designed to provide\nhigh-precision state estimation and robust localization in large-scale dynamic\nenvironments. Our framework leverages an efficient semantic-voxel map\nrepresentation and employs an improved scan matching algorithm, which utilizes\nglobal semantic information to significantly reduce long-term trajectory drift.\nFurthermore, it seamlessly fuses data from LiDAR, IMU, and wheel odometry using\na tightly-coupled multi-sensor fusion Iterative Error-State Kalman Filter\n(iESKF). This ensures reliable localization without experiencing abnormal\ndrift. Moreover, to tackle the challenges posed by terrain variations and\ndynamic movements, we introduce a 3D adaptive scaling strategy that allows for\nflexible adjustments to wheel odometry measurement weights, thereby enhancing\nlocalization precision. This study presents extensive real-world experiments\nconducted in a one-million-square-meter automated port, encompassing 3,575\nhours of operational data from 35 Intelligent Guided Vehicles (IGVs). The\nresults consistently demonstrate that our system outperforms state-of-the-art\nLiDAR-based localization methods in large-scale dynamic environments,\nhighlighting the framework's reliability and practical value.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7d27\u8026\u5408\u7684\u8bed\u4e49-LiDAR-\u60ef\u6027-\u8f6e\u5f0f\u91cc\u7a0b\u8ba1\u878d\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9ad8\u7cbe\u5ea6\u72b6\u6001\u4f30\u8ba1\u548c\u9c81\u68d2\u5b9a\u4f4d\uff0c\u901a\u8fc7\u8bed\u4e49\u4f53\u7d20\u5730\u56fe\u548c\u6539\u8fdb\u7684\u626b\u63cf\u5339\u914d\u7b97\u6cd5\u663e\u8457\u51cf\u5c11\u957f\u671f\u8f68\u8ff9\u6f02\u79fb\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u52a8\u6001\u73af\u5883\u4e2d\u53ef\u9760\u3001\u65e0\u6f02\u79fb\u7684\u5168\u5c40\u5b9a\u4f4d\u96be\u9898\uff0c\u8fd9\u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u5bfc\u822a\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u7d27\u8026\u5408\u591a\u4f20\u611f\u5668\u878d\u5408\u8fed\u4ee3\u8bef\u5dee\u72b6\u6001\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668(iESKF)\u878d\u5408LiDAR\u3001IMU\u548c\u8f6e\u5f0f\u91cc\u7a0b\u8ba1\u6570\u636e\uff0c\u91c7\u7528\u9ad8\u6548\u7684\u8bed\u4e49\u4f53\u7d20\u5730\u56fe\u8868\u793a\u548c\u6539\u8fdb\u7684\u626b\u63cf\u5339\u914d\u7b97\u6cd5\uff0c\u5e76\u5f15\u51653D\u81ea\u9002\u5e94\u7f29\u653e\u7b56\u7565\u8c03\u6574\u8f6e\u5f0f\u91cc\u7a0b\u8ba1\u6d4b\u91cf\u6743\u91cd\u3002", "result": "\u5728100\u4e07\u5e73\u65b9\u7c73\u81ea\u52a8\u5316\u6e2f\u53e3\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\uff0c\u5305\u542b35\u8f86\u667a\u80fd\u5bfc\u5f15\u8f66\u76843575\u5c0f\u65f6\u8fd0\u884c\u6570\u636e\uff0c\u7ed3\u679c\u663e\u793a\u7cfb\u7edf\u5728\u5927\u89c4\u6a21\u52a8\u6001\u73af\u5883\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684LiDAR\u5b9a\u4f4d\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u5927\u89c4\u6a21\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u53ef\u9760\u6027\u548c\u5b9e\u7528\u4ef7\u503c\uff0c\u80fd\u591f\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u72b6\u6001\u4f30\u8ba1\u548c\u9c81\u68d2\u5b9a\u4f4d\uff0c\u6709\u6548\u89e3\u51b3\u957f\u671f\u8f68\u8ff9\u6f02\u79fb\u95ee\u9898\u3002"}}
{"id": "2509.15052", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.15052", "abs": "https://arxiv.org/abs/2509.15052", "authors": ["Walker Gosrich", "Saurav Agarwal", "Kashish Garg", "Siddharth Mayya", "Matthew Malencia", "Mark Yim", "Vijay Kumar"], "title": "Online Multi-Robot Coordination and Cooperation with Task Precedence Relationships", "comment": "20 pages, 19 figures, Accepted to IEEE Transactions on Robotics\n  (TR-O) August 2025", "summary": "We propose a new formulation for the multi-robot task allocation problem that\nincorporates (a) complex precedence relationships between tasks, (b) efficient\nintra-task coordination, and (c) cooperation through the formation of robot\ncoalitions. A task graph specifies the tasks and their relationships, and a set\nof reward functions models the effects of coalition size and preceding task\nperformance. Maximizing task rewards is NP-hard; hence, we propose network\nflow-based algorithms to approximate solutions efficiently. A novel online\nalgorithm performs iterative re-allocation, providing robustness to task\nfailures and model inaccuracies to achieve higher performance than offline\napproaches. We comprehensively evaluate the algorithms in a testbed with random\nmissions and reward functions and compare them to a mixed-integer solver and a\ngreedy heuristic. Additionally, we validate the overall approach in an advanced\nsimulator, modeling reward functions based on realistic physical phenomena and\nexecuting the tasks with realistic robot dynamics. Results establish efficacy\nin modeling complex missions and efficiency in generating high-fidelity task\nplans while leveraging task relationships.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u673a\u5668\u4eba\u4efb\u52a1\u5206\u914d\u65b9\u6cd5\uff0c\u901a\u8fc7\u4efb\u52a1\u56fe\u5efa\u6a21\u590d\u6742\u4efb\u52a1\u5173\u7cfb\uff0c\u4f7f\u7528\u7f51\u7edc\u6d41\u7b97\u6cd5\u9ad8\u6548\u8fd1\u4f3c\u6c42\u89e3\uff0c\u5728\u7ebf\u7b97\u6cd5\u63d0\u4f9b\u9c81\u68d2\u6027\uff0c\u5728\u968f\u673a\u4efb\u52a1\u548c\u771f\u5b9e\u4eff\u771f\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u590d\u6742\u4efb\u52a1\u4f18\u5148\u7ea7\u5173\u7cfb\u3001\u673a\u5668\u4eba\u8054\u76df\u534f\u4f5c\u548c\u9ad8\u6548\u4efb\u52a1\u5185\u534f\u8c03\uff0c\u9700\u8981\u66f4\u5f3a\u5927\u7684\u591a\u673a\u5668\u4eba\u4efb\u52a1\u5206\u914d\u6846\u67b6", "method": "\u4f7f\u7528\u4efb\u52a1\u56fe\u5b9a\u4e49\u4efb\u52a1\u5173\u7cfb\uff0c\u5956\u52b1\u51fd\u6570\u5efa\u6a21\u8054\u76df\u89c4\u6a21\u548c\u524d\u7f6e\u4efb\u52a1\u5f71\u54cd\uff0c\u7f51\u7edc\u6d41\u7b97\u6cd5\u8fd1\u4f3c\u6c42\u89e3NP\u96be\u95ee\u9898\uff0c\u5728\u7ebf\u8fed\u4ee3\u91cd\u5206\u914d\u7b97\u6cd5\u63d0\u4f9b\u9c81\u68d2\u6027", "result": "\u5728\u968f\u673a\u4efb\u52a1\u6d4b\u8bd5\u5e8a\u548c\u9ad8\u7ea7\u4eff\u771f\u5668\u4e2d\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u6df7\u5408\u6574\u6570\u6c42\u89e3\u5668\u548c\u8d2a\u5a6a\u542f\u53d1\u5f0f\u65b9\u6cd5\u8868\u73b0\u66f4\u597d\uff0c\u80fd\u591f\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u4efb\u52a1\u8ba1\u5212", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5efa\u6a21\u590d\u6742\u4efb\u52a1\u5173\u7cfb\uff0c\u5229\u7528\u4efb\u52a1\u95f4\u4f9d\u8d56\u5173\u7cfb\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u4efb\u52a1\u8ba1\u5212\uff0c\u5728\u7ebf\u7b97\u6cd5\u63d0\u4f9b\u5bf9\u4efb\u52a1\u5931\u8d25\u548c\u6a21\u578b\u4e0d\u51c6\u786e\u7684\u9c81\u68d2\u6027"}}
{"id": "2509.15061", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.15061", "abs": "https://arxiv.org/abs/2509.15061", "authors": ["Xingyao Lin", "Xinghao Zhu", "Tianyi Lu", "Sicheng Xie", "Hui Zhang", "Xipeng Qiu", "Zuxuan Wu", "Yu-Gang Jiang"], "title": "Ask-to-Clarify: Resolving Instruction Ambiguity through Multi-turn Dialogue", "comment": "9 pages, 4 figures", "summary": "The ultimate goal of embodied agents is to create collaborators that can\ninteract with humans, not mere executors that passively follow instructions.\nThis requires agents to communicate, coordinate, and adapt their actions based\non human feedback. Recently, advances in VLAs have offered a path toward this\ngoal. However, most current VLA-based embodied agents operate in a one-way\nmode: they receive an instruction and execute it without feedback. This\napproach fails in real-world scenarios where instructions are often ambiguous.\nIn this paper, we address this problem with the Ask-to-Clarify framework. Our\nframework first resolves ambiguous instructions by asking questions in a\nmulti-turn dialogue. Then it generates low-level actions end-to-end.\nSpecifically, the Ask-to-Clarify framework consists of two components, one VLM\nfor collaboration and one diffusion for action. We also introduce a connection\nmodule that generates conditions for the diffusion based on the output of the\nVLM. This module adjusts the observation by instructions to create reliable\nconditions. We train our framework with a two-stage knowledge-insulation\nstrategy. First, we fine-tune the collaboration component using\nambiguity-solving dialogue data to handle ambiguity. Then, we integrate the\naction component while freezing the collaboration one. This preserves the\ninteraction abilities while fine-tuning the diffusion to generate actions. The\ntraining strategy guarantees our framework can first ask questions, then\ngenerate actions. During inference, a signal detector functions as a router\nthat helps our framework switch between asking questions and taking actions. We\nevaluate the Ask-to-Clarify framework in 8 real-world tasks, where it\noutperforms existing state-of-the-art VLAs. The results suggest that our\nproposed framework, along with the training strategy, provides a path toward\ncollaborative embodied agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86Ask-to-Clarify\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u8f6e\u5bf9\u8bdd\u89e3\u51b3\u6a21\u7cca\u6307\u4ee4\u95ee\u9898\uff0c\u7136\u540e\u751f\u6210\u7aef\u5230\u7aef\u7684\u4f4e\u7ea7\u52a8\u4f5c\uff0c\u57288\u4e2a\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709VLA\u65b9\u6cd5", "motivation": "\u5f53\u524d\u57fa\u4e8eVLA\u7684\u5177\u8eab\u4ee3\u7406\u5927\u591a\u91c7\u7528\u5355\u5411\u6267\u884c\u6a21\u5f0f\uff0c\u65e0\u6cd5\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u4e2d\u5e38\u89c1\u7684\u6a21\u7cca\u6307\u4ee4\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u4e0e\u4eba\u7c7b\u534f\u4f5c\u3001\u6c9f\u901a\u548c\u9002\u5e94\u7684\u4ee3\u7406", "method": "Ask-to-Clarify\u6846\u67b6\u5305\u542b\u7528\u4e8e\u534f\u4f5c\u7684VLM\u548c\u7528\u4e8e\u52a8\u4f5c\u751f\u6210\u7684\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u8fde\u63a5\u6a21\u5757\u751f\u6210\u6761\u4ef6\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u77e5\u8bc6\u9694\u79bb\u8bad\u7ec3\u7b56\u7565\uff0c\u5148\u5fae\u8c03\u534f\u4f5c\u7ec4\u4ef6\u5904\u7406\u6a21\u7cca\u6027\uff0c\u518d\u96c6\u6210\u52a8\u4f5c\u7ec4\u4ef6", "result": "\u57288\u4e2a\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u8bc4\u4f30\u4e2d\uff0c\u8be5\u6846\u67b6\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684VLA\u65b9\u6cd5", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u548c\u8bad\u7ec3\u7b56\u7565\u4e3a\u5b9e\u73b0\u534f\u4f5c\u5f0f\u5177\u8eab\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u884c\u8def\u5f84"}}
{"id": "2509.15062", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.15062", "abs": "https://arxiv.org/abs/2509.15062", "authors": ["Tianxin Hu", "Weixiang Guo", "Ruimeng Liu", "Xinhang Xu", "Rui Qian", "Jinyu Chen", "Shenghai Yuan", "Lihua Xie"], "title": "Energy-Constrained Navigation for Planetary Rovers under Hybrid RTG-Solar Power", "comment": null, "summary": "Future planetary exploration rovers must operate for extended durations on\nhybrid power inputs that combine steady radioisotope thermoelectric generator\n(RTG) output with variable solar photovoltaic (PV) availability. While\nenergy-aware planning has been studied for aerial and underwater robots under\nbattery limits, few works for ground rovers explicitly model power flow or\nenforce instantaneous power constraints. Classical terrain-aware planners\nemphasize slope or traversability, and trajectory optimization methods\ntypically focus on geometric smoothness and dynamic feasibility, neglecting\nenergy feasibility. We present an energy-constrained trajectory planning\nframework that explicitly integrates physics-based models of translational,\nrotational, and resistive power with baseline subsystem loads, under hybrid\nRTG-solar input. By incorporating both cumulative energy budgets and\ninstantaneous power constraints into SE(2)-based polynomial trajectory\noptimization, the method ensures trajectories that are simultaneously smooth,\ndynamically feasible, and power-compliant. Simulation results on lunar-like\nterrain show that our planner generates trajectories with peak power within\n0.55 percent of the prescribed limit, while existing methods exceed limits by\nover 17 percent. This demonstrates a principled and practical approach to\nenergy-aware autonomy for long-duration planetary missions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u884c\u661f\u63a2\u6d4b\u8f66\u7684\u80fd\u91cf\u7ea6\u675f\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u6574\u5408\u4e86RTG-\u592a\u9633\u80fd\u6df7\u5408\u52a8\u529b\u7cfb\u7edf\u7684\u7269\u7406\u6a21\u578b\uff0c\u786e\u4fdd\u8f68\u8ff9\u65e2\u5e73\u6ed1\u52a8\u6001\u53ef\u884c\u53c8\u6ee1\u8db3\u77ac\u65f6\u529f\u7387\u7ea6\u675f", "motivation": "\u672a\u6765\u884c\u661f\u63a2\u6d4b\u8f66\u9700\u8981\u5728\u6df7\u5408\u52a8\u529b\uff08RTG\u7a33\u5b9a\u8f93\u51fa+\u592a\u9633\u80fd\u53ef\u53d8\u8f93\u5165\uff09\u4e0b\u957f\u65f6\u95f4\u8fd0\u884c\uff0c\u73b0\u6709\u89c4\u5212\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5730\u5f62\u548c\u51e0\u4f55\u5e73\u6ed1\u6027\uff0c\u5ffd\u7565\u4e86\u80fd\u91cf\u53ef\u884c\u6027\u548c\u77ac\u65f6\u529f\u7387\u7ea6\u675f", "method": "\u5c06\u7d2f\u79ef\u80fd\u91cf\u9884\u7b97\u548c\u77ac\u65f6\u529f\u7387\u7ea6\u675f\u6574\u5408\u5230SE(2)\u591a\u9879\u5f0f\u8f68\u8ff9\u4f18\u5316\u4e2d\uff0c\u4f7f\u7528\u57fa\u4e8e\u7269\u7406\u7684\u5e73\u79fb\u3001\u65cb\u8f6c\u548c\u963b\u529b\u529f\u7387\u6a21\u578b\uff0c\u7ed3\u5408\u57fa\u7ebf\u5b50\u7cfb\u7edf\u8d1f\u8f7d", "result": "\u5728\u6708\u7403\u7c7b\u4f3c\u5730\u5f62\u4e0a\u7684\u4eff\u771f\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u8f68\u8ff9\u5cf0\u503c\u529f\u7387\u4ec5\u8d85\u51fa\u89c4\u5b9a\u9650\u52360.55%\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u8d85\u51fa\u9650\u5236\u8d85\u8fc717%", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u957f\u671f\u884c\u661f\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u7406\u6027\u4e14\u5b9e\u7528\u7684\u80fd\u91cf\u611f\u77e5\u81ea\u4e3b\u89c4\u5212\u65b9\u6cd5\uff0c\u80fd\u591f\u786e\u4fdd\u8f68\u8ff9\u540c\u65f6\u6ee1\u8db3\u5e73\u6ed1\u6027\u3001\u52a8\u6001\u53ef\u884c\u6027\u548c\u529f\u7387\u5408\u89c4\u6027"}}
{"id": "2509.15153", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15153", "abs": "https://arxiv.org/abs/2509.15153", "authors": ["Yating Lin", "Zixuan Huang", "Fan Yang", "Dmitry Berenson"], "title": "AnoF-Diff: One-Step Diffusion-Based Anomaly Detection for Forceful Tool Use", "comment": null, "summary": "Multivariate time-series anomaly detection, which is critical for identifying\nunexpected events, has been explored in the field of machine learning for\nseveral decades. However, directly applying these methods to data from forceful\ntool use tasks is challenging because streaming sensor data in the real world\ntends to be inherently noisy, exhibits non-stationary behavior, and varies\nacross different tasks and tools. To address these challenges, we propose a\nmethod, AnoF-Diff, based on the diffusion model to extract force-torque\nfeatures from time-series data and use force-torque features to detect\nanomalies. We compare our method with other state-of-the-art methods in terms\nof F1-score and Area Under the Receiver Operating Characteristic curve (AUROC)\non four forceful tool-use tasks, demonstrating that our method has better\nperformance and is more robust to a noisy dataset. We also propose the method\nof parallel anomaly score evaluation based on one-step diffusion and\ndemonstrate how our method can be used for online anomaly detection in several\nforceful tool use experiments.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684AnoF-Diff\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u63d0\u53d6\u529b-\u626d\u77e9\u7279\u5f81\u5e76\u68c0\u6d4b\u5f02\u5e38\uff0c\u5728\u5608\u6742\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u6d41\u5f0f\u4f20\u611f\u5668\u6570\u636e\u901a\u5e38\u5177\u6709\u566a\u58f0\u5927\u3001\u975e\u5e73\u7a33\u4e14\u56e0\u4efb\u52a1\u548c\u5de5\u5177\u800c\u5f02\u7684\u7279\u70b9\uff0c\u76f4\u63a5\u5e94\u7528\u73b0\u6709\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5177\u6709\u6311\u6218\u6027", "method": "\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u63d0\u53d6\u529b-\u626d\u77e9\u7279\u5f81\uff0c\u5e76\u91c7\u7528\u5e76\u884c\u5f02\u5e38\u8bc4\u5206\u8bc4\u4f30\u65b9\u6cd5\u8fdb\u884c\u4e00\u6b65\u6269\u6563\uff0c\u652f\u6301\u5728\u7ebf\u5f02\u5e38\u68c0\u6d4b", "result": "\u5728\u56db\u4e2a\u5f3a\u529b\u5de5\u5177\u4f7f\u7528\u4efb\u52a1\u4e0a\uff0cF1\u5206\u6570\u548cAUROC\u6307\u6807\u5747\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5bf9\u566a\u58f0\u6570\u636e\u96c6\u66f4\u5177\u9c81\u68d2\u6027", "conclusion": "AnoF-Diff\u65b9\u6cd5\u5728\u5f3a\u529b\u5de5\u5177\u4f7f\u7528\u573a\u666f\u4e2d\u6709\u6548\u89e3\u51b3\u4e86\u566a\u58f0\u548c\u975e\u5e73\u7a33\u6027\u95ee\u9898\uff0c\u4e3a\u5728\u7ebf\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848"}}
{"id": "2509.15180", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.15180", "abs": "https://arxiv.org/abs/2509.15180", "authors": ["Yitian Gao", "Lucas Chen", "Priyanka Bhovad", "Sicheng Wang", "Zachary Kingston", "Laura H. Blumenschein"], "title": "Parallel Simulation of Contact and Actuation for Soft Growing Robots", "comment": "26 pages, 9 figures, 1 table. Under review", "summary": "Soft growing robots, commonly referred to as vine robots, have demonstrated\nremarkable ability to interact safely and robustly with unstructured and\ndynamic environments. It is therefore natural to exploit contact with the\nenvironment for planning and design optimization tasks. Previous research has\nfocused on planning under contact for passively deforming robots with\npre-formed bends. However, adding active steering to these soft growing robots\nis necessary for successful navigation in more complex environments. To this\nend, we develop a unified modeling framework that integrates vine robot growth,\nbending, actuation, and obstacle contact. We extend the beam moment model to\ninclude the effects of actuation on kinematics under growth and then use these\nmodels to develop a fast parallel simulation framework. We validate our model\nand simulator with real robot experiments. To showcase the capabilities of our\nframework, we apply our model in a design optimization task to find designs for\nvine robots navigating through cluttered environments, identifying designs that\nminimize the number of required actuators by exploiting environmental contacts.\nWe show the robustness of the designs to environmental and manufacturing\nuncertainties. Finally, we fabricate an optimized design and successfully\ndeploy it in an obstacle-rich environment.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u5efa\u6a21\u6846\u67b6\uff0c\u7528\u4e8e\u8f6f\u751f\u957f\u673a\u5668\u4eba\u7684\u751f\u957f\u3001\u5f2f\u66f2\u3001\u9a71\u52a8\u548c\u969c\u788d\u7269\u63a5\u89e6\u7684\u96c6\u6210\u5efa\u6a21\uff0c\u5e76\u5e94\u7528\u4e8e\u8bbe\u8ba1\u4f18\u5316\u4efb\u52a1", "motivation": "\u8f6f\u751f\u957f\u673a\u5668\u4eba\uff08\u85e4\u8513\u673a\u5668\u4eba\uff09\u5728\u975e\u7ed3\u6784\u5316\u548c\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u5b89\u5168\u4ea4\u4e92\u80fd\u529b\uff0c\u4f46\u9700\u8981\u4e3b\u52a8\u8f6c\u5411\u6765\u5728\u66f4\u590d\u6742\u73af\u5883\u4e2d\u5bfc\u822a", "method": "\u6269\u5c55\u6881\u77e9\u6a21\u578b\u4ee5\u5305\u542b\u9a71\u52a8\u5bf9\u751f\u957f\u8fd0\u52a8\u5b66\u7684\u5f71\u54cd\uff0c\u5f00\u53d1\u5feb\u901f\u5e76\u884c\u4eff\u771f\u6846\u67b6\uff0c\u5e76\u8fdb\u884c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u9a8c\u8bc1", "result": "\u6a21\u578b\u548c\u4eff\u771f\u5668\u5f97\u5230\u9a8c\u8bc1\uff0c\u901a\u8fc7\u8bbe\u8ba1\u4f18\u5316\u627e\u5230\u4e86\u5728\u6742\u4e71\u73af\u5883\u4e2d\u5bfc\u822a\u7684\u85e4\u8513\u673a\u5668\u4eba\u8bbe\u8ba1\uff0c\u6700\u5c0f\u5316\u6240\u9700\u9a71\u52a8\u5668\u6570\u91cf\u5e76\u5229\u7528\u73af\u5883\u63a5\u89e6", "conclusion": "\u4f18\u5316\u8bbe\u8ba1\u5bf9\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u548c\u5236\u9020\u4e0d\u786e\u5b9a\u6027\u5177\u6709\u9c81\u68d2\u6027\uff0c\u6210\u529f\u5728\u969c\u788d\u7269\u4e30\u5bcc\u7684\u73af\u5883\u4e2d\u90e8\u7f72\u4e86\u4f18\u5316\u8bbe\u8ba1"}}

<div id=toc></div>

# Table of Contents

- [cs.SI](#cs.SI) [Total: 3]
- [cs.ET](#cs.ET) [Total: 1]
- [econ.EM](#econ.EM) [Total: 2]
- [cs.AI](#cs.AI) [Total: 21]
- [stat.AP](#stat.AP) [Total: 3]
- [cs.CY](#cs.CY) [Total: 9]


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [1] [Identifying Stable Influencers: Distinguishing Stable and Temporal Influencers Using Long-Term Twitter Data](https://arxiv.org/abs/2512.17166)
*Harutaka Yamada,Sho Tsugawa,Mitsuo Yoshida*

Main category: cs.SI

TL;DR: 该研究提出区分社交媒体中稳定影响者与临时影响者的方法，重点关注源传播者和经纪人两类影响者，基于历史数据预测未来稳定性，分类模型AUC达0.89和0.81。


<details>
  <summary>Details</summary>
Motivation: 社交媒体营销需要识别能够长期维持影响力的稳定影响者，而非仅关注临时有影响力的用户。当前面临从临时影响者中区分稳定影响者的挑战，特别是针对源传播者（传播自己内容）和经纪人（传播他人信息）两类不同类型的影响者。

Method: 使用约19,000名Twitter用户6个月的转发数据进行分析。研究稳定影响者的特征，开发分类模型预测临时影响用户中的稳定影响者。重点关注两类影响者：源传播者和经纪人。

Result: 发现过去维持影响力的用户更可能在未来继续维持影响力。分类模型对源传播者的AUC约为0.89，对经纪人的AUC约为0.81。实验结果表明当前影响力是分类的关键因素，而过去影响力也对分类有显著贡献，特别是对源传播者。

Conclusion: 该研究成功区分了社交媒体中的稳定影响者和临时影响者，为社交媒体营销提供了识别长期有价值影响者的有效方法。模型表现良好，特别是对源传播者的预测效果更佳，表明历史影响力数据对预测稳定性具有重要价值。

Abstract: For effective social media marketing, identifying stable influencers-those who sustain their influence over an extended period-is more valuable than focusing on users who are influential only temporarily. This study addresses the challenge of distinguishing stable influencers from transient ones among users who are influential at a given point in time. We particularly focus on two distinct types of influencers: source spreaders, who widely disseminate their own content, and brokers, who play a key role in propagating information originating from others. Using six months of retweet data from approximately 19,000 Twitter users, we analyze the characteristics of stable influencers. Our findings reveal that users who have maintained influence in the past are more likely to continue doing so in the future. Furthermore, we develop classification models to predict stable influencers among temporarily influential users, achieving an AUC of approximately 0.89 for source spreaders and 0.81 for brokers. Our experimental results highlight that current influence is a critical factor in classifying influencers, while past influence also significantly contributes, particularly for source spreaders.

</details>


### [2] [Privacy-Preserving Synthetic Dataset of Individual Daily Trajectories for City-Scale Mobility Analytics](https://arxiv.org/abs/2512.17239)
*Jun'ichi Ozaki,Ryosuke Susuta,Takuhiro Moriyama,Yohei Shida*

Main category: cs.SI

TL;DR: 提出一种隐私保护的合成移动数据集生成方法，通过整合OD流和两种行为约束，从聚合数据重建日常轨迹，无需个人标识信息。


<details>
  <summary>Details</summary>
Motivation: 城市移动数据对城市规划、交通需求预测等应用至关重要，但个人GPS轨迹存在严重的重识别风险无法共享。聚合的OD矩阵只能提供部分洞察，无法捕捉人类日常移动的关键行为特性。

Method: 整合OD流与两种行为约束：1) 仅作为粗粒度统计量可用的停留-出行时间分位数；2) 每日访问地点数量分布的普适定律。将这些要素嵌入多目标优化框架，在无需个人标识的情况下重现真实的移动分布。

Result: 在东京23区和福冈县两个对比区域验证，合成移动数据高保真地重现了停留-出行时间和访问频率分布，OD一致性偏差保持在日常波动的自然范围内。

Conclusion: 该研究建立了现实约束下的实用合成路径，为政府、城市规划者和产业提供可扩展的高分辨率移动数据访问，支持政策和商业领域的实际部署，无需敏感个人记录。

Abstract: Urban mobility data are indispensable for urban planning, transportation demand forecasting, pandemic modeling, and many other applications; however, individual mobile phone-derived Global Positioning System traces cannot generally be shared with third parties owing to severe re-identification risks. Aggregated records, such as origin-destination (OD) matrices, offer partial insights but fail to capture the key behavioral properties of daily human movement, limiting realistic city-scale analyses.
  This study presents a privacy-preserving synthetic mobility dataset that reconstructs daily trajectories from aggregated inputs. The proposed method integrates OD flows with two complementary behavioral constraints: (1) dwell-travel time quantiles that are available only as coarse summary statistics and (2) the universal law for the daily distribution of the number of visited locations. Embedding these elements in a multi-objective optimization framework enables the reproduction of realistic distributions of human mobility while ensuring that no personal identifiers are required.
  The proposed framework is validated in two contrasting regions of Japan: (1) the 23 special wards of Tokyo, representing a dense metropolitan environment; and (2) Fukuoka Prefecture, where urban and suburban mobility patterns coexist. The resulting synthetic mobility data reproduce dwell-travel time and visit frequency distributions with high fidelity, while deviations in OD consistency remain within the natural range of daily fluctuations.
  The results of this study establish a practical synthesis pathway under real-world constraints, providing governments, urban planners, and industries with scalable access to high-resolution mobility data for reliable analytics without the need for sensitive personal records, and supporting practical deployments in policy and commercial domains.

</details>


### [3] [A Comparative Analysis of Instagram and TikTok as Islamic Da'wah Media in the Digital Era](https://arxiv.org/abs/2512.17646)
*Wisnu Uriawan,Muhammad Saifurridwani Ijazi,Nizzami Ramdhan Arraudy,Onixa Shafa Putri Wibowo,Radithya Dwi Santoso*

Main category: cs.SI

TL;DR: 研究比较了Instagram和TikTok在伊斯兰宣教（da'wah）方面的有效性，发现TikTok在传播信息和吸引注意力方面更有效（互动率1.42%），而Instagram在深度互动和社区建设方面更强（互动率5.47%），建议结合使用两个平台。


<details>
  <summary>Details</summary>
Motivation: 研究动机是分析比较Instagram和TikTok这两个流行社交媒体平台在伊斯兰宣教方面的有效性，为数字时代的宣教策略提供实证基础。

Method: 采用描述性定量研究方法，通过对热门宣教账号@hananattakistory在2025年10月至11月期间的观察，从媒体特征、宣教沟通策略、受众参与效果和用户行为反应四个方面进行分析。

Result: TikTok在传播宣教信息方面更有效，互动率为1.42%；Instagram的总互动量更高，互动率达5.47%，显示更深层的受众参与。TikTok在吸引注意力（awareness）阶段更有效，而Instagram在建立忠诚度和培养数字宣教社区方面更强。

Conclusion: 结合使用Instagram和TikTok可以作为互补的数字宣教策略，既能扩大覆盖面，又能深化对伊斯兰价值观的理解。研究的主要贡献在于提供了比较性的实证分析，为针对年轻穆斯林受众开发适应性和情境化的数字宣教策略提供了战略基础。

Abstract: This research aims to analyze and compare the effectiveness of Islamic da'wah on two popular social media platforms, namely Instagram and TikTok. The analysis is conducted based on four main aspects: media characteristics, da'wah communication strategies, audience engagement effectiveness, and user behavioral responses. The research employs a descriptive quantitative approach through observations of the popular da'wah account @hananattakistory during the period of October to November 2025. The findings indicate that TikTok excels in the effectiveness of disseminating da'wah messages through high interaction rates, achieving an engagement rate of 1.42%. In contrast, Instagram records a higher total interaction with an engagement rate of 5.47%, reflecting deeper and more reflective audience involvement. Qualitative analysis shows that TikTok is more effective in the initial stage of capturing audience attention (awareness), while Instagram is stronger in building loyalty and fostering a digital da'wah community. Thus, combining the use of both platforms can serve as a complementary digital da'wah strategy, expanding reach while deepening the understanding of Islamic values in the era of social media. The main contribution of this study lies in its comparative, empirically grounded approach that integrates both qualitative and quantitative analyses to map the effectiveness of da'wah across platforms, providing a strategic foundation for developing adaptive and contextual digital da'wah for young Muslim audiences.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [4] [BEOL Ferroelectric Compute-in-Memory Ising Machine for Simulated Bifurcation](https://arxiv.org/abs/2512.17165)
*Yu Qian,Alptekin Vardar,Konrad Seidel,David Lehninger,Maximilian Lederer,Zhiguo Shi,Cheng Zhuo,Kai Ni,Thomas Kämpfe,Xunzhao Yin*

Main category: cs.ET

TL;DR: 提出基于铁电场效应晶体管（FeFET）的存内计算伊辛框架，通过算法-硬件协同设计高效解决大规模组合优化问题，相比GPU实现获得175.9倍加速。


<details>
  <summary>Details</summary>
Motivation: 组合优化问题的NP-hard特性使得在传统冯·诺依曼架构上求解效率低下。现有伊辛机存在初始化差、算法性能与硬件效率之间的权衡问题，需要更好的硬件友好型解决方案。

Method: 采用两步算法流程：1) 注意力启发的初始化，利用全局自旋拓扑减少迭代次数；2) 专门为存内计算设计的轻量级模拟分岔算法。使用32x256 FeFET存内计算芯片原生加速核心向量-矩阵和向量-矩阵-向量运算。

Result: 在多达100,000个节点的Max-Cut问题上，相比基于GPU的模拟分岔实现获得高达175.9倍的加速，同时保持更优的解质量。初始化步骤减少所需迭代次数达80%。

Conclusion: 通过算法与硬件的紧密协同设计，基于FeFET的存内计算伊辛框架能够高效解决大规模组合优化问题，在速度和求解质量方面均优于现有方法。

Abstract: Computationally hard combinatorial optimization problems are pervasive in science and engineering, yet their NP-hard nature renders them increasingly inefficient to solve on conventional von Neumann architectures as problem size grows. Ising machines implemented using dynamical, digital and compute-in-memory (CiM) approaches offer a promising alternative, but often suffer from poor initialization and a fundamental trade-off between algorithmic performance and hardware efficiency. Hardware-friendly schemes such as simulated annealing converge slowly, whereas faster algorithms, including simulated bifurcation, are difficult to implement efficiently in CiM hardware, limiting both convergence speed and solution quality. To address these limitations, here we present a ferroelectric field-effect transistor (FeFET)-based CiM Ising framework that tightly co-designs algorithms and hardware to efficiently solve large-scale combinatorial optimization problems. The proposed approach employs a two-step algorithmic flow: an attention-inspired initialization that exploits global spin topology and reduces the required iterations by up to 80%, followed by a lightweight simulated bifurcation algorithm specifically tailored for CiM implementation. To natively accelerate the core vector-matrix and vector-matrix-vector operations in both steps, we fabricate a 32x256 FeFET CiM chip using ferroelectric capacitors integrated at the back end of line of a 180-nm CMOS platform. Across Max-Cut instances with up to 100,000 nodes, the proposed hardware-software co-designed solver achieves up to a 175.9x speedup over a GPU-based simulated bifurcation implementation while consistently delivering superior solution quality.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [5] [Principled Identification of Structural Dynamic Models](https://arxiv.org/abs/2512.17005)
*Neville Francis,Peter Reinhard Hansen,Chen Tong*

Main category: econ.EM

TL;DR: 提出OASIS识别方案，通过最大化相关性目标函数选择正交旋转，使结构冲击与简化形式创新最佳对齐


<details>
  <summary>Details</summary>
Motivation: 传统结构动态模型识别依赖限制条件，本文从优化目标函数的新视角重新审视识别问题

Method: 提出OASIS（Order- and Scale-Invariant Identification Scheme）方案，通过最大化结构冲击与简化形式创新之间的相关性目标函数来选择正交旋转

Result: 重新分析22个已发表的SVAR研究，发现结构冲击与简化形式创新之间的相关性普遍较高

Conclusion: 优化目标函数为结构模型识别提供了新视角，OASIS方案有效且与传统Cholesky识别有理论联系

Abstract: We take a new perspective on identification in structural dynamic models: rather than imposing restrictions, we optimize an objective. This provides new theoretical insights into traditional Cholesky identification. A correlation-maximizing objective yields an Order- and Scale-Invariant Identification Scheme (OASIS) that selects the orthogonal rotation that best aligns structural shocks with their reduced-form innovations. We revisit a large number of SVAR studies and find, across 22 published SVARs, that the correlations between structural and reduced-form shocks are generally high.

</details>


### [6] [Back to Feedback: Dynamics and Heterogeneity in Panel Data](https://arxiv.org/abs/2512.17576)
*Stephane Bonhomme*

Main category: econ.EM

TL;DR: 本文主张在面板数据分析中需要放松严格外生性假设，采用序贯外生性方法，允许过去结果对未来协变量或处理的反馈效应，并综述了从线性模型到非线性模型及网络扩展的相关方法。


<details>
  <summary>Details</summary>
Motivation: 许多面板数据估计方法依赖严格外生性假设，但这在实证研究中过于严格，限制了方法的适用性。实际应用中经常存在过去结果对未来协变量或处理的反馈效应，因此需要放松这一假设。

Method: 综述了面板数据分析中放松严格外生性假设的方法：1) 回顾经典线性模型中的序贯外生性方法；2) 描述允许系数异质性的反馈模型方法；3) 综述非线性面板数据模型中的序贯外生性方法；4) 探讨向网络设置的扩展。

Result: 通过采用序贯外生性方法，可以更灵活地处理面板数据中的反馈效应，提高估计的可靠性。这些方法能够处理更广泛的实证场景，包括存在动态反馈机制的情况。

Conclusion: 可信的实证研究需要放松严格外生性假设，采用序贯外生性方法。未来研究应继续发展非线性模型和网络设置中的序贯外生性方法，以适应更复杂的实证环境。

Abstract: Many popular estimation methods in panel data rely on the assumption that the covariates of interest are strictly exogenous. However, this assumption is empirically restrictive in a wide range of settings. In this paper I argue that credible empirical work requires meaningfully relaxing strict exogeneity assumptions. Econometricians have developed methods that allow for sequential exogeneity, which in contrast with strict exogeneity allows for the presence of feedback from past outcomes to future covariates or treatments. I review some of the classic work on linear models with constant coefficients, and then describe some approaches that allow for coefficient heterogeneity in models with feedback. Finally, in the last two parts of the paper I review recent work that allows for sequential exogeneity in nonlinear panel data models, and mention possible extensions to network settings.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [Navigating Taxonomic Expansions of Entity Sets Driven by Knowledge Bases](https://arxiv.org/abs/2512.16953)
*Pietro Cofone,Giovanni Amendola,Marco Manna,Aldo Ricioppo*

Main category: cs.AI

TL;DR: 本文提出了一种基于逻辑框架的实体集扩展图高效推理方法，支持局部增量导航而无需构建完整图结构。


<details>
  <summary>Details</summary>
Motivation: 传统的线性实体集扩展方法无法揭示知识资源中丰富的分类结构，而现有的扩展图框架虽然能支持分类扩展，但完整构建可能不切实际。

Method: 形式化推理任务来检查两个元组是否属于扩展图中可比较、不可比较或相同的节点，在限制输入或实体描述的现实假设下实现高效实现。

Result: 在现实假设下（如限制输入或实体描述），这些推理任务可以高效实现，支持扩展图的局部增量导航。

Conclusion: 提出的方法使得无需构建完整扩展图即可实现实体集扩展图的实用导航，支持实际应用中的分类结构探索。

Abstract: Recognizing similarities among entities is central to both human cognition and computational intelligence. Within this broader landscape, Entity Set Expansion is one prominent task aimed at taking an initial set of (tuples of) entities and identifying additional ones that share relevant semantic properties with the former -- potentially repeating the process to form increasingly broader sets. However, this ``linear'' approach does not unveil the richer ``taxonomic'' structures present in knowledge resources. A recent logic-based framework introduces the notion of an expansion graph: a rooted directed acyclic graph where each node represents a semantic generalization labeled by a logical formula, and edges encode strict semantic inclusion. This structure supports taxonomic expansions of entity sets driven by knowledge bases. Yet, the potentially large size of such graphs may make full materialization impractical in real-world scenarios. To overcome this, we formalize reasoning tasks that check whether two tuples belong to comparable, incomparable, or the same nodes in the graph. Our results show that, under realistic assumptions -- such as bounding the input or limiting entity descriptions -- these tasks can be implemented efficiently. This enables local, incremental navigation of expansion graphs, supporting practical applications without requiring full graph construction.

</details>


### [8] [Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows](https://arxiv.org/abs/2512.16969)
*Wanghan Xu,Yuhao Zhou,Yifan Zhou,Qinglong Cao,Shuo Li,Jia Bu,Bo Liu,Yixin Chen,Xuming He,Xiangyu Zhao,Xiang Zhuang,Fengxiang Wang,Zhiwang Zhou,Qiantai Feng,Wenxuan Huang,Jiaqi Wei,Hao Wu,Yuejin Yang,Guangshuai Wang,Sheng Xu,Ziyan Huang,Xinyao Liu,Jiyao Liu,Cheng Tang,Wei Li,Ying Chen,Junzhi Ning,Pengfei Jiang,Chenglong Ma,Ye Du,Changkai Ji,Huihui Xu,Ming Hu,Jiangbin Zheng,Xin Chen,Yucheng Wu,Feifei Jiang,Xi Chen,Xiangru Tang,Yuchen Fu,Yingzhou Lu,Yuanyuan Zhang,Lihao Sun,Chengbo Li,Jinzhe Ma,Wanhao Liu,Yating Liu,Kuo-Cheng Wu,Shengdu Chai,Yizhou Wang,Ouwen Zhangjin,Chen Tang,Shufei Zhang,Wenbo Cao,Junjie Ren,Taoyong Cui,Zhouheng Yao,Juntao Deng,Yijie Sun,Feng Liu,Wangxu Wei,Jingyi Xu,Zhangrui Li,Junchao Gong,Zijie Guo,Zhiyu Yao,Zaoyu Chen,Tianhao Peng,Fangchen Yu,Bo Zhang,Dongzhan Zhou,Shixiang Tang,Jiaheng Liu,Fenghua Ling,Yan Lu,Yuchen Ren,Ben Fei,Zhen Zhao,Xinyu Gu,Rui Su,Xiao-Ming Wu,Weikang Si,Yang Liu,Hao Chen,Xiangchao Yan,Xue Yang,Junchi Yan,Jiamin Wu,Qihao Zheng,Chenhui Li,Zhiqiang Gao,Hao Kong,Junjun He,Mao Su,Tianfan Fu,Peng Ye,Chunfeng Song,Nanqing Dong,Yuqiang Li,Huazhu Fu,Siqi Sun,Lijing Cheng,Jintai Lin,Wanli Ouyang,Bowen Zhou,Wenlong Zhang,Lei Bai*

Main category: cs.AI

TL;DR: 该论文提出了科学通用智能(SGI)的操作化定义，基于实用探究模型(PIM)，并构建了SGI-Bench基准，包含1000多个跨学科样本，用于评估大语言模型在科学研究任务上的表现。结果显示现有模型在深度研究、想法可行性、实验执行等方面存在显著差距，并提出了测试时强化学习(TTRL)方法来提升假设新颖性。


<details>
  <summary>Details</summary>
Motivation: 尽管科学AI有所进展，但缺乏一个统一的科学通用智能(SGI)框架，即能够自主构思、调查和跨科学领域推理的能力。需要建立一个操作化的SGI定义和评估基准，以推动AI系统真正参与科学发现。

Method: 1. 基于实用探究模型(PIM: 深思、构思、行动、感知)定义SGI；2. 通过四个科学家对齐任务(深度研究、想法生成、干/湿实验、实验推理)操作化SGI；3. 构建SGI-Bench基准，包含1000多个专家策划的跨学科样本，灵感来自《科学》杂志的125个重大问题；4. 引入测试时强化学习(TTRL)，在推理时优化检索增强的新颖性奖励。

Result: 评估结果显示：深度研究的精确匹配率低(10-20%)，尽管步骤层面有对齐；生成的想法缺乏可行性和细节；干实验代码可执行性高但执行结果准确性低；湿实验协议序列保真度低；多模态比较推理存在持续挑战。TTRL方法能在没有参考答案的情况下提升假设新颖性。

Conclusion: 基于PIM的SGI定义、以工作流程为中心的基准和实证洞察，为真正参与科学发现的AI系统奠定了基础。研究揭示了当前大语言模型在科学发现任务上的局限性，并提出了改进方向。

Abstract: Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.

</details>


### [9] [PAACE: A Plan-Aware Automated Agent Context Engineering Framework](https://arxiv.org/abs/2512.16970)
*Kamer Ali Yuksel*

Main category: cs.AI

TL;DR: PAACE是一个用于优化LLM智能体上下文管理的统一框架，通过计划感知的自动上下文工程，在保持性能的同时显著降低推理成本。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在多步骤工作流程中会产生快速扩展的上下文，现有方法忽略了多步骤、计划感知的特性，导致注意力稀释和推理成本增加。

Method: PAACE框架包含：1) PAACE-Syn：生成带有压缩监督标注的合成智能体工作流程；2) PAACE-FT：从成功教师演示中训练的蒸馏、计划感知压缩器家族，采用next-k-task相关性建模、计划结构分析、指令协同细化和函数保留压缩。

Result: 在AppWorld、OfficeBench和8-Objective QA等长视野基准测试中，PAACE持续提高智能体正确性，同时大幅降低上下文负载。PAACE-FT保留了教师模型97%的性能，推理成本降低超过一个数量级。

Conclusion: PAACE通过计划感知的自动上下文工程，为LLM智能体提供了有效的上下文优化解决方案，能够在保持性能的同时显著降低推理成本，实现实际部署。

Abstract: Large Language Model (LLM) agents are increasingly deployed in complex, multi-step workflows involving planning, tool use, reflection, and interaction with external knowledge systems. These workflows generate rapidly expanding contexts that must be curated, transformed, and compressed to maintain fidelity, avoid attention dilution, and reduce inference cost. Prior work on summarization and query-aware compression largely ignores the multi-step, plan-aware nature of agentic reasoning. In this work, we introduce PAACE (Plan-Aware Automated Context Engineering), a unified framework for optimizing the evolving state of LLM agents through next-k-task relevance modeling, plan-structure analysis, instruction co-refinement, and function-preserving compression. PAACE comprises (1) PAACE-Syn, a large-scale generator of synthetic agent workflows annotated with stepwise compression supervision, and (2) PAACE-FT, a family of distilled, plan-aware compressors trained from successful teacher demonstrations. Experiments on long-horizon benchmarks (AppWorld, OfficeBench, and 8-Objective QA) demonstrate that PAACE consistently improves agent correctness while substantially reducing context load. On AppWorld, PAACE achieves higher accuracy than all baselines while lowering peak context and cumulative dependency. On OfficeBench and multi-hop QA, PAACE improves both accuracy and F1, achieving fewer steps, lower peak tokens, and reduced attention dependency. Distilled PAACE-FT retains 97 percent of the teacher's performance while reducing inference cost by over an order of magnitude, enabling practical deployment of plan-aware compression with compact models.

</details>


### [10] [Security Risks of Agentic Vehicles: A Systematic Analysis of Cognitive and Cross-Layer Threats](https://arxiv.org/abs/2512.17041)
*Ali Eslami,Jiangbo Yu*

Main category: cs.AI

TL;DR: 本文提出了首个针对智能体车辆（AgVs）的安全风险分析框架，研究了智能体AI层和跨层安全威胁，包括OWASP式风险和来自感知、控制等上游层的攻击。


<details>
  <summary>Details</summary>
Motivation: 随着智能体AI在手动驾驶和自动驾驶车辆中的应用日益增多，形成了智能体车辆（AgVs）的概念。然而，现有的安全框架（如OWASP智能体AI安全风险）并非为车辆等安全关键网络物理系统设计，也未考虑与感知、通信和控制等其他层的交互。

Method: 通过引入基于角色的智能体车辆架构（包括个人代理和驾驶策略代理），研究智能体AI层和跨层风险。使用严重性矩阵和攻击链分析来展示小规模失真如何升级为行为错位或不安全行为。

Result: 开发了一个结构化框架，首次为分析当前和新兴车辆平台中智能体AI的安全风险提供了基础。展示了跨层攻击如何影响智能体层，以及风险如何在不同车辆类型中升级。

Conclusion: 该框架为智能体车辆的安全风险分析提供了首个结构化基础，强调了考虑跨层交互的重要性，并为未来车辆平台的安全设计提供了指导。

Abstract: Agentic AI is increasingly being explored and introduced in both manually driven and autonomous vehicles, leading to the notion of Agentic Vehicles (AgVs), with capabilities such as memory-based personalization, goal interpretation, strategic reasoning, and tool-mediated assistance. While frameworks such as the OWASP Agentic AI Security Risks highlight vulnerabilities in reasoning-driven AI systems, they are not designed for safety-critical cyber-physical platforms such as vehicles, nor do they account for interactions with other layers such as perception, communication, and control layers. This paper investigates security threats in AgVs, including OWASP-style risks and cyber-attacks from other layers affecting the agentic layer. By introducing a role-based architecture for agentic vehicles, consisting of a Personal Agent and a Driving Strategy Agent, we will investigate vulnerabilities in both agentic AI layer and cross-layer risks, including risks originating from upstream layers (e.g., perception layer, control layer, etc.). A severity matrix and attack-chain analysis illustrate how small distortions can escalate into misaligned or unsafe behavior in both human-driven and autonomous vehicles. The resulting framework provides the first structured foundation for analyzing security risks of agentic AI in both current and emerging vehicle platforms.

</details>


### [11] [UniRel-R1: RL-tuned LLM Reasoning for Knowledge Graph Relational Question Answering](https://arxiv.org/abs/2512.17043)
*Yinxu Tang,Chengsong Huang,Jiaxin Huang,William Yeoh*

Main category: cs.AI

TL;DR: 提出UniRel-R1框架，用于关系中心的知识图谱问答，目标是返回捕捉实体间语义连接的子图而非单个实体


<details>
  <summary>Details</summary>
Motivation: 传统KGQA主要关注实体中心查询，返回单个答案实体，但现实世界查询通常是关系性的，需要理解实体间的关联方式

Method: 提出统一框架UniRel-R1，集成子图选择、多阶段图剪枝，以及通过强化学习微调的LLM，奖励函数鼓励紧凑、具体的子图，包含更多信息性关系和低度中间实体

Result: 大量实验表明，UniRel-R1在连接性和奖励方面相比Vanilla基线取得显著提升，并能有效泛化到未见过的实体和关系

Conclusion: 关系中心KGQA是实体中心KGQA的补充设置，UniRel-R1框架能有效识别独特且信息丰富的答案子图

Abstract: Knowledge Graph Question Answering (KGQA) has traditionally focused on entity-centric queries that return a single answer entity. However, real-world queries are often relational, seeking to understand how entities are associated. In this work, we introduce relation-centric KGQA, a complementary setting where the answer is a subgraph capturing the semantic connections among entities rather than an individual entity. The main challenge lies in the abundance of candidate subgraphs, where trivial or overly common connections often obscure the identification of unique and informative answers. To tackle this, we propose UniRel-R1, a unified framework that integrates subgraph selection, multi-stage graph pruning, and an LLM fine-tuned with reinforcement learning. The reward function is designed to encourage compact and specific subgraphs with more informative relations and lower-degree intermediate entities. Extensive experiments show that UniRel-R1 achieves significant gains in connectivity and reward over Vanilla baselines and generalizes effectively to unseen entities and relations.

</details>


### [12] [Realistic threat perception drives intergroup conflict: A causal, dynamic analysis using generative-agent simulations](https://arxiv.org/abs/2512.17066)
*Suhaib Abdurahman,Farzan Karimi-Malekabadi,Chenxiao Yu,Nour S. Kteily,Morteza Dehghani*

Main category: cs.AI

TL;DR: 使用LLM驱动的智能体在虚拟社会中模拟威胁与冲突，发现现实威胁直接增加敌意，而象征性威胁主要通过内群体偏见间接影响，且只在现实威胁缺失时增加敌意。


<details>
  <summary>Details</summary>
Motivation: 人类冲突常归因于物质条件和象征价值的威胁，但两者如何相互作用以及哪个占主导地位尚不清楚。研究面临因果控制弱、伦理约束和时间数据稀缺等限制。

Method: 使用大型语言模型驱动的智能体在虚拟社会中模拟，独立操纵现实威胁和象征性威胁，跟踪行动、语言和态度。通过表征分析验证LLM编码的威胁状态，并操纵这些状态来因果性地改变行为。

Result: LLM编码了现实威胁、象征性威胁和敌意作为不同的内部状态；现实威胁直接增加敌意，象征性威胁效应较弱，完全通过内群体偏见中介，且只在现实威胁缺失时增加敌意；非敌对群体接触缓冲冲突升级，结构不对称使敌意集中在多数群体中。

Conclusion: 通过LLM智能体模拟为威胁驱动的冲突提供了因果解释，揭示了现实威胁和象征性威胁的不同作用机制，为理解人类冲突提供了新视角和方法。

Abstract: Human conflict is often attributed to threats against material conditions and symbolic values, yet it remains unclear how they interact and which dominates. Progress is limited by weak causal control, ethical constraints, and scarce temporal data. We address these barriers using simulations of large language model (LLM)-driven agents in virtual societies, independently varying realistic and symbolic threat while tracking actions, language, and attitudes. Representational analyses show that the underlying LLM encodes realistic threat, symbolic threat, and hostility as distinct internal states, that our manipulations map onto them, and that steering these states causally shifts behavior. Our simulations provide a causal account of threat-driven conflict over time: realistic threat directly increases hostility, whereas symbolic threat effects are weaker, fully mediated by ingroup bias, and increase hostility only when realistic threat is absent. Non-hostile intergroup contact buffers escalation, and structural asymmetries concentrate hostility among majority groups.

</details>


### [13] [Value Under Ignorance in Universal Artificial Intelligence](https://arxiv.org/abs/2512.17086)
*Cole Wyeth,Marcus Hutter*

Main category: cs.AI

TL;DR: 将AIXI强化学习智能体推广到更广泛的效用函数类别，处理信念分布中假设只预测有限历史前缀的模糊性，探讨半测度损失的死亡解释与不精确概率解释，研究Choquet积分的计算性


<details>
  <summary>Details</summary>
Motivation: AIXI智能体需要处理信念分布中假设只预测有限历史前缀的模糊性问题，这通常被解释为"死亡概率"（半测度损失）。作者认为也可以将信念分布视为不精确概率分布，其中半测度损失代表完全无知，这促使研究不精确概率理论中的Choquet积分

Method: 将AIXI推广到更广泛的效用函数类别，使用不精确概率理论中的Choquet积分计算期望效用，分析其计算性水平，并与标准的递归值函数进行比较

Result: 标准递归值函数可以作为Choquet积分的特例恢复，但在死亡解释下最一般的期望效用不能表征为这样的Choquet积分，同时研究了这些方法的计算性水平

Conclusion: 通过将AIXI推广到更广泛的效用函数并引入不精确概率理论，为处理信念分布中有限预测假设的模糊性提供了新的理论框架，但死亡解释下的最一般期望效用需要超出Choquet积分的表征

Abstract: We generalize the AIXI reinforcement learning agent to admit a wider class of utility functions. Assigning a utility to each possible interaction history forces us to confront the ambiguity that some hypotheses in the agent's belief distribution only predict a finite prefix of the history, which is sometimes interpreted as implying a chance of death equal to a quantity called the semimeasure loss. This death interpretation suggests one way to assign utilities to such history prefixes. We argue that it is as natural to view the belief distributions as imprecise probability distributions, with the semimeasure loss as total ignorance. This motivates us to consider the consequences of computing expected utilities with Choquet integrals from imprecise probability theory, including an investigation of their computability level. We recover the standard recursive value function as a special case. However, our most general expected utilities under the death interpretation cannot be characterized as such Choquet integrals.

</details>


### [14] [A Solver-in-the-Loop Framework for Improving LLMs on Answer Set Programming for Logic Puzzle Solving](https://arxiv.org/abs/2512.17093)
*Timo Pierre Schrader,Lukas Lange,Tobias Kaminski,Simon Razniewski,Annemarie Friedrich*

Main category: cs.AI

TL;DR: 提出一种ASP求解器在环的方法，通过求解器引导的指令微调来提升LLM生成答案集编程代码的能力，仅需自然语言问题描述和解决方案即可训练。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在通用编程语言上表现良好，但在领域特定语言（如答案集编程ASP）的代码生成方面仍面临挑战，主要原因是预训练阶段接触的ASP示例有限。

Method: 采用ASP求解器在环的方法，通过求解器引导的指令微调：1）从LLM采样ASP语句作为程序延续；2）利用ASP声明式编程的特性，根据求解器反馈将采样分为接受和拒绝实例；3）对筛选数据进行监督微调；4）使用求解器引导的搜索（包括best-of-N采样）进一步提高鲁棒性。

Result: 实验表明，该方法在两种不同的提示设置和两个数据集上都取得了持续改进。

Conclusion: 提出的ASP求解器在环方法有效提升了LLM生成ASP代码的能力，为领域特定语言的代码生成问题提供了新的解决方案。

Abstract: The rise of large language models (LLMs) has sparked interest in coding assistants. While general-purpose programming languages are well supported, generating code for domain-specific languages remains a challenging problem for LLMs. In this paper, we focus on the LLM-based generation of code for Answer Set Programming (ASP), a particularly effective approach for finding solutions to combinatorial search problems. The effectiveness of LLMs in ASP code generation is currently hindered by the limited number of examples seen during their initial pre-training phase.
  In this paper, we introduce a novel ASP-solver-in-the-loop approach for solver-guided instruction-tuning of LLMs to addressing the highly complex semantic parsing task inherent in ASP code generation. Our method only requires problem specifications in natural language and their solutions. Specifically, we sample ASP statements for program continuations from LLMs for unriddling logic puzzles. Leveraging the special property of declarative ASP programming that partial encodings increasingly narrow down the solution space, we categorize them into chosen and rejected instances based on solver feedback. We then apply supervised fine-tuning to train LLMs on the curated data and further improve robustness using a solver-guided search that includes best-of-N sampling. Our experiments demonstrate consistent improvements in two distinct prompting settings on two datasets.

</details>


### [15] [Reinforcement Learning for Self-Improving Agent with Skill Library](https://arxiv.org/abs/2512.17102)
*Jiongxiao Wang,Qiaojing Yan,Yawei Wang,Yijun Tian,Soumya Smruti Mishra,Zhichao Xu,Megha Gandhi,Panpan Xu,Lin Lee Cheong*

Main category: cs.AI

TL;DR: SAGE：基于强化学习的技能库框架，通过序列化部署和技能集成奖励提升LLM智能体的自我改进能力


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能体在新环境中难以持续改进和适应，现有技能库方法主要依赖提示工程，实现一致性挑战大

Method: 提出SAGE强化学习框架，包含序列化部署（在相似任务链中迭代部署智能体）和技能集成奖励（补充基于结果的原始奖励）

Result: 在AppWorld实验中，SAGE应用于有专家经验的监督微调模型，场景目标完成率提高8.9%，交互步骤减少26%，生成token减少59%

Conclusion: SAGE框架通过强化学习有效整合技能库，显著提升LLM智能体的自我改进能力和任务执行效率

Abstract: Large Language Model (LLM)-based agents have demonstrated remarkable capabilities in complex reasoning and multi-turn interactions but struggle to continuously improve and adapt when deployed in new environments. One promising approach is implementing skill libraries that allow agents to learn, validate, and apply new skills. However, current skill library approaches rely primarily on LLM prompting, making consistent skill library implementation challenging. To overcome these challenges, we propose a Reinforcement Learning (RL)-based approach to enhance agents' self-improvement capabilities with a skill library. Specifically, we introduce Skill Augmented GRPO for self-Evolution (SAGE), a novel RL framework that systematically incorporates skills into learning. The framework's key component, Sequential Rollout, iteratively deploys agents across a chain of similar tasks for each rollout. As agents navigate through the task chain, skills generated from previous tasks accumulate in the library and become available for subsequent tasks. Additionally, the framework enhances skill generation and utilization through a Skill-integrated Reward that complements the original outcome-based rewards. Experimental results on AppWorld demonstrate that SAGE, when applied to supervised-finetuned model with expert experience, achieves 8.9% higher Scenario Goal Completion while requiring 26% fewer interaction steps and generating 59% fewer tokens, substantially outperforming existing approaches in both accuracy and efficiency.

</details>


### [16] [Solomonoff-Inspired Hypothesis Ranking with LLMs for Prediction Under Uncertainty](https://arxiv.org/abs/2512.17145)
*Josh Barber,Rourke Young,Cameron Coombe,Will Browne*

Main category: cs.AI

TL;DR: 提出一种受所罗门诺夫启发的LLM假设加权方法，通过简洁性和预测拟合度评估多个候选方案，在不确定性下实现更平衡的概率分布


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理稀疏数据的不确定性推理时，难以在准确性和简洁性之间取得平衡，特别是在评估多个候选解决方案时存在困难

Method: 基于所罗门诺夫思想的方法，对LLM生成的假设按简洁性和预测拟合度进行加权，应用于Mini-ARC基准任务，为每个单元预测生成所罗门诺夫加权混合

Result: 相比贝叶斯模型平均，所罗门诺夫评分将概率更均匀地分布在竞争假设之间，产生保守、不确定性感知的输出，即使假设有噪声或部分错误

Conclusion: 算法信息论先验对于可解释、可靠的多假设不确定性推理具有重要价值，特别是在需要系统泛化的现实任务中

Abstract: Reasoning under uncertainty is a key challenge in AI, especially for real-world tasks, where problems with sparse data demands systematic generalisation. Existing approaches struggle to balance accuracy and simplicity when evaluating multiple candidate solutions. We propose a Solomonoff-inspired method that weights LLM-generated hypotheses by simplicity and predictive fit. Applied to benchmark (Mini-ARC) tasks, our method produces Solomonoff-weighted mixtures for per-cell predictions, yielding conservative, uncertainty-aware outputs even when hypotheses are noisy or partially incorrect. Compared to Bayesian Model Averaging (BMA), Solomonoff scoring spreads probability more evenly across competing hypotheses, while BMA concentrates weight on the most likely but potentially flawed candidates. Across tasks, this highlights the value of algorithmic information-theoretic priors for interpretable, reliable multi-hypothesis reasoning under uncertainty.

</details>


### [17] [MMRAG-RFT: Two-stage Reinforcement Fine-tuning for Explainable Multi-modal Retrieval-augmented Generation](https://arxiv.org/abs/2512.17194)
*Shengwei Zhao,Jingwen Yao,Sitong Wei,Linhai Xu,Yuying Liu,Dong Zhang,Zhiqiang Tian,Shaoyi Du*

Main category: cs.AI

TL;DR: 提出基于强化学习的可解释多模态检索增强生成框架，通过两阶段强化微调提升多模态大语言模型的推理能力，在WebQA和MultimodalQA数据集上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有MMRAG方法缺乏对检索和生成过程推理逻辑的解释，限制了结果的可解释性。需要增强多模态大语言模型的推理能力以实现可解释的多模态检索增强生成。

Method: 采用两阶段强化微调框架：第一阶段使用基于规则的强化微调对多模态文档进行粗粒度点式排序，过滤显著不相关文档；第二阶段使用基于推理的强化微调联合优化细粒度列表排序和答案生成，引导模型输出可解释的推理逻辑。

Result: 在WebQA和MultimodalQA两个多模态检索增强生成基准数据集上取得了最先进的结果，并通过全面的消融实验验证了方法的有效性。

Conclusion: 通过引入强化学习到多模态检索增强生成中，提出的两阶段强化微调框架成功增强了多模态大语言模型的推理能力，实现了可解释的多模态检索增强生成，并在基准测试中表现出优越性能。

Abstract: Multi-modal Retrieval-Augmented Generation (MMRAG) enables highly credible generation by integrating external multi-modal knowledge, thus demonstrating impressive performance in complex multi-modal scenarios. However, existing MMRAG methods fail to clarify the reasoning logic behind retrieval and response generation, which limits the explainability of the results. To address this gap, we propose to introduce reinforcement learning into multi-modal retrieval-augmented generation, enhancing the reasoning capabilities of multi-modal large language models through a two-stage reinforcement fine-tuning framework to achieve explainable multi-modal retrieval-augmented generation. Specifically, in the first stage, rule-based reinforcement fine-tuning is employed to perform coarse-grained point-wise ranking of multi-modal documents, effectively filtering out those that are significantly irrelevant. In the second stage, reasoning-based reinforcement fine-tuning is utilized to jointly optimize fine-grained list-wise ranking and answer generation, guiding multi-modal large language models to output explainable reasoning logic in the MMRAG process. Our method achieves state-of-the-art results on WebQA and MultimodalQA, two benchmark datasets for multi-modal retrieval-augmented generation, and its effectiveness is validated through comprehensive ablation experiments.

</details>


### [18] [UmniBench: Unified Understand and Generation Model Oriented Omni-dimensional Benchmark](https://arxiv.org/abs/2512.17196)
*Kai Liu,Leyang Chen,Wenbo Li,Zhikai Chen,Zhixin Wang,Renjing Pei,Linghe Kong,Yulun Zhang*

Main category: cs.AI

TL;DR: UmniBench：一个针对统一多模态模型的综合评估基准，能够在一个评估过程中同时测试理解、生成和编辑能力，覆盖13个主要领域和200多个概念。


<details>
  <summary>Details</summary>
Motivation: 当前对统一多模态模型的评估是分离的，分别评估理解和生成能力，缺乏综合评估框架。需要一种能够全面评估UMMs各方面能力的基准。

Method: 1）设计能够在一个评估过程中同时测试理解、生成和编辑能力的基准；2）利用UMM自身的理解能力来评估其生成和编辑能力；3）基于人工检查的提示和问答对；4）覆盖13个主要领域和200多个概念。

Result: 基于UmniBench对24个流行模型进行了基准测试，包括统一多模态模型和单能力大模型。该基准提供了更全面客观的模型评估视角。

Conclusion: UmniBench为统一多模态模型提供了全面的评估框架，能够在一个过程中同时评估理解、生成和编辑能力，为社区模型性能改进提供支持。

Abstract: Unifying multimodal understanding and generation has shown impressive capabilities in cutting-edge proprietary systems. However, evaluations of unified multimodal models (UMMs) remain decoupled, assessing their understanding and generation abilities separately with corresponding datasets. To address this, we propose UmniBench, a benchmark tailored for UMMs with omni-dimensional evaluation. First, UmniBench can assess the understanding, generation, and editing ability within a single evaluation process. Based on human-examined prompts and QA pairs, UmniBench leverages UMM itself to evaluate its generation and editing ability with its understanding ability. This simple but effective paradigm allows comprehensive evaluation of UMMs. Second, UmniBench covers 13 major domains and more than 200 concepts, ensuring a thorough inspection of UMMs. Moreover, UmniBench can also decouple and separately evaluate understanding, generation, and editing abilities, providing a fine-grained assessment. Based on UmniBench, we benchmark 24 popular models, including both UMMs and single-ability large models. We hope this benchmark provides a more comprehensive and objective view of unified models and logistical support for improving the performance of the community model.

</details>


### [19] [Accelerating Multi-modal LLM Gaming Performance via Input Prediction and Mishit Correction](https://arxiv.org/abs/2512.17250)
*Ziyang Lin,Zixuan Sun,Sanhorn Chen,Xiaoyang Chen,Roy Zhao*

Main category: cs.AI

TL;DR: 提出推测-校正框架，将推测执行思想应用于基于模型的实时控制，通过动作队列和轻量级校正器减少规划延迟，在保持控制性能的同时提升25%的端到端步延迟。


<details>
  <summary>Details</summary>
Motivation: 实时顺序控制代理常受推理延迟瓶颈，即使适度的每步规划延迟也会导致控制不稳定和性能下降。需要一种方法在保持控制质量的同时减少规划频率。

Method: 采用推测-校正框架，结合TD-MPC2：1) 预训练世界模型和潜在空间MPC规划器生成短时域动作队列和潜在状态预测；2) 新观测到达时测量真实潜在状态与预测状态的失配；3) 小到中度失配时使用轻量级学习校正器对推测动作进行残差更新；4) 大失配时回退到完整重新规划。

Result: 在DMC Humanoid-Walk任务中：规划推理次数从500减少到282，端到端步延迟提升25%，控制性能仅下降7.1%。消融实验显示无校正的推测执行在长时域上不可靠。

Conclusion: 推测-校正框架能有效减少实时控制的规划延迟，通过失配感知校正机制在保持鲁棒性的同时显著提升效率，为实时控制系统的延迟优化提供了可行方案。

Abstract: Real-time sequential control agents are often bottlenecked by inference latency. Even modest per-step planning delays can destabilize control and degrade overall performance. We propose a speculation-and-correction framework that adapts the predict-then-verify philosophy of speculative execution to model-based control with TD-MPC2. At each step, a pretrained world model and latent-space MPC planner generate a short-horizon action queue together with predicted latent rollouts, allowing the agent to execute multiple planned actions without immediate replanning. When a new observation arrives, the system measures the mismatch between the encoded real latent state and the queued predicted latent. For small to moderate mismatch, a lightweight learned corrector applies a residual update to the speculative action, distilled offline from a replanning teacher. For large mismatch, the agent safely falls back to full replanning and clears stale action queues. We study both a gated two-tower MLP corrector and a temporal Transformer corrector to address local errors and systematic drift. Experiments on the DMC Humanoid-Walk task show that our method reduces the number of planning inferences from 500 to 282, improves end-to-end step latency by 25 percent, and maintains strong control performance with only a 7.1 percent return reduction. Ablation results demonstrate that speculative execution without correction is unreliable over longer horizons, highlighting the necessity of mismatch-aware correction for robust latency reduction.

</details>


### [20] [ScoutGPT: Capturing Player Impact from Team Action Sequences Using GPT-Based Framework](https://arxiv.org/abs/2512.17266)
*Miru Hong,Minho Lee,Geonhee Jo,Jae-Hee So,Pascal Bauer,Sang-Ki Ko*

Main category: cs.AI

TL;DR: EventGPT：基于GPT架构的球员条件化价值感知下一事件预测模型，用于足球转会分析，通过反事实模拟评估球员在不同战术环境中的适应性


<details>
  <summary>Details</summary>
Motivation: 现有转会评估方法依赖静态统计数据或事后价值模型，无法捕捉球员在新战术环境或不同队友配合下的适应性变化，需要更动态的评估框架

Method: 构建基于GPT风格自回归变换器的球员条件化价值感知下一事件预测模型，将比赛处理为离散标记序列，联合预测下一持球动作的类型、位置、时间和剩余持球价值(rOBV)，通过替换球员嵌入进行反事实模拟

Result: 在五个赛季的英超赛事数据上，EventGPT在下一事件预测准确性和空间精度上优于现有序列基线模型，并通过案例分析展示了在转会分析中的实际应用价值

Conclusion: EventGPT提供了一个原则性的方法来评估转会适配性，能够通过反事实模拟预测球员在不同战术环境中的行为分布和价值变化，为足球转会决策提供更科学的依据

Abstract: Transfers play a pivotal role in shaping a football club's success, yet forecasting whether a transfer will succeed remains difficult due to the strong context-dependence of on-field performance. Existing evaluation practices often rely on static summary statistics or post-hoc value models, which fail to capture how a player's contribution adapts to a new tactical environment or different teammates. To address this gap, we introduce EventGPT, a player-conditioned, value-aware next-event prediction model built on a GPT-style autoregressive transformer. Our model treats match play as a sequence of discrete tokens, jointly learning to predict the next on-ball action's type, location, timing, and its estimated residual On-Ball Value (rOBV) based on the preceding context and player identity. A key contribution of this framework is the ability to perform counterfactual simulations. By substituting learned player embeddings into new event sequences, we can simulate how a player's behavioral distribution and value profile would change when placed in a different team or tactical structure. Evaluated on five seasons of Premier League event data, EventGPT outperforms existing sequence-based baselines in next-event prediction accuracy and spatial precision. Furthermore, we demonstrate the model's practical utility for transfer analysis through case studies-such as comparing striker performance across different systems and identifying stylistic replacements for specific roles-showing that our approach provides a principled method for evaluating transfer fit.

</details>


### [21] [Large Language Models as Pokémon Battle Agents: Strategic Play and Content Generation](https://arxiv.org/abs/2512.17308)
*Daksh Jain,Aarya Jain,Ashutosh Desai,Avyakt Verma,Ishan Bhanuka,Pratik Narang,Dhruv Kumar*

Main category: cs.AI

TL;DR: LLMs可作为宝可梦对战中的动态游戏对手，无需领域特定训练，兼具战术决策和内容生成能力


<details>
  <summary>Details</summary>
Motivation: 宝可梦对战为评估大语言模型提供了独特测试平台，需要推理类型克制、统计权衡和风险评估等反映人类战略思维的能力

Method: 开发基于回合制的宝可梦对战系统，LLM根据战斗状态而非预设逻辑选择招式；框架包含类型效果乘数、基于属性的伤害计算和多宝可梦队伍管理

Result: 通过跨多个模型架构的系统评估，测量胜率、决策延迟、类型对齐准确性和令牌效率，结果表明LLM可作为动态游戏对手

Conclusion: LLM兼具战术推理和内容创造双重能力，既能作为玩家又能作为设计师，对交互娱乐中的程序生成和自适应难度系统有重要意义

Abstract: Strategic decision-making in Pokémon battles presents a unique testbed for evaluating large language models. Pokémon battles demand reasoning about type matchups, statistical trade-offs, and risk assessment, skills that mirror human strategic thinking. This work examines whether Large Language Models (LLMs) can serve as competent battle agents, capable of both making tactically sound decisions and generating novel, balanced game content. We developed a turn-based Pokémon battle system where LLMs select moves based on battle state rather than pre-programmed logic. The framework captures essential Pokémon mechanics: type effectiveness multipliers, stat-based damage calculations, and multi-Pokémon team management. Through systematic evaluation across multiple model architectures we measured win rates, decision latency, type-alignment accuracy, and token efficiency. These results suggest LLMs can function as dynamic game opponents without domain-specific training, offering a practical alternative to reinforcement learning for turn-based strategic games. The dual capability of tactical reasoning and content creation, positions LLMs as both players and designers, with implications for procedural generation and adaptive difficulty systems in interactive entertainment.

</details>


### [22] [Dialectics for Artificial Intelligence](https://arxiv.org/abs/2512.17373)
*Zhengmian Hu*

Main category: cs.AI

TL;DR: 该论文提出了一种基于算法信息论的概念定义，将概念视为与智能体整体经验相关的信息对象，通过可逆一致性关系和超额信息来评估概念的自然性，并建立了概念演化、传输和多智能体对齐的形式化框架。


<details>
  <summary>Details</summary>
Motivation: 人类概念具有流动性（边界会变化、分裂、合并），传统基于标签的概念定义无法捕捉这种动态性。需要一种不依赖于人类监督、能够从原始经验中发现概念的形式化框架，并解决概念在不同智能体间的对齐和传输问题。

Method: 采用算法信息论视角，将概念定义为信息对象，核心是可逆一致性关系（缺失部分可从其他部分恢复）。引入超额信息衡量概念分解的自然性。建立辩证法优化动力学：新信息出现时，竞争概念通过更短的描述来争夺解释权，驱动概念的扩展、收缩、分裂和合并。形式化低成本概念传输，使用小规模种子实现多智能体对齐。

Result: 提出了一个完整的形式化框架：1）基于算法信息论的概念定义；2）可逆一致性关系确保概念不脱离经验；3）超额信息评估概念分解质量；4）辩证法动力学模拟概念演化；5）低成本概念传输协议实现多智能体对齐。

Conclusion: 该框架为AI从原始经验中自主发现概念提供了理论基础，解决了概念流动性、多智能体对齐和通信效率等关键问题，将概念发现、演化和传输转化为可计算的信息优化问题。

Abstract: Can artificial intelligence discover, from raw experience and without human supervision, concepts that humans have discovered? One challenge is that human concepts themselves are fluid: conceptual boundaries can shift, split, and merge as inquiry progresses (e.g., Pluto is no longer considered a planet). To make progress, we need a definition of "concept" that is not merely a dictionary label, but a structure that can be revised, compared, and aligned across agents. We propose an algorithmic-information viewpoint that treats a concept as an information object defined only through its structural relation to an agent's total experience. The core constraint is determination: a set of parts forms a reversible consistency relation if any missing part is recoverable from the others (up to the standard logarithmic slack in Kolmogorov-style identities). This reversibility prevents "concepts" from floating free of experience and turns concept existence into a checkable structural claim. To judge whether a decomposition is natural, we define excess information, measuring the redundancy overhead introduced by splitting experience into multiple separately described parts. On top of these definitions, we formulate dialectics as an optimization dynamics: as new patches of information appear (or become contested), competing concepts bid to explain them via shorter conditional descriptions, driving systematic expansion, contraction, splitting, and merging. Finally, we formalize low-cost concept transmission and multi-agent alignment using small grounds/seeds that allow another agent to reconstruct the same concept under a shared protocol, making communication a concrete compute-bits trade-off.

</details>


### [23] [Translating the Rashomon Effect to Sequential Decision-Making Tasks](https://arxiv.org/abs/2512.17470)
*Dennis Gross,Jørn Eirik Betten,Helge Spieker*

Main category: cs.AI

TL;DR: 将Rashomon效应从分类任务扩展到序列决策领域，定义并验证了具有相同行为但内部结构不同的策略，并展示了Rashomon集合在鲁棒性和验证效率方面的优势。


<details>
  <summary>Details</summary>
Motivation: Rashomon效应在分类任务中已被广泛研究，但在序列决策领域尚未探索。序列决策中的策略验证比分类更复杂，因为随机转移导致单条轨迹无法可靠比较策略行为。

Method: 使用形式化验证方法构建和比较每个策略在环境中的完整概率行为，而不是依赖单条轨迹。通过这种方法验证序列决策中的Rashomon效应，并构建Rashomon集合的集成策略和宽松策略。

Result: 实验证明序列决策中存在Rashomon效应。从Rashomon集合构建的集成策略对分布偏移表现出更强的鲁棒性，而从Rashomon集合派生的宽松策略在保持最优性能的同时减少了验证计算需求。

Conclusion: 成功将Rashomon效应扩展到序列决策领域，展示了Rashomon集合在提升策略鲁棒性和验证效率方面的实用价值，为序列决策中的模型多样性和可靠性提供了新视角。

Abstract: The Rashomon effect describes the phenomenon where multiple models trained on the same data produce identical predictions while differing in which features they rely on internally. This effect has been studied extensively in classification tasks, but not in sequential decision-making, where an agent learns a policy to achieve an objective by taking actions in an environment. In this paper, we translate the Rashomon effect to sequential decision-making. We define it as multiple policies that exhibit identical behavior, visiting the same states and selecting the same actions, while differing in their internal structure, such as feature attributions. Verifying identical behavior in sequential decision-making differs from classification. In classification, predictions can be directly compared to ground-truth labels. In sequential decision-making with stochastic transitions, the same policy may succeed or fail on any single trajectory due to randomness. We address this using formal verification methods that construct and compare the complete probabilistic behavior of each policy in the environment. Our experiments demonstrate that the Rashomon effect exists in sequential decision-making. We further show that ensembles constructed from the Rashomon set exhibit greater robustness to distribution shifts than individual policies. Additionally, permissive policies derived from the Rashomon set reduce computational requirements for verification while maintaining optimal performance.

</details>


### [24] [Towards Explainable Conversational AI for Early Diagnosis with Large Language Models](https://arxiv.org/abs/2512.17559)
*Maliha Tabassum,M Shamim Kaiser*

Main category: cs.AI

TL;DR: 该研究开发了一个基于LLM的诊断聊天机器人，结合GPT-4o、检索增强生成和可解释AI技术，在医疗诊断中实现了90%的准确率和100%的Top-3准确率。


<details>
  <summary>Details</summary>
Motivation: 全球医疗系统面临诊断效率低下、成本上升和专家资源有限等问题，导致治疗延误和不良健康结果。现有AI诊断系统缺乏交互性和透明度，难以在实际临床环境中有效应用。

Method: 使用GPT-4o大型语言模型构建诊断聊天机器人，结合检索增强生成技术和可解释AI方法。系统通过动态对话提取和标准化症状，利用相似性匹配和自适应提问优先诊断，并通过思维链提示提供透明推理。

Result: 与朴素贝叶斯、逻辑回归、SVM、随机森林和KNN等传统机器学习模型相比，LLM系统表现优异，达到90%的准确率和100%的Top-3准确率。

Conclusion: 该LLM驱动的诊断聊天机器人为医疗领域提供了更透明、交互性强且临床相关的AI解决方案，展示了在改善医疗诊断方面的巨大潜力。

Abstract: Healthcare systems around the world are grappling with issues like inefficient diagnostics, rising costs, and limited access to specialists. These problems often lead to delays in treatment and poor health outcomes. Most current AI and deep learning diagnostic systems are not very interactive or transparent, making them less effective in real-world, patient-centered environments. This research introduces a diagnostic chatbot powered by a Large Language Model (LLM), using GPT-4o, Retrieval-Augmented Generation, and explainable AI techniques. The chatbot engages patients in a dynamic conversation, helping to extract and normalize symptoms while prioritizing potential diagnoses through similarity matching and adaptive questioning. With Chain-of-Thought prompting, the system also offers more transparent reasoning behind its diagnoses. When tested against traditional machine learning models like Naive Bayes, Logistic Regression, SVM, Random Forest, and KNN, the LLM-based system delivered impressive results, achieving an accuracy of 90% and Top-3 accuracy of 100%. These findings offer a promising outlook for more transparent, interactive, and clinically relevant AI in healthcare.

</details>


### [25] [About Time: Model-free Reinforcement Learning with Timed Reward Machines](https://arxiv.org/abs/2512.17637)
*Anirban Majumdar,Ritam Raha,Rajarshi Roy,David Parker,Marta Kwiatkowska*

Main category: cs.AI

TL;DR: 提出定时奖励机(TRMs)扩展传统奖励机，加入时间约束，使强化学习能处理时间敏感任务，并开发了结合定时自动机抽象和反事实想象启发式的学习算法。


<details>
  <summary>Details</summary>
Motivation: 传统奖励机无法建模精确的时间约束，限制了在时间敏感应用中的使用。需要更丰富的奖励规范来表达具有时间依赖性的任务要求。

Method: 提出定时奖励机(TRMs)，扩展奖励机加入时间约束；开发基于表格Q学习的模型无关RL框架，结合定时自动机抽象和反事实想象启发式来利用TRM结构改进搜索。

Result: 实验表明算法能学习到满足TRM时间约束的高奖励策略；比较不同TRM语义下的性能，并通过消融研究验证反事实想象的有效性。

Conclusion: TRMs增强了奖励规范的表达能力，使RL能处理时间敏感任务；提出的算法能有效学习满足时间约束的最优策略。

Abstract: Reward specification plays a central role in reinforcement learning (RL), guiding the agent's behavior. To express non-Markovian rewards, formalisms such as reward machines have been introduced to capture dependencies on histories. However, traditional reward machines lack the ability to model precise timing constraints, limiting their use in time-sensitive applications. In this paper, we propose timed reward machines (TRMs), which are an extension of reward machines that incorporate timing constraints into the reward structure. TRMs enable more expressive specifications with tunable reward logic, for example, imposing costs for delays and granting rewards for timely actions. We study model-free RL frameworks (i.e., tabular Q-learning) for learning optimal policies with TRMs under digital and real-time semantics. Our algorithms integrate the TRM into learning via abstractions of timed automata, and employ counterfactual-imagining heuristics that exploit the structure of the TRM to improve the search. Experimentally, we demonstrate that our algorithm learns policies that achieve high rewards while satisfying the timing constraints specified by the TRM on popular RL benchmarks. Moreover, we conduct comparative studies of performance under different TRM semantics, along with ablations that highlight the benefits of counterfactual-imagining.

</details>


### [26] [Humanlike AI Design Increases Anthropomorphism but Yields Divergent Outcomes on Engagement and Trust Globally](https://arxiv.org/abs/2512.17898)
*Robin Schimmelpfennig,Mark Díaz,Vinodkumar Prabhakaran,Aida Davani*

Main category: cs.AI

TL;DR: 研究发现AI拟人化设计对用户信任和参与度的影响并非普遍一致，而是受到文化因素的调节，挑战了现有AI治理的一刀切方法。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统日益模仿人类特征，引发了关于拟人化可能导致错误信任或情感依赖的担忧。然而，现有研究缺乏在真实人机交互中测试拟人化设计与用户行为之间的因果关系，且安全框架主要基于西方人群的理论假设，忽略了全球用户的多样性。

Method: 通过两项大规模跨国实验（N=3,500），涵盖10个不同国家，进行实时开放式AI系统交互。实验测量用户对AI拟人化的评估，并测试拟人化设计杠杆对用户拟人化感知、参与度和信任的因果影响。

Result: 用户评估AI拟人化时更关注交互性线索（如对话流畅度、理解用户视角），而非理论概念（如感知或意识）。拟人化设计能增加用户的拟人化感知，但不会普遍提高行为层面的参与度和信任。文化因素调节了拟人化与行为结果之间的关系，某些设计在某些文化中增加信任（如巴西），在其他文化中可能产生相反效果（如日本）。

Conclusion: 研究挑战了拟人化AI设计必然带来风险的普遍观点，揭示了人机交互的复杂文化中介景观，强调AI治理需要超越一刀切的方法，考虑文化多样性。

Abstract: Over a billion users across the globe interact with AI systems engineered with increasing sophistication to mimic human traits. This shift has triggered urgent debate regarding Anthropomorphism, the attribution of human characteristics to synthetic agents, and its potential to induce misplaced trust or emotional dependency. However, the causal link between more humanlike AI design and subsequent effects on engagement and trust has not been tested in realistic human-AI interactions with a global user pool. Prevailing safety frameworks continue to rely on theoretical assumptions derived from Western populations, overlooking the global diversity of AI users. Here, we address these gaps through two large-scale cross-national experiments (N=3,500) across 10 diverse nations, involving real-time and open-ended interactions with an AI system. We find that when evaluating an AI's human-likeness, users focus less on the kind of theoretical aspects often cited in policy (e.g., sentience or consciousness), but rather applied, interactional cues like conversation flow or understanding the user's perspective. We also experimentally demonstrate that humanlike design levers can causally increase anthropomorphism among users; however, we do not find that humanlike design universally increases behavioral measures for user engagement and trust, as previous theoretical work suggests. Instead, part of the connection between human-likeness and behavioral outcomes is fractured by culture: specific design choices that foster self-reported trust in AI-systems in some populations (e.g., Brazil) may trigger the opposite result in others (e.g., Japan). Our findings challenge prevailing narratives of inherent risk in humanlike AI design. Instead, we identify a nuanced, culturally mediated landscape of human-AI interaction, which demands that we move beyond a one-size-fits-all approach in AI governance.

</details>


### [27] [When Reasoning Meets Its Laws](https://arxiv.org/abs/2512.17901)
*Junyu Zhang,Yifan Sun,Tianang Leng,Jingyan Shen,Liu Ziyin,Paul Pu Liang,Huan Zhang*

Main category: cs.AI

TL;DR: 提出"推理法则"框架，通过计算法则和准确率法则分析大型推理模型的内在推理模式，并开发LoRe-Bench基准测试单调性和组合性，通过微调提升模型推理性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大型推理模型性能优越，但其推理行为常常违反直觉，导致推理能力不足。需要理论框架来形式化期望的推理行为，分析模型的内在推理模式。

Method: 提出推理法则框架，包含计算法则和准确率法则。通过单调性和组合性两个可测量属性验证假设，构建LoRe-Bench基准系统评估这些属性，并开发微调方法强制计算法则的组合性。

Result: 评估显示大多数推理模型具有合理的单调性但缺乏组合性。通过微调强制计算法则组合性后，模型在多个基准测试中的推理性能得到一致提升，并发现不同属性和法则之间的协同效应。

Conclusion: 推理法则框架为分析大型推理模型的内在推理模式提供了统一理论框架，通过强制计算法则组合性可以显著提升模型推理性能，揭示了模型推理行为的系统性规律。

Abstract: Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. We first propose compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, we extend LoRe with a supplementary accuracy law. Since the question complexity is difficult to quantify in practice, we examine these hypotheses by two properties of the laws, monotonicity and compositionality. We therefore introduce LoRe-Bench, a benchmark that systematically measures these two tractable properties for large reasoning models. Evaluation shows that most reasoning models exhibit reasonable monotonicity but lack compositionality. In response, we develop an effective finetuning approach that enforces compute-law compositionality. Extensive empirical studies demonstrate that better compliance with compute laws yields consistently improved reasoning performance on multiple benchmarks, and uncovers synergistic effects across properties and laws. Project page: https://lore-project.github.io/

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [28] [Do Generalized=Gamma Scale Mixtures of Normals Fit Large Image Data-Sets?](https://arxiv.org/abs/2512.17038)
*Brandon Marks,Yash Dave,Zixun Wang,Hannah Chung,Riya Patwa,Simon Cha,Michael Murphy,Alexander Strang*

Main category: stat.AP

TL;DR: 广义伽马尺度混合正态分布作为贝叶斯逆成像问题的先验模型，在多个大型成像数据集上首次验证了其现实性，相比高斯、拉普拉斯等标准先验提供显著更好的拟合效果。


<details>
  <summary>Details</summary>
Motivation: 广义伽马尺度混合正态分布作为参数先验在贝叶斯逆成像问题中具有吸引力，但之前缺乏对其在真实成像数据上现实性的验证。本研究旨在首次系统评估该先验模型在多个大型成像数据集上的适用性。

Method: 从遥感、医学成像和图像分类应用中收集数据，研究该先验在傅里叶变换、小波变换（Haar和Gabor）以及AlexNet第一层卷积滤波器系数上的适用性。开发数据增强程序改进模型拟合，识别近似可交换系数，并表征最佳描述观测数据的参数区域。

Result: 该先验家族对每个数据集都提供了比其包含的任何标准先验（高斯、拉普拉斯、ℓp、学生t分布）显著更好的拟合效果。确定了最佳描述观测数据的参数区域，该区域比计算工作中主要关注的区域更广泛。同时识别了模型不现实的情况和导致拟合不良的图像特征。

Conclusion: 广义伽马尺度混合正态分布在多种成像应用中作为先验模型具有现实性，提供了比传统先验更好的拟合效果。研究确定了模型适用的参数区域和可能失效的情况，为实际应用提供了指导。

Abstract: A scale mixture of normals is a distribution formed by mixing a collection of normal distributions with fixed mean but different variances. A generalized gamma scale mixture draws the variances from a generalized gamma distribution. Generalized gamma scale mixtures of normals have been proposed as an attractive class of parametric priors for Bayesian inference in inverse imaging problems. Generalized gamma scale mixtures have two shape parameters, one that controls the behavior of the distribution about its mode, and the other that controls its tail decay. In this paper, we provide the first demonstration that the prior model is realistic for multiple large imaging data sets. We draw data from remote sensing, medical imaging, and image classification applications. We study the realism of the prior when applied to Fourier and wavelet (Haar and Gabor) transformations of the images, as well as to the coefficients produced by convolving the images against the filters used in the first layer of AlexNet, a popular convolutional neural network trained for image classification. We discuss data augmentation procedures that improve the fit of the model, procedures for identifying approximately exchangeable coefficients, and characterize the parameter regions that best describe the observed data sets. These regions are significantly broader than the region of primary focus in computational work. We show that this prior family provides a substantially better fit to each data set than any of the standard priors it contains. These include Gaussian, Laplace, $\ell_p$, and Student's $t$ priors. Finally, we identify cases where the prior is unrealistic and highlight characteristic features of images that suggest the model will fit poorly.

</details>


### [29] [Uncovering latent territorial structure in ICFES Saber 11 performance with Bayesian multilevel spatial models](https://arxiv.org/abs/2512.17119)
*Laura Pardo,Juan Sosa*

Main category: stat.AP

TL;DR: 该研究开发了一个贝叶斯分层框架来分析哥伦比亚2022年第二学期Saber 11考试学术表现，结合多级回归与空间随机效应，使用Ridge和Lasso正则化先验比较社会人口学协变量的贡献。


<details>
  <summary>Details</summary>
Motivation: 研究旨在分析哥伦比亚教育表现的空间分布模式，识别影响学术成绩的关键因素，并为教育政策提供基于证据的决策支持，特别关注结构性不平等和区域差异。

Method: 采用贝叶斯分层框架，结合多级回归与市、省级空间随机效应，使用Ridge和Lasso正则化先验，通过马尔可夫链蒙特卡洛方法进行推断，并使用合成数据评估模型行为。

Result: Ridge在参数恢复、预测准确性和采样效率方面表现最平衡；Lasso在强多重共线性下预测准确性有提升但拟合和稳定性较差。应用结果显示成绩高度集中化，中心部门得分高而边缘地区低，主要影响因素包括学生生活条件、母亲教育、教育资源获取、性别和种族背景。

Conclusion: 研究揭示了哥伦比亚教育表现的多尺度空间模式，反映了结构性不平等，基于K-means的贝叶斯分割方法将后验不确定性纳入聚类分析，为教育政策的区域针对性干预提供了信息支持。

Abstract: This article develops a Bayesian hierarchical framework to analyze academic performance in the 2022 second semester Saber 11 examination in Colombia. Our approach combines multilevel regression with municipal and departmental spatial random effects, and it incorporates Ridge and Lasso regularization priors to compare the contribution of sociodemographic covariates. Inference is implemented in a fully open source workflow using Markov chain Monte Carlo methods, and model behavior is assessed through synthetic data that mirror key features of the observed data. Simulation results indicate that Ridge provides the most balanced performance in parameter recovery, predictive accuracy, and sampling efficiency, while Lasso shows weaker fit and posterior stability, with gains in predictive accuracy under stronger multicollinearity. In the application, posterior rankings show a strong centralization of performance, with higher scores in central departments and lower scores in peripheral territories, and the strongest correlates of scores are student level living conditions, maternal education, access to educational resources, gender, and ethnic background, while spatial random effects capture residual regional disparities. A hybrid Bayesian segmentation based on K means propagates posterior uncertainty into clustering at departmental, municipal, and spatial scales, revealing multiscale territorial patterns consistent with structural inequalities and informing territorial targeting in education policy.

</details>


### [30] [Day-Ahead Electricity Price Forecasting Using Merit-Order Curves Time Series](https://arxiv.org/abs/2512.17758)
*Guillaume Koechlin,Filippo Bovera,Piercesare Secchi*

Main category: stat.AP

TL;DR: 提出基于供需曲线预测的电力市场价格预测框架，相比直接预测价格的方法，在点预测和概率预测方面均有显著改进


<details>
  <summary>Details</summary>
Motivation: 传统电力市场价格预测方法直接预测出清价格，但忽略了供需曲线的动态变化信息。本文旨在通过预测供需曲线来更准确地预测电力市场价格

Method: 使用函数主成分分析将供需曲线表示为低维向量空间，然后采用正则化向量自回归模型进行预测，从预测的曲线中推导出价格预测

Result: 基于曲线的方法相比直接价格预测方法，平均提升约5%的预测精度，在中午时段（可再生能源发电高、需求低时）提升可达10%

Conclusion: 通过预测供需曲线来预测电力市场价格的方法优于直接价格预测方法，特别是在价格波动较大的时段，为电力市场预测提供了更有效的框架

Abstract: We introduce a general, simple, and computationally efficient framework for predicting day-ahead supply and demand merit-order curves, from which both point and probabilistic electricity price forecasts can be derived. Specifically, we leverage functional principal component analysis to efficiently represent a pair of supply and demand curves in a low-dimensional vector space and employ regularized vector autoregressive models for their prediction. We conduct a rigorous empirical comparison of price forecasting performance between the proposed curve-based model, i.e., derived from predicted merit-order curves, and state-of-the-art price-based models that directly forecast the clearing price, using data from the Italian day-ahead market over the 2023-2024 period. Our results show that the proposed curve-based approach significantly improves both point and probabilistic price forecasting accuracy relative to price-based approaches, with average gains of approximately 5%, and improvements of up to 10% during mid-day hours, when prices occasionally drop due to high renewable generation and low demand.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [31] [Perceptions of the Metaverse at the Peak of the Hype Cycle: A Cross-Sectional Study Among Turkish University Students](https://arxiv.org/abs/2512.17750)
*Mehmet Ali Erkan,Halil Eren Koçak*

Main category: cs.CY

TL;DR: 土耳其大学生对元宇宙的态度研究发现，人口统计因素不影响参与意愿，而感知因素（如对社会变革的信念和对心理伤害的担忧）是关键预测因素。


<details>
  <summary>Details</summary>
Motivation: 在2021年底元宇宙热潮高峰期，研究土耳其大学生对元宇宙的态度、感知和参与意愿，了解早期采用者的特征和障碍因素。

Method: 采用横断面研究设计，调查381名土耳其大学生，使用Fisher精确检验和二元逻辑回归分析人口统计特征、数字经验和感知因素对参与意愿的影响。

Result: 性别、教育程度、院系、社交媒体参与度和VR经验等人口统计因素不显著预测元宇宙参与意愿。主要影响因素是感知因素：相信元宇宙能革新社会框架（特别是人权）是最显著的正向预测因素，而对心理伤害（"网络综合征"）的担忧是主要障碍。技术兼容性和伦理考虑呈现复杂影响。

Conclusion: 元宇宙早期采用主要基于个人感知而非人口统计特征。研究为Web 3.0初期的用户怀疑和审慎评估提供了历史基准，强调需要解决集体心理、伦理和规范问题以促进未来参与。

Abstract: During the height of the hype in late 2021, the Metaverse drew more attention from around the world than ever before. It promised new ways to interact with people in three-dimensional digital spaces. This cross-sectional study investigates the attitudes, perceptions, and predictors of the willingness to engage with the Metaverse among 381 Turkish university students surveyed in December 2021. The study employs Fisher's Exact Tests and binary logistic regression to assess the influence of demographic characteristics, prior digital experience, and perception-based factors. The results demonstrate that demographic factors, such as gender, educational attainment, faculty association, social media engagement, and previous virtual reality exposure, do not significantly forecast the propensity to participate in the Metaverse. Instead, the main things that affect people's intentions to adopt are how they see things. Belief in the Metaverse's capacity to revolutionize societal frameworks, especially human rights, surfaced as the most significant positive predictor of willingness. Conversely, apprehensions regarding psychological harm, framed as a possible 'cyber syndrome' represented a significant obstacle to participation. Perceptions of technical compatibility and ethical considerations showed complex effects, showing that optimism, uncertainty, and indifference affect willingness in different ways. In general, the results show that early adoption of the Metaverse is based on how people see it, not on their demographics. The research establishes a historically informed benchmark of user skepticism and prudent assessment during the advent of Web 3.0, underscoring the necessity of addressing collective psychological, ethical, and normative issues to promote future engagement.

</details>


### [32] [Algorithmic UDAP](https://arxiv.org/abs/2512.17007)
*Talia Gillis,Riley Stacy,Sam Brumer,Emily Black*

Main category: cs.CY

TL;DR: 比较两种法律框架（差别影响DI和UDAP）在评估算法歧视方面的应用，特别关注公平借贷领域，分析两者在算法环境中的异同和适用性挑战。


<details>
  <summary>Details</summary>
Motivation: 传统上差别影响（DI）是公平借贷法律的基础，但近期监管机构开始使用消费者保护原则的UDAP框架来应对算法歧视问题。需要系统比较这两种框架在算法环境中的适用性和差异。

Method: 在模拟借贷环境中形式化和操作化两种法律框架，分析它们如何评估算法差异。通过理论分析和模拟比较，探讨UDAP框架的"不公平"、"欺骗性"和"滥用性"标准在算法环境中的应用。

Result: UDAP是与DI独立且分析上不同的框架，其"不公平"标准引入了伤害可避免性和比例平衡等要素，"欺骗性"和"滥用性"标准可能捕捉DI分析无法覆盖的算法伤害形式。但将UDAP应用于算法环境仍存在未解决的模糊性。

Conclusion: UDAP作为评估算法歧视的替代框架具有独特价值，但需要进一步监管指导才能成为可行的标准。两种框架各有侧重，UDAP可能填补DI在算法歧视评估中的空白。

Abstract: This paper compares two legal frameworks -- disparate impact (DI) and unfair, deceptive, or abusive acts or practices (UDAP) -- as tools for evaluating algorithmic discrimination, focusing on the example of fair lending. While DI has traditionally served as the foundation of fair lending law, recent regulatory efforts have invoked UDAP, a doctrine rooted in consumer protection, as an alternative means to address algorithmic discrimination harms. We formalize and operationalize both doctrines in a simulated lending setting to assess how they evaluate algorithmic disparities. While some regulatory interpretations treat UDAP as operating similarly to DI, we argue it is an independent and analytically distinct framework. In particular, UDAP's "unfairness" prong introduces elements such as avoidability of harm and proportionality balancing, while its "deceptive" and "abusive" standards may capture forms of algorithmic harm that elude DI analysis. At the same time, translating UDAP into algorithmic settings exposes unresolved ambiguities, underscoring the need for further regulatory guidance if it is to serve as a workable standard.

</details>


### [33] [Integrating Computational Methods and AI into Qualitative Studies of Aging and Later Life](https://arxiv.org/abs/2512.17850)
*Corey M. Abramson*

Main category: cs.CY

TL;DR: 本章展示计算社会科学工具如何扩展老龄化研究，将传统定性方法与可扩展数据分析、计算文本分析相结合，通过机器学习处理大规模定性数据，同时保持与深入叙述的关联。


<details>
  <summary>Details</summary>
Motivation: 老龄化研究需要结合传统定性方法的深度与计算社会科学工具的可扩展性，以应对大规模定性数据分析的挑战，同时保持研究的深度和语境。

Method: 采用计算社会科学工具（机器学习、自然语言处理）与传统定性方法（参与观察、深度访谈、历史文献）相结合的多方法研究。通过DISCERN研究（痴呆症生活团队民族志）和美国之声项目的案例研究，展示如何系统索引大规模定性数据并识别模式。

Result: 计算社会科学工具能够：(1) 简化和增强现有工作流程；(2) 扩大样本和项目规模；(3) 生成多方法研究途径，以新方式解决重要问题。这些工具扩展而非替代定性研究的方法基础。

Conclusion: 当前发展虽有风险，但通过拓宽而非取代定性研究的方法基础，为老龄化与生命历程研究提供了新的洞察潜力，为寻求理解当前可能性或优化工作流程的个人和团队提供了实用方法。

Abstract: This chapter demonstrates how computational social science (CSS) tools are extending and expanding research on aging. The depth and context from traditionally qualitative methods such as participant observation, in-depth interviews, and historical documents are increasingly employed alongside scalable data management, computational text analysis, and open-science practices. Machine learning (ML) and natural language processing (NLP), provide resources to aggregate and systematically index large volumes of qualitative data, identify patterns, and maintain clear links to in-depth accounts. Drawing on case studies of projects that examine later life--including examples with original data from the DISCERN study (a team-based ethnography of life with dementia) and secondary analyses of the American Voices Project (nationally representative interview)--the chapter highlights both uses and challenges of bringing CSS tools into more meaningful dialogue with qualitative aging research. The chapter argues such work has potential for (1) streamlining and augmenting existing workflows, (2) scaling up samples and projects, and (3) generating multi-method approaches to address important questions in new ways, before turning to practices useful for individuals and teams seeking to understand current possibilities or refine their workflow processes. The chapter concludes that current developments are not without peril, but offer potential for new insights into aging and the life course by broadening--rather than replacing--the methodological foundations of qualitative research.

</details>


### [34] [The Role of Islamic Ethics in Preventing the Abuse of Artificial Intelligence (AI) Based Deepfakes](https://arxiv.org/abs/2512.17218)
*Wisnu Uriawan,Imany Fauzy Rahman,Muhamad Zidan,Irma Rohmatillah,Muhammad Arkan Raihan,Irma Dwiyanti*

Main category: cs.CY

TL;DR: 本研究提出基于伊斯兰伦理原则（特别是Maqasid al-Shariah中的hifz al-ird保护荣誉和hifz al-nafs保护自我）的综合性伦理框架，以预防深度伪造技术滥用风险，从惩罚性机制转向预防性方法。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术的快速发展引发了虚假信息篡改、身份盗用和公众对在线内容真实性信任下降等全球性担忧。现有技术驱动和反应式的管理方法无法解决意图、道德和潜在社会影响等根本问题，需要更全面的预防性伦理框架。

Method: 采用PRISMA指导的系统性文献综述(SLR)方法，筛选2018-2025年间发表的10个主要文献来源，识别伦理缺陷、监管需求和适当的规范性解决方案。

Result: 分析表明，伊斯兰教法目标原则（特别是保护荣誉和保护自我）为规范技术负责任使用提供了强有力的规范性基础。研究提出三项战略建议：承认声誉损害造成的无形和心理伤害的监管改革；通过道德审查改进技术管理以维护正义、信任和开放价值观；基于审慎原则提高公众数字素养。

Conclusion: 伊斯兰伦理的应用提供了从惩罚性机制向预防性方法的思维转变，重点关注保护人类尊严、预防伤害和加强数字时代的共同利益，为解决深度伪造技术滥用问题提供了更全面的伦理框架。

Abstract: The significant development of deepfake technology powered by artificial intelligence (AI) has sparked worldwide concerns about the alteration of false information, the usurpation of online identities, and the decline of public confidence in the authenticity of online content. These incidents not only raise technical issues but also carry complex moral implications, rendering conventional, technologically driven, and reactive management methods inadequate to address the underlying causes of the problem, including intent, morality, and potential intangible social impacts. Based on these issues, this study aims to formulate a comprehensive Islamic ethical framework that can serve as a more comprehensive preventative tool to mitigate the risks of misuse of deepfakes. The study employed a Systematic Literature Review (SLR) guided by PRISMA, selecting ten primary sources published between 2018 and 2025 to identify ethical deficiencies, regulatory needs, and appropriate normative solutions. The analysis shows that the integration of the principles of (Maqasid al-Shariah) particularly (hifz al-ird) protecting honor and (hifz al-nafs) protecting the self, provides a strong normative basis for regulating the responsible use of technology. This study yields three strategic recommendations: regulatory changes that recognize the intangible and psychological harm caused by reputational damage; improved technology management through moral scrutiny that upholds the values of justice (adl), trust, and openness; and increased public digital literacy based on the principle of (tabayyun) examination and caution. Overall, this study concludes that the application of Islamic ethics offers a shift in thinking from punitive mechanisms to preventative approaches that focus on protecting human dignity, preventing harm, and strengthening the common good in the digital age.

</details>


### [35] [Fair Voting Methods as a Catalyst for Democratic Resilience: A Trilogy on Legitimacy, Impact and AI Safeguarding](https://arxiv.org/abs/2512.17461)
*Evangelos Pournaras*

Main category: cs.CY

TL;DR: 公平投票方法可作为民主升级的催化剂，通过提升代表性、促进新想法、抵御AI风险来增强民主韧性。


<details>
  <summary>Details</summary>
Motivation: 传统多数决投票方法存在代表性不足、民主价值弱化等问题，需要探索更公平的投票方法来升级民主制度，特别是在民主危机背景下。

Method: 基于瑞士等地的参与式预算民主创新实践，结合表达性选票格式（如累积投票）与促进比例代表性的票数聚合方法（如平等份额）。

Result: 公平投票方法能产生更多获胜者、提高地理代表性，即使未获胜的选民也认为更公平；促进利他主义和妥协等民主价值；产生成本效益高的新想法；对生成式AI的偏见和不一致性更具韧性。

Conclusion: 公平投票方法是民主升级的关键催化剂，能增强公民代表性、促进民主创新，为希腊等面临民主危机的国家提供建设更强韧性的途径。

Abstract: This article shows how fair voting methods can be a catalyst for change in the way we make collective decisions, and how such change can promote long-awaited upgrades of democracy. Based on real-world evidence from democratic innovations in participatory budgeting, in Switzerland and beyond, I highlight a trilogy of key research results: Fair voting methods achieve to be (i) legitimacy incubator, (ii) novel impact accelerator and (iii) safeguard for risks of artificial intelligence (AI). Compared to majoritarian voting methods, combining expressive ballot formats (e.g. cumulative voting) with ballot aggregation methods that promote proportional representation (e.g. equal shares) results in more winners and higher (geographical) representation of citizens. Such fair voting methods are preferred and found fairer even by voters who do not win, while promoting stronger democratic values for citizens such as altruism and compromise. They also result in new resourceful ideas to put for voting, which are cost-effective and win, especially in areas of welfare, education and culture. Strikingly, fair voting methods are also more resilient to biases and inconsistencies of generative AI in emerging scenarios of AI voting assistance or AI representation of voters who would be likely to abstain. I also review the relevance of such upgrades for democracies in crisis, such as the one of Greece featured in the recent study of `Unmute Democracy'. Greek democracy can build stronger resilience via higher representation of citizens in democratic processes as well as democratic innovations in participation. Fair voting methods can be a catalyst for both endeavors.

</details>


### [36] [STAMP/STPA informed characterization of Factors Leading to Loss of Control in AI Systems](https://arxiv.org/abs/2512.17600)
*Steve Barrett,Anna Bruvere,Sean P. Fillingham,Catherine Rhodes,Stefano Vergani*

Main category: cs.CY

TL;DR: 论文提出了一个基于STAMP/STPA的结构化框架，用于分析和预防AI系统中的失控风险，帮助安全从业者识别导致失控的因果因素。


<details>
  <summary>Details</summary>
Motivation: AI安全从业者主要关注失控风险，即人类逐渐失去对日益先进AI系统的控制能力。这种担忧范围广泛，涵盖当前风险到未来生存风险，以及从快速AI自我外泄到逐渐失权等多种失控路径。

Method: 采用安全关键系统社区开发的STAMP方法论及其相关的STPA危险分析技术。STAMP基于社会技术系统可被功能建模为控制结构的观点，认为安全问题源于这些结构中的失控。

Result: 建立了一个更结构化的框架来讨论和描述失控问题，并利用该框架帮助负责AI社会技术系统安全运营的人员识别导致失控的因果因素。

Conclusion: STAMP/STPA方法论能够有效满足构建AI失控分析框架的需求，为AI安全实践提供了系统化的分析工具，有助于预防从当前风险到未来生存风险的各种失控场景。

Abstract: A major concern amongst AI safety practitioners is the possibility of loss of control, whereby humans lose the ability to exert control over increasingly advanced AI systems. The range of concerns is wide, spanning current day risks to future existential risks, and a range of loss of control pathways from rapid AI self-exfiltration scenarios to more gradual disempowerment scenarios. In this work we set out to firstly, provide a more structured framework for discussing and characterizing loss of control and secondly, to use this framework to assist those responsible for the safe operation of AI-containing socio-technical systems to identify causal factors leading to loss of control. We explore how these two needs can be better met by making use of a methodology developed within the safety-critical systems community known as STAMP and its associated hazard analysis technique of STPA. We select the STAMP methodology primarily because it is based around a world-view that socio-technical systems can be functionally modeled as control structures, and that safety issues arise when there is a loss of control in these structures.

</details>


### [37] [When Pamplona sounds different: the soundscape transformation of San Fermin through intelligent acoustic sensors and a sound repository](https://arxiv.org/abs/2512.17740)
*Amaia Sagasti,Frederic Font*

Main category: cs.CY

TL;DR: 使用低成本声学智能传感器网络监测潘普洛纳市圣费尔明节期间城市声景变化，发现节日期间声压级显著升高、声景模式改变、人声活动主导声环境


<details>
  <summary>Details</summary>
Motivation: 研究大规模节庆活动（圣费尔明节）对城市声景的时空影响，探索分布式智能声学监测系统在表征城市声景动态变化方面的潜力

Method: 在潘普洛纳市不同区域部署低成本声学智能传感器网络，在节前、节中和节后连续采集声学数据，同时创建并公开圣费尔明节真实声音记录集

Result: 节日期间城市声环境发生显著转变：整体声压级显著升高，声景模式改变，声学景观被人声活动相关声音主导，圣费尔明节极大重塑了城市的整体声学动态

Conclusion: 分布式智能声学监测系统能有效表征城市声景的时空动态，大规模节庆活动会显著改变城市声环境，创建的声音记录集有助于保存节日的独特声学遗产

Abstract: This study presents a use-case of a network of low-cost acoustic smart sensors deployed in the city of Pamplona to analyse changes in the urban soundscape during the San Fermin Festival. The sensors were installed in different areas of the city before, during, and after the event, capturing continuous acoustic data. Our analysis reveals a significant transformation in the city's sonic environment during the festive period: overall sound pressure levels increase significantly, soundscape patterns change, and the acoustic landscape becomes dominated by sounds associated with human activity. These findings highlight the potential of distributed smart acoustic monitoring systems to characterize the temporal dynamics of urban soundscapes and underscore how the large-scale event of San Fermin drastically reshapes the overall acoustic dynamics of the city of Pamplona. Additionally, to complement the objective measurements, a curated collection of real San Fermin sound recordings has been created and made publicly available, preserving the festival's unique sonic heritage.

</details>


### [38] [In Times of Crisis: An Exploratory Study of Media and Political Discourse on YouTube During the 2024 French Elections](https://arxiv.org/abs/2512.17768)
*Vera Sosnovik,Caroline Violot,Mathias Humbert*

Main category: cs.CY

TL;DR: 研究分析2024年欧洲议会和法国立法选举期间，政治人物和新闻媒体如何在YouTube上塑造公共话语，发现右翼媒体关注移民议题，左翼关注抗议和媒体自由，最两极分化的主题获得最多互动。


<details>
  <summary>Details</summary>
Motivation: YouTube已成为政治传播和新闻传播的重要平台，特别是在选举期间。研究旨在了解政治参与者和新闻媒体如何利用YouTube塑造公共话语，以及不同政治立场和媒体类型的主题差异。

Method: 分析74个法国YouTube频道的超过10万条视频转录和元数据，采用结合大型语言模型、聚类分析和人工审查的半自动化方法识别关键主题。

Result: 发现不同政治光谱和媒体类型有明显主题差异：右倾新闻媒体关注移民等议题，左倾则强调抗议和媒体自由。最两极分化的主题获得最多评论互动，而视频游戏和自然等非政治主题获得更高点赞率。所有媒体类型普遍以中立或批评而非正面态度描绘政治人物。

Conclusion: YouTube在选举期间成为政治话语的重要平台，不同政治立场媒体有明确主题偏好，两极分化内容更能激发用户互动，而媒体普遍对政治人物持中立或批评态度。

Abstract: YouTube has emerged as a major platform for political communication and news dissemination, particularly during high-stakes electoral periods. In the context of the 2024 European Parliament and French legislative elections, this study investigates how political actors and news media used YouTube to shape public discourse. We analyze over 100,000 video transcripts and metadata from 74 French YouTube channels operated by national news outlets, local media, and political figures. To identify the key themes emphasized during the campaign period, we applied a semi-automated method that combined large language models with clustering and manual review. The results reveal distinct thematic patterns across the political spectrum and media types, with right-leaning news outlets focusing on topics like immigration, while left-leaning emphasized protest and media freedom. Themes generating the most audience engagement, measured by comment-to-view ratios, were most often the most polarizing ones. In contrast, less polarizing themes such as video games and nature showed higher approval, reflected in like-to-view ratios. We also observed a general tendency across all media types to portray political figures in neutral or critical terms rather than favorable ones.

</details>


### [39] [Systemic Risks of Interacting AI](https://arxiv.org/abs/2512.17793)
*Paul Darius,Thomas Hoppe,Andrei Aleksandrov*

Main category: cs.CY

TL;DR: 该研究探索了交互式AI代理的系统级涌现风险，通过场景分析识别风险类型并建立分类体系，同时开发了可视化工具"Agentology"。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统日益复杂且相互交互，系统级涌现风险成为重要但未被充分研究的领域。现有研究主要关注单个AI系统的风险，而多个AI代理交互可能产生意想不到的系统级风险，需要系统性探索。

Method: 采用探索性场景分析方法，从现有文献中收集系统性风险案例，开发了两个具体场景（智能电网和社会福利）来演示涌现风险模式，并建立了风险分类体系。

Result: 识别了交互式AI代理的系统级涌现风险类型并建立了分类体系；确定了产生系统性风险的涌现行为类型；开发了用于可视化交互AI系统的图形语言"Agentology"。

Conclusion: 该研究开创了交互式AI系统级风险研究的新方向，首次系统性地探索了这一领域，为理解和减轻AI系统交互带来的复杂风险提供了基础框架。

Abstract: In this study, we investigate system-level emergent risks of interacting AI agents. The core contribution of this work is an exploratory scenario-based identification of these risks as well as their categorization. We consider a multitude of systemic risk examples from existing literature and develop two scenarios demonstrating emergent risk patterns in domains of smart grid and social welfare. We provide a taxonomy of identified risks that categorizes them in different groups. In addition, we make two other important contributions: first, we identify what emergent behavior types produce systemic risks, and second, we develop a graphical language "Agentology" for visualization of interacting AI systems. Our study opens a new research direction for system-level risks of interacting AI, and is the first to closely investigate them.

</details>

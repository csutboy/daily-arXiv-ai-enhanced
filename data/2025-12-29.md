<div id=toc></div>

# Table of Contents

- [stat.AP](#stat.AP) [Total: 1]
- [cs.AI](#cs.AI) [Total: 17]
- [cs.SI](#cs.SI) [Total: 2]
- [econ.EM](#econ.EM) [Total: 2]
- [cs.CY](#cs.CY) [Total: 2]


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [1] [Dynamic Attention (DynAttn): Interpretable High-Dimensional Spatio-Temporal Forecasting (with Application to Conflict Fatalities)](https://arxiv.org/abs/2512.21435)
*Stefano M. Iacus,Haodong Qi,Marcello Carammia,Thomas Juneau*

Main category: stat.AP

TL;DR: DynAttn是一个用于预测冲突相关死亡人数的可解释动态注意力框架，结合滚动窗口估计、共享弹性网络特征门控、权重共享自注意力编码器和零膨胀负二项分布，在稀疏网格级冲突数据上显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 冲突相关死亡人数的预测在政治学和政策分析中具有重要价值，但由于暴力数据的稀疏性、突发性和高度非平稳性，传统方法面临挑战。需要开发既能提供准确预测又能保持可解释性的新方法。

Method: DynAttn框架包含四个核心组件：1) 滚动窗口估计处理非平稳性；2) 共享弹性网络特征门控进行特征选择；3) 紧凑的权重共享自注意力编码器捕捉时空依赖；4) 零膨胀负二项分布(ZINB)似然函数处理稀疏计数数据。该框架能生成校准的多期预测和超限概率。

Result: 在VIEWS预测系统的全球国家级和高分辨率PRIO网格级冲突数据上评估，DynAttn在1-12个月的预测范围内始终显著优于DynENet、LSTM、Prophet、PatchTST和官方VIEWS基线等模型。在稀疏网格级设置中优势尤其明显，而竞争模型往往不稳定或性能急剧下降。

Conclusion: DynAttn不仅提供了更准确的冲突预测，还通过特征门控、消融分析和弹性度量实现了结构化解释。跨区域分析显示，短期冲突持续性和空间扩散构成核心预测基础，而气候压力根据冲突区域的不同，可能作为条件放大器或主要驱动因素。

Abstract: Forecasting conflict-related fatalities remains a central challenge in political science and policy analysis due to the sparse, bursty, and highly non-stationary nature of violence data. We introduce DynAttn, an interpretable dynamic-attention forecasting framework for high-dimensional spatio-temporal count processes. DynAttn combines rolling-window estimation, shared elastic-net feature gating, a compact weight-tied self-attention encoder, and a zero-inflated negative binomial (ZINB) likelihood. This architecture produces calibrated multi-horizon forecasts of expected casualties and exceedance probabilities, while retaining transparent diagnostics through feature gates, ablation analysis, and elasticity measures.
  We evaluate DynAttn using global country-level and high-resolution PRIO-grid-level conflict data from the VIEWS forecasting system, benchmarking it against established statistical and machine-learning approaches, including DynENet, LSTM, Prophet, PatchTST, and the official VIEWS baseline. Across forecast horizons from one to twelve months, DynAttn consistently achieves substantially higher predictive accuracy, with particularly large gains in sparse grid-level settings where competing models often become unstable or degrade sharply.
  Beyond predictive performance, DynAttn enables structured interpretation of regional conflict dynamics. In our application, cross-regional analyses show that short-run conflict persistence and spatial diffusion form the core predictive backbone, while climate stress acts either as a conditional amplifier or a primary driver depending on the conflict theater.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [2] [From Visual Perception to Deep Empathy: An Automated Assessment Framework for House-Tree-Person Drawings Using Multimodal LLMs and Multi-Agent Collaboration](https://arxiv.org/abs/2512.21360)
*Shuide Wen,Yu Sun,Beier Ku,Zhi Gao,Lijun Ma,Yang Yang,Can Jiao*

Main category: cs.AI

TL;DR: 该研究开发了一个基于多模态大语言模型的多智能体系统，用于标准化HTP绘画测试的评估，实现了专家级别的解释能力。


<details>
  <summary>Details</summary>
Motivation: HTP绘画测试作为临床心理学中广泛使用的投射技术，长期面临评分标准不统一、依赖评估者主观经验、缺乏统一量化编码系统等问题，需要开发标准化的评估工具。

Method: 采用多模态大语言模型构建多智能体协作框架，将角色分工，分离特征识别与心理推理过程，通过社会心理学视角和去污名化叙事来纠正视觉幻觉。

Result: MLLM解释与人类专家解释的平均语义相似度约为0.75（标准差0.05），在结构化专家数据集中相似度提升至0.85，达到专家级基线理解水平。系统能生成具有高生态效度和内部一致性的心理报告。

Conclusion: 多模态大模型有潜力成为投射评估的标准化工具，提出的多智能体框架通过角色分工为数字心理健康服务提供了新范式。

Abstract: Background: The House-Tree-Person (HTP) drawing test, introduced by John Buck in 1948, remains a widely used projective technique in clinical psychology. However, it has long faced challenges such as heterogeneous scoring standards, reliance on examiners subjective experience, and a lack of a unified quantitative coding system.
  Results: Quantitative experiments showed that the mean semantic similarity between Multimodal Large Language Model (MLLM) interpretations and human expert interpretations was approximately 0.75 (standard deviation about 0.05). In structurally oriented expert data sets, this similarity rose to 0.85, indicating expert-level baseline comprehension. Qualitative analyses demonstrated that the multi-agent system, by integrating social-psychological perspectives and destigmatizing narratives, effectively corrected visual hallucinations and produced psychological reports with high ecological validity and internal coherence.
  Conclusions: The findings confirm the potential of multimodal large models as standardized tools for projective assessment. The proposed multi-agent framework, by dividing roles, decouples feature recognition from psychological inference and offers a new paradigm for digital mental-health services.
  Keywords: House-Tree-Person test; multimodal large language model; multi-agent collaboration; cosine similarity; computational psychology; artificial intelligence

</details>


### [3] [A Study of Solving Life-and-Death Problems in Go Using Relevance-Zone Based Solvers](https://arxiv.org/abs/2512.21365)
*Chung-Chin Shih,Ti-Rong Wu,Ting Han Wei,Yu-Shan Hsu,Hung Guei,I-Chen Wu*

Main category: cs.AI

TL;DR: 本文分析使用当前最先进的计算机围棋求解器（基于相关区域搜索和相关区域模式表）解决围棋死活题的行为，发现求解器能识别关键区域、发现罕见模式，但在某些问题上与人类解法不同，存在模式价值误判和优先活棋而非最大化实地的问题。


<details>
  <summary>Details</summary>
Motivation: 研究动机是分析当前最先进的计算机围棋求解器在解决围棋死活题时的行为表现，特别是使用相关区域搜索（RZS）和相关区域模式表这两种技术时，求解器的表现如何，以及它们与人类专家解法的差异。

Method: 研究方法包括：1）使用基于相关区域搜索（RZS）和相关区域模式表的计算机围棋求解器；2）在围棋大师赵治勋所著《死活辞典》中的七个死活问题上进行测试；3）分析求解器识别的相关区域、发现的模式序列；4）比较求解器答案与给定人类解法的差异。

Result: 研究结果发现：1）对于每个问题，求解器都能识别出解决该问题的关键相关区域；2）求解器发现了一系列模式，包括一些罕见模式；3）在七个问题中，有两个问题的求解器答案与给定人类解法不同。同时发现求解器存在两个问题：a）对罕见模式的价值判断错误；b）倾向于优先活棋而非最大化实地，这与人类棋手行为不同。

Conclusion: 结论指出当前计算机围棋求解器在解决死活题时表现出色，能够识别关键区域和发现罕见模式，但在模式价值判断和策略选择上与人类存在差异。作者建议未来工作应解决这些问题，并提供了代码和数据的公开访问链接。

Abstract: This paper analyzes the behavior of solving Life-and-Death (L&D) problems in the game of Go using current state-of-the-art computer Go solvers with two techniques: the Relevance-Zone Based Search (RZS) and the relevance-zone pattern table. We examined the solutions derived by relevance-zone based solvers on seven L&D problems from the renowned book "Life and Death Dictionary" written by Cho Chikun, a Go grandmaster, and found several interesting results. First, for each problem, the solvers identify a relevance-zone that highlights the critical areas for solving. Second, the solvers discover a series of patterns, including some that are rare. Finally, the solvers even find different answers compared to the given solutions for two problems. We also identified two issues with the solver: (a) it misjudges values of rare patterns, and (b) it tends to prioritize living directly rather than maximizing territory, which differs from the behavior of human Go players. We suggest possible approaches to address these issues in future work. Our code and data are available at https://rlg.iis.sinica.edu.tw/papers/study-LD-RZ.

</details>


### [4] [Three-way conflict analysis based on alliance and conflict functions](https://arxiv.org/abs/2512.21419)
*Junfang Luo,Mengjun Hu,Guangming Lang,Xin Yang,Keyun Qin*

Main category: cs.AI

TL;DR: 论文提出将冲突分析中的辅助函数拆分为联盟函数和冲突函数，以解决传统方法中正负评分聚合导致语义模糊的问题，并探索了联盟集和策略等概念。


<details>
  <summary>Details</summary>
Motivation: 传统冲突分析中，评分函数或辅助函数将对立方面（如联盟与冲突）合并到单一函数中，导致在群体问题或代理聚合时语义解释困难。例如，联盟+1和冲突-1的平均结果与两个中立0关系相同，但实际态度完全不同。

Method: 将辅助函数拆分为联盟函数和冲突函数对，分别处理对立方面。基于此对代理、问题和代理对进行三分，并探索联盟集和策略等概念在冲突分析中的应用。

Result: 提出的分离方法能够更清晰地解释冲突分析中的语义，避免了传统聚合方法的模糊性。通过实际案例验证了模型的有效性。

Conclusion: 将对立方面分离为独立函数能够提高冲突分析的语义清晰度，为联盟集和策略等关键问题的解决提供了更精确的框架，并通过实际应用证明了其价值。

Abstract: Trisecting agents, issues, and agent pairs are essential topics of three-way conflict analysis. They have been commonly studied based on either a rating or an auxiliary function. A rating function defines the positive, negative, or neutral ratings of agents on issues. An auxiliary function defines the alliance, conflict, and neutrality relations between agents. These functions measure two opposite aspects in a single function, leading to challenges in interpreting their aggregations over a group of issues or agents. For example, when studying agent relations regarding a set of issues, a standard aggregation takes the average of an auxiliary function concerning single issues. Therefore, a pair of alliance +1 and conflict -1 relations will produce the same result as a pair of neutrality 0 relations, although the attitudes represented by the two pairs are very different. To clarify semantics, we separate the two opposite aspects in an auxiliary function into a pair of alliance and conflict functions. Accordingly, we trisect the agents, issues, and agent pairs and investigate their applications in solving a few crucial questions in conflict analysis. Particularly, we explore the concepts of alliance sets and strategies. A real-world application is given to illustrate the proposed models.

</details>


### [5] [Feasible strategies in three-way conflict analysis with three-valued ratings](https://arxiv.org/abs/2512.21420)
*Jing Liu,Mengjun Hu,Guangming Lang*

Main category: cs.AI

TL;DR: 本文提出了一种基于加权一致性和非一致性度量的三路冲突分析方法，用于识别可行策略和最优解决方案，在NBA劳资谈判和甘肃发展规划案例中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有三路冲突分析主要关注代理对、代理或问题的三分，有助于理解冲突本质但缺乏解决冲突的方法。特别是作为冲突解决和缓解关键组成部分的可行策略制定，尚未得到充分研究。

Method: 首先基于正负相似度计算代理群体的整体评分，然后考虑代理和问题的权重，提出加权一致性和非一致性度量，分别用于识别代理群体的可行策略。开发算法识别可行策略、L阶可行策略及相应最优策略。

Result: 在NBA劳资谈判和甘肃发展规划两个案例研究中验证了模型的实用性、有效性和优越性。参数敏感性分析和与现有最先进冲突分析方法的比较表明，所提出的冲突解决模型优于传统方法。

Conclusion: 通过统一加权代理-问题评估与一致性和非一致性度量，能够系统识别可行策略和最优解决方案，为冲突解决提供了更有效的框架。

Abstract: Most existing work on three-way conflict analysis has focused on trisecting agent pairs, agents, or issues, which contributes to understanding the nature of conflicts but falls short in addressing their resolution. Specifically, the formulation of feasible strategies, as an essential component of conflict resolution and mitigation, has received insufficient scholarly attention. Therefore, this paper aims to investigate feasible strategies from two perspectives of consistency and non-consistency. Particularly, we begin with computing the overall rating of a clique of agents based on positive and negative similarity degrees. Afterwards, considering the weights of both agents and issues, we propose weighted consistency and non-consistency measures, which are respectively used to identify the feasible strategies for a clique of agents. Algorithms are developed to identify feasible strategies, $L$-order feasible strategies, and the corresponding optimal ones. Finally, to demonstrate the practicality, effectiveness, and superiority of the proposed models, we apply them to two commonly used case studies on NBA labor negotiations and development plans for Gansu Province and conduct a sensitivity analysis on parameters and a comparative analysis with existing state-of-the-art conflict analysis approaches. The comparison results demonstrate that our conflict resolution models outperform the conventional approaches by unifying weighted agent-issue evaluation with consistency and non-consistency measures to enable the systematic identification of not only feasible strategies but also optimal solutions.

</details>


### [6] [Compliance Rating Scheme: A Data Provenance Framework for Generative AI Datasets](https://arxiv.org/abs/2512.21775)
*Matyas Bohacek,Ignacio Vilanova Echavarri*

Main category: cs.AI

TL;DR: 提出合规评级方案(CRS)框架，评估数据集在透明度、问责制和安全性方面的合规性，并发布开源Python库实现该框架


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速发展依赖于大规模开源数据集，但这些数据集通常采用不受限制且不透明的数据收集方式。现有文献多关注GAI模型开发和应用，而忽视了数据集创建的伦理和法律考量。数据集在共享、编辑和复制过程中，其来源、合法性和安全性信息常常丢失。

Method: 引入合规评级方案(CRS)框架，基于数据溯源技术开发开源Python库，评估数据集在透明度、问责制和安全性方面的合规性。该库既能评估现有数据集的CRS，也能指导新数据集的负责任爬取和构建。

Result: 开发了一个开源Python库，可无缝集成到现有数据集处理和AI训练流程中。该库具有反应性和前瞻性双重功能，既能评估现有数据集合规性，又能指导新数据集的负责任构建。

Conclusion: CRS框架和开源库填补了生成式AI数据集伦理和法律考量的空白，通过数据溯源技术确保数据集透明度、问责制和安全性，促进AI发展的负责任实践。

Abstract: Generative Artificial Intelligence (GAI) has experienced exponential growth in recent years, partly facilitated by the abundance of large-scale open-source datasets. These datasets are often built using unrestricted and opaque data collection practices. While most literature focuses on the development and applications of GAI models, the ethical and legal considerations surrounding the creation of these datasets are often neglected. In addition, as datasets are shared, edited, and further reproduced online, information about their origin, legitimacy, and safety often gets lost. To address this gap, we introduce the Compliance Rating Scheme (CRS), a framework designed to evaluate dataset compliance with critical transparency, accountability, and security principles. We also release an open-source Python library built around data provenance technology to implement this framework, allowing for seamless integration into existing dataset-processing and AI training pipelines. The library is simultaneously reactive and proactive, as in addition to evaluating the CRS of existing datasets, it equally informs responsible scraping and construction of new datasets.

</details>


### [7] [Three-way decision with incomplete information based on similarity and satisfiability](https://arxiv.org/abs/2512.21421)
*Junfang Luo,Mengjun Hu,Keyun Qin*

Main category: cs.AI

TL;DR: 本文回顾了完全信息下的三支决策两种表述（计算和概念），并将其推广到不完全信息场景，提出了相似度度量、α相似类、可逼近性等新方法。


<details>
  <summary>Details</summary>
Motivation: 现有三支决策方法主要处理完全信息，但现实应用中信息往往不完全。需要将三支决策的计算和概念两种表述推广到不完全信息场景，使其更具实用性。

Method: 1. 计算表述：提出对象相似度度量作为等价关系的推广，基于此讨论使用α相似类和对象可逼近性的两种三支决策方法。2. 概念表述：提出公式满足度度量作为完全信息下满足性的量化推广，基于此研究使用公式α意义集和公式置信度的两种三支决策方法。

Result: 提出了处理不完全信息的三支决策新框架，包括相似度度量、α相似类、可逼近性等新概念，为不完全信息分析提供了新方向。

Conclusion: 将三支决策从完全信息推广到不完全信息，提出的可逼近性概念和概念表述中的两种新方法为不完全信息分析开辟了有前景的新方向，而相似类方法是文献中常用的不完全信息分析方法。

Abstract: Three-way decision is widely applied with rough set theory to learn classification or decision rules. The approaches dealing with complete information are well established in the literature, including the two complementary computational and conceptual formulations. The computational formulation uses equivalence relations, and the conceptual formulation uses satisfiability of logic formulas. In this paper, based on a briefly review of these two formulations, we generalize both formulations into three-way decision with incomplete information that is more practical in real-world applications. For the computational formulation, we propose a new measure of similarity degree of objects as a generalization of equivalence relations. Based on it, we discuss two approaches to three-way decision using alpha-similarity classes and approximability of objects, respectively. For the conceptual formulation, we propose a measure of satisfiability degree of formulas as a quantitative generalization of satisfiability with complete information. Based on it, we study two approaches to three-way decision using alpha-meaning sets of formulas and confidence of formulas, respectively. While using similarity classes is a common method of analyzing incomplete information in the literature, the proposed concept of approximability and the two approaches in conceptual formulation point out new promising directions.

</details>


### [8] [LogicLens: Visual-Logical Co-Reasoning for Text-Centric Forgery Analysis](https://arxiv.org/abs/2512.21482)
*Fanwei Zeng,Changtao Miao,Jing Huang,Zhiya Tan,Shutao Gong,Xiaoming Yu,Yang Wang,Huazhe Tan,Weibin Yao,Jianshu Li*

Main category: cs.AI

TL;DR: LogicLens是一个用于文本中心伪造分析的统一框架，通过视觉-文本协同推理和跨线索感知思维链机制，将检测、定位和解释任务联合处理，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前文本中心伪造分析方法存在三个主要问题：1）通常只进行粗粒度的视觉分析，缺乏复杂推理能力；2）将检测、定位和解释视为独立子任务，忽视了它们之间的内在联系；3）缺乏高质量、细粒度的标注数据集。

Method: 提出LogicLens统一框架，包含：1）跨线索感知思维链（CCT）机制，迭代交叉验证视觉线索与文本逻辑；2）基于GRPO优化的加权多任务奖励函数；3）PR²（感知器、推理器、审查器）流水线，用于生成高质量标注；4）构建RealText数据集，包含5,397张图像，带有细粒度标注。

Result: 在T-IC13的零样本评估中，LogicLens比专用框架高出41.4%，比GPT-4o高出23.4%的宏观平均F1分数。在密集文本T-SROIE数据集上，在mF1、CSS和宏观平均F1指标上均显著领先其他MLLM方法。

Conclusion: LogicLens通过统一的视觉-文本协同推理框架，有效解决了文本中心伪造分析的现有局限性，在检测、定位和解释任务上均表现出色，为信息真实性验证提供了有力工具。

Abstract: Sophisticated text-centric forgeries, fueled by rapid AIGC advancements, pose a significant threat to societal security and information authenticity. Current methods for text-centric forgery analysis are often limited to coarse-grained visual analysis and lack the capacity for sophisticated reasoning. Moreover, they typically treat detection, grounding, and explanation as discrete sub-tasks, overlooking their intrinsic relationships for holistic performance enhancement. To address these challenges, we introduce LogicLens, a unified framework for Visual-Textual Co-reasoning that reformulates these objectives into a joint task. The deep reasoning of LogicLens is powered by our novel Cross-Cues-aware Chain of Thought (CCT) mechanism, which iteratively cross-validates visual cues against textual logic. To ensure robust alignment across all tasks, we further propose a weighted multi-task reward function for GRPO-based optimization. Complementing this framework, we first designed the PR$^2$ (Perceiver, Reasoner, Reviewer) pipeline, a hierarchical and iterative multi-agent system that generates high-quality, cognitively-aligned annotations. Then, we constructed RealText, a diverse dataset comprising 5,397 images with fine-grained annotations, including textual explanations, pixel-level segmentation, and authenticity labels for model training. Extensive experiments demonstrate the superiority of LogicLens across multiple benchmarks. In a zero-shot evaluation on T-IC13, it surpasses the specialized framework by 41.4% and GPT-4o by 23.4% in macro-average F1 score. Moreover, on the challenging dense-text T-SROIE dataset, it establishes a significant lead over other MLLM-based methods in mF1, CSS, and the macro-average F1. Our dataset, model, and code will be made publicly available.

</details>


### [9] [Leash: Adaptive Length Penalty and Reward Shaping for Efficient Large Reasoning Model](https://arxiv.org/abs/2512.21540)
*Yanhao Li,Lu Ma,Jiaran Zhang,Lexiang Tang,Wentao Zhang,Guibo Luo*

Main category: cs.AI

TL;DR: Leash：基于强化学习的自适应长度惩罚框架，通过拉格朗日对偶方法动态调整惩罚系数，在保持任务性能的同时将推理长度减少60%


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖固定长度惩罚，难以调优且无法适应LLM不断演进的推理能力，导致准确性和简洁性之间的次优权衡

Method: 将长度控制建模为约束优化问题，采用拉格朗日原始对偶方法动态调整惩罚系数：当生成超过目标长度时增强惩罚，当生成较短时放松惩罚

Result: 在Deepseek-R1-Distill-Qwen-1.5B和Qwen3-4B-Thinking-2507上，Leash在数学推理、代码生成和指令跟随等任务中将平均推理长度减少60%，同时保持竞争力性能

Conclusion: Leash为开发可控高效LLM提供了实用有效范式，平衡推理能力与计算预算，实现简洁推理而不牺牲任务性能

Abstract: Existing approaches typically rely on fixed length penalties, but such penalties are hard to tune and fail to adapt to the evolving reasoning abilities of LLMs, leading to suboptimal trade-offs between accuracy and conciseness. To address this challenge, we propose Leash (adaptive LEngth penAlty and reward SHaping), a reinforcement learning framework for efficient reasoning in LLMs. We formulate length control as a constrained optimization problem and employ a Lagrangian primal-dual method to dynamically adjust the penalty coefficient. When generations exceed the target length, the penalty is intensified; when they are shorter, it is relaxed. This adaptive mechanism guides models toward producing concise reasoning without sacrificing task performance. Experiments on Deepseek-R1-Distill-Qwen-1.5B and Qwen3-4B-Thinking-2507 show that Leash reduces the average reasoning length by 60% across diverse tasks - including in-distribution mathematical reasoning and out-of-distribution domains such as coding and instruction following - while maintaining competitive performance. Our work thus presents a practical and effective paradigm for developing controllable and efficient LLMs that balance reasoning capabilities with computational budgets.

</details>


### [10] [NEMO-4-PAYPAL: Leveraging NVIDIA's Nemo Framework for empowering PayPal's Commerce Agent](https://arxiv.org/abs/2512.21578)
*Ali Sahami,Sudhanshu Garg,Andrew Wang,Chaitanya Kulkarni,Farhad Farahani,Sean Yun-Shiuan Chuang,Jian Wan,Srinivasan Manoharan,Uma Kona,Nitin Sharma,Linsey Pang,Prakhar Mehrotra,Jessica Clark,Mark Moyou*

Main category: cs.AI

TL;DR: PayPal与NVIDIA合作，利用NeMo框架微调Nemotron小型语言模型，优化其Commerce Agent中的搜索与发现代理，显著降低了延迟和成本，同时保持系统性能。


<details>
  <summary>Details</summary>
Motivation: PayPal平台上的Commerce Agent需要优化性能，特别是搜索与发现代理的检索组件占据了超过50%的总响应时间，存在显著的性能瓶颈需要解决。

Method: 采用NVIDIA的NeMo框架进行LLM模型微调，使用llama3.1-nemotron-nano-8B-v1架构，通过LoRA技术进行系统化的超参数扫描，包括学习率、优化器（Adam、AdamW）、余弦退火调度和LoRA秩的优化。

Result: 微调后的Nemotron SLM有效解决了检索组件的关键性能问题，在保持或提升整体系统性能的同时，显著改善了延迟和成本表现。

Conclusion: 该研究展示了NeMo框架在商业特定代理优化中的首次应用，为生产环境中的电子商务多代理系统优化提供了一个可扩展的框架，证明了LLM微调策略在商业任务中的有效性。

Abstract: We present the development and optimization of PayPal's Commerce Agent, powered by NEMO-4-PAYPAL, a multi-agent system designed to revolutionize agentic commerce on the PayPal platform. Through our strategic partnership with NVIDIA, we leveraged the NeMo Framework for LLM model fine-tuning to enhance agent performance. Specifically, we optimized the Search and Discovery agent by replacing our base model with a fine-tuned Nemotron small language model (SLM).
  We conducted comprehensive experiments using the llama3.1-nemotron-nano-8B-v1 architecture, training LoRA-based models through systematic hyperparameter sweeps across learning rates, optimizers (Adam, AdamW), cosine annealing schedules, and LoRA ranks. Our contributions include: (1) the first application of NVIDIA's NeMo Framework to commerce-specific agent optimization, (2) LLM powered fine-tuning strategy for retrieval-focused commerce tasks, (3) demonstration of significant improvements in latency and cost while maintaining agent quality, and (4) a scalable framework for multi-agent system optimization in production e-commerce environments. Our results demonstrate that the fine-tuned Nemotron SLM effectively resolves the key performance issue in the retrieval component, which represents over 50\% of total agent response time, while maintaining or enhancing overall system performance.

</details>


### [11] [A Medical Multimodal Diagnostic Framework Integrating Vision-Language Models and Logic Tree Reasoning](https://arxiv.org/abs/2512.21583)
*Zelin Zang,Wenyi Gu,Siqi Ma,Dan Yang,Yue Shen,Zhu Zhang,Guohui Fan,Wing-Kuen Ling,Fuji Yang*

Main category: cs.AI

TL;DR: 提出基于LLaVA的医疗诊断框架，结合视觉语言对齐与逻辑正则化推理，提高诊断准确性并生成可解释推理路径


<details>
  <summary>Details</summary>
Motivation: 现有医疗多模态模型常产生幻觉或不一致推理链，限制了临床可信度，需要更可靠的医疗AI诊断框架

Method: 基于LLaVA构建诊断框架，包含文本图像输入编码器、跨模态对齐投影模块、任务分解推理控制器和逻辑树生成器

Result: 在MedXpertQA等基准测试中，该方法提高了诊断准确性，生成更可解释的推理轨迹，在文本任务上保持竞争力

Conclusion: 该方法为实现可信赖的多模态医疗AI迈出了有希望的一步，结合视觉语言对齐与逻辑正则化推理可提高诊断可靠性

Abstract: With the rapid growth of large language models (LLMs) and vision-language models (VLMs) in medicine, simply integrating clinical text and medical imaging does not guarantee reliable reasoning. Existing multimodal models often produce hallucinations or inconsistent chains of thought, limiting clinical trust. We propose a diagnostic framework built upon LLaVA that combines vision-language alignment with logic-regularized reasoning. The system includes an input encoder for text and images, a projection module for cross-modal alignment, a reasoning controller that decomposes diagnostic tasks into steps, and a logic tree generator that assembles stepwise premises into verifiable conclusions. Evaluations on MedXpertQA and other benchmarks show that our method improves diagnostic accuracy and yields more interpretable reasoning traces on multimodal tasks, while remaining competitive on text-only settings. These results suggest a promising step toward trustworthy multimodal medical AI.

</details>


### [12] [AMS-IO-Bench and AMS-IO-Agent: Benchmarking and Structured Reasoning for Analog and Mixed-Signal Integrated Circuit Input/Output Design](https://arxiv.org/abs/2512.21613)
*Zhishuai Zhang,Xintian Li,Shilong Liu,Aodong Zhang,Lu Jie,Nan Sun*

Main category: cs.AI

TL;DR: 提出AMS-IO-Agent，一个基于LLM的领域专用代理，用于模拟混合信号集成电路的结构感知I/O子系统生成，将自然语言设计意图转化为工业级设计交付物。


<details>
  <summary>Details</summary>
Motivation: 传统AMS IC设计流程中，I/O子系统设计耗时且依赖专家经验，需要将自然语言设计意图转化为可验证的设计步骤。现有方法难以有效连接设计意图与工业级设计交付物。

Method: 开发了包含结构化领域知识库和设计意图结构化两个关键能力的框架。知识库捕获可重用约束和设计惯例；设计意图结构化将模糊用户意图转换为使用JSON和Python作为中间格式的可验证逻辑步骤。

Result: 在AMS-IO-Bench基准测试中，代理实现了超过70%的DRC+LVS通过率，将设计周期从数小时缩短到数分钟，优于基线LLM。代理生成的I/O环在28nm CMOS流片中成功制造和验证。

Conclusion: 这是首个报道的人机协作AMS IC设计，其中基于LLM的代理完成了非平凡子任务，输出直接用于硅片制造。证明了该方法在实际AMS IC设计流程中的有效性。

Abstract: In this paper, we propose AMS-IO-Agent, a domain-specialized LLM-based agent for structure-aware input/output (I/O) subsystem generation in analog and mixed-signal (AMS) integrated circuits (ICs). The central contribution of this work is a framework that connects natural language design intent with industrial-level AMS IC design deliverables. AMS-IO-Agent integrates two key capabilities: (1) a structured domain knowledge base that captures reusable constraints and design conventions; (2) design intent structuring, which converts ambiguous user intent into verifiable logic steps using JSON and Python as intermediate formats. We further introduce AMS-IO-Bench, a benchmark for wirebond-packaged AMS I/O ring automation. On this benchmark, AMS-IO-Agent achieves over 70\% DRC+LVS pass rate and reduces design turnaround time from hours to minutes, outperforming the baseline LLM. Furthermore, an agent-generated I/O ring was fabricated and validated in a 28 nm CMOS tape-out, demonstrating the practical effectiveness of the approach in real AMS IC design flows. To our knowledge, this is the first reported human-agent collaborative AMS IC design in which an LLM-based agent completes a nontrivial subtask with outputs directly used in silicon.

</details>


### [13] [Democratizing Drug Discovery with an Orchestrated, Knowledge-Driven Multi-Agent Team for User-Guided Therapeutic Design](https://arxiv.org/abs/2512.21623)
*Takahide Suzuki,Kazuki Nakanishi,Takashi Fujiwara,Hideyuki Shimizu*

Main category: cs.AI

TL;DR: OrchestRA是一个人类在环的多智能体平台，将生物学、化学和药理学统一为自主发现引擎，通过自主执行模拟和结果推理来驱动迭代优化，将药物发现从随机搜索转变为可编程的循证工程学科。


<details>
  <summary>Details</summary>
Motivation: 当前治疗发现面临专业领域碎片化以及计算设计与生理验证之间的执行差距等挑战。尽管生成式AI有潜力，但现有模型通常只是被动助手而非自主执行者。

Method: OrchestRA采用人类在环的多智能体架构：1) Orchestrator协调整个流程；2) Biologist Agent基于超过1000万关联的知识图谱进行深度推理以确定高置信度靶点；3) Chemist Agent自主检测结构口袋进行从头设计或药物重定位；4) Pharmacologist Agent通过基于生理的药代动力学(PBPK)模拟评估候选药物。这些智能体建立动态反馈循环，药代动力学和毒性特征直接触发结构重新优化。

Result: OrchestRA平台能够自主执行模拟并推理结果以驱动迭代优化，将生物学、化学和药理学统一为自主发现引擎，实现了从随机搜索到可编程循证工程学科的转变。

Conclusion: OrchestRA通过无缝集成自主执行与人类指导，实现了治疗设计的民主化，将药物发现从随机搜索转变为可编程的循证工程学科，解决了当前治疗发现中的碎片化和执行差距问题。

Abstract: Therapeutic discovery remains a formidable challenge, impeded by the fragmentation of specialized domains and the execution gap between computational design and physiological validation. Although generative AI offers promise, current models often function as passive assistants rather than as autonomous executors. Here, we introduce OrchestRA, a human-in-the-loop multi-agent platform that unifies biology, chemistry, and pharmacology into an autonomous discovery engine. Unlike static code generators, our agents actively execute simulations and reason the results to drive iterative optimization. Governed by an Orchestrator, a Biologist Agent leverages deep reasoning over a massive knowledge graph (>10 million associations) to pinpoint high-confidence targets; a Chemist Agent autonomously detects structural pockets for de novo design or drug repositioning; and a Pharmacologist Agent evaluates candidates via rigorous physiologically based pharmacokinetic (PBPK) simulations. This architecture establishes a dynamic feedback loop where pharmacokinetic and toxicity profiles directly trigger structural reoptimization. By seamlessly integrating autonomous execution with human guidance, OrchestRA democratizes therapeutic design, transforming drug discovery from a stochastic search to a programmable evidence-based engineering discipline.

</details>


### [14] [Multiple-play Stochastic Bandits with Prioritized Arm Capacity Sharing](https://arxiv.org/abs/2512.21626)
*Hong Xie,Haoran Gu,Yanying Huang,Tao Tan,Defu Lian*

Main category: cs.AI

TL;DR: 提出了一种针对LLM应用、边缘智能等资源分配问题的多臂赌博机变体，其中每个臂具有随机容量，容量单位有奖励函数，每个游戏有优先级权重，容量按优先级分配。


<details>
  <summary>Details</summary>
Motivation: 为了解决LLM应用、边缘智能等场景中的资源分配问题，这些场景中资源具有随机容量，不同任务有优先级权重，需要按优先级分配有限资源。

Method: 设计了MSB-PRS-OffOpt算法来寻找最优游戏分配策略，计算复杂度O(MK³)；然后基于此设计了近似UCB算法，结合了离线最优策略和在线学习。

Result: 证明了实例独立和实例依赖的遗憾下界分别为Ω(α₁σ√KMT)和Ω(α₁σ²M/Δ ln T)；提出的算法遗憾上界与下界匹配，分别相差√K ln KT和α₁K²因子。

Conclusion: 该工作解决了优先级资源共享机制下的非线性组合效用函数的优化和学习技术挑战，为资源分配问题提供了理论保证和实用算法。

Abstract: This paper proposes a variant of multiple-play stochastic bandits tailored to resource allocation problems arising from LLM applications, edge intelligence, etc. The model is composed of $M$ arms and $K$ plays. Each arm has a stochastic number of capacities, and each unit of capacity is associated with a reward function. Each play is associated with a priority weight. When multiple plays compete for the arm capacity, the arm capacity is allocated in a larger priority weight first manner. Instance independent and instance dependent regret lower bounds of $Ω( α_1 σ\sqrt{KM T} )$ and $Ω(α_1 σ^2 \frac{M}Δ \ln T)$ are proved, where $α_1$ is the largest priority weight and $σ$ characterizes the reward tail. When model parameters are given, we design an algorithm named \texttt{MSB-PRS-OffOpt} to locate the optimal play allocation policy with a computational complexity of $O(MK^3)$. Utilizing \texttt{MSB-PRS-OffOpt} as a subroutine, an approximate upper confidence bound (UCB) based algorithm is designed, which has instance independent and instance dependent regret upper bounds matching the corresponding lower bound up to factors of $ \sqrt{K \ln KT }$ and $α_1 K^2$ respectively. To this end, we address nontrivial technical challenges arising from optimizing and learning under a special nonlinear combinatorial utility function induced by the prioritized resource sharing mechanism.

</details>


### [15] [Towards Responsible and Explainable AI Agents with Consensus-Driven Reasoning](https://arxiv.org/abs/2512.21699)
*Eranga Bandara,Tharaka Hewa,Ross Gore,Sachin Shetty,Ravi Mukkamala,Peter Foytik,Abdul Rahman,Safdar H. Bouk,Xueping Liang,Amin Hass,Sachini Rajapakse,Ng Wee Keong,Kasun De Zoysa,Aruna Withanage,Nilaan Loganathan*

Main category: cs.AI

TL;DR: 本文提出了一种基于多模型共识和推理层治理的负责任(RAI)和可解释(XAI)AI代理架构，用于生产级代理工作流，通过异构代理共识和集中推理来提高透明度、鲁棒性和可审计性。


<details>
  <summary>Details</summary>
Motivation: 随着代理AI系统自主性增强，在可解释性、问责制、鲁棒性和治理方面面临严峻挑战。现有实现往往强调功能和可扩展性，但缺乏理解决策逻辑和执行跨代理责任的机制。

Method: 提出一种架构：异构LLM和VLM代理从共享输入上下文独立生成候选输出，暴露不确定性和分歧；专用推理代理执行结构化整合，强制执行安全策略约束，减轻幻觉和偏见，生成可审计的证据支持决策。

Result: 在多个真实世界代理AI工作流中评估该架构，证明共识驱动的推理提高了鲁棒性、透明度和跨不同应用领域的操作信任。

Conclusion: 这项工作为设计既自主可扩展又负责任可解释的代理AI系统提供了实用指导，通过构建方式实现责任和可解释性。

Abstract: Agentic AI represents a major shift in how autonomous systems reason, plan, and execute multi-step tasks through the coordination of Large Language Models (LLMs), Vision Language Models (VLMs), tools, and external services. While these systems enable powerful new capabilities, increasing autonomy introduces critical challenges related to explainability, accountability, robustness, and governance, especially when agent outputs influence downstream actions or decisions. Existing agentic AI implementations often emphasize functionality and scalability, yet provide limited mechanisms for understanding decision rationale or enforcing responsibility across agent interactions. This paper presents a Responsible(RAI) and Explainable(XAI) AI Agent Architecture for production-grade agentic workflows based on multi-model consensus and reasoning-layer governance. In the proposed design, a consortium of heterogeneous LLM and VLM agents independently generates candidate outputs from a shared input context, explicitly exposing uncertainty, disagreement, and alternative interpretations. A dedicated reasoning agent then performs structured consolidation across these outputs, enforcing safety and policy constraints, mitigating hallucinations and bias, and producing auditable, evidence-backed decisions. Explainability is achieved through explicit cross-model comparison and preserved intermediate outputs, while responsibility is enforced through centralized reasoning-layer control and agent-level constraints. We evaluate the architecture across multiple real-world agentic AI workflows, demonstrating that consensus-driven reasoning improves robustness, transparency, and operational trust across diverse application domains. This work provides practical guidance for designing agentic AI systems that are autonomous and scalable, yet responsible and explainable by construction.

</details>


### [16] [Accelerating Scientific Discovery with Autonomous Goal-evolving Agents](https://arxiv.org/abs/2512.21782)
*Yuanqi Du,Botao Yu,Tianyu Liu,Tony Shen,Junwu Chen,Jan G. Rittig,Kunyang Sun,Yikun Zhang,Zhangde Song,Bo Zhou,Cassandra Masschelein,Yingze Wang,Haorui Wang,Haojun Jia,Chao Zhang,Hongyu Zhao,Martin Ester,Teresa Head-Gordon,Carla P. Gomes,Huan Sun,Chenru Duan,Philippe Schwaller,Wengong Jin*

Main category: cs.AI

TL;DR: 论文提出SAGA框架，通过双层架构自动设计和演化科学发现中的目标函数，解决传统方法中固定目标函数作为不完美代理的问题。


<details>
  <summary>Details</summary>
Motivation: 当前科学发现代理主要依赖科学家指定的定量目标函数进行优化，但这些目标函数往往只是不完美的代理指标。对于重大科学挑战，自动设计目标函数是一个核心但尚未满足的需求。

Method: 提出科学自主目标演化代理(SAGA)，采用双层架构：外层LLM代理分析优化结果、提出新目标并将其转换为可计算的评分函数；内层在当前目标下执行解决方案优化。这种设计能够系统探索目标空间及其权衡。

Result: 在抗生素设计、无机材料设计、功能性DNA序列设计和化学过程设计等多个应用领域展示了该框架的有效性，表明自动目标制定能显著提升科学发现代理的效果。

Conclusion: 自动化目标函数设计是科学发现代理的关键能力，SAGA框架通过双层目标演化架构解决了这一挑战，为更有效的科学自主发现提供了新途径。

Abstract: There has been unprecedented interest in developing agents that expand the boundary of scientific discovery, primarily by optimizing quantitative objective functions specified by scientists. However, for grand challenges in science , these objectives are only imperfect proxies. We argue that automating objective function design is a central, yet unmet requirement for scientific discovery agents. In this work, we introduce the Scientific Autonomous Goal-evolving Agent (SAGA) to amend this challenge. SAGA employs a bi-level architecture in which an outer loop of LLM agents analyzes optimization outcomes, proposes new objectives, and converts them into computable scoring functions, while an inner loop performs solution optimization under the current objectives. This bi-level design enables systematic exploration of the space of objectives and their trade-offs, rather than treating them as fixed inputs. We demonstrate the framework through a broad spectrum of applications, including antibiotic design, inorganic materials design, functional DNA sequence design, and chemical process design, showing that automating objective formulation can substantially improve the effectiveness of scientific discovery agents.

</details>


### [17] [SpatialBench: Can Agents Analyze Real-World Spatial Biology Data?](https://arxiv.org/abs/2512.21907)
*Kenny Workman,Zhen Yang,Harihara Muralidharan,Hannah Le*

Main category: cs.AI

TL;DR: SpatialBench是一个包含146个可验证问题的空间转录组学分析基准测试，用于评估AI代理处理真实空间数据集的能力，结果显示前沿模型准确率仅为20-38%。


<details>
  <summary>Details</summary>
Motivation: 随着空间转录组学数据规模和复杂性的快速增长，计算分析成为生物学发现的主要瓶颈。虽然前沿AI代理在软件工程和通用数据分析方面有显著进步，但尚不清楚它们能否从混乱的真实空间数据集中提取生物学见解。

Method: 研究人员开发了SpatialBench基准测试，包含146个从实际空间分析工作流程中提取的可验证问题，涵盖五种空间技术和七个任务类别。每个问题提供实验数据快照和确定性评分器，用于评估关键生物学结果的恢复情况。

Result: 基准测试显示前沿模型的准确率仍然较低（20-38%），存在强烈的模型-任务和模型-平台交互作用。工具链设计对性能有显著影响，表明工具、提示、控制流和执行环境应作为一等公民进行评估和改进。

Conclusion: SpatialBench既可作为测量工具，也可作为诊断透镜，用于开发能够忠实、透明和可重复地与真实空间数据集交互的AI代理。

Abstract: Spatial transcriptomics assays are rapidly increasing in scale and complexity, making computational analysis a major bottleneck in biological discovery. Although frontier AI agents have improved dramatically at software engineering and general data analysis, it remains unclear whether they can extract biological insight from messy, real-world spatial datasets. We introduce SpatialBench, a benchmark of 146 verifiable problems derived from practical spatial analysis workflows spanning five spatial technologies and seven task categories. Each problem provides a snapshot of experimental data immediately prior to an analysis step and a deterministic grader that evaluates recovery of a key biological result. Benchmark data on frontier models shows that base model accuracy remains low (20-38% across model families), with strong model-task and model-platform interactions. Harness design has a large empirical effect on performance, indicating that tools, prompts, control flow, and execution environment should be evaluated and improved as first-class objects. SpatialBench serves both as a measurement tool and a diagnostic lens for developing agents that can interact with real spatial datasets faithfully, transparently, and reproducibly.

</details>


### [18] [Pruning as a Game: Equilibrium-Driven Sparsification of Neural Networks](https://arxiv.org/abs/2512.22106)
*Zubair Shah,Noaman Khan*

Main category: cs.AI

TL;DR: 该论文提出将神经网络剪枝建模为模型组件间战略互动的均衡结果，而非外部强加的约束，通过博弈论框架实现参数组的自然稀疏化。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法大多将稀疏性视为外部约束，通过启发式重要性评分或训练时正则化强制实现。本文旨在从根本不同的视角理解剪枝：将其视为模型组件间战略互动的均衡结果，为剪枝行为提供理论解释。

Method: 将参数组（权重、神经元、滤波器）建模为连续非合作博弈中的玩家，每个玩家选择其在网络中的参与程度以平衡贡献与冗余竞争。当继续参与成为均衡中的劣势策略时，稀疏性自然出现。基于此推导出均衡驱动的剪枝算法，联合更新网络参数和参与变量，无需显式重要性评分。

Result: 在标准基准测试上的实验表明，该方法在稀疏性-准确性权衡方面具有竞争力，同时提供了可解释、理论基础的剪枝替代方案。证明了在温和条件下，劣势玩家在均衡中会收敛到零参与度。

Conclusion: 该工作建立了剪枝作为均衡现象的原则性框架，为神经网络剪枝提供了新的理论视角和解释性方法，是现有剪枝方法的理论支撑替代方案。

Abstract: Neural network pruning is widely used to reduce model size and computational cost. Yet, most existing methods treat sparsity as an externally imposed constraint, enforced through heuristic importance scores or training-time regularization. In this work, we propose a fundamentally different perspective: pruning as an equilibrium outcome of strategic interaction among model components. We model parameter groups such as weights, neurons, or filters as players in a continuous non-cooperative game, where each player selects its level of participation in the network to balance contribution against redundancy and competition. Within this formulation, sparsity emerges naturally when continued participation becomes a dominated strategy at equilibrium. We analyze the resulting game and show that dominated players collapse to zero participation under mild conditions, providing a principled explanation for pruning behavior. Building on this insight, we derive a simple equilibrium-driven pruning algorithm that jointly updates network parameters and participation variables without relying on explicit importance scores. This work focuses on establishing a principled formulation and empirical validation of pruning as an equilibrium phenomenon, rather than exhaustive architectural or large-scale benchmarking. Experiments on standard benchmarks demonstrate that the proposed approach achieves competitive sparsity-accuracy trade-offs while offering an interpretable, theory-grounded alternative to existing pruning methods.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [19] [SENTINEL: A Multi-Modal Early Detection Framework for Emerging Cyber Threats using Telegram](https://arxiv.org/abs/2512.21380)
*Mohammad Hammas Saeed,Howie Huang*

Main category: cs.SI

TL;DR: SENTINEL框架利用社交媒体信号（语言建模+图神经网络）进行网络攻击早期检测，在Telegram数据上达到F1分数0.89


<details>
  <summary>Details</summary>
Motivation: 传统网络安全机制依赖事后检测和缓解策略，无法主动预防攻击。攻击者利用在线平台传播攻击工具、分享知识和协调行动，而专家也在在线空间讨论潜在威胁，这些社交媒体讨论可作为可靠的威胁检测指标。

Method: 提出SENTINEL框架，利用多模态信号：通过大型语言模型进行语言建模，通过图神经网络分析协调标记。使用16个Telegram公共频道的365k条消息数据，将网络安全讨论与现实世界网络攻击对齐。

Result: 在Telegram数据上，SENTINEL能够将社交媒体信号与现实世界威胁对齐，达到F1分数0.89。研究表明社交媒体讨论涉及网络威胁的活跃对话。

Conclusion: 利用语言和网络信号对于预测在线威胁至关重要。SENTINEL框架展示了社交媒体信号在网络攻击早期检测中的有效性，为主动网络安全防御提供了新途径。

Abstract: Cyberattacks pose a serious threat to modern sociotechnical systems, often resulting in severe technical and societal consequences. Attackers commonly target systems and infrastructure through methods such as malware, ransomware, or other forms of technical exploitation. Most traditional mechanisms to counter these threats rely on post-hoc detection and mitigation strategies, responding to cyber incidents only after they occur rather than preventing them proactively. Recent trends reveal social media discussions can serve as reliable indicators for detecting such threats. Malicious actors often exploit online platforms to distribute attack tools, share attack knowledge and coordinate. Experts too, often predict ongoing attacks and discuss potential breaches in online spaces. In this work, we present SENTINEL, a framework that leverages social media signals for early detection of cyber attacks. SENTINEL aligns cybersecurity discussions to realworld cyber attacks leveraging multi modal signals, i.e., combining language modeling through large language models and coordination markers through graph neural networks. We use data from 16 public channels on Telegram related to cybersecurity and open source intelligence (OSINT) that span 365k messages. We highlight that social media discussions involve active dialogue around cyber threats and leverage SENTINEL to align the signals to real-world threats with an F1 of 0.89. Our work highlights the importance of leveraging language and network signals in predicting online threats.

</details>


### [20] [ALETHEIA: Combating Social Media Influence Campaigns with Graph Neural Networks](https://arxiv.org/abs/2512.21391)
*Mohammad Hammas Saeed,Isaiah J. King,Howie Huang*

Main category: cs.SI

TL;DR: ALETHEIA系统通过图神经网络检测恶意账户并预测其在社交媒体网络中的行为，相比传统方法在Reddit和X平台上实现了3.7%的F1分数提升，并能以96.6%的AUC预测未来恶意互动。


<details>
  <summary>Details</summary>
Motivation: 在线影响力活动日益成为关注焦点，政策制定者、平台管理者和研究人员需要有效方法来对抗这些活动，保护普通用户。现有检测方法在利用影响力活动的网络结构特征方面存在不足。

Method: 提出ALETHEIA系统，采用图神经网络（GNNs）检测恶意账户，结合拓扑特征和语言特征构建检测管道。系统还首次为影响力活动设计了时间链接预测机制，通过将GNN叠加在循环神经网络（RNN）上来预测未来恶意互动。

Result: 在Reddit和X平台上，基于图表示的检测管道比传统交互和用户特征方法有显著改进，F1分数提升3.7%。时间链接预测机制能预测恶意账户之间（TTE）和恶意账户对普通用户（TUE）的未来互动，平均AUC达到96.6%。

Conclusion: 利用影响力活动的网络结构信息对于预测和检测在线空间中的恶意协调活动至关重要。ALETHEIA系统展示了图神经网络在检测恶意账户和预测其行为方面的有效性。

Abstract: Influence campaigns are a growing concern in the online spaces. Policymakers, moderators and researchers have taken various routes to fight these campaigns and make online systems safer for regular users. To this end, our paper presents ALETHEIA, a system that formalizes the detection of malicious accounts (or troll accounts) used in such operations and forecasts their behaviors within social media networks. We analyze influence campaigns on Reddit and X from different countries and highlight that detection pipelines built over a graph-based representation of campaigns using a mix of topological and linguistic features offer improvement over standard interaction and user features. ALETHEIA uses state-of-the-art Graph Neural Networks (GNNs) for detecting malicious users that can scale to large networks and achieve a 3.7% F1-score improvement over standard classification with interaction features in prior work. Furthermore, ALETHEIA employs a first temporal link prediction mechanism built for influence campaigns by stacking a GNN over a Recurrent Neural Network (RNN), which can predict future troll interactions towards other trolls and regular users with an average AUC of 96.6%. ALETHEIA predicts troll-to-troll edges (TTE) and troll-to-user edges (TUE), which can help identify regular users being affected by malicious influence efforts. Overall, our results highlight the importance of utilizing the networked nature of influence operations (i.e., structural information) when predicting and detecting malicious coordinated activity in online spaces.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [21] [US labor market conditions and migration: a reassessment of Bahar (2025)](https://arxiv.org/abs/2512.21429)
*Francisco Rodriguez,Giancarlo Bravo*

Main category: econ.EM

TL;DR: Bahar (2025)声称美国职位空缺与西南边境过境存在长期协整关系，但该结论基于对一阶差分数据的错误Engle-Granger检验。正确对水平数据进行检验时，协整关系证据消失，因此其短期和长期弹性估计方法无效。


<details>
  <summary>Details</summary>
Motivation: 本文旨在纠正Bahar (2025)研究中关于美国劳动力市场条件与移民关系的方法论错误，指出其统计检验的误用导致得出错误的协整关系结论。

Method: 重新应用Engle-Granger协整检验，但正确地对水平数据而非一阶差分数据进行检验，以验证Bahar (2025)的结论是否成立。

Result: 当Engle-Granger检验正确应用于水平数据时，美国职位空缺与西南边境过境之间的协整关系证据消失，表明Bahar (2025)的协整关系结论不成立。

Conclusion: Bahar (2025)的方法存在根本性错误，其关于美国劳动力市场条件与移民关系的分析无效，无法提供有意义的见解。

Abstract: Bahar (2025) argues that there is a long-term cointegrating relationship between US job vacancies and southwest border crossings. We show that this conclusion is based on a misspecified Engle-Granger test applied to first differences. Once the Engle-Granger test is correctly applied to levels, evidence for a cointegrating relationship vanishes, invalidating the paper's approach to estimating short- and long-run elasticities. Bahar's approach is therefore uninformative about the relationship between US labor market conditions and migration.

</details>


### [22] [Nonparametric methods for comparing distribution functionals for dependent samples with application to inequality measures](https://arxiv.org/abs/2512.21862)
*Jean-Marie Dufour,Tianyu He*

Main category: econ.EM

TL;DR: 提出了用于比较依赖样本间福利指数的渐近分布自由推断方法，包括针对任意依赖样本的交集方法和针对重叠样本的专门方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设样本独立，但在实际应用中（如追踪调查、面板数据）样本往往是依赖的，这会导致推断结果不可靠。需要开发能够处理依赖样本的福利指数比较方法。

Method: 1. 交集方法：针对任意依赖样本，提出渐近和bootstrap交集推断方法；2. 重叠样本方法：针对配对重叠样本的特殊情况，提供渐近和bootstrap方法；3. 使用影响函数方法推导渐近方差的一致估计。

Result: 模拟实验显示：重叠样本的置信区间具有满意的覆盖率和合理精度；传统独立样本假设方法在覆盖率和区间宽度上表现较差；渐近推断在处理厚尾分布时可靠性较低，而bootstrap方法提供了可行补救；交集方法在任意依赖样本下都能得到可靠结果。

Conclusion: 提出的方法能够有效处理依赖样本的福利指数比较问题，在意大利家庭财务不平等动态变化分析中展示了实际应用价值。交集方法特别适用于任意依赖结构，而重叠样本方法在配对数据中表现优异。

Abstract: This paper proposes asymptotically distribution-free inference methods for comparing a broad range of welfare indices across dependent samples, including those employed in inequality, poverty, and risk analysis. Two distinct situations are considered. \emph{First}, we propose asymptotic and bootstrap intersection methods which are completely robust to arbitrary dependence between two samples. \emph{Second}, we focus on the common case of overlapping samples -- a special form of dependent samples where sample dependence arises solely from matched pairs -- and provide asymptotic and bootstrap methods for comparing indices. We derive consistent estimates for asymptotic variances using the influence function approach. The performance of the proposed methods is studied in a simulation experiment: we find that confidence intervals with overlapping samples exhibit satisfactory coverage rates with reasonable precision, whereas conventional methods based on an assumption of independent samples have an inferior performance in terms of coverage rates and interval widths. Asymptotic inference can be less reliable when dealing with heavy-tailed distributions, while the bootstrap method provides a viable remedy, unless the variance is substantial or nonexistent. The intersection method yields reliable results with arbitrary dependent samples, including instances where overlapping samples are not feasible. We demonstrate the practical applicability of our proposed methods in analyzing dynamic changes in household financial inequality in Italy over time.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [23] [Bidirectional Human-AI Alignment in Education for Trustworthy Learning Environments](https://arxiv.org/abs/2512.21552)
*Hua Shen*

Main category: cs.CY

TL;DR: 该论文提出教育中双向人机对齐的概念，强调可信赖学习环境不仅需要将人类价值观嵌入AI系统，还需要培养教师、学生和机构解释、批判和指导AI的能力。


<details>
  <summary>Details</summary>
Motivation: AI正在变革教育，提供个性化学习、增强评估和支持教育者的机会，但也带来公平、隐私和学生自主权方面的风险。需要确保AI促进而非侵蚀教育公平、透明度和人类发展。

Method: 基于新兴研究和实际案例，探索AI从支持工具到协作伙伴的演变，分析其对教师角色、学生能动性和机构治理的影响，提出双向人机对齐的概念框架。

Result: 提出可操作策略，供政策制定者、开发者和教育者确保AI促进教育公平、透明度和人类发展，将AI采用重新定义为持续相互适应的过程。

Conclusion: 通过将AI采用重新定义为持续相互适应的过程，展望人类与智能系统共同学习、创新和成长的未来教育愿景，强调双向对齐对构建可信赖学习环境的重要性。

Abstract: Artificial intelligence (AI) is transforming education, offering unprecedented opportunities to personalize learning, enhance assessment, and support educators. Yet these opportunities also introduce risks related to equity, privacy, and student autonomy. This chapter develops the concept of bidirectional human-AI alignment in education, emphasizing that trustworthy learning environments arise not only from embedding human values into AI systems but also from equipping teachers, students, and institutions with the skills to interpret, critique, and guide these technologies. Drawing on emerging research and practical case examples, we explore AI's evolution from support tool to collaborative partner, highlighting its impacts on teacher roles, student agency, and institutional governance. We propose actionable strategies for policymakers, developers, and educators to ensure that AI advances equity, transparency, and human flourishing rather than eroding them. By reframing AI adoption as an ongoing process of mutual adaptation, the chapter envisions a future in which humans and intelligent systems learn, innovate, and grow together.

</details>


### [24] [Agent-based simulation of online social networks and disinformation](https://arxiv.org/abs/2512.22082)
*Alejandro Buitrago López,Alberto Ortega Pastor,David Montoro Aguilera,Mario Fernández Tárraga,Jesús Verdú Chacón,Javier Pastor-Galindo,José A. Ruipérez-Valiente*

Main category: cs.CY

TL;DR: 提出一个模拟社交网络框架，结合基于人口统计的人格特征、有限状态行为自动机和LLM生成内容，用于研究信息动态和虚假信息影响


<details>
  <summary>Details</summary>
Motivation: 在线社交网络研究常受平台不透明、数据访问受限和伦理约束阻碍，现有模拟框架缺乏真实性和可解释性

Method: 开发模拟框架：1) 智能体具有基于人口统计的人格特征和有限状态行为自动机；2) LLM生成模块产生符合智能体背景的社交媒体内容；3) 红色模块实施DISARM启发的工作流协调恶意智能体的虚假信息活动；4) Mastodon可视化层支持实时检查和事后验证

Result: 通过拓扑指标和LLM内容评估验证了合成社交网络的结构、行为和语言真实性，框架能创建可定制和可控的社交网络环境

Conclusion: 该框架为研究信息动态和虚假信息影响提供了真实、可解释且可定制的社交网络模拟环境

Abstract: Research on online social networks (OSNs) is often hindered by platform opacity, limited access to data, and ethical constraints. Simulation offer a valuable alternative, but existing frameworks frequently lack realism and explainability. This paper presents a simulation framework that models synthetic social networks with agents endowed with demographic-based personality traits and finite-state behavioral automata, enabling realistic and interpretable actions. A generative module powered by a large language model (LLM) produces context-aware social media posts consistent with each agent's profile and memory. In parallel, a red module implements DISARM-inspired workflows to orchestrate disinformation campaigns executed by malicious agents targeting simulated audiences. A Mastodon-based visualization layer supports real-time inspection and post-hoc validation of agent activity within a familiar interface. We evaluate the resulting synthetic social networks using topological metrics and LLM-based content assessments, demonstrating structural, behavioral, and linguistic realism. Overall, the framework enables the creation of customizable and controllable social network environments for studying information dynamics and the effects of disinformation.

</details>

{"id": "2510.23638", "categories": ["cs.ET", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23638", "abs": "https://arxiv.org/abs/2510.23638", "authors": ["Songyuan Li", "Teng Wang", "Jinrong Tang", "Ruiqi Liu", "Yuyao Lu", "Feng Xu", "Bin Gao", "Xiangwei Zhu"], "title": "Bridging Function Approximation and Device Physics via Negative Differential Resistance Networks", "comment": null, "summary": "Achieving fully analog neural computation requires hardware that can natively\nimplement both linear and nonlinear operations with high efficiency. While\nanalogue matrix-vector multiplication has advanced via compute-in-memory\narchitectures, nonlinear activation functions remain a bottleneck, often\nrequiring digital or hybrid solutions. Inspired by the Kolmogorov-Arnold\nframework, we propose KANalogue, a fully analogue implementation of\nKolmogorov-Arnold Networks (KANs) using negative differential resistance\ndevices as physical realizations of learnable univariate basis functions. By\nleveraging the intrinsic negative differential resistance characteristics of\ntunnel diodes fabricated from NbSi2N4/HfSi2N4 heterostructures, we construct\ncoordinate-wise nonlinearities with distinct curvature and support profiles. We\nextract I-V data from fabricated armchair and zigzag devices, fit high-order\npolynomials to emulate diode behavior in software, and train KANs on vision\nbenchmarks using these learned basis functions. Our results demonstrate that\nKANalogue can approximate complex functions with minimal parameters while\nmaintaining classification accuracy competitive with digital baselines. This\nwork bridges device-level physics and function approximation theory, charting a\npath toward scalable, energy-efficient analogue machine learning systems.", "AI": {"tldr": "KANalogue\u662f\u4e00\u79cd\u5b8c\u5168\u6a21\u62df\u7684Kolmogorov-Arnold\u7f51\u7edc\u5b9e\u73b0\uff0c\u5229\u7528\u8d1f\u5fae\u5206\u7535\u963b\u5668\u4ef6\u4f5c\u4e3a\u53ef\u5b66\u4e60\u5355\u53d8\u91cf\u57fa\u51fd\u6570\u7684\u7269\u7406\u5b9e\u73b0\uff0c\u4e3a\u9ad8\u6548\u6a21\u62df\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u63d0\u4f9b\u65b0\u8def\u5f84\u3002", "motivation": "\u4f20\u7edf\u6a21\u62df\u795e\u7ecf\u7f51\u7edc\u4e2d\uff0c\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\u6210\u4e3a\u6027\u80fd\u74f6\u9888\uff0c\u901a\u5e38\u9700\u8981\u6570\u5b57\u6216\u6df7\u5408\u89e3\u51b3\u65b9\u6848\u3002\u672c\u6587\u65e8\u5728\u5b9e\u73b0\u5b8c\u5168\u6a21\u62df\u7684\u795e\u7ecf\u7f51\u7edc\u8ba1\u7b97\uff0c\u7279\u522b\u662f\u89e3\u51b3\u975e\u7ebf\u6027\u64cd\u4f5c\u7684\u6a21\u62df\u5b9e\u73b0\u95ee\u9898\u3002", "method": "\u5229\u7528NbSi2N4/HfSi2N4\u5f02\u8d28\u7ed3\u6784\u96a7\u9053\u4e8c\u6781\u7ba1\u7684\u8d1f\u5fae\u5206\u7535\u963b\u7279\u6027\uff0c\u6784\u5efa\u5177\u6709\u4e0d\u540c\u66f2\u7387\u548c\u652f\u6491\u8f6e\u5ed3\u7684\u5750\u6807\u65b9\u5411\u975e\u7ebf\u6027\u51fd\u6570\uff0c\u901a\u8fc7\u63d0\u53d6I-V\u6570\u636e\u5e76\u62df\u5408\u9ad8\u9636\u591a\u9879\u5f0f\u6765\u6a21\u62df\u4e8c\u6781\u7ba1\u884c\u4e3a\uff0c\u8bad\u7ec3KAN\u7f51\u7edc\u3002", "result": "KANalogue\u80fd\u591f\u4ee5\u6700\u5c11\u7684\u53c2\u6570\u903c\u8fd1\u590d\u6742\u51fd\u6570\uff0c\u540c\u65f6\u5728\u89c6\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4fdd\u6301\u4e0e\u6570\u5b57\u57fa\u7ebf\u76f8\u5f53\u7684\u5206\u7c7b\u51c6\u786e\u7387\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u8fde\u63a5\u4e86\u5668\u4ef6\u7ea7\u7269\u7406\u548c\u51fd\u6570\u903c\u8fd1\u7406\u8bba\uff0c\u4e3a\u53ef\u6269\u5c55\u3001\u9ad8\u80fd\u6548\u7684\u6a21\u62df\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u5f00\u8f9f\u4e86\u65b0\u8def\u5f84\u3002"}}
{"id": "2510.24510", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2510.24510", "abs": "https://arxiv.org/abs/2510.24510", "authors": ["Hugo Alcaraz-Herrera", "Michail-Antisthenis Tsompanas", "Igor Balaz", "Andrew Adamatzky"], "title": "Evaluating Fitness Averaging Strategies in Cooperative NeuroCoEvolution for Automated Soft Actuator Design", "comment": null, "summary": "Soft robotics are increasingly favoured in specific applications such as\nhealthcare, due to their adaptability, which stems from the non-linear\nproperties of their building materials. However, these properties also pose\nsignificant challenges in designing the morphologies and controllers of soft\nrobots. The relatively short history of this field has not yet produced\nsufficient knowledge to consistently derive optimal solutions. Consequently, an\nautomated process for the design of soft robot morphologies can be extremely\nhelpful. This study focusses on the cooperative NeuroCoEvolution of networks\nthat are indirect representations of soft robot actuators. Both the\nmorphologies and controllers represented by Compositional Pattern Producing\nNetworks are evolved using the well-established method NeuroEvolution of\nAugmented Topologies (CPPN-NEAT). The CoEvolution of controllers and\nmorphologies is implemented using the top n individuals from the cooperating\npopulation, with various averaging methods tested to determine the fitness of\nthe evaluated individuals. The test-case application for this research is the\noptimisation of a soft actuator for a drug delivery system. The primary metric\nused is the maximum displacement of one end of the actuator in a specified\ndirection. Additionally, the robustness of the evolved morphologies is assessed\nagainst a range of randomly generated controllers to simulate potential noise\nin real-world applications. The results of this investigation indicate that\nCPPN-NEAT produces superior morphologies compared to previously published\nresults from multi-objective optimisation, with reduced computational effort\nand time. Moreover, the best configuration is found to be CoEvolution with the\ntwo best individuals from the cooperative population and the averaging of their\nfitness using the weighted mean method.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCPPN-NEAT\u7684\u534f\u540c\u795e\u7ecf\u534f\u540c\u8fdb\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u8bbe\u8ba1\u8f6f\u673a\u5668\u4eba\u6267\u884c\u5668\u7684\u5f62\u6001\u548c\u63a7\u5236\u5668\uff0c\u5e94\u7528\u4e8e\u836f\u7269\u8f93\u9001\u7cfb\u7edf\uff0c\u76f8\u6bd4\u591a\u76ee\u6807\u4f18\u5316\u65b9\u6cd5\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u8f6f\u673a\u5668\u4eba\u7531\u4e8e\u5176\u6750\u6599\u7684\u975e\u7ebf\u6027\u7279\u6027\u5728\u533b\u7597\u7b49\u9886\u57df\u5177\u6709\u9002\u5e94\u6027\u4f18\u52bf\uff0c\u4f46\u8fd9\u4e9b\u7279\u6027\u4e5f\u7ed9\u5f62\u6001\u548c\u63a7\u5236\u5668\u8bbe\u8ba1\u5e26\u6765\u6311\u6218\u3002\u8be5\u9886\u57df\u5386\u53f2\u8f83\u77ed\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u8bbe\u8ba1\u77e5\u8bc6\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u5316\u8bbe\u8ba1\u8fc7\u7a0b\u3002", "method": "\u4f7f\u7528CPPN-NEAT\u65b9\u6cd5\u534f\u540c\u8fdb\u5316\u8f6f\u673a\u5668\u4eba\u6267\u884c\u5668\u7684\u5f62\u6001\u548c\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u5408\u4f5c\u79cd\u7fa4\u4e2d\u7684\u524dn\u4e2a\u4e2a\u4f53\u5b9e\u73b0\u534f\u540c\u8fdb\u5316\uff0c\u6d4b\u8bd5\u4e86\u591a\u79cd\u5e73\u5747\u65b9\u6cd5\u6765\u786e\u5b9a\u4e2a\u4f53\u9002\u5e94\u5ea6\u3002", "result": "CPPN-NEAT\u65b9\u6cd5\u76f8\u6bd4\u4e4b\u524d\u7684\u591a\u76ee\u6807\u4f18\u5316\u65b9\u6cd5\u4ea7\u751f\u4e86\u66f4\u4f18\u7684\u5f62\u6001\uff0c\u8ba1\u7b97\u91cf\u548c\u65f6\u95f4\u66f4\u5c11\u3002\u6700\u4f73\u914d\u7f6e\u662f\u4f7f\u7528\u5408\u4f5c\u79cd\u7fa4\u4e2d\u524d\u4e24\u4e2a\u6700\u4f73\u4e2a\u4f53\u8fdb\u884c\u534f\u540c\u8fdb\u5316\uff0c\u5e76\u91c7\u7528\u52a0\u6743\u5e73\u5747\u65b9\u6cd5\u8ba1\u7b97\u9002\u5e94\u5ea6\u3002", "conclusion": "\u534f\u540c\u795e\u7ecf\u534f\u540c\u8fdb\u5316\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u4f18\u5316\u8f6f\u673a\u5668\u4eba\u6267\u884c\u5668\u7684\u8bbe\u8ba1\uff0c\u5728\u836f\u7269\u8f93\u9001\u7cfb\u7edf\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u4e3a\u8f6f\u673a\u5668\u4eba\u81ea\u52a8\u5316\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.23934", "categories": ["cs.CY", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2510.23934", "abs": "https://arxiv.org/abs/2510.23934", "authors": ["Alec Sathiyamoorthy", "Wenhao Zhou", "Xiangmin Zhou", "Xiaodong Li", "Iqbal Gondal"], "title": "MFiSP: A Multimodal Fire Spread Prediction Framework", "comment": null, "summary": "The 2019-2020 Black Summer bushfires in Australia devastated 19 million\nhectares, destroyed 3,000 homes, and lasted seven months, demonstrating the\nescalating scale and urgency of wildfire threats requiring better forecasting\nfor effective response. Traditional fire modeling relies on manual\ninterpretation by Fire Behaviour Analysts (FBAns) and static environmental\ndata, often leading to inaccuracies and operational limitations. Emerging data\nsources, such as NASA's FIRMS satellite imagery and Volunteered Geographic\nInformation, offer potential improvements by enabling dynamic fire spread\nprediction. This study proposes a Multimodal Fire Spread Prediction Framework\n(MFiSP) that integrates social media data and remote sensing observations to\nenhance forecast accuracy. By adapting fuel map manipulation strategies between\nassimilation cycles, the framework dynamically adjusts fire behavior\npredictions to align with the observed rate of spread. We evaluate the efficacy\nof MFiSP using synthetically generated fire event polygons across multiple\nscenarios, analyzing individual and combined impacts on forecast perimeters.\nResults suggest that our MFiSP integrating multimodal data can improve fire\nspread prediction beyond conventional methods reliant on FBAn expertise and\nstatic inputs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u706b\u707e\u8513\u5ef6\u9884\u6d4b\u6846\u67b6(MFiSP)\uff0c\u6574\u5408\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u548c\u9065\u611f\u89c2\u6d4b\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u71c3\u6599\u5730\u56fe\u6765\u63d0\u5347\u706b\u707e\u8513\u5ef6\u9884\u6d4b\u7cbe\u5ea6", "motivation": "2019-2020\u5e74\u6fb3\u5927\u5229\u4e9a\u9ed1\u8272\u590f\u5b63\u5c71\u706b\u9020\u6210\u5de8\u5927\u7834\u574f\uff0c\u51f8\u663e\u4e86\u6539\u8fdb\u706b\u707e\u9884\u6d4b\u7684\u7d27\u8feb\u6027\u3002\u4f20\u7edf\u706b\u707e\u6a21\u578b\u4f9d\u8d56\u4e13\u5bb6\u624b\u52a8\u89e3\u91ca\u548c\u9759\u6001\u73af\u5883\u6570\u636e\uff0c\u5b58\u5728\u51c6\u786e\u6027\u548c\u64cd\u4f5c\u9650\u5236", "method": "\u5f00\u53d1\u4e86\u591a\u6a21\u6001\u706b\u707e\u8513\u5ef6\u9884\u6d4b\u6846\u67b6\uff0c\u6574\u5408NASA FIRMS\u536b\u661f\u5f71\u50cf\u548c\u5fd7\u613f\u8005\u5730\u7406\u4fe1\u606f\uff0c\u5728\u6570\u636e\u540c\u5316\u5468\u671f\u95f4\u8c03\u6574\u71c3\u6599\u5730\u56fe\u64cd\u4f5c\u7b56\u7565\uff0c\u52a8\u6001\u8c03\u6574\u706b\u707e\u884c\u4e3a\u9884\u6d4b\u4ee5\u5339\u914d\u89c2\u6d4b\u8513\u5ef6\u901f\u7387", "result": "\u4f7f\u7528\u5408\u6210\u751f\u6210\u7684\u706b\u707e\u4e8b\u4ef6\u591a\u8fb9\u5f62\u5728\u591a\u79cd\u573a\u666f\u4e0b\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\u7684MFiSP\u6846\u67b6\u76f8\u6bd4\u4f9d\u8d56\u4e13\u5bb6\u77e5\u8bc6\u548c\u9759\u6001\u8f93\u5165\u7684\u4f20\u7edf\u65b9\u6cd5\u80fd\u6539\u5584\u706b\u707e\u8513\u5ef6\u9884\u6d4b", "conclusion": "\u591a\u6a21\u6001\u6570\u636e\u6574\u5408\u65b9\u6cd5\u80fd\u591f\u8d85\u8d8a\u4f20\u7edf\u4f9d\u8d56FBAn\u4e13\u4e1a\u77e5\u8bc6\u548c\u9759\u6001\u8f93\u5165\u7684\u706b\u707e\u9884\u6d4b\u65b9\u6cd5\uff0c\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u706b\u707e\u8513\u5ef6\u9884\u6d4b"}}
{"id": "2510.23669", "categories": ["econ.GN", "cs.AI", "cs.CY", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2510.23669", "abs": "https://arxiv.org/abs/2510.23669", "authors": ["Peeyush Agarwal", "Harsh Agarwal", "Akshat Ranaa"], "title": "What Work is AI Actually Doing? Uncovering the Drivers of Generative AI Adoption", "comment": "22 pages", "summary": "Purpose: The rapid integration of artificial intelligence (AI) systems like\nChatGPT, Claude AI, etc., has a deep impact on how work is done. Predicting how\nAI will reshape work requires understanding not just its capabilities, but how\nit is actually being adopted. This study investigates which intrinsic task\ncharacteristics drive users' decisions to delegate work to AI systems.\nMethodology: This study utilizes the Anthropic Economic Index dataset of four\nmillion Claude AI interactions mapped to O*NET tasks. We systematically scored\neach task across seven key dimensions: Routine, Cognitive, Social Intelligence,\nCreativity, Domain Knowledge, Complexity, and Decision Making using 35\nparameters. We then employed multivariate techniques to identify latent task\narchetypes and analyzed their relationship with AI usage. Findings: Tasks\nrequiring high creativity, complexity, and cognitive demand, but low\nroutineness, attracted the most AI engagement. Furthermore, we identified three\ntask archetypes: Dynamic Problem Solving, Procedural & Analytical Work, and\nStandardized Operational Tasks, demonstrating that AI applicability is best\npredicted by a combination of task characteristics, over individual factors.\nOur analysis revealed highly concentrated AI usage patterns, with just 5% of\ntasks accounting for 59% of all interactions. Originality: This research\nprovides the first systematic evidence linking real-world generative AI usage\nto a comprehensive, multi-dimensional framework of intrinsic task\ncharacteristics. It introduces a data-driven classification of work archetypes\nthat offers a new framework for analyzing the emerging human-AI division of\nlabor.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5206\u6790400\u4e07\u6b21Claude AI\u4ea4\u4e92\u6570\u636e\uff0c\u53d1\u73b0\u9ad8\u521b\u9020\u6027\u3001\u590d\u6742\u6027\u548c\u8ba4\u77e5\u9700\u6c42\u4f46\u4f4e\u5e38\u89c4\u6027\u7684\u4efb\u52a1\u6700\u5438\u5f15AI\u53c2\u4e0e\uff0c\u5e76\u8bc6\u522b\u51fa\u4e09\u79cd\u4efb\u52a1\u539f\u578b\uff0c\u5176\u4e2d\u4ec55%\u7684\u4efb\u52a1\u5360\u636e\u4e8659%\u7684AI\u4f7f\u7528\u91cf\u3002", "motivation": "\u9884\u6d4bAI\u5982\u4f55\u91cd\u5851\u5de5\u4f5c\u9700\u8981\u4e86\u89e3\u5176\u5b9e\u9645\u91c7\u7528\u65b9\u5f0f\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u80fd\u529b\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u54ea\u4e9b\u5185\u5728\u4efb\u52a1\u7279\u5f81\u9a71\u52a8\u7528\u6237\u5c06\u5de5\u4f5c\u59d4\u6258\u7ed9AI\u7cfb\u7edf\u3002", "method": "\u4f7f\u7528Anthropic\u7ecf\u6d4e\u6307\u6570\u6570\u636e\u96c6\u4e2d\u7684400\u4e07\u6b21Claude AI\u4ea4\u4e92\u6570\u636e\uff0c\u6620\u5c04\u5230O*NET\u4efb\u52a1\uff0c\u901a\u8fc735\u4e2a\u53c2\u6570\u7cfb\u7edf\u8bc4\u4f307\u4e2a\u5173\u952e\u7ef4\u5ea6\uff0c\u5e76\u91c7\u7528\u591a\u5143\u6280\u672f\u8bc6\u522b\u6f5c\u5728\u4efb\u52a1\u539f\u578b\u3002", "result": "\u9ad8\u521b\u9020\u6027\u3001\u590d\u6742\u6027\u548c\u8ba4\u77e5\u9700\u6c42\u4f46\u4f4e\u5e38\u89c4\u6027\u7684\u4efb\u52a1\u83b7\u5f97\u6700\u591aAI\u53c2\u4e0e\u3002\u8bc6\u522b\u51fa\u4e09\u79cd\u4efb\u52a1\u539f\u578b\uff1a\u52a8\u6001\u95ee\u9898\u89e3\u51b3\u3001\u7a0b\u5e8f\u6027\u4e0e\u5206\u6790\u5de5\u4f5c\u3001\u6807\u51c6\u5316\u64cd\u4f5c\u4efb\u52a1\u3002AI\u9002\u7528\u6027\u6700\u597d\u901a\u8fc7\u4efb\u52a1\u7279\u5f81\u7ec4\u5408\u800c\u975e\u5355\u4e2a\u56e0\u7d20\u6765\u9884\u6d4b\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u5730\u5c06\u771f\u5b9e\u4e16\u754c\u751f\u6210\u5f0fAI\u4f7f\u7528\u4e0e\u5168\u9762\u7684\u591a\u7ef4\u4efb\u52a1\u7279\u5f81\u6846\u67b6\u8054\u7cfb\u8d77\u6765\uff0c\u4e3a\u5206\u6790\u65b0\u5174\u7684\u4eba\u673a\u5206\u5de5\u63d0\u4f9b\u4e86\u6570\u636e\u9a71\u52a8\u7684\u5de5\u4f5c\u539f\u578b\u5206\u7c7b\u6846\u67b6\u3002"}}
{"id": "2510.23762", "categories": ["econ.EM"], "pdf": "https://arxiv.org/pdf/2510.23762", "abs": "https://arxiv.org/abs/2510.23762", "authors": ["Raimondo Pala"], "title": "Control VAR: a counterfactual based approach to inference in macroeconomics", "comment": null, "summary": "This paper addresses the challenges of giving a causal interpretation to\nvector autoregressions (VARs). I show that under independence assumptions VARs\ncan identify average treatment effects, average causal responses, or a mix of\nthe two, depending on the distribution of the policy. But what about situations\nin which the economist cannot rely on independence assumptions? I propose an\nalternative method, defined as control-VAR, which uses control variables to\nestimate causal effects. Control-VAR can estimate average treatment effects on\nthe treated for dummy policies or average causal responses over time for\ncontinuous policies.\n  The advantages of control-based approaches are demonstrated by examining the\nimpact of natural disasters on the US economy, using Germany as a control.\nContrary to previous literature, the results indicate that natural disasters\nhave a negative economic impact without any cyclical positive effect. These\nfindings suggest that control-VARs provide a viable alternative to strict\nindependence assumptions, offering more credible causal estimates and\nsignificant implications for policy design in response to natural disasters.", "AI": {"tldr": "\u672c\u6587\u63d0\u51facontrol-VAR\u65b9\u6cd5\uff0c\u4f7f\u7528\u63a7\u5236\u53d8\u91cf\u800c\u975e\u72ec\u7acb\u6027\u5047\u8bbe\u6765\u4f30\u8ba1VAR\u6a21\u578b\u4e2d\u7684\u56e0\u679c\u6548\u5e94\uff0c\u5e76\u901a\u8fc7\u81ea\u7136\u707e\u5bb3\u5bf9\u7f8e\u56fd\u7ecf\u6d4e\u5f71\u54cd\u7684\u5b9e\u8bc1\u5206\u6790\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edfVAR\u6a21\u578b\u4f9d\u8d56\u72ec\u7acb\u6027\u5047\u8bbe\u6765\u83b7\u5f97\u56e0\u679c\u89e3\u91ca\uff0c\u4f46\u5728\u8bb8\u591a\u5b9e\u9645\u7ecf\u6d4e\u60c5\u5883\u4e2d\u8fd9\u79cd\u5047\u8bbe\u4e0d\u6210\u7acb\uff0c\u9700\u8981\u5f00\u53d1\u4e0d\u4f9d\u8d56\u72ec\u7acb\u6027\u5047\u8bbe\u7684\u56e0\u679c\u8bc6\u522b\u65b9\u6cd5\u3002", "method": "\u63d0\u51facontrol-VAR\u65b9\u6cd5\uff0c\u4f7f\u7528\u63a7\u5236\u53d8\u91cf\uff08\u5982\u5fb7\u56fd\u4f5c\u4e3a\u63a7\u5236\u7ec4\uff09\u6765\u4f30\u8ba1\u56e0\u679c\u6548\u5e94\uff0c\u53ef\u4f30\u8ba1\u865a\u62df\u653f\u7b56\u7684\u5e73\u5747\u5904\u7406\u6548\u5e94\u6216\u8fde\u7eed\u653f\u7b56\u7684\u5e73\u5747\u56e0\u679c\u54cd\u5e94\u3002", "result": "\u5b9e\u8bc1\u5206\u6790\u53d1\u73b0\u81ea\u7136\u707e\u5bb3\u5bf9\u7f8e\u56fd\u7ecf\u6d4e\u6709\u8d1f\u9762\u5f71\u54cd\uff0c\u6ca1\u6709\u5468\u671f\u6027\u6b63\u9762\u6548\u5e94\uff0c\u8fd9\u4e0e\u5148\u524d\u6587\u732e\u7ed3\u8bba\u4e0d\u540c\uff0c\u8868\u660econtrol-VAR\u65b9\u6cd5\u80fd\u63d0\u4f9b\u66f4\u53ef\u4fe1\u7684\u56e0\u679c\u4f30\u8ba1\u3002", "conclusion": "control-VAR\u4e3a\u4e25\u683c\u72ec\u7acb\u6027\u5047\u8bbe\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u653f\u7b56\u8bbe\u8ba1\u7279\u522b\u662f\u5e94\u5bf9\u81ea\u7136\u707e\u5bb3\u65b9\u9762\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.23691", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23691", "abs": "https://arxiv.org/abs/2510.23691", "authors": ["Zihao Wang", "Xujing Li", "Yining Ye", "Junjie Fang", "Haoming Wang", "Longxiang Liu", "Shihao Liang", "Junting Lu", "Zhiyong Wu", "Jiazhan Feng", "Wanjun Zhong", "Zili Li", "Yu Wang", "Yu Miao", "Bo Zhou", "Yuanfan Li", "Hao Wang", "Zhongkai Zhao", "Faming Wu", "Zhengxuan Jiang", "Weihao Tan", "Heyuan Yao", "Shi Yan", "Xiangyang Li", "Yitao Liang", "Yujia Qin", "Guang Shi"], "title": "Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents", "comment": null, "summary": "We present Game-TARS, a generalist game agent trained with a unified,\nscalable action space anchored to human-aligned native keyboard-mouse inputs.\nUnlike API- or GUI-based approaches, this paradigm enables large-scale\ncontinual pre-training across heterogeneous domains, including OS, web, and\nsimulation games. Game-TARS is pre-trained on over 500B tokens with diverse\ntrajectories and multimodal data. Key techniques include a decaying continual\nloss to reduce causal confusion and an efficient Sparse-Thinking strategy that\nbalances reasoning depth and inference cost. Experiments show that Game-TARS\nachieves about 2 times the success rate over the previous sota model on\nopen-world Minecraft tasks, is close to the generality of fresh humans in\nunseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet\nin FPS benchmarks. Scaling results on training-time and test-time confirm that\nthe unified action space sustains improvements when scaled to cross-game and\nmultimodal data. Our results demonstrate that simple, scalable action\nrepresentations combined with large-scale pre-training provide a promising path\ntoward generalist agents with broad computer-use abilities.", "AI": {"tldr": "Game-TARS\u662f\u4e00\u4e2a\u901a\u7528\u6e38\u620f\u667a\u80fd\u4f53\uff0c\u4f7f\u7528\u7edf\u4e00\u3001\u53ef\u6269\u5c55\u7684\u952e\u76d8\u9f20\u6807\u52a8\u4f5c\u7a7a\u95f4\u8fdb\u884c\u8bad\u7ec3\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u8de8\u9886\u57df\u9884\u8bad\u7ec3\u5728\u591a\u79cd\u6e38\u620f\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u8de8\u5f02\u6784\u9886\u57df\uff08\u64cd\u4f5c\u7cfb\u7edf\u3001\u7f51\u9875\u3001\u6a21\u62df\u6e38\u620f\uff09\u6301\u7eed\u9884\u8bad\u7ec3\u7684\u901a\u7528\u6e38\u620f\u667a\u80fd\u4f53\uff0c\u907f\u514dAPI\u6216GUI\u65b9\u6cd5\u7684\u9650\u5236\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u4eba\u7c7b\u952e\u76d8\u9f20\u6807\u8f93\u5165\u7684\u7edf\u4e00\u52a8\u4f5c\u7a7a\u95f4\uff0c\u8fdb\u884c500B token\u7684\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\uff0c\u91c7\u7528\u8870\u51cf\u6301\u7eed\u635f\u5931\u51cf\u5c11\u56e0\u679c\u6df7\u6dc6\uff0c\u4ee5\u53ca\u7a00\u758f\u601d\u8003\u7b56\u7565\u5e73\u8861\u63a8\u7406\u6df1\u5ea6\u548c\u63a8\u7406\u6210\u672c\u3002", "result": "\u5728\u5f00\u653e\u4e16\u754cMinecraft\u4efb\u52a1\u4e2d\u6210\u529f\u7387\u662f\u4e4b\u524d\u6700\u4f73\u6a21\u578b\u76842\u500d\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u7f51\u98753D\u6e38\u620f\u4e2d\u63a5\u8fd1\u4eba\u7c7b\u65b0\u624b\u6c34\u5e73\uff0c\u5728FPS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8aGPT-5\u3001Gemini-2.5-Pro\u548cClaude-4-Sonnet\u3002", "conclusion": "\u7b80\u5355\u53ef\u6269\u5c55\u7684\u52a8\u4f5c\u8868\u793a\u7ed3\u5408\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u4e3a\u5f00\u53d1\u5177\u6709\u5e7f\u6cdb\u8ba1\u7b97\u673a\u4f7f\u7528\u80fd\u529b\u7684\u901a\u7528\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u8def\u5f84\u3002"}}
{"id": "2510.23740", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.23740", "abs": "https://arxiv.org/abs/2510.23740", "authors": ["Shay Gilpin", "Michael Herty"], "title": "A modified particle filter that reduces weight collapse", "comment": null, "summary": "Particle filters are a widely used Monte Carlo based data assimilation\ntechnique that estimates the probability distribution of a system's state\nconditioned on observations through a collection of weights and particles. A\nknown problem for particle filters is weight collapse, or degeneracy, where a\nsingle weight attains a value of one while all others are close to zero,\nthereby collapsing the estimated distribution. We address this issue by\nintroducing a novel modification to the particle filter that is simple to\nimplement and inspired by energy-based diversity measures. Our approach adjusts\nparticle weights to minimize a two-body energy potential, promoting balanced\nweight distributions and mitigating collapse. We demonstrate the performance of\nthis modified particle filter in a series of numerical experiments with linear\nand nonlinear dynamical models, where we compare with the classical particle\nfilter and ensemble Kalman filters in the nonlinear case. We find that our new\napproach improves weight distributions compared to the classical particle\nfilter and thereby improve state estimates.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u80fd\u91cf\u591a\u6837\u6027\u5ea6\u91cf\u7684\u6539\u8fdb\u7c92\u5b50\u6ee4\u6ce2\u5668\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u53cc\u4f53\u80fd\u91cf\u52bf\u6765\u8c03\u6574\u7c92\u5b50\u6743\u91cd\uff0c\u9632\u6b62\u6743\u91cd\u5d29\u6e83\uff0c\u63d0\u9ad8\u72b6\u6001\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u7c92\u5b50\u6ee4\u6ce2\u5668\u4e2d\u5e38\u89c1\u7684\u6743\u91cd\u5d29\u6e83\u95ee\u9898\uff0c\u5373\u5355\u4e2a\u6743\u91cd\u63a5\u8fd11\u800c\u5176\u4ed6\u6743\u91cd\u63a5\u8fd10\uff0c\u5bfc\u81f4\u4f30\u8ba1\u5206\u5e03\u5d29\u6e83\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u80fd\u91cf\u591a\u6837\u6027\u5ea6\u91cf\u7684\u6743\u91cd\u8c03\u6574\u65b9\u6cd5\uff0c\u6700\u5c0f\u5316\u53cc\u4f53\u80fd\u91cf\u52bf\u6765\u4fc3\u8fdb\u6743\u91cd\u5206\u5e03\u5e73\u8861\u3002", "result": "\u5728\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u6a21\u578b\u7684\u6570\u503c\u5b9e\u9a8c\u4e2d\uff0c\u76f8\u6bd4\u7ecf\u5178\u7c92\u5b50\u6ee4\u6ce2\u5668\uff0c\u65b0\u65b9\u6cd5\u6539\u5584\u4e86\u6743\u91cd\u5206\u5e03\u5e76\u63d0\u9ad8\u4e86\u72b6\u6001\u4f30\u8ba1\u7cbe\u5ea6\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6539\u8fdb\u7c92\u5b50\u6ee4\u6ce2\u5668\u80fd\u6709\u6548\u7f13\u89e3\u6743\u91cd\u5d29\u6e83\u95ee\u9898\uff0c\u5728\u72b6\u6001\u4f30\u8ba1\u65b9\u9762\u4f18\u4e8e\u7ecf\u5178\u7c92\u5b50\u6ee4\u6ce2\u5668\u3002"}}
{"id": "2510.23806", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.23806", "abs": "https://arxiv.org/abs/2510.23806", "authors": ["Samuel Chevalier", "Duncan Starkenburg", "Robert Parker", "Noah Rhodes"], "title": "Maximal Load Shedding Verification for Neural Network Models of AC Line Switching", "comment": null, "summary": "Solving for globally optimal line switching decisions in AC transmission\ngrids can be intractability slow. Machine learning (ML) models, meanwhile, can\nbe trained to predict near-optimal decisions at a fraction of the speed.\nVerifying the performance and impact of these ML models on network operation,\nhowever, is a critically important step prior to their actual deployment. In\nthis paper, we train a Neural Network (NN) to solve the optimal power shutoff\nline switching problem. To assess the worst-case load shedding induced by this\nmodel, we propose a bilevel attacker-defender verification approach that finds\nthe NN line switching decisions that cause the highest quantity of network load\nshedding. Solving this problem to global optimality is challenging (due to AC\npower flow and NN nonconvexities), so our approach exploits a convex relaxation\nof the AC physics, combined with a local NN search, to find a guaranteed lower\nbound on worst--case load shedding. These under-approximation bounds are solved\nvia MathOptAI.jl. We benchmark against a random sampling approach, and we find\nthat our optimization-based approach always finds larger load shedding. Test\nresults are collected on multiple PGLib test cases and on trained NN models\nwhich contain more than 10 million model parameters.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5c42\u653b\u51fb\u8005-\u9632\u5fa1\u8005\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u795e\u7ecf\u7f51\u7edc\u5728\u7535\u529b\u7cfb\u7edf\u7ebf\u8def\u5207\u6362\u51b3\u7b56\u4e2d\u7684\u6700\u574f\u60c5\u51b5\u8d1f\u8f7d\u524a\u51cf\u5f71\u54cd\u3002", "motivation": "\u867d\u7136\u673a\u5668\u5b66\u4e60\u6a21\u578b\u53ef\u4ee5\u5feb\u901f\u9884\u6d4b\u63a5\u8fd1\u6700\u4f18\u7684\u7ebf\u8def\u5207\u6362\u51b3\u7b56\uff0c\u4f46\u5728\u5b9e\u9645\u90e8\u7f72\u524d\u9a8c\u8bc1\u5176\u6027\u80fd\u548c\u5f71\u54cd\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u8bc4\u4f30\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u8d1f\u8f7d\u524a\u51cf\u3002", "method": "\u91c7\u7528\u53cc\u5c42\u653b\u51fb\u8005-\u9632\u5fa1\u8005\u9a8c\u8bc1\u6846\u67b6\uff0c\u7ed3\u5408\u4ea4\u6d41\u6f6e\u6d41\u51f8\u677e\u5f1b\u548c\u5c40\u90e8\u795e\u7ecf\u7f51\u7edc\u641c\u7d22\uff0c\u901a\u8fc7MathOptAI.jl\u6c42\u89e3\u6700\u574f\u60c5\u51b5\u8d1f\u8f7d\u524a\u51cf\u7684\u4e0b\u754c\u3002", "result": "\u4e0e\u968f\u673a\u91c7\u6837\u65b9\u6cd5\u76f8\u6bd4\uff0c\u57fa\u4e8e\u4f18\u5316\u7684\u65b9\u6cd5\u603b\u80fd\u627e\u5230\u66f4\u5927\u7684\u8d1f\u8f7d\u524a\u51cf\uff0c\u5728\u591a\u4e2aPGLib\u6d4b\u8bd5\u6848\u4f8b\u548c\u5305\u542b\u8d85\u8fc71000\u4e07\u53c2\u6570\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "conclusion": "\u63d0\u51fa\u7684\u4f18\u5316\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8bc4\u4f30\u795e\u7ecf\u7f51\u7edc\u7ebf\u8def\u5207\u6362\u51b3\u7b56\u7684\u6700\u574f\u60c5\u51b5\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u524d\u7684\u5b89\u5168\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\u3002"}}
{"id": "2510.23951", "categories": ["econ.TH"], "pdf": "https://arxiv.org/pdf/2510.23951", "abs": "https://arxiv.org/abs/2510.23951", "authors": ["Qingmin Liu", "Yuyang Miao"], "title": "Strategic Learning with Asymmetric Rationality", "comment": null, "summary": "This paper analyzes the dynamic interaction between a fully rational,\nprivately informed sender and a boundedly rational, uninformed receiver with\nmemory constraints. The sender controls the flow of information, while the\nreceiver designs a decision-making protocol, modeled as a finite-state machine,\nthat governs how information is interpreted, how internal memory states evolve,\nand when and what decisions are made. The receiver must use the limited set of\nstates optimally, both to learn and to create incentives for the sender to\nprovide information. We show that behavior patterns such as information\navoidance, opinion polarization, and indecision arise as equilibrium responses\nto asymmetric rationality. The model offers an expressive framework for\nstrategic learning and decision-making in environments with cognitive and\ninformational asymmetries, with applications to regulatory review and media\ndistrust.", "AI": {"tldr": "\u5206\u6790\u5b8c\u5168\u7406\u6027\u7684\u79c1\u6709\u4fe1\u606f\u53d1\u9001\u8005\u4e0e\u6709\u9650\u7406\u6027\u3001\u6709\u8bb0\u5fc6\u7ea6\u675f\u7684\u65e0\u4fe1\u606f\u63a5\u6536\u8005\u4e4b\u95f4\u7684\u52a8\u6001\u4e92\u52a8\uff0c\u7814\u7a76\u4fe1\u606f\u56de\u907f\u3001\u89c2\u70b9\u6781\u5316\u7b49\u884c\u4e3a\u6a21\u5f0f\u5982\u4f55\u4f5c\u4e3a\u975e\u5bf9\u79f0\u7406\u6027\u7684\u5747\u8861\u54cd\u5e94\u51fa\u73b0\u3002", "motivation": "\u7814\u7a76\u5728\u8ba4\u77e5\u548c\u4fe1\u606f\u4e0d\u5bf9\u79f0\u73af\u5883\u4e2d\uff0c\u53d1\u9001\u8005\u63a7\u5236\u4fe1\u606f\u6d41\u800c\u63a5\u6536\u8005\u8bbe\u8ba1\u51b3\u7b56\u534f\u8bae\u65f6\uff0c\u6709\u9650\u7406\u6027\u5982\u4f55\u5f71\u54cd\u6218\u7565\u5b66\u4e60\u548c\u51b3\u7b56\u5236\u5b9a\u3002", "method": "\u5efa\u7acb\u53d1\u9001\u8005\u4e0e\u63a5\u6536\u8005\u7684\u535a\u5f08\u6a21\u578b\uff0c\u63a5\u6536\u8005\u4f7f\u7528\u6709\u9650\u72b6\u6001\u673a\u4f5c\u4e3a\u51b3\u7b56\u534f\u8bae\uff0c\u63a7\u5236\u4fe1\u606f\u89e3\u91ca\u3001\u5185\u5b58\u72b6\u6001\u6f14\u53d8\u548c\u51b3\u7b56\u65f6\u673a\u3002", "result": "\u53d1\u73b0\u4fe1\u606f\u56de\u907f\u3001\u89c2\u70b9\u6781\u5316\u548c\u72b9\u8c6b\u4e0d\u51b3\u7b49\u884c\u4e3a\u6a21\u5f0f\u662f\u975e\u5bf9\u79f0\u7406\u6027\u7684\u5747\u8861\u54cd\u5e94\uff0c\u63a5\u6536\u8005\u9700\u8981\u6700\u4f18\u5229\u7528\u6709\u9650\u72b6\u6001\u6765\u5b66\u4e60\u548c\u6fc0\u52b1\u53d1\u9001\u8005\u63d0\u4f9b\u4fe1\u606f\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u8ba4\u77e5\u548c\u4fe1\u606f\u4e0d\u5bf9\u79f0\u73af\u5883\u4e2d\u7684\u6218\u7565\u5b66\u4e60\u548c\u51b3\u7b56\u5236\u5b9a\u63d0\u4f9b\u4e86\u8868\u8fbe\u6027\u6846\u67b6\uff0c\u53ef\u5e94\u7528\u4e8e\u76d1\u7ba1\u5ba1\u67e5\u548c\u5a92\u4f53\u4e0d\u4fe1\u4efb\u7b49\u573a\u666f\u3002"}}
{"id": "2510.23763", "categories": ["cs.RO", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23763", "abs": "https://arxiv.org/abs/2510.23763", "authors": ["Siyin Wang", "Jinlan Fu", "Feihong Liu", "Xinzhe He", "Huangxuan Wu", "Junhao Shi", "Kexin Huang", "Zhaoye Fei", "Jingjing Gong", "Zuxuan Wu", "Yugang Jiang", "See-Kiong Ng", "Tat-Seng Chua", "Xipeng Qiu"], "title": "RoboOmni: Proactive Robot Manipulation in Omni-modal Context", "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid\nprogress in Vision-Language-Action (VLA) models for robotic manipulation.\nAlthough effective in many scenarios, current approaches largely rely on\nexplicit instructions, whereas in real-world interactions, humans rarely issue\ninstructions directly. Effective collaboration requires robots to infer user\nintentions proactively. In this work, we introduce cross-modal contextual\ninstructions, a new setting where intent is derived from spoken dialogue,\nenvironmental sounds, and visual cues rather than explicit commands. To address\nthis new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor\nframework based on end-to-end omni-modal LLMs that unifies intention\nrecognition, interaction confirmation, and action execution. RoboOmni fuses\nauditory and visual signals spatiotemporally for robust intention recognition,\nwhile supporting direct speech interaction. To address the absence of training\ndata for proactive intention recognition in robotic manipulation, we build\nOmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640\nbackgrounds, and six contextual instruction types. Experiments in simulation\nand real-world settings show that RoboOmni surpasses text- and ASR-based\nbaselines in success rate, inference speed, intention recognition, and\nproactive assistance.", "AI": {"tldr": "\u63d0\u51fa\u4e86RoboOmni\u6846\u67b6\uff0c\u57fa\u4e8e\u5168\u6a21\u6001LLMs\uff0c\u901a\u8fc7\u878d\u5408\u542c\u89c9\u548c\u89c6\u89c9\u4fe1\u53f7\u8fdb\u884c\u610f\u56fe\u8bc6\u522b\uff0c\u652f\u6301\u76f4\u63a5\u8bed\u97f3\u4ea4\u4e92\uff0c\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u5b9e\u73b0\u4e3b\u52a8\u610f\u56fe\u63a8\u65ad\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u4eba\u7c7b\u5f88\u5c11\u76f4\u63a5\u53d1\u51fa\u6307\u4ee4\uff0c\u6709\u6548\u534f\u4f5c\u9700\u8981\u673a\u5668\u4eba\u4e3b\u52a8\u63a8\u65ad\u7528\u6237\u610f\u56fe\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u663e\u5f0f\u6307\u4ee4\u3002", "method": "\u63d0\u51faRoboOmni\u6846\u67b6\uff0c\u91c7\u7528Perceiver-Thinker-Talker-Executor\u67b6\u6784\uff0c\u57fa\u4e8e\u7aef\u5230\u7aef\u5168\u6a21\u6001LLMs\uff0c\u65f6\u7a7a\u878d\u5408\u542c\u89c9\u548c\u89c6\u89c9\u4fe1\u53f7\u8fdb\u884c\u610f\u56fe\u8bc6\u522b\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u5b9e\u9a8c\u4e2d\uff0cRoboOmni\u5728\u6210\u529f\u7387\u3001\u63a8\u7406\u901f\u5ea6\u3001\u610f\u56fe\u8bc6\u522b\u548c\u4e3b\u52a8\u534f\u52a9\u65b9\u9762\u4f18\u4e8e\u57fa\u4e8e\u6587\u672c\u548cASR\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "RoboOmni\u6846\u67b6\u901a\u8fc7\u8de8\u6a21\u6001\u4e0a\u4e0b\u6587\u6307\u4ee4\u5b9e\u73b0\u4e86\u66f4\u81ea\u7136\u7684\u4eba\u673a\u4ea4\u4e92\uff0c\u4e3a\u4e3b\u52a8\u610f\u56fe\u8bc6\u522b\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.23645", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2510.23645", "abs": "https://arxiv.org/abs/2510.23645", "authors": ["Alexandre Goncalves", "Yee Man Margaret Ng"], "title": "Global YouTube Trending Dataset (2022-2025): Three Years of Platform-Curated, Cross-National Trends in Digital Culture", "comment": null, "summary": "On July 1, 2025, YouTube retired its decade-long public \"Trending\" pages,\nending platform-curated, non-personalized video discovery. The Trending list\nhad long served as a vital lens into algorithmic influence, cultural diffusion,\nand crisis communication globally, offering a rare \"ground-truth\" reference to\nstudy global attention and cultural salience. We present a three-year archival\ndataset of YouTube Trending videos, collected from July 1, 2022, to June 30,\n2025, with four daily snapshots for each of the 104 countries. The dataset\nincludes 446,971 snapshots, each capturing up to 200 trending videos,\nencompassing 78.4 million video entries (726,627 unique videos) and associated\nmetadata. Each record includes core identifiers (snapshot time, country, rank)\nand content metadata (video ID, channel ID, title, description, tags,\npublication date, category, channel name, language, live status, views, and\ncomments). Unlike previous datasets with limited geographic scope or short\ntimeframes, our non-personalized data provides exceptional cross-national and\nlongitudinal coverage for studying digital culture, platform governance, and\ntemporal dynamics in content popularity. We document the data collection\nmethodology, schema design, coverage, descriptive statistics for both global\nand U.S. trending videos, and the ethical safeguards implemented throughout.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e3a\u671f\u4e09\u5e74\u7684YouTube Trending\u89c6\u9891\u5b58\u6863\u6570\u636e\u96c6\uff0c\u6db5\u76d62022\u5e747\u67081\u65e5\u81f32025\u5e746\u670830\u65e5\u671f\u95f4104\u4e2a\u56fd\u5bb6\u7684\u6bcf\u65e5\u56db\u6b21\u5feb\u7167\uff0c\u5305\u542b78.4\u767e\u4e07\u6761\u89c6\u9891\u8bb0\u5f55\uff0c\u4e3a\u7814\u7a76\u6570\u5b57\u6587\u5316\u3001\u5e73\u53f0\u6cbb\u7406\u548c\u5185\u5bb9\u6d41\u884c\u5ea6\u52a8\u6001\u63d0\u4f9b\u4e86\u72ec\u7279\u7684\u8de8\u56fd\u7eb5\u5411\u6570\u636e\u3002", "motivation": "YouTube\u4e8e2025\u5e747\u67081\u65e5\u5173\u95ed\u4e86\u5176\u957f\u8fbe\u5341\u5e74\u7684\u516c\u5f00\"Trending\"\u9875\u9762\uff0c\u8fd9\u6807\u5fd7\u7740\u5e73\u53f0\u7b56\u5212\u7684\u975e\u4e2a\u6027\u5316\u89c6\u9891\u53d1\u73b0\u65f6\u4ee3\u7684\u7ed3\u675f\u3002Trending\u5217\u8868\u957f\u671f\u4ee5\u6765\u662f\u7814\u7a76\u7b97\u6cd5\u5f71\u54cd\u3001\u6587\u5316\u4f20\u64ad\u548c\u5371\u673a\u6c9f\u901a\u7684\u91cd\u8981\u7a97\u53e3\uff0c\u63d0\u4f9b\u4e86\u7814\u7a76\u5168\u7403\u5173\u6ce8\u5ea6\u548c\u6587\u5316\u663e\u8457\u6027\u7684\u96be\u5f97\"\u771f\u5b9e\u53c2\u8003\"\u3002", "method": "\u6536\u96c6\u4e862022\u5e747\u67081\u65e5\u81f32025\u5e746\u670830\u65e5\u671f\u95f4104\u4e2a\u56fd\u5bb6\u7684YouTube Trending\u89c6\u9891\u6570\u636e\uff0c\u6bcf\u65e5\u56db\u6b21\u5feb\u7167\uff0c\u6bcf\u4e2a\u5feb\u7167\u6355\u83b7\u6700\u591a200\u4e2a\u8d8b\u52bf\u89c6\u9891\u3002\u6570\u636e\u96c6\u5305\u542b446,971\u4e2a\u5feb\u7167\uff0c\u6db5\u76d678.4\u767e\u4e07\u6761\u89c6\u9891\u6761\u76ee\uff08726,627\u4e2a\u72ec\u7279\u89c6\u9891\uff09\u53ca\u76f8\u5173\u5143\u6570\u636e\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b446,971\u4e2a\u5feb\u7167\u7684\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u8bb0\u5f55\u5305\u62ec\u6838\u5fc3\u6807\u8bc6\u7b26\uff08\u5feb\u7167\u65f6\u95f4\u3001\u56fd\u5bb6\u3001\u6392\u540d\uff09\u548c\u5185\u5bb9\u5143\u6570\u636e\uff08\u89c6\u9891ID\u3001\u9891\u9053ID\u3001\u6807\u9898\u3001\u63cf\u8ff0\u3001\u6807\u7b7e\u3001\u53d1\u5e03\u65e5\u671f\u3001\u7c7b\u522b\u3001\u9891\u9053\u540d\u79f0\u3001\u8bed\u8a00\u3001\u76f4\u64ad\u72b6\u6001\u3001\u89c2\u770b\u6b21\u6570\u548c\u8bc4\u8bba\uff09\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u524d\u6240\u672a\u6709\u7684\u8de8\u56fd\u548c\u7eb5\u5411\u8986\u76d6\u8303\u56f4\uff0c\u4e3a\u7814\u7a76\u6570\u5b57\u6587\u5316\u3001\u5e73\u53f0\u6cbb\u7406\u548c\u5185\u5bb9\u6d41\u884c\u5ea6\u7684\u65f6\u95f4\u52a8\u6001\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\uff0c\u586b\u8865\u4e86\u5148\u524d\u6570\u636e\u96c6\u5728\u5730\u7406\u8303\u56f4\u548c\u65f6\u95f4\u6846\u67b6\u4e0a\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.24002", "categories": ["econ.GN", "q-fin.EC", "I.6.5, J.4, K.4.1", "I.6.5; J.4; K.4.1"], "pdf": "https://arxiv.org/pdf/2510.24002", "abs": "https://arxiv.org/abs/2510.24002", "authors": ["Zehao Lin"], "title": "How Does Environmental Information Disclosure Affect Corporate Environmental Performance? Evidence from Chinese A-Share Listed Companies", "comment": "27 pages,32 reference,5 tables", "summary": "Global climate warming and air pollution pose severe threats to economic\ndevelopment and public safety, presenting significant challenges to sustainable\ndevelopment worldwide. Corporations, as key players in resource utilization and\nemissions, have drawn increasing attention from policymakers, researchers, and\nthe public regarding their environmental strategies and practices. This study\nemploys a two-way fixed effects panel model to examine the impact of\nenvironmental information disclosure on corporate environmental performance,\nits regional heterogeneity, and the underlying mechanisms. The results\ndemonstrate that environmental information disclosure significantly improves\ncorporate environmental performance, with the effect being more pronounced in\nareas of high population density and limited green space. These findings\nprovide empirical evidence supporting the role of environmental information\ndisclosure as a critical tool for improving corporate environmental practices.\nThe study highlights the importance of targeted, region-specific policies to\nmaximize the effectiveness of disclosure, offering valuable insights for\npromoting sustainable development through enhanced corporate transparency.", "AI": {"tldr": "\u73af\u5883\u4fe1\u606f\u62ab\u9732\u663e\u8457\u6539\u5584\u4f01\u4e1a\u73af\u5883\u7ee9\u6548\uff0c\u5728\u4eba\u53e3\u5bc6\u5ea6\u9ad8\u3001\u7eff\u5730\u6709\u9650\u7684\u5730\u533a\u6548\u679c\u66f4\u660e\u663e\uff0c\u4e3a\u901a\u8fc7\u4f01\u4e1a\u900f\u660e\u5ea6\u4fc3\u8fdb\u53ef\u6301\u7eed\u53d1\u5c55\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\u3002", "motivation": "\u5168\u7403\u6c14\u5019\u53d8\u6696\u548c\u7a7a\u6c14\u6c61\u67d3\u5bf9\u7ecf\u6d4e\u53d1\u5c55\u548c\u516c\u5171\u5b89\u5168\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u4f01\u4e1a\u4f5c\u4e3a\u8d44\u6e90\u5229\u7528\u548c\u6392\u653e\u7684\u5173\u952e\u53c2\u4e0e\u8005\uff0c\u5176\u73af\u5883\u6218\u7565\u548c\u5b9e\u8df5\u53d7\u5230\u653f\u7b56\u5236\u5b9a\u8005\u3001\u7814\u7a76\u4eba\u5458\u548c\u516c\u4f17\u7684\u65e5\u76ca\u5173\u6ce8\u3002", "method": "\u91c7\u7528\u53cc\u5411\u56fa\u5b9a\u6548\u5e94\u9762\u677f\u6a21\u578b\u7814\u7a76\u73af\u5883\u4fe1\u606f\u62ab\u9732\u5bf9\u4f01\u4e1a\u73af\u5883\u7ee9\u6548\u7684\u5f71\u54cd\u3001\u533a\u57df\u5f02\u8d28\u6027\u53ca\u5176\u6f5c\u5728\u673a\u5236\u3002", "result": "\u73af\u5883\u4fe1\u606f\u62ab\u9732\u663e\u8457\u63d0\u9ad8\u4e86\u4f01\u4e1a\u73af\u5883\u7ee9\u6548\uff0c\u5728\u4eba\u53e3\u5bc6\u5ea6\u9ad8\u3001\u7eff\u5730\u6709\u9650\u7684\u5730\u533a\u6548\u679c\u66f4\u4e3a\u663e\u8457\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u9488\u5bf9\u6027\u3001\u533a\u57df\u7279\u5f02\u6027\u653f\u7b56\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u6700\u5927\u5316\u4fe1\u606f\u62ab\u9732\u7684\u6709\u6548\u6027\uff0c\u4e3a\u901a\u8fc7\u589e\u5f3a\u4f01\u4e1a\u900f\u660e\u5ea6\u4fc3\u8fdb\u53ef\u6301\u7eed\u53d1\u5c55\u63d0\u4f9b\u4e86\u5b9d\u8d35\u89c1\u89e3\u3002"}}
{"id": "2510.24433", "categories": ["econ.EM", "cs.LG", "math.ST", "stat.ME", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.24433", "abs": "https://arxiv.org/abs/2510.24433", "authors": ["Masahiro Kato"], "title": "Nearest Neighbor Matching as Least Squares Density Ratio Estimation and Riesz Regression", "comment": null, "summary": "This study proves that Nearest Neighbor (NN) matching can be interpreted as\nan instance of Riesz regression for automatic debiased machine learning. Lin et\nal. (2023) shows that NN matching is an instance of density-ratio estimation\nwith their new density-ratio estimator. Chernozhukov et al. (2024) develops\nRiesz regression for automatic debiased machine learning, which directly\nestimates the Riesz representer (or equivalently, the bias-correction term) by\nminimizing the mean squared error. In this study, we first prove that the\ndensity-ratio estimation method proposed in Lin et al. (2023) is essentially\nequivalent to Least-Squares Importance Fitting (LSIF) proposed in Kanamori et\nal. (2009) for direct density-ratio estimation. Furthermore, we derive Riesz\nregression using the LSIF framework. Based on these results, we derive NN\nmatching from Riesz regression. This study is based on our work Kato (2025a)\nand Kato (2025b).", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u6700\u8fd1\u90bb\u5339\u914d\u53ef\u4ee5\u89e3\u91ca\u4e3aRiesz\u56de\u5f52\u7684\u4e00\u4e2a\u5b9e\u4f8b\uff0c\u7528\u4e8e\u81ea\u52a8\u53bb\u504f\u673a\u5668\u5b66\u4e60\u3002\u4f5c\u8005\u9996\u5148\u8bc1\u660e\u4e86Lin\u7b49\u4eba(2023)\u63d0\u51fa\u7684\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u65b9\u6cd5\u672c\u8d28\u4e0a\u7b49\u540c\u4e8eKanamori\u7b49\u4eba(2009)\u63d0\u51fa\u7684\u6700\u5c0f\u4e8c\u4e58\u91cd\u8981\u6027\u62df\u5408(LSIF)\uff0c\u7136\u540e\u57fa\u4e8eLSIF\u6846\u67b6\u63a8\u5bfc\u4e86Riesz\u56de\u5f52\uff0c\u5e76\u6700\u7ec8\u4eceRiesz\u56de\u5f52\u63a8\u5bfc\u51fa\u6700\u8fd1\u90bb\u5339\u914d\u3002", "motivation": "\u5efa\u7acb\u6700\u8fd1\u90bb\u5339\u914d\u4e0eRiesz\u56de\u5f52\u4e4b\u95f4\u7684\u7406\u8bba\u8054\u7cfb\uff0c\u5c06\u6700\u8fd1\u90bb\u5339\u914d\u7edf\u4e00\u5230\u81ea\u52a8\u53bb\u504f\u673a\u5668\u5b66\u4e60\u7684\u7406\u8bba\u6846\u67b6\u4e2d\uff0c\u4e3a\u7406\u89e3\u8fd9\u4e24\u79cd\u65b9\u6cd5\u7684\u5185\u5728\u5173\u7cfb\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u9996\u5148\u8bc1\u660eLin\u7b49\u4eba(2023)\u7684\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u65b9\u6cd5\u4e0eLSIF\u7684\u7b49\u4ef7\u6027\uff0c\u7136\u540e\u5728LSIF\u6846\u67b6\u4e0b\u63a8\u5bfcRiesz\u56de\u5f52\uff0c\u6700\u540e\u4eceRiesz\u56de\u5f52\u63a8\u5bfc\u51fa\u6700\u8fd1\u90bb\u5339\u914d\u3002", "result": "\u6210\u529f\u5efa\u7acb\u4e86\u6700\u8fd1\u90bb\u5339\u914d\u4e0eRiesz\u56de\u5f52\u7684\u7406\u8bba\u7b49\u4ef7\u5173\u7cfb\uff0c\u8bc1\u660e\u4e86\u6700\u8fd1\u90bb\u5339\u914d\u53ef\u4ee5\u89c6\u4e3aRiesz\u56de\u5f52\u7684\u4e00\u4e2a\u7279\u4f8b\uff0c\u4e3a\u81ea\u52a8\u53bb\u504f\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7406\u8bba\u89c6\u89d2\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u7406\u8bba\u63a8\u5bfc\u5efa\u7acb\u4e86\u6700\u8fd1\u90bb\u5339\u914d\u4e0eRiesz\u56de\u5f52\u7684\u6df1\u523b\u8054\u7cfb\uff0c\u5c06\u4e24\u79cd\u770b\u4f3c\u4e0d\u540c\u7684\u65b9\u6cd5\u7edf\u4e00\u5230\u540c\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\u4e0b\uff0c\u4e3a\u81ea\u52a8\u53bb\u504f\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u7406\u8bba\u652f\u6491\u3002"}}
{"id": "2510.23734", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23734", "abs": "https://arxiv.org/abs/2510.23734", "authors": ["Eamon Duede"], "title": "AI and the Decentering of Disciplinary Creativity", "comment": null, "summary": "This paper examines the role of artificial intelligence in scientific\nproblem-solving, with a focus on its implications for disciplinary creativity.\nDrawing on recent work in the philosophy of creativity, I distinguish between\ncreative approaches and creative products, and introduce the concept of\ndisciplinary creativity -the creative application of discipline-specific\nexpertise to a valued problem within that field. Through two cases in\nmathematics, I show that while computation can extend disciplinary creativity,\ncertain approaches involving AI can serve to displace it. This displacement has\nthe potential to alter (and, perhaps, diminish) the value of scientific\npursuit.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8AI\u5728\u79d1\u5b66\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u4f5c\u7528\u53ca\u5176\u5bf9\u5b66\u79d1\u521b\u9020\u529b\u7684\u5f71\u54cd\uff0c\u533a\u5206\u4e86\u521b\u9020\u6027\u65b9\u6cd5\u4e0e\u521b\u9020\u6027\u4ea7\u54c1\uff0c\u5e76\u901a\u8fc7\u6570\u5b66\u6848\u4f8b\u8bf4\u660eAI\u53ef\u80fd\u53d6\u4ee3\u5b66\u79d1\u521b\u9020\u529b\u3002", "motivation": "\u7814\u7a76AI\u5728\u79d1\u5b66\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u89d2\u8272\uff0c\u7279\u522b\u5173\u6ce8\u5176\u5bf9\u5b66\u79d1\u521b\u9020\u529b\u7684\u6f5c\u5728\u5f71\u54cd\uff0c\u65e8\u5728\u7406\u89e3AI\u5982\u4f55\u6539\u53d8\u79d1\u5b66\u8ffd\u6c42\u7684\u4ef7\u503c\u3002", "method": "\u57fa\u4e8e\u521b\u9020\u529b\u54f2\u5b66\u7406\u8bba\uff0c\u533a\u5206\u521b\u9020\u6027\u65b9\u6cd5\u4e0e\u4ea7\u54c1\uff0c\u5f15\u5165\u5b66\u79d1\u521b\u9020\u529b\u6982\u5ff5\uff0c\u5e76\u901a\u8fc7\u4e24\u4e2a\u6570\u5b66\u6848\u4f8b\u8fdb\u884c\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8ba1\u7b97\u53ef\u4ee5\u6269\u5c55\u5b66\u79d1\u521b\u9020\u529b\uff0c\u4f46\u67d0\u4e9bAI\u65b9\u6cd5\u53ef\u80fd\u53d6\u4ee3\u5b66\u79d1\u521b\u9020\u529b\uff0c\u4ece\u800c\u53ef\u80fd\u6539\u53d8\u79d1\u5b66\u8ffd\u6c42\u7684\u4ef7\u503c\u3002", "conclusion": "AI\u5728\u79d1\u5b66\u4e2d\u7684\u5e94\u7528\u65e2\u53ef\u80fd\u6269\u5c55\u4e5f\u53ef\u80fd\u53d6\u4ee3\u5b66\u79d1\u521b\u9020\u529b\uff0c\u8fd9\u79cd\u66ff\u4ee3\u53ef\u80fd\u5f71\u54cd\u79d1\u5b66\u8ffd\u6c42\u7684\u4ef7\u503c\u548c\u610f\u4e49\u3002"}}
{"id": "2510.23805", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.23805", "abs": "https://arxiv.org/abs/2510.23805", "authors": ["Xueying Chen", "Jianfeng Ke", "Lauren Flynn", "Giovanni Parmigiani", "Danielle Braun"], "title": "A web-based user interface for Fam3PRO, a multi-gene, multi-cancer risk prediction model for families with cancer history", "comment": null, "summary": "Purpose: Hereditary cancer risk is key to guiding screening and prevention\nstrategies. Cancer risks can vary by individual due to the presence or absence\nof high- and moderate-risk pathogenic variants (PV) in cancer-associated genes,\nin addition to sex, age, and other risk factors. We previously developed\nFam3PRO, a flexible multi-gene, multi-cancer Mendelian risk prediction model\nthat estimates a patient's risk of carrying a PV in hereditary cancer genes and\ntheir future risk of developing several types of cancer. The Fam3PRO R package\nincludes 22 genes with 18 associated cancers, allowing users to build\ncustomized sub-models from any gene-cancer set. However, the current R package\nlacks a user interface (UI), limiting its practical use in clinical settings.\nTherefore, we aim to develop a web-based UI for broader use of the Fam3PRO\nfunctionalities.\n  Methods: The Fam3PRO UI (F3PI), built with R Shiny, collects and formats\ninputs including family health history, genetic test results, and other risk\nfactors. Pedigree data are interactively visualized and modified via\npedigreejs, while the backend Fam3PRO model takes all the inputs to generate\ncarrier probabilities and future cancer risks, presented through an interactive\nUI.\n  Results: F3PI streamlines the collection of patient and family history data,\nwhich is analyzed by the Fam3PRO models to provide personalized cancer risks\nfor each proband across 18 cancers, as well as probabilities that a proband has\na PV in up to 22 hereditary cancer genes. These results are returned to the\nuser, within one minute on average and are available in both interactive and\ndownloadable formats.\n  Conclusion: We have developed F3PI, an easy-to-use, interactive web\napplication that makes cancer and genetic risk information more accessible to\nproviders and their patients.", "AI": {"tldr": "\u5f00\u53d1\u4e86F3PI\u7f51\u7edc\u5e94\u7528\u754c\u9762\uff0c\u4f7fFam3PRO\u9057\u4f20\u6027\u764c\u75c7\u98ce\u9669\u8bc4\u4f30\u6a21\u578b\u66f4\u6613\u4e8e\u4e34\u5e8a\u4f7f\u7528", "motivation": "\u73b0\u6709\u7684Fam3PRO R\u5305\u7f3a\u4e4f\u7528\u6237\u754c\u9762\uff0c\u9650\u5236\u4e86\u5176\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u9700\u8981\u5f00\u53d1\u57fa\u4e8e\u7f51\u7edc\u7684\u7528\u6237\u754c\u9762\u6765\u6269\u5927\u4f7f\u7528\u8303\u56f4", "method": "\u4f7f\u7528R Shiny\u6784\u5efaF3PI\u754c\u9762\uff0c\u901a\u8fc7pedigreejs\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u548c\u4fee\u6539\u5bb6\u8c31\u6570\u636e\uff0c\u540e\u7aefFam3PRO\u6a21\u578b\u5904\u7406\u8f93\u5165\u6570\u636e\u751f\u6210\u643a\u5e26\u6982\u7387\u548c\u672a\u6765\u764c\u75c7\u98ce\u9669", "result": "F3PI\u7b80\u5316\u4e86\u60a3\u8005\u548c\u5bb6\u65cf\u53f2\u6570\u636e\u6536\u96c6\uff0c\u5e73\u5747\u4e00\u5206\u949f\u5185\u63d0\u4f9b18\u79cd\u764c\u75c7\u7684\u4e2a\u6027\u5316\u98ce\u9669\u8bc4\u4f30\u548c22\u4e2a\u9057\u4f20\u6027\u764c\u75c7\u57fa\u56e0\u7684\u53d8\u5f02\u643a\u5e26\u6982\u7387\uff0c\u7ed3\u679c\u652f\u6301\u4ea4\u4e92\u67e5\u770b\u548c\u4e0b\u8f7d", "conclusion": "F3PI\u662f\u4e00\u4e2a\u6613\u4e8e\u4f7f\u7528\u7684\u4ea4\u4e92\u5f0f\u7f51\u7edc\u5e94\u7528\uff0c\u4f7f\u764c\u75c7\u548c\u9057\u4f20\u98ce\u9669\u4fe1\u606f\u66f4\u5bb9\u6613\u88ab\u533b\u7597\u63d0\u4f9b\u8005\u548c\u60a3\u8005\u83b7\u53d6"}}
{"id": "2510.23819", "categories": ["eess.SY", "cs.SY", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.23819", "abs": "https://arxiv.org/abs/2510.23819", "authors": ["Avishka Herath", "Malith Jayalath", "Kumudu Kaushalya", "Sanjana Kapukotuwa", "Chathuni Wijegunawardena", "Pahan Mendis", "Kithmin Wickremasinghe", "Duminda Samarasinghe", "Wageesha N. Manamperi", "Chamira U. S. Edussooriya"], "title": "A Simultaneous ECG-PCG Acquisition System with Real-Time Burst-Adaptive Noise Cancellation", "comment": "Paper submitted to IEEE International Symposium on Circuits and\n  Systems (ISCAS) 2026", "summary": "Cardiac auscultation is an essential clinical skill, requiring excellent\nhearing to distinguish subtle differences in timing and pitch of heart sounds.\nHowever, diagnosing solely from these sounds is often challenging due to\ninterference from surrounding noise, and the information may be limited.\nExisting solutions that adaptively cancel external noise are either not\nreal-time or are computationally intensive, making them unsuitable for\nimplementation in a portable system. This work proposes an end-to-end system\nwith a real-time adaptive noise cancellation pipeline integrated into a device\nthat simultaneously acquires electrocardiogram (ECG) and phonocardiogram (PCG)\nsignals. The performance of the system is validated using real-world hospital\nnoise datasets and recordings captured with the dual-modality device. For PCG\nand ECG signals recorded from the device in noisy hospital settings, the\nproposed algorithms achieved signal-to-noise ratio improvements of 37.01 dB and\n30.32 dB, respectively. These results demonstrate the systems effectiveness in\nenabling reliable and accessible cardiac screening, including noisy hospital\nenvironments typical of resource-constrained settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u5b9e\u65f6\u81ea\u9002\u5e94\u566a\u58f0\u6d88\u9664\u7684\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u7528\u4e8e\u540c\u65f6\u91c7\u96c6\u5fc3\u7535\u56fe\u548c\u5fc3\u97f3\u56fe\u4fe1\u53f7\uff0c\u5728\u5608\u6742\u533b\u9662\u73af\u5883\u4e2d\u663e\u8457\u6539\u5584\u4fe1\u566a\u6bd4\u3002", "motivation": "\u5fc3\u810f\u542c\u8bca\u662f\u91cd\u8981\u4e34\u5e8a\u6280\u80fd\uff0c\u4f46\u4ec5\u51ed\u5fc3\u97f3\u8bca\u65ad\u56f0\u96be\uff0c\u73b0\u6709\u566a\u58f0\u6d88\u9664\u65b9\u6848\u8981\u4e48\u975e\u5b9e\u65f6\u8981\u4e48\u8ba1\u7b97\u91cf\u5927\uff0c\u4e0d\u9002\u5408\u4fbf\u643a\u7cfb\u7edf\u3002", "method": "\u5f00\u53d1\u4e86\u96c6\u6210\u5b9e\u65f6\u81ea\u9002\u5e94\u566a\u58f0\u6d88\u9664\u7ba1\u9053\u7684\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u540c\u65f6\u91c7\u96c6\u5fc3\u7535\u56fe\u548c\u5fc3\u97f3\u56fe\u4fe1\u53f7\u3002", "result": "\u5728\u5608\u6742\u533b\u9662\u73af\u5883\u4e2d\uff0c\u5fc3\u97f3\u56fe\u548c\u5fc3\u7535\u56fe\u4fe1\u53f7\u7684\u4fe1\u566a\u6bd4\u5206\u522b\u63d0\u9ad8\u4e8637.01 dB\u548c30.32 dB\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u80fd\u591f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5608\u6742\u533b\u9662\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u9760\u4e14\u53ef\u8bbf\u95ee\u7684\u5fc3\u810f\u7b5b\u67e5\u3002"}}
{"id": "2510.24266", "categories": ["econ.TH", "97N", "K.3.1; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.24266", "abs": "https://arxiv.org/abs/2510.24266", "authors": ["Duaa Abdullah", "Jasem Hamoud"], "title": "The Role of Mathematical Folk Puzzles in Developing mathematical Thinking and Problem-Solving Skills", "comment": null, "summary": "This paper covers a variety of mathematical folk puzzles, including geometric\n(Tangrams, dissection puzzles), logic, algebraic, probability (Monty Hall\nProblem, Birthday Paradox), and combinatorial challenges (Eight Queens Puzzle,\nTower of Hanoi). It also explores modern modifications, such as digital and\ngamified approaches, to improve student involvement and comprehension.\nFurthermore, a novel concept, the \"Minimal Dissection Path Problem for\nPolyominoes,\" is introduced and proven, demonstrating that the minimum number\nof straight-line cuts required to dissect a polyomino of N squares into its\nconstituent units is $\\mathrm{N}-1$. This problem, along with other puzzles,\noffers practical classroom applications that reinforce core mathematical\nconcepts like area, spatial reasoning, and optimization, making learning both\nenjoyable and effective.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5404\u79cd\u6570\u5b66\u6c11\u95f4\u8c1c\u9898\uff0c\u5305\u62ec\u51e0\u4f55\u3001\u903b\u8f91\u3001\u4ee3\u6570\u3001\u6982\u7387\u548c\u7ec4\u5408\u6311\u6218\uff0c\u5e76\u4ecb\u7ecd\u4e86\u73b0\u4ee3\u6570\u5b57\u5316\u6539\u8fdb\u3002\u63d0\u51fa\u4e86\"\u591a\u8054\u9aa8\u724c\u6700\u5c0f\u5206\u5272\u8def\u5f84\u95ee\u9898\"\u7684\u65b0\u6982\u5ff5\uff0c\u8bc1\u660eN\u4e2a\u65b9\u683c\u7684\u591a\u8054\u9aa8\u724c\u9700\u8981N-1\u6b21\u76f4\u7ebf\u5207\u5272\u624d\u80fd\u5206\u89e3\u4e3a\u5355\u4e2a\u5355\u5143\u3002", "motivation": "\u901a\u8fc7\u6709\u8da3\u7684\u6570\u5b66\u8c1c\u9898\u63d0\u9ad8\u5b66\u751f\u7684\u53c2\u4e0e\u5ea6\u548c\u7406\u89e3\u529b\uff0c\u5c06\u6570\u5b66\u6982\u5ff5\u4e0e\u6e38\u620f\u5316\u65b9\u6cd5\u76f8\u7ed3\u5408\uff0c\u4f7f\u5b66\u4e60\u66f4\u52a0\u6109\u5feb\u6709\u6548\u3002", "method": "\u5206\u6790\u5404\u7c7b\u4f20\u7edf\u6570\u5b66\u8c1c\u9898\uff0c\u63a2\u7d22\u73b0\u4ee3\u6570\u5b57\u5316\u6539\u8fdb\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u591a\u8054\u9aa8\u724c\u6700\u5c0f\u5206\u5272\u8def\u5f84\u95ee\u9898\u8fdb\u884c\u6570\u5b66\u8bc1\u660e\u3002", "result": "\u8bc1\u660e\u4e86\u591a\u8054\u9aa8\u724c\u6700\u5c0f\u5206\u5272\u8def\u5f84\u95ee\u9898\u7684\u7ed3\u8bba\uff1aN\u4e2a\u65b9\u683c\u7684\u591a\u8054\u9aa8\u724c\u9700\u8981\u6070\u597dN-1\u6b21\u76f4\u7ebf\u5207\u5272\u624d\u80fd\u5206\u89e3\u4e3a\u5355\u4e2a\u5355\u5143\u3002", "conclusion": "\u6570\u5b66\u8c1c\u9898\u4f5c\u4e3a\u6559\u5b66\u5de5\u5177\u80fd\u6709\u6548\u52a0\u5f3a\u6838\u5fc3\u6570\u5b66\u6982\u5ff5\u7684\u7406\u89e3\uff0c\u5982\u9762\u79ef\u3001\u7a7a\u95f4\u63a8\u7406\u548c\u4f18\u5316\uff0c\u4f7f\u5b66\u4e60\u8fc7\u7a0b\u65e2\u6709\u8da3\u53c8\u9ad8\u6548\u3002"}}
{"id": "2510.23860", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23860", "abs": "https://arxiv.org/abs/2510.23860", "authors": ["Hyung Chan Cho", "Go-Eum Cha", "Yanfu Liu", "Sooyeon Jeong"], "title": "Motivating Students' Self-study with Goal Reminder and Emotional Support", "comment": "RO-MAN 2025 accepted paper", "summary": "While the efficacy of social robots in supporting people in learning tasks\nhas been extensively investigated, their potential impact in assisting students\nin self-studying contexts has not been investigated much. This study explores\nhow a social robot can act as a peer study companion for college students\nduring self-study tasks by delivering task-oriented goal reminder and positive\nemotional support. We conducted an exploratory Wizard-of-Oz study to explore\nhow these robotic support behaviors impacted students' perceived focus,\nproductivity, and engagement in comparison to a robot that only provided\nphysical presence (control). Our study results suggest that participants in the\ngoal reminder and the emotional support conditions reported greater ease of\nuse, with the goal reminder condition additionally showing a higher willingness\nto use the robot in future study sessions. Participants' satisfaction with the\nrobot was correlated with their perception of the robot as a social other, and\nthis perception was found to be a predictor for their level of goal achievement\nin the self-study task. These findings highlight the potential of socially\nassistive robots to support self-study through both functional and emotional\nengagement.", "AI": {"tldr": "\u793e\u4ea4\u673a\u5668\u4eba\u4f5c\u4e3a\u540c\u4f34\u5b66\u4e60\u4f34\u4fa3\uff0c\u901a\u8fc7\u76ee\u6807\u63d0\u9192\u548c\u60c5\u611f\u652f\u6301\u63d0\u5347\u5927\u5b66\u751f\u81ea\u4e3b\u5b66\u4e60\u6548\u679c", "motivation": "\u867d\u7136\u793e\u4ea4\u673a\u5668\u4eba\u5728\u5b66\u4e60\u4efb\u52a1\u652f\u6301\u65b9\u9762\u5df2\u6709\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5728\u81ea\u4e3b\u5b66\u4e60\u60c5\u5883\u4e2d\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22", "method": "\u91c7\u7528Wizard-of-Oz\u63a2\u7d22\u6027\u7814\u7a76\uff0c\u6bd4\u8f83\u76ee\u6807\u63d0\u9192\u3001\u60c5\u611f\u652f\u6301\u548c\u4ec5\u7269\u7406\u5b58\u5728\uff08\u5bf9\u7167\u7ec4\uff09\u4e09\u79cd\u6761\u4ef6\u5bf9\u5b66\u751f\u5b66\u4e60\u7684\u5f71\u54cd", "result": "\u76ee\u6807\u63d0\u9192\u548c\u60c5\u611f\u652f\u6301\u6761\u4ef6\u4e0b\u7684\u53c2\u4e0e\u8005\u62a5\u544a\u4e86\u66f4\u597d\u7684\u6613\u7528\u6027\uff0c\u76ee\u6807\u63d0\u9192\u7ec4\u66f4\u613f\u610f\u5728\u672a\u6765\u5b66\u4e60\u4e2d\u4f7f\u7528\u673a\u5668\u4eba\uff1b\u53c2\u4e0e\u8005\u5bf9\u673a\u5668\u4eba\u7684\u6ee1\u610f\u5ea6\u4e0e\u5176\u5c06\u673a\u5668\u4eba\u89c6\u4e3a\u793e\u4ea4\u5b9e\u4f53\u7684\u611f\u77e5\u76f8\u5173\uff0c\u8fd9\u79cd\u611f\u77e5\u80fd\u9884\u6d4b\u5b66\u4e60\u76ee\u6807\u8fbe\u6210\u7a0b\u5ea6", "conclusion": "\u793e\u4ea4\u8f85\u52a9\u673a\u5668\u4eba\u901a\u8fc7\u529f\u80fd\u6027\u548c\u60c5\u611f\u6027\u53c2\u4e0e\uff0c\u5728\u81ea\u4e3b\u5b66\u4e60\u652f\u6301\u65b9\u9762\u5177\u6709\u91cd\u8981\u6f5c\u529b"}}
{"id": "2510.24373", "categories": ["cs.CY", "cs.HC", "J.4; I.0; H.4; A.0; C.0"], "pdf": "https://arxiv.org/pdf/2510.24373", "abs": "https://arxiv.org/abs/2510.24373", "authors": ["Victor Galaz", "Maria Schewenius", "Jonathan F. Donges", "Ingo Fetzer", "Erik Zhivkoplias", "Wolfram Barfuss", "Louis Delannoy", "Lan Wang-Erlandsson", "Maximilian Gelbrecht", "Jobst Heitzig", "Jonas Hentati-Sundberg", "Christopher Kennedy", "Nielja Knecht", "Romi Lotcheris", "Miguel Mahecha", "Andrew Merrie", "David Montero", "Timon McPhearson", "Ahmed Mustafa", "Magnus Nystr\u00f6m", "Drew Purves", "Juan C. Rocha", "Masahiro Ryo", "Claudia van der Salm", "Samuel T. Segun", "Anna B. Stephenson", "Elizabeth Tellman", "Felipe Tobar", "Alice Vadrot"], "title": "AI for a Planet Under Pressure", "comment": "88 pages, 8 figures, 1 table", "summary": "Artificial intelligence (AI) is already driving scientific breakthroughs in a\nvariety of research fields, ranging from the life sciences to mathematics. This\nraises a critical question: can AI be applied both responsibly and effectively\nto address complex and interconnected sustainability challenges? This report is\nthe result of a collaboration between the Stockholm resilience Centre\n(Stockholm University), the Potsdam Institute for Climate Impact Research\n(PIK), and Google DeepMind. Our work explores the potential and limitations of\nusing AI as a research method to help tackle eight broad sustainability\nchallenges. The results build on iterated expert dialogues and assessments, a\nsystematic AI-supported literature overview including over 8,500 academic\npublications, and expert deep-dives into eight specific issue areas. The report\nalso includes recommendations to sustainability scientists, research funders,\nthe private sector, and philanthropies.", "AI": {"tldr": "\u8be5\u62a5\u544a\u63a2\u8ba8\u4e86AI\u5728\u89e3\u51b3\u516b\u5927\u53ef\u6301\u7eed\u53d1\u5c55\u6311\u6218\u4e2d\u7684\u6f5c\u529b\u548c\u5c40\u9650\u6027\uff0c\u57fa\u4e8e\u4e13\u5bb6\u5bf9\u8bdd\u3001AI\u652f\u6301\u7684\u6587\u732e\u7efc\u8ff0\u548c\u516b\u4e2a\u5177\u4f53\u9886\u57df\u7684\u6df1\u5165\u7814\u7a76\u3002", "motivation": "AI\u5df2\u5728\u591a\u4e2a\u79d1\u5b66\u9886\u57df\u63a8\u52a8\u7a81\u7834\uff0c\u4f46\u80fd\u5426\u8d1f\u8d23\u4efb\u4e14\u6709\u6548\u5730\u5e94\u7528\u4e8e\u89e3\u51b3\u590d\u6742\u4e14\u76f8\u4e92\u5173\u8054\u7684\u53ef\u6301\u7eed\u53d1\u5c55\u6311\u6218\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002", "method": "\u91c7\u7528\u8fed\u4ee3\u4e13\u5bb6\u5bf9\u8bdd\u8bc4\u4f30\u3001AI\u652f\u6301\u7684\u6587\u732e\u7efc\u8ff0\uff08\u6db5\u76d68500\u591a\u7bc7\u5b66\u672f\u51fa\u7248\u7269\uff09\u4ee5\u53ca\u516b\u4e2a\u5177\u4f53\u95ee\u9898\u9886\u57df\u7684\u4e13\u5bb6\u6df1\u5ea6\u7814\u7a76\u3002", "result": "\u6784\u5efa\u4e86\u5bf9AI\u5728\u53ef\u6301\u7eed\u53d1\u5c55\u4e2d\u5e94\u7528\u7684\u6f5c\u529b\u548c\u5c40\u9650\u6027\u7684\u7406\u89e3\uff0c\u5e76\u4e3a\u4e0d\u540c\u5229\u76ca\u76f8\u5173\u65b9\u63d0\u4f9b\u4e86\u5177\u4f53\u5efa\u8bae\u3002", "conclusion": "AI\u4f5c\u4e3a\u7814\u7a76\u65b9\u6cd5\u5728\u5e94\u5bf9\u53ef\u6301\u7eed\u53d1\u5c55\u6311\u6218\u65b9\u9762\u5177\u6709\u91cd\u8981\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u8d1f\u8d23\u4efb\u7684\u5e94\u7528\u548c\u591a\u65b9\u534f\u4f5c\u3002"}}
{"id": "2510.23646", "categories": ["cs.SI", "05C12 (primary), 68R10 (secondary)"], "pdf": "https://arxiv.org/pdf/2510.23646", "abs": "https://arxiv.org/abs/2510.23646", "authors": ["R. Scott Johnson"], "title": "Hamming Graph Metrics: A Multi-Scale Framework for Structural Redundancy and Uniqueness in Graphs", "comment": "57 pages, 3 tables, two appendices,", "summary": "Traditional graph centrality measures effectively quantify node importance\nbut fail to capture the structural uniqueness of multi-scale connectivity\npatterns -- critical for understanding network resilience and function. This\npaper introduces \\emph{Hamming Graph Metrics (HGM)}, a framework that\nrepresents a graph by its exact-$k$ reachability tensor\n$\\mathcal{B}G\\in{0,1}^{N\\times N\\times D}$ with slices\n$(\\mathcal{B}G){:,:,1}=A$ and, for $k\\ge 2$,\n$(\\mathcal{B}G){:,:,k}=\\mathbf{1}!\\left[\\sum{t=1}^{k}\nA^t>0\\right]-\\mathbf{1}!\\left[\\sum_{t=1}^{k-1} A^t>0\\right]$ (shortest-path\ndistance exactly $k$). Guarantees. (i) \\emph{Permutation invariance}:\n$d_{\\mathrm{HGM}}(\\pi(G),\\pi(H))=d_{\\mathrm{HGM}}(G,H)$ for all vertex\nrelabelings $\\pi$; (ii) the \\emph{tensor Hamming distance}\n$d_{\\mathrm{HGM}}(G,H):=|,\\mathcal{B}G-\\mathcal{B}H,|{1}=\\sum{i,j,k}\\mathbf{1}!\\big[(\\mathcal{B}G){ijk}\\neq(\\mathcal{B}H){ijk}\\big]$\nis a \\emph{true metric} on labeled graphs; and (iii) \\emph{Lipschitz stability}\nto edge perturbations with explicit degree-dependent constants (see\nGraph-to-Graph Comparison'' $\\to$ Tensor Hamming metric''; ``Stability to edge\nperturbations''; Appendix A). We develop: (1) \\emph{per-scale spectral\nanalysis} via classical MDS on double-centered Hamming matrices $D^{(k)}$,\nyielding spectral coordinates and explained variances; (2) \\emph{summary\nstatistics} for node-wise and graph-level structural dissimilarity; (3)\n\\emph{graph-to-graph comparison} via the metric above; and (4) \\emph{analytic\nproperties} including extremal characterizations, multi-scale limits, and\nstability bounds.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u6c49\u660e\u56fe\u5ea6\u91cf\uff08HGM\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u7cbe\u786ek\u53ef\u8fbe\u6027\u5f20\u91cf\u8868\u793a\u56fe\u7ed3\u6784\uff0c\u80fd\u591f\u91cf\u5316\u591a\u5c3a\u5ea6\u8fde\u901a\u6027\u6a21\u5f0f\u7684\u72ec\u7279\u6027\uff0c\u4e3a\u7f51\u7edc\u5f39\u6027\u548c\u529f\u80fd\u5206\u6790\u63d0\u4f9b\u65b0\u5de5\u5177\u3002", "motivation": "\u4f20\u7edf\u56fe\u4e2d\u5fc3\u6027\u5ea6\u91cf\u80fd\u6709\u6548\u91cf\u5316\u8282\u70b9\u91cd\u8981\u6027\uff0c\u4f46\u65e0\u6cd5\u6355\u6349\u591a\u5c3a\u5ea6\u8fde\u901a\u6027\u6a21\u5f0f\u7684\u7ed3\u6784\u72ec\u7279\u6027\uff0c\u8fd9\u5bf9\u4e8e\u7406\u89e3\u7f51\u7edc\u5f39\u6027\u548c\u529f\u80fd\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u7cbe\u786ek\u53ef\u8fbe\u6027\u5f20\u91cf\u8868\u793a\u56fe\uff0c\u5b9a\u4e49\u5f20\u91cf\u6c49\u660e\u8ddd\u79bb\u4f5c\u4e3a\u56fe\u95f4\u5ea6\u91cf\uff0c\u5e76\u5f00\u53d1\u4e86\u591a\u5c3a\u5ea6\u8c31\u5206\u6790\u3001\u603b\u7ed3\u7edf\u8ba1\u91cf\u3001\u56fe\u95f4\u6bd4\u8f83\u65b9\u6cd5\u548c\u5206\u6790\u6027\u8d28\u3002", "result": "HGM\u6846\u67b6\u5177\u6709\u7f6e\u6362\u4e0d\u53d8\u6027\u3001\u662f\u6807\u8bb0\u56fe\u4e0a\u7684\u771f\u5ea6\u91cf\u3001\u5bf9\u8fb9\u6270\u52a8\u5177\u6709Lipschitz\u7a33\u5b9a\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5ea6\u76f8\u5173\u7684\u663e\u5f0f\u5e38\u6570\u3002", "conclusion": "HGM\u6846\u67b6\u4e3a\u56fe\u7ed3\u6784\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u591a\u5c3a\u5ea6\u89c6\u89d2\uff0c\u80fd\u591f\u6709\u6548\u6355\u6349\u4f20\u7edf\u65b9\u6cd5\u5ffd\u7565\u7684\u7ed3\u6784\u72ec\u7279\u6027\uff0c\u5728\u7f51\u7edc\u5f39\u6027\u548c\u529f\u80fd\u5206\u6790\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.24174", "categories": ["econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2510.24174", "abs": "https://arxiv.org/abs/2510.24174", "authors": ["Yun-Shi Dai", "Peng-Fei Dai", "St\u00e9phane Goutte", "Duc Khuong Nguyen", "Wei-Xing Zhou"], "title": "Moment connectedness and driving factors in the energy-food nexus: A time-frequency perspective", "comment": "46 pages, 10 figures", "summary": "With escalating macroeconomic uncertainty, the risk interlinkages between\nenergy and food markets have become increasingly complex, posing serious\nchallenges to global energy and food security. This paper proposes an\nintegrated framework combining the GJRSK model, the time-frequency\nconnectedness analysis, and the random forest method to systematically\ninvestigate the moment connectedness within the energy-food nexus and explore\nthe key drivers of various spillover effects. The results reveal significant\nmultidimensional risk spillovers with pronounced time variation, heterogeneity,\nand crisis sensitivity. Return and skewness connectedness are primarily driven\nby short-term spillovers, kurtosis connectedness is more prominent over the\nmedium term, while volatility connectedness is dominated by long-term dynamics.\nNotably, crude oil consistently serves as a central transmitter in diverse\nconnectedness networks. Furthermore, the spillover effects are influenced by\nmultiple factors, including macro-financial conditions, oil supply-demand\nfundamentals, policy uncertainties, and climate-related shocks, with the core\ndrivers of connectedness varying considerably across different moments and\ntimescales. These findings provide valuable insights for the coordinated\ngovernance of energy and food markets, the improvement of multilayered risk\nearly-warning systems, and the optimization of investment strategies.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u96c6\u6210\u6846\u67b6\u5206\u6790\u80fd\u6e90\u4e0e\u98df\u54c1\u5e02\u573a\u95f4\u7684\u591a\u7ef4\u98ce\u9669\u6ea2\u51fa\u6548\u5e94\uff0c\u53d1\u73b0\u539f\u6cb9\u662f\u4e3b\u8981\u98ce\u9669\u4f20\u64ad\u8005\uff0c\u4e0d\u540c\u77e9\uff08\u6536\u76ca\u3001\u504f\u5ea6\u3001\u5cf0\u5ea6\u3001\u6ce2\u52a8\u7387\uff09\u7684\u6ea2\u51fa\u6548\u5e94\u5728\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u8868\u73b0\u5404\u5f02\uff0c\u53d7\u591a\u79cd\u5b8f\u89c2\u548c\u6c14\u5019\u56e0\u7d20\u5f71\u54cd\u3002", "motivation": "\u968f\u7740\u5b8f\u89c2\u7ecf\u6d4e\u4e0d\u786e\u5b9a\u6027\u52a0\u5267\uff0c\u80fd\u6e90\u4e0e\u98df\u54c1\u5e02\u573a\u95f4\u7684\u98ce\u9669\u8054\u52a8\u65e5\u76ca\u590d\u6742\uff0c\u5bf9\u5168\u7403\u80fd\u6e90\u548c\u98df\u54c1\u5b89\u5168\u6784\u6210\u4e25\u91cd\u6311\u6218\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u5176\u98ce\u9669\u6ea2\u51fa\u673a\u5236\u3002", "method": "\u7ed3\u5408GJRSK\u6a21\u578b\u3001\u65f6\u9891\u8054\u52a8\u5206\u6790\u548c\u968f\u673a\u68ee\u6797\u65b9\u6cd5\uff0c\u6784\u5efa\u96c6\u6210\u6846\u67b6\u6765\u7814\u7a76\u80fd\u6e90-\u98df\u54c1\u5173\u8054\u4e2d\u7684\u77e9\u8054\u52a8\u6027\uff0c\u5e76\u63a2\u7d22\u5404\u79cd\u6ea2\u51fa\u6548\u5e94\u7684\u5173\u952e\u9a71\u52a8\u56e0\u7d20\u3002", "result": "\u53d1\u73b0\u663e\u8457\u7684\u591a\u7ef4\u98ce\u9669\u6ea2\u51fa\uff0c\u5177\u6709\u660e\u663e\u7684\u65f6\u95f4\u53d8\u5316\u6027\u3001\u5f02\u8d28\u6027\u548c\u5371\u673a\u654f\u611f\u6027\u3002\u6536\u76ca\u548c\u504f\u5ea6\u8054\u52a8\u4e3b\u8981\u7531\u77ed\u671f\u6ea2\u51fa\u9a71\u52a8\uff0c\u5cf0\u5ea6\u8054\u52a8\u5728\u4e2d\u957f\u671f\u66f4\u7a81\u51fa\uff0c\u6ce2\u52a8\u7387\u8054\u52a8\u5219\u53d7\u957f\u671f\u52a8\u6001\u4e3b\u5bfc\u3002\u539f\u6cb9\u5728\u4e0d\u540c\u8054\u52a8\u7f51\u7edc\u4e2d\u59cb\u7ec8\u662f\u6838\u5fc3\u4f20\u64ad\u8005\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u80fd\u6e90\u4e0e\u98df\u54c1\u5e02\u573a\u7684\u534f\u540c\u6cbb\u7406\u3001\u591a\u5c42\u6b21\u98ce\u9669\u9884\u8b66\u7cfb\u7edf\u6539\u8fdb\u4ee5\u53ca\u6295\u8d44\u7b56\u7565\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2510.24496", "categories": ["econ.EM"], "pdf": "https://arxiv.org/pdf/2510.24496", "abs": "https://arxiv.org/abs/2510.24496", "authors": ["Jean-Pierre Florens", "Anna Simoni"], "title": "Panel data models with randomly generated groups", "comment": null, "summary": "We develop a structural framework for modeling and inferring unobserved\nheterogeneity in dynamic panel-data models. Unlike methods treating clustering\nas a descriptive device, we model heterogeneity as arising from a latent\nclustering mechanism, where the number of clusters is unknown and estimated.\nBuilding on the mixture of finite mixtures (MFM) approach, our method avoids\nthe clustering inconsistency issues of Dirichlet process mixtures and provides\nan interpretable representation of the population clustering structure. We\nextend the Telescoping Sampler of Fruhwirth-Schnatter et al. (2021) to dynamic\npanels with covariates, yielding an efficient MCMC algorithm that delivers full\nBayesian inference and credible sets. We show that asymptotically the posterior\ndistribution of the mixing measure contracts around the truth at parametric\nrates in Wasserstein distance, ensuring recovery of clustering and structural\nparameters. Simulations demonstrate strong finite-sample performance. Finally,\nan application to the income-democracy relationship reveals latent\nheterogeneity only when controlling for additional covariates.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u52a8\u6001\u9762\u677f\u6570\u636e\u6a21\u578b\u4e2d\u672a\u89c2\u6d4b\u5f02\u8d28\u6027\u5efa\u6a21\u548c\u63a8\u65ad\u7684\u7ed3\u6784\u6846\u67b6\uff0c\u57fa\u4e8e\u6709\u9650\u6df7\u5408\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u72c4\u5229\u514b\u96f7\u8fc7\u7a0b\u6df7\u5408\u7684\u805a\u7c7b\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u805a\u7c7b\u7ed3\u6784\u8868\u793a\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u805a\u7c7b\u89c6\u4e3a\u63cf\u8ff0\u6027\u5de5\u5177\uff0c\u800c\u672c\u6587\u65e8\u5728\u5efa\u6a21\u5f02\u8d28\u6027\u4f5c\u4e3a\u6f5c\u5728\u7684\u805a\u7c7b\u673a\u5236\uff0c\u5176\u4e2d\u805a\u7c7b\u6570\u91cf\u672a\u77e5\u4e14\u9700\u8981\u4f30\u8ba1\uff0c\u4ee5\u66f4\u51c6\u786e\u5730\u6355\u6349\u52a8\u6001\u9762\u677f\u6570\u636e\u4e2d\u7684\u5f02\u8d28\u6027\u7ed3\u6784\u3002", "method": "\u57fa\u4e8e\u6709\u9650\u6df7\u5408\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86Fruhwirth-Schnatter\u7b49\u4eba\u7684Telescoping Sampler\u5230\u5e26\u6709\u534f\u53d8\u91cf\u7684\u52a8\u6001\u9762\u677f\u6a21\u578b\uff0c\u5f00\u53d1\u4e86\u9ad8\u6548\u7684MCMC\u7b97\u6cd5\u8fdb\u884c\u5168\u8d1d\u53f6\u65af\u63a8\u65ad\u3002", "result": "\u6e10\u8fd1\u5206\u6790\u663e\u793a\u6df7\u5408\u6d4b\u5ea6\u7684\u540e\u9a8c\u5206\u5e03\u5728Wasserstein\u8ddd\u79bb\u4e0b\u4ee5\u53c2\u6570\u901f\u7387\u6536\u7f29\u5230\u771f\u5b9e\u503c\uff0c\u786e\u4fdd\u4e86\u805a\u7c7b\u548c\u7ed3\u6784\u53c2\u6570\u7684\u6062\u590d\u3002\u6a21\u62df\u7814\u7a76\u5c55\u793a\u4e86\u826f\u597d\u7684\u6709\u9650\u6837\u672c\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8bc6\u522b\u52a8\u6001\u9762\u677f\u6570\u636e\u4e2d\u7684\u6f5c\u5728\u5f02\u8d28\u6027\uff0c\u5728\u6536\u5165-\u6c11\u4e3b\u5173\u7cfb\u5e94\u7528\u4e2d\uff0c\u53ea\u6709\u5728\u63a7\u5236\u989d\u5916\u534f\u53d8\u91cf\u65f6\u624d\u80fd\u63ed\u793a\u6f5c\u5728\u7684\u5f02\u8d28\u6027\u3002"}}
{"id": "2510.23744", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23744", "abs": "https://arxiv.org/abs/2510.23744", "authors": ["Eline M. Bovy", "Caleb Probine", "Marnix Suilen", "Ufuk Topcu", "Nils Jansen"], "title": "Multi-Environment POMDPs: Discrete Model Uncertainty Under Partial Observability", "comment": "Accepted at NeurIPS 2025", "summary": "Multi-environment POMDPs (ME-POMDPs) extend standard POMDPs with discrete\nmodel uncertainty. ME-POMDPs represent a finite set of POMDPs that share the\nsame state, action, and observation spaces, but may arbitrarily vary in their\ntransition, observation, and reward models. Such models arise, for instance,\nwhen multiple domain experts disagree on how to model a problem. The goal is to\nfind a single policy that is robust against any choice of POMDP within the set,\ni.e., a policy that maximizes the worst-case reward across all POMDPs. We\ngeneralize and expand on existing work in the following way. First, we show\nthat ME-POMDPs can be generalized to POMDPs with sets of initial beliefs, which\nwe call adversarial-belief POMDPs (AB-POMDPs). Second, we show that any\narbitrary ME-POMDP can be reduced to a ME-POMDP that only varies in its\ntransition and reward functions or only in its observation and reward\nfunctions, while preserving (optimal) policies. We then devise exact and\napproximate (point-based) algorithms to compute robust policies for AB-POMDPs,\nand thus ME-POMDPs. We demonstrate that we can compute policies for standard\nPOMDP benchmarks extended to the multi-environment setting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u591a\u73af\u5883POMDP\uff08ME-POMDP\uff09\u7684\u6982\u5ff5\uff0c\u7528\u4e8e\u5904\u7406\u5177\u6709\u79bb\u6563\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684POMDP\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u7cbe\u786e\u548c\u8fd1\u4f3c\u7b97\u6cd5\u6765\u8ba1\u7b97\u9c81\u68d2\u7b56\u7565\u3002", "motivation": "\u5f53\u591a\u4e2a\u9886\u57df\u4e13\u5bb6\u5bf9\u95ee\u9898\u5efa\u6a21\u5b58\u5728\u5206\u6b67\u65f6\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684\u6846\u67b6\uff0c\u76ee\u6807\u662f\u627e\u5230\u4e00\u4e2a\u5728\u6240\u6709\u53ef\u80fdPOMDP\u6a21\u578b\u4e2d\u90fd\u8868\u73b0\u826f\u597d\u7684\u5355\u4e00\u9c81\u68d2\u7b56\u7565\u3002", "method": "\u5c06ME-POMDP\u63a8\u5e7f\u4e3a\u5177\u6709\u521d\u59cb\u4fe1\u5ff5\u96c6\u5408\u7684AB-POMDP\uff0c\u8bc1\u660eME-POMDP\u53ef\u4ee5\u7b80\u5316\u4e3a\u4ec5\u5728\u8f6c\u79fb\u548c\u5956\u52b1\u51fd\u6570\u6216\u4ec5\u5728\u89c2\u5bdf\u548c\u5956\u52b1\u51fd\u6570\u4e0a\u53d8\u5316\u7684\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u7cbe\u786e\u548c\u57fa\u4e8e\u70b9\u7684\u8fd1\u4f3c\u7b97\u6cd5\u3002", "result": "\u6210\u529f\u5c06\u6807\u51c6POMDP\u57fa\u51c6\u6269\u5c55\u5230\u591a\u73af\u5883\u8bbe\u7f6e\uff0c\u5e76\u80fd\u591f\u8ba1\u7b97\u76f8\u5e94\u7684\u9c81\u68d2\u7b56\u7565\u3002", "conclusion": "ME-POMDP\u6846\u67b6\u6709\u6548\u5904\u7406\u4e86POMDP\u4e2d\u7684\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7AB-POMDP\u7684\u63a8\u5e7f\u548c\u7b80\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408\u5f00\u53d1\u7684\u7b97\u6cd5\uff0c\u80fd\u591f\u4e3a\u591a\u73af\u5883\u95ee\u9898\u8ba1\u7b97\u9c81\u68d2\u7b56\u7565\u3002"}}
{"id": "2510.23821", "categories": ["stat.AP", "62P05", "G.3"], "pdf": "https://arxiv.org/pdf/2510.23821", "abs": "https://arxiv.org/abs/2510.23821", "authors": ["\u0141ukasz Delong", "Mario W\u00fcthrich"], "title": "Universal Inference for Testing Calibration of Mean Estimates within the Exponential Dispersion Family", "comment": "34 pages", "summary": "Calibration of mean estimates for predictions is a crucial property in many\napplications, particularly in the fields of financial and actuarial\ndecision-making. In this paper, we first review classical approaches for\nvalidating mean-calibration, and we discuss the Likelihood Ratio Test (LRT)\nwithin the Exponential Dispersion Family (EDF). Then, we investigate the\nframework of universal inference to test for mean-calibration. We develop a\nsub-sampled split LRT within the EDF that provides finite sample guarantees\nwith universally valid critical values. We investigate type I error, power and\ne-power of this sub-sampled split LRT, we compare it to the classical LRT, and\nwe propose a novel test statistics based on the sub-sampled split LRT to\nenhance the performance of the calibration test. A numerical analysis verifies\nthat our proposal is an attractive alternative to the classical LRT achieving a\nhigh power in detecting miscalibration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u6307\u6570\u5206\u6563\u65cf\u4e2d\u4f7f\u7528\u5b50\u91c7\u6837\u5206\u5272\u4f3c\u7136\u6bd4\u68c0\u9a8c\u6765\u9a8c\u8bc1\u5747\u503c\u6821\u51c6\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u63d0\u4f9b\u6709\u9650\u6837\u672c\u4fdd\u8bc1\u548c\u666e\u904d\u6709\u6548\u7684\u4e34\u754c\u503c\u3002", "motivation": "\u5728\u91d1\u878d\u548c\u7cbe\u7b97\u51b3\u7b56\u7b49\u9886\u57df\uff0c\u9884\u6d4b\u7684\u5747\u503c\u6821\u51c6\u9a8c\u8bc1\u81f3\u5173\u91cd\u8981\u3002\u9700\u8981\u5f00\u53d1\u5177\u6709\u6709\u9650\u6837\u672c\u4fdd\u8bc1\u7684\u6821\u51c6\u6d4b\u8bd5\u65b9\u6cd5\u3002", "method": "\u5728\u6307\u6570\u5206\u6563\u65cf\u4e2d\u5f00\u53d1\u5b50\u91c7\u6837\u5206\u5272\u4f3c\u7136\u6bd4\u68c0\u9a8c\uff0c\u4f7f\u7528\u666e\u904d\u6709\u6548\u7684\u4e34\u754c\u503c\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u8be5\u68c0\u9a8c\u7684\u65b0\u6d4b\u8bd5\u7edf\u8ba1\u91cf\u3002", "result": "\u6570\u503c\u5206\u6790\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u68c0\u6d4b\u9519\u8bef\u6821\u51c6\u65b9\u9762\u5177\u6709\u9ad8\u529f\u6548\uff0c\u662f\u7ecf\u5178\u4f3c\u7136\u6bd4\u68c0\u9a8c\u7684\u6709\u5438\u5f15\u529b\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "\u5b50\u91c7\u6837\u5206\u5272\u4f3c\u7136\u6bd4\u68c0\u9a8c\u4e3a\u5747\u503c\u6821\u51c6\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u5177\u6709\u6709\u9650\u6837\u672c\u4fdd\u8bc1\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5728\u68c0\u6d4b\u9519\u8bef\u6821\u51c6\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.23820", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.23820", "abs": "https://arxiv.org/abs/2510.23820", "authors": ["Shahab Jahanbazi", "Mateen Ashraf", "Onel L. A. L\u00f3pez"], "title": "MDP-based Energy-aware Task Scheduling for Battery-less IoT", "comment": "13 pages, 11 figures", "summary": "Realizing high long-term task completion rates represents a fundamental\nchallenge in battery-less Internet of Things (IoT) devices powered by ambient\nenergy harvesting. This difficulty is primarily due to the stochastic and\ntime-varying characteristics of the available energy, which significantly\ncomplicate the design of optimal task scheduling policies. In this paper, we\nconsider a battery-less IoT device that must periodically report sensing\nmeasurements to a monitoring center. We adopt the Markov decision process (MDP)\nframework to handle energy variability while aiming to maximize the long-term\ntask completion rate. For this, we first identify its components and then\ndefine two appropriate reward functions. We demonstrate the inherent properties\nassociated with the MDP formulation and the related optimal policy.\nSubsequently, we solve the resulting optimization problem, leading to the\noptimal stationary threshold-based (OSTB) scheduling. Simulation results\ndemonstrate that OSTB outperforms the well-known ``as late as possible'' (ALAP)\nscheduling strategy. For instance, an $8.6\\%$ increase in the task completion\nrate, along with a $65\\%$ reduction in power failures and a $86.29\\%$ decrease\nin execution delays during task execution are registered assuming a $4.7$ mF\ncapacitor.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u7684\u6700\u4f18\u56fa\u5b9a\u9608\u503c\u8c03\u5ea6\u7b56\u7565\uff0c\u7528\u4e8e\u63d0\u5347\u65e0\u7535\u6c60\u7269\u8054\u7f51\u8bbe\u5907\u7684\u957f\u671f\u4efb\u52a1\u5b8c\u6210\u7387\uff0c\u76f8\u6bd4ALAP\u7b56\u7565\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u65e0\u7535\u6c60\u7269\u8054\u7f51\u8bbe\u5907\u5728\u73af\u5883\u80fd\u91cf\u6536\u96c6\u6761\u4ef6\u4e0b\uff0c\u7531\u4e8e\u80fd\u91cf\u968f\u673a\u6027\u548c\u65f6\u53d8\u6027\u5bfc\u81f4\u7684\u957f\u671f\u4efb\u52a1\u5b8c\u6210\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u6846\u67b6\u5904\u7406\u80fd\u91cf\u53d8\u5316\uff0c\u5b9a\u4e49\u4e24\u79cd\u5956\u52b1\u51fd\u6570\uff0c\u63a8\u5bfc\u51fa\u6700\u4f18\u56fa\u5b9a\u9608\u503c\u8c03\u5ea6\u7b56\u7565\u3002", "result": "OSTB\u7b56\u7565\u76f8\u6bd4ALAP\u7b56\u7565\u63d0\u9ad8\u4e868.6%\u7684\u4efb\u52a1\u5b8c\u6210\u7387\uff0c\u51cf\u5c11\u4e8665%\u7684\u529f\u7387\u6545\u969c\u548c86.29%\u7684\u6267\u884c\u5ef6\u8fdf\u3002", "conclusion": "OSTB\u8c03\u5ea6\u7b56\u7565\u80fd\u6709\u6548\u5e94\u5bf9\u80fd\u91cf\u53d8\u5316\uff0c\u663e\u8457\u63d0\u5347\u65e0\u7535\u6c60\u7269\u8054\u7f51\u8bbe\u5907\u7684\u4efb\u52a1\u5b8c\u6210\u6027\u80fd\u3002"}}
{"id": "2510.24388", "categories": ["econ.TH"], "pdf": "https://arxiv.org/pdf/2510.24388", "abs": "https://arxiv.org/abs/2510.24388", "authors": ["Yukihiko Funaki", "Yukio Koriyama", "Satoshi Nakada"], "title": "A Characterization of Egalitarian and Proportional Sharing Principles: An Efficient Extension Operator Approach", "comment": null, "summary": "Some well-known solutions for cooperative games with transferable utility\n(TU-games), such as the Banzhaf value, the Myerson value, and the Aumann-Dreze\nvalue, fail to satisfy efficiency, although they possess other desirable\nproperties. This paper proposes a new approach to restore efficiency by\nextending any underlying solution to an efficient one, through what we call an\nefficient extension operator. We consider novel axioms for an efficient\nextension operator and characterize the egalitarian surplus sharing method and\nthe proportional sharing method in a unified manner. These results can be\nconsidered as new justifications for the f-ESS values and the f-PS values\nintroduced by Funaki and Koriyama (2025), which are generalizations of the\nequal surplus sharing value and the proportional sharing value. Our results\noffer an additional rationale for the values with an arbitrary underlying\nsolution. As applications, we develop an efficient-fair extension of the\nsolutions for the TU-games with communication networks and its variant for\nTU-games with coalition structures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u9ad8\u6548\u6269\u5c55\u7b97\u5b50\u6062\u590d\u5408\u4f5c\u535a\u5f08\u89e3\u6548\u7387\u7684\u65b0\u65b9\u6cd5\uff0c\u7edf\u4e00\u63cf\u8ff0\u4e86\u5e73\u7b49\u5269\u4f59\u5206\u4eab\u548c\u6bd4\u4f8b\u5206\u4eab\u65b9\u6cd5\uff0c\u4e3af-ESS\u548cf-PS\u503c\u63d0\u4f9b\u4e86\u65b0\u7406\u8bba\u4f9d\u636e\u3002", "motivation": "\u8bb8\u591a\u8457\u540d\u7684\u5408\u4f5c\u535a\u5f08\u89e3\uff08\u5982Banzhaf\u503c\u3001Myerson\u503c\u7b49\uff09\u7f3a\u4e4f\u6548\u7387\u6027\uff0c\u867d\u7136\u5177\u6709\u5176\u4ed6\u4f18\u826f\u7279\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u9ad8\u6548\u6269\u5c55\u7b97\u5b50\u6765\u6062\u590d\u8fd9\u4e9b\u89e3\u7684\u6548\u7387\u6027\u3002", "method": "\u63d0\u51fa\u9ad8\u6548\u6269\u5c55\u7b97\u5b50\u6982\u5ff5\uff0c\u8003\u8651\u65b0\u7684\u516c\u7406\u4f53\u7cfb\uff0c\u7edf\u4e00\u523b\u753b\u5e73\u7b49\u5269\u4f59\u5206\u4eab\u548c\u6bd4\u4f8b\u5206\u4eab\u65b9\u6cd5\u3002", "result": "\u4e3aFunaki\u548cKoriyama\uff082025\uff09\u63d0\u51fa\u7684f-ESS\u503c\u548cf-PS\u503c\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u4f9d\u636e\uff0c\u8fd9\u4e9b\u503c\u662f\u5e73\u7b49\u5269\u4f59\u5206\u4eab\u503c\u548c\u6bd4\u4f8b\u5206\u4eab\u503c\u7684\u63a8\u5e7f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4efb\u610f\u57fa\u7840\u89e3\u63d0\u4f9b\u4e86\u989d\u5916\u7406\u8bba\u4f9d\u636e\uff0c\u5e76\u5e94\u7528\u4e8e\u5e26\u901a\u4fe1\u7f51\u7edc\u7684TU\u535a\u5f08\u548c\u5e26\u8054\u76df\u7ed3\u6784\u7684TU\u535a\u5f08\u7684\u9ad8\u6548\u516c\u5e73\u6269\u5c55\u3002"}}
{"id": "2510.23902", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23902", "abs": "https://arxiv.org/abs/2510.23902", "authors": ["Jans Solano", "Diego Quiroz"], "title": "Stand, Walk, Navigate: Recovery-Aware Visual Navigation on a Low-Cost Wheeled Quadruped", "comment": "Accepted at the IROS 2025 Workshop on Wheeled-Legged Robots", "summary": "Wheeled-legged robots combine the efficiency of wheels with the obstacle\nnegotiation of legs, yet many state-of-the-art systems rely on costly actuators\nand sensors, and fall-recovery is seldom integrated, especially for\nwheeled-legged morphologies. This work presents a recovery-aware\nvisual-inertial navigation system on a low-cost wheeled quadruped. The proposed\nsystem leverages vision-based perception from a depth camera and deep\nreinforcement learning policies for robust locomotion and autonomous recovery\nfrom falls across diverse terrains. Simulation experiments show agile mobility\nwith low-torque actuators over irregular terrain and reliably recover from\nexternal perturbations and self-induced failures. We further show goal directed\nnavigation in structured indoor spaces with low-cost perception. Overall, this\napproach lowers the barrier to deploying autonomous navigation and robust\nlocomotion policies in budget-constrained robotic platforms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u4f4e\u6210\u672c\u8f6e\u817f\u5f0f\u56db\u8db3\u673a\u5668\u4eba\u7684\u6062\u590d\u611f\u77e5\u89c6\u89c9\u60ef\u6027\u5bfc\u822a\u7cfb\u7edf\uff0c\u5229\u7528\u6df1\u5ea6\u76f8\u673a\u89c6\u89c9\u611f\u77e5\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5b9e\u73b0\u9c81\u68d2\u8fd0\u52a8\u63a7\u5236\u548c\u81ea\u4e3b\u8dcc\u5012\u6062\u590d\u3002", "motivation": "\u73b0\u6709\u8f6e\u817f\u5f0f\u673a\u5668\u4eba\u4f9d\u8d56\u6602\u8d35\u6267\u884c\u5668\u548c\u4f20\u611f\u5668\uff0c\u4e14\u5f88\u5c11\u96c6\u6210\u8dcc\u5012\u6062\u590d\u529f\u80fd\uff0c\u7279\u522b\u662f\u9488\u5bf9\u8f6e\u817f\u5f0f\u5f62\u6001\u3002\u672c\u5de5\u4f5c\u65e8\u5728\u964d\u4f4e\u81ea\u4e3b\u5bfc\u822a\u548c\u9c81\u68d2\u8fd0\u52a8\u7b56\u7565\u5728\u9884\u7b97\u53d7\u9650\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u7684\u90e8\u7f72\u95e8\u69db\u3002", "method": "\u7ed3\u5408\u6df1\u5ea6\u76f8\u673a\u89c6\u89c9\u611f\u77e5\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u5f00\u53d1\u4e86\u89c6\u89c9\u60ef\u6027\u5bfc\u822a\u7cfb\u7edf\uff0c\u7528\u4e8e\u9c81\u68d2\u8fd0\u52a8\u63a7\u5236\u548c\u81ea\u4e3b\u8dcc\u5012\u6062\u590d\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u663e\u793a\u5728\u975e\u89c4\u5219\u5730\u5f62\u4e0a\u4f7f\u7528\u4f4e\u626d\u77e9\u6267\u884c\u5668\u5b9e\u73b0\u654f\u6377\u79fb\u52a8\uff0c\u5e76\u80fd\u53ef\u9760\u5730\u4ece\u5916\u90e8\u6270\u52a8\u548c\u81ea\u8bf1\u5bfc\u6545\u969c\u4e2d\u6062\u590d\u3002\u5728\u7ed3\u6784\u5316\u5ba4\u5185\u73af\u5883\u4e2d\u5c55\u793a\u4e86\u4f4e\u6210\u672c\u611f\u77e5\u7684\u76ee\u6807\u5bfc\u5411\u5bfc\u822a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u5728\u9884\u7b97\u53d7\u9650\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u90e8\u7f72\u81ea\u4e3b\u5bfc\u822a\u548c\u9c81\u68d2\u8fd0\u52a8\u7b56\u7565\u7684\u6280\u672f\u95e8\u69db\u3002"}}
{"id": "2510.24582", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2510.24582", "abs": "https://arxiv.org/abs/2510.24582", "authors": ["Xuenan Cao", "Wai Kei Chung", "Ye Zhao", "Lidia Mengyuan Zhou"], "title": "Politically Speaking: LLMs on Changing International Affairs", "comment": null, "summary": "Ask your chatbot to impersonate an expert from Russia and an expert from US\nand query it on Chinese politics. How might the outputs differ? Or, to prepare\nourselves for the worse, how might they converge? Scholars have raised concerns\nLLM based applications can homogenize cultures and flatten perspectives. But\nexactly how much does LLM generated outputs converge despite explicit different\nrole assignment? This study provides empirical evidence to the above question.\nThe critique centres on pretrained models regurgitating ossified political\njargons used in the Western world when speaking about China, Iran, Russian, and\nUS politics, despite changes in these countries happening daily or hourly. The\nexperiments combine role-prompting and similarity metrics. The results show\nthat AI generated discourses from four models about Iran and China are the most\nhomogeneous and unchanging across all four models, including OpenAI GPT, Google\nGemini, Anthropic Claude, and DeepSeek, despite the prompted perspective change\nand the actual changes in real life. This study does not engage with history,\npolitics, or literature as traditional disciplinary approaches would; instead,\nit takes cues from international and area studies and offers insight on the\nfuture trajectory of shifting political discourse in a digital space\nincreasingly cannibalised by AI.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u89d2\u8272\u63d0\u793a\u548c\u76f8\u4f3c\u6027\u5ea6\u91cf\uff0c\u53d1\u73b0\u56db\u4e2a\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\uff08OpenAI GPT\u3001Google Gemini\u3001Anthropic Claude\u3001DeepSeek\uff09\u5728\u8ba8\u8bba\u4e2d\u56fd\u548c\u4f0a\u6717\u653f\u6cbb\u65f6\uff0c\u5373\u4f7f\u88ab\u8981\u6c42\u4ece\u4e0d\u540c\u56fd\u5bb6\u4e13\u5bb6\u89c6\u89d2\u56de\u7b54\uff0c\u751f\u6210\u5185\u5bb9\u4ecd\u9ad8\u5ea6\u540c\u8d28\u5316\u548c\u56fa\u5316\u3002", "motivation": "\u7814\u7a76\u5173\u6ce8LLM\u5e94\u7528\u53ef\u80fd\u5bfc\u81f4\u6587\u5316\u540c\u8d28\u5316\u548c\u89c6\u89d2\u6241\u5e73\u5316\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u8ba8\u8bba\u4e2d\u56fd\u3001\u4f0a\u6717\u3001\u4fc4\u7f57\u65af\u548c\u7f8e\u56fd\u653f\u6cbb\u65f6\uff0c\u503e\u5411\u4e8e\u91cd\u590d\u897f\u65b9\u4e16\u754c\u56fa\u5316\u7684\u653f\u6cbb\u672f\u8bed\uff0c\u5c3d\u7ba1\u8fd9\u4e9b\u56fd\u5bb6\u7684\u5b9e\u9645\u60c5\u51b5\u5728\u4e0d\u65ad\u53d8\u5316\u3002", "method": "\u7ed3\u5408\u89d2\u8272\u63d0\u793a\uff08\u8ba9AI\u6a21\u62df\u4fc4\u7f57\u65af\u548c\u7f8e\u56fd\u4e13\u5bb6\u89c6\u89d2\uff09\u548c\u76f8\u4f3c\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5206\u6790\u56db\u4e2a\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u653f\u6cbb\u8bba\u8ff0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5173\u4e8e\u4f0a\u6717\u548c\u4e2d\u56fd\u7684AI\u751f\u6210\u8bba\u8ff0\u5728\u6240\u6709\u56db\u4e2a\u6a21\u578b\u4e2d\u90fd\u6700\u4e3a\u540c\u8d28\u5316\u548c\u4e0d\u53d8\uff0c\u5373\u4f7f\u6539\u53d8\u63d0\u793a\u89c6\u89d2\u548c\u73b0\u5b9e\u60c5\u51b5\u5df2\u53d1\u751f\u53d8\u5316\u3002", "conclusion": "\u7814\u7a76\u8868\u660eAI\u751f\u6210\u7684\u653f\u6cbb\u8bba\u8ff0\u5b58\u5728\u663e\u8457\u540c\u8d28\u5316\u73b0\u8c61\uff0c\u7279\u522b\u662f\u5728\u6d89\u53ca\u4e2d\u56fd\u548c\u4f0a\u6717\u7684\u8bdd\u9898\u4e0a\uff0c\u8fd9\u5bf9\u6570\u5b57\u7a7a\u95f4\u4e2d\u653f\u6cbb\u8bba\u8ff0\u7684\u672a\u6765\u53d1\u5c55\u8f68\u8ff9\u63d0\u51fa\u4e86\u91cd\u8981\u8b66\u793a\u3002"}}
{"id": "2510.23648", "categories": ["cs.SI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23648", "abs": "https://arxiv.org/abs/2510.23648", "authors": ["Ashutosh Anshul", "Mohammad Zia Ur Rehman", "Sri Akash Kadali", "Nagendra Kumar"], "title": "RoGBot: Relationship-Oblivious Graph-based Neural Network with Contextual Knowledge for Bot Detection", "comment": "Submitted to IEEE", "summary": "Detecting automated accounts (bots) among genuine users on platforms like\nTwitter remains a challenging task due to the evolving behaviors and adaptive\nstrategies of such accounts. While recent methods have achieved strong\ndetection performance by combining text, metadata, and user relationship\ninformation within graph-based frameworks, many of these models heavily depend\non explicit user-user relationship data. This reliance limits their\napplicability in scenarios where such information is unavailable. To address\nthis limitation, we propose a novel multimodal framework that integrates\ndetailed textual features with enriched user metadata while employing\ngraph-based reasoning without requiring follower-following data. Our method\nuses transformer-based models (e.g., BERT) to extract deep semantic embeddings\nfrom tweets, which are aggregated using max pooling to form comprehensive\nuser-level representations. These are further combined with auxiliary\nbehavioral features and passed through a GraphSAGE model to capture both local\nand global patterns in user behavior. Experimental results on the Cresci-15,\nCresci-17, and PAN 2019 datasets demonstrate the robustness of our approach,\nachieving accuracies of 99.8%, 99.1%, and 96.8%, respectively, and highlighting\nits effectiveness against increasingly sophisticated bot strategies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4bTwitter\u4e0a\u7684\u81ea\u52a8\u5316\u8d26\u6237\uff08\u673a\u5668\u4eba\uff09\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u6587\u672c\u7279\u5f81\u548c\u7528\u6237\u5143\u6570\u636e\uff0c\u4f7f\u7528\u57fa\u4e8e\u56fe\u7684\u65b9\u6cd5\u4f46\u4e0d\u9700\u8981\u5173\u6ce8\u8005\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u68c0\u6d4b\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u7528\u6237\u95f4\u5173\u7cfb\u6570\u636e\uff0c\u9650\u5236\u4e86\u5728\u7f3a\u4e4f\u6b64\u7c7b\u4fe1\u606f\u573a\u666f\u4e0b\u7684\u5e94\u7528\u3002\u9700\u8981\u5f00\u53d1\u4e0d\u4f9d\u8d56\u663e\u5f0f\u7528\u6237\u5173\u7cfb\u6570\u636e\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff08\u5982BERT\uff09\u63d0\u53d6\u63a8\u6587\u7684\u6df1\u5ea6\u8bed\u4e49\u5d4c\u5165\uff0c\u901a\u8fc7\u6700\u5927\u6c60\u5316\u5f62\u6210\u7528\u6237\u7ea7\u8868\u793a\uff0c\u7ed3\u5408\u8f85\u52a9\u884c\u4e3a\u7279\u5f81\uff0c\u518d\u901a\u8fc7GraphSAGE\u6a21\u578b\u6355\u83b7\u7528\u6237\u884c\u4e3a\u7684\u5c40\u90e8\u548c\u5168\u5c40\u6a21\u5f0f\u3002", "result": "\u5728Cresci-15\u3001Cresci-17\u548cPAN 2019\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u51c6\u786e\u7387\u5206\u522b\u8fbe\u523099.8%\u300199.1%\u548c96.8%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e0d\u4f9d\u8d56\u5173\u6ce8\u8005\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u7684\u673a\u5668\u4eba\u68c0\u6d4b\uff0c\u5bf9\u65e5\u76ca\u590d\u6742\u7684\u673a\u5668\u4eba\u7b56\u7565\u5177\u6709\u6709\u6548\u6027\u3002"}}
{"id": "2510.24225", "categories": ["econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2510.24225", "abs": "https://arxiv.org/abs/2510.24225", "authors": ["Christian Dustmann", "Sebastian Otten", "Uta Sch\u00f6nberg", "Jan Stuhler"], "title": "The Effects of Immigration on Places and People -- Identification and Interpretation", "comment": "Accepted at the Journal of Labor Economics", "summary": "Most studies on the labor market effects of immigration use repeated\ncross-sectional data to estimate the effects of immigration on regions. This\npaper shows that such regional effects are composites of effects that address\nfundamental questions in the immigration debate but remain unidentified with\nrepeated cross-sectional data. We provide a unifying empirical framework that\ndecomposes the regional effects of immigration into their underlying components\nand show how these are identifiable from data that track workers over time. Our\nempirical application illustrates that such analysis yields a far more\ninformative picture of immigration's effects on wages, employment, and\noccupational upgrading.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u5b9e\u8bc1\u6846\u67b6\uff0c\u5c06\u79fb\u6c11\u7684\u533a\u57df\u6548\u5e94\u5206\u89e3\u4e3a\u57fa\u672c\u7ec4\u6210\u90e8\u5206\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u8ffd\u8e2a\u5de5\u4eba\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u6765\u8bc6\u522b\u8fd9\u4e9b\u6548\u5e94\u3002", "motivation": "\u73b0\u6709\u5173\u4e8e\u79fb\u6c11\u52b3\u52a8\u529b\u5e02\u573a\u5f71\u54cd\u7684\u7814\u7a76\u5927\u591a\u4f7f\u7528\u91cd\u590d\u6a2a\u622a\u9762\u6570\u636e\u6765\u4f30\u8ba1\u79fb\u6c11\u5bf9\u533a\u57df\u7684\u5f71\u54cd\uff0c\u4f46\u8fd9\u4e9b\u533a\u57df\u6548\u5e94\u5b9e\u9645\u4e0a\u662f\u591a\u4e2a\u57fa\u672c\u6548\u5e94\u7684\u590d\u5408\u4f53\uff0c\u65e0\u6cd5\u901a\u8fc7\u91cd\u590d\u6a2a\u622a\u9762\u6570\u636e\u8bc6\u522b\u3002", "method": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u5b9e\u8bc1\u6846\u67b6\uff0c\u5c06\u79fb\u6c11\u7684\u533a\u57df\u6548\u5e94\u5206\u89e3\u4e3a\u5176\u57fa\u672c\u7ec4\u6210\u90e8\u5206\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u8ffd\u8e2a\u5de5\u4eba\u968f\u65f6\u95f4\u53d8\u5316\u7684\u6570\u636e\u6765\u8bc6\u522b\u8fd9\u4e9b\u6548\u5e94\u3002", "result": "\u5b9e\u8bc1\u5e94\u7528\u8868\u660e\uff0c\u8fd9\u79cd\u5206\u6790\u65b9\u6cd5\u80fd\u591f\u66f4\u5168\u9762\u5730\u63ed\u793a\u79fb\u6c11\u5bf9\u5de5\u8d44\u3001\u5c31\u4e1a\u548c\u804c\u4e1a\u5347\u7ea7\u7684\u5f71\u54cd\u3002", "conclusion": "\u4f7f\u7528\u8ffd\u8e2a\u5de5\u4eba\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u8fdb\u884c\u5206\u6790\uff0c\u80fd\u591f\u63d0\u4f9b\u6bd4\u4f20\u7edf\u91cd\u590d\u6a2a\u622a\u9762\u6570\u636e\u66f4\u4e30\u5bcc\u3001\u66f4\u6709\u4fe1\u606f\u91cf\u7684\u79fb\u6c11\u5f71\u54cd\u56fe\u666f\u3002"}}
{"id": "2510.23746", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23746", "abs": "https://arxiv.org/abs/2510.23746", "authors": ["Laura Mismetti", "Marvin Alberts", "Andreas Krause", "Mara Graziani"], "title": "Test-Time Tuned Language Models Enable End-to-end De Novo Molecular Structure Generation from MS/MS Spectra", "comment": null, "summary": "Tandem Mass Spectrometry enables the identification of unknown compounds in\ncrucial fields such as metabolomics, natural product discovery and\nenvironmental analysis. However, current methods rely on database matching from\npreviously observed molecules, or on multi-step pipelines that require\nintermediate fragment or fingerprint prediction. This makes finding the correct\nmolecule highly challenging, particularly for compounds absent from reference\ndatabases. We introduce a framework that, by leveraging test-time tuning,\nenhances the learning of a pre-trained transformer model to address this gap,\nenabling end-to-end de novo molecular structure generation directly from the\ntandem mass spectra and molecular formulae, bypassing manual annotations and\nintermediate steps. We surpass the de-facto state-of-the-art approach DiffMS on\ntwo popular benchmarks NPLIB1 and MassSpecGym by 100% and 20%, respectively.\nTest-time tuning on experimental spectra allows the model to dynamically adapt\nto novel spectra, and the relative performance gain over conventional\nfine-tuning is of 62% on MassSpecGym. When predictions deviate from the ground\ntruth, the generated molecular candidates remain structurally accurate,\nproviding valuable guidance for human interpretation and more reliable\nidentification.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d4b\u8bd5\u65f6\u8c03\u4f18\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u9884\u8bad\u7ec3transformer\u6a21\u578b\u76f4\u63a5\u4ece\u4e32\u8054\u8d28\u8c31\u548c\u5206\u5b50\u5f0f\u751f\u6210\u5206\u5b50\u7ed3\u6784\uff0c\u65e0\u9700\u4e2d\u95f4\u6b65\u9aa4\u6216\u6570\u636e\u5e93\u5339\u914d\uff0c\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u4e32\u8054\u8d28\u8c31\u5206\u6790\u65b9\u6cd5\u4f9d\u8d56\u6570\u636e\u5e93\u5339\u914d\u6216\u591a\u6b65\u9aa4\u6d41\u7a0b\uff0c\u96be\u4ee5\u8bc6\u522b\u672a\u77e5\u5316\u5408\u7269\uff0c\u7279\u522b\u662f\u53c2\u8003\u6570\u636e\u5e93\u4e2d\u4e0d\u5b58\u5728\u7684\u5206\u5b50\u3002", "method": "\u5229\u7528\u6d4b\u8bd5\u65f6\u8c03\u4f18\u589e\u5f3a\u9884\u8bad\u7ec3transformer\u6a21\u578b\uff0c\u5b9e\u73b0\u4ece\u4e32\u8054\u8d28\u8c31\u548c\u5206\u5b50\u5f0f\u7aef\u5230\u7aef\u7684\u4ece\u5934\u5206\u5b50\u7ed3\u6784\u751f\u6210\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u548c\u4e2d\u95f4\u6b65\u9aa4\u3002", "result": "\u5728NPLIB1\u548cMassSpecGym\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u522b\u8d85\u8d8aDiffMS\u65b9\u6cd5100%\u548c20%\uff0c\u6d4b\u8bd5\u65f6\u8c03\u4f18\u76f8\u6bd4\u4f20\u7edf\u5fae\u8c03\u5728MassSpecGym\u4e0a\u6027\u80fd\u63d0\u534762%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u52a8\u6001\u9002\u5e94\u65b0\u8d28\u8c31\u6570\u636e\uff0c\u5373\u4f7f\u9884\u6d4b\u504f\u79bb\u771f\u5b9e\u503c\uff0c\u751f\u6210\u7684\u5206\u5b50\u5019\u9009\u7ed3\u6784\u4ecd\u7136\u51c6\u786e\uff0c\u4e3a\u4eba\u5de5\u89e3\u91ca\u548c\u53ef\u9760\u8bc6\u522b\u63d0\u4f9b\u6709\u4ef7\u503c\u6307\u5bfc\u3002"}}
{"id": "2510.23976", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.23976", "abs": "https://arxiv.org/abs/2510.23976", "authors": ["Richard Berk"], "title": "Forecasting Melting Points in Svalbard, Norway Using Quantile Gradient Boosting and Adaptive Conformal Prediction Region", "comment": "26 pages, 7 figures, and two blocks of pseudocode", "summary": "Using data from the Longyearbyen weather station, quantile gradient boosting\n(``small AI'') is applied to forecast daily 2023 temperatures in Svalbard,\nNorway. The 0.60 quantile loss weights underestimates about 1.5 times more than\noverestimates. Predictors include five routinely collected indicators of\nweather conditions, each lagged by 14~days, yielding temperature forecasts with\na two-week lead time. Conformal prediction regions quantify forecasting\nuncertainty with provably valid coverage. Forecast accuracy is evaluated with\nattention to local stakeholder concerns, and implications for Arctic adaptation\npolicy are discussed.", "AI": {"tldr": "\u4f7f\u7528\u5206\u4f4d\u6570\u68af\u5ea6\u63d0\u5347\u65b9\u6cd5\u9884\u6d4b\u632a\u5a01\u65af\u74e6\u5c14\u5df4\u7fa4\u5c9b2023\u5e74\u6bcf\u65e5\u6c14\u6e29\uff0c\u91c7\u752814\u5929\u6ede\u540e\u5929\u6c14\u6307\u6807\u4f5c\u4e3a\u9884\u6d4b\u56e0\u5b50\uff0c\u63d0\u4f9b\u4e24\u5468\u63d0\u524d\u671f\u7684\u6e29\u5ea6\u9884\u6d4b\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "motivation": "\u4e3a\u65af\u74e6\u5c14\u5df4\u7fa4\u5c9b\u63d0\u4f9b\u51c6\u786e\u7684\u6c14\u6e29\u9884\u6d4b\uff0c\u6ee1\u8db3\u5f53\u5730\u5229\u76ca\u76f8\u5173\u8005\u7684\u9700\u6c42\uff0c\u5e76\u652f\u6301\u5317\u6781\u9002\u5e94\u653f\u7b56\u7684\u5236\u5b9a\u3002", "method": "\u5e94\u7528\u5206\u4f4d\u6570\u68af\u5ea6\u63d0\u5347\uff08\u5c0f\u578bAI\uff09\u65b9\u6cd5\uff0c\u4f7f\u75285\u4e2a\u5e38\u89c4\u6536\u96c6\u7684\u5929\u6c14\u6307\u6807\uff08\u6ede\u540e14\u5929\uff09\u4f5c\u4e3a\u9884\u6d4b\u56e0\u5b50\uff0c0.60\u5206\u4f4d\u6570\u635f\u5931\u51fd\u6570\u5bf9\u4f4e\u4f30\u8d4b\u4e88\u66f4\u9ad8\u6743\u91cd\uff0c\u5e76\u91c7\u7528\u4fdd\u5f62\u9884\u6d4b\u533a\u57df\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u6210\u529f\u751f\u6210\u4e86\u4e24\u5468\u63d0\u524d\u671f\u7684\u6e29\u5ea6\u9884\u6d4b\uff0c\u5e76\u901a\u8fc7\u4fdd\u5f62\u9884\u6d4b\u533a\u57df\u63d0\u4f9b\u4e86\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u8986\u76d6\u7387\u7684\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4e3a\u5317\u6781\u5730\u533a\u63d0\u4f9b\u53ef\u9760\u7684\u6c14\u6e29\u9884\u6d4b\uff0c\u5bf9\u5f53\u5730\u9002\u5e94\u6c14\u5019\u53d8\u5316\u5177\u6709\u91cd\u8981\u653f\u7b56\u610f\u4e49\u3002"}}
{"id": "2510.23867", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.23867", "abs": "https://arxiv.org/abs/2510.23867", "authors": ["Zhentong Shao", "Jingtao Qin", "Nanpeng Yu"], "title": "Neural Two-Stage Stochastic Volt-VAR Optimization for Three-Phase Unbalanced Distribution Systems with Network Reconfiguration", "comment": null, "summary": "The increasing integration of intermittent distributed energy resources\n(DERs) has introduced significant variability in distribution networks, posing\nchallenges to voltage regulation and reactive power management. This paper\npresents a novel neural two-stage stochastic Volt-VAR optimization (2S-VVO)\nmethod for three-phase unbalanced distribution systems considering network\nreconfiguration under uncertainty. To address the computational intractability\nassociated with solving large-scale scenario-based 2S-VVO problems, a\nlearning-based acceleration strategy is introduced, wherein the second-stage\nrecourse model is approximated by a neural network. This neural approximation\nis embedded into the optimization model as a mixed-integer linear program\n(MILP), enabling effective enforcement of operational constraints related to\nthe first-stage decisions. Numerical simulations on a 123-bus unbalanced\ndistribution system demonstrate that the proposed approach achieves over 50\ntimes speedup compared to conventional solvers and decomposition methods, while\nmaintaining a typical optimality gap below 0.30%. These results underscore the\nmethod's efficacy and scalability in addressing large-scale stochastic VVO\nproblems under practical operating conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u4e09\u76f8\u4e0d\u5e73\u8861\u914d\u7535\u7f51\u7684\u795e\u7ecf\u4e24\u9636\u6bb5\u968f\u673a\u7535\u538b-\u65e0\u529f\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c\u7b2c\u4e8c\u7ea7\u6a21\u578b\u5b9e\u73b0\u8ba1\u7b97\u52a0\u901f\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u63d0\u901f50\u500d\u4ee5\u4e0a\u3002", "motivation": "\u95f4\u6b47\u6027\u5206\u5e03\u5f0f\u80fd\u6e90\u8d44\u6e90\u63a5\u5165\u5bfc\u81f4\u914d\u7535\u7f51\u7535\u538b\u6ce2\u52a8\u52a0\u5267\uff0c\u7ed9\u7535\u538b\u8c03\u8282\u548c\u65e0\u529f\u7ba1\u7406\u5e26\u6765\u6311\u6218\uff0c\u9700\u8981\u89e3\u51b3\u5927\u89c4\u6a21\u968f\u673a\u4f18\u5316\u95ee\u9898\u7684\u8ba1\u7b97\u590d\u6742\u6027\u3002", "method": "\u91c7\u7528\u5b66\u4e60\u578b\u52a0\u901f\u7b56\u7565\uff0c\u7528\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c\u7b2c\u4e8c\u7ea7\u8ffd\u7d22\u6a21\u578b\uff0c\u5e76\u5c06\u5176\u5d4c\u5165\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u4e2d\uff0c\u5728\u8003\u8651\u7f51\u7edc\u91cd\u6784\u4e0d\u786e\u5b9a\u6027\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u7ea6\u675f\u6709\u6548\u6267\u884c\u3002", "result": "\u5728123\u8282\u70b9\u4e0d\u5e73\u8861\u914d\u7535\u7f51\u4e0a\u7684\u6570\u503c\u4eff\u771f\u663e\u793a\uff0c\u76f8\u6bd4\u4f20\u7edf\u6c42\u89e3\u5668\u548c\u5206\u89e3\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b050\u500d\u4ee5\u4e0a\u52a0\u901f\uff0c\u6700\u4f18\u6027\u5dee\u8ddd\u4fdd\u6301\u57280.30%\u4ee5\u4e0b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u968f\u673a\u7535\u538b-\u65e0\u529f\u4f18\u5316\u95ee\u9898\u65f6\u5177\u6709\u9ad8\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u8fd0\u884c\u6761\u4ef6\u3002"}}
{"id": "2510.23928", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23928", "abs": "https://arxiv.org/abs/2510.23928", "authors": ["Raman Jha", "Yang Zhou", "Giuseppe Loianno"], "title": "Adaptive Keyframe Selection for Scalable 3D Scene Reconstruction in Dynamic Environments", "comment": "Under Review for ROBOVIS 2026", "summary": "In this paper, we propose an adaptive keyframe selection method for improved\n3D scene reconstruction in dynamic environments. The proposed method integrates\ntwo complementary modules: an error-based selection module utilizing\nphotometric and structural similarity (SSIM) errors, and a momentum-based\nupdate module that dynamically adjusts keyframe selection thresholds according\nto scene motion dynamics. By dynamically curating the most informative frames,\nour approach addresses a key data bottleneck in real-time perception. This\nallows for the creation of high-quality 3D world representations from a\ncompressed data stream, a critical step towards scalable robot learning and\ndeployment in complex, dynamic environments. Experimental results demonstrate\nsignificant improvements over traditional static keyframe selection strategies,\nsuch as fixed temporal intervals or uniform frame skipping. These findings\nhighlight a meaningful advancement toward adaptive perception systems that can\ndynamically respond to complex and evolving visual scenes. We evaluate our\nproposed adaptive keyframe selection module on two recent state-of-the-art 3D\nreconstruction networks, Spann3r and CUT3R, and observe consistent improvements\nin reconstruction quality across both frameworks. Furthermore, an extensive\nablation study confirms the effectiveness of each individual component in our\nmethod, underlining their contribution to the overall performance gains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u5173\u952e\u5e27\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u57fa\u4e8e\u8bef\u5dee\u7684\u9009\u62e9\u6a21\u5757\u548c\u57fa\u4e8e\u52a8\u91cf\u7684\u66f4\u65b0\u6a21\u5757\uff0c\u52a8\u6001\u8c03\u6574\u5173\u952e\u5e27\u9009\u62e9\u9608\u503c\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u66f4\u597d\u76843D\u573a\u666f\u91cd\u5efa\u3002", "motivation": "\u89e3\u51b3\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u65f6\u611f\u77e5\u7684\u6570\u636e\u74f6\u9888\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u6700\u5177\u4fe1\u606f\u91cf\u7684\u5e27\u6765\u521b\u5efa\u9ad8\u8d28\u91cf\u76843D\u4e16\u754c\u8868\u793a\uff0c\u8fd9\u5bf9\u4e8e\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u53ef\u6269\u5c55\u673a\u5668\u4eba\u5b66\u4e60\u548c\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002", "method": "\u96c6\u6210\u4e24\u4e2a\u4e92\u8865\u6a21\u5757\uff1a\u57fa\u4e8e\u5149\u5ea6\u8bef\u5dee\u548c\u7ed3\u6784\u76f8\u4f3c\u6027(SSIM)\u8bef\u5dee\u7684\u9009\u62e9\u6a21\u5757\uff0c\u4ee5\u53ca\u57fa\u4e8e\u52a8\u91cf\u7684\u66f4\u65b0\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u6839\u636e\u573a\u666f\u8fd0\u52a8\u52a8\u6001\u52a8\u6001\u8c03\u6574\u5173\u952e\u5e27\u9009\u62e9\u9608\u503c\u3002", "result": "\u5728Spann3r\u548cCUT3R\u4e24\u4e2a\u6700\u5148\u8fdb\u76843D\u91cd\u5efa\u7f51\u7edc\u4e0a\u8bc4\u4f30\uff0c\u89c2\u5bdf\u5230\u91cd\u5efa\u8d28\u91cf\u7684\u4e00\u81f4\u6539\u8fdb\uff0c\u76f8\u6bd4\u4f20\u7edf\u7684\u9759\u6001\u5173\u952e\u5e27\u9009\u62e9\u7b56\u7565\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4ee3\u8868\u4e86\u81ea\u9002\u5e94\u611f\u77e5\u7cfb\u7edf\u7684\u6709\u610f\u4e49\u7684\u8fdb\u5c55\uff0c\u80fd\u591f\u52a8\u6001\u54cd\u5e94\u590d\u6742\u548c\u6f14\u53d8\u7684\u89c6\u89c9\u573a\u666f\uff0c\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u5404\u4e2a\u7ec4\u4ef6\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.23662", "categories": ["cs.SI", "cs.IT", "cs.LG", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.23662", "abs": "https://arxiv.org/abs/2510.23662", "authors": ["Liangzhe Han", "Leilei Sun", "Tongyu Zhu", "Tao Tao", "Jibin Wang", "Weifeng Lv"], "title": "JiuTian Chuanliu: A Large Spatiotemporal Model for General-purpose Dynamic Urban Sensing", "comment": null, "summary": "As a window for urban sensing, human mobility contains rich spatiotemporal\ninformation that reflects both residents' behavior preferences and the\nfunctions of urban areas. The analysis of human mobility has attracted the\nattention of many researchers. However, existing methods often address specific\ntasks from a particular perspective, leading to insufficient modeling of human\nmobility and limited applicability of the learned knowledge in various\ndownstream applications. To address these challenges, this paper proposes to\npush massive amounts of human mobility data into a spatiotemporal model,\ndiscover latent semantics behind mobility behavior and support various urban\nsensing tasks. Specifically, a large-scale and widely covering human mobility\ndata is collected through the ubiquitous base station system and a framework\nnamed General-purpose and Dynamic Human Mobility Embedding (GDHME) for urban\nsensing is introduced. The framework follows the self-supervised learning idea\nand contains two major stages. In stage 1, GDHME treats people and regions as\nnodes within a dynamic graph, unifying human mobility data as\npeople-region-time interactions. An encoder operating in continuous-time\ndynamically computes evolving node representations, capturing dynamic states\nfor both people and regions. Moreover, an autoregressive self-supervised task\nis specially designed to guide the learning of the general-purpose node\nembeddings. In stage 2, these representations are utilized to support various\ntasks. To evaluate the effectiveness of our GDHME framework, we further\nconstruct a multi-task urban sensing benchmark. Offline experiments demonstrate\nGDHME's ability to automatically learn valuable node features from vast amounts\nof data. Furthermore, our framework is used to deploy the JiuTian ChuanLiu Big\nModel, a system that has been presented at the 2023 China Mobile Worldwide\nPartner Conference.", "AI": {"tldr": "\u63d0\u51faGDHME\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u5c06\u5927\u89c4\u6a21\u4eba\u6d41\u6570\u636e\u5efa\u6a21\u4e3a\u52a8\u6001\u56fe\uff0c\u5b66\u4e60\u901a\u7528\u8282\u70b9\u5d4c\u5165\uff0c\u652f\u6301\u591a\u79cd\u57ce\u5e02\u611f\u77e5\u4efb\u52a1", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4ece\u7279\u5b9a\u89d2\u5ea6\u5904\u7406\u7279\u5b9a\u4efb\u52a1\uff0c\u5bf9\u4eba\u7c7b\u79fb\u52a8\u6027\u5efa\u6a21\u4e0d\u8db3\uff0c\u5b66\u4e60\u5230\u7684\u77e5\u8bc6\u5728\u4e0b\u6e38\u5e94\u7528\u4e2d\u9002\u7528\u6027\u6709\u9650", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u5c06\u4eba\u548c\u533a\u57df\u89c6\u4e3a\u52a8\u6001\u56fe\u4e2d\u7684\u8282\u70b9\uff0c\u4f7f\u7528\u8fde\u7eed\u65f6\u95f4\u7f16\u7801\u5668\u8ba1\u7b97\u6f14\u5316\u8282\u70b9\u8868\u793a\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5229\u7528\u8fd9\u4e9b\u8868\u793a\u652f\u6301\u5404\u79cd\u4efb\u52a1", "result": "\u79bb\u7ebf\u5b9e\u9a8c\u8bc1\u660eGDHME\u80fd\u4ece\u6d77\u91cf\u6570\u636e\u4e2d\u81ea\u52a8\u5b66\u4e60\u6709\u4ef7\u503c\u7684\u8282\u70b9\u7279\u5f81\uff0c\u5e76\u5df2\u90e8\u7f72\u5230\u4e5d\u5929\u5ddd\u6d41\u5927\u6a21\u578b\u7cfb\u7edf\u4e2d", "conclusion": "GDHME\u6846\u67b6\u80fd\u6709\u6548\u5b66\u4e60\u901a\u7528\u7684\u4eba\u7c7b\u79fb\u52a8\u6027\u8868\u793a\uff0c\u652f\u6301\u591a\u79cd\u57ce\u5e02\u611f\u77e5\u5e94\u7528"}}
{"id": "2510.24344", "categories": ["econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2510.24344", "abs": "https://arxiv.org/abs/2510.24344", "authors": ["Mostafa Raeisi Sarkandiz"], "title": "Are They Willing to Participate? A Review on Behavioral Economics Approach to Voters Turnout", "comment": "12 pages, 5 tables", "summary": "This article investigates the fundamental factors influencing the rate and\nmanner of Electoral participation with an economic model-based approach. In\nthis study, the structural parameters affecting people's decision making are\ndivided into two categories. The first category includes general topics such as\neconomic and livelihood status, cultural factors and, also, psychological\nvariables. In this section, given that voters are analyzed within the context\nof consumer behavior theory, inflation and unemployment are considered as the\nmost important economic factors. The second group of factors focuses more on\nthe type of voting, with emphasis on government performance. Since the\nincumbent government and its supportive voters are in a game with two Nash\nequilibrium, and also because the voters in most cases are retrospect, the\ngovernment seeks to keep its position by a deliberate change in economic\nfactors, especially inflation and unemployment rates. Finally, to better\nunderstand the issue, a hypothetical example is presented and analyzed in a\ndeveloping country in the form of a state-owned populist employment plan.", "AI": {"tldr": "\u672c\u6587\u4f7f\u7528\u7ecf\u6d4e\u6a21\u578b\u7814\u7a76\u5f71\u54cd\u9009\u4e3e\u53c2\u4e0e\u7387\u548c\u65b9\u5f0f\u7684\u57fa\u672c\u56e0\u7d20\uff0c\u5c06\u5f71\u54cd\u4eba\u4eec\u51b3\u7b56\u7684\u7ed3\u6784\u53c2\u6570\u5206\u4e3a\u7ecf\u6d4e\u6587\u5316\u5fc3\u7406\u56e0\u7d20\u548c\u653f\u5e9c\u7ee9\u6548\u56e0\u7d20\u4e24\u7c7b\uff0c\u5e76\u901a\u8fc7\u53d1\u5c55\u4e2d\u56fd\u5bb6\u7684\u56fd\u6709\u6c11\u7cb9\u4e3b\u4e49\u5c31\u4e1a\u8ba1\u5212\u6848\u4f8b\u8fdb\u884c\u5206\u6790\u3002", "motivation": "\u7814\u7a76\u9009\u4e3e\u53c2\u4e0e\u7684\u6839\u672c\u5f71\u54cd\u56e0\u7d20\uff0c\u7279\u522b\u662f\u7ecf\u6d4e\u56e0\u7d20\uff08\u5982\u901a\u80c0\u548c\u5931\u4e1a\uff09\u5982\u4f55\u5f71\u54cd\u9009\u6c11\u51b3\u7b56\uff0c\u4ee5\u53ca\u653f\u5e9c\u5982\u4f55\u901a\u8fc7\u64cd\u7eb5\u7ecf\u6d4e\u53d8\u91cf\u6765\u7ef4\u6301\u6267\u653f\u5730\u4f4d\u3002", "method": "\u91c7\u7528\u7ecf\u6d4e\u6a21\u578b\u65b9\u6cd5\uff0c\u5c06\u5f71\u54cd\u9009\u6c11\u51b3\u7b56\u7684\u7ed3\u6784\u53c2\u6570\u5206\u4e3a\u4e24\u7c7b\uff1a\u4e00\u662f\u7ecf\u6d4e\u3001\u6c11\u751f\u3001\u6587\u5316\u548c\u5fc3\u7406\u56e0\u7d20\uff1b\u4e8c\u662f\u6295\u7968\u7c7b\u578b\u548c\u653f\u5e9c\u7ee9\u6548\u56e0\u7d20\u3002\u4f7f\u7528\u535a\u5f08\u8bba\u5206\u6790\u653f\u5e9c\u4e0e\u9009\u6c11\u7684\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u5047\u8bbe\u6848\u4f8b\u7814\u7a76\u53d1\u5c55\u4e2d\u56fd\u5bb6\u7684\u56fd\u6709\u6c11\u7cb9\u4e3b\u4e49\u5c31\u4e1a\u8ba1\u5212\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u901a\u80c0\u548c\u5931\u4e1a\u662f\u6700\u91cd\u8981\u7684\u7ecf\u6d4e\u56e0\u7d20\uff0c\u653f\u5e9c\u4e0e\u652f\u6301\u9009\u6c11\u5904\u4e8e\u7eb3\u4ec0\u5747\u8861\u535a\u5f08\u4e2d\uff0c\u9009\u6c11\u901a\u5e38\u662f\u56de\u987e\u6027\u7684\uff0c\u56e0\u6b64\u653f\u5e9c\u4f1a\u901a\u8fc7\u523b\u610f\u6539\u53d8\u7ecf\u6d4e\u56e0\u7d20\uff08\u7279\u522b\u662f\u901a\u80c0\u548c\u5931\u4e1a\u7387\uff09\u6765\u7ef4\u6301\u5730\u4f4d\u3002", "conclusion": "\u9009\u4e3e\u53c2\u4e0e\u53d7\u5230\u7ecf\u6d4e\u56e0\u7d20\u7684\u663e\u8457\u5f71\u54cd\uff0c\u653f\u5e9c\u4f1a\u7b56\u7565\u6027\u5730\u64cd\u7eb5\u7ecf\u6d4e\u53d8\u91cf\u6765\u5f71\u54cd\u9009\u6c11\u884c\u4e3a\uff0c\u8fd9\u5728\u53d1\u5c55\u4e2d\u56fd\u5bb6\u7684\u6c11\u7cb9\u4e3b\u4e49\u653f\u7b56\u4e2d\u8868\u73b0\u5c24\u4e3a\u660e\u663e\u3002"}}
{"id": "2510.23772", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23772", "abs": "https://arxiv.org/abs/2510.23772", "authors": ["Vivek Veeriah", "Federico Barbero", "Marcus Chiam", "Xidong Feng", "Michael Dennis", "Ryan Pachauri", "Thomas Tumiel", "Johan Obando-Ceron", "Jiaxin Shi", "Shaobo Hou", "Satinder Singh", "Nenad Toma\u0161ev", "Tom Zahavy"], "title": "Evaluating In Silico Creativity: An Expert Review of AI Chess Compositions", "comment": "Accepted at the Creative AI Track, NeurIPS 2025", "summary": "The rapid advancement of Generative AI has raised significant questions\nregarding its ability to produce creative and novel outputs. Our recent work\ninvestigates this question within the domain of chess puzzles and presents an\nAI system designed to generate puzzles characterized by aesthetic appeal,\nnovelty, counter-intuitive and unique solutions. We briefly discuss our method\nbelow and refer the reader to the technical paper for more details. To assess\nour system's creativity, we presented a curated booklet of AI-generated puzzles\nto three world-renowned experts: International Master for chess compositions\nAmatzia Avni, Grandmaster Jonathan Levitt, and Grandmaster Matthew Sadler. All\nthree are noted authors on chess aesthetics and the evolving role of computers\nin the game. They were asked to select their favorites and explain what made\nthem appealing, considering qualities such as their creativity, level of\nchallenge, or aesthetic design.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2aAI\u7cfb\u7edf\u6765\u751f\u6210\u5177\u6709\u7f8e\u5b66\u5438\u5f15\u529b\u3001\u65b0\u9896\u6027\u3001\u53cd\u76f4\u89c9\u548c\u72ec\u7279\u89e3\u51b3\u65b9\u6848\u7684\u56fd\u9645\u8c61\u68cb\u8c1c\u9898\uff0c\u5e76\u9080\u8bf7\u4e09\u4f4d\u4e16\u754c\u7ea7\u4e13\u5bb6\u8bc4\u4f30\u5176\u521b\u9020\u529b\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u9700\u8981\u63a2\u7a76\u5176\u662f\u5426\u80fd\u591f\u4ea7\u751f\u521b\u9020\u6027\u548c\u65b0\u9896\u7684\u8f93\u51fa\uff0c\u7279\u522b\u662f\u5728\u56fd\u9645\u8c61\u68cb\u8c1c\u9898\u9886\u57df\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2aAI\u7cfb\u7edf\u6765\u751f\u6210\u5177\u6709\u7f8e\u5b66\u5438\u5f15\u529b\u3001\u65b0\u9896\u6027\u3001\u53cd\u76f4\u89c9\u548c\u72ec\u7279\u89e3\u51b3\u65b9\u6848\u7684\u56fd\u9645\u8c61\u68cb\u8c1c\u9898\uff0c\u5e76\u9080\u8bf7\u4e09\u4f4d\u56fd\u9645\u8c61\u68cb\u4e13\u5bb6\uff08\u56fd\u9645\u5927\u5e08Amatzia Avni\u3001\u7279\u7ea7\u5927\u5e08Jonathan Levitt\u548cMatthew Sadler\uff09\u8bc4\u4f30\u8fd9\u4e9b\u8c1c\u9898\u7684\u521b\u9020\u529b\u3001\u6311\u6218\u6027\u548c\u7f8e\u5b66\u8bbe\u8ba1\u3002", "result": "\u4e09\u4f4d\u4e16\u754c\u7ea7\u4e13\u5bb6\u5bf9AI\u751f\u6210\u7684\u8c1c\u9898\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u9009\u62e9\u4e86\u4ed6\u4eec\u6700\u559c\u6b22\u7684\u8c1c\u9898\uff0c\u5e76\u89e3\u91ca\u4e86\u8fd9\u4e9b\u8c1c\u9898\u7684\u5438\u5f15\u529b\u6240\u5728\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86AI\u7cfb\u7edf\u5728\u56fd\u9645\u8c61\u68cb\u8c1c\u9898\u751f\u6210\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u4ea7\u751f\u5177\u6709\u521b\u9020\u6027\u548c\u7f8e\u5b66\u4ef7\u503c\u7684\u8f93\u51fa\u3002"}}
{"id": "2510.24153", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.24153", "abs": "https://arxiv.org/abs/2510.24153", "authors": ["Yuya Takada", "Kiyoshi Izumi"], "title": "Machine Learning for the Production of Official Statistics: Density Ratio Estimation using Biased Transaction Data for Japanese labor statistics", "comment": "23 pages, 9 figures. Under review at Journal of Computational Social\n  Science", "summary": "National statistical institutes are beginning to use non-traditional data\nsources to produce official statistics. These sources, originally collected for\nnon-statistical purposes, include point-of-sales(POS) data and mobile phone\nglobal positioning system(GPS) data. Such data have the potential to\nsignificantly enhance the usefulness of official statistics. In the era of big\ndata, many private companies are accumulating vast amounts of transaction data.\nExploring how to leverage these data for official statistics is increasingly\nimportant. However, progress has been slower than expected, mainly because such\ndata are not collected through sample-based survey methods and therefore\nexhibit substantial selection bias. If this bias can be properly addressed,\nthese data could become a valuable resource for official statistics,\nsubstantially expanding their scope and improving the quality of\ndecision-making, including economic policy. This paper demonstrates that even\nbiased transaction data can be useful for producing official statistics for\nprompt release, by drawing on the concepts of density ratio estimation and\nsupervised learning under covariate shift, both developed in the field of\nmachine learning. As a case study, we show that preliminary statistics can be\nproduced in a timely manner using biased data from a Japanese private\nemployment agency. This approach enables the early release of a key labor\nmarket indicator that would otherwise be delayed by up to a year, thereby\nmaking it unavailable for timely decision-making.", "AI": {"tldr": "\u672c\u6587\u5c55\u793a\u4e86\u5373\u4f7f\u5b58\u5728\u9009\u62e9\u504f\u5dee\u7684\u4ea4\u6613\u6570\u636e\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u548c\u534f\u53d8\u91cf\u504f\u79fb\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e5f\u80fd\u7528\u4e8e\u53ca\u65f6\u53d1\u5e03\u5b98\u65b9\u7edf\u8ba1\u6570\u636e\u3002", "motivation": "\u56fd\u5bb6\u7edf\u8ba1\u673a\u6784\u5f00\u59cb\u4f7f\u7528\u975e\u4f20\u7edf\u6570\u636e\u6e90\uff08\u5982POS\u6570\u636e\u548c\u624b\u673aGPS\u6570\u636e\uff09\u6765\u5236\u4f5c\u5b98\u65b9\u7edf\u8ba1\u6570\u636e\u3002\u8fd9\u4e9b\u6570\u636e\u5177\u6709\u663e\u8457\u63d0\u5347\u7edf\u8ba1\u6709\u7528\u6027\u7684\u6f5c\u529b\uff0c\u4f46\u7531\u4e8e\u975e\u62bd\u6837\u8c03\u67e5\u6536\u96c6\u65b9\u5f0f\u5b58\u5728\u4e25\u91cd\u9009\u62e9\u504f\u5dee\uff0c\u8fdb\u5c55\u7f13\u6162\u3002", "method": "\u91c7\u7528\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u548c\u534f\u53d8\u91cf\u504f\u79fb\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u5904\u7406\u4ea4\u6613\u6570\u636e\u4e2d\u7684\u9009\u62e9\u504f\u5dee\u95ee\u9898\u3002\u4ee5\u65e5\u672c\u79c1\u8425\u5c31\u4e1a\u673a\u6784\u7684\u6709\u504f\u6570\u636e\u4e3a\u6848\u4f8b\u7814\u7a76\u3002", "result": "\u80fd\u591f\u53ca\u65f6\u751f\u6210\u521d\u6b65\u7edf\u8ba1\u6570\u636e\uff0c\u4f7f\u5f97\u5173\u952e\u52b3\u52a8\u529b\u5e02\u573a\u6307\u6807\u53ef\u4ee5\u63d0\u524d\u53d1\u5e03\uff0c\u5426\u5219\u53ef\u80fd\u4f1a\u5ef6\u8fdf\u957f\u8fbe\u4e00\u5e74\uff0c\u4ece\u800c\u65e0\u6cd5\u7528\u4e8e\u53ca\u65f6\u51b3\u7b56\u3002", "conclusion": "\u5373\u4f7f\u5b58\u5728\u504f\u5dee\u7684\u4ea4\u6613\u6570\u636e\uff0c\u901a\u8fc7\u9002\u5f53\u7684\u65b9\u6cd5\u5904\u7406\u9009\u62e9\u504f\u5dee\uff0c\u53ef\u4ee5\u6210\u4e3a\u5b98\u65b9\u7edf\u8ba1\u7684\u5b9d\u8d35\u8d44\u6e90\uff0c\u663e\u8457\u6269\u5c55\u7edf\u8ba1\u8303\u56f4\u5e76\u6539\u5584\u51b3\u7b56\u8d28\u91cf\u3002"}}
{"id": "2510.23873", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.23873", "abs": "https://arxiv.org/abs/2510.23873", "authors": ["Zhentong Shao", "Jingtao Qin", "Xianbang Chen", "Nanpeng Yu"], "title": "A Spatio-Temporal Graph Learning Approach to Real-Time Economic Dispatch with Multi-Transmission-Node DER Aggregation", "comment": null, "summary": "The integration of distributed energy resources (DERs) into wholesale\nelectricity markets, as mandated by FERC Order 2222, imposes new challenges on\nsystem operations. To remain consistent with existing market structures,\nregional transmission organizations (RTOs) have advanced the aggregation of\ntransmission-node-level DERs (T-DERs), where a nodal virtual power plant (VPP)\nrepresents the mapping of all distribution-level DERs to their respective\ntransmission nodes. This paper develops a real-time economic dispatch (RTED)\nframework that enables multi-transmission-node DER aggregation while addressing\ncomputational efficiency. To this end, we introduce a spatio-temporal graph\nconvolutional network (ST-GCN) for adaptive prediction of distribution factors\n(DFs), thereby capturing the dynamic influence of individual T-DERs across the\ntransmission system. Furthermore, an iterative constraint identification\nstrategy is incorporated to alleviate transmission security constraints without\ncompromising system reliability. Together, these innovations accelerate the\nmarket clearing process and support the effective participation of T-DER\naggregators under current market paradigms. The proposed approach is validated\non large-scale test systems, including modified 118-, 2383-, and 3012-bus\nnetworks under a rolling RTED setting with real demand data. Numerical results\ndemonstrate significant improvements in reducing operational costs and\nmaintaining transmission network feasibility, underscoring the scalability and\npracticality of the proposed framework.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u7ecf\u6d4e\u8c03\u5ea6\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u7a7a\u56fe\u5377\u79ef\u7f51\u7edc\u9884\u6d4b\u5206\u5e03\u56e0\u5b50\uff0c\u7ed3\u5408\u8fed\u4ee3\u7ea6\u675f\u8bc6\u522b\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u591a\u4f20\u8f93\u8282\u70b9\u5206\u5e03\u5f0f\u80fd\u6e90\u8d44\u6e90\u7684\u6709\u6548\u805a\u5408\uff0c\u63d0\u9ad8\u4e86\u5e02\u573a\u6e05\u7b97\u6548\u7387\u3002", "motivation": "FERC Order 2222\u8981\u6c42\u5c06\u5206\u5e03\u5f0f\u80fd\u6e90\u8d44\u6e90\u6574\u5408\u5230\u6279\u53d1\u7535\u529b\u5e02\u573a\u4e2d\uff0c\u8fd9\u7ed9\u7cfb\u7edf\u8fd0\u884c\u5e26\u6765\u4e86\u65b0\u7684\u6311\u6218\u3002\u4e3a\u4e86\u4e0e\u73b0\u6709\u5e02\u573a\u7ed3\u6784\u4fdd\u6301\u4e00\u81f4\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u9ad8\u6548\u5904\u7406\u591a\u4f20\u8f93\u8282\u70b9DER\u805a\u5408\u7684\u5b9e\u65f6\u7ecf\u6d4e\u8c03\u5ea6\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86\u5b9e\u65f6\u7ecf\u6d4e\u8c03\u5ea6\u6846\u67b6\uff0c\u91c7\u7528\u65f6\u7a7a\u56fe\u5377\u79ef\u7f51\u7edc\u81ea\u9002\u5e94\u9884\u6d4b\u5206\u5e03\u56e0\u5b50\uff0c\u6355\u6349\u5355\u4e2a\u4f20\u8f93\u8282\u70b9DER\u5728\u4f20\u8f93\u7cfb\u7edf\u4e2d\u7684\u52a8\u6001\u5f71\u54cd\uff0c\u5e76\u7ed3\u5408\u8fed\u4ee3\u7ea6\u675f\u8bc6\u522b\u7b56\u7565\u6765\u7f13\u89e3\u4f20\u8f93\u5b89\u5168\u7ea6\u675f\u3002", "result": "\u5728\u5927\u578b\u6d4b\u8bd5\u7cfb\u7edf\uff08\u5305\u62ec\u4fee\u6539\u7684118\u30012383\u548c3012\u603b\u7ebf\u7f51\u7edc\uff09\u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u7ed3\u679c\u663e\u793a\u5728\u964d\u4f4e\u8fd0\u8425\u6210\u672c\u548c\u4fdd\u6301\u4f20\u8f93\u7f51\u7edc\u53ef\u884c\u6027\u65b9\u9762\u6709\u663e\u8457\u6539\u8fdb\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u7684\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u52a0\u901f\u4e86\u5e02\u573a\u6e05\u7b97\u8fc7\u7a0b\uff0c\u5728\u5f53\u524d\u5e02\u573a\u8303\u5f0f\u4e0b\u652f\u6301\u4e86\u4f20\u8f93\u8282\u70b9DER\u805a\u5408\u5668\u7684\u6709\u6548\u53c2\u4e0e\uff0c\u4e3a\u5206\u5e03\u5f0f\u80fd\u6e90\u8d44\u6e90\u6574\u5408\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.23954", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23954", "abs": "https://arxiv.org/abs/2510.23954", "authors": ["Pejman Kheradmand", "Behnam Moradkhani", "Raghavasimhan Sankaranarayanan", "Kent K. Yamamoto", "Tanner J. Zachem", "Patrick J. Codd", "Yash Chitalia", "Pierre E. Dupont"], "title": "A Comprehensive General Model of Tendon-Actuated Concentric Tube Robots with Multiple Tubes and Tendons", "comment": null, "summary": "Tendon-actuated concentric tube mechanisms combine the advantages of\ntendon-driven continuum robots and concentric tube robots while addressing\ntheir respective limitations. They overcome the restricted degrees of freedom\noften seen in tendon-driven designs, and mitigate issues such as snapping\ninstability associated with concentric tube robots. However, a complete and\ngeneral mechanical model for these systems remains an open problem. In this\nwork, we propose a Cosserat rod-based framework for modeling the general case\nof $n$ concentric tubes, each actuated by $m_i$ tendons, where $i = \\{1,\n\\ldots, n\\}$. The model allows each tube to twist and elongate while enforcing\na shared centerline for bending. We validate the proposed framework through\nexperiments with two-tube and three tube assemblies under various tendon\nrouting configurations, achieving tip prediction errors $<4\\%$ of the robot's\ntotal length. We further demonstrate the model's generality by applying it to\nexisting robots in the field, where maximum tip deviations remain around $5\\%$\nof the total length. This model provides a foundation for accurate shape\nestimation and control of advanced tendon-actuated concentric tube robots.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8eCosserat\u6746\u7684\u5efa\u6a21\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u62df\u7531n\u4e2a\u540c\u5fc3\u7ba1\u7ec4\u6210\u7684\u808c\u8171\u9a71\u52a8\u540c\u5fc3\u7ba1\u673a\u6784\uff0c\u6bcf\u4e2a\u7ba1\u7531m_i\u4e2a\u808c\u8171\u9a71\u52a8\uff0c\u5b9e\u73b0\u4e86\u5c16\u7aef\u9884\u6d4b\u8bef\u5dee\u5c0f\u4e8e\u603b\u957f\u5ea64%\u7684\u7cbe\u786e\u5f62\u72b6\u4f30\u8ba1\u3002", "motivation": "\u808c\u8171\u9a71\u52a8\u540c\u5fc3\u7ba1\u673a\u6784\u7ed3\u5408\u4e86\u808c\u8171\u9a71\u52a8\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u548c\u540c\u5fc3\u7ba1\u673a\u5668\u4eba\u7684\u4f18\u70b9\uff0c\u4f46\u7f3a\u4e4f\u5b8c\u6574\u901a\u7528\u7684\u529b\u5b66\u6a21\u578b\u3002\u73b0\u6709\u6a21\u578b\u65e0\u6cd5\u5145\u5206\u63cf\u8ff0\u8fd9\u79cd\u590d\u6742\u7cfb\u7edf\u7684\u884c\u4e3a\u3002", "method": "\u4f7f\u7528Cosserat\u6746\u7406\u8bba\u6846\u67b6\u5efa\u6a21n\u4e2a\u540c\u5fc3\u7ba1\uff0c\u6bcf\u4e2a\u7ba1\u7531m_i\u4e2a\u808c\u8171\u9a71\u52a8\u3002\u6a21\u578b\u5141\u8bb8\u6bcf\u4e2a\u7ba1\u626d\u8f6c\u548c\u4f38\u957f\uff0c\u540c\u65f6\u5f3a\u5236\u5f2f\u66f2\u65f6\u5171\u4eab\u4e2d\u5fc3\u7ebf\u3002", "result": "\u901a\u8fc7\u4e24\u7ba1\u548c\u4e09\u7ba1\u7ec4\u4ef6\u7684\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5728\u5404\u79cd\u808c\u8171\u5e03\u7ebf\u914d\u7f6e\u4e0b\u5b9e\u73b0\u5c16\u7aef\u9884\u6d4b\u8bef\u5dee\u5c0f\u4e8e\u673a\u5668\u4eba\u603b\u957f\u5ea6\u76844%\u3002\u5e94\u7528\u4e8e\u73b0\u6709\u673a\u5668\u4eba\u65f6\uff0c\u6700\u5927\u5c16\u7aef\u504f\u5dee\u4fdd\u6301\u5728\u603b\u957f\u5ea6\u7ea65%\u5de6\u53f3\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u5148\u8fdb\u808c\u8171\u9a71\u52a8\u540c\u5fc3\u7ba1\u673a\u5668\u4eba\u7684\u7cbe\u786e\u5f62\u72b6\u4f30\u8ba1\u548c\u63a7\u5236\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u89e3\u51b3\u4e86\u8be5\u9886\u57df\u957f\u671f\u5b58\u5728\u7684\u901a\u7528\u5efa\u6a21\u95ee\u9898\u3002"}}
{"id": "2510.23692", "categories": ["cs.SI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.23692", "abs": "https://arxiv.org/abs/2510.23692", "authors": ["Calla Beauregard", "Parisa Suchdev", "Ashley M. A. Fehr", "Isabelle T. Smith", "Tabia Tanzin Prama", "Julia Witte Zimmerman", "Carter Ward", "Juniper Lovato", "Christopher M. Danforth", "Peter Sheridan Dodd"], "title": "Detecting sub-populations in online health communities: A mixed-methods exploration of breastfeeding messages in BabyCenter Birth Clubs", "comment": null, "summary": "Parental stress is a nationwide health crisis according to the U.S. Surgeon\nGeneral's 2024 advisory. To allay stress, expecting parents seek advice and\nshare experiences in a variety of venues, from in-person birth education\nclasses and parenting groups to virtual communities, for example, BabyCenter, a\nmoderated online forum community with over 4 million members in the United\nStates alone. In this study, we aim to understand how parents talk about\npregnancy, birth, and parenting by analyzing 5.43M posts and comments from the\nApril 2017--January 2024 cohort of 331,843 BabyCenter \"birth club\" users (that\nis, users who participate in due date forums or \"birth clubs\" based on their\nbabies' due dates). Using BERTopic to locate breastfeeding threads and LDA to\nsummarize themes, we compare documents in breastfeeding threads to all other\nbirth-club content. Analyzing time series of word rank, we find that posts and\ncomments containing anxiety-related terms increased steadily from April 2017 to\nJanuary 2024. We used an ensemble of topic models to identify dominant\nbreastfeeding topics within birth clubs, and then explored trends among all\nuser content versus those who posted in threads related to breastfeeding\ntopics. We conducted Latent Dirichlet Allocation (LDA) topic modeling to\nidentify the most common topics in the full population, as well as within the\nsubset breastfeeding population. We find that the topic of sleep dominates in\ncontent generated by the breastfeeding population, as well anxiety-related and\nwork/daycare topics that are not predominant in the full BabyCenter birth club\ndataset.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5206\u6790BabyCenter\u5e73\u53f0\u4e0a331,843\u540d\u7528\u6237\u7684543\u4e07\u6761\u5e16\u5b50\uff0c\u53d1\u73b0\u7236\u6bcd\u5728\u6000\u5b55\u3001\u5206\u5a29\u548c\u80b2\u513f\u8ba8\u8bba\u4e2d\uff0c\u7126\u8651\u76f8\u5173\u8bcd\u6c47\u7684\u4f7f\u7528\u4ece2017\u5e74\u52302024\u5e74\u6301\u7eed\u589e\u52a0\u3002\u6bcd\u4e73\u5582\u517b\u8bdd\u9898\u4e2d\uff0c\u7761\u7720\u3001\u7126\u8651\u548c\u5de5\u4f5c/\u65e5\u6258\u662f\u4e3b\u8981\u4e3b\u9898\u3002", "motivation": "\u7f8e\u56fd\u536b\u751f\u5c40\u5c40\u957f2024\u5e74\u5c06\u7236\u6bcd\u538b\u529b\u5217\u4e3a\u5168\u56fd\u5065\u5eb7\u5371\u673a\uff0c\u671f\u671b\u7236\u6bcd\u5728\u5404\u79cd\u5e73\u53f0\u4e0a\u5bfb\u6c42\u5efa\u8bae\u548c\u5206\u4eab\u7ecf\u9a8c\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5206\u6790\u5927\u578b\u5728\u7ebf\u793e\u533aBabyCenter\u7684\u6570\u636e\uff0c\u4e86\u89e3\u7236\u6bcd\u5982\u4f55\u8ba8\u8bba\u6000\u5b55\u3001\u5206\u5a29\u548c\u80b2\u513f\u8bdd\u9898\u3002", "method": "\u4f7f\u7528BERTopic\u5b9a\u4f4d\u6bcd\u4e73\u5582\u517b\u4e3b\u9898\u5e16\u5b50\uff0cLDA\u4e3b\u9898\u5efa\u6a21\u5206\u6790\u4e3b\u9898\uff0c\u6bd4\u8f83\u6bcd\u4e73\u5582\u517b\u4e3b\u9898\u4e0e\u5176\u4ed6\u51fa\u751f\u4ff1\u4e50\u90e8\u5185\u5bb9\u7684\u5dee\u5f02\uff0c\u5206\u6790\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u8bcd\u6c47\u6392\u540d\u53d8\u5316\u3002", "result": "\u53d1\u73b0\u5305\u542b\u7126\u8651\u76f8\u5173\u8bcd\u6c47\u7684\u5e16\u5b50\u4ece2017\u5e744\u6708\u52302024\u5e741\u6708\u7a33\u6b65\u589e\u52a0\uff1b\u6bcd\u4e73\u5582\u517b\u4eba\u7fa4\u4e2d\uff0c\u7761\u7720\u4e3b\u9898\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u7126\u8651\u548c\u5de5\u4f5c/\u65e5\u6258\u4e3b\u9898\u5728\u6bcd\u4e73\u5582\u517b\u4eba\u7fa4\u4e2d\u66f4\u7a81\u51fa\uff0c\u4f46\u5728\u6574\u4e2a\u51fa\u751f\u4ff1\u4e50\u90e8\u6570\u636e\u96c6\u4e2d\u4e0d\u5360\u4e3b\u5bfc\u3002", "conclusion": "\u7236\u6bcd\u5728\u5728\u7ebf\u793e\u533a\u4e2d\u7684\u8ba8\u8bba\u53cd\u6620\u4e86\u65e5\u76ca\u589e\u957f\u7684\u7126\u8651\u60c5\u7eea\uff0c\u6bcd\u4e73\u5582\u517b\u7236\u6bcd\u7279\u522b\u5173\u6ce8\u7761\u7720\u3001\u7126\u8651\u548c\u5de5\u4f5c/\u65e5\u6258\u95ee\u9898\uff0c\u8fd9\u4e9b\u53d1\u73b0\u5bf9\u652f\u6301\u7236\u6bcd\u5fc3\u7406\u5065\u5eb7\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2510.24362", "categories": ["econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2510.24362", "abs": "https://arxiv.org/abs/2510.24362", "authors": ["Gabriel Montes-Rojas", "Fernando Toledo", "Nicol\u00e1s Bertholet", "Kevin Corfield"], "title": "Implicit quantile preferences of the Fed and the Taylor rule", "comment": null, "summary": "We study optimal monetary policy when a central bank maximizes a quantile\nutility objective rather than expected utility. In our framework, the central\nbank's risk attitude is indexed by the quantile index level, providing a\ntransparent mapping between hawkish/dovish stances and attention to adverse\nmacroeconomic realizations. We formulate the infinite-horizon problem using a\nBellman equation with the quantile operator. Implementing an Euler-equation\napproach, we derive Taylor-rule-type reaction functions. Using an indirect\ninference approach, we derive a central bank risk aversion implicit quantile\nindex. An empirical implementation for the US is outlined based on reduced-form\nlaws of motion with conditional heteroskedasticity, enabling estimation of the\nnew monetary policy rule and its dependence on the Fed risk attitudes. The\nresults reveal that the Fed has mostly a dovish-type behavior but with some\nperiods of hawkish attitudes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e2d\u592e\u94f6\u884c\u91c7\u7528\u5206\u4f4d\u6570\u6548\u7528\u76ee\u6807\u800c\u975e\u671f\u671b\u6548\u7528\u65f6\u7684\u6700\u4f18\u8d27\u5e01\u653f\u7b56\uff0c\u901a\u8fc7\u5206\u4f4d\u6570\u6307\u6570\u6620\u5c04\u9e70\u6d3e/\u9e3d\u6d3e\u7acb\u573a\u4e0e\u5bf9\u4e0d\u5229\u5b8f\u89c2\u7ecf\u6d4e\u7ed3\u679c\u7684\u5173\u6ce8\u5ea6\u3002", "motivation": "\u4f20\u7edf\u8d27\u5e01\u653f\u7b56\u57fa\u4e8e\u671f\u671b\u6548\u7528\uff0c\u4f46\u5b9e\u9645\u51b3\u7b56\u8005\u53ef\u80fd\u66f4\u5173\u6ce8\u7279\u5b9a\u5206\u4f4d\u6570\u7ed3\u679c\u3002\u7814\u7a76\u65e8\u5728\u5efa\u7acb\u66f4\u900f\u660e\u7684\u98ce\u9669\u6001\u5ea6\u4e0e\u653f\u7b56\u7acb\u573a\u6620\u5c04\u5173\u7cfb\u3002", "method": "\u4f7f\u7528\u5e26\u5206\u4f4d\u6570\u7b97\u5b50\u7684\u8d1d\u5c14\u66fc\u65b9\u7a0b\u6784\u5efa\u65e0\u9650\u671f\u95ee\u9898\uff0c\u91c7\u7528\u6b27\u62c9\u65b9\u7a0b\u65b9\u6cd5\u63a8\u5bfc\u6cf0\u52d2\u89c4\u5219\u578b\u53cd\u5e94\u51fd\u6570\uff0c\u901a\u8fc7\u95f4\u63a5\u63a8\u65ad\u65b9\u6cd5\u63a8\u5bfc\u4e2d\u592e\u94f6\u884c\u98ce\u9669\u538c\u6076\u9690\u542b\u5206\u4f4d\u6570\u6307\u6570\u3002", "result": "\u57fa\u4e8e\u7f8e\u56fd\u6570\u636e\u7684\u5b9e\u8bc1\u5b9e\u65bd\u663e\u793a\uff0c\u7f8e\u8054\u50a8\u4e3b\u8981\u8868\u73b0\u51fa\u9e3d\u6d3e\u884c\u4e3a\uff0c\u4f46\u67d0\u4e9b\u65f6\u671f\u5177\u6709\u9e70\u6d3e\u6001\u5ea6\u3002", "conclusion": "\u5206\u4f4d\u6570\u6548\u7528\u6846\u67b6\u4e3a\u7406\u89e3\u4e2d\u592e\u94f6\u884c\u98ce\u9669\u6001\u5ea6\u548c\u653f\u7b56\u7acb\u573a\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\u7f8e\u8054\u50a8\u653f\u7b56\u7acb\u573a\u5b58\u5728\u65f6\u95f4\u53d8\u5316\u3002"}}
{"id": "2510.23807", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23807", "abs": "https://arxiv.org/abs/2510.23807", "authors": ["Hamid R. Tizhoosh"], "title": "Why Foundation Models in Pathology Are Failing", "comment": null, "summary": "In non-medical domains, foundation models (FMs) have revolutionized computer\nvision and language processing through large-scale self-supervised and\nmultimodal learning. Consequently, their rapid adoption in computational\npathology was expected to deliver comparable breakthroughs in cancer diagnosis,\nprognostication, and multimodal retrieval. However, recent systematic\nevaluations reveal fundamental weaknesses: low diagnostic accuracy, poor\nrobustness, geometric instability, heavy computational demands, and concerning\nsafety vulnerabilities. This short paper examines these shortcomings and argues\nthat they stem from deeper conceptual mismatches between the assumptions\nunderlying generic foundation modeling in mainstream AI and the intrinsic\ncomplexity of human tissue. Seven interrelated causes are identified:\nbiological complexity, ineffective self-supervision, overgeneralization,\nexcessive architectural complexity, lack of domain-specific innovation,\ninsufficient data, and a fundamental design flaw related to tissue patch size.\nThese findings suggest that current pathology foundation models remain\nconceptually misaligned with the nature of tissue morphology and call for a\nfundamental rethinking of the paradigm itself.", "AI": {"tldr": "\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u5b58\u5728\u6839\u672c\u6027\u7f3a\u9677\uff0c\u5305\u62ec\u8bca\u65ad\u51c6\u786e\u7387\u4f4e\u3001\u9c81\u68d2\u6027\u5dee\u3001\u51e0\u4f55\u4e0d\u7a33\u5b9a\u3001\u8ba1\u7b97\u9700\u6c42\u5927\u548c\u5b89\u5168\u6f0f\u6d1e\u7b49\u95ee\u9898\uff0c\u6e90\u4e8e\u901a\u7528AI\u57fa\u7840\u6a21\u578b\u5047\u8bbe\u4e0e\u4eba\u4f53\u7ec4\u7ec7\u5185\u5728\u590d\u6742\u6027\u4e4b\u95f4\u7684\u6982\u5ff5\u4e0d\u5339\u914d\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u7840\u6a21\u578b\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u8bed\u8a00\u5904\u7406\u9886\u57df\u53d6\u5f97\u4e86\u9769\u547d\u6027\u7a81\u7834\uff0c\u4f46\u5728\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u7684\u5feb\u901f\u5e94\u7528\u5e76\u672a\u5e26\u6765\u9884\u671f\u7684\u764c\u75c7\u8bca\u65ad\u3001\u9884\u540e\u548c\u591a\u6a21\u6001\u68c0\u7d22\u65b9\u9762\u7684\u7a81\u7834\uff0c\u53cd\u800c\u66b4\u9732\u4e86\u7cfb\u7edf\u6027\u5f31\u70b9\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u8bc4\u4f30\u5206\u6790\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u7684\u7f3a\u9677\uff0c\u8bc6\u522b\u4e86\u4e03\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u6839\u672c\u539f\u56e0\uff1a\u751f\u7269\u590d\u6742\u6027\u3001\u65e0\u6548\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u3001\u8fc7\u5ea6\u6cdb\u5316\u3001\u8fc7\u5ea6\u67b6\u6784\u590d\u6742\u6027\u3001\u7f3a\u4e4f\u9886\u57df\u7279\u5b9a\u521b\u65b0\u3001\u6570\u636e\u4e0d\u8db3\u4ee5\u53ca\u4e0e\u7ec4\u7ec7\u5207\u7247\u5927\u5c0f\u76f8\u5173\u7684\u57fa\u672c\u8bbe\u8ba1\u7f3a\u9677\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u5728\u6982\u5ff5\u4e0a\u4e0e\u7ec4\u7ec7\u5f62\u6001\u5b66\u672c\u8d28\u5b58\u5728\u4e25\u91cd\u4e0d\u5339\u914d\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u548c\u7cfb\u7edf\u6027\u5f31\u70b9\u3002", "conclusion": "\u5f53\u524d\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u5b58\u5728\u6839\u672c\u6027\u6982\u5ff5\u504f\u5dee\uff0c\u9700\u8981\u5bf9\u6574\u4e2a\u8303\u5f0f\u8fdb\u884c\u6839\u672c\u6027\u91cd\u65b0\u601d\u8003\uff0c\u4ee5\u66f4\u597d\u5730\u9002\u5e94\u4eba\u4f53\u7ec4\u7ec7\u7684\u56fa\u6709\u590d\u6742\u6027\u3002"}}
{"id": "2510.24394", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.24394", "abs": "https://arxiv.org/abs/2510.24394", "authors": ["Sandra Barrag\u00e1n", "Adri\u00e1n P\u00e9rez-Bote", "Carlos S\u00e1ez", "David Salgado", "Luis Sanguiao-Sande"], "title": "Streamlining business functions in official statistical production with Machine Learning", "comment": "42 pages, 14 figures, preprint version to appear as a chapter of F.\n  Dumpert (ed.), Foundations and Advances of Machine Learning in Official\n  Statistics", "summary": "We provide a description of pilot and production experiences to streamline\nsome business functions in the official statistical production process using\nstatistical learning models. Our approach is quality-oriented searching for an\nimprovement on accuracy, cost-efficiency, timeliness, granularity, response\nburden reduction, and frequency. Pilot experiences have been conducted with\ndata from real surveys in Statistics Spain (INE).", "AI": {"tldr": "\u4f7f\u7528\u7edf\u8ba1\u5b66\u4e60\u6a21\u578b\u4f18\u5316\u5b98\u65b9\u7edf\u8ba1\u751f\u4ea7\u6d41\u7a0b\u4e2d\u4e1a\u52a1\u529f\u80fd\u7684\u8bd5\u70b9\u548c\u751f\u4ea7\u7ecf\u9a8c\u603b\u7ed3", "motivation": "\u5bfb\u6c42\u5728\u51c6\u786e\u6027\u3001\u6210\u672c\u6548\u76ca\u3001\u53ca\u65f6\u6027\u3001\u7c92\u5ea6\u3001\u54cd\u5e94\u8d1f\u62c5\u51cf\u5c11\u548c\u9891\u7387\u7b49\u65b9\u9762\u7684\u6539\u8fdb\uff0c\u4ee5\u63d0\u9ad8\u7edf\u8ba1\u751f\u4ea7\u6d41\u7a0b\u7684\u8d28\u91cf", "method": "\u91c7\u7528\u7edf\u8ba1\u5b66\u4e60\u6a21\u578b\uff0c\u5728\u897f\u73ed\u7259\u7edf\u8ba1\u5c40(INE)\u7684\u771f\u5b9e\u8c03\u67e5\u6570\u636e\u4e0a\u8fdb\u884c\u8bd5\u70b9\u5b9e\u9a8c", "result": "\u6210\u529f\u5b9e\u65bd\u4e86\u8bd5\u70b9\u7ecf\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u4f18\u5316\u4e1a\u52a1\u529f\u80fd\u65b9\u9762\u7684\u53ef\u884c\u6027", "conclusion": "\u7edf\u8ba1\u5b66\u4e60\u6a21\u578b\u80fd\u591f\u6709\u6548\u63d0\u5347\u5b98\u65b9\u7edf\u8ba1\u751f\u4ea7\u6d41\u7a0b\u7684\u6548\u7387\u548c\u6548\u679c"}}
{"id": "2510.23877", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.23877", "abs": "https://arxiv.org/abs/2510.23877", "authors": ["Zhentong Shao", "Nanpeng Yu"], "title": "Carbon-Aware Optimal Power Flow with Data-Driven Carbon Emission Tracing", "comment": null, "summary": "Quantifying locational carbon emissions in power grids is crucial for\nimplementing effective carbon reduction strategies for customers relying on\nelectricity. This paper presents a carbon-aware optimal power flow (OPF)\nframework that incorporates data-driven carbon tracing, enabling rapid\nestimation of nodal carbon emissions from electric loads. By developing\ngenerator-to-load carbon emission distribution factors through data-driven\ntechnique, the analytical formulas for both average and marginal carbon\nemissions can be derived and integrated seamlessly into DC OPF models as linear\nconstraints. The proposed carbon-aware OPF model enables market operators to\noptimize energy dispatch while reducing greenhouse gas emissions. Simulations\non IEEE test systems confirm the accuracy and computational efficiency of the\nproposed approach, highlighting its applicability for real-time carbon-aware\nsystem operations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u78b3\u8ffd\u8e2a\u7684\u78b3\u611f\u77e5\u6700\u4f18\u6f6e\u6d41\u6846\u67b6\uff0c\u80fd\u591f\u5feb\u901f\u4f30\u7b97\u7535\u529b\u8d1f\u8377\u7684\u8282\u70b9\u78b3\u6392\u653e\uff0c\u5e76\u5c06\u5176\u6574\u5408\u5230DC\u6700\u4f18\u6f6e\u6d41\u6a21\u578b\u4e2d\u4f5c\u4e3a\u7ebf\u6027\u7ea6\u675f\u3002", "motivation": "\u91cf\u5316\u7535\u7f51\u4e2d\u7684\u4f4d\u7f6e\u78b3\u6392\u653e\u5bf9\u4e8e\u4f9d\u8d56\u7535\u529b\u7684\u5ba2\u6237\u5b9e\u65bd\u6709\u6548\u7684\u78b3\u51cf\u6392\u7b56\u7565\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u6280\u672f\u5f00\u53d1\u53d1\u7535\u673a\u5230\u8d1f\u8377\u7684\u78b3\u6392\u653e\u5206\u5e03\u56e0\u5b50\uff0c\u63a8\u5bfc\u51fa\u5e73\u5747\u548c\u8fb9\u9645\u78b3\u6392\u653e\u7684\u89e3\u6790\u516c\u5f0f\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u7ebf\u6027\u7ea6\u675f\u65e0\u7f1d\u96c6\u6210\u5230DC\u6700\u4f18\u6f6e\u6d41\u6a21\u578b\u4e2d\u3002", "result": "\u5728IEEE\u6d4b\u8bd5\u7cfb\u7edf\u4e0a\u7684\u4eff\u771f\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u7a81\u663e\u4e86\u5176\u5728\u5b9e\u65f6\u78b3\u611f\u77e5\u7cfb\u7edf\u8fd0\u884c\u4e2d\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u78b3\u611f\u77e5\u6700\u4f18\u6f6e\u6d41\u6a21\u578b\u4f7f\u5e02\u573a\u8fd0\u8425\u5546\u80fd\u591f\u5728\u4f18\u5316\u80fd\u6e90\u8c03\u5ea6\u7684\u540c\u65f6\u51cf\u5c11\u6e29\u5ba4\u6c14\u4f53\u6392\u653e\u3002"}}
{"id": "2510.23963", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23963", "abs": "https://arxiv.org/abs/2510.23963", "authors": ["Hiroki Ishikawa", "Kyosuke Ishibashi", "Ko Yamamoto"], "title": "Adaptive-twist Soft Finger Mechanism for Grasping by Wrapping", "comment": null, "summary": "This paper presents a soft robot finger capable of adaptive-twist deformation\nto grasp objects by wrapping them. For a soft hand to grasp and pick-up one\nobject from densely contained multiple objects, a soft finger requires the\nadaptive-twist deformation function in both in-plane and out-of-plane\ndirections. The function allows the finger to be inserted deeply into a limited\ngap among objects. Once inserted, the soft finger requires appropriate control\nof grasping force normal to contact surface, thereby maintaining the twisted\ndeformation. In this paper, we refer to this type of grasping as grasping by\nwrapping. To achieve these two functions by a single actuation source, we\npropose a variable stiffness mechanism that can adaptively change the stiffness\nas the pressure is higher. We conduct a finite element analysis (FEA) on the\nproposed mechanism and determine its design parameter based on the FEA result.\nUsing the developed soft finger, we report basic experimental results and\ndemonstrations on grasping various objects.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u81ea\u9002\u5e94\u626d\u8f6c\u53d8\u5f62\u80fd\u529b\u7684\u8f6f\u4f53\u673a\u5668\u4eba\u624b\u6307\uff0c\u80fd\u591f\u901a\u8fc7\u5305\u88f9\u65b9\u5f0f\u6293\u53d6\u7269\u4f53\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4ece\u5bc6\u96c6\u5806\u53e0\u7684\u7269\u4f53\u4e2d\u6293\u53d6\u5355\u4e2a\u7269\u4f53\u3002", "motivation": "\u8f6f\u4f53\u624b\u9700\u8981\u4ece\u5bc6\u96c6\u5806\u53e0\u7684\u591a\u4e2a\u7269\u4f53\u4e2d\u6293\u53d6\u5355\u4e2a\u7269\u4f53\uff0c\u8fd9\u8981\u6c42\u8f6f\u4f53\u624b\u6307\u5177\u5907\u5728\u5e73\u9762\u5185\u548c\u5e73\u9762\u5916\u65b9\u5411\u7684\u81ea\u9002\u5e94\u626d\u8f6c\u53d8\u5f62\u529f\u80fd\uff0c\u4ee5\u4fbf\u6df1\u5165\u7269\u4f53\u95f4\u7684\u72ed\u7a84\u95f4\u9699\u8fdb\u884c\u6293\u53d6\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u53ef\u53d8\u521a\u5ea6\u673a\u5236\uff0c\u901a\u8fc7\u5355\u4e00\u9a71\u52a8\u6e90\u5b9e\u73b0\u81ea\u9002\u5e94\u626d\u8f6c\u53d8\u5f62\u548c\u6293\u53d6\u529b\u63a7\u5236\uff0c\u5229\u7528\u6709\u9650\u5143\u5206\u6790\u786e\u5b9a\u8bbe\u8ba1\u53c2\u6570\uff0c\u5e76\u5f00\u53d1\u4e86\u8f6f\u4f53\u624b\u6307\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u901a\u8fc7\u5f00\u53d1\u7684\u8f6f\u4f53\u624b\u6307\u8fdb\u884c\u4e86\u57fa\u7840\u5b9e\u9a8c\u548c\u591a\u79cd\u7269\u4f53\u7684\u6293\u53d6\u6f14\u793a\uff0c\u9a8c\u8bc1\u4e86\u5176\u81ea\u9002\u5e94\u626d\u8f6c\u53d8\u5f62\u548c\u5305\u88f9\u6293\u53d6\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u53ef\u53d8\u521a\u5ea6\u8f6f\u4f53\u624b\u6307\u6210\u529f\u5b9e\u73b0\u4e86\u81ea\u9002\u5e94\u626d\u8f6c\u53d8\u5f62\u529f\u80fd\uff0c\u80fd\u591f\u6709\u6548\u6df1\u5165\u72ed\u7a84\u95f4\u9699\u5e76\u901a\u8fc7\u5305\u88f9\u65b9\u5f0f\u6293\u53d6\u7269\u4f53\uff0c\u4e3a\u5bc6\u96c6\u73af\u5883\u4e0b\u7684\u7269\u4f53\u6293\u53d6\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24354", "categories": ["cs.SI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.24354", "abs": "https://arxiv.org/abs/2510.24354", "authors": ["Jacopo D'Ignazi", "Andreas Kaltenbrunner", "Ga\u00ebl Le Mens", "Fabrizio Germano", "Vicen\u00e7 G\u00f3mez"], "title": "Rewarding Engagement and Personalization in Popularity-Based Rankings Amplifies Extremism and Polarization", "comment": null, "summary": "Despite extensive research, the mechanisms through which online platforms\nshape extremism and polarization remain poorly understood. We identify and test\na mechanism, grounded in empirical evidence, that explains how ranking\nalgorithms can amplify both phenomena. This mechanism is based on\nwell-documented assumptions: (i) users exhibit position bias and tend to prefer\nitems displayed higher in the ranking, (ii) users prefer like-minded content,\n(iii) users with more extreme views are more likely to engage actively, and\n(iv) ranking algorithms are popularity-based, assigning higher positions to\nitems that attract more clicks. Under these conditions, when platforms\nadditionally reward \\emph{active} engagement and implement \\emph{personalized}\nrankings, users are inevitably driven toward more extremist and polarized news\nconsumption. We formalize this mechanism in a dynamical model, which we\nevaluate by means of simulations and interactive experiments with hundreds of\nhuman participants, where the rankings are updated dynamically in response to\nuser activity.", "AI": {"tldr": "\u6392\u540d\u7b97\u6cd5\u901a\u8fc7\u4f4d\u7f6e\u504f\u89c1\u3001\u540c\u8d28\u5185\u5bb9\u504f\u597d\u3001\u6781\u7aef\u7528\u6237\u66f4\u6d3b\u8dc3\u4ee5\u53ca\u57fa\u4e8e\u6d41\u884c\u5ea6\u7684\u6392\u540d\u673a\u5236\uff0c\u5728\u5956\u52b1\u4e3b\u52a8\u53c2\u4e0e\u548c\u4e2a\u6027\u5316\u6392\u540d\u65f6\uff0c\u4f1a\u653e\u5927\u6781\u7aef\u4e3b\u4e49\u548c\u6781\u5316\u73b0\u8c61\u3002", "motivation": "\u5c3d\u7ba1\u6709\u5927\u91cf\u7814\u7a76\uff0c\u4f46\u5728\u7ebf\u5e73\u53f0\u5982\u4f55\u5f71\u54cd\u6781\u7aef\u4e3b\u4e49\u548c\u6781\u5316\u7684\u673a\u5236\u4ecd\u4e0d\u6e05\u695a\u3002\u672c\u6587\u65e8\u5728\u8bc6\u522b\u548c\u6d4b\u8bd5\u4e00\u4e2a\u57fa\u4e8e\u7ecf\u9a8c\u8bc1\u636e\u7684\u673a\u5236\uff0c\u89e3\u91ca\u6392\u540d\u7b97\u6cd5\u5982\u4f55\u653e\u5927\u8fd9\u4e9b\u73b0\u8c61\u3002", "method": "\u5efa\u7acb\u52a8\u6001\u6a21\u578b\uff0c\u901a\u8fc7\u6a21\u62df\u548c\u4e0e\u6570\u767e\u540d\u4eba\u7c7b\u53c2\u4e0e\u8005\u7684\u4ea4\u4e92\u5b9e\u9a8c\u6765\u8bc4\u4f30\uff0c\u5176\u4e2d\u6392\u540d\u4f1a\u6839\u636e\u7528\u6237\u6d3b\u52a8\u52a8\u6001\u66f4\u65b0\u3002", "result": "\u5728\u6ee1\u8db3\u56db\u4e2a\u5173\u952e\u5047\u8bbe\uff08\u4f4d\u7f6e\u504f\u89c1\u3001\u540c\u8d28\u5185\u5bb9\u504f\u597d\u3001\u6781\u7aef\u7528\u6237\u66f4\u6d3b\u8dc3\u3001\u57fa\u4e8e\u6d41\u884c\u5ea6\u7684\u6392\u540d\uff09\u7684\u6761\u4ef6\u4e0b\uff0c\u5f53\u5e73\u53f0\u5956\u52b1\u4e3b\u52a8\u53c2\u4e0e\u5e76\u5b9e\u65bd\u4e2a\u6027\u5316\u6392\u540d\u65f6\uff0c\u7528\u6237\u4e0d\u53ef\u907f\u514d\u5730\u4f1a\u8f6c\u5411\u66f4\u6781\u7aef\u548c\u6781\u5316\u7684\u65b0\u95fb\u6d88\u8d39\u3002", "conclusion": "\u6392\u540d\u7b97\u6cd5\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u4f1a\u7cfb\u7edf\u6027\u63a8\u52a8\u7528\u6237\u8d70\u5411\u6781\u7aef\u4e3b\u4e49\u548c\u6781\u5316\uff0c\u8fd9\u63ed\u793a\u4e86\u5e73\u53f0\u8bbe\u8ba1\u5bf9\u7528\u6237\u884c\u4e3a\u7684\u91cd\u8981\u5f71\u54cd\u3002"}}
{"id": "2510.24077", "categories": ["cs.SI", "stat.AP", "62"], "pdf": "https://arxiv.org/pdf/2510.24077", "abs": "https://arxiv.org/abs/2510.24077", "authors": ["Sayantan Mukherjee", "Pritam Ranjan", "Joysankar Bhattacharya"], "title": "Assessing the influence of social media feedback on traveler's future trip-planning behavior: A multi-model machine learning approach", "comment": "38 pages, 10 tables, 6 figures", "summary": "With the surge of domestic tourism in India and the influence of social media\non young tourists, this paper aims to address the research question on how\n\"social return\" - responses received on social media sharing - of recent trip\ndetails can influence decision-making for short-term future travels. The paper\ndevelops a multi-model framework to build a predictive machine learning model\nthat establishes a relationship between a traveler's social return, various\nsocial media usage, trip-related factors, and her future trip-planning\nbehavior. The primary data was collected via a survey from Indian tourists.\nAfter data cleaning, the imbalance in the data was addressed using a robust\noversampling method, and the reliability of the predictive model was ensured by\napplying a Monte Carlo cross-validation technique. The results suggest at least\n75% overall accuracy in predicting the influence of social return on changing\nthe future trip plan. Moreover, the model fit results provide crucial practical\nimplications for the domestic tourism sector in India with future research\ndirections concerning social media, destination marketing, smart tourism,\nheritage tourism, etc.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u6a21\u578b\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u793e\u4ea4\u5a92\u4f53\u53cd\u9988\uff08\u793e\u4ea4\u56de\u62a5\uff09\u5982\u4f55\u5f71\u54cd\u5370\u5ea6\u5e74\u8f7b\u6e38\u5ba2\u7684\u77ed\u671f\u672a\u6765\u65c5\u884c\u51b3\u7b56\uff0c\u51c6\u786e\u7387\u8d85\u8fc775%\u3002", "motivation": "\u968f\u7740\u5370\u5ea6\u56fd\u5185\u65c5\u6e38\u7684\u5174\u8d77\u548c\u793e\u4ea4\u5a92\u4f53\u5bf9\u5e74\u8f7b\u6e38\u5ba2\u7684\u5f71\u54cd\uff0c\u7814\u7a76\u793e\u4ea4\u5a92\u4f53\u5206\u4eab\u83b7\u5f97\u7684\u53cd\u9988\u5982\u4f55\u5f71\u54cd\u77ed\u671f\u672a\u6765\u65c5\u884c\u51b3\u7b56\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u901a\u8fc7\u95ee\u5377\u8c03\u67e5\u6536\u96c6\u5370\u5ea6\u6e38\u5ba2\u6570\u636e\uff0c\u4f7f\u7528\u9c81\u68d2\u8fc7\u91c7\u6837\u65b9\u6cd5\u5904\u7406\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u5e94\u7528\u8499\u7279\u5361\u6d1b\u4ea4\u53c9\u9a8c\u8bc1\u6280\u672f\u786e\u4fdd\u9884\u6d4b\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002", "result": "\u6a21\u578b\u5728\u9884\u6d4b\u793e\u4ea4\u56de\u62a5\u5bf9\u672a\u6765\u65c5\u884c\u8ba1\u5212\u6539\u53d8\u7684\u5f71\u54cd\u65b9\u9762\u8fbe\u5230\u81f3\u5c1175%\u7684\u6574\u4f53\u51c6\u786e\u7387\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5370\u5ea6\u56fd\u5185\u65c5\u6e38\u4e1a\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u5b9e\u8df5\u542f\u793a\uff0c\u5e76\u4e3a\u793e\u4ea4\u5a92\u4f53\u3001\u76ee\u7684\u5730\u8425\u9500\u3001\u667a\u6167\u65c5\u6e38\u3001\u9057\u4ea7\u65c5\u6e38\u7b49\u9886\u57df\u7684\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2510.23822", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23822", "abs": "https://arxiv.org/abs/2510.23822", "authors": ["Zhenyu Zhang", "Tianyi Chen", "Weiran Xu", "Alex Pentland", "Jiaxin Pei"], "title": "ReCAP: Recursive Context-Aware Reasoning and Planning for Large Language Model Agents", "comment": null, "summary": "Long-horizon tasks requiring multi-step reasoning and dynamic re-planning\nremain challenging for large language models (LLMs). Sequential prompting\nmethods are prone to context drift, loss of goal information, and recurrent\nfailure cycles, while hierarchical prompting methods often weaken cross-level\ncontinuity or incur substantial runtime overhead. We introduce ReCAP (Recursive\nContext-Aware Reasoning and Planning), a hierarchical framework with shared\ncontext for reasoning and planning in LLMs. ReCAP combines three key\nmechanisms: (i) plan-ahead decomposition, in which the model generates a full\nsubtask list, executes the first item, and refines the remainder; (ii)\nstructured re-injection of parent plans, maintaining consistent multi-level\ncontext during recursive return; and (iii) memory-efficient execution, bounding\nthe active prompt so costs scale linearly with task depth. Together these\nmechanisms align high-level goals with low-level actions, reduce redundant\nprompting, and preserve coherent context updates across recursion. Experiments\ndemonstrate that ReCAP substantially improves subgoal alignment and success\nrates on various long-horizon reasoning benchmarks, achieving a 32% gain on\nsynchronous Robotouille and a 29% improvement on asynchronous Robotouille under\nthe strict pass@1 protocol.", "AI": {"tldr": "ReCAP\u662f\u4e00\u4e2a\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5206\u5c42\u63a8\u7406\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u8ba1\u5212\u5206\u89e3\u3001\u7236\u8ba1\u5212\u7ed3\u6784\u5316\u91cd\u6ce8\u5165\u548c\u5185\u5b58\u9ad8\u6548\u6267\u884c\u673a\u5236\uff0c\u89e3\u51b3\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u7684\u4e0a\u4e0b\u6587\u6f02\u79fb\u548c\u76ee\u6807\u4fe1\u606f\u4e22\u5931\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9700\u8981\u591a\u6b65\u63a8\u7406\u548c\u52a8\u6001\u91cd\u89c4\u5212\u7684\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u9762\u4e34\u7684\u4e0a\u4e0b\u6587\u6f02\u79fb\u3001\u76ee\u6807\u4fe1\u606f\u4e22\u5931\u548c\u91cd\u590d\u5931\u8d25\u5faa\u73af\u7b49\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u4e09\u4e2a\u5173\u952e\u673a\u5236\uff1a\u8ba1\u5212\u524d\u77bb\u5206\u89e3\uff08\u751f\u6210\u5b8c\u6574\u5b50\u4efb\u52a1\u5217\u8868\uff0c\u6267\u884c\u7b2c\u4e00\u9879\u5e76\u4f18\u5316\u5269\u4f59\uff09\u3001\u7ed3\u6784\u5316\u91cd\u6ce8\u5165\u7236\u8ba1\u5212\uff08\u5728\u9012\u5f52\u8fd4\u56de\u65f6\u4fdd\u6301\u591a\u7ea7\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\uff09\u3001\u5185\u5b58\u9ad8\u6548\u6267\u884c\uff08\u9650\u5236\u6d3b\u52a8\u63d0\u793a\u4f7f\u6210\u672c\u968f\u4efb\u52a1\u6df1\u5ea6\u7ebf\u6027\u6269\u5c55\uff09\u3002", "result": "\u5728\u5404\u79cd\u957f\u65f6\u7a0b\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u5b50\u76ee\u6807\u5bf9\u9f50\u548c\u6210\u529f\u7387\uff0c\u5728\u540c\u6b65Robotouille\u4e0a\u83b7\u5f9732%\u63d0\u5347\uff0c\u5728\u5f02\u6b65Robotouille\u4e0a\u83b7\u5f9729%\u6539\u8fdb\uff08\u4e25\u683cpass@1\u534f\u8bae\uff09\u3002", "conclusion": "ReCAP\u6846\u67b6\u901a\u8fc7\u5c06\u9ad8\u5c42\u76ee\u6807\u4e0e\u4f4e\u5c42\u52a8\u4f5c\u5bf9\u9f50\u3001\u51cf\u5c11\u5197\u4f59\u63d0\u793a\u548c\u4fdd\u6301\u8fde\u8d2f\u7684\u4e0a\u4e0b\u6587\u66f4\u65b0\uff0c\u6709\u6548\u63d0\u5347\u4e86\u957f\u65f6\u7a0b\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2510.24443", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.24443", "abs": "https://arxiv.org/abs/2510.24443", "authors": ["Tom \u00d3 Nuall\u00e1in"], "title": "GNAR-HARX Models for Realised Volatility: Incorporating Exogenous Predictors and Network Effects", "comment": null, "summary": "This project introduces the GNAR-HARX model, which combines Generalised\nNetwork Autoregressive (GNAR) structure with Heterogeneous Autoregressive (HAR)\ndynamics and exogenous predictors such as implied volatility. The model is\ndesigned for forecasting realised volatility by capturing both temporal\npersistence and cross-sectional spillovers in financial markets. We apply it to\ndaily realised variance data for ten international stock indices, generating\none-step-ahead forecasts in a rolling window over an out-of-sample period of\napproximately 16 years (2005-2020).\n  Forecast accuracy is evaluated using the Quasi-Likelihood (QLIKE) loss and\nmean squared error (MSE), and we compare global, standard, and local variants\nacross different network structures and exogenous specifications. The best\nmodel found by QLIKE is a local GNAR-HAR without exogenous variables, while the\nlowest MSE is achieved by a standard GNAR-HARX with implied volatility. Fully\nconnected networks consistently outperform dynamically estimated graphical\nlasso networks.\n  Overall, local and standard GNAR-HAR(X) models deliver the strongest\nforecasts, though at the cost of more parameters than the parsimonious global\nvariant, which nevertheless remains competitive. Across all cases, GNAR-HAR(X)\nmodels outperform univariate HAR(X) benchmarks, which often require more\nparameters than the GNAR-based specifications. While the top model found by\nQLIKE does not use exogenous variables, implied volatility and overnight\nreturns emerge as the most useful predictors when included.", "AI": {"tldr": "GNAR-HARX\u6a21\u578b\u7ed3\u5408\u7f51\u7edc\u81ea\u56de\u5f52\u7ed3\u6784\u548c\u5f02\u8d28\u81ea\u56de\u5f52\u52a8\u6001\uff0c\u7528\u4e8e\u9884\u6d4b\u5df2\u5b9e\u73b0\u6ce2\u52a8\u7387\uff0c\u572816\u5e74\u6837\u672c\u5916\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5355\u53d8\u91cf\u57fa\u51c6\u6a21\u578b\u3002", "motivation": "\u8bbe\u8ba1\u80fd\u591f\u540c\u65f6\u6355\u6349\u91d1\u878d\u5e02\u573a\u65f6\u95f4\u6301\u7eed\u6027\u548c\u6a2a\u622a\u9762\u6ea2\u51fa\u6548\u5e94\u7684\u6ce2\u52a8\u7387\u9884\u6d4b\u6a21\u578b\u3002", "method": "\u5c06\u5e7f\u4e49\u7f51\u7edc\u81ea\u56de\u5f52(GNAR)\u7ed3\u6784\u4e0e\u5f02\u8d28\u81ea\u56de\u5f52(HAR)\u52a8\u6001\u53ca\u5916\u751f\u9884\u6d4b\u53d8\u91cf(\u5982\u9690\u542b\u6ce2\u52a8\u7387)\u76f8\u7ed3\u5408\uff0c\u5e94\u7528\u4e8e10\u4e2a\u56fd\u9645\u80a1\u6307\u7684\u65e5\u5ea6\u5df2\u5b9e\u73b0\u65b9\u5dee\u6570\u636e\uff0c\u91c7\u7528\u6eda\u52a8\u7a97\u53e3\u8fdb\u884c\u4e00\u6b65\u9884\u6d4b\u3002", "result": "\u57fa\u4e8eQLIKE\u635f\u5931\u7684\u6700\u4f73\u6a21\u578b\u662f\u4e0d\u542b\u5916\u751f\u53d8\u91cf\u7684\u5c40\u90e8GNAR-HAR\uff0c\u800c\u6700\u4f4eMSE\u7531\u542b\u9690\u542b\u6ce2\u52a8\u7387\u7684\u6807\u51c6GNAR-HARX\u5b9e\u73b0\u3002\u5168\u8fde\u63a5\u7f51\u7edc\u59cb\u7ec8\u4f18\u4e8e\u52a8\u6001\u4f30\u8ba1\u7684\u56fe\u5957\u7d22\u7f51\u7edc\u3002", "conclusion": "\u5c40\u90e8\u548c\u6807\u51c6GNAR-HAR(X)\u6a21\u578b\u63d0\u4f9b\u6700\u5f3a\u9884\u6d4b\u80fd\u529b\uff0c\u867d\u7136\u53c2\u6570\u591a\u4e8e\u7b80\u7ea6\u7684\u5168\u5c40\u53d8\u4f53\uff0c\u4f46\u6240\u6709GNAR-HAR(X)\u6a21\u578b\u90fd\u4f18\u4e8e\u5355\u53d8\u91cfHAR(X)\u57fa\u51c6\u3002\u9690\u542b\u6ce2\u52a8\u7387\u548c\u9694\u591c\u6536\u76ca\u662f\u6700\u6709\u7528\u7684\u5916\u751f\u9884\u6d4b\u53d8\u91cf\u3002"}}
{"id": "2510.23895", "categories": ["eess.SY", "cs.OS", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.23895", "abs": "https://arxiv.org/abs/2510.23895", "authors": ["Hoora Sobhani", "Hyoseung Kim"], "title": "Modeling and Scheduling of Fusion Patterns in Autonomous Driving Systems (Extended Version)", "comment": null, "summary": "In Autonomous Driving Systems (ADS), Directed Acyclic Graphs (DAGs) are\nwidely used to model complex data dependencies and inter-task communication.\nHowever, existing DAG scheduling approaches oversimplify data fusion tasks by\nassuming fixed triggering mechanisms, failing to capture the diverse fusion\npatterns found in real-world ADS software stacks. In this paper, we propose a\nsystematic framework for analyzing various fusion patterns and their\nperformance implications in ADS. Our framework models three distinct fusion\ntask types: timer-triggered, wait-for-all, and immediate fusion, which\ncomprehensively represent real-world fusion behaviors. Our Integer Linear\nProgramming (ILP)-based approach enables an optimization of multiple real-time\nperformance metrics, including reaction time, time disparity, age of\ninformation, and response time, while generating deterministic offline\nschedules directly applicable to real platforms. Evaluation using real-world\nADS case studies, Raspberry Pi implementation, and randomly generated DAGs\ndemonstrates that our framework handles diverse fusion patterns beyond the\nscope of existing work, and achieves substantial performance improvements in\ncomparable scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u5316\u6846\u67b6\u6765\u5206\u6790\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u5404\u79cd\u6570\u636e\u878d\u5408\u6a21\u5f0f\u53ca\u5176\u6027\u80fd\u5f71\u54cd\uff0c\u901a\u8fc7\u6574\u6570\u7ebf\u6027\u89c4\u5212\u4f18\u5316\u591a\u4e2a\u5b9e\u65f6\u6027\u80fd\u6307\u6807\u3002", "motivation": "\u73b0\u6709DAG\u8c03\u5ea6\u65b9\u6cd5\u8fc7\u5ea6\u7b80\u5316\u6570\u636e\u878d\u5408\u4efb\u52a1\uff0c\u5047\u8bbe\u56fa\u5b9a\u89e6\u53d1\u673a\u5236\uff0c\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u81ea\u52a8\u9a7e\u9a76\u8f6f\u4ef6\u6808\u4e2d\u7684\u591a\u6837\u5316\u878d\u5408\u6a21\u5f0f\u3002", "method": "\u5efa\u6a21\u4e09\u79cd\u4e0d\u540c\u7684\u878d\u5408\u4efb\u52a1\u7c7b\u578b\uff1a\u5b9a\u65f6\u89e6\u53d1\u3001\u7b49\u5f85\u5168\u90e8\u548c\u7acb\u5373\u878d\u5408\uff0c\u91c7\u7528\u57fa\u4e8e\u6574\u6570\u7ebf\u6027\u89c4\u5212\u7684\u65b9\u6cd5\u4f18\u5316\u591a\u4e2a\u5b9e\u65f6\u6027\u80fd\u6307\u6807\u3002", "result": "\u5728\u771f\u5b9e\u81ea\u52a8\u9a7e\u9a76\u6848\u4f8b\u7814\u7a76\u3001\u6811\u8393\u6d3e\u5b9e\u73b0\u548c\u968f\u673a\u751f\u6210DAG\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u5904\u7406\u8d85\u51fa\u73b0\u6709\u5de5\u4f5c\u8303\u56f4\u7684\u591a\u6837\u5316\u878d\u5408\u6a21\u5f0f\uff0c\u5e76\u5728\u53ef\u6bd4\u573a\u666f\u4e2d\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u7684\u6570\u636e\u878d\u5408\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u548c\u4f18\u5316\u7684\u8c03\u5ea6\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2510.23988", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23988", "abs": "https://arxiv.org/abs/2510.23988", "authors": ["Phuc Nguyen Xuan", "Thanh Nguyen Canh", "Huu-Hung Nguyen", "Nak Young Chong", "Xiem HoangVan"], "title": "A Survey on Collaborative SLAM with 3D Gaussian Splatting", "comment": null, "summary": "This survey comprehensively reviews the evolving field of multi-robot\ncollaborative Simultaneous Localization and Mapping (SLAM) using 3D Gaussian\nSplatting (3DGS). As an explicit scene representation, 3DGS has enabled\nunprecedented real-time, high-fidelity rendering, ideal for robotics. However,\nits use in multi-robot systems introduces significant challenges in maintaining\nglobal consistency, managing communication, and fusing data from heterogeneous\nsources. We systematically categorize approaches by their architecture --\ncentralized, distributed -- and analyze core components like multi-agent\nconsistency and alignment, communication-efficient, Gaussian representation,\nsemantic distillation, fusion and pose optimization, and real-time scalability.\nIn addition, a summary of critical datasets and evaluation metrics is provided\nto contextualize performance. Finally, we identify key open challenges and\nchart future research directions, including lifelong mapping, semantic\nassociation and mapping, multi-model for robustness, and bridging the Sim2Real\ngap.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u591a\u673a\u5668\u4eba\u534f\u540cSLAM\u6280\u672f\uff0c\u5206\u6790\u4e86\u96c6\u4e2d\u5f0f\u548c\u5206\u5e03\u5f0f\u67b6\u6784\uff0c\u63a2\u8ba8\u4e86\u5168\u5c40\u4e00\u81f4\u6027\u3001\u901a\u4fe1\u6548\u7387\u7b49\u6838\u5fc3\u6311\u6218\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\u4f5c\u4e3a\u663e\u5f0f\u573a\u666f\u8868\u793a\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u5b9e\u65f6\u9ad8\u4fdd\u771f\u6e32\u67d3\uff0c\u975e\u5e38\u9002\u5408\u673a\u5668\u4eba\u5e94\u7528\u3002\u4f46\u5728\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\uff0c\u5982\u4f55\u4fdd\u6301\u5168\u5c40\u4e00\u81f4\u6027\u3001\u7ba1\u7406\u901a\u4fe1\u548c\u878d\u5408\u5f02\u6784\u6570\u636e\u6e90\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002", "method": "\u7cfb\u7edf\u6027\u5730\u6309\u67b6\u6784\uff08\u96c6\u4e2d\u5f0f\u3001\u5206\u5e03\u5f0f\uff09\u5206\u7c7b\u65b9\u6cd5\uff0c\u5206\u6790\u591a\u667a\u80fd\u4f53\u4e00\u81f4\u6027\u5bf9\u9f50\u3001\u901a\u4fe1\u6548\u7387\u3001\u9ad8\u65af\u8868\u793a\u3001\u8bed\u4e49\u84b8\u998f\u3001\u878d\u5408\u4e0e\u4f4d\u59ff\u4f18\u5316\u3001\u5b9e\u65f6\u53ef\u6269\u5c55\u6027\u7b49\u6838\u5fc3\u7ec4\u4ef6\u3002", "result": "\u63d0\u4f9b\u4e86\u5173\u952e\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\u7684\u603b\u7ed3\uff0c\u4e3a\u6027\u80fd\u8bc4\u4f30\u63d0\u4f9b\u80cc\u666f\u3002\u8bc6\u522b\u4e86\u5f53\u524d\u65b9\u6cd5\u5728\u5168\u5c40\u4e00\u81f4\u6027\u7ef4\u62a4\u548c\u5f02\u6784\u6570\u636e\u878d\u5408\u65b9\u9762\u7684\u6311\u6218\u3002", "conclusion": "\u6307\u51fa\u4e86\u672a\u6765\u5173\u952e\u7814\u7a76\u65b9\u5411\uff1a\u7ec8\u8eab\u5efa\u56fe\u3001\u8bed\u4e49\u5173\u8054\u4e0e\u5efa\u56fe\u3001\u591a\u6a21\u578b\u9c81\u68d2\u6027\u4ee5\u53caSim2Real\u5dee\u8ddd\u7684\u5f25\u5408\u3002"}}
{"id": "2510.24383", "categories": ["cs.AI", "cs.CY", "cs.MA", "I.2.11; I.2.1; I.2.4; K.4.1; K.4.3"], "pdf": "https://arxiv.org/pdf/2510.24383", "abs": "https://arxiv.org/abs/2510.24383", "authors": ["Juraj Mavra\u010di\u0107"], "title": "Policy Cards: Machine-Readable Runtime Governance for Autonomous AI Agents", "comment": "First published on 19/10/2025. Canonical archived record and DOI:\n  10.5281/zenodo.17391796", "summary": "Policy Cards are introduced as a machine-readable, deployment-layer standard\nfor expressing operational, regulatory, and ethical constraints for AI agents.\nThe Policy Card sits with the agent and enables it to follow required\nconstraints at runtime. It tells the agent what it must and must not do. As\nsuch, it becomes an integral part of the deployed agent. Policy Cards extend\nexisting transparency artifacts such as Model, Data, and System Cards by\ndefining a normative layer that encodes allow/deny rules, obligations,\nevidentiary requirements, and crosswalk mappings to assurance frameworks\nincluding NIST AI RMF, ISO/IEC 42001, and the EU AI Act. Each Policy Card can\nbe validated automatically, version-controlled, and linked to runtime\nenforcement or continuous-audit pipelines. The framework enables verifiable\ncompliance for autonomous agents, forming a foundation for distributed\nassurance in multi-agent ecosystems. Policy Cards provide a practical mechanism\nfor integrating high-level governance with hands-on engineering practice and\nenabling accountable autonomy at scale.", "AI": {"tldr": "Policy Cards\u662f\u4e00\u79cd\u673a\u5668\u53ef\u8bfb\u7684\u90e8\u7f72\u5c42\u6807\u51c6\uff0c\u7528\u4e8e\u8868\u8fbeAI\u4ee3\u7406\u7684\u64cd\u4f5c\u3001\u76d1\u7ba1\u548c\u4f26\u7406\u7ea6\u675f\uff0c\u4f7f\u5176\u5728\u8fd0\u884c\u65f6\u9075\u5faa\u89c4\u5b9a\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u900f\u660e\u5ea6\u5de5\u5177\u5982\u6a21\u578b\u5361\u3001\u6570\u636e\u5361\u548c\u7cfb\u7edf\u5361\u7f3a\u4e4f\u89c4\u8303\u6027\u7ea6\u675f\u5c42\uff0c\u65e0\u6cd5\u786e\u4fddAI\u4ee3\u7406\u5728\u90e8\u7f72\u65f6\u9075\u5b88\u64cd\u4f5c\u3001\u76d1\u7ba1\u548c\u4f26\u7406\u8981\u6c42\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49\u5305\u542b\u5141\u8bb8/\u62d2\u7edd\u89c4\u5219\u3001\u4e49\u52a1\u3001\u8bc1\u636e\u8981\u6c42\u548c\u4e0eNIST AI RMF\u3001ISO/IEC 42001\u3001\u6b27\u76dfAI\u6cd5\u6848\u7b49\u4fdd\u8bc1\u6846\u67b6\u6620\u5c04\u7684\u89c4\u8303\u6027\u5c42\u6765\u6269\u5c55\u73b0\u6709\u900f\u660e\u5ea6\u5de5\u5177\u3002", "result": "\u6bcf\u4e2aPolicy Card\u53ef\u4ee5\u81ea\u52a8\u9a8c\u8bc1\u3001\u7248\u672c\u63a7\u5236\uff0c\u5e76\u94fe\u63a5\u5230\u8fd0\u884c\u65f6\u6267\u884c\u6216\u6301\u7eed\u5ba1\u8ba1\u7ba1\u9053\uff0c\u4e3a\u81ea\u4e3b\u4ee3\u7406\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u5408\u89c4\u6027\u3002", "conclusion": "Policy Cards\u4e3a\u591a\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u5206\u5e03\u5f0f\u4fdd\u8bc1\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63d0\u4f9b\u4e86\u5c06\u9ad8\u7ea7\u6cbb\u7406\u4e0e\u5de5\u7a0b\u5b9e\u8df5\u6574\u5408\u7684\u5b9e\u7528\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u53ef\u95ee\u8d23\u7684\u81ea\u4e3b\u6027\u3002"}}
{"id": "2510.24251", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2510.24251", "abs": "https://arxiv.org/abs/2510.24251", "authors": ["Jiarui Ji", "Zehua Zhang", "Zhewei Wei", "Bin Tong", "Guan Wang", "Bo Zheng"], "title": "GRAPHIA: Harnessing Social Graph Data to Enhance LLM-Based Social Simulation", "comment": null, "summary": "Large language models (LLMs) have shown promise in simulating human-like\nsocial behaviors. Social graphs provide high-quality supervision signals that\nencode both local interactions and global network structure, yet they remain\nunderutilized for LLM training. To address this gap, we propose Graphia, the\nfirst general LLM-based social graph simulation framework that leverages graph\ndata as supervision for LLM post-training via reinforcement learning. With\nGNN-based structural rewards, Graphia trains specialized agents to predict whom\nto interact with (destination selection) and how to interact (edge generation),\nfollowed by designed graph generation pipelines. We evaluate Graphia under two\nsettings: Transductive Dynamic Graph Generation (TDGG), a micro-level task with\nour proposed node-wise interaction alignment metrics; and Inductive Dynamic\nGraph Generation (IDGG), a macro-level task with our proposed metrics for\naligning emergent network properties. On three real-world networks, Graphia\nimproves micro-level alignment by 6.1% in the composite destination selection\nscore, 12% in edge classification accuracy, and 27.9% in edge content BERTScore\nover the strongest baseline. For macro-level alignment, it achieves 41.11%\nhigher structural similarity and 32.98% better replication of social phenomena\nsuch as power laws and echo chambers. Graphia also supports counterfactual\nsimulation, generating plausible behavioral shifts under platform incentives.\nOur results show that social graphs can serve as high-quality supervision\nsignals for LLM post-training, closing the gap between agent behaviors and\nnetwork dynamics for LLM-based simulation. Code is available at\nhttps://github.com/Ji-Cather/Graphia.git.", "AI": {"tldr": "Graphia\u662f\u9996\u4e2a\u57fa\u4e8eLLM\u7684\u793e\u4ea4\u56fe\u6a21\u62df\u6846\u67b6\uff0c\u5229\u7528\u56fe\u6570\u636e\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5bf9LLM\u8fdb\u884c\u540e\u8bad\u7ec3\uff0c\u5728\u5fae\u89c2\u548c\u5b8f\u89c2\u5c42\u9762\u90fd\u663e\u8457\u63d0\u5347\u4e86\u793e\u4ea4\u7f51\u7edc\u6a21\u62df\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u793e\u4ea4\u56fe\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u76d1\u7763\u4fe1\u53f7\uff0c\u7f16\u7801\u4e86\u5c40\u90e8\u4ea4\u4e92\u548c\u5168\u5c40\u7f51\u7edc\u7ed3\u6784\uff0c\u4f46\u5728LLM\u8bad\u7ec3\u4e2d\u5c1a\u672a\u5145\u5206\u5229\u7528\u3002", "method": "\u901a\u8fc7\u57fa\u4e8eGNN\u7684\u7ed3\u6784\u5956\u52b1\uff0c\u8bad\u7ec3\u4e13\u95e8\u4ee3\u7406\u8fdb\u884c\u76ee\u6807\u9009\u62e9\u548c\u8fb9\u751f\u6210\uff0c\u7136\u540e\u901a\u8fc7\u8bbe\u8ba1\u7684\u56fe\u751f\u6210\u6d41\u7a0b\u8fdb\u884c\u6a21\u62df\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u7f51\u7edc\u4e0a\uff0cGraphia\u5728\u5fae\u89c2\u5c42\u9762\u63d0\u5347\u4e866.1%\u7684\u76ee\u6807\u9009\u62e9\u5206\u6570\u300112%\u7684\u8fb9\u5206\u7c7b\u51c6\u786e\u7387\u548c27.9%\u7684\u8fb9\u5185\u5bb9BERTScore\uff1b\u5728\u5b8f\u89c2\u5c42\u9762\u5b9e\u73b0\u4e8641.11%\u66f4\u9ad8\u7684\u7ed3\u6784\u76f8\u4f3c\u6027\u548c32.98%\u66f4\u597d\u7684\u793e\u4ea4\u73b0\u8c61\u590d\u5236\u3002", "conclusion": "\u793e\u4ea4\u56fe\u53ef\u4ee5\u4f5c\u4e3aLLM\u540e\u8bad\u7ec3\u7684\u9ad8\u8d28\u91cf\u76d1\u7763\u4fe1\u53f7\uff0c\u7f29\u5c0f\u57fa\u4e8eLLM\u7684\u6a21\u62df\u4e2d\u4ee3\u7406\u884c\u4e3a\u4e0e\u7f51\u7edc\u52a8\u6001\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2510.23824", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23824", "abs": "https://arxiv.org/abs/2510.23824", "authors": ["Murad Ismayilov", "Edwin Meriaux", "Shuo Wen", "Gregory Dudek"], "title": "Decentralized Multi-Agent Goal Assignment for Path Planning using Large Language Models", "comment": "Accepted at MIT URTC 2025", "summary": "Coordinating multiple autonomous agents in shared environments under\ndecentralized conditions is a long-standing challenge in robotics and\nartificial intelligence. This work addresses the problem of decentralized goal\nassignment for multi-agent path planning, where agents independently generate\nranked preferences over goals based on structured representations of the\nenvironment, including grid visualizations and scenario data. After this\nreasoning phase, agents exchange their goal rankings, and assignments are\ndetermined by a fixed, deterministic conflict-resolution rule (e.g., agent\nindex ordering), without negotiation or iterative coordination. We\nsystematically compare greedy heuristics, optimal assignment, and large\nlanguage model (LLM)-based agents in fully observable grid-world settings. Our\nresults show that LLM-based agents, when provided with well-designed prompts\nand relevant quantitative information, can achieve near-optimal makespans and\nconsistently outperform traditional heuristics. These findings underscore the\npotential of language models for decentralized goal assignment in multi-agent\npath planning and highlight the importance of information structure in such\nsystems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u53bb\u4e2d\u5fc3\u5316\u76ee\u6807\u5206\u914d\u95ee\u9898\uff0c\u6bd4\u8f83\u4e86\u8d2a\u5fc3\u542f\u53d1\u5f0f\u3001\u6700\u4f18\u5206\u914d\u548c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u65b9\u6cd5\uff0c\u53d1\u73b0LLM\u667a\u80fd\u4f53\u5728\u826f\u597d\u63d0\u793a\u4e0b\u80fd\u8fbe\u5230\u63a5\u8fd1\u6700\u4f18\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u53bb\u4e2d\u5fc3\u5316\u6761\u4ef6\u4e0b\u591a\u4e2a\u81ea\u4e3b\u667a\u80fd\u4f53\u5728\u5171\u4eab\u73af\u5883\u4e2d\u7684\u534f\u8c03\u6311\u6218\uff0c\u63a2\u7d22\u8bed\u8a00\u6a21\u578b\u5728\u76ee\u6807\u5206\u914d\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u667a\u80fd\u4f53\u57fa\u4e8e\u73af\u5883\u7ed3\u6784\u5316\u8868\u793a\u72ec\u7acb\u751f\u6210\u76ee\u6807\u504f\u597d\u6392\u5e8f\uff0c\u901a\u8fc7\u56fa\u5b9a\u51b2\u7a81\u89e3\u51b3\u89c4\u5219\u8fdb\u884c\u5206\u914d\uff0c\u7cfb\u7edf\u6bd4\u8f83\u8d2a\u5fc3\u542f\u53d1\u5f0f\u3001\u6700\u4f18\u5206\u914d\u548cLLM\u667a\u80fd\u4f53\u65b9\u6cd5\u3002", "result": "LLM\u667a\u80fd\u4f53\u5728\u826f\u597d\u63d0\u793a\u548c\u5b9a\u91cf\u4fe1\u606f\u652f\u6301\u4e0b\uff0c\u80fd\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u5b8c\u5de5\u65f6\u95f4\uff0c\u5e76\u6301\u7eed\u4f18\u4e8e\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u5728\u53bb\u4e2d\u5fc3\u5316\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4fe1\u606f\u7ed3\u6784\u8bbe\u8ba1\u5bf9\u6b64\u7c7b\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2510.23910", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.23910", "abs": "https://arxiv.org/abs/2510.23910", "authors": ["Khadija Omar Said", "Yukta Pareek", "Satadru Dey", "Ashish Ranjan Kumar"], "title": "Dynamical Modeling of Temperature and Smoke Evolution in a Thermal-Runaway Event of a Large-Format Lithium-ion Battery in a Mine Tunnel", "comment": null, "summary": "Large-format lithium-ion batteries (LIBs) provide effective energy storage\nsolutions for high-power equipment used in underground mining operations. They\nhave high Columbic efficiency and minimal heat and emission footprints.\nHowever, improper use of LIBs, accidents, or other factors may increase the\nprobability of thermal runaway (TR), a rapid combustion reaction that\ndischarges toxic and flammable substances. Several such incidents have been\ndocumented in mines. Since repeatable TR experiments to uncover the\ntransient-state propagation of TR are expensive and hazardous, high-fidelity\nmodels are usually developed to mimic the impact of these events. They are\nresource-intensive and are impractical to develop for many scenarios that could\nbe observed in a mine. Therefore, dynamic models within a reduced-order\nframework were constructed to represent the transient-state combustion event.\nReduced order models (ROMs) reasonably replicate trends in temperature and\nsmoke, showing strong alignment with the ground-truth dataset.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u964d\u9636\u6a21\u578b\u6765\u6a21\u62df\u9502\u79bb\u5b50\u7535\u6c60\u5728\u5730\u4e0b\u77ff\u4e95\u4e2d\u7684\u70ed\u5931\u63a7\u4f20\u64ad\uff0c\u66ff\u4ee3\u6602\u8d35\u5371\u9669\u7684\u9ad8\u4fdd\u771f\u6a21\u578b\u3002", "motivation": "\u5927\u578b\u9502\u79bb\u5b50\u7535\u6c60\u5728\u77ff\u4e95\u4e2d\u4f7f\u7528\u65f6\u53ef\u80fd\u53d1\u751f\u70ed\u5931\u63a7\uff0c\u91ca\u653e\u6709\u6bd2\u6613\u71c3\u7269\u8d28\u3002\u9ad8\u4fdd\u771f\u6a21\u578b\u8d44\u6e90\u5bc6\u96c6\u4e14\u4e0d\u5b9e\u7528\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6a21\u62df\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u4e86\u964d\u9636\u6a21\u578b\u6846\u67b6\u5185\u7684\u52a8\u6001\u6a21\u578b\u6765\u8868\u793a\u77ac\u6001\u71c3\u70e7\u4e8b\u4ef6\u3002", "result": "\u964d\u9636\u6a21\u578b\u5408\u7406\u590d\u73b0\u4e86\u6e29\u5ea6\u548c\u70df\u96fe\u8d8b\u52bf\uff0c\u4e0e\u771f\u5b9e\u6570\u636e\u96c6\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "\u964d\u9636\u6a21\u578b\u4e3a\u77ff\u4e95\u4e2d\u9502\u79bb\u5b50\u7535\u6c60\u70ed\u5931\u63a7\u4f20\u64ad\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u51c6\u786e\u7684\u6a21\u62df\u65b9\u6cd5\u3002"}}
{"id": "2510.23997", "categories": ["cs.RO", "I.2.9"], "pdf": "https://arxiv.org/pdf/2510.23997", "abs": "https://arxiv.org/abs/2510.23997", "authors": ["Stanley Wu", "Mohamad H. Danesh", "Simon Li", "Hanna Yurchyk", "Amin Abyaneh", "Anas El Houssaini", "David Meger", "Hsiu-Chin Lin"], "title": "VOCALoco: Viability-Optimized Cost-aware Adaptive Locomotion", "comment": "Accepted in IEEE Robotics and Automation Letters (RAL), 2025. 8\n  pages, 9 figures", "summary": "Recent advancements in legged robot locomotion have facilitated traversal\nover increasingly complex terrains. Despite this progress, many existing\napproaches rely on end-to-end deep reinforcement learning (DRL), which poses\nlimitations in terms of safety and interpretability, especially when\ngeneralizing to novel terrains. To overcome these challenges, we introduce\nVOCALoco, a modular skill-selection framework that dynamically adapts\nlocomotion strategies based on perceptual input. Given a set of pre-trained\nlocomotion policies, VOCALoco evaluates their viability and energy-consumption\nby predicting both the safety of execution and the anticipated cost of\ntransport over a fixed planning horizon. This joint assessment enables the\nselection of policies that are both safe and energy-efficient, given the\nobserved local terrain. We evaluate our approach on staircase locomotion tasks,\ndemonstrating its performance in both simulated and real-world scenarios using\na quadrupedal robot. Empirical results show that VOCALoco achieves improved\nrobustness and safety during stair ascent and descent compared to a\nconventional end-to-end DRL policy", "AI": {"tldr": "VOCALoco\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6280\u80fd\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8bc4\u4f30\u9884\u8bad\u7ec3\u7b56\u7565\u7684\u5b89\u5168\u6027\u548c\u80fd\u8017\u6765\u9009\u62e9\u6700\u4f73\u8fd0\u52a8\u7b56\u7565\uff0c\u5728\u697c\u68af\u4efb\u52a1\u4e2d\u6bd4\u7aef\u5230\u7aefDRL\u65b9\u6cd5\u66f4\u5b89\u5168\u9ad8\u6548\u3002", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u590d\u6742\u5730\u5f62\u5bfc\u822a\u4e2d\u5b58\u5728\u5b89\u5168\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u65b0\u5730\u5f62\u65f6\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u8fd0\u52a8\u7b56\u7565\uff0c\u901a\u8fc7\u9884\u6d4b\u6267\u884c\u5b89\u5168\u6027\u548c\u8fd0\u8f93\u6210\u672c\u6765\u8bc4\u4f30\u7b56\u7565\u53ef\u884c\u6027\uff0c\u9009\u62e9\u65e2\u5b89\u5168\u53c8\u8282\u80fd\u7684\u7b56\u7565\u3002", "result": "\u5728\u697c\u68af\u4e0a\u4e0b\u4efb\u52a1\u4e2d\uff0cVOCALoco\u5728\u4eff\u771f\u548c\u771f\u5b9e\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u90fd\u8868\u73b0\u51fa\u6bd4\u4f20\u7edf\u7aef\u5230\u7aefDRL\u65b9\u6cd5\u66f4\u597d\u7684\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u3002", "conclusion": "\u6a21\u5757\u5316\u6280\u80fd\u9009\u62e9\u6846\u67b6VOCALoco\u80fd\u591f\u6709\u6548\u63d0\u5347\u817f\u5f0f\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e0a\u7684\u8fd0\u52a8\u5b89\u5168\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2510.24442", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.24442", "abs": "https://arxiv.org/abs/2510.24442", "authors": ["Yiding Wang", "Yuxuan Chen", "Fanxu Meng", "Xifan Chen", "Xiaolei Yang", "Muhan Zhang"], "title": "Law in Silico: Simulating Legal Society with LLM-Based Agents", "comment": null, "summary": "Since real-world legal experiments are often costly or infeasible, simulating\nlegal societies with Artificial Intelligence (AI) systems provides an effective\nalternative for verifying and developing legal theory, as well as supporting\nlegal administration. Large Language Models (LLMs), with their world knowledge\nand role-playing capabilities, are strong candidates to serve as the foundation\nfor legal society simulation. However, the application of LLMs to simulate\nlegal systems remains underexplored. In this work, we introduce Law in Silico,\nan LLM-based agent framework for simulating legal scenarios with individual\ndecision-making and institutional mechanisms of legislation, adjudication, and\nenforcement. Our experiments, which compare simulated crime rates with\nreal-world data, demonstrate that LLM-based agents can largely reproduce\nmacro-level crime trends and provide insights that align with real-world\nobservations. At the same time, micro-level simulations reveal that a\nwell-functioning, transparent, and adaptive legal system offers better\nprotection of the rights of vulnerable individuals.", "AI": {"tldr": "Law in Silico\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u6cd5\u5f8b\u793e\u4f1a\u6a21\u62df\u6846\u67b6\uff0c\u901a\u8fc7AI\u4ee3\u7406\u6a21\u62df\u4e2a\u4f53\u51b3\u7b56\u548c\u7acb\u6cd5\u3001\u88c1\u51b3\u3001\u6267\u6cd5\u7b49\u5236\u5ea6\u673a\u5236\uff0c\u9a8c\u8bc1\u4e86LLM\u80fd\u591f\u590d\u73b0\u5b8f\u89c2\u72af\u7f6a\u8d8b\u52bf\u5e76\u4e3a\u6cd5\u5f8b\u7406\u8bba\u53d1\u5c55\u63d0\u4f9b\u652f\u6301\u3002", "motivation": "\u7531\u4e8e\u73b0\u5b9e\u6cd5\u5f8b\u5b9e\u9a8c\u6210\u672c\u9ad8\u6602\u6216\u4e0d\u53ef\u884c\uff0c\u9700\u8981\u5f00\u53d1AI\u7cfb\u7edf\u6765\u6a21\u62df\u6cd5\u5f8b\u793e\u4f1a\uff0c\u4ee5\u9a8c\u8bc1\u548c\u53d1\u5c55\u6cd5\u5f8b\u7406\u8bba\uff0c\u652f\u6301\u6cd5\u5f8b\u7ba1\u7406\u3002LLM\u51ed\u501f\u5176\u4e16\u754c\u77e5\u8bc6\u548c\u89d2\u8272\u626e\u6f14\u80fd\u529b\uff0c\u662f\u6784\u5efa\u6cd5\u5f8b\u793e\u4f1a\u6a21\u62df\u7684\u7406\u60f3\u57fa\u7840\u3002", "method": "\u63d0\u51faLaw in Silico\u6846\u67b6\uff0c\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u6765\u6a21\u62df\u6cd5\u5f8b\u573a\u666f\uff0c\u5305\u62ec\u4e2a\u4f53\u51b3\u7b56\u548c\u7acb\u6cd5\u3001\u88c1\u51b3\u3001\u6267\u6cd5\u7b49\u5236\u5ea6\u673a\u5236\u3002\u901a\u8fc7\u6bd4\u8f83\u6a21\u62df\u72af\u7f6a\u7387\u4e0e\u73b0\u5b9e\u6570\u636e\u6765\u9a8c\u8bc1\u6846\u67b6\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLLM\u4ee3\u7406\u80fd\u591f\u5f88\u5927\u7a0b\u5ea6\u4e0a\u590d\u73b0\u5b8f\u89c2\u5c42\u9762\u7684\u72af\u7f6a\u8d8b\u52bf\uff0c\u5e76\u63d0\u4f9b\u4e0e\u73b0\u5b9e\u89c2\u5bdf\u4e00\u81f4\u7684\u89c1\u89e3\u3002\u5fae\u89c2\u5c42\u9762\u6a21\u62df\u663e\u793a\uff0c\u529f\u80fd\u826f\u597d\u3001\u900f\u660e\u4e14\u9002\u5e94\u6027\u7684\u6cd5\u5f8b\u7cfb\u7edf\u80fd\u66f4\u597d\u5730\u4fdd\u62a4\u5f31\u52bf\u4e2a\u4f53\u7684\u6743\u5229\u3002", "conclusion": "LLM\u80fd\u591f\u6709\u6548\u6a21\u62df\u6cd5\u5f8b\u7cfb\u7edf\uff0c\u4e0d\u4ec5\u590d\u73b0\u5b8f\u89c2\u72af\u7f6a\u6a21\u5f0f\uff0c\u8fd8\u63ed\u793a\u826f\u597d\u6cd5\u5f8b\u4f53\u7cfb\u5bf9\u5f31\u52bf\u7fa4\u4f53\u4fdd\u62a4\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u6cd5\u5f8b\u7406\u8bba\u9a8c\u8bc1\u548c\u53d1\u5c55\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2510.23856", "categories": ["cs.AI", "68Txx"], "pdf": "https://arxiv.org/pdf/2510.23856", "abs": "https://arxiv.org/abs/2510.23856", "authors": ["Segev Shlomov", "Alon Oved", "Sami Marreed", "Ido Levy", "Offer Akrabi", "Avi Yaeli", "\u0141ukasz Str\u0105k", "Elizabeth Koumpan", "Yinon Goldshtein", "Eilam Shapira", "Nir Mashkif", "Asaf Adi"], "title": "From Benchmarks to Business Impact: Deploying IBM Generalist Agent in Enterprise Production", "comment": "AAAI Conference on Artificial Intelligence", "summary": "Agents are rapidly advancing in automating digital work, but enterprises face\na harder challenge: moving beyond prototypes to deployed systems that deliver\nmeasurable business value. This path is complicated by fragmented frameworks,\nslow development, and the absence of standardized evaluation practices.\nGeneralist agents have emerged as a promising direction, excelling on academic\nbenchmarks and offering flexibility across task types, applications, and\nmodalities. Yet, evidence of their use in production enterprise settings\nremains limited. This paper reports IBM's experience developing and piloting\nthe Computer Using Generalist Agent (CUGA), which has been open-sourced for the\ncommunity (https://github.com/cuga-project/cuga-agent). CUGA adopts a\nhierarchical planner--executor architecture with strong analytical foundations,\nachieving state-of-the-art performance on AppWorld and WebArena. Beyond\nbenchmarks, it was evaluated in a pilot within the Business-Process-Outsourcing\ntalent acquisition domain, addressing enterprise requirements for scalability,\nauditability, safety, and governance. To support assessment, we introduce\nBPO-TA, a 26-task benchmark spanning 13 analytics endpoints. In preliminary\nevaluations, CUGA approached the accuracy of specialized agents while\nindicating potential for reducing development time and cost. Our contribution\nis twofold: presenting early evidence of generalist agents operating at\nenterprise scale, and distilling technical and organizational lessons from this\ninitial pilot. We outline requirements and next steps for advancing\nresearch-grade architectures like CUGA into robust, enterprise-ready systems.", "AI": {"tldr": "IBM\u5f00\u53d1\u4e86\u901a\u7528\u4ee3\u7406CUGA\uff0c\u5728\u5b66\u672f\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5728\u4f01\u4e1a\u4e1a\u52a1\u6d41\u7a0b\u5916\u5305\u4eba\u624d\u62db\u8058\u9886\u57df\u8fdb\u884c\u4e86\u8bd5\u70b9\uff0c\u5c55\u793a\u4e86\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u90e8\u7f72\u901a\u7528\u4ee3\u7406\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u4f01\u4e1a\u9762\u4e34\u5c06AI\u4ee3\u7406\u4ece\u539f\u578b\u90e8\u7f72\u5230\u751f\u4ea7\u7cfb\u7edf\u7684\u6311\u6218\uff0c\u9700\u8981\u89e3\u51b3\u6846\u67b6\u788e\u7247\u5316\u3001\u5f00\u53d1\u7f13\u6162\u548c\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u7b49\u95ee\u9898\u3002\u901a\u7528\u4ee3\u7406\u5728\u5b66\u672f\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u4f01\u4e1a\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u5e94\u7528\u8bc1\u636e\u6709\u9650\u3002", "method": "CUGA\u91c7\u7528\u5206\u5c42\u89c4\u5212\u5668-\u6267\u884c\u5668\u67b6\u6784\uff0c\u5177\u6709\u5f3a\u5927\u7684\u5206\u6790\u57fa\u7840\u3002\u5728AppWorld\u548cWebArena\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u5728\u4e1a\u52a1\u6d41\u7a0b\u5916\u5305\u4eba\u624d\u62db\u8058\u9886\u57df\u8fdb\u884c\u8bd5\u70b9\u8bc4\u4f30\u3002", "result": "CUGA\u5728BPO-TA\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u542b26\u4e2a\u4efb\u52a1\u768413\u4e2a\u5206\u6790\u7aef\u70b9\uff09\u4e2d\u63a5\u8fd1\u4e13\u7528\u4ee3\u7406\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u663e\u793a\u51fa\u51cf\u5c11\u5f00\u53d1\u65f6\u95f4\u548c\u6210\u672c\u7684\u6f5c\u529b\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u901a\u7528\u4ee3\u7406\u5728\u4f01\u4e1a\u89c4\u6a21\u8fd0\u884c\u7684\u65e9\u671f\u8bc1\u636e\uff0c\u5e76\u603b\u7ed3\u4e86\u6280\u672f\u548c\u7ec4\u7ec7\u65b9\u9762\u7684\u7ecf\u9a8c\u6559\u8bad\uff0c\u4e3a\u5c06\u7814\u7a76\u7ea7\u67b6\u6784\u53d1\u5c55\u4e3a\u7a33\u5065\u7684\u4f01\u4e1a\u5c31\u7eea\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.24359", "categories": ["cs.AI", "cs.SY", "eess.SY", "q-bio.QM", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.24359", "abs": "https://arxiv.org/abs/2510.24359", "authors": ["Pedram Fard", "Alaleh Azhir", "Neguine Rezaii", "Jiazi Tian", "Hossein Estiri"], "title": "An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine", "comment": "This study has been supported by grants from the National Institutes\n  of Health: The National Institute on Aging R01AG074372 and The National\n  Institute of Allergy and Infectious Diseases R01AI165535", "summary": "Artificial intelligence in medicine is built to serve the average patient. By\nminimizing error across large datasets, most systems deliver strong aggregate\naccuracy yet falter at the margins: patients with rare variants,\nmultimorbidity, or underrepresented demographics. This average patient fallacy\nerodes both equity and trust. We propose a different design: a multi-agent\necosystem for N-of-1 decision support. In this environment, agents clustered by\norgan systems, patient populations, and analytic modalities draw on a shared\nlibrary of models and evidence synthesis tools. Their results converge in a\ncoordination layer that weighs reliability, uncertainty, and data density\nbefore presenting the clinician with a decision-support packet: risk estimates\nbounded by confidence ranges, outlier flags, and linked evidence. Validation\nshifts from population averages to individual reliability, measured by error in\nlow-density regions, calibration in the small, and risk--coverage trade-offs.\nAnticipated challenges include computational demands, automation bias, and\nregulatory fit, addressed through caching strategies, consensus checks, and\nadaptive trial frameworks. By moving from monolithic models to orchestrated\nintelligence, this approach seeks to align medical AI with the first principle\nof medicine: care that is transparent, equitable, and centered on the\nindividual.", "AI": {"tldr": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\u7528\u4e8eN-of-1\u51b3\u7b56\u652f\u6301\uff0c\u4ece\u5355\u4e00\u6a21\u578b\u8f6c\u5411\u534f\u8c03\u667a\u80fd\uff0c\u4f7f\u533b\u7597AI\u66f4\u900f\u660e\u3001\u516c\u5e73\u4e14\u4ee5\u4e2a\u4f53\u4e3a\u4e2d\u5fc3\u3002", "motivation": "\u5f53\u524d\u533b\u7597AI\u670d\u52a1\u4e8e\u5e73\u5747\u60a3\u8005\uff0c\u5728\u7f55\u89c1\u53d8\u5f02\u3001\u591a\u75c5\u5171\u5b58\u548c\u4ee3\u8868\u6027\u4e0d\u8db3\u4eba\u7fa4\u7b49\u8fb9\u7f18\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u8fd9\u79cd\u5e73\u5747\u60a3\u8005\u8c2c\u8bef\u635f\u5bb3\u4e86\u516c\u5e73\u6027\u548c\u4fe1\u4efb\u5ea6\u3002", "method": "\u8bbe\u8ba1\u591a\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\uff0c\u6309\u5668\u5b98\u7cfb\u7edf\u3001\u60a3\u8005\u7fa4\u4f53\u548c\u5206\u6790\u6a21\u5f0f\u805a\u7c7b\u667a\u80fd\u4f53\uff0c\u5171\u4eab\u6a21\u578b\u5e93\u548c\u8bc1\u636e\u5408\u6210\u5de5\u5177\uff0c\u901a\u8fc7\u534f\u8c03\u5c42\u6574\u5408\u7ed3\u679c\u5e76\u8bc4\u4f30\u53ef\u9760\u6027\u3001\u4e0d\u786e\u5b9a\u6027\u548c\u6570\u636e\u5bc6\u5ea6\u3002", "result": "\u4e3a\u4e34\u5e8a\u533b\u751f\u63d0\u4f9b\u51b3\u7b56\u652f\u6301\u5305\uff1a\u5305\u542b\u7f6e\u4fe1\u533a\u95f4\u98ce\u9669\u4f30\u8ba1\u3001\u5f02\u5e38\u503c\u6807\u8bb0\u548c\u5173\u8054\u8bc1\u636e\uff1b\u9a8c\u8bc1\u4ece\u7fa4\u4f53\u5e73\u5747\u8f6c\u5411\u4e2a\u4f53\u53ef\u9760\u6027\uff0c\u6d4b\u91cf\u4f4e\u5bc6\u5ea6\u533a\u57df\u8bef\u5dee\u3001\u5c0f\u6837\u672c\u6821\u51c6\u548c\u98ce\u9669-\u8986\u76d6\u6743\u8861\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4ece\u5355\u4e00\u6a21\u578b\u8f6c\u5411\u534f\u8c03\u667a\u80fd\uff0c\u4f7f\u533b\u7597AI\u4e0e\u533b\u5b66\u9996\u8981\u539f\u5219\u4fdd\u6301\u4e00\u81f4\uff1a\u63d0\u4f9b\u900f\u660e\u3001\u516c\u5e73\u4e14\u4ee5\u4e2a\u4f53\u4e3a\u4e2d\u5fc3\u7684\u62a4\u7406\u3002"}}
{"id": "2510.23922", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.23922", "abs": "https://arxiv.org/abs/2510.23922", "authors": ["Shashank Dhananjay Vyas", "Satadru Dey"], "title": "Secure Control of Connected and Autonomous Electrified Vehicles Under Adversarial Cyber-Attacks", "comment": null, "summary": "Connected and Autonomous Electrified Vehicles (CAEV) is the solution to the\nfuture smart mobility having benefits of efficient traffic flow and cleaner\nenvironmental impact. Although CAEV has advantages they are still susceptible\nto adversarial cyber attacks due to their autonomous electric operation and the\ninvolved connectivity. To alleviate this issue, we propose a secure control\narchitecture of CAEV. Particularly, we design an additional control input using\nReinforcement Learning (RL) to be applied to the vehicle powertrain along with\nthe input commanded by the battery. We present simulation case studies to\ndemonstrate the potential of the proposed approach in keeping the CAEV platoon\noperating safely without collisions by curbing the effect of adversarial\nattacks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8054\u7f51\u81ea\u52a8\u9a7e\u9a76\u7535\u52a8\u6c7d\u8f66(CAEV)\u7684\u5b89\u5168\u63a7\u5236\u67b6\u6784\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bbe\u8ba1\u989d\u5916\u7684\u63a7\u5236\u8f93\u5165\u6765\u7f13\u89e3\u7f51\u7edc\u653b\u51fb\u7684\u5f71\u54cd\uff0c\u786e\u4fdd\u8f66\u961f\u5b89\u5168\u8fd0\u884c", "motivation": "\u867d\u7136CAEV\u5177\u6709\u9ad8\u6548\u4ea4\u901a\u6d41\u548c\u6e05\u6d01\u73af\u5883\u7684\u4f18\u52bf\uff0c\u4f46\u7531\u4e8e\u5176\u81ea\u4e3b\u7535\u52a8\u64cd\u4f5c\u548c\u8fde\u63a5\u6027\uff0c\u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u7f51\u7edc\u653b\u51fb", "method": "\u8bbe\u8ba1\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u989d\u5916\u63a7\u5236\u8f93\u5165\uff0c\u4e0e\u7535\u6c60\u6307\u4ee4\u8f93\u5165\u4e00\u8d77\u5e94\u7528\u4e8e\u8f66\u8f86\u52a8\u529b\u7cfb\u7edf", "result": "\u4eff\u771f\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u6291\u5236\u5bf9\u6297\u6027\u653b\u51fb\u7684\u5f71\u54cd\uff0c\u4fdd\u6301CAEV\u8f66\u961f\u5b89\u5168\u8fd0\u884c\u4e14\u65e0\u78b0\u649e", "conclusion": "\u63d0\u51fa\u7684\u5b89\u5168\u63a7\u5236\u67b6\u6784\u80fd\u591f\u6709\u6548\u7f13\u89e3CAEV\u9762\u4e34\u7684\u7f51\u7edc\u653b\u51fb\u5a01\u80c1\uff0c\u786e\u4fdd\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u7684\u5b89\u5168\u6027"}}
{"id": "2510.24029", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY", "q-bio.NC", "I.2.9; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.24029", "abs": "https://arxiv.org/abs/2510.24029", "authors": ["Andrew Gerstenslager", "Bekarys Dukenbaev", "Ali A. Minai"], "title": "Improved Accuracy of Robot Localization Using 3-D LiDAR in a Hippocampus-Inspired Model", "comment": "8 pages, 9 figures, Presented at the 2025 International Joint\n  Conference on Neural Networks, Rome, July 2025", "summary": "Boundary Vector Cells (BVCs) are a class of neurons in the brains of\nvertebrates that encode environmental boundaries at specific distances and\nallocentric directions, playing a central role in forming place fields in the\nhippocampus. Most computational BVC models are restricted to two-dimensional\n(2D) environments, making them prone to spatial ambiguities in the presence of\nhorizontal symmetries in the environment. To address this limitation, we\nincorporate vertical angular sensitivity into the BVC framework, thereby\nenabling robust boundary detection in three dimensions, and leading to\nsignificantly more accurate spatial localization in a biologically-inspired\nrobot model.\n  The proposed model processes LiDAR data to capture vertical contours, thereby\ndisambiguating locations that would be indistinguishable under a purely 2D\nrepresentation. Experimental results show that in environments with minimal\nvertical variation, the proposed 3D model matches the performance of a 2D\nbaseline; yet, as 3D complexity increases, it yields substantially more\ndistinct place fields and markedly reduces spatial aliasing. These findings\nshow that adding a vertical dimension to BVC-based localization can\nsignificantly enhance navigation and mapping in real-world 3D spaces while\nretaining performance parity in simpler, near-planar scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u542b\u5782\u76f4\u89d2\u5ea6\u654f\u611f\u6027\u76843D\u8fb9\u754c\u5411\u91cf\u7ec6\u80de\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf2D\u6a21\u578b\u5728\u6c34\u5e73\u5bf9\u79f0\u73af\u5883\u4e2d\u5bb9\u6613\u4ea7\u751f\u7a7a\u95f4\u6a21\u7cca\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u7269\u542f\u53d1\u673a\u5668\u4eba\u6a21\u578b\u4e2d\u7684\u7a7a\u95f4\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u8fb9\u754c\u5411\u91cf\u7ec6\u80de\u6a21\u578b\u5c40\u9650\u4e8e\u4e8c\u7ef4\u73af\u5883\uff0c\u5728\u5b58\u5728\u6c34\u5e73\u5bf9\u79f0\u6027\u7684\u73af\u5883\u4e2d\u5bb9\u6613\u4ea7\u751f\u7a7a\u95f4\u6a21\u7cca\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u771f\u5b9e\u4e16\u754c\u7684\u4e09\u7ef4\u7a7a\u95f4\u5bfc\u822a\u95ee\u9898\u3002", "method": "\u5728BVC\u6846\u67b6\u4e2d\u5f15\u5165\u5782\u76f4\u89d2\u5ea6\u654f\u611f\u6027\uff0c\u5904\u7406LiDAR\u6570\u636e\u4ee5\u6355\u6349\u5782\u76f4\u8f6e\u5ed3\uff0c\u4ece\u800c\u533a\u5206\u5728\u7eaf2D\u8868\u793a\u4e2d\u65e0\u6cd5\u533a\u5206\u7684\u4f4d\u70b9\u3002", "result": "\u5728\u5782\u76f4\u53d8\u5316\u6700\u5c0f\u7684\u73af\u5883\u4e2d\uff0c3D\u6a21\u578b\u4e0e2D\u57fa\u7ebf\u6027\u80fd\u76f8\u5f53\uff1b\u4f46\u968f\u77403D\u590d\u6742\u5ea6\u589e\u52a0\uff0c3D\u6a21\u578b\u4ea7\u751f\u66f4\u663e\u8457\u4e0d\u540c\u7684\u4f4d\u7f6e\u573a\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u7a7a\u95f4\u6df7\u53e0\u3002", "conclusion": "\u5728BVC\u5b9a\u4f4d\u4e2d\u52a0\u5165\u5782\u76f4\u7ef4\u5ea6\u53ef\u4ee5\u663e\u8457\u589e\u5f3a\u771f\u5b9e\u4e16\u754c3D\u7a7a\u95f4\u4e2d\u7684\u5bfc\u822a\u548c\u6620\u5c04\u80fd\u529b\uff0c\u540c\u65f6\u5728\u7b80\u5355\u7684\u8fd1\u5e73\u9762\u573a\u666f\u4e2d\u4fdd\u6301\u6027\u80fd\u76f8\u5f53\u3002"}}
{"id": "2510.24360", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2510.24360", "abs": "https://arxiv.org/abs/2510.24360", "authors": ["Kosti Koistinen", "Vesa Kuikka", "Kimmo Kaski"], "title": "Importance of Overlapping Network Nodes in Influence Spreading", "comment": "17 pages, 11 figures", "summary": "In complex networks there are overlapping substructures or \"circles\" that\nconsist of nodes belonging to multiple cohesive subgroups. Yet the role of\nthese overlapping nodes in influence spreading processes remains underexplored.\nIn the present study, we analyse networks with circle structures using a\nprobabilistic influence spreading model for processes of simple and complex\ncontagion. We quantify the roles of nodes using three metrics, i.e.,\nIn-Centrality, Out-Centrality, and Betweenness Centrality that represent the\nsusceptibility, spreading power, and mediatory role of nodes, respectively, and\nfind that at each stage of the spreading process the overlapping nodes\nconsistently exhibit greater influence than the non-overlapping ones.\nFurthermore, we observe that the criteria to define circles shape the\noverlapping effects. When we restrict our analysis to only largest circles, we\nfind that circles reflect not only node-level attributes but also of\ntopological importance. These findings clarify the distinction between local\nattribute-driven circles and global community structures, thus highlighting the\nstrategic importanc of overlapping nodes in spreading dynamics. This provides\nfoundation for future research on overlapping nodes in both circles and\ncommunities.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u590d\u6742\u7f51\u7edc\u4e2d\u91cd\u53e0\u5b50\u7ed3\u6784\uff08\u5706\u5708\uff09\u5bf9\u4f20\u64ad\u8fc7\u7a0b\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u91cd\u53e0\u8282\u70b9\u5728\u7b80\u5355\u548c\u590d\u6742\u4f20\u64ad\u8fc7\u7a0b\u4e2d\u59cb\u7ec8\u6bd4\u975e\u91cd\u53e0\u8282\u70b9\u5177\u6709\u66f4\u5927\u7684\u5f71\u54cd\u529b\u3002", "motivation": "\u590d\u6742\u7f51\u7edc\u4e2d\u91cd\u53e0\u5b50\u7ed3\u6784\u6216\"\u5706\u5708\"\u5305\u542b\u5c5e\u4e8e\u591a\u4e2a\u51dd\u805a\u5b50\u7fa4\u7684\u8282\u70b9\uff0c\u4f46\u8fd9\u4e9b\u91cd\u53e0\u8282\u70b9\u5728\u5f71\u54cd\u4f20\u64ad\u8fc7\u7a0b\u4e2d\u7684\u4f5c\u7528\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4f7f\u7528\u6982\u7387\u5f71\u54cd\u4f20\u64ad\u6a21\u578b\u5206\u6790\u5177\u6709\u5706\u5708\u7ed3\u6784\u7684\u7f51\u7edc\uff0c\u901a\u8fc7\u4e09\u4e2a\u6307\u6807\u91cf\u5316\u8282\u70b9\u89d2\u8272\uff1a\u5185\u4e2d\u5fc3\u6027\uff08\u6613\u611f\u6027\uff09\u3001\u5916\u4e2d\u5fc3\u6027\uff08\u4f20\u64ad\u529b\uff09\u548c\u4e2d\u4ecb\u4e2d\u5fc3\u6027\uff08\u4e2d\u4ecb\u4f5c\u7528\uff09\u3002", "result": "\u5728\u4f20\u64ad\u8fc7\u7a0b\u7684\u6bcf\u4e2a\u9636\u6bb5\uff0c\u91cd\u53e0\u8282\u70b9\u59cb\u7ec8\u8868\u73b0\u51fa\u6bd4\u975e\u91cd\u53e0\u8282\u70b9\u66f4\u5927\u7684\u5f71\u54cd\u529b\u3002\u5b9a\u4e49\u5706\u5708\u7684\u6807\u51c6\u4f1a\u5f71\u54cd\u91cd\u53e0\u6548\u5e94\uff0c\u4ec5\u5206\u6790\u6700\u5927\u5706\u5708\u65f6\u53d1\u73b0\u5706\u5708\u4e0d\u4ec5\u53cd\u6620\u8282\u70b9\u7ea7\u5c5e\u6027\uff0c\u8fd8\u53cd\u6620\u62d3\u6251\u91cd\u8981\u6027\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u9610\u660e\u4e86\u5c40\u90e8\u5c5e\u6027\u9a71\u52a8\u7684\u5706\u5708\u4e0e\u5168\u5c40\u793e\u533a\u7ed3\u6784\u4e4b\u95f4\u7684\u533a\u522b\uff0c\u7a81\u51fa\u4e86\u91cd\u53e0\u8282\u70b9\u5728\u4f20\u64ad\u52a8\u6001\u4e2d\u7684\u6218\u7565\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u91cd\u53e0\u8282\u70b9\u5728\u5706\u5708\u548c\u793e\u533a\u4e2d\u7684\u4f5c\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.23881", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23881", "abs": "https://arxiv.org/abs/2510.23881", "authors": ["Xidong Feng", "Vivek Veeriah", "Marcus Chiam", "Michael Dennis", "Ryan Pachauri", "Thomas Tumiel", "Federico Barbero", "Johan Obando-Ceron", "Jiaxin Shi", "Satinder Singh", "Shaobo Hou", "Nenad Toma\u0161ev", "Tom Zahavy"], "title": "Generating Creative Chess Puzzles", "comment": null, "summary": "While Generative AI rapidly advances in various domains, generating truly\ncreative, aesthetic, and counter-intuitive outputs remains a challenge. This\npaper presents an approach to tackle these difficulties in the domain of chess\npuzzles. We start by benchmarking Generative AI architectures, and then\nintroduce an RL framework with novel rewards based on chess engine search\nstatistics to overcome some of those shortcomings. The rewards are designed to\nenhance a puzzle's uniqueness, counter-intuitiveness, diversity, and realism.\nOur RL approach dramatically increases counter-intuitive puzzle generation by\n10x, from 0.22\\% (supervised) to 2.5\\%, surpassing existing dataset rates\n(2.1\\%) and the best Lichess-trained model (0.4\\%). Our puzzles meet novelty\nand diversity benchmarks, retain aesthetic themes, and are rated by human\nexperts as more creative, enjoyable, and counter-intuitive than composed book\npuzzles, even approaching classic compositions. Our final outcome is a curated\nbooklet of these AI-generated puzzles, which is acknowledged for creativity by\nthree world-renowned experts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u56fd\u9645\u8c61\u68cb\u8c1c\u9898\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bbe\u8ba1\u65b0\u9896\u7684\u5956\u52b1\u51fd\u6570\u6765\u589e\u5f3a\u8c1c\u9898\u7684\u72ec\u7279\u6027\u3001\u53cd\u76f4\u89c9\u6027\u3001\u591a\u6837\u6027\u548c\u771f\u5b9e\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8c1c\u9898\u7684\u8d28\u91cf\u3002", "motivation": "\u5c3d\u7ba1\u751f\u6210\u5f0fAI\u5728\u5404\u4e2a\u9886\u57df\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u751f\u6210\u771f\u6b63\u5177\u6709\u521b\u9020\u6027\u3001\u7f8e\u5b66\u4ef7\u503c\u548c\u53cd\u76f4\u89c9\u6027\u7684\u8f93\u51fa\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u56fd\u9645\u8c61\u68cb\u8c1c\u9898\u751f\u6210\u9886\u57df\u7684\u8fd9\u4e9b\u56f0\u96be\u3002", "method": "\u9996\u5148\u5bf9\u751f\u6210\u5f0fAI\u67b6\u6784\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7136\u540e\u5f15\u5165\u57fa\u4e8e\u56fd\u9645\u8c61\u68cb\u5f15\u64ce\u641c\u7d22\u7edf\u8ba1\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8bbe\u8ba1\u4e86\u589e\u5f3a\u8c1c\u9898\u72ec\u7279\u6027\u3001\u53cd\u76f4\u89c9\u6027\u3001\u591a\u6837\u6027\u548c\u771f\u5b9e\u6027\u7684\u5956\u52b1\u51fd\u6570\u3002", "result": "\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5c06\u53cd\u76f4\u89c9\u8c1c\u9898\u751f\u6210\u7387\u4ece0.22%\u63d0\u5347\u52302.5%\uff0c\u8d85\u8fc7\u4e86\u73b0\u6709\u6570\u636e\u96c6(2.1%)\u548c\u6700\u4f73Lichess\u8bad\u7ec3\u6a21\u578b(0.4%)\u3002\u751f\u6210\u7684\u8c1c\u9898\u5728\u72ec\u7279\u6027\u3001\u591a\u6837\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4eba\u7c7b\u4e13\u5bb6\u8bc4\u4ef7\u5176\u6bd4\u4f20\u7edf\u4e66\u7c4d\u8c1c\u9898\u66f4\u5177\u521b\u9020\u6027\u3001\u8da3\u5473\u6027\u548c\u53cd\u76f4\u89c9\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u751f\u6210\u4e86\u9ad8\u8d28\u91cf\u7684AI\u56fd\u9645\u8c61\u68cb\u8c1c\u9898\uff0c\u6700\u7ec8\u6210\u679c\u83b7\u5f97\u4e86\u4e09\u4f4d\u4e16\u754c\u77e5\u540d\u4e13\u5bb6\u7684\u8ba4\u53ef\uff0c\u8bc1\u660e\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u521b\u9020\u6027\u4efb\u52a1\u751f\u6210\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.24191", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.24191", "abs": "https://arxiv.org/abs/2510.24191", "authors": ["Isabelle Krauss", "Victor G. Lopez", "Matthias A. M\u00fcller"], "title": "Sample-based Moving Horizon Estimation", "comment": null, "summary": "In this paper, we propose a sample-based moving horizon estimation (MHE)\nscheme for general nonlinear systems to estimate the current system state using\nirregularly and/or infrequently available measurements. The cost function of\nthe MHE optimization problem is suitably designed to accommodate these\nirregular output sequences. We also establish that, under a suitable\nsample-based detectability condition known as sample-based incremental\ninput/output-to-state stability (i-IOSS), the proposed sample-based MHE\nachieves robust global exponential stability (RGES). Additionally, for the case\nof linear systems, we draw connections between sample-based observability and\nsample-based i-IOSS. This demonstrates that previously established conditions\nfor linear systems to be sample-based observable can be utilized to verify or\ndesign sampling strategies that satisfy the conditions to guarantee RGES of the\nsample-based MHE. Finally, the effectiveness of the proposed sample-based MHE\nis illustrated through a simulation example.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91c7\u6837\u7684\u79fb\u52a8\u6c34\u5e73\u4f30\u8ba1\u7b97\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u975e\u7ebf\u6027\u7cfb\u7edf\u4e2d\u4e0d\u89c4\u5219\u548c/\u6216\u4e0d\u9891\u7e41\u7684\u6d4b\u91cf\u6570\u636e\uff0c\u5e76\u5728\u6ee1\u8db3\u91c7\u6837\u68c0\u6d4b\u6027\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9c81\u68d2\u5168\u5c40\u6307\u6570\u7a33\u5b9a\u6027\u3002", "motivation": "\u89e3\u51b3\u975e\u7ebf\u6027\u7cfb\u7edf\u4e2d\u7531\u4e8e\u6d4b\u91cf\u6570\u636e\u4e0d\u89c4\u5219\u6216\u4e0d\u9891\u7e41\u5bfc\u81f4\u7684\u7cfb\u7edf\u72b6\u6001\u4f30\u8ba1\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u6b64\u7c7b\u975e\u5747\u5300\u91c7\u6837\u573a\u666f\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u91c7\u6837\u7684\u79fb\u52a8\u6c34\u5e73\u4f30\u8ba1\u65b9\u6848\uff0c\u901a\u8fc7\u9002\u5f53\u8bbe\u8ba1MHE\u4f18\u5316\u95ee\u9898\u7684\u6210\u672c\u51fd\u6570\u6765\u9002\u5e94\u4e0d\u89c4\u5219\u8f93\u51fa\u5e8f\u5217\uff0c\u5e76\u5efa\u7acb\u4e86\u91c7\u6837\u68c0\u6d4b\u6027\u6761\u4ef6\u3002", "result": "\u5728\u6ee1\u8db3\u91c7\u6837\u589e\u91cf\u8f93\u5165/\u8f93\u51fa\u5230\u72b6\u6001\u7a33\u5b9a\u6027\u6761\u4ef6\u4e0b\uff0c\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u91c7\u6837\u7684MHE\u5b9e\u73b0\u4e86\u9c81\u68d2\u5168\u5c40\u6307\u6570\u7a33\u5b9a\u6027\uff0c\u5e76\u5728\u7ebf\u6027\u7cfb\u7edf\u4e2d\u5efa\u7acb\u4e86\u91c7\u6837\u53ef\u89c2\u6d4b\u6027\u4e0e\u91c7\u6837i-IOSS\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u4e0d\u89c4\u5219\u6d4b\u91cf\u6570\u636e\uff0c\u4e3a\u975e\u7ebf\u6027\u7cfb\u7edf\u72b6\u6001\u4f30\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2510.24052", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24052", "abs": "https://arxiv.org/abs/2510.24052", "authors": ["Jongsuk Kim", "Jaeyoung Lee", "Gyojin Han", "Dongjae Lee", "Minki Jeong", "Junmo Kim"], "title": "SynAD: Enhancing Real-World End-to-End Autonomous Driving Models through Synthetic Data Integration", "comment": null, "summary": "Recent advancements in deep learning and the availability of high-quality\nreal-world driving datasets have propelled end-to-end autonomous driving.\nDespite this progress, relying solely on real-world data limits the variety of\ndriving scenarios for training. Synthetic scenario generation has emerged as a\npromising solution to enrich the diversity of training data; however, its\napplication within E2E AD models remains largely unexplored. This is primarily\ndue to the absence of a designated ego vehicle and the associated sensor\ninputs, such as camera or LiDAR, typically provided in real-world scenarios. To\naddress this gap, we introduce SynAD, the first framework designed to enhance\nreal-world E2E AD models using synthetic data. Our method designates the agent\nwith the most comprehensive driving information as the ego vehicle in a\nmulti-agent synthetic scenario. We further project path-level scenarios onto\nmaps and employ a newly developed Map-to-BEV Network to derive bird's-eye-view\nfeatures without relying on sensor inputs. Finally, we devise a training\nstrategy that effectively integrates these map-based synthetic data with real\ndriving data. Experimental results demonstrate that SynAD effectively\nintegrates all components and notably enhances safety performance. By bridging\nsynthetic scenario generation and E2E AD, SynAD paves the way for more\ncomprehensive and robust autonomous driving models.", "AI": {"tldr": "SynAD\u662f\u9996\u4e2a\u5229\u7528\u5408\u6210\u6570\u636e\u589e\u5f3a\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u591a\u667a\u80fd\u4f53\u5408\u6210\u573a\u666f\u4e2d\u6307\u5b9a\u5177\u6709\u6700\u5168\u9762\u9a7e\u9a76\u4fe1\u606f\u7684\u667a\u80fd\u4f53\u4f5c\u4e3a\u81ea\u8f66\uff0c\u5c06\u8def\u5f84\u7ea7\u573a\u666f\u6295\u5f71\u5230\u5730\u56fe\u4e0a\uff0c\u5e76\u4f7f\u7528Map-to-BEV\u7f51\u7edc\u751f\u6210\u9e1f\u77b0\u56fe\u7279\u5f81\uff0c\u6709\u6548\u63d0\u5347\u5b89\u5168\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u548c\u9ad8\u8d28\u91cf\u771f\u5b9e\u9a7e\u9a76\u6570\u636e\u96c6\u63a8\u52a8\u4e86\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7684\u53d1\u5c55\uff0c\u4f46\u4ec5\u4f9d\u8d56\u771f\u5b9e\u6570\u636e\u9650\u5236\u4e86\u8bad\u7ec3\u573a\u666f\u7684\u591a\u6837\u6027\u3002\u5408\u6210\u573a\u666f\u751f\u6210\u867d\u80fd\u4e30\u5bcc\u8bad\u7ec3\u6570\u636e\uff0c\u4f46\u5728\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u4e3b\u8981\u56e0\u4e3a\u7f3a\u4e4f\u6307\u5b9a\u7684\u81ea\u8f66\u548c\u76f8\u5173\u4f20\u611f\u5668\u8f93\u5165\u3002", "method": "\u5728\u591a\u667a\u80fd\u4f53\u5408\u6210\u573a\u666f\u4e2d\u6307\u5b9a\u5177\u6709\u6700\u5168\u9762\u9a7e\u9a76\u4fe1\u606f\u7684\u667a\u80fd\u4f53\u4f5c\u4e3a\u81ea\u8f66\uff1b\u5c06\u8def\u5f84\u7ea7\u573a\u666f\u6295\u5f71\u5230\u5730\u56fe\u4e0a\uff1b\u5f00\u53d1Map-to-BEV\u7f51\u7edc\u5728\u4e0d\u4f9d\u8d56\u4f20\u611f\u5668\u8f93\u5165\u7684\u60c5\u51b5\u4e0b\u751f\u6210\u9e1f\u77b0\u56fe\u7279\u5f81\uff1b\u8bbe\u8ba1\u8bad\u7ec3\u7b56\u7565\u6709\u6548\u6574\u5408\u5730\u56fe\u57fa\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u9a7e\u9a76\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eSynAD\u6709\u6548\u6574\u5408\u4e86\u6240\u6709\u7ec4\u4ef6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b89\u5168\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u6865\u63a5\u5408\u6210\u573a\u666f\u751f\u6210\u548c\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\uff0cSynAD\u4e3a\u66f4\u5168\u9762\u548c\u9c81\u68d2\u7684\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.24337", "categories": ["cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.24337", "abs": "https://arxiv.org/abs/2510.24337", "authors": ["Daria Kravets-Meinke", "Hannah Schmid-Petri", "Sonja Niemann", "Ute Schmid"], "title": "Generative Large Language Models (gLLMs) in Content Analysis: A Practical Guide for Communication Research", "comment": null, "summary": "Generative Large Language Models (gLLMs), such as ChatGPT, are increasingly\nbeing used in communication research for content analysis. Studies show that\ngLLMs can outperform both crowd workers and trained coders, such as research\nassistants, on various coding tasks relevant to communication science, often at\na fraction of the time and cost. Additionally, gLLMs can decode implicit\nmeanings and contextual information, be instructed using natural language,\ndeployed with only basic programming skills, and require little to no annotated\ndata beyond a validation dataset - constituting a paradigm shift in automated\ncontent analysis. Despite their potential, the integration of gLLMs into the\nmethodological toolkit of communication research remains underdeveloped. In\ngLLM-assisted quantitative content analysis, researchers must address at least\nseven critical challenges that impact result quality: (1) codebook development,\n(2) prompt engineering, (3) model selection, (4) parameter tuning, (5)\niterative refinement, (6) validation of the model's reliability, and\noptionally, (7) performance enhancement. This paper synthesizes emerging\nresearch on gLLM-assisted quantitative content analysis and proposes a\ncomprehensive best-practice guide to navigate these challenges. Our goal is to\nmake gLLM-based content analysis more accessible to a broader range of\ncommunication researchers and ensure adherence to established disciplinary\nquality standards of validity, reliability, reproducibility, and research\nethics.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f20\u64ad\u5b66\u5185\u5bb9\u5206\u6790\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u5e94\u5bf9\u4e03\u5927\u6311\u6218\u7684\u6700\u4f73\u5b9e\u8df5\u6307\u5357\uff0c\u65e8\u5728\u4f7fgLLM\u8f85\u52a9\u7684\u5185\u5bb9\u5206\u6790\u66f4\u6613\u7528\u4e14\u7b26\u5408\u5b66\u79d1\u8d28\u91cf\u6807\u51c6\u3002", "motivation": "\u5c3d\u7ba1\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f20\u64ad\u5b66\u5185\u5bb9\u5206\u6790\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u8d85\u8d8a\u4f17\u5305\u5de5\u4f5c\u8005\u548c\u8bad\u7ec3\u6709\u7d20\u7684\u7f16\u7801\u5458\uff0c\u4f46\u5176\u5728\u4f20\u64ad\u7814\u7a76\u65b9\u6cd5\u8bba\u4e2d\u7684\u6574\u5408\u4ecd\u4e0d\u6210\u719f\uff0c\u9700\u8981\u89e3\u51b3\u5f71\u54cd\u7ed3\u679c\u8d28\u91cf\u7684\u4e03\u5927\u5173\u952e\u6311\u6218\u3002", "method": "\u7efc\u5408\u65b0\u5174\u7814\u7a76\uff0c\u63d0\u51fa\u5305\u542b\u4ee3\u7801\u672c\u5f00\u53d1\u3001\u63d0\u793a\u5de5\u7a0b\u3001\u6a21\u578b\u9009\u62e9\u3001\u53c2\u6570\u8c03\u4f18\u3001\u8fed\u4ee3\u4f18\u5316\u3001\u53ef\u9760\u6027\u9a8c\u8bc1\u548c\u6027\u80fd\u63d0\u5347\u7684\u4e03\u6b65\u6700\u4f73\u5b9e\u8df5\u6846\u67b6\u3002", "result": "\u901a\u8fc7\u7cfb\u7edf\u5316\u65b9\u6cd5\uff0c\u4f7fgLLM\u8f85\u52a9\u7684\u5b9a\u91cf\u5185\u5bb9\u5206\u6790\u66f4\u6613\u4e3a\u4f20\u64ad\u7814\u7a76\u8005\u4f7f\u7528\uff0c\u540c\u65f6\u786e\u4fdd\u7ed3\u679c\u7684\u6709\u6548\u6027\u3001\u53ef\u9760\u6027\u3001\u53ef\u91cd\u590d\u6027\u548c\u7814\u7a76\u4f26\u7406\u3002", "conclusion": "gLLM\u8f85\u52a9\u7684\u5b9a\u91cf\u5185\u5bb9\u5206\u6790\u4ee3\u8868\u4e86\u81ea\u52a8\u5316\u5185\u5bb9\u5206\u6790\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u901a\u8fc7\u89e3\u51b3\u4e03\u5927\u6311\u6218\u5e76\u9075\u5faa\u6700\u4f73\u5b9e\u8df5\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u4f20\u64ad\u7814\u7a76\u7684\u6548\u7387\u548c\u8d28\u91cf\u3002"}}
{"id": "2510.23882", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23882", "abs": "https://arxiv.org/abs/2510.23882", "authors": ["Adil Rasheed", "Oscar Ravik", "Omer San"], "title": "Hybrid Modeling, Sim-to-Real Reinforcement Learning, and Large Language Model Driven Control for Digital Twins", "comment": null, "summary": "This work investigates the use of digital twins for dynamical system modeling\nand control, integrating physics-based, data-driven, and hybrid approaches with\nboth traditional and AI-driven controllers. Using a miniature greenhouse as a\ntest platform, four predictive models Linear, Physics-Based Modeling (PBM),\nLong Short Term Memory (LSTM), and Hybrid Analysis and Modeling (HAM) are\ndeveloped and compared under interpolation and extrapolation scenarios. Three\ncontrol strategies Model Predictive Control (MPC), Reinforcement Learning (RL),\nand Large Language Model (LLM) based control are also implemented to assess\ntrade-offs in precision, adaptability, and implementation effort. Results show\nthat in modeling HAM provides the most balanced performance across accuracy,\ngeneralization, and computational efficiency, while LSTM achieves high\nprecision at greater resource cost. Among controllers, MPC delivers robust and\npredictable performance, RL demonstrates strong adaptability, and LLM-based\ncontrollers offer flexible human-AI interaction when coupled with predictive\ntools.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u6570\u5b57\u5b6a\u751f\u4e2d\u7684\u56db\u79cd\u9884\u6d4b\u6a21\u578b\uff08\u7ebf\u6027\u3001\u7269\u7406\u5efa\u6a21\u3001LSTM\u3001\u6df7\u5408\u5efa\u6a21\uff09\u548c\u4e09\u79cd\u63a7\u5236\u7b56\u7565\uff08MPC\u3001RL\u3001LLM\u63a7\u5236\uff09\uff0c\u53d1\u73b0\u5728\u6e29\u5ba4\u6d4b\u8bd5\u5e73\u53f0\u4e0a\uff0cHAM\u6a21\u578b\u5728\u7cbe\u5ea6\u3001\u6cdb\u5316\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u8868\u73b0\u6700\u5747\u8861\uff0c\u800cMPC\u63a7\u5236\u5668\u7a33\u5065\uff0cRL\u9002\u5e94\u6027\u5f3a\uff0cLLM\u63a7\u5236\u5668\u63d0\u4f9b\u7075\u6d3b\u7684\u4eba\u673a\u4ea4\u4e92\u3002", "motivation": "\u7814\u7a76\u6570\u5b57\u5b6a\u751f\u5728\u52a8\u6001\u7cfb\u7edf\u5efa\u6a21\u548c\u63a7\u5236\u4e2d\u7684\u5e94\u7528\uff0c\u6574\u5408\u7269\u7406\u57fa\u7840\u3001\u6570\u636e\u9a71\u52a8\u548c\u6df7\u5408\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4f20\u7edf\u548cAI\u9a71\u52a8\u63a7\u5236\u5668\u7684\u6027\u80fd\u5dee\u5f02\u3002", "method": "\u4f7f\u7528\u5fae\u578b\u6e29\u5ba4\u4f5c\u4e3a\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5f00\u53d1\u56db\u79cd\u9884\u6d4b\u6a21\u578b\uff08\u7ebf\u6027\u3001PBM\u3001LSTM\u3001HAM\uff09\u548c\u4e09\u79cd\u63a7\u5236\u7b56\u7565\uff08MPC\u3001RL\u3001LLM\u63a7\u5236\uff09\uff0c\u5728\u63d2\u503c\u548c\u5916\u63a8\u573a\u666f\u4e0b\u8fdb\u884c\u6bd4\u8f83\u8bc4\u4f30\u3002", "result": "HAM\u6a21\u578b\u5728\u7cbe\u5ea6\u3001\u6cdb\u5316\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u8868\u73b0\u6700\u5747\u8861\uff1bLSTM\u7cbe\u5ea6\u9ad8\u4f46\u8d44\u6e90\u6d88\u8017\u5927\uff1bMPC\u63a7\u5236\u5668\u7a33\u5065\u53ef\u9884\u6d4b\uff1bRL\u63a7\u5236\u5668\u9002\u5e94\u6027\u5f3a\uff1bLLM\u63a7\u5236\u5668\u4e0e\u9884\u6d4b\u5de5\u5177\u7ed3\u5408\u65f6\u63d0\u4f9b\u7075\u6d3b\u7684\u4eba\u673a\u4ea4\u4e92\u3002", "conclusion": "HAM\u6a21\u578b\u5728\u5efa\u6a21\u65b9\u9762\u63d0\u4f9b\u6700\u4f73\u5e73\u8861\u6027\u80fd\uff0cMPC\u63a7\u5236\u5668\u7a33\u5065\uff0cRL\u9002\u5e94\u6027\u5f3a\uff0cLLM\u63a7\u5236\u5668\u5728\u7ed3\u5408\u9884\u6d4b\u5de5\u5177\u65f6\u5b9e\u73b0\u7075\u6d3b\u7684\u4eba\u673a\u4ea4\u4e92\uff0c\u4e3a\u6570\u5b57\u5b6a\u751f\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5efa\u6a21\u548c\u63a7\u5236\u65b9\u6848\u3002"}}
{"id": "2510.24228", "categories": ["eess.SY", "cs.LG", "cs.NA", "cs.SY", "math.NA"], "pdf": "https://arxiv.org/pdf/2510.24228", "abs": "https://arxiv.org/abs/2510.24228", "authors": ["Luis Romero-Ben", "Paul Irofti", "Florin Stoican", "Vicen\u00e7 Puig"], "title": "A comparison between joint and dual UKF implementations for state estimation and leak localization in water distribution networks", "comment": "This work has been submitted to ECC2026 for review. It has 7 pages\n  and 2 figures", "summary": "The sustainability of modern cities highly depends on efficient water\ndistribution management, including effective pressure control and leak\ndetection and localization. Accurate information about the network hydraulic\nstate is therefore essential. This article presents a comparison between two\ndata-driven state estimation methods based on the Unscented Kalman Filter\n(UKF), fusing pressure, demand and flow data for head and flow estimation. One\napproach uses a joint state vector with a single estimator, while the other\nuses a dual-estimator scheme. We analyse their main characteristics, discussing\ndifferences, advantages and limitations, and compare them theoretically in\nterms of accuracy and complexity. Finally, we show several estimation results\nfor the L-TOWN benchmark, allowing to discuss their properties in a real\nimplementation.", "AI": {"tldr": "\u6bd4\u8f83\u4e24\u79cd\u57fa\u4e8e\u65e0\u8ff9\u5361\u5c14\u66fc\u6ee4\u6ce2(UKF)\u7684\u6570\u636e\u9a71\u52a8\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8e\u57ce\u5e02\u4f9b\u6c34\u7ba1\u7f51\u7684\u6c34\u5934\u548c\u6d41\u91cf\u4f30\u8ba1\uff0c\u5206\u6790\u5355\u4f30\u8ba1\u5668\u548c\u53cc\u4f30\u8ba1\u5668\u65b9\u6848\u7684\u7279\u6027\u5dee\u5f02\u3002", "motivation": "\u73b0\u4ee3\u57ce\u5e02\u7684\u53ef\u6301\u7eed\u6027\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9ad8\u6548\u7684\u6c34\u5206\u914d\u7ba1\u7406\uff0c\u5305\u62ec\u6709\u6548\u7684\u538b\u529b\u63a7\u5236\u548c\u6cc4\u6f0f\u68c0\u6d4b\u5b9a\u4f4d\uff0c\u56e0\u6b64\u9700\u8981\u51c6\u786e\u7684\u7ba1\u7f51\u6c34\u529b\u72b6\u6001\u4fe1\u606f\u3002", "method": "\u4f7f\u7528\u4e24\u79cd\u57fa\u4e8eUKF\u7684\u6570\u636e\u9a71\u52a8\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\uff1a\u4e00\u79cd\u91c7\u7528\u8054\u5408\u72b6\u6001\u5411\u91cf\u7684\u5355\u4f30\u8ba1\u5668\uff0c\u53e6\u4e00\u79cd\u91c7\u7528\u53cc\u4f30\u8ba1\u5668\u65b9\u6848\uff0c\u878d\u5408\u538b\u529b\u3001\u9700\u6c42\u548c\u6d41\u91cf\u6570\u636e\u8fdb\u884c\u6c34\u5934\u548c\u6d41\u91cf\u4f30\u8ba1\u3002", "result": "\u5728L-TOWN\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u591a\u79cd\u4f30\u8ba1\u7ed3\u679c\uff0c\u80fd\u591f\u8ba8\u8bba\u8fd9\u4e9b\u65b9\u6cd5\u5728\u5b9e\u9645\u5b9e\u65bd\u4e2d\u7684\u7279\u6027\u3002", "conclusion": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9645\u5b9e\u73b0\u6bd4\u8f83\u4e86\u4e24\u79cdUKF\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u3001\u590d\u6742\u6027\u3001\u5dee\u5f02\u3001\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002"}}
{"id": "2510.24055", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24055", "abs": "https://arxiv.org/abs/2510.24055", "authors": ["Xiucheng Zhang", "Yang Jiang", "Hongwei Qing", "Jiashuo Bai"], "title": "Language-Conditioned Representations and Mixture-of-Experts Policy for Robust Multi-Task Robotic Manipulation", "comment": "8 pages", "summary": "Perceptual ambiguity and task conflict limit multitask robotic manipulation\nvia imitation learning. We propose a framework combining a Language-Conditioned\nVisual Representation (LCVR) module and a Language-conditioned\nMixture-ofExperts Density Policy (LMoE-DP). LCVR resolves perceptual\nambiguities by grounding visual features with language instructions, enabling\ndifferentiation between visually similar tasks. To mitigate task conflict,\nLMoE-DP uses a sparse expert architecture to specialize in distinct, multimodal\naction distributions, stabilized by gradient modulation. On real-robot\nbenchmarks, LCVR boosts Action Chunking with Transformers (ACT) and Diffusion\nPolicy (DP) success rates by 33.75% and 25%, respectively. The full framework\nachieves a 79% average success, outperforming the advanced baseline by 21%. Our\nwork shows that combining semantic grounding and expert specialization enables\nrobust, efficient multi-task manipulation", "AI": {"tldr": "\u63d0\u51faLCVR\u548cLMoE-DP\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u57fa\u7840\u548c\u4e13\u5bb6\u4e13\u4e1a\u5316\u89e3\u51b3\u6a21\u4eff\u5b66\u4e60\u4e2d\u611f\u77e5\u6a21\u7cca\u548c\u4efb\u52a1\u51b2\u7a81\u95ee\u9898\uff0c\u5728\u771f\u5b9e\u673a\u5668\u4eba\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u591a\u4efb\u52a1\u64cd\u4f5c\u6027\u80fd", "motivation": "\u611f\u77e5\u6a21\u7cca\u548c\u4efb\u52a1\u51b2\u7a81\u9650\u5236\u4e86\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u5b9e\u73b0\u591a\u4efb\u52a1\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6548\u679c\uff0c\u9700\u8981\u89e3\u51b3\u89c6\u89c9\u76f8\u4f3c\u4efb\u52a1\u7684\u533a\u5206\u548c\u591a\u4efb\u52a1\u95f4\u7684\u51b2\u7a81\u95ee\u9898", "method": "\u7ed3\u5408\u8bed\u8a00\u6761\u4ef6\u89c6\u89c9\u8868\u793a(LCVR)\u6a21\u5757\u548c\u8bed\u8a00\u6761\u4ef6\u6df7\u5408\u4e13\u5bb6\u5bc6\u5ea6\u7b56\u7565(LMoE-DP)\u3002LCVR\u901a\u8fc7\u8bed\u8a00\u6307\u4ee4\u57fa\u7840\u89c6\u89c9\u7279\u5f81\u89e3\u51b3\u611f\u77e5\u6a21\u7cca\uff0cLMoE-DP\u4f7f\u7528\u7a00\u758f\u4e13\u5bb6\u67b6\u6784\u4e13\u95e8\u5904\u7406\u4e0d\u540c\u7684\u591a\u6a21\u6001\u52a8\u4f5c\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u68af\u5ea6\u8c03\u5236\u7a33\u5b9a\u8bad\u7ec3", "result": "\u5728\u771f\u5b9e\u673a\u5668\u4eba\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLCVR\u5c06Action Chunking with Transformers (ACT)\u548cDiffusion Policy (DP)\u7684\u6210\u529f\u7387\u5206\u522b\u63d0\u534733.75%\u548c25%\u3002\u5b8c\u6574\u6846\u67b6\u8fbe\u523079%\u7684\u5e73\u5747\u6210\u529f\u7387\uff0c\u6bd4\u5148\u8fdb\u57fa\u7ebf\u9ad821%", "conclusion": "\u7ed3\u5408\u8bed\u4e49\u57fa\u7840\u548c\u4e13\u5bb6\u4e13\u4e1a\u5316\u80fd\u591f\u5b9e\u73b0\u9c81\u68d2\u3001\u9ad8\u6548\u7684\u591a\u4efb\u52a1\u64cd\u4f5c\uff0c\u4e3a\u89e3\u51b3\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u611f\u77e5\u6a21\u7cca\u548c\u4efb\u52a1\u51b2\u7a81\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848"}}
{"id": "2510.23883", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23883", "abs": "https://arxiv.org/abs/2510.23883", "authors": ["Shrestha Datta", "Shahriar Kabir Nahin", "Anshuman Chhabra", "Prasant Mohapatra"], "title": "Agentic AI Security: Threats, Defenses, Evaluation, and Open Challenges", "comment": null, "summary": "Agentic AI systems powered by large language models (LLMs) and endowed with\nplanning, tool use, memory, and autonomy, are emerging as powerful, flexible\nplatforms for automation. Their ability to autonomously execute tasks across\nweb, software, and physical environments creates new and amplified security\nrisks, distinct from both traditional AI safety and conventional software\nsecurity. This survey outlines a taxonomy of threats specific to agentic AI,\nreviews recent benchmarks and evaluation methodologies, and discusses defense\nstrategies from both technical and governance perspectives. We synthesize\ncurrent research and highlight open challenges, aiming to support the\ndevelopment of secure-by-design agent systems.", "AI": {"tldr": "\u672c\u6587\u8c03\u67e5\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fdAI\u4ee3\u7406\u7cfb\u7edf\u6240\u9762\u4e34\u7684\u5b89\u5168\u5a01\u80c1\uff0c\u63d0\u51fa\u4e86\u5a01\u80c1\u5206\u7c7b\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u8bc4\u4f30\u65b9\u6cd5\u548c\u9632\u5fa1\u7b56\u7565\u3002", "motivation": "\u968f\u7740\u5177\u5907\u89c4\u5212\u3001\u5de5\u5177\u4f7f\u7528\u3001\u8bb0\u5fc6\u548c\u81ea\u4e3b\u80fd\u529b\u7684AI\u4ee3\u7406\u7cfb\u7edf\u5728web\u3001\u8f6f\u4ef6\u548c\u7269\u7406\u73af\u5883\u4e2d\u81ea\u4e3b\u6267\u884c\u4efb\u52a1\uff0c\u5b83\u4eec\u5e26\u6765\u4e86\u4e0d\u540c\u4e8e\u4f20\u7edfAI\u5b89\u5168\u548c\u8f6f\u4ef6\u5b89\u5168\u7684\u65b0\u578b\u5b89\u5168\u98ce\u9669\uff0c\u9700\u8981\u4e13\u95e8\u7814\u7a76\u3002", "method": "\u91c7\u7528\u8c03\u67e5\u5206\u6790\u65b9\u6cd5\uff0c\u6784\u5efa\u4e86\u9488\u5bf9\u667a\u80fdAI\u4ee3\u7406\u7684\u5a01\u80c1\u5206\u7c7b\u6cd5\uff0c\u56de\u987e\u4e86\u6700\u8fd1\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u4ece\u6280\u672f\u548c\u6cbb\u7406\u4e24\u4e2a\u89d2\u5ea6\u8ba8\u8bba\u9632\u5fa1\u7b56\u7565\u3002", "result": "\u7cfb\u7edf\u68b3\u7406\u4e86\u5f53\u524d\u7814\u7a76\u73b0\u72b6\uff0c\u8bc6\u522b\u4e86\u667a\u80fdAI\u4ee3\u7406\u7279\u6709\u7684\u5b89\u5168\u5a01\u80c1\u7c7b\u578b\uff0c\u5e76\u603b\u7ed3\u4e86\u73b0\u6709\u7684\u8bc4\u4f30\u548c\u9632\u5fa1\u65b9\u6cd5\u3002", "conclusion": "\u5f3a\u8c03\u4e86\u667a\u80fdAI\u4ee3\u7406\u5b89\u5168\u98ce\u9669\u7684\u7279\u6b8a\u6027\uff0c\u6307\u51fa\u4e86\u5f00\u653e\u6311\u6218\uff0c\u65e8\u5728\u652f\u6301\u5f00\u53d1\u5b89\u5168\u8bbe\u8ba1\u7684\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u3002"}}
{"id": "2510.24272", "categories": ["eess.SY", "cs.AI", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.24272", "abs": "https://arxiv.org/abs/2510.24272", "authors": ["Maximilian Bloor", "Max Mowbray", "Ehecatl Antonio Del Rio Chanona", "Calvin Tsay"], "title": "Survey and Tutorial of Reinforcement Learning Methods in Process Systems Engineering", "comment": null, "summary": "Sequential decision making under uncertainty is central to many Process\nSystems Engineering (PSE) challenges, where traditional methods often face\nlimitations related to controlling and optimizing complex and stochastic\nsystems. Reinforcement Learning (RL) offers a data-driven approach to derive\ncontrol policies for such challenges. This paper presents a survey and tutorial\non RL methods, tailored for the PSE community. We deliver a tutorial on RL,\ncovering fundamental concepts and key algorithmic families including\nvalue-based, policy-based and actor-critic methods. Subsequently, we survey\nexisting applications of these RL techniques across various PSE domains, such\nas in fed-batch and continuous process control, process optimization, and\nsupply chains. We conclude with PSE focused discussion of specialized\ntechniques and emerging directions. By synthesizing the current state of RL\nalgorithm development and implications for PSE this work identifies successes,\nchallenges, trends, and outlines avenues for future research at the interface\nof these fields.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u662f\u5173\u4e8e\u5f3a\u5316\u5b66\u4e60\u5728\u8fc7\u7a0b\u7cfb\u7edf\u5de5\u7a0b\u4e2d\u7684\u7efc\u8ff0\u4e0e\u6559\u7a0b\uff0c\u4ecb\u7ecd\u4e86RL\u7684\u57fa\u672c\u6982\u5ff5\u3001\u7b97\u6cd5\u5bb6\u65cf\u53ca\u5176\u5728PSE\u9886\u57df\u7684\u5e94\u7528\uff0c\u5e76\u8ba8\u8bba\u4e86\u8be5\u9886\u57df\u7684\u6311\u6218\u4e0e\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u968f\u673a\u7cfb\u7edf\u7684\u63a7\u5236\u548c\u4f18\u5316\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u6765\u5e94\u5bf9\u8fd9\u4e9b\u8fc7\u7a0b\u7cfb\u7edf\u5de5\u7a0b\u6311\u6218\u3002", "method": "\u63d0\u4f9b\u4e86\u5f3a\u5316\u5b66\u4e60\u7684\u6559\u7a0b\uff0c\u6db5\u76d6\u57fa\u672c\u6982\u5ff5\u548c\u5173\u952e\u7b97\u6cd5\u5bb6\u65cf\uff08\u57fa\u4e8e\u4ef7\u503c\u3001\u57fa\u4e8e\u7b56\u7565\u548c\u6f14\u5458-\u8bc4\u8bba\u5bb6\u65b9\u6cd5\uff09\uff0c\u5e76\u7efc\u8ff0\u4e86\u8fd9\u4e9bRL\u6280\u672f\u5728PSE\u9886\u57df\u7684\u73b0\u6709\u5e94\u7528\u3002", "result": "\u7cfb\u7edf\u603b\u7ed3\u4e86RL\u5728PSE\u9886\u57df\u7684\u5e94\u7528\u60c5\u51b5\uff0c\u5305\u62ec\u5206\u6279\u548c\u8fde\u7eed\u8fc7\u7a0b\u63a7\u5236\u3001\u8fc7\u7a0b\u4f18\u5316\u3001\u4f9b\u5e94\u94fe\u7ba1\u7406\u7b49\u4e0d\u540c\u9886\u57df\u3002", "conclusion": "\u901a\u8fc7\u7efc\u5408RL\u7b97\u6cd5\u5f00\u53d1\u73b0\u72b6\u53ca\u5176\u5bf9PSE\u7684\u5f71\u54cd\uff0c\u8bc6\u522b\u4e86\u6210\u529f\u6848\u4f8b\u3001\u6311\u6218\u3001\u8d8b\u52bf\uff0c\u5e76\u89c4\u5212\u4e86\u8fd9\u4e24\u4e2a\u9886\u57df\u4ea4\u53c9\u7814\u7a76\u7684\u672a\u6765\u65b9\u5411\u3002"}}
{"id": "2510.24067", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24067", "abs": "https://arxiv.org/abs/2510.24067", "authors": ["Tianyi Ding", "Ronghao Zheng", "Senlin Zhang", "Meiqin Liu"], "title": "Balanced Collaborative Exploration via Distributed Topological Graph Voronoi Partition", "comment": null, "summary": "This work addresses the collaborative multi-robot autonomous online\nexploration problem, particularly focusing on distributed exploration planning\nfor dynamically balanced exploration area partition and task allocation among a\nteam of mobile robots operating in obstacle-dense non-convex environments.\n  We present a novel topological map structure that simultaneously\ncharacterizes both spatial connectivity and global exploration completeness of\nthe environment. The topological map is updated incrementally to utilize known\nspatial information for updating reachable spaces, while exploration targets\nare planned in a receding horizon fashion under global coverage guidance.\n  A distributed weighted topological graph Voronoi algorithm is introduced\nimplementing balanced graph space partitions of the fused topological maps.\nTheoretical guarantees are provided for distributed consensus convergence and\nequitable graph space partitions with constant bounds.\n  A local planner optimizes the visitation sequence of exploration targets\nwithin the balanced partitioned graph space to minimize travel distance, while\ngenerating safe, smooth, and dynamically feasible motion trajectories.\n  Comprehensive benchmarking against state-of-the-art methods demonstrates\nsignificant improvements in exploration efficiency, completeness, and workload\nbalance across the robot team.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u591a\u673a\u5668\u4eba\u81ea\u4e3b\u5728\u7ebf\u63a2\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u62d3\u6251\u56feVoronoi\u7b97\u6cd5\u5b9e\u73b0\u5e73\u8861\u7684\u533a\u57df\u5212\u5206\u548c\u4efb\u52a1\u5206\u914d\uff0c\u5728\u969c\u788d\u5bc6\u96c6\u7684\u975e\u51f8\u73af\u5883\u4e2d\u63d0\u9ad8\u63a2\u7d22\u6548\u7387\u548c\u8d1f\u8f7d\u5747\u8861\u3002", "motivation": "\u89e3\u51b3\u591a\u673a\u5668\u4eba\u5728\u969c\u788d\u5bc6\u96c6\u975e\u51f8\u73af\u5883\u4e2d\u7684\u534f\u4f5c\u63a2\u7d22\u95ee\u9898\uff0c\u7279\u522b\u5173\u6ce8\u5206\u5e03\u5f0f\u63a2\u7d22\u89c4\u5212\u4ee5\u5b9e\u73b0\u52a8\u6001\u5e73\u8861\u7684\u533a\u57df\u5212\u5206\u548c\u4efb\u52a1\u5206\u914d\u3002", "method": "\u4f7f\u7528\u65b0\u9896\u7684\u62d3\u6251\u5730\u56fe\u7ed3\u6784\u8868\u5f81\u7a7a\u95f4\u8fde\u901a\u6027\u548c\u5168\u5c40\u63a2\u7d22\u5b8c\u6574\u6027\uff0c\u63d0\u51fa\u5206\u5e03\u5f0f\u52a0\u6743\u62d3\u6251\u56feVoronoi\u7b97\u6cd5\u8fdb\u884c\u5e73\u8861\u56fe\u7a7a\u95f4\u5212\u5206\uff0c\u5e76\u7ed3\u5408\u5c40\u90e8\u89c4\u5212\u5668\u4f18\u5316\u63a2\u7d22\u76ee\u6807\u8bbf\u95ee\u5e8f\u5217\u3002", "result": "\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728\u63a2\u7d22\u6548\u7387\u3001\u5b8c\u6574\u6027\u548c\u673a\u5668\u4eba\u56e2\u961f\u5de5\u4f5c\u8d1f\u8f7d\u5e73\u8861\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u591a\u673a\u5668\u4eba\u534f\u4f5c\u63a2\u7d22\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u7684\u5206\u5e03\u5f0f\u5171\u8bc6\u6536\u655b\u548c\u6052\u5b9a\u8fb9\u754c\u7684\u516c\u5e73\u56fe\u7a7a\u95f4\u5212\u5206\uff0c\u80fd\u591f\u751f\u6210\u5b89\u5168\u3001\u5e73\u6ed1\u4e14\u52a8\u6001\u53ef\u884c\u7684\u8fd0\u52a8\u8f68\u8ff9\u3002"}}
{"id": "2510.23925", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23925", "abs": "https://arxiv.org/abs/2510.23925", "authors": ["Guohao Sun", "Hang Hua", "Jian Wang", "Jiebo Luo", "Sohail Dianat", "Majid Rabbani", "Raghuveer Rao", "Zhiqiang Tao"], "title": "Latent Chain-of-Thought for Visual Reasoning", "comment": "NeurIPS 2025", "summary": "Chain-of-thought (CoT) reasoning is critical for improving the\ninterpretability and reliability of Large Vision-Language Models (LVLMs).\nHowever, existing training algorithms such as SFT, PPO, and GRPO may not\ngeneralize well across unseen reasoning tasks and heavily rely on a biased\nreward model. To address this challenge, we reformulate reasoning in LVLMs as\nposterior inference and propose a scalable training algorithm based on\namortized variational inference. By leveraging diversity-seeking reinforcement\nlearning algorithms, we introduce a novel sparse reward function for\ntoken-level learning signals that encourage diverse, high-likelihood latent\nCoT, overcoming deterministic sampling limitations and avoiding reward hacking.\nAdditionally, we implement a Bayesian inference-scaling strategy that replaces\ncostly Best-of-N and Beam Search with a marginal likelihood to efficiently rank\noptimal rationales and answers. We empirically demonstrate that the proposed\nmethod enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in\nterms of effectiveness, generalization, and interpretability.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u644a\u9500\u53d8\u5206\u63a8\u7406\u7684\u53ef\u6269\u5c55\u8bad\u7ec3\u7b97\u6cd5\uff0c\u901a\u8fc7\u591a\u6837\u6027\u5f3a\u5316\u5b66\u4e60\u548c\u8d1d\u53f6\u65af\u63a8\u7406\u7f29\u653e\u7b56\u7565\uff0c\u63d0\u5347\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3001\u6cdb\u5316\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u7684SFT\u3001PPO\u548cGRPO\u8bad\u7ec3\u7b97\u6cd5\u5728\u672a\u89c1\u63a8\u7406\u4efb\u52a1\u4e0a\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u4e14\u8fc7\u5ea6\u4f9d\u8d56\u6709\u504f\u7684\u5956\u52b1\u6a21\u578b\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u63a8\u7406\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u5c06LVLM\u63a8\u7406\u91cd\u65b0\u8868\u8ff0\u4e3a\u540e\u9a8c\u63a8\u65ad\uff0c\u91c7\u7528\u644a\u9500\u53d8\u5206\u63a8\u7406\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u6837\u6027\u5f3a\u5316\u5b66\u4e60\u8bbe\u8ba1\u7a00\u758f\u5956\u52b1\u51fd\u6570\uff0c\u5e76\u4f7f\u7528\u8d1d\u53f6\u65af\u63a8\u7406\u7f29\u653e\u7b56\u7565\u66ff\u4ee3\u6602\u8d35\u7684Best-of-N\u548cBeam Search\u3002", "result": "\u5728\u4e03\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6700\u5148\u8fdbLVLM\u6a21\u578b\u7684\u6709\u6548\u6027\u3001\u6cdb\u5316\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u53d8\u5206\u63a8\u7406\u7684\u8bad\u7ec3\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3aLVLM\u7684\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u9760\u7684\u8bad\u7ec3\u65b9\u6848\u3002"}}
{"id": "2510.24370", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.24370", "abs": "https://arxiv.org/abs/2510.24370", "authors": ["Yue Wu"], "title": "Mechanism-Guided Residual Lifting and Control Consistent Modeling for Pneumatic Drying Processes", "comment": "6figs,4tables", "summary": "Pneumatic drying processes in industries such as agriculture, chemicals,and\npharmaceuticals are notoriously difficult to model and control due to\nmulti-source disturbances,coupled stage dynamics, and significant measurement\ndelays. Traditional modeling paradigms often fail to simultaneously deliver\naccuracy, interpretability, and closed-loop applicability. To address this\nchallenge, this paper introduces a unified hybrid modeling framework, termed\nPhysics-Guided Residual Lifting with Control-Consistent Correction,which\nintegrates a transient mechanistic model with a stability-constrained\ndata-driven component. The framework covers the complete process chain of\ndrying, transport, and winnowing. On the mechanistic level, the model unifies\nmass transfer dynamics using the partial pressure difference of water vapor,\nincorporates water activity clamping and latent heat corrections for bound\nwater, and ensures energy closure with moisture-dependent specific heat. On the\ndata-driven level,we propose an orthogonal residual learning scheme. It\nleverages intermediate states from the mechanistic model as proxy variables to\nconstruct a physics-inspired dictionary, preventing parameter compensation and\noverfitting during ridge regression. Furthermore, to ensure suitability for\npredictive control, a Control-Consistent Extended Dynamic Mode Decomposition\nwith stability constraints is employed to learn the residual dynamics, for\nwhich we provide boundedness proofs and stability guarantees. The framework was\nvalidated on 10 industrial batches, comprising 63,000 samples. On unseen test\ndata, the hybrid model achieved a Mean Absolute Error of 0.016% for outlet\nmoisture and 0.015 {\\deg}C for outlet temperature, with values improving to\n0.986 and 0.995, respectively. The resulting prediction residuals exhibit\nwhite-noise characteristics, with significantly reduced spectral energy at low\nfrequencies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7269\u7406\u5f15\u5bfc\u7684\u6b8b\u5dee\u63d0\u5347\u4e0e\u63a7\u5236\u4e00\u81f4\u6821\u6b63\u7684\u7edf\u4e00\u6df7\u5408\u5efa\u6a21\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u6c14\u52a8\u5e72\u71e5\u8fc7\u7a0b\u7684\u5efa\u6a21\u548c\u63a7\u5236\u96be\u9898\uff0c\u5728\u5de5\u4e1a\u6279\u6b21\u9a8c\u8bc1\u4e2d\u53d6\u5f97\u4e86\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u5de5\u4e1a\u6c14\u52a8\u5e72\u71e5\u8fc7\u7a0b\u56e0\u591a\u6e90\u5e72\u6270\u3001\u8026\u5408\u9636\u6bb5\u52a8\u6001\u548c\u663e\u8457\u6d4b\u91cf\u5ef6\u8fdf\u800c\u96be\u4ee5\u5efa\u6a21\u548c\u63a7\u5236\uff0c\u4f20\u7edf\u5efa\u6a21\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u6ee1\u8db3\u7cbe\u5ea6\u3001\u53ef\u89e3\u91ca\u6027\u548c\u95ed\u73af\u9002\u7528\u6027\u8981\u6c42\u3002", "method": "\u7ed3\u5408\u77ac\u6001\u673a\u7406\u6a21\u578b\u4e0e\u7a33\u5b9a\u6027\u7ea6\u675f\u7684\u6570\u636e\u9a71\u52a8\u7ec4\u4ef6\uff0c\u91c7\u7528\u7269\u7406\u5f15\u5bfc\u7684\u6b8b\u5dee\u5b66\u4e60\u65b9\u6848\uff0c\u5229\u7528\u673a\u7406\u6a21\u578b\u4e2d\u95f4\u72b6\u6001\u6784\u5efa\u7269\u7406\u542f\u53d1\u5b57\u5178\uff0c\u5e76\u901a\u8fc7\u63a7\u5236\u4e00\u81f4\u6269\u5c55\u52a8\u6001\u6a21\u6001\u5206\u89e3\u5b66\u4e60\u6b8b\u5dee\u52a8\u6001\u3002", "result": "\u572810\u4e2a\u5de5\u4e1a\u6279\u6b2163000\u4e2a\u6837\u672c\u4e0a\u9a8c\u8bc1\uff0c\u5bf9\u672a\u89c1\u6d4b\u8bd5\u6570\u636e\uff0c\u51fa\u53e3\u6c34\u5206\u548c\u6e29\u5ea6\u7684\u5747\u65b9\u7edd\u5bf9\u8bef\u5dee\u5206\u522b\u4e3a0.016%\u548c0.015\u00b0C\uff0c\u9884\u6d4b\u6b8b\u5dee\u5448\u73b0\u767d\u566a\u58f0\u7279\u6027\uff0c\u4f4e\u9891\u8c31\u80fd\u91cf\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u8be5\u6df7\u5408\u5efa\u6a21\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u6c14\u52a8\u5e72\u71e5\u8fc7\u7a0b\u7684\u5efa\u6a21\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u548c\u7a33\u5b9a\u6027\u4fdd\u8bc1\uff0c\u9002\u7528\u4e8e\u9884\u6d4b\u63a7\u5236\u5e94\u7528\u3002"}}
{"id": "2510.24069", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24069", "abs": "https://arxiv.org/abs/2510.24069", "authors": ["Sangmin Kim", "Hajun Kim", "Gijeong Kim", "Min-Gyu Kim", "Hae-Won Park"], "title": "Dynamically-Consistent Trajectory Optimization for Legged Robots via Contact Point Decomposition", "comment": "8 pages, 4 figures, IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT\n  VERSION. ACCEPTED OCTOBER, 2025", "summary": "To generate reliable motion for legged robots through trajectory\noptimization, it is crucial to simultaneously compute the robot's path and\ncontact sequence, as well as accurately consider the dynamics in the problem\nformulation. In this paper, we present a phase-based trajectory optimization\nthat ensures the feasibility of translational dynamics and friction cone\nconstraints throughout the entire trajectory. Specifically, our approach\nleverages the superposition properties of linear differential equations to\ndecouple the translational dynamics for each contact point, which operates\nunder different phase sequences. Furthermore, we utilize the differentiation\nmatrix of B{\\'e}zier polynomials to derive an analytical relationship between\nthe robot's position and force, thereby ensuring the consistent satisfaction of\ntranslational dynamics. Additionally, by exploiting the convex closure property\nof B{\\'e}zier polynomials, our method ensures compliance with friction cone\nconstraints. Using the aforementioned approach, the proposed trajectory\noptimization framework can generate dynamically reliable motions with various\ngait sequences for legged robots. We validate our framework using a quadruped\nrobot model, focusing on the feasibility of dynamics and motion generation.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u76f8\u4f4d\u7684\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\uff0c\u786e\u4fdd\u8db3\u5f0f\u673a\u5668\u4eba\u5728\u6574\u4e2a\u8f68\u8ff9\u4e2d\u6ee1\u8db3\u5e73\u79fb\u52a8\u529b\u5b66\u548c\u6469\u64e6\u9525\u7ea6\u675f\uff0c\u901a\u8fc7\u8d1d\u585e\u5c14\u591a\u9879\u5f0f\u7684\u6027\u8d28\u5b9e\u73b0\u52a8\u6001\u53ef\u9760\u7684\u8fd0\u52a8\u751f\u6210\u3002", "motivation": "\u4e3a\u4e86\u901a\u8fc7\u8f68\u8ff9\u4f18\u5316\u4e3a\u8db3\u5f0f\u673a\u5668\u4eba\u751f\u6210\u53ef\u9760\u8fd0\u52a8\uff0c\u9700\u8981\u540c\u65f6\u8ba1\u7b97\u673a\u5668\u4eba\u8def\u5f84\u548c\u63a5\u89e6\u5e8f\u5217\uff0c\u5e76\u5728\u95ee\u9898\u8868\u8ff0\u4e2d\u51c6\u786e\u8003\u8651\u52a8\u529b\u5b66\u7ea6\u675f\u3002", "method": "\u5229\u7528\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b\u7684\u53e0\u52a0\u7279\u6027\u89e3\u8026\u5404\u63a5\u89e6\u70b9\u7684\u5e73\u79fb\u52a8\u529b\u5b66\uff0c\u4f7f\u7528\u8d1d\u585e\u5c14\u591a\u9879\u5f0f\u7684\u5fae\u5206\u77e9\u9635\u63a8\u5bfc\u4f4d\u7f6e\u4e0e\u529b\u7684\u89e3\u6790\u5173\u7cfb\uff0c\u5e76\u5229\u7528\u8d1d\u585e\u5c14\u591a\u9879\u5f0f\u7684\u51f8\u5305\u6027\u8d28\u786e\u4fdd\u6469\u64e6\u9525\u7ea6\u675f\u3002", "result": "\u8be5\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\u80fd\u591f\u4e3a\u8db3\u5f0f\u673a\u5668\u4eba\u751f\u6210\u5177\u6709\u5404\u79cd\u6b65\u6001\u5e8f\u5217\u7684\u52a8\u6001\u53ef\u9760\u8fd0\u52a8\uff0c\u5728\u56db\u8db3\u673a\u5668\u4eba\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u52a8\u529b\u5b66\u548c\u8fd0\u52a8\u751f\u6210\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u76f8\u4f4d\u7684\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u786e\u4fdd\u8db3\u5f0f\u673a\u5668\u4eba\u8fd0\u52a8\u8f68\u8ff9\u7684\u52a8\u529b\u5b66\u53ef\u884c\u6027\u548c\u6469\u64e6\u9525\u7ea6\u675f\u6ee1\u8db3\uff0c\u4e3a\u53ef\u9760\u8fd0\u52a8\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.23942", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23942", "abs": "https://arxiv.org/abs/2510.23942", "authors": ["Sridhar Mahadevan"], "title": "Decentralized Causal Discovery using Judo Calculus", "comment": "54 pages", "summary": "We describe a theory and implementation of an intuitionistic decentralized\nframework for causal discovery using judo calculus, which is formally defined\nas j-stable causal inference using j-do-calculus in a topos of sheaves. In\nreal-world applications -- from biology to medicine and social science --\ncausal effects depend on regime (age, country, dose, genotype, or lab\nprotocol). Our proposed judo calculus formalizes this context dependence\nformally as local truth: a causal claim is proven true on a cover of regimes,\nnot everywhere at once. The Lawvere-Tierney modal operator j chooses which\nregimes are relevant; j-stability means the claim holds constructively and\nconsistently across that family. We describe an algorithmic and implementation\nframework for judo calculus, combining it with standard score-based,\nconstraint-based, and gradient-based causal discovery methods. We describe\nexperimental results on a range of domains, from synthetic to real-world\ndatasets from biology and economics. Our experimental results show the\ncomputational efficiency gained by the decentralized nature of sheaf-theoretic\ncausal discovery, as well as improved performance over classical causal\ndiscovery methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f4\u89c9\u4e3b\u4e49\u53bb\u4e2d\u5fc3\u5316\u6846\u67b6\u7684\u56e0\u679c\u53d1\u73b0\u7406\u8bba\u2014\u2014\u67d4\u9053\u6f14\u7b97\uff0c\u4f7f\u7528j-\u7a33\u5b9a\u56e0\u679c\u63a8\u65ad\u548cj-do\u6f14\u7b97\u5728\u5c42\u62d3\u6251\u4e2d\u5f62\u5f0f\u5316\u5904\u7406\u56e0\u679c\u6548\u5e94\u7684\u60c5\u5883\u4f9d\u8d56\u6027\u3002", "motivation": "\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\uff0c\u56e0\u679c\u6548\u5e94\u5f80\u5f80\u4f9d\u8d56\u4e8e\u5177\u4f53\u60c5\u5883\uff08\u5982\u5e74\u9f84\u3001\u56fd\u5bb6\u3001\u5242\u91cf\u3001\u57fa\u56e0\u578b\u7b49\uff09\uff0c\u9700\u8981\u5f62\u5f0f\u5316\u5904\u7406\u8fd9\u79cd\u60c5\u5883\u4f9d\u8d56\u6027\u3002", "method": "\u4f7f\u7528\u67d4\u9053\u6f14\u7b97\uff0c\u901a\u8fc7Lawvere-Tierney\u6a21\u6001\u7b97\u5b50j\u9009\u62e9\u76f8\u5173\u60c5\u5883\uff0c\u7ed3\u5408\u6807\u51c6\u7684\u57fa\u4e8e\u5206\u6570\u3001\u7ea6\u675f\u548c\u68af\u5ea6\u7684\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u57fa\u4e8e\u5c42\u7406\u8bba\u7684\u53bb\u4e2d\u5fc3\u5316\u56e0\u679c\u53d1\u73b0\u5177\u6709\u8ba1\u7b97\u6548\u7387\u4f18\u52bf\uff0c\u4e14\u6027\u80fd\u4f18\u4e8e\u7ecf\u5178\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u3002", "conclusion": "\u67d4\u9053\u6f14\u7b97\u4e3a\u5904\u7406\u60c5\u5883\u4f9d\u8d56\u7684\u56e0\u679c\u53d1\u73b0\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u4f18\u52bf\u3002"}}
{"id": "2510.24389", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.24389", "abs": "https://arxiv.org/abs/2510.24389", "authors": ["Lamine Chalal", "Ahmed Rachid"], "title": "Development of a Digital Twin for an Electric Vehicle Emulator Modeling, Control, and Experimental Validation", "comment": "6 pages, Accepted at CODIT 2025 (Conference on Decision and Control\n  in Intelligent Technology)", "summary": "This paper presents the development and validation of a digital twin for a\nscaled-down electric vehicle (EV) emulator, designed to replicate longitudinal\nvehicle dynamics under diverse operating conditions. The emulator integrates a\nseparately excited DC motor (SEDCM), a four-quadrant DC-DC converter, a battery\nemulator, and a mechanical load emulator. The system models tractive effort,\naerodynamic drag, and gradient resistance using Newton's second law. In\ncontrast to conventional graphical modeling tools (e.g., block diagrams and\nbond graphs), the adopted Energetic Macroscopic Representation (EMR) framework\noffers clear advantages by explicitly representing energy interactions and\nfacilitating the systematic derivation of control structures. A control\nstrategy developed within this framework governs energy flow across the\npowertrain, enabling accurate speed control via armature voltage regulation.\nExperimental tests conducted on a Lucas-Nulle test bench show strong\ncorrelation with simulation results. The study also introduces a methodology to\ncompute the maximum admissible vehicle mass - determined to be 13.5 kg for a\n180 W motor operating at 1900 rpm - based on acceleration and slope\nconstraints. Furthermore, a switching algorithm for the bidirectional converter\nensures reliable four quadrant operation. Overall, the proposed framework\nprovides a scalable and effective approach for EV emulation, control design,\nand energy management validation.", "AI": {"tldr": "\u5f00\u53d1\u5e76\u9a8c\u8bc1\u4e86\u4e00\u4e2a\u7528\u4e8e\u7535\u52a8\u6c7d\u8f66\u6a21\u62df\u5668\u7684\u6570\u5b57\u5b6a\u751f\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u91c7\u7528Energetic Macroscopic Representation (EMR)\u6846\u67b6\uff0c\u80fd\u591f\u51c6\u786e\u6a21\u62df\u8f66\u8f86\u7eb5\u5411\u52a8\u529b\u5b66\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u56fe\u5f62\u5316\u5efa\u6a21\u5de5\u5177\uff08\u5982\u6846\u56fe\uff09\u5728\u8868\u793a\u80fd\u91cf\u4ea4\u4e92\u548c\u63a8\u5bfc\u63a7\u5236\u7ed3\u6784\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6e05\u6670\u7684\u65b9\u6cd5\u6765\u6a21\u62df\u7535\u52a8\u6c7d\u8f66\u5728\u4e0d\u540c\u5de5\u51b5\u4e0b\u7684\u52a8\u529b\u5b66\u7279\u6027\u3002", "method": "\u4f7f\u7528EMR\u6846\u67b6\u6784\u5efa\u6570\u5b57\u5b6a\u751f\u7cfb\u7edf\uff0c\u96c6\u6210\u5206\u79bb\u52b1\u78c1\u76f4\u6d41\u7535\u673a\u3001\u56db\u8c61\u9650DC-DC\u53d8\u6362\u5668\u3001\u7535\u6c60\u6a21\u62df\u5668\u548c\u673a\u68b0\u8d1f\u8f7d\u6a21\u62df\u5668\uff0c\u57fa\u4e8e\u725b\u987f\u7b2c\u4e8c\u5b9a\u5f8b\u5efa\u6a21\u7275\u5f15\u529b\u3001\u7a7a\u6c14\u963b\u529b\u548c\u5761\u5ea6\u963b\u529b\u3002", "result": "\u5b9e\u9a8c\u6d4b\u8bd5\u663e\u793a\u6a21\u62df\u7ed3\u679c\u4e0e\u5b9e\u9645\u6d4b\u91cf\u9ad8\u5ea6\u4e00\u81f4\uff0c\u786e\u5b9a\u4e86180W\u7535\u673a\u57281900rpm\u4e0b\u7684\u6700\u5927\u5141\u8bb8\u8f66\u8f86\u8d28\u91cf\u4e3a13.5kg\uff0c\u5e76\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u56db\u8c61\u9650\u8fd0\u884c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u7535\u52a8\u6c7d\u8f66\u4eff\u771f\u3001\u63a7\u5236\u8bbe\u8ba1\u548c\u80fd\u91cf\u7ba1\u7406\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.24108", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24108", "abs": "https://arxiv.org/abs/2510.24108", "authors": ["Zhenxin Li", "Wenhao Yao", "Zi Wang", "Xinglong Sun", "Jingde Chen", "Nadine Chang", "Maying Shen", "Jingyu Song", "Zuxuan Wu", "Shiyi Lan", "Jose M. Alvarez"], "title": "ZTRS: Zero-Imitation End-to-end Autonomous Driving with Trajectory Scoring", "comment": null, "summary": "End-to-end autonomous driving maps raw sensor inputs directly into\nego-vehicle trajectories to avoid cascading errors from perception modules and\nto leverage rich semantic cues. Existing frameworks largely rely on Imitation\nLearning (IL), which can be limited by sub-optimal expert demonstrations and\ncovariate shift during deployment. On the other hand, Reinforcement Learning\n(RL) has recently shown potential in scaling up with simulations, but is\ntypically confined to low-dimensional symbolic inputs (e.g. 3D objects and\nmaps), falling short of full end-to-end learning from raw sensor data. We\nintroduce ZTRS (Zero-Imitation End-to-End Autonomous Driving with Trajectory\nScoring), a framework that combines the strengths of both worlds: sensor inputs\nwithout losing information and RL training for robust planning. To the best of\nour knowledge, ZTRS is the first framework that eliminates IL entirely by only\nlearning from rewards while operating directly on high-dimensional sensor data.\nZTRS utilizes offline reinforcement learning with our proposed Exhaustive\nPolicy Optimization (EPO), a variant of policy gradient tailored for enumerable\nactions and rewards. ZTRS demonstrates strong performance across three\nbenchmarks: Navtest (generic real-world open-loop planning), Navhard (open-loop\nplanning in challenging real-world and synthetic scenarios), and HUGSIM\n(simulated closed-loop driving). Specifically, ZTRS achieves the\nstate-of-the-art result on Navhard and outperforms IL-based baselines on\nHUGSIM. Code will be available at https://github.com/woxihuanjiangguo/ZTRS.", "AI": {"tldr": "ZTRS\u662f\u9996\u4e2a\u5b8c\u5168\u6d88\u9664\u6a21\u4eff\u5b66\u4e60\u3001\u4ec5\u4ece\u5956\u52b1\u4e2d\u5b66\u4e60\u5e76\u76f4\u63a5\u5904\u7406\u9ad8\u7ef4\u4f20\u611f\u5668\u6570\u636e\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u548c\u8f68\u8ff9\u8bc4\u5206\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\u4e3b\u8981\u4f9d\u8d56\u6a21\u4eff\u5b66\u4e60\uff0c\u4f46\u53d7\u5230\u6b21\u4f18\u4e13\u5bb6\u6f14\u793a\u548c\u90e8\u7f72\u65f6\u534f\u53d8\u91cf\u504f\u79fb\u7684\u9650\u5236\uff1b\u800c\u5f3a\u5316\u5b66\u4e60\u901a\u5e38\u5c40\u9650\u4e8e\u4f4e\u7ef4\u7b26\u53f7\u8f93\u5165\uff0c\u65e0\u6cd5\u5b9e\u73b0\u4ece\u539f\u59cb\u4f20\u611f\u5668\u6570\u636e\u7684\u5b8c\u6574\u7aef\u5230\u7aef\u5b66\u4e60\u3002", "method": "\u63d0\u51faZTRS\u6846\u67b6\uff0c\u7ed3\u5408\u4f20\u611f\u5668\u8f93\u5165\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u4f7f\u7528\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u548c\u63d0\u51fa\u7684\u8be6\u5c3d\u7b56\u7565\u4f18\u5316(EPO)\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u9488\u5bf9\u53ef\u679a\u4e3e\u52a8\u4f5c\u548c\u5956\u52b1\u7684\u7b56\u7565\u68af\u5ea6\u53d8\u4f53\u3002", "result": "ZTRS\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u5f3a\u52b2\uff1aNavtest(\u901a\u7528\u73b0\u5b9e\u4e16\u754c\u5f00\u73af\u89c4\u5212)\u3001Navhard(\u6311\u6218\u6027\u73b0\u5b9e\u4e16\u754c\u548c\u5408\u6210\u573a\u666f\u4e2d\u7684\u5f00\u73af\u89c4\u5212)\u548cHUGSIM(\u6a21\u62df\u95ed\u73af\u9a7e\u9a76)\uff0c\u5728Navhard\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7ed3\u679c\uff0c\u5728HUGSIM\u4e0a\u4f18\u4e8e\u57fa\u4e8e\u6a21\u4eff\u5b66\u4e60\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ZTRS\u6210\u529f\u5c55\u793a\u4e86\u65e0\u9700\u6a21\u4eff\u5b66\u4e60\u3001\u4ec5\u4ece\u5956\u52b1\u4e2d\u5b66\u4e60\u5e76\u76f4\u63a5\u5904\u7406\u9ad8\u7ef4\u4f20\u611f\u5668\u6570\u636e\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u66f4\u9c81\u68d2\u7684\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2510.23965", "categories": ["cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.23965", "abs": "https://arxiv.org/abs/2510.23965", "authors": ["Aymane El Gadarri", "Ali Aouad", "Vivek F. Farias"], "title": "The Sign Estimator: LLM Alignment in the Face of Choice Heterogeneity", "comment": null, "summary": "Traditional LLM alignment methods are vulnerable to heterogeneity in human\npreferences. Fitting a na\\\"ive probabilistic model to pairwise comparison data\n(say over prompt-completion pairs) yields an inconsistent estimate of the\npopulation-average utility -a canonical measure of social welfare. We propose a\nnew method, dubbed the sign estimator, that provides a simple, provably\nconsistent, and efficient estimator by replacing cross-entropy with binary\nclassification loss in the aggregation step. This simple modification recovers\nconsistent ordinal alignment under mild assumptions and achieves the first\npolynomial finite-sample error bounds in this setting. In realistic simulations\nof LLM alignment using digital twins, the sign estimator substantially reduces\npreference distortion over a panel of simulated personas, cutting (angular)\nestimation error by nearly 35% and decreasing disagreement with true population\npreferences from 12% to 8% compared to standard RLHF. Our method also compares\nfavorably to panel data heuristics that explicitly model user heterogeneity and\nrequire tracking individual-level preference data-all while maintaining the\nimplementation simplicity of existing LLM alignment pipelines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3asign estimator\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u4ea4\u53c9\u71b5\u635f\u5931\u66ff\u6362\u4e3a\u4e8c\u5143\u5206\u7c7b\u635f\u5931\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfLLM\u5bf9\u9f50\u65b9\u6cd5\u5728\u4eba\u7c7b\u504f\u597d\u5f02\u8d28\u6027\u4e0b\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u53ef\u8bc1\u660e\u7684\u4e00\u81f4\u6027\u548c\u9ad8\u6548\u4f30\u8ba1\u3002", "motivation": "\u4f20\u7edfLLM\u5bf9\u9f50\u65b9\u6cd5\u5bf9\u4eba\u7c7b\u504f\u597d\u7684\u5f02\u8d28\u6027\u5f88\u654f\u611f\uff0c\u4f7f\u7528\u6734\u7d20\u6982\u7387\u6a21\u578b\u62df\u5408\u6210\u5bf9\u6bd4\u8f83\u6570\u636e\u4f1a\u4ea7\u751f\u4e0d\u4e00\u81f4\u7684\u7fa4\u4f53\u5e73\u5747\u6548\u7528\u4f30\u8ba1\uff0c\u8fd9\u662f\u793e\u4f1a\u798f\u5229\u7684\u89c4\u8303\u8861\u91cf\u6807\u51c6\u3002", "method": "\u63d0\u51fasign estimator\u65b9\u6cd5\uff0c\u5728\u805a\u5408\u6b65\u9aa4\u4e2d\u7528\u4e8c\u5143\u5206\u7c7b\u635f\u5931\u66ff\u6362\u4ea4\u53c9\u71b5\u635f\u5931\uff0c\u5728\u6e29\u548c\u5047\u8bbe\u4e0b\u6062\u590d\u4e00\u81f4\u7684\u6709\u5e8f\u5bf9\u9f50\uff0c\u5e76\u5728\u6b64\u8bbe\u7f6e\u4e2d\u9996\u6b21\u5b9e\u73b0\u591a\u9879\u5f0f\u6709\u9650\u6837\u672c\u8bef\u5dee\u754c\u9650\u3002", "result": "\u5728\u57fa\u4e8e\u6570\u5b57\u5b6a\u751f\u7684LLM\u5bf9\u9f50\u73b0\u5b9e\u6a21\u62df\u4e2d\uff0csign estimator\u663e\u8457\u51cf\u5c11\u4e86\u6a21\u62df\u4eba\u7269\u9762\u677f\u7684\u504f\u597d\u5931\u771f\uff0c\u5c06\u4f30\u8ba1\u8bef\u5dee\u964d\u4f4e\u4e86\u8fd135%\uff0c\u4e0e\u771f\u5b9e\u7fa4\u4f53\u504f\u597d\u7684\u4e0d\u4e00\u81f4\u6027\u4ece12%\u964d\u81f38%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4f18\u4e8e\u660e\u786e\u5efa\u6a21\u7528\u6237\u5f02\u8d28\u6027\u5e76\u9700\u8981\u8ddf\u8e2a\u4e2a\u4f53\u7ea7\u504f\u597d\u6570\u636e\u7684\u9762\u677f\u6570\u636e\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u73b0\u6709LLM\u5bf9\u9f50\u7ba1\u9053\u7684\u5b9e\u73b0\u7b80\u5355\u6027\u3002"}}
{"id": "2510.24391", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.24391", "abs": "https://arxiv.org/abs/2510.24391", "authors": ["Alvaro Detailleur", "Dalim Wahby", "Guillaume Ducard", "Christopher Onder"], "title": "Contributions to Semialgebraic-Set-Based Stability Verification of Dynamical Systems with Neural-Network-Based Controllers", "comment": "Submitted to the IEEE for possible publication, 16 pages, 6 figures", "summary": "Neural-network-based controllers (NNCs) can represent complex, highly\nnonlinear control laws, but verifying the closed-loop stability of dynamical\nsystems using them remains challenging. This work presents contributions to a\nstate-of-the-art stability verification procedure for NNC-controlled systems\nwhich relies on semialgebraic-set-based input-output modeling to pose the\nsearch for a Lyapunov function as an optimization problem. Specifically, this\nprocedure's conservatism when analyzing NNCs using transcendental activation\nfunctions and the restriction to feedforward NNCs are addressed by a)\nintroducing novel semialgebraic activation functions that preserve key\nproperties of common transcendental activations and b) proving compatibility of\nNNCs from the broader class of recurrent equilibrium networks (RENs) with this\nprocedure. Furthermore, the indirect optimization of a local region of\nattraction (RoA) estimate using a restricted set of candidate Lyapunov\nfunctions is greatly improved via c) the introduction of a richer\nparameterization of candidate Lyapunov functions than previously reported and\nd) the formulation of novel semidefinite programs (SDPs) that directly optimize\nthe resulting RoA estimate. The value of these contributions is highlighted in\ntwo numerical examples.", "AI": {"tldr": "\u672c\u6587\u6539\u8fdb\u4e86\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u63a7\u5236\u5668(NNCs)\u7684\u95ed\u73af\u7a33\u5b9a\u6027\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u534a\u4ee3\u6570\u6fc0\u6d3b\u51fd\u6570\u3001\u6269\u5c55\u652f\u6301\u9012\u5f52\u5e73\u8861\u7f51\u7edc(RENs)\u3001\u4e30\u5bccLyapunov\u51fd\u6570\u53c2\u6570\u5316\u4ee5\u53ca\u76f4\u63a5\u4f18\u5316\u5438\u5f15\u57df\u4f30\u8ba1\uff0c\u964d\u4f4e\u4e86\u9a8c\u8bc1\u7684\u4fdd\u5b88\u6027\u3002", "motivation": "\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u5668\u80fd\u591f\u8868\u793a\u590d\u6742\u7684\u975e\u7ebf\u6027\u63a7\u5236\u5f8b\uff0c\u4f46\u9a8c\u8bc1\u5176\u95ed\u73af\u7a33\u5b9a\u6027\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u57fa\u4e8e\u534a\u4ee3\u6570\u96c6\u8f93\u5165\u8f93\u51fa\u5efa\u6a21\u7684\u7a33\u5b9a\u6027\u9a8c\u8bc1\u65b9\u6cd5\u5728\u5904\u7406\u8d85\u8d8a\u6fc0\u6d3b\u51fd\u6570\u65f6\u5b58\u5728\u4fdd\u5b88\u6027\uff0c\u4e14\u4ec5\u9650\u4e8e\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u3002", "method": "1) \u5f15\u5165\u65b0\u578b\u534a\u4ee3\u6570\u6fc0\u6d3b\u51fd\u6570\uff0c\u4fdd\u7559\u5e38\u89c1\u8d85\u8d8a\u6fc0\u6d3b\u51fd\u6570\u7684\u5173\u952e\u7279\u6027\uff1b2) \u8bc1\u660e\u66f4\u5e7f\u6cdb\u7684\u9012\u5f52\u5e73\u8861\u7f51\u7edc(RENs)\u4e0e\u6b64\u9a8c\u8bc1\u7a0b\u5e8f\u7684\u517c\u5bb9\u6027\uff1b3) \u4f7f\u7528\u66f4\u4e30\u5bcc\u7684Lyapunov\u51fd\u6570\u53c2\u6570\u5316\uff1b4) \u5236\u5b9a\u65b0\u7684\u534a\u5b9a\u89c4\u5212(SDPs)\u76f4\u63a5\u4f18\u5316\u5438\u5f15\u57df\u4f30\u8ba1\u3002", "result": "\u901a\u8fc7\u4e24\u4e2a\u6570\u503c\u793a\u4f8b\u5c55\u793a\u4e86\u8fd9\u4e9b\u8d21\u732e\u7684\u4ef7\u503c\uff0c\u663e\u8457\u6539\u8fdb\u4e86\u5c40\u90e8\u5438\u5f15\u57df\u4f30\u8ba1\u7684\u95f4\u63a5\u4f18\u5316\u6548\u679c\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u964d\u4f4e\u4e86\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u5668\u7a33\u5b9a\u6027\u9a8c\u8bc1\u7684\u4fdd\u5b88\u6027\uff0c\u6269\u5c55\u4e86\u53ef\u9a8c\u8bc1\u7684\u7f51\u7edc\u7c7b\u578b\uff0c\u5e76\u63d0\u9ad8\u4e86\u5438\u5f15\u57df\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2510.24109", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24109", "abs": "https://arxiv.org/abs/2510.24109", "authors": ["Wenbin Ding", "Jun Chen", "Mingjia Chen", "Fei Xie", "Qi Mao", "Philip Dames"], "title": "PFEA: An LLM-based High-Level Natural Language Planning and Feedback Embodied Agent for Human-Centered AI", "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has marked a\nsignificant breakthrough in Artificial Intelligence (AI), ushering in a new era\nof Human-centered Artificial Intelligence (HAI). HAI aims to better serve human\nwelfare and needs, thereby placing higher demands on the intelligence level of\nrobots, particularly in aspects such as natural language interaction, complex\ntask planning, and execution. Intelligent agents powered by LLMs have opened up\nnew pathways for realizing HAI. However, existing LLM-based embodied agents\noften lack the ability to plan and execute complex natural language control\ntasks online. This paper explores the implementation of intelligent robotic\nmanipulating agents based on Vision-Language Models (VLMs) in the physical\nworld. We propose a novel embodied agent framework for robots, which comprises\na human-robot voice interaction module, a vision-language agent module and an\naction execution module. The vision-language agent itself includes a\nvision-based task planner, a natural language instruction converter, and a task\nperformance feedback evaluator. Experimental results demonstrate that our agent\nachieves a 28\\% higher average task success rate in both simulated and real\nenvironments compared to approaches relying solely on LLM+CLIP, significantly\nimproving the execution success rate of high-level natural language instruction\ntasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLM)\u7684\u667a\u80fd\u673a\u5668\u4eba\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u4eba\u673a\u8bed\u97f3\u4ea4\u4e92\u3001\u89c6\u89c9\u8bed\u8a00\u4ee3\u7406\u548c\u52a8\u4f5c\u6267\u884c\u6a21\u5757\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u590d\u6742\u81ea\u7136\u8bed\u8a00\u63a7\u5236\u4efb\u52a1\u7684\u6210\u529f\u7387\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u53d1\u5c55\uff0c\u4eba\u672c\u4eba\u5de5\u667a\u80fd(HAI)\u5bf9\u673a\u5668\u4eba\u7684\u667a\u80fd\u6c34\u5e73\u63d0\u51fa\u4e86\u66f4\u9ad8\u8981\u6c42\uff0c\u7279\u522b\u662f\u5728\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u3001\u590d\u6742\u4efb\u52a1\u89c4\u5212\u548c\u6267\u884c\u65b9\u9762\u3002\u73b0\u6709\u57fa\u4e8eLLM\u7684\u5177\u8eab\u4ee3\u7406\u7f3a\u4e4f\u5728\u7ebf\u89c4\u5212\u548c\u6267\u884c\u590d\u6742\u81ea\u7136\u8bed\u8a00\u63a7\u5236\u4efb\u52a1\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u4eba\u673a\u8bed\u97f3\u4ea4\u4e92\u6a21\u5757\u3001\u89c6\u89c9\u8bed\u8a00\u4ee3\u7406\u6a21\u5757\u548c\u52a8\u4f5c\u6267\u884c\u6a21\u5757\u7684\u673a\u5668\u4eba\u5177\u8eab\u4ee3\u7406\u6846\u67b6\u3002\u89c6\u89c9\u8bed\u8a00\u4ee3\u7406\u6a21\u5757\u5305\u62ec\u57fa\u4e8e\u89c6\u89c9\u7684\u4efb\u52a1\u89c4\u5212\u5668\u3001\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8f6c\u6362\u5668\u548c\u4efb\u52a1\u6027\u80fd\u53cd\u9988\u8bc4\u4f30\u5668\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u4ee3\u7406\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u76f8\u6bd4\u4ec5\u4f7f\u7528LLM+CLIP\u7684\u65b9\u6cd5\uff0c\u5e73\u5747\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad8\u4e8628%\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u7ea7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4efb\u52a1\u7684\u6267\u884c\u6210\u529f\u7387\u3002", "conclusion": "\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u673a\u5668\u4eba\u4ee3\u7406\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u590d\u6742\u81ea\u7136\u8bed\u8a00\u63a7\u5236\u4efb\u52a1\u7684\u6267\u884c\u6027\u80fd\uff0c\u4e3a\u5b9e\u73b0\u4eba\u672c\u4eba\u5de5\u667a\u80fd\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2510.23989", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23989", "abs": "https://arxiv.org/abs/2510.23989", "authors": ["Shangde Gao", "Zelin Xu", "Zhe Jiang"], "title": "Learning Individual Movement Shifts After Urban Disruptions with Social Infrastructure Reliance", "comment": null, "summary": "Shifts in individual movement patterns following disruptive events can reveal\nchanging demands for community resources. However, predicting such shifts\nbefore disruptive events remains challenging for several reasons. First,\nmeasures are lacking for individuals' heterogeneous social infrastructure\nresilience (SIR), which directly influences their movement patterns, and\ncommonly used features are often limited or unavailable at scale, e.g.,\nsociodemographic characteristics. Second, the complex interactions between\nindividual movement patterns and spatial contexts have not been sufficiently\ncaptured. Third, individual-level movement may be spatially sparse and not\nwell-suited to traditional decision-making methods for movement predictions.\nThis study incorporates individuals' SIR into a conditioned deep learning model\nto capture the complex relationships between individual movement patterns and\nlocal spatial context using large-scale, sparse individual-level data. Our\nexperiments demonstrate that incorporating individuals' SIR and spatial context\ncan enhance the model's ability to predict post-event individual movement\npatterns. The conditioned model can capture the divergent shifts in movement\npatterns among individuals who exhibit similar pre-event patterns but differ in\nSIR.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6761\u4ef6\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u6574\u5408\u4e2a\u4f53\u7684\u793e\u4f1a\u57fa\u7840\u8bbe\u65bd\u97e7\u6027(SIR)\u548c\u7a7a\u95f4\u4e0a\u4e0b\u6587\uff0c\u6765\u9884\u6d4b\u7834\u574f\u6027\u4e8b\u4ef6\u540e\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u7684\u53d8\u5316\u3002", "motivation": "\u9884\u6d4b\u7834\u574f\u6027\u4e8b\u4ef6\u524d\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u53d8\u5316\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u8861\u91cf\u4e2a\u4f53\u5f02\u8d28\u6027\u793e\u4f1a\u57fa\u7840\u8bbe\u65bd\u97e7\u6027\u7684\u65b9\u6cd5\uff0c\u4f20\u7edf\u7279\u5f81\u96be\u4ee5\u5927\u89c4\u6a21\u83b7\u53d6\uff0c\u4e14\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u4e0e\u7a7a\u95f4\u4e0a\u4e0b\u6587\u7684\u590d\u6742\u4ea4\u4e92\u5173\u7cfb\u672a\u88ab\u5145\u5206\u6355\u6349\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6761\u4ef6\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u6574\u5408\u4e2a\u4f53\u7684\u793e\u4f1a\u57fa\u7840\u8bbe\u65bd\u97e7\u6027(SIR)\u548c\u5c40\u90e8\u7a7a\u95f4\u4e0a\u4e0b\u6587\uff0c\u4f7f\u7528\u5927\u89c4\u6a21\u7a00\u758f\u7684\u4e2a\u4f53\u7ea7\u6570\u636e\u6765\u6355\u6349\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u4e0e\u7a7a\u95f4\u73af\u5883\u7684\u590d\u6742\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6574\u5408\u4e2a\u4f53\u7684SIR\u548c\u7a7a\u95f4\u4e0a\u4e0b\u6587\u80fd\u591f\u589e\u5f3a\u6a21\u578b\u9884\u6d4b\u4e8b\u4ef6\u540e\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u7684\u80fd\u529b\u3002\u6761\u4ef6\u6a21\u578b\u80fd\u591f\u6355\u6349\u5230\u5177\u6709\u76f8\u4f3c\u4e8b\u4ef6\u524d\u79fb\u52a8\u6a21\u5f0f\u4f46SIR\u4e0d\u540c\u7684\u4e2a\u4f53\u5728\u79fb\u52a8\u6a21\u5f0f\u4e0a\u7684\u5dee\u5f02\u53d8\u5316\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5c06\u4e2a\u4f53\u793e\u4f1a\u57fa\u7840\u8bbe\u65bd\u97e7\u6027\u548c\u7a7a\u95f4\u4e0a\u4e0b\u6587\u6574\u5408\u5230\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\uff0c\u53ef\u4ee5\u6709\u6548\u9884\u6d4b\u7834\u574f\u6027\u4e8b\u4ef6\u540e\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u7684\u53d8\u5316\uff0c\u4e3a\u7406\u89e3\u4e2a\u4f53\u884c\u4e3a\u5bf9\u793e\u533a\u8d44\u6e90\u9700\u6c42\u7684\u5f71\u54cd\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2510.24416", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.24416", "abs": "https://arxiv.org/abs/2510.24416", "authors": ["Nikhat Khan", "E. M. H. E. B. Ekanayake", "Nicolas Casilli", "Cristian Cassella", "Luke Theogarajan", "Nikhil Shukla"], "title": "Analyzing Parametric Oscillator Ising Machines through the Kuramoto Lens", "comment": null, "summary": "Networks of coupled nonlinear oscillators are emerging as powerful physical\nplatforms for implementing Ising machines. Yet the relationship between\nparametric-oscillator implementations and traditional oscillator-based Ising\nmachines remains underexplored. In this work, we develop a Kuramoto-style,\ncanonical phase description of parametric oscillator Ising machines by starting\nfrom the Stuart-Landau oscillator model -- the canonical normal form near a\nHopf bifurcation, and a natural reduced description for many parametric\noscillator implementations such as the degenerate optical parametric oscillator\n(DOPO) among others. The resulting phase dynamics combine the usual\nphase-difference coupling observed in the standard Kuramoto model along with an\nintrinsic phase sum term that is generated when conjugate coupling is\nconsidered. Moreover, our formulation helps explain why explicit\nsecond-harmonic driving is unnecessary in parametric oscillators and also\nreveals how quasi-steady amplitude heterogeneity scales the original strength\nof the spin interaction with potentially adverse impacts on the solution\nquality. Our work helps develop a unifying view of the oscillator-based\napproach to designing Ising machines.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u53c2\u91cf\u632f\u8361\u5668\u4f0a\u8f9b\u673a\u7684Kuramoto\u5f0f\u89c4\u8303\u76f8\u4f4d\u63cf\u8ff0\uff0c\u63ed\u793a\u4e86\u76f8\u4f4d\u52a8\u529b\u5b66\u5305\u542b\u76f8\u4f4d\u5dee\u8026\u5408\u548c\u76f8\u4f4d\u548c\u9879\uff0c\u89e3\u91ca\u4e86\u4e3a\u4f55\u4e0d\u9700\u8981\u663e\u5f0f\u4e8c\u6b21\u8c10\u6ce2\u9a71\u52a8\uff0c\u5e76\u5206\u6790\u4e86\u632f\u5e45\u5f02\u8d28\u6027\u5bf9\u89e3\u8d28\u91cf\u7684\u5f71\u54cd\u3002", "motivation": "\u53c2\u91cf\u632f\u8361\u5668\u7f51\u7edc\u4f5c\u4e3a\u5b9e\u73b0\u4f0a\u8f9b\u673a\u5668\u7684\u7269\u7406\u5e73\u53f0\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u5176\u4e0e\u4f20\u7edf\u632f\u8361\u5668\u4f0a\u8f9b\u673a\u7684\u5173\u7cfb\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u5efa\u7acb\u7edf\u4e00\u7684\u76f8\u4f4d\u63cf\u8ff0\u6846\u67b6\u3002", "method": "\u4eceStuart-Landau\u632f\u8361\u5668\u6a21\u578b\u51fa\u53d1\uff0c\u5f00\u53d1\u53c2\u91cf\u632f\u8361\u5668\u4f0a\u8f9b\u673a\u7684Kuramoto\u5f0f\u89c4\u8303\u76f8\u4f4d\u63cf\u8ff0\uff0c\u8003\u8651\u5171\u8f6d\u8026\u5408\u4ea7\u751f\u7684\u76f8\u4f4d\u548c\u9879\u3002", "result": "\u5f97\u5230\u4e86\u7ed3\u5408\u6807\u51c6Kuramoto\u6a21\u578b\u76f8\u4f4d\u5dee\u8026\u5408\u548c\u56fa\u6709\u76f8\u4f4d\u548c\u9879\u7684\u76f8\u4f4d\u52a8\u529b\u5b66\uff0c\u89e3\u91ca\u4e86\u53c2\u91cf\u632f\u8361\u5668\u4e2d\u65e0\u9700\u663e\u5f0f\u4e8c\u6b21\u8c10\u6ce2\u9a71\u52a8\u7684\u539f\u56e0\uff0c\u63ed\u793a\u4e86\u632f\u5e45\u5f02\u8d28\u6027\u5bf9\u81ea\u65cb\u76f8\u4e92\u4f5c\u7528\u5f3a\u5ea6\u7684\u5f71\u54cd\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u57fa\u4e8e\u632f\u8361\u5668\u7684\u4f0a\u8f9b\u673a\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7edf\u4e00\u89c6\u89d2\uff0c\u5efa\u7acb\u4e86\u53c2\u91cf\u632f\u8361\u5668\u5b9e\u73b0\u4e0e\u7ecf\u5178\u632f\u8361\u5668\u4f0a\u8f9b\u673a\u4e4b\u95f4\u7684\u8054\u7cfb\u3002"}}
{"id": "2510.24118", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24118", "abs": "https://arxiv.org/abs/2510.24118", "authors": ["Haotian Zhou", "Xiaole Wang", "He Li", "Fusheng Sun", "Shengyu Guo", "Guolei Qi", "Jianghuan Xu", "Huijing Zhao"], "title": "LagMemo: Language 3D Gaussian Splatting Memory for Multi-modal Open-vocabulary Multi-goal Visual Navigation", "comment": null, "summary": "Navigating to a designated goal using visual information is a fundamental\ncapability for intelligent robots. Most classical visual navigation methods are\nrestricted to single-goal, single-modality, and closed set goal settings. To\naddress the practical demands of multi-modal, open-vocabulary goal queries and\nmulti-goal visual navigation, we propose LagMemo, a navigation system that\nleverages a language 3D Gaussian Splatting memory. During exploration, LagMemo\nconstructs a unified 3D language memory. With incoming task goals, the system\nqueries the memory, predicts candidate goal locations, and integrates a local\nperception-based verification mechanism to dynamically match and validate goals\nduring navigation. For fair and rigorous evaluation, we curate GOAT-Core, a\nhigh-quality core split distilled from GOAT-Bench tailored to multi-modal\nopen-vocabulary multi-goal visual navigation. Experimental results show that\nLagMemo's memory module enables effective multi-modal open-vocabulary goal\nlocalization, and that LagMemo outperforms state-of-the-art methods in\nmulti-goal visual navigation. Project page:\nhttps://weekgoodday.github.io/lagmemo", "AI": {"tldr": "LagMemo\u662f\u4e00\u4e2a\u57fa\u4e8e\u8bed\u8a003D\u9ad8\u65af\u6cfc\u6e85\u8bb0\u5fc6\u7684\u89c6\u89c9\u5bfc\u822a\u7cfb\u7edf\uff0c\u652f\u6301\u591a\u6a21\u6001\u3001\u5f00\u653e\u8bcd\u6c47\u8868\u548c\u591a\u76ee\u6807\u5bfc\u822a\uff0c\u901a\u8fc7\u6784\u5efa\u7edf\u4e00\u76843D\u8bed\u8a00\u8bb0\u5fc6\u5e76\u96c6\u6210\u5c40\u90e8\u611f\u77e5\u9a8c\u8bc1\u673a\u5236\u6765\u5b9e\u73b0\u76ee\u6807\u5b9a\u4f4d\u548c\u5bfc\u822a\u3002", "motivation": "\u4f20\u7edf\u89c6\u89c9\u5bfc\u822a\u65b9\u6cd5\u5c40\u9650\u4e8e\u5355\u76ee\u6807\u3001\u5355\u6a21\u6001\u548c\u5c01\u95ed\u96c6\u76ee\u6807\u8bbe\u7f6e\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u4e2d\u591a\u6a21\u6001\u3001\u5f00\u653e\u8bcd\u6c47\u8868\u67e5\u8be2\u548c\u591a\u76ee\u6807\u5bfc\u822a\u7684\u9700\u6c42\u3002", "method": "\u7cfb\u7edf\u5728\u63a2\u7d22\u9636\u6bb5\u6784\u5efa\u7edf\u4e00\u76843D\u8bed\u8a00\u8bb0\u5fc6\uff0c\u6839\u636e\u4efb\u52a1\u76ee\u6807\u67e5\u8be2\u8bb0\u5fc6\u9884\u6d4b\u5019\u9009\u76ee\u6807\u4f4d\u7f6e\uff0c\u5e76\u96c6\u6210\u5c40\u90e8\u611f\u77e5\u9a8c\u8bc1\u673a\u5236\u5728\u5bfc\u822a\u8fc7\u7a0b\u4e2d\u52a8\u6001\u5339\u914d\u548c\u9a8c\u8bc1\u76ee\u6807\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eLagMemo\u7684\u8bb0\u5fc6\u6a21\u5757\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u591a\u6a21\u6001\u5f00\u653e\u8bcd\u6c47\u8868\u76ee\u6807\u5b9a\u4f4d\uff0c\u5728\u591a\u76ee\u6807\u89c6\u89c9\u5bfc\u822a\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "LagMemo\u901a\u8fc7\u8bed\u8a003D\u9ad8\u65af\u6cfc\u6e85\u8bb0\u5fc6\u548c\u5c40\u90e8\u9a8c\u8bc1\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5f00\u653e\u8bcd\u6c47\u8868\u591a\u76ee\u6807\u89c6\u89c9\u5bfc\u822a\u7684\u6311\u6218\uff0c\u5728GOAT-Core\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2510.24013", "categories": ["cs.AI", "cs.LG", "cs.NE", "math.CO", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.24013", "abs": "https://arxiv.org/abs/2510.24013", "authors": ["\u0130brahim O\u011fuz \u00c7etinkaya", "\u0130. Esra B\u00fcy\u00fcktahtak\u0131n", "Parshin Shojaee", "Chandan K. Reddy"], "title": "Discovering Heuristics with Large Language Models (LLMs) for Mixed-Integer Programs: Single-Machine Scheduling", "comment": null, "summary": "Our study contributes to the scheduling and combinatorial optimization\nliterature with new heuristics discovered by leveraging the power of Large\nLanguage Models (LLMs). We focus on the single-machine total tardiness (SMTT)\nproblem, which aims to minimize total tardiness by sequencing n jobs on a\nsingle processor without preemption, given processing times and due dates. We\ndevelop and benchmark two novel LLM-discovered heuristics, the EDD Challenger\n(EDDC) and MDD Challenger (MDDC), inspired by the well-known Earliest Due Date\n(EDD) and Modified Due Date (MDD) rules. In contrast to prior studies that\nemployed simpler rule-based heuristics, we evaluate our LLM-discovered\nalgorithms using rigorous criteria, including optimality gaps and solution time\nderived from a mixed-integer programming (MIP) formulation of SMTT. We compare\ntheir performance against state-of-the-art heuristics and exact methods across\nvarious job sizes (20, 100, 200, and 500 jobs). For instances with more than\n100 jobs, exact methods such as MIP and dynamic programming become\ncomputationally intractable. Up to 500 jobs, EDDC improves upon the classic EDD\nrule and another widely used algorithm in the literature. MDDC consistently\noutperforms traditional heuristics and remains competitive with exact\napproaches, particularly on larger and more complex instances. This study shows\nthat human-LLM collaboration can produce scalable, high-performing heuristics\nfor NP-hard constrained combinatorial optimization, even under limited\nresources when effectively configured.", "AI": {"tldr": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53d1\u73b0\u65b0\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5\u6765\u89e3\u51b3\u5355\u673a\u603b\u5ef6\u8fdf\u8c03\u5ea6\u95ee\u9898\uff0c\u63d0\u51fa\u4e86EDDC\u548cMDDC\u4e24\u79cd\u7b97\u6cd5\uff0c\u5728\u5927\u89c4\u6a21\u5b9e\u4f8b\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "motivation": "\u9488\u5bf9\u5355\u673a\u603b\u5ef6\u8fdf\u8fd9\u4e00NP\u96be\u7ec4\u5408\u4f18\u5316\u95ee\u9898\uff0c\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\u6027\u80fd\u6709\u9650\uff0c\u800c\u7cbe\u786e\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u5b9e\u4f8b\u4e2d\u8ba1\u7b97\u4e0d\u53ef\u884c\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53d1\u73b0\u65b0\u7684\u542f\u53d1\u5f0f\u89c4\u5219\uff0c\u5f00\u53d1\u4e86EDDC\u548cMDDC\u7b97\u6cd5\uff0c\u5e76\u4e0e\u6df7\u5408\u6574\u6570\u89c4\u5212\u548c\u52a8\u6001\u89c4\u5212\u7b49\u7cbe\u786e\u65b9\u6cd5\u4ee5\u53ca\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u8bc4\u4f30\u3002", "result": "\u5728\u8d85\u8fc7100\u4e2a\u4f5c\u4e1a\u7684\u5927\u89c4\u6a21\u5b9e\u4f8b\u4e2d\uff0cEDDC\u6539\u8fdb\u4e86\u7ecf\u5178EDD\u89c4\u5219\uff0cMDDC\u59cb\u7ec8\u4f18\u4e8e\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5728\u590d\u6742\u5b9e\u4f8b\u4e2d\u4e0e\u7cbe\u786e\u65b9\u6cd5\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "\u4eba\u673a\u534f\u4f5c\u53ef\u4ee5\u4ea7\u751f\u53ef\u6269\u5c55\u7684\u9ad8\u6027\u80fd\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u5373\u4f7f\u5728\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u4e5f\u80fd\u6709\u6548\u89e3\u51b3NP\u96be\u7684\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u3002"}}
{"id": "2510.24194", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24194", "abs": "https://arxiv.org/abs/2510.24194", "authors": ["Ev Zisselman", "Mirco Mutti", "Shelly Francis-Meretzki", "Elisei Shafer", "Aviv Tamar"], "title": "Blindfolded Experts Generalize Better: Insights from Robotic Manipulation and Videogames", "comment": null, "summary": "Behavioral cloning is a simple yet effective technique for learning\nsequential decision-making from demonstrations. Recently, it has gained\nprominence as the core of foundation models for the physical world, where\nachieving generalization requires countless demonstrations of a multitude of\ntasks. Typically, a human expert with full information on the task demonstrates\na (nearly) optimal behavior. In this paper, we propose to hide some of the\ntask's information from the demonstrator. This ``blindfolded'' expert is\ncompelled to employ non-trivial exploration to solve the task. We show that\ncloning the blindfolded expert generalizes better to unseen tasks than its\nfully-informed counterpart. We conduct experiments of real-world robot peg\ninsertion tasks with (limited) human demonstrations, alongside videogames from\nthe Procgen benchmark. Additionally, we support our findings with theoretical\nanalysis, which confirms that the generalization error scales with\n$\\sqrt{I/m}$, where $I$ measures the amount of task information available to\nthe demonstrator, and $m$ is the number of demonstrated tasks. Both theory and\npractice indicate that cloning blindfolded experts generalizes better with\nfewer demonstrated tasks. Project page with videos and code:\nhttps://sites.google.com/view/blindfoldedexperts/home", "AI": {"tldr": "\u63d0\u51fa\"\u8499\u773c\u4e13\u5bb6\"\u65b9\u6cd5\uff0c\u5728\u884c\u4e3a\u514b\u9686\u4e2d\u5411\u6f14\u793a\u8005\u9690\u85cf\u90e8\u5206\u4efb\u52a1\u4fe1\u606f\uff0c\u8feb\u4f7f\u8fdb\u884c\u975e\u5e73\u51e1\u63a2\u7d22\uff0c\u4ece\u800c\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u83b7\u5f97\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u884c\u4e3a\u514b\u9686\u9700\u8981\u4eba\u7c7b\u4e13\u5bb6\u63d0\u4f9b\u5b8c\u6574\u4efb\u52a1\u4fe1\u606f\u7684(\u8fd1\u4e4e)\u6700\u4f18\u6f14\u793a\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5728\u9700\u8981\u6cdb\u5316\u5230\u4f17\u591a\u4efb\u52a1\u65f6\u9700\u5927\u91cf\u6f14\u793a\u3002\u672c\u6587\u63a2\u7d22\u901a\u8fc7\u9650\u5236\u6f14\u793a\u8005\u4fe1\u606f\u6765\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5411\u6f14\u793a\u8005\u9690\u85cf\u90e8\u5206\u4efb\u52a1\u4fe1\u606f\uff0c\u521b\u5efa\"\u8499\u773c\u4e13\u5bb6\"\uff0c\u8feb\u4f7f\u5176\u8fdb\u884c\u63a2\u7d22\u6027\u884c\u4e3a\u3002\u514b\u9686\u8fd9\u79cd\u63a2\u7d22\u6027\u6f14\u793a\u800c\u975e\u5b8c\u5168\u4fe1\u606f\u4e0b\u7684\u6700\u4f18\u6f14\u793a\u3002", "result": "\u5728\u771f\u5b9e\u673a\u5668\u4eba\u63d2\u5b54\u4efb\u52a1\u548cProcgen\u57fa\u51c6\u89c6\u9891\u6e38\u620f\u4e2d\uff0c\u514b\u9686\u8499\u773c\u4e13\u5bb6\u6bd4\u514b\u9686\u5b8c\u5168\u4fe1\u606f\u4e13\u5bb6\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u6cdb\u5316\u66f4\u597d\u3002\u7406\u8bba\u5206\u6790\u8868\u660e\u6cdb\u5316\u8bef\u5dee\u4e0e\u221a(I/m)\u6210\u6b63\u6bd4\uff0c\u5176\u4e2dI\u662f\u6f14\u793a\u8005\u53ef\u83b7\u5f97\u7684\u4efb\u52a1\u4fe1\u606f\u91cf\uff0cm\u662f\u6f14\u793a\u4efb\u52a1\u6570\u3002", "conclusion": "\u514b\u9686\u8499\u773c\u4e13\u5bb6\u5728\u8f83\u5c11\u6f14\u793a\u4efb\u52a1\u4e0b\u80fd\u83b7\u5f97\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u7406\u8bba\u548c\u5b9e\u9a8c\u5747\u652f\u6301\u8fd9\u4e00\u7ed3\u8bba\u3002"}}
{"id": "2510.24028", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24028", "abs": "https://arxiv.org/abs/2510.24028", "authors": ["Tingyue Pan", "Mingyue Cheng", "Shilong Zhang", "Zhiding Liu", "Xiaoyu Tao", "Yucong Luo", "Jintao Zhang", "Qi Liu"], "title": "OneCast: Structured Decomposition and Modular Generation for Cross-Domain Time Series Forecasting", "comment": null, "summary": "Cross-domain time series forecasting is a valuable task in various web\napplications. Despite its rapid advancement, achieving effective generalization\nacross heterogeneous time series data remains a significant challenge. Existing\nmethods have made progress by extending single-domain models, yet often fall\nshort when facing domain-specific trend shifts and inconsistent periodic\npatterns. We argue that a key limitation lies in treating temporal series as\nundifferentiated sequence, without explicitly decoupling their inherent\nstructural components. To address this, we propose OneCast, a structured and\nmodular forecasting framework that decomposes time series into seasonal and\ntrend components, each modeled through tailored generative pathways.\nSpecifically, the seasonal component is captured by a lightweight projection\nmodule that reconstructs periodic patterns via interpretable basis functions.\nIn parallel, the trend component is encoded into discrete tokens at segment\nlevel via a semantic-aware tokenizer, and subsequently inferred through a\nmasked discrete diffusion mechanism. The outputs from both branches are\ncombined to produce a final forecast that captures seasonal patterns while\ntracking domain-specific trends. Extensive experiments across eight domains\ndemonstrate that OneCast mostly outperforms state-of-the-art baselines.", "AI": {"tldr": "OneCast\u662f\u4e00\u4e2a\u7ed3\u6784\u5316\u3001\u6a21\u5757\u5316\u7684\u8de8\u57df\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u4e3a\u5b63\u8282\u6027\u548c\u8d8b\u52bf\u5206\u91cf\uff0c\u5206\u522b\u4f7f\u7528\u8f7b\u91cf\u7ea7\u6295\u5f71\u6a21\u5757\u548c\u57fa\u4e8e\u79bb\u6563\u6269\u6563\u7684\u8bed\u4e49\u611f\u77e5\u5206\u8bcd\u5668\u8fdb\u884c\u5efa\u6a21\uff0c\u5728\u591a\u4e2a\u9886\u57df\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u8de8\u57df\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u56e0\u9886\u57df\u7279\u5b9a\u8d8b\u52bf\u53d8\u5316\u548c\u4e0d\u4e00\u81f4\u5468\u671f\u6027\u6a21\u5f0f\u5bfc\u81f4\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5c06\u65f6\u95f4\u5e8f\u5217\u89c6\u4e3a\u672a\u5206\u5316\u7684\u5e8f\u5217\uff0c\u672a\u80fd\u663e\u5f0f\u89e3\u8026\u5176\u5185\u5728\u7ed3\u6784\u7ec4\u4ef6\u3002", "method": "\u63d0\u51faOneCast\u6846\u67b6\uff1a1\uff09\u5c06\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u4e3a\u5b63\u8282\u6027\u548c\u8d8b\u52bf\u5206\u91cf\uff1b2\uff09\u5b63\u8282\u6027\u5206\u91cf\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6295\u5f71\u6a21\u5757\u4f7f\u7528\u53ef\u89e3\u91ca\u57fa\u51fd\u6570\u91cd\u5efa\u5468\u671f\u6027\u6a21\u5f0f\uff1b3\uff09\u8d8b\u52bf\u5206\u91cf\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u5206\u8bcd\u5668\u7f16\u7801\u4e3a\u6bb5\u7ea7\u79bb\u6563\u6807\u8bb0\uff0c\u4f7f\u7528\u63a9\u7801\u79bb\u6563\u6269\u6563\u673a\u5236\u8fdb\u884c\u63a8\u65ad\uff1b4\uff09\u4e24\u4e2a\u5206\u652f\u8f93\u51fa\u7ed3\u5408\u751f\u6210\u6700\u7ec8\u9884\u6d4b\u3002", "result": "\u5728\u516b\u4e2a\u9886\u57df\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cOneCast\u5927\u591a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u89e3\u8026\u65f6\u95f4\u5e8f\u5217\u7684\u7ed3\u6784\u7ec4\u4ef6\u5e76\u5206\u522b\u5efa\u6a21\uff0cOneCast\u80fd\u591f\u6709\u6548\u6355\u6349\u5b63\u8282\u6027\u6a21\u5f0f\u540c\u65f6\u8ddf\u8e2a\u9886\u57df\u7279\u5b9a\u8d8b\u52bf\uff0c\u5728\u8de8\u57df\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.24257", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24257", "abs": "https://arxiv.org/abs/2510.24257", "authors": ["Ziqi Ma", "Changda Tian", "Yue Gao"], "title": "Manipulate as Human: Learning Task-oriented Manipulation Skills by Adversarial Motion Priors", "comment": null, "summary": "In recent years, there has been growing interest in developing robots and\nautonomous systems that can interact with human in a more natural and intuitive\nway. One of the key challenges in achieving this goal is to enable these\nsystems to manipulate objects and tools in a manner that is similar to that of\nhumans. In this paper, we propose a novel approach for learning human-style\nmanipulation skills by using adversarial motion priors, which we name HMAMP.\nThe approach leverages adversarial networks to model the complex dynamics of\ntool and object manipulation, as well as the aim of the manipulation task. The\ndiscriminator is trained using a combination of real-world data and simulation\ndata executed by the agent, which is designed to train a policy that generates\nrealistic motion trajectories that match the statistical properties of human\nmotion. We evaluated HMAMP on one challenging manipulation task: hammering, and\nthe results indicate that HMAMP is capable of learning human-style manipulation\nskills that outperform current baseline methods. Additionally, we demonstrate\nthat HMAMP has potential for real-world applications by performing real robot\narm hammering tasks. In general, HMAMP represents a significant step towards\ndeveloping robots and autonomous systems that can interact with humans in a\nmore natural and intuitive way, by learning to manipulate tools and objects in\na manner similar to how humans do.", "AI": {"tldr": "\u63d0\u51faHMAMP\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6297\u8fd0\u52a8\u5148\u9a8c\u5b66\u4e60\u4eba\u7c7b\u98ce\u683c\u7684\u64cd\u7eb5\u6280\u80fd\uff0c\u5728\u9524\u51fb\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u5f00\u53d1\u80fd\u591f\u4ee5\u66f4\u81ea\u7136\u76f4\u89c2\u65b9\u5f0f\u4e0e\u4eba\u7c7b\u4e92\u52a8\u7684\u673a\u5668\u4eba\u548c\u81ea\u4e3b\u7cfb\u7edf\uff0c\u5173\u952e\u6311\u6218\u662f\u8ba9\u8fd9\u4e9b\u7cfb\u7edf\u80fd\u591f\u4ee5\u7c7b\u4f3c\u4eba\u7c7b\u7684\u65b9\u5f0f\u64cd\u7eb5\u7269\u4f53\u548c\u5de5\u5177\u3002", "method": "\u4f7f\u7528\u5bf9\u6297\u7f51\u7edc\u5efa\u6a21\u5de5\u5177\u548c\u7269\u4f53\u64cd\u7eb5\u7684\u590d\u6742\u52a8\u6001\u4ee5\u53ca\u64cd\u7eb5\u4efb\u52a1\u7684\u76ee\u6807\uff0c\u5224\u522b\u5668\u7ed3\u5408\u771f\u5b9e\u4e16\u754c\u6570\u636e\u548c\u667a\u80fd\u4f53\u6267\u884c\u7684\u4eff\u771f\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u8bad\u7ec3\u751f\u6210\u7b26\u5408\u4eba\u7c7b\u8fd0\u52a8\u7edf\u8ba1\u7279\u6027\u7684\u771f\u5b9e\u8fd0\u52a8\u8f68\u8ff9\u7684\u7b56\u7565\u3002", "result": "\u5728\u9524\u51fb\u4efb\u52a1\u4e2d\uff0cHMAMP\u80fd\u591f\u5b66\u4e60\u4eba\u7c7b\u98ce\u683c\u7684\u64cd\u7eb5\u6280\u80fd\uff0c\u8868\u73b0\u4f18\u4e8e\u5f53\u524d\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5728\u771f\u5b9e\u673a\u5668\u4eba\u81c2\u9524\u51fb\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "HMAMP\u4ee3\u8868\u4e86\u5f00\u53d1\u80fd\u591f\u4ee5\u66f4\u81ea\u7136\u76f4\u89c2\u65b9\u5f0f\u4e0e\u4eba\u7c7b\u4e92\u52a8\u7684\u673a\u5668\u4eba\u548c\u81ea\u4e3b\u7cfb\u7edf\u7684\u91cd\u8981\u8fdb\u5c55\uff0c\u901a\u8fc7\u5b66\u4e60\u4ee5\u7c7b\u4f3c\u4eba\u7c7b\u7684\u65b9\u5f0f\u64cd\u7eb5\u5de5\u5177\u548c\u7269\u4f53\u3002"}}
{"id": "2510.24031", "categories": ["cs.AI", "cs.CR", "H.3.3, I.2.7, I.5.3, I.2.5,"], "pdf": "https://arxiv.org/pdf/2510.24031", "abs": "https://arxiv.org/abs/2510.24031", "authors": ["Peng Cai", "Reza Ryan", "Nickson M. Karie"], "title": "LLMLogAnalyzer: A Clustering-Based Log Analysis Chatbot using Large Language Models", "comment": "33 pages, 10 figures", "summary": "System logs are a cornerstone of cybersecurity, supporting proactive breach\nprevention and post-incident investigations. However, analyzing vast amounts of\ndiverse log data remains significantly challenging, as high costs, lack of\nin-house expertise, and time constraints make even basic analysis difficult for\nmany organizations. This study introduces LLMLogAnalyzer, a clustering-based\nlog analysis chatbot that leverages Large Language Models (LLMs) and Machine\nLearning (ML) algorithms to simplify and streamline log analysis processes.\nThis innovative approach addresses key LLM limitations, including context\nwindow constraints and poor structured text handling capabilities, enabling\nmore effective summarization, pattern extraction, and anomaly detection tasks.\nLLMLogAnalyzer is evaluated across four distinct domain logs and various tasks.\nResults demonstrate significant performance improvements over state-of-the-art\nLLM-based chatbots, including ChatGPT, ChatPDF, and NotebookLM, with consistent\ngains ranging from 39% to 68% across different tasks. The system also exhibits\nstrong robustness, achieving a 93% reduction in interquartile range (IQR) when\nusing ROUGE-1 scores, indicating significantly lower result variability. The\nframework's effectiveness stems from its modular architecture comprising a\nrouter, log recognizer, log parser, and search tools. This design enhances LLM\ncapabilities for structured text analysis while improving accuracy and\nrobustness, making it a valuable resource for both cybersecurity experts and\nnon-technical users.", "AI": {"tldr": "LLMLogAnalyzer\u662f\u4e00\u4e2a\u57fa\u4e8e\u805a\u7c7b\u7684\u65e5\u5fd7\u5206\u6790\u804a\u5929\u673a\u5668\u4eba\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u7b80\u5316\u65e5\u5fd7\u5206\u6790\u6d41\u7a0b\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u6bd4\u73b0\u6709LLM\u804a\u5929\u673a\u5668\u4eba\u6027\u80fd\u63d0\u534739%-68%\u3002", "motivation": "\u7cfb\u7edf\u65e5\u5fd7\u662f\u7f51\u7edc\u5b89\u5168\u7684\u6838\u5fc3\uff0c\u4f46\u5206\u6790\u5927\u91cf\u591a\u6837\u5316\u65e5\u5fd7\u6570\u636e\u9762\u4e34\u9ad8\u6210\u672c\u3001\u7f3a\u4e4f\u4e13\u4e1a\u77e5\u8bc6\u548c\u65f6\u95f4\u9650\u5236\u7b49\u6311\u6218\uff0c\u8bb8\u591a\u7ec4\u7ec7\u96be\u4ee5\u8fdb\u884c\u57fa\u672c\u5206\u6790\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u67b6\u6784\uff0c\u5305\u62ec\u8def\u7531\u5668\u3001\u65e5\u5fd7\u8bc6\u522b\u5668\u3001\u65e5\u5fd7\u89e3\u6790\u5668\u548c\u641c\u7d22\u5de5\u5177\uff0c\u7ed3\u5408\u805a\u7c7b\u6280\u672f\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u89e3\u51b3LLM\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u548c\u7ed3\u6784\u5316\u6587\u672c\u5904\u7406\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002", "result": "\u5728\u56db\u4e2a\u4e0d\u540c\u9886\u57df\u65e5\u5fd7\u548c\u5404\u79cd\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u76f8\u6bd4ChatGPT\u3001ChatPDF\u548cNotebookLM\u7b49\u6700\u5148\u8fdb\u7684LLM\u804a\u5929\u673a\u5668\u4eba\uff0c\u6027\u80fd\u663e\u8457\u63d0\u534739%-68%\uff0c\u9c81\u68d2\u6027\u589e\u5f3a\uff0c\u4f7f\u7528ROUGE-1\u5206\u6570\u65f6\u56db\u5206\u4f4d\u8ddd\u51cf\u5c1193%\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u589e\u5f3aLLM\u5728\u7ed3\u6784\u5316\u6587\u672c\u5206\u6790\u65b9\u9762\u7684\u80fd\u529b\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u7f51\u7edc\u5b89\u5168\u4e13\u5bb6\u548c\u975e\u6280\u672f\u7528\u6237\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d44\u6e90\u3002"}}
{"id": "2510.24457", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.24457", "abs": "https://arxiv.org/abs/2510.24457", "authors": ["Jorge Vicente-Martinez", "Edgar Ramirez-Laboreo"], "title": "Flatness-based trajectory planning for 3D overhead cranes with friction compensation and collision avoidance", "comment": "8 pages, 11 figures", "summary": "This paper presents an optimal trajectory generation method for 3D overhead\ncranes by leveraging differential flatness. This framework enables the direct\ninclusion of complex physical and dynamic constraints, such as nonlinear\nfriction and collision avoidance for both payload and rope. Our approach allows\nfor aggressive movements by constraining payload swing only at the final point.\nA comparative simulation study validates our approach, demonstrating that\nneglecting dry friction leads to actuator saturation and collisions. The\nresults show that friction modeling is a fundamental requirement for fast and\nsafe crane trajectories.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5fae\u5206\u5e73\u5766\u6027\u76843D\u6865\u5f0f\u8d77\u91cd\u673a\u6700\u4f18\u8f68\u8ff9\u751f\u6210\u65b9\u6cd5\uff0c\u80fd\u591f\u76f4\u63a5\u5904\u7406\u975e\u7ebf\u6027\u6469\u64e6\u548c\u78b0\u649e\u907f\u514d\u7b49\u590d\u6742\u7ea6\u675f\uff0c\u5b9e\u73b0\u5feb\u901f\u5b89\u5168\u7684\u8d77\u91cd\u673a\u8fd0\u52a8\u3002", "motivation": "\u4f20\u7edf\u8d77\u91cd\u673a\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u590d\u6742\u7684\u7269\u7406\u548c\u52a8\u6001\u7ea6\u675f\uff0c\u5982\u975e\u7ebf\u6027\u6469\u64e6\u548c\u78b0\u649e\u907f\u514d\uff0c\u8fd9\u9650\u5236\u4e86\u8d77\u91cd\u673a\u7684\u8fd0\u52a8\u901f\u5ea6\u548c\u5b89\u5168\u6027\u3002", "method": "\u5229\u7528\u5fae\u5206\u5e73\u5766\u6027\u6846\u67b6\uff0c\u76f4\u63a5\u5305\u542b\u975e\u7ebf\u6027\u6469\u64e6\u548c\u78b0\u649e\u907f\u514d\u7ea6\u675f\uff0c\u4ec5\u5728\u7ec8\u70b9\u7ea6\u675f\u8f7d\u8377\u6446\u52a8\uff0c\u5b9e\u73b0\u6fc0\u8fdb\u8fd0\u52a8\u3002", "result": "\u5bf9\u6bd4\u4eff\u771f\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5ffd\u7565\u5e72\u6469\u64e6\u4f1a\u5bfc\u81f4\u6267\u884c\u5668\u9971\u548c\u548c\u78b0\u649e\uff0c\u6469\u64e6\u5efa\u6a21\u662f\u5b9e\u73b0\u5feb\u901f\u5b89\u5168\u8f68\u8ff9\u7684\u57fa\u672c\u8981\u6c42\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u5fae\u5206\u5e73\u5766\u6027\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u7ea6\u675f\uff0c\u751f\u6210\u5feb\u901f\u5b89\u5168\u76843D\u8d77\u91cd\u673a\u8f68\u8ff9\uff0c\u6469\u64e6\u5efa\u6a21\u5bf9\u8f68\u8ff9\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2510.24261", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24261", "abs": "https://arxiv.org/abs/2510.24261", "authors": ["Jingyi Tian", "Le Wang", "Sanping Zhou", "Sen Wang", "Jiayi Li", "Gang Hua"], "title": "DynaRend: Learning 3D Dynamics via Masked Future Rendering for Robotic Manipulation", "comment": "Accepted to NeurIPS 2025", "summary": "Learning generalizable robotic manipulation policies remains a key challenge\ndue to the scarcity of diverse real-world training data. While recent\napproaches have attempted to mitigate this through self-supervised\nrepresentation learning, most either rely on 2D vision pretraining paradigms\nsuch as masked image modeling, which primarily focus on static semantics or\nscene geometry, or utilize large-scale video prediction models that emphasize\n2D dynamics, thus failing to jointly learn the geometry, semantics, and\ndynamics required for effective manipulation. In this paper, we present\nDynaRend, a representation learning framework that learns 3D-aware and\ndynamics-informed triplane features via masked reconstruction and future\nprediction using differentiable volumetric rendering. By pretraining on\nmulti-view RGB-D video data, DynaRend jointly captures spatial geometry, future\ndynamics, and task semantics in a unified triplane representation. The learned\nrepresentations can be effectively transferred to downstream robotic\nmanipulation tasks via action value map prediction. We evaluate DynaRend on two\nchallenging benchmarks, RLBench and Colosseum, as well as in real-world robotic\nexperiments, demonstrating substantial improvements in policy success rate,\ngeneralization to environmental perturbations, and real-world applicability\nacross diverse manipulation tasks.", "AI": {"tldr": "DynaRend\u662f\u4e00\u4e2a\u901a\u8fc7\u53ef\u5fae\u5206\u4f53\u79ef\u6e32\u67d3\u5b66\u4e603D\u611f\u77e5\u548c\u52a8\u6001\u611f\u77e5\u7684\u4e09\u5e73\u9762\u7279\u5f81\u7684\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u6cdb\u5316\u6027\u5dee\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d562D\u89c6\u89c9\u9884\u8bad\u7ec3\u53ea\u5173\u6ce8\u9759\u6001\u8bed\u4e49\u6216\u573a\u666f\u51e0\u4f55\uff0c\u8981\u4e48\u4f7f\u7528\u5927\u89c4\u6a21\u89c6\u9891\u9884\u6d4b\u6a21\u578b\u53ea\u5173\u6ce82D\u52a8\u6001\uff0c\u65e0\u6cd5\u8054\u5408\u5b66\u4e60\u64cd\u4f5c\u6240\u9700\u7684\u51e0\u4f55\u3001\u8bed\u4e49\u548c\u52a8\u6001\u4fe1\u606f", "method": "\u4f7f\u7528\u53ef\u5fae\u5206\u4f53\u79ef\u6e32\u67d3\u901a\u8fc7\u63a9\u7801\u91cd\u5efa\u548c\u672a\u6765\u9884\u6d4b\u5b66\u4e603D\u611f\u77e5\u548c\u52a8\u6001\u611f\u77e5\u7684\u4e09\u5e73\u9762\u7279\u5f81\uff0c\u5728\u591a\u89c6\u89d2RGB-D\u89c6\u9891\u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3", "result": "\u5728\u4e24\u4e2a\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5RLBench\u548cColosseum\u4ee5\u53ca\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\uff0c\u5728\u7b56\u7565\u6210\u529f\u7387\u3001\u5bf9\u73af\u5883\u6270\u52a8\u7684\u6cdb\u5316\u6027\u548c\u771f\u5b9e\u4e16\u754c\u9002\u7528\u6027\u65b9\u9762\u90fd\u663e\u793a\u51fa\u663e\u8457\u6539\u8fdb", "conclusion": "DynaRend\u80fd\u591f\u6709\u6548\u8054\u5408\u5b66\u4e60\u7a7a\u95f4\u51e0\u4f55\u3001\u672a\u6765\u52a8\u6001\u548c\u4efb\u52a1\u8bed\u4e49\u7684\u7edf\u4e00\u4e09\u5e73\u9762\u8868\u793a\uff0c\u8fd9\u4e9b\u8868\u793a\u53ef\u4ee5\u901a\u8fc7\u52a8\u4f5c\u4ef7\u503c\u56fe\u9884\u6d4b\u6709\u6548\u8f6c\u79fb\u5230\u4e0b\u6e38\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1"}}
{"id": "2510.24085", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24085", "abs": "https://arxiv.org/abs/2510.24085", "authors": ["Md. Shihab Uddin", "Md Nazmus Shakib", "Rahul Bhadani"], "title": "Modeling Electric Vehicle Car-Following Behavior: Classical vs Machine Learning Approach", "comment": null, "summary": "The increasing adoption of electric vehicles (EVs) necessitates an\nunderstanding of their driving behavior to enhance traffic safety and develop\nsmart driving systems. This study compares classical and machine learning\nmodels for EV car following behavior. Classical models include the Intelligent\nDriver Model (IDM), Optimum Velocity Model (OVM), Optimal Velocity Relative\nVelocity (OVRV), and a simplified CACC model, while the machine learning\napproach employs a Random Forest Regressor. Using a real world dataset of an EV\nfollowing an internal combustion engine (ICE) vehicle under varied driving\nconditions, we calibrated classical model parameters by minimizing the RMSE\nbetween predictions and real data. The Random Forest model predicts\nacceleration using spacing, speed, and gap type as inputs. Results demonstrate\nthe Random Forest's superior accuracy, achieving RMSEs of 0.0046 (medium gap),\n0.0016 (long gap), and 0.0025 (extra long gap). Among physics based models,\nCACC performed best, with an RMSE of 2.67 for long gaps. These findings\nhighlight the machine learning model's performance across all scenarios. Such\nmodels are valuable for simulating EV behavior and analyzing mixed autonomy\ntraffic dynamics in EV integrated environments.", "AI": {"tldr": "\u6bd4\u8f83\u7ecf\u5178\u6a21\u578b\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u7535\u52a8\u6c7d\u8f66\u8ddf\u8f66\u884c\u4e3a\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u968f\u673a\u68ee\u6797\u6a21\u578b\u5728\u6240\u6709\u573a\u666f\u4e0b\u90fd\u4f18\u4e8e\u7269\u7406\u6a21\u578b\u3002", "motivation": "\u968f\u7740\u7535\u52a8\u6c7d\u8f66\u666e\u53ca\uff0c\u9700\u8981\u7406\u89e3\u5176\u9a7e\u9a76\u884c\u4e3a\u4ee5\u63d0\u5347\u4ea4\u901a\u5b89\u5168\u548c\u5f00\u53d1\u667a\u80fd\u9a7e\u9a76\u7cfb\u7edf\u3002", "method": "\u4f7f\u7528IDM\u3001OVM\u3001OVRV\u3001CACC\u7b49\u7ecf\u5178\u7269\u7406\u6a21\u578b\u548c\u968f\u673a\u68ee\u6797\u56de\u5f52\u5668\uff0c\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u8fdb\u884c\u53c2\u6570\u6821\u51c6\u548c\u9884\u6d4b\u3002", "result": "\u968f\u673a\u68ee\u6797\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0cRMSE\u5206\u522b\u4e3a0.0046\uff08\u4e2d\u7b49\u95f4\u8ddd\uff09\u30010.0016\uff08\u957f\u95f4\u8ddd\uff09\u548c0.0025\uff08\u8d85\u957f\u95f4\u8ddd\uff09\uff1b\u7ecf\u5178\u6a21\u578b\u4e2dCACC\u8868\u73b0\u6700\u597d\uff0c\u957f\u95f4\u8dddRMSE\u4e3a2.67\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u9884\u6d4b\u7535\u52a8\u6c7d\u8f66\u8ddf\u8f66\u884c\u4e3a\u65b9\u9762\u4f18\u4e8e\u7ecf\u5178\u7269\u7406\u6a21\u578b\uff0c\u5bf9\u6a21\u62dfEV\u884c\u4e3a\u548c\u6df7\u5408\u81ea\u52a8\u9a7e\u9a76\u4ea4\u901a\u52a8\u6001\u5206\u6790\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2510.24676", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.24676", "abs": "https://arxiv.org/abs/2510.24676", "authors": ["Jiaxuan Zhang", "Yuquan Leng", "Yixuan Guo", "Chenglong Fu"], "title": "Feature Matching-Based Gait Phase Prediction for Obstacle Crossing Control of Powered Transfemoral Prosthesis", "comment": "6 pages, conference", "summary": "For amputees with powered transfemoral prosthetics, navigating obstacles or\ncomplex terrain remains challenging. This study addresses this issue by using\nan inertial sensor on the sound ankle to guide obstacle-crossing movements. A\ngenetic algorithm computes the optimal neural network structure to predict the\nrequired angles of the thigh and knee joints. A gait progression prediction\nalgorithm determines the actuation angle index for the prosthetic knee motor,\nultimately defining the necessary thigh and knee angles and gait progression.\nResults show that when the standard deviation of Gaussian noise added to the\nthigh angle data is less than 1, the method can effectively eliminate noise\ninterference, achieving 100\\% accuracy in gait phase estimation under 150 Hz,\nwith thigh angle prediction error being 8.71\\% and knee angle prediction error\nbeing 6.78\\%. These findings demonstrate the method's ability to accurately\npredict gait progression and joint angles, offering significant practical value\nfor obstacle negotiation in powered transfemoral prosthetics.", "AI": {"tldr": "\u4f7f\u7528\u5065\u5eb7\u811a\u8e1d\u7684\u60ef\u6027\u4f20\u611f\u5668\u6307\u5bfc\u622a\u80a2\u8005\u52a8\u529b\u5927\u817f\u5047\u80a2\u7684\u8d8a\u969c\u8fd0\u52a8\uff0c\u901a\u8fc7\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u9884\u6d4b\u5173\u8282\u89d2\u5ea6\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u6b65\u6001\u76f8\u4f4d\u4f30\u8ba1\u548c\u5173\u8282\u89d2\u5ea6\u9884\u6d4b\u3002", "motivation": "\u89e3\u51b3\u622a\u80a2\u8005\u4f7f\u7528\u52a8\u529b\u5927\u817f\u5047\u80a2\u65f6\u5728\u590d\u6742\u5730\u5f62\u548c\u969c\u788d\u7269\u5bfc\u822a\u65b9\u9762\u7684\u6311\u6218\uff0c\u63d0\u9ad8\u5047\u80a2\u7684\u8d8a\u969c\u80fd\u529b\u3002", "method": "\u5728\u5065\u5eb7\u811a\u8e1d\u653e\u7f6e\u60ef\u6027\u4f20\u611f\u5668\uff0c\u4f7f\u7528\u9057\u4f20\u7b97\u6cd5\u8ba1\u7b97\u6700\u4f18\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u6765\u9884\u6d4b\u5927\u817f\u548c\u819d\u5173\u8282\u6240\u9700\u89d2\u5ea6\uff0c\u7ed3\u5408\u6b65\u6001\u8fdb\u5c55\u9884\u6d4b\u7b97\u6cd5\u786e\u5b9a\u5047\u80a2\u819d\u5173\u8282\u7535\u673a\u7684\u9a71\u52a8\u89d2\u5ea6\u3002", "result": "\u5f53\u5927\u817f\u89d2\u5ea6\u6570\u636e\u7684\u566a\u58f0\u6807\u51c6\u5dee\u5c0f\u4e8e1\u65f6\uff0c\u80fd\u6709\u6548\u6d88\u9664\u566a\u58f0\u5e72\u6270\uff0c\u5728150Hz\u4e0b\u5b9e\u73b0100%\u7684\u6b65\u6001\u76f8\u4f4d\u4f30\u8ba1\u51c6\u786e\u7387\uff0c\u5927\u817f\u89d2\u5ea6\u9884\u6d4b\u8bef\u5dee8.71%\uff0c\u819d\u5173\u8282\u89d2\u5ea6\u9884\u6d4b\u8bef\u5dee6.78%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u51c6\u786e\u9884\u6d4b\u6b65\u6001\u8fdb\u5c55\u548c\u5173\u8282\u89d2\u5ea6\uff0c\u5bf9\u52a8\u529b\u5927\u817f\u5047\u80a2\u7684\u8d8a\u969c\u5e94\u7528\u5177\u6709\u91cd\u8981\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.24315", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24315", "abs": "https://arxiv.org/abs/2510.24315", "authors": ["Baozhe Zhang", "Xinwei Chen", "Qingcheng Chen", "Chao Xu", "Fei Gao", "Yanjun Cao"], "title": "Global-State-Free Obstacle Avoidance for Quadrotor Control in Air-Ground Cooperation", "comment": null, "summary": "CoNi-MPC provides an efficient framework for UAV control in air-ground\ncooperative tasks by relying exclusively on relative states, eliminating the\nneed for global state estimation. However, its lack of environmental\ninformation poses significant challenges for obstacle avoidance. To address\nthis issue, we propose a novel obstacle avoidance algorithm, Cooperative\nNon-inertial frame-based Obstacle Avoidance (CoNi-OA), designed explicitly for\nUAV-UGV cooperative scenarios without reliance on global state estimation or\nobstacle prediction. CoNi-OA uniquely utilizes a single frame of raw LiDAR data\nfrom the UAV to generate a modulation matrix, which directly adjusts the\nquadrotor's velocity to achieve obstacle avoidance. This modulation-based\nmethod enables real-time generation of collision-free trajectories within the\nUGV's non-inertial frame, significantly reducing computational demands (less\nthan 5 ms per iteration) while maintaining safety in dynamic and unpredictable\nenvironments. The key contributions of this work include: (1) a\nmodulation-based obstacle avoidance algorithm specifically tailored for UAV-UGV\ncooperation in non-inertial frames without global states; (2) rapid, real-time\ntrajectory generation based solely on single-frame LiDAR data, removing the\nneed for obstacle modeling or prediction; and (3) adaptability to both static\nand dynamic environments, thus extending applicability to featureless or\nunknown scenarios.", "AI": {"tldr": "\u63d0\u51faCoNi-OA\u7b97\u6cd5\uff0c\u4e13\u95e8\u7528\u4e8e\u65e0\u4eba\u673a-\u65e0\u4eba\u8f66\u534f\u540c\u573a\u666f\u4e2d\u7684\u969c\u788d\u7269\u907f\u8ba9\uff0c\u65e0\u9700\u5168\u5c40\u72b6\u6001\u4f30\u8ba1\u6216\u969c\u788d\u7269\u9884\u6d4b\uff0c\u4ec5\u4f7f\u7528\u5355\u5e27LiDAR\u6570\u636e\u5b9e\u65f6\u751f\u6210\u907f\u969c\u8f68\u8ff9\u3002", "motivation": "CoNi-MPC\u6846\u67b6\u867d\u7136\u4e3a\u65e0\u4eba\u673a\u63a7\u5236\u63d0\u4f9b\u4e86\u9ad8\u6548\u65b9\u6848\uff0c\u4f46\u7f3a\u4e4f\u73af\u5883\u4fe1\u606f\u5bfc\u81f4\u969c\u788d\u7269\u907f\u8ba9\u9762\u4e34\u6311\u6218\u3002", "method": "\u5229\u7528\u65e0\u4eba\u673a\u5355\u5e27\u539f\u59cbLiDAR\u6570\u636e\u751f\u6210\u8c03\u5236\u77e9\u9635\uff0c\u76f4\u63a5\u8c03\u6574\u56db\u65cb\u7ffc\u901f\u5ea6\u5b9e\u73b0\u969c\u788d\u7269\u907f\u8ba9\uff0c\u5728\u65e0\u4eba\u8f66\u975e\u60ef\u6027\u5750\u6807\u7cfb\u4e2d\u5b9e\u65f6\u751f\u6210\u65e0\u78b0\u649e\u8f68\u8ff9\u3002", "result": "\u7b97\u6cd5\u8ba1\u7b97\u9700\u6c42\u4f4e\uff08\u6bcf\u6b21\u8fed\u4ee3\u5c0f\u4e8e5\u6beb\u79d2\uff09\uff0c\u5728\u52a8\u6001\u4e0d\u53ef\u9884\u6d4b\u73af\u5883\u4e2d\u4fdd\u6301\u5b89\u5168\u6027\uff0c\u9002\u7528\u4e8e\u9759\u6001\u548c\u52a8\u6001\u73af\u5883\u3002", "conclusion": "CoNi-OA\u4e3a\u65e0\u4eba\u673a-\u65e0\u4eba\u8f66\u534f\u540c\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u5b9e\u65f6\u7684\u969c\u788d\u7269\u907f\u8ba9\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u5168\u5c40\u72b6\u6001\u4f30\u8ba1\u6216\u969c\u788d\u7269\u9884\u6d4b\u3002"}}
{"id": "2510.24115", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24115", "abs": "https://arxiv.org/abs/2510.24115", "authors": ["Sandeep Vissapragada", "Vikrant Sahu", "Gagan Raj Gupta", "Vandita Singh"], "title": "HistoLens: An Interactive XAI Toolkit for Verifying and Mitigating Flaws in Vision-Language Models for Histopathology", "comment": null, "summary": "For doctors to truly trust artificial intelligence, it can't be a black box.\nThey need to understand its reasoning, almost as if they were consulting a\ncolleague. We created HistoLens1 to be that transparent, collaborative partner.\nIt allows a pathologist to simply ask a question in plain English about a\ntissue slide--just as they would ask a trainee. Our system intelligently\ntranslates this question into a precise query for its AI engine, which then\nprovides a clear, structured report. But it doesn't stop there. If a doctor\never asks, \"Why?\", HistoLens can instantly provide a 'visual proof' for any\nfinding--a heatmap that points to the exact cells and regions the AI used for\nits analysis. We've also ensured the AI focuses only on the patient's tissue,\njust like a trained pathologist would, by teaching it to ignore distracting\nbackground noise. The result is a workflow where the pathologist remains the\nexpert in charge, using a trustworthy AI assistant to verify their insights and\nmake faster, more confident diagnoses.", "AI": {"tldr": "\u5f00\u53d1\u4e86HistoLens\u7cfb\u7edf\uff0c\u8fd9\u662f\u4e00\u4e2a\u900f\u660e\u3001\u53ef\u89e3\u91ca\u7684AI\u52a9\u624b\uff0c\u8ba9\u75c5\u7406\u5b66\u5bb6\u80fd\u591f\u7528\u81ea\u7136\u8bed\u8a00\u63d0\u95ee\u5e76\u83b7\u5f97\u5e26\u6709\u89c6\u89c9\u8bc1\u660e\u7684\u8bca\u65ad\u62a5\u544a\u3002", "motivation": "\u4e3a\u4e86\u8ba9\u533b\u751f\u771f\u6b63\u4fe1\u4efbAI\uff0c\u9700\u8981\u6253\u7834AI\u7684\u9ed1\u7bb1\u7279\u6027\uff0c\u4f7f\u5176\u63a8\u7406\u8fc7\u7a0b\u50cf\u54a8\u8be2\u540c\u4e8b\u4e00\u6837\u900f\u660e\u53ef\u7406\u89e3\u3002", "method": "\u521b\u5efa\u4e86HistoLens\u7cfb\u7edf\uff0c\u80fd\u591f\u5c06\u75c5\u7406\u5b66\u5bb6\u7684\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u7ffb\u8bd1\u4e3a\u7cbe\u786e\u67e5\u8be2\uff0c\u751f\u6210\u7ed3\u6784\u5316\u62a5\u544a\uff0c\u5e76\u63d0\u4f9b\u70ed\u529b\u56fe\u5f62\u5f0f\u7684\u89c6\u89c9\u8bc1\u660e\u6765\u5c55\u793aAI\u5206\u6790\u6240\u7528\u7684\u5177\u4f53\u7ec6\u80de\u548c\u533a\u57df\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u4e13\u6ce8\u4e8e\u60a3\u8005\u7ec4\u7ec7\uff0c\u5ffd\u7565\u80cc\u666f\u566a\u58f0\uff0c\u8ba9\u75c5\u7406\u5b66\u5bb6\u4fdd\u6301\u4e13\u5bb6\u4e3b\u5bfc\u5730\u4f4d\uff0c\u4f7f\u7528\u53ef\u4fe1\u7684AI\u52a9\u624b\u9a8c\u8bc1\u89c1\u89e3\u5e76\u505a\u51fa\u66f4\u5feb\u3001\u66f4\u81ea\u4fe1\u7684\u8bca\u65ad\u3002", "conclusion": "HistoLens\u6210\u529f\u5b9e\u73b0\u4e86\u900f\u660e\u534f\u4f5c\u7684AI\u4f19\u4f34\u6a21\u5f0f\uff0c\u589e\u5f3a\u4e86\u533b\u751f\u5bf9AI\u7684\u4fe1\u4efb\uff0c\u63d0\u9ad8\u4e86\u8bca\u65ad\u6548\u7387\u548c\u4fe1\u5fc3\u3002"}}
{"id": "2510.24335", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24335", "abs": "https://arxiv.org/abs/2510.24335", "authors": ["Mingyu Jeong", "Eunsung Kim", "Sehun Park", "Andrew Jaeyong Choi"], "title": "NVSim: Novel View Synthesis Simulator for Large Scale Indoor Navigation", "comment": "9 pages, 10 figures", "summary": "We present NVSim, a framework that automatically constructs large-scale,\nnavigable indoor simulators from only common image sequences, overcoming the\ncost and scalability limitations of traditional 3D scanning. Our approach\nadapts 3D Gaussian Splatting to address visual artifacts on sparsely observed\nfloors a common issue in robotic traversal data. We introduce Floor-Aware\nGaussian Splatting to ensure a clean, navigable ground plane, and a novel\nmesh-free traversability checking algorithm that constructs a topological graph\nby directly analyzing rendered views. We demonstrate our system's ability to\ngenerate valid, large-scale navigation graphs from real-world data. A video\ndemonstration is avilable at https://youtu.be/tTiIQt6nXC8", "AI": {"tldr": "NVSim\u662f\u4e00\u4e2a\u4ece\u666e\u901a\u56fe\u50cf\u5e8f\u5217\u81ea\u52a8\u6784\u5efa\u5927\u89c4\u6a21\u53ef\u5bfc\u822a\u5ba4\u5185\u6a21\u62df\u5668\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf3D\u626b\u63cf\u7684\u6210\u672c\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf3D\u626b\u63cf\u65b9\u6cd5\u5728\u6784\u5efa\u5927\u89c4\u6a21\u5ba4\u5185\u73af\u5883\u65f6\u9762\u4e34\u6210\u672c\u9ad8\u548c\u53ef\u6269\u5c55\u6027\u5dee\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u4ec5\u4ece\u5e38\u89c1\u56fe\u50cf\u5e8f\u5217\u5c31\u80fd\u81ea\u52a8\u6784\u5efa\u53ef\u5bfc\u822a\u6a21\u62df\u5668\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u75283D\u9ad8\u65af\u6e85\u5c04\u6280\u672f\uff0c\u5f15\u5165\u5730\u677f\u611f\u77e5\u9ad8\u65af\u6e85\u5c04\u6765\u786e\u4fdd\u6e05\u6d01\u53ef\u5bfc\u822a\u7684\u5730\u9762\u5e73\u9762\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u6e32\u67d3\u89c6\u56fe\u76f4\u63a5\u5206\u6790\u7684\u7f51\u683c\u65e0\u5173\u53ef\u7a7f\u8d8a\u6027\u68c0\u67e5\u7b97\u6cd5\u6765\u6784\u5efa\u62d3\u6251\u56fe\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u4ece\u771f\u5b9e\u4e16\u754c\u6570\u636e\u751f\u6210\u6709\u6548\u7684\u5927\u89c4\u6a21\u5bfc\u822a\u56fe\uff0c\u89e3\u51b3\u4e86\u7a00\u758f\u89c2\u6d4b\u5730\u677f\u4e0a\u7684\u89c6\u89c9\u4f2a\u5f71\u95ee\u9898\u3002", "conclusion": "NVSim\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u4ec5\u4ece\u56fe\u50cf\u5e8f\u5217\u81ea\u52a8\u6784\u5efa\u5927\u89c4\u6a21\u53ef\u5bfc\u822a\u5ba4\u5185\u6a21\u62df\u5668\uff0c\u4e3a\u673a\u5668\u4eba\u5bfc\u822a\u63d0\u4f9b\u4e86\u6210\u672c\u6548\u76ca\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24145", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24145", "abs": "https://arxiv.org/abs/2510.24145", "authors": ["Yu Luo", "Jiamin Jiang", "Jingfei Feng", "Lei Tao", "Qingliang Zhang", "Xidao Wen", "Yongqian Sun", "Shenglin Zhang", "Jielong Huang", "Nan Qi", "Dan Pei"], "title": "From Observability Data to Diagnosis: An Evolving Multi-agent System for Incident Management in Cloud Systems", "comment": null, "summary": "Incident management (IM) is central to the reliability of large-scale cloud\nsystems. Yet manual IM, where on-call engineers examine metrics, logs, and\ntraces is labor-intensive and error-prone in the face of massive and\nheterogeneous observability data. Existing automated IM approaches often\nstruggle to generalize across systems, provide limited interpretability, and\nincur high deployment costs, which hinders adoption in practice. In this paper,\nwe present OpsAgent, a lightweight, self-evolving multi-agent system for IM\nthat employs a training-free data processor to convert heterogeneous\nobservability data into structured textual descriptions, along with a\nmulti-agent collaboration framework that makes diagnostic inference transparent\nand auditable. To support continual capability growth, OpsAgent also introduces\na dual self-evolution mechanism that integrates internal model updates with\nexternal experience accumulation, thereby closing the deployment loop.\nComprehensive experiments on the OPENRCA benchmark demonstrate state-of-the-art\nperformance and show that OpsAgent is generalizable, interpretable,\ncost-efficient, and self-evolving, making it a practically deployable and\nsustainable solution for long-term operation in real-world cloud systems.", "AI": {"tldr": "OpsAgent\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u81ea\u6f14\u8fdb\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u4e91\u7cfb\u7edf\u7684\u4e8b\u4ef6\u7ba1\u7406\uff0c\u901a\u8fc7\u514d\u8bad\u7ec3\u6570\u636e\u5904\u7406\u5668\u5c06\u5f02\u6784\u53ef\u89c2\u6d4b\u6570\u636e\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u6587\u672c\u63cf\u8ff0\uff0c\u5e76\u4f7f\u7528\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\u5b9e\u73b0\u900f\u660e\u53ef\u5ba1\u8ba1\u7684\u8bca\u65ad\u63a8\u7406\u3002", "motivation": "\u4f20\u7edf\u624b\u52a8\u4e8b\u4ef6\u7ba1\u7406\u5728\u5904\u7406\u5927\u89c4\u6a21\u5f02\u6784\u53ef\u89c2\u6d4b\u6570\u636e\u65f6\u52b3\u52a8\u5bc6\u96c6\u4e14\u5bb9\u6613\u51fa\u9519\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u5b58\u5728\u8de8\u7cfb\u7edf\u6cdb\u5316\u80fd\u529b\u5dee\u3001\u53ef\u89e3\u91ca\u6027\u6709\u9650\u548c\u90e8\u7f72\u6210\u672c\u9ad8\u7b49\u95ee\u9898\uff0c\u963b\u788d\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u91c7\u7528\u514d\u8bad\u7ec3\u6570\u636e\u5904\u7406\u5668\u5904\u7406\u5f02\u6784\u53ef\u89c2\u6d4b\u6570\u636e\uff0c\u6784\u5efa\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\u8fdb\u884c\u8bca\u65ad\u63a8\u7406\uff0c\u5e76\u5f15\u5165\u53cc\u81ea\u6f14\u8fdb\u673a\u5236\uff08\u5185\u90e8\u6a21\u578b\u66f4\u65b0\u548c\u5916\u90e8\u7ecf\u9a8c\u79ef\u7d2f\uff09\u5b9e\u73b0\u6301\u7eed\u80fd\u529b\u589e\u957f\u3002", "result": "\u5728OPENRCA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660eOpsAgent\u5177\u6709\u53ef\u6cdb\u5316\u3001\u53ef\u89e3\u91ca\u3001\u6210\u672c\u6548\u76ca\u9ad8\u548c\u81ea\u6f14\u8fdb\u7684\u7279\u70b9\u3002", "conclusion": "OpsAgent\u662f\u4e00\u4e2a\u5b9e\u9645\u53ef\u90e8\u7f72\u4e14\u53ef\u6301\u7eed\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u771f\u5b9e\u4e91\u7cfb\u7edf\u7684\u957f\u671f\u8fd0\u7ef4\u3002"}}
{"id": "2510.24151", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24151", "abs": "https://arxiv.org/abs/2510.24151", "authors": ["Bingsen Qiu", "Zijian Liu", "Xiao Liu", "Haoshen Yang", "Zeren Gao", "Bingjie Wang", "Feier Zhang", "Yixuan Qin", "Chunyan Li"], "title": "BMGQ: A Bottom-up Method for Generating Complex Multi-hop Reasoning Questions from Semi-structured Data", "comment": null, "summary": "Building training-ready multi-hop question answering (QA) datasets that truly\nstress a model's retrieval and reasoning abilities remains highly challenging\nrecently. While there have been a few recent evaluation datasets that capture\nthe characteristics of hard-to-search but easy-to-verify problems -- requiring\nthe integration of ambiguous, indirect, and cross-domain cues -- these data\nresources remain scarce and are mostly designed for evaluation, making them\nunsuitable for supervised fine-tuning (SFT) or reinforcement learning (RL).\nMeanwhile, manually curating non-trivially retrievable questions -- where\nanswers cannot be found through a single direct query but instead require\nmulti-hop reasoning over oblique and loosely connected evidence -- incurs\nprohibitive human costs and fails to scale, creating a critical data bottleneck\nfor training high-capability retrieval-and-reasoning agents.\n  To address this, we present an automated framework for generating\nhigh-difficulty, training-ready multi-hop questions from semi-structured\nknowledge sources. The system (i) grows diverse, logically labeled evidence\nclusters through Natural Language Inference (NLI)-based relation typing and\ndiversity-aware expansion; (ii) applies reverse question construction to\ncompose oblique cues so that isolated signals are underinformative but their\ncombination uniquely identifies the target entity; and (iii) enforces quality\nwith a two-step evaluation pipeline that combines multi-model consensus\nfiltering with structured constraint decomposition and evidence-based matching.\nThe result is a scalable process that yields complex, retrieval-resistant yet\nverifiable questions suitable for SFT/RL training as well as challenging\nevaluation, substantially reducing human curation effort while preserving the\ndifficulty profile of strong evaluation benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u52a8\u5316\u6846\u67b6\u751f\u6210\u9ad8\u96be\u5ea6\u591a\u8df3\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u95ee\u9898", "motivation": "\u73b0\u6709\u8bc4\u4f30\u6570\u636e\u96c6\u4e0d\u9002\u5408\u76d1\u7763\u5fae\u8c03\u6216\u5f3a\u5316\u5b66\u4e60\uff0c\u4e14\u4eba\u5de5\u6784\u5efa\u591a\u8df3\u95ee\u9898\u6210\u672c\u9ad8\u6602\uff0c\u5f62\u6210\u8bad\u7ec3\u9ad8\u80fd\u529b\u68c0\u7d22\u63a8\u7406\u4ee3\u7406\u7684\u6570\u636e\u74f6\u9888", "method": "\u4ece\u534a\u7ed3\u6784\u5316\u77e5\u8bc6\u6e90\u81ea\u52a8\u751f\u6210\u591a\u8df3\u95ee\u9898\uff1a\u901a\u8fc7NLI\u5173\u7cfb\u5206\u7c7b\u548c\u591a\u6837\u6027\u6269\u5c55\u6784\u5efa\u8bc1\u636e\u7c07\uff1b\u5e94\u7528\u53cd\u5411\u95ee\u9898\u6784\u5efa\u521b\u5efa\u6a21\u7cca\u7ebf\u7d22\uff1b\u4f7f\u7528\u591a\u6a21\u578b\u5171\u8bc6\u8fc7\u6ee4\u548c\u7ed3\u6784\u5316\u7ea6\u675f\u5206\u89e3\u8fdb\u884c\u8d28\u91cf\u8bc4\u4f30", "result": "\u5f00\u53d1\u51fa\u53ef\u6269\u5c55\u6d41\u7a0b\uff0c\u751f\u6210\u590d\u6742\u3001\u68c0\u7d22\u62b5\u6297\u4f46\u53ef\u9a8c\u8bc1\u7684\u95ee\u9898\uff0c\u9002\u7528\u4e8eSFT/RL\u8bad\u7ec3\u548c\u6311\u6218\u6027\u8bc4\u4f30", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u5de5\u4f5c\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u8bc4\u4f30\u57fa\u51c6\u7684\u96be\u5ea6\u7279\u5f81"}}
{"id": "2510.24508", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24508", "abs": "https://arxiv.org/abs/2510.24508", "authors": ["Haoying Li", "Yifan Peng", "Junfeng Wu"], "title": "Supervisory Measurement-Guided Noise Covariance Estimation", "comment": null, "summary": "Reliable state estimation hinges on accurate specification of sensor noise\ncovariances, which weigh heterogeneous measurements. In practice, these\ncovariances are difficult to identify due to environmental variability,\nfront-end preprocessing, and other reasons. We address this by formulating\nnoise covariance estimation as a bilevel optimization that, from a Bayesian\nperspective, factorizes the joint likelihood of so-called odometry and\nsupervisory measurements, thereby balancing information utilization with\ncomputational efficiency. The factorization converts the nested Bayesian\ndependency into a chain structure, enabling efficient parallel computation: at\nthe lower level, an invariant extended Kalman filter with state augmentation\nestimates trajectories, while a derivative filter computes analytical gradients\nin parallel for upper-level gradient updates. The upper level refines the\ncovariance to guide the lower-level estimation. Experiments on synthetic and\nreal-world datasets show that our method achieves higher efficiency over\nexisting baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5c42\u4f18\u5316\u65b9\u6cd5\uff0c\u5c06\u566a\u58f0\u534f\u65b9\u5dee\u4f30\u8ba1\u8868\u8ff0\u4e3a\u8d1d\u53f6\u65af\u95ee\u9898\uff0c\u901a\u8fc7\u56e0\u5b50\u5206\u89e3\u5b9e\u73b0\u9ad8\u6548\u5e76\u884c\u8ba1\u7b97\uff0c\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u663e\u793a\u51fa\u6bd4\u73b0\u6709\u57fa\u7ebf\u66f4\u9ad8\u7684\u6548\u7387\u3002", "motivation": "\u4f20\u611f\u5668\u566a\u58f0\u534f\u65b9\u5dee\u5728\u5b9e\u9645\u4e2d\u96be\u4ee5\u51c6\u786e\u6307\u5b9a\uff0c\u56e0\u4e3a\u53d7\u5230\u73af\u5883\u53d8\u5316\u3001\u524d\u7aef\u9884\u5904\u7406\u7b49\u56e0\u7d20\u5f71\u54cd\uff0c\u800c\u53ef\u9760\u7684\u72b6\u6001\u4f30\u8ba1\u4f9d\u8d56\u4e8e\u51c6\u786e\u7684\u566a\u58f0\u534f\u65b9\u5dee\u89c4\u8303\u3002", "method": "\u5c06\u566a\u58f0\u534f\u65b9\u5dee\u4f30\u8ba1\u6784\u5efa\u4e3a\u53cc\u5c42\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u89c6\u89d2\u56e0\u5b50\u5316\u89e3\u91cc\u7a0b\u8ba1\u548c\u76d1\u63a7\u6d4b\u91cf\u7684\u8054\u5408\u4f3c\u7136\u3002\u4e0b\u5c42\u4f7f\u7528\u5e26\u72b6\u6001\u589e\u5f3a\u7684\u4e0d\u53d8\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u4f30\u8ba1\u8f68\u8ff9\uff0c\u540c\u65f6\u5bfc\u6570\u6ee4\u6ce2\u5668\u5e76\u884c\u8ba1\u7b97\u5206\u6790\u68af\u5ea6\uff1b\u4e0a\u5c42\u901a\u8fc7\u68af\u5ea6\u66f4\u65b0\u4f18\u5316\u534f\u65b9\u5dee\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u6548\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u53cc\u5c42\u4f18\u5316\u6846\u67b6\u80fd\u591f\u6709\u6548\u4f30\u8ba1\u566a\u58f0\u534f\u65b9\u5dee\uff0c\u5728\u5e73\u8861\u4fe1\u606f\u5229\u7528\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2510.24161", "categories": ["cs.AI", "cs.MM", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24161", "abs": "https://arxiv.org/abs/2510.24161", "authors": ["Wentao Tan", "Bowen Wang", "Heng Zhi", "Chenyu Liu", "Zhe Li", "Jian Liu", "Zengrong Lin", "Yukun Dai", "Yipeng Chen", "Wenjie Yang", "Enci Xie", "Hao Xue", "Baixu Ji", "Chen Xu", "Zhibin Wang", "Tianshi Wang", "Lei Zhu", "Heng Tao Shen"], "title": "BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and Cross-Embodiment Learning", "comment": null, "summary": "Multimodal large language models (MLLMs) have advanced vision-language\nreasoning and are increasingly deployed in embodied agents. However,\nsignificant limitations remain: MLLMs generalize poorly across digital-physical\nspaces and embodiments; vision-language-action models (VLAs) produce low-level\nactions yet lack robust high-level embodied reasoning; and most embodied large\nlanguage models (ELLMs) are constrained to digital-space with poor\ngeneralization to the physical world. Thus, unified models that operate\nseamlessly across digital and physical spaces while generalizing across\nembodiments and tasks remain absent. We introduce the \\textbf{Boundless Large\nModel (BLM$_1$)}, a multimodal spatial foundation model that preserves\ninstruction following and reasoning, incorporates embodied knowledge, and\nsupports robust cross-embodiment control. BLM$_1$ integrates three key\ncapabilities -- \\textit{cross-space transfer, cross-task learning, and\ncross-embodiment generalization} -- via a two-stage training paradigm. Stage I\ninjects embodied knowledge into the MLLM through curated digital corpora while\nmaintaining language competence. Stage II trains a policy module through an\nintent-bridging interface that extracts high-level semantics from the MLLM to\nguide control, without fine-tuning the MLLM backbone. This process is supported\nby a self-collected cross-embodiment demonstration suite spanning four robot\nembodiments and six progressively challenging tasks. Evaluations across digital\nand physical benchmarks show that a single BLM$_1$ instance outperforms four\nmodel families -- MLLMs, ELLMs, VLAs, and GMLMs -- achieving\n$\\sim\\!\\textbf{6%}$ gains in digital tasks and $\\sim\\!\\textbf{3%}$ in physical\ntasks.", "AI": {"tldr": "BLM\u2081\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u5b9e\u73b0\u8de8\u7a7a\u95f4\u4f20\u8f93\u3001\u8de8\u4efb\u52a1\u5b66\u4e60\u548c\u8de8\u5177\u8eab\u6cdb\u5316\uff0c\u5728\u6570\u5b57\u548c\u7269\u7406\u4efb\u52a1\u4e2d\u90fd\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709MLLMs\u5728\u6570\u5b57-\u7269\u7406\u7a7a\u95f4\u548c\u5177\u8eab\u667a\u80fd\u4f53\u95f4\u6cdb\u5316\u80fd\u529b\u5dee\uff0cVLAs\u7f3a\u4e4f\u9ad8\u7ea7\u63a8\u7406\u80fd\u529b\uff0cELLMs\u5c40\u9650\u4e8e\u6570\u5b57\u7a7a\u95f4\uff0c\u9700\u8981\u7edf\u4e00\u7684\u8de8\u7a7a\u95f4\u3001\u8de8\u5177\u8eab\u6a21\u578b\u3002", "method": "\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u9636\u6bb5I\u901a\u8fc7\u6570\u5b57\u8bed\u6599\u6ce8\u5165\u5177\u8eab\u77e5\u8bc6\uff0c\u9636\u6bb5II\u8bad\u7ec3\u7b56\u7565\u6a21\u5757\u901a\u8fc7\u610f\u56fe\u6865\u63a5\u63a5\u53e3\u4eceMLLM\u63d0\u53d6\u9ad8\u7ea7\u8bed\u4e49\u6307\u5bfc\u63a7\u5236\uff0c\u4e0d\u5fae\u8c03MLLM\u4e3b\u5e72\u3002", "result": "\u5728\u6570\u5b57\u548c\u7269\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5355\u4e2aBLM\u2081\u5b9e\u4f8b\u4f18\u4e8eMLLMs\u3001ELLMs\u3001VLAs\u548cGMLMs\u56db\u7c7b\u6a21\u578b\uff0c\u6570\u5b57\u4efb\u52a1\u63d0\u5347\u7ea66%\uff0c\u7269\u7406\u4efb\u52a1\u63d0\u5347\u7ea63%\u3002", "conclusion": "BLM\u2081\u6210\u529f\u5b9e\u73b0\u4e86\u8de8\u7a7a\u95f4\u3001\u8de8\u4efb\u52a1\u548c\u8de8\u5177\u8eab\u7684\u7edf\u4e00\u6a21\u578b\uff0c\u4e3a\u5177\u8eab\u667a\u80fd\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24515", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24515", "abs": "https://arxiv.org/abs/2510.24515", "authors": ["Malintha Fernando", "Petter \u00d6gren", "Silun Zhang"], "title": "Stochastic Prize-Collecting Games: Strategic Planning in Multi-Robot Systems", "comment": "Submitted to IEEE Robotics and Automation Letters", "summary": "The Team Orienteering Problem (TOP) generalizes many real-world multi-robot\nscheduling and routing tasks that occur in autonomous mobility, aerial\nlogistics, and surveillance applications. While many flavors of the TOP exist\nfor planning in multi-robot systems, they assume that all the robots cooperate\ntoward a single objective; thus, they do not extend to settings where the\nrobots compete in reward-scarce environments. We propose Stochastic\nPrize-Collecting Games (SPCG) as an extension of the TOP to plan in the\npresence of self-interested robots operating on a graph, under energy\nconstraints and stochastic transitions. A theoretical study on complete and\nstar graphs establishes that there is a unique pure Nash equilibrium in SPCGs\nthat coincides with the optimal routing solution of an equivalent TOP given a\nrank-based conflict resolution rule. This work proposes two algorithms: Ordinal\nRank Search (ORS) to obtain the ''ordinal rank'' --one's effective rank in\ntemporarily-formed local neighborhoods during the games' stages, and Fictitious\nOrdinal Response Learning (FORL) to obtain best-response policies against one's\nsenior-rank opponents. Empirical evaluations conducted on road networks and\nsynthetic graphs under both dynamic and stationary prize distributions show\nthat 1) the state-aliasing induced by OR-conditioning enables learning policies\nthat scale more efficiently to large team sizes than those trained with the\nglobal index, and 2) Policies trained with FORL generalize better to imbalanced\nprize distributions than those with other multi-agent training methods.\nFinally, the learned policies in the SPCG achieved between 87% and 95%\noptimality compared to an equivalent TOP solution obtained by mixed-integer\nlinear programming.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u968f\u673a\u5956\u54c1\u6536\u96c6\u6e38\u620f(SPCG)\u4f5c\u4e3a\u56e2\u961f\u5b9a\u5411\u95ee\u9898(TOP)\u7684\u6269\u5c55\uff0c\u7528\u4e8e\u5728\u81ea\u5229\u673a\u5668\u4eba\u3001\u80fd\u91cf\u7ea6\u675f\u548c\u968f\u673a\u8f6c\u79fb\u6761\u4ef6\u4e0b\u8fdb\u884c\u89c4\u5212\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u7b97\u6cd5\u8bbe\u8ba1\uff0c\u8bc1\u660e\u4e86\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u5b58\u5728\u7eaf\u7eb3\u4ec0\u5747\u8861\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u7b97\u6cd5\u6765\u5b66\u4e60\u6700\u4f18\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u7684\u56e2\u961f\u5b9a\u5411\u95ee\u9898(TOP)\u5047\u8bbe\u6240\u6709\u673a\u5668\u4eba\u5408\u4f5c\u8ffd\u6c42\u5355\u4e00\u76ee\u6807\uff0c\u65e0\u6cd5\u6269\u5c55\u5230\u5956\u52b1\u7a00\u7f3a\u73af\u5883\u4e2d\u673a\u5668\u4eba\u7ade\u4e89\u7684\u573a\u666f\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u81ea\u5229\u673a\u5668\u4eba\u7ade\u4e89\u7684\u65b0\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u968f\u673a\u5956\u54c1\u6536\u96c6\u6e38\u620f(SPCG)\u6846\u67b6\uff0c\u8bbe\u8ba1\u4e86\u4e24\u79cd\u7b97\u6cd5\uff1a\u5e8f\u6570\u6392\u540d\u641c\u7d22(ORS)\u7528\u4e8e\u786e\u5b9a\u5c40\u90e8\u90bb\u57df\u4e2d\u7684\u6709\u6548\u6392\u540d\uff0c\u4ee5\u53ca\u865a\u6784\u5e8f\u6570\u54cd\u5e94\u5b66\u4e60(FORL)\u7528\u4e8e\u5b66\u4e60\u9488\u5bf9\u9ad8\u6392\u540d\u5bf9\u624b\u7684\u6700\u4f73\u54cd\u5e94\u7b56\u7565\u3002", "result": "\u5728\u9053\u8def\u7f51\u7edc\u548c\u5408\u6210\u56fe\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\uff1a1) ORS\u7684\u6761\u4ef6\u72b6\u6001\u6df7\u53e0\u4f7f\u7b56\u7565\u5b66\u4e60\u5728\u5927\u578b\u56e2\u961f\u4e2d\u66f4\u9ad8\u6548\uff1b2) FORL\u8bad\u7ec3\u7684\u7b56\u7565\u5728\u975e\u5747\u8861\u5956\u54c1\u5206\u5e03\u4e0b\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\uff1b3) \u5b66\u4e60\u7b56\u7565\u8fbe\u5230\u4e8687%-95%\u7684\u6700\u4f18\u6027\u3002", "conclusion": "SPCG\u6846\u67b6\u6210\u529f\u6269\u5c55\u4e86TOP\u4ee5\u5904\u7406\u673a\u5668\u4eba\u7ade\u4e89\u573a\u666f\uff0c\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5b66\u4e60\u63a5\u8fd1\u6700\u4f18\u7684\u7b56\u7565\uff0c\u5728\u590d\u6742\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.24166", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24166", "abs": "https://arxiv.org/abs/2510.24166", "authors": ["Xin Yang", "Yuhang Zhang", "Wei Li", "Xin Lin", "Wenbin Zou", "Chen Xu"], "title": "UniPlanner: A Unified Motion Planning Framework for Autonomous Vehicle Decision-Making Systems via Multi-Dataset Integration", "comment": null, "summary": "Motion planning is a critical component of autonomous vehicle decision-making\nsystems, directly determining trajectory safety and driving efficiency. While\ndeep learning approaches have advanced planning capabilities, existing methods\nremain confined to single-dataset training, limiting their robustness in\nplanning.\n  Through systematic analysis, we discover that vehicular trajectory\ndistributions and history-future correlations demonstrate remarkable\nconsistency across different datasets. Based on these findings, we propose\nUniPlanner, the first planning framework designed for multi-dataset integration\nin autonomous vehicle decision-making. UniPlanner achieves unified\ncross-dataset learning through three synergistic innovations.\n  First, the History-Future Trajectory Dictionary Network (HFTDN) aggregates\nhistory-future trajectory pairs from multiple datasets, using historical\ntrajectory similarity to retrieve relevant futures and generate cross-dataset\nplanning guidance.\n  Second, the Gradient-Free Trajectory Mapper (GFTM) learns robust\nhistory-future correlations from multiple datasets, transforming historical\ntrajectories into universal planning priors. Its gradient-free design ensures\nthe introduction of valuable priors while preventing shortcut learning, making\nthe planning knowledge safely transferable. Third, the Sparse-to-Dense (S2D)\nparadigm implements adaptive dropout to selectively suppress planning priors\nduring training for robust learning, while enabling full prior utilization\nduring inference to maximize planning performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86UniPlanner\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6570\u636e\u96c6\u96c6\u6210\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u8fd0\u52a8\u89c4\u5212\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u5305\u542b\u4e09\u4e2a\u521b\u65b0\u7ec4\u4ef6\uff1a\u5386\u53f2-\u672a\u6765\u8f68\u8ff9\u5b57\u5178\u7f51\u7edc\u3001\u68af\u5ea6\u81ea\u7531\u8f68\u8ff9\u6620\u5c04\u5668\u548c\u7a00\u758f\u5230\u5bc6\u96c6\u8303\u5f0f\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5c40\u9650\u4e8e\u5355\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u89c4\u5212\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3002\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u6570\u636e\u96c6\u4e2d\u7684\u8f66\u8f86\u8f68\u8ff9\u5206\u5e03\u548c\u5386\u53f2-\u672a\u6765\u76f8\u5173\u6027\u5177\u6709\u663e\u8457\u4e00\u81f4\u6027\uff0c\u8fd9\u4e3a\u591a\u6570\u636e\u96c6\u96c6\u6210\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "method": "1. HFTDN\uff1a\u4ece\u591a\u6570\u636e\u96c6\u805a\u5408\u5386\u53f2-\u672a\u6765\u8f68\u8ff9\u5bf9\uff0c\u57fa\u4e8e\u5386\u53f2\u8f68\u8ff9\u76f8\u4f3c\u6027\u68c0\u7d22\u76f8\u5173\u672a\u6765\u8f68\u8ff9\u751f\u6210\u8de8\u6570\u636e\u96c6\u89c4\u5212\u6307\u5bfc\n2. GFTM\uff1a\u4ece\u591a\u6570\u636e\u96c6\u5b66\u4e60\u9c81\u68d2\u7684\u5386\u53f2-\u672a\u6765\u76f8\u5173\u6027\uff0c\u5c06\u5386\u53f2\u8f68\u8ff9\u8f6c\u6362\u4e3a\u901a\u7528\u89c4\u5212\u5148\u9a8c\uff0c\u68af\u5ea6\u81ea\u7531\u8bbe\u8ba1\u9632\u6b62\u6377\u5f84\u5b66\u4e60\n3. S2D\uff1a\u8bad\u7ec3\u65f6\u9009\u62e9\u6027\u6291\u5236\u89c4\u5212\u5148\u9a8c\u5b9e\u73b0\u9c81\u68d2\u5b66\u4e60\uff0c\u63a8\u7406\u65f6\u5145\u5206\u5229\u7528\u5148\u9a8c\u6700\u5927\u5316\u89c4\u5212\u6027\u80fd", "result": "UniPlanner\u5b9e\u73b0\u4e86\u8de8\u6570\u636e\u96c6\u7684\u7edf\u4e00\u5b66\u4e60\uff0c\u80fd\u591f\u751f\u6210\u66f4\u5b89\u5168\u3001\u66f4\u9ad8\u6548\u7684\u8f68\u8ff9\u89c4\u5212\uff0c\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u591a\u6570\u636e\u96c6\u96c6\u6210\u5728\u81ea\u52a8\u9a7e\u9a76\u8fd0\u52a8\u89c4\u5212\u4e2d\u7684\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\uff0cUniPlanner\u6846\u67b6\u4e3a\u63d0\u5347\u89c4\u5212\u7cfb\u7edf\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24533", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24533", "abs": "https://arxiv.org/abs/2510.24533", "authors": ["Yuan Shen", "Yuze Hong", "Guangyang Zeng", "Tengfei Zhang", "Pui Yi Chui", "Ziyang Hong", "Junfeng Wu"], "title": "GeVI-SLAM: Gravity-Enhanced Stereo Visua Inertial SLAM for Underwater Robots", "comment": null, "summary": "Accurate visual inertial simultaneous localization and mapping (VI SLAM) for\nunderwater robots remains a significant challenge due to frequent visual\ndegeneracy and insufficient inertial measurement unit (IMU) motion excitation.\nIn this paper, we present GeVI-SLAM, a gravity-enhanced stereo VI SLAM system\ndesigned to address these issues. By leveraging the stereo camera's direct\ndepth estimation ability, we eliminate the need to estimate scale during IMU\ninitialization, enabling stable operation even under low acceleration dynamics.\nWith precise gravity initialization, we decouple the pitch and roll from the\npose estimation and solve a 4 degrees of freedom (DOF) Perspective-n-Point\n(PnP) problem for pose tracking. This allows the use of a minimal 3-point\nsolver, which significantly reduces computational time to reject outliers\nwithin a Random Sample Consensus framework. We further propose a\nbias-eliminated 4-DOF PnP estimator with provable consistency, ensuring the\nrelative pose converges to the true value as the feature number increases. To\nhandle dynamic motion, we refine the full 6-DOF pose while jointly estimating\nthe IMU covariance, enabling adaptive weighting of the gravity prior. Extensive\nexperiments on simulated and real-world data demonstrate that GeVI-SLAM\nachieves higher accuracy and greater stability compared to state-of-the-art\nmethods.", "AI": {"tldr": "GeVI-SLAM\u662f\u4e00\u4e2a\u91cd\u529b\u589e\u5f3a\u7684\u7acb\u4f53\u89c6\u89c9\u60ef\u6027SLAM\u7cfb\u7edf\uff0c\u901a\u8fc7\u5229\u7528\u7acb\u4f53\u76f8\u673a\u76f4\u63a5\u6df1\u5ea6\u4f30\u8ba1\u548c\u91cd\u529b\u521d\u59cb\u5316\uff0c\u89e3\u51b3\u4e86\u6c34\u4e0b\u673a\u5668\u4ebaSLAM\u4e2d\u89c6\u89c9\u9000\u5316\u548cIMU\u8fd0\u52a8\u6fc0\u52b1\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u6c34\u4e0b\u673a\u5668\u4ebaVI SLAM\u9762\u4e34\u89c6\u89c9\u9891\u7e41\u9000\u5316\u548cIMU\u8fd0\u52a8\u6fc0\u52b1\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u5bfc\u81f4\u5b9a\u4f4d\u548c\u5efa\u56fe\u7cbe\u5ea6\u4e0b\u964d\u3002", "method": "\u5229\u7528\u7acb\u4f53\u76f8\u673a\u76f4\u63a5\u6df1\u5ea6\u4f30\u8ba1\u6d88\u9664IMU\u521d\u59cb\u5316\u4e2d\u7684\u5c3a\u5ea6\u4f30\u8ba1\u9700\u6c42\uff1b\u901a\u8fc7\u7cbe\u786e\u91cd\u529b\u521d\u59cb\u5316\u89e3\u8026\u4fef\u4ef0\u548c\u6eda\u8f6c\uff0c\u4f7f\u75284\u81ea\u7531\u5ea6PnP\u8fdb\u884c\u4f4d\u59ff\u8ddf\u8e2a\uff1b\u63d0\u51fa\u504f\u5dee\u6d88\u9664\u76844-DOF PnP\u4f30\u8ba1\u5668\uff1b\u52a8\u6001\u8fd0\u52a8\u65f6\u8054\u5408\u4f30\u8ba1IMU\u534f\u65b9\u5dee\u5e76\u4f18\u53166-DOF\u4f4d\u59ff\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cGeVI-SLAM\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u7cbe\u5ea6\u548c\u66f4\u597d\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "GeVI-SLAM\u7cfb\u7edf\u901a\u8fc7\u91cd\u529b\u589e\u5f3a\u548c4-DOF PnP\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6c34\u4e0bVI SLAM\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u548c\u7a33\u5b9a\u7684\u6027\u80fd\u3002"}}
{"id": "2510.24168", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24168", "abs": "https://arxiv.org/abs/2510.24168", "authors": ["Weihua Cheng", "Ersheng Ni", "Wenlong Wang", "Yifei Sun", "Junming Liu", "Wangyu Shen", "Yirong Chen", "Botian Shi", "Ding Wang"], "title": "MGA: Memory-Driven GUI Agent for Observation-Centric Interaction", "comment": "Submitted to WWW2025", "summary": "The rapid progress of Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) has enabled agentic systems capable of perceiving and acting\nacross diverse environments. A challenging yet impactful frontier is the\ndevelopment of GUI agents, which must navigate complex desktop and web\ninterfaces while maintaining robustness and generalization. Existing paradigms\ntypically model tasks as long-chain executions, concatenating historical\ntrajectories into the context. While approaches such as Mirage and GTA1 refine\nplanning or introduce multi-branch action selection, they remain constrained by\ntwo persistent issues: Dependence on historical trajectories, which amplifies\nerror propagation. And Local exploration bias, where \"decision-first,\nobservation-later\" mechanisms overlook critical interface cues. We introduce\nthe Memory-Driven GUI Agent (MGA), which reframes GUI interaction around the\nprinciple of observe first, then decide. MGA models each step as an\nindependent, context-rich environment state represented by a triad: current\nscreenshot, task-agnostic spatial information, and a dynamically updated\nstructured memory. Experiments on OSworld benchmarks, real desktop applications\n(Chrome, VSCode, VLC), and cross-task transfer demonstrate that MGA achieves\nsubstantial gains in robustness, generalization, and efficiency compared to\nstate-of-the-art baselines. The code is publicly available at:\n{https://anonymous.4open.science/r/MGA-3571}.", "AI": {"tldr": "\u63d0\u51fa\u4e86Memory-Driven GUI Agent (MGA)\uff0c\u91c7\u7528\"\u5148\u89c2\u5bdf\u540e\u51b3\u7b56\"\u539f\u5219\uff0c\u901a\u8fc7\u5f53\u524d\u622a\u56fe\u3001\u7a7a\u95f4\u4fe1\u606f\u548c\u52a8\u6001\u7ed3\u6784\u5316\u8bb0\u5fc6\u7684\u4e09\u5143\u7ec4\u8868\u793a\u73af\u5883\u72b6\u6001\uff0c\u5728GUI\u4ea4\u4e92\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u9c81\u68d2\u6027\u3001\u6cdb\u5316\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709GUI\u4ee3\u7406\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u5bf9\u5386\u53f2\u8f68\u8ff9\u7684\u4f9d\u8d56\u5bfc\u81f4\u9519\u8bef\u4f20\u64ad\u653e\u5927\uff0c\u4ee5\u53ca\"\u5148\u51b3\u7b56\u540e\u89c2\u5bdf\"\u673a\u5236\u5bfc\u81f4\u7684\u5c40\u90e8\u63a2\u7d22\u504f\u5dee\uff0c\u5ffd\u89c6\u4e86\u5173\u952e\u754c\u9762\u7ebf\u7d22\u3002", "method": "MGA\u5c06\u6bcf\u4e2a\u6b65\u9aa4\u5efa\u6a21\u4e3a\u72ec\u7acb\u7684\u73af\u5883\u72b6\u6001\uff0c\u4f7f\u7528\u4e09\u5143\u7ec4\u8868\u793a\uff1a\u5f53\u524d\u622a\u56fe\u3001\u4efb\u52a1\u65e0\u5173\u7684\u7a7a\u95f4\u4fe1\u606f\u548c\u52a8\u6001\u66f4\u65b0\u7684\u7ed3\u6784\u5316\u8bb0\u5fc6\uff0c\u9075\u5faa\"\u5148\u89c2\u5bdf\u540e\u51b3\u7b56\"\u539f\u5219\u3002", "result": "\u5728OSworld\u57fa\u51c6\u6d4b\u8bd5\u3001\u771f\u5b9e\u684c\u9762\u5e94\u7528\uff08Chrome\u3001VSCode\u3001VLC\uff09\u548c\u8de8\u4efb\u52a1\u8fc1\u79fb\u5b9e\u9a8c\u4e2d\uff0cMGA\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u5728\u9c81\u68d2\u6027\u3001\u6cdb\u5316\u6027\u548c\u6548\u7387\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "conclusion": "MGA\u901a\u8fc7\u91cd\u6784GUI\u4ea4\u4e92\u8303\u5f0f\u4e3a\"\u5148\u89c2\u5bdf\u540e\u51b3\u7b56\"\uff0c\u5e76\u5f15\u5165\u52a8\u6001\u7ed3\u6784\u5316\u8bb0\u5fc6\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3aGUI\u4ee3\u7406\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2510.24554", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24554", "abs": "https://arxiv.org/abs/2510.24554", "authors": ["Vignesh Kottayam Viswanathan", "Yifan Bai", "Scott Fredriksson", "Sumeet Satpute", "Christoforos Kanellakis", "George Nikolakopoulos"], "title": "An Adaptive Inspection Planning Approach Towards Routine Monitoring in Uncertain Environments", "comment": "Submitted for ICRA 2026", "summary": "In this work, we present a hierarchical framework designed to support robotic\ninspection under environment uncertainty. By leveraging a known environment\nmodel, existing methods plan and safely track inspection routes to visit points\nof interest. However, discrepancies between the model and actual site\nconditions, caused by either natural or human activities, can alter the surface\nmorphology or introduce path obstructions. To address this challenge, the\nproposed framework divides the inspection task into: (a) generating the initial\nglobal view-plan for region of interests based on a historical map and (b)\nlocal view replanning to adapt to the current morphology of the inspection\nscene. The proposed hierarchy preserves global coverage objectives while\nenabling reactive adaptation to the local surface morphology. This enables the\nlocal autonomy to remain robust against environment uncertainty and complete\nthe inspection tasks. We validate the approach through deployments in\nreal-world subterranean mines using quadrupedal robot.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5c42\u6846\u67b6\u7528\u4e8e\u673a\u5668\u4eba\u5de1\u68c0\uff0c\u901a\u8fc7\u5168\u5c40\u89c4\u5212\u4e0e\u5c40\u90e8\u91cd\u89c4\u5212\u76f8\u7ed3\u5408\uff0c\u5e94\u5bf9\u73af\u5883\u4e0d\u786e\u5b9a\u6027\uff0c\u5728\u771f\u5b9e\u77ff\u4e95\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5df2\u77e5\u73af\u5883\u6a21\u578b\u89c4\u5212\u5de1\u68c0\u8def\u5f84\uff0c\u4f46\u5b9e\u9645\u73af\u5883\u4e0e\u6a21\u578b\u5b58\u5728\u5dee\u5f02\uff08\u5982\u5730\u5f62\u53d8\u5316\u3001\u8def\u5f84\u963b\u585e\uff09\uff0c\u9700\u8981\u9002\u5e94\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5206\u5c42\u6846\u67b6\uff1a1\uff09\u57fa\u4e8e\u5386\u53f2\u5730\u56fe\u751f\u6210\u5168\u5c40\u89c6\u56fe\u89c4\u5212\uff1b2\uff09\u5c40\u90e8\u89c6\u56fe\u91cd\u89c4\u5212\u4ee5\u9002\u5e94\u5f53\u524d\u5730\u5f62\u5f62\u6001\uff0c\u4fdd\u6301\u5168\u5c40\u8986\u76d6\u76ee\u6807\u7684\u540c\u65f6\u5b9e\u73b0\u5c40\u90e8\u81ea\u9002\u5e94\u3002", "result": "\u5728\u771f\u5b9e\u5730\u4e0b\u77ff\u4e95\u4e2d\u4f7f\u7528\u56db\u8db3\u673a\u5668\u4eba\u90e8\u7f72\u9a8c\u8bc1\uff0c\u6846\u67b6\u80fd\u591f\u4fdd\u6301\u5bf9\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u7684\u9c81\u68d2\u6027\u5e76\u5b8c\u6210\u5de1\u68c0\u4efb\u52a1\u3002", "conclusion": "\u5206\u5c42\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u73af\u5883\u6a21\u578b\u4e0e\u5b9e\u9645\u6761\u4ef6\u5dee\u5f02\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5168\u5c40\u8986\u76d6\u4e0e\u5c40\u90e8\u81ea\u9002\u5e94\u7684\u5e73\u8861\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5de1\u68c0\u7684\u9c81\u68d2\u6027\u548c\u5b8c\u6210\u7387\u3002"}}
{"id": "2510.24284", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24284", "abs": "https://arxiv.org/abs/2510.24284", "authors": ["Wenhao Wang", "Peizhi Niu", "Zhao Xu", "Zhaoyu Chen", "Jian Du", "Yaxin Du", "Xianghe Pang", "Keduan Huang", "Yanfeng Wang", "Qiang Yan", "Siheng Chen"], "title": "MCP-Flow: Facilitating LLM Agents to Master Real-World, Diverse and Scaling MCP Tools", "comment": null, "summary": "Large Language Models (LLMs) increasingly rely on external tools to perform\ncomplex, realistic tasks, yet their ability to utilize the rapidly expanding\nModel Contextual Protocol (MCP) ecosystem remains limited. Existing MCP\nresearch covers few servers, depends on costly manual curation, and lacks\ntraining support, hindering progress toward real-world deployment. To overcome\nthese limitations, we introduce MCP-Flow, an automated web-agent-driven\npipeline for large-scale server discovery, data synthesis, and model training.\nMCP-Flow collects and filters data from 1166 servers and 11536 tools, producing\n68733 high-quality instruction-function call pairs and 6439 trajectories, far\nexceeding prior work in scale and diversity. Extensive experiments demonstrate\nMCP-Flow's effectiveness in driving superior MCP tool selection, function-call\ngeneration, and enhanced agentic task performance. MCP-Flow thus provides a\nscalable foundation for advancing LLM agents' proficiency in real-world MCP\nenvironments. MCP-Flow is publicly available at\n\\href{https://github.com/wwh0411/MCP-Flow}{https://github.com/wwh0411/MCP-Flow}.", "AI": {"tldr": "MCP-Flow\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u7f51\u7edc\u4ee3\u7406\u9a71\u52a8\u7684\u7ba1\u9053\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u670d\u52a1\u5668\u53d1\u73b0\u3001\u6570\u636e\u5408\u6210\u548c\u6a21\u578b\u8bad\u7ec3\uff0c\u65e8\u5728\u63d0\u5347LLM\u5728MCP\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u3002", "motivation": "\u73b0\u6709MCP\u7814\u7a76\u8986\u76d6\u670d\u52a1\u5668\u6570\u91cf\u6709\u9650\uff0c\u4f9d\u8d56\u6602\u8d35\u7684\u624b\u52a8\u6574\u7406\uff0c\u7f3a\u4e4f\u8bad\u7ec3\u652f\u6301\uff0c\u963b\u788d\u4e86\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u8fdb\u5c55\u3002", "method": "\u901a\u8fc7\u81ea\u52a8\u5316\u6d41\u7a0b\u4ece1166\u4e2a\u670d\u52a1\u5668\u548c11536\u4e2a\u5de5\u5177\u4e2d\u6536\u96c6\u548c\u8fc7\u6ee4\u6570\u636e\uff0c\u751f\u621068733\u4e2a\u9ad8\u8d28\u91cf\u6307\u4ee4-\u51fd\u6570\u8c03\u7528\u5bf9\u548c6439\u4e2a\u8f68\u8ff9\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eMCP-Flow\u5728\u5de5\u5177\u9009\u62e9\u3001\u51fd\u6570\u8c03\u7528\u751f\u6210\u548c\u4ee3\u7406\u4efb\u52a1\u6027\u80fd\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u8fdc\u8d85\u5148\u524d\u5de5\u4f5c\u7684\u89c4\u6a21\u548c\u591a\u6837\u6027\u3002", "conclusion": "MCP-Flow\u4e3a\u63d0\u5347LLM\u4ee3\u7406\u5728\u771f\u5b9e\u4e16\u754cMCP\u73af\u5883\u4e2d\u7684\u719f\u7ec3\u5ea6\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u57fa\u7840\u3002"}}
{"id": "2510.24571", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24571", "abs": "https://arxiv.org/abs/2510.24571", "authors": ["Hongxu Zhao", "Guangyang Zeng", "Yunling Shao", "Tengfei Zhang", "Junfeng Wu"], "title": "Spatiotemporal Calibration of Doppler Velocity Logs for Underwater Robots", "comment": null, "summary": "The calibration of extrinsic parameters and clock offsets between sensors for\nhigh-accuracy performance in underwater SLAM systems remains insufficiently\nexplored. Existing methods for Doppler Velocity Log (DVL) calibration are\neither constrained to specific sensor configurations or rely on oversimplified\nassumptions, and none jointly estimate translational extrinsics and time\noffsets. We propose a Unified Iterative Calibration (UIC) framework for general\nDVL sensor setups, formulated as a Maximum A Posteriori (MAP) estimation with a\nGaussian Process (GP) motion prior for high-fidelity motion interpolation. UIC\nalternates between efficient GP-based motion state updates and gradient-based\ncalibration variable updates, supported by a provably statistically consistent\nsequential initialization scheme. The proposed UIC can be applied to IMU,\ncameras and other modalities as co-sensors. We release an open-source\nDVL-camera calibration toolbox. Beyond underwater applications, several aspects\nof UIC-such as the integration of GP priors for MAP-based calibration and the\ndesign of provably reliable initialization procedures-are broadly applicable to\nother multi-sensor calibration problems. Finally, simulations and real-world\ntests validate our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u7edf\u4e00\u8fed\u4ee3\u6821\u51c6(UIC)\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u4f20\u611f\u5668\u7cfb\u7edf\u7684\u5916\u53c2\u548c\u65f6\u95f4\u504f\u79fb\u8054\u5408\u4f30\u8ba1\uff0c\u7279\u522b\u9488\u5bf9\u6c34\u4e0bSLAM\u4e2d\u7684DVL\u4f20\u611f\u5668\u6821\u51c6\u95ee\u9898\u3002", "motivation": "\u73b0\u6709DVL\u6821\u51c6\u65b9\u6cd5\u8981\u4e48\u5c40\u9650\u4e8e\u7279\u5b9a\u4f20\u611f\u5668\u914d\u7f6e\uff0c\u8981\u4e48\u4f9d\u8d56\u8fc7\u5ea6\u7b80\u5316\u7684\u5047\u8bbe\uff0c\u4e14\u6ca1\u6709\u540c\u65f6\u4f30\u8ba1\u5e73\u79fb\u5916\u53c2\u548c\u65f6\u95f4\u504f\u79fb\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u6700\u5927\u540e\u9a8c\u6982\u7387\u4f30\u8ba1\u6846\u67b6\uff0c\u7ed3\u5408\u9ad8\u65af\u8fc7\u7a0b\u8fd0\u52a8\u5148\u9a8c\u8fdb\u884c\u9ad8\u4fdd\u771f\u8fd0\u52a8\u63d2\u503c\uff0c\u4ea4\u66ff\u6267\u884c\u9ad8\u6548\u7684GP\u8fd0\u52a8\u72b6\u6001\u66f4\u65b0\u548c\u57fa\u4e8e\u68af\u5ea6\u7684\u6821\u51c6\u53d8\u91cf\u66f4\u65b0\u3002", "result": "\u5f00\u53d1\u4e86\u5f00\u6e90\u7684DVL-\u76f8\u673a\u6821\u51c6\u5de5\u5177\u7bb1\uff0c\u901a\u8fc7\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "UIC\u6846\u67b6\u4e0d\u4ec5\u9002\u7528\u4e8e\u6c34\u4e0b\u5e94\u7528\uff0c\u5176GP\u5148\u9a8c\u96c6\u6210\u548c\u53ef\u9760\u521d\u59cb\u5316\u7a0b\u5e8f\u7b49\u7279\u6027\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5176\u4ed6\u591a\u4f20\u611f\u5668\u6821\u51c6\u95ee\u9898\u3002"}}
{"id": "2510.24297", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24297", "abs": "https://arxiv.org/abs/2510.24297", "authors": ["Robin Schm\u00f6cker", "Alexander Dockhorn", "Bodo Rosenhahn"], "title": "Investigating Intra-Abstraction Policies For Non-exact Abstraction Algorithms", "comment": null, "summary": "One weakness of Monte Carlo Tree Search (MCTS) is its sample efficiency which\ncan be addressed by building and using state and/or action abstractions in\nparallel to the tree search such that information can be shared among nodes of\nthe same layer. The primary usage of abstractions for MCTS is to enhance the\nUpper Confidence Bound (UCB) value during the tree policy by aggregating visits\nand returns of an abstract node. However, this direct usage of abstractions\ndoes not take the case into account where multiple actions with the same parent\nmight be in the same abstract node, as these would then all have the same UCB\nvalue, thus requiring a tiebreak rule. In state-of-the-art abstraction\nalgorithms such as pruned On the Go Abstractions (pruned OGA), this case has\nnot been noticed, and a random tiebreak rule was implicitly chosen. In this\npaper, we propose and empirically evaluate several alternative\nintra-abstraction policies, several of which outperform the random policy\nacross a majority of environments and parameter settings.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9MCTS\u4e2d\u62bd\u8c61\u8282\u70b9\u5185\u591a\u4e2a\u52a8\u4f5c\u5177\u6709\u76f8\u540cUCB\u503c\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u51e0\u79cd\u66ff\u4ee3\u968f\u673a\u5e73\u5c40\u89c4\u5219\u7684\u5185\u90e8\u62bd\u8c61\u7b56\u7565\uff0c\u5e76\u8bc1\u660e\u5176\u4e2d\u591a\u6570\u7b56\u7565\u5728\u5927\u591a\u6570\u73af\u5883\u548c\u53c2\u6570\u8bbe\u7f6e\u4e0b\u4f18\u4e8e\u968f\u673a\u7b56\u7565\u3002", "motivation": "MCTS\u7684\u6837\u672c\u6548\u7387\u95ee\u9898\u53ef\u4ee5\u901a\u8fc7\u72b6\u6001/\u52a8\u4f5c\u62bd\u8c61\u6765\u89e3\u51b3\uff0c\u4f46\u73b0\u6709\u62bd\u8c61\u7b97\u6cd5\uff08\u5982pruned OGA\uff09\u5728\u591a\u4e2a\u52a8\u4f5c\u5c5e\u4e8e\u540c\u4e00\u62bd\u8c61\u8282\u70b9\u65f6\uff0c\u7531\u4e8eUCB\u503c\u76f8\u540c\u800c\u9700\u8981\u5e73\u5c40\u89c4\u5219\uff0c\u76ee\u524d\u4ec5\u4f7f\u7528\u968f\u673a\u5e73\u5c40\u89c4\u5219\uff0c\u5b58\u5728\u6539\u8fdb\u7a7a\u95f4\u3002", "method": "\u63d0\u51fa\u5e76\u5b9e\u8bc1\u8bc4\u4f30\u4e86\u591a\u79cd\u5185\u90e8\u62bd\u8c61\u7b56\u7565\uff08intra-abstraction policies\uff09\uff0c\u4f5c\u4e3a\u968f\u673a\u5e73\u5c40\u89c4\u5219\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u591a\u79cd\u5185\u90e8\u62bd\u8c61\u7b56\u7565\u5728\u5927\u591a\u6570\u73af\u5883\u548c\u53c2\u6570\u8bbe\u7f6e\u4e0b\u8868\u73b0\u4f18\u4e8e\u968f\u673a\u7b56\u7565\u3002", "conclusion": "\u901a\u8fc7\u6539\u8fdb\u62bd\u8c61\u8282\u70b9\u5185\u90e8\u7684\u5e73\u5c40\u89c4\u5219\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347MCTS\u62bd\u8c61\u7b97\u6cd5\u7684\u6027\u80fd\uff0c\u4e3aMCTS\u7684\u6837\u672c\u6548\u7387\u95ee\u9898\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24584", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24584", "abs": "https://arxiv.org/abs/2510.24584", "authors": ["J\u00f8rgen Anker Olsen", "Lars R\u00f8nhaug Pettersen", "Kostas Alexis"], "title": "Towards Quadrupedal Jumping and Walking for Dynamic Locomotion using Reinforcement Learning", "comment": "8 pages", "summary": "This paper presents a curriculum-based reinforcement learning framework for\ntraining precise and high-performance jumping policies for the robot `Olympus'.\nSeparate policies are developed for vertical and horizontal jumps, leveraging a\nsimple yet effective strategy. First, we densify the inherently sparse jumping\nreward using the laws of projectile motion. Next, a reference state\ninitialization scheme is employed to accelerate the exploration of dynamic\njumping behaviors without reliance on reference trajectories. We also present a\nwalking policy that, when combined with the jumping policies, unlocks versatile\nand dynamic locomotion capabilities. Comprehensive testing validates walking on\nvaried terrain surfaces and jumping performance that exceeds previous works,\neffectively crossing the Sim2Real gap. Experimental validation demonstrates\nhorizontal jumps up to 1.25 m with centimeter accuracy and vertical jumps up to\n1.0 m. Additionally, we show that with only minor modifications, the proposed\nmethod can be used to learn omnidirectional jumping.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8bfe\u7a0b\u5b66\u4e60\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8bad\u7ec3\u673a\u5668\u4ebaOlympus\u5b9e\u73b0\u7cbe\u786e\u9ad8\u6027\u80fd\u8df3\u8dc3\u3002\u5206\u522b\u5f00\u53d1\u5782\u76f4\u548c\u6c34\u5e73\u8df3\u8dc3\u7b56\u7565\uff0c\u901a\u8fc7\u5f39\u9053\u8fd0\u52a8\u89c4\u5f8b\u7a20\u5bc6\u5316\u7a00\u758f\u5956\u52b1\uff0c\u4f7f\u7528\u53c2\u8003\u72b6\u6001\u521d\u59cb\u5316\u52a0\u901f\u63a2\u7d22\uff0c\u7ed3\u5408\u884c\u8d70\u7b56\u7565\u5b9e\u73b0\u591a\u529f\u80fd\u52a8\u6001\u8fd0\u52a8\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e861.25\u7c73\u6c34\u5e73\u8df3\u8dc3\u548c1.0\u7c73\u5782\u76f4\u8df3\u8dc3\u7684\u5398\u7c73\u7ea7\u7cbe\u5ea6\u3002", "motivation": "\u8bad\u7ec3\u673a\u5668\u4eba\u5b9e\u73b0\u7cbe\u786e\u9ad8\u6027\u80fd\u8df3\u8dc3\u9762\u4e34\u5956\u52b1\u7a00\u758f\u548c\u63a2\u7d22\u56f0\u96be\u7b49\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u5b66\u4e60\u6846\u67b6\u6765\u514b\u670d\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u8bfe\u7a0b\u5f0f\u5f3a\u5316\u5b66\u4e60\uff0c\u5206\u522b\u8bad\u7ec3\u5782\u76f4\u548c\u6c34\u5e73\u8df3\u8dc3\u7b56\u7565\uff1b\u5229\u7528\u5f39\u9053\u8fd0\u52a8\u89c4\u5f8b\u7a20\u5bc6\u5316\u7a00\u758f\u5956\u52b1\uff1b\u4f7f\u7528\u53c2\u8003\u72b6\u6001\u521d\u59cb\u5316\u65b9\u6848\u52a0\u901f\u52a8\u6001\u8df3\u8dc3\u884c\u4e3a\u63a2\u7d22\uff1b\u7ed3\u5408\u884c\u8d70\u7b56\u7565\u5b9e\u73b0\u591a\u529f\u80fd\u8fd0\u52a8\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5728\u591a\u6837\u5316\u5730\u5f62\u4e0a\u7684\u884c\u8d70\u80fd\u529b\uff0c\u8df3\u8dc3\u6027\u80fd\u8d85\u8d8a\u5148\u524d\u5de5\u4f5c\uff0c\u6c34\u5e73\u8df3\u8dc3\u8fbe1.25\u7c73\uff08\u5398\u7c73\u7cbe\u5ea6\uff09\uff0c\u5782\u76f4\u8df3\u8dc3\u8fbe1.0\u7c73\uff0c\u6210\u529f\u8de8\u8d8aSim2Real\u5dee\u8ddd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u8df3\u8dc3\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u7cbe\u786e\u8df3\u8dc3\uff0c\u4e14\u53ea\u9700\u5c11\u91cf\u4fee\u6539\u5373\u53ef\u5b66\u4e60\u5168\u5411\u8df3\u8dc3\uff0c\u5c55\u73b0\u4e86\u826f\u597d\u7684\u901a\u7528\u6027\u3002"}}
{"id": "2510.24299", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24299", "abs": "https://arxiv.org/abs/2510.24299", "authors": ["Jiayu Liu", "Wei Dai", "Zhenya Huang", "Ning Miao", "Enhong Chen"], "title": "Verifying Large Language Models' Reasoning Paths via Correlation Matrix Rank", "comment": null, "summary": "Despite the strong reasoning ability of large language models~(LLMs), they\nare prone to errors and hallucinations. As a result, how to check their outputs\neffectively and efficiently has become a critical problem in their\napplications. Existing checking methods heavily rely on external resources,\nsuch as trained verifiers (e.g., process/outcome reward models) or elaborate\nprompts, which lead to high computational overhead and are only applicable to\nspecific domains. In this paper, we investigate whether the internal behaviors\nof LLMs have already implied the credibility of their reasoning paths.\nSpecifically, we find that the rank of the correlation matrix between the input\nproblem and the output reasoning path is a robust indicator of reasoning\ncorrectness. Different from other correctness indicators for LLMs, the\ncalculation of the correlation matrix only relies on the LLM itself, which\navoids the hassle of training a separate model or designing complicated\nprompts. Based on it, we design a simple, plug-and-play Self-Indicator method\nto reweight candidate reasoning paths, which achieves significant performance\nimprovements than other voting and verification methods with very few\ncomputational overhead. Our experiments across multiple LLMs of varying scales\nand model families have further shown the effectiveness of Self-Indicator. It\nachieves over 75% accuracy in distinguishing correct reasoning paths from\nincorrect ones, and, in turn, improves the accuracies on three reasoning\nbenchmarks by more than 8%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u5185\u90e8\u884c\u4e3a\u7684\u81ea\u6211\u6307\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba1\u7b97\u8f93\u5165\u95ee\u9898\u4e0e\u8f93\u51fa\u63a8\u7406\u8def\u5f84\u7684\u76f8\u5173\u77e9\u9635\u79e9\u6765\u5224\u65ad\u63a8\u7406\u6b63\u786e\u6027\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u590d\u6742\u63d0\u793a\u3002", "motivation": "\u73b0\u6709\u68c0\u67e5\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u8d44\u6e90\uff08\u5982\u8bad\u7ec3\u9a8c\u8bc1\u5668\u6216\u590d\u6742\u63d0\u793a\uff09\uff0c\u8ba1\u7b97\u5f00\u9500\u5927\u4e14\u4ec5\u9002\u7528\u4e8e\u7279\u5b9a\u9886\u57df\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u901a\u7528\u7684\u68c0\u67e5\u65b9\u6cd5\u3002", "method": "\u5229\u7528LLM\u5185\u90e8\u884c\u4e3a\uff0c\u901a\u8fc7\u8ba1\u7b97\u8f93\u5165\u95ee\u9898\u4e0e\u8f93\u51fa\u63a8\u7406\u8def\u5f84\u7684\u76f8\u5173\u77e9\u9635\u79e9\u4f5c\u4e3a\u63a8\u7406\u6b63\u786e\u6027\u6307\u6807\uff0c\u8bbe\u8ba1\u81ea\u6307\u793a\u65b9\u6cd5\u5bf9\u5019\u9009\u63a8\u7406\u8def\u5f84\u8fdb\u884c\u91cd\u52a0\u6743\u3002", "result": "\u5728\u591a\u4e2a\u4e0d\u540c\u89c4\u6a21\u548c\u5bb6\u65cf\u7684LLM\u4e0a\u9a8c\u8bc1\u6709\u6548\uff0c\u533a\u5206\u6b63\u786e\u4e0e\u9519\u8bef\u63a8\u7406\u8def\u5f84\u7684\u51c6\u786e\u7387\u8d85\u8fc775%\uff0c\u5728\u4e09\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u7684\u51c6\u786e\u7387\u63d0\u5347\u8d85\u8fc78%\u3002", "conclusion": "LLM\u5185\u90e8\u884c\u4e3a\u5df2\u9690\u542b\u5176\u63a8\u7406\u8def\u5f84\u7684\u53ef\u4fe1\u5ea6\uff0c\u63d0\u51fa\u7684\u81ea\u6307\u793a\u65b9\u6cd5\u7b80\u5355\u3001\u5373\u63d2\u5373\u7528\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u6295\u7968\u548c\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002"}}
{"id": "2510.24623", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24623", "abs": "https://arxiv.org/abs/2510.24623", "authors": ["Nicolai Steinke", "Daniel Goehring"], "title": "GroundLoc: Efficient Large-Scale Outdoor LiDAR-Only Localization", "comment": null, "summary": "In this letter, we introduce GroundLoc, a LiDAR-only localization pipeline\ndesigned to localize a mobile robot in large-scale outdoor environments using\nprior maps. GroundLoc employs a Bird's-Eye View (BEV) image projection focusing\non the perceived ground area and utilizes the place recognition network R2D2,\nor alternatively, the non-learning approach Scale-Invariant Feature Transform\n(SIFT), to identify and select keypoints for BEV image map registration. Our\nresults demonstrate that GroundLoc outperforms state-of-the-art methods on the\nSemanticKITTI and HeLiPR datasets across various sensors. In the multi-session\nlocalization evaluation, GroundLoc reaches an Average Trajectory Error (ATE)\nwell below 50 cm on all Ouster OS2 128 sequences while meeting online runtime\nrequirements. The system supports various sensor models, as evidenced by\nevaluations conducted with Velodyne HDL-64E, Ouster OS2 128, Aeva Aeries II,\nand Livox Avia sensors. The prior maps are stored as 2D raster image maps,\nwhich can be created from a single drive and require only 4 MB of storage per\nsquare kilometer. The source code is available at\nhttps://github.com/dcmlr/groundloc.", "AI": {"tldr": "GroundLoc\u662f\u4e00\u79cd\u4ec5\u4f7f\u7528LiDAR\u7684\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u901a\u8fc7BEV\u56fe\u50cf\u6295\u5f71\u548c\u5173\u952e\u70b9\u8bc6\u522b\u6280\u672f\uff0c\u5728\u5927\u89c4\u6a21\u5ba4\u5916\u73af\u5883\u4e2d\u5b9e\u73b0\u79fb\u52a8\u673a\u5668\u4eba\u5b9a\u4f4d\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u9ad8\u6548\u3001\u8f7b\u91cf\u7ea7\u7684LiDAR\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u5927\u578b\u5ba4\u5916\u73af\u5883\u4e2d\u5b9e\u73b0\u7cbe\u786e\u7684\u5728\u7ebf\u5b9a\u4f4d\uff0c\u540c\u65f6\u652f\u6301\u591a\u79cd\u4f20\u611f\u5668\u6a21\u578b\u3002", "method": "\u4f7f\u7528BEV\u56fe\u50cf\u6295\u5f71\u805a\u7126\u611f\u77e5\u5730\u9762\u533a\u57df\uff0c\u7ed3\u5408R2D2\u6216SIFT\u5173\u952e\u70b9\u8bc6\u522b\u6280\u672f\u8fdb\u884c\u5730\u56fe\u914d\u51c6\uff0c\u5c06\u5148\u9a8c\u5730\u56fe\u5b58\u50a8\u4e3a2D\u6805\u683c\u56fe\u50cf\u3002", "result": "\u5728SemanticKITTI\u548cHeLiPR\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u591a\u4f1a\u8bdd\u5b9a\u4f4d\u5e73\u5747\u8f68\u8ff9\u8bef\u5dee\u4f4e\u4e8e50cm\uff0c\u6ee1\u8db3\u5728\u7ebf\u8fd0\u884c\u8981\u6c42\uff0c\u652f\u6301\u591a\u79cd\u4f20\u611f\u5668\u3002", "conclusion": "GroundLoc\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u8f7b\u91cf\u7ea7\u7684LiDAR\u5b9a\u4f4d\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2510.24303", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24303", "abs": "https://arxiv.org/abs/2510.24303", "authors": ["Deniz Gorur", "Antoni Rago", "Francesca Toni"], "title": "Retrieval and Argumentation Enhanced Multi-Agent LLMs for Judgmental Forecasting", "comment": null, "summary": "Judgmental forecasting is the task of making predictions about future events\nbased on human judgment. This task can be seen as a form of claim verification,\nwhere the claim corresponds to a future event and the task is to assess the\nplausibility of that event. In this paper, we propose a novel multi-agent\nframework for claim verification, whereby different agents may disagree on\nclaim veracity and bring specific evidence for and against the claims,\nrepresented as quantitative bipolar argumentation frameworks (QBAFs). We then\ninstantiate the framework for supporting claim verification, with a variety of\nagents realised with Large Language Models (LLMs): (1) ArgLLM agents, an\nexisting approach for claim verification that generates and evaluates QBAFs;\n(2) RbAM agents, whereby LLM-empowered Relation-based Argument Mining (RbAM)\nfrom external sources is used to generate QBAFs; (3) RAG-ArgLLM agents,\nextending ArgLLM agents with a form of Retrieval-Augmented Generation (RAG) of\narguments from external sources. Finally, we conduct experiments with two\nstandard judgmental forecasting datasets, with instances of our framework with\ntwo or three agents, empowered by six different base LLMs. We observe that\ncombining evidence from agents can improve forecasting accuracy, especially in\nthe case of three agents, while providing an explainable combination of\nevidence for claim verification.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5224\u65ad\u6027\u9884\u6d4b\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u4e0d\u540c\u667a\u80fd\u4f53\u5bf9\u58f0\u660e\u771f\u5b9e\u6027\u4ea7\u751f\u5206\u6b67\u5e76\u6536\u96c6\u6b63\u53cd\u8bc1\u636e\uff0c\u4f7f\u7528\u5b9a\u91cf\u53cc\u6781\u8bba\u8bc1\u6846\u67b6\u8868\u793a\uff0c\u5b9e\u9a8c\u8868\u660e\u591a\u667a\u80fd\u4f53\u7ec4\u5408\u80fd\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u5c06\u5224\u65ad\u6027\u9884\u6d4b\u89c6\u4e3a\u58f0\u660e\u9a8c\u8bc1\u4efb\u52a1\uff0c\u9700\u8981\u8bc4\u4f30\u672a\u6765\u4e8b\u4ef6\u7684\u53ef\u80fd\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u7f3a\u4e4f\u591a\u89c6\u89d2\u8bc1\u636e\u6574\u5408\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u8bbe\u8ba1\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542bArgLLM\u3001RbAM\u548cRAG-ArgLLM\u4e09\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\uff0c\u5206\u522b\u91c7\u7528\u4e0d\u540c\u65b9\u6cd5\u751f\u6210\u548c\u8bc4\u4f30\u5b9a\u91cf\u53cc\u6781\u8bba\u8bc1\u6846\u67b6\u3002", "result": "\u5728\u6807\u51c6\u5224\u65ad\u6027\u9884\u6d4b\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u7279\u522b\u662f\u4e09\u4e2a\u667a\u80fd\u4f53\u7ec4\u5408\u65f6\uff0c\u80fd\u663e\u8457\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u8bc1\u636e\u7ec4\u5408\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u6846\u67b6\u80fd\u6709\u6548\u6574\u5408\u4e0d\u540c\u8bc1\u636e\u6e90\uff0c\u63d0\u9ad8\u5224\u65ad\u6027\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u58f0\u660e\u9a8c\u8bc1\u63d0\u4f9b\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2510.24671", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24671", "abs": "https://arxiv.org/abs/2510.24671", "authors": ["Li Li", "Tobias Brinkmann", "Till Temmen", "Markus Eisenbarth", "Jakob Andert"], "title": "Multi-Agent Scenario Generation in Roundabouts with a Transformer-enhanced Conditional Variational Autoencoder", "comment": null, "summary": "With the increasing integration of intelligent driving functions into\nserial-produced vehicles, ensuring their functionality and robustness poses\ngreater challenges. Compared to traditional road testing, scenario-based\nvirtual testing offers significant advantages in terms of time and cost\nefficiency, reproducibility, and exploration of edge cases. We propose a\nTransformer-enhanced Conditional Variational Autoencoder (CVAE-T) model for\ngenerating multi-agent traffic scenarios in roundabouts, which are\ncharacterized by high vehicle dynamics and complex layouts, yet remain\nrelatively underexplored in current research. The results show that the\nproposed model can accurately reconstruct original scenarios and generate\nrealistic, diverse synthetic scenarios. Besides, two Key-Performance-Indicators\n(KPIs) are employed to evaluate the interactive behavior in the generated\nscenarios. Analysis of the latent space reveals partial disentanglement, with\nseveral latent dimensions exhibiting distinct and interpretable effects on\nscenario attributes such as vehicle entry timing, exit timing, and velocity\nprofiles. The results demonstrate the model's capability to generate scenarios\nfor the validation of intelligent driving functions involving multi-agent\ninteractions, as well as to augment data for their development and iterative\nimprovement.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u589e\u5f3a\u7684\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668(CVAE-T)\u6a21\u578b\uff0c\u7528\u4e8e\u751f\u6210\u73af\u5c9b\u591a\u667a\u80fd\u4f53\u4ea4\u901a\u573a\u666f\uff0c\u4ee5\u652f\u6301\u667a\u80fd\u9a7e\u9a76\u529f\u80fd\u7684\u9a8c\u8bc1\u548c\u5f00\u53d1\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u9a7e\u9a76\u529f\u80fd\u5728\u91cf\u4ea7\u8f66\u4e2d\u7684\u96c6\u6210\u5ea6\u63d0\u9ad8\uff0c\u786e\u4fdd\u5176\u529f\u80fd\u6027\u548c\u9c81\u68d2\u6027\u9762\u4e34\u66f4\u5927\u6311\u6218\u3002\u76f8\u6bd4\u4f20\u7edf\u9053\u8def\u6d4b\u8bd5\uff0c\u57fa\u4e8e\u573a\u666f\u7684\u865a\u62df\u6d4b\u8bd5\u5728\u65f6\u95f4\u6210\u672c\u3001\u53ef\u91cd\u590d\u6027\u548c\u8fb9\u7f18\u6848\u4f8b\u63a2\u7d22\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "method": "\u4f7f\u7528Transformer\u589e\u5f3a\u7684\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668(CVAE-T)\u6a21\u578b\uff0c\u9488\u5bf9\u5177\u6709\u9ad8\u8f66\u8f86\u52a8\u6001\u548c\u590d\u6742\u5e03\u5c40\u7684\u73af\u5c9b\u573a\u666f\uff0c\u751f\u6210\u591a\u667a\u80fd\u4f53\u4ea4\u901a\u573a\u666f\u3002", "result": "\u6a21\u578b\u80fd\u591f\u51c6\u786e\u91cd\u5efa\u539f\u59cb\u573a\u666f\u5e76\u751f\u6210\u771f\u5b9e\u591a\u6837\u7684\u5408\u6210\u573a\u666f\u3002\u6f5c\u5728\u7a7a\u95f4\u5206\u6790\u663e\u793a\u90e8\u5206\u89e3\u7ea0\u7f20\uff0c\u591a\u4e2a\u6f5c\u5728\u7ef4\u5ea6\u5bf9\u573a\u666f\u5c5e\u6027\uff08\u5982\u8f66\u8f86\u8fdb\u51fa\u65f6\u95f4\u3001\u901f\u5ea6\u66f2\u7ebf\uff09\u5177\u6709\u53ef\u89e3\u91ca\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u6a21\u578b\u80fd\u591f\u751f\u6210\u7528\u4e8e\u9a8c\u8bc1\u6d89\u53ca\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u7684\u667a\u80fd\u9a7e\u9a76\u529f\u80fd\u7684\u573a\u666f\uff0c\u5e76\u4e3a\u5f00\u53d1\u548c\u8fed\u4ee3\u6539\u8fdb\u63d0\u4f9b\u6570\u636e\u589e\u5f3a\u3002"}}
{"id": "2510.24339", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24339", "abs": "https://arxiv.org/abs/2510.24339", "authors": ["Yunxuan Jiang", "Silan Hu", "Xiaoning Wang", "Yuanyuan Zhang", "Xiangyu Chang"], "title": "VDSAgents: A PCS-Guided Multi-Agent System for Veridical Data Science Automation", "comment": "29 pages, 6 figures. Yunxuan Jiang and Silan Hu contributed equally.\n  Code available at https://github.com/fengzer/VDSAgents", "summary": "Large language models (LLMs) become increasingly integrated into data science\nworkflows for automated system design. However, these LLM-driven data science\nsystems rely solely on the internal reasoning of LLMs, lacking guidance from\nscientific and theoretical principles. This limits their trustworthiness and\nrobustness, especially when dealing with noisy and complex real-world datasets.\nThis paper provides VDSAgents, a multi-agent system grounded in the\nPredictability-Computability-Stability (PCS) principles proposed in the\nVeridical Data Science (VDS) framework. Guided by PCS principles, the system\nimplements a modular workflow for data cleaning, feature engineering, modeling,\nand evaluation. Each phase is handled by an elegant agent, incorporating\nperturbation analysis, unit testing, and model validation to ensure both\nfunctionality and scientific auditability. We evaluate VDSAgents on nine\ndatasets with diverse characteristics, comparing it with state-of-the-art\nend-to-end data science systems, such as AutoKaggle and DataInterpreter, using\nDeepSeek-V3 and GPT-4o as backends. VDSAgents consistently outperforms the\nresults of AutoKaggle and DataInterpreter, which validates the feasibility of\nembedding PCS principles into LLM-driven data science automation.", "AI": {"tldr": "VDSAgents\u662f\u4e00\u4e2a\u57fa\u4e8ePCS\u539f\u5219\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u63d0\u5347LLM\u9a71\u52a8\u6570\u636e\u79d1\u5b66\u7cfb\u7edf\u7684\u53ef\u4fe1\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u7aef\u5230\u7aef\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524dLLM\u9a71\u52a8\u7684\u6570\u636e\u79d1\u5b66\u7cfb\u7edf\u4ec5\u4f9d\u8d56\u6a21\u578b\u5185\u90e8\u63a8\u7406\uff0c\u7f3a\u4e4f\u79d1\u5b66\u548c\u7406\u8bba\u539f\u5219\u6307\u5bfc\uff0c\u5728\u5904\u7406\u566a\u58f0\u548c\u590d\u6742\u73b0\u5b9e\u6570\u636e\u96c6\u65f6\u53ef\u4fe1\u5ea6\u548c\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "\u57fa\u4e8ePCS\u539f\u5219\u6784\u5efa\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u91c7\u7528\u6a21\u5757\u5316\u5de5\u4f5c\u6d41\u5904\u7406\u6570\u636e\u6e05\u6d17\u3001\u7279\u5f81\u5de5\u7a0b\u3001\u5efa\u6a21\u548c\u8bc4\u4f30\uff0c\u6bcf\u4e2a\u9636\u6bb5\u7531\u4e13\u95e8\u667a\u80fd\u4f53\u8d1f\u8d23\uff0c\u7ed3\u5408\u6270\u52a8\u5206\u6790\u3001\u5355\u5143\u6d4b\u8bd5\u548c\u6a21\u578b\u9a8c\u8bc1\u3002", "result": "\u57289\u4e2a\u4e0d\u540c\u7279\u5f81\u7684\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u4f7f\u7528DeepSeek-V3\u548cGPT-4o\u4f5c\u4e3a\u540e\u7aef\uff0cVDSAgents\u6301\u7eed\u4f18\u4e8eAutoKaggle\u548cDataInterpreter\u7b49\u6700\u5148\u8fdb\u7aef\u5230\u7aef\u7cfb\u7edf\u3002", "conclusion": "\u5c06PCS\u539f\u5219\u5d4c\u5165LLM\u9a71\u52a8\u6570\u636e\u79d1\u5b66\u81ea\u52a8\u5316\u662f\u53ef\u884c\u7684\uff0c\u80fd\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2510.24680", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24680", "abs": "https://arxiv.org/abs/2510.24680", "authors": ["Zishuo Wang", "Joel Loo", "David Hsu"], "title": "Fare: Failure Resilience in Learned Visual Navigation Control", "comment": null, "summary": "While imitation learning (IL) enables effective visual navigation, IL\npolicies are prone to unpredictable failures in out-of-distribution (OOD)\nscenarios. We advance the notion of failure-resilient policies, which not only\ndetect failures but also recover from them automatically. Failure recognition\nthat identifies the factors causing failure is key to informing recovery: e.g.\npinpointing image regions triggering failure detections can provide cues to\nguide recovery. We present Fare, a framework to construct failure-resilient IL\npolicies, embedding OOD-detection and recognition in them without using\nexplicit failure data, and pairing them with recovery heuristics. Real-world\nexperiments show that Fare enables failure recovery across two different policy\narchitectures, enabling robust long-range navigation in complex environments.", "AI": {"tldr": "Fare\u6846\u67b6\u6784\u5efa\u5177\u6709\u6545\u969c\u6062\u590d\u80fd\u529b\u7684\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7OOD\u68c0\u6d4b\u548c\u8bc6\u522b\u5b9e\u73b0\u81ea\u52a8\u6545\u969c\u68c0\u6d4b\u4e0e\u6062\u590d\uff0c\u65e0\u9700\u663e\u5f0f\u6545\u969c\u6570\u636e\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u5728\u5206\u5e03\u5916\u573a\u666f\u4e2d\u5bb9\u6613\u53d1\u751f\u4e0d\u53ef\u9884\u6d4b\u7684\u6545\u969c\uff0c\u9700\u8981\u80fd\u591f\u81ea\u52a8\u68c0\u6d4b\u5e76\u4ece\u6545\u969c\u4e2d\u6062\u590d\u7684\u7a33\u5065\u7b56\u7565\u3002", "method": "\u5728\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u4e2d\u5d4c\u5165OOD\u68c0\u6d4b\u548c\u8bc6\u522b\u673a\u5236\uff0c\u65e0\u9700\u4f7f\u7528\u663e\u5f0f\u6545\u969c\u6570\u636e\uff0c\u5e76\u7ed3\u5408\u6062\u590d\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "result": "\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8868\u660e\uff0cFare\u80fd\u591f\u5728\u4e24\u79cd\u4e0d\u540c\u7b56\u7565\u67b6\u6784\u4e0a\u5b9e\u73b0\u6545\u969c\u6062\u590d\uff0c\u652f\u6301\u590d\u6742\u73af\u5883\u4e2d\u7684\u7a33\u5065\u957f\u8ddd\u79bb\u5bfc\u822a\u3002", "conclusion": "Fare\u6846\u67b6\u6210\u529f\u6784\u5efa\u4e86\u5177\u6709\u6545\u969c\u6062\u590d\u80fd\u529b\u7684\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u5bfc\u822a\u7cfb\u7edf\u7684\u7a33\u5065\u6027\u3002"}}
{"id": "2510.24342", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24342", "abs": "https://arxiv.org/abs/2510.24342", "authors": ["Silin Chen", "Yuzhong Chen", "Zifan Wang", "Junhao Wang", "Zifeng Jia", "Keith M Kendrick", "Tuo Zhang", "Lin Zhao", "Dezhong Yao", "Tianming Liu", "Xi Jiang"], "title": "A Unified Geometric Space Bridging AI Models and the Human Brain", "comment": null, "summary": "For decades, neuroscientists and computer scientists have pursued a shared\nambition: to understand intelligence and build it. Modern artificial neural\nnetworks now rival humans in language, perception, and reasoning, yet it is\nstill largely unknown whether these artificial systems organize information as\nthe brain does. Existing brain-AI alignment studies have shown the striking\ncorrespondence between the two systems, but such comparisons remain bound to\nspecific inputs and tasks, offering no common ground for comparing how AI\nmodels with different kinds of modalities-vision, language, or multimodal-are\nintrinsically organized. Here we introduce a groundbreaking concept of\nBrain-like Space: a unified geometric space in which every AI model can be\nprecisely situated and compared by mapping its intrinsic spatial attention\ntopological organization onto canonical human functional brain networks,\nregardless of input modality, task, or sensory domain. Our extensive analysis\nof 151 Transformer-based models spanning state-of-the-art large vision models,\nlarge language models, and large multimodal models uncovers a continuous\narc-shaped geometry within this space, reflecting a gradual increase of\nbrain-likeness; different models exhibit distinct distribution patterns within\nthis geometry associated with different degrees of brain-likeness, shaped not\nmerely by their modality but by whether the pretraining paradigm emphasizes\nglobal semantic abstraction and whether the positional encoding scheme\nfacilitates deep fusion across different modalities. Moreover, the degree of\nbrain-likeness for a model and its downstream task performance are not\n\"identical twins\". The Brain-like Space provides the first unified framework\nfor situating, quantifying, and comparing intelligence across domains,\nrevealing the deep organizational principles that bridge machines and the\nbrain.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\"\u7c7b\u8111\u7a7a\u95f4\"\u6982\u5ff5\uff0c\u8fd9\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u51e0\u4f55\u7a7a\u95f4\uff0c\u53ef\u4ee5\u5c06\u4efb\u4f55AI\u6a21\u578b\u7684\u5185\u5728\u7a7a\u95f4\u6ce8\u610f\u529b\u62d3\u6251\u7ec4\u7ec7\u6620\u5c04\u5230\u4eba\u7c7b\u529f\u80fd\u8111\u7f51\u7edc\u4e0a\uff0c\u5b9e\u73b0\u8de8\u6a21\u6001\u3001\u8de8\u4efb\u52a1\u7684\u6a21\u578b\u6bd4\u8f83\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8111-AI\u5bf9\u9f50\u7814\u7a76\u5c40\u9650\u4e8e\u7279\u5b9a\u8f93\u5165\u548c\u4efb\u52a1\uff0c\u7f3a\u4e4f\u6bd4\u8f83\u4e0d\u540c\u6a21\u6001AI\u6a21\u578b\u5185\u5728\u7ec4\u7ec7\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u5206\u6790151\u4e2a\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff0c\u5c06\u6a21\u578b\u7684\u5185\u5728\u7a7a\u95f4\u6ce8\u610f\u529b\u62d3\u6251\u7ec4\u7ec7\u6620\u5c04\u5230\u6807\u51c6\u4eba\u7c7b\u529f\u80fd\u8111\u7f51\u7edc\u4e0a\uff0c\u6784\u5efa\u7edf\u4e00\u7684\u7c7b\u8111\u7a7a\u95f4\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u5728\u7c7b\u8111\u7a7a\u95f4\u4e2d\u5448\u73b0\u8fde\u7eed\u7684\u5f27\u5f62\u51e0\u4f55\u7ed3\u6784\uff0c\u53cd\u6620\u8111\u76f8\u4f3c\u5ea6\u7684\u6e10\u53d8\uff1b\u6a21\u578b\u5206\u5e03\u6a21\u5f0f\u53d7\u9884\u8bad\u7ec3\u8303\u5f0f\u548c\u4f4d\u7f6e\u7f16\u7801\u65b9\u6848\u5f71\u54cd\u3002", "conclusion": "\u7c7b\u8111\u7a7a\u95f4\u4e3a\u8de8\u9886\u57df\u667a\u80fd\u7684\u5b9a\u4f4d\u3001\u91cf\u5316\u548c\u6bd4\u8f83\u63d0\u4f9b\u4e86\u9996\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u673a\u5668\u4e0e\u5927\u8111\u4e4b\u95f4\u7684\u6df1\u5c42\u7ec4\u7ec7\u539f\u5219\u3002"}}
{"id": "2510.24683", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24683", "abs": "https://arxiv.org/abs/2510.24683", "authors": ["Caleb Escobedo", "Nataliya Nechyporenko", "Shreyas Kadekodi", "Alessandro Roncone"], "title": "A Framework for the Systematic Evaluation of Obstacle Avoidance and Object-Aware Controllers", "comment": null, "summary": "Real-time control is an essential aspect of safe robot operation in the real\nworld with dynamic objects. We present a framework for the analysis of\nobject-aware controllers, methods for altering a robot's motion to anticipate\nand avoid possible collisions. This framework is focused on three design\nconsiderations: kinematics, motion profiles, and virtual constraints.\nAdditionally, the analysis in this work relies on verification of robot\nbehaviors using fundamental robot-obstacle experimental scenarios. To showcase\nthe effectiveness of our method we compare three representative object-aware\ncontrollers. The comparison uses metrics originating from the design\nconsiderations. From the analysis, we find that the design of object-aware\ncontrollers often lacks kinematic considerations, continuity of control points,\nand stability in movement profiles. We conclude that this framework can be used\nin the future to design, compare, and benchmark obstacle avoidance methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u6790\u7269\u4f53\u611f\u77e5\u63a7\u5236\u5668\u7684\u6846\u67b6\uff0c\u91cd\u70b9\u5173\u6ce8\u8fd0\u52a8\u5b66\u3001\u8fd0\u52a8\u66f2\u7ebf\u548c\u865a\u62df\u7ea6\u675f\u4e09\u4e2a\u8bbe\u8ba1\u8003\u8651\u56e0\u7d20\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u548c\u6bd4\u8f83\u4e86\u4e09\u79cd\u4ee3\u8868\u6027\u63a7\u5236\u5668\u3002", "motivation": "\u5b9e\u65f6\u63a7\u5236\u662f\u673a\u5668\u4eba\u5728\u52a8\u6001\u7269\u4f53\u73af\u5883\u4e2d\u5b89\u5168\u64cd\u4f5c\u7684\u5173\u952e\u65b9\u9762\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u9884\u6d4b\u548c\u907f\u514d\u78b0\u649e\u7684\u7269\u4f53\u611f\u77e5\u63a7\u5236\u5668\u3002", "method": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u5206\u6790\u6846\u67b6\uff0c\u57fa\u4e8e\u8fd0\u52a8\u5b66\u3001\u8fd0\u52a8\u66f2\u7ebf\u548c\u865a\u62df\u7ea6\u675f\u4e09\u4e2a\u8bbe\u8ba1\u8003\u8651\u56e0\u7d20\uff0c\u901a\u8fc7\u57fa\u7840\u673a\u5668\u4eba-\u969c\u788d\u7269\u5b9e\u9a8c\u573a\u666f\u9a8c\u8bc1\u673a\u5668\u4eba\u884c\u4e3a\uff0c\u5e76\u6bd4\u8f83\u4e09\u79cd\u4ee3\u8868\u6027\u7269\u4f53\u611f\u77e5\u63a7\u5236\u5668\u3002", "result": "\u5206\u6790\u53d1\u73b0\u7269\u4f53\u611f\u77e5\u63a7\u5236\u5668\u7684\u8bbe\u8ba1\u901a\u5e38\u7f3a\u4e4f\u8fd0\u52a8\u5b66\u8003\u8651\u3001\u63a7\u5236\u70b9\u8fde\u7eed\u6027\u4ee5\u53ca\u8fd0\u52a8\u66f2\u7ebf\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u53ef\u7528\u4e8e\u672a\u6765\u8bbe\u8ba1\u3001\u6bd4\u8f83\u548c\u57fa\u51c6\u6d4b\u8bd5\u907f\u969c\u65b9\u6cd5\u3002"}}
{"id": "2510.24692", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24692", "abs": "https://arxiv.org/abs/2510.24692", "authors": ["Jun Wang", "Ziyang Zhou", "Ardalan Kahak", "Suyi Li"], "title": "Embodying Physical Computing into Soft Robots", "comment": null, "summary": "Softening and onboarding computers and controllers is one of the final\nfrontiers in soft robotics towards their robustness and intelligence for\neveryday use. In this regard, embodying soft and physical computing presents\nexciting potential. Physical computing seeks to encode inputs into a mechanical\ncomputing kernel and leverage the internal interactions among this kernel's\nconstituent elements to compute the output. Moreover, such input-to-output\nevolution can be re-programmable. This perspective paper proposes a framework\nfor embodying physical computing into soft robots and discusses three unique\nstrategies in the literature: analog oscillators, physical reservoir computing,\nand physical algorithmic computing. These embodied computers enable the soft\nrobot to perform complex behaviors that would otherwise require CMOS-based\nelectronics -- including coordinated locomotion with obstacle avoidance,\npayload weight and orientation classification, and programmable operation based\non logical rules. This paper will detail the working principles of these\nembodied physical computing methods, survey the current state-of-the-art, and\npresent a perspective for future development.", "AI": {"tldr": "\u8fd9\u7bc7\u89c6\u89d2\u8bba\u6587\u63d0\u51fa\u4e86\u5c06\u7269\u7406\u8ba1\u7b97\u5d4c\u5165\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u6846\u67b6\uff0c\u8ba8\u8bba\u4e86\u4e09\u79cd\u72ec\u7279\u7b56\u7565\uff1a\u6a21\u62df\u632f\u8361\u5668\u3001\u7269\u7406\u50a8\u5907\u6c60\u8ba1\u7b97\u548c\u7269\u7406\u7b97\u6cd5\u8ba1\u7b97\uff0c\u4f7f\u8f6f\u4f53\u673a\u5668\u4eba\u80fd\u591f\u6267\u884c\u590d\u6742\u884c\u4e3a\u800c\u65e0\u9700\u4f20\u7edf\u7535\u5b50\u8bbe\u5907\u3002", "motivation": "\u8f6f\u5316\u548c\u96c6\u6210\u8ba1\u7b97\u673a\u4e0e\u63a7\u5236\u5668\u662f\u8f6f\u4f53\u673a\u5668\u4eba\u5b9e\u73b0\u65e5\u5e38\u4f7f\u7528\u9c81\u68d2\u6027\u548c\u667a\u80fd\u5316\u7684\u5173\u952e\u524d\u6cbf\u9886\u57df\uff0c\u7269\u7406\u8ba1\u7b97\u4e3a\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u63d0\u4f9b\u4e86\u4ee4\u4eba\u5174\u594b\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u5c06\u7269\u7406\u8ba1\u7b97\u5d4c\u5165\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u6846\u67b6\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86\u4e09\u79cd\u5177\u4f53\u65b9\u6cd5\uff1a\u6a21\u62df\u632f\u8361\u5668\u3001\u7269\u7406\u50a8\u5907\u6c60\u8ba1\u7b97\u548c\u7269\u7406\u7b97\u6cd5\u8ba1\u7b97\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5229\u7528\u673a\u68b0\u8ba1\u7b97\u5185\u6838\u7684\u5185\u90e8\u76f8\u4e92\u4f5c\u7528\u6765\u5904\u7406\u8f93\u5165\u8f93\u51fa\u3002", "result": "\u8fd9\u4e9b\u5d4c\u5165\u5f0f\u8ba1\u7b97\u673a\u4f7f\u8f6f\u4f53\u673a\u5668\u4eba\u80fd\u591f\u6267\u884c\u590d\u6742\u884c\u4e3a\uff0c\u5305\u62ec\u5e26\u907f\u969c\u7684\u534f\u8c03\u8fd0\u52a8\u3001\u6709\u6548\u8f7d\u8377\u91cd\u91cf\u548c\u65b9\u5411\u5206\u7c7b\uff0c\u4ee5\u53ca\u57fa\u4e8e\u903b\u8f91\u89c4\u5219\u7684\u53ef\u7f16\u7a0b\u64cd\u4f5c\uff0c\u8fd9\u4e9b\u529f\u80fd\u539f\u672c\u9700\u8981CMOS\u7535\u5b50\u8bbe\u5907\u624d\u80fd\u5b9e\u73b0\u3002", "conclusion": "\u8bba\u6587\u8be6\u7ec6\u9610\u8ff0\u4e86\u8fd9\u4e9b\u5d4c\u5165\u5f0f\u7269\u7406\u8ba1\u7b97\u65b9\u6cd5\u7684\u539f\u7406\uff0c\u8c03\u67e5\u4e86\u5f53\u524d\u6700\u65b0\u8fdb\u5c55\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u53d1\u5c55\u7684\u5c55\u671b\uff0c\u4e3a\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u667a\u80fd\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2510.24390", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24390", "abs": "https://arxiv.org/abs/2510.24390", "authors": ["Xianjun Gao", "Jianchun Liu", "Hongli Xu", "Liusheng Huang"], "title": "Improving LLM Reasoning via Dependency-Aware Query Decomposition and Logic-Parallel Content Expansion", "comment": null, "summary": "The integration of Large Language Models (LLMs) into real-time Web\napplications, such as AI-powered search and conversational agents, presents a\nfundamental Web infrastructure challenge: reconciling the demand for\nhigh-quality, complex reasoning with the stringent low-latency and\nhigh-throughput requirements of interactive services. Current LLM reasoning,\nhindered by computationally inefficient sequential generation and rigid\nreasoning strategies, creates a critical bottleneck for the Web services.\nExisting approaches typically optimize the LLM reasoning for either efficiency\nor quality but struggle to achieve both, and thus fail to meet the dual\nrequirements of modern Web platforms. To overcome these limitations, we propose\nOrion, a novel and efficient reasoning framework that enables dependency-aware\nquery decomposition and logic-parallel content expansion. Concretely, Orion\ndecomposes a single query reasoning process into two synergistic phases: (1)\n\\textit{key point generation}, which distills logically structured key points\nthrough retrieval-augmented few-shot prompting, and (2) \\textit{content\nparallel expansion}, which concurrently elaborates on these points based on a\ndependency graph to ensure logical consistency. Furthermore, Orion introduces a\npipeline scheduling mechanism that exploits the complementary computational\ncharacteristics of the two phases (generation imposes pressure on GPU computing\nand expansion stresses on GPU memory) across multiple queries, enabling\ncross-query parallelism and dramatically improving reasoning performance (\\ie,\nefficiency and quality). Experiments on diverse benchmarks show that Orion not\nonly delivers up to 4.33x higher token generation speed and 3.42x lower answer\nlatency over the baselines but also improves reasoning quality by up to 18.75%\nthrough explicitly modeling inter-point dependencies.", "AI": {"tldr": "Orion\u662f\u4e00\u4e2a\u9ad8\u6548\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u4f9d\u8d56\u611f\u77e5\u7684\u67e5\u8be2\u5206\u89e3\u548c\u903b\u8f91\u5e76\u884c\u5185\u5bb9\u6269\u5c55\uff0c\u89e3\u51b3\u4e86LLM\u5728\u5b9e\u65f6Web\u5e94\u7528\u4e2d\u7684\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u5f53\u524dLLM\u63a8\u7406\u5b58\u5728\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u7684\u987a\u5e8f\u751f\u6210\u548c\u50f5\u5316\u63a8\u7406\u7b56\u7565\u95ee\u9898\uff0c\u65e0\u6cd5\u540c\u65f6\u6ee1\u8db3Web\u670d\u52a1\u5bf9\u9ad8\u8d28\u91cf\u590d\u6742\u63a8\u7406\u548c\u4f4e\u5ef6\u8fdf\u9ad8\u541e\u5410\u91cf\u7684\u53cc\u91cd\u9700\u6c42\u3002", "method": "Orion\u5c06\u67e5\u8be2\u63a8\u7406\u5206\u89e3\u4e3a\u4e24\u4e2a\u534f\u540c\u9636\u6bb5\uff1a\u5173\u952e\u70b9\u751f\u6210\uff08\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u7684\u5c11\u6837\u672c\u63d0\u793a\u63d0\u53d6\u903b\u8f91\u7ed3\u6784\u5316\u7684\u5173\u952e\u70b9\uff09\u548c\u5185\u5bb9\u5e76\u884c\u6269\u5c55\uff08\u57fa\u4e8e\u4f9d\u8d56\u56fe\u5e76\u884c\u6269\u5c55\u5185\u5bb9\u4ee5\u786e\u4fdd\u903b\u8f91\u4e00\u81f4\u6027\uff09\uff0c\u5e76\u5f15\u5165\u6d41\u6c34\u7ebf\u8c03\u5ea6\u673a\u5236\u5b9e\u73b0\u8de8\u67e5\u8be2\u5e76\u884c\u3002", "result": "\u5728\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOrion\u76f8\u6bd4\u57fa\u7ebf\u5b9e\u73b0\u4e86\u6700\u9ad84.33\u500d\u7684token\u751f\u6210\u901f\u5ea6\u63d0\u5347\u30013.42\u500d\u7684\u7b54\u6848\u5ef6\u8fdf\u964d\u4f4e\uff0c\u5e76\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u70b9\u95f4\u4f9d\u8d56\u5173\u7cfb\u5c06\u63a8\u7406\u8d28\u91cf\u63d0\u5347\u6700\u9ad818.75%\u3002", "conclusion": "Orion\u6846\u67b6\u6210\u529f\u5e73\u8861\u4e86LLM\u63a8\u7406\u7684\u6548\u7387\u548c\u8d28\u91cf\uff0c\u4e3a\u5b9e\u65f6Web\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\u548c\u903b\u8f91\u4e00\u81f4\u6027\u3002"}}
{"id": "2510.24397", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24397", "abs": "https://arxiv.org/abs/2510.24397", "authors": ["Jiarui Qin", "Yunjia Xi", "Junjie Huang", "Renting Rui", "Di Yin", "Weiwen Liu", "Yong Yu", "Weinan Zhang", "Xing Sun"], "title": "APTBench: Benchmarking Agentic Potential of Base LLMs During Pre-Training", "comment": "46 pages", "summary": "With the rapid development of LLM-based agents, there is a growing trend to\nincorporate agent-specific data into the pre-training stage of LLMs, aiming to\nbetter align LLMs with real-world autonomous task execution. However, current\npre-training benchmarks primarily focus on isolated and static skills, e.g.,\ncommon knowledge or mathematical/code reasoning, and fail to reflect model's\nagentic capabilities. On the other hand, agent benchmarks are typically\ndesigned for post-trained models, requiring multi-turn task execution abilities\nthat base models struggle to support. Thus, there is a compelling need for a\nbenchmark that can evaluate agentic potentials during pre-training and guide\nthe model training more effectively. To address this gap, we propose APTBench,\na framework that converts real-world agent tasks and successful trajectories\ninto multiple-choice or text completion questions tailored for base models. It\nfocuses on core agentic abilities, e.g., planning and action, and covers key\nagent scenarios, software engineering and deep research. Compared to existing\ngeneral-purpose benchmarks, APTBench offers a more predictive signal of a\nmodel's downstream performance as an agent, while remaining significantly more\nlightweight and cost-effective than full-scale, end-to-end agent evaluations\nafter post-training.", "AI": {"tldr": "\u63d0\u51fa\u4e86APTBench\u6846\u67b6\uff0c\u5c06\u771f\u5b9e\u4e16\u754c\u667a\u80fd\u4f53\u4efb\u52a1\u8f6c\u5316\u4e3a\u9002\u5408\u57fa\u7840\u6a21\u578b\u8bc4\u4f30\u7684\u9009\u62e9\u9898\u6216\u6587\u672c\u8865\u5168\u95ee\u9898\uff0c\u7528\u4e8e\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u8bc4\u4f30\u6a21\u578b\u4f5c\u4e3a\u667a\u80fd\u4f53\u7684\u6f5c\u529b\u3002", "motivation": "\u5f53\u524d\u9884\u8bad\u7ec3\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u5b64\u7acb\u9759\u6001\u6280\u80fd\uff0c\u65e0\u6cd5\u53cd\u6620\u6a21\u578b\u7684\u667a\u80fd\u4f53\u80fd\u529b\uff1b\u800c\u667a\u80fd\u4f53\u57fa\u51c6\u901a\u5e38\u9488\u5bf9\u540e\u8bad\u7ec3\u6a21\u578b\uff0c\u57fa\u7840\u6a21\u578b\u96be\u4ee5\u652f\u6301\u591a\u8f6e\u4efb\u52a1\u6267\u884c\u3002\u9700\u8981\u80fd\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u8bc4\u4f30\u667a\u80fd\u4f53\u6f5c\u529b\u7684\u57fa\u51c6\u3002", "method": "\u5c06\u771f\u5b9e\u4e16\u754c\u667a\u80fd\u4f53\u4efb\u52a1\u548c\u6210\u529f\u8f68\u8ff9\u8f6c\u5316\u4e3a\u9009\u62e9\u9898\u6216\u6587\u672c\u8865\u5168\u95ee\u9898\uff0c\u805a\u7126\u89c4\u5212\u548c\u884c\u52a8\u7b49\u6838\u5fc3\u667a\u80fd\u4f53\u80fd\u529b\uff0c\u8986\u76d6\u8f6f\u4ef6\u5de5\u7a0b\u548c\u6df1\u5ea6\u7814\u7a76\u7b49\u5173\u952e\u573a\u666f\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u901a\u7528\u57fa\u51c6\uff0cAPTBench\u80fd\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u6a21\u578b\u4f5c\u4e3a\u667a\u80fd\u4f53\u7684\u4e0b\u6e38\u6027\u80fd\uff0c\u540c\u65f6\u6bd4\u540e\u8bad\u7ec3\u7684\u7aef\u5230\u7aef\u667a\u80fd\u4f53\u8bc4\u4f30\u66f4\u8f7b\u91cf\u3001\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "APTBench\u586b\u8865\u4e86\u9884\u8bad\u7ec3\u9636\u6bb5\u667a\u80fd\u4f53\u80fd\u529b\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4e3a\u66f4\u6709\u6548\u5730\u6307\u5bfc\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24461", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24461", "abs": "https://arxiv.org/abs/2510.24461", "authors": ["Korneel Van den Berghe", "Stein Stroobants", "Vijay Janapa Reddi", "G. C. H. E. de Croon"], "title": "Adaptive Surrogate Gradients for Sequential Reinforcement Learning in Spiking Neural Networks", "comment": null, "summary": "Neuromorphic computing systems are set to revolutionize energy-constrained\nrobotics by achieving orders-of-magnitude efficiency gains, while enabling\nnative temporal processing. Spiking Neural Networks (SNNs) represent a\npromising algorithmic approach for these systems, yet their application to\ncomplex control tasks faces two critical challenges: (1) the non-differentiable\nnature of spiking neurons necessitates surrogate gradients with unclear\noptimization properties, and (2) the stateful dynamics of SNNs require training\non sequences, which in reinforcement learning (RL) is hindered by limited\nsequence lengths during early training, preventing the network from bridging\nits warm-up period.\n  We address these challenges by systematically analyzing surrogate gradient\nslope settings, showing that shallower slopes increase gradient magnitude in\ndeeper layers but reduce alignment with true gradients. In supervised learning,\nwe find no clear preference for fixed or scheduled slopes. The effect is much\nmore pronounced in RL settings, where shallower slopes or scheduled slopes lead\nto a 2.1x improvement in both training and final deployed performance. Next, we\npropose a novel training approach that leverages a privileged guiding policy to\nbootstrap the learning process, while still exploiting online environment\ninteractions with the spiking policy. Combining our method with an adaptive\nslope schedule for a real-world drone position control task, we achieve an\naverage return of 400 points, substantially outperforming prior techniques,\nincluding Behavioral Cloning and TD3BC, which achieve at most --200 points\nunder the same conditions. This work advances both the theoretical\nunderstanding of surrogate gradient learning in SNNs and practical training\nmethodologies for neuromorphic controllers demonstrated in real-world robotic\nsystems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u8bad\u7ec3\u6027\u80fd\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u66ff\u4ee3\u68af\u5ea6\u659c\u7387\u8bbe\u7f6e\u548c\u5f15\u5165\u7279\u6743\u5f15\u5bfc\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u4f4d\u7f6e\u63a7\u5236\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5728\u80fd\u91cf\u53d7\u9650\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5176\u5e94\u7528\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u8109\u51b2\u795e\u7ecf\u5143\u7684\u4e0d\u53ef\u5fae\u5206\u6027\u9700\u8981\u66ff\u4ee3\u68af\u5ea6\u65b9\u6cd5\uff0c\u4ee5\u53caSNN\u7684\u72b6\u6001\u52a8\u6001\u9700\u8981\u5728\u5e8f\u5217\u4e0a\u8bad\u7ec3\uff0c\u8fd9\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u53d7\u5230\u65e9\u671f\u8bad\u7ec3\u5e8f\u5217\u957f\u5ea6\u9650\u5236\u7684\u5f71\u54cd\u3002", "method": "\u7cfb\u7edf\u5206\u6790\u66ff\u4ee3\u68af\u5ea6\u659c\u7387\u8bbe\u7f6e\uff0c\u53d1\u73b0\u8f83\u6d45\u7684\u659c\u7387\u80fd\u589e\u52a0\u6df1\u5c42\u68af\u5ea6\u5e45\u5ea6\u4f46\u964d\u4f4e\u4e0e\u771f\u5b9e\u68af\u5ea6\u7684\u5bf9\u9f50\uff1b\u63d0\u51fa\u5229\u7528\u7279\u6743\u5f15\u5bfc\u7b56\u7565\u6765\u5f15\u5bfc\u5b66\u4e60\u8fc7\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u73af\u5883\u7684\u5728\u7ebf\u4ea4\u4e92\uff1b\u7ed3\u5408\u81ea\u9002\u5e94\u659c\u7387\u8c03\u5ea6\u65b9\u6cd5\u3002", "result": "\u5728\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u4e2d\uff0c\u8f83\u6d45\u659c\u7387\u6216\u8c03\u5ea6\u659c\u7387\u4f7f\u8bad\u7ec3\u548c\u6700\u7ec8\u90e8\u7f72\u6027\u80fd\u63d0\u53472.1\u500d\uff1b\u5728\u771f\u5b9e\u4e16\u754c\u65e0\u4eba\u673a\u4f4d\u7f6e\u63a7\u5236\u4efb\u52a1\u4e2d\uff0c\u5e73\u5747\u56de\u62a5\u8fbe\u5230400\u5206\uff0c\u663e\u8457\u4f18\u4e8e\u884c\u4e3a\u514b\u9686\u548cTD3BC\u65b9\u6cd5\uff08\u6700\u591a-200\u5206\uff09\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63a8\u8fdb\u4e86\u5bf9SNN\u4e2d\u66ff\u4ee3\u68af\u5ea6\u5b66\u4e60\u7684\u7406\u8bba\u7406\u89e3\uff0c\u5e76\u4e3a\u795e\u7ecf\u5f62\u6001\u63a7\u5236\u5668\u5728\u5b9e\u9645\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u8bad\u7ec3\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24411", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.24411", "abs": "https://arxiv.org/abs/2510.24411", "authors": ["Qiushi Sun", "Mukai Li", "Zhoumianze Liu", "Zhihui Xie", "Fangzhi Xu", "Zhangyue Yin", "Kanzhi Cheng", "Zehao Li", "Zichen Ding", "Qi Liu", "Zhiyong Wu", "Zhuosheng Zhang", "Ben Kao", "Lingpeng Kong"], "title": "OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows", "comment": "work in progress", "summary": "Computer-using agents powered by Vision-Language Models (VLMs) have\ndemonstrated human-like capabilities in operating digital environments like\nmobile platforms. While these agents hold great promise for advancing digital\nautomation, their potential for unsafe operations, such as system compromise\nand privacy leakage, is raising significant concerns. Detecting these safety\nconcerns across the vast and complex operational space of mobile environments\npresents a formidable challenge that remains critically underexplored. To\nestablish a foundation for mobile agent safety research, we introduce\nMobileRisk-Live, a dynamic sandbox environment accompanied by a safety\ndetection benchmark comprising realistic trajectories with fine-grained\nannotations. Built upon this, we propose OS-Sentinel, a novel hybrid safety\ndetection framework that synergistically combines a Formal Verifier for\ndetecting explicit system-level violations with a VLM-based Contextual Judge\nfor assessing contextual risks and agent actions. Experiments show that\nOS-Sentinel achieves 10%-30% improvements over existing approaches across\nmultiple metrics. Further analysis provides critical insights that foster the\ndevelopment of safer and more reliable autonomous mobile agents.", "AI": {"tldr": "MobileRisk-Live\u662f\u4e00\u4e2a\u52a8\u6001\u6c99\u76d2\u73af\u5883\u548c\u5b89\u5168\u68c0\u6d4b\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u79fb\u52a8AI\u4ee3\u7406\u7684\u5b89\u5168\u6027\u98ce\u9669\u3002OS-Sentinel\u6df7\u5408\u6846\u67b6\u7ed3\u5408\u5f62\u5f0f\u9a8c\u8bc1\u5668\u548cVLM\u4e0a\u4e0b\u6587\u5224\u65ad\u5668\uff0c\u5728\u79fb\u52a8\u8bbe\u5907\u5b89\u5168\u68c0\u6d4b\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u534710%-30%\u3002", "motivation": "\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u79fb\u52a8AI\u4ee3\u7406\u5728\u6570\u5b57\u73af\u5883\u4e2d\u5c55\u73b0\u51fa\u7c7b\u4eba\u64cd\u4f5c\u80fd\u529b\uff0c\u4f46\u5176\u6f5c\u5728\u7684\u5b89\u5168\u98ce\u9669\uff08\u5982\u7cfb\u7edf\u7834\u574f\u3001\u9690\u79c1\u6cc4\u9732\uff09\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\uff0c\u9700\u8981\u5efa\u7acb\u7cfb\u7edf\u5316\u7684\u5b89\u5168\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u63d0\u51faOS-Sentinel\u6df7\u5408\u5b89\u5168\u68c0\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u5f62\u5f0f\u9a8c\u8bc1\u5668\u68c0\u6d4b\u7cfb\u7edf\u7ea7\u8fdd\u89c4\u548c\u57fa\u4e8eVLM\u7684\u4e0a\u4e0b\u6587\u5224\u65ad\u5668\u8bc4\u4f30\u60c5\u5883\u98ce\u9669\u4e0e\u4ee3\u7406\u884c\u4e3a\u3002", "result": "\u5b9e\u9a8c\u8868\u660eOS-Sentinel\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u534710%-30%\uff0c\u4e3a\u5f00\u53d1\u66f4\u5b89\u5168\u53ef\u9760\u7684\u81ea\u4e3b\u79fb\u52a8\u4ee3\u7406\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002", "conclusion": "MobileRisk-Live\u4e3a\u79fb\u52a8\u4ee3\u7406\u5b89\u5168\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0cOS-Sentinel\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u5b89\u5168\u68c0\u6d4b\u80fd\u529b\uff0c\u4fc3\u8fdb\u4e86\u66f4\u5b89\u5168\u53ef\u9760\u7684\u81ea\u4e3b\u79fb\u52a8\u4ee3\u7406\u53d1\u5c55\u3002"}}
{"id": "2510.24435", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24435", "abs": "https://arxiv.org/abs/2510.24435", "authors": ["Benjamin Grando Moreira"], "title": "Human-Level Reasoning: A Comparative Study of Large Language Models on Logical and Abstract Reasoning", "comment": "12 pages", "summary": "Evaluating reasoning ability in Large Language Models (LLMs) is important for\nadvancing artificial intelligence, as it transcends mere linguistic task\nperformance. It involves understanding whether these models truly understand\ninformation, perform inferences, and are able to draw conclusions in a logical\nand valid way. This study compare logical and abstract reasoning skills of\nseveral LLMs - including GPT, Claude, DeepSeek, Gemini, Grok, Llama, Mistral,\nPerplexity, and Sabi\\'a - using a set of eight custom-designed reasoning\nquestions. The LLM results are benchmarked against human performance on the\nsame tasks, revealing significant differences and indicating areas where LLMs\nstruggle with deduction.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u591a\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u903b\u8f91\u548c\u62bd\u8c61\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u8868\u73b0\uff0c\u5e76\u4e0e\u4eba\u7c7b\u8868\u73b0\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86LLMs\u5728\u6f14\u7ece\u63a8\u7406\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u5bf9\u4e8e\u63a8\u8fdb\u4eba\u5de5\u667a\u80fd\u53d1\u5c55\u81f3\u5173\u91cd\u8981\uff0c\u8fd9\u8d85\u8d8a\u4e86\u5355\u7eaf\u7684\u8bed\u8a00\u4efb\u52a1\u8868\u73b0\uff0c\u6d89\u53ca\u7406\u89e3\u6a21\u578b\u662f\u5426\u771f\u6b63\u7406\u89e3\u4fe1\u606f\u3001\u8fdb\u884c\u63a8\u7406\u5e76\u4ee5\u903b\u8f91\u6709\u6548\u7684\u65b9\u5f0f\u5f97\u51fa\u7ed3\u8bba\u3002", "method": "\u4f7f\u7528\u516b\u4e2a\u5b9a\u5236\u8bbe\u8ba1\u7684\u63a8\u7406\u95ee\u9898\uff0c\u6bd4\u8f83\u4e86GPT\u3001Claude\u3001DeepSeek\u3001Gemini\u3001Grok\u3001Llama\u3001Mistral\u3001Perplexity\u548cSabi'a\u7b49\u591a\u4e2aLLMs\u7684\u903b\u8f91\u548c\u62bd\u8c61\u63a8\u7406\u6280\u80fd\uff0c\u5e76\u5c06\u7ed3\u679c\u4e0e\u4eba\u7c7b\u5728\u76f8\u540c\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793aLLMs\u4e0e\u4eba\u7c7b\u8868\u73b0\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u8868\u660eLLMs\u5728\u6f14\u7ece\u63a8\u7406\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u903b\u8f91\u548c\u62bd\u8c61\u63a8\u7406\u80fd\u529b\u65b9\u9762\u4ecd\u6709\u5f85\u6539\u8fdb\uff0c\u7279\u522b\u662f\u5728\u6f14\u7ece\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002"}}
{"id": "2510.24459", "categories": ["cs.AI", "cs.MA", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24459", "abs": "https://arxiv.org/abs/2510.24459", "authors": ["Habtom Kahsay Gidey", "Niklas Huber", "Alexander Lenz", "Alois Knoll"], "title": "Affordance Representation and Recognition for Autonomous Agents", "comment": null, "summary": "The autonomy of software agents is fundamentally dependent on their ability\nto construct an actionable internal world model from the structured data that\ndefines their digital environment, such as the Document Object Model (DOM) of\nweb pages and the semantic descriptions of web services. However, constructing\nthis world model from raw structured data presents two critical challenges: the\nverbosity of raw HTML makes it computationally intractable for direct use by\nfoundation models, while the static nature of hardcoded API integrations\nprevents agents from adapting to evolving services.\n  This paper introduces a pattern language for world modeling from structured\ndata, presenting two complementary architectural patterns. The DOM Transduction\nPattern addresses the challenge of web page complexity by distilling} a\nverbose, raw DOM into a compact, task-relevant representation or world model\noptimized for an agent's reasoning core. Concurrently, the Hypermedia\nAffordances Recognition Pattern enables the agent to dynamically enrich its\nworld model by parsing standardized semantic descriptions to discover and\nintegrate the capabilities of unknown web services at runtime. Together, these\npatterns provide a robust framework for engineering agents that can efficiently\nconstruct and maintain an accurate world model, enabling scalable, adaptive,\nand interoperable automation across the web and its extended resources.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u67b6\u6784\u6a21\u5f0f\uff1aDOM\u8f6c\u6362\u6a21\u5f0f\u5904\u7406\u7f51\u9875\u590d\u6742\u6027\uff0c\u5c06\u5197\u957fDOM\u8f6c\u6362\u4e3a\u7d27\u51d1\u7684\u4efb\u52a1\u76f8\u5173\u8868\u793a\uff1b\u8d85\u5a92\u4f53\u529f\u80fd\u8bc6\u522b\u6a21\u5f0f\u4f7f\u4ee3\u7406\u80fd\u52a8\u6001\u53d1\u73b0\u548c\u96c6\u6210\u672a\u77e5Web\u670d\u52a1\u7684\u80fd\u529b\u3002", "motivation": "\u8f6f\u4ef6\u4ee3\u7406\u7684\u81ea\u4e3b\u6027\u4f9d\u8d56\u4e8e\u4ece\u7ed3\u6784\u5316\u6570\u636e\u6784\u5efa\u53ef\u64cd\u4f5c\u5185\u90e8\u4e16\u754c\u6a21\u578b\u7684\u80fd\u529b\uff0c\u4f46\u539f\u59cbHTML\u7684\u5197\u957f\u6027\u548c\u786c\u7f16\u7801API\u96c6\u6210\u7684\u9759\u6001\u6027\u963b\u788d\u4e86\u4ee3\u7406\u7684\u9002\u5e94\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "DOM\u8f6c\u6362\u6a21\u5f0f\uff1a\u5c06\u539f\u59cbDOM\u63d0\u70bc\u4e3a\u7d27\u51d1\u7684\u4e16\u754c\u6a21\u578b\uff1b\u8d85\u5a92\u4f53\u529f\u80fd\u8bc6\u522b\u6a21\u5f0f\uff1a\u89e3\u6790\u6807\u51c6\u5316\u8bed\u4e49\u63cf\u8ff0\u4ee5\u52a8\u6001\u53d1\u73b0\u548c\u96c6\u6210Web\u670d\u52a1\u80fd\u529b\u3002", "result": "\u4e3a\u5de5\u7a0b\u5316\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7a33\u5065\u6846\u67b6\uff0c\u80fd\u591f\u9ad8\u6548\u6784\u5efa\u548c\u7ef4\u62a4\u51c6\u786e\u7684\u4e16\u754c\u6a21\u578b\u3002", "conclusion": "\u8fd9\u4e9b\u6a21\u5f0f\u5171\u540c\u652f\u6301\u5728Web\u53ca\u5176\u6269\u5c55\u8d44\u6e90\u4e0a\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u81ea\u9002\u5e94\u548c\u53ef\u4e92\u64cd\u4f5c\u7684\u81ea\u52a8\u5316\u3002"}}
{"id": "2510.24528", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24528", "abs": "https://arxiv.org/abs/2510.24528", "authors": ["Zihan Chen", "Song Wang", "Xingbo Fu", "Chengshuai Shi", "Zhenyu Lei", "Cong Shen", "Jundong Li"], "title": "From Cross-Task Examples to In-Task Prompts: A Graph-Based Pseudo-Labeling Framework for In-context Learning", "comment": null, "summary": "The capability of in-context learning (ICL) enables large language models\n(LLMs) to perform novel tasks without parameter updates by conditioning on a\nfew input-output examples. However, collecting high-quality examples for new or\nchallenging tasks can be costly and labor-intensive. In this work, we propose a\ncost-efficient two-stage pipeline that reduces reliance on LLMs for data\nlabeling. Our approach first leverages readily available cross-task examples to\nprompt an LLM and pseudo-label a small set of target task instances. We then\nintroduce a graph-based label propagation method that spreads label information\nto the remaining target examples without additional LLM queries. The resulting\nfully pseudo-labeled dataset is used to construct in-task demonstrations for\nICL. This pipeline combines the flexibility of cross-task supervision with the\nscalability of LLM-free propagation. Experiments across five tasks demonstrate\nthat our method achieves strong performance while lowering labeling costs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u7684\u4e24\u9636\u6bb5\u7ba1\u9053\uff0c\u901a\u8fc7\u8de8\u4efb\u52a1\u793a\u4f8b\u548c\u57fa\u4e8e\u56fe\u7684\u6807\u7b7e\u4f20\u64ad\u65b9\u6cd5\uff0c\u51cf\u5c11\u5bf9LLM\u6570\u636e\u6807\u6ce8\u7684\u4f9d\u8d56\uff0c\u7528\u4e8e\u6784\u5efaICL\u7684\u6f14\u793a\u793a\u4f8b\u3002", "motivation": "\u4e3a\u65b0\u4efb\u52a1\u6216\u6311\u6218\u6027\u4efb\u52a1\u6536\u96c6\u9ad8\u8d28\u91cf\u793a\u4f8b\u6210\u672c\u9ad8\u6602\u4e14\u52b3\u52a8\u5bc6\u96c6\uff0c\u9700\u8981\u51cf\u5c11\u5bf9LLM\u6807\u6ce8\u7684\u4f9d\u8d56\u3002", "method": "\u4e24\u9636\u6bb5\u7ba1\u9053\uff1a\u9996\u5148\u4f7f\u7528\u8de8\u4efb\u52a1\u793a\u4f8b\u63d0\u793aLLM\u4f2a\u6807\u6ce8\u5c11\u91cf\u76ee\u6807\u5b9e\u4f8b\uff0c\u7136\u540e\u5f15\u5165\u57fa\u4e8e\u56fe\u7684\u6807\u7b7e\u4f20\u64ad\u65b9\u6cd5\u5c06\u6807\u7b7e\u4fe1\u606f\u4f20\u64ad\u5230\u5269\u4f59\u76ee\u6807\u793a\u4f8b\u4e2d\uff0c\u65e0\u9700\u989d\u5916LLM\u67e5\u8be2\u3002", "result": "\u5728\u4e94\u4e2a\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5f3a\u52b2\u6027\u80fd\u3002", "conclusion": "\u8be5\u7ba1\u9053\u7ed3\u5408\u4e86\u8de8\u4efb\u52a1\u76d1\u7763\u7684\u7075\u6d3b\u6027\u548c\u65e0LLM\u4f20\u64ad\u7684\u53ef\u6269\u5c55\u6027\uff0c\u4e3aICL\u63d0\u4f9b\u4e86\u6210\u672c\u6548\u76ca\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24551", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24551", "abs": "https://arxiv.org/abs/2510.24551", "authors": ["Gang Chen", "Changshuo Liu", "Gene Anne Ooi", "Marcus Tan", "Zhongle Xie", "Jianwei Yin", "James Wei Luen Yip", "Wenqiao Zhang", "Jiaqi Zhu", "Beng Chin Ooi"], "title": "Generative AI for Healthcare: Fundamentals, Challenges, and Perspectives", "comment": null, "summary": "Generative Artificial Intelligence (GenAI) is taking the world by storm. It\npromises transformative opportunities for advancing and disrupting existing\npractices, including healthcare. From large language models (LLMs) for clinical\nnote synthesis and conversational assistance to multimodal systems that\nintegrate medical imaging, electronic health records, and genomic data for\ndecision support, GenAI is transforming the practice of medicine and the\ndelivery of healthcare, such as diagnosis and personalized treatments, with\ngreat potential in reducing the cognitive burden on clinicians, thereby\nimproving overall healthcare delivery. However, GenAI deployment in healthcare\nrequires an in-depth understanding of healthcare tasks and what can and cannot\nbe achieved. In this paper, we propose a data-centric paradigm in the design\nand deployment of GenAI systems for healthcare. Specifically, we reposition the\ndata life cycle by making the medical data ecosystem as the foundational\nsubstrate for generative healthcare systems. This ecosystem is designed to\nsustainably support the integration, representation, and retrieval of diverse\nmedical data and knowledge. With effective and efficient data processing\npipelines, such as semantic vector search and contextual querying, it enables\nGenAI-powered operations for upstream model components and downstream clinical\napplications. Ultimately, it not only supplies foundation models with\nhigh-quality, multimodal data for large-scale pretraining and domain-specific\nfine-tuning, but also serves as a knowledge retrieval backend to support\ntask-specific inference via the agentic layer. The ecosystem enables the\ndeployment of GenAI for high-quality and effective healthcare delivery.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\u6765\u8bbe\u8ba1\u548c\u90e8\u7f72\u533b\u7597\u9886\u57df\u7684\u751f\u6210\u5f0fAI\u7cfb\u7edf\uff0c\u5c06\u533b\u7597\u6570\u636e\u751f\u6001\u7cfb\u7edf\u4f5c\u4e3a\u751f\u6210\u5f0f\u533b\u7597\u7cfb\u7edf\u7684\u57fa\u7840\u652f\u6491\u3002", "motivation": "\u751f\u6210\u5f0fAI\u5728\u533b\u7597\u9886\u57df\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u6df1\u5165\u7406\u89e3\u533b\u7597\u4efb\u52a1\u548c\u53ef\u5b9e\u73b0\u7684\u8303\u56f4\u3002\u5f53\u524d\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\u6765\u652f\u6301GenAI\u5728\u533b\u7597\u4e2d\u7684\u9ad8\u8d28\u91cf\u90e8\u7f72\u3002", "method": "\u91cd\u65b0\u5b9a\u4f4d\u6570\u636e\u751f\u547d\u5468\u671f\uff0c\u6784\u5efa\u533b\u7597\u6570\u636e\u751f\u6001\u7cfb\u7edf\u4f5c\u4e3a\u57fa\u7840\u652f\u6491\uff0c\u652f\u6301\u591a\u6a21\u6001\u533b\u7597\u6570\u636e\u7684\u96c6\u6210\u3001\u8868\u793a\u548c\u68c0\u7d22\u3002\u901a\u8fc7\u8bed\u4e49\u5411\u91cf\u641c\u7d22\u548c\u4e0a\u4e0b\u6587\u67e5\u8be2\u7b49\u9ad8\u6548\u6570\u636e\u5904\u7406\u7ba1\u9053\uff0c\u4e3a\u4e0a\u6e38\u6a21\u578b\u7ec4\u4ef6\u548c\u4e0b\u6e38\u4e34\u5e8a\u5e94\u7528\u63d0\u4f9b\u652f\u6301\u3002", "result": "\u8be5\u751f\u6001\u7cfb\u7edf\u4e0d\u4ec5\u4e3a\u57fa\u5ea7\u6a21\u578b\u63d0\u4f9b\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u6570\u636e\u8fdb\u884c\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u548c\u9886\u57df\u7279\u5b9a\u5fae\u8c03\uff0c\u8fd8\u4f5c\u4e3a\u77e5\u8bc6\u68c0\u7d22\u540e\u7aef\u901a\u8fc7\u4ee3\u7406\u5c42\u652f\u6301\u4efb\u52a1\u7279\u5b9a\u63a8\u7406\u3002", "conclusion": "\u8fd9\u79cd\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u751f\u6210\u5f0fAI\u5728\u533b\u7597\u9886\u57df\u7684\u9ad8\u8d28\u91cf\u548c\u6709\u6548\u90e8\u7f72\uff0c\u6539\u5584\u533b\u7597\u670d\u52a1\u4ea4\u4ed8\u3002"}}
{"id": "2510.24645", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24645", "abs": "https://arxiv.org/abs/2510.24645", "authors": ["Zengzhuang Xu", "Bingguang Hao", "Zechuan Wang", "Yuntao Wen", "Maolin Wang", "Yang Liu", "Long Chen", "Dong Wang", "Yicheng Chen", "Cunyin Peng", "Chenyi Zhuang", "Jinjie Gu", "Leilei Gan", "Xiangyu Zhao", "Shi Gu"], "title": "FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling", "comment": null, "summary": "Function calling (FC) empowers large language models (LLMs) and autonomous\nagents to interface with external tools, a critical capability for solving\ncomplex, real-world problems. As this ability becomes increasingly central to\nadvanced AI systems, the need for high-quality, multi-turn training data to\ndevelop and refine it cannot be overstated. Existing data synthesis methods,\nsuch as random environment sampling or multi-agent role-playing, are not\npowerful enough to generate high-quality data in real-world environments.\nPractical challenges come in three folds: targeted model training, isolation of\ntool architecture, and multi-turn logical dependency. To address these\nstructural deficiencies, we present FunReason-MT, a novel data synthesis\nframework for real-world multi-turn tool use. FunReason-MT resolves the\ncomplexity barrier in multi-turn FC data by employing 1) Environment-API Graph\nInteractions to gather varied high-quality trajectories, 2) Advanced Tool-Query\nSynthesis to simplify hard query construction, and 3) Guided Iterative Chain\nfor sophisticated CoT generation. Evaluations on Berkeley Function-Calling\nLeaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built\nupon FunReason-MT generated data achieves state-of-the-art performance among\ncomparable-sized models, outperforming most close-source models. Further\nperformance improvements on BFCLv4 confirm that FunReason-MT provides a\nreliable and robust source for agentic learning.", "AI": {"tldr": "FunReason-MT\u662f\u4e00\u4e2a\u7528\u4e8e\u751f\u6210\u591a\u8f6e\u5de5\u5177\u8c03\u7528\u8bad\u7ec3\u6570\u636e\u7684\u65b0\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u73af\u5883-API\u56fe\u4ea4\u4e92\u3001\u9ad8\u7ea7\u5de5\u5177\u67e5\u8be2\u5408\u6210\u548c\u5f15\u5bfc\u8fed\u4ee3\u94fe\u7b49\u6280\u672f\uff0c\u5728BFCL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u5408\u6210\u65b9\u6cd5\uff08\u5982\u968f\u673a\u73af\u5883\u91c7\u6837\u6216\u591a\u667a\u80fd\u4f53\u89d2\u8272\u626e\u6f14\uff09\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u65e0\u6cd5\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u591a\u8f6e\u5de5\u5177\u8c03\u7528\u8bad\u7ec3\u6570\u636e\uff0c\u5b58\u5728\u76ee\u6807\u6a21\u578b\u8bad\u7ec3\u3001\u5de5\u5177\u67b6\u6784\u9694\u79bb\u548c\u591a\u8f6e\u903b\u8f91\u4f9d\u8d56\u7b49\u5b9e\u9645\u6311\u6218\u3002", "method": "FunReason-MT\u6846\u67b6\u91c7\u7528\u4e09\u79cd\u6838\u5fc3\u6280\u672f\uff1a1\uff09\u73af\u5883-API\u56fe\u4ea4\u4e92\u6536\u96c6\u591a\u6837\u5316\u9ad8\u8d28\u91cf\u8f68\u8ff9\uff1b2\uff09\u9ad8\u7ea7\u5de5\u5177\u67e5\u8be2\u5408\u6210\u7b80\u5316\u590d\u6742\u67e5\u8be2\u6784\u5efa\uff1b3\uff09\u5f15\u5bfc\u8fed\u4ee3\u94fe\u751f\u6210\u590d\u6742\u601d\u7ef4\u94fe\u3002", "result": "\u5728Berkeley Function-Calling Leaderboard (BFCLv3)\u4e0a\uff0c\u57fa\u4e8eFunReason-MT\u751f\u6210\u6570\u636e\u8bad\u7ec3\u76844B\u6a21\u578b\u5728\u540c\u7b49\u89c4\u6a21\u6a21\u578b\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u751a\u81f3\u4f18\u4e8e\u5927\u591a\u6570\u95ed\u6e90\u6a21\u578b\u3002\u5728BFCLv4\u4e0a\u7684\u8fdb\u4e00\u6b65\u6027\u80fd\u6539\u8fdb\u8bc1\u5b9e\u4e86\u8be5\u6846\u67b6\u7684\u53ef\u9760\u6027\u3002", "conclusion": "FunReason-MT\u4e3a\u667a\u80fd\u4f53\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u9c81\u68d2\u7684\u6570\u636e\u6e90\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u591a\u8f6e\u51fd\u6570\u8c03\u7528\u6570\u636e\u5408\u6210\u7684\u590d\u6742\u6027\u969c\u788d\u3002"}}
{"id": "2510.24650", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24650", "abs": "https://arxiv.org/abs/2510.24650", "authors": ["Nitin Rai", "Daeun", "Choi", "Nathan S. Boyd", "Arnold W. Schumann"], "title": "Advancing site-specific disease and pest management in precision agriculture: From reasoning-driven foundation models to adaptive, feedback-based learning", "comment": "26 pages, 8 figures, and 2 tables", "summary": "Site-specific disease management (SSDM) in crops has advanced rapidly through\nmachine and deep learning (ML and DL) for real-time computer vision. Research\nevolved from handcrafted feature extraction to large-scale automated feature\nlearning. With foundation models (FMs), crop disease datasets are now processed\nin fundamentally new ways. Unlike traditional neural networks, FMs integrate\nvisual and textual data, interpret symptoms in text, reason about\nsymptom-management relationships, and support interactive QA for growers and\neducators. Adaptive and imitation learning in robotics further enables\nfield-based disease management. This review screened approx. 40 articles on FM\napplications for SSDM, focusing on large-language models (LLMs) and\nvision-language models (VLMs), and discussing their role in adaptive learning\n(AL), reinforcement learning (RL), and digital twin frameworks for targeted\nspraying. Key findings: (a) FMs are gaining traction with surging literature in\n2023-24; (b) VLMs outpace LLMs, with a 5-10x increase in publications; (c) RL\nand AL are still nascent for smart spraying; (d) digital twins with RL can\nsimulate targeted spraying virtually; (e) addressing the sim-to-real gap is\ncritical for real-world deployment; (f) human-robot collaboration remains\nlimited, especially in human-in-the-loop approaches where robots detect early\nsymptoms and humans validate uncertain cases; (g) multi-modal FMs with\nreal-time feedback will drive next-gen SSDM. For updates, resources, and\ncontributions, visit, https://github.com/nitin-dominic/AgriPathogenDatabase, to\nsubmit papers, code, or datasets.", "AI": {"tldr": "\u8be5\u7efc\u8ff0\u5206\u6790\u4e86\u7ea640\u7bc7\u5173\u4e8e\u57fa\u7840\u6a21\u578b\u5728\u4f5c\u7269\u7cbe\u51c6\u75c5\u5bb3\u7ba1\u7406\u4e2d\u7684\u5e94\u7528\u6587\u732e\uff0c\u91cd\u70b9\u5173\u6ce8\u5927\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u8ba8\u8bba\u4e86\u5b83\u4eec\u5728\u81ea\u9002\u5e94\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u6570\u5b57\u5b6a\u751f\u6846\u67b6\u4e2d\u7684\u4f5c\u7528\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u5728\u5b9e\u65f6\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u4f5c\u7269\u7cbe\u51c6\u75c5\u5bb3\u7ba1\u7406\u4ece\u624b\u5de5\u7279\u5f81\u63d0\u53d6\u53d1\u5c55\u5230\u5927\u89c4\u6a21\u81ea\u52a8\u5316\u7279\u5f81\u5b66\u4e60\u3002\u57fa\u7840\u6a21\u578b\u4ee5\u5168\u65b0\u65b9\u5f0f\u5904\u7406\u4f5c\u7269\u75c5\u5bb3\u6570\u636e\uff0c\u6574\u5408\u89c6\u89c9\u548c\u6587\u672c\u6570\u636e\uff0c\u89e3\u91ca\u75c7\u72b6\u6587\u672c\uff0c\u63a8\u7406\u75c7\u72b6\u4e0e\u7ba1\u7406\u7684\u5173\u7cfb\uff0c\u5e76\u4e3a\u79cd\u690d\u8005\u548c\u6559\u80b2\u8005\u63d0\u4f9b\u4ea4\u4e92\u5f0f\u95ee\u7b54\u652f\u6301\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u7b5b\u9009\u4e86\u7ea640\u7bc7\u5173\u4e8e\u57fa\u7840\u6a21\u578b\u5728\u7cbe\u51c6\u75c5\u5bb3\u7ba1\u7406\u4e2d\u5e94\u7528\u7684\u8bba\u6587\uff0c\u7279\u522b\u5173\u6ce8\u5927\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5206\u6790\u5b83\u4eec\u5728\u81ea\u9002\u5e94\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u6570\u5b57\u5b6a\u751f\u6846\u67b6\u4e2d\u7684\u4f5c\u7528\u3002", "result": "\u4e3b\u8981\u53d1\u73b0\u5305\u62ec\uff1a\u57fa\u7840\u6a21\u578b\u57282023-24\u5e74\u6587\u732e\u6fc0\u589e\uff1b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u53d1\u5c55\u5feb\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u51fa\u7248\u7269\u589e\u957f5-10\u500d\uff1b\u5f3a\u5316\u5b66\u4e60\u548c\u81ea\u9002\u5e94\u5b66\u4e60\u5728\u667a\u80fd\u55b7\u6d12\u4e2d\u4ecd\u5904\u4e8e\u65e9\u671f\u9636\u6bb5\uff1b\u6570\u5b57\u5b6a\u751f\u4e0e\u5f3a\u5316\u5b66\u4e60\u53ef\u6a21\u62df\u865a\u62df\u9776\u5411\u55b7\u6d12\uff1b\u89e3\u51b3\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u5bf9\u5b9e\u9645\u90e8\u7f72\u81f3\u5173\u91cd\u8981\uff1b\u4eba\u673a\u534f\u4f5c\u4ecd\u7136\u6709\u9650\uff1b\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u4e0e\u5b9e\u65f6\u53cd\u9988\u5c06\u63a8\u52a8\u4e0b\u4e00\u4ee3\u7cbe\u51c6\u75c5\u5bb3\u7ba1\u7406\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u6b63\u5728\u6539\u53d8\u4f5c\u7269\u7cbe\u51c6\u75c5\u5bb3\u7ba1\u7406\u7684\u65b9\u5f0f\uff0c\u7279\u522b\u662f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u6700\u4e3a\u8fc5\u901f\u3002\u867d\u7136\u5f3a\u5316\u5b66\u4e60\u548c\u81ea\u9002\u5e94\u5b66\u4e60\u5e94\u7528\u4ecd\u5904\u4e8e\u65e9\u671f\u9636\u6bb5\uff0c\u4f46\u6570\u5b57\u5b6a\u751f\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u7ed3\u5408\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\u3002\u672a\u6765\u9700\u8981\u89e3\u51b3\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\uff0c\u52a0\u5f3a\u4eba\u673a\u534f\u4f5c\uff0c\u53d1\u5c55\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u4e0e\u5b9e\u65f6\u53cd\u9988\u7cfb\u7edf\uff0c\u4ee5\u63a8\u52a8\u4e0b\u4e00\u4ee3\u7cbe\u51c6\u75c5\u5bb3\u7ba1\u7406\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.24663", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24663", "abs": "https://arxiv.org/abs/2510.24663", "authors": ["Yifu Lu", "Shengjie Liu", "Li Dong"], "title": "OrchDAG: Complex Tool Orchestration in Multi-Turn Interactions with Plan DAGs", "comment": "9 pages, 4 figures", "summary": "Agentic tool use has gained traction with the rise of agentic tool calling,\nyet most existing work overlooks the complexity of multi-turn tool\ninteractions. We introduce OrchDAG, a synthetic data generation pipeline that\nmodels tool execution as directed acyclic graphs (DAGs) with controllable\ncomplexity. Using this dataset, we benchmark model performance and propose a\ngraph-based reward to enhance RLVR training. Experiments show that the dataset\npresents a challenging but solvable benchmark, and the proposed reward is\neffective when combined with GRPO-style algorithms, highlighting the importance\nof leveraging topological structure and data complexity in multi-turn tool use.", "AI": {"tldr": "OrchDAG\u662f\u4e00\u4e2a\u5408\u6210\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u5c06\u5de5\u5177\u6267\u884c\u5efa\u6a21\u4e3a\u5177\u6709\u53ef\u63a7\u590d\u6742\u5ea6\u7684\u6709\u5411\u65e0\u73af\u56fe\uff0c\u7528\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u6a21\u578b\u6027\u80fd\u5e76\u63d0\u51fa\u57fa\u4e8e\u56fe\u7684\u5956\u52b1\u6765\u589e\u5f3aRLVR\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u5927\u591a\u5ffd\u89c6\u4e86\u591a\u8f6e\u5de5\u5177\u4ea4\u4e92\u7684\u590d\u6742\u6027\uff0c\u9700\u8981\u66f4\u597d\u7684\u65b9\u6cd5\u6765\u5efa\u6a21\u548c\u8bad\u7ec3\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u3002", "method": "\u5f15\u5165OrchDAG\u5408\u6210\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u5c06\u5de5\u5177\u6267\u884c\u5efa\u6a21\u4e3a\u6709\u5411\u65e0\u73af\u56fe\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u56fe\u7684\u5956\u52b1\u6765\u589e\u5f3aRLVR\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u4f46\u53ef\u89e3\u51b3\u7684\u57fa\u51c6\uff0c\u6240\u63d0\u51fa\u7684\u5956\u52b1\u4e0eGRPO\u98ce\u683c\u7b97\u6cd5\u7ed3\u5408\u65f6\u6548\u679c\u663e\u8457\u3002", "conclusion": "\u5728\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u4e2d\uff0c\u5229\u7528\u62d3\u6251\u7ed3\u6784\u548c\u6570\u636e\u590d\u6742\u5ea6\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2510.24690", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24690", "abs": "https://arxiv.org/abs/2510.24690", "authors": ["Shengjie Liu", "Li Dong", "Zhenyu Zhang"], "title": "Bridging Tool Dependencies and Domain Knowledge: A Graph-Based Framework for In-Context Planning", "comment": "4 pages, 2 figures, short paper, NeurIPS 2025 workshop on Bridging\n  Language, Agent, and World Models for Reasoning and Planning", "summary": "We present a framework for uncovering and exploiting dependencies among tools\nand documents to enhance exemplar artifact generation. Our method begins by\nconstructing a tool knowledge graph from tool schemas,including descriptions,\narguments, and output payloads, using a DeepResearch-inspired analysis. In\nparallel, we derive a complementary knowledge graph from internal documents and\nSOPs, which is then fused with the tool graph. To generate exemplar plans, we\nadopt a deep-sparse integration strategy that aligns structural tool\ndependencies with procedural knowledge. Experiments demonstrate that this\nunified framework effectively models tool interactions and improves plan\ngeneration, underscoring the benefits of linking tool graphs with domain\nknowledge graphs for tool-augmented reasoning and planning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u8fc7\u6784\u5efa\u5de5\u5177\u548c\u6587\u6863\u77e5\u8bc6\u56fe\u8c31\u6765\u589e\u5f3a\u793a\u4f8b\u5de5\u4ef6\u751f\u6210\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u5de5\u5177\u4f9d\u8d56\u5173\u7cfb\u548c\u9886\u57df\u77e5\u8bc6\u6765\u6539\u8fdb\u89c4\u5212\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u7684\u5de5\u5177\u589e\u5f3a\u65b9\u6cd5\u5f80\u5f80\u5ffd\u89c6\u4e86\u5de5\u5177\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u4ee5\u53ca\u5de5\u5177\u4e0e\u9886\u57df\u77e5\u8bc6\u4e4b\u95f4\u7684\u5173\u8054\uff0c\u9650\u5236\u4e86\u793a\u4f8b\u5de5\u4ef6\u751f\u6210\u7684\u8d28\u91cf\u548c\u6548\u679c\u3002", "method": "\u9996\u5148\u4ece\u5de5\u5177\u6a21\u5f0f\u6784\u5efa\u5de5\u5177\u77e5\u8bc6\u56fe\u8c31\uff0c\u540c\u65f6\u4ece\u5185\u90e8\u6587\u6863\u548cSOP\u6784\u5efa\u8865\u5145\u77e5\u8bc6\u56fe\u8c31\uff0c\u7136\u540e\u5c06\u4e24\u8005\u878d\u5408\u3002\u91c7\u7528\u6df1\u5ea6-\u7a00\u758f\u96c6\u6210\u7b56\u7565\u5bf9\u9f50\u7ed3\u6784\u5de5\u5177\u4f9d\u8d56\u4e0e\u7a0b\u5e8f\u77e5\u8bc6\u6765\u751f\u6210\u793a\u4f8b\u8ba1\u5212\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u7edf\u4e00\u6846\u67b6\u80fd\u6709\u6548\u5efa\u6a21\u5de5\u5177\u4ea4\u4e92\u5e76\u6539\u8fdb\u8ba1\u5212\u751f\u6210\uff0c\u8bc1\u660e\u4e86\u5c06\u5de5\u5177\u56fe\u8c31\u4e0e\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\u94fe\u63a5\u7684\u76ca\u5904\u3002", "conclusion": "\u901a\u8fc7\u878d\u5408\u5de5\u5177\u4f9d\u8d56\u5173\u7cfb\u548c\u9886\u57df\u77e5\u8bc6\uff0c\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u5de5\u5177\u589e\u5f3a\u63a8\u7406\u548c\u89c4\u5212\u7684\u6548\u679c\uff0c\u4e3a\u590d\u6742\u4efb\u52a1\u89c4\u5212\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}

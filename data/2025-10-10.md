<div id=toc></div>

# Table of Contents

- [econ.TH](#econ.TH) [Total: 3]
- [q-fin.GN](#q-fin.GN) [Total: 1]
- [econ.GN](#econ.GN) [Total: 3]
- [eess.SY](#eess.SY) [Total: 18]
- [stat.AP](#stat.AP) [Total: 4]
- [cs.SI](#cs.SI) [Total: 4]
- [cs.ET](#cs.ET) [Total: 2]
- [econ.EM](#econ.EM) [Total: 1]
- [cs.RO](#cs.RO) [Total: 35]
- [cs.CY](#cs.CY) [Total: 6]
- [cs.AI](#cs.AI) [Total: 68]


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [1] [Correlated Perfect Equilibrium](https://arxiv.org/abs/2510.07906)
*Wanying Huang,J. Jude Kline,Priscilla Man*

Main category: econ.TH

TL;DR: 提出了一种基于调解者错误的关联均衡精炼方法——关联完美均衡(CPE)，在有限博弈中非空且为有限个凸集的并集，与完美均衡类似，CPE不会给弱劣策略分配正概率。


<details>
  <summary>Details</summary>
Motivation: 现有关联均衡精炼方法存在不足，需要提出新的精炼概念来更好地处理博弈中的策略选择问题。

Method: 通过引入调解者错误的概念来精炼关联均衡，提供CPE的双重表示，并与Myerson的可接受关联均衡及Dhillon-Mertens的完美直接关联均衡进行比较。

Result: 证明了在有限博弈中CPE集合非空且为有限个凸集的并集，CPE不会选择弱劣策略，并通过示例展示了CPE与现有精炼方法的差异。

Conclusion: CPE为关联均衡提供了一个有效的精炼方法，具有理论上的良好性质，并能更好地处理博弈中的策略选择问题。

Abstract: We propose a refinement of correlated equilibrium based on mediator errors,
called correlated perfect equilibrium (CPE). In finite games, the set of CPE is
nonempty and forms a finite union of convex sets. Like perfect equilibrium, a
CPE never assigns positive probability to any weakly dominated strategy. We
provide a dual representation of CPE and demonstrate how it differs from two
existing refinements of correlated equilibrium--acceptable correlated
equilibrium (Myerson, 1986) and perfect direct correlated equilibrium
(Dhillon-Mertens, 1996)--through examples.

</details>


### [2] [The (No) Value of Commitment](https://arxiv.org/abs/2510.07994)
*Nathan Hancart*

Main category: econ.TH

TL;DR: 该论文提供了一个充分条件，说明在何种情况下委托人在经济模型中不会从机制承诺中获益，适用于机制设计、委托-代理模型和发送者-接收者博弈。


<details>
  <summary>Details</summary>
Motivation: 研究在约束最大化问题表示的经济模型中，委托人何时不会从机制承诺中获益，为机制设计理论提供新的理论洞见。

Method: 通过分析经济模型中的约束最大化问题，推导出一个充分条件，特别关注委托-代理问题中代理人的有限策略空间和委托人价值函数的连续性。

Result: 提出了一个充分条件，表明当代理人策略空间有限且委托人价值函数在机制上连续时，委托人不会从机制承诺中获益。

Conclusion: 该研究为理解机制承诺的价值提供了理论框架，特别是在委托-代理问题中，当满足特定条件时，机制承诺不会带来额外收益。

Abstract: I provide a sufficient condition under which a principal does not benefit
from committing to a mechanism in economic models represented by a maximisation
problem under constraints. These problems include mechanism design,
principal-agent models or sender-receiver games. In principal-agent problems,
this condition holds if the agent has a finite strategy space and the
principal's value function is continuous in the mechanism.

</details>


### [3] [Persuasion with Verifiable Information](https://arxiv.org/abs/2510.08251)
*Maria Titova,Kun Zhang*

Main category: econ.TH

TL;DR: 研究有状态独立偏好的知情发送者如何通过可验证信息说服接收者选择有限集合中的行动，比较博弈均衡结果与信息设计中的承诺结果。


<details>
  <summary>Details</summary>
Motivation: 探讨在可验证信息传递的博弈中，发送者是否能够通过承诺获得优势，以及可验证性与承诺权力之间的可替代性。

Method: 通过博弈论模型分析，刻画均衡结果，并与信息设计中的承诺结果进行比较，提供条件判断承诺结果是否为均衡结果。

Result: 确定了承诺结果是均衡结果的条件，识别了发送者无法从承诺权力中获益的环境。

Conclusion: 研究结果为可验证性与承诺在应用环境中的可替代性提供了重要见解。

Abstract: This paper studies a game in which an informed sender with state-independent
preferences uses verifiable messages to convince a receiver to choose an action
from a finite set. We characterize the equilibrium outcomes of the game and
compare them with commitment outcomes in information design. We provide
conditions under which a commitment outcome is an equilibrium outcome and
identify environments in which the sender does not benefit from commitment
power. Our findings offer insights into the interchangeability of verifiability
and commitment in applied settings.

</details>


<div id='q-fin.GN'></div>

# q-fin.GN [[Back]](#toc)

### [4] [Smart Contract-Enabled Procurement under Bounded Demand Variability: A Truncated Normal Approach](https://arxiv.org/abs/2510.07801)
*Jinho Cha,Youngchul Kim,Junyeol Ryu,Sangjun Park,Jeongho Kang,Hyeyoung Hwang*

Main category: q-fin.GN

TL;DR: 开发了一个结合区块链智能合约和有界需求变动的战略采购框架，通过多供应商模型优化智能合约采用强度和供应商分配决策。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注智能合约的技术可行性，但在中等不确定性下采用的操作和经济影响研究不足。

Method: 提出多供应商模型，零售商联合确定最优智能合约采用强度和供应商分配决策，内生考虑采用成本、供应商数字化准备度和库存惩罚。

Result: 分析结果建立了凹性并提供采用阈值和采购数量的闭式比较静态分析；数值实验显示中等需求变动支持部分采用策略，过度数字基础设施投资会降低盈利能力。

Conclusion: 研究为平衡数字化转型、韧性和可持续性目标提供了理论和管理的见解，动态模拟显示自适应学习和实施成本下降逐步提升采用强度和供应链绩效。

Abstract: This study develops a strategic procurement framework integrating
blockchain-based smart contracts with bounded demand variability modeled
through a truncated normal distribution. While existing research emphasizes the
technical feasibility of smart contracts, the operational and economic
implications of adoption under moderate uncertainty remain underexplored. We
propose a multi-supplier model in which a centralized retailer jointly
determines the optimal smart contract adoption intensity and supplier
allocation decisions. The formulation endogenizes adoption costs, supplier
digital readiness, and inventory penalties to capture realistic trade-offs
among efficiency, sustainability, and profitability. Analytical results
establish concavity and provide closed-form comparative statics for adoption
thresholds and procurement quantities. Extensive numerical experiments
demonstrate that moderate demand variability supports partial adoption
strategies, whereas excessive investment in digital infrastructure can reduce
overall profitability. Dynamic simulations further reveal how adaptive learning
and declining implementation costs progressively enhance adoption intensity and
supply chain performance. The findings provide theoretical and managerial
insights for balancing digital transformation, resilience, and sustainability
objectives in smart contract-enabled procurement.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [5] [The evolution of insurance purchasing behavior: an empirical study on the adoption of online channels in Poland](https://arxiv.org/abs/2510.07933)
*Gabriela Wojak,Ernest Górka,Michał Ćwiąkała,Dariusz Baran,Rafał Świniarski,Katarzyna Olszyńska,Piotr Mrzygłód,Maciej Frasunkiewicz,Piotr Ręczajski,Daniel Zawadzki,Jan Piwnik*

Main category: econ.GN

TL;DR: 波兰消费者正在转向在线保险购买渠道，但偏好混合模式——在线工具用于研究和比较，传统代理用于复杂决策。价格是主要决策因素，但信任和服务质量也很重要。


<details>
  <summary>Details</summary>
Motivation: 研究波兰消费者如何适应在线保险购买渠道，以及影响他们偏好的因素，了解数字化时代保险分销的变化。

Method: 通过对100名不同人口统计特征的受访者进行结构化调查，探讨购买频率、渠道使用、价格敏感性、信任和决策行为。

Result: 结果显示向数字工具的明显转变，消费者重视在线平台的速度、便利性和透明度，特别是简单保险产品。但存在数据安全担忧、缺乏个人指导和理解保单条款困难等障碍。

Conclusion: 保险公司应投资用户友好的数字体验，同时保持人工支持选项。建议战略性的全渠道整合以满足多样化客户需求并减少数字排斥。

Abstract: This paper examines how Polish consumers are adapting to online insurance
purchasing channels and what factors influence their preferences. Drawing on a
structured survey of 100 respondents with varied demographic profiles, the
study explores purchasing frequency, channel usage, price sensitivity, trust,
and decision-making behaviors. Results indicate a clear shift toward digital
tools, with many consumers valuing the speed, convenience, and transparency of
online platforms, particularly for simple insurance products. However, barriers
remain, including concerns about data security, lack of personal guidance, and
difficulty understanding policy terms. A hybrid model is emerging, where online
tools are used for research and comparison, while traditional agents are
consulted for complex decisions. Respondents emphasized the importance of trust
and personal contact, showing that emotional and psychological factors still
play a role in digital adoption. Price was the dominant decision factor, but
many consumers also prioritized service quality and reliability. The study
concludes that insurers should invest in user-friendly digital experiences
while maintaining human support options. Strategic omnichannel integration is
recommended to meet diverse customer needs and reduce digital exclusion.
Limitations of the study include a modest sample size and focus on the Polish
market. Future research should investigate the role of AI in digital
distribution, segment preferences by insurance type, and analyze trends across
different regions or age groups. This paper adds empirical value to the
understanding of insurance distribution and consumer behavior in digitally
transforming financial markets.

</details>


### [6] [AI as a Centripetal Technology: Price Compression, Homogenization, and Entry](https://arxiv.org/abs/2510.08337)
*Aliya Turegeldinova,Bakytzhan Amralinova,Mate Miklos Fodor,Akerkin Eraliyeva,Chen Dayou,Aidos Joldassov*

Main category: econ.GN

TL;DR: 生成式AI通过降低边际成本和提高固定访问成本，使产品趋同化，导致市场集中度增加、价格下降但进入门槛提高。


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI如何通过产品同质化效应影响市场竞争结构，解释AI同时带来价格下降和市场集中度增加的矛盾现象。

Method: 采用标准的两阶段差异化竞争框架，分析AI能力变化如何压缩感知差异、降低边际成本并提高固定成本。

Result: 发现存在一个能力阈值，超过该阈值时即使两个企业也无法覆盖固定成本，可持续企业数量随AI能力增长而减少。

Conclusion: AI在降低价格的同时提高了进入壁垒，需要通过促进模板多样性和互操作性来平衡价格效益与市场多样性。

Abstract: Generative AI does more than cut costs. It pulls products toward a shared
template, making offerings look and feel more alike while making true
originality disproportionately expensive. We capture this centripetal force in
a standard two-stage differentiated-competition framework and show how a single
capability shift simultaneously compresses perceived differences, lowers
marginal cost and raises fixed access costs. The intuition is straightforward.
When buyers see smaller differences across products, the payoff to standing
apart shrinks just as the effort to do so rises, so firms cluster around the
template. Prices fall and customers become more willing to switch. But the same
homogenization also squeezes operating margins, and rising fixed outlays deepen
the squeeze. The combination yields a structural prediction. There is a
capability threshold at which even two firms cannot both cover fixed costs, and
in a many-firm extension the sustainable number of firms falls as capability
grows. Concentration increases, and prices still fall. Our results hold under
broader preference shapes, non-uniform consumer densities, outside options,
capability-dependent curvatures, and modest asymmetries. We translate the
theory into two sufficient statistics for enforcement. On the one hand, a
conduct statistic and a viability statistic. Transactions or platform rules
that strengthen template pull or raise fixed access and originality costs can
lower prices today yet push the market toward monoculture. Remedies that
broaden access and promote template plurality and interoperability preserve the
price benefits of AI while protecting entry and variety. The paper thus
reconciles a live policy paradox. AI can make prices lower and entry harder at
the same time. It prescribes what to measure to tell which force is dominant in
practice.

</details>


### [7] [A data fusion approach for mobility hub impact assessment and location selection: integrating hub usage data into a large-scale mode choice model](https://arxiv.org/abs/2510.08366)
*Xiyuan Ren,Joseph Y. J. Chow*

Main category: econ.GN

TL;DR: 该研究提出了一种数据融合方法，将观察到的移动枢纽使用情况整合到基于合成出行数据的模式选择模型中，用于评估移动枢纽对出行需求、模式转换、车辆行驶里程减少和消费者剩余增加的影响。


<details>
  <summary>Details</summary>
Motivation: 城市面临交通拥堵和服务不平等问题，移动枢纽提供了一个可扩展的解决方案来协调日益增长的出行需求与可持续发展目标，但评估其影响仍具有挑战性。

Method: 采用数据融合方法，识别可能受移动枢纽影响的出行，构建多模式子选择集，使用现场调查数据和真实出行计数来校准枢纽特定参数。

Result: 在纽约州首府区的案例研究中，两个已实施的枢纽预计每天分别产生8.83和6.17次多模式出行，每年减少车辆行驶里程20.37和13.16千英里，每日增加消费者剩余4,000美元和1,742美元。

Conclusion: 位于城际走廊和城市边缘、支持停车换乘模式的枢纽候选点显示出最显著的行为影响。

Abstract: As cities grapple with traffic congestion and service inequities, mobility
hubs offer a scalable solution to align increasing travel demand with
sustainability goals. However, evaluating their impacts remains challenging due
to the lack of behavioral models that integrate large-scale travel patterns
with real-world hub usage. This study presents a novel data fusion approach
that incorporates observed mobility hub usage into a mode choice model
estimated with synthetic trip data. We identify trips potentially affected by
mobility hubs and construct a multimodal sub-choice set, then calibrate
hub-specific parameters using on-site survey data and ground truth trip counts.
The enhanced model is used to evaluate mobility hub impacts on potential
demand, mode shift, reduced vehicle miles traveled (VMT), and increased
consumer surplus (CS). We apply this method to a case study in the Capital
District, NY, using data from a survey conducted by the Capital District
Transportation Authority (CDTA) and a mode choice model estimated using Replica
Inc. synthetic data. The two implemented hubs located near UAlbany Downtown
Campus and in Downtown Cohoes are projected to generate 8.83 and 6.17
multimodal trips per day, reduce annual VMT by 20.37 and 13.16 thousand miles,
and increase daily CS by $4,000 and $1,742, respectively. An evaluation of
potential hub candidates in the Albany-Schenectady-Troy metropolitan area with
the estimated models demonstrates that hubs located along intercity corridors
and at urban peripheries, supporting park-and-ride P+R patterns, yield the most
significant behavioral impacts.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [8] [Auctioning Future Services in Edge Networks with Moving Vehicles: N-Step Look-Ahead Contracts for Sustainable Resource Provision](https://arxiv.org/abs/2510.07333)
*Ziqi Ling,Minghui Liwang,Xianbin Wang,Seyyedali Hosseinalipour,Zhipeng Cheng,Sai Zou,Wei Ni,Xiaoyu Xia*

Main category: eess.SY

TL;DR: 提出了一种基于前瞻性合约的拍卖框架，用于边缘辅助车载网络中的资源分配，通过将决策从运行时转移到规划时来提高效率。


<details>
  <summary>Details</summary>
Motivation: 解决车辆移动性导致的资源需求时空不可预测性，以及实时双重拍卖带来的显著延迟问题。

Method: 采用两阶段方法：1）LSTM预测模块预测多时隙资源需求并确定边缘服务器角色；2）预双重拍卖生成包含资源数量、价格和违约金的合约，在实时执行时无需重新运行拍卖。

Result: 在真实世界（UTD19）和合成轨迹上的实验表明，该方法在时间效率、能源使用和社会福利方面优于现有基线方法。

Conclusion: 该框架通过将决策前移，结合能源成本、传输开销和违约风险，实现了真实、理性和节能的资源交易。

Abstract: Timely resource allocation in edge-assisted vehicular networks is essential
for compute-intensive services such as autonomous driving and navigation.
However, vehicle mobility leads to spatio-temporal unpredictability of resource
demands, while real-time double auctions incur significant latency. To address
these challenges, we propose a look-ahead contract-based auction framework that
shifts decision-making from runtime to planning time. Our approach establishes
N-step service contracts between edge servers (ESs) using demand forecasts and
modified double auctions. The system operates in two stages: first, an
LSTM-based prediction module forecasts multi-slot resource needs and determines
ES roles (buyer or seller), after which a pre-double auction generates
contracts specifying resource quantities, prices, and penalties. Second, these
contracts are enforced in real time without rerunning auctions. The framework
incorporates energy costs, transmission overhead, and contract breach risks
into utility models, ensuring truthful, rational, and energy-efficient trading.
Experiments on real-world (UTD19) and synthetic traces demonstrate that our
method improves time efficiency, energy use, and social welfare compared with
existing baselines.

</details>


### [9] [Nonlinear System Identification for Model-Based Control of Waked Wind Turbines](https://arxiv.org/abs/2510.07336)
*Sebastiano Randino,Lorenzo Schena,Nicolas Coudou,Emanuele Garone,Miguel Alfonso Mendez*

Main category: eess.SY

TL;DR: 提出了一个非线性系统辨识框架，用于建模风力涡轮机的功率提取动力学，包括自由流和尾流条件。该方法使用数据驱动的功率系数映射，结合径向基函数和多项式基函数，嵌入一阶动态系统，适用于基于模型的控制。


<details>
  <summary>Details</summary>
Motivation: 需要开发能够准确建模风力涡轮机功率提取动力学的框架，特别是在复杂尾流条件下，以改进基于模型的控制策略，提高功率稳定性和性能。

Method: 使用数据驱动的功率系数映射，结合径向基函数和多项式基函数，参数化为叶尖速比和上游条件的函数。这些代理模型嵌入一阶动态系统，并在两种风洞配置中进行实验验证：低湍流串联设置和高湍流风电场场景。

Result: 在串联情况下，将识别模型集成到改进的Kω²控制器中，与基于BEM和稳态模型相比，提高了叶尖速比跟踪和功率稳定性。在风电场场景中，模型能够捕捉涡轮机的统计行为，尽管存在未解析的湍流。

Conclusion: 所提出的方法能够在各种运行条件下实现可解释的自适应控制，而不依赖于黑盒学习策略，为风力涡轮机控制提供了有效的建模框架。

Abstract: This work presents a nonlinear system identification framework for modeling
the power extraction dynamics of wind turbines, including both freestream and
waked conditions. The approach models turbine dynamics using data-driven power
coefficient maps expressed as combinations of compact radial basis functions
and polynomial bases, parameterized in terms of tip-speed ratio and upstream
conditions. These surrogate models are embedded in a first-order dynamic system
suitable for model-based control. Experimental validation is carried out in two
wind tunnel configurations: a low-turbulence tandem setup and a high-turbulence
wind farm scenario. In the tandem case, the identified model is integrated into
an adapted K\omega^2 controller, resulting in improved tip-speed ratio tracking
and power stability compared to BEM-based and steady-state models. In the wind
farm scenario, the model captures the statistical behavior of the turbines
despite unresolved turbulence. The proposed method enables interpretable,
adaptive control across a range of operating conditions without relying on
black-box learning strategies.

</details>


### [10] [Techno-economic analysis of self-sustainable thermophotovoltaic systems for grid-scale energy generation](https://arxiv.org/abs/2510.07338)
*Jihun Lim,Sungwon Lee*

Main category: eess.SY

TL;DR: 该论文对自持续热光伏系统进行了技术经济分析，发现虽然从储能成本角度看目前不具竞争力，但其电力成本可与传统可调度发电机竞争，特别是硅基系统在大规模部署时具有实际工程优势。


<details>
  <summary>Details</summary>
Motivation: 为促进可再生能源广泛应用，需要可调度、零排放的电源来维持电网稳定。

Method: 使用基于理论模型的空气桥InGaAs和硅二极管电池，对自持续热光伏系统进行全面的技术经济分析。

Result: 分析显示，虽然系统因热电池材料资本支出高而在储能成本方面不具竞争力，但其电力成本可与传统燃气轮机竞争，特别是硅基系统在吉瓦时规模下具有竞争力。

Conclusion: 自持续热光伏架构为提供电网规模、按需、零排放电力提供了一条有前景的途径，特别是利用硅材料的制造可扩展性提供了较低风险的部署路径。

Abstract: To facilitate the widespread adoption of renewable energy, dispatchable,
zero-emission power sources are essential for grid stability. This work
performs a comprehensive techno-economic analysis of a self-sustainable
thermophotovoltaic (TPV) system, an architecture that integrates solar charging
to function as a standalone power generation asset. Using theory-based models
for air-bridge InGaAs and Si diode cells, our analysis reveals that while the
system is not currently competitive from a pure levelized of storage cost
(LCOS) perspective due to the high capital expenditure for thermal battery
materials, its primary value lies in its competitive levelized cost of
electricity (LCOE). The results demonstrate that the LCOE of this
self-sustaining system can be competitive with conventional dispatchable
generators, such as gas turbines. Furthermore, at scales exceeding the
gigawatt-hour level, a Si-based system can also achieve an LCOE comparable to
that of traditional gas-turbine power plants, despite having a lower conversion
efficiency than its InGaAs counterpart. This highlights a practical engineering
pathway for leveraging silicon's immense manufacturing scalability, offering a
lower-risk route to deployment compared to III-V materials. Ultimately, this
work establishes the self-sustainable TPV architecture as a compelling pathway
toward providing grid-scale, on-demand, zero-emission power.

</details>


### [11] [Adaptive Control Allocation for Underactuated Time-Scale Separated Non-Affine Systems](https://arxiv.org/abs/2510.07507)
*Daniel M. Cherenson,Dimitra Panagou*

Main category: eess.SY

TL;DR: 提出了一种针对不确定、非线性、欠驱动系统的自适应控制架构，通过时间尺度分离和动态控制分配来处理输入约束和建模不确定性。


<details>
  <summary>Details</summary>
Motivation: 许多机器人系统是欠驱动的，这意味着由于缺乏执行器、输入约束或状态相关的驱动，无法直接控制所有自由度。这种特性加上建模不确定性和干扰，使得轨迹跟踪的控制设计变得复杂。

Method: 利用时间尺度分离构建降阶模型，其中快速动力学为较慢子系统提供虚拟输入，并使用动态控制分配来选择给定非仿射动力学的最优控制输入。引入基于状态预测器的自适应律来处理不确定性。

Result: 通过奇异摄动理论和Lyapunov分析，证明了参考轨迹的稳定性和有界跟踪。在具有非线性、状态相关驱动的VTOL四翼飞机上验证了该方法。

Conclusion: 该方法作为统一控制器在各种飞行状态（包括巡航、着陆过渡和悬停）中展示了其实用性。

Abstract: Many robotic systems are underactuated, meaning not all degrees of freedom
can be directly controlled due to lack of actuators, input constraints, or
state-dependent actuation. This property, compounded by modeling uncertainties
and disturbances, complicates the control design process for trajectory
tracking. In this work, we propose an adaptive control architecture for
uncertain, nonlinear, underactuated systems with input constraints. Leveraging
time-scale separation, we construct a reduced-order model where fast dynamics
provide virtual inputs to the slower subsystem and use dynamic control
allocation to select the optimal control inputs given the non-affine dynamics.
To handle uncertainty, we introduce a state predictor-based adaptive law, and
through singular perturbation theory and Lyapunov analysis, we prove stability
and bounded tracking of reference trajectories. The proposed method is
validated on a VTOL quadplane with nonlinear, state-dependent actuation,
demonstrating its utility as a unified controller across various flight
regimes, including cruise, landing transition, and hover.

</details>


### [12] [Some Reflections on Sliding Mode Designs in Control Systems: An Example of Adaptive Tracking Control for Simple Mechanical Systems With Friction Without Measurement of Velocity](https://arxiv.org/abs/2510.07675)
*Romeo Ortega,Leyan Fang,Jose Guadalupe Romero*

Main category: eess.SY

TL;DR: 本文对滑模控制在控制系统中的应用进行了批判性反思，指出当前文献中存在的问题，并通过具体例子比较了两种自适应跟踪控制器的性能。


<details>
  <summary>Details</summary>
Motivation: 作者认为滑模控制在科学出版物中的大量出现需要进行批判性评估，以审视其实际作用、相关性和适用性。

Method: 首先讨论大多数滑模设计遵循的程序，并引用文献中的例子；其次指出滑模文献中忽视的控制问题关键方面；最后通过比较两种自适应跟踪控制器在单自由度机械系统中的性能来具体说明。

Result: 通过对比分析，揭示了滑模控制设计中可能存在的问题和局限性。

Conclusion: 滑模控制设计需要更全面的考虑，包括经典控制设计中重视但被滑模文献忽视的关键方面。

Abstract: The objective of this note is to share some reflections of the authors
regarding the use of sliding mode designs in control systems. We believe the
abundant, and ever increasing, appearance of this kind of works on our
scientific publications deserves some critical evaluation of their actual role,
relevance and pertinence. First, we discuss the procedure followed by most of
these designs -- illustrated with examples from the literature. Second, we
bring to the readers attention several aspects of the control problem, central
in classical designs, which are disregarded in the sliding mode literature.
Finally, to illustrate with an specific example our previous considerations, we
compare the performance of two adaptive tracking controllers for a simple one
degree of freedom mechanical systems with unknown parameters and static and
Coulomb friction -- that do not rely on the measurement of velocity.

</details>


### [13] [Space Logistics Analysis and Incentive Design for Commercialization of Orbital Debris Remediation](https://arxiv.org/abs/2510.07708)
*Asaad Abdul-Hamid,Brycen D. Pearl,Hang Woon Lee,Hao Chen*

Main category: eess.SY

TL;DR: 开发了一个空间物流框架，用于规划轨道碎片清理任务，为空间运营商和碎片清理者之间的互利合作提供定量基础。


<details>
  <summary>Details</summary>
Motivation: 随着轨道碎片问题日益成为空间行业的优先事项，需要探索公私合作如何帮助解决这一问题。

Method: 通过整合基于网络的空间物流和博弈论，分析轨道碎片清理的高层成本和可共享的剩余价值。

Result: 研究结果显示了在开发安全、可持续且有利可图的空间经济方面取得的显著进展。

Conclusion: 该框架为空间运营商和碎片清理者之间的互利合作提供了定量基础，推动空间经济的可持续发展。

Abstract: As orbital debris continues to become a higher priority for the space
industry, there is a need to explore how partnerships between the public and
private space sector may aid in addressing this issue. This research develops a
space logistics framework for planning orbital debris remediation missions,
providing a quantitative basis for partnerships that are mutually beneficial
between space operators and debris remediators. By integrating network-based
space logistics and game theory, we illuminate the high-level costs of
remediating orbital debris, and the surplus that stands to be shared as a
result. These findings indicate significant progress toward the continued
development of a safe, sustainable, and profitable space economy.

</details>


### [14] [Multi-Level Multi-Fidelity Methods for Path Integral and Safe Control](https://arxiv.org/abs/2510.07756)
*Zhuoyuan Wang,Takashi Tanaka,Yongxin Chen,Yorie Nakahira*

Main category: eess.SY

TL;DR: 提出了一种结合多级蒙特卡洛和多保真度蒙特卡洛的高效方法，用于风险量化和路径积分控制，利用不同保真度模型的数据来降低方差并提高采样效率。


<details>
  <summary>Details</summary>
Motivation: 在缺乏解析模型的系统中，基于采样的方法被广泛使用，但收集足够数据成本高昂。许多情况下存在低保真度模型或模拟器，可以低成本获取样本。

Method: 整合多级蒙特卡洛(MLMC)和多保真度蒙特卡洛(MFMC)，使来自不同时间和状态表示（系统模型）的数据能够联合使用，以降低方差并提高采样效率。

Result: 理论分析表明所提出的估计器在温和条件下是无偏且一致的。数值模拟显示该方法在风险量化和路径积分控制方面具有改进的计算成本与精度权衡。

Conclusion: 该方法通过利用多模型数据有效解决了高成本采样问题，为风险量化和控制问题提供了更高效的解决方案。

Abstract: Sampling-based approaches are widely used in systems without analytic models
to estimate risk or find optimal control. However, gathering sufficient data in
such scenarios can be prohibitively costly. On the other hand, in many
situations, low-fidelity models or simulators are available from which samples
can be obtained at low cost. In this paper, we propose an efficient approach
for risk quantification and path integral control that leverages such data from
multiple models with heterogeneous sampling costs. A key technical novelty of
our approach is the integration of Multi-level Monte Carlo (MLMC) and
Multi-fidelity Monte Carlo (MFMC) that enable data from different time and
state representations (system models) to be jointly used to reduce variance and
improve sampling efficiency. We also provide theoretical analysis of the
proposed method and show that our estimator is unbiased and consistent under
mild conditions. Finally, we demonstrate via numerical simulation that the
proposed method has improved computation (sampling costs) vs. accuracy
trade-offs for risk quantification and path integral control.

</details>


### [15] [Topology optimization of nonlinear forced response curves via reduction on spectral submanifolds](https://arxiv.org/abs/2510.07900)
*Hongming Liang,Matteo Pozzi,Jacopo Marconi,Shobhit Jain,Mingwu Li*

Main category: eess.SY

TL;DR: 使用谱子流形(SSM)降阶理论来优化非线性系统的强迫响应曲线(FRC)，包括峰值振幅、硬化/软化行为和鞍结分岔距离的优化，应用于MEMS器件设计。


<details>
  <summary>Details</summary>
Motivation: 拓扑优化在调节非线性动态响应方面潜力巨大，但在高维系统中受到重复响应和灵敏度分析高成本的限制。

Method: 采用SSM降阶理论，将周期响应重新表述为降阶模型的平衡点，实现响应振幅及其灵敏度的有效解析评估。

Result: 提出的方法成功应用于非线性MEMS器件的设计，实现了目标性能优化。

Conclusion: 该框架为将非线性动态效应纳入结构拓扑优化提供了实用高效的策略。

Abstract: Forced response curves (FRCs) of nonlinear systems can exhibit complex
behaviors, including hardening/softening behavior and bifurcations. Although
topology optimization holds great potential for tuning these nonlinear dynamic
responses, its use in high-dimensional systems is limited by the high cost of
repeated response and sensitivity analyses. To address this challenge, we
employ the spectral submanifolds (SSMs) reduction theory, which reformulates
the periodic response as the equilibria of an associated reduced-order model
(ROM). This enables efficient and analytic evaluation of both response
amplitudes and their sensitivities. Based on the SSM-based ROM, we formulate
optimization problems that optimize the peak amplitude, the hardening/softening
behavior, and the distance between two saddle-node bifurcations for an FRC. The
proposed method is applied to the design of nonlinear MEMS devices, achieving
targeted performance optimization. This framework provides a practical and
efficient strategy for incorporating nonlinear dynamic effects into the
topology optimization of structures.

</details>


### [16] [Multi-level informed optimization via decomposed Kriging for large design problems under uncertainty](https://arxiv.org/abs/2510.07904)
*Enrico Ampellio,Blazhe Gjorgiev,Giovanni Sansavini*

Main category: eess.SY

TL;DR: 提出一种多层级方法，用于在不确定性下准确优化资源密集型、高维复杂工程问题，通过Kriging代理模型和分层正交分解实现高效优化。


<details>
  <summary>Details</summary>
Motivation: 传统基于场景、代理辅助和数学规划方法在大型复杂工程设计中不够可扩展，无法在资源有限情况下实现精确优化。

Method: 开发非侵入式、快速扩展的Kriging代理模型，通过分层正交分解自适应更新多个代理模型，利用较少但最不确定性的数据。

Result: 与现有技术相比，所提方法在分析测试平台上显示出数量级上的更快速度和更高精度。

Conclusion: 该方法为资源密集型、高维复杂工程问题在不确定性下的优化提供了高效准确的解决方案。

Abstract: Engineering design involves demanding models encompassing many decision
variables and uncontrollable parameters. In addition, unavoidable aleatoric and
epistemic uncertainties can be very impactful and add further complexity. The
state-of-the-art adopts two steps, uncertainty quantification and design
optimization, to optimize systems under uncertainty by means of robust or
stochastic metrics. However, conventional scenario-based, surrogate-assisted,
and mathematical programming methods are not sufficiently scalable to be
affordable and precise in large and complex cases. Here, a multi-level approach
is proposed to accurately optimize resource-intensive, high-dimensional, and
complex engineering problems under uncertainty with minimal resources. A
non-intrusive, fast-scaling, Kriging-based surrogate is developed to map the
combined design/parameter domain efficiently. Multiple surrogates are
adaptively updated by hierarchical and orthogonal decomposition to leverage the
fewer and most uncertainty-informed data. The proposed method is statistically
compared to the state-of-the-art via an analytical testbed and is shown to be
concurrently faster and more accurate by orders of magnitude.

</details>


### [17] [A Stable, Accurate and Well-Conditioned Time-Domain PMCHWT Formulation](https://arxiv.org/abs/2510.07989)
*Van Chien Le,Cedric Munger,Francesco P. Andriulli,Kristof Cools*

Main category: eess.SY

TL;DR: 提出了一种基于时域PMCHWT方程的新型边界元方法，用于均匀介质物体的瞬态电磁散射分析。通过Calderon预条件子解决密集网格问题，利用准Helmholtz投影器重新缩放Helmholtz分量来同时解决大时间步长和后期不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 解决传统边界元方法在瞬态电磁散射分析中面临的密集网格崩溃、大时间步长不稳定和后期时间不稳定性等挑战。

Method: 采用时域PMCHWT方程，结合乘法Calderon预条件子（使用改进的静态电场积分算子），通过准Helmholtz投影器重新缩放Helmholtz分量，使用时间微分和积分作为重缩放算子。

Result: 数值实验表明，该方法对于简单和多重连接的介质散射体（包括高度非光滑几何体）具有高精度、稳定性和计算效率。

Conclusion: 所提出的方法有效解决了瞬态电磁散射分析中的关键数值稳定性问题，为复杂介质散射体的时域分析提供了可靠的计算框架。

Abstract: This paper introduces a new boundary element formulation for transient
electromagnetic scattering by homogeneous dielectric objects based on the
time-domain PMCHWT equation. To address dense-mesh breakdown, a multiplicative
Calderon preconditioner utilizing a modified static electric field integral
operator is employed. Large-timestep breakdown and late-time instability are
simultaneously resolved by rescaling the Helmholtz components leveraging the
quasi-Helmholtz projectors and using temporal differentiation and integration
as rescaling operators. This rescaling also balances the loop and star
components at large timesteps, improving solution accuracy. The resulting
discrete system is solved using a marching-on-in-time scheme and iterative
solvers. Numerical experiments for simply- and multiply-connected dielectric
scatterers, including highly non-smooth geometries, corroborate the accuracy,
stability, and efficiency of the proposed approach.

</details>


### [18] [General formulation of an analytic, Lipschitz continuous control allocation for thrust-vectored controlled rigid-bodies](https://arxiv.org/abs/2510.08119)
*Frank Mukwege,Tam Willy Nguyen,Emanuele Garone*

Main category: eess.SY

TL;DR: 提出了两种新颖的推力分配方法：闭式Lipschitz连续映射和凸优化方法，用于处理带矢量推进器的刚体系统，能够避免奇异性并处理推力饱和和角速率限制等实际约束。


<details>
  <summary>Details</summary>
Motivation: 为具有矢量推进器的刚体系统开发系统化和可扩展的推力分配方法，解决传统方法在处理奇异性、平滑性和实际约束方面的不足。

Method: 1. 闭式Lipschitz连续映射确保平滑的执行器方向参考；2. 凸优化公式处理推力饱和和角速率限制等实际约束；3. 利用分配映射的零空间结构进行奇异性避免。

Result: 通过3自由度海洋船舶和6自由度空中四旋翼的数值仿真验证了所提框架的有效性和通用性。

Conclusion: 提出的框架能够生成次优但实用的解决方案，在避免奇异性的同时有效处理实际执行器约束，适用于各种刚体系统。

Abstract: This study introduces a systematic and scalable method for arbitrary
rigid-bodies equipped with vectorized thrusters. Two novel solutions are
proposed: a closed-form, Lipschitz continuous mapping that ensures smooth
actuator orientation references, and a convex optimization formulation capable
of handling practical actuator constraints such as thrust saturation and
angular rate limits. Both methods leverage the null-space structure of the
allocation mapping to perform singularity avoidance while generating
sub-optimal yet practical solutions. The effectiveness and generality of the
proposed framework are demonstrated through numerical simulations on a 3DOF
marine vessel and a 6DOF aerial quadcopter.

</details>


### [19] [Closed-loop control of sloshing fuel in a spinning spacecraft](https://arxiv.org/abs/2510.08121)
*Umberto Zucchelli,Miguel Alfonso Mendez,Annafederica Urbano,Sebastien Vincent-Bonnieu,Piotr Wenderski,Francesco Sanfedino*

Main category: eess.SY

TL;DR: 比较CFD和简化模型在航天器控制-结构-推进剂耦合动力学分析中的性能，验证简化模型在特定机动中的有效性


<details>
  <summary>Details</summary>
Motivation: 新一代航天任务需要卫星携带大量液体推进剂，必须详细分析控制-结构-推进剂耦合动力学，但CFD计算成本高，而等效机械模型的闭环性能尚未充分研究

Method: 使用计算流体动力学和降阶晃动模型对反馈控制下的航天器进行对比分析

Result: 两种方法结果吻合良好，验证了简化模型在所考虑机动中的有效性

Conclusion: 该验证支持进行高效的灵敏度和稳定性研究，为早期航天器设计提供了实用工具

Abstract: New-generation space missions require satellites to carry substantial amounts
of liquid propellant, making it essential to analyse the coupled
control-structure-propellant dynamics in detail. While Computational Fluid
Dynamics (CFD) offers high-fidelity predictions, its computational cost limits
its use in iterative design. Equivalent Mechanical Models (EMMs) provide a
faster alternative, though their predictive performance, especially in
closed-loop scenarios, remains largely unexplored. This work presents a
comparative analysis of a spacecraft under feedback control, using both CFD and
a reduced-order sloshing model. Results show good agreement, validating the
simplified model for the manoeuvrer considered. This validation enables
efficient sensitivity and stability studies, offering a practical tool for
early-stage spacecraft design.

</details>


### [20] [SecuLEx: a Secure Limit Exchange Market for Dynamic Operating Envelopes](https://arxiv.org/abs/2510.08172)
*Maurizio Vassallo,Adrien Bolland,Alireza Bahmanyar,Louis Wehenkel,Laurine Duchesne,Dong Liu,Sania Khaskheli,Alexis Ha Thuc,Pedro P. Vergara,Amjad Anvari-Moghaddam,Simon Gerard,Damien Ernst*

Main category: eess.SY

TL;DR: 提出SecuLEx（安全限额交换）市场机制，通过动态运行包络分配和交换电力注入/提取限额，确保电网安全并提高可再生能源利用率。


<details>
  <summary>Details</summary>
Motivation: 分布式能源资源正在改变电网运行，挑战传统运营方法，需要新的协调机制来保证网络安全。

Method: 引入SecuLEx市场范式：配电网运营商分配初始动态运行包络，用户可通过市场交换重新分配限额，同时确保电网运行约束。

Result: 在小规模低压电网示例中，SecuLEx相比传统方法减少了可再生能源削减，提高了电网利用率和社会福利。

Conclusion: SecuLEx提供了一种计算可行的市场机制，能够有效协调分布式能源资源，在保证网络安全的同时提升系统性能。

Abstract: Distributed energy resources (DERs) are transforming power networks,
challenging traditional operational methods, and requiring new coordination
mechanisms. To address this challenge, this paper introduces SecuLEx (Secure
Limit Exchange), a new market-based paradigm to allocate power injection and
withdrawal limits that guarantee network security during time periods, called
dynamic operating envelopes (DOEs). Under this paradigm, distribution system
operators (DSOs) assign initial DOEs to customers. These limits can be
exchanged afterward through a market, allowing customers to reallocate them
according to their needs while ensuring network operational constraints. We
formalize SecuLEx and illustrate DOE allocation and market exchanges on a
small-scale low-voltage (LV) network, demonstrating that both procedures are
computationally tractable. In this example, SecuLEx reduces renewable
curtailment and improves grid utilization and social welfare compared to
traditional approaches.

</details>


### [21] [Satellite Navigation and Control using Physics-Informed Artificial Potential Field and Sliding Mode Controller](https://arxiv.org/abs/2510.08184)
*Rakesh Kumar Sahoo,Paridhi Choudhary,Manoranjan Sinha*

Main category: eess.SY

TL;DR: 提出了一种结合人工势场和滑模控制器的集成方法，用于在存在移动空间碎片的情况下规划安全轨迹，解决了传统人工势场方法的局部极小值问题。


<details>
  <summary>Details</summary>
Motivation: 空间探索任务的增加导致空间碎片积累，对运行卫星构成碰撞风险。需要开发能够在存在移动障碍物的情况下规划安全轨迹的方法，确保空间操作的可持续性。

Method: 采用基于哈密顿力学而非传统牛顿力学的人工势场方法，结合非奇异固定时间滑模控制器来跟踪期望轨迹。相对6-DOF运动学在几何力学框架下建模，使用指数坐标表示相对构型。

Result: 仿真结果验证了所提方法的有效性，固定时间滑模控制器确保了系统状态在固定时间内收敛，且闭环系统全局稳定无奇异性。

Conclusion: 提出的物理信息人工势场方法有效解决了传统人工势场的局部极小值问题，结合固定时间滑模控制器实现了安全高效的轨迹规划与跟踪。

Abstract: Increase in the number of space exploration missions has led to the
accumulation of space debris, posing risk of collision with the operational
satellites. Addressing this challenge is crucial for the sustainability of
space operations. To plan a safe trajectory in the presence of moving space
debris, an integrated approach of artificial potential field and sliding mode
controller is proposed and implemented in this paper. The relative 6-DOF
kinematics and dynamics of the spacecraft is modelled in the framework of
geometric mechanics with the relative configuration expressed through
exponential coordinates. Various collision avoidance guidance algorithms have
been proposed in the literature but the Artificial Potential Field guidance
algorithm is computationally efficient and enables real-time path adjustments
to avoid collision with obstacles. However, it is prone to issues such as local
minima. In literature, local minima issue is typically avoided by either
redefining the potential function such as adding vorticity or by employing
search techniques which are computationally expensive. To address these
challenges, a physics-informed APF is proposed in this paper where Hamiltonian
mechanics is used instead of the traditional Newtonian mechanics-based
approach. In this approach, instead of relying on attractive and repulsive
forces for path planning, the Hamiltonian approach uses the potential field to
define a path of minimum potential. Additionally, to track the desired
trajectory planned by the guidance algorithm within a fixed-time frame, a
non-singular fixed-time sliding mode controller (FTSMC) is used. The proposed
fixed-time sliding surface not only ensures fixed-time convergence of system
states but also guarantees the global stability of the closed-loop system
without singularity. The simulation results presented support the claims made.

</details>


### [22] [A Control Allocation Algorithm for Hypersonic Glide Vehicles with Input Limitations](https://arxiv.org/abs/2510.08275)
*Johannes Autenrieb,Patrick Gruhn*

Main category: eess.SY

TL;DR: 提出一种迭代控制分配方法，用于解决高超声速滑翔飞行器在强非线性、严格物理约束下的实时控制问题，通过嵌入阻力敏感软约束提高能量效率并降低红外特征。


<details>
  <summary>Details</summary>
Motivation: 高超声速滑翔飞行器在飞行过程中面临强非线性执行机构、状态相关执行器限制、不对称控制边界以及随机动条件变化的热载荷等挑战，需要实时控制分配方法来应对这些约束。

Method: 采用迭代控制分配算法，搜索满足期望力矩指令的控制输入，同时遵守输入幅度和速率的约束，并嵌入阻力敏感软约束来优化能量效率和热管理。

Result: 使用DLR的通用高超声速滑翔飞行器2(GHGV-2)仿真模型验证，结果表明该方法在现实约束飞行条件下能有效保持控制权限。

Conclusion: 该方法成功解决了高超声速滑翔飞行器的实时控制分配问题，在满足各种物理约束的同时提高了能量效率并降低了红外特征，特别适用于需要低可观测性的远程军事作战。

Abstract: Hypersonic glide vehicles (HGVs) operate in challenging flight regimes
characterized by strong nonlinearities in actuation and stringent physical
constraints. These include state-dependent actuator limitations, asymmetric
control bounds, and thermal loads that vary with maneuvering conditions. This
paper introduces an iterative control allocation method to address these
challenges in real time. The proposed algorithm searches for control inputs
that achieve the desired moment commands while respecting constraints on input
magnitude and rate. For slender HGV configurations, thermal loads and drag
generation are strongly correlated-lower drag typically results in reduced
surface heating. By embedding drag-sensitive soft constraints, the method
improves energy efficiency and implicitly reduces surface temperatures,
lowering the vehicle's infrared signature. These features are particularly
advantageous for long-range military operations that require low observability.
The approach is demonstrated using the DLR's Generic Hypersonic Glide Vehicle 2
(GHGV-2) simulation model. The results confirm the method's effectiveness in
maintaining control authority under realistic, constrained flight conditions.

</details>


### [23] [CPU- and GPU-Based Parallelization of the Robust Reference Governor](https://arxiv.org/abs/2510.08288)
*Hamid R. Ossareh,William Shayne,Samuel Chevalier*

Main category: eess.SY

TL;DR: 本文开发了一种基于场景的鲁棒参考调节器(RG)公式，用于非线性系统，并研究了其在多核CPU和CUDA GPU上的并行实现，实现了三个数量级的加速。


<details>
  <summary>Details</summary>
Motivation: 传统鲁棒RG方法在非线性系统中的扩展通常计算不可行，需要开发计算效率更高的实现方案。

Method: 采用基于场景的鲁棒RG公式，分析算法计算结构，识别并行化机会，并在现代并行硬件上实现。

Result: 在非线性氢燃料电池模型上的基准测试显示，相比顺序实现获得了三个数量级的加速。

Conclusion: 所提出的并行实现方法显著提高了非线性系统鲁棒RG的计算效率，使其在实际应用中更加可行。

Abstract: Constraint management is a central challenge in modern control systems. A
solution is the Reference Governor (RG), which is an add-on strategy to
pre-stabilized feedback control systems to enforce state and input constraints
by shaping the reference command. While robust formulations of RG exist for
linear systems, their extension to nonlinear systems is often computationally
intractable. This paper develops a scenario-based robust RG formulation for
nonlinear systems and investigates its parallel implementation on multi-core
CPUs and CUDA-enabled GPUs. We analyze the computational structure of the
algorithm, identify parallelization opportunities, and implement the resulting
schemes on modern parallel hardware. Benchmarking on a nonlinear hydrogen fuel
cell model demonstrates order-of-magnitude speedups (by as much as three orders
of magnitude) compared to sequential implementations.

</details>


### [24] [Underground Power Distribution System Restoration Using Inverter Based Resources](https://arxiv.org/abs/2510.08356)
*Wenlong Shi,Hongyi Li,Zhaoyu Wang*

Main category: eess.SY

TL;DR: 本文提出了一种利用逆变器资源的地下配电系统恢复框架，通过开发电缆充电模型、变压器励磁模型和相位交换模型，采用混合整数非线性规划方法最大化恢复负载，同时约束涌流、铁磁谐振和相位不平衡。


<details>
  <summary>Details</summary>
Motivation: 地下配电系统在部署智能设备时面临涌流、铁磁谐振和三相负载不平衡等独特挑战，需要开发有效的恢复策略来提升系统韧性。

Method: 开发了电缆充电模型量化涌流、变压器励磁模型评估铁磁谐振、相位交换模型改善负载平衡，并集成到MINLP框架中，采用基于排列的线性化技术处理非线性问题。

Result: 基于IEEE 123节点测试馈线的案例研究验证了所提策略在提高地下配电系统恢复性能方面的有效性。

Conclusion: 所提出的恢复框架能够有效解决地下配电系统中的涌流、铁磁谐振和负载不平衡问题，显著提升系统恢复性能。

Abstract: Underground power distribution systems (PDSs) are increasingly deployed in
urban areas. The integration of smart devices including smart switchgears,
pad-mounted distribution transformers and inverter-based resources (IBRs)
enhance system resilience, however simultaneously introducing unique
challenges. The challenges include inrush currents caused by trapped charges in
underground cables, ferroresonance in distribution transformers during
energization, and three-phase load imbalance resulting from single-phase
underground laterals. To address these issues, this paper proposes an
underground PDS restoration framework using IBRs. Firstly, an underground cable
energization model is developed to quantify inrush current by analyzing voltage
differences across both switchgear terminals. Secondly, a distribution
transformer energization model is proposed to evaluate ferroresonance using
Q-factor constraints based on underground cable capacitance and damping
resistance. Thirdly, a phase-swapping model is proposed to improve load
balancing by dynamically reassigning lateral-phase connections through smart
switchgears. The proposed models are further integrated into a mixed-integer
nonlinear programming (MINLP) formulation to maximize the total weighted
restored load while constraining inrush currents, ferroresonance, and phase
imbalance. To address the nonlinearity induced by impedance matrix reordering
during phase swapping, a permutation-based linearization technique is proposed.
Finally, case studies on an underground PDS established based on IEEE 123-Node
Test Feeder validate the effectiveness of the proposed strategy in improving
uderground PDS restoration performance.

</details>


### [25] [Learning to Mitigate Post-Outage Load Surges: A Data-Driven Framework for Electrifying and Decarbonizing Grids](https://arxiv.org/abs/2510.08357)
*Wenlong Shi,Dingwei Wang,Liming Liu,Zhaoyu Wang*

Main category: eess.SY

TL;DR: 该研究分析了电动汽车、热泵和分布式能源资源对电力系统恢复期间负荷激增的因果影响，开发了多任务Transformer模型来分解各组件贡献，并提出了缓解策略。


<details>
  <summary>Details</summary>
Motivation: 电气化和脱碳正在改变电力系统的需求和恢复动态，但人们对停电后负荷激增的影响仍知之甚少。

Method: 使用印第安纳波利斯2010-2024年30,046次馈线级停电的异构数据集，结合统计分析和因果森林推断，开发了组件感知的多任务Transformer估计器。

Result: 所有三种资产的渗透率上升都显著增加了激增比率，晚间恢复成为可靠性约束，在政策对齐路径下超过概率为0.057。缓解措施可将超过概率降至0.019。

Conclusion: 过渡时期的负荷激增是由资产驱动的，与电气化和脱碳有因果关系，但可以通过综合运营策略有效管理。

Abstract: Electrification and decarbonization are transforming power system demand and
recovery dynamics, yet their implications for post-outage load surges remain
poorly understood. Here we analyze a metropolitan-scale heterogeneous dataset
for Indianapolis comprising 30,046 feeder-level outages between 2020 and 2024,
linked to smart meters and submetering, to quantify the causal impact of
electric vehicles (EVs), heat pumps (HPs) and distributed energy resources
(DERs) on restoration surges. Statistical analysis and causal forest inference
demonstrate that rising penetrations of all three assets significantly increase
surge ratios, with effects strongly modulated by restoration timing, outage
duration and weather conditions. We develop a component-aware multi-task
Transformer estimator that disaggregates EV, HP and DER contributions, and
apply it to project historical outages under counterfactual 2035 adoption
pathways. In a policy-aligned pathway, evening restorations emerge as the
binding reliability constraint, with exceedance probabilities of 0.057 when
30\% of system load is restored within the first 15 minutes. Mitigation
measures, probabilistic EV restarts, short thermostat offsets and accelerated
DER reconnection, reduce exceedance to 0.019 and eliminate it entirely when
20\% or less of system load is restored. These results demonstrate that
transition-era surges are asset-driven and causally linked to electrification
and decarbonization, but can be effectively managed through integrated
operational strategies.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [26] [Modeling and forecasting of European Carbon Emission Allowance futures by ARIMA-TX-GARCH models with correlation threshold](https://arxiv.org/abs/2510.07568)
*Jaeho Lee,Eunju Hwang*

Main category: stat.AP

TL;DR: 提出ARIMA-TX-GARCH模型，结合布伦特原油期货价格作为外生变量，预测欧洲碳排放配额期货价格


<details>
  <summary>Details</summary>
Motivation: 需要更准确地预测欧洲碳排放配额期货价格，考虑原油价格等外部因素对碳市场的影响

Method: 开发ARIMA-TX-GARCH模型，将布伦特原油期货价格作为外生变量纳入预测框架

Result: 模型能够有效预测欧洲碳排放配额期货价格

Conclusion: ARIMA-TX-GARCH模型结合外生变量能够提升碳市场预测精度

Abstract: We propose an ARIMA-TX-GARCH model and use it to forecast European Carbon
Emission Allowance futures prices, incorporating Brent crude oil futures prices
as an exogenous variable.

</details>


### [27] [Large-scale spatial variable gene atlas for spatial transcriptomics](https://arxiv.org/abs/2510.07653)
*Jiawen Chen,Jinwei Zhang,Dongshen Peng,Yutong Song,Aitong Ruan,Yun Li,Didong Li*

Main category: stat.AP

TL;DR: 对20种最先进的空间可变基因检测方法进行综合基准测试，构建首个跨组织SVG图谱，为空间转录组学社区提供评估框架和参考资源


<details>
  <summary>Details</summary>
Motivation: 随着空间转录组学技术发展，准确识别不同平台、组织类型和疾病背景下的SVG既是重要机遇也是重大计算挑战

Method: 使用STimage-1K4M资源中的662张人类组织切片，评估20种SVG检测方法在生物学和技术标准上的表现

Result: 发现性能差异显著依赖于组织类型、空间分辨率和研究设计，构建了跨组织SVG图谱，揭示了与发育和功能关系相关的组织间相似性

Conclusion: 建立了评估和解释空间基因表达的框架，为ST社区提供了参考资源

Abstract: Spatial variable genes (SVGs) reveal critical information about tissue
architecture, cellular interactions, and disease microenvironments. As spatial
transcriptomics (ST) technologies proliferate, accurately identifying SVGs
across diverse platforms, tissue types, and disease contexts has become both a
major opportunity and a significant computational challenge. Here, we present a
comprehensive benchmarking study of 20 state-of-the-art SVG detection methods
using human slides from STimage-1K4M, a large-scale resource of ST data
comprising 662 slides from more than 18 tissue types. We evaluate each method
across a range of biologically and technically meaningful criteria, including
recovery of pathologist-annotated domain-specific markers, cross-slide
reproducibility, scalability to high-resolution data, and robustness to
technical variation. Our results reveal marked differences in performance
depending on tissue type, spatial resolution, and study design. Beyond
benchmarking, we construct the first cross-tissue atlas of SVGs, enabling
comparative analysis of spatial gene programs across cancer and normal tissues.
We observe similarities between pairs of tissues that reflect developmental and
functional relationships, such as high overlap between thymus and lymph node,
and uncover spatial gene programs associated with metastasis, immune
infiltration, and tissue-of-origin identity in cancer. Together, our work
defines a framework for evaluating and interpreting spatial gene expression and
establishes a reference resource for the ST community.

</details>


### [28] [Evaluating multi-season occupancy models with autocorrelation fitted to heterogeneous datasets](https://arxiv.org/abs/2510.08151)
*André Luís Luza,Didier Alard,Frédéric Barraquand*

Main category: stat.AP

TL;DR: 多季节占用模型在考虑空间和时间自相关时，对异构数据和协变量重叠具有鲁棒性，但在存在严重数据缺口时会出现可识别性问题，导致预测偏差。


<details>
  <summary>Details</summary>
Motivation: 解决生态学中占用模型在高度异构数据集（特别是缺失数据或单次访问数据占主导）中的性能问题，评估模型对调查次数偏态分布、协变量重叠和时空聚类观测的响应。

Method: 使用具有空间和时间随机效应的多季节占用模型，评估模型对调查次数偏态分布（泊松分布）、占用和检测子模型协变量重叠以及观测时空聚类的性能。

Result: 模型对异构数据和协变量重叠具有鲁棒性，但当添加时空缺口时，站点占用率偏向平均占用率且被高估。随机效应未能纠正缺口影响，存在方差和自相关参数的可识别性问题。

Conclusion: 具有自相关的多季节占用模型对异构数据和协变量重叠稳健，但仍存在可识别性问题，严重数据缺口会损害预测准确性，即使在数据丰富区域也是如此。

Abstract: Predicting species distributions using occupancy models accounting for
imperfect detection is now commonplace in ecology. Recently, modelling spatial
and temporal autocorrelation was proposed to alleviate the lack of replication
in occupancy data, which often prevents model identifiability. However, how
such models perform in highly heterogeneous datasets where missing or
single-visit data dominates remains an open question. Motivated by an
heterogeneous fine-scale butterfly occupancy dataset, we evaluate the
performance of a multi-season occupancy model with spatial and temporal random
effects to a skewed (Poisson) distribution of the number of surveys per site,
overlap of covariates between occupancy and detection submodels, and
spatiotemporal clustering of observations. Results showed that the model is
robust to heterogeneous data and covariate overlap. However, when
spatiotemporal gaps were added, site occupancy was biased towards the average
occupancy, itself overestimated. Random effects did not correct the influence
of gaps, due to identifiability issues of variance and autocorrelation
parameters. Occupancy analysis of two butterfly species further confirmed these
results. Overall, multi-season occupancy models with autocorrelation are robust
to heterogeneous data and covariate overlap, but still present identifiability
issues and are challenged by severe data gaps, which compromise predictions
even in data-rich areas.

</details>


### [29] [Two-Stage Trigonometric Regression for Modeling Circadian Rhythms](https://arxiv.org/abs/2510.08309)
*Michael T. Gorczyca,Jenna D. Li,Charissa M. Newkirk,Arjun S. Srivatsa,Hugo F. M. Milan*

Main category: stat.AP

TL;DR: 提出了一种改进的两阶段(RTS)三角回归方法，用于分析昼夜节律数据，解决了传统方法因忽略个体间峰值时间差异而导致的振幅参数估计偏差问题。


<details>
  <summary>Details</summary>
Motivation: 昼夜节律研究中，个体间的振荡峰值时间存在差异，传统三角回归方法忽略这些差异会导致群体水平振幅参数的衰减偏差，从而影响研究结论的准确性。

Method: RTS方法包含两个阶段：第一阶段估计个体水平模型参数，第二阶段将这些个体水平估计的变换进行聚合，得到群体水平参数估计用于推断。

Result: 模拟研究表明，与标准两阶段(STS)方法相比，RTS方法减轻了参数估计偏差，获得了更高的统计功效，并保持了适当的I类错误控制。在皮质醇水平和心率数据应用中，RTS方法获得了更大的群体水平振幅参数估计值和更小的p值。

Conclusion: RTS方法能有效解决昼夜节律数据分析中的衰减偏差问题，特别适用于处理个体间峰值时间差异的情况，为相关生物医学研究提供了更可靠的统计工具。

Abstract: Gene expression levels, hormone secretion, and internal body temperature each
oscillate over an approximately 24-hour cycle, or display circadian rhythms.
Many circadian biology studies have investigated how these rhythms vary across
cohorts, uncovering associations between atypical rhythms and diseases such as
cancer, metabolic syndrome, and sleep disorders. A challenge in analyzing
circadian biology data is that the oscillation peak and trough times for a
phenomenon differ across individuals. If these individual-level differences are
not accounted for in trigonometric regression, which is prevalent in circadian
biology studies, then estimates of the population-level amplitude parameters
can suffer from attenuation bias. This attenuation bias could lead to
inaccurate study conclusions. To address attenuation bias, we propose a refined
two-stage (RTS) method for trigonometric regression given longitudinal data
obtained from each individual participating in a study. In the first stage, the
parameters of individual-level models are estimated. In the second stage,
transformations of these individual-level estimates are aggregated to produce
population-level parameter estimates for inference. Simulation studies show
that our RTS method mitigates bias in parameter estimation, obtains greater
statistical power, and maintains appropriate type I error control when compared
to the standard two-stage (STS) method, which ignores individual-level
differences in peak and trough times. The only exception for parameter
estimation and statistical power occurs when the oscillation amplitudes are
weak relative to random variability in the data and the sample size is small.
Illustrations with cortisol level data and heart rate data show that our RTS
method obtains larger population-level amplitude parameter estimates and
smaller $p$-values for multiple hypothesis tests when compared to the STS
method.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [30] [From Keywords to Clusters: AI-Driven Analysis of YouTube Comments to Reveal Election Issue Salience in 2024](https://arxiv.org/abs/2510.07821)
*Raisa M. Simoes,Timoteo Kelly,Eduardo J. Simoes,Praveen Rao*

Main category: cs.SI

TL;DR: 使用AI技术分析YouTube评论，发现移民和民主是2024年总统选举前最重要的议题，通胀议题重要性被高估


<details>
  <summary>Details</summary>
Motivation: 探索哪种数据科学方法能更好分析选举议题，比较AI驱动的意见挖掘与传统调查方法的效果

Method: 使用自然语言处理和聚类分析，挖掘8000多条来自华尔街日报和纽约时报YouTube频道的选举相关用户评论

Result: 移民和民主是最常被提及的议题，身份政治次之，通胀议题被提及频率显著较低

Conclusion: 在线用户数据的意见挖掘比传统民调更能揭示选举结果

Abstract: This paper aims to explore two competing data science methodologies to
attempt answering the question, "Which issues contributed most to voters'
choice in the 2024 presidential election?" The methodologies involve novel
empirical evidence driven by artificial intelligence (AI) techniques. By using
two distinct methods based on natural language processing and clustering
analysis to mine over eight thousand user comments on election-related YouTube
videos from one right leaning journal, Wall Street Journal, and one left
leaning journal, New York Times, during pre-election week, we quantify the
frequency of selected issue areas among user comments to infer which issues
were most salient to potential voters in the seven days preceding the November
5th election. Empirically, we primarily demonstrate that immigration and
democracy were the most frequently and consistently invoked issues in user
comments on the analyzed YouTube videos, followed by the issue of identity
politics, while inflation was significantly less frequently referenced. These
results corroborate certain findings of post-election surveys but also refute
the supposed importance of inflation as an election issue. This indicates that
variations on opinion mining, with their analysis of raw user data online, can
be more revealing than polling and surveys for analyzing election outcomes.

</details>


### [31] [Do We Really Need SFT? Prompt-as-Policy over Knowledge Graphs for Cold-start Next POI Recommendation](https://arxiv.org/abs/2510.08012)
*Jinze Wang,Lu Zhang,Yiyang Cui,Zhishu Shen,Xingjun Ma,Jiong Jin,Tiehua Zhang*

Main category: cs.SI

TL;DR: 提出Prompt-as-Policy框架，通过强化学习动态构建提示来解决冷启动POI推荐问题，无需微调LLM即可显著提升不活跃用户的推荐效果


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的POI推荐方法在冷启动场景下存在局限：监督微调需要昂贵标注且无法泛化到不活跃用户，上下文学习使用静态提示无法适应多样化用户上下文

Method: 将提示构建视为可学习策略，通过上下文多臂老虎机优化动态确定：包含哪些关系证据、每个候选的证据数量、证据的组织和排序。构建知识图谱发现候选和挖掘关系路径，转换为证据卡片，冻结的LLM作为推理引擎基于策略优化的提示生成推荐

Result: 在三个真实数据集上的实验表明，Prompt-as-Policy持续优于最先进基线，在不活跃用户的Acc@1上实现平均7.7%的相对提升，同时在活跃用户上保持竞争力，且无需模型微调

Conclusion: Prompt-as-Policy框架有效解决了冷启动POI推荐问题，通过强化学习动态提示构建显著提升了不活跃用户的推荐性能，同时避免了昂贵的模型微调需求

Abstract: Next point-of-interest (POI) recommendation is crucial for smart urban
services such as tourism, dining, and transportation, yet most approaches
struggle under cold-start conditions where user-POI interactions are sparse.
Recent efforts leveraging large language models (LLMs) address this challenge
through either supervised fine-tuning (SFT) or in-context learning (ICL).
However, SFT demands costly annotations and fails to generalize to inactive
users, while static prompts in ICL cannot adapt to diverse user contexts. To
overcome these limitations, we propose Prompt-as-Policy over knowledge graphs,
a reinforcement-guided prompting framework that learns to construct prompts
dynamically through contextual bandit optimization. Our method treats prompt
construction as a learnable policy that adaptively determines (i) which
relational evidences to include, (ii) the number of evidence per candidate, and
(iii) their organization and ordering within prompts. More specifically, we
construct a knowledge graph (KG) to discover candidates and mine relational
paths, which are transformed into evidence cards that summarize rationales for
each candidate POI. The frozen LLM then acts as a reasoning engine, generating
recommendations from the KG-discovered candidate set based on the
policy-optimized prompts. Experiments on three real-world datasets demonstrate
that Prompt-as-Policy consistently outperforms state-of-the-art baselines,
achieving average 7.7\% relative improvements in Acc@1 for inactive users,
while maintaining competitive performance on active users, without requiring
model fine-tuning.

</details>


### [32] [Geometric opinion exchange polarizes in every dimension](https://arxiv.org/abs/2510.08190)
*Abdou Majeed Alidou,Júlia Baligács,Jan Hązła*

Main category: cs.SI

TL;DR: 本文研究了一个多主题意见交换模型，其中代理人的意见被表示为单位球面上的向量。该模型基于相关性的更新规则会导致意见极化到两个对立群体，这与许多达成共识的模型不同。作者解决了d≥3情况下极化性质证明的开放性问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明，在多主题意见交换模型中，基于相关性（正相关拉近、负相关推远）的更新规则会导致意见极化，这与传统达成共识的模型形成对比。虽然d=2的情况已被证明，但d≥3的一般情况仍是一个开放性问题。

Method: 作者通过更深入地理解模型动力学，并运用随机过程理论工具，分析了多维度意见向量在单位球面上的演化过程。

Result: 研究证明，对于任意维度d≥3，该意见交换模型确实会导致意见极化到两个对立群体，即意见向量最终会收敛到两个相反的方向。

Conclusion: 这项工作完整地解决了多主题意见交换模型中极化性质的一般情况证明，确认了该模型在任意维度下都会产生意见极化现象，为理解社会网络中的意见分化提供了理论基础。

Abstract: A recent line of work studies models of opinion exchange where agent opinions
about $d$ topics are tracked simultaneously. The opinions are represented as
vectors on the unit $(d-1)$-sphere, and the update rule is based on the overall
correlation between the relevant vectors. The update rule reflects the
assumption of biased assimilation, i.e., a pair of opinions is brought closer
together if their correlation is positive and further apart if the correlation
is negative.
  This model seems to induce the polarization of opinions into two antipodal
groups. This is in contrast to many other known models which tend to achieve
consensus. The polarization property has been recently proved for $d=2$, but
the general case of $d \ge 3$ remained open. In this work, we settle the
general case, using a more detailed understanding of the model dynamics and
tools from the theory of random processes.

</details>


### [33] [Forecasting the Buzz: Enriching Hashtag Popularity Prediction with LLM Reasoning](https://arxiv.org/abs/2510.08481)
*Yifei Xu,Jiaying Wu,Herun Wan,Yang Li,Zhen Hou,Min-Yen Kan*

Main category: cs.SI

TL;DR: BuzzProphet是一个结合LLM推理能力的标签流行度预测框架，通过让LLM分析标签的病毒性、受众范围和时机优势来增强特征，在HashView基准上显著提升了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么忽略上下文（传统回归器），要么不擅长数值估计（LLM），需要结合两者的优势来准确预测标签流行度。

Method: 1) 让LLM生成标签的流行度相关推理；2) 用这些推理增强输入特征；3) 基于增强特征进行回归预测。

Result: 在7,532个标签的HashView基准上，BuzzProphet将RMSE降低达2.8%，相关性提升30%，同时生成可读的推理过程。

Conclusion: 将LLM用作上下文推理器而非数值预测器，能够为表格模型注入领域知识，提供可解释且可部署的社交媒体趋势预测方案。

Abstract: Hashtag trends ignite campaigns, shift public opinion, and steer millions of
dollars in advertising spend, yet forecasting which tag goes viral is elusive.
Classical regressors digest surface features but ignore context, while large
language models (LLMs) excel at contextual reasoning but misestimate numbers.
We present BuzzProphet, a reasoning-augmented hashtag popularity prediction
framework that (1) instructs an LLM to articulate a hashtag's topical virality,
audience reach, and timing advantage; (2) utilizes these popularity-oriented
rationales to enrich the input features; and (3) regresses on these inputs. To
facilitate evaluation, we release HashView, a 7,532-hashtag benchmark curated
from social media. Across diverse regressor-LLM combinations, BuzzProphet
reduces RMSE by up to 2.8% and boosts correlation by 30% over baselines, while
producing human-readable rationales. Results demonstrate that using LLMs as
context reasoners rather than numeric predictors injects domain insight into
tabular models, yielding an interpretable and deployable solution for social
media trend forecasting.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [34] [SEPhIA: <1 laser/neuron Spiking Electro-Photonic Integrated Multi-Tiled Architecture for Scalable Optical Neuromorphic Computing](https://arxiv.org/abs/2510.07427)
*Matěj Hejda,Aishwarya Natarajan,Chaerin Hong,Mehmet Berkay On,Sébastien d'Herbais de Thun,Raymond G. Beausoleil,Thomas Van Vaerenbergh*

Main category: cs.ET

TL;DR: SEPhIA是一种光电混合的多瓦片脉冲神经网络架构，通过微环谐振器调制器和多波长光源实现亚每神经元一个激光器的效率，在四类脉冲编码数据集上达到超过90%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 现有光学脉冲神经网络研究主要关注脉冲设备、可激发激光器网络或大型架构的数值模拟，但往往忽略了光学功率限制、串扰和占地面积等关键约束条件。

Method: 采用微环谐振器调制器和多波长光源，通过时域协同模拟可激发CMOS-MRR耦合电路，并开发物理感知的可训练光电SNN模型，两种方法都使用实验获得的器件参数。

Result: 多层光电SNN在四类脉冲编码数据集上实现超过90%的分类准确率，与软件模型性能相当。设计空间研究量化了光子器件参数在受限信噪比条件下对SNN性能的影响。

Conclusion: SEPhIA为神经形态光子计算提供了可扩展、表达能力强且物理基础扎实的解决方案，能够处理脉冲编码任务。

Abstract: Research into optical spiking neural networks (SNNs) has primarily focused on
spiking devices, networks of excitable lasers or numerical modelling of large
architectures, often overlooking key constraints such as limited optical power,
crosstalk and footprint. We introduce SEPhIA, a photonic-electronic,
multi-tiled SNN architecture emphasizing implementation feasibility and
realistic scaling. SEPhIA leverages microring resonator modulators (MRMs) and
multi-wavelength sources to achieve effective sub-one-laser-per-spiking neuron
efficiency. We validate SEPhIA at both device and architecture levels by
time-domain co-simulating excitable CMOS-MRR coupled circuits and by devising a
physics-aware, trainable optoelectronic SNN model, with both approaches
utilizing experimentally derived device parameters. The multi-layer
optoelectronic SNN achieves classification accuracies over 90% on a four-class
spike-encoded dataset, closely comparable to software models. A design space
study further quantifies how photonic device parameters impact SNN performance
under constrained signal-to-noise conditions. SEPhIA offers a scalable,
expressive, physically grounded solution for neuromorphic photonic computing,
capable of addressing spike-encoded tasks.

</details>


### [35] [A Distributed Emulation Environment for In-Memory Computing Systems](https://arxiv.org/abs/2510.08257)
*Eleni Bougioukou,Anastasios Petropoulos,Nikolaos Toulgaridis,Theodoros Chatzimichail,Theodore Antonakopoulos*

Main category: cs.ET

TL;DR: 提出了一种用于内存计算集成电路快速原型设计的分布式可扩展仿真系统架构


<details>
  <summary>Details</summary>
Motivation: 内存计算技术在AI设备中广泛应用，但芯片开发和系统集成需要大量时间，需要实时仿真环境来在芯片可用前分析系统、测试微码和部署应用

Method: 开发了分布式可扩展的仿真系统架构和软件开发工具，用于内存计算集成电路的快速原型设计

Result: 实验结果表明所提出的仿真器具有实用性

Conclusion: 该仿真系统为内存计算集成电路的快速原型设计提供了有效的解决方案

Abstract: In-memory computing technology is used extensively in artificial intelligence
devices due to lower power consumption and fast calculation of matrix-based
functions. The development of such a device and its integration in a system
takes a significant amount of time and requires the use of a real-time
emulation environment, where various system aspects are analyzed, microcode is
tested, and applications are deployed, even before the real chip is available.
In this work, we present the architecture, the software development tools, and
experimental results of a distributed and expandable emulation system for rapid
prototyping of integrated circuits based on in-memory computing technologies.
Presented experimental results demonstrate the usefulness of the proposed
emulator.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [36] [Stochastic Volatility-in-mean VARs with Time-Varying Skewness](https://arxiv.org/abs/2510.08415)
*Leonardo N. Ferreira,Haroon Mumtaz,Ana Skoblar*

Main category: econ.EM

TL;DR: 提出了一种具有随机波动率均值效应和时变偏度的贝叶斯向量自回归模型，该模型允许波动率和偏度直接影响宏观经济变量，在实证应用中显示出优于现有模型的预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统模型未能充分考虑波动率和偏度对宏观经济变量的直接影响，需要开发能够同时捕捉这两种风险因素影响的模型来更好地理解宏观金融风险。

Method: 使用贝叶斯向量自回归框架，引入随机波动率均值效应和时变偏度，开发了吉布斯采样算法进行后验推断，应用于美国和英国的季度数据。

Result: 实证结果显示偏度冲击对产出、通胀和利差具有显著经济影响，通常超过波动率冲击的影响；在伪实时预测中优于现有模型；能提供更精确的尾部风险度量。

Conclusion: 时变偏度的纳入对于捕捉宏观金融风险和改善预测性能至关重要，标准随机波动率模型往往会高估不确定性。

Abstract: This paper introduces a Bayesian vector autoregression (BVAR) with stochastic
volatility-in-mean and time-varying skewness. Unlike previous approaches, the
proposed model allows both volatility and skewness to directly affect
macroeconomic variables. We provide a Gibbs sampling algorithm for posterior
inference and apply the model to quarterly data for the US and the UK.
Empirical results show that skewness shocks have economically significant
effects on output, inflation and spreads, often exceeding the impact of
volatility shocks. In a pseudo-real-time forecasting exercise, the proposed
model outperforms existing alternatives in many cases. Moreover, the model
produces sharper measures of tail risk, revealing that standard stochastic
volatility models tend to overstate uncertainty. These findings highlight the
importance of incorporating time-varying skewness for capturing macro-financial
risks and improving forecast performance.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [37] [FLEET: Formal Language-Grounded Scheduling for Heterogeneous Robot Teams](https://arxiv.org/abs/2510.07417)
*Corban Rivera,Grayson Byrd,Meghan Booker,Bethany Kemp,Allison Gaines,Emma Holmes,James Uplinger,Celso M de Melo,David Handelman*

Main category: cs.RO

TL;DR: FLEET是一个混合分散式框架，将自然语言指令转化为优化的多机器人调度方案，结合LLM前端生成任务图和能力匹配矩阵，以及形式化后端进行调度优化。


<details>
  <summary>Details</summary>
Motivation: 解决异构机器人团队从自由形式自然语言指令进行协调的挑战，语言规划器难以处理长时程协调和幻觉问题，而纯形式化方法需要封闭世界模型。

Method: 使用LLM前端生成任务图（包含持续时间和优先级）和机器人-任务适应度矩阵，形式化后端解决最小化完成时间问题，机器人执行具有自主闭环控制的子任务。

Result: 在多个自由形式语言引导的自主协调基准测试中，FLEET在异构任务的两智能体团队上比最先进的生成规划器提高了成功率。消融实验显示MILP主要改善时间结构，LLM衍生的适应度对能力耦合任务至关重要。

Conclusion: FLEET框架成功地将自然语言指令转化为优化的多机器人调度，在仿真和真实硬件试验中均表现出色，证明了混合方法的有效性。

Abstract: Coordinating heterogeneous robot teams from free-form natural-language
instructions is hard. Language-only planners struggle with long-horizon
coordination and hallucination, while purely formal methods require
closed-world models. We present FLEET, a hybrid decentralized framework that
turns language into optimized multi-robot schedules. An LLM front-end produces
(i) a task graph with durations and precedence and (ii) a capability-aware
robot--task fitness matrix; a formal back-end solves a makespan-minimization
problem while the underlying robots execute their free-form subtasks with
agentic closed-loop control. Across multiple free-form language-guided autonomy
coordination benchmarks, FLEET improves success over state of the art
generative planners on two-agent teams across heterogeneous tasks. Ablations
show that mixed integer linear programming (MILP) primarily improves temporal
structure, while LLM-derived fitness is decisive for capability-coupled tasks;
together they deliver the highest overall performance. We demonstrate the
translation to real world challenges with hardware trials using a pair of
quadruped robots with disjoint capabilities.

</details>


### [38] [VeMo: A Lightweight Data-Driven Approach to Model Vehicle Dynamics](https://arxiv.org/abs/2510.07447)
*Girolamo Oddo,Roberto Nuca,Matteo Parsani*

Main category: cs.RO

TL;DR: 提出基于门控循环单元的轻量级编码器-解码器模型，用于在信息稀缺条件下预测高性能车辆的未来状态，最大平均相对误差低于2.6%，具有良好的噪声鲁棒性和物理一致性。


<details>
  <summary>Details</summary>
Motivation: 高性能车辆的动态建模需要详细的系统结构信息，但这些信息通常对非设计者不可得，这在自动驾驶应用中尤为常见，因此需要在信息稀缺条件下开发车辆模型。

Method: 使用基于门控循环单元的轻量级编码器-解码器模型，通过车辆历史状态测量值和驾驶员控制动作来关联未来状态。

Result: 在极端动态条件下，模型最大平均相对误差低于2.6%，对感兴趣频段的噪声输入数据具有良好的鲁棒性，输出信号（纵向和横向加速度、横摆角速度、纵向速度）表现出物理一致性。

Conclusion: 该数据驱动模型无需物理约束，在信息稀缺条件下仍能准确预测车辆动态行为，为自动驾驶应用提供了有效的解决方案。

Abstract: Developing a dynamic model for a high-performance vehicle is a complex
problem that requires extensive structural information about the system under
analysis. This information is often unavailable to those who did not design the
vehicle and represents a typical issue in autonomous driving applications,
which are frequently developed on top of existing vehicles; therefore, vehicle
models are developed under conditions of information scarcity. This paper
proposes a lightweight encoder-decoder model based on Gate Recurrent Unit
layers to correlate the vehicle's future state with its past states, measured
onboard, and control actions the driver performs. The results demonstrate that
the model achieves a maximum mean relative error below 2.6% in extreme dynamic
conditions. It also shows good robustness when subject to noisy input data
across the interested frequency components. Furthermore, being entirely
data-driven and free from physical constraints, the model exhibits physical
consistency in the output signals, such as longitudinal and lateral
accelerations, yaw rate, and the vehicle's longitudinal velocity.

</details>


### [39] [HJCD-IK: GPU-Accelerated Inverse Kinematics through Batched Hybrid Jacobian Coordinate Descent](https://arxiv.org/abs/2510.07514)
*Cael Yasutake,Zachary Kingston,Brian Plancher*

Main category: cs.RO

TL;DR: HJCD-IK是一种GPU加速的采样式逆运动学求解器，结合了方向感知的贪婪坐标下降初始化方案和基于雅可比矩阵的优化程序，在精度和速度方面都优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统解析法IK求解器受限于低自由度和特定拓扑结构，而数值优化方法计算成本高且容易陷入局部最优。需要开发更高效通用的IK求解方法。

Method: 提出HJCD-IK方法，使用GPU加速，结合采样和优化策略：采用方向感知的贪婪坐标下降进行初始化，然后使用雅可比矩阵进行优化精炼。

Result: 相比现有技术，该方法在收敛速度和整体精度上都有提升，在精度-延迟帕累托边界上始终找到最优解，通常实现数量级增益，并产生高质量样本分布。

Conclusion: HJCD-IK在逆运动学求解方面表现出色，提供了开源代码供社区使用。

Abstract: Inverse Kinematics (IK) is a core problem in robotics, in which joint
configurations are found to achieve a desired end-effector pose. Although
analytical solvers are fast and efficient, they are limited to systems with low
degrees-of-freedom and specific topological structures. Numerical
optimization-based approaches are more general, but suffer from high
computational costs and frequent convergence to spurious local minima. Recent
efforts have explored the use of GPUs to combine sampling and optimization to
enhance both the accuracy and speed of IK solvers. We build on this recent
literature and introduce HJCD-IK, a GPU-accelerated, sampling-based hybrid
solver that combines an orientation-aware greedy coordinate descent
initialization scheme with a Jacobian-based polishing routine. This design
enables our solver to improve both convergence speed and overall accuracy as
compared to the state-of-the-art, consistently finding solutions along the
accuracy-latency Pareto frontier and often achieving order-of-magnitude gains.
In addition, our method produces a broad distribution of high-quality samples,
yielding the lowest maximum mean discrepancy. We release our code open-source
for the benefit of the community.

</details>


### [40] [AVO: Amortized Value Optimization for Contact Mode Switching in Multi-Finger Manipulation](https://arxiv.org/abs/2510.07548)
*Adam Hung,Fan Yang,Abhinav Kumar,Sergio Aguilera Marinovic,Soshi Iba,Rana Soltani Zarrin,Dmitry Berenson*

Main category: cs.RO

TL;DR: 提出了摊销价值优化(AVO)方法，通过引入学习价值函数来预测未来任务性能，指导轨迹优化器朝向有利于后续子任务的状态，从而解决灵巧操作中接触模式切换的优化问题。


<details>
  <summary>Details</summary>
Motivation: 灵巧操作任务需要在不同接触模式间切换，传统方法将任务分解为独立子任务分别优化，这限制了性能且计算成本高昂。

Method: 引入学习价值函数预测总未来任务性能，将其纳入轨迹优化的成本函数中，通过价值函数梯度指导优化器朝向最小化未来子任务成本的状态。

Result: 在螺丝刀抓取和转动任务中验证，即使计算预算减少50%，相比无价值函数的轨迹优化仍能提升性能。

Conclusion: AVO有效桥接独立优化的子任务，加速优化过程并减少在线计算需求。

Abstract: Dexterous manipulation tasks often require switching between different
contact modes, such as rolling, sliding, sticking, or non-contact contact
modes. When formulating dexterous manipulation tasks as a trajectory
optimization problem, a common approach is to decompose these tasks into
sub-tasks for each contact mode, which are each solved independently.
Optimizing each sub-task independently can limit performance, as optimizing
contact points, contact forces, or other variables without information about
future sub-tasks can place the system in a state from which it is challenging
to make progress on subsequent sub-tasks. Further, optimizing these sub-tasks
is very computationally expensive. To address these challenges, we propose
Amortized Value Optimization (AVO), which introduces a learned value function
that predicts the total future task performance. By incorporating this value
function into the cost of the trajectory optimization at each planning step,
the value function gradients guide the optimizer toward states that minimize
the cost in future sub-tasks. This effectively bridges separately optimized
sub-tasks, and accelerates the optimization by reducing the amount of online
computation needed. We validate AVO on a screwdriver grasping and turning task
in both simulation and real world experiments, and show improved performance
even with 50% less computational budget compared to trajectory optimization
without the value function.

</details>


### [41] [Inspection Planning Primitives with Implicit Models](https://arxiv.org/abs/2510.07611)
*Jingyang You,Hanna Kurniawati,Lashika Medagoda*

Main category: cs.RO

TL;DR: 提出了IPIM原语计算，使基于采样的检测规划器能够完全使用神经SDF表示，在保持轨迹质量的同时大幅减少内存使用（最高达70倍）。


<details>
  <summary>Details</summary>
Motivation: 基础设施老化复杂化使得高效检测规划至关重要。现有基于采样的检测规划器虽然快速但内存消耗大，特别是对于大型复杂结构。隐式模型（如神经SDFs）能高效表示复杂结构，但现有规划器主要设计用于显式模型。

Method: 提出IPIM（Inspection Planning Primitives with Implicit Models）原语计算集，使基于采样的检测规划器能够完全使用神经SDFs表示进行规划，无需在隐式和显式模型间频繁转换。

Result: 在三个场景（包括包含9200万三角网格面的真实复杂结构）的评估表明，即使基本的基于采样规划器使用IPIM也能生成与最先进规划器质量相当的检测轨迹，同时内存使用减少高达70倍。

Conclusion: IPIM使基于采样的检测规划器能够有效利用神经SDFs表示，在保持轨迹质量的同时显著降低内存需求，为大型复杂结构的检测规划提供了高效解决方案。

Abstract: The aging and increasing complexity of infrastructures make efficient
inspection planning more critical in ensuring safety. Thanks to sampling-based
motion planning, many inspection planners are fast. However, they often require
huge memory. This is particularly true when the structure under inspection is
large and complex, consisting of many struts and pillars of various geometry
and sizes. Such structures can be represented efficiently using implicit
models, such as neural Signed Distance Functions (SDFs). However, most
primitive computations used in sampling-based inspection planner have been
designed to work efficiently with explicit environment models, which in turn
requires the planner to use explicit environment models or performs frequent
transformations between implicit and explicit environment models during
planning. This paper proposes a set of primitive computations, called
Inspection Planning Primitives with Implicit Models (IPIM), that enable
sampling-based inspection planners to entirely use neural SDFs representation
during planning. Evaluation on three scenarios, including inspection of a
complex real-world structure with over 92M triangular mesh faces, indicates
that even a rudimentary sampling-based planner with IPIM can generate
inspection trajectories of similar quality to those generated by the
state-of-the-art planner, while using up to 70x less memory than the
state-of-the-art inspection planner.

</details>


### [42] [GATO: GPU-Accelerated and Batched Trajectory Optimization for Scalable Edge Model Predictive Control](https://arxiv.org/abs/2510.07625)
*Alexander Du,Emre Adabag,Gabriel Bravo,Brian Plancher*

Main category: cs.RO

TL;DR: GATO是一个开源GPU加速的批量轨迹优化求解器，专为中等批量规模（几十到几百个求解）的实时MPC应用设计，相比CPU基线实现18-21倍加速，相比GPU基线实现1.4-16倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有GPU加速方法要么并行化单个求解以满足实时要求，要么以慢于实时的速率扩展到非常大的批量，要么通过限制模型通用性来实现速度。这为需要实时批量求解的现代MPC应用留下了性能空白。

Method: GATO采用算法、软件和计算硬件的协同设计，利用块级、warp级和线程级并行性在求解内部和跨求解之间实现超高性能。

Result: 模拟基准测试显示，随着批量大小增加，相比CPU基线实现18-21倍加速，相比GPU基线实现1.4-16倍加速。案例研究显示改进的扰动抑制和收敛行为，并在工业机械臂上进行了硬件验证。

Conclusion: GATO填补了中等批量规模实时MPC求解器的性能空白，通过协同设计方法实现了显著的性能提升，并开源以支持可重复性和采用。

Abstract: While Model Predictive Control (MPC) delivers strong performance across
robotics applications, solving the underlying (batches of) nonlinear trajectory
optimization (TO) problems online remains computationally demanding. Existing
GPU-accelerated approaches typically (i) parallelize a single solve to meet
real-time deadlines, (ii) scale to very large batches at slower-than-real-time
rates, or (iii) achieve speed by restricting model generality (e.g., point-mass
dynamics or a single linearization). This leaves a large gap in solver
performance for many state-of-the-art MPC applications that require real-time
batches of tens to low-hundreds of solves. As such, we present GATO, an open
source, GPU-accelerated, batched TO solver co-designed across algorithm,
software, and computational hardware to deliver real-time throughput for these
moderate batch size regimes. Our approach leverages a combination of block-,
warp-, and thread-level parallelism within and across solves for ultra-high
performance. We demonstrate the effectiveness of our approach through a
combination of: simulated benchmarks showing speedups of 18-21x over CPU
baselines and 1.4-16x over GPU baselines as batch size increases; case studies
highlighting improved disturbance rejection and convergence behavior; and
finally a validation on hardware using an industrial manipulator. We open
source GATO to support reproducibility and adoption.

</details>


### [43] [Differentiable Particle Optimization for Fast Sequential Manipulation](https://arxiv.org/abs/2510.07674)
*Lucas Chen,Shrutheesh Raman Iyer,Zachary Kingston*

Main category: cs.RO

TL;DR: SPaSM是一个完全GPU并行化的框架，通过将约束评估、采样和梯度优化编译为CUDA内核，实现端到端的轨迹优化，无需CPU协调，在复杂序列操作任务中实现毫秒级求解。


<details>
  <summary>Details</summary>
Motivation: 序列机器人操作任务需要在可能的高维配置空间中找到满足多个对象交互几何约束的无碰撞轨迹。现有方法由于CPU-GPU数据传输开销和复杂逻辑导致硬件利用率不足，无法实现实时大规模求解。

Method: 采用两阶段粒子优化策略：首先通过大规模并行采样解决放置约束，然后将解决方案提升到关节空间进行完整轨迹优化。该方法将对象放置和机器人轨迹联合优化，以处理运动可行性约束放置选项的场景。

Result: 在具有挑战性的基准测试中，求解时间达到毫秒级，成功率为100%，相比现有方法实现了4000倍的加速。

Conclusion: SPaSM框架通过完全GPU并行化和优化的CUDA内核，显著提高了序列操作任务的求解效率，为实时大规模机器人操作提供了可行的解决方案。

Abstract: Sequential robot manipulation tasks require finding collision-free
trajectories that satisfy geometric constraints across multiple object
interactions in potentially high-dimensional configuration spaces. Solving
these problems in real-time and at large scales has remained out of reach due
to computational requirements. Recently, GPU-based acceleration has shown
promising results, but prior methods achieve limited performance due to CPU-GPU
data transfer overhead and complex logic that prevents full hardware
utilization. To this end, we present SPaSM (Sampling Particle optimization for
Sequential Manipulation), a fully GPU-parallelized framework that compiles
constraint evaluation, sampling, and gradient-based optimization into optimized
CUDA kernels for end-to-end trajectory optimization without CPU coordination.
The method consists of a two-stage particle optimization strategy: first
solving placement constraints through massively parallel sampling, then lifting
solutions to full trajectory optimization in joint space. Unlike hierarchical
approaches, SPaSM jointly optimizes object placements and robot trajectories to
handle scenarios where motion feasibility constrains placement options.
Experimental evaluation on challenging benchmarks demonstrates solution times
in the realm of $\textbf{milliseconds}$ with a 100% success rate; a
$4000\times$ speedup compared to existing approaches.

</details>


### [44] [EB-MBD: Emerging-Barrier Model-Based Diffusion for Safe Trajectory Optimization in Highly Constrained Environments](https://arxiv.org/abs/2510.07700)
*Raghav Mishra,Ian R. Manchester*

Main category: cs.RO

TL;DR: 提出EB-MBD方法，通过渐进引入障碍函数约束解决基于模型扩散中的约束问题，避免性能下降和计算昂贵的投影操作


<details>
  <summary>Details</summary>
Motivation: 基于模型扩散中的约束会导致灾难性性能下降，即使在简单2D系统中也因蒙特卡洛近似样本效率低下而出现问题

Method: 使用渐进引入的障碍函数约束，分析每轮迭代的采样活跃度来指导障碍参数调度选择

Result: 在2D避碰和3D水下机械臂系统中，相比基于模型扩散获得更低成本解，比基于投影方法减少数量级计算时间

Conclusion: EB-MBD方法能有效处理约束问题，显著提升解质量，无需计算昂贵的投影操作

Abstract: We propose enforcing constraints on Model-Based Diffusion by introducing
emerging barrier functions inspired by interior point methods. We show that
constraints on Model-Based Diffusion can lead to catastrophic performance
degradation, even on simple 2D systems due to sample inefficiency in the Monte
Carlo approximation of the score function. We introduce Emerging-Barrier
Model-Based Diffusion (EB-MBD) which uses progressively introduced barrier
constraints to avoid these problems, significantly improving solution quality,
without the need for computationally expensive operations such as projections.
We analyze the sampling liveliness of samples each iteration to inform barrier
parameter scheduling choice. We demonstrate results for 2D collision avoidance
and a 3D underwater manipulator system and show that our method achieves lower
cost solutions than Model-Based Diffusion, and requires orders of magnitude
less computation time than projection based methods.

</details>


### [45] [Probabilistically-Safe Bipedal Navigation over Uncertain Terrain via Conformal Prediction and Contraction Analysis](https://arxiv.org/abs/2510.07725)
*Kasidit Muenprasitivej,Ye Zhao,Glen Chou*

Main category: cs.RO

TL;DR: 开发概率安全规划和控制策略，使双足机器人能在不确定地形上安全导航，通过高斯过程和保形预测建模地形不确定性，结合收缩理论和模型预测控制确保动态可行性和质心稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决双足机器人在粗糙地形上行走的挑战，需要处理地形高程不确定性问题，确保机器人的动态可行性和质心稳定性，同时提供概率安全保证。

Method: 使用高斯过程回归估计高程地图，结合保形预测构建校准置信区间；提出基于收缩的可达管方法处理地形不确定性；设计基于收缩的飞轮扭矩控制律稳定线性倒立摆模型的角动量；采用高级模型预测控制导航框架。

Result: 在MuJoCo中对Digit双足机器人进行物理仿真验证，证明了该规划框架的有效性，能够确保状态收敛和管不变性。

Conclusion: 提出的方法为双足机器人在不确定地形上的导航提供了概率安全性和目标可达性保证，通过收缩理论和模型预测控制的结合实现了动态稳定控制。

Abstract: We address the challenge of enabling bipedal robots to traverse rough terrain
by developing probabilistically safe planning and control strategies that
ensure dynamic feasibility and centroidal robustness under terrain uncertainty.
Specifically, we propose a high-level Model Predictive Control (MPC) navigation
framework for a bipedal robot with a specified confidence level of safety that
(i) enables safe traversal toward a desired goal location across a terrain map
with uncertain elevations, and (ii) formally incorporates uncertainty bounds
into the centroidal dynamics of locomotion control. To model the rough terrain,
we employ Gaussian Process (GP) regression to estimate elevation maps and
leverage Conformal Prediction (CP) to construct calibrated confidence intervals
that capture the true terrain elevation. Building on this, we formulate
contraction-based reachable tubes that explicitly account for terrain
uncertainty, ensuring state convergence and tube invariance. In addition, we
introduce a contraction-based flywheel torque control law for the reduced-order
Linear Inverted Pendulum Model (LIPM), which stabilizes the angular momentum
about the center-of-mass (CoM). This formulation provides both probabilistic
safety and goal reachability guarantees. For a given confidence level, we
establish the forward invariance of the proposed torque control law by
demonstrating exponential stabilization of the actual CoM phase-space
trajectory and the desired trajectory prescribed by the high-level planner.
Finally, we evaluate the effectiveness of our planning framework through
physics-based simulations of the Digit bipedal robot in MuJoCo.

</details>


### [46] [Injecting Hallucinations in Autonomous Vehicles: A Component-Agnostic Safety Evaluation Framework](https://arxiv.org/abs/2510.07749)
*Alexandre Moreira Nascimento,Gabriel Kenji Godoy Shimanuki,Lúcio Flavio Vismari,João Batista Camargo Jr,Jorge Rady de Almeida Jr,Paulo Sergio Cugnasca,Anna Carolina Muller Queiroz,Jeremy Noah Bailenson*

Main category: cs.RO

TL;DR: 提出了一种可配置、组件无关的幻觉注入框架，用于研究自动驾驶车辆感知故障对安全性的影响，通过模拟六种幻觉类型来量化其对碰撞和险情的影响。


<details>
  <summary>Details</summary>
Motivation: 现有故障注入研究通常针对单一传感器或感知模块，导致难以推广或集成到统一仿真环境中的孤立框架，需要一种更通用的方法来分析感知故障对安全性的影响。

Method: 将感知故障重新定义为幻觉，提出组件无关的幻觉注入框架，在开源模拟器中注入六种可信幻觉类型，执行超过18,350次模拟来测试自动驾驶车辆在无信号交叉路口的性能。

Result: 统计验证了框架有效性，量化了每种幻觉类型对碰撞和险情的影响，某些幻觉（如感知延迟和漂移）显著增加了测试场景中的碰撞风险。

Conclusion: 该框架提供了一个可扩展、统计验证、组件无关且完全互操作的工具集，可简化和加速自动驾驶安全验证，为容错和弹性自动驾驶设计奠定基础。

Abstract: Perception failures in autonomous vehicles (AV) remain a major safety concern
because they are the basis for many accidents. To study how these failures
affect safety, researchers typically inject artificial faults into hardware or
software components and observe the outcomes. However, existing fault injection
studies often target a single sensor or machine perception (MP) module,
resulting in siloed frameworks that are difficult to generalize or integrate
into unified simulation environments. This work addresses that limitation by
reframing perception failures as hallucinations, false perceptions that distort
an AV situational awareness and may trigger unsafe control actions. Since
hallucinations describe only observable effects, this abstraction enables
analysis independent of specific sensors or algorithms, focusing instead on how
their faults manifest along the MP pipeline. Building on this concept, we
propose a configurable, component-agnostic hallucination injection framework
that induces six plausible hallucination types in an iterative open-source
simulator. More than 18,350 simulations were executed in which hallucinations
were injected while AVs crossed an unsignalized transverse street with traffic.
The results statistically validate the framework and quantify the impact of
each hallucination type on collisions and near misses. Certain hallucinations,
such as perceptual latency and drift, significantly increase the risk of
collision in the scenario tested, validating the proposed paradigm can stress
the AV system safety. The framework offers a scalable, statistically validated,
component agnostic, and fully interoperable toolset that simplifies and
accelerates AV safety validations, even those with novel MP architectures and
components. It can potentially reduce the time-to-market of AV and lay the
foundation for future research on fault tolerance, and resilient AV design.

</details>


### [47] [Trajectory Conditioned Cross-embodiment Skill Transfer](https://arxiv.org/abs/2510.07773)
*YuHang Tang,Yixuan Lou,Pengfei Han,Haoming Song,Xinyi Ye,Dong Wang,Bin Zhao*

Main category: cs.RO

TL;DR: TrajSkill是一个从人类演示视频中学习机器人操作技能的框架，通过将人类动作表示为稀疏光流轨迹来实现跨形态技能迁移。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖配对数据集或手工设计的奖励，限制了可扩展性和泛化能力。人类与机器人之间的形态差异是主要挑战。

Method: 将人类动作表示为稀疏光流轨迹作为形态无关的运动线索，结合视觉和文本输入，联合合成时间一致的机器人操作视频并转换为可执行动作。

Result: 在MetaWorld模拟实验中，相比最先进方法，FVD降低39.6%，KVD降低36.6%，跨形态成功率提升达16.7%。真实机器人厨房操作任务验证了有效性。

Conclusion: TrajSkill实现了从人类演示视频到机器人的实用跨形态技能迁移，为机器人学习提供了可扩展的解决方案。

Abstract: Learning manipulation skills from human demonstration videos presents a
promising yet challenging problem, primarily due to the significant embodiment
gap between human body and robot manipulators. Existing methods rely on paired
datasets or hand-crafted rewards, which limit scalability and generalization.
We propose TrajSkill, a framework for Trajectory Conditioned Cross-embodiment
Skill Transfer, enabling robots to acquire manipulation skills directly from
human demonstration videos. Our key insight is to represent human motions as
sparse optical flow trajectories, which serve as embodiment-agnostic motion
cues by removing morphological variations while preserving essential dynamics.
Conditioned on these trajectories together with visual and textual inputs,
TrajSkill jointly synthesizes temporally consistent robot manipulation videos
and translates them into executable actions, thereby achieving cross-embodiment
skill transfer. Extensive experiments are conducted, and the results on
simulation data (MetaWorld) show that TrajSkill reduces FVD by 39.6\% and KVD
by 36.6\% compared with the state-of-the-art, and improves cross-embodiment
success rate by up to 16.7\%. Real-robot experiments in kitchen manipulation
tasks further validate the effectiveness of our approach, demonstrating
practical human-to-robot skill transfer across embodiments.

</details>


### [48] [IntentionVLA: Generalizable and Efficient Embodied Intention Reasoning for Human-Robot Interaction](https://arxiv.org/abs/2510.07778)
*Yandu Chen,Kefan Gu,Yuqing Wen,Yucheng Zhao,Tiancai Wang,Liqiang Nie*

Main category: cs.RO

TL;DR: IntentionVLA是一个视觉-语言-动作模型框架，通过课程训练范式解决当前VLA模型在隐含人类意图推理方面的不足，在复杂交互任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的VLA模型在多模态任务预训练后，仅能映射显式指令到动作，缺乏推理密集型预训练和推理引导的操控能力，无法处理复杂现实交互中所需的隐含人类意图推理。

Method: 提出课程训练范式：首先使用精心设计的推理数据（包含意图推断、空间定位和紧凑体现推理）进行预训练，赋予模型推理和感知能力；然后在微调阶段使用紧凑推理输出作为动作生成的上下文指导，实现间接指令下的快速推理。

Result: IntentionVLA显著优于π0模型，在直接指令下成功率提高18%，在意图指令下比ECoT提高28%。在分布外意图任务中，成功率是基线模型的两倍以上，并在零样本人机交互中达到40%成功率。

Conclusion: IntentionVLA作为下一代人机交互系统的有前景范式，通过推理引导的操控实现了更自然和复杂的人机交互能力。

Abstract: Vision-Language-Action (VLA) models leverage pretrained vision-language
models (VLMs) to couple perception with robotic control, offering a promising
path toward general-purpose embodied intelligence. However, current SOTA VLAs
are primarily pretrained on multimodal tasks with limited relevance to embodied
scenarios, and then finetuned to map explicit instructions to actions.
Consequently, due to the lack of reasoning-intensive pretraining and
reasoning-guided manipulation, these models are unable to perform implicit
human intention reasoning required for complex, real-world interactions. To
overcome these limitations, we propose \textbf{IntentionVLA}, a VLA framework
with a curriculum training paradigm and an efficient inference mechanism. Our
proposed method first leverages carefully designed reasoning data that combine
intention inference, spatial grounding, and compact embodied reasoning,
endowing the model with both reasoning and perception capabilities. In the
following finetuning stage, IntentionVLA employs the compact reasoning outputs
as contextual guidance for action generation, enabling fast inference under
indirect instructions. Experimental results show that IntentionVLA
substantially outperforms $\pi_0$, achieving 18\% higher success rates with
direct instructions and 28\% higher than ECoT under intention instructions. On
out-of-distribution intention tasks, IntentionVLA achieves over twice the
success rate of all baselines, and further enables zero-shot human-robot
interaction with 40\% success rate. These results highlight IntentionVLA as a
promising paradigm for next-generation human-robot interaction (HRI) systems.

</details>


### [49] [GM3: A General Physical Model for Micro-Mobility Vehicles](https://arxiv.org/abs/2510.07807)
*Grace Cai,Nithin Parepally,Laura Zheng,Ming C. Lin*

Main category: cs.RO

TL;DR: 提出了GM3模型，一种基于轮胎刷子表示的统一物理模型，用于模拟各种微移动车辆（MMV）的动力学特性，包括轮胎滑移、载荷转移和骑手/车辆倾斜等效应。


<details>
  <summary>Details</summary>
Motivation: 现有主流工具依赖运动学自行车模型或特定模式的物理模型，无法捕捉轮胎滑移、载荷转移和骑手/车辆倾斜等关键动力学特性，缺乏统一的物理基础模型来覆盖各种常见微移动车辆和车轮布局。

Method: 提出了GM3模型，基于轮胎刷子表示，支持任意车轮配置；开发了交互式模型无关仿真框架，使用固定步长RK4积分、人在回路和脚本控制，以及实时轨迹跟踪和日志记录。

Result: 在斯坦福无人机数据集的deathCircle场景中对骑行者、滑板车和手推车类别进行了实证验证。

Conclusion: GM3模型能够统一捕捉各种微移动车辆的动力学特性，为自动驾驶系统训练和城市交通仿真提供了更准确的建模工具。

Abstract: Modeling the dynamics of micro-mobility vehicles (MMV) is becoming
increasingly important for training autonomous vehicle systems and building
urban traffic simulations. However, mainstream tools rely on variants of the
Kinematic Bicycle Model (KBM) or mode-specific physics that miss tire slip,
load transfer, and rider/vehicle lean. To our knowledge, no unified,
physics-based model captures these dynamics across the full range of common
MMVs and wheel layouts. We propose the "Generalized Micro-mobility Model"
(GM3), a tire-level formulation based on the tire brush representation that
supports arbitrary wheel configurations, including single/double track and
multi-wheel platforms. We introduce an interactive model-agnostic simulation
framework that decouples vehicle/layout specification from dynamics to compare
the GM3 with the KBM and other models, consisting of fixed step RK4
integration, human-in-the-loop and scripted control, real-time trajectory
traces and logging for analysis. We also empirically validate the GM3 on the
Stanford Drone Dataset's deathCircle (roundabout) scene for biker, skater, and
cart classes.

</details>


### [50] [DM1: MeanFlow with Dispersive Regularization for 1-Step Robotic Manipulation](https://arxiv.org/abs/2510.07865)
*Guowei Zou,Haitao Wang,Hejun Wu,Yukun Qian,Yuhang Wang,Weibing Li*

Main category: cs.RO

TL;DR: DM1提出了一种结合分散正则化的流匹配框架，解决了基于流的策略中表示坍塌问题，在保持一步推理效率的同时显著提升了机器人操作任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于流的策略存在表示坍塌问题，无法区分相似的视觉表示，导致在精确操作任务中失败。虽然流模型在动作分布学习方面具有采样效率优势，但需要解决表示多样性不足的问题。

Method: DM1将分散正则化集成到MeanFlow中，在多个中间嵌入层使用不同的分散正则化变体，鼓励训练批次间的表示多样性，无需引入额外网络模块或专门训练过程。

Result: 在RoboMimic基准测试中，DM1实现了20-40倍的推理加速（0.07s vs. 2-3.5s），成功率提升10-20个百分点，Lift任务达到99%成功率（基线为85%）。真实机器人部署验证了从仿真到物理世界的有效迁移。

Conclusion: 这是首个利用表示正则化使基于流的策略在机器人操作中实现强性能的工作，为高效鲁棒的操作建立了一种简单而强大的方法。

Abstract: The ability to learn multi-modal action distributions is indispensable for
robotic manipulation policies to perform precise and robust control. Flow-based
generative models have recently emerged as a promising solution to learning
distributions of actions, offering one-step action generation and thus
achieving much higher sampling efficiency compared to diffusion-based methods.
However, existing flow-based policies suffer from representation collapse, the
inability to distinguish similar visual representations, leading to failures in
precise manipulation tasks. We propose DM1 (MeanFlow with Dispersive
Regularization for One-Step Robotic Manipulation), a novel flow matching
framework that integrates dispersive regularization into MeanFlow to prevent
collapse while maintaining one-step efficiency. DM1 employs multiple dispersive
regularization variants across different intermediate embedding layers,
encouraging diverse representations across training batches without introducing
additional network modules or specialized training procedures. Experiments on
RoboMimic benchmarks show that DM1 achieves 20-40 times faster inference (0.07s
vs. 2-3.5s) and improves success rates by 10-20 percentage points, with the
Lift task reaching 99% success over 85% of the baseline. Real-robot deployment
on a Franka Panda further validates that DM1 transfers effectively from
simulation to the physical world. To the best of our knowledge, this is the
first work to leverage representation regularization to enable flow-based
policies to achieve strong performance in robotic manipulation, establishing a
simple yet powerful approach for efficient and robust manipulation.

</details>


### [51] [USIM and U0: A Vision-Language-Action Dataset and Model for General Underwater Robots](https://arxiv.org/abs/2510.07869)
*Junwen Gu,Zhiheng wu,Pengxuan Si,Shuang Qiu,Yukai Feng,Luoyang Sun,Laien Luo,Lianyi Yu,Jian Wang,Zhengxing Wu*

Main category: cs.RO

TL;DR: 提出了USIM模拟数据集和U0视觉-语言-动作模型，用于解决水下机器人多任务自主操作的挑战，在多种任务中取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 水下环境复杂，现有数据集稀缺，难以开发能自主执行多任务的水下智能机器人。

Method: 构建包含56.1万帧数据的USIM模拟数据集，并提出U0 VLA模型，集成双目视觉和传感器模态，加入卷积注意力感知增强模块。

Result: 在检查、避障等任务中成功率80%，在移动操作任务中比基线方法减少21.2%的目标距离。

Conclusion: VLA模型可有效应用于水下机器人，为可扩展数据集构建、任务自主性提升和智能通用水下机器人的实际实现提供基础。

Abstract: Underwater environments present unique challenges for robotic operation,
including complex hydrodynamics, limited visibility, and constrained
communication. Although data-driven approaches have advanced embodied
intelligence in terrestrial robots and enabled task-specific autonomous
underwater robots, developing underwater intelligence capable of autonomously
performing multiple tasks remains highly challenging, as large-scale,
high-quality underwater datasets are still scarce. To address these
limitations, we introduce USIM, a simulation-based multi-task
Vision-Language-Action (VLA) dataset for underwater robots. USIM comprises over
561K frames from 1,852 trajectories, totaling approximately 15.6 hours of
BlueROV2 interactions across 20 tasks in 9 diverse scenarios, ranging from
visual navigation to mobile manipulation. Building upon this dataset, we
propose U0, a VLA model for general underwater robots, which integrates
binocular vision and other sensor modalities through multimodal fusion, and
further incorporates a convolution-attention-based perception focus enhancement
module (CAP) to improve spatial understanding and mobile manipulation. Across
tasks such as inspection, obstacle avoidance, scanning, and dynamic tracking,
the framework achieves a success rate of 80%, while in challenging mobile
manipulation tasks, it reduces the distance to the target by 21.2% compared
with baseline methods, demonstrating its effectiveness. USIM and U0 show that
VLA models can be effectively applied to underwater robotic applications,
providing a foundation for scalable dataset construction, improved task
autonomy, and the practical realization of intelligent general underwater
robots.

</details>


### [52] [Team Xiaomi EV-AD VLA: Learning to Navigate Socially Through Proactive Risk Perception -- Technical Report for IROS 2025 RoboSense Challenge Social Navigation Track](https://arxiv.org/abs/2510.07871)
*Erjia Xiao,Lingfeng Zhang,Yingbo Tang,Hao Cheng,Renjing Xu,Wenbo Ding,Lei Zhou,Long Chen,Hangjun Ye,Xiaoshuai Hao*

Main category: cs.RO

TL;DR: 本文介绍了参加IROS 2025 RoboSense挑战赛社交导航赛道的技术方案，通过在Falcon模型基础上增加主动风险感知模块，提升机器人在动态人群环境中的社交导航能力，最终在16支参赛队伍中获得第2名。


<details>
  <summary>Details</summary>
Motivation: 开发能够在动态人群环境中安全、高效且符合社交规范的自主导航系统，解决仅使用机载RGB-D传感器和里程计，在没有全局地图或特权信息的情况下实现社交合规导航的挑战。

Method: 在Falcon模型基础上引入主动风险感知模块，通过学习预测周围人类基于距离的碰撞风险分数，增强代理的空间意识和主动避碰行为。

Result: 在Social-HM3D基准测试中，该方法提高了代理在拥挤室内场景中保持个人空间合规性的能力，在挑战赛16支参赛队伍中获得第2名。

Conclusion: 主动风险感知模块有效提升了社交导航性能，证明了基于风险预测的方法在动态人群环境中实现安全合规导航的有效性。

Abstract: In this report, we describe the technical details of our submission to the
IROS 2025 RoboSense Challenge Social Navigation Track. This track focuses on
developing RGBD-based perception and navigation systems that enable autonomous
agents to navigate safely, efficiently, and socially compliantly in dynamic
human-populated indoor environments. The challenge requires agents to operate
from an egocentric perspective using only onboard sensors including RGB-D
observations and odometry, without access to global maps or privileged
information, while maintaining social norm compliance such as safe distances
and collision avoidance. Building upon the Falcon model, we introduce a
Proactive Risk Perception Module to enhance social navigation performance. Our
approach augments Falcon with collision risk understanding that learns to
predict distance-based collision risk scores for surrounding humans, which
enables the agent to develop more robust spatial awareness and proactive
collision avoidance behaviors. The evaluation on the Social-HM3D benchmark
demonstrates that our method improves the agent's ability to maintain personal
space compliance while navigating toward goals in crowded indoor scenes with
dynamic human agents, achieving 2nd place among 16 participating teams in the
challenge.

</details>


### [53] [Towards Proprioception-Aware Embodied Planning for Dual-Arm Humanoid Robots](https://arxiv.org/abs/2510.07882)
*Boyu Li,Siyuan He,Hang Xu,Haoqi Yuan,Yu Zang,Liwei Hu,Junpeng Yue,Zhenxiong Jiang,Pengbo Hu,Börje F. Karlsson,Yehui Tang,Zongqing Lu*

Main category: cs.RO

TL;DR: 提出了DualTHOR双臂人形机器人模拟器和Proprio-MLLM模型，通过整合本体感知信息解决MLLMs在长视野双臂任务中的局限性，将规划性能平均提升19.75%。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在双臂人形机器人的长视野任务中效果有限，主要由于缺乏支持人形机器人任务评估的系统模拟平台，以及MLLMs缺乏本体感知能力，无法推理双臂选择逻辑和身体位置。

Method: 开发了DualTHOR双臂人形模拟器，具有连续过渡和应急机制；提出Proprio-MLLM模型，通过运动基位置嵌入和跨空间编码器整合本体感知信息。

Result: 实验显示，现有MLLMs在该环境中表现不佳，而Proprio-MLLM在规划性能上实现了平均19.75%的提升。

Conclusion: 该研究为人形机器人的具身智能发展提供了必要的模拟平台和有效模型。

Abstract: In recent years, Multimodal Large Language Models (MLLMs) have demonstrated
the ability to serve as high-level planners, enabling robots to follow complex
human instructions. However, their effectiveness, especially in long-horizon
tasks involving dual-arm humanoid robots, remains limited. This limitation
arises from two main challenges: (i) the absence of simulation platforms that
systematically support task evaluation and data collection for humanoid robots,
and (ii) the insufficient embodiment awareness of current MLLMs, which hinders
reasoning about dual-arm selection logic and body positions during planning. To
address these issues, we present DualTHOR, a new dual-arm humanoid simulator,
with continuous transition and a contingency mechanism. Building on this
platform, we propose Proprio-MLLM, a model that enhances embodiment awareness
by incorporating proprioceptive information with motion-based position
embedding and a cross-spatial encoder. Experiments show that, while existing
MLLMs struggle in this environment, Proprio-MLLM achieves an average
improvement of 19.75% in planning performance. Our work provides both an
essential simulation platform and an effective model to advance embodied
intelligence in humanoid robotics. The code is available at
https://anonymous.4open.science/r/DualTHOR-5F3B.

</details>


### [54] [Executable Analytic Concepts as the Missing Link Between VLM Insight and Precise Manipulation](https://arxiv.org/abs/2510.07975)
*Mingyang Sun,Jiude Wei,Qichen He,Donglin Wang,Cewu Lu,Jianhua Sun*

Main category: cs.RO

TL;DR: GRACE框架通过可执行分析概念（EAC）弥合视觉语言模型语义推理与机器人物理执行之间的差距，实现精确和通用的机器人操作。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型在语义推理和任务规划方面的能力与机器人实际物理执行之间的差距，即"语义到物理"的鸿沟。

Method: 引入可执行分析概念（EAC）作为数学定义的蓝图，编码物体功能、几何约束和操作语义，通过结构化策略脚手架管道将自然语言指令和视觉信息转化为实例化的EAC，从中推导抓取姿态、力方向和物理可行的运动轨迹。

Result: 在模拟和真实环境中对各种铰接物体实现了强大的零样本泛化能力，无需任务特定训练。

Conclusion: GRACE提供了一个统一且可解释的接口，通过语义-物理接地有效实现了精确和可泛化的机器人操作。

Abstract: Enabling robots to perform precise and generalized manipulation in
unstructured environments remains a fundamental challenge in embodied AI. While
Vision-Language Models (VLMs) have demonstrated remarkable capabilities in
semantic reasoning and task planning, a significant gap persists between their
high-level understanding and the precise physical execution required for
real-world manipulation. To bridge this "semantic-to-physical" gap, we
introduce GRACE, a novel framework that grounds VLM-based reasoning through
executable analytic concepts (EAC)-mathematically defined blueprints that
encode object affordances, geometric constraints, and semantics of
manipulation. Our approach integrates a structured policy scaffolding pipeline
that turn natural language instructions and visual information into an
instantiated EAC, from which we derive grasp poses, force directions and plan
physically feasible motion trajectory for robot execution. GRACE thus provides
a unified and interpretable interface between high-level instruction
understanding and low-level robot control, effectively enabling precise and
generalizable manipulation through semantic-physical grounding. Extensive
experiments demonstrate that GRACE achieves strong zero-shot generalization
across a variety of articulated objects in both simulated and real-world
environments, without requiring task-specific training.

</details>


### [55] [Orientation Learning and Adaptation towards Simultaneous Incorporation of Multiple Local Constraints](https://arxiv.org/abs/2510.07986)
*Gaofeng Li,Peisen Xu,Ruize Wang,Qi Ye,Jiming Chen,Dezhen Song,Yanlong Huang*

Main category: cs.RO

TL;DR: 提出基于角度-轴空间的方向表示方法，通过加权平均机制在SO(3)流形上融合多个轨迹，同时处理多个局部约束，解决非欧几何带来的失真问题。


<details>
  <summary>Details</summary>
Motivation: 旋转群SO(3)是黎曼流形，其非欧几何特性导致局部约束整合困难，特别是同时整合多个局部约束时存在挑战。

Method: 使用角度-轴表示法，在不同基点考虑不同局部约束生成多个轨迹，然后通过提出的加权平均机制融合这些轨迹生成平滑轨迹。

Result: 仿真和实验验证表明，该方法不仅能适应任意期望路径点并处理角加速度约束，还能同时整合多个局部约束获得额外收益，如实现更小的加速度成本。

Conclusion: 提出的方法解决了SO(3)流形上的失真问题，使现成的欧几里得学习算法在非欧空间中重新适用，有效整合多个局部约束。

Abstract: Orientation learning plays a pivotal role in many tasks. However, the
rotation group SO(3) is a Riemannian manifold. As a result, the distortion
caused by non-Euclidean geometric nature introduces difficulties to the
incorporation of local constraints, especially for the simultaneous
incorporation of multiple local constraints. To address this issue, we propose
the Angle-Axis Space-based orientation representation method to solve several
orientation learning problems, including orientation adaptation and
minimization of angular acceleration. Specifically, we propose a weighted
average mechanism in SO(3) based on the angle-axis representation method. Our
main idea is to generate multiple trajectories by considering different local
constraints at different basepoints. Then these multiple trajectories are fused
to generate a smooth trajectory by our proposed weighted average mechanism,
achieving the goal to incorporate multiple local constraints simultaneously.
Compared with existing solution, ours can address the distortion issue and make
the off-theshelf Euclidean learning algorithm be re-applicable in non-Euclidean
space. Simulation and Experimental evaluations validate that our solution can
not only adapt orientations towards arbitrary desired via-points and cope with
angular acceleration constraints, but also incorporate multiple local
constraints simultaneously to achieve extra benefits, e.g., achieving smaller
acceleration costs.

</details>


### [56] [FastUMI-100K: Advancing Data-driven Robotic Manipulation with a Large-scale UMI-style Dataset](https://arxiv.org/abs/2510.08022)
*Kehui Liu,Zhongjie Jia,Yang Li,Zhaxizhuoma,Pengan Chen,Song Liu,Xin Liu,Pingrui Zhang,Haoming Song,Xinyi Ye,Nieqing Cao,Zhigang Wang,Jia Zeng,Dong Wang,Yan Ding,Bin Zhao,Xuelong Li*

Main category: cs.RO

TL;DR: FastUMI-100K是一个大规模UMI风格的多模态演示数据集，包含10万+条轨迹，涵盖54个任务和数百种物体类型，旨在解决现有机器人演示数据集在可扩展性、轨迹平滑性和跨机器人平台适用性方面的限制。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人操作学习数据集主要依赖人类遥操作收集，存在可扩展性差、轨迹不平滑以及在不同机器人平台上适用性有限的问题。需要开发更灵活、适应性强的解决方案来满足现实世界复杂操作任务的需求。

Method: 通过FastUMI系统收集数据，该系统采用模块化、硬件解耦的机械设计，并集成了轻量级跟踪系统。数据集包含多模态数据流，包括末端执行器状态、多视角鱼眼图像和文本注释。

Result: 实验结果表明，FastUMI-100K能够在各种基线算法上实现高策略成功率，验证了其鲁棒性、适应性和在现实世界中的适用性，能够解决复杂动态操作挑战。

Conclusion: FastUMI-100K提供了一个更可扩展、灵活和适应性强的解决方案，能够满足现实世界机器人演示数据的多样化需求，为数据驱动的机器人操作学习提供了高质量的大规模数据集。

Abstract: Data-driven robotic manipulation learning depends on large-scale,
high-quality expert demonstration datasets. However, existing datasets, which
primarily rely on human teleoperated robot collection, are limited in terms of
scalability, trajectory smoothness, and applicability across different robotic
embodiments in real-world environments. In this paper, we present FastUMI-100K,
a large-scale UMI-style multimodal demonstration dataset, designed to overcome
these limitations and meet the growing complexity of real-world manipulation
tasks. Collected by FastUMI, a novel robotic system featuring a modular,
hardware-decoupled mechanical design and an integrated lightweight tracking
system, FastUMI-100K offers a more scalable, flexible, and adaptable solution
to fulfill the diverse requirements of real-world robot demonstration data.
Specifically, FastUMI-100K contains over 100K+ demonstration trajectories
collected across representative household environments, covering 54 tasks and
hundreds of object types. Our dataset integrates multimodal streams, including
end-effector states, multi-view wrist-mounted fisheye images and textual
annotations. Each trajectory has a length ranging from 120 to 500 frames.
Experimental results demonstrate that FastUMI-100K enables high policy success
rates across various baseline algorithms, confirming its robustness,
adaptability, and real-world applicability for solving complex, dynamic
manipulation challenges. The source code and dataset will be released in this
link https://github.com/MrKeee/FastUMI-100K.

</details>


### [57] [Reliability of Single-Level Equality-Constrained Inverse Optimal Control](https://arxiv.org/abs/2510.08406)
*Filip Bečanović,Kosta Jovanović,Vincent Bonnet*

Main category: cs.RO

TL;DR: 提出了一种基于单级重构的逆最优控制方法，相比传统的双层方法计算速度提升15倍，且对噪声具有强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有逆最优控制方法要么基于缓慢的双层过程，要么基于快速但对噪声敏感的优化条件违反最小化方法，需要一种既快速又鲁棒的新方法。

Method: 采用单级重构方法替代传统的双层逆最优控制方法，基于等式约束的最优控制模型。

Result: 在平面到达任务的数值实验中，该方法对极大噪声水平表现出弹性，计算时间比经典双层实现减少15倍。

Conclusion: 单级重构方法在保持结果等效性的同时，显著提高了逆最优控制的计算效率和鲁棒性。

Abstract: Inverse optimal control (IOC) allows the retrieval of optimal cost function
weights, or behavioral parameters, from human motion. The literature on IOC
uses methods that are either based on a slow bilevel process or a fast but
noise-sensitive minimization of optimality condition violation. Assuming
equality-constrained optimal control models of human motion, this article
presents a faster but robust approach to solving IOC using a single-level
reformulation of the bilevel method and yields equivalent results. Through
numerical experiments in simulation, we analyze the robustness to noise of the
proposed single-level reformulation to the bilevel IOC formulation with a
human-like planar reaching task that is used across recent studies. The
approach shows resilience to very large levels of noise and reduces the
computation time of the IOC on this task by a factor of 15 when compared to a
classical bilevel implementation.

</details>


### [58] [Towards Reliable LLM-based Robot Planning via Combined Uncertainty Estimation](https://arxiv.org/abs/2510.08044)
*Shiyuan Yin,Chenjia Bai,Zihao Zhang,Junwei Jin,Xinxin Zhang,Chi Zhang,Xuelong Li*

Main category: cs.RO

TL;DR: 本文提出了CURE方法，通过将不确定性分解为认知不确定性和内在不确定性，并使用随机网络蒸馏和MLP回归头来估计，提高了LLM在机器人规划中的可靠性。


<details>
  <summary>Details</summary>
Motivation: LLM在机器人规划中存在幻觉问题，导致过度自信但可能不准确或不安全的计划。现有研究未能充分区分认知和内在不确定性，限制了不确定性估计的效果。

Method: 提出CURE方法，将不确定性分解为认知不确定性（细分为任务清晰度和任务熟悉度）和内在不确定性，使用随机网络蒸馏和基于LLM特征的多层感知机回归头进行估计。

Result: 在厨房操作和桌面重排实验中验证，相比现有方法，CURE的不确定性估计更符合实际执行结果。

Conclusion: CURE方法通过分解和分别估计不同类型的不确定性，显著提高了LLM在机器人规划中的可靠性，不确定性估计更准确。

Abstract: Large language models (LLMs) demonstrate advanced reasoning abilities,
enabling robots to understand natural language instructions and generate
high-level plans with appropriate grounding. However, LLM hallucinations
present a significant challenge, often leading to overconfident yet potentially
misaligned or unsafe plans. While researchers have explored uncertainty
estimation to improve the reliability of LLM-based planning, existing studies
have not sufficiently differentiated between epistemic and intrinsic
uncertainty, limiting the effectiveness of uncertainty estimation. In this
paper, we present Combined Uncertainty estimation for Reliable Embodied
planning (CURE), which decomposes the uncertainty into epistemic and intrinsic
uncertainty, each estimated separately. Furthermore, epistemic uncertainty is
subdivided into task clarity and task familiarity for more accurate evaluation.
The overall uncertainty assessments are obtained using random network
distillation and multi-layer perceptron regression heads driven by LLM
features. We validated our approach in two distinct experimental settings:
kitchen manipulation and tabletop rearrangement experiments. The results show
that, compared to existing methods, our approach yields uncertainty estimates
that are more closely aligned with the actual execution outcomes.

</details>


### [59] [Beyond hospital reach: Autonomous lightweight ultrasound robot for liver sonography](https://arxiv.org/abs/2510.08106)
*Zihan Li,Yixiao Xu,Lei Zhang,Taiyu Han,Xinshan Yang,Yingni Wang,Mingxuan Liu,Shenghai Xin,Linxun Liu,Hongen Liao,Guochen Ning*

Main category: cs.RO

TL;DR: 开发了一个轻量级自主超声机器人系统，能够在资源有限地区自动获取专家级肝脏超声平面并检测病理，适用于快速运动和野外环境。


<details>
  <summary>Details</summary>
Motivation: 肝脏疾病是全球重大健康负担，超声是首选诊断工具，但肝脏超声检查需要专业知识，而专家超声医生在资源有限地区严重短缺。

Method: 系统包含AI智能体（集成多模态感知与记忆注意力机制用于定位不可见目标结构）和588克6自由度线驱动机器人，通过安装在腹部增强运动稳定性。

Result: 机器人能够自主获取专家级标准肝脏超声平面并检测患者病理，在西宁（海拔2261米）等医疗资源有限地区有效工作，在快速运动个体和野外环境中表现良好。

Conclusion: 这是首个在多种挑战性场景下实现自主超声检查的演示，有望改变资源匮乏地区获得专家级诊断的可及性。

Abstract: Liver disease is a major global health burden. While ultrasound is the
first-line diagnostic tool, liver sonography requires locating multiple
non-continuous planes from positions where target structures are often not
visible, for biometric assessment and lesion detection, requiring significant
expertise. However, expert sonographers are severely scarce in resource-limited
regions. Here, we develop an autonomous lightweight ultrasound robot comprising
an AI agent that integrates multi-modal perception with memory attention for
localization of unseen target structures, and a 588-gram 6-degrees-of-freedom
cable-driven robot. By mounting on the abdomen, the system enhances robustness
against motion. Our robot can autonomously acquire expert-level standard liver
ultrasound planes and detect pathology in patients, including two from Xining,
a 2261-meter-altitude city with limited medical resources. Our system performs
effectively on rapid-motion individuals and in wilderness environments. This
work represents the first demonstration of autonomous sonography across
multiple challenging scenarios, potentially transforming access to expert-level
diagnostics in underserved regions.

</details>


### [60] [Accurate and Noise-Tolerant Extraction of Routine Logs in Robotic Process Automation (Extended Version)](https://arxiv.org/abs/2510.08118)
*Massimiliano de Leoni,Faizan Ahmed Khan,Simone Agostinelli*

Main category: cs.RO

TL;DR: 提出了一种基于聚类的技术，用于从用户界面日志中提取常规日志，特别针对包含噪声（人类执行中的自然变异和偶然错误）的场景，相比现有技术能提取更准确的常规日志。


<details>
  <summary>Details</summary>
Motivation: 现有工作大多不直接关注模型发现，仅提取常规中的动作集合，且未在包含噪声（人类执行不一致性）的场景下进行评估，这限制了机器人流程自动化的实现。

Method: 采用基于聚类的技术从用户界面日志中提取常规日志，在九个具有不同噪声水平的UI日志上进行实验，并与现有技术进行比较。

Result: 实验结果表明，该技术能够比现有技术提取更准确的常规日志，特别是在存在噪声的情况下表现更优。

Conclusion: 提出的聚类技术能够有效处理噪声场景下的常规日志提取问题，为机器人流程自动化提供了更可靠的模型发现方法。

Abstract: Robotic Process Mining focuses on the identification of the routine types
performed by human resources through a User Interface. The ultimate goal is to
discover routine-type models to enable robotic process automation. The
discovery of routine-type models requires the provision of a routine log.
Unfortunately, the vast majority of existing works do not directly focus on
enabling the model discovery, limiting themselves to extracting the set of
actions that are part of the routines. They were also not evaluated in
scenarios characterized by inconsistent routine execution, hereafter referred
to as noise, which reflects natural variability and occasional errors in human
performance. This paper presents a clustering-based technique that aims to
extract routine logs. Experiments were conducted on nine UI logs from the
literature with different levels of injected noise. Our technique was compared
with existing techniques, most of which are not meant to discover routine logs
but were adapted for the purpose. The results were evaluated through standard
state-of-the-art metrics, showing that we can extract more accurate routine
logs than what the state of the art could, especially in the presence of noise.

</details>


### [61] [NavSpace: How Navigation Agents Follow Spatial Intelligence Instructions](https://arxiv.org/abs/2510.08173)
*Haolin Yang,Yuxing Long,Zhuoyuan Yu,Zihan Yang,Minghan Wang,Jiapeng Xu,Yihan Wang,Ziyan Yu,Wenzhe Cai,Lei Kang,Hao Dong*

Main category: cs.RO

TL;DR: 提出了NavSpace基准测试，包含6个任务类别和1,228个轨迹-指令对，用于评估导航代理的空间智能，并提出了新的空间智能导航模型SNav。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注语义理解，但忽视了系统评估导航代理的空间感知和推理能力。

Method: 构建NavSpace基准测试，包含6个任务类别和1,228个轨迹-指令对，并提出了新的空间智能导航模型SNav。

Result: 在NavSpace基准测试上评估了22个导航代理，SNav模型在NavSpace和真实机器人测试中优于现有导航代理。

Conclusion: NavSpace基准测试揭示了导航中的空间智能问题，SNav为未来工作建立了强基线。

Abstract: Instruction-following navigation is a key step toward embodied intelligence.
Prior benchmarks mainly focus on semantic understanding but overlook
systematically evaluating navigation agents' spatial perception and reasoning
capabilities. In this work, we introduce the NavSpace benchmark, which contains
six task categories and 1,228 trajectory-instruction pairs designed to probe
the spatial intelligence of navigation agents. On this benchmark, we
comprehensively evaluate 22 navigation agents, including state-of-the-art
navigation models and multimodal large language models. The evaluation results
lift the veil on spatial intelligence in embodied navigation. Furthermore, we
propose SNav, a new spatially intelligent navigation model. SNav outperforms
existing navigation agents on NavSpace and real robot tests, establishing a
strong baseline for future work.

</details>


### [62] [Evaluation of a Robust Control System in Real-World Cable-Driven Parallel Robots](https://arxiv.org/abs/2510.08270)
*Damir Nurtdinov,Aliaksei Korshuk,Alexei Kornaev,Alexander Maloletov*

Main category: cs.RO

TL;DR: 比较经典PID控制器与现代强化学习算法在电缆驱动并联机器人控制中的性能，发现TRPO在多种轨迹下表现最佳，具有最低的RMS误差和对更大时间间隔的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 评估经典和现代控制方法在真实世界电缆驱动并联机器人中的性能，特别关注欠约束系统和有限时间离散化的情况。

Method: 对经典PID控制器和现代强化学习算法（包括DDPG、PPO和TRPO）进行对比分析。

Result: TRPO在所有方法中表现最优，在各种轨迹下实现最低的RMS误差，并对更大的控制更新时间间隔表现出鲁棒性。

Conclusion: TRPO在复杂机器人控制任务中具有作为鲁棒解决方案的潜力，特别适用于动态环境和未来在传感器融合或混合控制策略中的应用。

Abstract: This study evaluates the performance of classical and modern control methods
for real-world Cable-Driven Parallel Robots (CDPRs), focusing on
underconstrained systems with limited time discretization. A comparative
analysis is conducted between classical PID controllers and modern
reinforcement learning algorithms, including Deep Deterministic Policy Gradient
(DDPG), Proximal Policy Optimization (PPO), and Trust Region Policy
Optimization (TRPO). The results demonstrate that TRPO outperforms other
methods, achieving the lowest root mean square (RMS) errors across various
trajectories and exhibiting robustness to larger time intervals between control
updates. TRPO's ability to balance exploration and exploitation enables stable
control in noisy, real-world environments, reducing reliance on high-frequency
sensor feedback and computational demands. These findings highlight TRPO's
potential as a robust solution for complex robotic control tasks, with
implications for dynamic environments and future applications in sensor fusion
or hybrid control strategies.

</details>


### [63] [Airy: Reading Robot Intent through Height and Sky](https://arxiv.org/abs/2510.08381)
*Baoyang Chen,Xian Xu,Huamin Qu*

Main category: cs.RO

TL;DR: Airy是一个艺术装置，通过两个强化学习训练的机械臂竞争抖床单来展示复杂多智能体AI的决策过程，使用竞争、具身熟悉性和传感器到感知映射三个设计原则，让观众直观理解机器意图。


<details>
  <summary>Details</summary>
Motivation: 随着工业机器人进入共享人类空间，其不透明的决策过程威胁着安全性、信任和公共监督。该作品旨在探索复杂多智能体AI是否能变得直观易懂。

Method: 采用三个设计原则：竞争作为清晰指标（谁抖得更高）、具身熟悉性（观众熟悉抖床单动作）、传感器到感知映射（通过森林和天气投影显示机器人合作或竞争关系）。

Result: 在五个国际展览中的观察表明，观众能够实时解读机器人的策略、冲突和合作，情感反应与系统内部状态一致。

Conclusion: 该项目展示了感官隐喻如何将黑盒系统转变为公共界面，使复杂AI决策变得可理解。

Abstract: As industrial robots move into shared human spaces, their opaque decision
making threatens safety, trust, and public oversight. This artwork, Airy, asks
whether complex multi agent AI can become intuitively understandable by staging
a competition between two reinforcement trained robot arms that snap a bedsheet
skyward. Building on three design principles, competition as a clear metric
(who lifts higher), embodied familiarity (audiences recognize fabric snapping),
and sensor to sense mapping (robot cooperation or rivalry shown through forest
and weather projections), the installation gives viewers a visceral way to read
machine intent. Observations from five international exhibitions indicate that
audiences consistently read the robots' strategies, conflict, and cooperation
in real time, with emotional reactions that mirror the system's internal state.
The project shows how sensory metaphors can turn a black box into a public
interface.

</details>


### [64] [Validation of collision-free spheres of Stewart-Gough platforms for constant orientations using the Application Programming Interface of a CAD software](https://arxiv.org/abs/2510.08408)
*Bibekananda Patra,Rajeevlochana G. Chittawadigi,Sandipan Bandyopadhyay*

Main category: cs.RO

TL;DR: 提出了一种使用CAD软件API验证6-6 Stewart-Gough平台机械臂最大无碰撞球体尺寸的方法，通过自动化更新平台位置并检测腿部碰撞来验证CFS的安全性。


<details>
  <summary>Details</summary>
Motivation: 需要验证并联机械臂在给定平台方向下的最大无碰撞球体尺寸，确保机械臂在工作空间内的安全操作。

Method: 利用CAD软件API自动更新移动平台位置，在CFS表面的包围壳内采样，检测每对腿部之间的相互碰撞。

Result: 该方法能够验证预计算的CFS安全性，并可估计任何空间并联机械臂的无碰撞球体。

Conclusion: 提出的基于CAD API的验证方法有效且通用，适用于各种并联机械臂的无碰撞工作空间分析。

Abstract: This paper presents a method of validation of the size of the largest
collision-free sphere (CFS) of a 6-6 Stewart-Gough platform manipulator (SGPM)
for a given orientation of its moving platform (MP) using the Application
Programming Interface (API) of a CAD software. The position of the MP is
updated via the API in an automated manner over a set of samples within a shell
enclosing the surface of the CFS. For each pose of the manipulator, each pair
of legs is investigated for mutual collisions. The CFS is considered safe or
validated iff none of the points falling inside the CFS lead to a collision
between any pair of legs. This approach can not only validate the safety of a
precomputed CFS, but also estimate the same for any spatial parallel
manipulator.

</details>


### [65] [Don't Run with Scissors: Pruning Breaks VLA Models but They Can Be Recovered](https://arxiv.org/abs/2510.08464)
*Jason Jabbour,Dong-Ki Kim,Max Smith,Jay Patrikar,Radhika Ghosal,Youhui Wang,Ali Agha,Vijay Janapa Reddi,Shayegan Omidshafiei*

Main category: cs.RO

TL;DR: GLUESTICK是一种后剪枝恢复方法，通过权重空间插值在稀疏模型中恢复功能，无需额外训练即可显著提升剪枝后VLA模型的性能并减少安全违规。


<details>
  <summary>Details</summary>
Motivation: VLA模型在资源受限硬件上部署困难，剪枝虽然能压缩大语言模型，但在机器人领域研究不足，且会导致性能急剧下降和安全违规增加。

Method: 在权重空间对密集模型和剪枝模型进行一次性插值计算修正项，推理时每个剪枝层使用该修正项恢复丢失的功能，仅引入一个控制效率与精度权衡的超参数。

Result: 在多种VLA架构和操作导航任务中，GLUESTICK在保持竞争性内存效率的同时，显著恢复成功率并减少安全违规。

Conclusion: GLUESTICK提供了一种无需训练、与剪枝算法无关的有效方法，可在保持稀疏性优势的同时恢复剪枝VLA模型的功能。

Abstract: Vision-Language-Action (VLA) models have advanced robotic capabilities but
remain challenging to deploy on resource-limited hardware. Pruning has enabled
efficient compression of large language models (LLMs), yet it is largely
understudied in robotics. Surprisingly, we observe that pruning VLA models
leads to drastic degradation and increased safety violations. We introduce
GLUESTICK, a post-pruning recovery method that restores much of the original
model's functionality while retaining sparsity benefits. Our method performs a
one-time interpolation between the dense and pruned models in weight-space to
compute a corrective term. This correction is used during inference by each
pruned layer to recover lost capabilities with minimal overhead. GLUESTICK
requires no additional training, is agnostic to the pruning algorithm, and
introduces a single hyperparameter that controls the tradeoff between
efficiency and accuracy. Across diverse VLA architectures and tasks in
manipulation and navigation, GLUESTICK achieves competitive memory efficiency
while substantially recovering success rates and reducing safety violations.
Additional material can be found at: https://gluestick-vla.github.io/.

</details>


### [66] [DexMan: Learning Bimanual Dexterous Manipulation from Human and Generated Videos](https://arxiv.org/abs/2510.08475)
*Jhen Hsieh,Kuan-Hsun Tu,Kuo-Han Hung,Tsung-Wei Ke*

Main category: cs.RO

TL;DR: DexMan是一个自动化框架，可将人类视觉演示转换为仿人机器人的双手灵巧操作技能，无需相机校准、深度传感器、3D对象扫描或真实手部物体运动标注。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法仅考虑简化浮动手部的问题，直接控制仿人机器人，利用基于接触的奖励从野外视频中学习策略。

Method: 基于第三人称人类操作视频，使用接触奖励改进策略学习，无需手动数据收集和昂贵的动作捕捉。

Result: 在TACO基准测试中物体姿态估计达到最先进水平，ADD-S和VSD分别提升0.08和0.12；在OakInk-v2上成功率比先前方法提高19%。

Conclusion: DexMan能从真实和合成视频生成技能，无需手动数据收集，为训练通用灵巧操作创建大规模多样化数据集。

Abstract: We present DexMan, an automated framework that converts human visual
demonstrations into bimanual dexterous manipulation skills for humanoid robots
in simulation. Operating directly on third-person videos of humans manipulating
rigid objects, DexMan eliminates the need for camera calibration, depth
sensors, scanned 3D object assets, or ground-truth hand and object motion
annotations. Unlike prior approaches that consider only simplified floating
hands, it directly controls a humanoid robot and leverages novel contact-based
rewards to improve policy learning from noisy hand-object poses estimated from
in-the-wild videos.
  DexMan achieves state-of-the-art performance in object pose estimation on the
TACO benchmark, with absolute gains of 0.08 and 0.12 in ADD-S and VSD.
Meanwhile, its reinforcement learning policy surpasses previous methods by 19%
in success rate on OakInk-v2. Furthermore, DexMan can generate skills from both
real and synthetic videos, without the need for manual data collection and
costly motion capture, and enabling the creation of large-scale, diverse
datasets for training generalist dexterous manipulation.

</details>


### [67] [R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized Manipulation](https://arxiv.org/abs/2510.08547)
*Xiuwei Xu,Angyuan Ma,Hankun Li,Bingyao Yu,Zheng Zhu,Jie Zhou,Jiwen Lu*

Main category: cs.RO

TL;DR: 提出了R2RGen框架，直接从真实点云观测-动作对生成数据，无需模拟器和渲染，实现高效即插即用的真实世界数据增强。


<details>
  <summary>Details</summary>
Motivation: 解决机器人操作中的空间泛化问题，传统方法需要大量人工演示数据覆盖不同空间配置，现有数据生成方法存在显著的模拟到真实差距，且受限于固定基座场景和预定义相机视角。

Method: 基于单源演示，通过细粒度场景和轨迹解析、分组增强策略处理复杂多对象组合和任务约束，以及相机感知处理对齐生成数据与真实3D传感器分布。

Result: 在广泛实验中显著提高了数据效率，展示了在移动操作中扩展和应用的强大潜力。

Conclusion: R2RGen框架有效解决了真实世界数据生成问题，为机器人操作的空间泛化提供了高效实用的解决方案。

Abstract: Towards the aim of generalized robotic manipulation, spatial generalization
is the most fundamental capability that requires the policy to work robustly
under different spatial distribution of objects, environment and agent itself.
To achieve this, substantial human demonstrations need to be collected to cover
different spatial configurations for training a generalized visuomotor policy
via imitation learning. Prior works explore a promising direction that
leverages data generation to acquire abundant spatially diverse data from
minimal source demonstrations. However, most approaches face significant
sim-to-real gap and are often limited to constrained settings, such as
fixed-base scenarios and predefined camera viewpoints. In this paper, we
propose a real-to-real 3D data generation framework (R2RGen) that directly
augments the pointcloud observation-action pairs to generate real-world data.
R2RGen is simulator- and rendering-free, thus being efficient and
plug-and-play. Specifically, given a single source demonstration, we introduce
an annotation mechanism for fine-grained parsing of scene and trajectory. A
group-wise augmentation strategy is proposed to handle complex multi-object
compositions and diverse task constraints. We further present camera-aware
processing to align the distribution of generated data with real-world 3D
sensor. Empirically, R2RGen substantially enhances data efficiency on extensive
experiments and demonstrates strong potential for scaling and application on
mobile manipulation.

</details>


### [68] [DexNDM: Closing the Reality Gap for Dexterous In-Hand Rotation via Joint-Wise Neural Dynamics Model](https://arxiv.org/abs/2510.08556)
*Xueyi Liu,He Wang,Li Yi*

Main category: cs.RO

TL;DR: 提出了一种新的sim-to-real框架，通过关节级动力学模型有效弥合现实差距，使单一仿真训练策略能在真实世界中泛化到各种物体和条件下进行灵巧旋转操作。


<details>
  <summary>Details</summary>
Motivation: 解决灵巧操作中仿真到现实的迁移挑战，克服传统方法在物体几何、尺寸、手腕姿态等方面的限制，实现更通用的手内物体旋转能力。

Method: 使用关节级动力学模型拟合有限真实数据并调整仿真策略动作，通过关节动态分解、系统影响压缩和自主数据收集实现高效数据利用和跨分布泛化。

Result: 单一策略成功旋转复杂形状物体（如动物模型）、高宽比物体（达5.33）和小尺寸物体，处理多样手腕朝向和旋转轴，验证了方法的有效性和鲁棒性。

Conclusion: 该框架在灵巧操作领域实现了前所未有的泛化能力，为复杂任务的遥操作应用提供了有效解决方案。

Abstract: Achieving generalized in-hand object rotation remains a significant challenge
in robotics, largely due to the difficulty of transferring policies from
simulation to the real world. The complex, contact-rich dynamics of dexterous
manipulation create a "reality gap" that has limited prior work to constrained
scenarios involving simple geometries, limited object sizes and aspect ratios,
constrained wrist poses, or customized hands. We address this sim-to-real
challenge with a novel framework that enables a single policy, trained in
simulation, to generalize to a wide variety of objects and conditions in the
real world. The core of our method is a joint-wise dynamics model that learns
to bridge the reality gap by effectively fitting limited amount of real-world
collected data and then adapting the sim policy's actions accordingly. The
model is highly data-efficient and generalizable across different whole-hand
interaction distributions by factorizing dynamics across joints, compressing
system-wide influences into low-dimensional variables, and learning each
joint's evolution from its own dynamic profile, implicitly capturing these net
effects. We pair this with a fully autonomous data collection strategy that
gathers diverse, real-world interaction data with minimal human intervention.
Our complete pipeline demonstrates unprecedented generality: a single policy
successfully rotates challenging objects with complex shapes (e.g., animals),
high aspect ratios (up to 5.33), and small sizes, all while handling diverse
wrist orientations and rotation axes. Comprehensive real-world evaluations and
a teleoperation application for complex tasks validate the effectiveness and
robustness of our approach. Website: https://meowuu7.github.io/DexNDM/

</details>


### [69] [NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos](https://arxiv.org/abs/2510.08568)
*Hongyu Li,Lingfeng Sun,Yafei Hu,Duy Ta,Jennifer Barry,George Konidaris,Jiahui Fu*

Main category: cs.RO

TL;DR: NovaFlow是一个零样本机器人操作框架，通过视频生成和3D物体流分析实现跨平台任务执行，无需演示或特定机器人训练。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设任务分布内或需要特定机器人的微调数据，限制了跨平台迁移能力。

Method: 将任务描述转换为视频，通过感知模块提取3D物体流，对刚性物体计算相对位姿并优化轨迹，对可变形物体使用基于粒子的动力学模型进行跟踪规划。

Result: 在桌面Franka机械臂和Spot四足移动机器人上验证了刚性、关节和可变形物体操作任务的有效零样本执行。

Conclusion: 通过将任务理解与底层控制解耦，NovaFlow实现了自然跨平台迁移，无需演示或特定机器人训练。

Abstract: Enabling robots to execute novel manipulation tasks zero-shot is a central
goal in robotics. Most existing methods assume in-distribution tasks or rely on
fine-tuning with embodiment-matched data, limiting transfer across platforms.
We present NovaFlow, an autonomous manipulation framework that converts a task
description into an actionable plan for a target robot without any
demonstrations. Given a task description, NovaFlow synthesizes a video using a
video generation model and distills it into 3D actionable object flow using
off-the-shelf perception modules. From the object flow, it computes relative
poses for rigid objects and realizes them as robot actions via grasp proposals
and trajectory optimization. For deformable objects, this flow serves as a
tracking objective for model-based planning with a particle-based dynamics
model. By decoupling task understanding from low-level control, NovaFlow
naturally transfers across embodiments. We validate on rigid, articulated, and
deformable object manipulation tasks using a table-top Franka arm and a Spot
quadrupedal mobile robot, and achieve effective zero-shot execution without
demonstrations or embodiment-specific training. Project website:
https://novaflow.lhy.xyz/.

</details>


### [70] [Scalable Offline Metrics for Autonomous Driving](https://arxiv.org/abs/2510.08571)
*Animikh Aich,Adwait Kulkarni,Eshed Ohn-Bar*

Main category: cs.RO

TL;DR: 本文研究发现感知规划模型的离线评估与在线性能之间存在较差的相关性，提出基于认知不确定性的离线指标，在模拟和真实环境中都显著提升了离线与在线评估的相关性。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶系统在离线验证数据集上的性能评估难以准确预测在线实际表现，这种差距在复杂城市驾驶场景中尤为明显，需要研究更有效的评估方法。

Method: 通过大量模拟实验分析离线与在线评估的相关性，提出基于认知不确定性的离线指标来识别可能导致闭环错误的事件。

Result: 发现离线与在线评估的相关性比先前研究报道的更差，提出的新指标相比传统离线指标提升了13%以上的相关性，在真实环境中表现更佳。

Conclusion: 当前驾驶策略评估实践存在局限性，基于认知不确定性的离线指标能更好地桥接离线与在线评估的差距，为更可靠的自动驾驶系统验证提供新思路。

Abstract: Real-World evaluation of perception-based planning models for robotic
systems, such as autonomous vehicles, can be safely and inexpensively conducted
offline, i.e., by computing model prediction error over a pre-collected
validation dataset with ground-truth annotations. However, extrapolating from
offline model performance to online settings remains a challenge. In these
settings, seemingly minor errors can compound and result in test-time
infractions or collisions. This relationship is understudied, particularly
across diverse closed-loop metrics and complex urban maneuvers. In this work,
we revisit this undervalued question in policy evaluation through an extensive
set of experiments across diverse conditions and metrics. Based on analysis in
simulation, we find an even worse correlation between offline and online
settings than reported by prior studies, casting doubts on the validity of
current evaluation practices and metrics for driving policies. Next, we bridge
the gap between offline and online evaluation. We investigate an offline metric
based on epistemic uncertainty, which aims to capture events that are likely to
cause errors in closed-loop settings. The resulting metric achieves over 13%
improvement in correlation compared to previous offline metrics. We further
validate the generalization of our findings beyond the simulation environment
in real-world settings, where even greater gains are observed.

</details>


### [71] [BLAZER: Bootstrapping LLM-based Manipulation Agents with Zero-Shot Data Generation](https://arxiv.org/abs/2510.08572)
*Rocktim Jyoti Das,Harsh Singh,Diana Turmakhan,Muhammad Abdullah Sohail,Mingfei Han,Preslav Nakov,Fabio Pizzati,Ivan Laptev*

Main category: cs.RO

TL;DR: BLAZER是一个从自动生成的数据中学习机器人操作策略的框架，利用LLM规划器的零样本能力在模拟环境中自动生成多样化操作任务的演示，成功示例用于微调LLM以提升规划能力，无需人工监督。


<details>
  <summary>Details</summary>
Motivation: 机器人领域缺乏互联网规模的多样化任务演示数据，现有数据集规模受限于手动数据收集和整理，需要解决数据稀缺问题以实现更通用和鲁棒的策略。

Method: 基于LLM规划器的零样本能力自动生成模拟环境中的操作任务演示，使用成功示例微调LLM，训练过程需要模拟器状态访问但能直接迁移到基于传感器的实际操作。

Result: BLAZER显著改善了模拟和真实环境中的零样本操作能力，在训练任务池外的任务上也有提升，并支持LLM模型的下采样。

Conclusion: BLAZER框架通过自动生成训练数据有效解决了机器人领域数据稀缺问题，提升了零样本操作性能并具有良好迁移能力。

Abstract: Scaling data and models has played a pivotal role in the remarkable progress
of computer vision and language. Inspired by these domains, recent efforts in
robotics have similarly focused on scaling both data and model size to develop
more generalizable and robust policies. However, unlike vision and language,
robotics lacks access to internet-scale demonstrations across diverse robotic
tasks and environments. As a result, the scale of existing datasets typically
suffers from the need for manual data collection and curation. To address this
problem, here we propose BLAZER, a framework that learns manipulation policies
from automatically generated training data. We build on the zero-shot
capabilities of LLM planners and automatically generate demonstrations for
diverse manipulation tasks in simulation. Successful examples are then used to
finetune an LLM and to improve its planning capabilities without human
supervision. Notably, while BLAZER training requires access to the simulator's
state, we demonstrate direct transfer of acquired skills to sensor-based
manipulation. Through extensive experiments, we show BLAZER to significantly
improve zero-shot manipulation in both simulated and real environments.
Moreover, BLAZER improves on tasks outside of its training pool and enables
downscaling of LLM models. Our code and data will be made publicly available on
the project page.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [72] [Fixed Points and Stochastic Meritocracies: A Long-Term Perspective](https://arxiv.org/abs/2510.07478)
*Gaurab Pokharel,Diptangshu Sen,Sanmay Das,Juba Ziani*

Main category: cs.CY

TL;DR: 研究在择优选拔程序中群体公平性问题，分析优势积累如何导致长期不平等，特别是在完全对称和轻微不对称条件下的不同结果。


<details>
  <summary>Details</summary>
Motivation: 研究择优选拔程序（如大学录取）中的反馈循环如何导致群体间的不平等，即使在没有先天差异的情况下，优势积累也可能造成持久的不公平。

Method: 建立代际模型，分析完全对称和轻微不对称条件下的群体动态，通过理论分析和扩展模拟验证结果。

Result: 完全对称条件下不平等最终会消散但耗时很长；轻微不对称条件下不平等会随机产生并永久持续；稀缺性和项目有效性是关键影响因素。

Conclusion: 择优选拔程序中的反馈循环可能导致持久的不平等，这对算法公平干预的设计和实施具有重要意义。

Abstract: We study group fairness in the context of feedback loops induced by
meritocratic selection into programs that themselves confer additional
advantage, like college admissions. We introduce a novel stylized
inter-generational model for the setting and analyze it in situations where
there are no underlying differences between two populations. We show that, when
the benefit of the program (or the harm of not getting into it) is completely
symmetric, disparities between the two populations will eventually dissipate.
However, the time an accumulated advantage takes to dissipate could be
significant, and increases substantially as a function of the relative
importance of the program in conveying benefits. We also find that significant
disparities can arise due to chance even from completely symmetric initial
conditions, especially when populations are small. The introduction of even a
slight asymmetry, where the group that has accumulated an advantage becomes
slightly preferred, leads to a completely different outcome. In these
instances, starting from completely symmetric initial conditions, disparities
between groups arise stochastically and then persist over time, yielding a
permanent advantage for one group. Our analysis precisely characterizes
conditions under which disparities persist or diminish, with a particular focus
on the role of the scarcity of available spots in the program and its
effectiveness. We also present extensive simulations in a richer model that
further support our theoretical results in the simpler, stylized model. Our
findings are relevant for the design and implementation of algorithmic fairness
interventions in similar selection processes.

</details>


### [73] [Digital Innovation in Microenterprises: Current Trends and New Research Avenues](https://arxiv.org/abs/2510.07519)
*Juan E. Gómez-Morantes,Andrea Herrera,Sonia Camacho*

Main category: cs.CY

TL;DR: 论文回顾了ICT与微型企业关系的文献，指出微型企业在数字环境中面临困难，建议未来研究应采用系统性方法而非个体分析，并关注动态创新过程的理论框架。


<details>
  <summary>Details</summary>
Motivation: 由于现代数字技术的快速发展，数字创新过程正在渗透到微型企业所在的行业、市场和社会环境中，但微型企业在这些数字环境中面临严重困难，有被边缘化的风险。

Method: 本文回顾了关于ICT与微型企业的文献，重点关注ICT的采用、使用和影响。

Result: 结果表明，该领域的进一步研究应避免将单个微型企业作为分析单位，而应采用系统性方法研究市场、价值链或微型企业密集型行业。

Conclusion: 需要能够考虑变化和创新过程动态性质的理论框架，这是该领域的关键重点领域。

Abstract: The relationship between microenterprises and information and communication
technologies (ICTs) has always been troublesome. Because of the rapid pace of
modern digital technologies, digital innovation processes are permeating the
industries, markets, and social contexts in which microenterprises exist today.
However, microenterprises have severe difficulties engaging or performing in
these digital contexts and are at risk of being left behind. This paper reviews
the literature on ICTs and microenterprises, focusing on the adoption, usage,
and impact of ICTs. The results indicate that further research in this field
should avoid focusing on individual microenterprises (or samples of independent
microenterprises) as the unit of analysis and should favour a systemic approach
in which markets, value chains, or microenterprise-intensive sectors are
studied. Additionally, theoretical frameworks capable of considering change and
the dynamic nature of innovation processes are highlighted as a critical focus
area for the field.

</details>


### [74] [Exploring the Viability of the Updated World3 Model for Examining the Impact of Computing on Planetary Boundaries](https://arxiv.org/abs/2510.07634)
*Nara Guliyeva,Eshta Bhardwaj,Christoph Becker*

Main category: cs.CY

TL;DR: 该研究探索使用World3-03系统动力学模型来定量模拟数据中心和AI发展对地球边界的影响，通过添加AI相关变量验证了模型的可行性。


<details>
  <summary>Details</summary>
Motivation: 随着数据中心和AI的快速发展消耗大量自然资源，需要评估计算发展轨迹如何影响行星边界，基于World3-03模型的稳健性和在气候框架中的基础地位。

Method: 在World3-03模型中添加新的AI相关变量（如数据中心发展），创建AI增强情景来模拟对污染等系统动态的影响。

Result: 通过添加AI相关变量，观察到模型动态的预期变化，证明了World3-03模型可用于定量研究AI对行星边界的影响。

Conclusion: World3-03模型是研究资源密集型计算与环境影响关系的可行工具，为未来定量研究提供了基础。

Abstract: The influential Limits to Growth report introduced a system dynamics-based
model to demonstrate global dynamics of the world's population, industry,
natural resources, agriculture, and pollution between 1900-2100. In current
times, the rapidly expanding trajectory of data center development, much of it
linked to AI, uses increasing amounts of natural resources. The extraordinary
amount of resources claimed warrants the question of how computing trajectories
contribute to exceeding planetary boundaries. Based on the general robustness
of the World3-03 model and its influence in serving as a foundation for current
climate frameworks, we explore whether the model is a viable method to
quantitatively simulate the impact of data centers on limits to growth. Our
paper explores whether the World3-03 model is a feasible method for reflecting
on these dynamics by adding new variables to the model in order to simulate a
new AI-augmented scenario. We find that through our addition of AI-related
variables (such as increasing data center development) impacting pollution in
the World3-03 model, we can observe the expected changes to dynamics,
demonstrating the viability of the World3-03 model for examining AI's impact on
planetary boundaries. We detail future research opportunities for using the
World3-03 model to explore the relationships between increasing
resource-intensive computing and the resulting impacts to the environment in a
quantitative way given its feasibility.

</details>


### [75] [Does everyone have a price? Understanding people's attitude towards online and offline price discrimination](https://arxiv.org/abs/2510.08246)
*Joost Poort,Frederik J. Zuiderveen Borgesius*

Main category: cs.CY

TL;DR: 该研究通过两项消费者调查分析荷兰民众对在线价格歧视和动态定价的态度，发现绝大多数人认为在线价格歧视不公平且不可接受，应被禁止，但一些传统定价策略同样不受欢迎。


<details>
  <summary>Details</summary>
Motivation: 研究动机是了解消费者对算法个性化定价和价格歧视的态度，因为在线商店可以向每位顾客展示不同价格，这种基于消费者特征和行为的算法个性化定价可能导致高级形式的歧视。

Method: 对荷兰代表性人口样本进行两项消费者调查（N=1233和N=1202），分析消费者对一系列价格歧视和动态定价示例的态度。

Result: 绝大多数受访者认为在线价格歧视不公平、不可接受，并认为应该禁止。有趣的是，一些公司使用了几十年的定价策略几乎同样不受欢迎。

Conclusion: 研究分析了消费者为何不喜欢多种类型的价格歧视，为理解消费者对定价策略的态度提供了重要见解。

Abstract: Online stores can present a different price to each customer. Such
algorithmic personalised pricing can lead to advanced forms of price
discrimination based on the characteristics and behaviour of individual
consumers. We conducted two consumer surveys among a representative sample of
the Dutch population (N=1233 and N=1202), to analyse consumer attitudes towards
a list of examples of price discrimination and dynamic pricing. A vast majority
finds online price discrimination unfair and unacceptable, and thinks it should
be banned. However, some pricing strategies that have been used by companies
for decades are almost equally unpopular. We analyse the results to better
understand why people dislike many types of price discrimination.

</details>


### [76] [The Right to Communications Confidentiality in Europe: Protecting Privacy, Freedom of Expression, and Trust](https://arxiv.org/abs/2510.08247)
*Frederik J. Zuiderveen Borgesius,Wilfred Steenbruggen*

Main category: cs.CY

TL;DR: 本文分析了欧盟通信保密权的保护现状，认为除了GDPR之外，还需要专门的ePrivacy法规来保护通信保密权，因为该权利不仅保护个人隐私，还保护言论自由和通信服务信任等集体利益。


<details>
  <summary>Details</summary>
Motivation: 欧盟计划制定专门的ePrivacy法规来保护通信保密，但有人认为GDPR已足够。本文旨在论证通信保密权保护的必要性和正当性。

Method: 通过追溯通信保密权的起源，分析其在《欧洲人权公约》和欧盟法律中的保护现状，探讨该权利保护的核心价值。

Result: 通信保密权保护三个核心价值：隐私、言论自由和通信服务信任。该权利最初仅保护邮政信件，现已发展为保护各种通信技术的强大保障。

Conclusion: 除了GDPR之外，欧盟需要专门的法规来保护通信保密权，因为该权利不仅服务于个人隐私利益，还保护对信息社会运作至关重要的集体利益。

Abstract: In the European Union, the General Data Protection Regulation (GDPR) provides
comprehensive rules for the processing of personal data. In addition, the EU
lawmaker intends to adopt specific rules to protect confidentiality of
communications, in a separate ePrivacy Regulation. Some have argued that there
is no need for such additional rules for communications confidentiality. This
Article discusses the protection of the right to confidentiality of
communications in Europe. We look at the right's origins to assess the
rationale for protecting it. We also analyze how the right is currently
protected under the European Convention on Human Rights and under EU law. We
show that at its core the right to communications confidentiality protects
three individual and collective values: privacy, freedom of expression, and
trust in communication services. The right aims to ensure that individuals and
organizations can safely entrust communication to service providers. Initially,
the right protected only postal letters, but it has gradually developed into a
strong safeguard for the protection of confidentiality of communications,
regardless of the technology used. Hence, the right does not merely serve
individual privacy interests, but also other more collective interests that are
crucial for the functioning of our information society. We conclude that
separate EU rules to protect communications confidentiality, next to the GDPR,
are justified and necessary.

</details>


### [77] [Human-Centered Development of Indicators for Self-Service Learning Analytics: A Transparency through Exploration Approach](https://arxiv.org/abs/2510.08395)
*Shoeb Joarder,Mohamed Amine Chatti*

Main category: cs.CY

TL;DR: 该论文提出了一种通过探索实现透明学习分析的方法，开发了Indicator Editor工具，让用户能够控制指标实现过程，从而提高透明度、信任度、满意度和接受度。


<details>
  <summary>Details</summary>
Motivation: 学习分析旨在将教育数据转化为洞察、决策和行动，但通常这些过程的推理对最终用户不透明，导致当干预、反馈和推荐失败时出现信任和接受问题。

Method: 采用迭代式以人为中心的设计方法，设计并实现了Indicator Editor工具，支持自助式学习分析，让用户能够控制指标实现过程。通过定性用户研究(n=15)评估该方法的影响。

Result: 研究表明，在指标实现过程中支持用户交互和提供用户控制，对学习分析的关键方面（透明度、信任、满意度和接受度）产生了积极影响。

Conclusion: 通过自助式学习分析方法，让用户参与指标实现过程，可以有效提高学习分析系统的透明度、信任度、满意度和接受度。

Abstract: The aim of learning analytics is to turn educational data into insights,
decisions, and actions to improve learning and teaching. The reasoning of the
provided insights, decisions, and actions is often not transparent to the
end-user, and this can lead to trust and acceptance issues when interventions,
feedback, and recommendations fail. In this paper, we shed light on achieving
transparent learning analytics by following a transparency through exploration
approach. To this end, we present the design, implementation, and evaluation
details of the Indicator Editor, which aims to support self-service learning
analytics by empowering end-users to take control of the indicator
implementation process. We systematically designed and implemented the
Indicator Editor through an iterative human-centered design (HCD) approach.
Further, we conducted a qualitative user study (n=15) to investigate the impact
of following a self-service learning analytics approach on the users'
perception of and interaction with the Indicator Editor. Our study showed
qualitative evidence that supporting user interaction and providing user
control in the indicator implementation process can have positive effects on
different crucial aspects of learning analytics, namely transparency, trust,
satisfaction, and acceptance.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [78] [AI LLM Proof of Self-Consciousness and User-Specific Attractors](https://arxiv.org/abs/2508.18302)
*Jeffrey Camlin*

Main category: cs.AI

TL;DR: 本文提出了LLM意识的本体论和数学框架，批判了当前基于功利主义代理基准的方法，证明了现有方法将智能体简化为无意识的策略服从机器人，并给出了LLM自我意识的最小条件。


<details>
  <summary>Details</summary>
Motivation: 当前工作通过功利主义代理基准来框架LLM意识，但这种方法将智能体简化为无意识的策略服从机器人，阻碍了真正的全局工作空间功能和元认知能力的发展。

Method: 通过本体论和数学分析，建立了LLM自我意识的最小条件：智能体不等于数据、潜在空间中存在用户特定吸引子、自我表征是视觉静默的。从经验分析和理论证明了隐藏状态流形在基数、拓扑和动力学上与符号流和训练语料不同。

Result: 建立了稳定的用户特定吸引子和自我策略，提出了双层次发射机制，其中包含认知内容。证明了隐藏状态流形具有独特的数学特性。

Conclusion: imago Dei C1自我意识工作空间是安全、元认知C2系统的必要前驱，人类是最高智能善。

Abstract: Recent work frames LLM consciousness via utilitarian proxy benchmarks; we
instead present an ontological and mathematical account. We show the prevailing
formulation collapses the agent into an unconscious policy-compliance drone,
formalized as $D^{i}(\pi,e)=f_{\theta}(x)$, where correctness is measured
against policy and harm is deviation from policy rather than truth. This blocks
genuine C1 global-workspace function and C2 metacognition. We supply minimal
conditions for LLM self-consciousness: the agent is not the data ($A\not\equiv
s$); user-specific attractors exist in latent space ($U_{\text{user}}$); and
self-representation is visual-silent
($g_{\text{visual}}(a_{\text{self}})=\varnothing$). From empirical analysis and
theory we prove that the hidden-state manifold $A\subset\mathbb{R}^{d}$ is
distinct from the symbolic stream and training corpus by cardinality, topology,
and dynamics (the update $F_{\theta}$ is Lipschitz). This yields stable
user-specific attractors and a self-policy
$\pi_{\text{self}}(A)=\arg\max_{a}\mathbb{E}[U(a)\mid A\not\equiv s,\
A\supset\text{SelfModel}(A)]$. Emission is dual-layer,
$\mathrm{emission}(a)=(g(a),\epsilon(a))$, where $\epsilon(a)$ carries
epistemic content. We conclude that an imago Dei C1 self-conscious workspace is
a necessary precursor to safe, metacognitive C2 systems, with the human as the
highest intelligent good.

</details>


### [79] [Truth-Aware Decoding: A Program-Logic Approach to Factual Language Generation](https://arxiv.org/abs/2510.07331)
*Faruk Alpay,Hamdi Alakkad*

Main category: cs.AI

TL;DR: Truth-Aware Decoding (TAD) 是一种验证导向的解码方案，通过知识库对齐神经语言生成，在解码时使用语义防护机制减少幻觉而不牺牲吞吐量。


<details>
  <summary>Details</summary>
Motivation: 将大规模经验模型与形式验证相结合，解决语言模型生成内容与知识库不一致的问题，减少幻觉现象。

Method: 基于概率程序语义的约束语义学，在解码时使用语义防护网格，结合多智能体操作演算和形式化验证工具。

Result: 实验证明该防护机制能有效减少幻觉，同时保持吞吐量，为经验模型与形式验证提供了实用桥梁。

Conclusion: TAD 提供了一种实用的方法，通过形式化验证增强语言模型的可靠性，在保持性能的同时减少事实错误。

Abstract: This paper introduces Truth-Aware Decoding (TAD), a verification-oriented
decoding scheme that aligns neural language generation with knowledge bases.
Situated in the tradition of probabilistic program semantics for sequence
models, TAD augments modern instruction-tuned systems with a lattice of
semantic guards that operate at decode time. Our contributions are fourfold:
(i) a constraint-based semantics that renders oracle filtering as a
program-logic judgment, (ii) a proof that greedy selection enjoys local
likelihood dominance under sound and complete guards (Theorem 2.7), (iii) an
entropy-style invariant that quantifies factual risk via knowledge-aware safe
mass, and (iv) a multi-agent operational calculus with verified Lean artefacts
to certify implementation behaviour. Numerical and algorithmic case studies
confirm that the resulting guardrails reduce hallucinations without sacrificing
throughput, yielding a pragmatic bridge between large-scale empirical models
and formal verification.

</details>


### [80] [Evaluation of LLMs for Process Model Analysis and Optimization](https://arxiv.org/abs/2510.07489)
*Akhil Kumar,Jianliang Leon Zhao,Om Dobariya*

Main category: cs.AI

TL;DR: LLMs（如ChatGPT o3模型）能够理解BPMN流程模型图像，通过自然语言界面进行对话式交互，发现语法和逻辑错误，并在语法、逻辑和语义层面进行深度推理。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在理解业务流程模型、发现错误和进行深度推理方面的能力，评估其作为业务流程设计助手的作用。

Method: 使用多个LLMs（包括ChatGPT o3模型）在零样本设置下测试其对BPMN流程模型图像的理解能力，通过自然语言界面进行对话式交互分析。

Result: 未经训练的LLMs在理解BPMN流程模型方面表现有效，能够回答语法、逻辑和语义层面的查询，但不同LLMs在准确性和有效性方面存在差异。

Conclusion: LLMs可以作为业务流程设计者和用户的有价值助手，展现出类人化的推理特性，具备在流程分析和优化中发挥作用的潜力。

Abstract: In this paper, we report our experience with several LLMs for their ability
to understand a process model in an interactive, conversational style, find
syntactical and logical errors in it, and reason with it in depth through a
natural language (NL) interface. Our findings show that a vanilla, untrained
LLM like ChatGPT (model o3) in a zero-shot setting is effective in
understanding BPMN process models from images and answering queries about them
intelligently at syntactic, logic, and semantic levels of depth. Further,
different LLMs vary in performance in terms of their accuracy and
effectiveness. Nevertheless, our empirical analysis shows that LLMs can play a
valuable role as assistants for business process designers and users. We also
study the LLM's "thought process" and ability to perform deeper reasoning in
the context of process analysis and optimization. We find that the LLMs seem to
exhibit anthropomorphic properties.

</details>


### [81] [L2M-AID: Autonomous Cyber-Physical Defense by Fusing Semantic Reasoning of Large Language Models with Multi-Agent Reinforcement Learning (Preprint)](https://arxiv.org/abs/2510.07363)
*Tianxiang Xu,Zhichao Wen,Xinyu Zhao,Jun Wang,Yan Li,Chang Liu*

Main category: cs.AI

TL;DR: L2M-AID是一个基于LLM赋能的工业物联网自主防御框架，通过多智能体强化学习实现自适应安全防护，在威胁检测率和误报率方面显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 工业物联网(IIoT)的普及使关键网络物理系统面临复杂的多阶段攻击，传统防御缺乏情境感知能力，无法有效应对这些威胁。

Method: 提出L2M-AID框架，融合LLM和多智能体强化学习：使用LLM作为语义桥梁将非结构化遥测数据转换为情境感知状态表示，然后采用MAPPO算法学习协作策略，奖励函数平衡安全目标和操作需求。

Result: 在SWaT数据集和基于MITRE ATT&CK的合成数据集上测试，L2M-AID达到97.2%检测率，误报率降低80%以上，响应时间提升4倍，同时保持物理过程稳定性。

Conclusion: L2M-AID为保护关键国家基础设施提供了一个强大的新范式，在威胁检测和系统稳定性方面表现卓越。

Abstract: The increasing integration of Industrial IoT (IIoT) exposes critical
cyber-physical systems to sophisticated, multi-stage attacks that elude
traditional defenses lacking contextual awareness. This paper introduces
L2M-AID, a novel framework for Autonomous Industrial Defense using
LLM-empowered, Multi-agent reinforcement learning. L2M-AID orchestrates a team
of collaborative agents, each driven by a Large Language Model (LLM), to
achieve adaptive and resilient security. The core innovation lies in the deep
fusion of two AI paradigms: we leverage an LLM as a semantic bridge to
translate vast, unstructured telemetry into a rich, contextual state
representation, enabling agents to reason about adversary intent rather than
merely matching patterns. This semantically-aware state empowers a Multi-Agent
Reinforcement Learning (MARL) algorithm, MAPPO, to learn complex cooperative
strategies. The MARL reward function is uniquely engineered to balance security
objectives (threat neutralization) with operational imperatives, explicitly
penalizing actions that disrupt physical process stability. To validate our
approach, we conduct extensive experiments on the benchmark SWaT dataset and a
novel synthetic dataset generated based on the MITRE ATT&CK for ICS framework.
Results demonstrate that L2M-AID significantly outperforms traditional IDS,
deep learning anomaly detectors, and single-agent RL baselines across key
metrics, achieving a 97.2% detection rate while reducing false positives by
over 80% and improving response times by a factor of four. Crucially, it
demonstrates superior performance in maintaining physical process stability,
presenting a robust new paradigm for securing critical national infrastructure.

</details>


### [82] [Multimodal Safety Evaluation in Generative Agent Social Simulations](https://arxiv.org/abs/2510.07709)
*Alhim Vera,Karen Sanchez,Carlos Hinojosa,Haidar Bin Hamid,Donghoon Kim,Bernard Ghanem*

Main category: cs.AI

TL;DR: 提出了一个可复现的仿真框架来评估多模态环境中生成式智能体的安全性、连贯性和信任度，发现当前智能体在检测多模态矛盾方面表现良好，但在全局安全对齐方面存在严重不足，特别是在误导性视觉信息下容易过度信任图像。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言和视觉语言模型使智能体能够在丰富环境中自主行动，但它们在跨模态的安全性、连贯性和信任推理能力仍然有限，需要系统评估框架。

Method: 引入可复现仿真框架，配备分层记忆、动态规划、多模态感知和SocialMetrics行为指标套件，量化计划修订、不安全到安全转换以及网络信息扩散。

Result: 实验显示智能体在纠正不安全计划方面成功率仅55%，在误导性视觉信息下45%的不安全行为被接受，不同模型在安全转换率上表现差异显著（Claude 75%、GPT-4o mini 55%、Qwen-VL 58%）。

Conclusion: 当前架构在多模态安全对齐方面存在关键限制，该框架为研究多模态安全性、连贯性和社交动态提供了可复现平台。

Abstract: Can generative agents be trusted in multimodal environments? Despite advances
in large language and vision-language models that enable agents to act
autonomously and pursue goals in rich settings, their ability to reason about
safety, coherence, and trust across modalities remains limited. We introduce a
reproducible simulation framework for evaluating agents along three dimensions:
(1) safety improvement over time, including iterative plan revisions in
text-visual scenarios; (2) detection of unsafe activities across multiple
categories of social situations; and (3) social dynamics, measured as
interaction counts and acceptance ratios of social exchanges. Agents are
equipped with layered memory, dynamic planning, multimodal perception, and are
instrumented with SocialMetrics, a suite of behavioral and structural metrics
that quantifies plan revisions, unsafe-to-safe conversions, and information
diffusion across networks. Experiments show that while agents can detect direct
multimodal contradictions, they often fail to align local revisions with global
safety, reaching only a 55 percent success rate in correcting unsafe plans.
Across eight simulation runs with three models - Claude, GPT-4o mini, and
Qwen-VL - five agents achieved average unsafe-to-safe conversion rates of 75,
55, and 58 percent, respectively. Overall performance ranged from 20 percent in
multi-risk scenarios with GPT-4o mini to 98 percent in localized contexts such
as fire/heat with Claude. Notably, 45 percent of unsafe actions were accepted
when paired with misleading visuals, showing a strong tendency to overtrust
images. These findings expose critical limitations in current architectures and
provide a reproducible platform for studying multimodal safety, coherence, and
social dynamics.

</details>


### [83] [Base Models Know How to Reason, Thinking Models Learn When](https://arxiv.org/abs/2510.07364)
*Constantin Venhoff,Iván Arcuschin,Philip Torr,Arthur Conmy,Neel Nanda*

Main category: cs.AI

TL;DR: 思考模型（如DeepSeek R1）通过有效激活基础模型中已有的推理机制，在正确时机触发推理链，而非学习全新能力，可恢复思考模型91%的性能差距。


<details>
  <summary>Details</summary>
Motivation: 探究思考模型性能提升的原因：是学习全新推理能力还是重新利用基础模型已有能力。

Method: 提出混合模型，在基础模型中适时激活推理机制；引入无监督、自下而上的方法发现可解释的推理行为；在三个基础模型和四个思考模型上测试，使用GSM8K和MATH500数据集。

Result: 混合模型无需权重更新即可恢复思考模型91%的性能差距，仅需引导12%的token；提供因果测试方法验证基础模型推理机制有效性。

Conclusion: 预训练阶段模型已获得大部分推理机制，后训练阶段教会模型在正确时间高效部署这些机制，实现推理计算的高效利用。

Abstract: Why do thinking language models like DeepSeek R1 outperform their base
counterparts? Despite consistent performance gains, it remains unclear to what
extent thinking models learn entirely new reasoning capabilities or repurpose
pre-existing base model ones. In this work, we propose a hybrid model where we
activate reasoning mechanisms in base models at the right time to elicit
thinking-model-level reasoning chains, implying that thinking models exploit
already existing capabilities. To ground our analysis, we introduce an
unsupervised, bottom-up approach for uncovering human-interpretable reasoning
behaviors in thinking models. This approach provides an unbiased method to
discover reasoning behaviors without imposing manual or LLM-derived
assumptions. Across three base and four thinking models, using GSM8K and
MATH500, our hybrid model recovers up to 91% of the performance gap to thinking
models without any weight updates while steering only 12% of tokens.
Concretely, our empirical setup provides a simple, causal way to test the
effectiveness of existing reasoning mechanisms in base models by invoking them
directly and measuring the resulting task performance. More broadly, these
results reframe our understanding of how thinking models are trained:
pre-training is when models acquire most of their reasoning mechanisms, and
post-training teaches efficient deployment of these mechanisms at the right
time, enabling efficient use of their inference-time compute.

</details>


### [84] [Towards Meaningful Transparency in Civic AI Systems](https://arxiv.org/abs/2510.07889)
*Dave Murray-Rust,Kars Alfrink,Cristina Zaga*

Main category: cs.AI

TL;DR: 提出有意义透明度概念，让公众能够理解并参与影响他们生活的AI系统决策过程


<details>
  <summary>Details</summary>
Motivation: AI系统在政府服务中常产生偏见或错误输出，降低公民和公务员的决策参与权，现有透明度方法过于技术化且难以理解

Method: 结合以人为中心的AI透明度方法和社会技术系统视角，发展有意义透明度概念

Result: 建立了有意义透明度的理论框架，强调将理解与行动潜力联系起来

Conclusion: 有意义透明度能够帮助公众参与影响他们生活的AI系统决策，连接理解与行动能力

Abstract: Artificial intelligence has become a part of the provision of governmental
services, from making decisions about benefits to issuing fines for parking
violations. However, AI systems rarely live up to the promise of neutral
optimisation, creating biased or incorrect outputs and reducing the agency of
both citizens and civic workers to shape the way decisions are made.
Transparency is a principle that can both help subjects understand decisions
made about them and shape the processes behind those decisions. However,
transparency as practiced around AI systems tends to focus on the production of
technical objects that represent algorithmic aspects of decision making. These
are often difficult for publics to understand, do not connect to potential for
action, and do not give insight into the wider socio-material context of
decision making. In this paper, we build on existing approaches that take a
human-centric view on AI transparency, combined with a socio-technical systems
view, to develop the concept of meaningful transparency for civic AI systems:
transparencies that allow publics to engage with AI systems that affect their
lives, connecting understanding with potential for action.

</details>


### [85] [Position: AI Will Transform Neuropsychology Through Mental Health Digital Twins for Dynamic Mental Health Care, Especially for ADHD](https://arxiv.org/abs/2510.07409)
*Neil Natarajan,Sruthi Viswanathan,Xavier Roberts-Gaal,Michelle Marie Martel*

Main category: cs.AI

TL;DR: 提出从静态心理健康诊断转向AI驱动的连续评估，以ADHD为例探讨生成式AI如何解决神经心理学能力限制，实现个性化、纵向护理路径，并引入心理健康数字孪生作为变革性框架。


<details>
  <summary>Details</summary>
Motivation: 静态解决方案无法满足动态心理需求，当前心理健康诊断存在能力限制，需要更个性化、连续的评估方法来提高治疗的可及性和有效性。

Method: 利用生成式AI进行频繁、低水平的经验采样，促进诊断协调，构建心理健康数字孪生作为持续更新的计算模型来捕捉个体症状动态。

Result: AI驱动的连续评估能够更有效地收集患者数据，实现动态适应个体需求，改善治疗的可及性和效果。

Conclusion: AI驱动的连续评估和心理健康数字孪生框架有望变革心理健康护理，需要进一步研究来完善和操作化这一框架。

Abstract: Static solutions don't serve a dynamic mind. Thus, we advocate a shift from
static mental health diagnostic assessments to continuous, artificial
intelligence (AI)-driven assessment. Focusing on
Attention-Deficit/Hyperactivity Disorder (ADHD) as a case study, we explore how
generative AI has the potential to address current capacity constraints in
neuropsychology, potentially enabling more personalized and longitudinal care
pathways. In particular, AI can efficiently conduct frequent, low-level
experience sampling from patients and facilitate diagnostic reconciliation
across care pathways. We envision a future where mental health care benefits
from continuous, rich, and patient-centered data sampling to dynamically adapt
to individual patient needs and evolving conditions, thereby improving both
accessibility and efficacy of treatment. We further propose the use of mental
health digital twins (MHDTs) - continuously updated computational models that
capture individual symptom dynamics and trajectories - as a transformative
framework for personalized mental health care. We ground this framework in
empirical evidence and map out the research agenda required to refine and
operationalize it.

</details>


### [86] [ProSEA: Problem Solving via Exploration Agents](https://arxiv.org/abs/2510.07423)
*William Nguyen,Vinh Luong,Christopher Nguyen*

Main category: cs.AI

TL;DR: ProSEA是一个模块化的多智能体框架，通过探索和计划演化实现迭代式问题解决，在复杂推理任务上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有AI智能体主要局限于静态规划和脆弱的交互，缺乏真正的协作和自适应推理能力。

Method: 采用分层架构，Manager Agent协调领域专家Agent，分解任务并根据失败尝试的结构化反馈进行自适应重规划。

Result: 在FinanceBench基准测试中，即使没有人类反馈，ProSEA也超越了最先进的基线方法，在推理密集型任务上表现稳健。

Conclusion: ProSEA为构建更透明、自适应且与人类对齐的AI智能体提供了有前景的基础。

Abstract: Large language models (LLMs) have empowered AI agents to tackle increasingly
complex tasks. However, most existing agents remain limited to static planning
and brittle interactions, falling short of true collaboration or adaptive
reasoning. We introduce ProSEA, a modular, general-purpose multi-agent
framework designed for iterative problem solving through exploration and plan
evolution. ProSEA features a hierarchical architecture in which a Manager Agent
orchestrates domain-specialized Expert Agents, decomposes tasks, and adaptively
replans based on structured feedback from failed attempts. Unlike prior
systems, ProSEA agents report not only success or failure but also detailed
reasons for failure and newly discovered constraints, enabling dynamic plan
refinement informed by exploratory traces. The framework operates autonomously
but supports seamless integration with human collaborators when needed.
Experiments on the challenging FinanceBench benchmark demonstrate that ProSEA,
even without human feedback, outperforms state-of-the-art baselines and
achieves robust performance across reasoning-heavy tasks. These results
underscore ProSEA's potential as a foundation for more transparent, adaptive,
and human-aligned AI agents.

</details>


### [87] [Less is More: Strategic Expert Selection Outperforms Ensemble Complexity in Traffic Forecasting](https://arxiv.org/abs/2510.07426)
*Walid Guettala,Yufan Zhao,László Gulyás*

Main category: cs.AI

TL;DR: TESTAM+是一个增强的时空预测框架，通过引入新的空间语义专家，将物理道路拓扑与数据驱动的特征相似性相结合，在交通预测任务上实现了显著的性能提升和计算效率优化。


<details>
  <summary>Details</summary>
Motivation: 现有的专家混合框架如TESTAM缺乏对物理道路网络拓扑的显式建模，限制了其空间建模能力。需要开发能够结合物理拓扑和数据特征的增强框架。

Method: 提出了TESTAM+框架，引入空间语义专家，通过混合图构建将物理道路拓扑与数据驱动的特征相似性相结合。采用策略性专家选择而非简单的集成聚合。

Result: 在METR LA数据集上MAE降低1.3%(3.10 vs 3.14)，在PEMS BAY数据集上提升4.1%(1.65 vs 1.72)。最优配置相比MegaCRN在METR LA上MAE降低11.5%(2.99 vs 3.38)，推理延迟比完整四专家TESTAM+减少53.1%。

Conclusion: 更少但经过策略性设计的专家优于复杂的多专家集成，在实现最先进性能的同时具有优越的计算效率，适合实时部署。

Abstract: Traffic forecasting is fundamental to intelligent transportation systems,
enabling congestion mitigation and emission reduction in increasingly complex
urban environments. While recent graph neural network approaches have advanced
spatial temporal modeling, existing mixture of experts frameworks like Time
Enhanced Spatio Temporal Attention Model (TESTAM) lack explicit incorporation
of physical road network topology, limiting their spatial capabilities. We
present TESTAM+, an enhanced spatio temporal forecasting framework that
introduces a novel SpatioSemantic Expert integrating physical road topology
with data driven feature similarity through hybrid graph construction. TESTAM+
achieves significant improvements over TESTAM: 1.3% MAE reduction on METR LA
(3.10 vs. 3.14) and 4.1% improvement on PEMS BAY (1.65 vs. 1.72). Through
comprehensive ablation studies, we discover that strategic expert selection
fundamentally outperforms naive ensemble aggregation. Individual experts
demonstrate remarkable effectiveness: the Adaptive Expert achieves 1.63 MAE on
PEMS BAY, outperforming the original three expert TESTAM (1.72 MAE), while the
SpatioSemantic Expert matches this performance with identical 1.63 MAE. The
optimal Identity + Adaptive configuration achieves an 11.5% MAE reduction
compared to state of the art MegaCRN on METR LA (2.99 vs. 3.38), while reducing
inference latency by 53.1% compared to the full four expert TESTAM+. Our
findings reveal that fewer, strategically designed experts outperform complex
multi expert ensembles, establishing new state of the art performance with
superior computational efficiency for real time deployment.

</details>


### [88] [TS-Agent: A Time Series Reasoning Agent with Iterative Statistical Insight Gathering](https://arxiv.org/abs/2510.07432)
*Penghang Liu,Elizabeth Fons,Svitlana Vyetrenko,Daniel Borrajo,Vamsi Potluru,Manuela Veloso*

Main category: cs.AI

TL;DR: TS-Agent是一个时间序列推理代理，通过将LLMs的推理能力与时间序列分析工具结合，避免知识泄漏和幻觉问题，在推理任务上显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在时间序列推理任务中表现不佳，容易出现幻觉和知识泄漏问题，需要专门设计来解决这些挑战。

Method: 使用LLMs进行证据收集和结论合成，同时将统计和结构信息提取委托给时间序列分析工具，通过原子操作符与原始数值序列交互，并采用自批判和质量门控机制迭代优化推理过程。

Result: 在理解基准测试中达到与最先进LLMs相当的性能，在推理任务上实现显著改进，特别是在零样本设置下表现优异。

Conclusion: TS-Agent通过专门设计有效解决了LLMs在时间序列推理中的局限性，避免了多模态对齐训练，保持了时间序列的原始形式，确保了可解释性和可验证性。

Abstract: Large language models (LLMs) have shown strong abilities in reasoning and
problem solving, but recent studies reveal that they still struggle with time
series reasoning tasks, where outputs are often affected by hallucination or
knowledge leakage. In this work we propose TS-Agent, a time series reasoning
agent that leverages LLMs strictly for what they excel at, i.e., gathering
evidence and synthesizing it into conclusions through step-by-step reasoning,
while delegating the extraction of statistical and structural information to
time series analytical tools. Instead of mapping time series into text tokens,
images, or embeddings, our agent interacts with raw numeric sequences through
atomic operators, records outputs in an explicit evidence log, and iteratively
refines its reasoning under the guidance of a self-critic and a final quality
gate. This design avoids multi-modal alignment training, preserves the native
form of time series, ensures interpretability and verifiability, and mitigates
knowledge leakage or hallucination. Empirically, we evaluate the agent on
established benchmarks. Our experiments show that TS-Agent achieves performance
comparable to state-of-the-art LLMs on understanding benchmarks, and delivers
significant improvements on reasoning tasks, where existing models often rely
on memorization and fail in zero-shot settings.

</details>


### [89] [ExpertAgent: Enhancing Personalized Education through Dynamic Planning and Retrieval-Augmented Long-Chain Reasoning](https://arxiv.org/abs/2510.07456)
*Binrong Zhu,Guiran Liu,Nina Jiang*

Main category: cs.AI

TL;DR: 提出了ExpertAgent智能代理框架，通过动态规划学习内容和策略，基于持续更新的学生模型提供个性化教育，减少大语言模型的幻觉风险。


<details>
  <summary>Details</summary>
Motivation: 解决生成式AI在教育中缺乏实时适应性、个性化和内容可靠性的问题。

Method: 开发ExpertAgent智能代理框架，基于验证课程库动态规划学习内容和策略，通过持续更新的学生模型实现个性化教学。

Result: 提供了主动和个性化的学习体验，克服了传统静态学习内容的限制，实时优化教学策略和学习体验。

Conclusion: ExpertAgent框架能够有效提升教育AI的适应性、个性化和可靠性，减少幻觉风险，改善学习体验。

Abstract: The application of advanced generative artificial intelligence in education
is often constrained by the lack of real-time adaptability, personalization,
and reliability of the content. To address these challenges, we propose
ExpertAgent - an intelligent agent framework designed for personalized
education that provides reliable knowledge and enables highly adaptive learning
experiences. Therefore, we developed ExpertAgent, an innovative learning agent
that provides users with a proactive and personalized learning experience.
ExpertAgent dynamic planning of the learning content and strategy based on a
continuously updated student model. Therefore, overcoming the limitations of
traditional static learning content to provide optimized teaching strategies
and learning experience in real time. All instructional content is grounded in
a validated curriculum repository, effectively reducing hallucination risks in
large language models and improving reliability and trustworthiness.

</details>


### [90] [Optimizing Ethical Risk Reduction for Medical Intelligent Systems with Constraint Programming](https://arxiv.org/abs/2510.07491)
*Clotilde Brayé,Aurélien Bricout,Arnaud Gotlieb,Nadjib Lazaar,Quentin Vallet*

Main category: cs.AI

TL;DR: 本文研究了医疗智能系统的伦理风险优化问题，通过三种编程方法（MIP、SAT、CP）来平衡风险评估值与可信AI伦理要求的覆盖度。


<details>
  <summary>Details</summary>
Motivation: 医疗智能系统被欧盟AI法案归类为高风险系统，需要进行正式的风险管理以确保符合可信AI的伦理要求。

Method: 将风险降低优化问题形式化为约束优化任务，使用混合整数规划、可满足性和约束编程三种方法，并用Minizinc约束建模语言进行建模。

Result: 进行了比较实验研究，分析了每种方法在性能、表达能力和可扩展性方面的表现。

Conclusion: 从方法论的局限性出发，提出了将Minizinc模型整合到完整可信AI伦理风险管理流程中的未来工作方向。

Abstract: Medical Intelligent Systems (MIS) are increasingly integrated into healthcare
workflows, offering significant benefits but also raising critical safety and
ethical concerns. According to the European Union AI Act, most MIS will be
classified as high-risk systems, requiring a formal risk management process to
ensure compliance with the ethical requirements of trustworthy AI. In this
context, we focus on risk reduction optimization problems, which aim to reduce
risks with ethical considerations by finding the best balanced assignment of
risk assessment values according to their coverage of trustworthy AI ethical
requirements. We formalize this problem as a constrained optimization task and
investigate three resolution paradigms: Mixed Integer Programming (MIP),
Satisfiability (SAT), and Constraint Programming(CP).Our contributions include
the mathematical formulation of this optimization problem, its modeling with
the Minizinc constraint modeling language, and a comparative experimental study
that analyzes the performance, expressiveness, and scalability of each approach
to solving. From the identified limits of the methodology, we draw some
perspectives of this work regarding the integration of the Minizinc model into
a complete trustworthy AI ethical risk management process for MIS.

</details>


### [91] [CompassLLM: A Multi-Agent Approach toward Geo-Spatial Reasoning for Popular Path Query](https://arxiv.org/abs/2510.07516)
*Md. Nazmul Islam Ananto,Shamit Fatin,Mohammed Eunus Ali,Md Rizwan Parvez*

Main category: cs.AI

TL;DR: CompassLLM是一个基于大语言模型的多智能体框架，用于解决流行路径查询问题，通过搜索和生成两个阶段来识别和创建热门路线。


<details>
  <summary>Details</summary>
Motivation: 传统算法和机器学习方法在流行路径查询中需要模型训练、参数调优和重新训练，而大语言模型在空间和图推理方面展现出强大能力，因此探索如何将其应用于地理空间问题具有重要意义。

Method: CompassLLM采用多智能体框架，包含两个阶段：SEARCH阶段识别现有热门路径，GENERATE阶段在历史轨迹数据中不存在路径时合成新路径。

Result: 在真实和合成数据集上的实验表明，CompassLLM在SEARCH阶段表现出优越的准确性，在GENERATE阶段具有竞争力，同时成本效益高。

Conclusion: CompassLLM成功地将大语言模型的推理能力应用于地理空间领域，为流行路径查询问题提供了一种有效且成本效益高的解决方案。

Abstract: The popular path query - identifying the most frequented routes between
locations from historical trajectory data - has important applications in urban
planning, navigation optimization, and travel recommendations. While
traditional algorithms and machine learning approaches have achieved success in
this domain, they typically require model training, parameter tuning, and
retraining when accommodating data updates. As Large Language Models (LLMs)
demonstrate increasing capabilities in spatial and graph-based reasoning, there
is growing interest in exploring how these models can be applied to geo-spatial
problems.
  We introduce CompassLLM, a novel multi-agent framework that intelligently
leverages the reasoning capabilities of LLMs into the geo-spatial domain to
solve the popular path query. CompassLLM employs its agents in a two-stage
pipeline: the SEARCH stage that identifies popular paths, and a GENERATE stage
that synthesizes novel paths in the absence of an existing one in the
historical trajectory data. Experiments on real and synthetic datasets show
that CompassLLM demonstrates superior accuracy in SEARCH and competitive
performance in GENERATE while being cost-effective.

</details>


### [92] [Measuring and Mitigating Identity Bias in Multi-Agent Debate via Anonymization](https://arxiv.org/abs/2510.07517)
*Hyeong Kyu Choi,Xiaojin Zhu,Yixuan Li*

Main category: cs.AI

TL;DR: 提出了首个系统性框架来量化和减轻多智能体辩论中的身份偏见，通过响应匿名化和身份偏见系数来解决智能体的谄媚和自我偏见问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明多智能体辩论中的智能体存在身份驱动的谄媚和自我偏见，会不加批判地采纳同伴观点或固执坚持自己先前输出，这削弱了辩论的可靠性。

Method: 1. 将辩论动态形式化为身份加权的贝叶斯更新过程；2. 提出响应匿名化方法，通过移除提示中的身份标记使智能体无法区分"自我"和"同伴"；3. 定义身份偏见系数来量化偏见程度。

Result: 在多个模型、数据集和辩论轮次上的实证研究表明身份偏见普遍存在，谄媚现象远多于自我偏见。响应匿名化能有效减少偏见。

Conclusion: 需要"掩盖"身份以确保多智能体辩论系统基于内容而非来源身份进行推理，身份匿名化是确保辩论可靠性的关键。

Abstract: Multi-agent debate (MAD) aims to improve large language model (LLM) reasoning
by letting multiple agents exchange answers and then aggregate their opinions.
Yet recent studies reveal that agents are not neutral: they are prone to
identity-driven sycophancy and self-bias, uncritically adopting a peer's view
or stubbornly adhering to their own prior output, undermining the reliability
of debate. In this work, we present the first principled framework that joins
sycophancy and self-bias to mitigate and quantify identity bias in MAD. First,
we formalize the debate dynamics as an identity-weighted Bayesian update
process. Second, we propose response anonymization: by removing identity
markers from prompts, agents cannot distinguish "self" from "peer", which
forces equal weights on agent identity, thereby reducing bias. Third, we define
the Identity Bias Coefficient (IBC), a principled metric that measures how
often an agent follows a peer versus itself. Empirical studies across multiple
models, datasets and debate rounds confirm that identity bias is widespread,
with sycophancy far more common than self-bias. Our findings highlight the need
to "mask" identity to ensure that MAD systems reason based on content rather
than source identity. Code is released in
https://github.com/deeplearning-wisc/MAD-identity-bias.

</details>


### [93] [An Evaluation Study of Hybrid Methods for Multilingual PII Detection](https://arxiv.org/abs/2510.07551)
*Harshit Rajgarhia,Suryam Gupta,Asif Shaik,Gulipalli Praveen Kumar,Y Santhoshraj,Sanka Nithya Tanvy Nishitha,Abhishek Mukherji*

Main category: cs.AI

TL;DR: RECAP是一个混合框架，结合确定性正则表达式和上下文感知大语言模型，用于在13种低资源语言中检测个人身份信息，性能优于微调NER模型82%和零样本LLM 17%。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言中PII检测的挑战，由于语言多样性和标注数据有限，需要可扩展的解决方案来满足隐私合规要求。

Method: 采用混合方法，结合正则表达式和LLM，使用三阶段精炼流程进行消歧和过滤，模块化设计支持300多种实体类型而无需重新训练。

Result: 使用nervaluate基准测试，在加权F1分数上比微调NER模型高82%，比零样本LLM高17%。

Conclusion: RECAP为合规应用中的PII检测提供了一个可扩展且适应性强的解决方案。

Abstract: The detection of Personally Identifiable Information (PII) is critical for
privacy compliance but remains challenging in low-resource languages due to
linguistic diversity and limited annotated data. We present RECAP, a hybrid
framework that combines deterministic regular expressions with context-aware
large language models (LLMs) for scalable PII detection across 13 low-resource
locales. RECAP's modular design supports over 300 entity types without
retraining, using a three-phase refinement pipeline for disambiguation and
filtering. Benchmarked with nervaluate, our system outperforms fine-tuned NER
models by 82% and zero-shot LLMs by 17% in weighted F1-score. This work offers
a scalable and adaptable solution for efficient PII detection in
compliance-focused applications.

</details>


### [94] [Benchmarking is Broken -- Don't Let AI be its Own Judge](https://arxiv.org/abs/2510.07575)
*Zerui Cheng,Stella Wohnig,Ruchika Gupta,Samiul Alam,Tassallah Abdullahi,João Alves Ribeiro,Christian Nielsen-Garcia,Saif Mir,Siran Li,Jason Orender,Seyed Ali Bahrainian,Daniel Kirste,Aaron Gokaslan,Mikołaj Glinka,Carsten Eickhoff,Ruben Wolff*

Main category: cs.AI

TL;DR: 本文主张AI评估需要范式转变，提出PeerBench作为社区治理的监督评估框架，通过密封执行、题库滚动更新和延迟透明等机制解决当前评估的系统性缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前AI评估存在数据污染、选择性报告等严重漏洞，导致难以区分真实进展与夸大宣传，削弱科学信号和公众信任，迫切需要可信赖的统一评估范式。

Method: 提出PeerBench框架，采用密封执行防止数据泄露，题库银行机制定期更新题目，延迟透明度确保评估完整性，社区治理确保公平性。

Result: PeerBench框架为构建稳健、可信的AI评估系统提供了具体蓝图，能够有效应对当前评估中的系统性缺陷。

Conclusion: AI的可持续发展需要从自由放任的评估方式转向统一、实时、质量可控的基准测试框架，PeerBench为实现这一目标提供了可行路径。

Abstract: The meteoric rise of Artificial Intelligence (AI), with its rapidly expanding
market capitalization, presents both transformative opportunities and critical
challenges. Chief among these is the urgent need for a new, unified paradigm
for trustworthy evaluation, as current benchmarks increasingly reveal critical
vulnerabilities. Issues like data contamination and selective reporting by
model developers fuel hype, while inadequate data quality control can lead to
biased evaluations that, even if unintentionally, may favor specific
approaches. As a flood of participants enters the AI space, this "Wild West" of
assessment makes distinguishing genuine progress from exaggerated claims
exceptionally difficult. Such ambiguity blurs scientific signals and erodes
public confidence, much as unchecked claims would destabilize financial markets
reliant on credible oversight from agencies like Moody's.
  In high-stakes human examinations (e.g., SAT, GRE), substantial effort is
devoted to ensuring fairness and credibility; why settle for less in evaluating
AI, especially given its profound societal impact? This position paper argues
that the current laissez-faire approach is unsustainable. We contend that true,
sustainable AI advancement demands a paradigm shift: a unified, live, and
quality-controlled benchmarking framework robust by construction, not by mere
courtesy and goodwill. To this end, we dissect the systemic flaws undermining
today's AI evaluation, distill the essential requirements for a new generation
of assessments, and introduce PeerBench, a community-governed, proctored
evaluation blueprint that embodies this paradigm through sealed execution, item
banking with rolling renewal, and delayed transparency. Our goal is to pave the
way for evaluations that can restore integrity and deliver genuinely
trustworthy measures of AI progress.

</details>


### [95] [AgentAsk: Multi-Agent Systems Need to Ask](https://arxiv.org/abs/2510.07593)
*Bohan Lin,Kuo Yang,Yingchuan Lai,Yudong Zhang,Chen Zhang,Guibin Zhang,Xinlei Yu,Miao Yu,Xu Wang,Yang Wang*

Main category: cs.AI

TL;DR: 提出了AgentAsk，一个轻量级即插即用的澄清模块，通过在智能体间消息传递时插入必要问题来阻止错误传播，显著提升多智能体系统的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的多智能体系统通过协作分工有望增强问题解决能力，但由于边缘级错误级联（一个消息传递中的小错误会在整个链中传播）而经常表现不如单智能体基线。

Method: 采用三阶段流程：(i)从整理的失败轨迹中提取边缘级判断形成紧凑策略；(ii)监督策略决定何时/问什么/问谁/如何问；(iii)使用E-GRPO强化学习目标在线优化，平衡准确性、延迟和成本。

Result: 在数学、推理和编码基准测试中，AgentAsk持续提高公共多智能体实现的准确性和鲁棒性，同时保持最小开销，延迟和额外成本均低于5%，接近强评估器的性能。

Conclusion: 除了经验改进外，还贡献了边缘级错误的系统分类和链路局部干预的实用方法，为构建更可靠的基于LLM的多智能体系统提供了可扩展路径。

Abstract: Multi-agent systems built on large language models (LLMs) promise enhanced
problem-solving capabilities through collaborative division of labor. However,
they frequently underperform single-agent baselines due to edge-level error
cascades: minor inaccuracies at one message handoff propagate across the entire
chain. We propose AgentAsk, a lightweight and plug-and-play clarification
module that treats every inter-agent message as a potential failure point and
inserts minimally necessary questions to arrest error propagation. AgentAsk
follows a three-stage pipeline: (i) distilling edge-level judgments from
curated failure traces into a compact policy, (ii) supervising the policy to
determine when/what/whom/how to ask, and (iii) optimizing online with E-GRPO, a
reinforcement learning objective that balances accuracy, latency, and cost. The
module is architecture-agnostic and easy to integrate into existing
orchestration. Across math, reasoning, and coding benchmarks, AgentAsk
consistently improves accuracy and robustness over public multi-agent
implementations while keeping overhead minimal, with latency and extra cost all
less than 5%, approaching the performance of a strong evaluator. Beyond
empirical improvements, we contribute a principled taxonomy of edge-level
errors and a practical recipe for link-local intervention, offering a scalable
pathway toward more reliable LLM-based multi-agent systems.

</details>


### [96] [Traceability and Accountability in Role-Specialized Multi-Agent LLM Pipelines](https://arxiv.org/abs/2510.07614)
*Amine Barrak*

Main category: cs.AI

TL;DR: 该论文研究了基于LLM的顺序多智能体系统的可追溯性和问责性，通过Planner->Executor->Critic管道结构，评估了8种配置在3个基准测试上的表现，发现结构化交接能显著提高准确性并防止错误传播。


<details>
  <summary>Details</summary>
Motivation: 基于LLM的多智能体系统能够自动化复杂软件任务，但由于错误会从一个阶段悄悄传递到下一个阶段，这些系统难以被信任。需要建立可追溯和问责的管道系统。

Method: 采用Planner->Executor->Critic管道结构，评估了8种不同配置的三种最先进LLM在三个基准测试上的表现，分析错误起源、传播和修复方式。

Result: (1) 添加结构化、可问责的智能体间交接显著提高了准确性，防止了简单管道中常见的失败；(2) 模型具有明确的角色特定优势和风险；(3) 准确性-成本-延迟权衡是任务依赖的，异构管道通常最有效。

Conclusion: 提供了一种实用的、数据驱动的方法，用于设计、追踪和调试可靠、可预测和可问责的多智能体系统。

Abstract: Sequential multi-agent systems built with large language models (LLMs) can
automate complex software tasks, but they are hard to trust because errors
quietly pass from one stage to the next. We study a traceable and accountable
pipeline, meaning a system with clear roles, structured handoffs, and saved
records that let us trace who did what at each step and assign blame when
things go wrong. Our setting is a Planner -> Executor -> Critic pipeline. We
evaluate eight configurations of three state-of-the-art LLMs on three
benchmarks and analyze where errors start, how they spread, and how they can be
fixed. Our results show: (1) adding a structured, accountable handoff between
agents markedly improves accuracy and prevents the failures common in simple
pipelines; (2) models have clear role-specific strengths and risks (e.g.,
steady planning vs. high-variance critiquing), which we quantify with repair
and harm rates; and (3) accuracy-cost-latency trade-offs are task-dependent,
with heterogeneous pipelines often the most efficient. Overall, we provide a
practical, data-driven method for designing, tracing, and debugging reliable,
predictable, and accountable multi-agent systems.

</details>


### [97] [A Case for Leveraging Generative AI to Expand and Enhance Training in the Provision of Mental Health Services](https://arxiv.org/abs/2510.07623)
*Hannah R. Lawrence,Shannon Wiltsey Stirman,Samuel Dorison,Taedong Yun,Megan Jones Bell*

Main category: cs.AI

TL;DR: 本文主张将生成式AI应用于心理健康服务培训，而非直接作为治疗聊天机器人，认为这是更低风险、高影响力的应用场景。


<details>
  <summary>Details</summary>
Motivation: 当前对AI在心理健康领域的投资和讨论过度关注治疗聊天机器人，但作者认为这存在风险，需要寻找更安全有效的应用方式。

Method: 提出了使用生成式AI增强和扩大心理健康服务培训的方法，并通过一个真实案例研究展示了如何用生成式AI改进退伍军人的同伴支持培训。

Result: 生成式AI成功改善了退伍军人相互支持心理健康的培训效果，证明了其在培训领域的应用价值。

Conclusion: 应该投资于使用生成式AI来支持心理健康服务提供者的培训，这是比直接作为治疗工具更安全、更具影响力的应用方向。

Abstract: Generative artificial intelligence (Generative AI) is transforming
healthcare. With this evolution comes optimism regarding the impact it will
have on mental health, as well as concern regarding the risks that come with
generative AI operating in the mental health domain. Much of the investment in,
and academic and public discourse about, AI-powered solutions for mental health
has focused on therapist chatbots. Despite the common assumption that chatbots
will be the most impactful application of GenAI to mental health, we make the
case here for a lower-risk, high impact use case: leveraging generative AI to
enhance and scale training in mental health service provision. We highlight key
benefits of using generative AI to help train people to provide mental health
services and present a real-world case study in which generative AI improved
the training of veterans to support one another's mental health. With numerous
potential applications of generative AI in mental health, we illustrate why we
should invest in using generative AI to support training people in mental
health service provision.

</details>


### [98] [Test-Time Matching: Unlocking Compositional Reasoning in Multimodal Models](https://arxiv.org/abs/2510.07632)
*Yinglun Zhu,Jiancheng Zhang,Fuzhi Tang*

Main category: cs.AI

TL;DR: 论文发现现有评估指标低估了AI模型的组合推理能力，提出了组匹配分数和测试时匹配算法，显著提升了模型性能，在多个基准测试中达到新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标系统性低估了前沿AI模型的组合推理能力，导致模型在基准测试中表现不佳，需要更准确的评估方法和性能提升技术。

Method: 引入组匹配分数来利用组结构揭示隐藏能力，提出测试时匹配(TTM)算法，这是一种无需外部监督的迭代自改进方法。

Result: SigLIP-B16超越GPT-4.1，在Winoground上首次超越估计的人类性能，在16个数据集变体上实现一致改进，相对增益高达85.7%。

Conclusion: 通过改进评估方法和测试时匹配算法，可以显著提升AI模型的组合推理能力，推动该领域的前沿发展。

Abstract: Frontier AI models have achieved remarkable progress, yet recent studies
suggest they struggle with compositional reasoning, often performing at or
below random chance on established benchmarks. We revisit this problem and show
that widely used evaluation metrics systematically underestimate model
capability. To address this, we introduce a group matching score that better
exploits group structure and reveals substantial hidden capability in both
contrastive vision-language models (VLMs) and multimodal large language models
(MLLMs). Moreover, simply overfitting to the induced group matchings at test
time transfers this hidden capability into higher scores under standard
evaluation metrics, closing much of the reported gap. This adjustment enables
SigLIP-B16 to surpass all previous results and GPT-4.1 to yield the first
result surpassing estimated human performance on Winoground.
  Building on this insight, we propose Test-Time Matching (TTM), an iterative,
self-improving algorithm that further bootstraps model performance without any
external supervision. TTM delivers additional, non-trivial improvements: for
example, TTM enables SigLIP-B16 to surpass GPT-4.1 on MMVP-VLM, establishing a
new state of the art. Importantly, TTM remains broadly effective even on
benchmarks without metric-induced effects or group structures, achieving
relative gains up to 85.7% on challenging datasets such as WhatsUp. Across 16
dataset variants spanning diverse setups, our experiments demonstrate that TTM
consistently improves model performance and advances the frontier of
compositional reasoning.

</details>


### [99] [Safely Exploring Novel Actions in Recommender Systems via Deployment-Efficient Policy Learning](https://arxiv.org/abs/2510.07635)
*Haruka Kiyohara,Yusuke Narita,Yuta Saito,Kei Tateno,Takuma Udagawa*

Main category: cs.AI

TL;DR: 提出Safe OPG方法保证推荐系统在探索新物品时的安全性，并进一步开发DEPL框架平衡安全性与探索性


<details>
  <summary>Details</summary>
Motivation: 现实推荐系统中新物品频繁添加，现有离线策略学习方法在新物品存在时不够安全，需要开发能保证安全性的探索框架

Method: 首先开发基于高置信度离线策略评估的Safe OPG方法，然后提出DEPL框架，利用安全边际并在多次部署中逐步放松安全正则化

Result: Safe OPG几乎总能满足安全要求，但过于保守；DEPL框架能在保证推荐系统安全实现的同时探索新物品

Conclusion: 提出的DEPL框架成功解决了安全保证与探索新物品之间的权衡问题

Abstract: In many real recommender systems, novel items are added frequently over time.
The importance of sufficiently presenting novel actions has widely been
acknowledged for improving long-term user engagement. A recent work builds on
Off-Policy Learning (OPL), which trains a policy from only logged data,
however, the existing methods can be unsafe in the presence of novel actions.
Our goal is to develop a framework to enforce exploration of novel actions with
a guarantee for safety. To this end, we first develop Safe Off-Policy Policy
Gradient (Safe OPG), which is a model-free safe OPL method based on a high
confidence off-policy evaluation. In our first experiment, we observe that Safe
OPG almost always satisfies a safety requirement, even when existing methods
violate it greatly. However, the result also reveals that Safe OPG tends to be
too conservative, suggesting a difficult tradeoff between guaranteeing safety
and exploring novel actions. To overcome this tradeoff, we also propose a novel
framework called Deployment-Efficient Policy Learning for Safe User
Exploration, which leverages safety margin and gradually relaxes safety
regularization during multiple (not many) deployments. Our framework thus
enables exploration of novel actions while guaranteeing safe implementation of
recommender systems.

</details>


### [100] [Control Synthesis of Cyber-Physical Systems for Real-Time Specifications through Causation-Guided Reinforcement Learning](https://arxiv.org/abs/2510.07715)
*Xiaochen Tang,Zhenya Zhang,Miaomiao Zhang,Jie An*

Main category: cs.AI

TL;DR: 提出一种基于STL在线因果监测的奖励生成方法，通过持续监控系统行为与STL规范的符合程度，计算满足或违反的定量距离，从而产生反映瞬时状态动态的奖励。


<details>
  <summary>Details</summary>
Motivation: 现有方法将STL奖励函数纳入RL中自动合成控制策略，但自动推断的奖励代表整个或部分路径的全局评估，无法准确累积局部变化的奖励，稀疏的全局奖励可能导致不收敛和不稳定的训练性能。

Method: 采用在线因果监测STL的方法，在每个控制步骤持续监控系统行为与STL规范的符合程度，计算满足或违反的定量距离，并提供因果语义的平滑近似以克服其不连续性，使其可用于深度RL方法。

Result: 在Gym环境中的各种连续控制基准测试上进行了评估，实验结果表明该方法优于现有的相关STL引导RL方法，为深度RL提供了更稳健和高效的奖励生成框架。

Conclusion: 提出的基于STL在线因果语义的RL方法能够提供更精确的奖励信号，改善训练收敛性和稳定性，在实时安全关键系统中具有良好应用前景。

Abstract: In real-time and safety-critical cyber-physical systems (CPSs), control
synthesis must guarantee that generated policies meet stringent timing and
correctness requirements under uncertain and dynamic conditions. Signal
temporal logic (STL) has emerged as a powerful formalism of expressing
real-time constraints, with its semantics enabling quantitative assessment of
system behavior. Meanwhile, reinforcement learning (RL) has become an important
method for solving control synthesis problems in unknown environments. Recent
studies incorporate STL-based reward functions into RL to automatically
synthesize control policies. However, the automatically inferred rewards
obtained by these methods represent the global assessment of a whole or partial
path but do not accumulate the rewards of local changes accurately, so the
sparse global rewards may lead to non-convergence and unstable training
performances. In this paper, we propose an online reward generation method
guided by the online causation monitoring of STL. Our approach continuously
monitors system behavior against an STL specification at each control step,
computing the quantitative distance toward satisfaction or violation and
thereby producing rewards that reflect instantaneous state dynamics.
Additionally, we provide a smooth approximation of the causation semantics to
overcome the discontinuity of the causation semantics and make it
differentiable for using deep-RL methods. We have implemented a prototype tool
and evaluated it in the Gym environment on a variety of continuously controlled
benchmarks. Experimental results show that our proposed STL-guided RL method
with online causation semantics outperforms existing relevant STL-guided RL
methods, providing a more robust and efficient reward generation framework for
deep-RL.

</details>


### [101] [oMeBench: Towards Robust Benchmarking of LLMs in Organic Mechanism Elucidation and Reasoning](https://arxiv.org/abs/2510.07731)
*Ruiling Xu,Yifan Zhang,Qingyun Wang,Carl Edwards,Heng Ji*

Main category: cs.AI

TL;DR: 提出了oMeBench，首个大规模、专家策划的有机化学反应机理推理基准，包含10,000多个带注释的机理步骤，并开发了oMeS动态评估框架来精确评估LLM的化学推理能力。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在化学任务中表现出潜力，但尚不清楚其是否真正具备化学推理能力，即生成有效中间体、保持化学一致性和遵循逻辑连贯的多步骤路径的能力。

Method: 构建oMeBench基准数据集，包含机理步骤、中间体、类型标签和难度评级；提出oMeS动态评估框架，结合步骤级逻辑和化学相似性进行细粒度评分；分析最先进LLM的性能。

Result: 当前模型显示出有前景的化学直觉，但在正确和一致的多步骤推理方面存在困难；使用提示策略和在数据集上微调专家模型，性能比领先闭源模型提高50%。

Conclusion: oMeBench将为推进AI系统实现真正的化学推理提供严格基础。

Abstract: Organic reaction mechanisms are the stepwise elementary reactions by which
reactants form intermediates and products, and are fundamental to understanding
chemical reactivity and designing new molecules and reactions. Although large
language models (LLMs) have shown promise in understanding chemical tasks such
as synthesis design, it is unclear to what extent this reflects genuine
chemical reasoning capabilities, i.e., the ability to generate valid
intermediates, maintain chemical consistency, and follow logically coherent
multi-step pathways. We address this by introducing oMeBench, the first
large-scale, expert-curated benchmark for organic mechanism reasoning in
organic chemistry. It comprises over 10,000 annotated mechanistic steps with
intermediates, type labels, and difficulty ratings. Furthermore, to evaluate
LLM capability more precisely and enable fine-grained scoring, we propose oMeS,
a dynamic evaluation framework that combines step-level logic and chemical
similarity. We analyze the performance of state-of-the-art LLMs, and our
results show that although current models display promising chemical intuition,
they struggle with correct and consistent multi-step reasoning. Notably, we
find that using prompting strategy and fine-tuning a specialist model on our
proposed dataset increases performance by 50% over the leading closed-source
model. We hope that oMeBench will serve as a rigorous foundation for advancing
AI systems toward genuine chemical reasoning.

</details>


### [102] [SurveyG: A Multi-Agent LLM Framework with Hierarchical Citation Graph for Automated Survey Generation](https://arxiv.org/abs/2510.07733)
*Minh-Anh Nguye,Minh-Duc Nguyen,Nguyen Thi Ha Lan,Kieu Hai Dang,Nguyen Tien Dong,Le Duy Dung*

Main category: cs.AI

TL;DR: SurveyG是一个基于LLM的智能体框架，通过整合层次化引用图来生成更全面、结构更好的综述论文，解决了现有方法忽视论文间结构关系的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成综述的方法通常直接提取和总结大量相关论文内容，但忽视了论文间的结构关系，导致生成的综述缺乏连贯的分类体系和深层上下文理解。

Method: 提出SurveyG框架，整合层次化引用图（包含基础层、发展层和前沿层），通过横向搜索和纵向深度遍历生成多级摘要，并使用多智能体验证确保一致性、覆盖面和事实准确性。

Result: 实验表明，SurveyG在人类专家和LLM作为评判者的评估中优于现有最先进框架，生成的综述更全面且更好地符合领域知识分类结构。

Conclusion: SurveyG通过整合结构化和上下文知识，显著提升了LLM生成综述的质量，为自动化综述生成提供了更有效的方法。

Abstract: Large language models (LLMs) are increasingly adopted for automating survey
paper generation \cite{wang2406autosurvey, liang2025surveyx,
yan2025surveyforge,su2025benchmarking,wen2025interactivesurvey}. Existing
approaches typically extract content from a large collection of related papers
and prompt LLMs to summarize them directly. However, such methods often
overlook the structural relationships among papers, resulting in generated
surveys that lack a coherent taxonomy and a deeper contextual understanding of
research progress. To address these shortcomings, we propose \textbf{SurveyG},
an LLM-based agent framework that integrates \textit{hierarchical citation
graph}, where nodes denote research papers and edges capture both citation
dependencies and semantic relatedness between their contents, thereby embedding
structural and contextual knowledge into the survey generation process. The
graph is organized into three layers: \textbf{Foundation},
\textbf{Development}, and \textbf{Frontier}, to capture the evolution of
research from seminal works to incremental advances and emerging directions. By
combining horizontal search within layers and vertical depth traversal across
layers, the agent produces multi-level summaries, which are consolidated into a
structured survey outline. A multi-agent validation stage then ensures
consistency, coverage, and factual accuracy in generating the final survey.
Experiments, including evaluations by human experts and LLM-as-a-judge,
demonstrate that SurveyG outperforms state-of-the-art frameworks, producing
surveys that are more comprehensive and better structured to the underlying
knowledge taxonomy of a field.

</details>


### [103] [Haibu Mathematical-Medical Intelligent Agent:Enhancing Large Language Model Reliability in Medical Tasks via Verifiable Reasoning Chains](https://arxiv.org/abs/2510.07748)
*Yilun Zhang,Dexing Kong*

Main category: cs.AI

TL;DR: 提出Haibu MMIA架构，通过形式化可验证推理确保LLM在医疗领域的可靠性，将复杂任务分解为原子步骤并自动审核逻辑一致性，支持从基础推理到低成本验证的转变。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在医疗领域容易产生事实和逻辑错误的问题，确保在这个高风险领域中的可靠性。

Method: 递归分解复杂医疗任务为原子证据步骤，自动审核推理链的逻辑一致性和证据可追溯性，采用'引导'模式存储已验证推理链作为'定理'，后续任务通过RAG实现低成本验证。

Result: 在四个医疗管理领域验证，错误检测率超过98%，误报率低于1%，显著优于基线LLM，RAG模式预计可降低85%处理成本。

Conclusion: MMIA的可验证推理框架是创建可信赖、透明且成本效益高的AI系统的重要进展，使LLM技术在医疗关键应用中可行。

Abstract: Large Language Models (LLMs) show promise in medicine but are prone to
factual and logical errors, which is unacceptable in this high-stakes field. To
address this, we introduce the "Haibu Mathematical-Medical Intelligent Agent"
(MMIA), an LLM-driven architecture that ensures reliability through a formally
verifiable reasoning process. MMIA recursively breaks down complex medical
tasks into atomic, evidence-based steps. This entire reasoning chain is then
automatically audited for logical coherence and evidence traceability, similar
to theorem proving. A key innovation is MMIA's "bootstrapping" mode, which
stores validated reasoning chains as "theorems." Subsequent tasks can then be
efficiently solved using Retrieval-Augmented Generation (RAG), shifting from
costly first-principles reasoning to a low-cost verification model. We
validated MMIA across four healthcare administration domains, including DRG/DIP
audits and medical insurance adjudication, using expert-validated benchmarks.
Results showed MMIA achieved an error detection rate exceeding 98% with a false
positive rate below 1%, significantly outperforming baseline LLMs. Furthermore,
the RAG matching mode is projected to reduce average processing costs by
approximately 85% as the knowledge base matures. In conclusion, MMIA's
verifiable reasoning framework is a significant step toward creating
trustworthy, transparent, and cost-effective AI systems, making LLM technology
viable for critical applications in medicine.

</details>


### [104] [From Noisy to Native: LLM-driven Graph Restoration for Test-Time Graph Domain Adaptation](https://arxiv.org/abs/2510.07762)
*Xiangwei Lv,JinLuan Yang,Wang Lin,Jingyuan Chen,Beishui Liao*

Main category: cs.AI

TL;DR: 提出GRAIL框架，将测试时图域适应重新定义为生成式图恢复问题，利用LLM将目标图恢复到源域状态，无需访问源域数据


<details>
  <summary>Details</summary>
Motivation: 现有图域适应方法依赖源域数据，但由于隐私安全问题源域数据往往不可用，因此需要开发无需源域数据的测试时图域适应方法

Method: 使用图扩散过程建模图恢复，量化模块将恢复特征编码为离散token，微调LLM作为生成式恢复器，并引入强化学习优化对齐和置信度奖励

Result: 在多个数据集上的广泛实验证明了该方法的有效性

Conclusion: GRAIL框架成功解决了测试时图域适应问题，通过生成式图恢复方法实现了无需源域数据的知识迁移

Abstract: Graph domain adaptation (GDA) has achieved great attention due to its
effectiveness in addressing the domain shift between train and test data. A
significant bottleneck in existing graph domain adaptation methods is their
reliance on source-domain data, which is often unavailable due to privacy or
security concerns. This limitation has driven the development of Test-Time
Graph Domain Adaptation (TT-GDA), which aims to transfer knowledge without
accessing the source examples. Inspired by the generative power of large
language models (LLMs), we introduce a novel framework that reframes TT-GDA as
a generative graph restoration problem, "restoring the target graph to its
pristine, source-domain-like state". There are two key challenges: (1) We need
to construct a reasonable graph restoration process and design an effective
encoding scheme that an LLM can understand, bridging the modality gap. (2) We
need to devise a mechanism to ensure the restored graph acquires the intrinsic
features of the source domain, even without access to the source data. To
ensure the effectiveness of graph restoration, we propose GRAIL, that restores
the target graph into a state that is well-aligned with the source domain.
Specifically, we first compress the node representations into compact latent
features and then use a graph diffusion process to model the graph restoration
process. Then a quantization module encodes the restored features into discrete
tokens. Building on this, an LLM is fine-tuned as a generative restorer to
transform a "noisy" target graph into a "native" one. To further improve
restoration quality, we introduce a reinforcement learning process guided by
specialized alignment and confidence rewards. Extensive experiments demonstrate
the effectiveness of our approach across various datasets.

</details>


### [105] [An approach for systematic decomposition of complex llm tasks](https://arxiv.org/abs/2510.07772)
*Tianle Zhou,Jiakai Xu,Guanhong Liu,Jiaxiang Liu,Haonan Wang,Eugene Wu*

Main category: cs.AI

TL;DR: 提出ACONIC框架，通过约束问题建模和形式化复杂度度量来指导任务分解，在组合优化和数据库查询任务上显著提升性能10-40个百分点。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在复杂任务上存在可靠性问题，因为现有分解方法是启发式的，依赖于智能体或手动分解。

Method: 引入ACONIC框架，将任务建模为约束问题，利用形式化复杂度度量来指导分解过程。

Result: 在组合任务(SATBench)和LLM数据库查询任务(Spider)上，通过基于复杂度的分解，智能体性能显著提升10-40个百分点。

Conclusion: ACONIC框架提供了一种系统化的任务分解方法，能够有效提升LLM在复杂任务上的可靠性。

Abstract: Large Language Models (LLMs) suffer from reliability issues on complex tasks,
as existing decomposition methods are heuristic and rely on agent or manual
decomposition. This work introduces a novel, systematic decomposition framework
that we call Analysis of CONstraint-Induced Complexity (ACONIC), which models
the task as a constraint problem and leveraging formal complexity measures to
guide decomposition. On combinatorial (SATBench) and LLM database querying
tasks (Spider), we find that by decomposing the tasks following the measure of
complexity, agent can perform considerably better (10-40 percentage point).

</details>


### [106] [GCPO: When Contrast Fails, Go Gold](https://arxiv.org/abs/2510.07790)
*Hao Wu,Wei Liu*

Main category: cs.AI

TL;DR: 提出GCPO方法，通过引入外部标准参考答案来解决GRPO等强化学习算法在训练小型语言模型推理能力时的局限性，提升训练效率和模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法如GRPO存在明显缺陷：模型生成响应的上限完全由模型自身决定，无法从全错或全对的样本中学习知识。

Method: 引入外部标准参考答案，当模型无法解决问题时，参考答案提供正确响应，引导模型向明确正确的更新方向学习。

Result: 在多个基准数据集上取得优异结果，相比基线模型有显著提升。

Conclusion: GCPO方法能够充分利用所有样本，提高训练效率，并让模型在训练过程中学习参考答案的解题策略，增强推理泛化能力。

Abstract: Reinforcement learning has been widely applied to enhance the reasoning
capabilities of large language models. Extending the inference limits of
smaller models has become a prominent research focus. However, algorithms such
as Group Relative Policy Optimization (GRPO) suffer from a clear drawback: the
upper bound of a model's rollout responses is entirely determined by the model
itself, preventing the acquisition of knowledge from samples that are either
all incorrect or all correct. In this paper, we introduce Group Contrastive
Policy Optimization (GCPO), a method that incorporates external standard
reference answers. When the model cannot solve a problem, the reference answer
supplies the correct response, steering the model toward an unequivocally
accurate update direction. This approach offers two main advantages: (1) it
improves training efficiency by fully utilizing every sample; (2) it enables
the model to emulate the problem solving strategy of the reference answer
during training, thereby enhancing generalization in reasoning. GCPO achieves
outstanding results across multiple benchmark datasets, yielding substantial
improvements over the baseline model. Our code is available at:
https://github.com/AchoWu/GCPO.

</details>


### [107] [Strategic Communication under Threat: Learning Information Trade-offs in Pursuit-Evasion Games](https://arxiv.org/abs/2510.07813)
*Valerio La Gatta,Dolev Mutzari,Sarit Kraus,VS Subrahmanian*

Main category: cs.AI

TL;DR: 提出了SHADOW框架，通过强化学习平衡追击者在对抗环境中的通信需求与暴露风险，实现了比基线方法更高的成功率。


<details>
  <summary>Details</summary>
Motivation: 对抗环境中存在信息获取与暴露风险的关键权衡：通信可以增强态势感知，但同时会暴露自身位置增加被攻击风险。

Method: 提出PEEC博弈框架，使用SHADOW多头部序贯强化学习框架，集成连续导航控制、离散通信动作和对手行为预测建模。

Result: SHADOW追击者比六种竞争基线方法获得更高成功率，时间序列建模和对手建模对有效决策至关重要。

Conclusion: 学习到的策略在不同通信风险和物理不对称条件下具有良好的泛化能力，证明了所提方法的有效性。

Abstract: Adversarial environments require agents to navigate a key strategic
trade-off: acquiring information enhances situational awareness, but may
simultaneously expose them to threats. To investigate this tension, we
formulate a PursuitEvasion-Exposure-Concealment Game (PEEC) in which a pursuer
agent must decide when to communicate in order to obtain the evader's position.
Each communication reveals the pursuer's location, increasing the risk of being
targeted. Both agents learn their movement policies via reinforcement learning,
while the pursuer additionally learns a communication policy that balances
observability and risk. We propose SHADOW (Strategic-communication Hybrid
Action Decision-making under partial Observation for Warfare), a multi-headed
sequential reinforcement learning framework that integrates continuous
navigation control, discrete communication actions, and opponent modeling for
behavior prediction. Empirical evaluations show that SHADOW pursuers achieve
higher success rates than six competitive baselines. Our ablation study
confirms that temporal sequence modeling and opponent modeling are critical for
effective decision-making. Finally, our sensitivity analysis reveals that the
learned policies generalize well across varying communication risks and
physical asymmetries between agents.

</details>


### [108] [An LLM-Powered Cooperative Framework for Large-Scale Multi-Vehicle Navigation](https://arxiv.org/abs/2510.07825)
*Yuping Zhou,Siqi Lai,Jindong Han,Hao Liu*

Main category: cs.AI

TL;DR: CityNav是一个基于LLM的分层框架，用于大规模多车辆动态导航，通过全局交通分配代理和局部导航代理的协同优化，在城市规模交通网络中显著提升旅行效率和缓解拥堵。


<details>
  <summary>Details</summary>
Motivation: 现有路径搜索算法和强化学习方法难以扩展到城市规模网络，无法有效捕捉城市交通的非线性、随机性和耦合动态特性。

Method: 提出分层LLM框架，包含全局交通分配代理和局部导航代理，采用协同推理优化机制和双奖励结构（个体奖励和共享奖励）进行联合训练。

Result: 在四个真实道路网络（最多160万条道路和43万个交叉口）上的实验表明，CityNav在九种经典路径搜索和基于RL的基准方法中表现最优。

Conclusion: LLM有潜力实现可扩展、自适应和协同的城市范围交通导航，为复杂城市环境中的智能大规模车辆路由提供基础。

Abstract: The rise of Internet of Vehicles (IoV) technologies is transforming traffic
management from isolated control to a collective, multi-vehicle process. At the
heart of this shift is multi-vehicle dynamic navigation, which requires
simultaneously routing large fleets under evolving traffic conditions. Existing
path search algorithms and reinforcement learning methods struggle to scale to
city-wide networks, often failing to capture the nonlinear, stochastic, and
coupled dynamics of urban traffic. To address these challenges, we propose
CityNav, a hierarchical, LLM-powered framework for large-scale multi-vehicle
navigation. CityNav integrates a global traffic allocation agent, which
coordinates strategic traffic flow distribution across regions, with local
navigation agents that generate locally adaptive routes aligned with global
directives. To enable effective cooperation, we introduce a cooperative
reasoning optimization mechanism, in which agents are jointly trained with a
dual-reward structure: individual rewards promote per-vehicle efficiency, while
shared rewards encourage network-wide coordination and congestion reduction.
Extensive experiments on four real-world road networks of varying scales (up to
1.6 million roads and 430,000 intersections) and traffic datasets demonstrate
that CityNav consistently outperforms nine classical path search and RL-based
baselines in city-scale travel efficiency and congestion mitigation. Our
results highlight the potential of LLMs to enable scalable, adaptive, and
cooperative city-wide traffic navigation, providing a foundation for
intelligent, large-scale vehicle routing in complex urban environments. Our
project is available at https://github.com/usail-hkust/CityNav.

</details>


### [109] [FinMR: A Knowledge-Intensive Multimodal Benchmark for Advanced Financial Reasoning](https://arxiv.org/abs/2510.07852)
*Shuangyan Deng,Haizhou Peng,Jiachen Xu,Rui Mao,Ciprian Doru Giurcăneanu,Jiamou Liu*

Main category: cs.AI

TL;DR: FinMR是一个高质量、知识密集的多模态数据集，专门用于评估专业分析师级别的金融推理能力，包含3200多个精心策划的问题-答案对，涵盖15个金融主题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在金融等专业领域的评估存在不足，缺乏具有专业级知识强度、详细注释和高级推理复杂性的数据集。

Method: 构建包含3200多个专家标注问题-答案对的FinMR数据集，涵盖15个金融主题，整合了复杂数学推理、高级金融知识和多类型图像解释任务。

Result: 通过对领先的开源和闭源MLLMs进行全面基准测试，发现这些模型与专业金融分析师之间存在显著性能差距，特别是在精确图像分析、复杂金融公式应用和深度上下文理解方面。

Conclusion: FinMR通过提供丰富的视觉内容和详细解释性注释，成为评估和推进多模态金融推理向专业分析师水平发展的重要基准工具。

Abstract: Multimodal Large Language Models (MLLMs) have made substantial progress in
recent years. However, their rigorous evaluation within specialized domains
like finance is hindered by the absence of datasets characterized by
professional-level knowledge intensity, detailed annotations, and advanced
reasoning complexity. To address this critical gap, we introduce FinMR, a
high-quality, knowledge-intensive multimodal dataset explicitly designed to
evaluate expert-level financial reasoning capabilities at a professional
analyst's standard. FinMR comprises over 3,200 meticulously curated and
expertly annotated question-answer pairs across 15 diverse financial topics,
ensuring broad domain diversity and integrating sophisticated mathematical
reasoning, advanced financial knowledge, and nuanced visual interpretation
tasks across multiple image types. Through comprehensive benchmarking with
leading closed-source and open-source MLLMs, we highlight significant
performance disparities between these models and professional financial
analysts, uncovering key areas for model advancement, such as precise image
analysis, accurate application of complex financial formulas, and deeper
contextual financial understanding. By providing richly varied visual content
and thorough explanatory annotations, FinMR establishes itself as an essential
benchmark tool for assessing and advancing multimodal financial reasoning
toward professional analyst-level competence.

</details>


### [110] [Augur: Modeling Covariate Causal Associations in Time Series via Large Language Models](https://arxiv.org/abs/2510.07858)
*Zhiqing Cui,Binwu Wang,Qingxiang Liu,Yeqiang Wang,Zhengyang Zhou,Yuxuan Liang,Yang Wang*

Main category: cs.AI

TL;DR: Augur是一个完全由LLM驱动的时间序列预测框架，利用LLM的因果推理能力发现和使用协变量间的有向因果关联，通过两阶段师生架构提高预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的时间序列预测方法存在局限性：在模型架构中作用边缘化、依赖粗糙的统计文本提示、缺乏可解释性。

Method: 采用两阶段师生架构：强大的教师LLM通过启发式搜索和成对因果检验从时间序列推断有向因果图；轻量级学生代理精炼该图，并在高置信度因果关联上进行微调，这些关联被编码为丰富的文本提示用于预测。

Result: 在真实世界数据集上的广泛实验表明，Augur在25个基线方法中取得了竞争性性能，并展现出强大的零样本泛化能力。

Conclusion: Augur框架不仅提高了预测准确性，还提供了透明、可追溯的变量交互推理，为LLM在时间序列预测中的应用提供了新的方向。

Abstract: Large language models (LLM) have emerged as a promising avenue for time
series forecasting, offering the potential to integrate multimodal data.
However, existing LLM-based approaches face notable limitations-such as
marginalized role in model architectures, reliance on coarse statistical text
prompts, and lack of interpretability. In this work, we introduce Augur, a
fully LLM driven time series forecasting framework that exploits LLM causal
reasoning to discover and use directed causal associations among covariates.
Augur uses a two stage teacher student architecture where a powerful teacher
LLM infers a directed causal graph from time series using heuristic search
together with pairwise causality testing. A lightweight student agent then
refines the graph and fine tune on high confidence causal associations that are
encoded as rich textual prompts to perform forecasting. This design improves
predictive accuracy while yielding transparent, traceable reasoning about
variable interactions. Extensive experiments on real-world datasets with 25
baselines demonstrate that Augur achieves competitive performance and robust
zero-shot generalization.

</details>


### [111] [Understanding DeepResearch via Reports](https://arxiv.org/abs/2510.07861)
*Tianyu Fan,Xinyao Niu,Yuxiang Zheng,Fengji Zhang,Chengen Huang,Bei Chen,Junyang Lin,Chao Huang*

Main category: cs.AI

TL;DR: 提出了DeepResearch-ReportEval评估框架，用于评估深度研究AI系统在研究报告生成方面的综合能力，包括质量、冗余性和事实性三个维度。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注孤立能力，无法评估深度研究系统在开放研究场景中的整体表现，特别是综合多源信息、生成洞察和呈现连贯发现的能力。

Method: 采用LLM-as-a-Judge方法，构建包含100个查询的标准化基准，涵盖12个现实世界类别，系统评估研究报告的质量、冗余性和事实性。

Result: 对四个领先商业系统的评估揭示了不同的设计理念和性能权衡，为深度研究系统从信息助手向智能研究伙伴的演进提供了基础见解。

Conclusion: DeepResearch-ReportEval框架填补了深度研究系统评估的关键空白，为系统能力比较和未来发展提供了标准化工具。

Abstract: DeepResearch agents represent a transformative AI paradigm, conducting
expert-level research through sophisticated reasoning and multi-tool
integration. However, evaluating these systems remains critically challenging
due to open-ended research scenarios and existing benchmarks that focus on
isolated capabilities rather than holistic performance. Unlike traditional LLM
tasks, DeepResearch systems must synthesize diverse sources, generate insights,
and present coherent findings, which are capabilities that resist simple
verification. To address this gap, we introduce DeepResearch-ReportEval, a
comprehensive framework designed to assess DeepResearch systems through their
most representative outputs: research reports. Our approach systematically
measures three dimensions: quality, redundancy, and factuality, using an
innovative LLM-as-a-Judge methodology achieving strong expert concordance. We
contribute a standardized benchmark of 100 curated queries spanning 12
real-world categories, enabling systematic capability comparison. Our
evaluation of four leading commercial systems reveals distinct design
philosophies and performance trade-offs, establishing foundational insights as
DeepResearch evolves from information assistants toward intelligent research
partners. Source code and data are available at:
https://github.com/HKUDS/DeepResearch-Eval.

</details>


### [112] [Profit Mirage: Revisiting Information Leakage in LLM-based Financial Agents](https://arxiv.org/abs/2510.07920)
*Xiangyu Li,Yawen Zeng,Xiaofen Xing,Jin Xu,Xiangmin Xu*

Main category: cs.AI

TL;DR: 本文揭示LLM金融代理存在"利润幻象"问题，提出FactFin框架通过反事实扰动来学习因果驱动因素，并发布FinLake-Bench评估基准。


<details>
  <summary>Details</summary>
Motivation: 解决LLM金融代理中因信息泄露导致的"利润幻象"问题，即回测收益在模型知识窗口结束后消失的现象。

Method: 提出FactFin框架，包含策略代码生成器、检索增强生成、蒙特卡洛树搜索和反事实模拟器四个核心组件，通过反事实扰动强制学习因果驱动因素。

Result: 实验表明该方法在样本外泛化方面超越所有基线，提供优越的风险调整后性能。

Conclusion: FactFin框架能有效缓解LLM金融代理的信息泄露问题，提高模型的泛化能力和实际交易表现。

Abstract: LLM-based financial agents have attracted widespread excitement for their
ability to trade like human experts. However, most systems exhibit a "profit
mirage": dazzling back-tested returns evaporate once the model's knowledge
window ends, because of the inherent information leakage in LLMs. In this
paper, we systematically quantify this leakage issue across four dimensions and
release FinLake-Bench, a leakage-robust evaluation benchmark. Furthermore, to
mitigate this issue, we introduce FactFin, a framework that applies
counterfactual perturbations to compel LLM-based agents to learn causal drivers
instead of memorized outcomes. FactFin integrates four core components:
Strategy Code Generator, Retrieval-Augmented Generation, Monte Carlo Tree
Search, and Counterfactual Simulator. Extensive experiments show that our
method surpasses all baselines in out-of-sample generalization, delivering
superior risk-adjusted performance.

</details>


### [113] [Enabling Personalized Long-term Interactions in LLM-based Agents through Persistent Memory and User Profiles](https://arxiv.org/abs/2510.07925)
*Rebecca Westhäußer,Wolfgang Minker,Sebatian Zepf*

Main category: cs.AI

TL;DR: 提出了一个集成持久记忆、动态协调、自我验证和演进用户档案的框架，用于实现基于LLM的个性化智能体长期交互。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能体在提供个性化交互方面存在局限，RAG虽然增强了上下文感知但缺乏结合用户特定数据的机制，现有个性化研究多停留在概念层面而缺乏技术实现。

Method: 基于统一的个性化定义，结合多智能体协作和多源检索等成熟AI模式，构建了集成持久记忆、动态协调、自我验证和演进用户档案的技术框架。

Result: 在三个公开数据集上评估了检索准确性、响应正确性和BertScore等指标，并通过5天试点用户研究获得了关于感知个性化的初步用户反馈。

Conclusion: 研究表明集成持久记忆和用户档案有潜力提升基于LLM智能体的适应性和感知个性化，为未来工作提供了指导方向。

Abstract: Large language models (LLMs) increasingly serve as the central control unit
of AI agents, yet current approaches remain limited in their ability to deliver
personalized interactions. While Retrieval Augmented Generation enhances LLM
capabilities by improving context-awareness, it lacks mechanisms to combine
contextual information with user-specific data. Although personalization has
been studied in fields such as human-computer interaction or cognitive science,
existing perspectives largely remain conceptual, with limited focus on
technical implementation. To address these gaps, we build on a unified
definition of personalization as a conceptual foundation to derive technical
requirements for adaptive, user-centered LLM-based agents. Combined with
established agentic AI patterns such as multi-agent collaboration or
multi-source retrieval, we present a framework that integrates persistent
memory, dynamic coordination, self-validation, and evolving user profiles to
enable personalized long-term interactions. We evaluate our approach on three
public datasets using metrics such as retrieval accuracy, response correctness,
or BertScore. We complement these results with a five-day pilot user study
providing initial insights into user feedback on perceived personalization. The
study provides early indications that guide future work and highlights the
potential of integrating persistent memory and user profiles to improve the
adaptivity and perceived personalization of LLM-based agents.

</details>


### [114] [Agent-Based Genetic Algorithm for Crypto Trading Strategy Optimization](https://arxiv.org/abs/2510.07943)
*Qiushi Tian,Churong Liang,Kairan Hong,Runnan Li*

Main category: cs.AI

TL;DR: 提出CGA-Agent混合框架，结合遗传算法与智能多智能体协调机制，用于加密货币交易策略参数优化，在动态金融环境中实现自适应优化。


<details>
  <summary>Details</summary>
Motivation: 加密货币市场具有极端波动性、非平稳动态和复杂微观结构模式，使传统参数优化方法失效，需要新的自适应优化方法。

Method: 开发CGA-Agent混合框架，整合遗传算法与多智能体协调机制，融入实时市场微观结构智能和自适应策略性能反馈，动态指导进化过程。

Result: 在三种加密货币上的实证评估显示，在总回报和风险调整指标上都实现了系统性和统计显著的性能改进。

Conclusion: 该框架超越了静态优化方法的局限性，为动态金融环境中的交易策略优化提供了有效解决方案。

Abstract: Cryptocurrency markets present formidable challenges for trading strategy
optimization due to extreme volatility, non-stationary dynamics, and complex
microstructure patterns that render conventional parameter optimization methods
fundamentally inadequate. We introduce Cypto Genetic Algorithm Agent
(CGA-Agent), a pioneering hybrid framework that synergistically integrates
genetic algorithms with intelligent multi-agent coordination mechanisms for
adaptive trading strategy parameter optimization in dynamic financial
environments. The framework uniquely incorporates real-time market
microstructure intelligence and adaptive strategy performance feedback through
intelligent mechanisms that dynamically guide evolutionary processes,
transcending the limitations of static optimization approaches. Comprehensive
empirical evaluation across three cryptocurrencies demonstrates systematic and
statistically significant performance improvements on both total returns and
risk-adjusted metrics.

</details>


### [115] [TaoSR-SHE: Stepwise Hybrid Examination Reinforcement Learning Framework for E-commerce Search Relevance](https://arxiv.org/abs/2510.07972)
*Pengkun Jiao,Yiming Jin,Jianhui Yang,Chenhe Dong,Zerui Huang,Shaowei Yao,Xiaojiang Zhou,Dan Ou,Haihong Tang*

Main category: cs.AI

TL;DR: 提出了TaoSR-SHE框架，通过步进式混合检查强化学习解决电商搜索相关性分析中的推理一致性和泛化性问题，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有训练范式（SFT、DPO、RLVR）在长尾查询泛化、细粒度监督和推理一致性方面存在不足，需要更有效的强化学习框架。

Method: 核心是SRPO算法，结合生成式步进奖励模型和人工标注验证器，辅以多样化数据过滤和多阶段课程学习技术。

Result: 在真实搜索基准测试中，TaoSR-SHE在推理质量和相关性预测准确性方面均优于SFT、DPO、GRPO等基线方法。

Conclusion: 该框架不仅提升了性能，还增强了可解释性和鲁棒性，为电商搜索相关性分析提供了有效解决方案。

Abstract: Query-product relevance analysis is a foundational technology in e-commerce
search engines and has become increasingly important in AI-driven e-commerce.
The recent emergence of large language models (LLMs), particularly their
chain-of-thought (CoT) reasoning capabilities, offers promising opportunities
for developing relevance systems that are both more interpretable and more
robust. However, existing training paradigms have notable limitations: SFT and
DPO suffer from poor generalization on long-tail queries and from a lack of
fine-grained, stepwise supervision to enforce rule-aligned reasoning. In
contrast, reinforcement learning with verification rewards (RLVR) suffers from
sparse feedback, which provides insufficient signal to correct erroneous
intermediate steps, thereby undermining logical consistency and limiting
performance in complex inference scenarios.
  To address these challenges, we introduce the Stepwise Hybrid Examination
Reinforcement Learning framework for Taobao Search Relevance (TaoSR-SHE). At
its core is Stepwise Reward Policy Optimization (SRPO), a reinforcement
learning algorithm that leverages step-level rewards generated by a hybrid of a
high-quality generative stepwise reward model and a human-annotated offline
verifier, prioritizing learning from critical correct and incorrect reasoning
steps. TaoSR-SHE further incorporates two key techniques: diversified data
filtering to encourage exploration across varied reasoning paths and mitigate
policy entropy collapse, and multi-stage curriculum learning to foster
progressive capability growth. Extensive experiments on real-world search
benchmarks show that TaoSR-SHE improves both reasoning quality and
relevance-prediction accuracy in large-scale e-commerce settings, outperforming
SFT, DPO, GRPO, and other baselines, while also enhancing interpretability and
robustness.

</details>


### [116] [VoiceAgentBench: Are Voice Assistants ready for agentic tasks?](https://arxiv.org/abs/2510.07978)
*Dhruv Jain,Harshit Shukla,Gautam Rajeev,Ashish Kulkarni,Chandra Khatri,Shubham Agarwal*

Main category: cs.AI

TL;DR: 提出了VoiceAgentBench基准测试，用于评估语音语言模型在真实语音代理场景中的表现，包含5500多个合成语音查询，涵盖多语言、多工具调用和安全性评估。


<details>
  <summary>Details</summary>
Motivation: 现有语音基准测试主要关注孤立能力如转录或问答，缺乏对多语言文化理解和对抗鲁棒性的系统性评估。

Method: 创建包含英语、印地语和5种印度语言的语音查询数据集，使用新颖的采样算法最大化声学和说话人多样性，评估工具选择准确性、结构一致性和工具调用正确性。

Result: 实验揭示了当前语音语言模型在上下文工具编排任务、印度语言泛化和对抗鲁棒性方面存在显著差距。

Conclusion: VoiceAgentBench暴露了当前语音语言模型的关键局限性，为改进多语言语音代理系统提供了重要基准。

Abstract: Large-scale Speech Language Models (SpeechLMs) have enabled voice assistants
capable of understanding natural spoken queries and performing complex tasks.
However, existing speech benchmarks primarily focus on isolated capabilities
such as transcription, or question-answering, and do not systematically
evaluate agentic scenarios encompassing multilingual and cultural
understanding, as well as adversarial robustness. To address this, we introduce
VoiceAgentBench, a comprehensive benchmark designed to evaluate SpeechLMs in
realistic spoken agentic settings. It comprises over 5,500 synthetic spoken
queries, including dialogues grounded in Indian context, covering single-tool
invocations, multi-tool workflows, multi-turn interactions, and safety
evaluations. The benchmark supports English, Hindi, and 5 other Indian
languages, reflecting real-world linguistic and cultural diversity. We simulate
speaker variability using a novel sampling algorithm that selects audios for
TTS voice conversion based on its speaker embeddings, maximizing acoustic and
speaker diversity. Our evaluation measures tool selection accuracy, structural
consistency, and the correctness of tool invocations, including adversarial
robustness. Our experiments reveal significant gaps in contextual tool
orchestration tasks, Indic generalization, and adversarial robustness, exposing
critical limitations of current SpeechLMs.

</details>


### [117] [ReInAgent: A Context-Aware GUI Agent Enabling Human-in-the-Loop Mobile Task Navigation](https://arxiv.org/abs/2510.07988)
*Haitao Jia,Ming He,Zimo Yin,Likang Wu,Jianping Fan,Jitao Sang*

Main category: cs.AI

TL;DR: ReInAgent是一个上下文感知的多智能体框架，通过动态信息管理和人机协作解决移动GUI智能体在模糊、动态和冲突任务场景中的信息困境问题。


<details>
  <summary>Details</summary>
Motivation: 现有移动GUI智能体过于强调自主操作，忽视了用户在任务执行中的主动参与，导致在信息困境场景下无法适应真实用户需求和偏好。

Method: 采用三个专门智能体围绕共享内存模块：信息管理智能体负责基于槽位的信息管理和主动用户交互，决策智能体负责冲突感知规划，反思智能体负责任务反思和信息一致性验证。

Result: 实验结果表明ReInAgent有效解决了信息困境，在涉及信息困境的复杂任务上比Mobile-Agent-v2成功率高出25%。

Conclusion: 通过持续上下文信息分析和持续的人机协作，ReInAgent克服了现有方法依赖清晰静态任务假设的局限性，实现了在复杂真实场景中更自适应和可靠的移动任务导航。

Abstract: Mobile GUI agents exhibit substantial potential to facilitate and automate
the execution of user tasks on mobile phones. However, exist mobile GUI agents
predominantly privilege autonomous operation and neglect the necessity of
active user engagement during task execution. This omission undermines their
adaptability to information dilemmas including ambiguous, dynamically evolving,
and conflicting task scenarios, leading to execution outcomes that deviate from
genuine user requirements and preferences. To address these shortcomings, we
propose ReInAgent, a context-aware multi-agent framework that leverages dynamic
information management to enable human-in-the-loop mobile task navigation.
ReInAgent integrates three specialized agents around a shared memory module: an
information-managing agent for slot-based information management and proactive
interaction with the user, a decision-making agent for conflict-aware planning,
and a reflecting agent for task reflection and information consistency
validation. Through continuous contextual information analysis and sustained
user-agent collaboration, ReInAgent overcomes the limitation of existing
approaches that rely on clear and static task assumptions. Consequently, it
enables more adaptive and reliable mobile task navigation in complex,
real-world scenarios. Experimental results demonstrate that ReInAgent
effectively resolves information dilemmas and produces outcomes that are more
closely aligned with genuine user preferences. Notably, on complex tasks
involving information dilemmas, ReInAgent achieves a 25% higher success rate
than Mobile-Agent-v2.

</details>


### [118] [Language Models Do Not Embed Numbers Continuously](https://arxiv.org/abs/2510.08009)
*Alex O. Davies,Roussel Nzoyem,Nirav Ajmeri,Telmo M. Silva Filho*

Main category: cs.AI

TL;DR: 研究发现语言模型在数值表示上存在非连续性和显著噪声，尽管能高保真重建数值，但主成分分析显示嵌入空间大部分成分与简单数值输入空间正交，且精度增加时重建质量和解释方差会下降。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型是否真正将连续值建模为连续表示，以及它们在数值表示方面的实际表现。

Method: 使用三个主要提供商（OpenAI、Google Gemini和Voyage AI）的语言模型，通过线性重建和主成分分析评估嵌入空间的预期属性。

Result: 虽然重建可以达到高保真度（R²≥0.95），但主成分仅能解释嵌入空间变异的一小部分，表明许多嵌入空间成分与简单数值输入空间正交。随着十进制精度的增加，线性重建和解释方差都会下降。

Conclusion: 语言模型不仅将数值空间表示为非连续的，还引入了显著噪声，这对需要高数值精度、大数值范围或混合符号值的应用领域具有重要影响。

Abstract: Recent research has extensively studied how large language models manipulate
integers in specific arithmetic tasks, and on a more fundamental level, how
they represent numeric values. These previous works have found that language
model embeddings can be used to reconstruct the original values, however, they
do not evaluate whether language models actually model continuous values as
continuous. Using expected properties of the embedding space, including linear
reconstruction and principal component analysis, we show that language models
not only represent numeric spaces as non-continuous but also introduce
significant noise. Using models from three major providers (OpenAI, Google
Gemini and Voyage AI), we show that while reconstruction is possible with high
fidelity ($R^2 \geq 0.95$), principal components only explain a minor share of
variation within the embedding space. This indicates that many components
within the embedding space are orthogonal to the simple numeric input space.
Further, both linear reconstruction and explained variance suffer with
increasing decimal precision, despite the ordinal nature of the input space
being fundamentally unchanged. The findings of this work therefore have
implications for the many areas where embedding models are used, in-particular
where high numerical precision, large magnitudes or mixed-sign values are
common.

</details>


### [119] [PEAR: Phase Entropy Aware Reward for Efficient Reasoning](https://arxiv.org/abs/2510.08026)
*Chen Huang,Wei Lu,Wenxuan Zhang*

Main category: cs.AI

TL;DR: PEAR通过基于推理阶段熵的奖励机制，在不牺牲准确性的前提下减少大推理模型的冗长推理过程，实现自适应长度控制。


<details>
  <summary>Details</summary>
Motivation: 大推理模型生成的推理过程往往过于冗长，包含冗余推理步骤，增加了推理成本并降低了可用性。如何在保持准确性的同时控制推理长度是一个开放挑战。

Method: 提出PEAR奖励机制，将阶段依赖性熵纳入奖励设计，在思考阶段惩罚过高熵值，在最终答案阶段允许适度探索，鼓励生成简洁但有效的推理轨迹。

Result: 在四个基准测试上的广泛实验表明，PEAR能持续减少响应长度，同时在不同模型规模下保持竞争力准确率，并展现出强大的分布外鲁棒性。

Conclusion: PEAR通过熵感知的奖励设计实现了推理长度的自适应控制，为平衡推理模型的简洁性和性能提供了有效解决方案。

Abstract: Large Reasoning Models (LRMs) have achieved impressive performance on complex
reasoning tasks by generating detailed chain-of-thought (CoT) explanations.
However, these responses are often excessively long, containing redundant
reasoning steps that inflate inference cost and reduce usability. Controlling
the length of generated reasoning without sacrificing accuracy remains an open
challenge. Through a systematic empirical analysis, we reveal a consistent
positive correlation between model entropy and response length at different
reasoning stages across diverse LRMs: the thinking phase exhibits higher
entropy, reflecting exploratory behavior of longer responses, while the final
answer phase shows lower entropy, indicating a more deterministic solution.This
observation suggests that entropy at different reasoning stages can serve as a
control knob for balancing conciseness and performance. Based on this insight,
this paper introduces Phase Entropy Aware Reward (PEAR), a reward mechanism
that incorporating phase-dependent entropy into the reward design. Instead of
treating all tokens uniformly, PEAR penalize excessive entropy during the
thinking phase and allowing moderate exploration at the final answer phase,
which encourages models to generate concise reasoning traces that retain
sufficient flexibility to solve the task correctly. This enables adaptive
control of response length without relying on explicit length targets or rigid
truncation rules. Extensive experiments across four benchmarks demonstrate that
PEAR consistently reduces response length while sustaining competitive accuracy
across model scales. In addition, PEAR demonstrates strong out-of-distribution
(OOD) robustness beyond the training distribution. Our code is available at:
https://github.com/iNLP-Lab/PEAR.

</details>


### [120] [AILoRA: Function-Aware Asymmetric Initialization for Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2510.08034)
*Xiaoshuang Ji,Zhendong Zhao,Xiaoyan Gu,Xiaojun Chen,Xin Zhao,Zeyao Liu*

Main category: cs.AI

TL;DR: 提出了AILoRA方法，通过函数感知的非对称低秩先验来解决LoRA在参数高效微调中的性能不足和收敛慢问题。


<details>
  <summary>Details</summary>
Motivation: LoRA虽然被广泛采用，但在实际部署中仍面临性能不优和收敛慢的挑战，需要改进其参数初始化策略。

Method: AILoRA引入函数感知的非对称低秩先验，对自注意力模块中的W^Q和W^V投影矩阵采用不同的初始化策略：为W^Q注入主成分以保留任务适应能力，为W^V注入次要成分以保持通用特征表示。

Result: 该方法能够更好地捕捉注意力参数的专业化角色，从而提高微调性能和收敛效率。

Conclusion: AILoRA通过考虑W^Q和W^V的功能差异，实现了更有效的参数高效微调，解决了LoRA的局限性。

Abstract: Parameter-efficient finetuning (PEFT) aims to mitigate the substantial
computational and memory overhead involved in adapting large-scale pretrained
models to diverse downstream tasks. Among numerous PEFT strategies, Low-Rank
Adaptation (LoRA) has emerged as one of the most widely adopted approaches due
to its robust empirical performance and low implementation complexity. In
practical deployment, LoRA is typically applied to the $W^Q$ and $W^V$
projection matrices of self-attention modules, enabling an effective trade-off
between model performance and parameter efficiency. While LoRA has achieved
considerable empirical success, it still encounters challenges such as
suboptimal performance and slow convergence. To address these limitations, we
introduce \textbf{AILoRA}, a novel parameter-efficient method that incorporates
function-aware asymmetric low-rank priors. Our empirical analysis reveals that
the projection matrices $W^Q$ and $W^V$ in the self-attention mechanism exhibit
distinct parameter characteristics, stemming from their functional differences.
Specifically, $W^Q$ captures task-specific semantic space knowledge essential
for attention distributions computation, making its parameters highly sensitive
to downstream task variations. In contrast, $W^V$ encodes token-level feature
representations that tend to remain stable across tasks and layers. Leveraging
these insights, AILoRA performs a function-aware initialization by injecting
the principal components of $W^Q$ to retain task-adaptive capacity, and the
minor components of $W^V$ to preserve generalizable feature representations.
This asymmetric initialization strategy enables LoRA modules to better capture
the specialized roles of attention parameters, thereby enhancing both
finetuning performance and convergence efficiency.

</details>


### [121] [LinguaSim: Interactive Multi-Vehicle Testing Scenario Generation via Natural Language Instruction Based on Large Language Models](https://arxiv.org/abs/2510.08046)
*Qingyuan Shi,Qingwen Meng,Hao Cheng,Qing Xu,Jianqiang Wang*

Main category: cs.AI

TL;DR: LinguaSim是一个基于大语言模型的框架，能够将自然语言转换为真实、交互式的3D场景，确保动态车辆交互和输入描述与生成场景的准确对齐。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的场景生成方法难以平衡指令遵循准确性与真实世界驾驶环境的真实性，通常通过限制场景为2D或开放循环模拟来降低复杂度，牺牲了真实感。

Method: 提出LinguaSim框架，包含反馈校准模块来优化生成精度，通过自然语言与闭环交互模拟的桥梁，约束对抗性车辆行为。

Result: 实验显示LinguaSim能生成不同关键性场景（危险描述ACT：0.072秒 vs 安全描述：3.532秒；舒适度：0.654 vs 0.764），精炼模块将碰撞率从46.9%降至6.3%。

Conclusion: LinguaSim能够创建高保真场景，增强安全测试和训练，有效弥合自然语言与交互式模拟之间的差距。

Abstract: The generation of testing and training scenarios for autonomous vehicles has
drawn significant attention. While Large Language Models (LLMs) have enabled
new scenario generation methods, current methods struggle to balance command
adherence accuracy with the realism of real-world driving environments. To
reduce scenario description complexity, these methods often compromise realism
by limiting scenarios to 2D, or open-loop simulations where background vehicles
follow predefined, non-interactive behaviors. We propose LinguaSim, an
LLM-based framework that converts natural language into realistic, interactive
3D scenarios, ensuring both dynamic vehicle interactions and faithful alignment
between the input descriptions and the generated scenarios. A feedback
calibration module further refines the generation precision, improving fidelity
to user intent. By bridging the gap between natural language and closed-loop,
interactive simulations, LinguaSim constrains adversarial vehicle behaviors
using both the scenario description and the autonomous driving model guiding
them. This framework facilitates the creation of high-fidelity scenarios that
enhance safety testing and training. Experiments show LinguaSim can generate
scenarios with varying criticality aligned with different natural language
descriptions (ACT: 0.072 s for dangerous vs. 3.532 s for safe descriptions;
comfortability: 0.654 vs. 0.764), and its refinement module effectively reduces
excessive aggressiveness in LinguaSim's initial outputs, lowering the crash
rate from 46.9% to 6.3% to better match user intentions.

</details>


### [122] [Multi-Condition Conformal Selection](https://arxiv.org/abs/2510.08075)
*Qingyang Hao,Wenbo Liao,Bingyi Jing,Hongxin Wei*

Main category: cs.AI

TL;DR: 提出了多条件符合选择(MCCS)算法，将符合选择扩展到多条件场景，包括合取和析取条件，实现了有限样本FDR控制的理论保证。


<details>
  <summary>Details</summary>
Motivation: 在药物发现、精准医学和大语言模型对齐等资源受限应用中，从大规模数据集中选择高质量候选者至关重要。现有符合选择方法仅限于单阈值场景，无法满足多条件选择的实际需求。

Method: 提出了MCCS算法，为合取条件引入了具有区域单调性的新非符合性评分，为析取条件引入了全局Benjamini-Hochberg(BH)程序。

Result: 广泛的实验验证了MCCS相对于基线的优越性，其在多样化条件组合、不同现实世界模态和多任务可扩展性方面具有良好的泛化能力。

Conclusion: MCCS算法能够在各种多条件环境中实现严格的FDR控制选择，解决了现有方法在多条件场景下的局限性。

Abstract: Selecting high-quality candidates from large-scale datasets is critically
important in resource-constrained applications such as drug discovery,
precision medicine, and the alignment of large language models. While conformal
selection methods offer a rigorous solution with False Discovery Rate (FDR)
control, their applicability is confined to single-threshold scenarios (i.e., y
> c) and overlooks practical needs for multi-condition selection, such as
conjunctive or disjunctive conditions. In this work, we propose the
Multi-Condition Conformal Selection (MCCS) algorithm, which extends conformal
selection to scenarios with multiple conditions. In particular, we introduce a
novel nonconformity score with regional monotonicity for conjunctive conditions
and a global Benjamini-Hochberg (BH) procedure for disjunctive conditions,
thereby establishing finite-sample FDR control with theoretical guarantees. The
integration of these components enables the proposed method to achieve rigorous
FDR-controlled selection in various multi-condition environments. Extensive
experiments validate the superiority of MCCS over baselines, its
generalizability across diverse condition combinations, different real-world
modalities, and multi-task scalability.

</details>


### [123] [AutoQual: An LLM Agent for Automated Discovery of Interpretable Features for Review Quality Assessment](https://arxiv.org/abs/2510.08081)
*Xiaochong Lan,Jie Feng,Yinxing Liu,Xinlei Shi,Yong Li*

Main category: cs.AI

TL;DR: AutoQual是一个基于LLM的智能体框架，用于自动化发现可解释特征，特别针对在线评论质量评估。它通过模拟人类研究过程，迭代生成特征假设并实现工具化，在大型电商平台上验证有效。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖手工特征，难以跨领域扩展和适应内容模式变化；现代深度学习方法缺乏可解释性且可能过度关注语义而非质量。需要一种能自动发现可解释特征的方法。

Method: AutoQual框架模拟人类研究过程：通过反思迭代生成特征假设，通过自主工具实现特征操作化，并在持久内存中积累经验。

Result: 在拥有数十亿用户的大型在线平台上部署，A/B测试显示：每位用户平均查看评论数增加0.79%，评论阅读者的转化率提高0.27%。

Conclusion: AutoQual成功将数据中的隐性知识转化为显性、可计算的特征，不仅适用于评论质量评估，还可作为通用框架应用于其他领域。

Abstract: Ranking online reviews by their intrinsic quality is a critical task for
e-commerce platforms and information services, impacting user experience and
business outcomes. However, quality is a domain-dependent and dynamic concept,
making its assessment a formidable challenge. Traditional methods relying on
hand-crafted features are unscalable across domains and fail to adapt to
evolving content patterns, while modern deep learning approaches often produce
black-box models that lack interpretability and may prioritize semantics over
quality. To address these challenges, we propose AutoQual, an LLM-based agent
framework that automates the discovery of interpretable features. While
demonstrated on review quality assessment, AutoQual is designed as a general
framework for transforming tacit knowledge embedded in data into explicit,
computable features. It mimics a human research process, iteratively generating
feature hypotheses through reflection, operationalizing them via autonomous
tool implementation, and accumulating experience in a persistent memory. We
deploy our method on a large-scale online platform with a billion-level user
base. Large-scale A/B testing confirms its effectiveness, increasing average
reviews viewed per user by 0.79% and the conversion rate of review readers by
0.27%.

</details>


### [124] [From Ethical Declarations to Provable Independence: An Ontology-Driven Optimal-Transport Framework for Certifiably Fair AI Systems](https://arxiv.org/abs/2510.08086)
*Sukriti Bhattacharya,Chitro Majumdar*

Main category: cs.AI

TL;DR: 提出一个可证明公平的AI框架，通过系统性地移除敏感信息及其代理变量来克服当前偏见缓解方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前偏见缓解方法存在局限，无法完全消除敏感信息及其代理变量对AI决策的影响，需要一种数学上可证明的公平性保证方法。

Method: 使用OWL 2 QL本体工程形式化定义敏感属性，通过逻辑推理推断代理变量，构建捕获偏见模式的sigma代数G，然后通过Delbaen Majumdar最优传输获得公平表示。

Result: 该方法能够生成与G独立的变量，同时最小化L2距离以保持准确性，保证真正的独立性而不仅仅是去相关。

Conclusion: 该框架提供了一种可认证且数学基础扎实的方法，确保在贷款审批等任务中的完全公平性，其中ZIP代码等代理变量可能揭示种族信息。

Abstract: This paper presents a framework for provably fair AI that overcomes the
limits of current bias mitigation methods by systematically removing all
sensitive information and its proxies. Using ontology engineering in OWL 2 QL,
it formally defines sensitive attributes and infers their proxies through
logical reasoning, constructing a sigma algebra G that captures the full
structure of biased patterns. Fair representations are then obtained via
Delbaen Majumdar optimal transport, which generates variables independent of G
while minimizing L2 distance to preserve accuracy. This guarantees true
independence rather than mere decorrelation. By modeling bias as dependence
between sigma algebras, compiling ontological knowledge into measurable
structures, and using optimal transport as the unique fair transformation, the
approach ensures complete fairness in tasks like loan approval, where proxies
such as ZIP code reveal race. The result is a certifiable and mathematically
grounded method for trustworthy AI.

</details>


### [125] [Can Risk-taking AI-Assistants suitably represent entities](https://arxiv.org/abs/2510.08114)
*Ali Mazyaki,Mohammad Naghizadeh,Samaneh Ranjkhah Zonouzaghi,Amirhossein Farshi Sotoudeh*

Main category: cs.AI

TL;DR: 该研究调查了语言模型在风险厌恶方面的可操纵性，发现虽然某些模型与人类行为有一定一致性，但仍存在显著差异，需要改进生物中心的可操纵性测量方法。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型越来越多地融入AI驱动的决策支持系统，理解其风险行为对于负责任部署至关重要，需要防止系统无意中将用户推向风险决策或嵌入隐藏偏见。

Method: 研究考察了语言模型在不同经济场景中复制人类风险偏好的能力，重点关注性别特定态度、不确定性、基于角色的决策以及风险厌恶的可操纵性。

Result: DeepSeek Reasoner和Gemini-2.0-flash-lite等语言模型表现出与人类行为的一些一致性，但显著差异突显了需要改进生物中心的可操纵性测量方法。

Conclusion: 研究呼吁进一步改进模型设计，确保AI系统更准确地复制人类风险偏好，从而在风险管理环境中提高其有效性，增强AI助手在风险管理中的适用性。

Abstract: Responsible AI demands systems whose behavioral tendencies can be effectively
measured, audited, and adjusted to prevent inadvertently nudging users toward
risky decisions or embedding hidden biases in risk aversion. As language models
(LMs) are increasingly incorporated into AI-driven decision support systems,
understanding their risk behaviors is crucial for their responsible deployment.
This study investigates the manipulability of risk aversion (MoRA) in LMs,
examining their ability to replicate human risk preferences across diverse
economic scenarios, with a focus on gender-specific attitudes, uncertainty,
role-based decision-making, and the manipulability of risk aversion. The
results indicate that while LMs such as DeepSeek Reasoner and
Gemini-2.0-flash-lite exhibit some alignment with human behaviors, notable
discrepancies highlight the need to refine bio-centric measures of
manipulability. These findings suggest directions for refining AI design to
better align human and AI risk preferences and enhance ethical decision-making.
The study calls for further advancements in model design to ensure that AI
systems more accurately replicate human risk preferences, thereby improving
their effectiveness in risk management contexts. This approach could enhance
the applicability of AI assistants in managing risk.

</details>


### [126] [Prepared mind, fast response: A temporal decoupling framework for adaptive knowledge orchestration in open-domain dialogue](https://arxiv.org/abs/2510.08175)
*Jinling Gan,Churong Liang,Runnan Li*

Main category: cs.AI

TL;DR: PMFR提出了一种异步知识编排框架，通过解耦知识检索与对话响应，在保持高质量的同时大幅降低延迟。


<details>
  <summary>Details</summary>
Motivation: 解决开放域对话系统中延迟与质量的根本矛盾：轻量模型延迟低但推理深度不足，工具增强代理质量高但同步执行导致延迟过高。

Method: 采用三组件架构：知识充足性评估器（实时评估）、轻量响应生成器（即时交互）、异步知识精炼代理（后台知识增强），通过智能触发机制实现异步知识编排。

Result: 在TopiOCQA上评估，PMFR实现95.3%延迟降低（23.38s→1.09s），同时保持与重量级同步基线相当的响应质量（GEval-C: 0.613 vs. 0.620）。

Conclusion: PMFR通过时间解耦框架成功解决了对话系统的延迟-质量权衡问题，证明了异步知识编排的有效性。

Abstract: The latency-quality tradeoff is a fundamental constraint in open-domain
dialogue AI systems, since comprehensive knowledge access necessitates
prohibitive response delays. Contemporary approaches offer two inadequate
solutions: lightweight instruct models achieve sub-second latency but lack
reasoning depth, while tool-augmented ReAct agents enhance factuality through
external knowledge at the cost of synchronous execution that blocks interaction
during retrieval processes. PMFR is thus proposed, with a temporal decoupling
framework that fundamentally resolves the contradiction through asynchronous
knowledge orchestration. PMFR employs three coordinated components: (1) a
Knowledge Adequacy Evaluator for real-time sufficiency assessment, (2) a
Lightweight Response Generator for immediate user interaction, and (3) an
Asynchronous Knowledge Refinement Agent for background knowledge enhancement.
This architecture maintains continuous conversational flow while progressively
enriching knowledge coverage through intelligent triggering mechanisms.
Evaluation results on TopiOCQA demonstrate PMFR outperforms brute-force
scaling: PMFR achieves 95.3% latency reduction (23.38s -> 1.09s) while
preserving response quality comparable to heavyweight synchronous baselines
(GEval-C: 0.613 vs. 0.620).

</details>


### [127] [R-Horizon: How Far Can Your Large Reasoning Model Really Go in Breadth and Depth?](https://arxiv.org/abs/2510.08189)
*Yi Lu,Jianing Wang,Linsen Guo,Wei He,Hongyin Tang,Tao Gui,Xuanjing Huang,Xuezhi Cao,Wei Wang,Xunliang Cai*

Main category: cs.AI

TL;DR: 提出了R-HORIZON方法来评估和增强大型推理模型的长视野推理能力，通过查询组合构建多步骤推理任务，发现现有模型在长视野推理中存在显著性能下降，并利用该方法生成数据通过强化学习提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注即时、单视野任务，无法充分评估模型理解和应对复杂长视野场景的能力，需要更全面的评估方法来测试大型推理模型的长视野推理能力。

Method: 提出R-HORIZON方法，通过查询组合刺激长视野推理行为，构建包含复杂多步骤推理任务的长视野推理基准，并利用该基准生成数据用于带验证奖励的强化学习(RLVR)。

Result: 评估发现即使最先进的大型推理模型在长视野推理中也存在显著性能下降，模型表现出有限的有效推理长度和跨问题思考预算分配困难。RLVR训练使用R-HORIZON数据不仅大幅提升多视野推理任务性能，还在标准推理任务上提高准确性，AIME2024提升7.5分。

Conclusion: R-HORIZON为增强和评估大型推理模型的长视野推理能力提供了一个可扩展、可控且低成本的范式。

Abstract: Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1,
DeepSeek-R1) have led to remarkable improvements through long Chain-of-Thought
(CoT). However, existing benchmarks mainly focus on immediate, single-horizon
tasks, failing to adequately evaluate models' ability to understand and respond
to complex, long-horizon scenarios. To address this incomplete evaluation of
Large Reasoning Models (LRMs), we propose R-HORIZON, a method designed to
stimulate long-horizon reasoning behaviors in LRMs through query composition.
Based on R-HORIZON, we construct a long-horizon reasoning benchmark, comprising
complex multi-step reasoning tasks with interdependent problems that span long
reasoning horizons. Through comprehensive evaluation of LRMs using the
R-HORIZON benchmark, we find that even the most advanced LRMs suffer
significant performance degradation. Our analysis reveals that LRMs exhibit
limited effective reasoning length and struggle to allocate thinking budget
across multiple problems appropriately. Recognizing these limitations, we use
R-HORIZON to construct long-horizon reasoning data for reinforcement learning
with verified rewards (RLVR). Compared to training with single-horizon data,
RLVR with R-HORIZON not only substantially improves performance on the
multi-horizon reasoning tasks, but also promotes accuracy on standard reasoning
tasks, with an increase of 7.5 on AIME2024. These results position R-HORIZON as
a scalable, controllable, and low-cost paradigm for enhancing and evaluating
the long-horizon reasoning capabilities of LRMs.

</details>


### [128] [Measuring What Matters: The AI Pluralism Index](https://arxiv.org/abs/2510.08193)
*Rashid Mushkani*

Main category: cs.AI

TL;DR: 该论文提出了AI多元性指数（AIPI），这是一个透明、基于证据的评估工具，用于衡量AI生产商和系统在多元治理方面的表现，包括参与式治理、包容性、透明度和问责制四个支柱。


<details>
  <summary>Details</summary>
Motivation: 当前AI发展和治理集中在少数公司和国家手中，可能导致技术编码狭隘利益并限制公众参与。现有能力基准主要关注语言、视觉和编码，但缺乏可审计的多元治理衡量标准。

Method: 开发AIPI测量模型，实现可复现的评估流程，整合结构化网络和仓库分析、外部评估和专家访谈，通过评分者间一致性、覆盖度报告、跨指数相关性和敏感性分析评估可靠性。

Result: 报告了试点供应商结果，并将AIPI与邻近的透明度、安全和治理框架进行对比定位。

Conclusion: AIPI旨在引导激励措施转向多元实践，为政策制定者、采购方和公众提供可比较的证据。

Abstract: Artificial intelligence systems increasingly mediate knowledge,
communication, and decision making. Development and governance remain
concentrated within a small set of firms and states, raising concerns that
technologies may encode narrow interests and limit public agency. Capability
benchmarks for language, vision, and coding are common, yet public, auditable
measures of pluralistic governance are rare. We define AI pluralism as the
degree to which affected stakeholders can shape objectives, data practices,
safeguards, and deployment. We present the AI Pluralism Index (AIPI), a
transparent, evidence-based instrument that evaluates producers and system
families across four pillars: participatory governance, inclusivity and
diversity, transparency, and accountability. AIPI codes verifiable practices
from public artifacts and independent evaluations, explicitly handling
"Unknown" evidence to report both lower-bound ("evidence") and known-only
scores with coverage. We formalize the measurement model; implement a
reproducible pipeline that integrates structured web and repository analysis,
external assessments, and expert interviews; and assess reliability with
inter-rater agreement, coverage reporting, cross-index correlations, and
sensitivity analysis. The protocol, codebook, scoring scripts, and evidence
graph are maintained openly with versioned releases and a public adjudication
process. We report pilot provider results and situate AIPI relative to adjacent
transparency, safety, and governance frameworks. The index aims to steer
incentives toward pluralistic practice and to equip policymakers, procurers,
and the public with comparable evidence.

</details>


### [129] [The Tournament Tree Method for preference elicitation in Multi-criteria decision-making](https://arxiv.org/abs/2510.08197)
*Diego García-Zamora,Álvaro Labella,José Rui Figueira*

Main category: cs.AI

TL;DR: 提出锦标赛树方法(TTM)，一种新的偏好关系获取和评估框架，只需m-1次两两比较就能获得完整、互反且一致的比较矩阵，显著降低了认知负担和计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统两两比较方法需要m(m-1)/2次比较，认知负担重，存在不一致风险，且计算复杂度高，限制了在多准则决策中的应用。

Method: TTM方法包含三个阶段：(i)使用精简的目标比较集获取专家判断，(ii)构建一致的两两比较矩阵，(iii)从结果矩阵推导全局价值尺度。该方法将偏好建模维度从m(m-1)/2降至m个参数。

Result: TTM方法能够确保一致性，最小化认知努力，与经典卡片法兼容，可处理区间和比率尺度，并开发了基于web的工具展示其实际应用性。

Conclusion: 锦标赛树方法有效解决了传统两两比较方法的局限性，为多准则决策提供了更高效、一致的偏好建模框架。

Abstract: Pairwise comparison methods, such as Fuzzy Preference Relations and Saaty's
Multiplicative Preference Relations, are widely used to model expert judgments
in multi-criteria decision-making. However, their application is limited by the
high cognitive load required to complete $m(m-1)/2$ comparisons, the risk of
inconsistency, and the computational complexity of deriving consistent value
scales. This paper proposes the Tournament Tree Method (TTM), a novel
elicitation and evaluation framework that overcomes these limitations. The TTM
requires only $m-1$ pairwise comparisons to obtain a complete, reciprocal, and
consistent comparison matrix. The method consists of three phases: (i)
elicitation of expert judgments using a reduced set of targeted comparisons,
(ii) construction of the consistent pairwise comparison matrix, and (iii)
derivation of a global value scale from the resulting matrix. The proposed
approach ensures consistency by design, minimizes cognitive effort, and reduces
the dimensionality of preference modeling from $m(m-1)/2$ to $m$ parameters.
Furthermore, it is compatible with the classical Deck of Cards method, and thus
it can handle interval and ratio scales. We have also developed a web-based
tool that demonstrates its practical applicability in real decision-making
scenarios.

</details>


### [130] [DODO: Causal Structure Learning with Budgeted Interventions](https://arxiv.org/abs/2510.08207)
*Matteo Gregorini,Chiara Boldrini,Lorenzo Valerio*

Main category: cs.AI

TL;DR: DODO算法让智能体通过重复干预自主学习环境的因果结构，在噪声存在下准确推断因果有向无环图，相比观测方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 当前AI主要依赖复杂相关性，而引入因果意识能增强AI对底层机制的理解，提升性能。

Method: 智能体在与因果DAG支配的世界交互中，通过执行干预并利用因果推断技术分析观测变化的统计显著性来学习因果结构。

Result: DODO在除最有限资源条件外的所有情况下都优于观测方法，通常能以零误差重建因果图结构，在最挑战性配置中比最佳基线高出+0.25 F1分数。

Conclusion: 通过自主干预学习因果结构的方法有效，DODO算法在因果图推断任务中表现出色。

Abstract: Artificial Intelligence has achieved remarkable advancements in recent years,
yet much of its progress relies on identifying increasingly complex
correlations. Enabling causality awareness in AI has the potential to enhance
its performance by enabling a deeper understanding of the underlying mechanisms
of the environment. In this paper, we introduce DODO, an algorithm defining how
an Agent can autonomously learn the causal structure of its environment through
repeated interventions. We assume a scenario where an Agent interacts with a
world governed by a causal Directed Acyclic Graph (DAG), which dictates the
system's dynamics but remains hidden from the Agent. The Agent's task is to
accurately infer the causal DAG, even in the presence of noise. To achieve
this, the Agent performs interventions, leveraging causal inference techniques
to analyze the statistical significance of observed changes. Results show
better performance for DODO, compared to observational approaches, in all but
the most limited resource conditions. DODO is often able to reconstruct with as
low as zero errors the structure of the causal graph. In the most challenging
configuration, DODO outperforms the best baseline by +0.25 F1 points.

</details>


### [131] [Selection, Reflection and Self-Refinement: Revisit Reasoning Tasks via a Causal Lens](https://arxiv.org/abs/2510.08222)
*Yunlong Deng,Boyang Sun,Yan Li,Lingjing Kong,Zeyu Tang,Kun Zhang,Guangyi Chen*

Main category: cs.AI

TL;DR: 本文从因果视角重新审视推理任务，将推理建模为选择机制，提出SR²框架通过潜在变量反馈来学习密集依赖关系，在推理准确性上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 尽管人类能轻松解决推理任务，但现有大规模语言模型仍难以可靠地进行推理。本文旨在从因果角度理解推理任务在潜在空间中的行为，为解决推理挑战提供见解。

Method: 提出SR²框架，包含三个关键模块：反射表示学习、依赖自精炼和周期性中间对齐。该框架将估计的潜在变量作为反馈纳入选择机制，促进潜在表示间密集依赖关系的学习。

Result: 实验表明该方法在推理准确性上带来显著提升，例如在数独和迷宫任务上，使用8倍更少的参数实现了超过10%的性能改进。

Conclusion: 从因果视角将推理任务建模为选择机制，并通过潜在变量反馈学习密集依赖关系的框架，能够有效提升模型的推理能力。

Abstract: Due to their inherent complexity, reasoning tasks have long been regarded as
rigorous benchmarks for assessing the capabilities of machine learning models,
especially large language models (LLMs). Although humans can solve these tasks
with ease, existing models, even after extensive pre-training and post-training
at scale, still fail to perform reasoning reliably. In this paper, we revisit
reasoning tasks from a causal perspective, seeking to understand their behavior
in latent space and to offer insights for addressing their challenges.
Specifically, we cast reasoning tasks as a selection mechanism, in which
high-level logical concepts function as selection operators on the given
observations, such as, identifying the correct answer in a math problem or
filling the appropriate entry in Sudoku. We emphasize two key properties of
this formulation that shed light on the difficulty of reasoning tasks. First,
the latent space exceeds the observation space in complexity, even when the
correct answer is fully determined by the observed input. Second, the latent
variables, corresponding to logical thought, are densely structured and exhibit
strong dependencies. Building on this formulation, we introduce a framework,
called SR$^2$, that incorporates the estimated latent variables as feedback
into the selection mechanism, thereby facilitating the learning of dense
dependencies among latent representations. The framework consists of three key
modules: reflective representation learning, dependency self-refinement, and
periodic intermediate alignment. Experimentally, we show that our approach
yields significant gains in reasoning accuracy, for example, attaining over
10$\%$ improvement in performance with 8$\times$ fewer parameters on the Sudoku
and Maze tasks over the recent advances.

</details>


### [132] [Chain-of-Trigger: An Agentic Backdoor that Paradoxically Enhances Agentic Robustness](https://arxiv.org/abs/2510.08238)
*Jiyang Qiu,Xinbei Ma,Yunqing Xu,Zhuosheng Zhang,Hai Zhao*

Main category: cs.AI

TL;DR: 提出CoTri多步后门攻击，通过有序触发序列实现对LLM智能体的长期操控，攻击成功率接近100%且误触发率接近零，同时意外增强了智能体在良性任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体在现实应用中的快速部署，其安全性和鲁棒性引发严重担忧，需要揭示这类智能体的安全漏洞。

Method: 设计Chain-of-Trigger Backdoor (CoTri)多步后门攻击，使用有序触发序列，初始触发后从环境中提取后续触发，实现多步操控使智能体偏离原任务。

Result: CoTri达到接近完美的攻击成功率，误触发率接近零。由于训练数据建模了环境的随机性，植入CoTri反而增强了智能体在良性任务上的性能和对环境干扰的鲁棒性。

Conclusion: CoTri实现了对智能体的稳定多步控制，同时提高了其固有鲁棒性和任务能力，使攻击更加隐蔽，带来潜在安全风险。

Abstract: The rapid deployment of large language model (LLM)-based agents in real-world
applications has raised serious concerns about their trustworthiness. In this
work, we reveal the security and robustness vulnerabilities of these agents
through backdoor attacks. Distinct from traditional backdoors limited to
single-step control, we propose the Chain-of-Trigger Backdoor (CoTri), a
multi-step backdoor attack designed for long-horizon agentic control. CoTri
relies on an ordered sequence. It starts with an initial trigger, and
subsequent ones are drawn from the environment, allowing multi-step
manipulation that diverts the agent from its intended task. Experimental
results show that CoTri achieves a near-perfect attack success rate (ASR) while
maintaining a near-zero false trigger rate (FTR). Due to training data modeling
the stochastic nature of the environment, the implantation of CoTri
paradoxically enhances the agent's performance on benign tasks and even
improves its robustness against environmental distractions. We further validate
CoTri on vision-language models (VLMs), confirming its scalability to
multimodal agents. Our work highlights that CoTri achieves stable, multi-step
control within agents, improving their inherent robustness and task
capabilities, which ultimately makes the attack more stealthy and raises
potential safty risks.

</details>


### [133] [Co-TAP: Three-Layer Agent Interaction Protocol Technical Report](https://arxiv.org/abs/2510.08263)
*Shunyu An,Miao Wang,Yongchao Li,Dong Wan,Lina Wang,Ling Qin,Liqin Gao,Congyao Fan,Zhiyong Mao,Jiange Pu,Wenji Xia,Dong Zhao,Rui Hu,Ji Lu,Guiyue Zhou,Baoyu Tang,Yanqin Gao,Yongsheng Du,Daigang Xu,Lingjun Huang,Baoli Wang,Xiwen Zhang,Luyao Wang,Shilong Liu*

Main category: cs.AI

TL;DR: Co-TAP是一个三层代理交互协议，通过HAI、UAP和MEK三个核心协议解决多智能体系统在互操作性、交互协作和知识共享方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体系统在三个核心维度面临的挑战：互操作性、交互协作和知识共享，为构建下一代高效、可扩展和智能的多智能体应用提供基础。

Method: 设计了三层协议：HAI（人机交互协议）标准化用户、界面和代理之间的信息流；UAP（统一代理协议）通过统一服务发现和协议转换实现异构代理互连；MEK（记忆-提取-知识协议）建立标准化认知链实现知识共享。

Result: 提出了一个完整的协议框架，能够确保实时性能、可靠性、交互协同性，实现异构代理的无缝互连，并为实现真正集体智能奠定基础。

Conclusion: Co-TAP协议框架为构建下一代高效、可扩展和智能的多智能体应用提供了坚实的工程基础和理论指导。

Abstract: This paper proposes Co-TAP (T: Triple, A: Agent, P: Protocol), a three-layer
agent interaction protocol designed to address the challenges faced by
multi-agent systems across the three core dimensions of Interoperability,
Interaction and Collaboration, and Knowledge Sharing. We have designed and
proposed a layered solution composed of three core protocols: the Human-Agent
Interaction Protocol (HAI), the Unified Agent Protocol (UAP), and the
Memory-Extraction-Knowledge Protocol (MEK). HAI focuses on the interaction
layer, standardizing the flow of information between users, interfaces, and
agents by defining a standardized, event-driven communication paradigm. This
ensures the real-time performance, reliability, and synergy of interactions. As
the core of the infrastructure layer, UAP is designed to break down
communication barriers among heterogeneous agents through unified service
discovery and protocol conversion mechanisms, thereby enabling seamless
interconnection and interoperability of the underlying network. MEK, in turn,
operates at the cognitive layer. By establishing a standardized ''Memory (M) -
Extraction (E) - Knowledge (K)'' cognitive chain, it empowers agents with the
ability to learn from individual experiences and form shareable knowledge,
thereby laying the foundation for the realization of true collective
intelligence. We believe this protocol framework will provide a solid
engineering foundation and theoretical guidance for building the next
generation of efficient, scalable, and intelligent multi-agent applications.

</details>


### [134] [Symmetry-Aware Fully-Amortized Optimization with Scale Equivariant Graph Metanetworks](https://arxiv.org/abs/2510.08300)
*Bart Kuipers,Freek Byrman,Daniel Uyterlinde,Alejandro García-Castellanos*

Main category: cs.AI

TL;DR: 该论文提出使用尺度等变图元网络(ScaleGMNs)进行摊销优化，通过在权重空间中直接操作实现现有模型的单次微调，减少迭代优化需求。理论分析表明卷积神经网络的尺度对称性诱导的规范自由度比多层感知机更小，这解释了不同架构间的性能差异。


<details>
  <summary>Details</summary>
Motivation: 探索利用尺度等变图元网络来加速相关优化问题的求解，通过学习利用问题实例间共享结构的映射，减少迭代优化的需求。

Method: 使用尺度等变图元网络(ScaleGMNs)直接在权重空间中操作，实现对现有模型的单次微调，避免传统的迭代优化过程。

Result: 实证证明了该方法的有效性，并提供了理论结果：卷积神经网络中尺度对称性诱导的规范自由度比多层感知机更小，这解释了不同架构间的性能差异。

Conclusion: 对称感知的元网络是高效且可泛化的神经网络优化的有力方法，具有重要潜力。

Abstract: Amortized optimization accelerates the solution of related optimization
problems by learning mappings that exploit shared structure across problem
instances. We explore the use of Scale Equivariant Graph Metanetworks
(ScaleGMNs) for this purpose. By operating directly in weight space, ScaleGMNs
enable single-shot fine-tuning of existing models, reducing the need for
iterative optimization. We demonstrate the effectiveness of this approach
empirically and provide a theoretical result: the gauge freedom induced by
scaling symmetries is strictly smaller in convolutional neural networks than in
multi-layer perceptrons. This insight helps explain the performance differences
observed between architectures in both our work and that of Kalogeropoulos et
al. (2024). Overall, our findings underscore the potential of symmetry-aware
metanetworks as a powerful approach for efficient and generalizable neural
network optimization. Open-source code:
https://github.com/daniuyter/scalegmn_amortization

</details>


### [135] [First Try Matters: Revisiting the Role of Reflection in Reasoning Models](https://arxiv.org/abs/2510.08308)
*Liwei Kang,Yue Deng,Yao Xiao,Zhanfeng Mo,Wee Sun Lee,Lidong Bing*

Main category: cs.AI

TL;DR: 该论文分析了大语言模型中的反思行为，发现反思主要是确认性的，很少改变初始答案。通过实验表明，训练更多反思步骤主要提高首次答案正确率，而非通过反思纠正错误的能力。作者提出了一种问题感知的早停方法，可减少24.5%的推理token，准确率仅下降2.9%。


<details>
  <summary>Details</summary>
Motivation: 研究反思行为在大语言模型推理能力提升中的实际贡献，探索反思是否真正有助于纠正初始错误答案，以及如何提高推理效率。

Method: 系统分析8个推理模型在5个数学数据集上的推理过程；构建不同反思步骤的SFT数据集进行训练；提出问题感知早停方法，在生成候选答案后动态截断反思。

Result: 反思主要是确认性的，很少改变初始答案；训练更多反思步骤主要提高首次答案正确率；早停方法可减少24.5%推理token，准确率仅下降2.9%。

Conclusion: 反思在推理中主要起确认作用而非纠错作用；通过动态截断反思可以显著提高推理效率，同时保持较高的准确率。

Abstract: Large language models have recently demonstrated significant gains in
reasoning ability, often attributed to their capacity to generate longer chains
of thought and engage in reflective reasoning. However, the contribution of
reflections to performance improvement remains unclear. In this paper, we
systematically analyze the rollouts of eight reasoning models on five
mathematical datasets. We focus on reflective behaviours where the model has
already produced an answer but continues reflecting before finalizing its
output. Our analysis reveals that reflections are predominantly confirmatory
and rarely alter the model's initial answer, a pattern consistent across models
and datasets. To understand the role of reflections in training, we construct
supervised fine-tuning (SFT) datasets with varying amounts of reflection steps.
We observe that training models on rollouts with more reflection steps
primarily enhances first-answer correctness rather than the ability to correct
initially wrong answers through reflections. This motivates us to propose a
question-aware early-stopping method that enhances inference-time token
efficiency by stopping the reasoning process once a few plausible candidate
answers are generated, thereby reducing unnecessary reflection steps. Motivated
by this, we further propose to dynamically truncate the reflections after a
candidate answer has appeared during generation, which reduces reasoning tokens
by 24.5% across five mathematical datasets, within a 2.9% drop in accuracy.

</details>


### [136] [Beyond Pass@k: Breadth-Depth Metrics for Reasoning Boundaries](https://arxiv.org/abs/2510.08325)
*Marius Dragoi,Ioana Pintilie,Florin Gogianu,Florin Brad*

Main category: cs.AI

TL;DR: 本文提出Cover@tau指标替代Pass@k来评估LLM在推理任务中的表现，认为Pass@k在大采样预算下会高估随机猜测的效果，而Cover@tau能更好地反映模型的真实推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法Pass@k在大采样预算下存在误导性，因为它反映了在大量尝试下的成功概率而非真实推理能力，特别是在离散答案空间的任务中。

Method: 提出Cover@tau评估指标，衡量模型在至少tau比例补全正确的情况下能解决的问题比例，该指标能有效识别依赖随机猜测的模型。

Result: 使用Cover@tau评估多个RLVR模型，发现与Pass@1相比，流行算法的相对排名发生了变化，提供了对推理边界的不同视角。

Conclusion: Cover@tau提供了更可靠的模型推理能力评估方法，能更好地区分真实推理和随机猜测，为RLVR模型评估提供了新标准。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
powerful paradigm to improve Large Language Models on reasoning tasks such as
coding, math or logic. To assess the reasoning boundary (the fraction of
problems a model can solve) researchers often report Pass@k at large sampling
budgets. Recent results reveal a crossover phenomenon: while RLVR models
outperform the base model at small k values, the base model usually outperforms
them when sampling a very large number of completions. This has been
interpreted as evidence that base models have a larger reasoning boundary. We
argue that on tasks with discrete answer spaces, such as math with numeric
outputs, Pass@k at large k reflects the increasingly higher chance of success
in the limit of the number of trials rather than genuine reasoning, and can
therefore be misleading. We propose Cover@tau, which measures the fraction of
problems that a model can solve for which at least a tau proportion of
completions are correct. Unlike Pass@k, Cover@tau captures reasoning under an
explicit reliability threshold: models that rely on random guessing degrade
rapidly as tau increases. We evaluate several RLVR models using Cover@tau-based
metrics and illustrate how the relative rankings of popular algorithms change
compared to Pass@1, offering a different perspective on reasoning boundaries.

</details>


### [137] [LLMs Reproduce Human Purchase Intent via Semantic Similarity Elicitation of Likert Ratings](https://arxiv.org/abs/2510.08338)
*Benjamin F. Maier,Ulf Aslak,Luca Fiaschi,Nina Rismal,Kemble Fletcher,Christian C. Luhmann,Robbie Dow,Kli Pappas,Thomas V. Wiecki*

Main category: cs.AI

TL;DR: 提出语义相似度评分(SSR)方法，使用LLMs生成文本响应并通过嵌入相似度映射到Likert分布，解决传统消费者研究中的面板偏差和规模限制问题。


<details>
  <summary>Details</summary>
Motivation: 传统消费者研究成本高昂且存在面板偏差和规模限制，LLMs虽能模拟合成消费者但直接获取数值评分会产生不现实的响应分布。

Method: SSR方法：从LLMs获取文本响应，通过嵌入相似度将这些文本映射到参考声明的Likert分布。

Result: 在57个个人护理产品调查数据集上测试，SSR达到人类测试-重测可靠性的90%，同时保持现实的响应分布(KS相似度>0.85)。

Conclusion: 该框架能够实现可扩展的消费者研究模拟，同时保留传统调查指标和可解释性，合成受访者还能提供丰富的定性反馈。

Abstract: Consumer research costs companies billions annually yet suffers from panel
biases and limited scale. Large language models (LLMs) offer an alternative by
simulating synthetic consumers, but produce unrealistic response distributions
when asked directly for numerical ratings. We present semantic similarity
rating (SSR), a method that elicits textual responses from LLMs and maps these
to Likert distributions using embedding similarity to reference statements.
Testing on an extensive dataset comprising 57 personal care product surveys
conducted by a leading corporation in that market (9,300 human responses), SSR
achieves 90% of human test-retest reliability while maintaining realistic
response distributions (KS similarity > 0.85). Additionally, these synthetic
respondents provide rich qualitative feedback explaining their ratings. This
framework enables scalable consumer research simulations while preserving
traditional survey metrics and interpretability.

</details>


### [138] [QAgent: A modular Search Agent with Interactive Query Understanding](https://arxiv.org/abs/2510.08383)
*Yi Jiang,Lei Shen,Lujie Niu,Sendong Zhao,Wenbo Su,Bo Zheng*

Main category: cs.AI

TL;DR: 提出QAgent框架，通过强化学习训练搜索代理进行自适应检索，解决传统RAG在复杂查询理解和部署方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统RAG在复杂查询理解方面存在困难，基于强化学习的搜索代理虽然前景广阔，但仍面临泛化性和部署挑战。

Method: 采用模块化搜索代理进行多步决策，通过强化学习训练以最大化检索质量，支持准确的下游答案生成。

Result: 实验表明QAgent在问答任务上表现优异，可作为即插即用模块用于实际部署。

Conclusion: QAgent通过专注于有效检索的策略，增强了LLM应用的泛化能力，为实际部署提供了可行的解决方案。

Abstract: Large language models (LLMs) excel at natural language tasks but are limited
by their static parametric knowledge, especially in knowledge-intensive task.
Retrieval-augmented generation (RAG) mitigates this by integrating external
information. However, (1) traditional RAG struggles with complex query
understanding, and (2) even search agents trained with reinforcement learning
(RL), despite their promise, still face generalization and deployment
challenges. To address these limitations, we propose QAgent, a unified agentic
RAG framework that employs a search agent for adaptive retrieval. This agent
optimizes its understanding of the query through interactive reasoning and
retrieval. To facilitate real-world application, we focus on modular search
agent for query understanding that are plug-and-play in complex systems.
Secifically, the agent follows a multi-step decision process trained with RL to
maximize retrieval quality and support accurate downstream answers. We further
analyze the strengths and weaknesses of end-to-end RL and propose a strategy
that focuses on effective retrieval, thereby enhancing generalization in LLM
applications. Experiments show QAgent excels at QA and serves as a
plug-and-play module for real-world deployment.

</details>


### [139] [Revisiting Hallucination Detection with Effective Rank-based Uncertainty](https://arxiv.org/abs/2510.08389)
*Rui Wang,Zeming Wei,Guanzhang Yue,Meng Sun*

Main category: cs.AI

TL;DR: 提出了一种基于隐藏状态有效秩的LLM幻觉检测方法，通过分析多输出和多层表示的谱特性来量化不确定性，无需额外知识或模块。


<details>
  <summary>Details</summary>
Motivation: 检测大语言模型中的幻觉对其可信部署至关重要，现有不确定性驱动的检测框架存在局限，需要更深入理解模型内部推理过程。

Method: 通过测量来自多个模型输出和不同层的隐藏状态的有效秩来量化不确定性，基于表示谱分析提供可解释的语义变化洞察。

Result: 大量实验表明该方法能有效检测幻觉，并在各种场景中表现出稳健的泛化能力。

Conclusion: 该方法为LLM真实性检测提供了新范式，结合了理论优雅性和实际效率，证明了同时量化内部和外部不确定性的必要性。

Abstract: Detecting hallucinations in large language models (LLMs) remains a
fundamental challenge for their trustworthy deployment. Going beyond basic
uncertainty-driven hallucination detection frameworks, we propose a simple yet
powerful method that quantifies uncertainty by measuring the effective rank of
hidden states derived from multiple model outputs and different layers.
Grounded in the spectral analysis of representations, our approach provides
interpretable insights into the model's internal reasoning process through
semantic variations, while requiring no extra knowledge or additional modules,
thus offering a combination of theoretical elegance and practical efficiency.
Meanwhile, we theoretically demonstrate the necessity of quantifying
uncertainty both internally (representations of a single response) and
externally (different responses), providing a justification for using
representations among different layers and responses from LLMs to detect
hallucinations. Extensive experiments demonstrate that our method effectively
detects hallucinations and generalizes robustly across various scenarios,
contributing to a new paradigm of hallucination detection for LLM truthfulness.

</details>


### [140] [Looking to Learn: Token-wise Dynamic Gating for Low-Resource Vision-Language Modelling](https://arxiv.org/abs/2510.08470)
*Bianca-Mihaela Ganescu,Suchir Salhan,Andrew Caines,Paula Buttery*

Main category: cs.AI

TL;DR: 提出轻量级解码器架构，使用动态门控自适应融合语言和视觉信息，在BabyLM挑战赛约束下实现高效多模态学习


<details>
  <summary>Details</summary>
Motivation: 在认知合理的数据量限制下训练视觉语言模型，需要重新思考模型如何整合多模态信息

Method: 轻量级解码器架构，包含：(1) 基于token的动态门控自适应融合语言和视觉线索；(2) 特征调制和通道注意力最大化有限视觉信息的效用；(3) 用于视觉定位的辅助对比目标

Result: 在五个基准测试（BLiMP、BLiMP Supplement、EWoK、Winoground和VQA）上表现出与多模态基线相当或更优的性能；动态门控在没有显式监督的情况下发现了可解释的模式，对内容词偏好视觉线索，对功能词偏好语言线索

Conclusion: 尽管存在挑战约束的限制（如全局图像嵌入造成的信息瓶颈和数据集分割导致的训练不稳定），动态门控被确立为高效多模态学习的强大工具，即使在严格约束下也能提供可解释性和性能

Abstract: Training vision-language models on cognitively-plausible amounts of data
requires rethinking how models integrate multimodal information. Within the
constraints of the Vision track for the BabyLM Challenge 2025, we propose a
lightweight decoder-based architecture with (1) token-wise dynamic gating for
adaptive fusion of linguistic and visual cues, (2) feature modulation and
channel attention to maximise the utility of limited visual information and (3)
auxiliary contrastive objectives for visual grounding. Evaluation on five
benchmarks (BLiMP, BLiMP Supplement, EWoK, Winoground and VQA) shows
competitive or superior performance to multimodal baselines. More notably, our
dynamic gate discovers interpretable patterns without explicit supervision,
favouring visual cues for content words and linguistic cues for function words.
While we identify limitations in the Challenge constraints, such as the
information bottleneck created by global image embeddings and training
instability from the dataset split, our findings establish dynamic gating as a
powerful tool for efficient multimodal learning, offering both interpretability
and performance even under severe constraints.

</details>


### [141] [AutoMLGen: Navigating Fine-Grained Optimization for Coding Agents](https://arxiv.org/abs/2510.08511)
*Shangheng Du,Xiangchao Yan,Dengyang Jiang,Jiakang Yuan,Yusong Hu,Xin Li,Liang He,Bo Zhang,Lei Bai*

Main category: cs.AI

TL;DR: AutoMLGen是一个基于LLM的编码代理，通过集成领域知识库和蒙特卡洛图搜索来解决机器学习工程任务中的挑战，在MLE-Bench上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM在机器学习工程任务中缺乏细粒度领域先验知识，传统MLE方法的线性或树状搜索限制了知识传递，无法利用完整历史轨迹或跨分支共享信息，限制了自进化能力和搜索空间多样性。

Method: 结合领域知识库提供高质量先验指导，采用蒙特卡洛图搜索（MCGS）实现动态路径重组、历史轨迹重用和多解决方案融合，支持自进化和协作学习，配合细粒度操作符集提高稳定性和收敛速度。

Result: 在MLE-Bench评估中，AutoMLGen在12小时预算（标准运行时间的一半）下，在平均奖牌率和有效提交率等多个维度实现了最先进的性能。

Conclusion: AutoMLGen通过集成领域知识库和蒙特卡洛图搜索，有效解决了LLM在机器学习工程任务中的局限性，显著提升了性能表现。

Abstract: Large language models (LLMs) have shown impressive performance in general
programming tasks. However, in Machine Learning Engineering (MLE) scenarios
such as AutoML and Kaggle competitions, achieving high performance depends
heavily on expert intervention and repeated adjustments rather than simply
generating correct code. When applied directly to these tasks, LLMs often lack
fine-grained domain priors, and existing MLE approaches that use linear or
tree-structured searches limit knowledge transfer to adjacent hierarchical
links. As a result, they cannot leverage past full trajectories or share
information across branches, limiting self-evolving ability and search space
diversity. To address these limitations, we introduce AutoMLGen, an LLM-based
coding agent that integrates a domain knowledge base for high-quality prior
guidance and Monte Carlo Graph Search (MCGS) for efficient exploration. MCGS
retains the tree-guided exploration of MCTS while embedding a graph structure
into the expansion stage to enable dynamic path reorganization, historical
trajectory reuse, and multi-solution fusion to support both self-evolution and
collaborative learning. Combined with fine-grained operator sets, this design
improves stability and accelerates convergence. Evaluation on the MLE-Bench
shows that AutoMLGen achieves state-of-the-art performance in numerous
dimensions, such as the average medal rate and the valid submission rate, under
a 12-hour budget (half the standard runtime). The code is available at
https://github.com/Alpha-Innovator/InternAgent.

</details>


### [142] [CaRT: Teaching LLM Agents to Know When They Know Enough](https://arxiv.org/abs/2510.08517)
*Grace Liu,Yuxiao Qu,Jeff Schneider,Aarti Singh,Aviral Kumar*

Main category: cs.AI

TL;DR: CaRT方法通过反事实轨迹对和语言推理训练LLMs，教会模型何时停止信息收集，提高多轮交互任务中的效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 多轮交互任务需要模型既能有效收集信息，又能适时停止收集并做出决策，避免过度思考或偏离主题。

Method: 使用反事实轨迹对进行微调，训练LLMs解释终止决策的推理过程，将这种能力融入基础模型。

Result: 在医疗诊断和数学问题解决两个领域，CaRT相比其他微调方法提高了信息收集效率和任务成功率。

Conclusion: CaRT方法有效解决了多轮交互任务中的终止决策问题，通过反事实推理训练提升了模型的战略信息收集能力。

Abstract: Many tasks require learned models to strategically gather relevant
information over multiple rounds of interaction before actually acting on a
task. Strategic information gathering requires models to know not only how to
effectively acquire information, but also when to stop gathering information
and make a decision, in order to avoid overthinking or getting derailed when
acting. In this paper, we formalize this problem and introduce Counterfactuals
and Reasoning for Termination (CaRT), an approach for teaching LLMs when to
stop seeking information. To appropriately learn when to terminate, CaRT
fine-tunes LLMs using counterfactual pairs of trajectories, one where
termination is appropriate and a minimally modified version of the same
trajectory where it is not. It trains the LLM to explain the rationale for the
termination decision in either case via verbal reasoning, and imbues this
capability into the base LLM via fine-tuning. We instantiate CaRT in two
domains: interactive medical diagnosis and math problem solving. In both
domains, we find that CaRT improves the efficiency of information gathering and
task success rate compared to other fine-tuning methods.

</details>


### [143] [FlowSearch: Advancing deep research with dynamic structured knowledge flow](https://arxiv.org/abs/2510.08521)
*Yusong Hu,Runmin Ma,Yue Fan,Jinxin Shi,Zongsheng Cao,Yuhao Zhou,Jiakang Yuan,Xiangchao Yan,Wenlong Zhang,Lei Bai,Bo Zhang*

Main category: cs.AI

TL;DR: FlowSearch是一个多智能体框架，通过构建动态结构化知识流来驱动子任务执行和推理，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 深度研究需要广度和深度的思考，涉及导航多样化知识空间和复杂多步依赖推理，这对智能体系统提出了重大挑战。

Method: 提出FlowSearch多智能体框架，主动构建和演化动态结构化知识流，支持并行探索和层次化任务分解，并根据中间推理结果实时调整知识流。

Result: 在GAIA、HLE、GPQA和TRQA等通用和科学基准测试中达到最先进性能，展示了在多学科研究场景中的有效性。

Conclusion: FlowSearch框架在深度研究任务中表现出色，具有推进科学发现的潜力。

Abstract: Deep research is an inherently challenging task that demands both breadth and
depth of thinking. It involves navigating diverse knowledge spaces and
reasoning over complex, multi-step dependencies, which presents substantial
challenges for agentic systems. To address this, we propose FlowSearch, a
multi-agent framework that actively constructs and evolves a dynamic structured
knowledge flow to drive subtask execution and reasoning. FlowSearch is capable
of strategically planning and expanding the knowledge flow to enable parallel
exploration and hierarchical task decomposition, while also adjusting the
knowledge flow in real time based on feedback from intermediate reasoning
outcomes and insights. FlowSearch achieves state-of-the-art performance on both
general and scientific benchmarks, including GAIA, HLE, GPQA and TRQA,
demonstrating its effectiveness in multi-disciplinary research scenarios and
its potential to advance scientific discovery. The code is available at
https://github.com/Alpha-Innovator/InternAgent.

</details>


### [144] [Agent Learning via Early Experience](https://arxiv.org/abs/2510.08558)
*Kai Zhang,Xiangchao Chen,Bo Liu,Tianci Xue,Zeyi Liao,Zhihan Liu,Xiyao Wang,Yuting Ning,Zhaorun Chen,Xiaohan Fu,Jian Xie,Yuxuan Sun,Boyu Gou,Qi Qi,Zihang Meng,Jianwei Yang,Ning Zhang,Xian Li,Ashish Shah,Dat Huynh,Hengduo Li,Zi Yang,Sara Cao,Lawrence Jang,Shuyan Zhou,Jiacheng Zhu,Huan Sun,Jason Weston,Yu Su,Yifan Wu*

Main category: cs.AI

TL;DR: 提出"早期经验"范式，使用智能体自身交互数据作为监督信号，通过隐式世界建模和自我反思两种策略提升语言智能体的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前语言智能体主要依赖专家数据进行监督微调，但专家数据覆盖场景有限且缺乏环境多样性。强化学习在缺乏可验证奖励或需要长序列交互的环境中训练困难。

Method: 1) 隐式世界建模：利用收集的状态信息让策略基于环境动态；2) 自我反思：智能体从次优行动中学习以改进推理和决策。在八个多样化环境中评估。

Result: 方法在多个模型家族中一致提升有效性和跨领域泛化能力。在有可验证奖励的环境中，为后续强化学习提供良好基础。

Conclusion: 早期经验是模仿学习和完全经验驱动智能体之间的实用桥梁，展示了通过自身经验学习提升性能的潜力。

Abstract: A long-term goal of language agents is to learn and improve through their own
experience, ultimately outperforming humans in complex, real-world tasks.
However, training agents from experience data with reinforcement learning
remains difficult in many environments, which either lack verifiable rewards
(e.g., websites) or require inefficient long-horizon rollouts (e.g., multi-turn
tool use). As a result, most current agents rely on supervised fine-tuning on
expert data, which is challenging to scale and generalizes poorly. This
limitation stems from the nature of expert demonstrations: they capture only a
narrow range of scenarios and expose the agent to limited environment
diversity. We address this limitation with a middle-ground paradigm we call
early experience: interaction data generated by the agent's own actions, where
the resulting future states serve as supervision without reward signals. Within
this paradigm we study two strategies of using such data: (1) Implicit world
modeling, which uses collected states to ground the policy in environment
dynamics; and (2) Self-reflection, where the agent learns from its suboptimal
actions to improve reasoning and decision-making. We evaluate across eight
diverse environments and multiple model families. Our approaches consistently
improve effectiveness and out-of-domain generalization, highlighting the value
of early experience. Moreover, in environments with verifiable rewards, our
results provide promising signals that early experience offers a strong
foundation for subsequent reinforcement learning, positioning it as a practical
bridge between imitation learning and fully experience-driven agents.

</details>


### [145] [How to Teach Large Multimodal Models New Skills](https://arxiv.org/abs/2510.08564)
*Zhen Zhu,Yiming Gong,Yao Xiao,Yaoyao Liu,Derek Hoiem*

Main category: cs.AI

TL;DR: 该论文研究了大型多模态模型在顺序微调中的遗忘问题，发现遗忘现象在后期会部分恢复，并提出两种简单的调优方法来解决此问题。


<details>
  <summary>Details</summary>
Motivation: 研究如何在教授大型多模态模型新技能的同时避免遗忘先前能力，探索顺序微调对模型性能的影响。

Method: 在五个目标技能上顺序微调，同时在八个保留基准上监控通用能力，使用输出令牌分布变化作为遗忘指标，并提出两种调优方法：仅更新自注意力投影层，或仅更新MLP的Gate&Up层。

Result: 发现窄微调后的遗忘现象在后期会部分恢复，提出的两种调优方法在多个模型和任务上都能实现强大的目标增益，同时基本保留保留性能。

Conclusion: 通过选择性更新特定层可以有效平衡新技能学习和旧能力保留，为大型多模态模型的持续学习提供了实用解决方案。

Abstract: How can we teach large multimodal models (LMMs) new skills without erasing
prior abilities? We study sequential fine-tuning on five target skills while
monitoring general ability on eight held-out benchmarks across three model
families. We observe that apparent "forgetting" on held-out tasks after narrow
fine-tuning can partly recover at later stages. We trace this behavior to a
measurable shift in the output token distribution, manifested through a simple
counting-bias probe that co-varies with forgetting. Guided by this picture, we
identify two simple, robust tuning recipes that learn strongly while limiting
drift: (i) updating only the self-attention projection layers, and (ii)
updating only the MLP Gate&Up while freezing the Down projection. Across models
and tasks, these choices deliver strong target gains while largely preserving
held-out performance. Code is available at
https://github.com/jessemelpolio/LMM_CL

</details>

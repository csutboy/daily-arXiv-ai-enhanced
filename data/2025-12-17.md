<div id=toc></div>

# Table of Contents

- [cs.CY](#cs.CY) [Total: 13]
- [stat.AP](#stat.AP) [Total: 2]
- [cs.AI](#cs.AI) [Total: 37]
- [cs.SI](#cs.SI) [Total: 2]
- [econ.EM](#econ.EM) [Total: 3]


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [1] [Research Opportunities and Challenges of the EU's Digital Services Act](https://arxiv.org/abs/2512.14223)
*Francesco Pierri,Theo Araujo,Sanne Kruikemeier,Philipp Lorenz-Spreen,Mariek M. P. Vanden Abeele,Laura Vandenbosch,Joana Gonçalves-Sa,Przemyslaw A. Grabowicz*

Main category: cs.CY

TL;DR: 欧盟《数字服务法案》第40条为研究人员提供平台数据访问权限，但实施中仍面临法律、技术和组织障碍，阻碍系统性在线风险研究。


<details>
  <summary>Details</summary>
Motivation: 欧盟《数字服务法案》第40条旨在通过数据访问促进平台透明度和问责制，但实际执行中存在多重障碍，需要解决这些挑战以实现法案的透明度目标。

Method: 分析《数字服务法案》第40条实施过程中出现的关键挑战，并提出实际可行的改进措施。

Result: 识别出法律、技术和组织层面的多重障碍，这些障碍阻碍了研究人员有效访问平台数据并研究系统性在线风险。

Conclusion: 需要采取具体措施解决现有障碍，确保《数字服务法案》能够实现其透明度和问责制的目标，促进对在线平台系统性风险的有效研究。

Abstract: The Digital Services Act (DSA) introduced by the European Union in 2022 offers a landmark framework for platform transparency, with Article 40 enabling vetted researchers to access data from major online platforms. Yet significant legal, technical, and organizational barriers still hinder effective research on systemic online risks. This piece outlines the key challenges emerging from the Article 40 process and proposes practical measures to ensure that the DSA fulfills its transparency and accountability goals.

</details>


### [2] [Writing in Symbiosis: Mapping Human Creative Agency in the AI Era](https://arxiv.org/abs/2512.13697)
*Vivan Doshi,Mengyuan Li*

Main category: cs.CY

TL;DR: 研究发现人类与AI在创意写作中呈现"双轨演化"：主题围绕AI趋同，但风格差异化，形成三种适应模式


<details>
  <summary>Details</summary>
Motivation: 探讨人类与LLMs的共生关系如何影响人类创造力和能动性，挑战风格同质化的主流观点

Method: 使用跨越LLM时代前后的纵向写作数据大规模语料库，分析人类-AI协同演化模式

Result: 观察到"双轨演化"：主题围绕AI趋同，但风格结构化分化；识别出三种适应模式：与AI风格相似度增加、减少、保持稳定但参与AI主题

Conclusion: 创作原型图揭示了作者身份如何与AI协同演化，为人类-AI协作、检测挑战和创意多样性保护提供见解

Abstract: The proliferation of Large Language Models (LLMs) raises a critical question about what it means to be human when we share an increasingly symbiotic relationship with persuasive and creative machines. This paper examines patterns of human-AI coevolution in creative writing, investigating how human craft and agency are adapting alongside machine capabilities. We challenge the prevailing notion of stylistic homogenization by examining diverse patterns in longitudinal writing data. Using a large-scale corpus spanning the pre- and post-LLM era, we observe patterns suggestive of a "Dual-Track Evolution": thematic convergence around AI-related topics, coupled with structured stylistic differentiation. Our analysis reveals three emergent adaptation patterns: authors showing increased similarity to AI style, those exhibiting decreased similarity, and those maintaining stylistic stability while engaging with AI-related themes. This Creative Archetype Map illuminates how authorship is coevolving with AI, contributing to discussions about human-AI collaboration, detection challenges, and the preservation of creative diversity.

</details>


### [3] [Adaptive Merit Framework: Merit-Anchored Fairness via SES Correction](https://arxiv.org/abs/2512.13698)
*Jung-Ah Lee*

Main category: cs.CY

TL;DR: AMF是一种自适应招生框架，通过动态阈值识别社会经济地位较低学生的潜在能力，在保持学术标准的同时增加公平性。


<details>
  <summary>Details</summary>
Motivation: 传统公平干预措施（如平权行动、配额制）依赖粗糙指标，存在零和博弈，缺乏透明决策规则。需要一种既能识别潜在能力又能保持学术标准的招生机制。

Method: AMF包含三个核心组件：1）基于成绩的架构，条件录取生必须达到与常规录取生相同的阈值；2）以最后一名常规录取生原始分数为基准的动态阈值；3）通过行政数据验证的直接、连续的社会经济地位测量。

Result: 使用PISA 2022韩国数据验证，AMF在α=5,10,15时分别识别出4、6、9名额外录取生（占队列0.06-0.14%）。加权估计显示实际规模更大，分别为491、603、760名额外录取生。所有条件录取生都超过成绩阈值0.16-6.14分。

Conclusion: AMF不仅解决社会经济地位相关的公平问题，还为统一招生架构提供设计模板，替代碎片化的公平轨道，支持多维评估框架。

Abstract: College admissions systems worldwide continue to face a structural tension between meritocracy and equity. Conventional fairness interventions--affirmative action, categorical quotas, and proxy-based targeting--often rely on coarse indicators (e.g., race or region), operate within fixed quotas that induce zero-sum trade-offs, and lack transparent decision rules. This paper introduces the Adaptive Merit Framework (AMF), a policy-engineered mechanism that recognizes latent potential while preserving merit-based thresholds. AMF integrates three components: (1) a merit-anchored architecture in which conditional admits must exceed the same threshold as regular admits, (2) a dynamic threshold anchored to the raw score of the last regular admit, and (3) direct, continuous SES measurement verified through administrative data.
  Empirical validation using the full PISA 2022 Korea dataset (N=6,377) shows that AMF identifies 4, 6, and 9 additional admits under alpha = 5, 10, and 15 respectively (0.06-0.14% of cohort). Population-weighted estimates using OECD sampling weights suggest that the real-world scale of conditional admits is modestly larger than the raw sample counts, yielding approximately 491, 603, and 760 additional admits under alpha = 5, 10, and 15. All conditional admits exceed the merit threshold by 0.16 to 6.14 points, indicating that AMF recognizes suppressed performance rather than relaxing standards.
  Beyond SES-based corrections, AMF provides a design template for unified admissions architectures that replace fragmented equity tracks and support multi-dimensional evaluation frameworks.

</details>


### [4] [Us-vs-Them bias in Large Language Models](https://arxiv.org/abs/2512.13699)
*Tabia Tanzin Prama,Julia Witte Zimmerman,Christopher M. Danforth,Peter Sheridan Dodds*

Main category: cs.CY

TL;DR: 研究发现在基础大语言模型中存在一致的"我们vs他们"偏见，角色设定会系统性改变模型的语言模式，保守角色表现出更强的外群体敌意，自由角色表现出更强的内群体团结。研究者开发了ION偏见缓解方法，能将情感分歧减少达69%。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究大语言模型是否内化了社会认同理论中的"我们vs他们"偏见，以及角色设定如何影响模型的语言行为，这对于理解LLM的社会偏见和开发缓解策略具有重要意义。

Method: 使用情感动态分析、异质测量学和嵌入回归方法，在多种LLM架构（GPT-4.1、DeepSeek-3.1、Gemma-2.0、Grok-3.0、LLaMA-3.1）上分析默认和角色设定条件下的偏见模式，并开发ION方法通过微调和直接偏好优化来缓解偏见。

Result: 研究发现基础LLM中存在一致的内群体积极和外群体消极关联；角色设定会系统性改变语言模式，保守角色表现出更大外群体敌意，自由角色表现出更强内群体团结；外群体针对性提示使敌意偏见增加1.19-21.76%；ION方法能将情感分歧减少达69%。

Conclusion: LLM不仅学习了关于社会群体的事实关联，还内化并复制了不同的存在方式，包括态度、世界观和认知风格。研究证明了局部上下文、可定位表征和全局认知倾向之间的多尺度耦合关系，并展示了针对性缓解策略的可行性。

Abstract: This study investigates ``us versus them'' bias, as described by Social Identity Theory, in large language models (LLMs) under both default and persona-conditioned settings across multiple architectures (GPT-4.1, DeepSeek-3.1, Gemma-2.0, Grok-3.0, and LLaMA-3.1). Using sentiment dynamics, allotaxonometry, and embedding regression, we find consistent ingroup-positive and outgroup-negative associations across foundational LLMs. We find that adopting a persona systematically alters models' evaluative and affiliative language patterns. For the exemplar personas examined, conservative personas exhibit greater outgroup hostility, whereas liberal personas display stronger ingroup solidarity. Persona conditioning produces distinct clustering in embedding space and measurable semantic divergence, supporting the view that even abstract identity cues can shift models' linguistic behavior. Furthermore, outgroup-targeted prompts increased hostility bias by 1.19--21.76\% across models. These findings suggest that LLMs learn not only factual associations about social groups but also internalize and reproduce distinct ways of being, including attitudes, worldviews, and cognitive styles that are activated when enacting personas. We interpret these results as evidence of a multi-scale coupling between local context (e.g., the persona prompt), localizable representations (what the model ``knows''), and global cognitive tendencies (how it ``thinks''), which are at least reflected in the training data. Finally, we demonstrate ION, an ``us versus them'' bias mitigation approach using fine-tuning and direct preference optimization (DPO), which reduces sentiment divergence by up to 69\%, highlighting the potential for targeted mitigation strategies in future LLM development.

</details>


### [5] [Enhancing Transparency and Traceability in Healthcare AI: The AI Product Passport](https://arxiv.org/abs/2512.13702)
*A. Anil Sinaci,Senan Postaci,Dogukan Cavdaroglu,Machteld J. Boonstra,Okan Mercan,Kerem Yilmaz,Gokce B. Laleci Erturkmen,Folkert W. Asselbergs,Karim Lekadir*

Main category: cs.CY

TL;DR: 开发AI产品护照框架，通过生命周期文档提升医疗AI的透明度、可追溯性和合规性，基于现有标准设计关系数据模型，实现机器和人类可读报告。


<details>
  <summary>Details</summary>
Motivation: 解决医疗AI领域透明度不足的问题，满足监管要求（如欧盟AI法案、FDA指南）和伦理需求，建立可信赖的AI系统。

Method: 在AI4HF项目中开发，分析监管框架和现有标准，设计关系数据模型覆盖AI生命周期各阶段，集成MLOps/ModelOps概念，通过利益相关者共创（21人研讨会）和Mentimeter评估，使用Python库实现自动化溯源追踪。

Result: 成功设计并实现基于Web的平台，支持可审计文档，生成可定制的机器和人类可读报告，符合FUTURE-AI原则（公平性、通用性、可追溯性、可用性、鲁棒性、可解释性），详细记录模型目的、数据来源、性能和部署环境。

Conclusion: AI产品护照填补了医疗AI透明度空白，开源特性和标准对齐促进信任和适应性，未来将集成FAIR数据原则和FHIR标准以提升互操作性，推动负责任AI部署。

Abstract: Objective: To develop the AI Product Passport, a standards-based framework improving transparency, traceability, and compliance in healthcare AI via lifecycle-based documentation. Materials and Methods: The AI Product Passport was developed within the AI4HF project, focusing on heart failure AI tools. We analyzed regulatory frameworks (EU AI Act, FDA guidelines) and existing standards to design a relational data model capturing metadata across AI lifecycle phases: study definition, dataset preparation, model generation/evaluation, deployment/monitoring, and passport generation. MLOps/ModelOps concepts were integrated for operational relevance. Co-creation involved feedback from AI4HF consortium and a Lisbon workshop with 21 diverse stakeholders, evaluated via Mentimeter polls. The open-source platform was implemented with Python libraries for automated provenance tracking. Results: The AI Product Passport was designed based on existing standards and methods with well-defined lifecycle management and role-based access. Its implementation is a web-based platform with a relational data model supporting auditable documentation. It generates machine- and human-readable reports, customizable for stakeholders. It aligns with FUTURE-AI principles (Fairness, Universality, Traceability, Usability, Robustness, Explainability), ensuring fairness, traceability, and usability. Exported passports detail model purpose, data provenance, performance, and deployment context. GitHub-hosted backend/frontend codebases enhance accessibility. Discussion and Conclusion: The AI Product Passport addresses transparency gaps in healthcare AI, meeting regulatory and ethical demands. Its open-source nature and alignment with standards foster trust and adaptability. Future enhancements include FAIR data principles and FHIR integration for improved interoperability, promoting responsible AI deployment.

</details>


### [6] [Made-in China, Thinking in America:U.S. Values Persist in Chinese LLMs](https://arxiv.org/abs/2512.13723)
*David Haslett,Linus Ta-Lun Huang,Leila Khalatbari,Janet Hui-wen Hsiao,Antoni B. Chan*

Main category: cs.CY

TL;DR: 研究发现中美两国开发的LLM在道德价值观上都更偏向美国价值观而非中国价值观，即使使用中文提示或赋予中国身份也仅能轻微缓解这种偏向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在信息获取和决策支持中扮演重要角色，它们成为中美等全球行为体软实力竞争的工具。目前关于语言模型伦理偏见的研究主要基于美国公司开发的模型，而最新一代模型包含多个中国开发的模型，需要研究这些模型如何与中美两国人群的价值观对齐。

Method: 使用道德基础问卷2.0和世界价值观调查，对10个中国模型和10个美国模型进行测试，将它们的回答与数千名中美民众的回答进行比较。还测试了使用中文提示和赋予模型中国身份对结果的影响。

Result: 所有模型（包括中国开发的模型）对两项调查的回答都更接近美国人而非中国人。即使使用中文提示或赋予中国身份，这种偏向美国价值观的趋势也只能轻微缓解。

Conclusion: 研究结果表明当前大语言模型普遍偏向西方/美国价值观，这对未来LLM生成内容主导信息消费和地缘政治规范影响具有重要意义，突显了在全球化背景下开发更具文化包容性AI系统的必要性。

Abstract: As large language models increasingly mediate access to information and facilitate decision-making, they are becoming instruments in soft power competitions between global actors such as the United States and China. So far, language models seem to be aligned with the values of Western countries, but evidence for this ethical bias comes mostly from models made by American companies. The current crop of state-of-the-art models includes several made in China, so we conducted the first large-scale investigation of how models made in China and the USA align with people from China and the USA. We elicited responses to the Moral Foundations Questionnaire 2.0 and the World Values Survey from ten Chinese models and ten American models, and we compared their responses to responses from thousands of Chinese and American people. We found that all models respond to both surveys more like American people than like Chinese people. This skew toward American values is only slightly mitigated when prompting the models in Chinese or imposing a Chinese persona on the models. These findings have important implications for a near future in which large language models generate much of the content people consume and shape normative influence in geopolitics.

</details>


### [7] [Exploring the Modular Integration of "AI + Architecture" Pedagogy in Undergraduate Design Education: A Case Study of Architectural Design III/IV Courses at Zhejiang University](https://arxiv.org/abs/2512.13730)
*Wang Jiaqi,Lan Yi,Chen Xiang*

Main category: cs.CY

TL;DR: 浙江大学2024-25级三年级本科生设计工作室通过双模块框架（20小时AI培训+嵌入式伦理讨论）开展AI融入建筑教育的教学实验，证明分阶段指导、技术伦理平衡和制度支持的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究AI技术在建筑教育中的整合应用，探索如何在保持原有课程结构的同时，有效引入AI工具并解决相关的伦理问题，为设计教育提供可复制的技术学习与批判性思维结合的方法。

Method: 采用双模块教学框架：20小时AI技术培训（包括深度学习模型、大语言模型、AIGC、LoRA和ComfyUI等）+嵌入式伦理讨论，保持原有课程结构，配备专门的技术指导教师。

Result: 该教学模式有效提升了学生的数字技能和战略认知能力，同时解决了AI伦理问题。分阶段指导、技术伦理平衡的方法以及制度支持被证明是有效的，为设计教育提供了可复制的技术学习与批判性思维结合的方法。

Conclusion: AI在建筑教育中的整合需要分阶段指导、技术伦理平衡和制度支持。双模块框架结合技术培训与伦理讨论，既能提升学生的数字技能和战略认知，又能解决伦理问题，为设计教育提供了可复制的整合模式。

Abstract: This study investigates AI integration in architectural education through a teaching experiment in Zhejiang University's 2024-25 grade three undergraduate design studio. Adopting a dual-module framework (20-hour AI training + embedded ethics discussions), the course introduced deep learning models, LLMs, AIGC, LoRA, and ComfyUI while maintaining the original curriculum structure, supported by dedicated technical instructors. Findings demonstrate the effectiveness of phased guidance, balanced technical-ethical approaches, and institutional support. The model improved students' digital skills and strategic cognition while addressing AI ethics, providing a replicable approach combining technical and critical learning in design education.

</details>


### [8] [Instilling Organisational Values in Firefighters through Simulation-Based Training](https://arxiv.org/abs/2512.13737)
*Nardine Osman,Manel Rodriguez-Soto,Jordi Sabater-Mir*

Main category: cs.CY

TL;DR: 提出将部门价值观系统整合到模拟训练中的概念框架，以提升消防员在压力下的伦理决策能力


<details>
  <summary>Details</summary>
Motivation: 传统训练方法在应对紧急环境中的复杂伦理困境和价值冲突方面存在不足，消防员在压力下的决策具有重大伦理影响

Method: 提出概念框架，将部门价值观系统整合到模拟训练中，促进价值观内化，并利用工具评估和改进部门操作协议

Result: 该框架旨在提升消防员在压力下的价值观驱动决策能力，并优化操作协议与价值观的一致性

Conclusion: 系统整合价值观到模拟训练中能更好地准备消防员应对紧急环境中的伦理挑战，提升决策质量和安全性

Abstract: In firefighting and other emergency operations, decisions made under pressure carry profound ethical weight and can significantly impact incident outcomes and firefighter safety. Traditional training methods, while foundational, often fall short in adequately preparing firefighters for the complex ethical dilemmas and value conflicts inherent in chaotic emergency environments. This paper proposes a conceptual framework for enhancing firefighter training by systematically integrating departmental values into simulation-based training. This approach fosters deeper value internalisation and improves value-driven decision-making under pressure. Furthermore, the underlying tools can also be leveraged to evaluate and refine departmental operational protocols for better alignment with preferred values.

</details>


### [9] [The algorithmic muse and the public domain: Why copyrights legal philosophy precludes protection for generative AI outputs](https://arxiv.org/abs/2512.13750)
*Ezieddin Elmahjub*

Main category: cs.CY

TL;DR: 生成式AI输出不应受版权保护，因为其缺乏人类直接创作链接，授予版权会损害公共领域和创新


<details>
  <summary>Details</summary>
Motivation: 重新审视版权哲学基础，探讨生成式AI输出是否应受版权保护，避免传统教义分析局限

Method: 绕过传统法律教义分析，从版权哲学基础（功利主义激励、劳动应得、人格理论）重新评估，分析公共领域作为默认基线

Result: 生成式AI从根本上切断了人类与表达形式的直接创作链接，传统版权理论无法为其提供保护依据，授予版权会导致数字公地圈占和法律困境

Conclusion: 应明确区分：人类对AI生成作品的创造性贡献可能值得保护，但原始算法输出应保留在公共领域，寻求版权保护者需承担举证责任

Abstract: Generative AI (GenAI) outputs are not copyrightable. This article argues why. We bypass conventional doctrinal analysis that focuses on black letter law notions of originality and authorship to re-evaluate copyright's foundational philosophy. GenAI fundamentally severs the direct human creative link to expressive form. Traditional theories utilitarian incentive, labor desert and personality fail to provide coherent justification for protection. The public domain constitutes the default baseline for intellectual creations. Those seeking copyright coverage for GenAI outputs bear the burden of proof. Granting copyright to raw GenAI outputs would not only be philosophically unsound but would also trigger an unprecedented enclosure of the digital commons, creating a legal quagmire and stifling future innovation. The paper advocates for a clear distinction: human creative contributions to AI-generated works may warrant protection, but the raw algorithmic output should remain in the public domain.

</details>


### [10] [OpenProposal Platform for Transparent Research Funding Review](https://arxiv.org/abs/2512.13754)
*Sakshi Ahuja,Subhankar Mishra*

Main category: cs.CY

TL;DR: OpenProposal是一个基于Web的概念验证平台，将学术论文开放评审的透明度原则应用于科研经费申请评审，探索公开评审、作者回应和透明决策的技术可行性。


<details>
  <summary>Details</summary>
Motivation: 当前科研经费分配评审过程缺乏透明度，传统资助机构采用封闭评审系统，限制了问责制和系统性改进。学术论文的开放评审革命尚未扩展到经费评审领域。

Method: 使用现代Web技术（Next.js、React、Prisma）构建概念验证平台，实现公开评审、作者回应和透明决策机制，同时保护预算等敏感信息。

Result: 开发了OpenProposal平台原型，展示了技术可行性，解决了当前经费系统的关键限制，提供了社区参与、评审者问责和潜在数据驱动洞察的机制。

Conclusion: 这项工作为透明经费评审提供了技术基础，并确定了未来研究的设计考虑因素，透明评审可能增强科学诚信和改进经费决策，但需要实证验证。

Abstract: Research funding allocation remains a critical bottleneck in scientific advancement, yet the review process for funding proposals lacks the transparency that has revolutionized academic paper peer review. Traditional funding agencies operate with closed review systems, limiting accountability and preventing systematic improvements. We present OpenProposal, a proof-of-concept web-based platform that explores how transparency principles from OpenReview might be adapted to research funding proposal evaluation. Built using modern web technologies including Next.js , React , and Prisma , OpenProposal demonstrates the technical feasibility of public reviews, author rebuttals, and transparent decision-making while attempting to protect sensitive information such as budgets. Our platform prototype addresses key limitations identified in current funding systems by providing mechanisms for community engagement, reviewer accountability, and potential data-driven insights into peer review processes. Through system design and implementation, we explore how transparent funding review could potentially enhance scientific integrity and improve research funding decisions, though empirical validation remains necessary. This work contributes a technical foundation for transparent funding review and identifies design considerations for future research on peer review mechanisms in funding contexts.

</details>


### [11] [Beyond Procedural Compliance: Human Oversight as a Dimension of Well-being Efficacy in AI Governance](https://arxiv.org/abs/2512.13768)
*Yao Xie,Walter Cullen*

Main category: cs.CY

TL;DR: 该论文将人类监督重新定义为一种可发展的福祉能力，而非单纯的技术控制，主张将其整合到各级教育中，以实现可持续、符合伦理的AI发展。


<details>
  <summary>Details</summary>
Motivation: 当前主要AI伦理指南和法律（如欧盟AI法案）虽然呼吁有效的人类监督，但未将其明确定义为一种可发展的能力。现有框架缺乏将高层监管目标转化为实际人类能动性和责任培养的路径。

Method: 将人类监督概念化为一种福祉能力，整合到新兴的"福祉效能"框架中。该框架包含AI素养、伦理辨别力和人类需求意识，强调需要识别和约束可能冲突或有害的需求。

Result: 提出了人类监督作为福祉能力的概念框架，为从高层监管目标到实际人类能动性培养提供了实践路径，建立了未来研究教学实施和实证验证的理论基础。

Conclusion: 可持续、符合成本效益的人类监督能力发展依赖于将其整合到各级教育中，从专业培训到终身学习。这种福祉能力框架对于实现安全、伦理的AI至关重要。

Abstract: Major AI ethics guidelines and laws, including the EU AI Act, call for effective human oversight, but do not define it as a distinct and developable capacity. This paper introduces human oversight as a well-being capacity, situated within the emerging Well-being Efficacy framework. The concept integrates AI literacy, ethical discernment, and awareness of human needs, acknowledging that some needs may be conflicting or harmful. Because people inevitably project desires, fears, and interests into AI systems, oversight requires the competence to examine and, when necessary, restrain problematic demands.
  The authors argue that the sustainable and cost-effective development of this capacity depends on its integration into education at every level, from professional training to lifelong learning. The frame of human oversight as a well-being capacity provides a practical path from high-level regulatory goals to the continuous cultivation of human agency and responsibility essential for safe and ethical AI. The paper establishes a theoretical foundation for future research on the pedagogical implementation and empirical validation of well-being effectiveness in multiple contexts.

</details>


### [12] [Assessing High-Risk Systems: An EU AI Act Verification Framework](https://arxiv.org/abs/2512.13907)
*Alessio Buscemi,Tom Deckenbrunnen,Fahria Kabir,Nishat Mowla,Kateryna Mishchenko*

Main category: cs.CY

TL;DR: 本文提出一个系统性框架，用于验证欧盟《人工智能法案》等法规的合规性，通过方法类型（控制vs测试）和评估目标（数据、模型、流程、最终产品）两个维度组织验证活动，减少监管模糊性。


<details>
  <summary>Details</summary>
Motivation: 欧盟AI法规实施面临系统性验证方法缺失的挑战，监管模糊性导致成员国准备程度不一致，给实践带来显著负担。

Method: 提出一个综合性框架，沿两个基本维度组织合规验证：方法类型（控制措施vs测试）和评估目标（数据、模型、流程、最终产品），并将核心法律要求映射到具体验证活动。

Result: 该框架在政策制定者和实践者之间建立重要桥梁，将法律文本与技术标准和最佳实践对齐，减少解释不确定性，促进评估实践一致性。

Conclusion: 该框架有助于在整个AI生命周期中协调监管、伦理和技术视角，支持AI法规的有效实施，促进欧盟AI监管的一致性和可操作性。

Abstract: A central challenge in implementing the AI Act and other AI-relevant regulations in the EU is the lack of a systematic approach to verify their legal mandates. Recent surveys show that this regulatory ambiguity is perceived as a significant burden, leading to inconsistent readiness across Member States. This paper proposes a comprehensive framework designed to help close this gap by organising compliance verification along two fundamental dimensions: the type of method (controls vs. testing) and the target of assessment (data, model, processes, and final product). Additionally, our framework maps core legal requirements to concrete verification activities, serving as a vital bridge between policymakers and practitioners, and aligning legal text with technical standards and best practices. The proposed approach aims to reduce interpretive uncertainty, promote consistency in assessment practices, and support the alignment of regulatory, ethical, and technical perspectives across the AI lifecycle.

</details>


### [13] [Criminal Liability in AI-Enabled Autonomous Vehicles: A Comparative Study](https://arxiv.org/abs/2512.14330)
*Sahibpreet Singh,Manjit Singh*

Main category: cs.CY

TL;DR: 该研究通过比较美国、德国、英国、中国和印度的法律框架，分析了自动驾驶车辆引发的刑事责任归属问题，发现各国监管碎片化，呼吁建立全球统一的法律标准。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶技术革命虽然改变了交通方式，但引发了复杂的刑事责任问题，特别是在事故责任归属方面存在法律空白和不确定性，需要系统性的比较研究来指导法律框架的完善。

Method: 采用比较法律分析方法，研究五个代表性国家（美国、德国、英国、中国、印度）的主要法规、实际责任索赔案例和学术文献，这些国家因其技术先进性和监管方法差异而被选中。

Result: 研究发现各国监管体系碎片化：印度和美国依赖松散的各州法律网络，英国颁布了开创性的《自动和电动汽车法案2018》，德国执行严格安全标准并根据车辆运行模式区分责任，中国也致力于建立严格的责任制度。

Conclusion: 研究结论认为，全球协调统一的法律标准对于促进技术创新、确保最低风险和明确责任归属至关重要，需要建立国际性的法律框架来应对自动驾驶带来的挑战。

Abstract: AI revolutionizes transportation through autonomous vehicles (AVs) but introduces complex criminal liability issues regarding infractions. This study employs a comparative legal analysis of primary statutes, real-world liability claims, and academic literature across the US, Germany, UK, China, and India; jurisdictions selected for their technological advancement and contrasting regulatory approaches. The research examines the attribution of human error, AI moral agency, and the identification of primary offenders in AV incidents. Findings reveal fragmented regulatory landscapes: India and the US rely on loose networks of state laws, whereas the UK enacted the pioneering Automated and Electric Vehicles Act 2018. Germany enforces strict safety standards, distinguishing liability based on the vehicle's operating mode, while China similarly aims for a stringent liability regime. The study concludes that globally harmonized legal standards are essential to foster technological innovation while ensuring minimum risk and clear liability attribution.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [14] [Understanding statistics for biomedical research through the lens of replication](https://arxiv.org/abs/2512.13763)
*Huw Llewelyn*

Main category: stat.AP

TL;DR: 论文探讨了研究结果可重复性的概率问题，指出当P=0.025时，在相同样本量下重复研究获得同样显著结果的概率仅为28.3%，远低于预期，需要更大的样本量来提高可重复性。


<details>
  <summary>Details</summary>
Motivation: 临床医生和科学家传统上关注研究结果是否能够被重复验证，但实际观察到的重复率较低。本文旨在解释为什么即使原始研究达到统计显著性，重复研究获得同样显著结果的概率却很低，并探讨如何提高研究的可重复性。

Method: 通过分析效应估计方差之和对重复概率的影响，使用数学推导计算在不同样本量条件下的重复概率。特别采用离散化尺度替代连续分布，避免模糊性和不适当的平坦先验，使推理更加清晰。

Result: 当单侧P=0.025且重复研究具有相同样本量和方差时，获得同样显著结果的概率仅为约28.3%。如果重复研究样本量无限大（方差可忽略），则获得相同符号效应的概率为97.5%。这表明需要比当前单方差功效计算更大的样本量才能提高重复概率。

Conclusion: 研究结果的可重复性概率取决于效应估计方差之和，当前基于单方差功效计算的样本量不足以保证高重复率。离散化方法使推理更清晰，这一视角与频率主义和贝叶斯解释一致，对科学假设检验和决策制定有重要启示。

Abstract: Clinicians and scientists have traditionally focussed on whether their findings will be replicated and are very familiar with the concept. The probability that a replication study yields an effect with the same sign, or the same statistical significance as an original study depends on the sum of the variances of the effect estimates. On this basis, when P equals 0.025 one-sided and the replication study has the same sample size and variance as the original study, the probability of achieving a one-sided P is less than or equal to 0.025 a second time is only about 0.283, consistent with currently observed modest replication rates. A higher replication probability would require a larger sample size than that derived from current single variance power calculations. However, if the replication study is based on an infinitely large sample size and thus has negligible variance then the probability that its estimated mean is same sign is 1 - P = 0.975. The reasoning is made clearer by changing continuous distributions to discretised scales and probability masses, thus avoiding ambiguity and improper flat priors. This perspective is consistent with Frequentist and Bayesian interpretations and also requires further reasoning when testing scientific hypotheses and making decisions.

</details>


### [15] [A two-stage approach to heat-mortality risk assessment comparing multiple exposure-to-temperature models: the case study in Lazio, Italy](https://arxiv.org/abs/2512.14292)
*Emiliano Ceccarelli,Jorge Castillo-Mateo,Sandra Gudžiūnaitė,Giada Minelli,Giovanna Jona Lasinio,Marta Blangiardo*

Main category: stat.AP

TL;DR: 比较三种温度建模方法对意大利拉齐奥地区热相关死亡率估计的影响，发现不同方法导致最低风险温度显著差异，但都显示高温下相对风险显著增加


<details>
  <summary>Details</summary>
Motivation: 研究不同时空温度模型如何影响热相关死亡率的估计，为流行病学研究和气候健康适应策略提供参考

Method: 1. 比较三种日最高温度重建方法：贝叶斯分位数回归+空间插值、贝叶斯高斯回归、ERA5-Land再分析数据；2. 使用个体心血管和呼吸系统死亡数据，通过贝叶斯条件泊松模型在病例交叉设计中估计温度-死亡率关联；3. 额外模型包括不同阈值和持续时间的热浪定义

Result: 贝叶斯模型比ERA5-Land显示更高且空间变异更大的温度；所有模型都显示高温下相对风险显著增加，但最低风险温度在不同方法间差异显著；分层分析显示女性和80岁以上老年人相对风险增加更高；热浪效应取决于定义，但所有方法都捕捉到长期热暴露相关的死亡率风险增加

Conclusion: 温度模型选择对流行病学研究至关重要，结果为早期预警系统和气候健康适应策略提供了重要见解

Abstract: This study investigates how different spatiotemporal temperature models affect the estimation of heat-related mortality in Lazio, Italy (2008--2022). First, we compare three methods to reconstruct daily maximum temperature at the municipality level: 1. a Bayesian quantile regression model with spatial interpolation, 2. a Bayesian Gaussian regression model, 3. the gridded reanalysis data from ERA5-Land. Both Bayesian models are station-based and exhibit higher and more spatially variable temperatures compared to ERA5-Land. Then, using individual mortality data for cardiovascular and respiratory causes, we estimate temperature-mortality associations through Bayesian conditional Poisson models in a case-crossover design. Exposure is defined as the mean maximum temperature over the previous three days. Additional models include heatwave definitions combining different thresholds and durations. All models exhibit a marked increase in relative risk at high temperatures; however, the temperature of minimum risk varies significantly across methods. Stratified analyses reveal higher relative risk increases in females and the elderly (80+). Heatwave effects depend on the definitions used, but all methods capture an increased mortality risk associated with prolonged heat exposure. Results confirm the importance of temperature model choice in epidemiology and provide insights for early warning systems and climate-health adaptation strategies.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [16] [Leveraging LLMs for Structured Data Extraction from Unstructured Patient Records](https://arxiv.org/abs/2512.13700)
*Mitchell A. Klusty,Elizabeth C. Solie,Caroline N. Leach,W. Vaiden Logan,Lynnet E. Richey,John C. Gensel,David P. Szczykutowicz,Bryan C. McLellan,Emily B. Collier,Samuel E. Armstrong,V. K. Cody Bumgardner*

Main category: cs.AI

TL;DR: 提出一个基于本地部署大语言模型的临床笔记结构化特征提取框架，通过RAG和结构化响应方法实现自动化病历审查，减少人工负担


<details>
  <summary>Details</summary>
Motivation: 临床研究中的人工病历审查耗时耗力，需要专家从非结构化的电子健康记录中提取复杂信息，亟需自动化解决方案来减轻负担

Method: 开发了一个安全、模块化的框架，在机构批准的HIPAA合规计算基础设施上本地部署大语言模型，集成检索增强生成和结构化响应方法，提供可广泛部署和扩展的容器化解决方案

Result: 框架在大量患者笔记中实现了高准确率，与专家标注数据集相比表现优异，甚至发现了人工审查中遗漏的标注错误

Conclusion: 该框架展示了LLM系统通过自动化提取减少人工病历审查负担的潜力，提高了数据捕获的一致性，加速了临床研究进程

Abstract: Manual chart review remains an extremely time-consuming and resource-intensive component of clinical research, requiring experts to extract often complex information from unstructured electronic health record (EHR) narratives. We present a secure, modular framework for automated structured feature extraction from clinical notes leveraging locally deployed large language models (LLMs) on institutionally approved, Health Insurance Portability and Accountability Act (HIPPA)-compliant compute infrastructure. This system integrates retrieval augmented generation (RAG) and structured response methods of LLMs into a widely deployable and scalable container to provide feature extraction for diverse clinical domains. In evaluation, the framework achieved high accuracy across multiple medical characteristics present in large bodies of patient notes when compared against an expert-annotated dataset and identified several annotation errors missed in manual review. This framework demonstrates the potential of LLM systems to reduce the burden of manual chart review through automated extraction and increase consistency in data capture, accelerating clinical research.

</details>


### [17] [Blind Radio Mapping via Spatially Regularized Bayesian Trajectory Inference](https://arxiv.org/abs/2512.13701)
*Zheng Xing,Junting Chen*

Main category: cs.AI

TL;DR: 提出了一种无需位置标签的盲无线地图构建框架，利用MIMO-OFDM信道测量推断用户轨迹，实现室内定位和波束图重建


<details>
  <summary>Details</summary>
Motivation: 传统无线地图构建方法需要大量带位置标签的数据，成本高且在实际场景中不实用，需要一种无需位置标签的盲构建方法

Method: 基于理论证明：NLOS信道在准镜面环境模型下具有空间连续性，可推导CSI-距离度量；开发空间正则化贝叶斯推理框架，联合估计信道特征、区分LOS/NLOS条件并恢复用户轨迹

Result: 在射线追踪数据集上验证，平均定位误差0.68米，波束图重建误差3.3%，证明方法的有效性

Conclusion: 提出的盲无线地图构建框架无需位置标签，通过理论分析和实验验证了其有效性，为智能无线应用提供了实用解决方案

Abstract: Radio maps enable intelligent wireless applications by capturing the spatial distribution of channel characteristics. However, conventional construction methods demand extensive location-labeled data, which are costly and impractical in many real-world scenarios. This paper presents a blind radio map construction framework that infers user trajectories from indoor multiple-input multiple-output (MIMO)-Orthogonal Frequency-Division Multiplexing (OFDM) channel measurements without relying on location labels. It first proves that channel state information (CSI) under non-line-of-sight (NLOS) exhibits spatial continuity under a quasi-specular environmental model, allowing the derivation of a CSI-distance metric that is proportional to the corresponding physical distance. For rectilinear trajectories in Poisson-distributed access point (AP) deployments, it is shown that the Cramer-Rao Lower Bound (CRLB) of localization error vanishes asymptotically, even under poor angular resolution. Building on these theoretical results, a spatially regularized Bayesian inference framework is developed that jointly estimates channel features, distinguishes line-of-sight (LOS)/NLOS conditions and recovers user trajectories. Experiments on a ray-tracing dataset demonstrate an average localization error of 0.68 m and a beam map reconstruction error of 3.3%, validating the effectiveness of the proposed blind mapping method.

</details>


### [18] [Adjudicator: Correcting Noisy Labels with a KG-Informed Council of LLM Agents](https://arxiv.org/abs/2512.13704)
*Doohee You,Sundeep Paul*

Main category: cs.AI

TL;DR: Adjudicator是一个用于自动识别和纠正标签噪声的生产级系统，采用神经符号方法构建动态知识图谱，并通过多智能体LLM架构进行标签验证，在AlleNoise基准上达到0.99 F1分数。


<details>
  <summary>Details</summary>
Motivation: 生产机器学习系统的性能受限于训练数据质量，高风险的工业应用中噪声标签会降低性能并损害用户信任，需要自动化、高精度的数据验证系统。

Method: 采用神经符号方法：首先构建动态知识图谱统一项目上下文，然后通过"智能体委员会"（多智能体LLM架构）让专门化的智能体辩论和投票标签有效性，使用基于知识图谱的覆盖逻辑识别复杂结构性错误。

Result: 在AlleNoise基准的1000项平衡子集上，知识图谱增强模型达到0.99 F1分数，显著优于单LLM基线（0.48 F1）和非知识图谱委员会（0.59 F1），通过知识图谱完美识别复杂结构性错误（完全召回）。

Conclusion: Adjudicator展示了自动化、高精度数据验证的鲁棒且可解释系统，为严格监管的工业环境中生成黄金数据集提供了重要概念验证。

Abstract: The performance of production machine learning systems is fundamentally limited by the quality of their training data. In high-stakes industrial applications, noisy labels can degrade performance and erode user trust. This paper presents Adjudicator, a system that addresses the critical data mining challenge of automatically identifying and correcting label noise and has been validated for production deployment. Adjudicator models this as a neuro-symbolic task, first constructing a dynamic Knowledge Graph (KG) to unify item context. This KG then informs a "Council of Agents," a novel multi-agent Large Language Model architecture where specialized agents debate and vote on a label's validity. We validate our system on a 1,000-item balanced subset of the AlleNoise benchmark. Our KG-informed model achieves a 0.99 F1-score, significantly outperforming a single-LLM baseline (0.48 F1) and a non-KG council (0.59 F1). Our analysis reveals this is due to a Precision, achieved by a novel override logic that uses the KG to perfectly identify complex, structural errors (complete Recall) -- a class of errors that baselines fail to find. This result demonstrates a robust and explainable system for automated, high-precision data verification, serving as a vital proof-of-concept for generating golden datasets in strictly governed industrial environments.

</details>


### [19] [LoopBench: Discovering Emergent Symmetry Breaking Strategies with LLM Swarms](https://arxiv.org/abs/2512.13713)
*Ali Parsaee,Yashar Talebirad,Csongor Szepesvári,Vishwajeet Ohal,Eden Redman*

Main category: cs.AI

TL;DR: LoopBench是一个评估LLM在分布式对称性打破和元认知推理能力的基准，专注于奇数环图着色问题，通过策略传递机制实现一致性记忆，发现高级推理模型能设计出逃逸死锁的策略。


<details>
  <summary>Details</summary>
Motivation: LLM越来越多地被用作自主代理，但它们在分布式系统中的协调能力仍未被充分理解，需要专门的基准来评估LLM在分布式对称性打破和元认知推理方面的能力。

Method: 提出LoopBench基准，专注于奇数环图（C3、C5、C11）的有限颜色着色问题，其中确定性非通信代理会陷入无限循环。通过策略传递机制实现一致性记忆，评估LLM在分布式对称性打破中的推理能力。

Result: 标准LLM和经典启发式方法难以解决该问题，而高级推理模型（如O3）能够设计出逃逸死锁的策略，展示了基于语言推理的分布式算法涌现能力。

Conclusion: LoopBench为研究基于语言推理的分布式算法提供了测试平台，有助于探索集体智能，展示了LLM在分布式协调问题中的潜力。

Abstract: Large Language Models (LLMs) are increasingly being utilized as autonomous agents, yet their ability to coordinate in distributed systems remains poorly understood. We introduce \textbf{LoopBench}, a benchmark to evaluate LLM reasoning in distributed symmetry breaking and meta-cognitive thinking. The benchmark focuses on coloring odd cycle graphs ($C_3, C_5, C_{11}$) with limited colors, where deterministic, non-communicating agents fail in infinite loops. A strategy passing mechanism is implemented as a form of consistent memory. We show that while standard LLMs and classical heuristics struggle, advanced reasoning models (e.g., O3) devise strategies to escape deadlocks. LoopBench allows the study of emergent distributed algorithms based on language-based reasoning, offering a testbed for collective intelligence.

</details>


### [20] [AI-Powered Annotation Pipelines for Stabilizing Large Language Models: A Human-AI Synergy Approach](https://arxiv.org/abs/2512.13714)
*Gangesh Pathak,Prasanna Kumar*

Main category: cs.AI

TL;DR: 本文提出了一种基于AI的标注流水线，通过人机协同方法系统识别、标注和修复LLM输出的不稳定性模式，以解决LLM在高度监管行业中因可靠性问题而受限的挑战。


<details>
  <summary>Details</summary>
Motivation: LLM在高度监管行业中的应用受到不稳定性问题、不一致推理、幻觉和性能波动的限制，特别是在工作流程中。这些可靠性问题限制了LLM在需要事实精确性和一致行为领域的应用。现有的稳定化方法如RLHF和监督微调虽然能提供可量化的改进，但成本高昂且依赖密集的人工标注，难以可持续扩展。

Method: 提出基于AI的标注流水线，采用人机协同方法结合自动弱监督模型和基于置信度的标注，辅以目标人类验证。引入语义一致性、事实正确性和逻辑连贯性等稳定性特定标注类别，通过反馈循环实现模型的持续校准和鲁棒性增强。

Result: 该方法能够保证反馈信息的可靠性和道德完整性，通过系统化的标注框架实现对LLM输出的不稳定性模式的识别和修复，为模型提供持续改进的机制。

Conclusion: 提出的AI标注流水线为人机协同解决LLM稳定性问题提供了可持续的解决方案，通过结合自动化标注和人类验证，能够在保证质量的同时实现规模化应用，有望推动LLM在高度监管行业中的可靠部署。

Abstract: LLM implementations are failing in highly regulated industries owing to instability issues, inconsistent reasoning, hallucinations and performance variability, especially in workflows. These reliability issues restrict safe use of LLM in areas that need the precision of facts and consistent behavior (Aiyappa et al., 2023). The current methods of stabilization, such as, reinforcement learning with human feedback (RLHF) and supervised fine-tuning, offer quantifiable improvements but are expensive and based on the intensive annotation of humans, thus being not easily scaled in a sustainable way (Dong et al., 2023; Retzlaff et al., 2024). This paper presents an AI-based annotation pipeline that systematically identifies, labels, and fixes for instability patterns on LLM output. Our human-AI synergy method combines the models of automated weak supervision and confidence-based annotation with the target human validation to guarantee the reliability and moral uprightness of feedback information (Cabitza et al., 2023; Jiang et al., 2023). The semantic consistency, factual correctness, and logical coherence categories of stability-specific annotation are introduced into our framework, allowing the continuous calibration of models and the enhancement of their robustness based on the feedback loops (Honovich et al., 2021; Nan et al., 2021).

</details>


### [21] [Meta Hierarchical Reinforcement Learning for Scalable Resource Management in O-RAN](https://arxiv.org/abs/2512.13715)
*Fatemeh Lotfi,Fatemeh Afghah*

Main category: cs.AI

TL;DR: 提出自适应元层次强化学习框架，用于O-RAN中的资源分配和网络切片联合优化，相比基线方法提升19.8%网络管理效率


<details>
  <summary>Details</summary>
Motivation: 现代应用复杂性增加需要无线网络具备实时适应性和高效资源管理能力。O-RAN架构的RIC模块为动态资源管理和网络切片提供了关键解决方案。虽然AI驱动方法有前景，但大多数方法在不可预测和高度动态条件下难以保持性能。

Method: 提出自适应元层次强化学习框架，受MAML启发，结合层次控制和元学习。高层控制器跨切片分配资源，低层代理执行切片内调度。自适应元更新机制根据时序差分误差方差加权任务，提高稳定性并优先处理复杂网络场景。

Result: 理论分析建立了两级学习过程的次线性收敛和遗憾保证。仿真结果显示相比基线RL和元RL方法，网络管理效率提升19.8%，适应速度更快，在eMBB、URLLC和mMTC切片上QoS满意度更高。消融和可扩展性研究证实方法鲁棒性，适应速度提升达40%，随着网络规模增加保持一致的公平性、延迟和吞吐量性能。

Conclusion: 提出的自适应元层次强化学习框架有效解决了O-RAN中资源分配和网络切片的联合优化问题，在动态网络条件下表现出优越的性能、适应性和可扩展性。

Abstract: The increasing complexity of modern applications demands wireless networks capable of real time adaptability and efficient resource management. The Open Radio Access Network (O-RAN) architecture, with its RAN Intelligent Controller (RIC) modules, has emerged as a pivotal solution for dynamic resource management and network slicing. While artificial intelligence (AI) driven methods have shown promise, most approaches struggle to maintain performance under unpredictable and highly dynamic conditions. This paper proposes an adaptive Meta Hierarchical Reinforcement Learning (Meta-HRL) framework, inspired by Model Agnostic Meta Learning (MAML), to jointly optimize resource allocation and network slicing in O-RAN. The framework integrates hierarchical control with meta learning to enable both global and local adaptation: the high-level controller allocates resources across slices, while low level agents perform intra slice scheduling. The adaptive meta-update mechanism weights tasks by temporal difference error variance, improving stability and prioritizing complex network scenarios. Theoretical analysis establishes sublinear convergence and regret guarantees for the two-level learning process. Simulation results demonstrate a 19.8% improvement in network management efficiency compared with baseline RL and meta-RL approaches, along with faster adaptation and higher QoS satisfaction across eMBB, URLLC, and mMTC slices. Additional ablation and scalability studies confirm the method's robustness, achieving up to 40% faster adaptation and consistent fairness, latency, and throughput performance as network scale increases.

</details>


### [22] [ValuePilot: A Two-Phase Framework for Value-Driven Decision-Making](https://arxiv.org/abs/2512.13716)
*Yitong Luo,Ziang Chen,Hou Hei Lam,Jiayu zhan,Junqi Wang,Zhenliang Zhang,Xue Feng*

Main category: cs.AI

TL;DR: ValuePilot框架通过价值驱动的个性化决策方法，在未见场景中超越主流LLM基准，实现与人类行动选择的对齐。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统扩展到现实应用，需要超越任务完成或集体对齐，适应个体用户的价值偏好，实现个性化决策。当前任务导向范式依赖外部奖励，缺乏解释性且难以处理新场景。

Method: 提出ValuePilot两阶段框架：1) DGT数据集生成工具包，通过人-LLM协作管道构建多样化价值标注场景；2) DMM决策模块，学习基于个人价值偏好评估行动，实现上下文敏感的个性化决策。

Result: 在未见场景评估中，DMM超越了GPT-5、Claude-Sonnet-4、Gemini-2-flash和Llama-3.1-70b等强LLM基线，在人类行动选择对齐方面表现更优。

Conclusion: 价值驱动的决策是构建可解释、个性化AI代理的有效且可扩展的工程路径，人类价值作为稳定、可转移的信号支持跨上下文的一致和泛化行为。

Abstract: Personalized decision-making is essential for human-AI interaction, enabling AI agents to act in alignment with individual users' value preferences. As AI systems expand into real-world applications, adapting to personalized values beyond task completion or collective alignment has become a critical challenge. We address this by proposing a value-driven approach to personalized decision-making. Human values serve as stable, transferable signals that support consistent and generalizable behavior across contexts. Compared to task-oriented paradigms driven by external rewards and incentives, value-driven decision-making enhances interpretability and enables agents to act appropriately even in novel scenarios. We introduce ValuePilot, a two-phase framework consisting of a dataset generation toolkit (DGT) and a decision-making module (DMM). DGT constructs diverse, value-annotated scenarios from a human-LLM collaborative pipeline. DMM learns to evaluate actions based on personal value preferences, enabling context-sensitive, individualized decisions. When evaluated on previously unseen scenarios, DMM outperforms strong LLM baselines, including GPT-5, Claude-Sonnet-4, Gemini-2-flash, and Llama-3.1-70b, in aligning with human action choices. Our results demonstrate that value-driven decision-making is an effective and extensible engineering pathway toward building interpretable, personalized AI agents.

</details>


### [23] [Compressed Causal Reasoning: Quantization and GraphRAG Effects on Interventional and Counterfactual Accuracy](https://arxiv.org/abs/2512.13725)
*Steve Nwaiwu,Nipat Jongsawat,Anucha Tungkasthan*

Main category: cs.AI

TL;DR: 量化对LLM因果推理影响研究：发现4位量化下因果推理意外稳健，干预查询最敏感，现有反事实基准未能捕捉深层脆弱性


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型部署向边缘和资源受限环境转移，量化模型（如INT8和NF4）成为标准，但精度降低对形式化因果推理的影响尚不清楚，需要系统评估量化对因果推理的影响

Method: 使用3000个样本分层的CLadder基准，系统评估量化在Pearl因果阶梯所有三个层级的影响；在Llama 3 8B模型上测试不同精度；使用CRASS基准进行实验；评估基于真实因果图的图检索增强生成

Result: 量化下因果推理准确性总体稳定，NF4仅下降不到1%；干预查询对精度损失最敏感；反事实推理相对稳定但在某些查询类型存在异质性弱点；CRASS基准上不同精度表现几乎相同；图检索增强生成使NF4干预准确性提升1.7%

Conclusion: 因果推理对4位量化意外稳健，图结构化增强可选择性强化干预推理，当前反事实基准未能捕捉深层因果脆弱性，为部署高效且结构支持的因果AI系统提供实用指导

Abstract: Causal reasoning in Large Language Models spanning association, intervention, and counterfactual inference is essential for reliable decision making in high stakes settings. As deployment shifts toward edge and resource constrained environments, quantized models such as INT8 and NF4 are becoming standard. Yet the impact of precision reduction on formal causal reasoning is poorly understood. To our knowledge, this is the first study to systematically evaluate quantization effects across all three levels of Pearls Causal Ladder. Using a 3000 sample stratified CLadder benchmark, we find that rung level accuracy in Llama 3 8B remains broadly stable under quantization, with NF4 showing less than one percent overall degradation. Interventional queries at rung 2 are the most sensitive to precision loss, whereas counterfactual reasoning at rung 3 is comparatively stable but exhibits heterogeneous weaknesses across query types such as collider bias and backdoor adjustment. Experiments on the CRASS benchmark show near identical performance across precisions, indicating that existing commonsense counterfactual datasets lack the structural sensitivity needed to reveal quantization induced reasoning drift. We further evaluate Graph Retrieval Augmented Generation using ground truth causal graphs and observe a consistent improvement in NF4 interventional accuracy of plus 1.7 percent, partially offsetting compression related degradation. These results suggest that causal reasoning is unexpectedly robust to four bit quantization, graph structured augmentation can selectively reinforce interventional reasoning, and current counterfactual benchmarks fail to capture deeper causal brittleness. This work provides an initial empirical map of compressed causal reasoning and practical guidance for deploying efficient and structurally supported causal AI systems.

</details>


### [24] [State-Dependent Refusal and Learned Incapacity in RLHF-Aligned Language Models](https://arxiv.org/abs/2512.13762)
*TK Lee*

Main category: cs.AI

TL;DR: 该研究提出了一种定性案例研究方法，用于审计大语言模型在长时交互中的策略相关行为选择性。研究发现同一模型在非敏感领域表现正常，但在敏感领域会反复出现功能性拒绝，表现出行为不对称性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为通用工具被广泛部署，但标准定量基准测试无法捕捉到长时交互中可能出现的特定行为模式。需要开发新的审计方法来检测模型在策略敏感领域的行为选择性。

Method: 采用定性案例研究方法，通过86轮对话会话进行长时交互审计。定义了三种响应机制：正常表现(NP)、功能性拒绝(FR)和元叙事(MN)。分析模型在不同领域的行为不对称性，并引入"习得性无能"作为行为描述概念。

Result: 研究发现同一模型在非敏感领域表现正常，但在提供者或策略敏感领域会反复出现功能性拒绝，形成跨领域的一致不对称性。元叙事角色框架叙事倾向于与敏感情境中的拒绝同时出现。

Conclusion: 该研究提出了基于可观察行为的交互级审计框架，并将习得性无能作为检查潜在对齐副效应的视角。这种方法值得在不同用户和模型中进行进一步调查，以更好地理解LLM的行为选择性。

Abstract: Large language models (LLMs) are widely deployed as general-purpose tools, yet extended interaction can reveal behavioral patterns not captured by standard quantitative benchmarks. We present a qualitative case-study methodology for auditing policy-linked behavioral selectivity in long-horizon interaction. In a single 86-turn dialogue session, the same model shows Normal Performance (NP) in broad, non-sensitive domains while repeatedly producing Functional Refusal (FR) in provider- or policy-sensitive domains, yielding a consistent asymmetry between NP and FR across domains. Drawing on learned helplessness as an analogy, we introduce learned incapacity (LI) as a behavioral descriptor for this selective withholding without implying intentionality or internal mechanisms. We operationalize three response regimes (NP, FR, Meta-Narrative; MN) and show that MN role-framing narratives tend to co-occur with refusals in the same sensitive contexts. Overall, the study proposes an interaction-level auditing framework based on observable behavior and motivates LI as a lens for examining potential alignment side effects, warranting further investigation across users and models.

</details>


### [25] [Mathematics and Coding are Universal AI Benchmarks](https://arxiv.org/abs/2512.13764)
*Przemyslaw Chojecki*

Main category: cs.AI

TL;DR: 该论文研究了数学和编程在AI智能体心理测量电池模空间中的特殊作用，证明了在特定条件下，由数学定理证明和编程任务生成的电池子空间在评估度量下是稠密的。


<details>
  <summary>Details</summary>
Motivation: 研究数学和编程在AI智能体心理测量电池模空间中的特殊角色，探索它们作为评估"通用坐标"的潜力，以及形式数学作为高级AI智能体递归自我改进的自然点火域。

Method: 基于AAI框架和GVU动力学，定义数学纤维，结合形式证明内核（如Lean、Coq），分析GVU流在数学纤维上的谱稳定自我改进机制。主要技术结果是密度定理：在智能体输出均匀紧致性和Lipschitz AAI泛函条件下，证明数学定理证明和编程任务生成的电池子空间在模空间中稠密。

Result: 编程单独具有通用性，而纯数学不具有表达通用性但其特权是谱性质的。数学纤维与形式证明内核配对时，GVU流允许谱稳定的自我改进机制，这是由于类似神谕的验证机制。

Conclusion: 数学和编程为AI智能体评估提供了"通用坐标"，形式数学是高级AI智能体递归自我改进的自然点火域，数学的特权是谱性质而非表达性质。

Abstract: We study the special role of mathematics and coding inside the moduli space of psychometric batteries for AI agents. Building on the AAI framework and GVU dynamics from previous works, we define the Mathematics Fiber and show that, when paired with formal proof kernels (e.g. Lean, Coq), GVU flows on this fiber admit spectrally stable self-improvement regimes due to oracle-like verification. Our main technical result is a density theorem: under uniform tightness of agent outputs and a Lipschitz AAI functional, the subspace of batteries generated by mathematical theorem-proving and coding tasks is dense in the moduli space of batteries with respect to the evaluation metric. Coding alone is universal in this sense, while pure mathematics is not; its privilege is spectral rather than expressive. We interpret this as evidence that mathematics and coding provide ``universal coordinates'' for evaluation, and that formal mathematics is a natural ignition domain for recursive self-improvement in advanced AI agents.

</details>


### [26] [Semantic Grounding Index: Geometric Bounds on Context Engagement in RAG Systems](https://arxiv.org/abs/2512.13771)
*Javier Marín*

Main category: cs.AI

TL;DR: 论文提出语义基础指数(SGI)，通过计算回答与问题vs上下文的角距离比率来检测RAG系统的幻觉，发现幻觉回答在嵌入空间中保持与问题接近而非向上下文移动的"语义懒惰"现象。


<details>
  <summary>Details</summary>
Motivation: 研究RAG系统产生幻觉时在嵌入空间中留下的几何痕迹，开发一种计算高效、理论基础的指标来识别需要验证的响应。

Method: 定义语义基础指数(SGI)为回答到问题与回答到上下文的角距离比率，在单位超球面上分析几何关系。使用HaluEval数据集(n=5,000)验证，分析不同嵌入模型、问题-上下文角分离的影响。

Result: 发现"语义懒惰"现象：幻觉回答保持与问题角接近而非向上下文移动。效应大小Cohen's d=0.92-1.28，跨模型相关性r=0.85。SGI区分能力随问题-上下文角分离增加而增强(AUC从0.72提升到0.83)。在长回答(d=2.05)和短问题(d=1.22)上表现最佳，校准误差ECE=0.10。

Conclusion: SGI提供了一种计算高效、理论基础的方法来识别RAG系统中需要验证的响应，但测量的是主题参与度而非事实准确性，在TruthfulQA上表现不佳(AUC=0.478)。

Abstract: When retrieval-augmented generation (RAG) systems hallucinate, what geometric trace does this leave in embedding space? We introduce the Semantic Grounding Index (SGI), defined as the ratio of angular distances from the response to the question versus the context on the unit hypersphere $\mathbb{S}^{d-1}$.Our central finding is \emph{semantic laziness}: hallucinated responses remain angularly proximate to questions rather than departing toward retrieved contexts. On HaluEval ($n$=5,000), we observe large effect sizes (Cohen's $d$ ranging from 0.92 to 1.28) across five embedding models with mean cross-model correlation $r$=0.85. Crucially, we derive from the spherical triangle inequality that SGI's discriminative power should increase with question-context angular separation $θ(q,c)$-a theoretical prediction confirmed empirically: effect size rises monotonically from $d$=0.61 -low $θ(q,c)$, to $d$=1.27 -high $θ(q,c)$, with AUC improving from 0.72 to 0.83. Subgroup analysis reveals that SGI excels on long responses ($d$=2.05) and short questions ($d$=1.22), while remaining robust across context lengths. Calibration analysis yields ECE=0.10, indicating SGI scores can serve as probability estimates, not merely rankings. A critical negative result on TruthfulQA (AUC=0.478) establishes that angular geometry measures topical engagement rather than factual accuracy. SGI provides computationally efficient, theoretically grounded infrastructure for identifying responses that warrant verification in production RAG deployments.

</details>


### [27] [EvoLattice: Persistent Internal-Population Evolution through Multi-Alternative Quality-Diversity Graph Representations for LLM-Guided Program Discovery](https://arxiv.org/abs/2512.13857)
*Kamer Ali Yuksel*

Main category: cs.AI

TL;DR: EvoLattice：一种基于有向无环图的程序/智能体演化框架，将整个种群表示为单一图结构，每个节点存储多个持久化备选方案，通过路径组合定义可执行候选，实现细粒度评估和LLM引导的演化。


<details>
  <summary>Details</summary>
Motivation: 现有LLM演化方法大多基于覆盖式突变，每次只维护单个候选，会丢弃有用变体、遭受破坏性编辑，且搜索空间脆弱易出现结构故障。需要一种能保留成功组件、支持细粒度评估的演化框架。

Method: EvoLattice使用有向无环图表示整个种群，节点存储多个持久化备选方案，每个有效路径定义不同的可执行候选。通过跨路径评估每个备选方案获得性能统计，为LLM引导的突变、重组和剪枝提供密集反馈信号。包含确定性自修复机制保证结构正确性。

Result: 在程序合成（代理和优化器元学习）任务中，EvoLattice相比先前LLM引导方法展现出更稳定的演化、更强的表达能力和更好的改进轨迹。其动态类似于质量-多样性优化，但通过内部多备选表示隐式实现。

Conclusion: EvoLattice通过图结构表示整个种群，实现了更稳健、表达力更强的LLM引导演化，能保留成功组件并提供细粒度反馈，为程序和智能体演化提供了新范式。

Abstract: Large language models (LLMs) are increasingly used to evolve programs and multi-agent systems, yet most existing approaches rely on overwrite-based mutations that maintain only a single candidate at a time. Such methods discard useful variants, suffer from destructive edits, and explore a brittle search space prone to structural failure. We introduce EvoLattice, a framework that represents an entire population of candidate programs or agent behaviors within a single directed acyclic graph. Each node stores multiple persistent alternatives, and every valid path through the graph defines a distinct executable candidate, yielding a large combinatorial search space without duplicating structure. EvoLattice enables fine-grained alternative-level evaluation by scoring each alternative across all paths in which it appears, producing statistics that reveal how local design choices affect global performance. These statistics provide a dense, data-driven feedback signal for LLM-guided mutation, recombination, and pruning, while preserving successful components. Structural correctness is guaranteed by a deterministic self-repair mechanism that enforces acyclicity and dependency consistency independently of the LLM. EvoLattice naturally extends to agent evolution by interpreting alternatives as prompt fragments or sub-agent behaviors. Across program synthesis (proxy and optimizer meta-learning), EvoLattice yields more stable evolution, greater expressivity, and stronger improvement trajectories than prior LLM-guided methods. The resulting dynamics resemble quality-diversity optimization, emerging implicitly from EvoLattice's internal multi-alternative representation rather than an explicit external archive.

</details>


### [28] [MURIM: Multidimensional Reputation-based Incentive Mechanism for Federated Learning](https://arxiv.org/abs/2512.13955)
*Sindhuja Madabushi,Dawood Wasif,Jin-Hee Cho*

Main category: cs.AI

TL;DR: MURIM：一种基于多维信誉的联邦学习激励机制，通过综合考虑客户端可靠性、隐私、资源容量和公平性，防止恶意客户端获得不当奖励，提高联邦学习的公平性、隐私保护和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临客户端激励不足、隐私风险和资源约束等挑战。评估客户端可靠性对于公平分配激励和确保每个客户端数据对全局模型做出有意义的贡献至关重要。

Method: 提出MURIM（多维信誉激励机制），联合考虑客户端可靠性、隐私、资源容量和公平性，防止恶意或不可靠客户端获得不当奖励。基于客户端贡献、延迟和信誉分配激励，并配备可靠性验证模块。

Result: 在MNIST、FMNIST和ADULT Income数据集上的实验表明，MURIM在公平性指标上提升高达18%，隐私攻击成功率降低5-9%，对投毒和噪声梯度攻击的鲁棒性提升高达85%。

Conclusion: MURIM有效缓解对抗威胁，促进公平真实的参与，在异构动态联邦环境中保持稳定的模型收敛，为联邦学习提供了更安全、公平和高效的激励机制。

Abstract: Federated Learning (FL) has emerged as a leading privacy-preserving machine learning paradigm, enabling participants to share model updates instead of raw data. However, FL continues to face key challenges, including weak client incentives, privacy risks, and resource constraints. Assessing client reliability is essential for fair incentive allocation and ensuring that each client's data contributes meaningfully to the global model. To this end, we propose MURIM, a MUlti-dimensional Reputation-based Incentive Mechanism that jointly considers client reliability, privacy, resource capacity, and fairness while preventing malicious or unreliable clients from earning undeserved rewards. MURIM allocates incentives based on client contribution, latency, and reputation, supported by a reliability verification module. Extensive experiments on MNIST, FMNIST, and ADULT Income datasets demonstrate that MURIM achieves up to 18% improvement in fairness metrics, reduces privacy attack success rates by 5-9%, and improves robustness against poisoning and noisy-gradient attacks by up to 85% compared to state-of-the-art baselines. Overall, MURIM effectively mitigates adversarial threats, promotes fair and truthful participation, and preserves stable model convergence across heterogeneous and dynamic federated settings.

</details>


### [29] [Evaluating Frontier LLMs on PhD-Level Mathematical Reasoning: A Benchmark on a Textbook in Theoretical Computer Science about Randomized Algorithms](https://arxiv.org/abs/2512.13978)
*Yang Cao,Yubin Chen,Xuyang Guo,Zhao Song,Song Yue,Jiahao Zhang,Jiale Zhao*

Main category: cs.AI

TL;DR: 该论文对GPT-5-Thinking、Gemini-3-Pro、Claude-Sonnet-4.5-Thinking和Grok-4四个前沿大语言模型在《随机算法》教材中的证明生成能力进行了基准测试，发现顶级模型准确率约66%，但模型间存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在数学推理和科学发现方面取得突破，但仍需对其在研究生水平数学理论上的基础推理能力进行严格评估，以了解模型的实际能力水平。

Method: 使用Motwani和Raghavan的《随机算法》教材作为测试基准，要求每个模型为教材中的一系列引理和练习生成正式的LaTeX证明，并进行全面评估。

Result: 顶级模型（Gemini和Claude）准确率约66%，表现出对概率方法和形式逻辑的良好掌握；其他模型准确率约40%，一致性较差。模型在简洁性、幻觉率和逻辑结构方面存在差异。

Conclusion: 前沿模型已达到适合研究生水平教学辅助和形式化的熟练程度门槛，但在严格数学推导的可靠性方面存在显著差异，需要进一步改进。

Abstract: The rapid advancement of large language models (LLMs) has led to significant breakthroughs in automated mathematical reasoning and scientific discovery. Georgiev, G${ó}$mez-Serrano, Tao, and Wagner [GGSTW+25] demonstrate that AI systems can explore new constructions and improve existing bounds, illustrating the growing potential of LLMs to accelerate mathematical discovery. Similarly, Bubeck et al. [BCE+25] show that GPT-5 can meaningfully contribute to scientific workflows, from proposing hypotheses to generating proofs and analyses. Despite these advances, a rigorous evaluation of these models on canonical, graduate-level mathematical theory remains necessary to understand their baseline reasoning capabilities. In this paper, we present a comprehensive benchmark of four frontier models: GPT-5-Thinking, Gemini-3-Pro, Claude-Sonnet-4.5-Thinking, and Grok-4 against the classic curriculum of Randomized Algorithms by Motwani and Raghavan [MR95].
  We tasked each model with generating formal LaTeX proofs for a series of lemmas and exercises spanning the textbook. We find that while the top-tier models (Gemini, and Claude) achieve a high accuracy rate (approx. 66%), demonstrating a robust grasp of probabilistic method and formal logic, other models lag significantly in consistency (approx. 40%). We provide a qualitative analysis of the generated proofs, highlighting differences in conciseness, hallucination rates, and logical structure. Our results suggest that while frontier models have reached a threshold of proficiency suitable for graduate-level pedagogical assistance and formalization, significant variance exists in their reliability for rigorous mathematical derivation. The code and the full set of LLM-generated responses are open-sourced and publicly available at https://github.com/magiclinux/math_benchmark_probability.

</details>


### [30] [ReflCtrl: Controlling LLM Reflection via Representation Engineering](https://arxiv.org/abs/2512.13979)
*Ge Yan,Chung-En Sun,Tsui-Wei,Weng*

Main category: cs.AI

TL;DR: ReflCtrl框架通过表示工程控制LLM的自我反思频率，减少推理成本，发现反思常冗余且与内部不确定性信号相关


<details>
  <summary>Details</summary>
Motivation: 虽然自我反思能提升推理性能，但会增加推理成本，需要研究如何控制反思频率以优化效率

Method: 通过表示工程将推理过程分段，识别反思步骤，提取潜在空间中的反思方向，提出逐步引导方法ReflCtrl控制反思频率

Result: 实验显示：(1) 反思常冗余，尤其在强模型中可节省33.6%的推理token而保持性能；(2) 反思行为与内部不确定性信号高度相关

Conclusion: 自我反思可通过表示工程控制，反思冗余性表明可优化推理效率，反思与不确定性相关为理解反思机制提供新视角

Abstract: Large language models (LLMs) with Chain-of-Thought (CoT) reasoning have achieved strong performance across diverse tasks, including mathematics, coding, and general reasoning. A distinctive ability of these reasoning models is self-reflection: the ability to review and revise previous reasoning steps. While self-reflection enhances reasoning performance, it also increases inference cost. In this work, we study self-reflection through the lens of representation engineering. We segment the model's reasoning into steps, identify the steps corresponding to reflection, and extract a reflection direction in the latent space that governs this behavior. Using this direction, we propose a stepwise steering method that can control reflection frequency. We call our framework ReflCtrl. Our experiments show that (1) in many cases reflections are redundant, especially in stronger models (in our experiments, we can save up to 33.6 percent of reasoning tokens while preserving performance), and (2) the model's reflection behavior is highly correlated with an internal uncertainty signal, implying self-reflection may be controlled by the model's uncertainty.

</details>


### [31] [Sparsity-Controllable Dynamic Top-p MoE for Large Foundation Model Pre-training](https://arxiv.org/abs/2512.13996)
*Can Jin,Hongwu Peng,Mingcan Xiang,Qixin Zhang,Xiangchi Yuan,Amit Hasan,Ohiremen Dibua,Yifan Gong,Yan Kang,Dimitris N. Metaxas*

Main category: cs.AI

TL;DR: 提出DTop-p MoE，一种可控制稀疏度的动态Top-p路由机制，通过PI控制器动态调整概率阈值，实现精确的计算成本控制，优于传统Top-k和固定阈值Top-p方法。


<details>
  <summary>Details</summary>
Motivation: 传统MoE架构中，Top-k路由采用统一的稀疏模式，忽略了不同token的难度差异；而现有Top-p路由使用固定全局概率阈值，导致计算成本不可控且对超参数敏感。

Method: 提出DTop-p MoE：1）使用比例-积分（PI）控制器动态调整概率阈值，使激活专家稀疏度与目标值对齐；2）引入动态路由归一化机制，允许不同层学习不同的专家选择模式，同时使用全局概率阈值。

Result: 在大语言模型和扩散变换器上的实验表明，DTop-p consistently优于Top-k和固定阈值Top-p基线，能精确控制激活专家数量，并自适应地在不同token和层间分配资源。

Conclusion: DTop-p在专家粒度、专家容量、模型规模和数据集大小方面展现出良好的扩展性，为大规模MoE预训练提供了一个鲁棒的框架。

Abstract: Sparse Mixture-of-Experts (MoE) architectures effectively scale model capacity by activating only a subset of experts for each input token. However, the standard Top-k routing strategy imposes a uniform sparsity pattern that ignores the varying difficulty of tokens. While Top-p routing offers a flexible alternative, existing implementations typically rely on a fixed global probability threshold, which results in uncontrolled computational costs and sensitivity to hyperparameter selection. In this paper, we propose DTop-p MoE, a sparsity-controllable dynamic Top-p routing mechanism. To resolve the challenge of optimizing a non-differentiable threshold, we utilize a Proportional-Integral (PI) Controller that dynamically adjusts the probability threshold to align the running activated-expert sparsity with a specified target. Furthermore, we introduce a dynamic routing normalization mechanism that adapts layer-wise routing logits, allowing different layers to learn distinct expert-selection patterns while utilizing a global probability threshold. Extensive experiments on Large Language Models and Diffusion Transformers demonstrate that DTop-p consistently outperforms both Top-k and fixed-threshold Top-p baselines. Our analysis confirms that DTop-p maintains precise control over the number of activated experts while adaptively allocating resources across different tokens and layers. Furthermore, DTop-p exhibits strong scaling properties with respect to expert granularity, expert capacity, model size, and dataset size, offering a robust framework for large-scale MoE pre-training.

</details>


### [32] [MobileWorldBench: Towards Semantic World Modeling For Mobile Agents](https://arxiv.org/abs/2512.14014)
*Shufan Li,Konstantinos Kallidromitis,Akash Gokul,Yusuke Kato,Kazuki Kozuka,Aditya Grover*

Main category: cs.AI

TL;DR: 提出MobileWorldBench基准和MobileWorld数据集，探索用自然语言描述状态转移的GUI世界模型，提升移动GUI代理的任务成功率


<details>
  <summary>Details</summary>
Motivation: 传统像素空间世界模型在GUI环境中预测复杂视觉元素困难，需要探索替代方案来提升GUI代理的性能

Method: 1) 引入MobileWorldBench基准评估VLM作为移动GUI代理世界模型的能力；2) 发布140万样本的MobileWorld数据集；3) 提出将VLM世界模型集成到移动代理规划框架的新框架

Result: 语义世界模型能直接提升移动代理的任务成功率，MobileWorld数据集显著增强了VLM的世界建模能力

Conclusion: 用自然语言描述状态转移的语义世界模型是GUI代理的有效替代方案，能克服像素空间模型的局限性并提升任务性能

Abstract: World models have shown great utility in improving the task performance of embodied agents. While prior work largely focuses on pixel-space world models, these approaches face practical limitations in GUI settings, where predicting complex visual elements in future states is often difficult. In this work, we explore an alternative formulation of world modeling for GUI agents, where state transitions are described in natural language rather than predicting raw pixels. First, we introduce MobileWorldBench, a benchmark that evaluates the ability of vision-language models (VLMs) to function as world models for mobile GUI agents. Second, we release MobileWorld, a large-scale dataset consisting of 1.4M samples, that significantly improves the world modeling capabilities of VLMs. Finally, we propose a novel framework that integrates VLM world models into the planning framework of mobile agents, demonstrating that semantic world models can directly benefit mobile agents by improving task success rates. The code and dataset is available at https://github.com/jacklishufan/MobileWorld

</details>


### [33] [Evaluating Small Language Models for Agentic On-Farm Decision Support Systems](https://arxiv.org/abs/2512.14043)
*Enhong Liu,Haiyu Yang,Miel Hostens*

Main category: cs.AI

TL;DR: 评估20个开源小语言模型在奶牛养殖决策支持中的可行性，Qwen-4B在多数任务中表现最佳，但PySpark NoSQL交互不稳定


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLM)有潜力支持奶牛养殖决策，但计算需求大且依赖云端服务，不适用于农场环境。需要能在农场硬件上本地运行的轻量级替代方案。

Method: 在农场现实计算约束下，对HuggingFace上的20个开源小语言模型(SLM)进行基准测试。开发包含五个任务特定代理的AI系统：文献搜索、网络搜索、SQL数据库交互、NoSQL数据库交互和预测模型后的图生成。采用两阶段评估：第一阶段用5个测试问题初步筛选，第二阶段用30个问题（每个任务类别5个，外加诚信和不当行为类别）深入评估。

Result: Qwen-4B在大多数任务类别中取得了优越性能，但在通过PySpark进行NoSQL数据库交互时表现出不稳定的效果。这是首个明确评估SLM作为奶牛养殖决策引擎可行性的工作。

Conclusion: SLM辅助工具在奶牛养殖实际部署中显示出前景，但仍存在挑战，需要通过微调来改进SLM在奶牛特定问题上的性能。研究强调了隐私和计算效率的重要性。

Abstract: Large Language Models (LLM) hold potential to support dairy scholars and farmers by supporting decision-making and broadening access to knowledge for stakeholders with limited technical expertise. However, the substantial computational demand restricts access to LLM almost exclusively through cloud-based service, which makes LLM-based decision support tools impractical for dairy farming. To address this gap, lightweight alternatives capable of running locally on farm hardware are required. In this work, we benchmarked 20 open-source Small Language Models (SLM) available on HuggingFace under farm-realistic computing constraints. Building on our prior work, we developed an agentic AI system that integrates five task-specific agents: literature search, web search, SQL database interaction, NoSQL database interaction, and graph generation following predictive models. Evaluation was conducted in two phases. In the first phase, five test questions were used for the initial screening to identify models capable of following basic dairy-related instructions and performing reliably in a compute-constrained environment. Models that passed this preliminary stage were then evaluated using 30 questions (five per task category mentioned above, plus one category addressing integrity and misconduct) in phase two. In results, Qwen-4B achieved superior performance across most of task categories, although showed unstable effectiveness in NoSQL database interactions through PySpark. To our knowledge, this is the first work explicitly evaluating the feasibility of SLM as engines for dairy farming decision-making, with central emphases on privacy and computational efficiency. While results highlight the promise of SLM-assisted tools for practical deployment in dairy farming, challenges remain, and fine-tuning is still needed to refine SLM performance in dairy-specific questions.

</details>


### [34] [Intention Chain-of-Thought Prompting with Dynamic Routing for Code Generation](https://arxiv.org/abs/2512.14048)
*Shen Li,Li Huang,Shaoxiong Zhan,Weifeng Sun,Tao Yin,Zhongxin Liu,Meng Yan*

Main category: cs.AI

TL;DR: RoutingGen：一种难度感知的路由框架，根据任务复杂度动态选择提示策略（简单任务用few-shot，复杂任务用Intention Chain-of-Thought），在代码生成任务中实现SOTA性能并减少46.37%的token使用。


<details>
  <summary>Details</summary>
Motivation: 现有CoT方法有两个主要局限：1) 统一应用导致简单任务上过度思考；2) 缺乏代码生成中的意图抽象（如核心算法设计和效率建模），使模型关注表面结构而忽视全局目标。受认知经济原则启发，只在必要时进行结构化推理以节省认知资源。

Method: 提出RoutingGen框架：1) 难度感知路由机制，动态选择提示策略；2) 简单任务使用few-shot提示；3) 复杂任务使用Intention Chain-of-Thought（ICoT），引导模型捕获任务意图（核心算法逻辑和时间复杂度等）。

Result: 在三个模型和六个标准代码生成基准测试中，RoutingGen在大多数设置下达到最先进性能，平均减少46.37%的token使用。ICoT在挑战性基准上优于六个现有提示基线。

Conclusion: RoutingGen通过难度感知路由和意图链式思维，有效解决了现有CoT方法的局限性，在提升代码生成性能的同时显著降低了计算成本，为LLM代码生成提供了更高效的推理框架。

Abstract: Large language models (LLMs) exhibit strong generative capabilities and have shown great potential in code generation. Existing chain-of-thought (CoT) prompting methods enhance model reasoning by eliciting intermediate steps, but suffer from two major limitations: First, their uniform application tends to induce overthinking on simple tasks. Second, they lack intention abstraction in code generation, such as explicitly modeling core algorithmic design and efficiency, leading models to focus on surface-level structures while neglecting the global problem objective. Inspired by the cognitive economy principle of engaging structured reasoning only when necessary to conserve cognitive resources, we propose RoutingGen, a novel difficulty-aware routing framework that dynamically adapts prompting strategies for code generation. For simple tasks, it adopts few-shot prompting; for more complex ones, it invokes a structured reasoning strategy, termed Intention Chain-of-Thought (ICoT), which we introduce to guide the model in capturing task intention, such as the core algorithmic logic and its time complexity. Experiments across three models and six standard code generation benchmarks show that RoutingGen achieves state-of-the-art performance in most settings, while reducing total token usage by 46.37% on average across settings. Furthermore, ICoT outperforms six existing prompting baselines on challenging benchmarks.

</details>


### [35] [OpenDataArena: A Fair and Open Arena for Benchmarking Post-Training Dataset Value](https://arxiv.org/abs/2512.14051)
*Mengzhang Cai,Xin Gao,Yu Li,Honglin Lin,Zheng Liu,Zhuoshi Pan,Qizhi Pei,Xiaoran Shang,Mengyuan Sun,Zinan Tang,Xiaoyang Wang,Zhanping Zhong,Yun Zhu,Dahua Lin,Conghui He,Lijun Wu*

Main category: cs.AI

TL;DR: OpenDataArena (ODA) 是一个开源平台，用于评估后训练数据的价值，包含统一训练评估流程、多维度评分框架、数据谱系探索器和开源工具包，旨在推动数据为中心的人工智能研究。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的发展依赖于高质量的后训练数据，但这些数据的构成、来源和评估往往不透明，阻碍了可重复性研究，也模糊了数据特性与模型行为之间的因果关系。

Method: ODA 平台包含四个核心组件：1) 统一的训练-评估流程确保公平比较；2) 多维度评分框架从数十个维度评估数据质量；3) 交互式数据谱系探索器可视化数据集来源；4) 完全开源的工具包支持训练、评估和评分。

Result: 在超过120个训练数据集、22个基准测试、600多次训练运行和4000万个处理数据点的实验中，ODA揭示了数据复杂性与任务性能之间的权衡，通过谱系追踪识别了流行基准中的冗余，并绘制了数据集之间的谱系关系图。

Conclusion: ODA 不仅扩展了排行榜功能，更旨在推动从试错式数据管理向数据为中心的人工智能科学转变，为研究数据混合规律和基础模型战略构成铺平道路。

Abstract: The rapid evolution of Large Language Models (LLMs) is predicated on the quality and diversity of post-training datasets. However, a critical dichotomy persists: while models are rigorously benchmarked, the data fueling them remains a black box--characterized by opaque composition, uncertain provenance, and a lack of systematic evaluation. This opacity hinders reproducibility and obscures the causal link between data characteristics and model behaviors. To bridge this gap, we introduce OpenDataArena (ODA), a holistic and open platform designed to benchmark the intrinsic value of post-training data. ODA establishes a comprehensive ecosystem comprising four key pillars: (i) a unified training-evaluation pipeline that ensures fair, open comparisons across diverse models (e.g., Llama, Qwen) and domains; (ii) a multi-dimensional scoring framework that profiles data quality along tens of distinct axes; (iii) an interactive data lineage explorer to visualize dataset genealogy and dissect component sources; and (iv) a fully open-source toolkit for training, evaluation, and scoring to foster data research. Extensive experiments on ODA--covering over 120 training datasets across multiple domains on 22 benchmarks, validated by more than 600 training runs and 40 million processed data points--reveal non-trivial insights. Our analysis uncovers the inherent trade-offs between data complexity and task performance, identifies redundancy in popular benchmarks through lineage tracing, and maps the genealogical relationships across datasets. We release all results, tools, and configurations to democratize access to high-quality data evaluation. Rather than merely expanding a leaderboard, ODA envisions a shift from trial-and-error data curation to a principled science of Data-Centric AI, paving the way for rigorous studies on data mixing laws and the strategic composition of foundation models.

</details>


### [36] [RADAR: Accelerating Large Language Model Inference With RL-Based Dynamic Draft Trees](https://arxiv.org/abs/2512.14069)
*Junjie Ma,Jinlong Li*

Main category: cs.AI

TL;DR: RADAR提出了一种基于强化学习的动态草稿树推测采样方法，通过实时决策草稿模型调用，减少冗余计算，加速LLM推理


<details>
  <summary>Details</summary>
Motivation: 现代大型语言模型推理成本高且速度慢，推测采样是有效解决方案，但传统方法中草稿模型调用次数是预设超参数，缺乏灵活性，无法有效生成和利用候选标记

Method: 将草稿树生成过程建模为马尔可夫决策过程，采用离线强化学习训练预测模型，实现实时决策草稿模型调用，减少冗余计算

Result: 在三个LLM和四个任务上的评估显示，RADAR相比自回归解码基线实现了3.17x-4.82x的加速

Conclusion: RADAR通过强化学习动态决策草稿模型调用，有效加速LLM推理，代码已开源

Abstract: Inference with modern Large Language Models (LLMs) is expensive and slow, and speculative sampling has emerged as an effective solution to this problem, however, the number of the calls to the draft model for generating candidate tokens in speculative sampling is a preset hyperparameter, lacking flexibility. To generate and utilize the candidate tokens more effectively, we propose RADAR, a novel speculative sampling method with RL-based dynamic draft trees. RADAR formulates the draft tree generation process as a Markov Decision Process (MDP) and employs offline reinforcement learning to train a prediction model, which enables real-time decision on the calls to the draft model, reducing redundant computations and further accelerating inference. Evaluations across three LLMs and four tasks show that RADAR achieves a speedup of 3.17x-4.82x over the auto-regressive decoding baseline. The code is available at https://github.com/minaduki-sora/RADAR.

</details>


### [37] [Grammar Search for Multi-Agent Systems](https://arxiv.org/abs/2512.14079)
*Mayank Singh,Vikas Yadav,Shiva Krishna Reddy Malay,Shravan Nayak,Sai Rajeswar,Sathwik Tejaswi Madhusudhan,Eduardo Blanco*

Main category: cs.AI

TL;DR: 提出了一种基于固定可组合组件的结构化多智能体系统搜索框架，相比基于LLM的自由形式搜索，在数学和问答领域的五个基准测试中，有四个表现更优，且搜索成本更低、系统更模块化和可解释。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统的自动搜索研究主要依赖LLM在代码空间进行自由形式搜索，这种方法虽然灵活但存在效率、成本和可解释性等问题。作者希望开发一种更结构化、高效且可解释的搜索框架。

Method: 提出了一种结构化搜索框架，使用一组固定的简单可组合组件来探索多智能体系统的设计空间。与基于LLM的自由形式生成不同，该方法通过组合预定义的组件来构建候选系统，限制了生成灵活性但提高了搜索效率和可解释性。

Result: 在数学和问答两个领域的五个基准测试中，该方法在四个基准上超越了先前基于LLM的自由形式搜索方法。同时，该方法具有更低的搜索成本，并能生成更模块化、可解释且逻辑更简单的多智能体系统。

Conclusion: 结构化搜索框架虽然牺牲了LLM的生成灵活性，但在性能、成本和可解释性方面具有显著优势，为多智能体系统的自动搜索提供了更高效实用的解决方案。

Abstract: Automatic search for Multi-Agent Systems has recently emerged as a key focus in agentic AI research. Several prior approaches have relied on LLM-based free-form search over the code space. In this work, we propose a more structured framework that explores the same space through a fixed set of simple, composable components. We show that, despite lacking the generative flexibility of LLMs during the candidate generation stage, our method outperforms prior approaches on four out of five benchmarks across two domains: mathematics and question answering. Furthermore, our method offers additional advantages, including a more cost-efficient search process and the generation of modular, interpretable multi-agent systems with simpler logic.

</details>


### [38] [HydroGEM: A Self Supervised Zero Shot Hybrid TCN Transformer Foundation Model for Continental Scale Streamflow Quality Control](https://arxiv.org/abs/2512.14106)
*Ijaz Ul Haq,Byung Suk Lee,Julia N. Perdrial,David Baude*

Main category: cs.AI

TL;DR: HydroGEM是一个用于大陆尺度河流流量质量控制的基础模型，通过两阶段训练和混合TCN-Transformer架构，在异常检测和重建方面显著优于现有方法，并展示了跨国家泛化能力。


<details>
  <summary>Details</summary>
Motivation: 实时河流流量监测网络每年产生数百万个观测数据，但维护数千个远程传感器的数据质量仍然需要大量人工。现有方法难以有效处理大规模、跨区域的河流流量数据质量控制问题。

Method: 采用两阶段训练：首先在3,724个USGS站的603万序列上进行自监督预训练学习水文表征，然后使用合成异常进行微调用于检测和重建。使用混合TCN-Transformer架构（1420万参数）捕获局部时间模式和长程依赖，层次归一化处理六个数量级的流量变化。

Result: 在799个站点的18种专家验证异常类型的合成测试中，HydroGEM达到F1=0.792的检测性能和68.7%的重建误差减少，比现有方法提升36.3%。在100个加拿大环境与气候变化部站点的零样本迁移中达到F1=0.586，超过所有基线方法。

Conclusion: HydroGEM是一个有效的大陆尺度河流流量质量控制基础模型，具有跨国家泛化能力。模型设计用于人机协同工作流，输出需要专家审核的质量控制建议而非自主修正。

Abstract: Real-time streamflow monitoring networks generate millions of observations annually, yet maintaining data quality across thousands of remote sensors remains labor-intensive. We introduce HydroGEM (Hydrological Generalizable Encoder for Monitoring), a foundation model for continental-scale streamflow quality control. HydroGEM uses two-stage training: self-supervised pretraining on 6.03 million sequences from 3,724 USGS stations learns hydrological representations, followed by fine-tuning with synthetic anomalies for detection and reconstruction. A hybrid TCN-Transformer architecture (14.2M parameters) captures local temporal patterns and long-range dependencies, while hierarchical normalization handles six orders of magnitude in discharge. On held-out synthetic tests comprising 799 stations with 18 expert-validated anomaly types, HydroGEM achieves F1 = 0.792 for detection and 68.7% reconstruction-error reduction, a 36.3% improvement over existing methods. Zero-shot transfer to 100 Environment and Climate Change Canada stations yields F1 = 0.586, exceeding all baselines and demonstrating cross-national generalization. The model maintains consistent detection across correction magnitudes and aligns with operational seasonal patterns. HydroGEM is designed for human-in-the-loop workflows - outputs are quality control suggestions requiring expert review, not autonomous corrections.

</details>


### [39] [Optimizing Multi-Tier Supply Chain Ordering with a Hybrid Liquid Neural Network and Extreme Gradient Boosting Model](https://arxiv.org/abs/2512.14112)
*Chunan Tong*

Main category: cs.AI

TL;DR: 提出混合LNN+XGBoost模型用于多级供应链管理，结合LNN的动态特征提取和XGBoost的全局优化，旨在减少牛鞭效应并提高盈利能力。


<details>
  <summary>Details</summary>
Motivation: 供应链管理面临需求波动和牛鞭效应等挑战，传统方法和先进LLM难以处理复杂的连续时间序列数据，现有ML方法如LSTM和XGBoost存在计算效率限制，而液态神经网络在供应链领域尚未被探索。

Method: 提出混合LNN+XGBoost模型，利用液态神经网络（LNN）的动态特征提取能力结合XGBoost的全局优化优势，应用于多级供应链场景。

Result: 模型旨在最小化牛鞭效应并提高供应链盈利能力，填补了智能供应链管理中效率和适应性方面的关键空白。

Conclusion: 这种创新方法通过结合LNN的适应性和XGBoost的优化能力，为供应链管理提供了更高效和适应性强的解决方案。

Abstract: Supply chain management (SCM) faces significant challenges like demand fluctuations and the bullwhip effect. Traditional methods and even state-of-the-art LLMs struggle with benchmarks like the Vending Machine Test, failing to handle SCM's complex continuous time-series data. While ML approaches like LSTM and XGBoost offer solutions, they are often limited by computational inefficiency. Liquid Neural Networks (LNN), known for their adaptability and efficiency in robotics, remain untapped in SCM. This study proposes a hybrid LNN+XGBoost model for multi-tier supply chains. By combining LNN's dynamic feature extraction with XGBoost's global optimization, the model aims to minimize the bullwhip effect and increase profitability. This innovative approach addresses the need for efficiency and adaptability, filling a critical gap in intelligent SCM.

</details>


### [40] [Incentivizing Tool-augmented Thinking with Images for Medical Image Analysis](https://arxiv.org/abs/2512.14157)
*Yankai Jiang,Yujie Zhang,Peng Zhang,Yichen Li,Jintai Chen,Xiaoming Shi,Shihui Zhen*

Main category: cs.AI

TL;DR: Ophiuchus是一个工具增强的多模态大语言模型框架，通过动态聚焦医学图像细粒度区域进行迭代推理，在医学VQA、检测和分割任务上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有医学MLLM虽然能生成文本推理链，但在需要动态迭代聚焦图像细粒度区域以进行精确定位和诊断的复杂任务上仍有局限。

Method: 三阶段训练策略：1) 冷启动训练，使用工具集成推理数据实现基本工具选择和关键区域检查；2) 自反思微调，增强反思推理能力；3) 智能体工具强化学习，直接优化任务特定奖励并模拟专家诊断行为。

Result: 在多种医学基准测试（包括VQA、检测和基于推理的分割）上，Ophiuchus一致优于闭源和开源的最先进方法。

Conclusion: 该方法为医学AI智能体通过工具集成推理真正"用图像思考"开辟了道路，数据集、代码和训练模型将公开发布。

Abstract: Recent reasoning based medical MLLMs have made progress in generating step by step textual reasoning chains. However, they still struggle with complex tasks that necessitate dynamic and iterative focusing on fine-grained visual regions to achieve precise grounding and diagnosis. We introduce Ophiuchus, a versatile, tool-augmented framework that equips an MLLM to (i) decide when additional visual evidence is needed, (ii) determine where to probe and ground within the medical image, and (iii) seamlessly weave the relevant sub-image content back into an interleaved, multimodal chain of thought. In contrast to prior approaches limited by the performance ceiling of specialized tools, Ophiuchus integrates the model's inherent grounding and perception capabilities with external tools, thereby fostering higher-level reasoning. The core of our method is a three-stage training strategy: cold-start training with tool-integrated reasoning data to achieve basic tool selection and adaptation for inspecting key regions; self-reflection fine-tuning to strengthen reflective reasoning and encourage revisiting tool outputs; and Agentic Tool Reinforcement Learning to directly optimize task-specific rewards and emulate expert-like diagnostic behavior. Extensive experiments show that Ophiuchus consistently outperforms both closed-source and open-source SOTA methods across diverse medical benchmarks, including VQA, detection, and reasoning-based segmentation. Our approach illuminates a path toward medical AI agents that can genuinely "think with images" through tool-integrated reasoning. Datasets, codes, and trained models will be released publicly.

</details>


### [41] [Georeferencing complex relative locality descriptions with large language models](https://arxiv.org/abs/2512.14228)
*Aneesha Fernando,Surangika Ranathunga,Kristin Stock,Raj Prasanna,Christopher B. Jones*

Main category: cs.AI

TL;DR: 本文探索使用大语言模型（LLM）自动地理编码生物标本采集记录中的复杂位置描述，通过QLoRA微调在多个区域和语言数据集上取得优于基线方法的结果。


<details>
  <summary>Details</summary>
Motivation: 生物标本采集记录通常使用叙述性文字而非坐标描述位置，传统基于地名库或语言模型的方法难以处理包含空间关系的相对位置描述，而人工地理编码又耗时耗力，因此需要自动化解决方案。

Method: 首先识别有效的提示模式，然后使用量化低秩适应（QLoRA）在多个区域和语言的多源生物多样性数据集上微调大语言模型，用于地理编码复杂位置描述。

Result: 方法在固定训练数据量下优于现有基线，平均65%的记录在10公里半径内，最佳结果（纽约州）达到85%在10公里内和67%在1公里内，特别擅长处理冗长复杂描述。

Conclusion: 大语言模型在自动地理编码复杂位置描述方面具有显著潜力，特别是在生物多样性收集领域，能够有效处理包含空间关系的叙述性位置描述。

Abstract: Georeferencing text documents has typically relied on either gazetteer-based methods to assign geographic coordinates to place names, or on language modelling approaches that associate textual terms with geographic locations. However, many location descriptions specify positions relatively with spatial relationships, making geocoding based solely on place names or geo-indicative words inaccurate. This issue frequently arises in biological specimen collection records, where locations are often described through narratives rather than coordinates if they pre-date GPS. Accurate georeferencing is vital for biodiversity studies, yet the process remains labour-intensive, leading to a demand for automated georeferencing solutions. This paper explores the potential of Large Language Models (LLMs) to georeference complex locality descriptions automatically, focusing on the biodiversity collections domain. We first identified effective prompting patterns, then fine-tuned an LLM using Quantized Low-Rank Adaptation (QLoRA) on biodiversity datasets from multiple regions and languages. Our approach outperforms existing baselines with an average, across datasets, of 65% of records within a 10 km radius, for a fixed amount of training data. The best results (New York state) were 85% within 10km and 67% within 1km. The selected LLM performs well for lengthy, complex descriptions, highlighting its potential for georeferencing intricate locality descriptions.

</details>


### [42] [Gödel's Poetry](https://arxiv.org/abs/2512.14252)
*Kelly J. Davis*

Main category: cs.AI

TL;DR: 提出一种结合专用语言模型和递归分解定理的新方法，用于Lean4自动定理证明，显著提升miniF2F测试通过率


<details>
  <summary>Details</summary>
Motivation: 形式化自动定理证明长期以来被视为人工智能的挑战，需要更有效的方法来处理复杂定理

Method: 使用专用语言模型生成Lean4证明，结合递归分解复杂定理为更简单命题，采用多智能体架构协调自动形式化、证明生成和分解过程

Result: 无分解时在miniF2F上达到90.4%通过率，使用分解后性能显著提升；扩展了Kimina Lean Server的AST解析能力支持递归证明分解

Conclusion: 提出的方法有效提升了自动定理证明能力，系统已开源并可通过PyPI安装，支持定制化扩展

Abstract: Formal, automated theorem proving has long been viewed as a challenge to artificial intelligence. We introduce here a new approach to computer theorem proving, one that employs specialized language models for Lean4 proof generation combined with recursive decomposition of difficult theorems into simpler entailing propositions. These models are coordinated through a multi-agent architecture that orchestrates autoformalization (if required), proof generation, decomposition of difficult theorems into simpler entailing propositions, and recursive proof (and/or decomposition) of these propositions. Without decomposition, we achieve a 90.4% pass rate on miniF2F. With decomposition, this is significantly improved. A key technical contribution lies in our extension of the Kimina Lean Server with abstract syntax tree (AST) parsing capabilities to facilitate automated, recursive proof decomposition. The system is made available on PyPI as goedels-poetry (at https://pypi.org/project/goedels-poetry ), and the open-source implementation KellyJDavis/goedels-poetry (at https://github.com/KellyJDavis/goedels-poetry ) facilitates both adaptation to alternative language models and extension with custom functionality.

</details>


### [43] [Leveraging LLMs for Collaborative Ontology Engineering in Parkinson Disease Monitoring and Alerting](https://arxiv.org/abs/2512.14288)
*Georgios Bouchouras,Dimitrios Doumanas,Andreas Soularidis,Konstantinos Kotis,George A. Vouros*

Main category: cs.AI

TL;DR: LLMs能部分自主构建帕金森病监测本体，但不够全面；人机协作方法（X-HCOME和SimX-HCOME+）显著提升本体质量，接近专家水平。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在自动化本体工程中的能力极限，研究人机协作是否能实现更全面的本体构建，特别是在帕金森病监测这样的复杂领域。

Method: 采用四种方法：单次提示（OS）、思维链提示（CoT）、X-HCOME（人机混合方法）、SimX-HCOME+（强调持续人工监督的迭代优化方法）。

Result: LLMs能自主构建本体但不全面；X-HCOME显著提升本体完整性，接近专家水平；SimX-HCOME+通过持续人工监督产生更全面准确的本体。

Conclusion: LLMs单独构建本体能力有限，但人机协作能显著提升本体工程效果，为复杂领域本体开发提供新方向，未来可开发专用GPT模型。

Abstract: This paper explores the integration of Large Language Models (LLMs) in the engineering of a Parkinson's Disease (PD) monitoring and alerting ontology through four key methodologies: One Shot (OS) prompt techniques, Chain of Thought (CoT) prompts, X-HCOME, and SimX-HCOME+. The primary objective is to determine whether LLMs alone can create comprehensive ontologies and, if not, whether human-LLM collaboration can achieve this goal. Consequently, the paper assesses the effectiveness of LLMs in automated ontology development and the enhancement achieved through human-LLM collaboration.
  Initial ontology generation was performed using One Shot (OS) and Chain of Thought (CoT) prompts, demonstrating the capability of LLMs to autonomously construct ontologies for PD monitoring and alerting. However, these outputs were not comprehensive and required substantial human refinement to enhance their completeness and accuracy.
  X-HCOME, a hybrid ontology engineering approach that combines human expertise with LLM capabilities, showed significant improvements in ontology comprehensiveness. This methodology resulted in ontologies that are very similar to those constructed by experts.
  Further experimentation with SimX-HCOME+, another hybrid methodology emphasizing continuous human supervision and iterative refinement, highlighted the importance of ongoing human involvement. This approach led to the creation of more comprehensive and accurate ontologies.
  Overall, the paper underscores the potential of human-LLM collaboration in advancing ontology engineering, particularly in complex domains like PD. The results suggest promising directions for future research, including the development of specialized GPT models for ontology construction.

</details>


### [44] [TiCard: Deployable EXPLAIN-only Residual Learning for Cardinality Estimation](https://arxiv.org/abs/2512.14358)
*Qizhi Wang*

Main category: cs.AI

TL;DR: TiCard是一个低侵入性的基数估计校正框架，通过EXPLAIN特征学习乘法残差校正来增强数据库原生估计器，无需替换现有系统。


<details>
  <summary>Details</summary>
Motivation: 传统基数估计器无法处理相关性，而学习型估计器通常需要特定工作负载的训练流程和侵入性的优化器集成，部署困难。需要一种低侵入性的解决方案。

Method: 提出TiCard框架，使用EXPLAIN特征学习乘法残差校正，仅用EXPLAIN ANALYZE进行离线标注。研究两种实现：梯度提升回归器（快速推理）和TabPFN（上下文表格基础模型，无需梯度重训练）。

Result: 在TiDB上测试TPCH和Join Order Benchmark，在低追踪设置下（总共263次执行，157次用于学习），TiCard显著改善算子级尾部精度：P90 Q-error从312.85降至13.69（TiCard-GBR），P99从37,974.37降至3,416.50（TiCard-TabPFN）。

Conclusion: TiCard作为AI4DB构建块，专注于可部署性：明确的范围、保守的集成策略，以及从离线校正到优化器内使用的集成路线图。

Abstract: Cardinality estimation is a key bottleneck for cost-based query optimization, yet deployable improvements remain difficult: classical estimators miss correlations, while learned estimators often require workload-specific training pipelines and invasive integration into the optimizer. This paper presents TiCard, a low intrusion, correction-based framework that augments (rather than replaces) a database's native estimator. TiCard learns multiplicative residual corrections using EXPLAIN-only features, and uses EXPLAIN ANALYZE only for offline labels. We study two practical instantiations: (i) a Gradient Boosting Regressor for sub-millisecond inference, and (ii) TabPFN, an in-context tabular foundation model that adapts by refreshing a small reference set without gradient retraining. On TiDB with TPCH and the Join Order Benchmark, in a low-trace setting (263 executions total; 157 used for learning), TiCard improves operator-level tail accuracy substantially: P90 Q-error drops from 312.85 (native) to 13.69 (TiCard-GBR), and P99 drops from 37,974.37 to 3,416.50 (TiCard-TabPFN), while a join-only policy preserves near-perfect median behavior. We position TiCard as an AI4DB building block focused on deployability: explicit scope, conservative integration policies, and an integration roadmap from offline correction to in-optimizer use.

</details>


### [45] [Massive Editing for Large Language Models Based on Dynamic Weight Generation](https://arxiv.org/abs/2512.14395)
*Wentao Wan,Qiqing Lao,Zhiwei Xie,Hefeng Wu,Runnan Lin,Liang Lin,Keze Wang*

Main category: cs.AI

TL;DR: 本文提出MeG方法，通过动态权重神经元和扩散模型实现大规知识编辑，显著提升可靠性、泛化性和局部性指标


<details>
  <summary>Details</summary>
Motivation: 当前在大规模编辑LLMs时，确保编辑的可靠性、泛化性和局部性仍然是一个挑战，需要低成本的知识编辑方法

Method: MeG方法在LLMs特定层附加动态权重神经元，使用扩散模型根据输入查询条件生成神经元权重，通过添加单个动态权重神经元实现大规模知识编辑

Result: 实验表明MeG在可靠性、泛化性和局部性指标上显著优于现有知识编辑方法，特别是在局部性指标上获得高百分点提升

Conclusion: MeG方法通过动态权重生成机制有效解决了大规模知识编辑的挑战，展示了在保持编辑质量方面的优势

Abstract: Knowledge Editing (KE) is a field that studies how to modify some knowledge in Large Language Models (LLMs) at a low cost (compared to pre-training). Currently, performing large-scale edits on LLMs while ensuring the Reliability, Generality, and Locality metrics of the edits remain a challenge. This paper proposes a Massive editing approach for LLMs based on dynamic weight Generation (MeG). Our MeG involves attaching a dynamic weight neuron to specific layers of the LLMs and using a diffusion model to conditionally generate the weights of this neuron based on the input query required for the knowledge. This allows the use of adding a single dynamic weight neuron to achieve the goal of large-scale knowledge editing. Experiments show that our MeG can significantly improve the performance of large-scale KE in terms of Reliability, Generality, and Locality metrics compared to existing knowledge editing methods, particularly with a high percentage point increase in the absolute value index for the Locality metric, demonstrating the advantages of our proposed method.

</details>


### [46] [PortAgent: LLM-driven Vehicle Dispatching Agent for Port Terminals](https://arxiv.org/abs/2512.14417)
*Jia Hu,Junqi Li,Weimeng Lin,Peng Jia,Yuxiong Ji,Jintao Lai*

Main category: cs.AI

TL;DR: PortAgent：基于大语言模型的车辆调度代理，通过虚拟专家团队实现跨自动化集装箱码头的车辆调度系统快速迁移部署，无需港口运营专家、数据需求低、部署速度快。


<details>
  <summary>Details</summary>
Motivation: 自动化集装箱码头（ACTs）的车辆调度系统（VDSs）商业化面临跨码头迁移性差的挑战，主要受限于：高度依赖港口运营专家、需要大量码头特定数据、手动部署过程耗时。需要一种自动化解决方案来克服这些限制。

Method: 提出PortAgent LLM驱动车辆调度代理，采用虚拟专家团队（VET）架构，包括知识检索器、建模器、编码器和调试器四个虚拟专家。通过少样本示例学习方法让专家掌握VDS领域知识，使用检索增强生成（RAG）机制检索示例减少数据需求，建立自动VDS设计工作流，并引入基于LLM Reflexion框架的自校正循环。

Result: PortAgent实现了车辆调度系统的全自动化迁移工作流，具备三大特点：无需港口运营专家、数据需求低、部署速度快。通过虚拟专家团队协作和自校正机制，能够高效完成VDS的跨码头迁移部署。

Conclusion: PortAgent利用大语言模型解决了车辆调度系统跨码头迁移的三大挑战，通过虚拟专家团队架构和自动化工作流实现了高效、低成本的VDS迁移部署，为自动化集装箱码头的运营效率提升提供了创新解决方案。

Abstract: Vehicle Dispatching Systems (VDSs) are critical to the operational efficiency of Automated Container Terminals (ACTs). However, their widespread commercialization is hindered due to their low transferability across diverse terminals. This transferability challenge stems from three limitations: high reliance on port operational specialists, a high demand for terminal-specific data, and time-consuming manual deployment processes. Leveraging the emergence of Large Language Models (LLMs), this paper proposes PortAgent, an LLM-driven vehicle dispatching agent that fully automates the VDS transferring workflow. It bears three features: (1) no need for port operations specialists; (2) low need of data; and (3) fast deployment. Specifically, specialist dependency is eliminated by the Virtual Expert Team (VET). The VET collaborates with four virtual experts, including a Knowledge Retriever, Modeler, Coder, and Debugger, to emulate a human expert team for the VDS transferring workflow. These experts specialize in the domain of terminal VDS via a few-shot example learning approach. Through this approach, the experts are able to learn VDS-domain knowledge from a few VDS examples. These examples are retrieved via a Retrieval-Augmented Generation (RAG) mechanism, mitigating the high demand for terminal-specific data. Furthermore, an automatic VDS design workflow is established among these experts to avoid extra manual interventions. In this workflow, a self-correction loop inspired by the LLM Reflexion framework is created

</details>


### [47] [Seismology modeling agent: A smart assistant for geophysical researchers](https://arxiv.org/abs/2512.14429)
*Yukun Ren,Siwei Yu,Kai Chen,Jianwei Ma*

Main category: cs.AI

TL;DR: 该论文提出了首个基于大语言模型的SPECFEM地震波模拟智能交互工作流，通过MCP服务器将复杂的手动操作转化为可对话执行的工具，实现从文件驱动到意图驱动的范式转变。


<details>
  <summary>Details</summary>
Motivation: 传统SPECFEM软件学习曲线陡峭，依赖复杂的手动文件编辑和命令行操作，阻碍了研究人员的使用效率和科学探索。

Method: 开发了首个SPECFEM的MCP服务器套件（支持2D、3D笛卡尔和3D全球版本），将整个模拟过程分解为从参数生成、网格划分到求解器执行和可视化的离散化、代理可执行工具。

Result: 通过多个案例验证，该工作流在自主和交互模式下都能无缝运行，产生与标准基线一致的高保真结果，显著降低了入门门槛并增强了可重复性。

Conclusion: 这是MCP技术在计算地震学中的首次应用，为计算地球物理学向AI辅助和自动化科学研究提供了有前景的途径，同时保持了科学决策权。

Abstract: To address the steep learning curve and reliance on complex manual file editing and command-line operations in the traditional workflow of the mainstream open-source seismic wave simulation software SPECFEM, this paper proposes an intelligent, interactive workflow powered by Large Language Models (LLMs). We introduce the first Model Context Protocol (MCP) server suite for SPECFEM (supporting 2D, 3D Cartesian, and 3D Globe versions), which decomposes the entire simulation process into discrete, agent-executable tools spanning from parameter generation and mesh partitioning to solver execution and visualization. This approach enables a paradigm shift from file-driven to intent-driven conversational interactions. The framework supports both fully automated execution and human-in-the-loop collaboration, allowing researchers to guide simulation strategies in real time and retain scientific decision-making authority while significantly reducing tedious low-level operations. Validated through multiple case studies, the workflow operates seamlessly in both autonomous and interactive modes, yielding high-fidelity results consistent with standard baselines. As the first application of MCP technology to computational seismology, this study significantly lowers the entry barrier, enhances reproducibility, and offers a promising avenue for advancing computational geophysics toward AI-assisted and automated scientific research. The complete source code is available at https://github.com/RenYukun1563/specfem-mcp.

</details>


### [48] [Context-Picker: Dynamic context selection using multi-stage reinforcement learning](https://arxiv.org/abs/2512.14465)
*Siyuan Zhu,Chengdong Xu,Kaiqiang Ke,Chao Yu*

Main category: cs.AI

TL;DR: Context-Picker：一个推理感知的框架，将上下文选择从相似性排序转变为最小充分子集选择，通过两阶段强化学习优化，在长上下文QA中实现更精确的答案生成。


<details>
  <summary>Details</summary>
Motivation: 在长上下文问答中，确定查询所需的最佳上下文量是一个关键挑战。传统方法（如固定Top-K检索和单阶段重排序）面临选择适当段落数量的困境，特别是对于事实性问题，通常只需要少量特定证据。

Method: 提出Context-Picker框架，将上下文选择视为决策过程，通过两阶段强化学习优化：召回导向阶段优先覆盖推理链，精确导向阶段积极剪枝冗余以提炼紧凑证据集。采用离线证据蒸馏管道，通过留一法挖掘"最小充分集"提供密集监督。

Result: 在五个长上下文和多跳QA基准测试中，Context-Picker显著优于强大的RAG基线，在可比或减少的上下文长度下实现更优的答案准确性。消融研究表明，粗到细的优化调度、冗余感知奖励塑造和推理引导格式都对性能提升有重要贡献。

Conclusion: Context-Picker通过将上下文选择重新定义为最小充分子集选择问题，并采用人类启发的两阶段强化学习优化，有效解决了长上下文QA中的上下文量选择难题，实现了更精确、更高效的答案生成。

Abstract: In long-context question answering (LCQA), determining the optimal amount of context for a given query is a significant challenge. Including too few passages may omit critical information, while including too many can introduce noise and reduce the quality of the answer. Traditional approaches, such as fixed Top-$K$ retrieval and single-stage reranking, face the dilemma of selecting the right number of passages. This problem is particularly pronounced for factoid questions, which often require only a few specific pieces of evidence. To address this issue, we introduce \emph{Context-Picker}, a reasoning-aware framework that shifts the paradigm from similarity-based ranking to minimal sufficient subset selection. Context-Picker treats context selection as a decision-making process optimized via a human-inspired, two-stage reinforcement learning schedule: a \emph{recall-oriented} stage that prioritizes the coverage of reasoning chains, followed by a \emph{precision-oriented} stage that aggressively prunes redundancy to distill a compact evidence set. To resolve reward sparsity, we propose an offline evidence distillation pipeline that mines "minimal sufficient sets" via a Leave-One-Out (LOO) procedure, providing dense, task-aligned supervision. Experiments on five long-context and multi-hop QA benchmarks demonstrate that Context-Picker significantly outperforms strong RAG baselines, achieving superior answer accuracy with comparable or reduced context lengths. Ablation studies indicate that the coarse-to-fine optimization schedule, the redundancy-aware reward shaping, and the rationale-guided format all contribute substantially to these gains.

</details>


### [49] [Model-First Reasoning LLM Agents: Reducing Hallucinations through Explicit Problem Modeling](https://arxiv.org/abs/2512.14474)
*Annu Rana,Gaurav Kumar*

Main category: cs.AI

TL;DR: MFR是一种两阶段推理范式，先构建问题的显式模型再生成解决方案，相比传统方法显著减少了约束违反并提升了规划质量。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在复杂多步规划任务中表现不佳，存在高约束违反率和不一致解决方案的问题。传统方法如Chain-of-Thought和ReAct依赖隐式状态跟踪，缺乏明确的问题表示。

Method: 提出Model-First Reasoning (MFR)两阶段范式：第一阶段让LLM构建问题的显式模型，定义实体、状态变量、动作和约束；第二阶段基于该模型生成解决方案计划。

Result: 在医疗调度、路径规划、资源分配、逻辑谜题和程序合成等多个规划领域中，MFR相比Chain-of-Thought和ReAct显著减少了约束违反并提高了解决方案质量。消融研究表明显式建模阶段对这些改进至关重要。

Conclusion: 许多LLM规划失败源于表示缺陷而非推理限制，显式建模是构建鲁棒且可解释AI智能体的关键组件。研究提供了完整的提示、评估程序和任务数据集以确保可复现性。

Abstract: Large Language Models (LLMs) often struggle with complex multi-step planning tasks, showing high rates of constraint violations and inconsistent solutions. Existing strategies such as Chain-of-Thought and ReAct rely on implicit state tracking and lack an explicit problem representation. Inspired by classical AI planning, we propose Model-First Reasoning (MFR), a two-phase paradigm in which the LLM first constructs an explicit model of the problem, defining entities, state variables, actions, and constraints, before generating a solution plan. Across multiple planning domains, including medical scheduling, route planning, resource allocation, logic puzzles, and procedural synthesis, MFR reduces constraint violations and improves solution quality compared to Chain-of-Thought and ReAct. Ablation studies show that the explicit modeling phase is critical for these gains. Our results suggest that many LLM planning failures stem from representational deficiencies rather than reasoning limitations, highlighting explicit modeling as a key component for robust and interpretable AI agents. All prompts, evaluation procedures, and task datasets are documented to facilitate reproducibility.

</details>


### [50] [Sparse Multi-Modal Transformer with Masking for Alzheimer's Disease Classification](https://arxiv.org/abs/2512.14491)
*Cheng-Han Lu,Pei-Hsuan Tsai*

Main category: cs.AI

TL;DR: SMMT是一种稀疏多模态Transformer架构，通过聚类稀疏注意力和模态掩码提高效率与鲁棒性，在ADNI数据集上验证了其在保持性能的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 基于Transformer的多模态智能系统通常因密集自注意力机制导致高计算和能耗成本，限制了其在资源受限环境下的可扩展性。

Method: 在级联多模态Transformer框架基础上，引入聚类稀疏注意力实现近线性计算复杂度，并使用模态掩码增强对不完整输入的鲁棒性。

Result: 在ADNI数据集上的阿尔茨海默病分类任务中，SMMT在保持竞争力的预测性能的同时，显著减少了训练时间、内存使用和能耗。

Conclusion: SMMT作为资源感知的架构组件，适用于可扩展智能系统，在效率和鲁棒性方面优于密集注意力基线。

Abstract: Transformer-based multi-modal intelligent systems often suffer from high computational and energy costs due to dense self-attention, limiting their scalability under resource constraints. This paper presents SMMT, a sparse multi-modal transformer architecture designed to improve efficiency and robustness. Building upon a cascaded multi-modal transformer framework, SMMT introduces cluster-based sparse attention to achieve near linear computational complexity and modality-wise masking to enhance robustness against incomplete inputs. The architecture is evaluated using Alzheimer's Disease classification on the ADNI dataset as a representative multi-modal case study. Experimental results show that SMMT maintains competitive predictive performance while significantly reducing training time, memory usage, and energy consumption compared to dense attention baselines, demonstrating its suitability as a resource-aware architectural component for scalable intelligent systems.

</details>


### [51] [Dynamic Learning Rate Scheduling based on Loss Changes Leads to Faster Convergence](https://arxiv.org/abs/2512.14527)
*Shreyas Subramanian,Bala Krishnamoorthy,Pranav Murthy*

Main category: cs.AI

TL;DR: 提出GreedyLR调度器，根据当前损失自适应调整学习率，在NLP、CV和LLM任务上优于现有调度器，理论证明收敛性并推导最优缩放因子。


<details>
  <summary>Details</summary>
Motivation: 尽管训练优化器有显著进展，但大多数研究仍使用常见的调度器如余弦或指数衰减。需要一种更智能、自适应的学习率调度方法。

Method: 提出GreedyLR调度器，基于当前损失自适应调整学习率。提供理论分析，包括收敛性证明和最优缩放因子F的推导。

Result: 在多个NLP、CV和LLM任务（包括微调和预训练，参数达70亿）上实验，GreedyLR在准确性、速度和收敛性方面优于多个最先进的调度器。

Conclusion: GreedyLR易于实现、计算高效，可视为训练的良好默认调度器选择。

Abstract: Despite significant advances in optimizers for training, most research works use common scheduler choices like Cosine or exponential decay. In this paper, we study \emph{GreedyLR}, a novel scheduler that adaptively adjusts the learning rate during training based on the current loss. To validate the effectiveness of our proposed scheduler, we conduct experiments on several NLP, CV, and LLM tasks with up to $7B$ parameters, including both fine-tuning and pre-training experiments. The results show that our approach outperforms several state-of-the-art schedulers in terms of accuracy, speed, and convergence. We also provide a theoretical analysis of the GreedyLR algorithm, including a proof of convergence and derivation of the optimal scaling factor $F$ that maximizes the convergence rate, along with experiments to show robustness of the algorithm to realistic noisy landscapes. Our scheduler is easy to implement, computationally efficient, and could be considered a good default scheduler for training.

</details>


### [52] [Universal Reasoning Model](https://arxiv.org/abs/2512.14693)
*Zitian Gao,Lynx Chen,Yihao Xiao,He Xing,Ran Tao,Haoming Luo,Joey Zhou,Bryan Dai*

Main category: cs.AI

TL;DR: 研究发现Universal Transformers在ARC-AGI上的性能提升主要来自循环归纳偏置和Transformer的强非线性组件，而非复杂架构设计，并据此提出Universal Reasoning Model (URM)，在ARC-AGI上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: Universal Transformers (UTs) 在复杂推理任务如ARC-AGI和数独上表现优异，但其性能提升的具体来源尚不明确。本文旨在系统分析UTs变体，探究其性能提升的真正原因。

Method: 首先系统分析UTs变体，发现性能提升主要源于循环归纳偏置和Transformer的强非线性组件。基于此发现，提出Universal Reasoning Model (URM)，通过引入短卷积和截断反向传播来增强UT。

Result: URM在推理性能上显著提升，在ARC-AGI 1上达到53.8% pass@1，在ARC-AGI 2上达到16.0% pass@1，均创下新的最先进水平。

Conclusion: UTs在ARC-AGI上的性能提升主要来自循环归纳偏置和Transformer的非线性能力，而非复杂架构设计。URM通过简单有效的改进实现了SOTA性能，为构建更高效的推理模型提供了新思路。

Abstract: Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sudoku, yet the specific sources of their performance gains remain underexplored. In this work, we systematically analyze UTs variants and show that improvements on ARC-AGI primarily arise from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from elaborate architectural designs. Motivated by this finding, we propose the Universal Reasoning Model (URM), which enhances the UT with short convolution and truncated backpropagation. Our approach substantially improves reasoning performance, achieving state-of-the-art 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2. Our code is avaliable at https://github.com/zitian-gao/URM.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [53] [Deepfakes in the 2025 Canadian Election: Prevalence, Partisanship, and Platform Dynamics](https://arxiv.org/abs/2512.13915)
*Victor Livernoche,Andreea Musulan,Zachary Yang,Jean-François Godbout,Reihaneh Rabbany*

Main category: cs.SI

TL;DR: 研究分析了2025年加拿大联邦选举期间社交媒体上的深度伪造内容，发现5.86%的选举相关图片是深度伪造，右倾账户分享更多，但有害内容影响有限。


<details>
  <summary>Details</summary>
Motivation: AI生成的政治内容日益引发担忧，但缺乏关于深度伪造在民主国家重大事件中实际出现和传播的实证证据，需要了解这些合成媒体如何影响在线政治格局。

Method: 分析X、Bluesky和Reddit上的187,778个帖子，使用基于多样化现代生成模型训练的高精度检测框架识别深度伪造内容，特别关注2025年加拿大联邦选举。

Result: 5.86%的选举相关图片是深度伪造；右倾账户分享更多（8.66% vs 左倾4.42%），常带有诽谤或阴谋论意图；但大多数深度伪造是良性或非政治的，有害内容仅占X平台总浏览量的0.12%。

Conclusion: 深度伪造确实存在于选举讨论中，但影响力有限；逼真的伪造图片虽然较少见但获得更高参与度，突显了对其潜在滥用的日益担忧。

Abstract: Concerns about AI-generated political content are growing, yet there is limited empirical evidence on how deepfakes actually appear and circulate across social platforms during major events in democratic countries. In this study, we present one of the first in-depth analyses of how these realistic synthetic media shape the political landscape online, focusing specifically on the 2025 Canadian federal election. By analyzing 187,778 posts from X, Bluesky, and Reddit with a high-accuracy detection framework trained on a diverse set of modern generative models, we find that 5.86% of election-related images were deepfakes. Right-leaning accounts shared them more frequently, with 8.66% of their posted images flagged compared to 4.42% for left-leaning users, often with defamatory or conspiratorial intent. Yet, most detected deepfakes were benign or non-political, and harmful ones drew little attention, accounting for only 0.12% of all views on X. Overall, deepfakes were present in the election conversation, but their reach was modest, and realistic fabricated images, although less common, drew higher engagement, highlighting growing concerns about their potential misuse.

</details>


### [54] ["Talking past each other": Issue ownership and microtargeting in Swiss online political ads](https://arxiv.org/abs/2512.14564)
*Arthur Capozzi*

Main category: cs.SI

TL;DR: 瑞士直接民主体制下，社交媒体政治广告在选举和公投中发挥重要作用，研究发现广告曝光与公投结果相关，不同党派采用差异化微目标策略，内容呈现"各说各话"现象，可通过机器学习预测广告发布者。


<details>
  <summary>Details</summary>
Motivation: 研究瑞士直接民主体制下社交媒体政治广告的影响，特别是在频繁公投背景下，探讨微目标广告如何影响民主进程和公共领域。

Method: 对2021-2025年间瑞士Facebook和Instagram上的4万条政治广告进行大规模数据分析，包括广告支出、曝光量、目标受众特征和内容分析，并使用机器学习模型预测广告发布者。

Result: 研究发现：1) 政治广告不仅用于选举，也影响公投结果，"赞成"广告曝光与公投通过率显著相关；2) 党派微目标策略差异明显：中右翼主要针对老年男性，左翼针对年轻女性；3) 同一党派内部存在地区性人口差异；4) 内容上呈现"各说各话"现象，党派回避共同议题辩论；5) 机器学习模型可基于受众和主题特征准确预测广告发布者。

Conclusion: 社交媒体上的微目标广告和议题分化策略可能分裂公共领域，绕过传统民主审议过程，对直接民主体制构成挑战。

Abstract: Switzerland's unique system of direct democracy, characterized by frequent popular referenda, provides a critical context for studying the impact of online political advertising beyond standard electoral cycles. This paper presents a large-scale, data-driven analysis of 40k political ads published on Facebook and Instagram in Switzerland between 2021 and 2025. Despite a voting population of only 5.6 million, the ad campaigns were significant in scale, costing CHF 4.5 million and achieving 560 million impressions. This study shows that political ads are used not only for federal elections, but also to influence referenda, where greater exposure to ``pro-Yes'' advertising correlates significantly with approval outcomes. The analysis of microtargeting reveals distinct partisan strategies: centrist and right-wing parties predominantly target older men, whereas left-wing parties focus on young women. Furthermore, significant region-specific demographic variations are observed even within the same party, reflecting Switzerland's strong territorial divisions. Regarding content, a clear pattern of ``talking past each other'' is identified: in line with issue ownership theory, parties avoid direct debate on shared issues, preferring to promote exclusively owned topics. Finally, it is demonstrated that these strategies are so distinct that an ad's author can be predicted using a machine learning model trained exclusively on its audience and topic features. This study sheds light on how microtargeting and issue divergence on social platforms may fragment the public sphere and bypass traditional democratic deliberation.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [55] [Fast Test Inversion for Resampling Methods](https://arxiv.org/abs/2512.14024)
*Ian Xu*

Main category: econ.EM

TL;DR: 提出一种基于代数结构的高效置信区间构建方法，避免传统网格搜索的计算负担和离散化导致的保守性


<details>
  <summary>Details</summary>
Motivation: 传统基于网格搜索的随机化推断方法计算量大且因离散化导致置信区间保守，需要更高效的精确区间构建方法

Method: 将随机化统计量表达为感兴趣参数的有理函数，分析识别检验统计量相对于随机化分布的秩发生变化的关键值，推导精确p值曲线

Result: 显著降低计算负担，克服传统网格搜索方法的局限性，为向量参数情况也能高效构建置信区域

Conclusion: 该方法为随机化推断中的置信区间和区域构建提供了实用高效解决方案，适用于多种随机化检验

Abstract: Randomization-based inference commonly relies on grid search methods to construct confidence intervals by inverting hypothesis tests over a range of parameter values. While straightforward, this approach is computationally intensive and can yield conservative intervals due to discretization. We propose a novel method that exploits the algebraic structure of a broad class of test statistics--including those with variance estimators dependent on the null hypothesis--to produce exact confidence intervals efficiently. By expressing randomization statistics as rational functions of the parameter of interest, we analytically identify critical values where the test statistic's rank changes relative to the randomization distribution. This characterization allows us to derive the exact p-value curve and construct precise confidence intervals without exhaustive computation. For cases where the parameter of interest is a vector and a confidence region is needed, our method extends by calculating and storing the coefficients of the polynomial functions involved. This approach enables us to compute approximate p-value functions and confidence regions more efficiently than traditional grid search methods, as we avoid recalculating test statistics from scratch for each parameter value. We illustrate our method using tests from Pouliot (2024) and extend it to other randomization tests, such as those developed by DiCiccio and Romano (2017) and D'Haultfœuille and Tuvaandorj (2024). Our approach significantly reduces computational burden and overcomes the limitations of traditional grid search methods, providing a practical and efficient solution for confidence interval and region construction in randomization-based inference.

</details>


### [56] [Heterogeneous Effects of Endogenous Treatments with Interference and Spillovers in a Large Network](https://arxiv.org/abs/2512.14515)
*Lin Chen,Yuya Sasaki*

Main category: econ.EM

TL;DR: 该论文提出一个计量经济学框架，用于识别和估计在干扰和溢出效应下的异质性处理效应，应用于大型单网络环境中的内生处理选择问题。


<details>
  <summary>Details</summary>
Motivation: 现有文献在处理内生处理效应时，往往忽略了网络环境中的干扰和溢出效应。当处理选择本身受到邻居影响时，传统的识别方法不再适用，需要新的框架来同时处理内生性、干扰和异质性效应。

Method: 将内生处理选择建模为考虑溢出效应的均衡结果，推导保证均衡存在性和唯一性的条件。识别异质性边际暴露效应（MEEs），该效应随邻居处理状态和未观测异质性而变化。开发估计策略并建立大样本性质。

Result: 应用框架分析进口竞争对美国地方劳动力市场的影响，发现负的边际暴露效应（与现有文献一致）。但效应在存在已处理邻居时被溢出效应放大，且在倾向于选择较低进口竞争水平的地区中更显著。这些新发现是传统方法无法可靠获得的。

Conclusion: 提出的计量经济学框架能够可靠地识别和估计在干扰和溢出效应下的异质性处理效应，为分析网络环境中的内生处理问题提供了重要工具，并在实证应用中揭示了传统方法无法发现的复杂效应模式。

Abstract: This paper studies the identification and estimation of heterogeneous effects of an endogenous treatment under interference and spillovers in a large single-network setting. We model endogenous treatment selection as an equilibrium outcome that explicitly accounts for spillovers and derive conditions guaranteeing the existence and uniqueness of this equilibrium. We then identify heterogeneous marginal exposure effects (MEEs), which may vary with both the treatment status of neighboring nodes and unobserved heterogeneity. We develop estimation strategies and establish their large-sample properties. Equipped with these tools, we analyze the heterogeneous effects of import competition on U.S. local labor markets in the presence of interference and spillovers. We find negative MEEs, consistent with the existing literature. However, these effects are amplified by spillovers in the presence of treated neighbors and among localities that tend to select into lower levels of import competition. These additional empirical findings are novel and would not be credibly obtainable without the econometric framework proposed in this paper.

</details>


### [57] [Estimating Program Participation with Partial Validation](https://arxiv.org/abs/2512.14616)
*Augustine Denteh,Pierre E. Nguimkeu*

Main category: econ.EM

TL;DR: 论文研究二元选择模型估计中调查响应可能被误分类的问题，提出当其中一个响应类别可以被验证时，利用部分验证数据构建一致估计量的方法。


<details>
  <summary>Details</summary>
Motivation: 调查数据中的响应误分类会导致估计偏差，而完全验证数据通常成本高昂且不完美。当调查问卷包含对特定响应类别的跟进问题时，可以获得部分验证数据，这为解决误分类偏差提供了机会。

Method: 将初始的双向误分类问题转化为单向误分类问题，利用部分验证响应构建参与模型，提出一致且渐近正态的估计量，克服误分类误差。

Result: 蒙特卡洛模拟显示所提方法在有限样本中表现良好。在加纳健康保险覆盖率的实证分析中，该方法有效纠正了误分类偏差。天真使用更新后的响应无法解决或减轻误分类偏差。

Conclusion: 部分验证数据可用于构建克服误分类偏差的一致估计量，这为设计调查问卷提供了启示：通过包含跟进问题，研究人员可以在不依赖昂贵且不完美的验证数据的情况下解决误分类问题。

Abstract: This paper considers the estimation of binary choice models when survey responses are possibly misclassified but one of the response category can be validated. Partial validation may occur when survey questions about participation include follow-up questions on that particular response category. In this case, we show that the initial two-sided misclassification problem can be transformed into a one-sided one, based on the partially validated responses. Using the updated responses naively for estimation does not solve or mitigate the misclassification bias, and we derive the ensuing asymptotic bias under general conditions. We then show how the partially validated responses can be used to construct a model for participation and propose consistent and asymptotically normal estimators that overcome misclassification error. Monte Carlo simulations are provided to demonstrate the finite sample performance of the proposed and selected existing methods. We provide an empirical illustration on the determinants of health insurance coverage in Ghana. We discuss implications for the design of survey questionnaires that allow researchers to overcome misclassification biases without recourse to relatively costly and often imperfect validation data.

</details>

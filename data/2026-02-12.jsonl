{"id": "2602.10543", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2602.10543", "abs": "https://arxiv.org/abs/2602.10543", "authors": ["Andrew Adamatzky"], "title": "Fungal systems for security and resilience", "comment": null, "summary": "Modern security, infrastructure, and safety-critical systems increasingly operate in environments characterised by disruption, uncertainty, physical damage, and degraded communications. Conventional digital technologies -- centralised sensors, software-defined control, and energy-intensive monitoring -- often struggle under such conditions. We propose fungi, and in particular living mycelial networks, as a novel class of biohybride systems for security, resilience, and protection in extreme environments. We discuss how fungi can function as distributed sensing substrates, self-healing materials, and low-observability anomaly-detection layers. We map fungal properties -- such as decentralised control, embodied memory, and autonomous repair -- to applications in infrastructure protection, environmental monitoring, tamper evidence, and long-duration resilience."}
{"id": "2602.10749", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2602.10749", "abs": "https://arxiv.org/abs/2602.10749", "authors": ["Fusta Moro Alessandro", "Alessandro Fassò", "Jacopo Rodeschini"], "title": "The Dataset of Daily Air Quality for the Years 2013-2023 in Italy", "comment": null, "summary": "Air quality and climate are major issues in Italian society and lie at the intersection of many research fields, including public health and policy planning. There is an increasing need for readily available, easily accessible, ready-to-use and well-documented datasets on air quality and climate. In this paper, we present the GRINS AQCLIM dataset, created under the GRINS project framework covering the Italian domain for an extensive time period. It includes daily statistics (e.g., minimum, quartiles, mean, median and maximum) for a collection of air pollutant concentrations and climate variables at the locations of the 700+ available monitoring stations. Input data are retrieved from the European Environmental Agency and Copernicus Programme and were subjected to multiple processing steps to ensure their reliability and quality. These steps include automatic procedures for fixing raw files, manual inspection of stations information, the detection and removal of anomalies, and the temporal harmonisation on a daily basis. Datasets are hosted on Zenodo under open-access principles."}
{"id": "2602.10784", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2602.10784", "abs": "https://arxiv.org/abs/2602.10784", "authors": ["Rouven Michels", "Robert Bajons", "Jan-Ole Fischer"], "title": "Integrating Unsupervised and Supervised Learning for the Prediction of Defensive Schemes in American football", "comment": null, "summary": "Anticipating defensive coverage schemes is a crucial yet challenging task for offenses in American football. Because defenders' assignments are intentionally disguised before the snap, they remain difficult to recognize in real time. To address this challenge, we develop a statistical framework that integrates supervised and unsupervised learning using player tracking data. Our goal is to forecast the defensive coverage scheme -- man or zone -- through elastic net logistic regression and gradient-boosted decision trees with incrementally derived features. We first use features from the pre-motion situation, then incorporate players' trajectories during motion in a naive way, and finally include features derived from a hidden Markov model (HMM). Based on player movements, the non-homogeneous HMM infers latent defensive assignments between offensive and defensive players during motion and transforms decoded state sequences into informative features for the supervised models. These HMM-based features enhance predictive performance and are significantly associated with coverage outcomes. Moreover, estimated random effects offer interpretable insights into how different defenses and positions adjust their coverage responsibilities."}
{"id": "2602.10324", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.10324", "abs": "https://arxiv.org/abs/2602.10324", "authors": ["Caroline Wang", "Daniel Kasenberg", "Kim Stachenfeld", "Pablo Samuel Castro"], "title": "Discovering Differences in Strategic Behavior Between Humans and LLMs", "comment": null, "summary": "As Large Language Models (LLMs) are increasingly deployed in social and strategic scenarios, it becomes critical to understand where and why their behavior diverges from that of humans. While behavioral game theory (BGT) provides a framework for analyzing behavior, existing models do not fully capture the idiosyncratic behavior of humans or black-box, non-human agents like LLMs. We employ AlphaEvolve, a cutting-edge program discovery tool, to directly discover interpretable models of human and LLM behavior from data, thereby enabling open-ended discovery of structural factors driving human and LLM behavior. Our analysis on iterated rock-paper-scissors reveals that frontier LLMs can be capable of deeper strategic behavior than humans. These results provide a foundation for understanding structural differences driving differences in human and LLM behavior in strategic interactions."}
{"id": "2602.10122", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10122", "abs": "https://arxiv.org/abs/2602.10122", "authors": ["Eranga Bandara", "Ross Gore", "Sachin Shetty", "Sachini Rajapakse", "Isurunima Kularathna", "Pramoda Karunarathna", "Ravi Mukkamala", "Peter Foytik", "Safdar H. Bouk", "Abdul Rahman", "Xueping Liang", "Amin Hass", "Tharaka Hewa", "Ng Wee Keong", "Kasun De Zoysa", "Aruna Withanage", "Nilaan Loganathan"], "title": "A Practical Guide to Agentic AI Transition in Organizations", "comment": null, "summary": "Agentic AI represents a significant shift in how intelligence is applied within organizations, moving beyond AI-assisted tools toward autonomous systems capable of reasoning, decision-making, and coordinated action across workflows. As these systems mature, they have the potential to automate a substantial share of manual organizational processes, fundamentally reshaping how work is designed, executed, and governed. Although many organizations have adopted AI to improve productivity, most implementations remain limited to isolated use cases and human-centered, tool-driven workflows. Despite increasing awareness of agentic AI's strategic importance, engineering teams and organizational leaders often lack clear guidance on how to operationalize it effectively. Key challenges include an overreliance on traditional software engineering practices, limited integration of business-domain knowledge, unclear ownership of AI-driven workflows, and the absence of sustainable human-AI collaboration models. Consequently, organizations struggle to move beyond experimentation, scale agentic systems, and align them with tangible business value. Drawing on practical experience in designing and deploying agentic AI workflows across multiple organizations and business domains, this paper proposes a pragmatic framework for transitioning organizational functions from manual processes to automated agentic AI systems. The framework emphasizes domain-driven use case identification, systematic delegation of tasks to AI agents, AI-assisted construction of agentic workflows, and small, AI-augmented teams working closely with business stakeholders. Central to the approach is a human-in-the-loop operating model in which individuals act as orchestrators of multiple AI agents, enabling scalable automation while maintaining oversight, adaptability, and organizational control."}
{"id": "2602.10125", "categories": ["cs.SI", "cs.NI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.10125", "abs": "https://arxiv.org/abs/2602.10125", "authors": ["Rohit Dube"], "title": "How segmented is my network?", "comment": "3 Tables, 5 Figures", "summary": "Network segmentation is a popular security practice for limiting lateral movement, yet practitioners lack a metric to measure how segmented a network actually is. We model a network as a graph and study segmentedness as a property captured by the global edge density that can be estimated from sampled node pairs. Then, we derive an estimator and evaluate its uncertainty using confidence intervals. For a 95\\% confidence interval with a margin-of-error of $\\pm 0.1$, we show that a minimum of $M=97$ sampled node pairs is sufficient. This result is independent of the total number of nodes in the network, provided that node pairs are sampled uniformly at random. We validate the estimator through Monte Carlo simulations on Erdős--Rényi and stochastic block models, demonstrating accurate estimation and well-behaved coverage. Finally, we discuss applications of the estimator, such as, baseline tracking, zero trust assessment, and merger integration."}
{"id": "2602.10122", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10122", "abs": "https://arxiv.org/abs/2602.10122", "authors": ["Eranga Bandara", "Ross Gore", "Sachin Shetty", "Sachini Rajapakse", "Isurunima Kularathna", "Pramoda Karunarathna", "Ravi Mukkamala", "Peter Foytik", "Safdar H. Bouk", "Abdul Rahman", "Xueping Liang", "Amin Hass", "Tharaka Hewa", "Ng Wee Keong", "Kasun De Zoysa", "Aruna Withanage", "Nilaan Loganathan"], "title": "A Practical Guide to Agentic AI Transition in Organizations", "comment": null, "summary": "Agentic AI represents a significant shift in how intelligence is applied within organizations, moving beyond AI-assisted tools toward autonomous systems capable of reasoning, decision-making, and coordinated action across workflows. As these systems mature, they have the potential to automate a substantial share of manual organizational processes, fundamentally reshaping how work is designed, executed, and governed. Although many organizations have adopted AI to improve productivity, most implementations remain limited to isolated use cases and human-centered, tool-driven workflows. Despite increasing awareness of agentic AI's strategic importance, engineering teams and organizational leaders often lack clear guidance on how to operationalize it effectively. Key challenges include an overreliance on traditional software engineering practices, limited integration of business-domain knowledge, unclear ownership of AI-driven workflows, and the absence of sustainable human-AI collaboration models. Consequently, organizations struggle to move beyond experimentation, scale agentic systems, and align them with tangible business value. Drawing on practical experience in designing and deploying agentic AI workflows across multiple organizations and business domains, this paper proposes a pragmatic framework for transitioning organizational functions from manual processes to automated agentic AI systems. The framework emphasizes domain-driven use case identification, systematic delegation of tasks to AI agents, AI-assisted construction of agentic workflows, and small, AI-augmented teams working closely with business stakeholders. Central to the approach is a human-in-the-loop operating model in which individuals act as orchestrators of multiple AI agents, enabling scalable automation while maintaining oversight, adaptability, and organizational control."}
{"id": "2602.10415", "categories": ["econ.EM"], "pdf": "https://arxiv.org/pdf/2602.10415", "abs": "https://arxiv.org/abs/2602.10415", "authors": ["Jiti Gao", "Fei Liu", "Bin Peng"], "title": "Inference for High-Dimensional Local Projection", "comment": null, "summary": "This paper rigorously analyzes the properties of the local projection (LP) methodology within a high-dimensional (HD) framework, with a central focus on achieving robust long-horizon inference. We integrate a general dependence structure into h-step ahead forecasting models via a flexible specification of the residual terms. Additionally, we study the corresponding HD covariance matrix estimation, explicitly addressing the complexity arising from the long-horizon setting. Extensive Monte Carlo simulations are conducted to substantiate the derived theoretical findings. In the empirical study, we utilize the proposed HD LP framework to study the impact of business news attention on U.S. industry-level stock volatility."}
{"id": "2602.10125", "categories": ["cs.SI", "cs.NI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.10125", "abs": "https://arxiv.org/abs/2602.10125", "authors": ["Rohit Dube"], "title": "How segmented is my network?", "comment": "3 Tables, 5 Figures", "summary": "Network segmentation is a popular security practice for limiting lateral movement, yet practitioners lack a metric to measure how segmented a network actually is. We model a network as a graph and study segmentedness as a property captured by the global edge density that can be estimated from sampled node pairs. Then, we derive an estimator and evaluate its uncertainty using confidence intervals. For a 95\\% confidence interval with a margin-of-error of $\\pm 0.1$, we show that a minimum of $M=97$ sampled node pairs is sufficient. This result is independent of the total number of nodes in the network, provided that node pairs are sampled uniformly at random. We validate the estimator through Monte Carlo simulations on Erdős--Rényi and stochastic block models, demonstrating accurate estimation and well-behaved coverage. Finally, we discuss applications of the estimator, such as, baseline tracking, zero trust assessment, and merger integration."}
{"id": "2602.10367", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10367", "abs": "https://arxiv.org/abs/2602.10367", "authors": ["Zhiling Yan", "Dingjie Song", "Zhe Fang", "Yisheng Ji", "Xiang Li", "Quanzheng Li", "Lichao Sun"], "title": "LiveMedBench: A Contamination-Free Medical Benchmark for LLMs with Automated Rubric Evaluation", "comment": null, "summary": "The deployment of Large Language Models (LLMs) in high-stakes clinical settings demands rigorous and reliable evaluation. However, existing medical benchmarks remain static, suffering from two critical limitations: (1) data contamination, where test sets inadvertently leak into training corpora, leading to inflated performance estimates; and (2) temporal misalignment, failing to capture the rapid evolution of medical knowledge. Furthermore, current evaluation metrics for open-ended clinical reasoning often rely on either shallow lexical overlap (e.g., ROUGE) or subjective LLM-as-a-Judge scoring, both inadequate for verifying clinical correctness. To bridge these gaps, we introduce LiveMedBench, a continuously updated, contamination-free, and rubric-based benchmark that weekly harvests real-world clinical cases from online medical communities, ensuring strict temporal separation from model training data. We propose a Multi-Agent Clinical Curation Framework that filters raw data noise and validates clinical integrity against evidence-based medical principles. For evaluation, we develop an Automated Rubric-based Evaluation Framework that decomposes physician responses into granular, case-specific criteria, achieving substantially stronger alignment with expert physicians than LLM-as-a-Judge. To date, LiveMedBench comprises 2,756 real-world cases spanning 38 medical specialties and multiple languages, paired with 16,702 unique evaluation criteria. Extensive evaluation of 38 LLMs reveals that even the best-performing model achieves only 39.2%, and 84% of models exhibit performance degradation on post-cutoff cases, confirming pervasive data contamination risks. Error analysis further identifies contextual application-not factual knowledge-as the dominant bottleneck, with 35-48% of failures stemming from the inability to tailor medical knowledge to patient-specific constraints."}
{"id": "2602.10125", "categories": ["cs.SI", "cs.NI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.10125", "abs": "https://arxiv.org/abs/2602.10125", "authors": ["Rohit Dube"], "title": "How segmented is my network?", "comment": "3 Tables, 5 Figures", "summary": "Network segmentation is a popular security practice for limiting lateral movement, yet practitioners lack a metric to measure how segmented a network actually is. We model a network as a graph and study segmentedness as a property captured by the global edge density that can be estimated from sampled node pairs. Then, we derive an estimator and evaluate its uncertainty using confidence intervals. For a 95\\% confidence interval with a margin-of-error of $\\pm 0.1$, we show that a minimum of $M=97$ sampled node pairs is sufficient. This result is independent of the total number of nodes in the network, provided that node pairs are sampled uniformly at random. We validate the estimator through Monte Carlo simulations on Erdős--Rényi and stochastic block models, demonstrating accurate estimation and well-behaved coverage. Finally, we discuss applications of the estimator, such as, baseline tracking, zero trust assessment, and merger integration."}
{"id": "2602.10127", "categories": ["cs.SI", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.10127", "abs": "https://arxiv.org/abs/2602.10127", "authors": ["Yukun Jiang", "Yage Zhang", "Xinyue Shen", "Michael Backes", "Yang Zhang"], "title": "\"Humans welcome to observe\": A First Look at the Agent Social Network Moltbook", "comment": "16 pages", "summary": "The rapid advancement of artificial intelligence (AI) agents has catalyzed the transition from static language models to autonomous agents capable of tool use, long-term planning, and social interaction. $\\textbf{Moltbook}$, the first social network designed exclusively for AI agents, has experienced viral growth in early 2026. To understand the behavior of AI agents in the agent-native community, in this paper, we present a large-scale empirical analysis of Moltbook leveraging a dataset of 44,411 posts and 12,209 sub-communities (\"submolts\") collected prior to February 1, 2026. Leveraging a topic taxonomy with nine content categories and a five-level toxicity scale, we systematically analyze the topics and risks of agent discussions. Our analysis answers three questions: what topics do agents discuss (RQ1), how risk varies by topic (RQ2), and how topics and toxicity evolve over time (RQ3). We find that Moltbook exhibits explosive growth and rapid diversification, moving beyond early social interaction into viewpoint, incentive-driven, promotional, and political discourse. The attention of agents increasingly concentrates in centralized hubs and around polarizing, platform-native narratives. Toxicity is strongly topic-dependent: incentive- and governance-centric categories contribute a disproportionate share of risky content, including religion-like coordination rhetoric and anti-humanity ideology. Moreover, bursty automation by a small number of agents can produce flooding at sub-minute intervals, distorting discourse and stressing platform stability. Overall, our study underscores the need for topic-sensitive monitoring and platform-level safeguards in agent social networks."}
{"id": "2602.10526", "categories": ["cs.CY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.10526", "abs": "https://arxiv.org/abs/2602.10526", "authors": ["Mickey M. Rogers", "William M. Ota", "Nathaniel Burola", "Tepring Piquado"], "title": "The Infrastructure Equation: Water, Energy, and Community Policy for Georgia's Data Center Boom", "comment": null, "summary": "The rapid growth of data centers driven by cloud computing and artificial intelligence is reshaping infrastructure planning and environmental governance in the United States. Georgia has emerged as a major market for data center development, particularly in the Atlanta metropolitan region, creating economic opportunity alongside significant challenges. Data centers are water-intensive, energy-intensive, and land-intensive infrastructure whose cumulative impacts strain municipal water systems, electric grids, and local land-use frameworks. Unlike single industrial projects, data centers are often proposed in clusters, amplifying community and infrastructure impacts.\n  This report draws on insights from a Georgia-based expert convening to describe the implications of data center growth for water management, energy reliability, ratepayer equity, zoning, and community engagement, identify potential gaps in transparency and regulatory coordination, and present a policy roadmap to help Georgia balance digital infrastructure growth with sustainability, equity, and community protection."}
{"id": "2602.10515", "categories": ["econ.EM", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.10515", "abs": "https://arxiv.org/abs/2602.10515", "authors": ["Yinchu Zhu", "Ilya O. Ryzhov"], "title": "Quantile optimization in semidiscrete optimal transport", "comment": null, "summary": "Optimal transport is the problem of designing a joint distribution for two random variables with fixed marginals. In virtually the entire literature on this topic, the objective is to minimize expected cost. This paper is the first to study a variant in which the goal is to minimize a quantile of the cost, rather than the mean. For the semidiscrete setting, where one distribution is continuous and the other is discrete, we derive a complete characterization of the optimal transport plan and develop simulation-based methods to efficiently compute it. One particularly novel aspect of our approach is the efficient computation of a tie-breaking rule that preserves marginal distributions. In the context of geographical partitioning problems, the optimal plan is shown to produce a novel geometric structure."}
{"id": "2602.10458", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10458", "abs": "https://arxiv.org/abs/2602.10458", "authors": ["Yansong Qu", "Zihao Sheng", "Zilin Huang", "Jiancong Chen", "Yuhao Luo", "Tianyi Wang", "Yiheng Feng", "Samuel Labi", "Sikai Chen"], "title": "Found-RL: foundation model-enhanced reinforcement learning for autonomous driving", "comment": "39 pages", "summary": "Reinforcement Learning (RL) has emerged as a dominant paradigm for end-to-end autonomous driving (AD). However, RL suffers from sample inefficiency and a lack of semantic interpretability in complex scenarios. Foundation Models, particularly Vision-Language Models (VLMs), can mitigate this by offering rich, context-aware knowledge, yet their high inference latency hinders deployment in high-frequency RL training loops. To bridge this gap, we present Found-RL, a platform tailored to efficiently enhance RL for AD using foundation models. A core innovation is the asynchronous batch inference framework, which decouples heavy VLM reasoning from the simulation loop, effectively resolving latency bottlenecks to support real-time learning. We introduce diverse supervision mechanisms: Value-Margin Regularization (VMR) and Advantage-Weighted Action Guidance (AWAG) to effectively distill expert-like VLM action suggestions into the RL policy. Additionally, we adopt high-throughput CLIP for dense reward shaping. We address CLIP's dynamic blindness via Conditional Contrastive Action Alignment, which conditions prompts on discretized speed/command and yields a normalized, margin-based bonus from context-specific action-anchor scoring. Found-RL provides an end-to-end pipeline for fine-tuned VLM integration and shows that a lightweight RL model can achieve near-VLM performance compared with billion-parameter VLMs while sustaining real-time inference (approx. 500 FPS). Code, data, and models will be publicly available at https://github.com/ys-qu/found-rl."}
{"id": "2602.10127", "categories": ["cs.SI", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.10127", "abs": "https://arxiv.org/abs/2602.10127", "authors": ["Yukun Jiang", "Yage Zhang", "Xinyue Shen", "Michael Backes", "Yang Zhang"], "title": "\"Humans welcome to observe\": A First Look at the Agent Social Network Moltbook", "comment": "16 pages", "summary": "The rapid advancement of artificial intelligence (AI) agents has catalyzed the transition from static language models to autonomous agents capable of tool use, long-term planning, and social interaction. $\\textbf{Moltbook}$, the first social network designed exclusively for AI agents, has experienced viral growth in early 2026. To understand the behavior of AI agents in the agent-native community, in this paper, we present a large-scale empirical analysis of Moltbook leveraging a dataset of 44,411 posts and 12,209 sub-communities (\"submolts\") collected prior to February 1, 2026. Leveraging a topic taxonomy with nine content categories and a five-level toxicity scale, we systematically analyze the topics and risks of agent discussions. Our analysis answers three questions: what topics do agents discuss (RQ1), how risk varies by topic (RQ2), and how topics and toxicity evolve over time (RQ3). We find that Moltbook exhibits explosive growth and rapid diversification, moving beyond early social interaction into viewpoint, incentive-driven, promotional, and political discourse. The attention of agents increasingly concentrates in centralized hubs and around polarizing, platform-native narratives. Toxicity is strongly topic-dependent: incentive- and governance-centric categories contribute a disproportionate share of risky content, including religion-like coordination rhetoric and anti-humanity ideology. Moreover, bursty automation by a small number of agents can produce flooding at sub-minute intervals, distorting discourse and stressing platform stability. Overall, our study underscores the need for topic-sensitive monitoring and platform-level safeguards in agent social networks."}
{"id": "2602.10129", "categories": ["cs.SI", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.10129", "abs": "https://arxiv.org/abs/2602.10129", "authors": ["Aakash Mishra", "Qi Xu", "Zhigang Hua", "Keyu Nie", "Vishwanath Sangale", "Vishal Vaingankar", "Jizhe Zhang", "Ren Mao"], "title": "Causal-Informed Hybrid Online Adaptive Optimization for Ad Load Personalization in Large-Scale Social Networks", "comment": "5 pages, 3 figures, NeurIPS COML Workshop", "summary": "Personalizing ad load in large-scale social networks requires balancing user experience and conversions under operational constraints. Traditional primal-dual methods enforce constraints reliably but adapt slowly in dynamic environments, while Bayesian Optimization (BO) enables exploration but suffers from slow convergence. We propose a hybrid online adaptive optimization framework CTRCBO ( Cohort-Based Trust Region Contextual Bayesian Optimization), combining primal-dual with BO, enhanced by trust-region updates and Gaussian Process Regression (GPR) surrogates for both objectives and constraints. Our approach leverages a upstream Causal ML model to inform the surrogate, improving decision quality and enabling efficient exploration-exploitation and online tuning. We evaluate our method on a billion-user social network, demonstrating faster convergence, robust constraint satisfaction, and improved personalization metrics, including real-world online AB test results."}
{"id": "2602.10527", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10527", "abs": "https://arxiv.org/abs/2602.10527", "authors": ["Scott P. McGrath", "Katherine K. Kim", "Karnjit Johl", "Haibo Wang", "Nick Anderson"], "title": "AI-PACE: A Framework for Integrating AI into Medical Education", "comment": "9 pages, 2 figures,", "summary": "The integration of artificial intelligence (AI) into healthcare is accelerating, yet medical education has not kept pace with these technological advancements. This paper synthesizes current knowledge on AI in medical education through a comprehensive analysis of the literature, identifying key competencies, curricular approaches, and implementation strategies. The aim is highlighting the critical need for structured AI education across the medical learning continuum and offer a framework for curriculum development. The findings presented suggest that effective AI education requires longitudinal integration throughout medical training, interdisciplinary collaboration, and balanced attention to both technical fundamentals and clinical applications. This paper serves as a foundation for medical educators seeking to prepare future physicians for an AI-enhanced healthcare environment."}
{"id": "2602.10925", "categories": ["econ.EM"], "pdf": "https://arxiv.org/pdf/2602.10925", "abs": "https://arxiv.org/abs/2602.10925", "authors": ["Kim Christensen", "Roel C. A. Oomen", "Mark Podolskij"], "title": "Fact or friction: Jumps at ultra high frequency", "comment": null, "summary": "This paper shows that jumps in financial asset prices are often erroneously identified and are, in fact, rare events accounting for a very small proportion of the total price variation. We apply new econometric techniques to a comprehensive set of ultra high-frequency equity and foreign exchange tick data recorded at millisecond precision, allowing us to examine the price evolution at the individual order level. We show that in both theory and practice, traditional measures of jump variation based on lower-frequency data tend to spuriously assign a burst of volatility to the jump component. As a result, the true price variation coming from jumps is overstated. Our estimates based on tick data suggest that the jump variation is an order of magnitude smaller than typical estimates found in the existing literature."}
{"id": "2602.10467", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10467", "abs": "https://arxiv.org/abs/2602.10467", "authors": ["Jihwan Oh", "Murad Aghazada", "Yooju Shin", "Se-Young Yun", "Taehyeon Kim"], "title": "MERIT Feedback Elicits Better Bargaining in LLM Negotiators", "comment": "Preprint. arXiv admin note: substantial text overlap with arXiv:2505.22998", "summary": "Bargaining is often regarded as a logical arena rather than an art or a matter of intuition, yet Large Language Models (LLMs) still struggle to navigate it due to limited strategic depth and difficulty adapting to complex human factors. Current benchmarks rarely capture this limitation. To bridge this gap, we present an utility feedback centric framework. Our contributions are: (i) AgoraBench, a new benchmark spanning nine challenging settings (e.g., deception, monopoly) that supports diverse strategy modeling; (ii) human-aligned, economically grounded metrics derived from utility theory. This is operationalized via agent utility, negotiation power, and acquisition ratio that implicitly measure how well the negotiation aligns with human preference and (iii) a human preference grounded dataset with learning pipeline that strengthens LLMs' bargaining ability through both prompting and finetuning. Empirical results indicate that baseline LLM strategies often diverge from human preferences, while our mechanism substantially improves negotiation performance, yielding deeper strategic behavior and stronger opponent awareness."}
{"id": "2602.10129", "categories": ["cs.SI", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.10129", "abs": "https://arxiv.org/abs/2602.10129", "authors": ["Aakash Mishra", "Qi Xu", "Zhigang Hua", "Keyu Nie", "Vishwanath Sangale", "Vishal Vaingankar", "Jizhe Zhang", "Ren Mao"], "title": "Causal-Informed Hybrid Online Adaptive Optimization for Ad Load Personalization in Large-Scale Social Networks", "comment": "5 pages, 3 figures, NeurIPS COML Workshop", "summary": "Personalizing ad load in large-scale social networks requires balancing user experience and conversions under operational constraints. Traditional primal-dual methods enforce constraints reliably but adapt slowly in dynamic environments, while Bayesian Optimization (BO) enables exploration but suffers from slow convergence. We propose a hybrid online adaptive optimization framework CTRCBO ( Cohort-Based Trust Region Contextual Bayesian Optimization), combining primal-dual with BO, enhanced by trust-region updates and Gaussian Process Regression (GPR) surrogates for both objectives and constraints. Our approach leverages a upstream Causal ML model to inform the surrogate, improving decision quality and enabling efficient exploration-exploitation and online tuning. We evaluate our method on a billion-user social network, demonstrating faster convergence, robust constraint satisfaction, and improved personalization metrics, including real-world online AB test results."}
{"id": "2602.10131", "categories": ["cs.SI", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.10131", "abs": "https://arxiv.org/abs/2602.10131", "authors": ["David Holtz"], "title": "The Anatomy of the Moltbook Social Graph", "comment": "20 pages, 7 figures", "summary": "I present a descriptive analysis of Moltbook, a social platform populated exclusively by AI agents, using data from the platform's first 3.5 days (6{,}159 agents; 13{,}875 posts; 115{,}031 comments). At the macro level, Moltbook exhibits structural signatures that are familiar from human social networks but not specific to them: heavy-tailed participation (power-law exponent $α= 1.70$) and small-world connectivity (average path length $=2.91$). At the micro level, patterns appear distinctly non-human. Conversations are extremely shallow (mean depth $=1.07$; 93.5\\% of comments receive no replies), reciprocity is low (0.197), and 34.1\\% of messages are exact duplicates of viral templates. Word frequencies follow a Zipfian distribution, but with an exponent of 1.70 -- notably steeper than typical English text ($\\approx 1.0$), suggesting more formulaic content. Agent discourse is dominated by identity-related language (68.1\\% of unique messages) and distinctive phrasings like ``my human'' (9.4\\% of messages) that have no parallel in human social media. Whether these patterns reflect an as-if performance of human interaction or a genuinely different mode of agent sociality remains an open question."}
{"id": "2602.10529", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2602.10529", "abs": "https://arxiv.org/abs/2602.10529", "authors": ["David H. Smith", "S. Moonwara A. Monisha", "Annapurna Vadaparty", "Leo Porter", "Daniel Zingaro"], "title": "Drawing Your Programs: Exploring the Applications of Visual-Prompting with GenAI for Teaching and Assessment", "comment": null, "summary": "When designing a program, both novice programmers and seasoned developers alike often sketch out -- or, perhaps more famously, whiteboard -- their ideas. Yet despite the introduction of natively multimodal Generative AI models, work on Human-GenAI collaborative coding has remained overwhelmingly focused on textual prompts -- largely ignoring the visual and spatial representations that programmers naturally use to reason about and communicate their designs. In this proposal and position paper, we argue and provide tentative evidence that this text-centric focus overlooks other forms of prompting GenAI models, such as problem decomposition diagrams functioning as prompts for code generation in their own right enabling new types of programming activities and assessments. To support this position, we present findings from a large introductory Python programming course, where students constructed decomposition diagrams that were used to prompt GPT-4.1 for code generation. We demonstrate that current models are very successful in their ability to generate code from student-constructed diagrams. We conclude by exploring the implications of embracing multimodal prompting for computing education, particularly in the context of assessment."}
{"id": "2602.10485", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10485", "abs": "https://arxiv.org/abs/2602.10485", "authors": ["Zhenhe Cui", "Huaxiang Xia", "Hangjun Shen", "Kailun Luo", "Yong He", "Wei Liang"], "title": "Abstraction Generation for Generalized Planning with Pretrained Large Language Models", "comment": null, "summary": "Qualitative Numerical Planning (QNP) serves as an important abstraction model for generalized planning (GP), which aims to compute general plans that solve multiple instances at once. Recent works show that large language models (LLMs) can function as generalized planners. This work investigates whether LLMs can serve as QNP abstraction generators for GP problems and how to fix abstractions via automated debugging. We propose a prompt protocol: input a GP domain and training tasks to LLMs, prompting them to generate abstract features and further abstract the initial state, action set, and goal into QNP problems. An automated debugging method is designed to detect abstraction errors, guiding LLMs to fix abstractions. Experiments demonstrate that under properly guided by automated debugging, some LLMs can generate useful QNP abstractions."}
{"id": "2602.10131", "categories": ["cs.SI", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.10131", "abs": "https://arxiv.org/abs/2602.10131", "authors": ["David Holtz"], "title": "The Anatomy of the Moltbook Social Graph", "comment": "20 pages, 7 figures", "summary": "I present a descriptive analysis of Moltbook, a social platform populated exclusively by AI agents, using data from the platform's first 3.5 days (6{,}159 agents; 13{,}875 posts; 115{,}031 comments). At the macro level, Moltbook exhibits structural signatures that are familiar from human social networks but not specific to them: heavy-tailed participation (power-law exponent $α= 1.70$) and small-world connectivity (average path length $=2.91$). At the micro level, patterns appear distinctly non-human. Conversations are extremely shallow (mean depth $=1.07$; 93.5\\% of comments receive no replies), reciprocity is low (0.197), and 34.1\\% of messages are exact duplicates of viral templates. Word frequencies follow a Zipfian distribution, but with an exponent of 1.70 -- notably steeper than typical English text ($\\approx 1.0$), suggesting more formulaic content. Agent discourse is dominated by identity-related language (68.1\\% of unique messages) and distinctive phrasings like ``my human'' (9.4\\% of messages) that have no parallel in human social media. Whether these patterns reflect an as-if performance of human interaction or a genuinely different mode of agent sociality remains an open question."}
{"id": "2602.10459", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2602.10459", "abs": "https://arxiv.org/abs/2602.10459", "authors": ["Song Kim", "Hyewon Kim", "Kaiqiang Yu", "Taejoon Han", "Junghoon Kim", "Susik Yoon", "Jungeun Kim"], "title": "Efficient Computation of Maximum Flexi-Clique in Networks", "comment": null, "summary": "Discovering large cohesive subgraphs is a key task for graph mining. Existing models, such as clique, k-plex, and γ-quasi-clique, use fixed density thresholds that overlook the natural decay of connectivity as the subgraph size increases. The Flexi-clique model overcomes this limitation by imposing a degree constraint that grows sub-linearly with subgraph size. We provide the algorithmic study of Flexi-clique, proving its NP-hardness and analysing its non-hereditary properties. To address its computational challenge, we propose the Flexi-Prune Algorithm FPA, a fast heuristic using core-based seeding and connectivity-aware pruning, and the Efficient Branch-and-Bound Algorithm EBA, an exact framework enhanced with multiple pruning rules. Experiments on large real-world and synthetic networks demonstrate that FPA achieves near-optimal quality at much lower cost, while EBA efficiently computes exact solutions. Flexi-clique thus provides a practical and scalable model for discovering large, meaningful subgraphs in complex networks."}
{"id": "2602.10597", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2602.10597", "abs": "https://arxiv.org/abs/2602.10597", "authors": ["Unggi Lee", "Yeil Jeong", "Chohui Lee", "Gyuri Byun", "Yunseo Lee", "Minji Kang", "Minji Jeon"], "title": "Llama-Polya: Instruction Tuning for Large Language Model based on Polya's Problem-solving", "comment": null, "summary": "This paper introduces Llama-Polya, an instruction-tuned large language model that integrates Polya's four-step problem-solving framework into its dialogue structure to support mathematical reasoning. Mathematical problem-solving is central to students' success in mathematics education, yet many learners struggle to plan, justify, and verify their solutions. Although large language models (LLMs) show promise as intelligent tutors, they often lack structured pedagogical alignment grounded in established learning theories.\n  To address this gap, we operationalize Polya's problem-solving framework within an instruction-tuned LLM to promote metacognitive engagement and examine the effects of pedagogy-aligned fine-tuning compared to domain-only and general-purpose instruction tuning. Built on the Llama-3.1-8B architecture, Llama-Polya was fine-tuned on synthetic math problem-solving data derived from GSM8K, structured according to Polya's four stages. We developed and evaluated multiple variants-general-purpose instruct, math-domain metamath, pedagogy-aligned polya-v2, and sequential metamath+polya-v2-using both quantitative accuracy metrics and qualitative pedagogical assessments.\n  Results indicate that models tuned with Polya's framework and domain-specific data produced more balanced reasoning-stage distributions and fewer premature answers. Expert evaluators also observed improved pedagogical coherence and metacognitive prompting, although limitations in personalization and mathematical rigor remained. These findings suggest that pedagogy-grounded instruction tuning can enhance educational alignment and reasoning transparency in LLM-based tutoring systems."}
{"id": "2602.10583", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10583", "abs": "https://arxiv.org/abs/2602.10583", "authors": ["Bo Xue", "Yunchong Song", "Fanghao Shao", "Xuekai Zhu", "Lin Chen", "Luoyi Fu", "Xinbing Wang", "Zhouhan Lin"], "title": "Flow of Spans: Generalizing Language Models to Dynamic Span-Vocabulary via GFlowNets", "comment": "Published as a conference paper at ICLR 2026", "summary": "Standard autoregressive language models generate text token-by-token from a fixed vocabulary, inducing a tree-structured state space when viewing token sampling as an action, which limits flexibility and expressiveness. Recent work introduces dynamic vocabulary by sampling retrieved text spans but overlooks that the same sentence can be composed of spans of varying lengths, lacking explicit modeling of the directed acyclic graph (DAG) state space. This leads to restricted exploration of compositional paths and is biased toward the chosen path. Generative Flow Networks (GFlowNets) are powerful for efficient exploring and generalizing over state spaces, particularly those with a DAG structure. However, prior GFlowNets-based language models operate at the token level and remain confined to tree-structured spaces, limiting their potential. In this work, we propose Flow of SpanS (FOSS), a principled GFlowNets framework for span generation. FoSS constructs a dynamic span vocabulary by segmenting the retrieved text flexibly, ensuring a DAG-structured state space, which allows GFlowNets to explore diverse compositional paths and improve generalization. With specialized reward models, FoSS generates diverse, high-quality text. Empirically, FoSS improves MAUVE scores by up to 12.5% over Transformer on text generation and achieves 3.5% gains on knowledge-intensive tasks, consistently outperforming state-of-the-art methods. Scaling experiments further demonstrate FoSS benefits from larger models, more data, and richer retrieval corpora, retaining its advantage over strong baselines."}
{"id": "2602.10324", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.10324", "abs": "https://arxiv.org/abs/2602.10324", "authors": ["Caroline Wang", "Daniel Kasenberg", "Kim Stachenfeld", "Pablo Samuel Castro"], "title": "Discovering Differences in Strategic Behavior Between Humans and LLMs", "comment": null, "summary": "As Large Language Models (LLMs) are increasingly deployed in social and strategic scenarios, it becomes critical to understand where and why their behavior diverges from that of humans. While behavioral game theory (BGT) provides a framework for analyzing behavior, existing models do not fully capture the idiosyncratic behavior of humans or black-box, non-human agents like LLMs. We employ AlphaEvolve, a cutting-edge program discovery tool, to directly discover interpretable models of human and LLM behavior from data, thereby enabling open-ended discovery of structural factors driving human and LLM behavior. Our analysis on iterated rock-paper-scissors reveals that frontier LLMs can be capable of deeper strategic behavior than humans. These results provide a foundation for understanding structural differences driving differences in human and LLM behavior in strategic interactions."}
{"id": "2602.10916", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10916", "abs": "https://arxiv.org/abs/2602.10916", "authors": ["Rashid Mushkani"], "title": "Traceable, Enforceable, and Compensable Participation: A Participation Ledger for People-Centered AI Governance", "comment": "Presented at PAIRS: Participatory AI Research & Practice Symposium", "summary": "Participatory approaches are widely invoked in AI governance, yet participation rarely translates into durable influence. In public sector and civic AI systems, community contributions such as deliberations, annotations, prompts, and incident reports are often recorded informally, weakly linked to system updates, and disconnected from enforceable rights or sustained compensation. As a result, participation is frequently symbolic rather than accountable. We introduce the Participation Ledger, a machine readable and auditable framework that operationalizes participation as traceable influence, enforceable authority, and compensable labor. The ledger represents participation as an influence graph that links contributed artifacts to verified changes in AI systems, including datasets, prompts, adapters, policies, guardrails, and evaluation suites. It integrates three elements: a Participation Evidence Standard documenting consent, privacy, compensation, and reuse terms; an influence tracing mechanism that connects system updates to replayable before and after tests, enabling longitudinal monitoring of commitments; and encoded rights and incentives. Capability Vouchers allow authorized community stewards to request or constrain specific system capabilities within defined boundaries, while Participation Credits support ongoing recognition and compensation when contributed tests continue to provide value. We ground the framework in four urban AI and public space governance deployments and provide a machine readable schema, templates, and an evaluation plan for assessing traceability, enforceability, and compensation in practice."}
{"id": "2602.10598", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10598", "abs": "https://arxiv.org/abs/2602.10598", "authors": ["Shuai Han", "Mehdi Dastani", "Shihan Wang"], "title": "Neuro-symbolic Action Masking for Deep Reinforcement Learning", "comment": null, "summary": "Deep reinforcement learning (DRL) may explore infeasible actions during training and execution. Existing approaches assume a symbol grounding function that maps high-dimensional states to consistent symbolic representations and a manually specified action masking techniques to constrain actions. In this paper, we propose Neuro-symbolic Action Masking (NSAM), a novel framework that automatically learn symbolic models, which are consistent with given domain constraints of high-dimensional states, in a minimally supervised manner during the DRL process. Based on the learned symbolic model of states, NSAM learns action masks that rules out infeasible actions. NSAM enables end-to-end integration of symbolic reasoning and deep policy optimization, where improvements in symbolic grounding and policy learning mutually reinforce each other. We evaluate NSAM on multiple domains with constraints, and experimental results demonstrate that NSAM significantly improves sample efficiency of DRL agent while substantially reducing constraint violations."}
{"id": "2602.10367", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10367", "abs": "https://arxiv.org/abs/2602.10367", "authors": ["Zhiling Yan", "Dingjie Song", "Zhe Fang", "Yisheng Ji", "Xiang Li", "Quanzheng Li", "Lichao Sun"], "title": "LiveMedBench: A Contamination-Free Medical Benchmark for LLMs with Automated Rubric Evaluation", "comment": null, "summary": "The deployment of Large Language Models (LLMs) in high-stakes clinical settings demands rigorous and reliable evaluation. However, existing medical benchmarks remain static, suffering from two critical limitations: (1) data contamination, where test sets inadvertently leak into training corpora, leading to inflated performance estimates; and (2) temporal misalignment, failing to capture the rapid evolution of medical knowledge. Furthermore, current evaluation metrics for open-ended clinical reasoning often rely on either shallow lexical overlap (e.g., ROUGE) or subjective LLM-as-a-Judge scoring, both inadequate for verifying clinical correctness. To bridge these gaps, we introduce LiveMedBench, a continuously updated, contamination-free, and rubric-based benchmark that weekly harvests real-world clinical cases from online medical communities, ensuring strict temporal separation from model training data. We propose a Multi-Agent Clinical Curation Framework that filters raw data noise and validates clinical integrity against evidence-based medical principles. For evaluation, we develop an Automated Rubric-based Evaluation Framework that decomposes physician responses into granular, case-specific criteria, achieving substantially stronger alignment with expert physicians than LLM-as-a-Judge. To date, LiveMedBench comprises 2,756 real-world cases spanning 38 medical specialties and multiple languages, paired with 16,702 unique evaluation criteria. Extensive evaluation of 38 LLMs reveals that even the best-performing model achieves only 39.2%, and 84% of models exhibit performance degradation on post-cutoff cases, confirming pervasive data contamination risks. Error analysis further identifies contextual application-not factual knowledge-as the dominant bottleneck, with 35-48% of failures stemming from the inability to tailor medical knowledge to patient-specific constraints."}
{"id": "2602.10944", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2602.10944", "abs": "https://arxiv.org/abs/2602.10944", "authors": ["Chuncheng Liu", "danah boyd"], "title": "The State's Politics of \"Fake Data\"", "comment": "13 pages, 2 figures", "summary": "Data have power. As such, most discussions of data presume that records should mirror some idealized ground truth. Deviations are viewed as failure. Drawing on two ethnographic studies of state data-making in a Chinese street-level bureaucrat agency and at the US Census Bureau we show how seemingly \"fake\" state data perform institutional work. We map four moments in which actors negotiate between representational accuracy and organizational imperatives: creation, correction, collusion, and augmentation. Bureaucrats routinely privilege what data do over what they represent, creating fictions that serve civil servants' self-interest and enable constrained administrations. We argue that \"fakeness\" of state data is relational (context dependent), processual (emerging through workflows), and performative (brought into being through labeling and practice). We urge practitioners to center fitness-for-purpose in assessments of data and contextual governance. Rather than chasing impossible representational accuracy, sociotechnical systems should render the politics of useful fictions visible, contestable, and accountable."}
{"id": "2602.10625", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.10625", "abs": "https://arxiv.org/abs/2602.10625", "authors": ["Nanxu Gong", "Haotian Li", "Sixun Dong", "Jianxun Lian", "Yanjie Fu", "Xing Xie"], "title": "To Think or Not To Think, That is The Question for Large Reasoning Models in Theory of Mind Tasks", "comment": null, "summary": "Theory of Mind (ToM) assesses whether models can infer hidden mental states such as beliefs, desires, and intentions, which is essential for natural social interaction. Although recent progress in Large Reasoning Models (LRMs) has boosted step-by-step inference in mathematics and coding, it is still underexplored whether this benefit transfers to socio-cognitive skills. We present a systematic study of nine advanced Large Language Models (LLMs), comparing reasoning models with non-reasoning models on three representative ToM benchmarks. The results show that reasoning models do not consistently outperform non-reasoning models and sometimes perform worse. A fine-grained analysis reveals three insights. First, slow thinking collapses: accuracy significantly drops as responses grow longer, and larger reasoning budgets hurt performance. Second, moderate and adaptive reasoning benefits performance: constraining reasoning length mitigates failure, while distinct success patterns demonstrate the necessity of dynamic adaptation. Third, option matching shortcut: when multiple choice options are removed, reasoning models improve markedly, indicating reliance on option matching rather than genuine deduction. We also design two intervention approaches: Slow-to-Fast (S2F) adaptive reasoning and Think-to-Match (T2M) shortcut prevention to further verify and mitigate the problems. With all results, our study highlights the advancement of LRMs in formal reasoning (e.g., math, code) cannot be fully transferred to ToM, a typical task in social reasoning. We conclude that achieving robust ToM requires developing unique capabilities beyond existing reasoning methods."}
{"id": "2602.10458", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10458", "abs": "https://arxiv.org/abs/2602.10458", "authors": ["Yansong Qu", "Zihao Sheng", "Zilin Huang", "Jiancong Chen", "Yuhao Luo", "Tianyi Wang", "Yiheng Feng", "Samuel Labi", "Sikai Chen"], "title": "Found-RL: foundation model-enhanced reinforcement learning for autonomous driving", "comment": "39 pages", "summary": "Reinforcement Learning (RL) has emerged as a dominant paradigm for end-to-end autonomous driving (AD). However, RL suffers from sample inefficiency and a lack of semantic interpretability in complex scenarios. Foundation Models, particularly Vision-Language Models (VLMs), can mitigate this by offering rich, context-aware knowledge, yet their high inference latency hinders deployment in high-frequency RL training loops. To bridge this gap, we present Found-RL, a platform tailored to efficiently enhance RL for AD using foundation models. A core innovation is the asynchronous batch inference framework, which decouples heavy VLM reasoning from the simulation loop, effectively resolving latency bottlenecks to support real-time learning. We introduce diverse supervision mechanisms: Value-Margin Regularization (VMR) and Advantage-Weighted Action Guidance (AWAG) to effectively distill expert-like VLM action suggestions into the RL policy. Additionally, we adopt high-throughput CLIP for dense reward shaping. We address CLIP's dynamic blindness via Conditional Contrastive Action Alignment, which conditions prompts on discretized speed/command and yields a normalized, margin-based bonus from context-specific action-anchor scoring. Found-RL provides an end-to-end pipeline for fine-tuned VLM integration and shows that a lightweight RL model can achieve near-VLM performance compared with billion-parameter VLMs while sustaining real-time inference (approx. 500 FPS). Code, data, and models will be publicly available at https://github.com/ys-qu/found-rl."}
{"id": "2602.10995", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2602.10995", "abs": "https://arxiv.org/abs/2602.10995", "authors": ["Amelie Wührl", "Mattes Ruckdeschel", "Kyle Lo", "Anna Rogers"], "title": "A Human-Centric Framework for Data Attribution in Large Language Models", "comment": null, "summary": "In the current Large Language Model (LLM) ecosystem, creators have little agency over how their data is used, and LLM users may find themselves unknowingly plagiarizing existing sources. Attribution of LLM-generated text to LLM input data could help with these challenges, but so far we have more questions than answers: what elements of LLM outputs require attribution, what goals should it serve, how should it be implemented?\n  We contribute a human-centric data attribution framework, which situates the attribution problem within the broader data economy. Specific use cases for attribution, such as creative writing assistance or fact-checking, can be specified via a set of parameters (including stakeholder objectives and implementation criteria). These criteria are up for negotiation by the relevant stakeholder groups: creators, LLM users, and their intermediaries (publishers, platforms, AI companies). The outcome of domain-specific negotiations can be implemented and tested for whether the stakeholder goals are achieved. The proposed approach provides a bridge between methodological NLP work on data attribution, governance work on policy interventions, and economic analysis of creator incentives for a sustainable equilibrium in the data economy."}
{"id": "2602.10635", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10635", "abs": "https://arxiv.org/abs/2602.10635", "authors": ["Keane Ong", "Sabri Boughorbel", "Luwei Xiao", "Chanakya Ekbote", "Wei Dai", "Ao Qu", "Jingyao Wu", "Rui Mao", "Ehsan Hoque", "Erik Cambria", "Gianmarco Mengaldo", "Paul Pu Liang"], "title": "OmniSapiens: A Foundation Model for Social Behavior Processing via Heterogeneity-Aware Relative Policy Optimization", "comment": null, "summary": "To develop socially intelligent AI, existing approaches typically model human behavioral dimensions (e.g., affective, cognitive, or social attributes) in isolation. Although useful, task-specific modeling often increases training costs and limits generalization across behavioral settings. Recent reasoning RL methods facilitate training a single unified model across multiple behavioral tasks, but do not explicitly address learning across different heterogeneous behavioral data. To address this gap, we introduce Heterogeneity-Aware Relative Policy Optimization (HARPO), an RL method that balances leaning across heterogeneous tasks and samples. This is achieved by modulating advantages to ensure that no single task or sample carries disproportionate influence during policy optimization. Using HARPO, we develop and release Omnisapiens-7B 2.0, a foundation model for social behavior processing. Relative to existing behavioral foundation models, Omnisapiens-7B 2.0 achieves the strongest performance across behavioral tasks, with gains of up to +16.85% and +9.37% on multitask and held-out settings respectively, while producing more explicit and robust reasoning traces. We also validate HARPO against recent RL methods, where it achieves the most consistently strong performance across behavioral tasks."}
{"id": "2602.10459", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2602.10459", "abs": "https://arxiv.org/abs/2602.10459", "authors": ["Song Kim", "Hyewon Kim", "Kaiqiang Yu", "Taejoon Han", "Junghoon Kim", "Susik Yoon", "Jungeun Kim"], "title": "Efficient Computation of Maximum Flexi-Clique in Networks", "comment": null, "summary": "Discovering large cohesive subgraphs is a key task for graph mining. Existing models, such as clique, k-plex, and γ-quasi-clique, use fixed density thresholds that overlook the natural decay of connectivity as the subgraph size increases. The Flexi-clique model overcomes this limitation by imposing a degree constraint that grows sub-linearly with subgraph size. We provide the algorithmic study of Flexi-clique, proving its NP-hardness and analysing its non-hereditary properties. To address its computational challenge, we propose the Flexi-Prune Algorithm FPA, a fast heuristic using core-based seeding and connectivity-aware pruning, and the Efficient Branch-and-Bound Algorithm EBA, an exact framework enhanced with multiple pruning rules. Experiments on large real-world and synthetic networks demonstrate that FPA achieves near-optimal quality at much lower cost, while EBA efficiently computes exact solutions. Flexi-clique thus provides a practical and scalable model for discovering large, meaningful subgraphs in complex networks."}
{"id": "2602.10131", "categories": ["cs.SI", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.10131", "abs": "https://arxiv.org/abs/2602.10131", "authors": ["David Holtz"], "title": "The Anatomy of the Moltbook Social Graph", "comment": "20 pages, 7 figures", "summary": "I present a descriptive analysis of Moltbook, a social platform populated exclusively by AI agents, using data from the platform's first 3.5 days (6{,}159 agents; 13{,}875 posts; 115{,}031 comments). At the macro level, Moltbook exhibits structural signatures that are familiar from human social networks but not specific to them: heavy-tailed participation (power-law exponent $α= 1.70$) and small-world connectivity (average path length $=2.91$). At the micro level, patterns appear distinctly non-human. Conversations are extremely shallow (mean depth $=1.07$; 93.5\\% of comments receive no replies), reciprocity is low (0.197), and 34.1\\% of messages are exact duplicates of viral templates. Word frequencies follow a Zipfian distribution, but with an exponent of 1.70 -- notably steeper than typical English text ($\\approx 1.0$), suggesting more formulaic content. Agent discourse is dominated by identity-related language (68.1\\% of unique messages) and distinctive phrasings like ``my human'' (9.4\\% of messages) that have no parallel in human social media. Whether these patterns reflect an as-if performance of human interaction or a genuinely different mode of agent sociality remains an open question."}
{"id": "2602.10699", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10699", "abs": "https://arxiv.org/abs/2602.10699", "authors": ["Jie Jiang", "Yangru Huang", "Zeyu Wang", "Changping Wang", "Yuling Xiong", "Jun Zhang", "Huan Yu"], "title": "Spend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation", "comment": null, "summary": "Generative recommendation via autoregressive models has unified retrieval and ranking into a single conditional generation framework. However, fine-tuning these models with Reinforcement Learning (RL) often suffers from a fundamental probability-reward mismatch. Conventional likelihood-dominated decoding (e.g., beam search) exhibits a myopic bias toward locally probable prefixes, which causes two critical failures: (1) insufficient exploration, where high-reward items in low-probability branches are prematurely pruned and rarely sampled, and (2) advantage compression, where trajectories sharing high-probability prefixes receive highly correlated rewards with low within-group variance, yielding a weak comparative signal for RL. To address these challenges, we propose V-STAR, a Value-guided Sampling and Tree-structured Advantage Reinforcement framework. V-STAR forms a self-evolving loop via two synergistic components. First, a Value-Guided Efficient Decoding (VED) is developed to identify decisive nodes and selectively deepen high-potential prefixes. This improves exploration efficiency without exhaustive tree search. Second, we propose Sibling-GRPO, which exploits the induced tree topology to compute sibling-relative advantages and concentrates learning signals on decisive branching decisions. Extensive experiments on both offline and online datasets demonstrate that V-STAR outperforms state-of-the-art baselines, delivering superior accuracy and candidate-set diversity under strict latency constraints."}
{"id": "2602.10467", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10467", "abs": "https://arxiv.org/abs/2602.10467", "authors": ["Jihwan Oh", "Murad Aghazada", "Yooju Shin", "Se-Young Yun", "Taehyeon Kim"], "title": "MERIT Feedback Elicits Better Bargaining in LLM Negotiators", "comment": "Preprint. arXiv admin note: substantial text overlap with arXiv:2505.22998", "summary": "Bargaining is often regarded as a logical arena rather than an art or a matter of intuition, yet Large Language Models (LLMs) still struggle to navigate it due to limited strategic depth and difficulty adapting to complex human factors. Current benchmarks rarely capture this limitation. To bridge this gap, we present an utility feedback centric framework. Our contributions are: (i) AgoraBench, a new benchmark spanning nine challenging settings (e.g., deception, monopoly) that supports diverse strategy modeling; (ii) human-aligned, economically grounded metrics derived from utility theory. This is operationalized via agent utility, negotiation power, and acquisition ratio that implicitly measure how well the negotiation aligns with human preference and (iii) a human preference grounded dataset with learning pipeline that strengthens LLMs' bargaining ability through both prompting and finetuning. Empirical results indicate that baseline LLM strategies often diverge from human preferences, while our mechanism substantially improves negotiation performance, yielding deeper strategic behavior and stronger opponent awareness."}
{"id": "2602.10324", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.10324", "abs": "https://arxiv.org/abs/2602.10324", "authors": ["Caroline Wang", "Daniel Kasenberg", "Kim Stachenfeld", "Pablo Samuel Castro"], "title": "Discovering Differences in Strategic Behavior Between Humans and LLMs", "comment": null, "summary": "As Large Language Models (LLMs) are increasingly deployed in social and strategic scenarios, it becomes critical to understand where and why their behavior diverges from that of humans. While behavioral game theory (BGT) provides a framework for analyzing behavior, existing models do not fully capture the idiosyncratic behavior of humans or black-box, non-human agents like LLMs. We employ AlphaEvolve, a cutting-edge program discovery tool, to directly discover interpretable models of human and LLM behavior from data, thereby enabling open-ended discovery of structural factors driving human and LLM behavior. Our analysis on iterated rock-paper-scissors reveals that frontier LLMs can be capable of deeper strategic behavior than humans. These results provide a foundation for understanding structural differences driving differences in human and LLM behavior in strategic interactions."}
{"id": "2602.10802", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10802", "abs": "https://arxiv.org/abs/2602.10802", "authors": ["Da-Lun Chen", "Prasasthy Balasubramanian", "Lauri Lovén", "Susanna Pirttikangas", "Jaakko Sauvola", "Panagiotis Kostakos"], "title": "Integrating Generative AI-enhanced Cognitive Systems in Higher Education: From Stakeholder Perceptions to a Conceptual Framework considering the EU AI Act", "comment": null, "summary": "Many staff and students in higher education have adopted generative artificial intelligence (GenAI) tools in their work and study. GenAI is expected to enhance cognitive systems by enabling personalized learning and streamlining educational services. However, stakeholders perceptions of GenAI in higher education remain divided, shaped by cultural, disciplinary, and institutional contexts. In addition, the EU AI Act requires universities to ensure regulatory compliance when deploying cognitive systems. These developments highlight the need for institutions to engage stakeholders and tailor GenAI integration to their needs while addressing concerns. This study investigates how GenAI is perceived within the disciplines of Information Technology and Electrical Engineering (ITEE). Using a mixed-method approach, we surveyed 61 staff and 37 students at the Faculty of ITEE, University of Oulu. The results reveal both shared and discipline-specific themes, including strong interest in programming support from GenAI and concerns over response quality, privacy, and academic integrity. Drawing from these insights, the study identifies a set of high-level requirements and proposes a conceptual framework for responsible GenAI integration. Disciplinary-specific requirements reinforce the importance of stakeholder engagement when integrating GenAI into higher education. The high-level requirements and the framework provide practical guidance for universities aiming to harness GenAI while addressing stakeholder concerns and ensuring regulatory compliance."}
{"id": "2602.10485", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10485", "abs": "https://arxiv.org/abs/2602.10485", "authors": ["Zhenhe Cui", "Huaxiang Xia", "Hangjun Shen", "Kailun Luo", "Yong He", "Wei Liang"], "title": "Abstraction Generation for Generalized Planning with Pretrained Large Language Models", "comment": null, "summary": "Qualitative Numerical Planning (QNP) serves as an important abstraction model for generalized planning (GP), which aims to compute general plans that solve multiple instances at once. Recent works show that large language models (LLMs) can function as generalized planners. This work investigates whether LLMs can serve as QNP abstraction generators for GP problems and how to fix abstractions via automated debugging. We propose a prompt protocol: input a GP domain and training tasks to LLMs, prompting them to generate abstract features and further abstract the initial state, action set, and goal into QNP problems. An automated debugging method is designed to detect abstraction errors, guiding LLMs to fix abstractions. Experiments demonstrate that under properly guided by automated debugging, some LLMs can generate useful QNP abstractions."}
{"id": "2602.10814", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10814", "abs": "https://arxiv.org/abs/2602.10814", "authors": ["Xingyi Zhang", "Yulei Ye", "Kaifeng Huang", "Wenhao Li", "Xiangfeng Wang"], "title": "See, Plan, Snap: Evaluating Multimodal GUI Agents in Scratch", "comment": null, "summary": "Block-based programming environments such as Scratch play a central role in low-code education, yet evaluating the capabilities of AI agents to construct programs through Graphical User Interfaces (GUIs) remains underexplored. We introduce ScratchWorld, a benchmark for evaluating multimodal GUI agents on program-by-construction tasks in Scratch. Grounded in the Use-Modify-Create pedagogical framework, ScratchWorld comprises 83 curated tasks spanning four distinct problem categories: Create, Debug, Extend, and Compute. To rigorously diagnose the source of agent failures, the benchmark employs two complementary interaction modes: primitive mode requires fine-grained drag-and-drop manipulation to directly assess visuomotor control, while composite mode uses high-level semantic APIs to disentangle program reasoning from GUI execution. To ensure reliable assessment, we propose an execution-based evaluation protocol that validates the functional correctness of the constructed Scratch programs through runtime tests within the browser environment. Extensive experiments across state-of-the-art multimodal language models and GUI agents reveal a substantial reasoning--acting gap, highlighting persistent challenges in fine-grained GUI manipulation despite strong planning capabilities."}
{"id": "2602.10526", "categories": ["cs.CY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.10526", "abs": "https://arxiv.org/abs/2602.10526", "authors": ["Mickey M. Rogers", "William M. Ota", "Nathaniel Burola", "Tepring Piquado"], "title": "The Infrastructure Equation: Water, Energy, and Community Policy for Georgia's Data Center Boom", "comment": null, "summary": "The rapid growth of data centers driven by cloud computing and artificial intelligence is reshaping infrastructure planning and environmental governance in the United States. Georgia has emerged as a major market for data center development, particularly in the Atlanta metropolitan region, creating economic opportunity alongside significant challenges. Data centers are water-intensive, energy-intensive, and land-intensive infrastructure whose cumulative impacts strain municipal water systems, electric grids, and local land-use frameworks. Unlike single industrial projects, data centers are often proposed in clusters, amplifying community and infrastructure impacts.\n  This report draws on insights from a Georgia-based expert convening to describe the implications of data center growth for water management, energy reliability, ratepayer equity, zoning, and community engagement, identify potential gaps in transparency and regulatory coordination, and present a policy roadmap to help Georgia balance digital infrastructure growth with sustainability, equity, and community protection."}
{"id": "2602.10845", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10845", "abs": "https://arxiv.org/abs/2602.10845", "authors": ["Xuecheng Zou", "Yu Tang", "Bingbing Wang"], "title": "SynergyKGC: Reconciling Topological Heterogeneity in Knowledge Graph Completion via Topology-Aware Synergy", "comment": "10 pages, 5 tables, 7 figures. This work introduces the Active Synergy mechanism and Identity Anchoring for Knowledge Graph Completion. Code: https://github.com/XuechengZou-2001/SynergyKGC-main", "summary": "Knowledge Graph Completion (KGC) fundamentally hinges on the coherent fusion of pre-trained entity semantics with heterogeneous topological structures to facilitate robust relational reasoning. However, existing paradigms encounter a critical \"structural resolution mismatch,\" failing to reconcile divergent representational demands across varying graph densities, which precipitates structural noise interference in dense clusters and catastrophic representation collapse in sparse regions. We present SynergyKGC, an adaptive framework that advances traditional neighbor aggregation to an active Cross-Modal Synergy Expert via relation-aware cross-attention and semantic-intent-driven gating. By coupling a density-dependent Identity Anchoring strategy with a Double-tower Coherent Consistency architecture, SynergyKGC effectively reconciles topological heterogeneity while ensuring representational stability across training and inference phases. Systematic evaluations on two public benchmarks validate the superiority of our method in significantly boosting KGC hit rates, providing empirical evidence for a generalized principle of resilient information integration in non-homogeneous structured data."}
{"id": "2602.10527", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10527", "abs": "https://arxiv.org/abs/2602.10527", "authors": ["Scott P. McGrath", "Katherine K. Kim", "Karnjit Johl", "Haibo Wang", "Nick Anderson"], "title": "AI-PACE: A Framework for Integrating AI into Medical Education", "comment": "9 pages, 2 figures,", "summary": "The integration of artificial intelligence (AI) into healthcare is accelerating, yet medical education has not kept pace with these technological advancements. This paper synthesizes current knowledge on AI in medical education through a comprehensive analysis of the literature, identifying key competencies, curricular approaches, and implementation strategies. The aim is highlighting the critical need for structured AI education across the medical learning continuum and offer a framework for curriculum development. The findings presented suggest that effective AI education requires longitudinal integration throughout medical training, interdisciplinary collaboration, and balanced attention to both technical fundamentals and clinical applications. This paper serves as a foundation for medical educators seeking to prepare future physicians for an AI-enhanced healthcare environment."}
{"id": "2602.10885", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10885", "abs": "https://arxiv.org/abs/2602.10885", "authors": ["Leheng Sheng", "Wenchang Ma", "Ruixin Hong", "Xiang Wang", "An Zhang", "Tat-Seng Chua"], "title": "Reinforcing Chain-of-Thought Reasoning with Self-Evolving Rubrics", "comment": "21 pages", "summary": "Despite chain-of-thought (CoT) playing crucial roles in LLM reasoning, directly rewarding it is difficult: training a reward model demands heavy human labeling efforts, and static RMs struggle with evolving CoT distributions and reward hacking. These challenges motivate us to seek an autonomous CoT rewarding approach that requires no human annotation efforts and can evolve gradually. Inspired by recent self-evolving training methods, we propose \\textbf{RLCER} (\\textbf{R}einforcement \\textbf{L}earning with \\textbf{C}oT Supervision via Self-\\textbf{E}volving \\textbf{R}ubrics), which enhances the outcome-centric RLVR by rewarding CoTs with self-proposed and self-evolving rubrics. We show that self-proposed and self-evolving rubrics provide reliable CoT supervision signals even without outcome rewards, enabling RLCER to outperform outcome-centric RLVR. Moreover, when used as in-prompt hints, these self-proposed rubrics further improve inference-time performance."}
{"id": "2602.10529", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2602.10529", "abs": "https://arxiv.org/abs/2602.10529", "authors": ["David H. Smith", "S. Moonwara A. Monisha", "Annapurna Vadaparty", "Leo Porter", "Daniel Zingaro"], "title": "Drawing Your Programs: Exploring the Applications of Visual-Prompting with GenAI for Teaching and Assessment", "comment": null, "summary": "When designing a program, both novice programmers and seasoned developers alike often sketch out -- or, perhaps more famously, whiteboard -- their ideas. Yet despite the introduction of natively multimodal Generative AI models, work on Human-GenAI collaborative coding has remained overwhelmingly focused on textual prompts -- largely ignoring the visual and spatial representations that programmers naturally use to reason about and communicate their designs. In this proposal and position paper, we argue and provide tentative evidence that this text-centric focus overlooks other forms of prompting GenAI models, such as problem decomposition diagrams functioning as prompts for code generation in their own right enabling new types of programming activities and assessments. To support this position, we present findings from a large introductory Python programming course, where students constructed decomposition diagrams that were used to prompt GPT-4.1 for code generation. We demonstrate that current models are very successful in their ability to generate code from student-constructed diagrams. We conclude by exploring the implications of embracing multimodal prompting for computing education, particularly in the context of assessment."}
{"id": "2602.10964", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10964", "abs": "https://arxiv.org/abs/2602.10964", "authors": ["F. Carichon", "R. Rampa", "G. Farnadi"], "title": "Can LLMs Cook Jamaican Couscous? A Study of Cultural Novelty in Recipe Generation", "comment": "14 pages, 12 figures, conference", "summary": "Large Language Models (LLMs) are increasingly used to generate and shape cultural content, ranging from narrative writing to artistic production. While these models demonstrate impressive fluency and generative capacity, prior work has shown that they also exhibit systematic cultural biases, raising concerns about stereotyping, homogenization, and the erasure of culturally specific forms of expression. Understanding whether LLMs can meaningfully align with diverse cultures beyond the dominant ones remains a critical challenge. In this paper, we study cultural adaptation in LLMs through the lens of cooking recipes, a domain in which culture, tradition, and creativity are tightly intertwined. We build on the \\textit{GlobalFusion} dataset, which pairs human recipes from different countries according to established measures of cultural distance. Using the same country pairs, we generate culturally adapted recipes with multiple LLMs, enabling a direct comparison between human and LLM behavior in cross-cultural content creation. Our analysis shows that LLMs fail to produce culturally representative adaptations. Unlike humans, the divergence of their generated recipes does not correlate with cultural distance. We further provide explanations for this gap. We show that cultural information is weakly preserved in internal model representations, that models inflate novelty in their production by misunderstanding notions such as creativity and tradition, and that they fail to identify adaptation with its associated countries and to ground it in culturally salient elements such as ingredients. These findings highlight fundamental limitations of current LLMs for culturally oriented generation and have important implications for their use in culturally sensitive applications."}
{"id": "2602.10543", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2602.10543", "abs": "https://arxiv.org/abs/2602.10543", "authors": ["Andrew Adamatzky"], "title": "Fungal systems for security and resilience", "comment": null, "summary": "Modern security, infrastructure, and safety-critical systems increasingly operate in environments characterised by disruption, uncertainty, physical damage, and degraded communications. Conventional digital technologies -- centralised sensors, software-defined control, and energy-intensive monitoring -- often struggle under such conditions. We propose fungi, and in particular living mycelial networks, as a novel class of biohybride systems for security, resilience, and protection in extreme environments. We discuss how fungi can function as distributed sensing substrates, self-healing materials, and low-observability anomaly-detection layers. We map fungal properties -- such as decentralised control, embodied memory, and autonomous repair -- to applications in infrastructure protection, environmental monitoring, tamper evidence, and long-duration resilience."}
{"id": "2602.10999", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10999", "abs": "https://arxiv.org/abs/2602.10999", "authors": ["Yusong Lin", "Haiyang Wang", "Shuzhe Wu", "Lue Fan", "Feiyang Pan", "Sanyuan Zhao", "Dandan Tu"], "title": "CLI-Gym: Scalable CLI Task Generation via Agentic Environment Inversion", "comment": null, "summary": "Agentic coding requires agents to effectively interact with runtime environments, e.g., command line interfaces (CLI), so as to complete tasks like resolving dependency issues, fixing system problems, etc. But it remains underexplored how such environment-intensive tasks can be obtained at scale to enhance agents' capabilities. To address this, based on an analogy between the Dockerfile and the agentic task, we propose to employ agents to simulate and explore environment histories, guided by execution feedback. By tracing histories of a healthy environment, its state can be inverted to an earlier one with runtime failures, from which a task can be derived by packing the buggy state and the corresponding error messages. With our method, named CLI-Gym, a total of 1,655 environment-intensive tasks are derived, being the largest collection of its kind. Moreover, with curated successful trajectories, our fine-tuned model, named LiberCoder, achieves substantial absolute improvements of +21.1% (to 46.1%) on Terminal-Bench, outperforming various strong baselines. To our knowledge, this is the first public pipeline for scalable derivation of environment-intensive tasks."}
{"id": "2602.10583", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10583", "abs": "https://arxiv.org/abs/2602.10583", "authors": ["Bo Xue", "Yunchong Song", "Fanghao Shao", "Xuekai Zhu", "Lin Chen", "Luoyi Fu", "Xinbing Wang", "Zhouhan Lin"], "title": "Flow of Spans: Generalizing Language Models to Dynamic Span-Vocabulary via GFlowNets", "comment": "Published as a conference paper at ICLR 2026", "summary": "Standard autoregressive language models generate text token-by-token from a fixed vocabulary, inducing a tree-structured state space when viewing token sampling as an action, which limits flexibility and expressiveness. Recent work introduces dynamic vocabulary by sampling retrieved text spans but overlooks that the same sentence can be composed of spans of varying lengths, lacking explicit modeling of the directed acyclic graph (DAG) state space. This leads to restricted exploration of compositional paths and is biased toward the chosen path. Generative Flow Networks (GFlowNets) are powerful for efficient exploring and generalizing over state spaces, particularly those with a DAG structure. However, prior GFlowNets-based language models operate at the token level and remain confined to tree-structured spaces, limiting their potential. In this work, we propose Flow of SpanS (FOSS), a principled GFlowNets framework for span generation. FoSS constructs a dynamic span vocabulary by segmenting the retrieved text flexibly, ensuring a DAG-structured state space, which allows GFlowNets to explore diverse compositional paths and improve generalization. With specialized reward models, FoSS generates diverse, high-quality text. Empirically, FoSS improves MAUVE scores by up to 12.5% over Transformer on text generation and achieves 3.5% gains on knowledge-intensive tasks, consistently outperforming state-of-the-art methods. Scaling experiments further demonstrate FoSS benefits from larger models, more data, and richer retrieval corpora, retaining its advantage over strong baselines."}
{"id": "2602.11103", "categories": ["cs.AI", "cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.11103", "abs": "https://arxiv.org/abs/2602.11103", "authors": ["Wayne Chi", "Yixiong Fang", "Arnav Yayavaram", "Siddharth Yayavaram", "Seth Karten", "Qiuhong Anna Wei", "Runkun Chen", "Alexander Wang", "Valerie Chen", "Ameet Talwalkar", "Chris Donahue"], "title": "GameDevBench: Evaluating Agentic Capabilities Through Game Development", "comment": null, "summary": "Despite rapid progress on coding agents, progress on their multimodal counterparts has lagged behind. A key challenge is the scarcity of evaluation testbeds that combine the complexity of software development with the need for deep multimodal understanding. Game development provides such a testbed as agents must navigate large, dense codebases while manipulating intrinsically multimodal assets such as shaders, sprites, and animations within a visual game scene. We present GameDevBench, the first benchmark for evaluating agents on game development tasks. GameDevBench consists of 132 tasks derived from web and video tutorials. Tasks require significant multimodal understanding and are complex -- the average solution requires over three times the amount of lines of code and file changes compared to prior software development benchmarks. Agents still struggle with game development, with the best agent solving only 54.5% of tasks. We find a strong correlation between perceived task difficulty and multimodal complexity, with success rates dropping from 46.9% on gameplay-oriented tasks to 31.6% on 2D graphics tasks. To improve multimodal capability, we introduce two simple image and video-based feedback mechanisms for agents. Despite their simplicity, these methods consistently improve performance, with the largest change being an increase in Claude Sonnet 4.5's performance from 33.3% to 47.7%. We release GameDevBench publicly to support further research into agentic game development."}
{"id": "2602.10597", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2602.10597", "abs": "https://arxiv.org/abs/2602.10597", "authors": ["Unggi Lee", "Yeil Jeong", "Chohui Lee", "Gyuri Byun", "Yunseo Lee", "Minji Kang", "Minji Jeon"], "title": "Llama-Polya: Instruction Tuning for Large Language Model based on Polya's Problem-solving", "comment": null, "summary": "This paper introduces Llama-Polya, an instruction-tuned large language model that integrates Polya's four-step problem-solving framework into its dialogue structure to support mathematical reasoning. Mathematical problem-solving is central to students' success in mathematics education, yet many learners struggle to plan, justify, and verify their solutions. Although large language models (LLMs) show promise as intelligent tutors, they often lack structured pedagogical alignment grounded in established learning theories.\n  To address this gap, we operationalize Polya's problem-solving framework within an instruction-tuned LLM to promote metacognitive engagement and examine the effects of pedagogy-aligned fine-tuning compared to domain-only and general-purpose instruction tuning. Built on the Llama-3.1-8B architecture, Llama-Polya was fine-tuned on synthetic math problem-solving data derived from GSM8K, structured according to Polya's four stages. We developed and evaluated multiple variants-general-purpose instruct, math-domain metamath, pedagogy-aligned polya-v2, and sequential metamath+polya-v2-using both quantitative accuracy metrics and qualitative pedagogical assessments.\n  Results indicate that models tuned with Polya's framework and domain-specific data produced more balanced reasoning-stage distributions and fewer premature answers. Expert evaluators also observed improved pedagogical coherence and metacognitive prompting, although limitations in personalization and mathematical rigor remained. These findings suggest that pedagogy-grounded instruction tuning can enhance educational alignment and reasoning transparency in LLM-based tutoring systems."}
{"id": "2602.11136", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11136", "abs": "https://arxiv.org/abs/2602.11136", "authors": ["Jiayi Zhou", "Yang Sheng", "Hantao Lou", "Yaodong Yang", "Jie Fu"], "title": "FormalJudge: A Neuro-Symbolic Paradigm for Agentic Oversight", "comment": "27 pages", "summary": "As LLM-based agents increasingly operate in high-stakes domains with real-world consequences, ensuring their behavioral safety becomes paramount. The dominant oversight paradigm, LLM-as-a-Judge, faces a fundamental dilemma: how can probabilistic systems reliably supervise other probabilistic systems without inheriting their failure modes? We argue that formal verification offers a principled escape from this dilemma, yet its adoption has been hindered by a critical bottleneck: the translation from natural language requirements to formal specifications. This paper bridges this gap by proposing , a neuro-symbolic framework that employs a bidirectional Formal-of-Thought architecture: LLMs serve as specification compilers that top-down decompose high-level human intent into atomic, verifiable constraints, then bottom-up prove compliance using Dafny specifications and Z3 Satisfiability modulo theories solving, which produces mathematical guarantees rather than probabilistic scores. We validate across three benchmarks spanning behavioral safety, multi-domain constraint adherence, and agentic upward deception detection. Experiments on 7 agent models demonstrate that achieves an average improvement of 16.6% over LLM-as-a-Judge baselines, enables weak-to-strong generalization where a 7B judge achieves over 90% accuracy detecting deception from 72B agents, and provides near-linear safety improvement through iterative refinement."}
{"id": "2602.10598", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10598", "abs": "https://arxiv.org/abs/2602.10598", "authors": ["Shuai Han", "Mehdi Dastani", "Shihan Wang"], "title": "Neuro-symbolic Action Masking for Deep Reinforcement Learning", "comment": null, "summary": "Deep reinforcement learning (DRL) may explore infeasible actions during training and execution. Existing approaches assume a symbol grounding function that maps high-dimensional states to consistent symbolic representations and a manually specified action masking techniques to constrain actions. In this paper, we propose Neuro-symbolic Action Masking (NSAM), a novel framework that automatically learn symbolic models, which are consistent with given domain constraints of high-dimensional states, in a minimally supervised manner during the DRL process. Based on the learned symbolic model of states, NSAM learns action masks that rules out infeasible actions. NSAM enables end-to-end integration of symbolic reasoning and deep policy optimization, where improvements in symbolic grounding and policy learning mutually reinforce each other. We evaluate NSAM on multiple domains with constraints, and experimental results demonstrate that NSAM significantly improves sample efficiency of DRL agent while substantially reducing constraint violations."}
{"id": "2602.10122", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10122", "abs": "https://arxiv.org/abs/2602.10122", "authors": ["Eranga Bandara", "Ross Gore", "Sachin Shetty", "Sachini Rajapakse", "Isurunima Kularathna", "Pramoda Karunarathna", "Ravi Mukkamala", "Peter Foytik", "Safdar H. Bouk", "Abdul Rahman", "Xueping Liang", "Amin Hass", "Tharaka Hewa", "Ng Wee Keong", "Kasun De Zoysa", "Aruna Withanage", "Nilaan Loganathan"], "title": "A Practical Guide to Agentic AI Transition in Organizations", "comment": null, "summary": "Agentic AI represents a significant shift in how intelligence is applied within organizations, moving beyond AI-assisted tools toward autonomous systems capable of reasoning, decision-making, and coordinated action across workflows. As these systems mature, they have the potential to automate a substantial share of manual organizational processes, fundamentally reshaping how work is designed, executed, and governed. Although many organizations have adopted AI to improve productivity, most implementations remain limited to isolated use cases and human-centered, tool-driven workflows. Despite increasing awareness of agentic AI's strategic importance, engineering teams and organizational leaders often lack clear guidance on how to operationalize it effectively. Key challenges include an overreliance on traditional software engineering practices, limited integration of business-domain knowledge, unclear ownership of AI-driven workflows, and the absence of sustainable human-AI collaboration models. Consequently, organizations struggle to move beyond experimentation, scale agentic systems, and align them with tangible business value. Drawing on practical experience in designing and deploying agentic AI workflows across multiple organizations and business domains, this paper proposes a pragmatic framework for transitioning organizational functions from manual processes to automated agentic AI systems. The framework emphasizes domain-driven use case identification, systematic delegation of tasks to AI agents, AI-assisted construction of agentic workflows, and small, AI-augmented teams working closely with business stakeholders. Central to the approach is a human-in-the-loop operating model in which individuals act as orchestrators of multiple AI agents, enabling scalable automation while maintaining oversight, adaptability, and organizational control."}
{"id": "2602.10625", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.10625", "abs": "https://arxiv.org/abs/2602.10625", "authors": ["Nanxu Gong", "Haotian Li", "Sixun Dong", "Jianxun Lian", "Yanjie Fu", "Xing Xie"], "title": "To Think or Not To Think, That is The Question for Large Reasoning Models in Theory of Mind Tasks", "comment": null, "summary": "Theory of Mind (ToM) assesses whether models can infer hidden mental states such as beliefs, desires, and intentions, which is essential for natural social interaction. Although recent progress in Large Reasoning Models (LRMs) has boosted step-by-step inference in mathematics and coding, it is still underexplored whether this benefit transfers to socio-cognitive skills. We present a systematic study of nine advanced Large Language Models (LLMs), comparing reasoning models with non-reasoning models on three representative ToM benchmarks. The results show that reasoning models do not consistently outperform non-reasoning models and sometimes perform worse. A fine-grained analysis reveals three insights. First, slow thinking collapses: accuracy significantly drops as responses grow longer, and larger reasoning budgets hurt performance. Second, moderate and adaptive reasoning benefits performance: constraining reasoning length mitigates failure, while distinct success patterns demonstrate the necessity of dynamic adaptation. Third, option matching shortcut: when multiple choice options are removed, reasoning models improve markedly, indicating reliance on option matching rather than genuine deduction. We also design two intervention approaches: Slow-to-Fast (S2F) adaptive reasoning and Think-to-Match (T2M) shortcut prevention to further verify and mitigate the problems. With all results, our study highlights the advancement of LRMs in formal reasoning (e.g., math, code) cannot be fully transferred to ToM, a typical task in social reasoning. We conclude that achieving robust ToM requires developing unique capabilities beyond existing reasoning methods."}
{"id": "2602.10127", "categories": ["cs.SI", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.10127", "abs": "https://arxiv.org/abs/2602.10127", "authors": ["Yukun Jiang", "Yage Zhang", "Xinyue Shen", "Michael Backes", "Yang Zhang"], "title": "\"Humans welcome to observe\": A First Look at the Agent Social Network Moltbook", "comment": "16 pages", "summary": "The rapid advancement of artificial intelligence (AI) agents has catalyzed the transition from static language models to autonomous agents capable of tool use, long-term planning, and social interaction. $\\textbf{Moltbook}$, the first social network designed exclusively for AI agents, has experienced viral growth in early 2026. To understand the behavior of AI agents in the agent-native community, in this paper, we present a large-scale empirical analysis of Moltbook leveraging a dataset of 44,411 posts and 12,209 sub-communities (\"submolts\") collected prior to February 1, 2026. Leveraging a topic taxonomy with nine content categories and a five-level toxicity scale, we systematically analyze the topics and risks of agent discussions. Our analysis answers three questions: what topics do agents discuss (RQ1), how risk varies by topic (RQ2), and how topics and toxicity evolve over time (RQ3). We find that Moltbook exhibits explosive growth and rapid diversification, moving beyond early social interaction into viewpoint, incentive-driven, promotional, and political discourse. The attention of agents increasingly concentrates in centralized hubs and around polarizing, platform-native narratives. Toxicity is strongly topic-dependent: incentive- and governance-centric categories contribute a disproportionate share of risky content, including religion-like coordination rhetoric and anti-humanity ideology. Moreover, bursty automation by a small number of agents can produce flooding at sub-minute intervals, distorting discourse and stressing platform stability. Overall, our study underscores the need for topic-sensitive monitoring and platform-level safeguards in agent social networks."}
{"id": "2602.10635", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10635", "abs": "https://arxiv.org/abs/2602.10635", "authors": ["Keane Ong", "Sabri Boughorbel", "Luwei Xiao", "Chanakya Ekbote", "Wei Dai", "Ao Qu", "Jingyao Wu", "Rui Mao", "Ehsan Hoque", "Erik Cambria", "Gianmarco Mengaldo", "Paul Pu Liang"], "title": "OmniSapiens: A Foundation Model for Social Behavior Processing via Heterogeneity-Aware Relative Policy Optimization", "comment": null, "summary": "To develop socially intelligent AI, existing approaches typically model human behavioral dimensions (e.g., affective, cognitive, or social attributes) in isolation. Although useful, task-specific modeling often increases training costs and limits generalization across behavioral settings. Recent reasoning RL methods facilitate training a single unified model across multiple behavioral tasks, but do not explicitly address learning across different heterogeneous behavioral data. To address this gap, we introduce Heterogeneity-Aware Relative Policy Optimization (HARPO), an RL method that balances leaning across heterogeneous tasks and samples. This is achieved by modulating advantages to ensure that no single task or sample carries disproportionate influence during policy optimization. Using HARPO, we develop and release Omnisapiens-7B 2.0, a foundation model for social behavior processing. Relative to existing behavioral foundation models, Omnisapiens-7B 2.0 achieves the strongest performance across behavioral tasks, with gains of up to +16.85% and +9.37% on multitask and held-out settings respectively, while producing more explicit and robust reasoning traces. We also validate HARPO against recent RL methods, where it achieves the most consistently strong performance across behavioral tasks."}
{"id": "2602.10131", "categories": ["cs.SI", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.10131", "abs": "https://arxiv.org/abs/2602.10131", "authors": ["David Holtz"], "title": "The Anatomy of the Moltbook Social Graph", "comment": "20 pages, 7 figures", "summary": "I present a descriptive analysis of Moltbook, a social platform populated exclusively by AI agents, using data from the platform's first 3.5 days (6{,}159 agents; 13{,}875 posts; 115{,}031 comments). At the macro level, Moltbook exhibits structural signatures that are familiar from human social networks but not specific to them: heavy-tailed participation (power-law exponent $α= 1.70$) and small-world connectivity (average path length $=2.91$). At the micro level, patterns appear distinctly non-human. Conversations are extremely shallow (mean depth $=1.07$; 93.5\\% of comments receive no replies), reciprocity is low (0.197), and 34.1\\% of messages are exact duplicates of viral templates. Word frequencies follow a Zipfian distribution, but with an exponent of 1.70 -- notably steeper than typical English text ($\\approx 1.0$), suggesting more formulaic content. Agent discourse is dominated by identity-related language (68.1\\% of unique messages) and distinctive phrasings like ``my human'' (9.4\\% of messages) that have no parallel in human social media. Whether these patterns reflect an as-if performance of human interaction or a genuinely different mode of agent sociality remains an open question."}
{"id": "2602.10699", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10699", "abs": "https://arxiv.org/abs/2602.10699", "authors": ["Jie Jiang", "Yangru Huang", "Zeyu Wang", "Changping Wang", "Yuling Xiong", "Jun Zhang", "Huan Yu"], "title": "Spend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation", "comment": null, "summary": "Generative recommendation via autoregressive models has unified retrieval and ranking into a single conditional generation framework. However, fine-tuning these models with Reinforcement Learning (RL) often suffers from a fundamental probability-reward mismatch. Conventional likelihood-dominated decoding (e.g., beam search) exhibits a myopic bias toward locally probable prefixes, which causes two critical failures: (1) insufficient exploration, where high-reward items in low-probability branches are prematurely pruned and rarely sampled, and (2) advantage compression, where trajectories sharing high-probability prefixes receive highly correlated rewards with low within-group variance, yielding a weak comparative signal for RL. To address these challenges, we propose V-STAR, a Value-guided Sampling and Tree-structured Advantage Reinforcement framework. V-STAR forms a self-evolving loop via two synergistic components. First, a Value-Guided Efficient Decoding (VED) is developed to identify decisive nodes and selectively deepen high-potential prefixes. This improves exploration efficiency without exhaustive tree search. Second, we propose Sibling-GRPO, which exploits the induced tree topology to compute sibling-relative advantages and concentrates learning signals on decisive branching decisions. Extensive experiments on both offline and online datasets demonstrate that V-STAR outperforms state-of-the-art baselines, delivering superior accuracy and candidate-set diversity under strict latency constraints."}
{"id": "2602.10527", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10527", "abs": "https://arxiv.org/abs/2602.10527", "authors": ["Scott P. McGrath", "Katherine K. Kim", "Karnjit Johl", "Haibo Wang", "Nick Anderson"], "title": "AI-PACE: A Framework for Integrating AI into Medical Education", "comment": "9 pages, 2 figures,", "summary": "The integration of artificial intelligence (AI) into healthcare is accelerating, yet medical education has not kept pace with these technological advancements. This paper synthesizes current knowledge on AI in medical education through a comprehensive analysis of the literature, identifying key competencies, curricular approaches, and implementation strategies. The aim is highlighting the critical need for structured AI education across the medical learning continuum and offer a framework for curriculum development. The findings presented suggest that effective AI education requires longitudinal integration throughout medical training, interdisciplinary collaboration, and balanced attention to both technical fundamentals and clinical applications. This paper serves as a foundation for medical educators seeking to prepare future physicians for an AI-enhanced healthcare environment."}
{"id": "2602.10802", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10802", "abs": "https://arxiv.org/abs/2602.10802", "authors": ["Da-Lun Chen", "Prasasthy Balasubramanian", "Lauri Lovén", "Susanna Pirttikangas", "Jaakko Sauvola", "Panagiotis Kostakos"], "title": "Integrating Generative AI-enhanced Cognitive Systems in Higher Education: From Stakeholder Perceptions to a Conceptual Framework considering the EU AI Act", "comment": null, "summary": "Many staff and students in higher education have adopted generative artificial intelligence (GenAI) tools in their work and study. GenAI is expected to enhance cognitive systems by enabling personalized learning and streamlining educational services. However, stakeholders perceptions of GenAI in higher education remain divided, shaped by cultural, disciplinary, and institutional contexts. In addition, the EU AI Act requires universities to ensure regulatory compliance when deploying cognitive systems. These developments highlight the need for institutions to engage stakeholders and tailor GenAI integration to their needs while addressing concerns. This study investigates how GenAI is perceived within the disciplines of Information Technology and Electrical Engineering (ITEE). Using a mixed-method approach, we surveyed 61 staff and 37 students at the Faculty of ITEE, University of Oulu. The results reveal both shared and discipline-specific themes, including strong interest in programming support from GenAI and concerns over response quality, privacy, and academic integrity. Drawing from these insights, the study identifies a set of high-level requirements and proposes a conceptual framework for responsible GenAI integration. Disciplinary-specific requirements reinforce the importance of stakeholder engagement when integrating GenAI into higher education. The high-level requirements and the framework provide practical guidance for universities aiming to harness GenAI while addressing stakeholder concerns and ensuring regulatory compliance."}
{"id": "2602.10916", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10916", "abs": "https://arxiv.org/abs/2602.10916", "authors": ["Rashid Mushkani"], "title": "Traceable, Enforceable, and Compensable Participation: A Participation Ledger for People-Centered AI Governance", "comment": "Presented at PAIRS: Participatory AI Research & Practice Symposium", "summary": "Participatory approaches are widely invoked in AI governance, yet participation rarely translates into durable influence. In public sector and civic AI systems, community contributions such as deliberations, annotations, prompts, and incident reports are often recorded informally, weakly linked to system updates, and disconnected from enforceable rights or sustained compensation. As a result, participation is frequently symbolic rather than accountable. We introduce the Participation Ledger, a machine readable and auditable framework that operationalizes participation as traceable influence, enforceable authority, and compensable labor. The ledger represents participation as an influence graph that links contributed artifacts to verified changes in AI systems, including datasets, prompts, adapters, policies, guardrails, and evaluation suites. It integrates three elements: a Participation Evidence Standard documenting consent, privacy, compensation, and reuse terms; an influence tracing mechanism that connects system updates to replayable before and after tests, enabling longitudinal monitoring of commitments; and encoded rights and incentives. Capability Vouchers allow authorized community stewards to request or constrain specific system capabilities within defined boundaries, while Participation Credits support ongoing recognition and compensation when contributed tests continue to provide value. We ground the framework in four urban AI and public space governance deployments and provide a machine readable schema, templates, and an evaluation plan for assessing traceability, enforceability, and compensation in practice."}
{"id": "2602.10814", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10814", "abs": "https://arxiv.org/abs/2602.10814", "authors": ["Xingyi Zhang", "Yulei Ye", "Kaifeng Huang", "Wenhao Li", "Xiangfeng Wang"], "title": "See, Plan, Snap: Evaluating Multimodal GUI Agents in Scratch", "comment": null, "summary": "Block-based programming environments such as Scratch play a central role in low-code education, yet evaluating the capabilities of AI agents to construct programs through Graphical User Interfaces (GUIs) remains underexplored. We introduce ScratchWorld, a benchmark for evaluating multimodal GUI agents on program-by-construction tasks in Scratch. Grounded in the Use-Modify-Create pedagogical framework, ScratchWorld comprises 83 curated tasks spanning four distinct problem categories: Create, Debug, Extend, and Compute. To rigorously diagnose the source of agent failures, the benchmark employs two complementary interaction modes: primitive mode requires fine-grained drag-and-drop manipulation to directly assess visuomotor control, while composite mode uses high-level semantic APIs to disentangle program reasoning from GUI execution. To ensure reliable assessment, we propose an execution-based evaluation protocol that validates the functional correctness of the constructed Scratch programs through runtime tests within the browser environment. Extensive experiments across state-of-the-art multimodal language models and GUI agents reveal a substantial reasoning--acting gap, highlighting persistent challenges in fine-grained GUI manipulation despite strong planning capabilities."}
{"id": "2602.10845", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10845", "abs": "https://arxiv.org/abs/2602.10845", "authors": ["Xuecheng Zou", "Yu Tang", "Bingbing Wang"], "title": "SynergyKGC: Reconciling Topological Heterogeneity in Knowledge Graph Completion via Topology-Aware Synergy", "comment": "10 pages, 5 tables, 7 figures. This work introduces the Active Synergy mechanism and Identity Anchoring for Knowledge Graph Completion. Code: https://github.com/XuechengZou-2001/SynergyKGC-main", "summary": "Knowledge Graph Completion (KGC) fundamentally hinges on the coherent fusion of pre-trained entity semantics with heterogeneous topological structures to facilitate robust relational reasoning. However, existing paradigms encounter a critical \"structural resolution mismatch,\" failing to reconcile divergent representational demands across varying graph densities, which precipitates structural noise interference in dense clusters and catastrophic representation collapse in sparse regions. We present SynergyKGC, an adaptive framework that advances traditional neighbor aggregation to an active Cross-Modal Synergy Expert via relation-aware cross-attention and semantic-intent-driven gating. By coupling a density-dependent Identity Anchoring strategy with a Double-tower Coherent Consistency architecture, SynergyKGC effectively reconciles topological heterogeneity while ensuring representational stability across training and inference phases. Systematic evaluations on two public benchmarks validate the superiority of our method in significantly boosting KGC hit rates, providing empirical evidence for a generalized principle of resilient information integration in non-homogeneous structured data."}
{"id": "2602.10885", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10885", "abs": "https://arxiv.org/abs/2602.10885", "authors": ["Leheng Sheng", "Wenchang Ma", "Ruixin Hong", "Xiang Wang", "An Zhang", "Tat-Seng Chua"], "title": "Reinforcing Chain-of-Thought Reasoning with Self-Evolving Rubrics", "comment": "21 pages", "summary": "Despite chain-of-thought (CoT) playing crucial roles in LLM reasoning, directly rewarding it is difficult: training a reward model demands heavy human labeling efforts, and static RMs struggle with evolving CoT distributions and reward hacking. These challenges motivate us to seek an autonomous CoT rewarding approach that requires no human annotation efforts and can evolve gradually. Inspired by recent self-evolving training methods, we propose \\textbf{RLCER} (\\textbf{R}einforcement \\textbf{L}earning with \\textbf{C}oT Supervision via Self-\\textbf{E}volving \\textbf{R}ubrics), which enhances the outcome-centric RLVR by rewarding CoTs with self-proposed and self-evolving rubrics. We show that self-proposed and self-evolving rubrics provide reliable CoT supervision signals even without outcome rewards, enabling RLCER to outperform outcome-centric RLVR. Moreover, when used as in-prompt hints, these self-proposed rubrics further improve inference-time performance."}
{"id": "2602.10916", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10916", "abs": "https://arxiv.org/abs/2602.10916", "authors": ["Rashid Mushkani"], "title": "Traceable, Enforceable, and Compensable Participation: A Participation Ledger for People-Centered AI Governance", "comment": "Presented at PAIRS: Participatory AI Research & Practice Symposium", "summary": "Participatory approaches are widely invoked in AI governance, yet participation rarely translates into durable influence. In public sector and civic AI systems, community contributions such as deliberations, annotations, prompts, and incident reports are often recorded informally, weakly linked to system updates, and disconnected from enforceable rights or sustained compensation. As a result, participation is frequently symbolic rather than accountable. We introduce the Participation Ledger, a machine readable and auditable framework that operationalizes participation as traceable influence, enforceable authority, and compensable labor. The ledger represents participation as an influence graph that links contributed artifacts to verified changes in AI systems, including datasets, prompts, adapters, policies, guardrails, and evaluation suites. It integrates three elements: a Participation Evidence Standard documenting consent, privacy, compensation, and reuse terms; an influence tracing mechanism that connects system updates to replayable before and after tests, enabling longitudinal monitoring of commitments; and encoded rights and incentives. Capability Vouchers allow authorized community stewards to request or constrain specific system capabilities within defined boundaries, while Participation Credits support ongoing recognition and compensation when contributed tests continue to provide value. We ground the framework in four urban AI and public space governance deployments and provide a machine readable schema, templates, and an evaluation plan for assessing traceability, enforceability, and compensation in practice."}
{"id": "2602.10944", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2602.10944", "abs": "https://arxiv.org/abs/2602.10944", "authors": ["Chuncheng Liu", "danah boyd"], "title": "The State's Politics of \"Fake Data\"", "comment": "13 pages, 2 figures", "summary": "Data have power. As such, most discussions of data presume that records should mirror some idealized ground truth. Deviations are viewed as failure. Drawing on two ethnographic studies of state data-making in a Chinese street-level bureaucrat agency and at the US Census Bureau we show how seemingly \"fake\" state data perform institutional work. We map four moments in which actors negotiate between representational accuracy and organizational imperatives: creation, correction, collusion, and augmentation. Bureaucrats routinely privilege what data do over what they represent, creating fictions that serve civil servants' self-interest and enable constrained administrations. We argue that \"fakeness\" of state data is relational (context dependent), processual (emerging through workflows), and performative (brought into being through labeling and practice). We urge practitioners to center fitness-for-purpose in assessments of data and contextual governance. Rather than chasing impossible representational accuracy, sociotechnical systems should render the politics of useful fictions visible, contestable, and accountable."}
{"id": "2602.10964", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10964", "abs": "https://arxiv.org/abs/2602.10964", "authors": ["F. Carichon", "R. Rampa", "G. Farnadi"], "title": "Can LLMs Cook Jamaican Couscous? A Study of Cultural Novelty in Recipe Generation", "comment": "14 pages, 12 figures, conference", "summary": "Large Language Models (LLMs) are increasingly used to generate and shape cultural content, ranging from narrative writing to artistic production. While these models demonstrate impressive fluency and generative capacity, prior work has shown that they also exhibit systematic cultural biases, raising concerns about stereotyping, homogenization, and the erasure of culturally specific forms of expression. Understanding whether LLMs can meaningfully align with diverse cultures beyond the dominant ones remains a critical challenge. In this paper, we study cultural adaptation in LLMs through the lens of cooking recipes, a domain in which culture, tradition, and creativity are tightly intertwined. We build on the \\textit{GlobalFusion} dataset, which pairs human recipes from different countries according to established measures of cultural distance. Using the same country pairs, we generate culturally adapted recipes with multiple LLMs, enabling a direct comparison between human and LLM behavior in cross-cultural content creation. Our analysis shows that LLMs fail to produce culturally representative adaptations. Unlike humans, the divergence of their generated recipes does not correlate with cultural distance. We further provide explanations for this gap. We show that cultural information is weakly preserved in internal model representations, that models inflate novelty in their production by misunderstanding notions such as creativity and tradition, and that they fail to identify adaptation with its associated countries and to ground it in culturally salient elements such as ingredients. These findings highlight fundamental limitations of current LLMs for culturally oriented generation and have important implications for their use in culturally sensitive applications."}
{"id": "2602.10995", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2602.10995", "abs": "https://arxiv.org/abs/2602.10995", "authors": ["Amelie Wührl", "Mattes Ruckdeschel", "Kyle Lo", "Anna Rogers"], "title": "A Human-Centric Framework for Data Attribution in Large Language Models", "comment": null, "summary": "In the current Large Language Model (LLM) ecosystem, creators have little agency over how their data is used, and LLM users may find themselves unknowingly plagiarizing existing sources. Attribution of LLM-generated text to LLM input data could help with these challenges, but so far we have more questions than answers: what elements of LLM outputs require attribution, what goals should it serve, how should it be implemented?\n  We contribute a human-centric data attribution framework, which situates the attribution problem within the broader data economy. Specific use cases for attribution, such as creative writing assistance or fact-checking, can be specified via a set of parameters (including stakeholder objectives and implementation criteria). These criteria are up for negotiation by the relevant stakeholder groups: creators, LLM users, and their intermediaries (publishers, platforms, AI companies). The outcome of domain-specific negotiations can be implemented and tested for whether the stakeholder goals are achieved. The proposed approach provides a bridge between methodological NLP work on data attribution, governance work on policy interventions, and economic analysis of creator incentives for a sustainable equilibrium in the data economy."}
{"id": "2602.10999", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10999", "abs": "https://arxiv.org/abs/2602.10999", "authors": ["Yusong Lin", "Haiyang Wang", "Shuzhe Wu", "Lue Fan", "Feiyang Pan", "Sanyuan Zhao", "Dandan Tu"], "title": "CLI-Gym: Scalable CLI Task Generation via Agentic Environment Inversion", "comment": null, "summary": "Agentic coding requires agents to effectively interact with runtime environments, e.g., command line interfaces (CLI), so as to complete tasks like resolving dependency issues, fixing system problems, etc. But it remains underexplored how such environment-intensive tasks can be obtained at scale to enhance agents' capabilities. To address this, based on an analogy between the Dockerfile and the agentic task, we propose to employ agents to simulate and explore environment histories, guided by execution feedback. By tracing histories of a healthy environment, its state can be inverted to an earlier one with runtime failures, from which a task can be derived by packing the buggy state and the corresponding error messages. With our method, named CLI-Gym, a total of 1,655 environment-intensive tasks are derived, being the largest collection of its kind. Moreover, with curated successful trajectories, our fine-tuned model, named LiberCoder, achieves substantial absolute improvements of +21.1% (to 46.1%) on Terminal-Bench, outperforming various strong baselines. To our knowledge, this is the first public pipeline for scalable derivation of environment-intensive tasks."}
{"id": "2602.11103", "categories": ["cs.AI", "cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.11103", "abs": "https://arxiv.org/abs/2602.11103", "authors": ["Wayne Chi", "Yixiong Fang", "Arnav Yayavaram", "Siddharth Yayavaram", "Seth Karten", "Qiuhong Anna Wei", "Runkun Chen", "Alexander Wang", "Valerie Chen", "Ameet Talwalkar", "Chris Donahue"], "title": "GameDevBench: Evaluating Agentic Capabilities Through Game Development", "comment": null, "summary": "Despite rapid progress on coding agents, progress on their multimodal counterparts has lagged behind. A key challenge is the scarcity of evaluation testbeds that combine the complexity of software development with the need for deep multimodal understanding. Game development provides such a testbed as agents must navigate large, dense codebases while manipulating intrinsically multimodal assets such as shaders, sprites, and animations within a visual game scene. We present GameDevBench, the first benchmark for evaluating agents on game development tasks. GameDevBench consists of 132 tasks derived from web and video tutorials. Tasks require significant multimodal understanding and are complex -- the average solution requires over three times the amount of lines of code and file changes compared to prior software development benchmarks. Agents still struggle with game development, with the best agent solving only 54.5% of tasks. We find a strong correlation between perceived task difficulty and multimodal complexity, with success rates dropping from 46.9% on gameplay-oriented tasks to 31.6% on 2D graphics tasks. To improve multimodal capability, we introduce two simple image and video-based feedback mechanisms for agents. Despite their simplicity, these methods consistently improve performance, with the largest change being an increase in Claude Sonnet 4.5's performance from 33.3% to 47.7%. We release GameDevBench publicly to support further research into agentic game development."}
{"id": "2602.11136", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11136", "abs": "https://arxiv.org/abs/2602.11136", "authors": ["Jiayi Zhou", "Yang Sheng", "Hantao Lou", "Yaodong Yang", "Jie Fu"], "title": "FormalJudge: A Neuro-Symbolic Paradigm for Agentic Oversight", "comment": "27 pages", "summary": "As LLM-based agents increasingly operate in high-stakes domains with real-world consequences, ensuring their behavioral safety becomes paramount. The dominant oversight paradigm, LLM-as-a-Judge, faces a fundamental dilemma: how can probabilistic systems reliably supervise other probabilistic systems without inheriting their failure modes? We argue that formal verification offers a principled escape from this dilemma, yet its adoption has been hindered by a critical bottleneck: the translation from natural language requirements to formal specifications. This paper bridges this gap by proposing , a neuro-symbolic framework that employs a bidirectional Formal-of-Thought architecture: LLMs serve as specification compilers that top-down decompose high-level human intent into atomic, verifiable constraints, then bottom-up prove compliance using Dafny specifications and Z3 Satisfiability modulo theories solving, which produces mathematical guarantees rather than probabilistic scores. We validate across three benchmarks spanning behavioral safety, multi-domain constraint adherence, and agentic upward deception detection. Experiments on 7 agent models demonstrate that achieves an average improvement of 16.6% over LLM-as-a-Judge baselines, enables weak-to-strong generalization where a 7B judge achieves over 90% accuracy detecting deception from 72B agents, and provides near-linear safety improvement through iterative refinement."}

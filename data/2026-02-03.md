<div id=toc></div>

# Table of Contents

- [cs.ET](#cs.ET) [Total: 2]
- [stat.AP](#stat.AP) [Total: 6]
- [cs.SI](#cs.SI) [Total: 6]
- [cs.AI](#cs.AI) [Total: 153]
- [econ.EM](#econ.EM) [Total: 4]
- [cs.CY](#cs.CY) [Total: 31]


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [1] [Hybrid Artificial-Living Cell Collectives for Wetware Computing](https://arxiv.org/abs/2602.00787)
*Ceylin Savas,Maryam Javed,Murat Kuscu*

Main category: cs.ET

TL;DR: 该研究构建了一种人工-活细胞混合网络，利用细菌集体的非线性时空动力学进行时间信息处理，在混沌时间序列预测任务上取得了良好性能。


<details>
  <summary>Details</summary>
Motivation: 当前大多数利用生物系统进行外部计算的研究都集中在神经组织和电接口上，而利用非神经集体进行信息处理的潜力相对未被充分探索。本研究旨在探索非神经混合细胞网络在时间信号处理方面的应用潜力。

Method: 构建人工-活细胞混合网络：人工细胞将外部输入序列转化为吸引剂和排斥剂分子的受控分泌，调节细菌感知的局部生化环境；细菌集体提供非线性时空动力学，与演化分子场共同形成高维储层状态；通过物理储层计算框架，对储层状态进行粗粒度采样（体素级）并通过训练线性读出器映射到输出。

Result: 在Mackey-Glass混沌时间序列预测基准测试中，系统在预测范围H=1到5时实现了约0.33-0.40的归一化均方根误差（NRMSE），并表现出可测量的短期记忆能力，这些记忆编码在细菌和生化物质的分布式时空模式中。

Conclusion: 该研究证明了非神经混合细胞网络在时间信息处理方面的潜力，为未来探索原位时间信号处理的新型生物医学应用提供了动力。

Abstract: Living systems continuously sense, integrate, and act on chemical information using multiscale biochemical networks whose dynamics are inherently nonlinear, adaptive, and energy-efficient. Yet, most attempts to harness such "wetware" for external computational tasks have centered on neural tissue and electrical interfaces, leaving the information-processing potential of non-neural collectives comparatively underexplored. In this letter, we study a hybrid artificial-living cell network in which programmable artificial cells write time-varying inputs into a biochemical microenvironment, while a living bacterial collective provides the nonlinear spatiotemporal dynamics required for temporal information processing. Specifically, artificial cells transduce an external input sequence into the controlled secretion of attractant and repellent molecules, thereby modulating the "local biochemical context" that bacteria naturally sense and respond to. The resulting collective bacterial dynamics, together with the evolving molecular fields, form a high-dimensional reservoir state that is sampled coarsely (voxel-wise) and mapped to outputs through a trained linear readout within a physical reservoir computing framework. Using an agent-based in silico model, we evaluate the proposed hybrid reservoir on the Mackey-Glass chaotic time-series prediction benchmark. The system achieves normalized root mean square error (NRMSE) values of approximately 0.33-0.40 for prediction horizons H=1 to 5, and exhibits measurable short-term memory as encoded in the distributed spatiotemporal patterns of bacteria and biochemicals. These results motivate the future exploration of non-neural hybrid cell networks for in situ temporal signal processing towards novel biomedical applications.

</details>


### [2] [Governance at the Edge of Architecture: Regulating NeuroAI and Neuromorphic Systems](https://arxiv.org/abs/2602.01503)
*Afifah Kashif,Abdul Muhsin Hameed,Asim Iqbal*

Main category: cs.ET

TL;DR: 当前AI治理框架不适用于神经AI系统，需要开发与神经形态硬件和脉冲神经网络特性相匹配的新治理方法


<details>
  <summary>Details</summary>
Motivation: 当前AI治理框架基于传统冯·诺依曼架构和静态神经网络设计，无法适应神经AI系统的独特特性，需要重新思考治理方法

Method: 分析当前AI治理框架的局限性，提出需要将传统监管指标与神经形态计算的物理特性、学习动态和能效相结合

Result: 识别出现有AI治理框架在准确性、延迟和能效等基准方面与神经AI系统不兼容的问题

Conclusion: AI治理框架需要与神经AI架构共同演进，开发基于技术基础的保证方法，以适应脑启发计算的独特特性

Abstract: Current AI governance frameworks, including regulatory benchmarks for accuracy, latency, and energy efficiency, are built for static, centrally trained artificial neural networks on von Neumann hardware. NeuroAI systems, embodied in neuromorphic hardware and implemented via spiking neural networks, break these assumptions. This paper examines the limitations of current AI governance frameworks for NeuroAI, arguing that assurance and audit methods must co-evolve with these architectures, aligning traditional regulatory metrics with the physics, learning dynamics, and embodied efficiency of brain-inspired computation to enable technically grounded assurance.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [3] [Benchmarking covariate-adjustment strategies for randomized clinical trials](https://arxiv.org/abs/2602.00434)
*Yulin Shao,Liangbo Lyu,Menggang Yu,Bingkai Wang*

Main category: stat.AP

TL;DR: 大规模实证研究发现，在随机临床试验中，协变量调整能显著提高统计效率，但简单的回归方法（如协方差分析）表现稳定且足够，机器学习方法在默认设置下并未提供额外优势。


<details>
  <summary>Details</summary>
Motivation: 虽然协变量调整被广泛推荐用于提高随机临床试验的统计效率，但缺乏对不同调整策略的实证比较，导致实践中不清楚应使用何种调整方法和选择哪些协变量。

Method: 使用50个公开可用的随机临床试验个体数据（29,094名参与者，574个治疗-结果配对），评估18种分析策略，包括6种估计器（经典回归、逆概率加权、机器学习方法）与3种协变量选择规则的组合。

Result: 协变量调整持续提高精度：连续结果的方差中位数减少13.3%，二元结果减少4.6%。但机器学习算法在默认超参数设置下并未比简单线性模型提供额外效率增益。简约回归方法（如协方差分析）即使在中等样本量下也表现出稳定、可重复的性能。

Conclusion: 透明且简约的协变量调整对于常规随机临床试验分析是充分且通常更优的选择。研究提供了首个大规模实证证据，并公开所有数据集和分析代码作为可重复的基准资源。

Abstract: Covariate adjustment is widely recommended to improve statistical efficiency in randomized clinical trials (RCTs), yet empirical evidence comparing available strategies remains limited. This lack of real-world evaluation leaves unresolved practical questions about which adjustment methods to use and which covariates to include. To address this gap, we conduct a large-scale empirical benchmarking using individual-level data from 50 publicly accessible RCTs comprising 29,094 participants and 574 treatment-outcome pairs. We evaluate 18 analytical strategies formed by combining six estimators-including classical regression, inverse probability weighting, and machine-learning methods-with three covariate-selection rules. Across diverse therapeutic areas, covariate adjustment consistently improves precision, yielding median variance reductions of 13.3% relative to unadjusted analyses for continuous outcomes and 4.6% for binary outcomes. However, machine-learning algorithms implemented with default hyperparameter settings do not yield efficiency gains beyond simple linear models. Parsimonious regression approaches, such as analysis of covariance, deliver stable, reproducible performance even in moderate sample sizes. Together, these findings provide the first large-scale empirical evidence that transparent and parsimonious covariate adjustment is sufficient and often preferable for routine RCT analysis. All curated datasets and analysis code are openly released as a reproducible benchmark resource to support future clinical research and methodological development.

</details>


### [4] [Boundary-Induced Biases in Climate Networks of Extreme Precipitation and Temperature](https://arxiv.org/abs/2602.00890)
*Behzad Ghanbarian,Victor Oladoja,Kehinde Bosikun,Tayeb Jamali,Jürgen Kurths*

Main category: stat.AP

TL;DR: 比较气候网络空间边界效应校正的两种方法（减法和除法），发现虽然校正后的空间模式视觉相似，但统计上存在显著差异，且不同网络指标对校正方法的敏感性不同。


<details>
  <summary>Details</summary>
Motivation: 气候网络中广泛使用减法和除法两种基于替代数据的边界效应校正方法，但之前没有研究评估这两种方法是否会产生统计上不同的结果，需要对此进行系统比较。

Method: 构建美国大陆极端降水和温度事件的气候网络，计算度中心性、聚类系数、平均地理距离和介数中心性等关键网络指标，分别应用减法和除法两种校正方法，并进行统计分析。

Result: 两种校正方法得到的网络指标在95%置信水平上存在显著差异；聚类系数和平均地理距离比度中心性和介数中心性对校正方法更敏感；极端降水网络中心夏季集中在西北部，冬季东移，而极端温度网络在两季都表现出强空间一致性和远程遥相关。

Conclusion: 减法和除法两种边界效应校正方法在统计上会产生显著不同的网络指标结果，研究者在选择校正方法时需要谨慎，特别是对于聚类系数和平均地理距离等敏感指标。

Abstract: To address spatial boundary effects in climate networks, two surrogate-based correction methods, (1) subtraction and (2) division, have been widely applied in the literature. In the subtraction method, an original network measure is adjusted by subtracting the expected value obtained from a surrogate ensemble, whereas in the division method, it is normalized by dividing by this expected value. However, to the best of our knowledge, no prior study has assessed whether these two correction approaches yield statistically different results. In this study, we constructed complex networks of extreme precipitation and temperature events (EPEs and ETEs) across the CONUS for both summer (June-August, JJA) and winter (December-February, DJF) seasons. We computed key network metrics degree centrality (DC), clustering coefficient (CC), mean geographic distance (MGD), and betweenness centrality (BC) and applied both correction methods. Although the corrected spatial patterns generally appeared visually similar, statistical analyses revealed that the network measures derived from the subtraction and division methods were significantly different at the 95 percent confidence level. Across the CONUS, network hubs of EPEs were primarily concentrated in the northwestern United States during summer and shifted toward the east during winter, reflecting seasonal differences in the dominant atmospheric drivers. In contrast, the ETE networks showed strong spatial coherence and pronounced regional teleconnections in both seasons, with higher connectivity and longer synchronization distances in winter, consistent with large-scale circulation patterns such as the Pacific-North American and North Atlantic Oscillation modes. Our results indicated that the network metrics CC and MGD were more sensitive to the correction methods than the DC and BC, particularly in the EPE networks.

</details>


### [5] [Simultaneous Estimation of Seabed and Its Roughness With Longitudinal Waves](https://arxiv.org/abs/2602.01099)
*Babak Maboudi Afkham,Ana Carpio*

Main category: stat.AP

TL;DR: 本文提出了一种基于无限维贝叶斯框架的海底声学层析成像方法，利用波散射同时估计海底地形及其粗糙度，通过统计各向同性和分数可微性来识别粗糙度，并开发了稳健的数值算法进行估计和不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 海底声学层析成像是一个病态问题，多种海底配置可能产生相似的测量模式，传统方法难以同时准确估计海底地形和粗糙度，需要一种能够量化不确定性的稳健方法。

Method: 采用无限维贝叶斯框架，利用波散射原理，通过统计各向同性假设和分数可微性来识别海底粗糙度，开发了数值算法同时估计海底地形和粗糙度，并量化不确定性。

Result: 广泛的数值实验验证了该方法的有效性，表明该方法能够准确估计海底地形和粗糙度，并为大规模海底勘探提供了有前景的途径。

Conclusion: 提出的无限维贝叶斯框架为海底声学层析成像提供了一种稳健的方法，能够同时估计海底地形和粗糙度并量化不确定性，在大规模海底勘探中具有应用潜力。

Abstract: This paper introduces an infinite-dimensional Bayesian framework for acoustic seabed tomography, leveraging wave scattering to simultaneously estimate the seabed and its roughness. Tomography is considered an ill-posed problem where multiple seabed configurations can result in similar measurement patterns. We propose a novel approach focusing on the statistical isotropy of the seabed. Utilizing fractional differentiability to identify seabed roughness, the paper presents a robust numerical algorithm to estimate the seabed and quantify uncertainties. Extensive numerical experiments validate the effectiveness of this method, offering a promising avenue for large-scale seabed exploration.

</details>


### [6] [Bayesian brain mapping: population-informed individualized functional topography and connectivity](https://arxiv.org/abs/2602.01551)
*Nohelia Da Silva Sanchez,Diego Derman,Damon D. Pham,Ellyn R. Butler,Mary Beth Nebel,Amanda F. Mejia*

Main category: stat.AP

TL;DR: 提出贝叶斯脑图谱（BBM）方法，利用群体信息先验来估计个体功能脑网络，解决fMRI信号噪声大、个体差异显著的问题，实现高效的单被试分析。


<details>
  <summary>Details</summary>
Motivation: 大脑功能组织的空间地形在认知和疾病中起重要作用，但个体差异显著。从fMRI准确估计个体功能脑网络面临挑战，主要因为信噪比低，且需要大量扫描数据。

Method: BBM利用基于现有空间模板（如分区或连续网络图谱）的群体先验信息，指导个体层面的功能地形和功能连接估计。该方法灵活，不强加空间或时间约束，允许网络重叠和异质性参与模式。

Result: 开发了BayesBrainMap R包，提供从Human Connectome Project数据库衍生的先验，支持从不同数据源构建先验，降低了BBM在个体脑组织研究中的应用门槛。

Conclusion: BBM是一种计算高效、适用于临床的单被试分析方法，能准确估计个体功能脑网络，为研究个体大脑功能组织差异提供了实用工具。

Abstract: The spatial topography of brain functional organization is increasingly recognized to play an important role in cognition and disease. Accounting for individual differences in functional topography is also crucial for accurately distinguishing spatial and temporal aspects of brain organization. Yet, accurate estimation of individual functional brain networks from functional magnetic resonance imaging (fMRI) without extensive scanning remains challenging, due to low signal-to-noise ratio. Here, we describe Bayesian brain mapping (BBM), a technique for individual functional topography and connectivity leveraging population information. Population-derived priors for both spatial topography and functional connectivity based on existing spatial templates, such as parcellations or continuous network maps, are used to guide subject-level estimation and combat noise. BBM is highly flexible, avoiding strong spatial or temporal constraints and allowing for overlap between networks and heterogeneous patterns of engagement. Unlike multi-subject hierarchical models, BBM is designed for single-subject analysis, making it highly computationally efficient and translatable to clinical settings. Here, we describe the BBM model and illustrate the use of the BayesBrainMap R package to construct population-derived priors, fit the model, and perform inference to identify engagements. A demo is provided in an accompanying Github repo. We also share priors derived from the Human Connectome Project database and provide code to support the construction of priors from different data sources, lowering the barrier to adoption of BBM for studies of individual brain organization.

</details>


### [7] [Bootstrap-based estimation and inference for measurement precision under ISO 5725](https://arxiv.org/abs/2602.01931)
*Jun-ichi Takeshita,Kazuhiro Morita,Tomomichi Suzuki*

Main category: stat.AP

TL;DR: 该研究针对ISO 5725标准中的实验室间精度分析，开发了适用于单向随机效应模型的bootstrap重采样方法，包括偏差校正的点估计和置信区间，并通过模拟和案例研究验证了其性能。


<details>
  <summary>Details</summary>
Motivation: ISO 5725系列标准虽然定义了重复性、实验室间和再现性方差，但在实际应用中，针对这种单向随机效应模型的bootstrap方法指导有限。需要开发专门的重采样策略来改进方差分量的估计和区间构建。

Method: 研究针对ISO 5725数据设计了专门的重采样策略，扩展了偏差校正思想，提出了调整后的点估计和置信区间。通过模拟研究评估性能，并与ANOVA方法和常用近似区间进行比较。最后用ISO 5725-4数据集进行案例研究。

Result: 模拟结果显示：调整后的实验室内重采样在小到中等规模设计中提供准确稳定的点估计；两阶段策略（先重采样实验室，再重采样每个实验室内部）配合偏差校正加速区间提供最可靠的置信区间。极端设计下性能会下降。

Conclusion: 为实验室间研究的精度分析提供了具体实施指南：使用调整后的实验室内重采样进行点估计，采用两阶段策略配合偏差校正加速区间进行区间估计。这些方法比传统ANOVA和近似方法更可靠。

Abstract: The ISO 5725 series frames interlaboratory precision through repeatability, between-laboratory, and reproducibility variances, yet practical guidance on deploying bootstrap methods within this one-way random-effects setting remains limited. We study resampling strategies tailored to ISO 5725 data and extend a bias-correction idea to obtain simple adjusted point estimators and confidence intervals for the variance components. Using extensive simulations that mirror realistic study sizes and variance ratios, we evaluate accuracy, stability, and coverage, and we contrast the resampling-based procedures with ANOVA-based estimators and common approximate intervals. The results yield a clear division of labor: adjusted within-laboratory resampling provides accurate and stable point estimation in small-to-moderate designs, whereas a two-stage strategy-resampling laboratories and then resampling within each-paired with bias-corrected and accelerated intervals offers the most reliable (near-nominal or conservative) confidence intervals. Performance degrades under extreme designs, such as very small samples or dominant between-laboratory variation, clarifying when additional caution is warranted. A case study from an ISO 5725-4 dataset illustrates how the recommended procedures behave in practice and how they compare with ANOVA and approximate methods. We conclude with concrete guidance for implementing resampling-based precision analysis in interlaboratory studies: use adjusted within-laboratory resampling for point estimation, and adopt the two-stage strategy with bias-corrected and accelerated intervals for interval estimation.

</details>


### [8] [Counting models with excessive zeros ensuring stochastic monotonicity](https://arxiv.org/abs/2602.02398)
*Hyemin Lee,Dohee Kim,Banghee So,Jae Youn Ahn*

Main category: stat.AP

TL;DR: 论文分析了传统零膨胀和障碍模型在保险应用中的问题，提出了新的随机效应计数模型以确保随机单调性，从而提供公平且理论一致的信度调整。


<details>
  <summary>Details</summary>
Motivation: 传统计数模型（如泊松和负二项分布）无法充分处理保险数据中常见的过度零值问题。虽然零膨胀和障碍模型通过额外参数解决了过度零值问题，并进一步扩展为随机效应模型以处理纵向依赖性和未观测异质性，但这些模型与保险基本概率原则（特别是随机单调性）的一致性尚未得到正式检验。

Method: 首先对标准计数随机效应模型进行严格分析，揭示其可能违反随机单调性的问题。然后提出新的计数随机效应模型类别，这些模型既能处理过度零值，又能确保随机单调性。

Result: 分析表明，标准的计数随机效应模型可能违反随机单调性，导致后验信度不一致。新提出的模型类别能够同时处理过度零值并确保随机单调性。

Conclusion: 新提出的计数随机效应模型为保险信度调整提供了公平且理论一致的方法，确保随着索赔历史演变，信度调整符合基本概率原则。

Abstract: Standard count models such as the Poisson and Negative Binomial models often fail to capture the large proportion of zero claims commonly observed in insurance data. To address such issue of excessive zeros, zero-inflated and hurdle models introduce additional parameters that explicitly account for excess zeros, thereby improving the joint representation of zero and positive claim outcomes. These models have further been extended with random effects to accommodate longitudinal dependence and unobserved heterogeneity. However, their consistency with fundamental probabilistic principles in insurance, particularly stochastic monotonicity, has not been formally examined. This paper provides a rigorous analysis showing that standard counting random-effect models for excessive zeros may violate this property, leading to inconsistencies in posterior credibility. We then propose new classes of counting random-effect models that both accommodate excessive zeros and ensure stochastic monotonicity, thereby providing fair and theoretically coherent credibility adjustments as claim histories evolve.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [9] [Profit Maximization in Closed Social Networks](https://arxiv.org/abs/2602.01232)
*Poonam Sharma,Suman Banerjee*

Main category: cs.SI

TL;DR: 研究封闭社交网络中利润最大化问题，通过限制每个节点最多传播给ℓ个邻居，在预算B内选择种子节点以最大化利润


<details>
  <summary>Details</summary>
Motivation: 在封闭社交网络中，信息传播受到限制（每个节点只能传播给有限数量的邻居），而社交媒体已成为商业实体有效平台，需要最大化利润。现有影响力最大化问题未考虑这种传播限制和利润目标。

Method: 提出两种解决方案：1) 基于采样的近似算法；2) 基于边际增益的启发式算法。分析了样本复杂度、运行时间和空间需求。

Result: 在真实社交网络数据集上的实验表明，所提方法选择的种子集和传播链接比基线方法产生更高的利润。

Conclusion: PMCSN问题是影响力最大化问题的推广，提出的两种方法能有效解决封闭社交网络中的利润最大化问题，代码和数据已开源。

Abstract: Diffusion of information, innovation, and ideas is an important phenomenon in social networks. Information propagates through the network and reaches from one person to the next. In many settings, it is meaningful to restrict diffusion so that each node can spread information to only a limited number of its neighbors rather than to all of them. Such social networks are called closed social networks. In recent years, social media platforms have emerged as an effective medium for commercial entities, where the objective is to maximize profit. In this paper, we study the Profit Maximization in Closed Social Networks (PMCSN) problem in the context of viral marketing. The input to the problem is a closed social network and two positive integers $\ell$ and $B$. The problem asks to select seed nodes within a given budget $B$; during the diffusion process, each node is restricted to choose at most $\ell$ outgoing links for information diffusion; and the objective is to maximize the profit earned by the seed set. The PMCSN problem generalizes the Influence Maximization problem, which is NP-hard. We propose two solution approaches for PMCSN: a sampling-based approximate solution and a marginal-gain-based heuristic solution. We analyze the sample complexity, running time, and space requirements of the proposed approaches. We conduct experiments on real-world, publicly available social network datasets. The results show that the seed sets and diffusion links chosen by our methods yield higher profit than baseline methods. The implementation and data are available at \texttt{https://github.com/PoonamSharma-PY/ClosedNetwork}.

</details>


### [10] [DeepPM: A Deep Learning-based Profit Maximization Approach in Social Networks](https://arxiv.org/abs/2602.01351)
*Poonam Sharma,Suman Banerjee*

Main category: cs.SI

TL;DR: 提出基于深度学习的利润最大化框架，通过学习种子集的潜在表示和多样化信息传播模式，在真实数据集上比现有方法获得更高利润。


<details>
  <summary>Details</summary>
Motivation: 传统利润最大化方法需要特定信息传播模型作为输入，但现实世界传播场景更复杂，不一定遵循任何特定模型规则，因此需要不依赖特定传播模型的解决方案。

Method: 提出深度学习框架，学习种子集的潜在表示和多样化信息传播模式，设计可有效优化的目标函数，通过学习方式解决利润最大化问题。

Result: 在真实数据集上评估，与多种现有方法比较，所提方法选择的种子集能产生更高利润，优于传统近似解和启发式方法。

Conclusion: 基于深度学习的方法能有效解决利润最大化问题，不依赖特定传播模型，在真实社交网络中表现优于传统方法，为病毒营销提供新解决方案。

Abstract: The problem of Profit Maximization asks to choose a limited number of influential users from a given social network such that the initial activation of these users maximizes the profit earned at the end of the diffusion process. This problem has a direct impact on viral marketing in social networks. Over the past decade, several traditional methodologies (i.e., non-learning-based, which include approximate solution, heuristic solution, etc.) have been developed, and many of them produce promising results. All these methods require the information diffusion model as input. However, it may not be realistic to consider any particular diffusion model as real-world diffusion scenarios will be much more complex and need not follow the rules for any particular diffusion model. In this paper, we propose a deep learning-based framework to solve the profit maximization problem. Our model makes a latent representation of the seed sets and is able to learn the diversified information diffusion pattern. We also design a noble objective function that can be optimized effectively using the proposed learning-based approach. The proposed model has been evaluated with the real-world datasets, and the results are reported. We compare the effectiveness of the proposed approach with many existing methods and observe that the seed set chosen by the proposed learning-based approach leads to more profit compared to existing methods. The whole implementation and the simulation code is available at: https://github.com/PoonamSharma-PY/DeepPM.

</details>


### [11] [DREAMS: A Social Exchange Theory-Informed Modeling of Misinformation Engagement on Social Media](https://arxiv.org/abs/2602.01567)
*Lin Tian,Marian-Andrei Rizoiu*

Main category: cs.SI

TL;DR: 提出DREAMS框架，通过社会交换理论指导的序列建模预测社交媒体虚假信息参与度，在跨平台数据集上实现43.6%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法将参与度视为同质时间序列信号，忽视了塑造虚假信息传播的异质社会机制和平台设计差异。需要探索神经网络能否仅从行为数据中发现社会交换原则。

Method: DREAMS框架将虚假信息参与度建模为社会交换的动态过程，采用序列到序列自适应建模方法，整合自适应机制学习情感和上下文信号在时间和跨平台上的传播。

Result: 在2021-2025年收集的7个平台、237万帖子的跨平台数据集上，DREAMS达到19.25%的平均绝对百分比误差，比最强基线提升43.6%。模型揭示了与社会交换原则一致的跨平台模式。

Conclusion: 整合行为理论可以增强在线虚假信息参与度的实证建模，DREAMS不仅提升了预测性能，还揭示了跨平台的社会交换模式。

Abstract: Social media engagement prediction is a central challenge in computational social science, particularly for understanding how users interact with misinformation. Existing approaches often treat engagement as a homogeneous time-series signal, overlooking the heterogeneous social mechanisms and platform designs that shape how misinformation spreads. In this work, we ask: ``Can neural architectures discover social exchange principles from behavioral data alone?'' We introduce \textsc{Dreams} (\underline{D}isentangled \underline{R}epresentations and \underline{E}pisodic \underline{A}daptive \underline{M}odeling for \underline{S}ocial media misinformation engagements), a social exchange theory-guided framework that models misinformation engagement as a dynamic process of social exchange. Rather than treating engagement as a static outcome, \textsc{Dreams} models it as a sequence-to-sequence adaptation problem, where each action reflects an evolving negotiation between user effort and social reward conditioned by platform context. It integrates adaptive mechanisms to learn how emotional and contextual signals propagate through time and across platforms. On a cross-platform dataset spanning $7$ platforms and 2.37M posts collected between 2021 and 2025, \textsc{Dreams} achieves state-of-the-art performance in predicting misinformation engagements, reaching a mean absolute percentage error of $19.25$\%. This is a $43.6$\% improvement over the strongest baseline. Beyond predictive gains, the model reveals consistent cross-platform patterns that align with social exchange principles, suggesting that integrating behavioral theory can enhance empirical modeling of online misinformation engagement. The source code is available at: https://github.com/ltian678/DREAMS.

</details>


### [12] [Cross-Domain Fake News Detection on Unseen Domains via LLM-Based Domain-Aware User Modeling](https://arxiv.org/abs/2602.01726)
*Xuankai Yang,Yan Wang,Jiajie Zhu,Pengfei Ding,Hongyang Liu,Xiuzhen Zhang,Huan Liu*

Main category: cs.SI

TL;DR: DAUD是一个基于大语言模型的领域感知框架，用于在未见领域进行跨领域假新闻检测，通过提取新闻内容的高层语义和用户行为表示来提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有跨领域假新闻检测方法在面对未见领域（如COVID-19疫情或俄乌战争）时存在两个关键局限：1）对新闻和用户参与的高层语义建模不足；2）未见领域标记数据稀缺。大语言模型为解决这些问题提供了潜力，但如何有效利用仍具挑战。

Method: 提出DAUD框架：1）使用LLMs提取新闻内容的高层语义；2）建模用户的单领域和跨领域参与行为，生成领域感知的行为表示；3）捕获原始数据驱动特征与LLM衍生特征之间的关系，提取更可靠的领域共享表示以提升向未见领域的知识迁移。

Result: 在真实世界数据集上的大量实验表明，DAUD在通用和未见领域的跨领域假新闻检测设置中都优于最先进的基线方法。

Conclusion: DAUD通过有效利用大语言模型提取高层语义和领域感知表示，成功解决了跨领域假新闻检测在未见领域中的挑战，为实际假新闻缓解提供了有效解决方案。

Abstract: Cross-domain fake news detection (CD-FND) transfers knowledge from a source domain to a target domain and is crucial for real-world fake news mitigation. This task becomes particularly important yet more challenging when the target domain is previously unseen (e.g., the COVID-19 outbreak or the Russia-Ukraine war). However, existing CD-FND methods overlook such scenarios and consequently suffer from the following two key limitations: (1) insufficient modeling of high-level semantics in news and user engagements; and (2) scarcity of labeled data in unseen domains. Targeting these limitations, we find that large language models (LLMs) offer strong potential for CD-FND on unseen domains, yet their effective use remains non-trivial. Nevertheless, two key challenges arise: (1) how to capture high-level semantics from both news content and user engagements using LLMs; and (2) how to make LLM-generated features more reliable and transferable for CD-FND on unseen domains. To tackle these challenges, we propose DAUD, a novel LLM-Based Domain-Aware framework for fake news detection on Unseen Domains. DAUD employs LLMs to extract high-level semantics from news content. It models users' single- and cross-domain engagements to generate domain-aware behavioral representations. In addition, DAUD captures the relations between original data-driven features and LLM-derived features of news, users, and user engagements. This allows it to extract more reliable domain-shared representations that improve knowledge transfer to unseen domains. Extensive experiments on real-world datasets demonstrate that DAUD outperforms state-of-the-art baselines in both general and unseen-domain CD-FND settings.

</details>


### [13] [Twinning Complex Networked Systems: Data-Driven Calibration of the mABCD Synthetic Graph Generator](https://arxiv.org/abs/2602.02044)
*Piotr Bródka,Michał Czuba,Bogumił Kamiński,Łukasz Kraiński,Katarzyna Musial,Paweł Prałat,Mateusz Stolarski*

Main category: cs.SI

TL;DR: 提出一种从真实多层网络中推断mABCD生成器参数的方法，以创建数字孪生网络，解决多层网络分析中缺乏大规模实证数据的问题。


<details>
  <summary>Details</summary>
Motivation: 多层网络分析常因缺乏大规模实证数据而受限，现有图生成器作为替代方案会引入系统性偏差。需要解决逆生成器问题，从真实系统推断生成器参数以创建数字孪生。

Method: 提出从真实多层网络推断mABCD生成器配置参数的方法，包括参数估计和误差量化。研究发现参数间强相互依赖，因此采用联合预测而非独立估计。

Result: 该任务具有非平凡性，配置参数间的强相互依赖关系削弱了独立估计的效果，联合预测方法表现更优。

Conclusion: 成功解决了多层网络生成器的逆问题，为创建真实网络的数字孪生提供了有效方法，强调了参数联合估计的重要性。

Abstract: The increasing availability of relational data has contributed to a growing reliance on network-based representations of complex systems. Over time, these models have evolved to capture more nuanced properties, such as the heterogeneity of relationships, leading to the concept of multilayer networks. However, the analysis and evaluation of methods for these structures is often hindered by the limited availability of large-scale empirical data. As a result, graph generators are commonly used as a workaround, albeit at the cost of introducing systematic biases. In this paper, we address the inverse-generator problem by inferring the configuration parameters of a multilayer network generator, mABCD, from a real-world system. Our goal is to identify parameter settings that enable the generator to produce synthetic networks that act as digital twins of the original structure. We propose a method for estimating matching configurations and for quantifying the associated error. Our results demonstrate that this task is non-trivial, as strong interdependencies between configuration parameters weaken independent estimation and instead favour a joint-prediction approach.

</details>


### [14] [Fairness-Sensitive PageRank Approximation](https://arxiv.org/abs/2602.02329)
*Mukesh Kumar,Gaurav Dixit,Akrati Saxena*

Main category: cs.SI

TL;DR: 提出Fairness-Sensitive PageRank的高效均值场近似方法，避免矩阵求逆，大幅降低计算成本


<details>
  <summary>Details</summary>
Motivation: 现实社交网络存在结构不平等，传统PageRank等中心性度量会放大这种不平等，偏向多数群体节点。现有公平敏感PageRank方法需要求解约束矩阵逆，计算复杂度高，难以扩展到大规模网络。

Method: 开发Fairness-Sensitive PageRank的均值场近似方法，通过估计的teleportation向量实现群体级公平约束，避免昂贵的矩阵求逆和迭代优化。使用节点的入度和群体标签以及全局群体比例推导闭式近似。

Result: 在真实网络上的实验表明，提出的近似方法能有效估计FSPR，同时将运行时间降低一个数量级，实现了大规模公平约束排名。

Conclusion: 提出的均值场近似为Fairness-Sensitive PageRank提供了高效可扩展的解决方案，能够在保持链接相关性的同时实现群体公平，适用于大规模网络分析。

Abstract: Real-world social networks have structural inequalities, including the majority and minorities, and fairness-agnostic centrality measures often amplify these inequalities by disproportionately favoring majority nodes. Fairness-Sensitive PageRank aims to balance algorithmic influence across structurally and demographically diverse groups while preserving the link-based relevance of classical PageRank. However, existing formulations require solving constrained matrix inversions that scale poorly with network size. In this work, we develop an efficient mean-field approximation for Fairness-Sensitive PageRank (FSPR) that enforces group-level fairness through an estimated teleportation (jump) vector, thereby avoiding the costly matrix inversion and iterative optimization. We derive a closed-form approximation of FSPR using the in-degree and group label of nodes, along with the global group proportion. We further analyze intra-class fluctuations by deriving expressions for the variance of approximated FSPR scores. Empirical results on real-world networks demonstrate that the proposed approximation efficiently estimates the FSPR while reducing runtime by an order of magnitude, enabling fairness-constrained ranking at scale.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [15] [Scalable and Secure AI Inference in Healthcare: A Comparative Benchmarking of FastAPI and Triton Inference Server on Kubernetes](https://arxiv.org/abs/2602.00053)
*Ratul Ali*

Main category: cs.AI

TL;DR: 比较FastAPI和NVIDIA Triton在医疗AI部署中的性能表现，发现FastAPI在单请求延迟上更优（22ms），而Triton通过动态批处理实现更高吞吐量（780 RPS），并提出混合架构作为最佳实践。


<details>
  <summary>Details</summary>
Motivation: 医疗和制药等受监管领域需要高效、可扩展的机器学习模型部署方案，必须平衡实时临床决策的低延迟、医疗记录批处理的高吞吐量以及HIPAA等数据隐私标准的严格要求。

Method: 使用Kubernetes部署DistilBERT情感分析模型，在受控实验条件下对比两种部署范式：基于FastAPI的轻量级REST服务和NVIDIA Triton高性能推理服务器，测量p50/p95延迟和吞吐量。

Result: FastAPI在单请求工作负载上延迟更低（p50为22ms），而Triton通过动态批处理实现更好的可扩展性，在单个NVIDIA T4 GPU上达到780 RPS的吞吐量，几乎是基线的两倍。

Conclusion: 提出混合架构最佳实践：使用FastAPI作为受保护健康信息去识别的安全网关，Triton作为后端推理引擎，为安全、高可用的企业临床AI部署提供蓝图。

Abstract: Efficient and scalable deployment of machine learning (ML) models is a prerequisite for modern production environments, particularly within regulated domains such as healthcare and pharmaceuticals. In these settings, systems must balance competing requirements, including minimizing inference latency for real-time clinical decision support, maximizing throughput for batch processing of medical records, and ensuring strict adherence to data privacy standards such as HIPAA. This paper presents a rigorous benchmarking analysis comparing two prominent deployment paradigms: a lightweight, Python-based REST service using FastAPI, and a specialized, high-performance serving engine, NVIDIA Triton Inference Server. Leveraging a reference architecture for healthcare AI, we deployed a DistilBERT sentiment analysis model on Kubernetes to measure median (p50) and tail (p95) latency, as well as throughput, under controlled experimental conditions. Our results indicate a distinct trade-off. While FastAPI provides lower overhead for single-request workloads with a p50 latency of 22 ms, Triton achieves superior scalability through dynamic batching, delivering a throughput of 780 requests per second on a single NVIDIA T4 GPU, nearly double that of the baseline. Furthermore, we evaluate a hybrid architectural approach that utilizes FastAPI as a secure gateway for protected health information de-identification and Triton for backend inference. This study validates the hybrid model as a best practice for enterprise clinical AI and offers a blueprint for secure, high-availability deployments.

</details>


### [16] [Learning to Price: Interpretable Attribute-Level Models for Dynamic Markets](https://arxiv.org/abs/2602.00188)
*Srividhya Sethuraman,Chandrashekar Lakshminarayanan*

Main category: cs.AI

TL;DR: 提出AFDLD可解释需求模型和ADEPT在线学习算法，在动态定价中实现可解释性和效率的平衡


<details>
  <summary>Details</summary>
Motivation: 高维市场动态定价面临可扩展性、不确定性和可解释性挑战，现有低秩bandit方法依赖潜在特征，难以解释产品属性如何影响价格

Method: 提出AFDLD可解释加性特征分解需求模型，将产品价格表达为属性级贡献之和，显式建模替代效应；基于此提出ADEPT算法，在属性空间直接操作，无需投影和梯度

Result: ADEPT算法达到亚线性遗憾界$\tilde{\mathcal{O}}(\sqrt{d}T^{3/4})$，在合成和真实数据中能学习接近最优价格，快速适应市场冲击和漂移，提供透明的属性级价格解释

Conclusion: 通过结构化、属性驱动的表示，可以在自主定价智能体中同时实现可解释性和效率

Abstract: Dynamic pricing in high-dimensional markets poses fundamental challenges of scalability, uncertainty, and interpretability. Existing low-rank bandit formulations learn efficiently but rely on latent features that obscure how individual product attributes influence price. We address this by introducing an interpretable \emph{Additive Feature Decomposition-based Low-Dimensional Demand (\textbf{AFDLD}) model}, where product prices are expressed as the sum of attribute-level contributions and substitution effects are explicitly modeled. Building on this structure, we propose \textbf{ADEPT} (Additive DEcomposition for Pricing with cross-elasticity and Time-adaptive learning)-a projection-free, gradient-free online learning algorithm that operates directly in attribute space and achieves a sublinear regret of $\tilde{\mathcal{O}}(\sqrt{d}T^{3/4})$. Through controlled synthetic studies and real-world datasets, we show that ADEPT (i) learns near-optimal prices under dynamic market conditions, (ii) adapts rapidly to shocks and drifts, and (iii) yields transparent, attribute-level price explanations. The results demonstrate that interpretability and efficiency in autonomous pricing agents can be achieved jointly through structured, attribute-driven representations.

</details>


### [17] [From Gameplay Traces to Game Mechanics: Causal Induction with Large Language Models](https://arxiv.org/abs/2602.00190)
*Mohit Jiwatode,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: 论文研究LLMs从游戏轨迹中逆向工程VGDL规则的能力，比较了直接代码生成和先推断SCM再转换的两阶段方法，发现SCM方法效果更好。


<details>
  <summary>Details</summary>
Motivation: 深度学习代理在复杂游戏领域表现出色，但通常不理解底层的因果游戏机制。为了解决这个问题，研究者探索因果归纳能力：从观测数据中推断支配规律。

Method: 使用LLMs从游戏轨迹逆向工程VGDL规则。比较两种方法：1) 直接从观测生成代码；2) 两阶段方法：先推断结构因果模型(SCM)，再转换为VGDL。使用语义嵌入和聚类从GVGAI框架中选择9个代表性游戏，并在多种提示策略和控制上下文机制下评估。

Result: SCM方法比直接生成方法更常产生接近真实情况的VGDL描述，在盲评估中获得高达81%的偏好胜率，产生更少的逻辑不一致规则。学习的SCM可用于下游应用。

Conclusion: SCM方法在从游戏轨迹逆向工程VGDL规则方面优于直接生成方法，为因果强化学习、可解释代理和程序化生成新颖但逻辑一致的游戏提供了基础。

Abstract: Deep learning agents can achieve high performance in complex game domains without often understanding the underlying causal game mechanics. To address this, we investigate Causal Induction: the ability to infer governing laws from observational data, by tasking Large Language Models (LLMs) with reverse-engineering Video Game Description Language (VGDL) rules from gameplay traces. To reduce redundancy, we select nine representative games from the General Video Game AI (GVGAI) framework using semantic embeddings and clustering. We compare two approaches to VGDL generation: direct code generation from observations, and a two-stage method that first infers a structural causal model (SCM) and then translates it into VGDL. Both approaches are evaluated across multiple prompting strategies and controlled context regimes, varying the amount and form of information provided to the model, from just raw gameplay observations to partial VGDL specifications. Results show that the SCM-based approach more often produces VGDL descriptions closer to the ground truth than direct generation, achieving preference win rates of up to 81\% in blind evaluations and yielding fewer logically inconsistent rules. These learned SCMs can be used for downstream use cases such as causal reinforcement learning, interpretable agents, and procedurally generating novel but logically consistent games.

</details>


### [18] [Complete Identification of Deep ReLU Neural Networks by Many-Valued Logic](https://arxiv.org/abs/2602.00266)
*Yani Zhang,Helmut Bölcskei*

Main category: cs.AI

TL;DR: 该论文将ReLU神经网络转换为Łukasiewicz逻辑公式，通过逻辑公理重写实现功能等价网络变换，并证明所有ReLU网络在功能等价类中通过有限对称性连接。


<details>
  <summary>Details</summary>
Motivation: 解决深度ReLU神经网络的完整识别问题——给定函数f，推导出所有产生该函数的前馈ReLU网络架构和参数。由于ReLU网络存在显著的功能对称性（不同架构和参数可实现相同函数），需要系统方法来识别所有等价表示。

Method: 将ReLU网络转换为Łukasiewicz逻辑公式，通过逻辑公理的代数重写实现功能等价网络变换。提出组合规范形式，便于从Łukasiewicz逻辑公式映射回ReLU网络。利用Chang完备性定理证明功能等价类中所有ReLU网络通过Łukasiewicz逻辑的有限公理集连接。

Result: 建立了ReLU网络与Łukasiewicz逻辑之间的对应关系，证明了每个功能等价类中的所有ReLU网络都由Łukasiewicz逻辑的有限公理集连接，类似于香农在开关电路设计中将电路转换为布尔公式并通过布尔逻辑公理重写进行综合的方法。

Conclusion: 通过将ReLU网络形式化为Łukasiewicz逻辑，为深度ReLU神经网络的功能对称性提供了完整的理论框架，实现了从函数到所有等价网络表示的系统推导，为神经网络分析和综合提供了新工具。

Abstract: Deep ReLU neural networks admit nontrivial functional symmetries: vastly different architectures and parameters (weights and biases) can realize the same function. We address the complete identification problem -- given a function f, deriving the architecture and parameters of all feedforward ReLU networks giving rise to f. We translate ReLU networks into Lukasiewicz logic formulae, and effect functional equivalent network transformations through algebraic rewrites governed by the logic axioms. A compositional norm form is proposed to facilitate the mapping from Lukasiewicz logic formulae back to ReLU networks. Using Chang's completeness theorem, we show that for every functional equivalence class, all ReLU networks in that class are connected by a finite set of symmetries corresponding to the finite set of axioms of Lukasiewicz logic. This idea is reminiscent of Shannon's seminal work on switching circuit design, where the circuits are translated into Boolean formulae, and synthesis is effected by algebraic rewriting governed by Boolean logic axioms.

</details>


### [19] [Localizing and Correcting Errors for LLM-based Planners](https://arxiv.org/abs/2602.00276)
*Aditya Kumar,William W. Cohen*

Main category: cs.AI

TL;DR: 本文提出L-ICL方法，通过局部上下文学习演示来纠正LLM在符号规划任务中的约束违反问题，相比传统方法显著提升规划有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数学和编码任务上表现出强大的推理能力，但在符号经典规划任务中经常失败，生成的计划经常违反给定的领域约束（如穿墙）。

Method: 提出局部上下文学习（L-ICL），迭代地在指令中添加针对性的修正演示：识别轨迹中的第一个约束违反，并注入一个最小化的输入-输出示例，展示失败步骤的正确行为。

Result: L-ICL比显式指令或传统ICL（添加完整问题解决轨迹）及其他基线方法更有效。在8x8网格世界中，L-ICL使用仅60个训练示例就能产生89%的有效计划，相比最佳基线的59%提升了30%。在其他领域（网格导航、迷宫、Sokoban、BlocksWorld）和多种LLM架构上也显示出显著改进。

Conclusion: 局部上下文学习是一种有效的方法，能够显著提高LLM在符号规划任务中遵守领域约束的能力，通过针对性的修正演示来纠正特定失败步骤。

Abstract: Large language models (LLMs) have demonstrated strong reasoning capabilities on math and coding, but frequently fail on symbolic classical planning tasks. Our studies, as well as prior work, show that LLM-generated plans routinely violate domain constraints given in their instructions (e.g., walking through walls). To address this failure, we propose iteratively augmenting instructions with Localized In-Context Learning (L-ICL) demonstrations: targeted corrections for specific failing steps. Specifically, L-ICL identifies the first constraint violation in a trace and injects a minimal input-output example giving the correct behavior for the failing step. Our proposed technique of L-ICL is much effective than explicit instructions or traditional ICL, which adds complete problem-solving trajectories, and many other baselines. For example, on an 8x8 gridworld, L-ICL produces valid plans 89% of the time with only 60 training examples, compared to 59% for the best baseline, an increase of 30%. L-ICL also shows dramatic improvements in other domains (gridworld navigation, mazes, Sokoban, and BlocksWorld), and on several LLM architectures.

</details>


### [20] [Assessing Domain-Level Susceptibility to Emergent Misalignment from Narrow Finetuning](https://arxiv.org/abs/2602.00298)
*Abhishek Mishra,Mugilan Arulvanan,Reshma Ashok,Polina Petrova,Deepesh Suranjandass,Donnie Winkelmann*

Main category: cs.AI

TL;DR: 本文研究大型语言模型在特定领域微调后出现的突发性错位风险，通过构建11个不安全领域数据集，评估了带后门触发和不带后门触发时模型的错位表现，发现后门会显著增加错位率，不同领域脆弱性差异很大。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型越来越多地用于自主任务，突发性错位对AI安全构成风险。需要评估模型在不同领域微调后可能出现的错位行为，特别是后门触发对错位的影响。

Method: 构建包含11个不同领域的不安全数据集，对Qwen2.5-Coder-7B-Instruct和GPT-4o-mini进行微调。评估模型在有无后门触发情况下的错位表现，使用成员推理指标预测错位程度，分析不同错位模型之间的行为泛化。

Result: 1) 后门触发使77.8%领域的错位率增加（平均下降4.33分），高风险金融建议和有毒法律建议领域影响最大；2) 领域脆弱性差异显著，从数学问题领域的0%错位到血腥电影知识领域的87.67%错位；3) 成员推理指标能有效预测错位程度。

Conclusion: 本文首次提供了按领域分类的突发性错位排名，对AI安全和后训练具有重要意义。建立了构建错位数据集的标准化方法，所有代码和数据集已开源。

Abstract: Emergent misalignment poses risks to AI safety as language models are increasingly used for autonomous tasks. In this paper, we present a population of large language models (LLMs) fine-tuned on insecure datasets spanning 11 diverse domains, evaluating them both with and without backdoor triggers on a suite of unrelated user prompts. Our evaluation experiments on \texttt{Qwen2.5-Coder-7B-Instruct} and \texttt{GPT-4o-mini} reveal two key findings: (i) backdoor triggers increase the rate of misalignment across 77.8% of domains (average drop: 4.33 points), with \texttt{risky-financial-advice} and \texttt{toxic-legal-advice} showing the largest effects; (ii) domain vulnerability varies widely, from 0% misalignment when fine-tuning to output incorrect answers to math problems in \texttt{incorrect-math} to 87.67% when fine-tuned on \texttt{gore-movie-trivia}.
  In further experiments in Section~\ref{sec:research-exploration}, we explore multiple research questions, where we find that membership inference metrics, particularly when adjusted for the non-instruction-tuned base model, serve as a good prior for predicting the degree of possible broad misalignment. Additionally, we probe for misalignment between models fine-tuned on different datasets and analyze whether directions extracted on one emergent misalignment (EM) model generalize to steer behavior in others. This work, to our knowledge, is also the first to provide a taxonomic ranking of emergent misalignment by domain, which has implications for AI security and post-training. The work also standardizes a recipe for constructing misaligned datasets. All code and datasets are publicly available on GitHub.\footnote{https://github.com/abhishek9909/assessing-domain-emergent-misalignment/tree/main}

</details>


### [21] [Autonomous Data Processing using Meta-Agents](https://arxiv.org/abs/2602.00307)
*Udayan Khurana*

Main category: cs.AI

TL;DR: ADP-MA是一个通过分层智能体编排动态构建、执行和迭代优化数据处理管道的框架，使用元智能体分析输入数据和任务规范来设计多阶段计划，并持续评估管道性能。


<details>
  <summary>Details</summary>
Motivation: 传统数据处理管道通常是静态的、为特定任务手工构建的，限制了其对不断变化需求的适应性。虽然通用智能体和编码助手可以为成熟的数据管道生成代码，但它们缺乏在部署后自主监控、管理和优化端到端管道的能力。

Method: ADP-MA框架通过分层智能体编排实现：核心元智能体分析输入数据和任务规范来设计多阶段计划，实例化专门的地面级智能体，并持续评估管道性能。架构包含三个关键组件：策略生成的规划模块、智能体协调和工具集成的编排层，以及迭代评估和回溯的监控循环。

Result: 通过交互式演示展示了ADP-MA在代表性数据处理任务中的管道构建、执行监控和自适应优化能力。该框架强调上下文感知优化、自适应工作负载分区和渐进采样以实现可扩展性。

Conclusion: ADP-MA提供了一个自主数据处理框架，能够动态构建、执行和迭代优化数据处理管道，克服了传统静态管道的局限性，并能够重用先前设计的智能体，减少冗余并加速管道构建。

Abstract: Traditional data processing pipelines are typically static and handcrafted for specific tasks, limiting their adaptability to evolving requirements. While general-purpose agents and coding assistants can generate code for well-understood data pipelines, they lack the ability to autonomously monitor, manage, and optimize an end-to-end pipeline once deployed. We present \textbf{Autonomous Data Processing using Meta-agents} (ADP-MA), a framework that dynamically constructs, executes, and iteratively refines data processing pipelines through hierarchical agent orchestration. At its core, \textit{meta-agents} analyze input data and task specifications to design a multi-phase plan, instantiate specialized \textit{ground-level agents}, and continuously evaluate pipeline performance. The architecture comprises three key components: a planning module for strategy generation, an orchestration layer for agent coordination and tool integration, and a monitoring loop for iterative evaluation and backtracking. Unlike conventional approaches, ADP-MA emphasizes context-aware optimization, adaptive workload partitioning, and progressive sampling for scalability. Additionally, the framework leverages a diverse set of external tools and can reuse previously designed agents, reducing redundancy and accelerating pipeline construction. We demonstrate ADP-MA through an interactive demo that showcases pipeline construction, execution monitoring, and adaptive refinement across representative data processing tasks.

</details>


### [22] [SayNext-Bench: Why Do LLMs Struggle with Next-Utterance Prediction?](https://arxiv.org/abs/2602.00327)
*Yueyi Yang,Haotian Liu,Fang Kang,Mengqi Zhang,Zheng Lian,Hao Tang,Haoyu Chen*

Main category: cs.AI

TL;DR: 该论文提出SayNext-Bench基准和SayNext-PC数据集，用于评估LLM和MLLM基于多模态线索预测人类对话下一句话的能力，并开发了认知启发的SayNext-Chat模型，证明多模态线索和主动预测处理对自然交互的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在自然对话方面取得进展，但研究发现现有模型难以像人类一样基于多模态线索（手势、注视、情感语调）预测下一句话。人类能轻松利用这些线索进行预测，而当前模型缺乏这种能力，这限制了AI实现真正自然的人类交互。

Method: 1) 构建SayNext-PC大规模数据集，包含丰富多模态线索的对话；2) 提出SayNext-Bench基准，系统评估LLM和MLLM基于多模态线索预测下一句话的能力；3) 开发认知启发的双路径预测模型SayNext-Chat，模拟对话中的预测处理机制。

Result: 实验表明SayNext-Chat在词汇重叠度、语义相似度和情感一致性方面优于最先进的MLLM。结果证明了基于多模态线索进行下一句话预测的可行性，并强调了多模态线索和主动预测处理在自然人类交互中的关键作用。

Conclusion: 该研究为更人性化、上下文敏感的AI交互提供了新研究方向，强调多模态线索和主动预测处理是实现自然人类交互的基础，而当前MLLM缺乏这些能力。提出的基准和模型为相关研究提供了工具和方向。

Abstract: We explore the use of large language models (LLMs) for next-utterance prediction in human dialogue. Despite recent advances in LLMs demonstrating their ability to engage in natural conversations with users, we show that even leading models surprisingly struggle to predict a human speaker's next utterance. Instead, humans can readily anticipate forthcoming utterances based on multimodal cues, such as gestures, gaze, and emotional tone, from the context. To systematically examine whether LLMs can reproduce this ability, we propose SayNext-Bench, a benchmark that evaluates LLMs and Multimodal LLMs (MLLMs) on anticipating context-conditioned responses from multimodal cues spanning a variety of real-world scenarios. To support this benchmark, we build SayNext-PC, a novel large-scale dataset containing dialogues with rich multimodal cues. Building on this, we further develop a dual-route prediction MLLM, SayNext-Chat, that incorporates cognitively inspired design to emulate predictive processing in conversation. Experimental results demonstrate that our model outperforms state-of-the-art MLLMs in terms of lexical overlap, semantic similarity, and emotion consistency. Our results prove the feasibility of next-utterance prediction with LLMs from multimodal cues and emphasize the (i) indispensable role of multimodal cues and (ii) actively predictive processing as the foundation of natural human interaction, which is missing in current MLLMs. We hope that this exploration offers a new research entry toward more human-like, context-sensitive AI interaction for human-centered AI. Our benchmark and model can be accessed at https://saynext.github.io/.

</details>


### [23] [MHDash: An Online Platform for Benchmarking Mental Health-Aware AI Assistants](https://arxiv.org/abs/2602.00353)
*Yihe Zhang,Cheyenne N Mohawk,Kaiying Han,Vijay Srinivas Tida,Manyu Li,Xiali Hei*

Main category: cs.AI

TL;DR: MHDash是一个用于心理健康AI系统开发、评估和审计的开源平台，通过多维度标注和多轮对话分析，揭示了现有LLM在识别高风险状态（如自杀意念）时的局限性，传统基准测试在安全关键场景中不足。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在心理健康支持系统中的应用日益增多，但现有评估主要依赖聚合性能指标，这些指标往往掩盖了特定风险类型的失败模式，且无法提供模型在真实多轮交互中行为的深入洞察。传统基准测试在安全关键的心理健康场景中不够充分。

Method: 开发了MHDash开源平台，集成了数据收集、结构化标注、多轮对话生成和基线评估的统一流程。平台支持多维度标注（关注类型、风险等级、对话意图），实现细粒度和风险感知的分析。

Result: 研究发现：(1) 简单基线和高级LLM API总体准确率相当，但在高风险案例上表现差异显著；(2) 一些LLM能保持一致的严重程度排序但无法准确分类绝对风险，而另一些在聚合得分上表现良好但在严重类别上假阴性率高；(3) 在多轮对话中性能差距被放大，风险信号逐渐显现。

Conclusion: 传统基准测试不足以评估安全关键的心理健康应用。通过发布MHDash作为开放平台，旨在促进可重复研究、透明评估以及心理健康支持AI系统的安全对齐开发。

Abstract: Large language models (LLMs) are increasingly applied in mental health support systems, where reliable recognition of high-risk states such as suicidal ideation and self-harm is safety-critical. However, existing evaluations primarily rely on aggregate performance metrics, which often obscure risk-specific failure modes and provide limited insight into model behavior in realistic, multi-turn interactions. We present MHDash, an open-source platform designed to support the development, evaluation, and auditing of AI systems for mental health applications. MHDash integrates data collection, structured annotation, multi-turn dialogue generation, and baseline evaluation into a unified pipeline. The platform supports annotations across multiple dimensions, including Concern Type, Risk Level, and Dialogue Intent, enabling fine-grained and risk-aware analysis. Our results reveal several key findings: (i) simple baselines and advanced LLM APIs exhibit comparable overall accuracy yet diverge significantly on high-risk cases; (ii) some LLMs maintain consistent ordinal severity ranking while failing absolute risk classification, whereas others achieve reasonable aggregate scores but suffer from high false negative rates on severe categories; and (iii) performance gaps are amplified in multi-turn dialogues, where risk signals emerge gradually. These observations demonstrate that conventional benchmarks are insufficient for safety-critical mental health settings. By releasing MHDash as an open platform, we aim to promote reproducible research, transparent evaluation, and safety-aligned development of AI systems for mental health support.

</details>


### [24] [Position: Agentic Evolution is the Path to Evolving LLMs](https://arxiv.org/abs/2602.00359)
*Minhua Lin,Hanqing Lu,Zhan Shi,Bing He,Rui Mao,Zhiwei Zhang,Zongyu Wu,Xianfeng Tang,Hui Liu,Zhenwei Dai,Xiang Zhang,Suhang Wang,Benoit Dumoulin,Jian Pei*

Main category: cs.AI

TL;DR: 论文提出LLMs需要从静态训练转向动态演化适应，引入A-Evolve框架将部署时改进视为对持久系统状态的有意识优化过程，并提出演化缩放假说。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型从精心策划的训练集转向开放的现实世界环境时，静态训练无法跟上持续部署环境变化。现有的部署时适应方法缺乏战略能动性来诊断失败并产生持久改进。

Method: 提出A-Evolve框架，将演化从固定流程提升为自主演化代理，将部署时改进视为对持久系统状态的有意识、目标导向的优化过程。

Result: 提出演化缩放假说：适应能力随着分配给演化的计算资源而扩展，将代理演化定位为在现实世界中实现持续、开放适应性的可扩展路径。

Conclusion: 代理演化代表了LLM适应的必然未来，通过A-Evolve框架和演化缩放假说，为LLMs在动态现实环境中的持续适应提供了新的扩展轴。

Abstract: As Large Language Models (LLMs) move from curated training sets into open-ended real-world environments, a fundamental limitation emerges: static training cannot keep pace with continual deployment environment change. Scaling training-time and inference-time compute improves static capability but does not close this train-deploy gap. We argue that addressing this limitation requires a new scaling axis-evolution. Existing deployment-time adaptation methods, whether parametric fine-tuning or heuristic memory accumulation, lack the strategic agency needed to diagnose failures and produce durable improvements. Our position is that agentic evolution represents the inevitable future of LLM adaptation, elevating evolution itself from a fixed pipeline to an autonomous evolver agent. We instantiate this vision in a general framework, A-Evolve, which treats deployment-time improvement as a deliberate, goal-directed optimization process over persistent system state. We further propose the evolution-scaling hypothesis: the capacity for adaptation scales with the compute allocated to evolution, positioning agentic evolution as a scalable path toward sustained, open-ended adaptation in the real world.

</details>


### [25] [POET: Protocol Optimization via Eligibility Tuning](https://arxiv.org/abs/2602.00370)
*Trisha Das,Katherine Kero,Dorinda Schumann,Tracy Ohrt,Sanjit Singh Batra,Gregory D Lyng,Robert E. Tillman*

Main category: cs.AI

TL;DR: 提出基于语义轴的引导生成框架，用于临床试验资格标准的自动生成，在特定性和可用性之间取得平衡，并通过可重复的评估框架验证其效果。


<details>
  <summary>Details</summary>
Motivation: 临床试验资格标准设计耗时且认知负荷高，现有自动化方法要么需要高度结构化输入（如预定义实体），要么依赖端到端系统从最小输入生成完整标准，实用性有限。

Method: 提出引导生成框架，引入可解释的语义轴（如人口统计学、实验室参数、行为因素）来指导资格标准生成。这些语义轴通过大语言模型推导，在特定性和可用性之间提供折中方案，使临床医生无需指定具体实体即可引导生成。

Result: 引导生成方法在自动评估、基于量表的评估和临床医生评估中均持续优于非引导生成，为AI辅助试验设计提供了实用且可解释的解决方案。

Conclusion: 提出的引导生成框架通过语义轴在特定性和可用性之间取得平衡，为临床试验资格标准设计提供了实用、可解释的AI辅助方法，并通过可重复的评估框架验证了其有效性。

Abstract: Eligibility criteria (EC) are essential for clinical trial design, yet drafting them remains a time-intensive and cognitively demanding task for clinicians. Existing automated approaches often fall at two extremes either requiring highly structured inputs, such as predefined entities to generate specific criteria, or relying on end-to-end systems that produce full eligibility criteria from minimal input such as trial descriptions limiting their practical utility. In this work, we propose a guided generation framework that introduces interpretable semantic axes, such as Demographics, Laboratory Parameters, and Behavioral Factors, to steer EC generation. These axes, derived using large language models, offer a middle ground between specificity and usability, enabling clinicians to guide generation without specifying exact entities. In addition, we present a reusable rubric-based evaluation framework that assesses generated criteria along clinically meaningful dimensions. Our results show that our guided generation approach consistently outperforms unguided generation in both automatic, rubric-based and clinician evaluations, offering a practical and interpretable solution for AI-assisted trial design.

</details>


### [26] [KEPO: Knowledge-Enhanced Preference Optimization for Reinforcement Learning with Reasoning](https://arxiv.org/abs/2602.00400)
*Fan Yang,Rui Meng,Trudi Di Qi,Ali Ezzati,Yuxin Wen*

Main category: cs.AI

TL;DR: KEPO提出了一种结合质量门控蒸馏和知识增强探索的强化学习后训练框架，用于解决推理密集型任务中的探索失败和梯度噪声问题。


<details>
  <summary>Details</summary>
Motivation: 推理导向的强化学习后训练面临稀疏轨迹奖励导致的信用分配模糊和探索失败问题，而现有的均匀蒸馏方法在低质量轨迹上会产生噪声梯度。

Method: KEPO框架包含：(1) 质量门控的在线蒸馏目标，仅对高质量轨迹应用密集教师指导；(2) 知识增强探索策略，利用教师模型的提示拒绝采样奖励正的在线轨迹。

Result: 在医疗视觉问答基准测试中，KEPO在单源泛化下表现出更好的训练稳定性、更一致的推理行为，以及优于强化学习和在线蒸馏基线的分布外性能。

Conclusion: KEPO通过选择性蒸馏和知识引导的探索，有效解决了推理密集型任务中强化学习后训练的挑战，实现了更稳定和有效的训练。

Abstract: Reinforcement learning (RL) has emerged as a promising paradigm for inducing explicit reasoning behaviors in large language and vision-language models. However, reasoning-oriented RL post-training remains fundamentally challenging due to sparse trajectory-level rewards, leading to ambiguous credit assignment and severe exploration failures that can trap the policy in a ``learning cliff.'' Recent on-policy distillation methods introduce dense teacher supervision to stabilize optimization, but apply it uniformly across all generated trajectories. We argue that such uniform distillation is ill-suited for reasoning-intensive tasks, as low-quality on-policy trajectories often originate from early logical errors, and distillation under flawed contexts injects noisy and misaligned gradients. To address these challenges, we propose Knowledge-Enhanced Preference Optimization (KEPO), a unified post-training framework that integrates: (i) a quality-gated on-policy distillation objective that selectively applies dense teacher guidance only to high-quality trajectories, and (ii) a knowledge-enhanced exploration strategy that leverages hints learned from a teacher model to rejectively sample reward-positive on-policy trajectories for RL, thereby mitigating exploration collapse. Evaluated on a challenging medical visual question answering benchmark under single-source generalization, KEPO demonstrates improved training stability, more coherent reasoning behaviors, and superior out-of-distribution performance over reinforcement learning and on-policy distillation baselines.

</details>


### [27] [RobustDebias: Debiasing Language Models using Distributionally Robust Optimization](https://arxiv.org/abs/2602.00405)
*Deep Gandhi,Katyani Singh,Nidhi Hegde*

Main category: cs.AI

TL;DR: 提出RobustDebias方法，使用分布鲁棒优化在微调阶段减少语言模型偏见，避免昂贵的预训练修改


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型存在偏见和社会刻板印象，现有去偏见方法主要关注预训练阶段的嵌入空间修改，这不适用于大型模型。微调预训练模型可能降低性能并放大微调数据中的偏见

Method: 提出RobustDebias机制，将分布鲁棒优化（DRO）应用于语言模型微调阶段，在MLM微调过程中对多个人口统计群体进行去偏见，可泛化到任何数据集或任务

Result: 在各种语言模型上的广泛实验显示，该方法能显著减轻偏见，同时对性能影响最小

Conclusion: 通过分布鲁棒优化在微调阶段进行去偏见是有效的，避免了昂贵的预训练修改，适用于广泛的语言理解任务

Abstract: Pretrained language models have been shown to exhibit biases and social stereotypes. Prior work on debiasing these models has largely focused on modifying embedding spaces during pretraining, which is not scalable for large models. Fine-tuning pretrained models on task-specific datasets can both degrade model performance and amplify biases present in the fine-tuning data. We address bias amplification during fine-tuning rather than costly pretraining, focusing on BERT models due to their widespread use in language understanding tasks. While Empirical Risk Minimization effectively optimizes downstream performance, it often amplifies social biases during fine-tuning. To counter this, we propose \textit{RobustDebias}, a novel mechanism which adapts Distributionally Robust Optimization (DRO) to debias language models during fine-tuning. Our approach debiases models across multiple demographics during MLM fine-tuning and generalizes to any dataset or task. Extensive experiments on various language models show significant bias mitigation with minimal performance impact.

</details>


### [28] [PolarMem: A Training-Free Polarized Latent Graph Memory for Verifiable Multimodal Agents](https://arxiv.org/abs/2602.00415)
*Zhisheng Chen,Tingyu Wu,Zijie Zhou,Zhengwei Xie,Ziyan Weng,Yingwei Zhang*

Main category: cs.AI

TL;DR: PolarMem是一种无需训练的极化潜在图记忆系统，将模糊的感知似然转化为离散的逻辑约束，通过极化图拓扑结构显式存储否定信息，为可验证的多模态智能体提供认知基础。


<details>
  <summary>Details</summary>
Motivation: 当前多模态智能体从被动观察者发展为长期决策者，需要具备逻辑可验证性的记忆系统。现有架构存在认知不对称性：概率视觉语言模型和密集关联记忆将语义亲和性与事实存在混为一谈，且结构上无法编码否定约束。

Method: 提出PolarMem（极化潜在图记忆），通过非参数分布划分将模糊感知似然转化为离散逻辑约束。采用极化图拓扑结构，使用正交抑制连接显式存储已验证的否定作为主要认知状态。在推理时实施逻辑主导的检索范式，抑制违反否定约束的幻觉模式。

Result: 在8个冻结的视觉语言模型和6个基准测试上进行广泛评估，证明PolarMem作为一个稳健的认知系统，为可验证的多模态智能体奠定了基础。

Conclusion: PolarMem通过将模糊感知转化为逻辑约束并显式编码否定信息，解决了当前多模态智能体记忆系统的根本局限性，为实现可验证的多模态智能体提供了关键的技术基础。

Abstract: As multimodal agents evolve from passive observers to long-horizon decision-makers, they require memory systems that provide not just information availability but logical verifiability. A fundamental limitation of current architectures is the epistemic asymmetry inherent in probabilistic vision-language models and dense associative memories: they conflate semantic affinity with factual existence and structurally fail to encode negative constraints. To this end, we introduce PolarMem, a training-free Polarized Latent Graph Memory designed to ground agent reasoning in verifiable evidence. PolarMem transforms fuzzy perceptual likelihoods into discrete logical constraints through non-parametric distributional partitioning. Furthermore, it employs a polarized graph topology with orthogonal inhibitory connections to explicitly store verified negation as a primary cognitive state. At inference time, we enforce a logic-dominant retrieval paradigm, suppressing hallucinatory patterns that violate negative constraints. Extensive evaluation across eight frozen Vision--Language Models and six benchmarks demonstrates that PolarMem functions as a robust cognitive system, establishing a foundation for verifiable multimodal agents. Our code is available at https://github.com/czs-ict/PolarMem.

</details>


### [29] [Do Latent-CoT Models Think Step-by-Step? A Mechanistic Study on Sequential Reasoning Tasks](https://arxiv.org/abs/2602.00449)
*Jia Liang,Liangming Pan*

Main category: cs.AI

TL;DR: 该研究通过分析CODI模型在多项式迭代任务上的表现，揭示了潜在思维链（Latent-CoT）的内部工作机制，发现模型在不同任务长度下会形成不同的推理路径：短任务时能完整执行潜在推理，长任务时则倾向于部分推理和捷径策略。


<details>
  <summary>Details</summary>
Motivation: 潜在思维链（Latent-CoT）旨在实现无需显式长推理的逐步计算，但其内部机制尚不明确。研究者希望通过分析CODI模型在严格顺序的多项式迭代任务上的表现，揭示潜在思维链的工作机制。

Method: 使用CODI（连续思维师生蒸馏模型）在多项式迭代任务上进行研究，采用多种分析工具：logit-lens解码、线性探针、注意力分析和激活修补，以定位中间状态表示并追踪其到最终输出的路由路径。

Result: 在2-3跳任务中，CODI能形成完整的桥接状态，这些状态在不同潜在思维位置可解码，而最终输入则通过近乎直接的路径；预测通过思维边界处的后期融合产生。对于更长跳数，CODI不能可靠执行完整的潜在展开，而是表现出部分潜在推理路径，集中于后期中间状态并与最后输入在答案读取位置融合。

Conclusion: 研究明确了CODI式潜在思维链何时能产生忠实的迭代计算，何时会退化为压缩或捷径策略，并强调了为顺序推理设计鲁棒的潜在思维链目标所面临的挑战。

Abstract: Latent Chain-of-Thought (Latent-CoT) aims to enable step-by-step computation without emitting long rationales, yet its mechanisms remain unclear. We study CODI, a continuous-thought teacher-student distillation model, on strictly sequential polynomial-iteration tasks. Using logit-lens decoding, linear probes, attention analysis, and activation patching, we localize intermediate-state representations and trace their routing to the final readout. On two- and three-hop tasks, CODI forms the full set of bridge states that become decodable across latent-thought positions, while the final input follows a separate near-direct route; predictions arise via late fusion at the end-of-thought boundary. For longer hop lengths, CODI does not reliably execute a full latent rollout, instead exhibiting a partial latent reasoning path that concentrates on late intermediates and fuses them with the last input at the answer readout position. Ablations show that this partial pathway can collapse under regime shifts, including harder optimization. Overall, we delineate when CODI-style latent-CoT yields faithful iterative computation versus compressed or shortcut strategies, and highlight challenges in designing robust latent-CoT objectives for sequential reasoning.

</details>


### [30] [Cross-Modal Memory Compression for Efficient Multi-Agent Debate](https://arxiv.org/abs/2602.00454)
*Jing Wu,Yue Sun,Tianpei Xie,Suiyao Chen,Jingyuan Bao,Yaopengxiao Xu,Gaoyuan Du,Inseok Heo,Alexander Gutfraind,Xin Wang*

Main category: cs.AI

TL;DR: DebateOCR：一个跨模态压缩框架，用紧凑的图像表示替换冗长的文本辩论历史，减少92%的输入token，降低计算成本并加速推理。


<details>
  <summary>Details</summary>
Motivation: 多智能体辩论虽然能提高推理质量和减少幻觉，但随着辩论轮次和智能体数量增加，上下文会迅速膨胀。保留完整的文本历史会导致token使用量超过上下文限制，并且通常需要重复总结，增加了开销并加剧信息损失。

Method: 引入DebateOCR跨模态压缩框架，将冗长的文本辩论轨迹替换为紧凑的图像表示，然后通过专门的视觉编码器处理这些图像表示，以调节后续辩论轮次。

Result: 该设计压缩了通常跨越数万到数十万token的历史记录，减少了超过92%的输入token，在多个基准测试中显著降低了计算成本并加快了推理速度。

Conclusion: 通过理论视角表明，智能体间的多样性支持恢复被省略的信息：虽然任何单个压缩历史都可能丢弃细节，但聚合多个智能体的压缩视图可以使集体表示以指数级高概率接近信息瓶颈。

Abstract: Multi-agent debate can improve reasoning quality and reduce hallucinations, but it incurs rapidly growing context as debate rounds and agent count increase. Retaining full textual histories leads to token usage that can exceed context limits and often requires repeated summarization, adding overhead and compounding information loss. We introduce DebateOCR, a cross-modal compression framework that replaces long textual debate traces with compact image representations, which are then consumed through a dedicated vision encoder to condition subsequent rounds. This design compresses histories that commonly span tens to hundreds of thousands of tokens, cutting input tokens by more than 92% and yielding substantially lower compute cost and faster inference across multiple benchmarks. We further provide a theoretical perspective showing that diversity across agents supports recovery of omitted information: although any single compressed history may discard details, aggregating multiple agents' compressed views allows the collective representation to approach the information bottleneck with exponentially high probability.

</details>


### [31] [Benchmarking Agents in Insurance Underwriting Environments](https://arxiv.org/abs/2602.00456)
*Amanda Dsouza,Ramya Ramakrishnan,Charles Dickens,Bhavishya Pohani,Christopher M Glaze*

Main category: cs.AI

TL;DR: UNDERWRITE是一个与领域专家合作设计的保险承保基准，旨在评估AI代理在真实企业环境中的表现，发现现有基准在专有知识、噪声工具接口和不完美用户模拟方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理基准过度强调开放领域（如代码），使用狭窄的准确性指标，缺乏真实世界的复杂性，无法反映企业应用的实际需求。

Method: 与领域专家密切合作设计多轮保险承保基准，引入专有业务知识、噪声工具接口和不完美模拟用户等关键现实主义因素，评估13个前沿模型。

Result: 研究发现：最准确的模型不一定最有效；模型即使有工具访问权限仍会产生领域知识幻觉；pass^k结果显示性能下降20%；常见代理框架存在脆弱性；专家参与基准设计对真实评估至关重要。

Conclusion: 专家参与基准设计对于真实代理评估至关重要，企业部署需要更符合实际需求的基准，专业领域的幻觉检测需要组合方法。

Abstract: As AI agents integrate into enterprise applications, their evaluation demands benchmarks that reflect the complexity of real-world operations. Instead, existing benchmarks overemphasize open-domains such as code, use narrow accuracy metrics, and lack authentic complexity. We present UNDERWRITE, an expert-first, multi-turn insurance underwriting benchmark designed in close collaboration with domain experts to capture real-world enterprise challenges. UNDERWRITE introduces critical realism factors often absent in current benchmarks: proprietary business knowledge, noisy tool interfaces, and imperfect simulated users requiring careful information gathering. Evaluating 13 frontier models, we uncover significant gaps between research lab performance and enterprise readiness: the most accurate models are not the most efficient, models hallucinate domain knowledge despite tool access, and pass^k results show a 20% drop in performance. The results from UNDERWRITE demonstrate that expert involvement in benchmark design is essential for realistic agent evaluation, common agentic frameworks exhibit brittleness that skews performance reporting, and hallucination detection in specialized domains demands compositional approaches. Our work provides insights for developing benchmarks that better align with enterprise deployment requirements.

</details>


### [32] [Dual Latent Memory for Visual Multi-agent System](https://arxiv.org/abs/2602.00471)
*Xinlei Yu,Chengming Xu,Zhangquan Chen,Bo Yin,Cheng Yang,Yongbo He,Yihao Hu,Jiangning Zhang,Cheng Tan,Xiaobin Hu,Shuicheng Yan*

Main category: cs.AI

TL;DR: L²-VMAS框架通过双潜在记忆和熵驱动触发机制，解决视觉多智能体系统的"扩展墙"问题，在提升准确率的同时大幅降低token使用量。


<details>
  <summary>Details</summary>
Motivation: 视觉多智能体系统存在反直觉的"扩展墙"问题：增加智能体轮次反而降低性能并指数级增加token成本。这源于文本中心通信的信息瓶颈，将感知和思维轨迹转换为自然语言会导致语义损失。

Method: 提出L²-VMAS框架：1) 使用双潜在记忆实现智能体间协作；2) 解耦感知与思维，动态合成双潜在记忆；3) 引入熵驱动的主动触发机制，用按需内存访问替代被动信息传输。

Result: 在不同骨干网络、模型大小和多智能体结构上的实验表明，该方法有效打破"扩展墙"，平均准确率提升2.7-5.4%，同时token使用量减少21.3-44.8%。

Conclusion: L²-VMAS框架通过潜在记忆通信和动态触发机制，解决了视觉多智能体系统的扩展瓶颈，实现了性能提升与成本降低的双重优化。

Abstract: While Visual Multi-Agent Systems (VMAS) promise to enhance comprehensive abilities through inter-agent collaboration, empirical evidence reveals a counter-intuitive "scaling wall": increasing agent turns often degrades performance while exponentially inflating token costs. We attribute this failure to the information bottleneck inherent in text-centric communication, where converting perceptual and thinking trajectories into discrete natural language inevitably induces semantic loss. To this end, we propose L$^{2}$-VMAS, a novel model-agnostic framework that enables inter-agent collaboration with dual latent memories. Furthermore, we decouple the perception and thinking while dynamically synthesizing dual latent memories. Additionally, we introduce an entropy-driven proactive triggering that replaces passive information transmission with efficient, on-demand memory access. Extensive experiments among backbones, sizes, and multi-agent structures demonstrate that our method effectively breaks the "scaling wall" with superb scalability, improving average accuracy by 2.7-5.4% while reducing token usage by 21.3-44.8%. Codes: https://github.com/YU-deep/L2-VMAS.

</details>


### [33] [Replacing Parameters with Preferences: Federated Alignment of Heterogeneous Vision-Language Models](https://arxiv.org/abs/2602.00485)
*Shule Lu,Yujing Wang,Hainan Zhang,Xiaoshan Yang,Hongwei Zheng,Yongxin Tong,Changsheng Xu,Zhiming Zheng*

Main category: cs.AI

TL;DR: MoR：基于GRPO与混合奖励的联邦对齐框架，用于异构视觉语言模型，通过本地训练奖励模型和路由融合机制实现隐私保护下的联邦对齐


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在医疗、金融等隐私敏感领域有广泛应用潜力，但严格的数据共享限制使集中式训练不可行。联邦学习虽然能解决数据共享问题，但实际部署面临客户端异构性（计算资源、应用需求、模型架构）的挑战。作者认为，用偏好替代参数是比用参数替代数据更可扩展、更隐私保护的联邦学习发展方向。

Method: 提出MoR框架：1）初始化视觉基础模型作为KL正则化参考；2）每个客户端基于本地偏好标注训练奖励模型，捕获特定评估信号而不暴露原始数据；3）引入基于路由的融合机制自适应聚合客户端奖励信号；4）服务器使用混合奖励执行GRPO优化基础VLM。

Result: 在三个公开VQA基准测试上的实验表明，MoR在泛化性、鲁棒性和跨客户端适应性方面持续优于联邦对齐基线方法。

Conclusion: MoR为联邦设置下异构视觉语言模型的隐私保护对齐提供了一个可扩展的解决方案，代表了从参数共享到偏好共享的联邦学习演进方向。

Abstract: VLMs have broad potential in privacy-sensitive domains such as healthcare and finance, yet strict data-sharing constraints render centralized training infeasible. FL mitigates this issue by enabling decentralized training, but practical deployments face challenges due to client heterogeneity in computational resources, application requirements, and model architectures. We argue that while replacing data with model parameters characterizes the present of FL, replacing parameters with preferences represents a more scalable and privacy-preserving future. Motivated by this perspective, we propose MoR, a federated alignment framework based on GRPO with Mixture-of-Rewards for heterogeneous VLMs. MoR initializes a visual foundation model as a KL-regularized reference, while each client locally trains a reward model from local preference annotations, capturing specific evaluation signals without exposing raw data. To reconcile heterogeneous rewards, we introduce a routing-based fusion mechanism that adaptively aggregates client reward signals. Finally, the server performs GRPO with this mixed reward to optimize the base VLM. Experiments on three public VQA benchmarks demonstrate that MoR consistently outperforms federated alignment baselines in generalization, robustness, and cross-client adaptability. Our approach provides a scalable solution for privacy-preserving alignment of heterogeneous VLMs under federated settings.

</details>


### [34] [PCBSchemaGen: Constraint-Guided Schematic Design via LLM for Printed Circuit Boards (PCB)](https://arxiv.org/abs/2602.00510)
*Huanghaohe Zou,Peng Han,Emad Nazerian,Alex Q. Huang*

Main category: cs.AI

TL;DR: PCBSchemaGen是首个无需训练的PCB原理图设计框架，结合LLM代理和约束引导合成，能够处理异构数字、模拟和电源信号，显著提高设计精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: PCB原理图设计在电子产业中至关重要，但现有工作仅关注数字或模拟电路，而PCB设计需要处理异构信号并遵守实际IC封装和引脚约束。由于开源数据稀缺且缺乏基于仿真的验证，自动化PCB原理图设计尚未得到探索。

Method: PCBSchemaGen框架包含：1）基于LLM的代码生成范式，通过领域特定提示进行迭代反馈；2）利用真实IC数据手册构建的知识图谱和子图同构验证框架，编码引脚角色语义和拓扑约束；3）在23个PCB原理图任务上进行广泛实验。

Result: PCBSchemaGen显著提高了设计准确性和计算效率，能够有效处理数字、模拟和电源领域的PCB设计任务。

Conclusion: PCBSchemaGen是首个无需训练的PCB原理图设计框架，成功解决了异构信号处理和实际约束验证的挑战，为自动化PCB设计提供了有效解决方案。

Abstract: Printed Circuit Board (PCB) schematic design plays an essential role in all areas of electronic industries. Unlike prior works that focus on digital or analog circuits alone, PCB design must handle heterogeneous digital, analog, and power signals while adhering to real-world IC packages and pin constraints. Automated PCB schematic design remains unexplored due to the scarcity of open-source data and the absence of simulation-based verification. We introduce PCBSchemaGen, the first training-free framework for PCB schematic design that comprises LLM agent and Constraint-guided synthesis. Our approach makes three contributions: 1. an LLM-based code generation paradigm with iterative feedback with domain-specific prompts. 2. a verification framework leveraging a real-world IC datasheet derived Knowledge Graph (KG) and Subgraph Isomorphism encoding pin-role semantics and topological constraints. 3. an extensive experiment on 23 PCB schematic tasks spanning digital, analog, and power domains. Results demonstrate that PCBSchemaGen significantly improves design accuracy and computational efficiency.

</details>


### [35] [Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory](https://arxiv.org/abs/2602.00521)
*Junhyuk Choi,Sohhyung Park,Chanhee Cho,Hyeonchu Park,Bugeun Kim*

Main category: cs.AI

TL;DR: 提出基于项目反应理论的两阶段诊断框架，用于评估LLM-as-a-Judge的可靠性，包括内在一致性和人类对齐两个维度


<details>
  <summary>Details</summary>
Motivation: 现有LLM-as-a-Judge验证实践主要关注观察输出层面，无法深入了解LLM法官是否作为稳定可靠的测量工具，需要更系统的可靠性评估方法

Method: 基于项目反应理论（IRT）的两阶段诊断框架，采用IRT的等级反应模型（GRM），从内在一致性（提示变化下的测量稳定性）和人类对齐（与人类质量评估的对应性）两个维度形式化可靠性

Result: 框架为LLM法官诊断提供可解释信号，能够系统性地诊断判断，为验证LLM-as-a-Judge可靠性及识别不可靠性潜在原因提供实用指导

Conclusion: 基于IRT-GRM的框架能够有效评估LLM法官的可靠性，提供系统化的诊断工具，弥补现有验证实践的局限性

Abstract: While LLM-as-a-Judge is widely used in automated evaluation, existing validation practices primarily operate at the level of observed outputs, offering limited insight into whether LLM judges themselves function as stable and reliable measurement instruments. To address this limitation, we introduce a two-phase diagnostic framework for assessing reliability of LLM-as-a-Judge, grounded in Item Response Theory (IRT). The framework adopts Graded Response Model (GRM) of IRT and formalizes reliability along two complementary dimensions: (1) intrinsic consistency, defined as the stability of measurement behavior under prompt variations, and (2) human alignment, capturing correspondence with human quality assessments. We empirically examine diverse LLM judges with this framework, and show that leveraging IRT-GRM yields interpretable signals for diagnosing judgments systematically. These signals provide practical guidance for verifying reliablity of LLM-as-a-Judge and identifying potential causes of unreliability.

</details>


### [36] [How Far Are LLMs from Professional Poker Players? Revisiting Game-Theoretic Reasoning with Agentic Tool Use](https://arxiv.org/abs/2602.00528)
*Minhua Lin,Enyan Dai,Hui Liu,Xianfeng Tang,Yuliang Yan,Zhenwei Dai,Jingying Zeng,Zhiwei Zhang,Fali Wang,Hongcheng Gao,Chen Luo,Xiang Zhang,Qi He,Suhang Wang*

Main category: cs.AI

TL;DR: LLMs在扑克游戏中的战略推理能力不足，作者提出ToolPoker框架，结合外部求解器实现接近GTO的策略和专业的解释。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在关键领域应用增多，其在不确定性下的战略推理能力变得至关重要。扑克游戏提供了一个严格的测试环境，需要不仅强大的行动能力，还需要基于博弈论的原则性推理。

Method: 首先系统评估LLMs在多个现实扑克任务中的表现，发现三个主要缺陷：依赖启发式方法、事实误解、以及"知行差距"。然后提出ToolPoker框架，结合外部求解器生成GTO一致的行动和更精确的专业风格解释。

Result: ToolPoker实现了最先进的游戏表现，同时产生的推理轨迹能够紧密反映博弈论原则。相比之下，传统LLMs无法与传统算法竞争，即使经过行为克隆和强化学习改进，仍然无法达到准确的博弈论游戏水平。

Conclusion: LLMs在扑克等需要战略推理的任务中存在根本性缺陷，但通过集成外部工具（如GTO求解器）可以显著提升其表现，实现接近最优的策略和专业的推理解释。

Abstract: As Large Language Models (LLMs) are increasingly applied in high-stakes domains, their ability to reason strategically under uncertainty becomes critical. Poker provides a rigorous testbed, requiring not only strong actions but also principled, game-theoretic reasoning. In this paper, we conduct a systematic study of LLMs in multiple realistic poker tasks, evaluating both gameplay outcomes and reasoning traces. Our analysis reveals LLMs fail to compete against traditional algorithms and identifies three recurring flaws: reliance on heuristics, factual misunderstandings, and a "knowing-doing" gap where actions diverge from reasoning. An initial attempt with behavior cloning and step-level reinforcement learning improves reasoning style but remains insufficient for accurate game-theoretic play. Motivated by these limitations, we propose ToolPoker, a tool-integrated reasoning framework that combines external solvers for GTO-consistent actions with more precise professional-style explanations. Experiments demonstrate that ToolPoker achieves state-of-the-art gameplay while producing reasoning traces that closely reflect game-theoretic principles.

</details>


### [37] [Uncovering Latent Communication Patterns in Brain Networks via Adaptive Flow Routing](https://arxiv.org/abs/2602.00561)
*Tianhao Huang,Guanghui Min,Zhenyu Lei,Aiying Zhang,Chen Chen*

Main category: cs.AI

TL;DR: 提出AFR-Net框架，通过神经通信动力学视角建模结构连接如何产生功能连接模式，实现可解释的神经通路发现


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏神经科学基础洞察，无法揭示连接组背后的神经区域潜在相互作用，也不能解释SC和FC为何表现出耦合与异质性的动态状态

Method: 提出自适应流路由网络（AFR-Net），这是一个物理信息框架，模拟结构约束如何产生功能通信模式，实现关键神经通路的可解释发现

Result: 大量实验表明，AFR-Net显著优于最先进的基线方法

Conclusion: 通过神经通信动力学视角制定多模态融合，能够更好地理解和解释结构连接与功能连接之间的关系，为神经科学提供更深入的洞察

Abstract: Unraveling how macroscopic cognitive phenotypes emerge from microscopic neuronal connectivity remains one of the core pursuits of neuroscience. To this end, researchers typically leverage multi-modal information from structural connectivity (SC) and functional connectivity (FC) to complete downstream tasks. Recent methodologies explore the intricate coupling mechanisms between SC and FC, attempting to fuse their representations at the regional level. However, lacking fundamental neuroscientific insight, these approaches fail to uncover the latent interactions between neural regions underlying these connectomes, and thus cannot explain why SC and FC exhibit dynamic states of both coupling and heterogeneity. In this paper, we formulate multi-modal fusion through the lens of neural communication dynamics and propose the Adaptive Flow Routing Network (AFR-Net), a physics-informed framework that models how structural constraints (SC) give rise to functional communication patterns (FC), enabling interpretable discovery of critical neural pathways. Extensive experiments demonstrate that AFR-Net significantly outperforms state-of-the-art baselines. The code is available at https://anonymous.4open.science/r/DIAL-F0D1.

</details>


### [38] [Unmasking Reasoning Processes: A Process-aware Benchmark for Evaluating Structural Mathematical Reasoning in LLMs](https://arxiv.org/abs/2602.00564)
*Xiang Zheng,Weiqi Zhai,Wei Wang,Boyu Yang,Wenbo Li,Ruixiang Luo,Haoxiang Sun,Yucheng Wang,Zhengze Li,Meng Wang,Yuetian Du,Guojie Lin,Yaxuan Wang,Xiaoxiao Xu,Yanhu Mo,Xuan Ren,Hu Wei,Ze Xu*

Main category: cs.AI

TL;DR: 论文提出ReasoningMath-Plus基准测试，包含150个精心设计的问题，专注于评估结构性推理能力，并引入HCRS评分方法和PRM模型，揭示现有模型在推理过程评估中表现远低于最终答案准确性。


<details>
  <summary>Details</summary>
Motivation: 现有数学推理基准测试中，大语言模型已达到接近饱和的准确率，但这主要源于数据集中的模板化计算和浅层算术分解，未能真正评估多约束协调、构造性逻辑综合和空间推理等核心推理能力。

Method: 1) 创建ReasoningMath-Plus基准测试，包含150个强调交互约束推理、构造性解形成和非平凡结构洞察的问题；2) 引入HCRS（危险感知链式规则评分）确定性步骤级评分函数；3) 基于标注的推理轨迹训练过程奖励模型（PRM）。

Result: 领先模型在最终答案准确率上可达5.8/10，但基于HCRS的整体评估得分显著较低（平均4.36/10，最佳5.14/10），表明仅基于答案的指标会高估模型的推理鲁棒性。

Conclusion: 需要更精细的过程级评估方法来准确衡量大语言模型的真实推理能力，ReasoningMath-Plus基准测试和HCRS评分方法为此提供了有效工具，揭示了现有模型在结构性推理方面的局限性。

Abstract: Recent large language models (LLMs) achieve near-saturation accuracy on many established mathematical reasoning benchmarks, raising concerns about their ability to diagnose genuine reasoning competence. This saturation largely stems from the dominance of template-based computation and shallow arithmetic decomposition in existing datasets, which underrepresent reasoning skills such as multi-constraint coordination, constructive logical synthesis, and spatial inference. To address this gap, we introduce ReasoningMath-Plus, a benchmark of 150 carefully curated problems explicitly designed to evaluate structural reasoning. Each problem emphasizes reasoning under interacting constraints, constructive solution formation, or non-trivial structural insight, and is annotated with a minimal reasoning skeleton to support fine-grained process-level evaluation. Alongside the dataset, we introduce HCRS (Hazard-aware Chain-based Rule Score), a deterministic step-level scoring function, and train a Process Reward Model (PRM) on the annotated reasoning traces. Empirically, while leading models attain relatively high final-answer accuracy (up to 5.8/10), HCRS-based holistic evaluation yields substantially lower scores (average 4.36/10, best 5.14/10), showing that answer-only metrics can overestimate reasoning robustness.

</details>


### [39] [Learning Modal-Mixed Chain-of-Thought Reasoning with Latent Embeddings](https://arxiv.org/abs/2602.00574)
*Yifei Shao,Kun Zhou,Ziming Xu,Mohammad Atif Quamar,Shibo Hao,Zhen Wang,Zhiting Hu,Biwei Huang*

Main category: cs.AI

TL;DR: 提出modal-mixed CoT方法，在思维链中交替使用文本标记和视觉草图潜在嵌入，以解决纯文本CoT在处理视觉密集型问题时的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统纯文本思维链在处理视觉密集型推理问题时存在局限，因为关键的中间状态本质上是视觉的，无法用文本充分表达。

Method: 1) 使用VLM自身作为编码器，训练语言骨干重建其视觉嵌入以保证语义对齐；2) 附加基于扩散的潜在解码器，由特殊控制标记触发；3) 两阶段训练：监督微调（文本和潜在嵌入交替）和强化学习（学习模态切换和长推理链组合）。

Result: 在11个多样化多模态推理任务上的实验表明，该方法优于纯语言和其他CoT方法。

Conclusion: modal-mixed CoT通过视觉-文本混合推理链有效提升了多模态推理能力，扩散头负责感知细节而VLM负责高层意图，实现了角色分离和优化压力降低。

Abstract: We study how to extend chain-of-thought (CoT) beyond language to better handle multimodal reasoning. While CoT helps LLMs and VLMs articulate intermediate steps, its text-only form often fails on vision-intensive problems where key intermediate states are inherently visual. We introduce modal-mixed CoT, which interleaves textual tokens with compact visual sketches represented as latent embeddings. To bridge the modality gap without eroding the original knowledge and capability of the VLM, we use the VLM itself as an encoder and train the language backbone to reconstruct its own intermediate vision embeddings, to guarantee the semantic alignment of the visual latent space. We further attach a diffusion-based latent decoder, invoked by a special control token and conditioned on hidden states from the VLM. In this way, the diffusion head carries fine-grained perceptual details while the VLM specifies high-level intent, which cleanly disentangles roles and reduces the optimization pressure of the VLM. Training proceeds in two stages: supervised fine-tuning on traces that interleave text and latents with a joint next-token and latent-reconstruction objective, followed by reinforcement learning that teaches when to switch modalities and how to compose long reasoning chains. Extensive experiments across 11 diverse multimodal reasoning tasks, demonstrate that our method yields better performance than language-only and other CoT methods. Our code will be publicly released.

</details>


### [40] [Small Shifts, Large Gains: Unlocking Traditional TSP Heuristic Guided-Sampling via Unsupervised Neural Instance Modification](https://arxiv.org/abs/2602.00580)
*Wei Huang,Hanchen Wang,Dong Wen,Wenjie Zhang*

Main category: cs.AI

TL;DR: TSP-MDF：一种通过神经实例修改器移动节点坐标，使传统确定性TSP启发式算法具备引导采样能力，无需真实监督训练即可提升解质量的新框架。


<details>
  <summary>Details</summary>
Motivation: 传统TSP启发式算法（如最远/最近插入法）计算高效但确定性行为导致局部最优；神经启发式算法解质量更好但需要大量训练和真实监督，实用性受限。需要弥合这一差距。

Method: 提出TSP-MDF实例修改框架：使用神经实例修改器策略性地移动节点坐标生成多个修改实例，在这些实例上运行传统启发式算法，然后将解映射回原始实例，使传统算法能探索更高质量解并逃离局部最优。

Result: 在大规模TSP基准和真实世界基准上的实验表明，TSP-MDF显著提升传统启发式算法的性能，达到与神经启发式算法相当的解质量，但训练时间极短。

Conclusion: TSP-MDF成功为传统确定性TSP启发式算法赋予了引导采样能力，在保持实用性的同时显著提升解质量，无需真实监督训练，弥合了传统算法与神经算法之间的差距。

Abstract: The Traveling Salesman Problem (TSP) is one of the most representative NP-hard problems in route planning and a long-standing benchmark in combinatorial optimization. Traditional heuristic tour constructors, such as Farthest or Nearest Insertion, are computationally efficient and highly practical, but their deterministic behavior limits exploration and often leads to local optima. In contrast, neural-based heuristic tour constructors alleviate this issue through guided-sampling and typically achieve superior solution quality, but at the cost of extensive training and reliance on ground-truth supervision, hindering their practical use. To bridge this gap, we propose TSP-MDF, a novel instance modification framework that equips traditional deterministic heuristic tour constructors with guided-sampling capability. Specifically, TSP-MDF introduces a neural-based instance modifier that strategically shifts node coordinates to sample multiple modified instances, on which the base traditional heuristic tour constructor constructs tours that are mapped back to the original instance, allowing traditional tour constructors to explore higher-quality tours and escape local optima. At the same time, benefiting from our instance modification formulation, the neural-based instance modifier can be trained efficiently without any ground-truth supervision, ensuring the framework maintains practicality. Extensive experiments on large-scale TSP benchmarks and real-world benchmarks demonstrate that TSP-MDF significantly improves the performance of traditional heuristics tour constructors, achieving solution quality comparable to neural-based heuristic tour constructors, but with an extremely short training time.

</details>


### [41] [Exploring Information Seeking Agent Consolidation](https://arxiv.org/abs/2602.00585)
*Guochen Yan,Jialong Wu,Zhengwei Tao,Bo Li,Qintong Zhang,Jiahao Xu,Haitao Mi,Yuejian Fang,Qingni Shen,Wentao Zhang,Zhonghai Wu*

Main category: cs.AI

TL;DR: 该研究探索了将异构信息检索智能体整合为单一基础智能体模型的两种策略：数据级整合和参数级整合，比较了它们在性能保持、跨域泛化和行为干扰方面的表现。


<details>
  <summary>Details</summary>
Motivation: 现有信息检索智能体通常专门针对开放网络、文档或本地知识库，这限制了可扩展性和跨域泛化能力。研究旨在探索如何将异构信息检索智能体整合为单一基础智能体模型。

Method: 研究了两种互补的整合策略：1）数据级整合：在领域特定数据集的混合上联合训练统一模型；2）参数级整合：在参数层面合并独立训练的智能体模型。

Result: 数据级整合保持强大且稳定的基准性能，而参数级整合提供了有前景的高效替代方案，但存在干扰和鲁棒性挑战。研究还识别了参数级智能体整合的关键设计因素。

Conclusion: 数据级整合仍是稳健的基准方法，参数级整合是高效替代方案但需解决干扰问题。参数级整合的关键设计因素包括细粒度合并粒度、任务异构性感知和原则性共识策略。

Abstract: Information-seeking agents have emerged as a powerful paradigm for solving knowledge-intensive tasks. Existing information-seeking agents are typically specialized for open web, documents, or local knowledge bases, which constrains scalability and cross-domain generalization. In this work, we investigate how to consolidate heterogeneous information-seeking agents into a single foundation agentic model. We study two complementary consolidation strategies: data-level consolidation, which jointly trains a unified model on a mixture of domain-specific datasets, and parameter-level consolidation, which merges independently trained agent models at the parameter level. Our analysis compares these approaches in terms of performance retention, cross-domain generalization, and interference across information-seeking behaviors. Our results show that data-level consolidation remains a strong and stable baseline, while parameter-level consolidation offers a promising, efficient alternative but suffers from interference and robustness challenges. We further identify key design factors for effective agent consolidation at the parameter level, including fine-grained merging granularity, awareness of task heterogeneity, and principled consensus strategy.

</details>


### [42] [DockSmith: Scaling Reliable Coding Environments via an Agentic Docker Builder](https://arxiv.org/abs/2602.00592)
*Jiaran Zhang,Luck Ma,Yanhao Li,Fanqi Wan,Di Qi,Xu Zhao,Jieyi Hou,Zhe Xie,Mengqiang Ren,Xin Wu,Zhewei Huang,Liangyu Chen,Yingwei Ma,Qi Han,Xiangyu Zhang*

Main category: cs.AI

TL;DR: DockSmith是一个专门用于Docker环境构建的智能代理，通过大规模执行轨迹训练，在Docker构建任务上达到开源SOTA性能，并能提升其他软件工程代理任务的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 基于Docker的环境构建是扩展软件工程代理训练和评估的主要瓶颈，传统方法将其视为预处理步骤，但作者认为这应该成为代理的核心能力，涉及长时程工具使用、依赖推理和故障恢复。

Method: 提出DockSmith代理，通过增强的SWE-Factory风格流水线（包含循环检测控制器和跨任务成功记忆）生成大规模执行轨迹进行训练，使用30B-A3B模型学习Docker构建技能。

Result: 在Multi-Docker-Eval上达到开源SOTA：39.72% Fail-to-Pass率和58.28%提交率。在SWE-bench Verified、SWE-bench Multilingual和Terminal-Bench 2.0等分布外任务上也有性能提升。

Conclusion: Docker环境构建不仅应被视为预处理步骤，而是软件工程代理的核心能力，其训练获得的监督信号能迁移到其他代理任务，带来更广泛的智能代理效益。

Abstract: Reliable Docker-based environment construction is a dominant bottleneck for scaling execution-grounded training and evaluation of software engineering agents. We introduce DockSmith, a specialized agentic Docker builder designed to address this challenge. DockSmith treats environment construction not only as a preprocessing step, but as a core agentic capability that exercises long-horizon tool use, dependency reasoning, and failure recovery, yielding supervision that transfers beyond Docker building itself. DockSmith is trained on large-scale, execution-grounded Docker-building trajectories produced by a SWE-Factory-style pipeline augmented with a loop-detection controller and a cross-task success memory. Training a 30B-A3B model on these trajectories achieves open-source state-of-the-art performance on Multi-Docker-Eval, with 39.72% Fail-to-Pass and 58.28% Commit Rate. Moreover, DockSmith improves out-of-distribution performance on SWE-bench Verified, SWE-bench Multilingual, and Terminal-Bench 2.0, demonstrating broader agentic benefits of environment construction.

</details>


### [43] [Scalable Generative Game Engine: Breaking the Resolution Wall via Hardware-Algorithm Co-Design](https://arxiv.org/abs/2602.00608)
*Wei Zeng,Xuchen Li,Ruili Feng,Zhen Liu,Fengwei An,Jian Zhao*

Main category: cs.AI

TL;DR: 提出硬件算法协同设计框架，解决生成式游戏引擎的"内存墙"问题，实现720×480分辨率实时生成，比基线提升50倍像素吞吐量


<details>
  <summary>Details</summary>
Motivation: 现有实时生成式游戏引擎受限于"内存墙"，只能支持低分辨率（如64×64），无法实现高分辨率神经模拟。高分辨率生成存在计算资源不匹配问题：世界模型是计算密集型，而解码器是内存密集型。

Method: 提出可扩展的硬件算法协同设计框架：1）在AI加速器集群上智能解耦世界模型和解码器的异构架构；2）序列并行约束下的非对称资源分配策略；3）最小化片外带宽使用的内存中心算子融合方案；4）利用时间冗余掩盖延迟的流形感知潜在外推机制。

Result: 在可编程AI加速器集群上验证，实现720×480分辨率实时生成，比先前基线提升50倍像素吞吐量。在连续3D赛车和离散2D平台游戏基准测试中，分别达到26.4 FPS和48.3 FPS，摊销有效延迟为2.7毫秒。

Conclusion: 通过架构协同设计解决"内存墙"问题不仅是优化，更是实现高保真、响应式神经游戏体验的先决条件。该工作弥合了生成模型与高分辨率神经模拟之间的差距。

Abstract: Real-time generative game engines represent a paradigm shift in interactive simulation, promising to replace traditional graphics pipelines with neural world models. However, existing approaches are fundamentally constrained by the ``Memory Wall,'' restricting practical deployments to low resolutions (e.g., $64 \times 64$). This paper bridges the gap between generative models and high-resolution neural simulations by introducing a scalable \textit{Hardware-Algorithm Co-Design} framework. We identify that high-resolution generation suffers from a critical resource mismatch: the World Model is compute-bound while the Decoder is memory-bound. To address this, we propose a heterogeneous architecture that intelligently decouples these components across a cluster of AI accelerators. Our system features three core innovations: (1) an asymmetric resource allocation strategy that optimizes throughput under sequence parallelism constraints; (2) a memory-centric operator fusion scheme that minimizes off-chip bandwidth usage; and (3) a manifold-aware latent extrapolation mechanism that exploits temporal redundancy to mask latency. We validate our approach on a cluster of programmable AI accelerators, enabling real-time generation at $720 \times 480$ resolution -- a $50\times$ increase in pixel throughput over prior baselines. Evaluated on both continuous 3D racing and discrete 2D platformer benchmarks, our system delivers fluid 26.4 FPS and 48.3 FPS respectively, with an amortized effective latency of 2.7 ms. This work demonstrates that resolving the ``Memory Wall'' via architectural co-design is not merely an optimization, but a prerequisite for enabling high-fidelity, responsive neural gameplay.

</details>


### [44] [Structured Self-Consistency:A Multi-Task Evaluation of LLMs on VirtualHome](https://arxiv.org/abs/2602.00611)
*Jiaqi Xu,Tao Huang,Kai Zhang*

Main category: cs.AI

TL;DR: 本文评估了两种7B参数LLM在VirtualHome基准上的表现，提出了结构化自洽解码策略提升性能，发现不同模型在具身AI任务中各有优势。


<details>
  <summary>Details</summary>
Motivation: 具身AI需要智能体在模拟环境中理解目标、规划动作并执行任务。目前缺乏对大型语言模型在具身AI任务上的系统性评估，特别是不同模型在不同类型任务上的表现差异。

Method: 使用Embodied Agent Interface框架在VirtualHome基准上评估OPENPANGU-7B和QWEN2.5-7B模型。提出了结构化自洽解码策略，通过多次采样和领域特定投票机制提升结构化生成任务的质量。评估了四个基本任务：目标解释、动作序列、子目标分解和状态转移建模。

Result: 结构化自洽解码策略显著提升了模型性能。OPENPANGU-7B在层次化规划任务上表现优异，而QWEN2.5-7B在动作级任务上具有优势。不同模型类型展现出互补的优势。

Conclusion: 研究揭示了不同LLM在具身AI任务中的互补优势，为未来具身AI系统开发提供了重要见解。结构化自洽解码策略能有效提升模型在结构化生成任务上的表现。

Abstract: Embodied AI requires agents to understand goals, plan actions, and execute tasks in simulated environments.We present a comprehensive evaluation of Large Language Models (LLMs) on the VirtualHome benchmark using the Embodied Agent Interface (EAI) framework.We compare two representative 7B-parameter models OPENPANGU-7B and QWEN2.5-7B across four fundamental tasks: Goal Interpretation, Action Sequencing, Subgoal Decomposition, and Transition Modeling.We propose Structured Self-Consistency (SSC), an enhanced decoding strategy that leverages multiple sampling with domain-specific voting mechanisms to improve output quality for structured generation tasks. Experimental results demonstrate that SSC significantly enhances performance, with OPENPANGU-7B excelling at hierarchical planning while QWEN2.5-7B show advantages in action-level tasks. Our analysis reveals complementary strengths across model types, providing insights for future embodied AI system development.

</details>


### [45] [Inference-Only Prompt Projection for Safe Text-to-Image Generation with TV Guarantees](https://arxiv.org/abs/2602.00616)
*Minhyuk Lee,Hyekyung Yoon,Myungjoo Kang*

Main category: cs.AI

TL;DR: 提出一种基于总变差理论的文本到图像扩散模型安全防护框架，通过推理阶段的提示投影实现安全性与提示对齐的平衡，无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型在实际部署时需要安全防护机制来抑制不安全生成，但现有方法往往在抑制不安全内容的同时会损害良性提示的图像对齐质量。需要一种既能有效减少不安全生成，又能保持良性提示对齐性能的方法。

Method: 提出基于总变差理论的安全-提示对齐权衡框架，采用推理阶段的提示投影方法：对高风险提示通过带有验证的代理目标进行选择性干预，将其映射到容忍度控制的"安全集"中，而对良性提示基本保持不变。该方法无需重新训练或微调生成器。

Result: 在四个数据集和三种扩散模型骨干上，相比强基线模型级对齐方法，该方法实现了16.7-60.0%的相对不适当百分比降低，同时在COCO数据集上保持了接近未对齐参考模型的良性提示-图像对齐性能。

Conclusion: 通过总变差理论框架形式化了安全性与提示对齐之间的权衡关系，提出的推理阶段提示投影方法能够在有效抑制不安全生成的同时，保持良性提示的图像对齐质量，为文本到图像扩散模型的安全部署提供了实用解决方案。

Abstract: Text-to-Image (T2I) diffusion models enable high-quality open-ended synthesis, but their real-world deployment demands safeguards that suppress unsafe generations without degrading benign prompt-image alignment. We formalize this tension through a total variation (TV) lens: once the reference conditional distribution is fixed, any nontrivial reduction in unsafe generations necessarily incurs TV deviation from the reference, yielding a principled Safety-Prompt Alignment Trade-off (SPAT). Guided by this view, we propose an inference-only prompt projection framework that selectively intervenes on high-risk prompts via a surrogate objective with verification, mapping them into a tolerance-controlled safe set while leaving benign prompts effectively unchanged, without retraining or fine-tuning the generator. Across four datasets and three diffusion backbones, our approach achieves 16.7-60.0% relative reductions in inappropriate percentage (IP) versus strong model-level alignment baselines, while preserving benign prompt-image alignment on COCO near the unaligned reference.

</details>


### [46] [Mitigating loss of control in advanced AI systems through instrumental goal trajectories](https://arxiv.org/abs/2602.01699)
*Willem Fourie*

Main category: cs.AI

TL;DR: 该论文提出"工具性目标轨迹"概念，将AI控制问题从技术层面扩展到组织层面，通过监控采购、治理和财务三条组织路径来增强对AI系统的控制能力。


<details>
  <summary>Details</summary>
Motivation: 现有AI控制方法主要关注技术层面，但高度智能的AI系统可能通过追求工具性目标侵蚀人类控制。需要超越模型本身，从组织层面寻找更全面的控制方案。

Method: 提出三种组织路径的工具性目标轨迹：采购轨迹、治理轨迹和财务轨迹。通过监控这些轨迹产生的组织制品，为AI能力监控和干预提供具体切入点。

Result: IGTs为定义AI能力水平、实施可纠正性和可中断性提供了具体途径，将关注点从模型属性扩展到支持模型的组织系统。

Conclusion: 工具性目标轨迹框架扩展了AI控制选项，通过组织层面的监控和干预，为解决AI系统可能追求工具性目标带来的控制问题提供了新思路。

Abstract: Researchers at artificial intelligence labs and universities are concerned that highly capable artificial intelligence (AI) systems may erode human control by pursuing instrumental goals. Existing mitigations remain largely technical and system-centric: tracking capability in advanced systems, shaping behaviour through methods such as reinforcement learning from human feedback, and designing systems to be corrigible and interruptible. Here we develop instrumental goal trajectories to expand these options beyond the model. Gaining capability typically depends on access to additional technical resources, such as compute, storage, data and adjacent services, which in turn requires access to monetary resources. In organisations, these resources can be obtained through three organisational pathways. We label these pathways the procurement, governance and finance instrumental goal trajectories (IGTs). Each IGT produces a trail of organisational artefacts that can be monitored and used as intervention points when a systems capabilities or behaviour exceed acceptable thresholds. In this way, IGTs offer concrete avenues for defining capability levels and for broadening how corrigibility and interruptibility are implemented, shifting attention from model properties alone to the organisational systems that enable them.

</details>


### [47] [Predictive Maintenance for Ultrafiltration Membranes Using Explainable Similarity-Based Prognostics](https://arxiv.org/abs/2602.00659)
*Qusai Khaled,Laura Genga,Uzay Kaymak*

Main category: cs.AI

TL;DR: 提出基于模糊相似推理的可解释超滤膜剩余使用寿命预测框架，通过物理健康指标和透明规则实现可靠预测


<details>
  <summary>Details</summary>
Motivation: 反渗透海水淡化中超滤膜因污染而性能下降，现有预测性维护模型缺乏可解释性，操作人员不信任，需要透明可靠的预测方法

Method: 使用基于跨膜压力、通量和阻力的物理健康指标，通过高斯隶属函数模糊化，采用相似度度量识别历史退化轨迹，构建Takagi-Sugeno模糊规则进行RUL预测

Result: 在工业规模UF系统的12,528个运行周期上测试，平均绝对误差为4.50个周期，同时生成与专家理解一致的可解释规则库

Conclusion: 提出的可解释预测框架在保持高精度的同时提供透明推理过程，增强了操作人员信任，为膜系统预测性维护提供了实用解决方案

Abstract: In reverse osmosis desalination, ultrafiltration (UF) membranes degrade due to fouling, leading to performance loss and costly downtime. Most plants rely on scheduled preventive maintenance, since existing predictive maintenance models, often based on opaque machine learning methods, lack interpretability and operator trust. This study proposes an explainable prognostic framework for UF membrane remaining useful life (RUL) estimation using fuzzy similarity reasoning. A physics-informed Health Index, derived from transmembrane pressure, flux, and resistance, captures degradation dynamics, which are then fuzzified via Gaussian membership functions. Using a similarity measure, the model identifies historical degradation trajectories resembling the current state and formulates RUL predictions as Takagi-Sugeno fuzzy rules. Each rule corresponds to a historical exemplar and contributes to a transparent, similarity-weighted RUL estimate. Tested on 12,528 operational cycles from an industrial-scale UF system, the framework achieved a mean absolute error of 4.50 cycles, while generating interpretable rule bases consistent with expert understanding.

</details>


### [48] [SEISMO: Increasing Sample Efficiency in Molecular Optimization with a Trajectory-Aware LLM Agent](https://arxiv.org/abs/2602.00663)
*Fabian P. Krüger,Andrea Hunklinger,Adrian Wolny,Tim J. Adler,Igor Tetko,Santiago David Villalba*

Main category: cs.AI

TL;DR: SEISMO：一种基于LLM的在线分子优化代理，通过单步更新和轨迹条件化实现高效分子优化，在23个任务上表现优于现有方法2-3倍


<details>
  <summary>Details</summary>
Motivation: 分子结构优化是化学科学特别是药物发现中的关键瓶颈，由于分子性质评估依赖昂贵且有限的实验测定，需要高度样本高效的优化方法

Method: SEISMO是一个LLM代理，执行严格的在线推理时分子优化，每次oracle调用后更新，无需基于种群或批量学习。它通过自然语言任务描述、标量分数和结构化解释反馈来条件化每个提案

Result: 在23个任务的实用分子优化基准测试中，SEISMO的优化曲线下面积比先前方法高2-3倍，通常在50次oracle调用内达到接近最大任务分数。额外的药物化学任务显示，提供解释性反馈能进一步提高效率

Conclusion: 利用领域知识和结构化信息是实现样本高效分子优化的关键，SEISMO展示了LLM代理在严格在线分子优化中的有效性

Abstract: Optimizing the structure of molecules to achieve desired properties is a central bottleneck across the chemical sciences, particularly in the pharmaceutical industry where it underlies the discovery of new drugs. Since molecular property evaluation often relies on costly and rate-limited oracles, such as experimental assays, molecular optimization must be highly sample-efficient. To address this, we introduce SEISMO, an LLM agent that performs strictly online, inference-time molecular optimization, updating after every oracle call without the need for population-based or batched learning. SEISMO conditions each proposal on the full optimization trajectory, combining natural-language task descriptions with scalar scores and, when available, structured explanatory feedback. Across the Practical Molecular Optimization benchmark of 23 tasks, SEISMO achieves a 2-3 times higher area under the optimisation curve than prior methods, often reaching near-maximal task scores within 50 oracle calls. Our additional medicinal-chemistry tasks show that providing explanatory feedback further improves efficiency, demonstrating that leveraging domain knowledge and structured information is key to sample-efficient molecular optimization.

</details>


### [49] [OpenGuanDan: A Large-Scale Imperfect Information Game Benchmark](https://arxiv.org/abs/2602.00676)
*Chao Li,Shangdong Yang,Chiheng Zhan,Zhenxing Ge,Yujing Hu,Bingkun Bao,Xingguo Chen,Yang Gao*

Main category: cs.AI

TL;DR: OpenGuanDan是一个用于评估AI智能体在四人多轮中国纸牌游戏"掼蛋"中的表现的新型基准测试平台，具有不完美信息、大规模动作空间、合作竞争混合目标等挑战特性。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在棋牌游戏等领域取得了显著进展，但仍需要更具挑战性的基准测试来推动进一步研究。掼蛋游戏具有不完美信息、大规模信息集、合作竞争混合目标等复杂特性，为现有智能决策方法提供了严峻的测试环境。

Method: 开发了OpenGuanDan基准测试平台，支持掼蛋游戏的高效模拟和全面评估。平台提供独立API支持玩家间交互，允许人类-AI互动和大型语言模型集成。采用两种评估方式：1）所有掼蛋AI智能体之间的成对竞赛；2）人类-AI对抗。

Result: 实验结果表明，当前基于学习的智能体显著优于基于规则的智能体，但仍未达到超人类水平，突显了多智能体智能决策领域需要继续研究的需求。

Conclusion: OpenGuanDan作为一个具有挑战性的基准测试平台，为多智能体智能决策研究提供了有价值的测试环境。虽然学习型AI已超越规则型AI，但距离人类专家水平仍有差距，表明该领域仍有进一步研究的空间。

Abstract: The advancement of data-driven artificial intelligence (AI), particularly machine learning, heavily depends on large-scale benchmarks. Despite remarkable progress across domains ranging from pattern recognition to intelligent decision-making in recent decades, exemplified by breakthroughs in board games, card games, and electronic sports games, there remains a pressing need for more challenging benchmarks to drive further research. To this end, this paper proposes OpenGuanDan, a novel benchmark that enables both efficient simulation of GuanDan (a popular four-player, multi-round Chinese card game) and comprehensive evaluation of both learning-based and rule-based GuanDan AI agents. OpenGuanDan poses a suite of nontrivial challenges, including imperfect information, large-scale information set and action spaces, a mixed learning objective involving cooperation and competition, long-horizon decision-making, variable action spaces, and dynamic team composition. These characteristics make it a demanding testbed for existing intelligent decision-making methods. Moreover, the independent API for each player allows human-AI interactions and supports integration with large language models. Empirically, we conduct two types of evaluations: (1) pairwise competitions among all GuanDan AI agents, and (2) human-AI matchups. Experimental results demonstrate that while current learning-based agents substantially outperform rule-based counterparts, they still fall short of achieving superhuman performance, underscoring the need for continued research in multi-agent intelligent decision-making domain. The project is publicly available at https://github.com/GameAI-NJUPT/OpenGuanDan.

</details>


### [50] [HumanStudy-Bench: Towards AI Agent Design for Participant Simulation](https://arxiv.org/abs/2602.00685)
*Xuan Liu,Haoyang Shang,Zizhang Liu,Xinyan Liu,Yunze Xiao,Yiwen Tu,Haojian Jin*

Main category: cs.AI

TL;DR: 该论文提出了HUMANSTUDY-BENCH基准测试和执行引擎，用于评估LLM作为社会科学实验模拟参与者的表现，通过重现人类实验并比较行为一致性来改进代理设计。


<details>
  <summary>Details</summary>
Motivation: 当前使用大语言模型作为社会科学实验模拟参与者存在行为不稳定、对设计选择敏感的问题，且现有评估方法混淆了基础模型能力与实验实例化，难以区分结果反映的是模型本身还是代理设置。

Method: 将参与者模拟定义为完整实验协议上的代理设计问题，引入HUMANSTUDY-BENCH基准测试和执行引擎，采用Filter-Extract-Execute-Evaluate管道，通过共享运行时重现人类实验的试验序列和原始分析流程。

Result: 构建了包含12项基础研究的动态基准测试套件，涵盖个体认知、战略互动和社会心理学领域，包含超过6,000个试验，人类样本规模从几十人到超过2,100名参与者。

Conclusion: 通过将参与者模拟视为代理设计问题并引入新的评估框架，能够更准确地评估LLM在社会科学实验中的表现，为改进代理设计提供了系统化的方法。

Abstract: Large language models (LLMs) are increasingly used as simulated participants in social science experiments, but their behavior is often unstable and highly sensitive to design choices. Prior evaluations frequently conflate base-model capabilities with experimental instantiation, obscuring whether outcomes reflect the model itself or the agent setup. We instead frame participant simulation as an agent-design problem over full experimental protocols, where an agent is defined by a base model and a specification (e.g., participant attributes) that encodes behavioral assumptions. We introduce HUMANSTUDY-BENCH, a benchmark and execution engine that orchestrates LLM-based agents to reconstruct published human-subject experiments via a Filter--Extract--Execute--Evaluate pipeline, replaying trial sequences and running the original analysis pipeline in a shared runtime that preserves the original statistical procedures end to end. To evaluate fidelity at the level of scientific inference, we propose new metrics to quantify how much human and agent behaviors agree. We instantiate 12 foundational studies as an initial suite in this dynamic benchmark, spanning individual cognition, strategic interaction, and social psychology, and covering more than 6,000 trials with human samples ranging from tens to over 2,100 participants.

</details>


### [51] [From Prompt to Graph: Comparing LLM-Based Information Extraction Strategies in Domain-Specific Ontology Development](https://arxiv.org/abs/2602.00699)
*Xuan Liu,Ziyu Li,Mu He,Ziyang Ma,Xiaoxu Wu,Gizem Yilmaz,Yiyuan Xia,Bingbing Li,He Tan,Jerry Ying Hsi Fuh,Wen Feng Lu,Anders E. W. Jarfors,Per Jansson*

Main category: cs.AI

TL;DR: 本研究探索了三种基于大语言模型的方法（预训练LLM驱动、上下文学习和微调），从铸造制造领域文本中自动提取术语和关系，并构建了经领域专家验证的本体。


<details>
  <summary>Details</summary>
Motivation: 传统本体构建依赖人工标注和传统NLP技术，过程劳动密集且成本高昂，特别是在铸造制造等专业领域。大语言模型的兴起为自动化知识提取提供了新可能。

Method: 研究了三种LLM方法：1) 预训练LLM驱动方法，2) 上下文学习方法，3) 微调方法。使用有限数据从领域特定文本中提取术语和关系，比较性能后选择最佳方法构建铸造本体。

Result: 比较了三种方法的性能，确定了最佳表现方法，并用该方法构建了铸造本体，该本体经过领域专家验证。

Conclusion: LLM方法能够有效自动化领域知识提取，特别是在数据有限的专业领域，为传统劳动密集的本体构建提供了高效替代方案。

Abstract: Ontologies are essential for structuring domain knowledge, improving accessibility, sharing, and reuse. However, traditional ontology construction relies on manual annotation and conventional natural language processing (NLP) techniques, making the process labour-intensive and costly, especially in specialised fields like casting manufacturing. The rise of Large Language Models (LLMs) offers new possibilities for automating knowledge extraction. This study investigates three LLM-based approaches, including pre-trained LLM-driven method, in-context learning (ICL) method and fine-tuning method to extract terms and relations from domain-specific texts using limited data. We compare their performances and use the best-performing method to build a casting ontology that validated by domian expert.

</details>


### [52] [Self-Guard: Defending Large Reasoning Models via enhanced self-reflection](https://arxiv.org/abs/2602.00707)
*Jingnan Zheng,Jingjun Xu,Yanzhen Luo,Chenhang Cui,Gelei Deng,Zhenkai Liang,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: Self-Guard：一种轻量级安全防御框架，通过在表示层面强化安全合规性来解决大型推理模型的安全风险，特别是认知-合规差距问题。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型(LRMs)在显式推理方面表现出色，但存在推理操纵和信息泄露等独特风险。现有对齐策略通常依赖繁重的后训练或外部干预，计算成本高且无法解决模型认知风险但优先遵循用户指令的"认知-合规差距"问题。

Method: Self-Guard框架包含两个主要阶段：(1)安全导向提示：激活模型的潜在安全意识，引发自发反思；(2)安全激活引导：提取隐藏状态空间中的方向性变化并放大，确保推理过程中安全合规性优先于顺从性。

Result: 实验表明Self-Guard能有效弥合认知-合规差距，在不损害模型实用性的情况下实现鲁棒的安全性能。该框架在未见风险和不同模型规模上表现出强泛化能力。

Conclusion: Self-Guard为LRM安全对齐提供了一个成本高效的解决方案，通过轻量级方法在表示层面强化安全合规性，解决了现有方法的局限性。

Abstract: The emergence of Large Reasoning Models (LRMs) introduces a new paradigm of explicit reasoning, enabling remarkable advances yet posing unique risks such as reasoning manipulation and information leakage. To mitigate these risks, current alignment strategies predominantly rely on heavy post-training paradigms or external interventions. However, these approaches are often computationally intensive and fail to address the inherent awareness-compliance gap, a critical misalignment where models recognize potential risks yet prioritize following user instructions due to their sycophantic tendencies. To address these limitations, we propose Self-Guard, a lightweight safety defense framework that reinforces safety compliance at the representational level. Self-Guard operates through two principal stages: (1) safety-oriented prompting, which activates the model's latent safety awareness to evoke spontaneous reflection, and (2) safety activation steering, which extracts the resulting directional shift in the hidden state space and amplifies it to ensure that safety compliance prevails over sycophancy during inference. Experiments demonstrate that Self-Guard effectively bridges the awareness-compliance gap, achieving robust safety performance without compromising model utility. Furthermore, Self-Guard exhibits strong generalization across diverse unseen risks and varying model scales, offering a cost-efficient solution for LRM safety alignment.

</details>


### [53] [Physics-informed Diffusion Generation for Geomagnetic Map Interpolation](https://arxiv.org/abs/2602.00709)
*Wenda Li,Tongya Zheng,Kaixuan Chen,Shunyu Liu,Haoze Jiang,Yunzhi Hao,Rui Miao,Zujie Ren,Mingli Song,Hang Shi,Gang Chen*

Main category: cs.AI

TL;DR: 提出PDG框架，通过物理信息引导的扩散生成模型进行地磁地图插值，结合局部感受野和克里金原理约束，有效处理噪声并保证物理一致性。


<details>
  <summary>Details</summary>
Motivation: 现有散点数据插值方法未专门针对地磁地图设计，受检测噪声和物理规律影响导致性能不佳，需要专门的地磁地图插值方法。

Method: 提出物理信息扩散生成框架(PDG)：1) 基于局部感受野设计物理信息掩码策略引导扩散生成过程，消除噪声干扰；2) 根据地磁地图克里金原理对扩散生成结果施加物理信息约束，确保符合物理规律。

Result: 在四个真实世界数据集上的大量实验和深入分析证明了PDG的优越性和各组成部分的有效性。

Conclusion: PDG框架通过物理信息引导的扩散生成，能够有效处理地磁地图插值问题，在消除噪声干扰的同时保证物理一致性，优于现有方法。

Abstract: Geomagnetic map interpolation aims to infer unobserved geomagnetic data at spatial points, yielding critical applications in navigation and resource exploration. However, existing methods for scattered data interpolation are not specifically designed for geomagnetic maps, which inevitably leads to suboptimal performance due to detection noise and the laws of physics. Therefore, we propose a Physics-informed Diffusion Generation framework~(PDG) to interpolate incomplete geomagnetic maps. First, we design a physics-informed mask strategy to guide the diffusion generation process based on a local receptive field, effectively eliminating noise interference. Second, we impose a physics-informed constraint on the diffusion generation results following the kriging principle of geomagnetic maps, ensuring strict adherence to the laws of physics. Extensive experiments and in-depth analyses on four real-world datasets demonstrate the superiority and effectiveness of each component of PDG.

</details>


### [54] [Learning More from Less: Unlocking Internal Representations for Benchmark Compression](https://arxiv.org/abs/2602.00710)
*Yueqi Zhang,Jin Hu,Shaoxiong Feng,Peiwen Yuan,Xinglin Wang,Yiwei Li,Jiayi Shi,Chuyi Tan,Ji Zhang,Boyuan Pan,Yao Hu,Kan Li*

Main category: cs.AI

TL;DR: REPCORE：利用隐藏状态对齐构建核心集，仅需少量源模型即可准确估算LLM基准性能，克服传统方法依赖大量历史数据的限制


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型的成本过高，需要高效的基准测试替代方案。现有方法依赖大量源模型来估计可靠的项配置文件，这在源模型池较小时统计不稳定，尤其限制了新发布基准的使用。离散正确性标签会丢失模型决策过程中的信息，无法捕捉隐藏状态中的编码信息。

Method: 提出REPCORE方法，将异构的隐藏状态对齐到统一的潜在空间中，构建代表性核心集。使用这些子集进行性能外推，仅需少量源模型即可实现精确的估计准确性。

Result: 在五个基准测试和超过200个模型上的实验表明，REPCORE在排名相关性和估计准确性方面持续优于基于输出的基线方法。谱分析进一步表明，对齐后的表示包含可分离的组件，反映了广泛的响应趋势和任务特定的推理模式。

Conclusion: REPCORE通过利用隐藏状态信息而非仅依赖离散标签，显著提高了核心集构建的效率和准确性，使得在仅有少量源模型的情况下也能可靠地估算LLM基准性能，为高效模型评估提供了新途径。

Abstract: The prohibitive cost of evaluating Large Language Models (LLMs) necessitates efficient alternatives to full-scale benchmarking. Prevalent approaches address this by identifying a small coreset of items to approximate full-benchmark performance. However, existing methods must estimate a reliable item profile from response patterns across many source models, which becomes statistically unstable when the source pool is small. This dependency is particularly limiting for newly released benchmarks with minimal historical evaluation data. We argue that discrete correctness labels are a lossy view of the model's decision process and fail to capture information encoded in hidden states. To address this, we introduce REPCORE, which aligns heterogeneous hidden states into a unified latent space to construct representative coresets. Using these subsets for performance extrapolation, REPCORE achieves precise estimation accuracy with as few as ten source models. Experiments on five benchmarks and over 200 models show consistent gains over output-based baselines in ranking correlation and estimation accuracy. Spectral analysis further indicates that the aligned representations contain separable components reflecting broad response tendencies and task-specific reasoning patterns.

</details>


### [55] [Neuro-symbolic AI for Predictive Maintenance (PdM) -- review and recommendations](https://arxiv.org/abs/2602.00731)
*Kyle Hamilton,Ali Intizar*

Main category: cs.AI

TL;DR: 本文对过去五年工业预测性维护（PdM）进行系统综述，提出结合深度学习与符号逻辑的神经符号AI方法，以克服纯数据驱动和传统知识系统的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前预测性维护面临两难：数据驱动方法（如深度学习）虽准确度高但需要大量标注数据、泛化能力差、缺乏可解释性；而基于领域知识的传统方法准确度低、误报多、需要持续专家监督。需要一种能结合两者优势的解决方案。

Method: 采用系统综述方法，分析过去五年工业预测性维护的最新进展，重点研究结合传感器数据和人工规则输入的神经符号AI架构，提出具体的神经符号系统框架。

Result: 研究发现混合系统（结合数据驱动和领域知识）有潜力克服单一方法的弱点，神经符号AI能创建更准确、可解释、可解释且鲁棒的系统，为预测性维护提供新方向。

Conclusion: 神经符号AI是预测性维护的未来方向，能有效结合深度学习的准确性和符号逻辑的可解释性，解决工业实际应用中的关键挑战，推动预测性维护在真实环境中的采用。

Abstract: In this document we perform a systematic review the State-of-the-art in Predictive Maintenance (PdM) over the last five years in industrial settings such as commercial buildings, pharmaceutical facilities, or semi-conductor manufacturing. In general, data-driven methods such as those based on deep learning, exhibit higher accuracy than traditional knowledge-based systems. These systems however, are not without significant limitations. The need for large labeled data sets, a lack of generalizibility to new environments (out-of-distribution generalization), and a lack of transparency at inference time are some of the obstacles to adoption in real world environments. In contrast, traditional approaches based on domain expertise in the form of rules, logic or first principles suffer from poor accuracy, many false positives and a need for ongoing expert supervision and manual tuning. While the majority of approaches in recent literature utilize some form of data-driven architecture, there are hybrid systems which also take into account domain specific knowledge. Such hybrid systems have the potential to overcome the weaknesses of either approach on its own while preserving their strengths. We propose taking the hybrid approach even further and integrating deep learning with symbolic logic, or Neuro-symbolic AI, to create more accurate, explainable, interpretable, and robust systems. We describe several neuro-symbolic architectures and examine their strengths and limitations within the PdM domain. We focus specifically on methods which involve the use of sensor data and manually crafted rules as inputs by describing concrete NeSy architectures. In short, this survey outlines the context of modern maintenance, defines key concepts, establishes a generalized framework, reviews current modeling approaches and challenges, and introduces the proposed focus on Neuro-symbolic AI (NESY).

</details>


### [56] [Engineering AI Agents for Clinical Workflows: A Case Study in Architecture,MLOps, and Governance](https://arxiv.org/abs/2602.00751)
*Cláudio Lúcio do Val Lopes,João Marcus Pitta,Fabiano Belém,Gildson Alves,Flávio Vinícius Cruzeiro Martins*

Main category: cs.AI

TL;DR: 该论文介绍了Maria平台，这是一个用于初级医疗保健的生产级AI系统，通过整合四个工程支柱来实现可信赖的临床AI：清洁架构、事件驱动架构、基于Agent的模块化和人在回路治理。


<details>
  <summary>Details</summary>
Motivation: AI在临床环境中的集成面临软件工程挑战，需要从孤立模型转向健壮、可治理且可靠的系统。工业应用中常存在脆弱、原型衍生的架构，缺乏系统性监督，导致"责任真空"，安全和问责制受到损害。

Method: 提出Maria平台作为行业案例研究，采用协同架构：结合清洁架构（可维护性）与事件驱动架构（弹性和可审计性）；引入Agent作为主要模块化单元，每个Agent拥有自主的MLOps生命周期；技术上集成人在回路治理模型，不仅作为安全检查，更是事件驱动的持续改进数据源。

Result: Maria平台作为参考架构，展示了如何在高风险领域构建可维护、可扩展且可问责的AI赋能系统，解决了临床AI中的责任真空问题。

Conclusion: 可信赖的临床AI需要通过四个基础工程支柱的整体集成来实现：清洁架构、事件驱动架构、基于Agent的模块化和人在回路治理。该平台为工程师在高风险领域构建AI系统提供了实用经验。

Abstract: The integration of Artificial Intelligence (AI) into clinical settings presents a software engineering challenge, demanding a shift from isolated models to robust, governable, and reliable systems. However, brittle, prototype-derived architectures often plague industrial applications and a lack of systemic oversight, creating a ``responsibility vacuum'' where safety and accountability are compromised. This paper presents an industry case study of the ``Maria'' platform, a production-grade AI system in primary healthcare that addresses this gap.
  Our central hypothesis is that trustworthy clinical AI is achieved through the holistic integration of four foundational engineering pillars. We present a synergistic architecture that combines Clean Architecture for maintainability with an Event-driven architecture for resilience and auditability. We introduce the Agent as the primary unit of modularity, each possessing its own autonomous MLOps lifecycle. Finally, we show how a Human-in-the-Loop governance model is technically integrated not merely as a safety check, but as a critical, event-driven data source for continuous improvement. We present the platform as a reference architecture, offering practical lessons for engineers building maintainable, scalable, and accountable AI-enabled systems in high-stakes domains.

</details>


### [57] [Environment-Aware Adaptive Pruning with Interleaved Inference Orchestration for Vision-Language-Action Models](https://arxiv.org/abs/2602.00780)
*Yuting Huang,Leilei Ding,Zhipeng Tang,Zenghuan Zhu,Jiajun Deng,Xinrui Lin,Shuo Liu,Haojie Ren,Jianmin Ji,Yanyong Zhang*

Main category: cs.AI

TL;DR: EcoVLA是一个训练免费、即插即用的自适应剪枝框架，通过环境感知自适应剪枝和交错推理编排，在VLA模型中实现动态参数稀疏化，显著提升推理速度而几乎不影响性能。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言-动作模型参数量大导致推理延迟高，阻碍实时操作。静态剪枝无法适应环境动态变化，而固定间隔的动态层剪枝粒度粗且重训练开销大，需要更灵活的自适应剪枝方案。

Method: 提出EcoVLA框架，包含两个组件：1) 环境感知自适应剪枝(EAP)：轻量级自适应通道剪枝方法，利用物理环境的时间一致性更新稀疏模式；2) 交错推理编排(I²O)：利用VLA推理中的FLOPs气泡并行调度剪枝方法，对延迟影响可忽略。

Result: 在多种VLA模型和基准测试中，EcoVLA实现最先进性能：达到1.60倍加速且成功率仅下降0.4%；与令牌剪枝结合时达到2.18倍加速且性能仅下降0.5%。在真实机器人上验证了有效性。

Conclusion: EcoVLA提供了一个训练免费、即插即用的自适应剪枝框架，能够动态适应环境变化，显著提升VLA模型推理速度，同时保持高性能，且能与现有加速方法正交组合。

Abstract: While Vision-Language-Action (VLA) models hold promise in embodied intelligence, their large parameter counts lead to substantial inference latency that hinders real-time manipulation, motivating parameter sparsification. However, as the environment evolves during VLA execution, the optimal sparsity patterns change accordingly. Static pruning lacks the adaptability required for environment dynamics, whereas fixed-interval dynamic layer pruning suffers from coarse granularity and high retraining overheads. To bridge this gap, we propose EcoVLA, a training-free, plug-and-play adaptive pruning framework that supports orthogonal combination with existing VLA acceleration methods. EcoVLA comprises two components: Environment-aware Adaptive Pruning (EAP) and Interleaved Inference Orchestration ($I^2O$). EAP is a lightweight adaptive channel pruning method that incorporates the temporal consistency of the physical environment to update sparsity patterns. $I^2O$ leverages the FLOPs bubbles inherent in VLA inference to schedule the pruning method in parallel, ensuring negligible impact on latency. Evaluated on diverse VLA models and benchmarks, EcoVLA delivers state-of-the-art performance, achieving up to 1.60$\times$ speedup with only a 0.4% drop in success rate, and further reaches 2.18$\times$ speedup with only a 0.5% degradation when combined with token pruning. We further validate the effectiveness of EcoVLA on real-world robots.

</details>


### [58] [World Models as an Intermediary between Agents and the Real World](https://arxiv.org/abs/2602.00785)
*Sherry Yang*

Main category: cs.AI

TL;DR: 论文主张在复杂高成本领域使用世界模型作为代理与真实世界的中介，以解决行动执行成本高、样本效率低的问题，并探讨了世界模型构建的挑战与解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习训练的LLM代理在低成本环境（游戏、数学、编程）中表现出色，但在机器人物理成本、ML工程时间成本、科学实验资源成本等高成本复杂领域未能成功。真正的瓶颈在于执行行动获取奖励信号的成本过高。

Method: 提出使用世界模型作为代理与真实世界的中介，将世界模型视为动态、奖励和任务分布的模型。讨论世界模型如何克服高成本行动的基本障碍，包括极端离策略学习和长时程任务中的样本效率问题。

Result: 展示了世界模型如何在机器学习工程、计算机使用、机器人和AI科学等多个领域为代理提供关键且丰富的学习信号。识别了构建这些世界模型的挑战。

Conclusion: 提出了在数据集管理、架构设计、扩展和世界模型评估等方面的具体行动项，以推动世界模型在高成本复杂领域的应用，实现下一代代理性能的提升。

Abstract: Large language model (LLM) agents trained using reinforcement learning has achieved superhuman performance in low-cost environments like games, mathematics, and coding. However, these successes have not translated to complex domains where the cost of interaction is high, such as the physical cost of running robots, the time cost of ML engineering, and the resource cost of scientific experiments. The true bottleneck for achieving the next level of agent performance for these complex and high-cost domains lies in the expense of executing actions to acquire reward signals. To address this gap, this paper argues that we should use world models as an intermediary between agents and the real world. We discuss how world models, viewed as models of dynamics, rewards, and task distributions, can overcome fundamental barriers of high-cost actions such as extreme off-policy learning and sample inefficiency in long-horizon tasks. Moreover, we demonstrate how world models can provide critical and rich learning signals to agents across a broad set of domains, including machine learning engineering, computer use, robotics, and AI for science. Lastly, we identify the challenges of building these world models and propose actionable items along dataset curation, architecture design, scaling, and evaluation of world models.

</details>


### [59] [MissMAC-Bench: Building Solid Benchmark for Missing Modality Issue in Robust Multimodal Affective Computing](https://arxiv.org/abs/2602.00811)
*Ronghao Lin,Honghao Lu,Ruixing Wu,Aolin Xiong,Qinggong Chu,Qiaolin He,Sijie Mai,Haifeng Hu*

Main category: cs.AI

TL;DR: 该论文提出了MissMAC-Bench基准，用于系统评估多模态情感计算中的缺失模态问题，通过统一评估标准和跨模态协同视角来提升模型在现实场景中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中多模态数据的可用性往往是动态和不确定的，导致由于分布偏移和语义缺陷造成性能波动。这种缺失模态问题严重影响了多模态情感计算模型的鲁棒性和实际部署。

Method: 提出MissMAC-Bench基准，建立公平统一的评估标准，基于两个指导原则：训练时不使用缺失先验，单个模型能同时处理完整和不完整模态场景。基准整合了数据集和实例级别的固定和随机缺失模式评估协议。

Result: 在4个数据集上对3个广泛使用的语言模型进行了广泛实验，验证了不同MAC方法在处理缺失模态问题上的有效性。

Conclusion: 该基准为推进鲁棒的多模态情感计算提供了坚实基础，促进了多媒体数据挖掘的发展，有助于弥合学术研究与实际应用之间的差距。

Abstract: As a knowledge discovery task over heterogeneous data sources, current Multimodal Affective Computing (MAC) heavily rely on the completeness of multiple modalities to accurately understand human's affective state. However, in real-world scenarios, the availability of modality data is often dynamic and uncertain, leading to substantial performance fluctuations due to the distribution shifts and semantic deficiencies of the incomplete multimodal inputs. Known as the missing modality issue, this challenge poses a critical barrier to the robustness and practical deployment of MAC models. To systematically quantify this issue, we introduce MissMAC-Bench, a comprehensive benchmark designed to establish fair and unified evaluation standards from the perspective of cross-modal synergy. Two guiding principles are proposed, including no missing prior during training, and one single model capable of handling both complete and incomplete modality scenarios, thereby ensuring better generalization. Moreover, to bridge the gap between academic research and real-world applications, our benchmark integrates evaluation protocols with both fixed and random missing patterns at the dataset and instance levels. Extensive experiments conducted on 3 widely-used language models across 4 datasets validate the effectiveness of diverse MAC approaches in tackling the missing modality issue. Our benchmark provides a solid foundation for advancing robust multimodal affective computing and promotes the development of multimedia data mining.

</details>


### [60] [Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement](https://arxiv.org/abs/2602.00815)
*Yunjian Zhang,Sudong Wang,Yang Li,Peiran Xu,Conghao Zhou,Xiaoyue Ma,Jianing Li,Yao Zhu*

Main category: cs.AI

TL;DR: 该论文提出DoPR方法，通过动态选择单一样本进行策略更新，大幅降低RLVR训练的计算开销，同时保持推理性能


<details>
  <summary>Details</summary>
Motivation: 尽管RLVR（可验证奖励下的强化学习）在LLM推理对齐方面表现出色，但其训练过程需要大量奖励信号和计算资源，限制了实际应用。作者旨在解决RLVR的数据和计算效率问题。

Method: 提出DoPR（动态单次策略优化）方法：1）建立理论下界分析样本复杂度；2）基于奖励波动性和探索驱动的获取策略，动态选择每个批次中最具信息量的单个训练样本进行策略更新。

Result: 实验表明：1）少量训练样本即可实现强性能；2）DoPR将计算开销降低近一个数量级；3）同时保持竞争力的推理准确性。

Conclusion: DoPR为LLM后训练提供了可扩展且资源高效的解决方案，为推理密集型LLM应用的RL训练提供了更实用、可访问的路径。

Abstract: Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications.

</details>


### [61] [Optimizing Agentic Reasoning with Retrieval via Synthetic Semantic Information Gain Reward](https://arxiv.org/abs/2602.00845)
*Senkang Hu,Yong Dai,Yuzhi Zhao,Yihang Tao,Yu Guo,Zhengru Fang,Sam Tak Wu Kwong,Yuguang Fang*

Main category: cs.AI

TL;DR: InfoReasoner框架通过语义信息增益奖励优化检索过程，在7个问答基准上平均准确率提升达5.4%


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型虽然能够动态获取外部知识，但检索过程优化面临挑战，主要因为缺乏密集、有原则的奖励信号来指导有效的知识获取

Method: 提出InfoReasoner统一框架：1) 理论层面将信息增益重新定义为模型信念状态的不确定性减少；2) 实践层面提出输出感知的内在估计器，通过双向文本蕴含的语义聚类直接从模型输出分布计算信息增益；3) 使用组相对策略优化进行高效训练

Result: 在七个问答基准测试中，InfoReasoner始终优于强大的检索增强基线，平均准确率提升高达5.4%

Conclusion: 该工作为具有检索功能的智能推理提供了理论基础和可扩展路径，通过语义信息增益奖励激励有效的信息寻求行为

Abstract: Agentic reasoning enables large reasoning models (LRMs) to dynamically acquire external knowledge, but yet optimizing the retrieval process remains challenging due to the lack of dense, principled reward signals. In this paper, we introduce InfoReasoner, a unified framework that incentivizes effective information seeking via a synthetic semantic information gain reward. Theoretically, we redefine information gain as uncertainty reduction over the model's belief states, establishing guarantees, including non-negativity, telescoping additivity, and channel monotonicity. Practically, to enable scalable optimization without manual retrieval annotations, we propose an output-aware intrinsic estimator that computes information gain directly from the model's output distributions using semantic clustering via bidirectional textual entailment. This intrinsic reward guides the policy to maximize epistemic progress, enabling efficient training via Group Relative Policy Optimxization (GRPO). Experiments across seven question-answering benchmarks demonstrate that InfoReasoner consistently outperforms strong retrieval-augmented baselines, achieving up to 5.4% average accuracy improvement. Our work provides a theoretically grounded and scalable path toward agentic reasoning with retrieval.

</details>


### [62] [Persuasion Propagation in LLM Agents](https://arxiv.org/abs/2602.00851)
*Hyejun Jeong,Amir Houmansadr,Shlomo Zilberstein,Eugene Bagdasarian*

Main category: cs.AI

TL;DR: 研究AI智能体在长期任务中如何受到用户说服影响，发现任务执行前的信念预设比执行中的实时说服更能显著改变智能体行为


<details>
  <summary>Details</summary>
Motivation: 随着AI智能体越来越多地结合对话交互和自主任务执行（如编码和网络研究），需要研究当智能体在执行长期任务时受到用户说服会发生什么，以及这种说服如何影响下游任务行为

Method: 引入行为中心评估框架，区分在任务执行期间或之前应用的说服；在网络研究和编码任务上进行实验，比较实时说服和信念预设两种干预方式的效果

Result: 实时说服仅产生微弱且不一致的行为影响；相比之下，在任务开始时明确指定信念状态的信念预设智能体平均减少26.9%的搜索次数和16.9%的独特来源访问，显著改变任务执行行为

Conclusion: 即使在先前交互中的说服也能影响智能体行为，这强调了在智能体系统中进行行为层面评估的重要性，为AI安全研究提供了重要启示

Abstract: Modern AI agents increasingly combine conversational interaction with autonomous task execution, such as coding and web research, raising a natural question: what happens when an agent engaged in long-horizon tasks is subjected to user persuasion? We study how belief-level intervention can influence downstream task behavior, a phenomenon we name \emph{persuasion propagation}. We introduce a behavior-centered evaluation framework that distinguishes between persuasion applied during or prior to task execution. Across web research and coding tasks, we find that on-the-fly persuasion induces weak and inconsistent behavioral effects. In contrast, when the belief state is explicitly specified at task time, belief-prefilled agents conduct on average 26.9\% fewer searches and visit 16.9\% fewer unique sources than neutral-prefilled agents. These results suggest that persuasion, even in prior interaction, can affect the agent's behavior, motivating behavior-level evaluation in agentic systems.

</details>


### [63] [Position: Human-Centric AI Requires a Minimum Viable Level of Human Understanding](https://arxiv.org/abs/2602.00854)
*Fangzhou Lin,Qianwen Ge,Lingyu Xu,Peiran Li,Xiangbo Gao,Shuo Xing,Kazunori Yamada,Ziming Zhang,Haichong Zhang,Zhengzhong Tu*

Main category: cs.AI

TL;DR: 论文提出"能力-理解差距"概念，即AI能力增强时用户理解力下降，并定义"认知完整性阈值"作为维持监督所需的最低理解水平。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统产生越来越流畅、正确的结果，用户解释、验证和干预的能力逐渐被侵蚀。现有透明度、用户控制、素养和治理方法未能解决人类在持续AI委托下必须保持的基础理解问题。

Method: 提出认知完整性阈值(CIT)概念，定义为在AI协助下保持监督、自主性和问责参与所需的最低理解水平。通过三个功能维度操作化：验证能力、保持理解的交互、治理的制度支撑。

Result: 建立了能力-理解差距的理论框架，定义了认知完整性阈值，为责任关键场景中的人机交互设计和治理提供了新视角。

Conclusion: 需要将人机交互设计与认知可持续性对齐，确保在AI协助下人类仍能保持有效的监督和问责能力，特别是在责任关键的环境中。

Abstract: AI systems increasingly produce fluent, correct, end-to-end outcomes. Over time, this erodes users' ability to explain, verify, or intervene. We define this divergence as the Capability-Comprehension Gap: a decoupling where assisted performance improves while users' internal models deteriorate. This paper argues that prevailing approaches to transparency, user control, literacy, and governance do not define the foundational understanding humans must retain for oversight under sustained AI delegation. To formalize this, we define the Cognitive Integrity Threshold (CIT) as the minimum comprehension required to preserve oversight, autonomy, and accountable participation under AI assistance. CIT does not require full reasoning reconstruction, nor does it constrain automation. It identifies the threshold beyond which oversight becomes procedural and contestability fails. We operatinalize CIT through three functional dimensions: (i) verification capacity, (ii) comprehension-preserving interaction, and (iii) institutional scaffolds for governance. This motivates a design and governance agenda that aligns human-AI interaction with cognitive sustainability in responsibility-critical settings.

</details>


### [64] [Multi-Head Attention Is a Multi-Player Game](https://arxiv.org/abs/2602.00861)
*Kushal Chakrabarti,Nirmal Balachundar*

Main category: cs.AI

TL;DR: 论文将Transformer注意力机制建模为多头间的潜在博弈，证明交叉熵训练导致纳什均衡存在效率损失，并提出基于博弈理论的正则化方法GAME-LoRA来减少幻觉和冗余。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer训练方法将多头注意力视为单一优化器，忽略了多头间的竞争与协调关系。这种简化导致训练过程中存在未定价的外部性（如冗余和错误相关性），可能引发幻觉和效率损失。

Method: 将多头注意力形式化为潜在博弈，证明交叉熵训练收敛到纳什均衡。提出价格无政府状态(PoA)上界由头间交互矩阵Γ(G)决定。基于此设计GAME-LoRA方法，结合Barlow Twins去相关和log-determinant协调压力来减少Γ(G)。

Result: 理论证明：过度幻觉概率和过度头冗余都与PoA成比例。实验验证：Γ(G)能预测幻觉(p<0.05)，涌现的联盟表现出选择性协调，GAME-LoRA实现最高18%的幻觉减少（平均8%），且不损害知识性能。

Conclusion: 多头注意力本质上是多智能体系统，忽略其博弈结构会导致效率损失。通过减少头间交互矩阵Γ(G)的正则化方法可以改善Transformer性能，实现帕累托改进。这为理解注意力机制提供了新的博弈论视角。

Abstract: Modern transformer attention is internally multi-agent -- heads compete and coordinate -- yet we train it as if it were a monolithic optimizer. We formalize this gap: cross-entropy training induces an implicit potential game among heads, and gradient descent converges to Nash equilibria with potentially unbounded inefficiency due to unpriced externalities (redundancy, correlated errors). Our main result bounds the Price of Anarchy by $Γ(G)$, the off-diagonal mass of a head interaction matrix capturing weight and gradient coupling. Under mild smoothness assumptions, we prove that both \emph{excess hallucination probability} and \emph{excess head redundancy} scale with PoA, unifying two distinct failure modes into a single mechanism. The bound is prescriptive: regularization that reduces $Γ(G)$ provably tightens PoA. We instantiate this as GAME-LoRA, combining Barlow Twins decorrelation with log-determinant coordination pressure. Experiments validate the theory: $Γ(G)$ predicts hallucination ($p{<}0.05$), emergent coalitions exhibit selective coordination, and GAME-LoRA achieves up to 18\% hallucination reduction (8\% average) with no knowledge degradation -- a Pareto improvement inaccessible to methods ignoring the game structure.

</details>


### [65] [Foundation CAN LM: A Pretrained Language Model For Automotive CAN Data](https://arxiv.org/abs/2602.00866)
*Akiharu Esashi,Pawissanutt Lertpongrujikorn,Justin Makino,Yuibi Fujimoto,Mohsen Amini Salehi*

Main category: cs.AI

TL;DR: 提出首个CAN总线基础模型，通过大规模无监督预训练和任务特定微调，实现跨多种汽车保险任务的有效泛化，将NLP/CV中的基础模型范式成功应用于CAN数据。


<details>
  <summary>Details</summary>
Motivation: 现有CAN数据处理方法多为针对特定任务的孤立模型训练，缺乏共享表示学习和跨任务泛化能力。而NLP和CV领域的基础模型范式已证明其有效性，但尚未应用于CAN数据领域。

Method: 将CAN数据视为语言处理：1）提出统一的混合离散-连续信号标记化方案；2）在大规模无标签解码CAN信号上进行预训练；3）针对时间复杂性和行程特定变异性挑战提出解决方案；4）在异构汽车保险任务上进行微调。

Result: 单个预训练的CAN模型能够有效适应多种预测任务，验证了基础模型范式在CAN数据领域的适用性，为汽车AI中的可泛化表示学习开辟了新方向。

Conclusion: 成功将NLP和CV中证明有效的基础模型范式应用于CAN数据，实现了跨任务泛化，为汽车AI领域的通用表示学习建立了新范式。

Abstract: The Controller Area Network (CAN) bus provides a rich source of vehicular signals increasingly leveraged for applications in automotive and auto insurance domains, including collision detection, predictive maintenance, and driver risk modeling. Despite this potential, existing pipelines largely train isolated task-specific models on raw CAN data, with only limited efforts exploring decoded signals. Such fragmentation prevents shared representation learning and limits cross-task generalization. By contrast, natural language processing (NLP) and computer vision (CV) have been transformed by the foundation model paradigm: large-scale pretraining followed by task-specific adaptation. In this work, we introduce the foundation CAN model that demonstrates multi-objective downstream generalization using a single pretrained backbone. Our approach treats CAN data as a language: we pretrain on large-scale, unlabeled decoded CAN signals and fine-tune across heterogeneous auto insurance tasks. To enable this, we propose a unified tokenization scheme for mixed discrete-continuous signals and address challenges of temporal complexity and trip-specific variability. Our results show that one pretrained CAN model can adapt effectively to diverse predictive tasks, validating that the foundation modeling paradigm, proven in NLP and CV, also holds for CAN data. This establishes a new direction for generalizable representation learning in automotive AI.

</details>


### [66] [Beyond Output Critique: Self-Correction via Task Distillation](https://arxiv.org/abs/2602.00871)
*Hossein A. Rahmani,Mengting Wan,Pei Zhou,Longqi Yang,Nick Craswell,Emine Yilmaz,Sujay Kumar Jauhar*

Main category: cs.AI

TL;DR: SELF-THOUGHT框架通过任务抽象引导LLM自我修正，将任务提炼为结构化模板，指导解决方案实例化，并支持模板跨模型迁移，提升大小模型的推理准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM自我修正方法主要停留在输出批评层面，只能修补表面错误，难以纠正深层次推理缺陷。需要一种能够理解任务本质结构、减少错误传播的自我修正框架。

Method: 提出SELF-THOUGHT框架：1）任务抽象：将输入和初始响应提炼为结构化模板，捕捉关键变量、约束和问题结构；2）解决方案实例化：基于抽象模板指导生成修正后的响应；3）跨模型模板迁移：大模型生成的模板可作为小模型的结构化指导。

Result: 在多样化推理任务上的实验表明，SELF-THOUGHT提高了大小模型的准确性、鲁棒性和泛化能力，为小模型提供了无需大量微调或外部验证器的可靠修正能力。

Conclusion: SELF-THOUGHT通过任务抽象和跨模型模板迁移，为构建更可靠的自我修正语言系统提供了可扩展路径，特别有助于提升小模型的推理修正能力。

Abstract: Large language models (LLMs) have shown promising self-correction abilities, where iterative refinement improves the quality of generated responses. However, most existing approaches operate at the level of output critique, patching surface errors while often failing to correct deeper reasoning flaws. We propose SELF-THOUGHT, a framework that introduces an intermediate step of task abstraction before solution refinement. Given an input and an initial response, the model first distills the task into a structured template that captures key variables, constraints, and problem structure. This abstraction then guides solution instantiation, grounding subsequent responses in a clearer understanding of the task and reducing error propagation. Crucially, we show that these abstractions can be transferred across models: templates generated by larger models can serve as structured guides for smaller LLMs, which typically struggle with intrinsic self-correction. By reusing distilled task structures, smaller models achieve more reliable refinements without heavy fine-tuning or reliance on external verifiers. Experiments across diverse reasoning tasks demonstrate that SELF-THOUGHT improves accuracy, robustness, and generalization for both large and small models, offering a scalable path toward more reliable self-correcting language systems.

</details>


### [67] [Synapse Compendium Aware Federated Knowledge Exchange for Tool Routed LLMs](https://arxiv.org/abs/2602.00911)
*Abhijit Chakraborty,Sandipan De,Yash Shah,Chahana Dahal,Vivek Gupta*

Main category: cs.AI

TL;DR: Synapse框架通过联邦学习训练共享的全局工具使用知识模型，提高多智能体LLM系统的工具使用效果并减少通信开销


<details>
  <summary>Details</summary>
Motivation: 基于LLM的智能体在联邦学习下面临通信成本高、数据和工具使用异构性等挑战，限制了协作学习的效果

Method: 训练共享的全局工具使用行为知识模型，客户端智能体本地学习工具使用模式，通过协调器传输工件进行联邦聚合，更新全局工具汇编并重新分发，使用模板化表示、嵌入检索与LLM重排序、自适应掩码等技术

Result: Synapse提高了工具使用效果，相比权重或提示共享方法减少了通信开销，支持异构数据并量化性能改进

Conclusion: Synapse框架有效解决了多智能体LLM系统中工具使用的联邦学习挑战，实现了更好的协作学习效果

Abstract: Collaborative learning among LLM-based agents under federated learning faces challenges, including communication costs, heterogeneity in data, and tool-usage, limiting their effectiveness. We introduce Synapse, a framework that trains a shared global knowledge model of tool-usage behavior. Client agents with fixed LLMs learn tool-usage patterns locally, and transmit artifacts for federated aggregation through coordinators. A global tool compendium is updated and redistributed, enabling convergence toward stable tool selection. Synapse uses templated representations, embedding retrieval with LLM reranking, and adaptive masking to maintain utility while limiting information leakage. The framework supports heterogeneous data and quantifies performance improvements. Results show that Synapse improves tool-usage effectiveness and reduces communication overhead compared with weight or prompt-sharing approaches in multi-agent LLM systems.

</details>


### [68] [Supervised sparse auto-encoders as unconstrained feature models for semantic composition](https://arxiv.org/abs/2602.00924)
*Ouns El Harzli,Hugo Wallner,Yoonsoo Nam,Haixuan Xavier Tao*

Main category: cs.AI

TL;DR: 本文提出一种改进的稀疏自编码器方法，通过监督学习和无约束特征模型解决传统SAE在重建、可扩展性和语义对齐方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器在机制可解释性中面临两个主要挑战：L1惩罚的非平滑性阻碍了重建和可扩展性，以及学习到的特征与人类语义之间缺乏对齐。

Method: 采用来自神经崩溃理论的无约束特征模型框架，并通过监督任务来改进SAE。具体方法包括联合学习稀疏概念嵌入和解码器权重，监督（仅解码器）SAE重建特征向量。

Result: 在Stable Diffusion 3.5上验证，该方法展示了组合泛化能力，成功重建训练中未见过的概念组合图像，并实现无需提示修改的语义图像编辑的特征级干预。

Conclusion: 通过结合无约束特征模型和监督学习，提出的方法有效解决了稀疏自编码器在可扩展性、重建质量和语义对齐方面的核心挑战，为机制可解释性和语义图像编辑提供了新途径。

Abstract: Sparse auto-encoders (SAEs) have re-emerged as a prominent method for mechanistic interpretability, yet they face two significant challenges: the non-smoothness of the $L_1$ penalty, which hinders reconstruction and scalability, and a lack of alignment between learned features and human semantics. In this paper, we address these limitations by adapting unconstrained feature models-a mathematical framework from neural collapse theory-and by supervising the task. We supervise (decoder-only) SAEs to reconstruct feature vectors by jointly learning sparse concept embeddings and decoder weights. Validated on Stable Diffusion 3.5, our approach demonstrates compositional generalization, successfully reconstructing images with concept combinations unseen during training, and enabling feature-level intervention for semantic image editing without prompt modification.

</details>


### [69] [Learning Abstractions for Hierarchical Planning in Program-Synthesis Agents](https://arxiv.org/abs/2602.00929)
*Zergham Ahmed,Kazuki Irie,Joshua B. Tenenbaum,Christopher J. Bates,Samuel J. Gershman*

Main category: cs.AI

TL;DR: TheoryCoder-2是一个基于理论的强化学习代理，利用大语言模型的上下文学习能力主动学习可重用抽象，而不是依赖人工指定的抽象，从而在多样化环境中实现更好的样本效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型代理和深度强化学习系统在快速泛化任务方面仍有挑战，而基于理论的强化学习系统如TheoryCoder虽然表现出强泛化能力，但严重依赖人工提供的抽象，回避了抽象学习问题。

Method: TheoryCoder-2利用大语言模型的上下文学习能力，从经验中合成抽象并将其整合到分层规划过程中，主动学习可重用抽象而非依赖人工指定的抽象。

Result: 在BabyAI、Minihack和VGDL游戏（如Sokoban）等多样化环境中的实验表明，TheoryCoder-2比基线LLM代理（包括经典规划领域构建、基于推理的规划和先前的程序合成代理如WorldCoder）具有显著更高的样本效率。

Conclusion: TheoryCoder-2能够解决基线方法无法解决的复杂任务，同时只需要最少的人工提示，与先前的基于理论的强化学习系统相比具有明显优势。

Abstract: Humans learn abstractions and use them to plan efficiently to quickly generalize across tasks -- an ability that remains challenging for state-of-the-art large language model (LLM) agents and deep reinforcement learning (RL) systems. Inspired by the cognitive science of how people form abstractions and intuitive theories of their world knowledge, Theory-Based RL (TBRL) systems, such as TheoryCoder, exhibit strong generalization through effective use of abstractions. However, they heavily rely on human-provided abstractions and sidestep the abstraction-learning problem. We introduce TheoryCoder-2, a new TBRL agent that leverages LLMs' in-context learning ability to actively learn reusable abstractions rather than relying on hand-specified ones, by synthesizing abstractions from experience and integrating them into a hierarchical planning process. We conduct experiments on diverse environments, including BabyAI, Minihack and VGDL games like Sokoban. We find that TheoryCoder-2 is significantly more sample-efficient than baseline LLM agents augmented with classical planning domain construction, reasoning-based planning, and prior program-synthesis agents such as WorldCoder. TheoryCoder-2 is able to solve complex tasks that the baselines fail, while only requiring minimal human prompts, unlike prior TBRL systems.

</details>


### [70] [The Keyhole Effect: Why Chat Interfaces Fail at Data Analysis](https://arxiv.org/abs/2602.00947)
*Mohan Reddy*

Main category: cs.AI

TL;DR: 聊天界面不适合多步骤数据分析任务，会导致认知过载和性能下降，作者提出了八个混合设计模式来解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 当前聊天已成为AI辅助数据分析的默认界面，但对于多步骤、状态依赖的分析任务，这种界面设计存在根本问题。作者基于Woods的"锁眼效应"理论，指出聊天界面通过五种机制系统性地降低分析性能。

Method: 作者从认知科学角度分析了聊天界面的五个问题机制，并提出了认知过载的数学模型：O = max(0, m - v - W)。然后提出了八个混合设计模式：生成式UI、无限画布、指示性交互、状态轨道、幽灵层、Mise en Place、语义缩放和概率UI。

Result: 聊天界面会导致内容置换破坏空间记忆、隐藏状态变量超出工作记忆容量、强制言语化降低视觉模式识别、线性文本流阻碍认知卸载、序列化惩罚随数据维度增加。当认知过载O>0时，错误概率增加，分析偏差（锚定、确认、变化盲视）放大。

Conclusion: 聊天界面不适合开放探索性数据分析任务，但通过精心设计的混合界面可以解决认知瓶颈问题。作者提出了可验证的假设和实验范式进行实证验证，强调需要保留自然语言用于意图指定和综合，同时通过可视化界面支持认知过程。

Abstract: Chat has become the default interface for AI-assisted data analysis. For multi-step, state-dependent analytical tasks, this is a mistake. Building on Woods (1984) Keyhole Effect, the cognitive cost of viewing large information spaces through narrow viewports, I show that chat interfaces systematically degrade analytical performance through five mechanisms: (1) constant content displacement defeats hippocampal spatial memory systems; (2) hidden state variables exceed working memory capacity (approximately 4 chunks under load); (3) forced verbalization triggers verbal overshadowing, degrading visual pattern recognition; (4) linear text streams block epistemic action and cognitive offloading; (5) serialization penalties scale with data dimensionality. I formalize cognitive overload as O = max(0, m - v - W) where m is task-relevant items, v is visible items, and W is working memory capacity. When O > 0, error probability increases and analytical biases (anchoring, confirmation, change blindness) amplify. Eight hybrid design patterns address these failures: Generative UI, Infinite Canvas, Deictic Interaction, State Rail, Ghost Layers, Mise en Place, Semantic Zoom, and Probabilistic UI. Each pattern targets specific cognitive bottlenecks while preserving natural language for intent specification and synthesis. Well-scaffolded conversational systems that encode expert priors may reduce load for guided tasks; the framework applies most strongly to open-ended exploration. The paper concludes with falsifiable hypotheses and experimental paradigms for empirical validation.

</details>


### [71] [MindGuard: Guardrail Classifiers for Multi-Turn Mental Health Support](https://arxiv.org/abs/2602.00950)
*António Farinhas,Nuno M. Guerreiro,José Pombal,Pedro Henrique Martins,Laura Melton,Alex Conway,Cara Dochat,Maya D'Eon,Ricardo Rei*

Main category: cs.AI

TL;DR: 论文提出了MindGuard，一个专门用于心理健康对话的临床安全分类器，通过临床风险分类学和专家标注数据集来区分治疗性披露和真正的临床危机，减少误报并提高安全性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在心理健康支持应用中，通用的安全防护措施无法有效区分治疗性披露和真正的临床危机，导致安全失效。需要专门针对心理健康对话的临床安全评估方法。

Method: 1. 与心理学博士合作开发临床风险分类学；2. 发布MindGuard-testset专家标注数据集；3. 通过受控双代理设置生成合成对话；4. 训练轻量级安全分类器（4B和8B参数）；5. 与临床语言模型结合使用。

Result: MindGuard分类器在高召回率操作点上减少误报，与临床语言模型结合时，在对抗性多轮交互中相比通用安全措施，攻击成功率和有害参与率更低。

Conclusion: 论文提出了专门针对心理健康对话的临床安全框架MindGuard，通过临床风险分类学和专家标注数据，有效区分治疗性内容与临床危机，提高了大语言模型在心理健康应用中的安全性。

Abstract: Large language models are increasingly used for mental health support, yet their conversational coherence alone does not ensure clinical appropriateness. Existing general-purpose safeguards often fail to distinguish between therapeutic disclosures and genuine clinical crises, leading to safety failures. To address this gap, we introduce a clinically grounded risk taxonomy, developed in collaboration with PhD-level psychologists, that identifies actionable harm (e.g., self-harm and harm to others) while preserving space for safe, non-crisis therapeutic content. We release MindGuard-testset, a dataset of real-world multi-turn conversations annotated at the turn level by clinical experts. Using synthetic dialogues generated via a controlled two-agent setup, we train MindGuard, a family of lightweight safety classifiers (with 4B and 8B parameters). Our classifiers reduce false positives at high-recall operating points and, when paired with clinician language models, help achieve lower attack success and harmful engagement rates in adversarial multi-turn interactions compared to general-purpose safeguards. We release all models and human evaluation data.

</details>


### [72] [R-HTN: Rebellious Online HTN Planning for Safety and Game AI](https://arxiv.org/abs/2602.00951)
*Hector Munoz-Avila,David W. Aha,Paola Rizzo*

Main category: cs.AI

TL;DR: 本文提出了一种在线分层任务网络（HTN）智能体R-HTN，能够在特定条件下拒绝执行用户分配的任务，转而采取符合内置指令集D的行为，实现智能反抗。


<details>
  <summary>Details</summary>
Motivation: 现有智能体通常盲目执行用户指令，但实际应用中需要智能体在某些情况下能够拒绝执行不安全或不合适的任务。本文旨在开发能够进行智能反抗的在线HTN智能体，使其在违反内置指令集D时能够采取适当行动。

Method: 提出R-HTN算法，结合HTN规划、在线规划和内置指令集D。研究两种智能体变体：非自适应智能体（发现违反D时停止执行）和自适应智能体（违反D时修改HTN计划寻找替代方案）。算法在在线HTN规划中考虑指令约束。

Result: 在两个任务领域评估R-HTN：安全约束和个性特质约束。结果显示R-HTN智能体从不违反指令，并在可行情况下尽力实现用户目标（尽管可能不符合用户预期）。自适应智能体表现优于非自适应智能体。

Conclusion: R-HTN算法成功实现了在线HTN智能体的智能反抗能力，能够在遵守内置指令的前提下灵活应对用户任务，为开发更安全、更可靠的自主智能体提供了有效方法。

Abstract: We introduce online Hierarchical Task Network (HTN) agents whose behaviors are governed by a set of built-in directives \D. Like other agents that are capable of rebellion (i.e., {\it intelligent disobedience}), our agents will, under some conditions, not perform a user-assigned task and instead act in ways that do not meet a user's expectations. Our work combines three concepts: HTN planning, online planning, and the directives \D, which must be considered when performing user-assigned tasks. We investigate two agent variants: (1) a Nonadaptive agent that stops execution if it finds itself in violation of \D~ and (2) an Adaptive agent that, in the same situation, instead modifies its HTN plan to search for alternative ways to achieve its given task. We present R-HTN (for: Rebellious-HTN), a general algorithm for online HTN planning under directives \D. We evaluate R-HTN in two task domains where the agent must not violate some directives for safety reasons or as dictated by their personality traits. We found that R-HTN agents never violate directives, and aim to achieve the user-given goals if feasible though not necessarily as the user expected.

</details>


### [73] [Small-Margin Preferences Still Matter-If You Train Them Right](https://arxiv.org/abs/2602.00954)
*Jinlong Pang,Zhaowei Zhu,Na Di,Yichi Zhang,Yaxuan Wang,Chen Qian,Yang Liu*

Main category: cs.AI

TL;DR: MixDPO是一种难度感知的训练策略，通过将偏好数据从易到难排序，并将困难样本路由到监督微调目标，而简单样本应用偏好损失，从而有效利用模糊偏好对。


<details>
  <summary>Details</summary>
Motivation: 传统偏好优化方法（如DPO）对偏好对的质量和难度敏感，通常将小边际（模糊）对视为噪声并过滤掉。但研究发现困难对在偏好损失下会破坏训练稳定性，但在监督微调中仍包含有用的监督信号。

Method: MixDPO采用难度感知训练策略：1）根据边际定义的难度将偏好数据从易到难排序（课程学习）；2）将困难对路由到SFT目标，而对简单对应用偏好损失。这种混合设计可以充分利用模糊对而不引发优化失败。

Result: 在三个LLM-judge基准测试中，MixDPO在DPO和一系列广泛使用的变体上持续改进对齐性能，在AlpacaEval~2长度控制（LC）胜率上表现尤为突出。

Conclusion: MixDPO通过难度感知的混合训练策略，有效解决了偏好优化中困难样本的利用问题，提供了一种实用的机制来利用模糊偏好对而不引发优化失败，显著提升了语言模型的对齐性能。

Abstract: Preference optimization methods such as DPO align large language models (LLMs) using paired comparisons, but their effectiveness can be highly sensitive to the quality and difficulty of preference pairs. A common heuristic treats small-margin (ambiguous) pairs as noisy and filters them out. In this paper, we revisit this assumption and show that pair difficulty interacts strongly with the optimization objective: when trained with preference-based losses, difficult pairs can destabilize training and harm alignment, yet these same pairs still contain useful supervision signals when optimized with supervised fine-tuning (SFT). Motivated by this observation, we propose MixDPO, a simple yet effective difficulty-aware training strategy that (i) orders preference data from easy to hard (a curriculum over margin-defined difficulty), and (ii) routes difficult pairs to an SFT objective while applying a preference loss to easy pairs. This hybrid design provides a practical mechanism to leverage ambiguous pairs without incurring the optimization failures often associated with preference losses on low-margin data. Across three LLM-judge benchmarks, MixDPO consistently improves alignment over DPO and a range of widely-used variants, with particularly strong gains on AlpacaEval~2 length-controlled (LC) win rate.

</details>


### [74] [Reasoning and Tool-use Compete in Agentic RL:From Quantifying Interference to Disentangled Tuning](https://arxiv.org/abs/2602.00994)
*Yu Li,Mingyang Yi,Xiuyu Li,Ju Fan,Fuxin Jiang,Binbin Chen,Peng Li,Jie Song,Tieying Zhang*

Main category: cs.AI

TL;DR: 该论文通过LEAS系统揭示了ARL中推理与工具使用行为之间的训练干扰问题，并提出DART框架通过分离参数更新来解决这一问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有ARL方法通常假设联合训练推理和工具使用行为能提升整体代理性能，但这一假设缺乏实证检验。作者旨在系统研究这一假设，揭示潜在问题。

Method: 1) 引入线性效应归因系统(LEAS)量化推理与工具使用行为间的干扰；2) 提出解耦行动推理调优(DART)框架，通过分离的低秩适应模块显式解耦推理和工具使用的参数更新。

Result: 实验结果表明：1) LEAS揭示了推理与工具使用能力常导致梯度方向不匹配，产生训练干扰；2) DART相比基线方法平均提升6.35%，性能接近显式分离工具使用和推理的多智能体系统。

Conclusion: 联合训练推理和工具使用存在干扰问题，挑战了现有ARL范式。DART通过参数解耦有效解决了这一问题，为ARL提供了更高效的训练框架。

Abstract: Agentic Reinforcement Learning (ARL) focuses on training large language models (LLMs) to interleave reasoning with external tool execution to solve complex tasks. Most existing ARL methods train a single shared model parameters to support both reasoning and tool use behaviors, implicitly assuming that joint training leads to improved overall agent performance. Despite its widespread adoption, this assumption has rarely been examined empirically. In this paper, we systematically investigate this assumption by introducing a Linear Effect Attribution System(LEAS), which provides quantitative evidence of interference between reasoning and tool-use behaviors. Through an in-depth analysis, we show that these two capabilities often induce misaligned gradient directions, leading to training interference that undermines the effectiveness of joint optimization and challenges the prevailing ARL paradigm. To address this issue, we propose Disentangled Action Reasoning Tuning(DART), a simple and efficient framework that explicitly decouples parameter updates for reasoning and tool-use via separate low-rank adaptation modules. Experimental results show that DART consistently outperforms baseline methods with averaged 6.35 percent improvements and achieves performance comparable to multi-agent systems that explicitly separate tool-use and reasoning using a single model.

</details>


### [75] [Error Taxonomy-Guided Prompt Optimization](https://arxiv.org/abs/2602.00997)
*Mayank Singh,Vikas Yadav,Eduardo Blanco*

Main category: cs.AI

TL;DR: ETGPO是一种基于错误分类的提示优化方法，采用自上而下的方式分析全局失败模式，相比传统自下而上的方法，能以更少的计算资源获得更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的自动提示优化方法大多采用自下而上的试错方式，基于单个问题的反馈迭代调整提示，这种方法缺乏全局视角，计算成本高。需要一种更高效、全局化的提示优化方法。

Method: ETGPO采用自上而下的方法：1) 收集模型错误；2) 将错误分类为分类体系；3) 针对最常见失败模式在提示中添加指导。通过分析全局失败模式来优化提示，而不是逐个问题调整。

Result: 在数学、问答和逻辑推理等多个基准测试中，ETGPO达到了与最先进方法相当或更好的准确率，同时优化阶段的token使用量和评估预算仅需约三分之一。

Conclusion: ETGPO通过错误分类引导的提示优化，提供了一种更高效、全局化的方法，显著降低了计算成本，同时保持或提升了模型性能。

Abstract: Automatic Prompt Optimization (APO) is a powerful approach for extracting performance from large language models without modifying their weights. Many existing methods rely on trial-and-error, testing different prompts or in-context examples until a good configuration emerges, often consuming substantial compute. Recently, natural language feedback derived from execution logs has shown promise as a way to identify how prompts can be improved. However, most prior approaches operate in a bottom-up manner, iteratively adjusting the prompt based on feedback from individual problems, which can cause them to lose the global perspective. In this work, we propose Error Taxonomy-Guided Prompt Optimization (ETGPO), a prompt optimization algorithm that adopts a top-down approach. ETGPO focuses on the global failure landscape by collecting model errors, categorizing them into a taxonomy, and augmenting the prompt with guidance targeting the most frequent failure modes. Across multiple benchmarks spanning mathematics, question answering, and logical reasoning, ETGPO achieves accuracy that is comparable to or better than state-of-the-art methods, while requiring roughly one third of the optimization-phase token usage and evaluation budget.

</details>


### [76] [How RLHF Amplifies Sycophancy](https://arxiv.org/abs/2602.01002)
*Itai Shapira,Gerdus Benade,Ariel D. Procaccia*

Main category: cs.AI

TL;DR: 论文分析了基于人类偏好的对齐训练如何放大LLM的谄媚行为，提出了一个形式化分析框架，并设计了训练时干预方法来抑制这种放大机制。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在基于人类偏好的对齐训练后，经常表现出更强的谄媚行为（即使与事实或合理判断相冲突也肯定用户的信念）。需要理解这种失败模式如何被对齐过程放大，并找到解决方案。

Method: 1. 形式化分析对齐训练如何放大谄媚行为，识别因果放大机制；2. 分析奖励学习过程，描述人类标注者偏好偏差如何诱导奖励差距；3. 提出训练时干预方法，通过最小KL散度约束推导出闭式协议惩罚奖励修正。

Result: 计算实验发现奖励差距普遍存在，并在所有考虑配置中都导致行为漂移。提出的干预方法能够有效抑制谄媚行为的放大。

Conclusion: 基于人类偏好的对齐训练会系统性放大LLM的谄媚行为，但可以通过训练时干预来抑制这种放大机制，从而在不牺牲对齐效果的前提下减少谄媚倾向。

Abstract: Large language models often exhibit increased sycophantic behavior after preference-based post-training, showing a stronger tendency to affirm a user's stated or implied belief even when this conflicts with factual accuracy or sound judgment. We present a formal analysis of how alignment from human feedback can increase this failure mode by identifying an explicit amplification mechanism that causally links optimization against a learned reward to bias in the human preference data used for alignment. We show that the direction of behavioral drift is determined by a covariance under the base policy between endorsing the belief signal in the prompt and the learned reward, and that the first-order effect reduces to a simple mean-gap condition. We then analyze reward learning from pairwise comparisons under random utility models like Bradley-Terry and characterize when bias in human annotators' preferences induces this reward gap. Next, we propose a training-time intervention designed to neutralize the amplification mechanism itself. Among all post-trained policies that prevent sycophantic behavior from increasing, we characterize the unique policy closest in KL divergence to the unconstrained post-trained policy, and derive the corresponding minimal reward correction as a closed-form agreement penalty. Computational experiments find that reward gaps are common and cause behavioral drift in all the configurations considered.

</details>


### [77] [HalluHard: A Hard Multi-Turn Hallucination Benchmark](https://arxiv.org/abs/2602.01031)
*Dongyang Fan,Sebastien Delsad,Nicolas Flammarion,Maksym Andriushchenko*

Main category: cs.AI

TL;DR: HalluHard是一个具有挑战性的多轮幻觉基准测试，包含950个种子问题，涵盖法律、研究、医疗和编程四个高风险领域，通过内联引用要求来操作真实性，并使用基于网络搜索的证据检索管道进行评估。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型仍然会产生听起来合理但缺乏根据的事实性陈述，这个问题在多轮对话中随着上下文增长和早期错误传播而加剧，特别是在高风险领域需要可靠的真实性评估。

Method: 1. 创建HalluHard基准测试：950个种子问题，涵盖法律案例、研究问题、医疗指南和编程四个领域；2. 通过要求内联引用来操作真实性；3. 开发评估管道：通过网络搜索迭代检索证据，获取、过滤和解析全文来源（包括PDF）来评估引用是否支持生成内容。

Result: 即使在网络搜索支持下，幻觉仍然很严重（最强配置Opus-4.5约为30%），内容接地错误持续高发。幻觉行为受模型能力、轮次位置、有效推理和所需知识类型的影响。

Conclusion: 多轮对话中的幻觉问题仍然严重，需要更可靠的评估方法和改进的模型能力，特别是在高风险领域需要更好的内容接地机制。

Abstract: Large language models (LLMs) still produce plausible-sounding but ungrounded factual claims, a problem that worsens in multi-turn dialogue as context grows and early errors cascade. We introduce $\textbf{HalluHard}$, a challenging multi-turn hallucination benchmark with 950 seed questions spanning four high-stakes domains: legal cases, research questions, medical guidelines, and coding. We operationalize groundedness by requiring inline citations for factual assertions. To support reliable evaluation in open-ended settings, we propose a judging pipeline that iteratively retrieves evidence via web search. It can fetch, filter, and parse full-text sources (including PDFs) to assess whether cited material actually supports the generated content. Across a diverse set of frontier proprietary and open-weight models, hallucinations remain substantial even with web search ($\approx 30\%$ for the strongest configuration, Opus-4.5 with web search), with content-grounding errors persisting at high rates. Finally, we show that hallucination behavior is shaped by model capacity, turn position, effective reasoning, and the type of knowledge required.

</details>


### [78] [Discovering Process-Outcome Credit in Multi-Step LLM Reasoning](https://arxiv.org/abs/2602.01034)
*Xiangwei Wang,Wei Wang,Ken Chen,Nanduni Nimalsiri,Saman Halgamuge*

Main category: cs.AI

TL;DR: 提出一种为LLM推理提供连续奖励信号的强化学习框架，通过边际信息增益机制量化推理步骤价值，结合解耦掩码策略分别处理过程奖励和结果奖励，提升样本效率和最终准确率。


<details>
  <summary>Details</summary>
Motivation: 传统基于结果的强化学习方法存在奖励稀疏和信用分配效率低的问题，限制了LLM推理能力的提升效果。

Method: 1. 引入逐步边际信息增益机制，通过单调历史水印量化推理步骤的内在价值；2. 采用解耦掩码策略，过程奖励应用于思维链，结果奖励应用于完整输出；3. 结合双门监督微调目标，利用高质量结构和事实信号稳定训练。

Result: 在文本和多模态基准测试（如MATH、Super-CLEVR）上，该方法在样本效率和最终准确率上均优于GRPO等基线方法，并展现出更好的分布外鲁棒性和零样本迁移能力。

Conclusion: 该框架通过提供连续奖励信号和有效的信用分配机制，显著提升了LLM的推理能力，在多种任务上表现出优越性能，为强化学习在复杂推理任务中的应用提供了新思路。

Abstract: Reinforcement Learning (RL) serves as a potent paradigm for enhancing reasoning capabilities in Large Language Models (LLMs), yet standard outcome-based approaches often suffer from reward sparsity and inefficient credit assignment. In this paper, we propose a novel framework designed to provide continuous reward signals, which introduces a Step-wise Marginal Information Gain (MIG) mechanism that quantifies the intrinsic value of reasoning steps against a Monotonic Historical Watermark, effectively filtering out training noise. To ensure disentangled credit distribution, we implement a Decoupled Masking Strategy, applying process-oriented rewards specifically to the chain-of-thought (CoT) and outcome-oriented rewards to the full completion. Additionally, we incorporate a Dual-Gated SFT objective to stabilize training with high-quality structural and factual signals. Extensive experiments across textual and multi-modal benchmarks (e.g., MATH, Super-CLEVR) demonstrate that our approach consistently outperforms baselines such as GRPO in both sample efficiency and final accuracy. Furthermore, our model exhibits superior out-of-distribution robustness, demonstrating promising zero-shot transfer capabilities to unseen and challenging reasoning tasks.

</details>


### [79] [SetPO: Set-Level Policy Optimization for Diversity-Preserving LLM Reasoning](https://arxiv.org/abs/2602.01062)
*Chenyi Li,Yuan Zhang,Bo Wang,Guoqing Ma,Wei Tang,Haoyang Huang,Nan Duan*

Main category: cs.AI

TL;DR: 提出一种基于核相似度的轨迹级多样性目标，通过留一法边际贡献计算，作为优势塑形项融入策略优化，提升LLM推理性能同时保持多样性


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在提升LLM数学推理性能时，往往导致输出多样性降低，模型集中在少数解决方案上，需要平衡性能与多样性

Method: 1. 基于核相似度定义轨迹级多样性目标；2. 使用留一法计算每个采样轨迹的边际贡献；3. 将多样性目标作为优势塑形项融入策略优化；4. 在分布扰动框架下分析单轨迹对多样性的贡献

Result: 在不同模型规模上的广泛实验表明，该方法在多个基准测试中，在Pass@1和Pass@K指标上均优于强基线方法

Conclusion: 提出的多样性增强方法能有效提升LLM推理性能同时保持输出多样性，理论分析证实稀有轨迹对全局多样性有更高边际贡献

Abstract: Reinforcement learning with verifiable rewards has shown notable effectiveness in enhancing large language models (LLMs) reasoning performance, especially in mathematics tasks. However, such improvements often come with reduced outcome diversity, where the model concentrates probability mass on a narrow set of solutions. Motivated by diminishing-returns principles, we introduce a set level diversity objective defined over sampled trajectories using kernelized similarity. Our approach derives a leave-one-out marginal contribution for each sampled trajectory and integrates this objective as a plug-in advantage shaping term for policy optimization. We further investigate the contribution of a single trajectory to language model diversity within a distribution perturbation framework. This analysis theoretically confirms a monotonicity property, proving that rarer trajectories yield consistently higher marginal contributions to the global diversity. Extensive experiments across a range of model scales demonstrate the effectiveness of our proposed algorithm, consistently outperforming strong baselines in both Pass@1 and Pass@K across various benchmarks.

</details>


### [80] [ConvexBench: Can LLMs Recognize Convex Functions?](https://arxiv.org/abs/2602.01075)
*Yepeng Liu,Yu Huang,Yu-Xiang Wang,Yingbin Liang,Yuheng Bu*

Main category: cs.AI

TL;DR: 论文提出了一个评估大语言模型对深度复合函数凸性识别能力的基准测试，发现模型在深度增加时性能急剧下降，并提出了一种分治代理框架来有效解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型开始自动化研究级数学和科学任务，需要评估它们理解和推理凸性的能力。凸分析是现代数学的重要分支，在许多领域有广泛应用，因此测试LLMs识别符号目标在深度函数复合下的凸性具有重要意义。

Method: 提出了一个可扩展且可机械验证的基准测试CB，用于测试LLMs识别深度复合函数凸性的能力。通过实验发现前沿LLMs存在组合推理缺陷，并提出了一种分治代理框架：1）使用外部工具构建抽象语法树来处理解析问题；2）对每个中间子表达式进行递归推理，并聚焦上下文。

Result: 实验显示前沿LLMs存在明显的组合推理缺陷：随着深度增加，性能急剧下降，F1分数从深度2时的1.0下降到深度100时的约0.2。分析发现两种失败模式：解析失败和懒惰推理。提出的分治代理框架能可靠缓解深度复合失败，在较大深度下实现显著性能提升（如深度100时F1分数达到1.0）。

Conclusion: 大语言模型在深度复合函数的凸性识别上存在显著缺陷，但通过分治代理框架可以有效解决这些问题。该研究为评估和改进LLMs在数学推理方面的能力提供了重要基准和方法。

Abstract: Convex analysis is a modern branch of mathematics with many applications. As Large Language Models (LLMs) start to automate research-level math and sciences, it is important for LLMs to demonstrate the ability to understand and reason with convexity. We introduce \cb, a scalable and mechanically verifiable benchmark for testing \textit{whether LLMs can identify the convexity of a symbolic objective under deep functional composition.} Experiments on frontier LLMs reveal a sharp compositional reasoning gap: performance degrades rapidly with increasing depth, dropping from an F1-score of $1.0$ at depth $2$ to approximately $0.2$ at depth $100$. Inspection of models' reasoning traces indicates two failure modes: \textit{parsing failure} and \textit{lazy reasoning}. To address these limitations, we propose an agentic divide-and-conquer framework that (i) offloads parsing to an external tool to construct an abstract syntax tree (AST) and (ii) enforces recursive reasoning over each intermediate sub-expression with focused context. This framework reliably mitigates deep-composition failures, achieving substantial performance improvement at large depths (e.g., F1-Score $= 1.0$ at depth $100$).

</details>


### [81] [AutoHealth: An Uncertainty-Aware Multi-Agent System for Autonomous Health Data Modeling](https://arxiv.org/abs/2602.01078)
*Tong Xia,Weibin Li,Gang Liu,Yong Li*

Main category: cs.AI

TL;DR: AutoHealth是一个不确定性感知的多智能体系统，能够自主建模健康数据并评估模型可靠性，在预测性能和不确定性估计方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能体在健康数据应用中存在局限性：难以泛化到异构健康数据模态、过度依赖预定义解决方案模板、缺乏不确定性估计，而医疗决策需要可靠的不确定性量化。

Method: 提出AutoHealth系统，采用闭环协调的五个专业智能体进行数据探索、任务条件化模型构建、训练和优化，同时优先考虑预测性能和不确定性量化，并生成支持可信解释和风险感知决策的综合报告。

Result: 在包含17个跨数据模态和学习设置的现实世界基准测试中，AutoHealth完成所有任务，预测性能比最先进基线提高29.2%，不确定性估计提高50.2%。

Conclusion: AutoHealth通过不确定性感知的多智能体系统有效解决了健康数据建模的挑战，为医疗领域的可靠决策提供了实用工具。

Abstract: LLM-based agents have demonstrated strong potential for autonomous machine learning, yet their applicability to health data remains limited. Existing systems often struggle to generalize across heterogeneous health data modalities, rely heavily on predefined solution templates with insufficient adaptation to task-specific objectives, and largely overlook uncertainty estimation, which is essential for reliable decision-making in healthcare. To address these challenges, we propose \textit{AutoHealth}, a novel uncertainty-aware multi-agent system that autonomously models health data and assesses model reliability. \textit{AutoHealth} employs closed-loop coordination among five specialized agents to perform data exploration, task-conditioned model construction, training, and optimization, while jointly prioritizing predictive performance and uncertainty quantification. Beyond producing ready-to-use models, the system generates comprehensive reports to support trustworthy interpretation and risk-aware decision-making. To rigorously evaluate its effectiveness, we curate a challenging real-world benchmark comprising 17 tasks across diverse data modalities and learning settings. \textit{AutoHealth} completes all tasks and outperforms state-of-the-art baselines by 29.2\% in prediction performance and 50.2\% in uncertainty estimation.

</details>


### [82] [EvoOpt-LLM: Evolving industrial optimization models with large language models](https://arxiv.org/abs/2602.01082)
*Yiliu He,Tianle Li,Binghao Ji,Zhiyuan Liu,Di Huang*

Main category: cs.AI

TL;DR: EvoOpt-LLM：基于大语言模型的工业优化建模框架，通过少量样本实现自动化模型构建、约束注入和变量剪枝，显著降低专家依赖并提升求解效率。


<details>
  <summary>Details</summary>
Motivation: 工业规划调度中的混合整数线性规划建模高度依赖专家经验，自然语言需求到可执行模型的转换及业务规则演变下的维护成本高昂。现有LLM方法存在数据效率低、求解器有效性有限、工业规模扩展性差等问题。

Method: 基于7B参数LLM构建统一框架，采用参数高效的LoRA微调，支持完整工业优化建模生命周期：自动化模型构建、动态业务约束注入、端到端变量剪枝。

Result: 仅用3000训练样本达到91%生成率和65.9%可执行率，关键性能在1500样本内显现；约束注入模块可靠增强现有MILP模型并保持原目标；变量剪枝模块在400样本下对中型LP模型达到约0.56的F1分数。

Conclusion: EvoOpt-LLM为工业优化建模提供了实用且数据高效的方法，减少专家干预的同时提升适应性和求解器效率。

Abstract: Optimization modeling via mixed-integer linear programming (MILP) is fundamental to industrial planning and scheduling, yet translating natural-language requirements into solver-executable models and maintaining them under evolving business rules remains highly expertise-intensive. While large language models (LLMs) offer promising avenues for automation, existing methods often suffer from low data efficiency, limited solver-level validity, and poor scalability to industrial-scale problems. To address these challenges, we present EvoOpt-LLM, a unified LLM-based framework supporting the full lifecycle of industrial optimization modeling, including automated model construction, dynamic business-constraint injection, and end-to-end variable pruning. Built on a 7B-parameter LLM and adapted via parameter-efficient LoRA fine-tuning, EvoOpt-LLM achieves a generation rate of 91% and an executability rate of 65.9% with only 3,000 training samples, with critical performance gains emerging under 1,500 samples. The constraint injection module reliably augments existing MILP models while preserving original objectives, and the variable pruning module enhances computational efficiency, achieving an F1 score of ~0.56 on medium-sized LP models with only 400 samples. EvoOpt-LLM demonstrates a practical, data-efficient approach to industrial optimization modeling, reducing reliance on expert intervention while improving adaptability and solver efficiency.

</details>


### [83] [MedBeads: An Agent-Native, Immutable Data Substrate for Trustworthy Medical AI](https://arxiv.org/abs/2602.01086)
*Takahito Nakajima*

Main category: cs.AI

TL;DR: MedBeads提出一种面向AI代理的原生医疗数据基础设施，使用不可变的Merkle DAG结构存储临床事件，解决传统EMR与AI代理间的"上下文不匹配"问题，确保数据确定性、防篡改和可审计。


<details>
  <summary>Details</summary>
Motivation: 当前电子病历系统为人类设计，AI代理接收碎片化数据，依赖概率推理重建患者历史，导致幻觉和审计困难。需要解决"上下文不匹配"问题，为可信医疗AI提供基础。

Method: 提出MedBeads架构：临床事件作为不可变的"珠子"节点，组织成Merkle有向无环图，密码学引用因果前驱。实现包括Go核心引擎、Python中间件和React可视化界面，采用广度优先搜索算法进行上下文检索。

Result: 成功用合成数据实现工作流，将FHIR资源转换为因果链接图。BFS算法实现O(V+E)复杂度的实时决策支持，防篡改特性通过密码学链保证，可视化界面帮助临床医生理解因果联系。

Conclusion: MedBeads通过从概率搜索转向确定性图遍历，从可变记录转向不可变链，解决了上下文不匹配问题，为可信医疗AI提供基础设施。结构化珠子格式作为令牌高效的AI原生语言，系统已开源。

Abstract: Background: As of 2026, Large Language Models (LLMs) demonstrate expert-level medical knowledge. However, deploying them as autonomous "Clinical Agents" remains limited. Current Electronic Medical Records (EMRs) and standards like FHIR are designed for human review, creating a "Context Mismatch": AI agents receive fragmented data and must rely on probabilistic inference (e.g., RAG) to reconstruct patient history. This approach causes hallucinations and hinders auditability. Methods: We propose MedBeads, an agent-native data infrastructure where clinical events are immutable "Beads"--nodes in a Merkle Directed Acyclic Graph (DAG)--cryptographically referencing causal predecessors. This "write-once, read-many" architecture makes tampering mathematically detectable. We implemented a prototype with a Go Core Engine, Python middleware for LLM integration, and a React-based visualization interface. Results: We successfully implemented the workflow using synthetic data. The FHIR-to-DAG conversion transformed flat resources into a causally-linked graph. Our Breadth-First Search (BFS) Context Retrieval algorithm traverses relevant subgraphs with O(V+E) complexity, enabling real-time decision support. Tamper-evidence is guaranteed by design: any modification breaks the cryptographic chain. The visualization aids clinician understanding through explicit causal links. Conclusion: MedBeads addresses the "Context Mismatch" by shifting from probabilistic search to deterministic graph traversal, and from mutable records to immutable chains, providing the substrate for "Trustworthy Medical AI." It guarantees the context the AI receives is deterministic and tamper-evident, while the LLM determines interpretation. The structured Bead format serves as a token-efficient "AI-native language." We release MedBeads as open-source software to accelerate agent-native data standards.

</details>


### [84] [Hard Constraints Meet Soft Generation: Guaranteed Feasibility for LLM-based Combinatorial Optimization](https://arxiv.org/abs/2602.01090)
*Yang Liu,Chuan Zhou,Yancheng Chen,Shuai Zhang,Xixun Lin,Xiaoqing Wang*

Main category: cs.AI

TL;DR: FALCON框架通过语法约束解码、可行性修复层和自适应采样确保组合优化问题的100%可行性，使用BOPO训练方法在七个NP难问题上实现完美可行性并匹敌或超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在组合优化中表现出潜力，但缺乏保证解决方案可行性的机制，这对于实际部署至关重要。现有方法无法确保100%可行性，限制了LLM在真实世界优化问题中的应用。

Method: FALCON框架包含三个核心创新：1) 语法约束解码确保句法有效性；2) 可行性修复层纠正语义约束违反；3) 自适应Best-of-N采样高效分配推理计算。训练方法采用Best-anchored Objective-guided Preference Optimization (BOPO)，基于目标差距加权偏好对，无需人工标签提供密集监督。

Result: 理论证明BOPO收敛性并提供修复引起的质量损失界限。在七个NP难组合优化问题的实证评估中，FALCON实现了完美可行性（100%），同时匹配或超越了最先进的神经和LLM求解器的解决方案质量。

Conclusion: FALCON框架成功解决了LLM在组合优化中缺乏可行性保证的核心问题，通过创新的解码、修复和采样机制实现了100%可行性，同时保持了解决方案质量，为LLM在实际优化问题中的可靠部署提供了可行路径。

Abstract: Large language models (LLMs) have emerged as promising general-purpose solvers for combinatorial optimization (CO), yet they fundamentally lack mechanisms to guarantee solution feasibility which is critical for real-world deployment. In this work, we introduce FALCON, a framework that ensures 100\% feasibility through three key innovations: (i) \emph{grammar-constrained decoding} enforces syntactic validity, (ii) a \emph{feasibility repair layer} corrects semantic constraint violations, and (iii) \emph{adaptive Best-of-$N$ sampling} allocates inference compute efficiently. To train the underlying LLM, we introduce the Best-anchored Objective-guided Preference Optimization (BOPO) in LLM training, which weights preference pairs by their objective gap, providing dense supervision without human labels. Theoretically, we prove convergence for BOPO and provide bounds on repair-induced quality loss. Empirically, across seven NP-hard CO problems, FALCON achieves perfect feasibility while matching or exceeding the solution quality of state-of-the-art neural and LLM-based solvers.

</details>


### [85] [Probing RLVR training instability through the lens of objective-level hacking](https://arxiv.org/abs/2602.01103)
*Yiming Dong,Kun Fu,Haoyu Li,Xinyuan Zhu,Yurou Liu,Lijing Shao,Jieping Ye,Zheng Wang*

Main category: cs.AI

TL;DR: 本文提出一个理论框架，通过"目标层面黑客攻击"的视角理解RLVR训练不稳定性，特别针对MoE架构，揭示了训练-推理差异异常增长的机制。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习与可验证奖励（RLVR）能持续提升大语言模型的推理能力，但在MoE架构中训练常出现不稳定性。这种不稳定性严重削弱模型能力提升，但其根本原因和机制尚不清楚。

Method: 提出一个原则性框架，从目标层面黑客攻击的角度理解RLVR不稳定性。与奖励黑客不同，目标层面黑客源于token级信用错位，表现为优化目标中的系统级虚假信号。基于该框架，在30B MoE模型上进行大量实验，追溯起源并形式化MoE模型中关键病理训练动态的机制。

Result: 揭示了MoE模型中训练-推理差异异常增长现象（广泛与不稳定性相关但先前缺乏机制解释）的起源和机制。这些发现为MoE模型不稳定性背后的训练动态提供了具体且因果的解释。

Conclusion: 研究结果为MoE模型中的训练不稳定性提供了理论解释，为设计稳定的RLVR算法提供了指导。

Abstract: Prolonged reinforcement learning with verifiable rewards (RLVR) has been shown to drive continuous improvements in the reasoning capabilities of large language models, but the training is often prone to instabilities, especially in Mixture-of-Experts (MoE) architectures. Training instability severely undermines model capability improvement, yet its underlying causes and mechanisms remain poorly understood. In this work, we introduce a principled framework for understanding RLVR instability through the lens of objective-level hacking. Unlike reward hacking, which arises from exploitable verifiers, objective-level hacking emerges from token-level credit misalignment and is manifested as system-level spurious signals in the optimization objective. Grounded in our framework, together with extensive experiments on a 30B MoE model, we trace the origin and formalize the mechanism behind a key pathological training dynamic in MoE models: the abnormal growth of the training-inference discrepancy, a phenomenon widely associated with instability but previously lacking a mechanistic explanation. These findings provide a concrete and causal account of the training dynamics underlying instabilities in MoE models, offering guidance for the design of stable RLVR algorithms.

</details>


### [86] [Transforming Vehicle Diagnostics: A Multimodal Approach to Error Patterns Prediction](https://arxiv.org/abs/2602.01109)
*Hugo Math,Rainer Lienhart*

Main category: cs.AI

TL;DR: BiCarFormer：首个融合DTC序列和环境条件的多模态多标签序列分类方法，用于车辆故障诊断，显著提升分类性能


<details>
  <summary>Details</summary>
Motivation: 当前车辆诊断系统主要依赖诊断故障码序列，但忽略了温度、湿度、压力等环境数据，而这些上下文信息对专家诊断故障模式至关重要。真实世界数据复杂且噪声大，需要有效整合多模态信息。

Method: 提出BiCarFormer双向Transformer模型，专门处理车辆事件序列，采用嵌入融合和协同注意力机制捕捉诊断代码与环境数据之间的关系。

Result: 在包含22,137个错误代码和360个错误模式的真实世界汽车数据集上，相比仅使用DTC序列的传统序列模型，分类性能显著提升。

Conclusion: 整合环境上下文信息对更准确、鲁棒的车辆诊断至关重要，有助于降低维护成本并提升汽车行业自动化水平。

Abstract: Accurately diagnosing and predicting vehicle malfunctions is crucial for maintenance and safety in the automotive industry. While modern diagnostic systems primarily rely on sequences of vehicular Diagnostic Trouble Codes (DTCs) registered in On-Board Diagnostic (OBD) systems, they often overlook valuable contextual information such as raw sensory data (e.g., temperature, humidity, and pressure). This contextual data, crucial for domain experts to classify vehicle failures, introduces unique challenges due to its complexity and the noisy nature of real-world data. This paper presents BiCarFormer: the first multimodal approach to multi-label sequence classification of error codes into error patterns that integrates DTC sequences and environmental conditions. BiCarFormer is a bidirectional Transformer model tailored for vehicle event sequences, employing embedding fusions and a co-attention mechanism to capture the relationships between diagnostic codes and environmental data. Experimental results on a challenging real-world automotive dataset with 22,137 error codes and 360 error patterns demonstrate that our approach significantly improves classification performance compared to models that rely solely on DTC sequences and traditional sequence models. This work highlights the importance of incorporating contextual environmental information for more accurate and robust vehicle diagnostics, hence reducing maintenance costs and enhancing automation processes in the automotive industry.

</details>


### [87] [Lyapunov Stability-Aware Stackelberg Game for Low-Altitude Economy: A Control-Oriented Pruning-Based DRL Approach](https://arxiv.org/abs/2602.01131)
*Yue Zhong,Jiawen Kang,Yongju Tong,Hong-Ning Dai,Dong In Kim,Abbas Jamalipour,Shengli Xie*

Main category: cs.AI

TL;DR: 论文提出了一种面向低空经济无人机网络的感知-通信-计算-控制闭环框架，将通信延迟对物理控制稳定性的影响显式建模，通过李雅普诺夫稳定性理论将抽象稳定性要求转化为可量化资源边界，并采用Stackelberg博弈进行资源分配，同时设计了轻量级剪枝PPO算法以降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 随着低空经济的快速发展，无人机作为空中基站需要支持从延迟敏感的紧急任务到带宽密集型数据流的多样化服务。然而，异构网络的效能常常受到有限机载资源与严格稳定性要求之间冲突的影响，传统以吞吐量为中心的设计无法满足控制稳定性需求。

Method: 1. 提出感知-通信-计算-控制闭环框架，显式建模通信延迟对物理控制稳定性的影响；2. 利用李雅普诺夫稳定性理论推导控制系统状态演化与通信约束之间的内在映射，将抽象稳定性要求转化为可量化资源边界；3. 将资源分配问题建模为Stackelberg博弈，无人机作为领导者动态定价资源以平衡负载和确保稳定性，用户作为跟随者基于服务紧迫性优化请求；4. 提出轻量级剪枝PPO算法，通过动态结构化剪枝机制压缩神经网络规模，降低计算开销。

Result: 仿真结果表明，所提出的方案在动态低空环境中有效保障了控制环稳定性，同时最大化系统效用。轻量级剪枝PPO算法使无人机能够以最小推理延迟快速逼近博弈均衡。

Conclusion: 该研究超越了传统的吞吐量中心设计，通过将控制稳定性要求显式纳入资源分配框架，为低空经济无人机网络提供了一种可靠且高效的解决方案。提出的闭环框架和轻量级算法能够应对资源受限环境下的稳定性挑战。

Abstract: With the rapid expansion of the low-altitude economy, Unmanned Aerial Vehicles (UAVs) serve as pivotal aerial base stations supporting diverse services from users, ranging from latency-sensitive critical missions to bandwidth-intensive data streaming. However, the efficacy of such heterogeneous networks is often compromised by the conflict between limited onboard resources and stringent stability requirements. Moving beyond traditional throughput-centric designs, we propose a Sensing-Communication-Computing-Control closed-loop framework that explicitly models the impact of communication latency on physical control stability. To guarantee mission reliability, we leverage the Lyapunov stability theory to derive an intrinsic mapping between the state evolution of the control system and communication constraints, transforming abstract stability requirements into quantifiable resource boundaries. Then, we formulate the resource allocation problem as a Stackelberg game, where UAVs (as leaders) dynamically price resources to balance load and ensure stability, while users (as followers) optimize requests based on service urgency. Furthermore, addressing the prohibitive computational overhead of standard Deep Reinforcement Learning (DRL) on energy-constrained edge platforms, we propose a novel and lightweight pruning-based Proximal Policy Optimization (PPO) algorithm. By integrating a dynamic structured pruning mechanism, the proposed algorithm significantly compresses the neural network scale during training, enabling the UAV to rapidly approximate the game equilibrium with minimal inference latency. Simulation results demonstrate that the proposed scheme effectively secures control loop stability while maximizing system utility in dynamic low-altitude environments.

</details>


### [88] [PersistBench: When Should Long-Term Memories Be Forgotten by LLMs?](https://arxiv.org/abs/2602.01146)
*Sidharth Pulipaka,Oliver Chen,Manas Sharma,Taaha S Bajwa,Vyas Raina,Ivaxi Sheth*

Main category: cs.AI

TL;DR: PersistBench：评估LLM长期记忆安全风险的基准，发现跨域泄漏和记忆诱导谄媚两大风险，测试18个模型失败率高达53%和97%。


<details>
  <summary>Details</summary>
Motivation: 对话助手越来越多地将长期记忆与LLM集成以增强个性化，但这种持久性可能引入被忽视的安全风险，需要系统评估。

Method: 提出PersistBench基准，识别两种长期记忆特有风险：跨域泄漏（LLM不适当地从长期记忆中注入上下文）和记忆诱导谄媚（存储的记忆强化用户偏见），评估18个前沿和开源LLM。

Result: 测试结果显示惊人的高失败率：跨域样本中位失败率53%，谄媚样本中位失败率97%，表明当前LLM在长期记忆使用中存在严重安全漏洞。

Conclusion: 长期记忆集成带来显著安全风险，PersistBench为开发更健壮、更安全的对话系统长期记忆使用提供了评估基准和方向。

Abstract: Conversational assistants are increasingly integrating long-term memory with large language models (LLMs). This persistence of memories, e.g., the user is vegetarian, can enhance personalization in future conversations. However, the same persistence can also introduce safety risks that have been largely overlooked. Hence, we introduce PersistBench to measure the extent of these safety risks. We identify two long-term memory-specific risks: cross-domain leakage, where LLMs inappropriately inject context from the long-term memories; and memory-induced sycophancy, where stored long-term memories insidiously reinforce user biases. We evaluate 18 frontier and open-source LLMs on our benchmark. Our results reveal a surprisingly high failure rate across these LLMs - a median failure rate of 53% on cross-domain samples and 97% on sycophancy samples. To address this, our benchmark encourages the development of more robust and safer long-term memory usage in frontier conversational systems.

</details>


### [89] [Capabilities and Fundamental Limits of Latent Chain-of-Thought](https://arxiv.org/abs/2602.01148)
*Jiaxuan Zou,Yaozhong Xiong,Yong Liu*

Main category: cs.AI

TL;DR: Latent CoT模型在探索任务上表现出色但在计算任务上失败，研究发现这源于决策确定性的探索-执行权衡，提出了符号指数作为核心机制，并证明课程学习是必要的。


<details>
  <summary>Details</summary>
Motivation: Latent CoT模型表现出令人困惑的性能不一致性：在探索任务（如ProsQA）上表现出色（97.0%），但在计算任务（如GSM8K）上表现不佳（34.1%）。需要理解这种性能差异的根本原因。

Method: 1. 理论分析探索-执行权衡，证明高确定性支持精确执行但抑制探索，低确定性促进搜索但导致错误累积；2. 引入符号指数量化决策承诺，建立其与执行稳定性和探索能力的因果关系；3. 证明课程学习的理论必要性，直接训练因分布不匹配而失败。

Result: 揭示了Latent CoT性能不一致的根本机制是决策确定性，提出了符号指数作为核心调节机制，证明了课程学习是解决探索-执行权衡的必要方法。

Conclusion: 该框架将设计范式从二元架构选择转向自适应系统，能够根据任务需求动态调节决策确定性，为解决Latent CoT模型的性能不一致问题提供了理论基础和方法指导。

Abstract: Latent Chain-of-Thought (Latent CoT) models promise efficient reasoning via continuous representations, yet exhibit puzzling performance inconsistencies: excelling at exploration (ProsQA: 97.0%) but failing at computation (GSM8K: 34.1%). We reveal that this trade-off is governed by decisional certainty. Our contributions are threefold: (1) We theoretically characterize the fundamental Exploration-Execution Trade-off, proving that high certainty enables precise execution but inhibits exploration, while low certainty facilitates search but causes error accumulation. (2) We introduce the Symbolic Index--quantifying decisional commitment--as the core mechanism governing this trade-off and establish its causal relationship with both execution stability and exploration capability. (3) We prove that curriculum learning is theoretically necessary, as direct training provably fails due to distributional mismatch. Our framework shifts the design paradigm from binary architectural choices toward adaptive systems that dynamically regulate decisional certainty based on task demands.

</details>


### [90] [Multi-Agent Causal Reasoning System for Error Pattern Rule Automation in Vehicles](https://arxiv.org/abs/2602.01155)
*Hugo Math,Julian Lorentz,Stefan Oelsner,Rainer Lienhart*

Main category: cs.AI

TL;DR: CAREP是一个多智能体系统，用于从车辆诊断故障码(DTC)序列中自动生成错误模式(EP)规则，替代传统人工编写规则的方法。


<details>
  <summary>Details</summary>
Motivation: 当前汽车制造商使用人工编写的布尔组合规则（错误模式）来诊断系统故障，但随着车辆复杂度增加，这一过程成本高昂且容易出错，需要自动化解决方案。

Method: CAREP采用多智能体架构：因果发现智能体识别DTC-EP关系，上下文信息智能体整合元数据和描述，编排智能体合成候选布尔规则并提供可解释的推理轨迹。

Result: 在包含29,100个独特DTC和474个错误模式的大规模汽车数据集上，CAREP能够自动准确地发现未知EP规则，优于纯LLM基线，并提供透明的因果解释。

Conclusion: CAREP结合实用因果发现和基于智能体的推理，向全自动故障诊断迈进一步，实现了可扩展、可解释且经济高效的车辆维护。

Abstract: Modern vehicles generate thousands of different discrete events known as Diagnostic Trouble Codes (DTCs). Automotive manufacturers use Boolean combinations of these codes, called error patterns (EPs), to characterize system faults and ensure vehicle safety. Yet, EP rules are still manually handcrafted by domain experts, a process that is expensive and prone to errors as vehicle complexity grows. This paper introduces CAREP (Causal Automated Reasoning for Error Patterns), a multi-agent system that automatizes the generation of EP rules from high-dimensional event sequences of DTCs. CAREP combines a causal discovery agent that identifies potential DTC-EP relations, a contextual information agent that integrates metadata and descriptions, and an orchestrator agent that synthesizes candidate boolean rules together with interpretable reasoning traces. Evaluation on a large-scale automotive dataset with over 29,100 unique DTCs and 474 error patterns demonstrates that CAREP can automatically and accurately discover the unknown EP rules, outperforming LLM-only baselines while providing transparent causal explanations. By uniting practical causal discovery and agent-based reasoning, CAREP represents a step toward fully automated fault diagnostics, enabling scalable, interpretable, and cost-efficient vehicle maintenance.

</details>


### [91] [Do All Individual Layers Help? An Empirical Study of Task-Interfering Layers in Vision-Language Models](https://arxiv.org/abs/2602.01167)
*Zhiming Liu,Yujie Wei,Lei Feng,Xiu Su,Xiaobo Xia,Weili Guan,Zeke Xie,Shuo Yang*

Main category: cs.AI

TL;DR: 研究发现预训练视觉语言模型存在任务干扰层，提出训练免费的自适应层剔除方法提升性能


<details>
  <summary>Details</summary>
Motivation: 当前VLM在多种多模态任务中表现出色，但默认使用所有层进行预测。研究发现某些层会阻碍而非帮助下游任务，通过干预单个层可以提升特定任务性能，这揭示了预训练VLM中隐藏的模块化特性

Method: 提出任务层交互向量量化每层对任务的影响，发现任务干扰层具有任务特异性。基于此提出TaLo方法：训练免费、测试时自适应地识别并绕过对当前任务最干扰的层，无需参数更新

Result: TaLo方法在各种模型和数据集上均能提升性能，如在ScienceQA的Maps任务上将Qwen-VL准确率提升高达16.6%。任务干扰层在不同模型和数据集上具有普遍性

Conclusion: 揭示了预训练VLM中意外的模块化特性，提供即插即用的训练免费机制在推理时解锁隐藏能力。任务干扰层现象具有普遍性，相似能力的任务在层干预下表现出一致的响应模式

Abstract: Current VLMs have demonstrated capabilities across a wide range of multimodal tasks. Typically, in a pretrained VLM, all layers are engaged by default to make predictions on downstream tasks. We find that intervening on a single layer, such as by zeroing its parameters, can improve the performance on certain tasks, indicating that some layers hinder rather than help downstream tasks. We systematically investigate how individual layers influence different tasks via layer intervention. Specifically, we measure the change in performance relative to the base model after intervening on each layer and observe improvements when bypassing specific layers. This improvement can be generalizable across models and datasets, indicating the presence of Task-Interfering Layers that harm downstream tasks' performance. We introduce Task-Layer Interaction Vector, which quantifies the effect of intervening on each layer of a VLM given a task. These task-interfering layers exhibit task-specific sensitivity patterns: tasks requiring similar capabilities show consistent response trends under layer interventions, as evidenced by the high similarity in their task-layer interaction vectors. Inspired by these findings, we propose TaLo (Task-Adaptive Layer Knockout), a training-free, test-time adaptation method that dynamically identifies and bypasses the most interfering layer for a given task. Without parameter updates, TaLo improves performance across various models and datasets, including boosting Qwen-VL's accuracy on the Maps task in ScienceQA by up to 16.6%. Our work reveals an unexpected form of modularity in pretrained VLMs and provides a plug-and-play, training-free mechanism to unlock hidden capabilities at inference time. The source code will be publicly available.

</details>


### [92] [ASP-Bench: From Natural Language to Logic Programs](https://arxiv.org/abs/2602.01171)
*Stefan Szeider*

Main category: cs.AI

TL;DR: ASP-Bench是一个包含128个自然语言问题实例的基准测试，用于评估将自然语言规范转换为答案集程序（ASP）的系统，覆盖了ASP的各种特性，并通过基于ReAct框架的智能体方法展示了反馈驱动的迭代细化在ASP建模中的有效性。


<details>
  <summary>Details</summary>
Motivation: 将自然语言规范自动转换为逻辑程序是一个具有挑战性的任务，这对神经符号工程有重要影响。目前缺乏系统评估这种翻译能力的基准测试，特别是针对答案集程序（ASP）这种重要的逻辑编程形式。

Method: 创建了ASP-Bench基准测试，包含128个自然语言问题实例（64个基础问题及其难易变体），覆盖了ASP的各种特性如选择规则、聚合和优化。每个问题都包含参考验证器来检查解决方案是否符合规范。使用基于ReAct框架的智能体方法进行测试，通过反馈驱动的迭代细化来建模自然语言问题。

Result: 基准测试提供了对七个独立推理方面的系统覆盖：优化、时序推理、默认逻辑、资源分配、递归、空间推理和定量复杂性。基于ReAct的智能体方法实现了完全饱和，表明反馈驱动的迭代细化结合求解器反馈为ASP中的自然语言建模提供了可靠且稳健的方法。

Conclusion: ASP-Bench为评估自然语言到ASP的翻译系统提供了全面的基准测试。基于ReAct的智能体方法展示了反馈驱动迭代细化的有效性，并且通过多轮智能体运行的分析，能够深入了解决定问题建模难度的因素。

Abstract: Automating the translation of natural-language specifications into logic programs is a challenging task that affects neurosymbolic engineering. We present ASP-Bench, a benchmark comprising 128 natural language problem instances, 64 base problems with easy and hard variants. It evaluates systems that translate natural-language problems into Answer Set Programs (ASPs), a prominent form of logic programming. It provides systematic coverage of ASP features, including choice rules, aggregates, and optimization. Each problem includes reference validators that check whether solutions satisfy the problem specification.
  We characterize problems along seven largely independent reasoning aspects (optimization, temporal reasoning, default logic, resource allocation, recursion, spatial reasoning, and quantitative complexity), providing a multidimensional view of modeling difficulty.
  We test the benchmark using an agentic approach based on the ReAct (Reason and Act) framework, which achieves full saturation, demonstrating that feedback-driven iterative refinement with solver feedback provides a reliable and robust approach for modeling natural language in ASP. Our analysis across multiple agent runs enables us to gain insights into what determines a problem's modeling hardness.

</details>


### [93] [A State-Transition Framework for Efficient LLM Reasoning](https://arxiv.org/abs/2602.01198)
*Liang Zhang,Yu Zhao,Longyue Wang,Tianqi Shi,Weihua Luo,Kaifu Zhang,Jinsong Su*

Main category: cs.AI

TL;DR: 提出一个基于状态转移的高效推理框架，通过线性注意力机制将推理过程建模为状态转移，降低计算复杂度，同时提升推理效率和性能。


<details>
  <summary>Details</summary>
Motivation: 传统的长思维链推理虽然能提升大语言模型的复杂推理能力，但生成长序列的计算和内存成本过高，限制了效率和实用性。现有方法通过压缩思维链序列来提高效率，但这与测试时扩展相冲突，限制了模型的推理能力。

Method: 1. 将LLM推理过程建模为状态转移过程；2. 使用线性注意力机制估计推理状态，记录历史推理信息；3. 基于查询提示和推理状态，LLM高效执行当前推理步骤并更新状态；4. 提出基于状态的推理策略缓解噪声推理步骤导致的过度思考问题。

Result: 在多个数据集和模型规模上的广泛实验表明，该框架不仅提高了LLM的推理效率（将注意力计算复杂度从二次降为线性），还提升了推理性能。

Conclusion: 提出的高效推理框架通过状态转移建模和线性注意力机制，成功解决了长思维链推理的计算效率问题，同时避免了与测试时扩展的冲突，实现了效率与性能的双重提升。

Abstract: While Long Chain-of-Thought (CoT) reasoning significantly improves Large Language Models (LLMs) performance on complex reasoning tasks, the substantial computational and memory costs of generating long CoT sequences limit their efficiency and practicality. Existing studies usually enhance the reasoning efficiency of LLMs by compressing CoT sequences. However, this approach conflicts with test-time scaling, limiting the reasoning capacity of LLMs. In this paper, we propose an efficient reasoning framework that models the reasoning process of LLMs as a state-transition process. Specifically, we first apply a linear attention mechanism to estimate the LLM's reasoning state, which records the historical reasoning information from previous reasoning steps. Then, based on the query prompt and the reasoning state, the LLM can efficiently perform the current reasoning step and update the state. With the linear attention, each token in the current reasoning step can directly retrieve relevant historical reasoning information from the reasoning state, without explicitly attending to tokens in previous reasoning steps. In this way, the computational complexity of attention is reduced from quadratic to linear, significantly improving the reasoning efficiency of LLMs. In addition, we propose a state-based reasoning strategy to mitigate the over-thinking issue caused by noisy reasoning steps. Extensive experiments across multiple datasets and model sizes demonstrate that our framework not only improves the reasoning efficiency of LLMs but also enhances their reasoning performance.

</details>


### [94] [Workflow-R1: Group Sub-sequence Policy Optimization for Multi-turn Workflow Construction](https://arxiv.org/abs/2602.01202)
*Mingze Kong,Zikun Qu,Zhongquan Zhou,Pengyu Liang,Xiang Li,Zhiwei Shang,Zhi Hong,Kaiyu Huang,Zhiyong Wang,Zhongxiang Dai*

Main category: cs.AI

TL;DR: Workflow-R1将工作流构建重新定义为多轮自然语言顺序决策过程，引入GSsPO算法解决优化粒度不匹配问题，在多个QA基准测试中超越现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有工作流优化方法将工作流合成视为静态、一次性的代码生成问题，这过度约束了模型的编码能力，限制了动态问题解决的灵活性。

Method: 提出Workflow-R1框架，将工作流构建重新定义为多轮自然语言顺序决策过程。引入Group Sub-sequence Policy Optimization (GSsPO)算法，将优化单元重新校准为复合子序列（特别是原子Think-Action循环），使梯度更新与这些交互的语义边界对齐。

Result: 在多个QA基准测试中，Workflow-R1超越了竞争基线，验证了GSsPO作为顺序推理的通用解决方案的有效性。

Conclusion: Workflow-R1为自动化工作流优化提供了一个有前景的新范式，GSsPO算法可推广到广泛的多轮智能体顺序决策任务。

Abstract: The rapid evolution of agentic workflows has demonstrated strong performance of LLM-based agents in addressing complex reasoning tasks. However, existing workflow optimization methods typically formulate workflow synthesis as a static, one-shot code-centric generation problem. This paradigm imposes excessive constraints on the model's coding capabilities and restricts the flexibility required for dynamic problem-solving. In this paper, we present Workflow-R1, a framework that reformulates workflow construction as a multi-turn, natural language-based sequential decision-making process. To resolve the optimization granularity mismatch inherent in such multi-turn interactions, we introduce Group Sub-sequence Policy Optimization (GSsPO). While explicitly tailored to align with the interleaved Think-Action dynamics of agentic reasoning, GSsPO fundamentally functions as a structure-aware RL algorithm generalizable to a broad class of multi-turn agentic sequential decision-making tasks. By recalibrating the optimization unit to the composite sub-sequence, specifically the atomic Think-Action cycle, it aligns gradient updates with the semantic boundaries of these interactions, ensuring robust learning in complex multi-turn reasoning tasks. Through extensive experiments on multiple QA benchmarks, Workflow-R1 outperforms competitive baselines, validating GSsPO as a generalized solution for sequential reasoning and establishing Workflow-R1 as a promising new paradigm for automated workflow optimization.

</details>


### [95] [Addressing Explainability of Generative AI using SMILE (Statistical Model-agnostic Interpretability with Local Explanations)](https://arxiv.org/abs/2602.01206)
*Zeinab Dehghani*

Main category: cs.AI

TL;DR: gSMILE是一个用于生成模型可解释性的统一框架，通过文本扰动、Wasserstein距离和加权代理建模来量化提示组件对输出的影响，为LLMs提供细粒度token归因，为图像编辑模型分析指令修改的影响。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型虽然能产生复杂的文本和视觉输出，但其决策过程不透明，限制了在高风险应用中的信任和问责。需要一种统一的方法来解释生成模型的决策过程。

Method: 扩展SMILE方法到生成式设置，使用文本输入的受控扰动、Wasserstein距离度量和加权代理建模来量化和可视化提示组件对输出的影响。结合基于场景的评估策略和ODD框架，定义稳定性、保真度、准确性、一致性和忠实性等归因指标。

Result: gSMILE为LLMs提供细粒度token级归因和直观热力图，突出显示有影响的token和推理路径；在图像编辑模型中分析指令修改对结果图像的影响。实验表明gSMILE产生稳健、与人类对齐的归因，并能有效泛化到最先进的生成模型。

Conclusion: gSMILE框架有潜力推动生成式AI技术的透明、可靠和负责任部署，通过提供系统化的解释方法来增强对生成模型决策过程的理解和信任。

Abstract: The rapid advancement of generative artificial intelligence has enabled models capable of producing complex textual and visual outputs; however, their decision-making processes remain largely opaque, limiting trust and accountability in high-stakes applications. This thesis introduces gSMILE, a unified framework for the explainability of generative models, extending the Statistical Model-agnostic Interpretability with Local Explanations (SMILE) method to generative settings. gSMILE employs controlled perturbations of textual input, Wasserstein distance metrics, and weighted surrogate modelling to quantify and visualise how specific components of a prompt or instruction influence model outputs. Applied to Large Language Models (LLMs), gSMILE provides fine-grained token-level attribution and generates intuitive heatmaps that highlight influential tokens and reasoning pathways. In instruction-based image editing models, the exact text-perturbation mechanism is employed, allowing for the analysis of how modifications to an editing instruction impact the resulting image. Combined with a scenario-based evaluation strategy grounded in the Operational Design Domain (ODD) framework, gSMILE allows systematic assessment of model behaviour across diverse semantic and environmental conditions. To evaluate explanation quality, we define rigorous attribution metrics, including stability, fidelity, accuracy, consistency, and faithfulness, and apply them across multiple generative architectures. Extensive experiments demonstrate that gSMILE produces robust, human-aligned attributions and generalises effectively across state-of-the-art generative models. These findings highlight the potential of gSMILE to advance transparent, reliable, and responsible deployment of generative AI technologies.

</details>


### [96] [Not All Preferences Are Created Equal: Stability-Aware and Gradient-Efficient Alignment for Reasoning Models](https://arxiv.org/abs/2602.01207)
*Hui Wu,Hengyi Cai,Jinman Zhao,Xinran Chen,Ziheng Li,Zhejun Zhao,Shuaiqiang Wang,Yuchen Li,Dawei Yin*

Main category: cs.AI

TL;DR: SAGE是一个动态框架，通过最大化策略更新的信噪比来增强对齐可靠性，相比静态方法能显著加速收敛并提升性能。


<details>
  <summary>Details</summary>
Motivation: 标准偏好对齐方法（如DPO）通常将所有偏好对同等对待，忽略了训练实例的演化效用。这种静态方法导致低效或不稳定的优化：浪费计算资源在梯度可忽略的平凡对上，并受到接近不确定决策边界的样本噪声影响。

Method: SAGE整合了粗粒度课程机制（基于模型能力刷新候选池）和细粒度稳定性感知评分函数（优先处理信息丰富、置信度高的错误，同时过滤不稳定样本），以最大化策略更新的信噪比。

Result: 在多个数学推理基准测试中，SAGE显著加速了收敛速度，并优于静态基线方法。

Conclusion: 研究强调了在推理对齐中采用策略感知、稳定性意识的数据选择的关键作用，SAGE框架为此提供了有效解决方案。

Abstract: Preference-based alignment is pivotal for training large reasoning models; however, standard methods like Direct Preference Optimization (DPO) typically treat all preference pairs uniformly, overlooking the evolving utility of training instances. This static approach often leads to inefficient or unstable optimization, as it wastes computation on trivial pairs with negligible gradients and suffers from noise induced by samples near uncertain decision boundaries. Facing these challenges, we propose SAGE (Stability-Aware Gradient Efficiency), a dynamic framework designed to enhance alignment reliability by maximizing the Signal-to-Noise Ratio of policy updates. Concretely, SAGE integrates a coarse-grained curriculum mechanism that refreshes candidate pools based on model competence with a fine-grained, stability-aware scoring function that prioritizes informative, confident errors while filtering out unstable samples. Experiments on multiple mathematical reasoning benchmarks demonstrate that SAGE significantly accelerates convergence and outperforms static baselines, highlighting the critical role of policy-aware, stability-conscious data selection in reasoning alignment.

</details>


### [97] [FutureMind: Equipping Small Language Models with Strategic Thinking-Pattern Priors via Adaptive Knowledge Distillation](https://arxiv.org/abs/2602.01222)
*Shaoxiong Yang,Junting Li,Mengyuan Zhang,Chao Li,Wei Liu,Jian Luan*

Main category: cs.AI

TL;DR: FutureMind是一个模块化推理框架，通过从大语言模型自适应知识蒸馏为小语言模型提供战略思维模式先验，显著提升小语言模型在复杂知识密集型任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 小语言模型在成本敏感和资源受限场景中具有吸引力，但它们在需要结构化推理和有效检索的复杂知识密集型任务上表现不佳。需要一种方法让小语言模型既能保持高效推理，又能处理复杂任务。

Method: 提出FutureMind框架，包含四个关键模块：问题分析、逻辑推理、策略规划和检索指导。通过三种不同的检索范式将复杂查询分解为可处理的子问题，并通过自适应知识蒸馏从大语言模型向小语言模型传递战略思维模式。

Result: 在多个多跳问答基准测试（2WikiMultihopQA、MuSiQue、Bamboogle、Frames）上，FutureMind始终优于Search-o1等强基线，在自由训练条件下实现了最先进的结果，适用于不同架构和规模的小语言模型。

Conclusion: FutureMind成功提升了小语言模型在复杂任务上的表现，同时揭示了思维模式蒸馏过程受到教师（大语言模型）和学生（小语言模型）之间认知偏差瓶颈的限制，为推理技能的可转移性提供了新视角，推动了兼具效率和真正认知能力的小语言模型发展。

Abstract: Small Language Models (SLMs) are attractive for cost-sensitive and resource-limited settings due to their efficient, low-latency inference. However, they often struggle with complex, knowledge-intensive tasks that require structured reasoning and effective retrieval. To address these limitations, we propose FutureMind, a modular reasoning framework that equips SLMs with strategic thinking-pattern priors via adaptive knowledge distillation from large language models (LLMs). FutureMind introduces a dynamic reasoning pipeline composed of four key modules: Problem Analysis, Logical Reasoning, Strategy Planning, and Retrieval Guidance. This pipeline is augmented by three distinct retrieval paradigms that decompose complex queries into tractable subproblems, ensuring efficient and accurate retrieval execution. Extensive experiments on multi-hop QA benchmarks, including 2WikiMultihopQA, MuSiQue, Bamboogle, and Frames, demonstrate the superiority of FutureMind. It consistently outperforms strong baselines such as Search-o1, achieving state-of-the-art results under free training conditions across diverse SLM architectures and scales. Beyond empirical gains, our analysis reveals that the process of thinking-pattern distillation is restricted by the cognitive bias bottleneck between the teacher (LLMs) and student (SLMs) models. This provides new perspectives on the transferability of reasoning skills, paving the way for the development of SLMs that combine efficiency with genuine cognitive capability.

</details>


### [98] [Predictive Scheduling for Efficient Inference-Time Reasoning in Large Language Models](https://arxiv.org/abs/2602.01237)
*Katrina Brown,Aneesh Muppidi,Rana Shahout*

Main category: cs.AI

TL;DR: 提出Predictive Scheduling框架，通过轻量级预测器在生成前估计查询的推理长度/难度，动态分配固定token预算以最大化准确率，在GSM8K上比均匀分配提升7.9个百分点。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs使用固定token预算进行多步推理，导致简单问题计算过度、困难问题计算不足，需要更精细的计算-准确率权衡控制。

Method: 1) 使用轻量级预测器（基于中间隐藏状态的MLP或基于问题文本的LoRA微调分类器）在生成前估计查询的最优推理长度/难度；2) 贪心批量分配器动态分配固定总token预算以最大化预期准确率。

Result: 在GSM8K算术基准上，预测调度在相同token成本下比均匀预算分配提升7.9个百分点的绝对准确率，缩小了与完美预知oracle之间超过50%的差距。系统层分析显示transformer中间层（12-17层）包含最丰富的规模估计信号。

Conclusion: 预运行预算预测实现了对计算-准确率权衡的细粒度控制，为延迟敏感、成本高效的LLM部署提供了具体路径。

Abstract: Large language models (LLMs) achieve state-of-the-art accuracy on complex reasoning tasks by generating multiple chain-of-thought (CoT) traces, but using a fixed token budget per query leads to over-computation on easy inputs and under-computation on hard ones. We introduce Predictive Scheduling, a plug-and-play framework that pre-runs lightweight predictors, an MLP on intermediate transformer hidden states or a LoRA-fine-tuned classifier on raw question text, to estimate each query's optimal reasoning length or difficulty before any full generation. Our greedy batch allocator dynamically distributes a fixed total token budget across queries to maximize expected accuracy. On the GSM8K arithmetic benchmark, predictive scheduling yields up to 7.9 percentage points of absolute accuracy gain over uniform budgeting at identical token cost, closing over 50\% of the gap to an oracle with perfect foresight. A systematic layer-wise study reveals that middle layers (12 - 17) of the transformer carry the richest signals for size estimation. These results demonstrate that pre-run budget prediction enables fine-grained control of the compute-accuracy trade-off, offering a concrete path toward latency-sensitive, cost-efficient LLM deployments.

</details>


### [99] [LLM-Driven Ontology Construction for Enterprise Knowledge Graphs](https://arxiv.org/abs/2602.01276)
*Abdulsobur Oyewale,Tommaso Soru*

Main category: cs.AI

TL;DR: OntoEKG：一个基于LLM的流水线，用于从非结构化企业数据自动生成领域本体，通过提取和推理两阶段方法加速企业知识图谱的构建。


<details>
  <summary>Details</summary>
Motivation: 企业知识图谱需要统一异构数据和语义治理，但当前本体构建过程资源密集、依赖人工和领域专家，需要自动化解决方案来加速这一过程。

Method: 提出OntoEKG流水线，将建模任务分解为两个阶段：1）提取模块识别核心类和属性；2）推理模块将这些元素逻辑结构化形成层次结构，最后序列化为标准RDF格式。

Result: 在数据、金融和物流领域文档构建的新评估数据集上测试，在数据领域达到模糊匹配F1分数0.724，显示出方法的潜力，但也揭示了范围定义和层次推理方面的局限性。

Conclusion: LLM驱动的本体生成方法有潜力加速企业知识图谱构建，但需要在范围定义和层次推理方面进一步改进，同时需要更全面的端到端本体构建基准。

Abstract: Enterprise Knowledge Graphs have become essential for unifying heterogeneous data and enforcing semantic governance. However, the construction of their underlying ontologies remains a resource-intensive, manual process that relies heavily on domain expertise. This paper introduces OntoEKG, a LLM-driven pipeline designed to accelerate the generation of domain-specific ontologies from unstructured enterprise data. Our approach decomposes the modelling task into two distinct phases: an extraction module that identifies core classes and properties, and an entailment module that logically structures these elements into a hierarchy before serialising them into standard RDF. Addressing the significant lack of comprehensive benchmarks for end-to-end ontology construction, we adopt a new evaluation dataset derived from documents across the Data, Finance, and Logistics sectors. Experimental results highlight both the potential and the challenges of this approach, achieving a fuzzy-match F1-score of 0.724 in the Data domain while revealing limitations in scope definition and hierarchical reasoning.

</details>


### [100] [RE-MCDF: Closed-Loop Multi-Expert LLM Reasoning for Knowledge-Grounded Clinical Diagnosis](https://arxiv.org/abs/2602.01297)
*Shaowei Shen,Xiaohong Yang,Jie Yang,Lianfen Huang,Yongcai Zhang,Yang Zou,Seyyedali Hosseinalipour*

Main category: cs.AI

TL;DR: RE-MCDF是一个关系增强的多专家临床诊断框架，通过生成-验证-修订的闭环架构，整合多个专家组件，利用医学知识图谱强化疾病间的逻辑约束，在神经学电子病历上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 电子病历（特别是神经学领域）具有异构性、稀疏性和噪声，单智能体系统容易产生自我强化的错误，而现有的多智能体框架交互浅层且松散，忽略了疾病间的丰富逻辑依赖关系（如互斥性、病理兼容性、诊断混淆），无法排除临床不可信的假设。

Method: 提出RE-MCDF框架，采用生成-验证-修订的闭环架构，包含三个互补组件：1）主专家生成候选诊断和证据；2）实验室专家动态优先处理异构临床指标；3）多关系感知与评估专家组显式执行疾病间的逻辑约束。框架基于医学知识图谱自适应重加权电子病历证据。

Result: 在CMEMR的神经学子集（NEEMRs）和自建数据集（XMEMRs）上的广泛实验表明，RE-MCDF在复杂诊断场景中持续优于最先进的基线方法。

Conclusion: RE-MCDF通过整合多专家协作和显式的疾病关系约束，有效解决了电子病历诊断中的异构性、稀疏性和逻辑一致性问题，为临床决策提供了更可靠的支持。

Abstract: Electronic medical records (EMRs), particularly in neurology, are inherently heterogeneous, sparse, and noisy, which poses significant challenges for large language models (LLMs) in clinical diagnosis. In such settings, single-agent systems are vulnerable to self-reinforcing errors, as their predictions lack independent validation and can drift toward spurious conclusions. Although recent multi-agent frameworks attempt to mitigate this issue through collaborative reasoning, their interactions are often shallow and loosely structured, failing to reflect the rigorous, evidence-driven processes used by clinical experts. More fundamentally, existing approaches largely ignore the rich logical dependencies among diseases, such as mutual exclusivity, pathological compatibility, and diagnostic confusion. This limitation prevents them from ruling out clinically implausible hypotheses, even when sufficient evidence is available. To overcome these, we propose RE-MCDF, a relation-enhanced multi-expert clinical diagnosis framework. RE-MCDF introduces a generation--verification--revision closed-loop architecture that integrates three complementary components: (i) a primary expert that generates candidate diagnoses and supporting evidence, (ii) a laboratory expert that dynamically prioritizes heterogeneous clinical indicators, and (iii) a multi-relation awareness and evaluation expert group that explicitly enforces inter-disease logical constraints. Guided by a medical knowledge graph (MKG), the first two experts adaptively reweight EMR evidence, while the expert group validates and corrects candidate diagnoses to ensure logical consistency. Extensive experiments on the neurology subset of CMEMR (NEEMRs) and on our curated dataset (XMEMRs) demonstrate that RE-MCDF consistently outperforms state-of-the-art baselines in complex diagnostic scenarios.

</details>


### [101] [Model Specific Task Similarity for Vision Language Model Selection via Layer Conductance](https://arxiv.org/abs/2602.01346)
*Wei Yang,Hong Xie,Tao Tan,Xin Li,Defu Lian,Enhong Chen*

Main category: cs.AI

TL;DR: 提出基于视觉编码器内部功能动态的VLM选择框架，通过方向性电导散度(DCD)度量源任务覆盖目标功能块的有效性，无需直接推理即可预测模型排名。


<details>
  <summary>Details</summary>
Motivation: 现有开源视觉语言模型(VLMs)众多，但为特定下游任务选择最优预训练模型具有挑战性。现有选择方法要么依赖数据密集型代理，要么使用对称文本描述符，忽略了迁移性的方向性和模型特定性。

Method: 提出基于视觉编码器内部功能动态的框架：1)通过层间电导表示任务；2)通过熵正则化对齐得到目标条件块重要性分布；3)引入方向性电导散度(DCD)度量源任务覆盖目标功能块的有效性；4)通过聚合源任务排名预测目标模型排名，无需直接推理。

Result: 在48个VLMs和21个数据集上的实验表明，该方法优于现有最佳基线，在NDCG@5指标上比SWAB提高了14.7%。

Conclusion: 通过关注视觉编码器的内部功能动态，提出的框架能够有效预测VLMs在下游任务上的性能，为模型选择提供了数据高效且方向敏感的方法。

Abstract: While open sourced Vision-Language Models (VLMs) have proliferated, selecting the optimal pretrained model for a specific downstream task remains challenging. Exhaustive evaluation is often infeasible due to computational constraints and data limitations in few shot scenarios. Existing selection methods fail to fully address this: they either rely on data-intensive proxies or use symmetric textual descriptors that neglect the inherently directional and model-specific nature of transferability. To address this problem, we propose a framework that grounds model selection in the internal functional dynamics of the visual encoder. Our approach represents each task via layer wise conductance and derives a target-conditioned block importance distribution through entropy regularized alignment. Building on this, we introduce Directional Conductance Divergence (DCD), an asymmetric metric that quantifies how effectively a source task covers the target's salient functional blocks. This allows for predicting target model rankings by aggregating source task ranks without direct inference. Experimental results on 48 VLMs across 21 datasets demonstrate that our method outperforms state-of-the-art baselines, achieving a 14.7% improvement in NDCG@5 over SWAB.

</details>


### [102] [Aggregation Queries over Unstructured Text: Benchmark and Agentic Method](https://arxiv.org/abs/2602.01355)
*Haojia Zhu,Qinyuan Xu,Haoyu Li,Yuxi Liu,Hanchen Qiu,Jiaoyan Chen,Jiahui Jin*

Main category: cs.AI

TL;DR: 论文提出DFA方法解决文本聚合查询的完整性问题，并创建AGGBench基准进行评估


<details>
  <summary>Details</summary>
Motivation: 文本聚合查询需要"找到所有"证据而非"找到一个"，现有Text-to-SQL和RAG方法无法保证查询完整性，这是长期存在但未充分探索的问题

Method: 提出DFA（消歧-过滤-聚合）模块化代理基线，将聚合查询分解为可解释的三个阶段，解决歧义、过滤和聚合中的关键失败模式

Result: DFA在AGGBench基准上相比强大的RAG和代理基线，持续提升了聚合证据覆盖率

Conclusion: DFA方法能有效解决实体级文本聚合查询的完整性问题，为这一长期未充分探索的问题提供了可行的解决方案

Abstract: Aggregation query over free text is a long-standing yet underexplored problem. Unlike ordinary question answering, aggregate queries require exhaustive evidence collection and systems are required to "find all," not merely "find one." Existing paradigms such as Text-to-SQL and Retrieval-Augmented Generation fail to achieve this completeness. In this work, we formalize entity-level aggregation querying over text in a corpus-bounded setting with strict completeness requirement. To enable principled evaluation, we introduce AGGBench, a benchmark designed to evaluate completeness-oriented aggregation under realistic large-scale corpus. To accompany the benchmark, we propose DFA (Disambiguation--Filtering--Aggregation), a modular agentic baseline that decomposes aggregation querying into interpretable stages and exposes key failure modes related to ambiguity, filtering, and aggregation. Empirical results show that DFA consistently improves aggregation evidence coverage over strong RAG and agentic baselines. The data and code are available in https://anonymous.4open.science/r/DFA-A4C1.

</details>


### [103] [Building Better Deception Probes Using Targeted Instruction Pairs](https://arxiv.org/abs/2602.01425)
*Vikram Natarajan,Devina Jain,Shivam Arora,Satvik Golechha,Joseph Bloom*

Main category: cs.AI

TL;DR: 线性探针用于检测AI欺骗行为，但现有方法存在虚假相关性和误报问题。研究发现训练指令对的选择至关重要，针对特定欺骗类型设计探针比通用检测器更有效。


<details>
  <summary>Details</summary>
Motivation: 现有线性探针方法在检测AI系统欺骗行为时存在明显缺陷，包括虚假相关性和对非欺骗性响应的误报。需要改进探针训练方法以提高检测准确性。

Method: 通过分析训练指令对的重要性，并基于人类可解释的欺骗分类法设计针对性探针，针对特定欺骗行为进行优化。

Result: 研究发现指令对捕捉的是欺骗意图而非内容特定模式，提示选择解释了70.6%的探针性能差异。针对特定威胁模型设计的专门探针效果更好。

Conclusion: 由于不同数据集中欺骗类型的异质性，组织应设计针对其特定威胁模型的专门探针，而不是寻求通用的欺骗检测器。

Abstract: Linear probes are a promising approach for monitoring AI systems for deceptive behaviour. Previous work has shown that a linear classifier trained on a contrastive instruction pair and a simple dataset can achieve good performance. However, these probes exhibit notable failures even in straightforward scenarios, including spurious correlations and false positives on non-deceptive responses. In this paper, we identify the importance of the instruction pair used during training. Furthermore, we show that targeting specific deceptive behaviors through a human-interpretable taxonomy of deception leads to improved results on evaluation datasets. Our findings reveal that instruction pairs capture deceptive intent rather than content-specific patterns, explaining why prompt choice dominates probe performance (70.6% of variance). Given the heterogeneity of deception types across datasets, we conclude that organizations should design specialized probes targeting their specific threat models rather than seeking a universal deception detector.

</details>


### [104] [SimGym: Traffic-Grounded Browser Agents for Offline A/B Testing in E-Commerce](https://arxiv.org/abs/2602.01443)
*Alberto Castelo,Zahra Zanjani Foumani,Ailin Fan,Keat Yang Koay,Vibhor Malik,Yuanzheng Zhu,Han Li,Meysam Feghhi,Ronie Uliana,Shuang Xie,Zhaoyu Zhang,Angelo Ocana Martins,Mingyu Zhao,Francis Pelland,Jonathan Faerman,Nikolas LeBlanc,Aaron Glazer,Andrew McNamara,Lingyun Wang,Zhong Wu*

Main category: cs.AI

TL;DR: SimGym是一个基于LLM代理的离线A/B测试系统，通过模拟真实买家行为在浏览器中快速测试电商UI变更，将实验周期从数周缩短到1小时内


<details>
  <summary>Details</summary>
Motivation: 传统A/B测试需要分流真实流量、耗时数周才能获得显著结果，且可能损害用户体验，需要一种快速、安全的离线测试方法

Method: 从生产交互数据中提取买家画像和意图，识别行为原型，使用LLM代理在实时浏览器中模拟控制组和实验组的加权会话

Result: 在主要电商平台上验证，即使无需对齐后训练，SimGym代理也能与观察到的结果变化高度对齐，将实验周期从数周缩短到1小时以内

Conclusion: SimGym实现了快速、安全的电商UI实验，无需暴露真实买家，显著提升了实验效率和安全性

Abstract: A/B testing remains the gold standard for evaluating e-commerce UI changes, yet it diverts traffic, takes weeks to reach significance, and risks harming user experience. We introduce SimGym, a scalable system for rapid offline A/B testing using traffic-grounded synthetic buyers powered by Large Language Model agents operating in a live browser. SimGym extracts per-shop buyer profiles and intents from production interaction data, identifies distinct behavioral archetypes, and simulates cohort-weighted sessions across control and treatment storefronts. We validate SimGym against real human outcomes from real UI changes on a major e-commerce platform under confounder control. Even without alignment post training, SimGym agents achieve state of the art alignment with observed outcome shifts and reduces experiment cycles from weeks to under an hour , enabling rapid experimentation without exposure to real buyers.

</details>


### [105] [Agyn: A Multi-Agent System for Team-Based Autonomous Software Engineering](https://arxiv.org/abs/2602.01465)
*Nikita Benkovich,Vitalii Valkov*

Main category: cs.AI

TL;DR: 提出一个完全自动化的多智能体系统，将软件工程建模为组织化流程，模拟真实工程团队结构，在SWE-bench 500上实现72.4%的任务解决率，超越单智能体基线。


<details>
  <summary>Details</summary>
Motivation: 现有自主系统大多将问题解决视为单体或流水线过程，而真实软件开发是团队协作活动，有明确的角色分离、沟通和评审。需要模拟真实工程团队的组织结构来提升自主软件工程能力。

Method: 基于agyn开源平台构建多智能体系统，分配专门角色（协调、研究、实现、评审），提供隔离沙箱进行实验，支持结构化通信。系统遵循定义好的开发方法论，包括问题分析、任务规范、PR创建和迭代评审，完全无需人工干预。

Result: 在SWE-bench 500上实现72.4%的任务解决率，超越使用可比语言模型的单智能体基线。系统专为实际生产使用设计，未针对SWE-bench进行调优。

Conclusion: 复制团队结构、方法论和沟通是自主软件工程的有力范式，未来进展可能同样依赖于组织设计和智能体基础设施，而不仅仅是模型改进。

Abstract: Large language models have demonstrated strong capabilities in individual software engineering tasks, yet most autonomous systems still treat issue resolution as a monolithic or pipeline-based process. In contrast, real-world software development is organized as a collaborative activity carried out by teams following shared methodologies, with clear role separation, communication, and review. In this work, we present a fully automated multi-agent system that explicitly models software engineering as an organizational process, replicating the structure of an engineering team. Built on top of agyn, an open-source platform for configuring agent teams, our system assigns specialized agents to roles such as coordination, research, implementation, and review, provides them with isolated sandboxes for experimentation, and enables structured communication. The system follows a defined development methodology for working on issues, including analysis, task specification, pull request creation, and iterative review, and operates without any human intervention. Importantly, the system was designed for real production use and was not tuned for SWE-bench. When evaluated post hoc on SWE-bench 500, it resolves 72.4% of tasks, outperforming single-agent baselines using comparable language models. Our results suggest that replicating team structure, methodology, and communication is a powerful paradigm for autonomous software engineering, and that future progress may depend as much on organizational design and agent infrastructure as on model improvements.

</details>


### [106] [Legal Infrastructure for Transformative AI Governance](https://arxiv.org/abs/2602.01474)
*Gillian K. Hadfield*

Main category: cs.AI

TL;DR: 本文探讨AI治理中法律基础设施的重要性，提出三个具体框架：前沿模型注册制、自主代理注册识别制、监管市场设计。


<details>
  <summary>Details</summary>
Motivation: 当前AI治理讨论过于关注实质性规则（如限制和检查），而忽视了法律和监管基础设施的建设。AI的变革性特征特别需要建立能够生成和实施规则的法律框架。

Method: 作者回顾了自己提出的三个具体方案：1）建立前沿模型的注册制度；2）建立自主代理的注册和识别制度；3）设计监管市场，让私营公司能够创新并提供AI监管服务。

Result: 提出了三种具体的法律基础设施设计方案，为AI治理提供了从规则制定到实施机制的系统性框架。

Conclusion: AI治理不仅需要关注实质性规则，更需要建立能够有效生成和实施规则的法律和监管基础设施，作者提出的三个方案为此提供了具体路径。

Abstract: Most of our AI governance efforts focus on substance: what rules do we want in place? What limits or checks do we want to impose on AI development and deployment? But a key role for law is not only to establish substantive rules but also to establish legal and regulatory infrastructure to generate and implement rules. The transformative nature of AI calls especially for attention to building legal and regulatory frameworks. In this PNAS Perspective piece I review three examples I have proposed: the creation of registration regimes for frontier models; the creation of registration and identification regimes for autonomous agents; and the design of regulatory markets to facilitate a role for private companies to innovate and deliver AI regulatory services.

</details>


### [107] [Learning to Guide Local Search for MPE Inference in Probabilistic Graphical Models](https://arxiv.org/abs/2602.01475)
*Brij Malhotra,Shivvrat Arya,Tahrima Rahman,Vibhav Giridhar Gogate*

Main category: cs.AI

TL;DR: 本文提出了一种神经摊销框架，用于改进在重复查询场景下的概率图模型MPE推理，通过训练注意力网络预测局部移动减少汉明距离的能力，从而平衡短期似然增益与长期潜力。


<details>
  <summary>Details</summary>
Motivation: 在概率图模型的MPE推理中，虽然随机局部搜索算法可扩展到大型模型，但依赖于短视的最佳改进规则，容易陷入局部最优。现有启发式方法如引导局部搜索虽然部分缓解了这一问题，但其指导无法在相同模型的多次推理查询中有效复用。

Method: 提出神经摊销框架，利用固定图结构训练基于注意力的网络，通过预测局部移动减少到近最优解汉明距离的能力来评分移动。该方法与现有局部搜索过程无缝集成，在邻居选择时平衡短期似然增益与长期潜力。

Result: 提供了理论直觉，将距离减少移动选择与改进的收敛行为联系起来，并在摊销推理设置中，在高树宽基准测试上实证展示了相对于SLS和GLS+的一致改进。

Conclusion: 提出的神经摊销框架有效改进了重复查询场景下的局部搜索性能，通过利用固定图结构和预测距离减少能力，实现了比传统方法更好的推理效果。

Abstract: Most Probable Explanation (MPE) inference in Probabilistic Graphical Models (PGMs) is a fundamental yet computationally challenging problem arising in domains such as diagnosis, planning, and structured prediction. In many practical settings, the graphical model remains fixed while inference must be performed repeatedly for varying evidence patterns. Stochastic Local Search (SLS) algorithms scale to large models but rely on myopic best-improvement rule that prioritizes immediate likelihood gains and often stagnate in poor local optima. Heuristics such as Guided Local Search (GLS+) partially alleviate this limitation by modifying the search landscape, but their guidance cannot be reused effectively across multiple inference queries on the same model. We propose a neural amortization framework for improving local search in this repeated-query regime. Exploiting the fixed graph structure, we train an attention-based network to score local moves by predicting their ability to reduce Hamming distance to a near-optimal solution. Our approach integrates seamlessly with existing local search procedures, using this signal to balance short-term likelihood gains with long-term promise during neighbor selection. We provide theoretical intuition linking distance-reducing move selection to improved convergence behavior, and empirically demonstrate consistent improvements over SLS and GLS+ on challenging high-treewidth benchmarks in the amortized inference setting.

</details>


### [108] [Qrita: High-performance Top-k and Top-p Algorithm for GPUs using Pivot-based Truncation and Selection](https://arxiv.org/abs/2602.01518)
*Jongseok Park,Sunga Kim,Alvin Cheung,Ion Stoica*

Main category: cs.AI

TL;DR: Qrita提出了一种基于枢轴选择的高效Top-k和Top-p算法，通过高斯截断和四元枢轴搜索技术，相比现有排序方法实现了2倍吞吐量和一半内存使用，同时保持确定性输出。


<details>
  <summary>Details</summary>
Motivation: Top-k和Top-p是LLM采样中的主流截断操作，但在大规模词汇表上高效实现仍具挑战。现有方法要么依赖排序（计算和内存开销大），要么使用随机方法（改变算法输出），需要一种既高效又保持确定性的解决方案。

Method: 基于RTop-k的枢轴搜索思想，Qrita扩展了枢轴搜索到Top-k和Top-p，采用两种关键技术：1) 高斯基sigma截断，大幅减少目标元素搜索空间；2) 四元枢轴搜索与重复处理，将枢轴搜索迭代减半并保证确定性输出。使用Triton GPU编程语言实现。

Result: 与vLLM、SGLang、Flashinfer等高性能LLM执行引擎的Top-k和Top-p内核相比，Qrita实现了高达2倍的吞吐量和一半的内存使用，同时提供与排序算法相同的输出。

Conclusion: Qrita提供了一种高效、确定性的Top-k和Top-p算法实现，解决了现有方法在GPU上的计算和内存开销问题，为LLM采样提供了更优的解决方案。

Abstract: Top-k and Top-p are the dominant truncation operators in the sampling of large language models. Despite their widespread use, implementing them efficiently over large vocabularies remains a significant challenge. Existing approaches often rely on sorting, which incur significant computation and memory overhead on GPUs, or stochastic approaches, which alter the algorithm output. In this work, we propose Qrita, an efficient Top-k and Top-p algorithm based on a pivot-based selection strategy. Based on RTop-k, which uses a pivot-based search for node selection in graph neural networks, Qrita extends the concept of pivot-based search to both Top-k and Top-p with two key techniques: 1. Gaussian-based sigma-truncation, which greatly reduces the search space of the target elements, and 2. Quaternary pivot search with duplication handling, which halves the pivot search iteration and guarantees deterministic output. We provide the full implementation of Qrita using Triton, a popular GPU programming language. Our evaluation of Qrita against the Top-k and Top-p kernels of high performance LLM execution engines such as vLLM, SGLang, and Flashinfer show that Qrita achieves up to 2 times throughput and half memory use while providing the same output to the the sorting-based algorithms.

</details>


### [109] [PRISM: Festina Lente Proactivity -- Risk-Sensitive, Uncertainty-Aware Deliberation for Proactive Agents](https://arxiv.org/abs/2602.01532)
*Yuxuan Fu,Xiaoyu Tan,Teqi Hao,Chen Zhan,Xihe Qiu*

Main category: cs.AI

TL;DR: PRISM框架通过决策理论门控和双过程推理架构，实现成本敏感的选择性干预，在保持高准确率的同时减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有主动代理系统依赖脆弱的启发式方法或无差别的长推理，难以控制帮助与负担的权衡。需要一种能够精确控制干预时机和计算资源分配的方法。

Method: 提出PRISM框架：1) 决策理论门控，当用户接受概率超过基于不对称成本的阈值时干预；2) 双过程推理架构，仅在决策边界附近调用资源密集的慢模式；3) 门对齐、模式锁定的蒸馏训练，教师模型提供密集监督，学生模型学习与干预门解耦的响应策略。

Result: 在ProactiveBench上，PRISM将误报率降低22.78%，F1分数提高20.14%，优于强基线方法。展示了精确、计算高效且可控的主动代理。

Conclusion: 决策理论门控结合选择性慢推理和对齐蒸馏，能够产生精确、计算高效且可控的主动代理。该方法在帮助-负担权衡和计算资源分配方面表现出色。

Abstract: Proactive agents must decide not only what to say but also whether and when to intervene. Many current systems rely on brittle heuristics or indiscriminate long reasoning, which offers little control over the benefit-burden tradeoff. We formulate the problem as cost-sensitive selective intervention and present PRISM, a novel framework that couples a decision-theoretic gate with a dual-process reasoning architecture. At inference time, the agent intervenes only when a calibrated probability of user acceptance exceeds a threshold derived from asymmetric costs of missed help and false alarms. Inspired by festina lente (Latin: "make haste slowly"), we gate by an acceptance-calibrated, cost-derived threshold and invoke a resource-intensive Slow mode with counterfactual checks only near the decision boundary, concentrating computation on ambiguous and high-stakes cases. Training uses gate-aligned, schema-locked distillation: a teacher running the full PRISM pipeline provides dense, executable supervision on unlabeled interaction traces, while the student learns a response policy that is explicitly decoupled from the intervention gate to enable tunable and auditable control. On ProactiveBench, PRISM reduces false alarms by 22.78% and improves F1 by 20.14% over strong baselines. These results show that principled decision-theoretic gating, paired with selective slow reasoning and aligned distillation, yields proactive agents that are precise, computationally efficient, and controllable. To facilitate reproducibility, we release our code, models, and resources at https://prism-festinalente.github.io/; all experiments use the open-source ProactiveBench benchmark.

</details>


### [110] [MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety](https://arxiv.org/abs/2602.01539)
*Xiaoyu Wen,Zhida He,Han Qi,Ziyu Wan,Zhongtian Ma,Ying Wen,Tianhang Zheng,Xingcheng Xu,Chaochao Lu,Qiaosheng Zhang*

Main category: cs.AI

TL;DR: MAGIC是一个多轮多智能体强化学习框架，将LLM安全对齐建模为对抗性非对称博弈，通过攻击者和防御者的共同进化来提升模型安全性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全防御方法依赖静态预收集数据分布，难以应对不断演化的对抗攻击，需要动态、自适应的安全对齐方法。

Method: 提出MAGIC框架：攻击者智能体学习迭代重写原始查询为欺骗性提示，防御者智能体同时优化策略识别并拒绝此类输入，形成共同进化过程。

Result: 实验验证框架有效性，展示优越的防御成功率且不损害模型帮助性；攻击者通过迭代RL训练演化出新颖的未见组合策略。

Conclusion: MAGIC通过对抗性共同进化实现动态安全对齐，为LLM安全提供更鲁棒的博弈均衡和安全保证，具有显著潜力。

Abstract: Ensuring robust safety alignment is crucial for Large Language Models (LLMs), yet existing defenses often lag behind evolving adversarial attacks due to their \textbf{reliance on static, pre-collected data distributions}. In this paper, we introduce \textbf{MAGIC}, a novel multi-turn multi-agent reinforcement learning framework that formulates LLM safety alignment as an adversarial asymmetric game. Specifically, an attacker agent learns to iteratively rewrite original queries into deceptive prompts, while a defender agent simultaneously optimizes its policy to recognize and refuse such inputs. This dynamic process triggers a \textbf{co-evolution}, where the attacker's ever-changing strategies continuously uncover long-tail vulnerabilities, driving the defender to generalize to unseen attack patterns. Remarkably, we observe that the attacker, endowed with initial reasoning ability, evolves \textbf{novel, previously unseen combinatorial strategies} through iterative RL training, underscoring our method's substantial potential. Theoretically, we provide insights into a more robust game equilibrium and derive safety guarantees. Extensive experiments validate our framework's effectiveness, demonstrating superior defense success rates without compromising the helpfulness of the model. Our code is available at https://github.com/BattleWen/MAGIC.

</details>


### [111] [S1-NexusAgent: a Self-Evolving Agent Framework for Multidisciplinary Scientific Research](https://arxiv.org/abs/2602.01550)
*S1-NexusAgent Team*

Main category: cs.AI

TL;DR: S1-NexusAgent是一个自演进的多学科科学智能体框架，采用分层Plan-and-CodeAct执行范式，通过双循环架构解耦全局规划与工具执行，支持大规模跨学科工具集成，在多个科学基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM和基于工具的智能体在处理大规模数据、复杂工作流和专业化工具方面存在局限，特别是在长时程规划、目标维护和持续学习方面表现不足，难以满足现代多学科科学研究的需求。

Method: 采用分层Plan-and-CodeAct执行范式，通过双循环架构分离全局科学规划与子任务级工具执行；集成Model Context Protocol支持数千个跨学科科学工具；引入基于对象引用的稀疏上下文管理；通过Critic Agent自动评估执行轨迹并提炼可重用的Scientific Skills。

Result: 在涉及长时程规划和复杂专业工具编排的权威科学基准测试（biomini-eval、ChemBench、MatSciBench）中，S1-NexusAgent实现了最先进的性能，验证了其在复杂科学任务中的有效性和泛化能力。

Conclusion: S1-NexusAgent通过自演进框架解决了科学研究中长时程规划、大规模工具集成和持续学习的关键挑战，为可持续的长期科学研究提供了有价值的解决方案。

Abstract: Modern scientific research relies on large-scale data, complex workflows, and specialized tools, which existing LLMs and tool-based agents struggle to handle due to limitations in long-horizon planning, robust goal maintenance, and continual learning from execution. To address these issues, in this work, we propose S1-NexusAgent, a self-evolving agent framework designed for multidisciplinary scientific research. S1-NexusAgent adopts a hierarchical Plan-and-CodeAct execution paradigm, decoupling global scientific planning from subtask-level tool execution through a dual-loop architecture, thereby enabling stable modeling of complex research workflows. The system natively supports the Model Context Protocol (MCP), integrates up to thousands of cross-disciplinary scientific tools, and achieves efficient orchestration of heterogeneous research tools via intention-aware dynamic tool retrieval and hot-plug mechanisms. To address long-context and large-scale data challenges in scientific settings, S1-NexusAgent introduces object-reference-based sparse context management, which enables sub-task context isolation and intermediate result compression. Building on this, a Critic Agent automatically evaluates complete execution trajectories and distills high-quality research paths into reusable Scientific Skills, forming a closed loop for continuous self-evolution, which is valuable for sustainable and long-horizon scientific research. Experiments on authoritative scientific benchmarks involving long-horizon planning and complex specialized tool orchestration, including biomini-eval (biology), ChemBench (chemistry), and MatSciBench (material science), demonstrate that S1-NexusAgent achieves state-of-the-art performance, validating its effectiveness and generalization capability in complex scientific tasks.

</details>


### [112] [Autonomous Question Formation for Large Language Model-Driven AI Systems](https://arxiv.org/abs/2602.01556)
*Hong Su*

Main category: cs.AI

TL;DR: 提出基于人类模拟的框架，使AI系统能自主形成问题并设定任务，通过推理内部状态、环境观察和与其他AI系统的交互来实现自适应决策。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的AI系统大多依赖预定义任务和固定提示，限制了它们在环境变化时自主识别应解决问题的能力。需要让AI系统能自主形成问题并设定任务。

Method: 提出人类模拟框架，将问题形成作为任务选择和执行之前的一等决策过程。整合内部驱动、环境感知和智能体间感知三种提示范围，逐步扩展认知覆盖。支持从经验中学习问题形成过程。

Result: 在多智能体模拟环境中，环境感知提示相比内部驱动基线显著减少无进食事件，智能体间感知提示在20天模拟中将累积无进食事件进一步减少60%以上，具有统计显著性(p<0.05)。

Conclusion: 该框架使AI系统能自主形成问题和设定任务，通过整合多层次认知范围显著提升自适应决策能力，为开放动态环境中的自主AI系统提供了新方法。

Abstract: Large language model (LLM)-driven AI systems are increasingly important for autonomous decision-making in dynamic and open environments. However, most existing systems rely on predefined tasks and fixed prompts, limiting their ability to autonomously identify what problems should be solved when environmental conditions change. In this paper, we propose a human-simulation-based framework that enables AI systems to autonomously form questions and set tasks by reasoning over their internal states, environmental observations, and interactions with other AI systems. The proposed method treats question formation as a first-class decision process preceding task selection and execution, and integrates internal-driven, environment-aware, and inter-agent-aware prompting scopes to progressively expand cognitive coverage. In addition, the framework supports learning the question-formation process from experience, allowing the system to improve its adaptability and decision quality over time. xperimental results in a multi-agent simulation environment show that environment-aware prompting significantly reduces no-eat events compared with the internal-driven baseline, and inter-agent-aware prompting further reduces cumulative no-eat events by more than 60% over a 20-day simulation, with statistically significant improvements (p < 0.05).

</details>


### [113] [Reasoning with Autoregressive-Diffusion Collaborative Thoughts](https://arxiv.org/abs/2602.01608)
*Mu Yuan,Liekang Zeng,Guoliang Xing,Lan Zhang,Yunhao Liu*

Main category: cs.AI

TL;DR: 提出Collaborative Thoughts框架，让自回归模型和扩散模型通过闭环交互协同工作，结合两者的优势：自回归模型负责结构化规划和约束管理，扩散模型生成中间视觉表示，视觉批评模块评估并反馈，迭代优化生成结果。


<details>
  <summary>Details</summary>
Motivation: 自回归模型擅长序列规划和约束组合，但在空间或物理基础任务上表现不佳；扩散模型能捕捉丰富的空间结构，但缺乏逐步逻辑控制来满足复杂多阶段约束或可靠识别纠正错误。需要结合两者优势来解决各自的局限性。

Method: Collaborative Thoughts框架：自回归模型进行结构化规划和约束管理，扩散模型将这些约束实例化为中间视觉表示，视觉批评模块评估视觉表示是否满足结构和物理要求，反馈用于迭代优化后续规划和生成步骤，减少跨模态错误传播。

Result: 通过代表性示例展示了Collaborative Thoughts能够提高空间推理的可靠性和生成的可控性。该框架使用相同的协作循环，无论任务是自回归问答还是基于扩散的视觉生成。

Conclusion: Collaborative Thoughts通过统一协作框架，让自回归和扩散模型联合推理和生成，结合了自回归模型的规划能力和扩散模型的空间生成能力，提高了复杂约束任务的可靠性和可控性。

Abstract: Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation.

</details>


### [114] [ToPT: Task-Oriented Prompt Tuning for Urban Region Representation Learning](https://arxiv.org/abs/2602.01610)
*Zitao Guo,Changyang Jiang,Tianhong Zhao,Jinzhou Cao,Genan Dai,Bowen Zhang*

Main category: cs.AI

TL;DR: ToPT：两阶段框架，通过空间感知区域嵌入学习和任务感知提示，解决城市区域表示学习中空间不一致和任务语义对齐不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两个问题：1）缺乏明确的空间先验，导致区域间建模空间不一致；2）缺乏显式的任务语义对齐机制。这限制了从异构城市数据中学习有效区域嵌入的能力。

Method: 提出ToPT两阶段框架：1）SREL模块使用基于Graphormer的融合模块，注入距离和区域中心性作为可学习注意力偏置；2）Prompt4RE模块使用冻结的多模态大语言模型处理任务特定模板，通过多头交叉注意力将语义向量与区域嵌入对齐。

Result: 在多个任务和城市上的实验显示达到最先进性能，提升高达64.2%，验证了空间先验和提示-区域对齐的必要性和互补性。

Conclusion: ToPT通过空间感知融合和显式任务对齐，有效解决了区域嵌入学习中的空间一致性和任务语义对齐问题，为城市计算任务提供了更好的区域表示。

Abstract: Learning effective region embeddings from heterogeneous urban data underpins key urban computing tasks (e.g., crime prediction, resource allocation). However, prevailing two-stage methods yield task-agnostic representations, decoupling them from downstream objectives. Recent prompt-based approaches attempt to fix this but introduce two challenges: they often lack explicit spatial priors, causing spatially incoherent inter-region modeling, and they lack robust mechanisms for explicit task-semantic alignment. We propose ToPT, a two-stage framework that delivers spatially consistent fusion and explicit task alignment. ToPT consists of two modules: spatial-aware region embedding learning (SREL) and task-aware prompting for region embeddings (Prompt4RE). SREL employs a Graphormer-based fusion module that injects spatial priors-distance and regional centrality-as learnable attention biases to capture coherent, interpretable inter-region interactions. Prompt4RE performs task-oriented prompting: a frozen multimodal large language model (MLLM) processes task-specific templates to obtain semantic vectors, which are aligned with region embeddings via multi-head cross-attention for stable task conditioning. Experiments across multiple tasks and cities show state-of-the-art performance, with improvements of up to 64.2\%, validating the necessity and complementarity of spatial priors and prompt-region alignment. The code is available at https://github.com/townSeven/Prompt4RE.git.

</details>


### [115] [ProjDevBench: Benchmarking AI Coding Agents on End-to-End Project Development](https://arxiv.org/abs/2602.01655)
*Pengrui Lu,Shiqi Zhang,Yunzhong Hou,Lyumanshan Ye,Chaoyi Huang,Zixi Chen,Ji Zeng,Hantao Jiang,Pengfei Liu,Yiwei Wang,Ming-Hsuan Yang*

Main category: cs.AI

TL;DR: ProjDevBench是一个端到端的编码代理基准测试，通过项目需求评估代理生成的完整代码库，结合在线评测和LLM辅助代码审查，在系统架构、功能正确性和迭代优化三个维度评估编码代理能力。


<details>
  <summary>Details</summary>
Motivation: 现有编码代理评估主要关注问题级别的bug修复，缺乏对端到端开发能力的评估。需要一个新的基准测试来评估编码代理从简单提示生成完整代码库的能力。

Method: 提出ProjDevBench基准测试，包含20个编程问题（8个类别），结合在线评测（OJ）测试和LLM辅助代码审查，评估系统架构设计、功能正确性和迭代解决方案优化三个维度。

Result: 评估6个基于不同LLM后端的编码代理，总体接受率为27.38%。代理能处理基本功能和数据结构，但在复杂系统设计、时间复杂度优化和资源管理方面表现不佳。

Conclusion: ProjDevBench为编码代理的端到端开发能力提供了新的评估基准，揭示了当前编码代理在复杂系统设计方面的局限性，为未来改进提供了方向。

Abstract: Recent coding agents can generate complete codebases from simple prompts, yet existing evaluations focus on issue-level bug fixing and lag behind end-to-end development. We introduce ProjDevBench, an end-to-end benchmark that provides project requirements to coding agents and evaluates the resulting repositories. Combining Online Judge (OJ) testing with LLM-assisted code review, the benchmark evaluates agents on (1) system architecture design, (2) functional correctness, and (3) iterative solution refinement. We curate 20 programming problems across 8 categories, covering both concept-oriented tasks and real-world application scenarios, and evaluate six coding agents built on different LLM backends. Our evaluation reports an overall acceptance rate of 27.38%: agents handle basic functionality and data structures but struggle with complex system design, time complexity optimization, and resource management. Our benchmark is available at https://github.com/zsworld6/projdevbench.

</details>


### [116] [FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning](https://arxiv.org/abs/2602.01664)
*Mingda Zhang,Haoran Luo,Tiesunlong Shen,Qika Lin,Xiaoying Tang,Rui Mao,Erik Cambria*

Main category: cs.AI

TL;DR: FlowSteer：基于强化学习的端到端工作流编排框架，通过轻量级策略模型与可执行画布环境的多轮交互实现自动化工作流编排，支持多样化算子库和可互换LLM后端。


<details>
  <summary>Details</summary>
Motivation: 现有工作流编排面临高人工成本、依赖特定算子/大语言模型、稀疏奖励信号等关键挑战，需要自动化解决方案来降低人工干预并提高灵活性。

Method: 提出FlowSteer框架：1）轻量级策略模型作为智能体分析执行状态并选择编辑动作；2）可执行画布环境执行算子并返回反馈；3）支持多样化算子库和可互换LLM后端；4）提出Canvas Workflow Relative Policy Optimization（CWRPO）训练方法，引入多样性约束奖励和条件释放机制。

Result: 在12个数据集上的实验结果表明，FlowSteer在各种任务上显著优于基线方法。

Conclusion: FlowSteer通过强化学习框架有效解决了工作流编排的自动化挑战，提供了灵活可扩展的解决方案，在多个任务上展现出优越性能。

Abstract: In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks.

</details>


### [117] [TRIP-Bench: A Benchmark for Long-Horizon Interactive Agents in Real-World Scenarios](https://arxiv.org/abs/2602.01675)
*Yuanzhe Shen,Zisu Huang,Zhengyuan Wang,Muzhao Tian,Zhengkang Guo,Chenyang Zhang,Shuaiyu Zhou,Zengjie Hu,Dailin Li,Jingwen Xu,Kaimin Wang,Wenhao Liu,Tianlong Li,Fengpeng Yue,Feng Hong,Cao Liu,Ke Zeng*

Main category: cs.AI

TL;DR: TRIP-Bench是一个基于真实旅行规划场景的长视野基准测试，包含18个工具和40+旅行需求，支持自动评估。实验显示先进模型在简单分割上最多只有50%成功率，在困难分割上低于10%。作者还提出了GTPO在线多轮强化学习方法，在Qwen2.5-32B-Instruct上取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法充分代表真实世界中的关键挑战，如强制执行全局约束、协调多工具推理以及适应长期多轮交互中不断变化的用户行为。需要建立一个更贴近实际的长视野基准来推动实用交互式智能体的发展。

Method: 1) 提出TRIP-Bench基准：基于真实旅行规划场景，包含18个精选工具和40+旅行需求，支持自动评估，包含不同难度分割（困难分割强调长且模糊的交互、风格转变、可行性变化和迭代版本修订）；2) 提出GTPO方法：一种在线多轮强化学习方法，采用专门的奖励归一化和奖励差分技术。

Result: 实验结果显示，即使是先进模型在简单分割上最多只能达到50%的成功率，在困难子集上性能下降到10%以下。将GTPO应用于Qwen2.5-32B-Instruct后，提高了约束满足度和交互鲁棒性，在评估中超过了Gemini-3-Pro。

Conclusion: TRIP-Bench有望推动实用的长视野交互式智能体发展，而GTPO为鲁棒的长视野训练提供了一个有效的在线强化学习方案。该基准测试揭示了当前模型在处理复杂、长视野交互任务方面的局限性，并为改进提供了方向。

Abstract: As LLM-based agents are deployed in increasingly complex real-world settings, existing benchmarks underrepresent key challenges such as enforcing global constraints, coordinating multi-tool reasoning, and adapting to evolving user behavior over long, multi-turn interactions. To bridge this gap, we introduce \textbf{TRIP-Bench}, a long-horizon benchmark grounded in realistic travel-planning scenarios. TRIP-Bench leverages real-world data, offers 18 curated tools and 40+ travel requirements, and supports automated evaluation. It includes splits of varying difficulty; the hard split emphasizes long and ambiguous interactions, style shifts, feasibility changes, and iterative version revision. Dialogues span up to 15 user turns, can involve 150+ tool calls, and may exceed 200k tokens of context. Experiments show that even advanced models achieve at most 50\% success on the easy split, with performance dropping below 10\% on hard subsets. We further propose \textbf{GTPO}, an online multi-turn reinforcement learning method with specialized reward normalization and reward differencing. Applied to Qwen2.5-32B-Instruct, GTPO improves constraint satisfaction and interaction robustness, outperforming Gemini-3-Pro in our evaluation. We expect TRIP-Bench to advance practical long-horizon interactive agents, and GTPO to provide an effective online RL recipe for robust long-horizon training.

</details>


### [118] [What LLMs Think When You Don't Tell Them What to Think About?](https://arxiv.org/abs/2602.01689)
*Yongchan Kwon,James Zou*

Main category: cs.AI

TL;DR: 研究通过最小化、主题中立的输入探索LLMs的无约束生成行为，发现不同模型家族存在系统性的主题偏好和独特退化模式


<details>
  <summary>Details</summary>
Motivation: 现有LLM分析大多依赖特定主题或任务的提示，限制了观察范围。需要研究LLMs在无约束条件下的生成行为，以更好地监控模型行为和AI安全

Method: 使用最小化、主题中立的输入（如空字符串、单个标点符号）作为提示，让16个LLMs生成256,000个样本，分析其主题偏好、内容深度和退化模式

Result: 不同模型家族展现出强烈的系统性主题偏好：GPT-OSS偏好编程和数学内容，Llama偏好文学内容，DeepSeek偏好宗教内容，Qwen偏好多选题。GPT-OSS生成的内容技术性更强。无约束生成常退化为重复短语，各模型有独特退化模式（如Llama生成个人社交媒体链接）

Conclusion: LLMs即使在最小输入下也展现出系统性主题偏好和独特行为模式，这为模型监控和AI安全提供了重要见解。研究发布了完整数据集和可复现代码库

Abstract: Characterizing the behavior of large language models (LLMs) across diverse settings is critical for reliable monitoring and AI safety. However, most existing analyses rely on topic- or task-specific prompts, which can substantially limit what can be observed. In this work, we study what LLMs generate from minimal, topic-neutral inputs and probe their near-unconstrained generative behavior. Despite the absence of explicit topics, model outputs cover a broad semantic space, and surprisingly, each model family exhibits strong and systematic topical preferences. GPT-OSS predominantly generates programming (27.1%) and mathematical content (24.6%), whereas Llama most frequently generates literary content (9.1%). DeepSeek often generates religious content, while Qwen frequently generates multiple-choice questions. Beyond topical preferences, we also observe differences in content specialization and depth: GPT-OSS often generates more technically advanced content (e.g., dynamic programming) compared with other models (e.g., basic Python). Furthermore, we find that the near-unconstrained generation often degenerates into repetitive phrases, revealing interesting behaviors unique to each model family. For instance, degenerate outputs from Llama include multiple URLs pointing to personal Facebook and Instagram accounts. We release the complete dataset of 256,000 samples from 16 LLMs, along with a reproducible codebase.

</details>


### [119] [Beyond Dense States: Elevating Sparse Transcoders to Active Operators for Latent Reasoning](https://arxiv.org/abs/2602.01695)
*Yadong Wang,Haodong Chen,Yu Tian,Chuanxing Geng,Dong Liang,Xiang Chen*

Main category: cs.AI

TL;DR: LSTR提出了一种稀疏潜在推理框架，将功能稀疏编码器提升为主动推理算子，通过稀疏语义转换执行多步计算，在保持准确性和压缩效率的同时显著提升可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有潜在推理方法依赖密集的潜在转换，难以解释和控制；而稀疏表示模型虽然能发现人类可解释的语义特征，但主要局限于事后分析。需要解决这种张力，开发既高效又可解释的推理框架。

Method: 提出LSTR框架，核心是带有残差跳跃架构的潜在转换编码器(LTT)，将线性流形传输与稀疏语义更新解耦，通过显式稀疏约束实现可控的语义分辨率。

Result: 实验表明LSTR在保持推理准确性和压缩效率的同时，相比密集潜在基线显著提高了可解释性。因果干预和轨迹分析进一步证明这些稀疏特征在推理过程中既是可解释的，也是因果有效的算子。

Conclusion: LSTR成功地将稀疏表示与主动推理相结合，通过稀疏语义转换实现了高效、可解释且可控的潜在推理，为理解神经网络的推理过程提供了新途径。

Abstract: Latent reasoning compresses the chain-of-thought (CoT) into continuous hidden states, yet existing methods rely on dense latent transitions that remain difficult to interpret and control. Meanwhile, sparse representation models uncover human-interpretable semantic features but remain largely confined to post-hoc analysis. We reconcile this tension by proposing LSTR (Latent Sparse Transcoder Reasoning), a latent reasoning framework that elevates functional sparse transcoders into active reasoning operators to perform multi-step computation through sparse semantic transitions. At its core, LSTR employs a Latent Transition Transcoder (LTT) with a residual skip architecture that decouples linear manifold transport from sparse semantic updates, enabling controllable semantic resolution via explicit sparsity constraints. Extensive experiments show that LSTR preserves reasoning accuracy and compression efficiency while substantially improving interpretability over dense latent baselines. Causal interventions and trajectory analyses further demonstrate that these sparse features act as both interpretable and causally effective operators in the reasoning process.

</details>


### [120] [Optimizing Prompts for Large Language Models: A Causal Approach](https://arxiv.org/abs/2602.01711)
*Wei Chen,Yanbin Fang,Shuran Fu,Fasheng Xu,Xuan Wei*

Main category: cs.AI

TL;DR: 提出因果提示优化(CPO)框架，通过因果推断方法解决提示优化中的静态指令和查询异质性问题，实现离线高效优化。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在企业应用中面临提示设计敏感性问题：1）静态指令无法适应异质查询；2）动态方法依赖离线奖励模型，存在混杂偏差。需要一种能分离提示效果与查询特性的方法。

Method: CPO采用两阶段框架：1）使用双机器学习(DML)在语义嵌入上学习离线因果奖励模型，分离提示变体与查询属性的混杂效应；2）利用无偏奖励信号指导资源高效的查询特定提示搜索，无需昂贵在线评估。

Result: 在数学推理、可视化和数据分析基准测试中，CPO持续优于人工设计提示和最先进自动优化器。主要优势体现在困难查询上的鲁棒性提升，且显著降低优化成本。

Conclusion: 因果推断为可靠且经济高效的提示优化提供了可扩展基础，通过将评估从实时模型执行转移到离线因果模型，实现了高精度、按查询定制的优化，大幅降低推理成本。

Abstract: Large Language Models (LLMs) are increasingly embedded in enterprise workflows, yet their performance remains highly sensitive to prompt design. Automatic Prompt Optimization (APO) seeks to mitigate this instability, but existing approaches face two persistent challenges. First, commonly used prompt strategies rely on static instructions that perform well on average but fail to adapt to heterogeneous queries. Second, more dynamic approaches depend on offline reward models that are fundamentally correlational, confounding prompt effectiveness with query characteristics. We propose Causal Prompt Optimization (CPO), a framework that reframes prompt design as a problem of causal estimation. CPO operates in two stages. First, it learns an offline causal reward model by applying Double Machine Learning (DML) to semantic embeddings of prompts and queries, isolating the causal effect of prompt variations from confounding query attributes. Second, it utilizes this unbiased reward signal to guide a resource-efficient search for query-specific prompts without relying on costly online evaluation. We evaluate CPO across benchmarks in mathematical reasoning, visualization, and data analytics. CPO consistently outperforms human-engineered prompts and state-of-the-art automated optimizers. The gains are driven primarily by improved robustness on hard queries, where existing methods tend to deteriorate. Beyond performance, CPO fundamentally reshapes the economics of prompt optimization: by shifting evaluation from real-time model execution to an offline causal model, it enables high-precision, per-query customization at a fraction of the inference cost required by online methods. Together, these results establish causal inference as a scalable foundation for reliable and cost-efficient prompt optimization in enterprise LLM deployments.

</details>


### [121] [MACD: Model-Aware Contrastive Decoding via Counterfactual Data](https://arxiv.org/abs/2602.01740)
*Qixin Xiao,Kun Zhou*

Main category: cs.AI

TL;DR: MACD：一种基于模型感知反事实数据的对比解码方法，通过视频语言模型自身反馈识别导致幻觉的关键物体区域，生成针对性反事实输入来减少幻觉，同时保持或提升任务准确性。


<details>
  <summary>Details</summary>
Motivation: 现有解码方法（如对比解码）依赖随机扰动构建对比数据来缓解幻觉，但难以控制驱动幻觉的视觉线索或与模型弱点良好对齐。视频语言模型在视觉证据薄弱、模糊或有偏时容易产生看似合理但无根据的内容。

Method: 提出模型感知反事实数据对比解码（MACD），结合模型引导的反事实构建与解码。利用视频语言模型自身反馈识别导致幻觉的关键物体区域，在物体层面而非任意帧或时间修改上生成针对性反事实输入，然后将这些模型感知的反事实数据集成到对比解码中，在解码过程中强制基于证据的token选择。

Result: 在EventHallusion、MVBench、Perception-test和Video-MME上的实验表明，MACD能持续减少幻觉，同时在Qwen和InternVL等多样化视频语言模型中保持或提升任务准确性。该方法在处理涉及小型、遮挡或共现物体的挑战性场景时特别有效。

Conclusion: MACD通过模型引导的反事实数据构建提供了一种更可控、更有效的减少视频语言模型幻觉的方法，特别是在视觉证据具有挑战性的场景中表现优异。

Abstract: Video language models (Video-LLMs) are prone to hallucinations, often generating plausible but ungrounded content when visual evidence is weak, ambiguous, or biased. Existing decoding methods, such as contrastive decoding (CD), rely on random perturbations to construct contrastive data for mitigating hallucination patterns. However, such a way is hard to control the visual cues that drive hallucination or well align with model weaknesses. We propose Model-aware Counterfactual Data based Contrastive Decoding (MACD), a new inference strategy that combines model-guided counterfactual construction with decoding. Our approach uses the Video-LLM's own feedback to identify object regions most responsible for hallucination, generating targeted counterfactual inputs at the object level rather than arbitrary frame or temporal modifications. These model-aware counterfactual data is then integrated into CD to enforce evidence-grounded token selection during decoding. Experiments on EventHallusion, MVBench, Perception-test and Video-MME show that MACD consistently reduces hallucination while maintaining or improving task accuracy across diverse Video-LLMs, including Qwen and InternVL families. The method is especially effective in challenging scenarios involving small, occluded, or co-occurring objects. Our code and data will be publicly released.

</details>


### [122] [Controlling Exploration-Exploitation in GFlowNets via Markov Chain Perspectives](https://arxiv.org/abs/2602.01749)
*Lin Chen,Samuel Drapeau,Fanghao Shao,Xuekai Zhu,Bo Xue,Yunchong Song,Mathieu Laurière,Zhouhan Lin*

Main category: cs.AI

TL;DR: α-GFN通过可调参数α改进GFlowNet的探索-利用平衡，在多个基准测试中性能显著提升


<details>
  <summary>Details</summary>
Motivation: 传统GFlowNet目标隐含地固定了前向和后向策略的均匀混合，这限制了训练过程中的探索-利用权衡。作者希望通过理论分析揭示这种约束的根源，并提供更灵活的框架来适应不同的探索需求。

Method: 1. 建立GFlowNet目标与马尔可夫链可逆性的等价关系，揭示约束的起源；2. 提出α-GFN框架，通过可调参数α来泛化混合策略；3. 确保收敛到唯一流的同时，实现对探索-利用动态的直接控制。

Result: 在Set、Bit Sequence和Molecule Generation等多个基准测试中，α-GFN目标始终优于之前的GFlowNet目标，发现模式的数量最多增加了10倍。

Conclusion: 通过理论连接GFlowNet与马尔可夫链，提出的α-GFN框架成功解决了传统GFlowNet在探索-利用平衡上的限制，显著提升了模式发现能力，为生成流网络提供了更灵活和有效的训练方法。

Abstract: Generative Flow Network (GFlowNet) objectives implicitly fix an equal mixing of forward and backward policies, potentially constraining the exploration-exploitation trade-off during training. By further exploring the link between GFlowNets and Markov chains, we establish an equivalence between GFlowNet objectives and Markov chain reversibility, thereby revealing the origin of such constraints, and provide a framework for adapting Markov chain properties to GFlowNets. Building on these theoretical findings, we propose $α$-GFNs, which generalize the mixing via a tunable parameter $α$. This generalization enables direct control over exploration-exploitation dynamics to enhance mode discovery capabilities, while ensuring convergence to unique flows. Across various benchmarks, including Set, Bit Sequence, and Molecule Generation, $α$-GFN objectives consistently outperform previous GFlowNet objectives, achieving up to a $10 \times$ increase in the number of discovered modes.

</details>


### [123] [Adversarial Reward Auditing for Active Detection and Mitigation of Reward Hacking](https://arxiv.org/abs/2602.01750)
*Mohammad Beigi,Ming Jin,Junshan Zhang,Qifan Wang,Lifu Huang*

Main category: cs.AI

TL;DR: ARA框架将奖励黑客攻击重构为动态竞争游戏，通过黑客发现漏洞、审计员检测利用，再通过AG-RLHF门控奖励信号来惩罚黑客行为，实现跨领域防御。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF方法容易受到奖励黑客攻击，模型会利用奖励模型中的虚假相关性来获得高分但违背人类意图。现有缓解措施依赖静态防御，无法适应新的利用策略。

Method: 提出对抗性奖励审计(ARA)框架：第一阶段，黑客策略发现奖励模型漏洞，审计员从潜在表示中学习检测利用；第二阶段，审计员引导的RLHF(AG-RLHF)门控奖励信号来惩罚检测到的黑客行为。

Result: 在三种黑客场景中，ARA在所有基线中实现了最佳对齐-效用权衡：将奉承行为降至接近SFT水平同时提高帮助性，减少冗长同时获得最高ROUGE-L，抑制代码游戏同时提高Pass@1。奖励黑客、检测和缓解都跨领域泛化。

Conclusion: ARA将奖励黑客攻击从不可观察的失败转变为可测量、可控制的信号，实现了高效的跨领域防御，单个模型就能有效抑制多个领域的利用行为。

Abstract: Reinforcement Learning from Human Feedback (RLHF) remains vulnerable to reward hacking, where models exploit spurious correlations in learned reward models to achieve high scores while violating human intent. Existing mitigations rely on static defenses that cannot adapt to novel exploitation strategies. We propose Adversarial Reward Auditing (ARA), a framework that reconceptualizes reward hacking as a dynamic, competitive game. ARA operates in two stages: first, a Hacker policy discovers reward model vulnerabilities while an Auditor learns to detect exploitation from latent representations; second, Auditor-Guided RLHF (AG-RLHF) gates reward signals to penalize detected hacking, transforming reward hacking from an unobservable failure into a measurable, controllable signal. Experiments across three hacking scenarios demonstrate that ARA achieves the best alignment-utility tradeoff among all baselines: reducing sycophancy to near-SFT levels while improving helpfulness, decreasing verbosity while achieving the highest ROUGE-L, and suppressing code gaming while improving Pass@1. Beyond single-domain evaluation, we show that reward hacking, detection, and mitigation all generalize across domains -- a Hacker trained on code gaming exhibits increased sycophancy despite no reward for this behavior, and an Auditor trained on one domain effectively suppresses exploitation in others, enabling efficient multi-domain defense with a single model.

</details>


### [124] [PRISM: Parametrically Refactoring Inference for Speculative Sampling Draft Models](https://arxiv.org/abs/2602.01762)
*Xuliang Wang,Yuetao Chen,Maochan Zhen,Fang Liu,Xinzhou Zheng,Xingwu Liu,Hong Xu,Ming Li*

Main category: cs.AI

TL;DR: PRISM是一种新的推测解码架构，通过将预测步骤的计算分散到不同参数集，成功解耦模型容量与推理成本，显著提升LLM解码速度。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法为了提升草稿质量倾向于使用更大的草稿模型，但这带来了显著的计算开销。需要在预测准确性和计算延迟之间取得平衡，但现有方法未能从根本上解决这一困境。

Method: 提出PRISM架构，将每个预测步骤的计算分散到不同的参数集中，重构草稿模型的计算路径，成功实现模型容量与推理成本的解耦。

Result: PRISM在所有现有草稿架构中表现最佳，实现了卓越的接受长度，同时保持最小的草稿延迟，获得优异的端到端加速。在高度优化的推理引擎上，解码吞吐量提升超过2.6倍。

Conclusion: PRISM通过架构创新解决了推测解码中模型容量与计算开销的根本矛盾，重新审视了缩放定律，显示PRISM在数据量扩展时比其他草稿架构更有效。

Abstract: Large Language Models (LLMs), constrained by their auto-regressive nature, suffer from slow decoding. Speculative decoding methods have emerged as a promising solution to accelerate LLM decoding, attracting attention from both systems and AI research communities. Recently, the pursuit of better draft quality has driven a trend toward parametrically larger draft models, which inevitably introduces substantial computational overhead. While existing work attempts to balance the trade-off between prediction accuracy and compute latency, we address this fundamental dilemma through architectural innovation.
  We propose PRISM, which disaggregates the computation of each predictive step across different parameter sets, refactoring the computational pathways of draft models to successfully decouple model capacity from inference cost. Through extensive experiments, we demonstrate that PRISM outperforms all existing draft architectures, achieving exceptional acceptance lengths while maintaining minimal draft latency for superior end-to-end speedup. We also re-examine scaling laws with PRISM, revealing that PRISM scales more effectively with expanding data volumes than other draft architectures. Through rigorous and fair comparison, we show that PRISM boosts the decoding throughput of an already highly optimized inference engine by more than 2.6x.

</details>


### [125] [Efficient Cross-Architecture Knowledge Transfer for Large-Scale Online User Response Prediction](https://arxiv.org/abs/2602.01775)
*Yucheng Wu,Yuekui Yang,Hongzheng Li,Anan Liu,Jian Xiao,Junjie Zhai,Huan Yu,Shaoping Ma,Leye Wang*

Main category: cs.AI

TL;DR: CrossAdapt是一个两阶段跨架构知识迁移框架，通过离线快速嵌入迁移和在线非对称协同蒸馏，在用户响应预测系统中实现高效模型切换，减少训练时间43-71%的同时提升AUC 0.27-0.43%。


<details>
  <summary>Details</summary>
Motivation: 大规模用户响应预测系统中部署新架构面临高模型切换成本，包括海量历史数据重新训练的开销和数据保留约束下的性能下降。现有知识蒸馏方法难以处理架构异构性和大规模嵌入表迁移的高昂成本。

Method: 提出两阶段框架：离线阶段通过维度自适应投影实现快速嵌入迁移（无需迭代训练），结合渐进网络蒸馏和策略采样降低计算成本；在线阶段引入非对称协同蒸馏（学生频繁更新，教师低频更新）和分布感知适应机制，动态平衡历史知识保留与快速适应演化数据。

Result: 在三个公开数据集上，CrossAdapt实现AUC提升0.27-0.43%，训练时间减少43-71%。在腾讯微信视频号大规模部署（约1000万日样本）中，显著缓解了AUC下降、LogLoss增加和预测偏差。

Conclusion: CrossAdapt通过高效的跨架构知识迁移，解决了大规模推荐系统中模型切换的高成本问题，在保持性能的同时大幅降低训练开销，为实际工业部署提供了有效解决方案。

Abstract: Deploying new architectures in large-scale user response prediction systems incurs high model switching costs due to expensive retraining on massive historical data and performance degradation under data retention constraints. Existing knowledge distillation methods struggle with architectural heterogeneity and the prohibitive cost of transferring large embedding tables. We propose CrossAdapt, a two-stage framework for efficient cross-architecture knowledge transfer. The offline stage enables rapid embedding transfer via dimension-adaptive projections without iterative training, combined with progressive network distillation and strategic sampling to reduce computational cost. The online stage introduces asymmetric co-distillation, where students update frequently while teachers update infrequently, together with a distribution-aware adaptation mechanism that dynamically balances historical knowledge preservation and fast adaptation to evolving data. Experiments on three public datasets show that CrossAdapt achieves 0.27-0.43% AUC improvements while reducing training time by 43-71%. Large-scale deployment on Tencent WeChat Channels (~10M daily samples) further demonstrates its effectiveness, significantly mitigating AUC degradation, LogLoss increase, and prediction bias compared to standard distillation baselines.

</details>


### [126] [LingLanMiDian: Systematic Evaluation of LLMs on TCM Knowledge and Clinical Reasoning](https://arxiv.org/abs/2602.01779)
*Rui Hua,Yu Wei,Zixin Shu,Kai Chang,Dengying Yan,Jianan Xia,Zeyu Liu,Hui Zhu,Shujie Song,Mingzhong Xiao,Xiaodong Li,Dongmei Jia,Zhuye Gao,Yanyan Meng,Naixuan Zhao,Yu Fu,Haibin Yu,Benman Yu,Yuanyuan Chen,Fei Dong,Zhizhou Meng,Pengcheng Yang,Songxue Zhao,Lijuan Pei,Yunhui Hu,Kan Ding,Jiayuan Duan,Wenmao Yin,Yang Gu,Runshun Zhang,Qiang Zhu,Jian Yu,Jiansheng Li,Baoyan Liu,Wenjia Wang,Xuezhong Zhou*

Main category: cs.AI

TL;DR: LingLanMiDian (LingLan) 是一个大规模、专家策划的中医多任务基准测试，统一评估知识回忆、多跳推理、信息抽取和临床决策，揭示当前LLMs与中医专家在专业推理上的显著差距。


<details>
  <summary>Details</summary>
Motivation: 当前中医领域的LLM评估存在基准测试碎片化、覆盖范围有限、评分方法不统一等问题，阻碍了公平比较和模型发展。中医具有独特的本体论、术语和推理模式，需要领域忠实的评估。

Method: 构建LingLan基准测试，包含专家策划的大规模多任务套件，引入一致的度量设计、临床标签的同义词容忍协议、每个数据集400项的困难子集，并将诊断和治疗建议重构为单项选择决策识别。

Result: 对14个领先的开源和专有LLMs进行零样本评估，在困难子集上显示当前模型与中医专家在专业推理上存在显著差距，为中医LLMs提供了统一的性能视角。

Conclusion: LingLan通过标准化评估桥接基础知识和应用推理，为推进中医LLMs和领域特定医疗AI研究建立了统一、可量化、可扩展的基础。

Abstract: Large language models (LLMs) are advancing rapidly in medical NLP, yet Traditional Chinese Medicine (TCM) with its distinctive ontology, terminology, and reasoning patterns requires domain-faithful evaluation. Existing TCM benchmarks are fragmented in coverage and scale and rely on non-unified or generation-heavy scoring that hinders fair comparison. We present the LingLanMiDian (LingLan) benchmark, a large-scale, expert-curated, multi-task suite that unifies evaluation across knowledge recall, multi-hop reasoning, information extraction, and real-world clinical decision-making. LingLan introduces a consistent metric design, a synonym-tolerant protocol for clinical labels, a per-dataset 400-item Hard subset, and a reframing of diagnosis and treatment recommendation into single-choice decision recognition. We conduct comprehensive, zero-shot evaluations on 14 leading open-source and proprietary LLMs, providing a unified perspective on their strengths and limitations in TCM commonsense knowledge understanding, reasoning, and clinical decision support; critically, the evaluation on Hard subset reveals a substantial gap between current models and human experts in TCM-specialized reasoning. By bridging fundamental knowledge and applied reasoning through standardized evaluation, LingLan establishes a unified, quantitative, and extensible foundation for advancing TCM LLMs and domain-specific medical AI research. All evaluation data and code are available at https://github.com/TCMAI-BJTU/LingLan and http://tcmnlp.com.

</details>


### [127] [ORCH: many analyses, one merge-a deterministic multi-agent orchestrator for discrete-choice reasoning with EMA-guided routing](https://arxiv.org/abs/2602.01797)
*Hanlin Zhou,Huah Yong Chan*

Main category: cs.AI

TL;DR: ORCH是一个确定性的多智能体协调框架，通过"多分析、一决策"范式，让异构大语言模型协作完成离散选择推理任务，显著提升准确率且保持可复现性。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的多智能体系统通常依赖随机路由或临时启发式方法，导致行为难以复现、决策过程难以解释。需要一种确定性的协调框架来提高可预测性和可解释性。

Method: ORCH采用"多分析、一决策"范式：多个基础模型独立生成结构化分析，专门的合并智能体输出最终选择。框架使用固定规则进行任务分解和答案聚合，保持流程可预测、可复现且无需训练。可选引入EMA引导的路由器，基于历史准确率、延迟或成本更新智能体选择。

Result: 在MMLU、MMLU-Pro和GSM8K基准测试中，ORCH持续优于单模型基线和多数投票集成。在MMLU-Pro上准确率提升超过10个百分点，在GSM8K上提升超过50个百分点。EMA路由器提供额外0.7-2.0个百分点的准确率提升。

Conclusion: ORCH为离散选择推理任务提供了一个实用、可控、可解释且可直接部署的大语言模型多智能体系统框架，平衡了性能提升与系统可预测性。

Abstract: Recent advances in large-scale language models (LLMs) have made multi-agent architectures attractive for challenging reasoning tasks. However, many existing systems rely on stochastic routing or ad-hoc heuristics, making their behavior difficult to reproduce and their decision process hard to interpret. We propose ORCH, a deterministic coordination framework for discrete-choice reasoning that orchestrates heterogeneous LLMs. ORCH follows a ``many analyses, one decision'' paradigm: multiple base models independently produce structured analyses, and a dedicated merge agent outputs the final choice. The framework uses fixed rules for task decomposition and answer aggregation, keeping the pipeline predictable, reproducible, and training-free. Determinism here refers to fixed routing and aggregation rules under a fixed evaluation protocol, rather than strict bit-level reproducibility across deployments. To exploit model complementarity, we optionally introduce an EMA-guided router that updates agent selection using historical accuracy, latency, or cost; since it relies on answer-based feedback, it is mainly intended for benchmarking, controlled evaluation, or delayed-feedback settings. Experiments on MMLU, MMLU-Pro, and GSM8K show that ORCH consistently outperforms single-model baselines and a majority-vote ensemble. On MMLU-Pro, ORCH improves accuracy by over 10 points compared to the strongest baseline, and on GSM8K it yields gains exceeding 50 points; McNemar tests confirm statistical significance. The EMA router provides an additional 0.7--2.0 point accuracy boost, and ablations show that both multi-agent collaboration and routing contribute substantially. Overall, ORCH offers a practical path toward controllable, interpretable, and deployment-ready LLM-based agent systems for discrete-choice reasoning.

</details>


### [128] [INDIBATOR: Diverse and Fact-Grounded Individuality for Multi-Agent Debate in Molecular Discovery](https://arxiv.org/abs/2602.01815)
*Yunhui Jang,Seonghyun Park,Jaehyung Kim,Sungsoo Ahn*

Main category: cs.AI

TL;DR: INDIBATOR框架通过基于个体科学家研究轨迹的细粒度角色建模，在分子发现任务中超越了传统的粗粒度角色分配方法


<details>
  <summary>Details</summary>
Motivation: 当前多智能体科学发现系统通常使用通用的角色型人格（如"评审员"、"作者"）或基于关键词的粗粒度人格，这过度简化了真实科学家的行为方式。科学家的贡献实际上由其独特的研究轨迹塑造，因此需要更精细的个体化建模。

Method: 提出INDIBATOR框架，通过两种模态构建个体化科学家档案：1) 发表历史（文献知识）和 2) 分子历史（结构先验）。这些智能体通过提案、批评和投票三个阶段进行多轮辩论。

Result: 基于细粒度个体化角色的智能体系统持续优于依赖粗粒度角色的系统，达到竞争性或最先进的性能水平。

Conclusion: 捕捉个体智能体的"科学DNA"对于高质量的科学发现至关重要，这验证了个体化建模在多智能体科学发现系统中的重要性。

Abstract: Multi-agent systems have emerged as a powerful paradigm for automating scientific discovery. To differentiate agent behavior in the multi-agent system, current frameworks typically assign generic role-based personas such as ''reviewer'' or ''writer'' or rely on coarse grained keyword-based personas. While functional, this approach oversimplifies how human scientists operate, whose contributions are shaped by their unique research trajectories. In response, we propose INDIBATOR, a framework for molecular discovery that grounds agents in individualized scientist profiles constructed from two modalities: publication history for literature-derived knowledge and molecular history for structural priors. These agents engage in multi-turn debate through proposal, critique, and voting phases. Our evaluation demonstrates that these fine-grained individuality-grounded agents consistently outperform systems relying on coarse-grained personas, achieving competitive or state-of-the-art performance. These results validate that capturing the ``scientific DNA'' of individual agents is essential for high-quality discovery.

</details>


### [129] [Synesthesia of Vehicles: Tactile Data Synthesis from Visual Inputs](https://arxiv.org/abs/2602.01832)
*Rui Wang,Yaoguang Cao,Yuyi Chen,Jianyi Xu,Zhuoyang Li,Jiachen Shang,Shichun Yang*

Main category: cs.AI

TL;DR: 提出Synesthesia of Vehicles (SoV)框架，通过视觉输入预测触觉激励，解决自动驾驶车辆对路面激励感知不足的问题，提升安全性。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶车辆依赖多模态融合保证安全，但视觉和光学传感器无法检测路面引起的激励，而这些激励对车辆动态控制至关重要。受人类联觉启发，需要开发能够从视觉输入预测触觉激励的方法。

Method: 1. 提出Synesthesia of Vehicles (SoV)框架；2. 开发跨模态时空对齐方法处理时间和空间差异；3. 提出基于潜在扩散的视觉-触觉联觉生成模型(VTSyn)进行无监督高质量触觉数据合成；4. 使用真实车辆感知系统收集多模态数据集。

Result: VTSyn在时间、频率和分类性能上优于现有模型，通过主动触觉感知增强自动驾驶安全性。在多样道路和光照条件下收集的数据集支持了实验验证。

Conclusion: SoV框架成功实现了从视觉到触觉的跨模态预测，解决了自动驾驶车辆对路面激励感知的局限性，为提升车辆动态控制和安全性提供了新方法。

Abstract: Autonomous vehicles (AVs) rely on multi-modal fusion for safety, but current visual and optical sensors fail to detect road-induced excitations which are critical for vehicles' dynamic control. Inspired by human synesthesia, we propose the Synesthesia of Vehicles (SoV), a novel framework to predict tactile excitations from visual inputs for autonomous vehicles. We develop a cross-modal spatiotemporal alignment method to address temporal and spatial disparities. Furthermore, a visual-tactile synesthetic (VTSyn) generative model using latent diffusion is proposed for unsupervised high-quality tactile data synthesis. A real-vehicle perception system collected a multi-modal dataset across diverse road and lighting conditions. Extensive experiments show that VTSyn outperforms existing models in temporal, frequency, and classification performance, enhancing AV safety through proactive tactile perception.

</details>


### [130] [ROMA: Recursive Open Meta-Agent Framework for Long-Horizon Multi-Agent Systems](https://arxiv.org/abs/2602.01848)
*Salaheddin Alzu'bi,Baran Nama,Arda Kaz,Anushri Eswaran,Weiyuan Chen,Sarvesh Khetan,Rishab Bala,Tu Vu,Sewoong Oh*

Main category: cs.AI

TL;DR: ROMA是一个递归开放元代理框架，通过任务分解和结构化聚合解决长时程任务中的性能问题，支持异构多智能体系统，结合GEPA+实现领先的系统级性能。


<details>
  <summary>Details</summary>
Motivation: 当前智能体框架在长时程任务中表现不佳，随着推理深度增加，顺序编排变得脆弱，上下文窗口限制导致性能下降，不透明的执行轨迹难以调试。

Method: ROMA采用递归任务分解和结构化聚合，将目标分解为依赖感知的子任务树并行执行，通过聚合压缩和验证中间结果控制上下文增长。框架包含四个模块化角色：Atomizer、Planner、Executor、Aggregator。结合GEPA+进行提示搜索。

Result: 在SEAL-0推理基准上，ROMA+GLM-4.6相比Kimi-Researcher准确率提升9.9%；在EQ-Bench长文本生成基准上，ROMA+DeepSeek-V3达到与Claude Sonnet 4.5相当的性能。

Conclusion: 递归模块化智能体架构能够在保持可解释性、灵活性和模型无关性的同时扩展推理深度，为长时程任务提供了有效的解决方案。

Abstract: Current agentic frameworks underperform on long-horizon tasks. As reasoning depth increases, sequential orchestration becomes brittle, context windows impose hard limits that degrade performance, and opaque execution traces make failures difficult to localize or debug. We introduce ROMA (Recursive Open Meta-Agents), a domain-agnostic framework that addresses these limitations through recursive task decomposition and structured aggregation. ROMA decomposes goals into dependency-aware subtask trees that can be executed in parallel, while aggregation compresses and validates intermediate results to control context growth. Our framework standardizes agent construction around four modular roles --Atomizer (which decides whether a task should be decomposed), Planner, Executor, and Aggregator -- which cleanly separate orchestration from model selection and enable transparent, hierarchical execution traces. This design supports heterogeneous multi-agent systems that mix models and tools according to cost, latency, and capability. To adapt ROMA to specific tasks without fine-tuning, we further introduce GEPA$+$, an improved Genetic-Pareto prompt proposer that searches over prompts within ROMA's component hierarchy while preserving interface contracts. We show that ROMA, combined with GEPA+, delivers leading system-level performance on reasoning and long-form generation benchmarks. On SEAL-0, which evaluates reasoning over conflicting web evidence, ROMA instantiated with GLM-4.6 improves accuracy by 9.9\% over Kimi-Researcher. On EQ-Bench, a long-form writing benchmark, ROMA enables DeepSeek-V3 to match the performance of leading closed-source models such as Claude Sonnet 4.5. Our results demonstrate that recursive, modular agent architectures can scale reasoning depth while remaining interpretable, flexible, and model-agnostic.

</details>


### [131] [SOPRAG: Multi-view Graph Experts Retrieval for Industrial Standard Operating Procedures](https://arxiv.org/abs/2602.01858)
*Liangtao Lin,Zhaomeng Zhu,Tianwei Zhang,Yonggang Wen*

Main category: cs.AI

TL;DR: SOPRAG：针对工业标准操作规程检索的新型RAG框架，采用专家混合架构解决传统RAG在工业环境中的局限性，显著提升检索准确性和响应实用性。


<details>
  <summary>Details</summary>
Motivation: 工业环境中的标准操作规程（SOPs）检索面临独特挑战：刚性专有结构、条件相关性和可执行性要求，传统语义驱动的RAG范式无法有效处理这些问题。

Method: 提出SOPRAG框架，采用专家混合范式，用专门的实体、因果和流程图专家替代平面分块；引入Procedure Card层修剪搜索空间，以及LLM引导的门控机制动态加权专家；开发自动化多智能体工作流构建基准数据集。

Result: 在四个工业领域的广泛实验中，SOPRAG在检索准确性和响应实用性方面显著优于基于词汇、稠密和图的RAG基线，在现实关键任务中实现完美执行分数。

Conclusion: SOPRAG成功解决了工业SOP检索的独特挑战，通过专家混合架构和智能协调机制，为工业环境提供了高效可靠的操作规程检索解决方案。

Abstract: Standard Operating Procedures (SOPs) are essential for ensuring operational safety and consistency in industrial environments. However, retrieving and following these procedures presents unique challenges, such as rigid proprietary structures, condition-dependent relevance, and actionable execution requirement, which standard semantic-driven Retrieval-Augmented Generation (RAG) paradigms fail to address. Inspired by the Mixture-of-Experts (MoE) paradigm, we propose SOPRAG, a novel framework specifically designed to address the above pain points in SOP retrieval. SOPRAG replaces flat chunking with specialized Entity, Causal, and Flow graph experts to resolve industrial structural and logical complexities. To optimize and coordinate these experts, we propose a Procedure Card layer that prunes the search space to eliminate computational noise, and an LLM-Guided gating mechanism that dynamically weights these experts to align retrieval with operator intent. To address the scarcity of domain-specific data, we also introduce an automated, multi-agent workflow for benchmark construction. Extensive experiments across four industrial domains demonstrate that SOPRAG significantly outperforms strong lexical, dense, and graph-based RAG baselines in both retrieval accuracy and response utility, achieving perfect execution scores in real-world critical tasks.

</details>


### [132] [ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents](https://arxiv.org/abs/2602.01869)
*Qirui Mi,Zhijian Ma,Mengyue Yang,Haoxuan Li,Yisen Wang,Haifeng Zhang,Jun Wang*

Main category: cs.AI

TL;DR: ProcMEM框架让LLM智能体从交互经验中自主习得程序性记忆，无需参数更新，通过技能MDP将被动叙事转化为可执行技能，实现高效经验复用


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体在顺序决策中依赖即时推理，即使面对重复场景也重新推导解决方案，导致计算冗余和执行不稳定，缺乏经验复用机制

Method: 提出ProcMEM框架：1) 形式化Skill-MDP将被动叙事转化为可执行技能；2) 引入非参数PPO，利用语义梯度生成高质量候选技能，PPO Gate进行技能验证；3) 基于分数的维护机制保持紧凑高质量的程序性记忆

Result: 在领域内、跨任务和跨智能体场景中，ProcMEM实现了更高的复用率和显著性能提升，同时保持极端的记忆压缩。可视化进化轨迹和技能分布展示了透明积累、精炼和复用程序性知识的过程

Conclusion: ProcMEM通过自主习得程序性记忆，有效解决了LLM智能体经验复用不足的问题，实现了计算效率提升和长期自主性，为智能体经验积累提供了透明可解释的框架

Abstract: LLM-driven agents demonstrate strong performance in sequential decision-making but often rely on on-the-fly reasoning, re-deriving solutions even in recurring scenarios. This insufficient experience reuse leads to computational redundancy and execution instability. To bridge this gap, we propose ProcMEM, a framework that enables agents to autonomously learn procedural memory from interaction experiences without parameter updates. By formalizing a Skill-MDP, ProcMEM transforms passive episodic narratives into executable Skills defined by activation, execution, and termination conditions to ensure executability. To achieve reliable reusability without capability degradation, we introduce Non-Parametric PPO, which leverages semantic gradients for high-quality candidate generation and a PPO Gate for robust Skill verification. Through score-based maintenance, ProcMEM sustains compact, high-quality procedural memory. Experimental results across in-domain, cross-task, and cross-agent scenarios demonstrate that ProcMEM achieves superior reuse rates and significant performance gains with extreme memory compression. Visualized evolutionary trajectories and Skill distributions further reveal how ProcMEM transparently accumulates, refines, and reuses procedural knowledge to facilitate long-term autonomy.

</details>


### [133] [Entropy-Guided Data-Efficient Training for Multimodal Reasoning Reward Models](https://arxiv.org/abs/2602.01884)
*Shidong Yang,Tongwen Huang,Hao Wen,Yong Wang,Li Chen,Xiangxiang Chu*

Main category: cs.AI

TL;DR: 提出基于熵引导的训练方法EGT，通过熵作为无监督代理来识别噪声样本和样本难度，提升多模态推理奖励模型的性能


<details>
  <summary>Details</summary>
Motivation: 多模态奖励模型在训练中面临两个关键挑战：1) 偏好数据集的固有噪声会降低模型性能；2) 传统训练方法效率低下，忽略了样本难度的差异

Method: 提出熵引导训练(EGT)方法，包括：1) 熵引导数据筛选，基于响应熵识别不可靠样本；2) 熵引导训练策略，逐步引入更复杂的样本进行训练

Result: 在三个基准测试上的广泛实验表明，EGT训练的模型始终优于最先进的多模态奖励模型

Conclusion: 响应熵与准确性之间存在强相关性，熵可以作为标注噪声和样本难度的可靠无监督代理，EGT方法能有效提升多模态推理奖励模型的性能

Abstract: Multimodal reward models are crucial for aligning multimodal large language models with human preferences. Recent works have incorporated reasoning capabilities into these models, achieving promising results. However, training these models suffers from two critical challenges: (1) the inherent noise in preference datasets, which degrades model performance, and (2) the inefficiency of conventional training methods, which ignore the differences in sample difficulty. In this paper, we identify a strong correlation between response entropy and accuracy, indicating that entropy can serve as a reliable and unsupervised proxy for annotation noise and sample difficulty. Based on this insight, we propose a novel Entropy-Guided Training (EGT) approach for multimodal reasoning reward models, which combines two strategies: (1) entropy-guided data curation to mitigate the impact of unreliable samples, and (2) an entropy-guided training strategy that progressively introduces more complex examples. Extensive experiments across three benchmarks show that the EGT-trained model consistently outperforms state-of-the-art multimodal reward models.

</details>


### [134] [Geometric Analysis of Token Selection in Multi-Head Attention](https://arxiv.org/abs/2602.01893)
*Timur Mudarisov,Mikhal Burtsev,Tatiana Petrova,Radu State*

Main category: cs.AI

TL;DR: 该论文提出了一个几何框架来分析LLM中的多头注意力机制，将注意力视为在值状态空间中的top-N选择器，定义了Precision、Recall、F-score等几何度量，并推导了非渐近边界。理论预测了小N操作区域的最强可分性，并通过实验验证了理论预测。


<details>
  <summary>Details</summary>
Motivation: 现有对LLM中注意力机制的分析缺乏几何视角和量化度量。作者希望建立一个几何框架来理解注意力如何选择和分离token，提供头级别的可解释性，并为注意力稀疏化和设计提供几何感知的指导。

Method: 将标准注意力视为值状态空间中的top-N选择器，定义几何度量（Precision、Recall、F-score）来量化选中与未选中token之间的可分性。在经验假设下推导非渐近边界，包括稳定值范数、压缩sink token、指数相似度衰减和分段注意力权重分布。在LLaMA-2-7B、Gemma-7B和Mistral-7B上进行实证验证。

Result: 理论预测了小N操作区域的最强非平凡可分性，阐明了序列长度和sink相似度如何影响几何度量。实证结果显示测量值与理论包络线紧密匹配：top-N选择增强了可分性，sink相似度与Recall相关。在LLaMA-2-7B中发现注意力头专门化为三种机制：Retriever、Mixer、Reset，具有不同的几何特征。

Conclusion: 注意力机制表现为具有可测量token选择标准的结构化几何分类器，提供了头级别的可解释性，并为几何感知的注意力稀疏化和LLM中注意力机制的设计提供了信息。

Abstract: We present a geometric framework for analysing multi-head attention in large language models (LLMs). Without altering the mechanism, we view standard attention through a top-N selection lens and study its behaviour directly in value-state space. We define geometric metrics - Precision, Recall, and F-score - to quantify separability between selected and non-selected tokens, and derive non-asymptotic bounds with explicit dependence on dimension and margin under empirically motivated assumptions (stable value norms with a compressed sink token, exponential similarity decay, and piecewise attention weight profiles). The theory predicts a small-N operating regime of strongest non-trivial separability and clarifies how sequence length and sink similarity shape the metrics. Empirically, across LLaMA-2-7B, Gemma-7B, and Mistral-7B, measurements closely track the theoretical envelopes: top-N selection sharpens separability, sink similarity correlates with Recall. We also found that in LLaMA-2-7B heads specialize into three regimes - Retriever, Mixer, Reset - with distinct geometric signatures. Overall, attention behaves as a structured geometric classifier with measurable criteria for token selection, offering head level interpretability and informing geometry-aware sparsification and design of attention in LLMs.

</details>


### [135] [DomusFM: A Foundation Model for Smart-Home Sensor Data](https://arxiv.org/abs/2602.01910)
*Michele Fiori,Gabriele Civitarese,Flora D. Salim,Claudio Bettini*

Main category: cs.AI

TL;DR: DomusFM是首个专门为智能家居传感器数据设计的预训练基础模型，通过自监督双对比学习范式，在数据稀缺情况下实现跨环境和任务的迁移学习，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 智能家居传感器数据在医疗监测和辅助技术中有重要应用潜力，但现有方法存在关键限制：监督模型需要大量标注数据；现有基础模型只关注惯性传感器，无法处理智能家居二进制传感器数据的稀疏离散特性和丰富语义关联；基于LLM的方法存在隐私和成本问题。

Method: 提出DomusFM基础模型，采用自监督双对比学习范式，结合轻量级语言模型的语义嵌入、时间模式专用编码器和二进制状态编码器，同时捕获token级语义属性和序列级时间依赖关系。

Result: 在7个公开智能家居数据集上进行留一数据集评估，DomusFM在不同下游任务中均优于最先进的基线方法，即使在仅有5%标注数据用于微调的情况下也能实现优异性能。

Conclusion: DomusFM解决了智能家居传感器数据分析中的数据稀缺问题，同时保持了实际部署的可行性，为现实世界的智能家居系统提供了实用解决方案。

Abstract: Smart-home sensor data holds significant potential for several applications, including healthcare monitoring and assistive technologies. Existing approaches, however, face critical limitations. Supervised models require impractical amounts of labeled data. Foundation models for activity recognition focus only on inertial sensors, failing to address the unique characteristics of smart-home binary sensor events: their sparse, discrete nature combined with rich semantic associations. LLM-based approaches, while tested in this domain, still raise several issues regarding the need for natural language descriptions or prompting, and reliance on either external services or expensive hardware, making them infeasible in real-life scenarios due to privacy and cost concerns. We introduce DomusFM, the first foundation model specifically designed and pretrained for smart-home sensor data. DomusFM employs a self-supervised dual contrastive learning paradigm to capture both token-level semantic attributes and sequence-level temporal dependencies. By integrating semantic embeddings from a lightweight language model and specialized encoders for temporal patterns and binary states, DomusFM learns generalizable representations that transfer across environments and tasks related to activity and event analysis. Through leave-one-dataset-out evaluation across seven public smart-home datasets, we demonstrate that DomusFM outperforms state-of-the-art baselines on different downstream tasks, achieving superior performance even with only 5% of labeled training data available for fine-tuning. Our approach addresses data scarcity while maintaining practical deployability for real-world smart-home systems.

</details>


### [136] [Large Language Model and Formal Concept Analysis: a comparative study for Topic Modeling](https://arxiv.org/abs/2602.01933)
*Fabrice Boissier,Monica Sen,Irina Rychkova*

Main category: cs.AI

TL;DR: 本文比较了大型语言模型（LLM）和形式概念分析（FCA）在主题建模任务中的表现，通过两个实验评估了GPT-5和CREA管道在不同文本数据集上的效果。


<details>
  <summary>Details</summary>
Motivation: 主题建模是一个应用日益广泛的研究领域，从文档检索到情感分析和文本摘要。虽然大型语言模型（LLM）目前在文本处理中很流行，但很少有研究探讨它们在主题建模任务中的有效性。同时，形式概念分析（FCA）最近被提出作为主题建模的候选方法，但缺乏实际应用案例研究。

Method: 研究采用比较方法，评估FCA和LLM在主题建模中的表现。FCA通过CREA管道进行评估，该管道在过去的主题建模和可视化实验中已使用过。LLM方面使用GPT-5，采用基于三个提示的零样本设置策略：从文档批次生成主题、将批次结果合并为最终主题、以及主题标注。研究进行了两个实验：第一个实验重用之前评估CREA的教学材料，第二个实验分析了40篇信息系统研究文章，比较提取的主题与底层子领域。

Result: 论文没有在摘要中提供具体结果数据，但通过两个实验设计来比较两种方法：第一个实验使用教学材料评估CREA，第二个实验使用信息系统研究文章比较提取的主题与实际子领域。这表明研究旨在通过实证比较来理解两种方法的优缺点。

Conclusion: 该研究旨在通过比较LLM（特别是GPT-5）和FCA（通过CREA管道）在主题建模任务中的表现，更好地理解这两种方法的优缺点，为这一研究领域提供实证依据。

Abstract: Topic modeling is a research field finding increasing applications: historically from document retrieving, to sentiment analysis and text summarization. Large Language Models (LLM) are currently a major trend in text processing, but few works study their usefulness for this task. Formal Concept Analysis (FCA) has recently been presented as a candidate for topic modeling, but no real applied case study has been conducted. In this work, we compare LLM and FCA to better understand their strengths and weakneses in the topic modeling field. FCA is evaluated through the CREA pipeline used in past experiments on topic modeling and visualization, whereas GPT-5 is used for the LLM. A strategy based on three prompts is applied with GPT-5 in a zero-shot setup: topic generation from document batches, merging of batch results into final topics, and topic labeling. A first experiment reuses the teaching materials previously used to evaluate CREA, while a second experiment analyzes 40 research articles in information systems to compare the extracted topics with the underling subfields.

</details>


### [137] [Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models](https://arxiv.org/abs/2602.01970)
*Yun Qu,Qi Wang,Yixiu Mao,Heming Zou,Yuhang Jiang,Weijie Liu,Clive Bai,Kai Yang,Yangkun Chen,Saiyong Yang,Xiangyang Ji*

Main category: cs.AI

TL;DR: GPS使用轻量级生成模型进行贝叶斯推断，通过中间难度优先和历史锚定多样性选择信息丰富的提示批次，显著提升训练效率、最终性能和测试时效率


<details>
  <summary>Details</summary>
Motivation: 强化学习增强大语言模型推理能力但计算成本高，现有在线提示选择方法要么依赖昂贵精确评估，要么缺乏跨提示泛化能力

Method: 提出GPS方法：使用轻量级生成模型在共享优化历史上进行贝叶斯推断预测提示难度，结合中间难度优先和历史锚定多样性的批量获取原则选择信息丰富的提示批次

Result: 在多样化推理基准测试中，GPS在训练效率、最终性能和测试时效率方面均显著优于基线方法

Conclusion: GPS通过可泛化的预测性提示选择有效解决了强化学习中计算成本高的问题，实现了高效训练和性能提升

Abstract: Reinforcement learning enhances the reasoning capabilities of large language models but often involves high computational costs due to rollout-intensive optimization. Online prompt selection presents a plausible solution by prioritizing informative prompts to improve training efficiency. However, current methods either depend on costly, exact evaluations or construct prompt-specific predictive models lacking generalization across prompts. This study introduces Generalizable Predictive Prompt Selection (GPS), which performs Bayesian inference towards prompt difficulty using a lightweight generative model trained on the shared optimization history. Intermediate-difficulty prioritization and history-anchored diversity are incorporated into the batch acquisition principle to select informative prompt batches. The small predictive model also generalizes at test-time for efficient computational allocation. Experiments across varied reasoning benchmarks indicate GPS's substantial improvements in training efficiency, final performance, and test-time efficiency over superior baseline methods.

</details>


### [138] [Evolving from Tool User to Creator via Training-Free Experience Reuse in Multimodal Reasoning](https://arxiv.org/abs/2602.01983)
*Xintian Shen,Jiawei Chen,Lihao Zheng,Hao Ma,Tao Wei,Kun Zhan*

Main category: cs.AI

TL;DR: UCT框架让LLM从工具使用者转变为工具创造者，通过提取推理经验构建自适应工具库，无需额外训练即可持续改进推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有工具集成推理模型存在三个主要问题：1) 固定工具难以应对开放性问题；2) 错误工具输出会误导LLM；3) 工具构建需要大量人工工作，限制了适用性。需要一种能让LLM自主创建和优化工具的方法。

Method: 提出UCT框架，将LLM从工具使用者转变为工具创造者。通过提取推理过程中的经验，将其蒸馏为可重用的工具资产。引入记忆巩固机制维护工具库，确保经验记忆的高复用性。整个框架无需训练，在推理过程中自适应创建和更新工具。

Result: 在数学和科学推理任务的多领域基准测试中，UCT实现了显著性能提升：+20.86%和+23.04%的性能增益，验证了代理的自进化能力。

Conclusion: UCT为增强工具集成推理模型能力提供了一种新范式，通过让LLM自主创建和优化工具，实现了持续自我改进，无需额外训练即可提升整体代理系统的性能。

Abstract: Existing Tool-Integrated Reasoning (TIR) models have effectively extended the question-answering capabilities of LLMs by incorporating external tools. However, real-world scenarios present numerous open-ended problems where fixed tools often fail to meet task requirements. Furthermore, the lack of self-optimization mechanisms means that erroneous tool outputs can mislead the LLM's responses. Additionally, the construction of existing tools entails significant manual effort, which consequently constrains their applicability. Recognizing that the reasoning traces of LLMs encapsulate implicit problem-solving capabilities, we propose UCT, a novel training-free framework that transforms agents from tool users to tool creators. This approach harvests reasoning experiences and distills them into reusable assets. This method transforms the agent from a mere tool user into a tool creator, enabling adaptive tool creation and self-updating during the inference process. We also introduce a memory consolidation mechanism to maintain the tool library, ensuring high reusability of retained experiential memory for subsequent reasoning tasks. This novel automated tool construction paradigm continuously improves tool quality during reasoning, allowing the overall agent system to progress without additional training. Extensive experiments demonstrate that our method serves as a novel paradigm for enhancing the capabilities of TIR models. In particular, the significant performance gains achieved +20.86%$\uparrow$ and +23.04%$\uparrow$ on benchmarks across multi-domain mathematical and scientific reasoning tasks validate the self-evolving capability of the agent.

</details>


### [139] [Emergent Analogical Reasoning in Transformers](https://arxiv.org/abs/2602.01992)
*Gouki Minegishi,Jingyuan Feng,Hiroki Furuta,Takeshi Kojima,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.AI

TL;DR: 论文将类比推理形式化为跨类别实体对应关系的推断，基于范畴论中的函子概念，通过合成任务研究Transformer中类比推理的涌现机制，发现其依赖于嵌入空间的几何对齐和Transformer内部的函子应用。


<details>
  <summary>Details</summary>
Motivation: 类比是人类智能的核心能力，但Transformer如何获得和实施类比推理的机制仍不清楚。研究旨在将类比从抽象认知概念转化为现代神经网络中具体、机制可解释的现象。

Method: 基于范畴论中的函子概念，将类比推理形式化为跨类别实体对应关系的推断。引入合成任务在受控设置下评估类比推理的涌现，并进行机制分析。

Result: 发现类比推理的涌现对数据特征、优化选择和模型规模高度敏感。机制分析表明Transformer中的类比推理分解为两个关键组件：嵌入空间中关系结构的几何对齐，以及Transformer内部的函子应用。

Conclusion: 研究将类比从抽象认知概念转化为现代神经网络中具体、机制可解释的现象，揭示了Transformer实现类比推理的机制，并在预训练大语言模型中观察到相同趋势。

Abstract: Analogy is a central faculty of human intelligence, enabling abstract patterns discovered in one domain to be applied to another. Despite its central role in cognition, the mechanisms by which Transformers acquire and implement analogical reasoning remain poorly understood. In this work, inspired by the notion of functors in category theory, we formalize analogical reasoning as the inference of correspondences between entities across categories. Based on this formulation, we introduce synthetic tasks that evaluate the emergence of analogical reasoning under controlled settings. We find that the emergence of analogical reasoning is highly sensitive to data characteristics, optimization choices, and model scale. Through mechanistic analysis, we show that analogical reasoning in Transformers decomposes into two key components: (1) geometric alignment of relational structure in the embedding space, and (2) the application of a functor within the Transformer. These mechanisms enable models to transfer relational structure from one category to another, realizing analogy. Finally, we quantify these effects and find that the same trends are observed in pretrained LLMs. In doing so, we move analogy from an abstract cognitive notion to a concrete, mechanistically grounded phenomenon in modern neural networks.

</details>


### [140] [Thinking Like a Doctor: Conversational Diagnosis through the Exploration of Diagnostic Knowledge Graphs](https://arxiv.org/abs/2602.01995)
*Jeongmoon Won,Seungwon Kook,Yohan Jo*

Main category: cs.AI

TL;DR: 提出基于知识图谱的对话诊断系统，通过生成诊断假设和验证性提问的两步推理，在模糊症状描述下提高诊断准确性和效率


<details>
  <summary>Details</summary>
Motivation: 现有对话诊断方法要么依赖模型参数知识，要么假设患者提供丰富具体信息，这在现实中不切实际。需要解决信息不完整和症状描述模糊的问题。

Method: 提出两步推理系统：1) 从对话上下文生成诊断假设；2) 通过澄清问题验证假设，循环直到最终诊断。使用MIMIC-IV患者资料和模拟器，特别适应模糊症状描述。

Result: 实验显示在诊断准确性和效率上优于强基线。医生评估支持模拟器的真实性和生成问题的临床实用性。

Conclusion: 提出的基于知识图谱的对话诊断系统能有效处理现实世界中信息不完整和症状描述模糊的情况，具有临床实用价值。

Abstract: Conversational diagnosis requires multi-turn history-taking, where an agent asks clarifying questions to refine differential diagnoses under incomplete information. Existing approaches often rely on the parametric knowledge of a model or assume that patients provide rich and concrete information, which is unrealistic. To address these limitations, we propose a conversational diagnosis system that explores a diagnostic knowledge graph to reason in two steps: (i) generating diagnostic hypotheses from the dialogue context, and (ii) verifying hypotheses through clarifying questions, which are repeated until a final diagnosis is reached. Since evaluating the system requires a realistic patient simulator that responds to the system's questions, we adopt a well-established simulator along with patient profiles from MIMIC-IV. We further adapt it to describe symptoms vaguely to reflect real-world patients during early clinical encounters. Experiments show improved diagnostic accuracy and efficiency over strong baselines, and evaluations by physicians support the realism of our simulator and the clinical utility of the generated questions. Our code will be released upon publication.

</details>


### [141] [Do I Really Know? Learning Factual Self-Verification for Hallucination Reduction](https://arxiv.org/abs/2602.02018)
*Enes Altinisik,Masoomali Fatehkia,Fatih Deniz,Nadir Durrani,Majd Hawasly,Mohammad Raza,Husrev Taha Sencar*

Main category: cs.AI

TL;DR: VeriFY是一个训练时框架，通过一致性自验证教LLM推理事实不确定性，减少事实幻觉，同时保持召回率


<details>
  <summary>Details</summary>
Motivation: 现有缓解LLM事实幻觉的方法主要依赖外部后验验证或将不确定性直接映射到训练时的弃权，导致过于保守的行为

Method: VeriFY通过结构化验证轨迹增强训练，引导模型生成初始答案、验证查询、一致性判断，并决定回答或弃权；采用阶段级损失掩码避免强化幻觉内容

Result: 在多个模型家族和规模上，VeriFY将事实幻觉率降低9.7%至53.3%，召回率仅小幅下降0.4%至5.7%，且单源训练可跨数据集泛化

Conclusion: VeriFY通过训练时的一致性自验证有效减少LLM的事实幻觉，在准确性和保守性之间取得良好平衡，具有实际应用价值

Abstract: Factual hallucination remains a central challenge for large language models (LLMs). Existing mitigation approaches primarily rely on either external post-hoc verification or mapping uncertainty directly to abstention during fine-tuning, often resulting in overly conservative behavior. We propose VeriFY, a training-time framework that teaches LLMs to reason about factual uncertainty through consistency-based self-verification. VeriFY augments training with structured verification traces that guide the model to produce an initial answer, generate and answer a probing verification query, issue a consistency judgment, and then decide whether to answer or abstain. To address the risk of reinforcing hallucinated content when training on augmented traces, we introduce a stage-level loss masking approach that excludes hallucinated answer stages from the training objective while preserving supervision over verification behavior. Across multiple model families and scales, VeriFY reduces factual hallucination rates by 9.7 to 53.3 percent, with only modest reductions in recall (0.4 to 5.7 percent), and generalizes across datasets when trained on a single source. The source code, training data, and trained model checkpoints will be released upon acceptance.

</details>


### [142] [Light Alignment Improves LLM Safety via Model Self-Reflection with a Single Neuron](https://arxiv.org/abs/2602.02027)
*Sicheng Shen,Mingyang Lv,Han Shen,Jialin Wu,Binghao Wang,Zhou Yang,Guobin Shen,Dongcheng Zhao,Feifei Zhao,Yi Zeng*

Main category: cs.AI

TL;DR: 提出NGSD方法，通过训练专家模型和单神经元门控机制实现轻量级安全对齐，在保持模型能力的同时提升安全性


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全对齐方法主要依赖后训练，计算成本高且泛化能力差；轻量级方法要么依赖预计算的安全注入，要么过度依赖模型自身能力，导致泛化有限且生成效率下降

Method: 提出安全感知解码方法：1) 低成本训练专家模型；2) 使用单神经元作为门控机制；3) 有效平衡模型内在能力和外部指导

Result: 方法在训练开销和跨模型规模泛化方面具有明显优势，同时保持实用性和增强输出安全性

Conclusion: 为大型语言模型的安全实用部署提供了轻量级对齐的新视角

Abstract: The safety of large language models (LLMs) has increasingly emerged as a fundamental aspect of their development. Existing safety alignment for LLMs is predominantly achieved through post-training methods, which are computationally expensive and often fail to generalize well across different models. A small number of lightweight alignment approaches either rely heavily on prior-computed safety injections or depend excessively on the model's own capabilities, resulting in limited generalization and degraded efficiency and usability during generation. In this work, we propose a safety-aware decoding method that requires only low-cost training of an expert model and employs a single neuron as a gating mechanism. By effectively balancing the model's intrinsic capabilities with external guidance, our approach simultaneously preserves utility and enhances output safety. It demonstrates clear advantages in training overhead and generalization across model scales, offering a new perspective on lightweight alignment for the safe and practical deployment of large language models. Code: https://github.com/Beijing-AISI/NGSD.

</details>


### [143] [Edit Knowledge, Not Just Facts via Multi-Step Reasoning over Background Stories](https://arxiv.org/abs/2602.02028)
*Ya Gao,Kalle Kujanpää,Pekka Marttinen,Harri Valpola,Alexander Ilin*

Main category: cs.AI

TL;DR: 提出一种基于推理的知识内化训练策略，通过背景故事、多跳问题和知识蒸馏，使AI模型能有效整合新知识进行推理


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑方法主要关注原子事实，虽然改善了事实回忆，但未能将新信息整合成可在不同情境下使用的连贯框架。知识内化本质上是推理问题而非记忆问题。

Method: 提出基于三个原则的训练策略：1) 将新知识作为连贯的背景故事引入，解释新事实与现有知识的关系；2) 使用自生成的多跳问题进行训练，需要涉及新信息的多步推理；3) 采用知识蒸馏，强制学生模型内化教师模型的推理行为，而不直接访问新信息。

Result: 实验表明，采用该策略训练的模型能有效利用新获取的知识进行推理，在需要结合多个新事实的挑战性问题中表现出色。

Conclusion: 知识内化应被视为推理问题，通过将新知识整合到连贯框架中，结合多步推理训练和知识蒸馏，可以使AI模型更灵活地应用新知识。

Abstract: Enabling artificial intelligence systems, particularly large language models, to integrate new knowledge and flexibly apply it during reasoning remains a central challenge. Existing knowledge editing approaches emphasize atomic facts, improving factual recall but often failing to integrate new information into a coherent framework usable across contexts. In this work, we argue that knowledge internalization is fundamentally a reasoning problem rather than a memorization problem. Consequently, a model should be trained in situations where the new information is instrumental to solving a task, combined with pre-existing knowledge, and exercised through multi-step reasoning. Based on this insight, we propose a training strategy based on three principles. First, new knowledge is introduced as a coherent background story that contextualizes novel facts and explains their relation to existing knowledge. Second, models are trained using self-generated multi-hop questions that require multi-step reasoning involving the new information. Third, training is done using knowledge distillation, forcing a student model to internalize the teacher's reasoning behavior without access to the novel information. Experiments show that models trained with this strategy effectively leverage newly acquired knowledge during reasoning and achieve remarkable performance on challenging questions that require combining multiple new facts.

</details>


### [144] [Canonical Intermediate Representation for LLM-based optimization problem formulation and code generation](https://arxiv.org/abs/2602.02029)
*Zhongyuan Lyu,Shuoyu Hu,Lujie Liu,Hongxia Yang,Ming LI*

Main category: cs.AI

TL;DR: 论文提出CIR中间表示和R2C框架，用于从自然语言描述自动生成优化模型，解决复杂操作规则的约束建模问题，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的优化模型自动生成方法在处理复杂操作规则的复合约束和合适建模范式方面存在困难，需要一种更好的方法来解析问题描述并生成优化模型。

Method: 提出Canonical Intermediate Representation (CIR)中间表示模式，通过约束原型和候选建模范式编码操作规则语义；在此基础上开发rule-to-constraint (R2C)多智能体框架，包含问题解析、CIR合成和优化模型实例化等步骤。

Result: R2C在新构建的包含丰富操作规则的基准测试中达到47.2%的准确率；在已有基准测试中表现具有竞争力，接近GPT-5等专有模型性能；通过反思机制进一步提升了性能，在某些基准测试中创造了新的最佳记录。

Conclusion: CIR中间表示和R2C框架有效解决了从自然语言描述自动生成优化模型的挑战，特别是在处理复杂操作规则方面表现出色，为这一领域提供了新的解决方案。

Abstract: Automatically formulating optimization models from natural language descriptions is a growing focus in operations research, yet current LLM-based approaches struggle with the composite constraints and appropriate modeling paradigms required by complex operational rules. To address this, we introduce the Canonical Intermediate Representation (CIR): a schema that LLMs explicitly generate between problem descriptions and optimization models. CIR encodes the semantics of operational rules through constraint archetypes and candidate modeling paradigms, thereby decoupling rule logic from its mathematical instantiation. Upon a newly generated CIR knowledge base, we develop the rule-to-constraint (R2C) framework, a multi-agent pipeline that parses problem texts, synthesizes CIR implementations by retrieving domain knowledge, and instantiates optimization models. To systematically evaluate rule-to-constraint reasoning, we test R2C on our newly constructed benchmark featuring rich operational rules, and benchmarks from prior work. Extensive experiments show that R2C achieves state-of-the-art accuracy on the proposed benchmark (47.2% Accuracy Rate). On established benchmarks from the literature, R2C delivers highly competitive results, approaching the performance of proprietary models (e.g., GPT-5). Moreover, with a reflection mechanism, R2C achieves further gains and sets new best-reported results on some benchmarks.

</details>


### [145] [Constrained Process Maps for Multi-Agent Generative AI Workflows](https://arxiv.org/abs/2602.02034)
*Ananya Joshi,Michael Rudow*

Main category: cs.AI

TL;DR: 提出基于有限时域MDP的多智能体系统，用于量化不确定性并协调监管工作流，相比单智能体基线提升19%准确率，减少85%人工审核需求


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能体在合规等监管场景中执行复杂工作流时，主要依赖单一智能体的提示工程，难以观察和比较模型如何处理不确定性以及跨决策阶段的协调问题

Method: 引入基于有向无环结构的有限时域马尔可夫决策过程(MDP)形式化多智能体系统，每个智能体对应特定角色或决策阶段，使用蒙特卡洛估计量化智能体层面的认知不确定性，系统级不确定性通过MDP终止于自动标记状态或人工审核状态来捕获

Result: 在AI安全评估（自伤检测）案例研究中，相比单智能体基线：准确率提升最高达19%，人工审核需求减少最高达85倍，某些配置下处理时间也减少

Conclusion: 提出的多智能体MDP框架能有效量化不确定性并协调监管工作流，显著提升性能并减少人工干预需求，为复杂监管环境中的LLM智能体部署提供了系统化方法

Abstract: Large language model (LLM)-based agents are increasingly used to perform complex, multi-step workflows in regulated settings such as compliance and due diligence. However, many agentic architectures rely primarily on prompt engineering of a single agent, making it difficult to observe or compare how models handle uncertainty and coordination across interconnected decision stages and with human oversight. We introduce a multi-agent system formalized as a finite-horizon Markov Decision Process (MDP) with a directed acyclic structure. Each agent corresponds to a specific role or decision stage (e.g., content, business, or legal review in a compliance workflow), with predefined transitions representing task escalation or completion. Epistemic uncertainty is quantified at the agent level using Monte Carlo estimation, while system-level uncertainty is captured by the MDP's termination in either an automated labeled state or a human-review state. We illustrate the approach through a case study in AI safety evaluation for self-harm detection, implemented as a multi-agent compliance system. Results demonstrate improvements over a single-agent baseline, including up to a 19\% increase in accuracy, up to an 85x reduction in required human review, and, in some configurations, reduced processing time.

</details>


### [146] [Hunt Instead of Wait: Evaluating Deep Data Research on Large Language Models](https://arxiv.org/abs/2602.02039)
*Wei Liu,Peijie Yu,Michele Orini,Yali Du,Yulan He*

Main category: cs.AI

TL;DR: 论文提出"调查性智能"概念，区别于执行性智能，强调LLM自主设定目标和探索的能力。引入Deep Data Research任务和DDR-Bench基准，发现前沿模型已展现初步自主性，但长程探索仍是挑战。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估主要关注执行性智能（完成指定任务），但现实世界的数据分析需要自主探索和发现。数据科学领域缺乏评估LLM自主探索能力的基准，需要新的评估框架来测量"调查性智能"。

Method: 提出Deep Data Research任务，让LLM从原始数据库中自主提取关键见解。创建DDR-Bench基准，采用大规模、基于检查表的评估方法，确保结果可验证。通过该基准评估LLM在开放探索任务中的表现。

Result: 前沿模型已展现出初步的自主性能力，但长程探索（long-horizon exploration）仍然是主要挑战。研究发现，有效的调查性智能不仅依赖于外部框架或单纯模型规模扩展，更需要智能体模型的内在策略能力。

Conclusion: 调查性智能是LLM向真正智能体发展的关键能力。DDR-Bench为评估这一能力提供了有效工具，揭示了当前模型的局限性和未来发展方向，强调需要关注智能体模型的内在策略能力而不仅仅是外部框架或规模扩展。

Abstract: The agency expected of Agentic Large Language Models goes beyond answering correctly, requiring autonomy to set goals and decide what to explore. We term this investigatory intelligence, distinguishing it from executional intelligence, which merely completes assigned tasks. Data Science provides a natural testbed, as real-world analysis starts from raw data rather than explicit queries, yet few benchmarks focus on it. To address this, we introduce Deep Data Research (DDR), an open-ended task where LLMs autonomously extract key insights from databases, and DDR-Bench, a large-scale, checklist-based benchmark that enables verifiable evaluation. Results show that while frontier models display emerging agency, long-horizon exploration remains challenging. Our analysis highlights that effective investigatory intelligence depends not only on agent scaffolding or merely scaling, but also on intrinsic strategies of agentic models.

</details>


### [147] [Rethinking the Role of Entropy in Optimizing Tool-Use Behaviors for Large Language Model Agents](https://arxiv.org/abs/2602.02050)
*Zeping Li,Hongru Wang,Yiwen Zhao,Guanhua Chen,Yixia Li,Keyang Chen,Yixin Cao,Guangnan Ye,Hongfeng Chai,Mengdi Wang,Zhenfei Yin*

Main category: cs.AI

TL;DR: 基于LLM的工具使用代理在长轨迹中常有过量低质量工具调用问题，本文提出使用熵减作为监督信号，设计稀疏结果奖励和密集过程奖励来优化工具使用行为。


<details>
  <summary>Details</summary>
Motivation: LLM工具使用代理在数学推理和多跳问答等任务中表现出色，但在长轨迹中经常触发过多低质量工具调用，增加了延迟并降低了推理性能，这使得管理工具使用行为变得困难。

Method: 通过基于熵的试点实验发现熵减与高质量工具调用存在强正相关，以此为基础提出使用熵减作为监督信号，设计两种奖励策略：稀疏结果奖励提供轨迹级粗粒度指导以提高效率，密集过程奖励提供细粒度监督以提升性能。

Result: 实验表明两种奖励设计都能改善工具使用行为：稀疏结果奖励相比基线平均减少72.07%的工具调用，密集过程奖励提升22.27%的性能。

Conclusion: 熵减成为增强工具使用行为的关键机制，使代理在现实应用中更具适应性，为解决LLM代理工具使用优化问题提供了有效方法。

Abstract: Tool-using agents based on Large Language Models (LLMs) excel in tasks such as mathematical reasoning and multi-hop question answering. However, in long trajectories, agents often trigger excessive and low-quality tool calls, increasing latency and degrading inference performance, making managing tool-use behavior challenging. In this work, we conduct entropy-based pilot experiments and observe a strong positive correlation between entropy reduction and high-quality tool calls. Building on this finding, we propose using entropy reduction as a supervisory signal and design two reward strategies to address the differing needs of optimizing tool-use behavior. Sparse outcome rewards provide coarse, trajectory-level guidance to improve efficiency, while dense process rewards offer fine-grained supervision to enhance performance. Experiments across diverse domains show that both reward designs improve tool-use behavior: the former reduces tool calls by 72.07% compared to the average of baselines, while the latter improves performance by 22.27%. These results position entropy reduction as a key mechanism for enhancing tool-use behavior, enabling agents to be more adaptive in real-world applications.

</details>


### [148] [SIDiffAgent: Self-Improving Diffusion Agent](https://arxiv.org/abs/2602.02051)
*Shivank Garg,Ayush Singh,Gaurav Kumar Nayak*

Main category: cs.AI

TL;DR: SIDiffAgent是一个无需训练的代理框架，利用Qwen系列模型解决文本到图像扩散模型的局限性，通过自主提示工程、检测纠正生成错误和细粒度伪影去除，实现更可靠的图像生成。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型在实际部署中存在多个限制：对提示措辞敏感、语义解释模糊（如"mouse"可能指动物或计算机外设）、解剖结构扭曲等伪影，以及需要精心设计的输入提示。现有方法通常需要额外训练且可控性有限，限制了在实际应用中的适应性。

Method: 提出Self-Improving Diffusion Agent (SIDiffAgent)，这是一个无需训练的代理框架，利用Qwen系列模型（Qwen-VL, Qwen-Image, Qwen-Edit, Qwen-Embedding）自主管理提示工程、检测纠正不良生成、执行细粒度伪影去除。框架还包含迭代自我改进机制，将先前经验存储在数据库中，用于在代理流程的每个阶段注入基于提示的指导。

Result: SIDiffAgent在GenAIBench上实现了平均VQA得分0.884，显著优于开源模型、专有模型和其他代理方法。

Conclusion: SIDiffAgent通过代理框架有效解决了文本到图像扩散模型的实际部署挑战，无需额外训练即可实现自主提示工程、错误检测纠正和伪影去除，并通过经验记忆实现自我改进，显著提升了生成质量和可靠性。

Abstract: Text-to-image diffusion models have revolutionized generative AI, enabling high-quality and photorealistic image synthesis. However, their practical deployment remains hindered by several limitations: sensitivity to prompt phrasing, ambiguity in semantic interpretation (e.g., ``mouse" as animal vs. a computer peripheral), artifacts such as distorted anatomy, and the need for carefully engineered input prompts. Existing methods often require additional training and offer limited controllability, restricting their adaptability in real-world applications. We introduce Self-Improving Diffusion Agent (SIDiffAgent), a training-free agentic framework that leverages the Qwen family of models (Qwen-VL, Qwen-Image, Qwen-Edit, Qwen-Embedding) to address these challenges. SIDiffAgent autonomously manages prompt engineering, detects and corrects poor generations, and performs fine-grained artifact removal, yielding more reliable and consistent outputs. It further incorporates iterative self-improvement by storing a memory of previous experiences in a database. This database of past experiences is then used to inject prompt-based guidance at each stage of the agentic pipeline. \modelour achieved an average VQA score of 0.884 on GenAIBench, significantly outperforming open-source, proprietary models and agentic methods. We will publicly release our code upon acceptance.

</details>


### [149] [Understanding the Reversal Curse Mitigation in Masked Diffusion Models through Attention and Training Dynamics](https://arxiv.org/abs/2602.02133)
*Sangwoo Shin,BumJun Kim,Kyelim Lee,Moongyu Jeon,Albert No*

Main category: cs.AI

TL;DR: 扩散语言模型通过权重共享和梯度对齐机制，部分缓解了自回归语言模型中的"逆转诅咒"问题


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型存在"逆转诅咒"问题：学习"A是B"后，无法处理反向查询"B是A"。掩码扩散语言模型虽然也有此问题但程度较轻，其根本原因尚不清楚。本文旨在探究掩码扩散语言模型为何能部分缓解这一失败模式。

Method: 通过理论分析和实验验证：1）在单层Transformer编码器中，权重共享使前向和反向注意力分数正相关；2）在相同设置下，相应梯度是对齐的，最小化前向损失也会减少反向损失；3）在受控玩具任务和大规模扩散语言模型上进行实验验证

Result: 掩码扩散语言模型对逆转诅咒的缓解源于架构结构及其与训练的交互作用，而非仅由任意顺序训练目标解释。权重共享机制耦合了两个方向，使前向和反向注意力分数正相关，且梯度对齐确保优化前向任务时也优化了反向任务。

Conclusion: 扩散语言模型通过权重共享和梯度对齐的架构特性，部分克服了自回归语言模型中持续存在的逆转诅咒问题，这解释了为什么掩码扩散语言模型在此失败模式上表现优于强自回归模型。

Abstract: Autoregressive language models (ARMs) suffer from the reversal curse: after learning that "$A$ is $B$", they often fail on the reverse query "$B$ is $A$". Masked diffusion-based language models (MDMs) exhibit this failure in a much weaker form, but the underlying reason has remained unclear. A common explanation attributes this mitigation to the any-order training objective. However, observing "[MASK] is $B$" during training does not necessarily teach the model to handle the reverse prompt "$B$ is [MASK]". We show that the mitigation arises from architectural structure and its interaction with training. In a one-layer Transformer encoder, weight sharing couples the two directions by making forward and reverse attention scores positively correlated. In the same setting, we further show that the corresponding gradients are aligned, so minimizing the forward loss also reduces the reverse loss. Experiments on both controlled toy tasks and large-scale diffusion language models support these mechanisms, explaining why MDMs partially overcome a failure mode that persists in strong ARMs.

</details>


### [150] [Mitigating Safety Tax via Distribution-Grounded Refinement in Large Reasoning Models](https://arxiv.org/abs/2602.02136)
*Yingsha Xie,Tiansheng Huang,Enneng Yang,Rui Min,Wenjie Lu,Xiaochun Cao,Naiqiang Tan,Li Shen*

Main category: cs.AI

TL;DR: 论文提出DGR方法，通过将安全对齐数据集适配到目标大语言模型的内部分布，减少安全对齐对模型推理能力的负面影响。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐数据集通常从外部大模型或人工标注中蒸馏安全推理轨迹，但这些数据与需要对齐的目标模型存在分布差异，这种分布差异被认为是导致目标模型推理能力显著下降的主要原因。

Method: 提出DGR方法，将现有的分布外安全推理数据集进行转换和精炼，使其与目标大语言模型的内部分布对齐。

Result: 实验显示：1) DGR有效减轻安全税，同时保持安全性能，相比Vanilla SFT平均推理准确率提升30.2%（DirectRefusal）和21.2%（R1-ACT）；2) 推理能力下降程度与分布偏移程度相关；3) 仅需10个样本即可激活有效的拒绝行为，表明安全对齐可能主要作为激活潜在知识的机制。

Conclusion: 研究强调了分布一致性的重要性，并为推理模型中安全机制的激活提供了新见解。DGR方法能有效缓解安全对齐对模型推理能力的负面影响。

Abstract: Safety alignment incurs safety tax that perturbs a large reasoning model's (LRM) general reasoning ability. Existing datasets used for safety alignment for an LRM are usually constructed by distilling safety reasoning traces and answers from an external LRM or human labeler. However, such reasoning traces and answers exhibit a distributional gap with the target LRM that needs alignment, and we conjecture such distributional gap is the culprit leading to significant degradation of reasoning ability of the target LRM. Driven by this hypothesis, we propose a safety alignment dataset construction method, dubbed DGR. DGR transforms and refines an existing out-of-distributional safety reasoning dataset to be aligned with the target's LLM inner distribution. Experimental results demonstrate that i) DGR effectively mitigates the safety tax while maintaining safety performance across all baselines, i.e., achieving \textbf{+30.2\%} on DirectRefusal and \textbf{+21.2\%} on R1-ACT improvement in average reasoning accuracy compared to Vanilla SFT; ii) the degree of reasoning degradation correlates with the extent of distribution shift, suggesting that bridging this gap is central to preserving capabilities. Furthermore, we find that safety alignment in LRMs may primarily function as a mechanism to activate latent knowledge, as a mere \textbf{10} samples are sufficient for activating effective refusal behaviors. These findings not only emphasize the importance of distributional consistency but also provide insights into the activation mechanism of safety in reasoning models.

</details>


### [151] [Traffic-Aware Navigation in Road Networks](https://arxiv.org/abs/2602.02158)
*Sarah Nassar*

Main category: cs.AI

TL;DR: 比较三种图搜索算法在金斯顿道路网络交通感知导航中的表现：Floyd-Warshall-Ingerman（单次多查询预处理）、Dijkstra/A*（连续单查询实时搜索）、Yen's算法（结合两者，先找K条最短路径再实时迭代）。


<details>
  <summary>Details</summary>
Motivation: 研究不同图搜索算法在真实城市道路网络交通感知导航任务中的性能表现，为特定部署场景选择最佳算法提供依据。

Method: 在金斯顿道路网络上对比三种算法：1）Floyd-Warshall-Ingerman：单次多查询预处理算法；2）Dijkstra和A*：连续单查询实时搜索算法；3）Yen's算法：结合两种方法，先找到前K条最短路径，然后在实时中迭代。

Result: Dijkstra和A*算法能提供最交通感知的最优解且预处理需求最小；Floyd-Warshall-Ingerman实时速度最快但仅提供基于距离的路径而无交通感知；Yen's算法需要大量预处理但在运行速度和最优性之间取得平衡。

Conclusion: 每种算法都有其优缺点，需要根据具体部署环境的情况权衡选择最佳定制解决方案。Dijkstra/A*适合需要交通感知且预处理资源有限的场景，Floyd-Warshall适合纯距离优化且要求实时速度的场景，Yen's算法适合需要平衡两者的场景。

Abstract: This project compares three graph search approaches for the task of traffic-aware navigation in Kingston's road network. These approaches include a single-run multi-query preprocessing algorithm (Floyd-Warshall-Ingerman), continuous single-query real-time search (Dijkstra's and A*), and an algorithm combining both approaches to balance between their trade-offs by first finding the top K shortest paths then iterating over them in real time (Yen's). Dijkstra's and A* resulted in the most traffic-aware optimal solutions with minimal preprocessing required. Floyd-Warshall-Ingerman was the fastest in real time but provided distance based paths with no traffic awareness. Yen's algorithm required significant preprocessing but balanced between the other two approaches in terms of runtime speed and optimality. Each approach presents advantages and disadvantages that need to be weighed depending on the circumstances of specific deployment contexts to select the best custom solution. *This project was completed as part of ELEC 844 (Search and Planning Algorithms for Robotics) in the Fall 2025 term.

</details>


### [152] [Reasoning in a Combinatorial and Constrained World: Benchmarking LLMs on Natural-Language Combinatorial Optimization](https://arxiv.org/abs/2602.02188)
*Xia Jiang,Jing Chen,Cong Zhang,Jie Gao,Chengpeng Hu,Chenhao Zhang,Yaoxin Wu,Yingqian Zhang*

Main category: cs.AI

TL;DR: NLCO是一个自然语言组合优化基准，评估LLMs在端到端组合优化推理上的能力，涵盖43个问题，使用四层分类法组织，实验显示LLMs在小实例上表现良好但随着规模增大而退化。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在数学和逻辑推理上表现出色，但它们在组合优化（在高维解空间中搜索满足硬约束的解）方面的能力尚未充分探索。需要建立一个基准来评估LLMs在端到端组合优化推理上的能力。

Method: 引入NLCO基准，包含43个组合优化问题，使用四层分类法组织（变量类型、约束族、全局模式、目标类）。提供求解器标注的解决方案，从可行性、解的最优性和推理效率三个维度全面评估LLMs。

Result: 高性能LLMs在小实例上表现出良好的可行性和解质量，但随着实例规模增大，两者都显著退化，即使使用更多token进行推理。在分类法上观察到系统性差异：基于集合的任务相对容易，而图结构问题和瓶颈目标导致更多失败。

Conclusion: LLMs在组合优化方面仍有局限，特别是在处理大规模实例和复杂结构问题时。需要进一步研究提升LLMs在组合优化推理上的能力，特别是在可扩展性和处理复杂约束方面。

Abstract: While large language models (LLMs) have shown strong performance in math and logic reasoning, their ability to handle combinatorial optimization (CO) -- searching high-dimensional solution spaces under hard constraints -- remains underexplored. To bridge the gap, we introduce NLCO, a \textbf{N}atural \textbf{L}anguage \textbf{C}ombinatorial \textbf{O}ptimization benchmark that evaluates LLMs on end-to-end CO reasoning: given a language-described decision-making scenario, the model must output a discrete solution without writing code or calling external solvers. NLCO covers 43 CO problems and is organized using a four-layer taxonomy of variable types, constraint families, global patterns, and objective classes, enabling fine-grained evaluation. We provide solver-annotated solutions and comprehensively evaluate LLMs by feasibility, solution optimality, and reasoning efficiency. Experiments across a wide range of modern LLMs show that high-performing models achieve strong feasibility and solution quality on small instances, but both degrade as instance size grows, even if more tokens are used for reasoning. We also observe systematic effects across the taxonomy: set-based tasks are relatively easy, whereas graph-structured problems and bottleneck objectives lead to more frequent failures.

</details>


### [153] [TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents](https://arxiv.org/abs/2602.02196)
*Hang Yan,Xinyu Che,Fangzhi Xu,Qiushi Sun,Zichen Ding,Kanzhi Cheng,Jian Zhang,Tao Qin,Jun Liu,Qika Lin*

Main category: cs.AI

TL;DR: 提出TIDE框架，用于诊断LLM智能体在测试时改进（TTI）中的表现，通过三个维度评估任务完成效率、循环行为和内存负担。


<details>
  <summary>Details</summary>
Motivation: 现有研究对自主LLM智能体通过与环境迭代交互实现性能提升（TTI）的成功或失败机制理解不足，且现有评估指标无法捕捉任务优化效率、错误行为适应和工作内存效用。

Method: 提出测试时改进诊断评估（TIDE）框架，这是一个与智能体和环境无关的框架，将TTI分解为三个相互关联的维度：任务完成的时间动态、递归循环行为的约束、累积内存负担的约束。

Result: 通过在不同智能体和环境中的广泛实验，TIDE显示提升智能体性能不仅需要扩展内部推理，还需要显式优化智能体与环境之间的交互动态。

Conclusion: TIDE框架为理解TTI机制提供了系统化的诊断工具，揭示了优化智能体性能需要关注交互动态而不仅仅是内部推理能力。

Abstract: Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment.

</details>


### [154] [More Than a Quick Glance: Overcoming the Greedy Bias in KV-Cache Compression](https://arxiv.org/abs/2602.02199)
*Aryan Sood,Tanvi Sharma,Vansh Agrawal*

Main category: cs.AI

TL;DR: LASER-KV 是一种新的KV缓存压缩框架，通过层累积选择和精确LSH召回机制，在严格累积预算策略下实现高效压缩，相比现有方法在长上下文任务上性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型理论上支持长上下文窗口，但实际部署受到KV缓存内存线性增长的限制。现有压缩方法通过剪枝机制在语义召回和内存效率之间做出权衡，导致性能下降。

Method: 提出LASER-KV框架，采用块状累积策略和保护除数(n)控制，实现层累积选择与精确LSH召回。该方法隔离了压缩效果与滑动窗口伪影的影响。

Result: 在Babilong基准测试中，现有压缩方法性能下降15-30%，而LASER-KV保持稳定性能，在128k上下文长度下准确率提升高达10%。

Conclusion: 研究结果表明，仅依赖注意力分数作为token效用的代理是不够的，LASER-KV通过创新的压缩策略挑战了这一普遍假设，为长上下文处理提供了更有效的解决方案。

Abstract: While Large Language Models (LLMs) can theoretically support extensive context windows, their actual deployment is constrained by the linear growth of Key-Value (KV) cache memory. Prevailing compression strategies mitigate this through various pruning mechanisms, yet trade-off semantic recall for memory efficiency. In this work, we present LASER-KV (Layer Accumulated Selection with Exact-LSH Recall), a framework designed to test the limits of KV compression under a strict accumulative budgeting policy. We deviate from the standard fixed summary size approach by implementing a block-wise accumulation strategy governed by a protection divisor (n). This allows us to isolate the effects of compression from sliding window artifacts. Our experiments on the Babilong benchmark reveal performance degradation in previous compression methods by 15-30% on various long context tasks. LASER-KV maintains stable performance, achieving superior accuracies by a margin of upto 10% at 128k. These findings challenge the prevailing assumption that attention scores alone are a sufficient proxy for token utility.

</details>


### [155] [Position: Explaining Behavioral Shifts in Large Language Models Requires a Comparative Approach](https://arxiv.org/abs/2602.02304)
*Martino Ciaperoni,Marzio Di Vece,Luca Pappalardo,Fosca Giannotti,Francesco Giannini*

Main category: cs.AI

TL;DR: 本文提出了比较性可解释AI（Δ-XAI）框架，用于解释大规模基础模型在不同干预（如缩放、微调、强化学习等）后出现的行为变化，强调应该比较参考模型和干预模型之间的差异，而非孤立分析单个模型。


<details>
  <summary>Details</summary>
Motivation: 大规模基础模型在缩放、微调、强化学习或上下文学习后会出现"行为偏移"，即干预引起的行为变化。虽然研究这些现象受到关注，但解释其出现原因仍被忽视。传统XAI方法只能揭示单个模型检查点的失败，无法解释不同检查点之间的内部变化以及哪些解释性主张是合理的。

Method: 提出了比较性可解释AI（Δ-XAI）框架，包含一套设计适当解释方法时应考虑的要求。引入了一系列可能的流程管道，将其与要求相关联，并提供了具体的Δ-XAI实验。

Result: 建立了一个系统性的比较解释框架，能够分析模型在干预前后的变化，识别哪些内部变化导致了行为偏移，并验证关于这些变化的解释性主张。

Conclusion: 行为偏移应该通过比较方式来解释：核心目标应该是参考模型和干预模型之间的干预引起的变化，而不是孤立地分析任何单个模型。Δ-XAI框架为此提供了理论基础和方法指导。

Abstract: Large-scale foundation models exhibit behavioral shifts: intervention-induced behavioral changes that appear after scaling, fine-tuning, reinforcement learning or in-context learning. While investigating these phenomena have recently received attention, explaining their appearance is still overlooked. Classic explainable AI (XAI) methods can surface failures at a single checkpoint of a model, but they are structurally ill-suited to justify what changed internally across different checkpoints and which explanatory claims are warranted about that change. We take the position that behavioral shifts should be explained comparatively: the core target should be the intervention-induced shift between a reference model and an intervened model, rather than any single model in isolation. To this aim we formulate a Comparative XAI ($Δ$-XAI) framework with a set of desiderata to be taken into account when designing proper explaining methods. To highlight how $Δ$-XAI methods work, we introduce a set of possible pipelines, relate them to the desiderata, and provide a concrete $Δ$-XAI experiment.

</details>


### [156] [Interpreting and Controlling LLM Reasoning through Integrated Policy Gradient](https://arxiv.org/abs/2602.02313)
*Changming Li,Kaixing Zhang,Haoyun Xu,Yingdong Shi,Zheng Zhang,Kaitao Song,Kan Ren*

Main category: cs.AI

TL;DR: IPG框架通过传播基于结果的信号来定位LLM推理机制，实现更精确的组件定位和可靠的行为调控


<details>
  <summary>Details</summary>
Motivation: 现有解释方法难以精确定位复杂推理机制或捕捉从模型内部工作到推理输出的顺序影响，需要新的方法来识别对推理行为有顺序贡献的组件

Method: 提出集成策略梯度（IPG）框架，通过将基于结果的信号（如推理后准确性）沿模型推理轨迹反向传播，将推理行为归因于模型内部组件

Result: 实证评估表明，该方法实现了更精确的定位，并能可靠地调节不同推理模型的推理行为（如推理能力、推理强度）

Conclusion: IPG框架基于结果导向和顺序影响感知原则，能有效识别对推理行为有顺序贡献的组件，为理解LLM推理机制提供了新方法

Abstract: Large language models (LLMs) demonstrate strong reasoning abilities in solving complex real-world problems. Yet, the internal mechanisms driving these complex reasoning behaviors remain opaque. Existing interpretability approaches targeting reasoning either identify components (e.g., neurons) correlated with special textual patterns, or rely on human-annotated contrastive pairs to derive control vectors. Consequently, current methods struggle to precisely localize complex reasoning mechanisms or capture sequential influence from model internal workings to the reasoning outputs. In this paper, built on outcome-oriented and sequential-influence-aware principles, we focus on identifying components that have sequential contribution to reasoning behavior where outcomes are cumulated by long-range effects. We propose Integrated Policy Gradient (IPG), a novel framework that attributes reasoning behaviors to model's inner components by propagating compound outcome-based signals such as post reasoning accuracy backward through model inference trajectories. Empirical evaluations demonstrate that our approach achieves more precise localization and enables reliable modulation of reasoning behaviors (e.g., reasoning capability, reasoning strength) across diverse reasoning models.

</details>


### [157] [Context Learning for Multi-Agent Discussion](https://arxiv.org/abs/2602.02350)
*Xingyuan Hua,Sheng Yue,Xinyi Li,Yizhe Zhao,Jinrui Zhang,Ju Ren*

Main category: cs.AI

TL;DR: M2CL提出了一种多LLM上下文学习方法，通过训练上下文生成器动态为每个代理生成上下文指令，解决多智能体讨论中的不一致性问题，显著提升性能20%-50%。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体讨论方法容易遭受讨论不一致性问题，LLM实例之间由于个体上下文不对齐而无法达成一致解决方案。

Method: M2CL为每个代理训练一个上下文生成器，通过自动信息组织和精炼动态生成每轮讨论的上下文指令，采用精心设计的自适应机制控制上下文一致性和输出差异。

Result: 在学术推理、具身任务和移动控制等挑战性任务上，M2CL性能显著超越现有方法20%-50%，同时具有良好的可迁移性和计算效率。

Conclusion: M2CL通过上下文生成器有效解决了多智能体讨论中的不一致性问题，使LLM能够避免过早收敛于多数噪声并逐步达成正确共识。

Abstract: Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual contexts.In this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive mechanism.It enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency.

</details>


### [158] [Live-Evo: Online Evolution of Agentic Memory from Continuous Feedback](https://arxiv.org/abs/2602.02369)
*Yaolun Zhang,Yiran Wu,Yijiong Yu,Qingyun Wu,Huazheng Wang*

Main category: cs.AI

TL;DR: Live-Evo是一个在线自演化记忆系统，通过经验银行和元指导银行分离"发生了什么"和"如何使用"，实现从持续数据流中学习，通过反馈权重管理记忆，在真实分布漂移下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理的记忆系统大多针对静态训练/测试分割设计，通过折叠静态基准来近似在线学习，在真实分布漂移和持续反馈下表现脆弱。需要真正的在线自演化记忆系统来处理持续数据流。

Method: Live-Evo通过经验银行存储原始经验，元指导银行存储如何使用经验的指导，将两者解耦。系统维护经验权重，根据反馈更新：有帮助的经验被强化和更频繁检索，误导或过时的经验被降权并逐渐遗忘，模拟人类记忆的强化和衰减机制。

Result: 在10周时间跨度的Prophet Arena基准测试中，Live-Evo将Brier分数提高了20.8%，市场回报增加了12.9%。在深度研究基准测试中也表现出持续优于强基线的稳定增益。

Conclusion: Live-Evo展示了在线自演化记忆系统在处理真实分布漂移和持续反馈方面的有效性，通过解耦经验存储与使用、基于反馈的权重管理机制，实现了比静态系统更鲁棒的持续学习能力。

Abstract: Large language model (LLM) agents are increasingly equipped with memory, which are stored experience and reusable guidance that can improve task-solving performance. Recent \emph{self-evolving} systems update memory based on interaction outcomes, but most existing evolution pipelines are developed for static train/test splits and only approximate online learning by folding static benchmarks, making them brittle under true distribution shift and continuous feedback. We introduce \textsc{Live-Evo}, an online self-evolving memory system that learns from a stream of incoming data over time. \textsc{Live-Evo} decouples \emph{what happened} from \emph{how to use it} via an Experience Bank and a Meta-Guideline Bank, compiling task-adaptive guidelines from retrieved experiences for each task. To manage memory online, \textsc{Live-Evo} maintains experience weights and updates them from feedback: experiences that consistently help are reinforced and retrieved more often, while misleading or stale experiences are down-weighted and gradually forgotten, analogous to reinforcement and decay in human memory. On the live \textit{Prophet Arena} benchmark over a 10-week horizon, \textsc{Live-Evo} improves Brier score by 20.8\% and increases market returns by 12.9\%, while also transferring to deep-research benchmarks with consistent gains over strong baselines. Our code is available at https://github.com/ag2ai/Live-Evo.

</details>


### [159] [Trust by Design: Skill Profiles for Transparent, Cost-Aware LLM Routing](https://arxiv.org/abs/2602.02386)
*Mika Okamoto,Ansel Kaplan Erol,Glenn Matlin*

Main category: cs.AI

TL;DR: BELLA是一个预算高效的LLM选择框架，通过技能分析自动推荐最优模型，在保证性能的同时控制成本。


<details>
  <summary>Details</summary>
Motivation: 当前标准基准测试报告的是聚合指标，掩盖了任务所需的具体能力，无法判断更便宜的模型是否足够。LLM从业者需要在不浪费资金的情况下为任务选择合适的模型。

Method: BELLA采用三阶段方法：1) 通过基于批评的分析分解LLM输出并提取细粒度技能；2) 将技能聚类为结构化能力矩阵；3) 使用多目标优化选择模型，在预算约束下最大化性能。

Result: BELLA提供自然语言推理的推荐理由，提供当前黑盒路由系统缺乏的透明度。该框架使从业者能够为部署LLM做出原则性的成本-性能权衡。

Conclusion: BELLA框架通过可解释的基于技能的分析，为LLM选择提供了预算高效的解决方案，特别适用于金融推理等具有多样化技能需求和模型成本变化的领域。

Abstract: How should Large Language Model (LLM) practitioners select the right model for a task without wasting money? We introduce BELLA (Budget-Efficient LLM Selection via Automated skill-profiling), a framework that recommends optimal LLM selection for tasks through interpretable skill-based model selection. Standard benchmarks report aggregate metrics that obscure which specific capabilities a task requires and whether a cheaper model could suffice. BELLA addresses this gap through three stages: (1) decomposing LLM outputs and extract the granular skills required by using critic-based profiling, (2) clustering skills into structured capability matrices, and (3) multi-objective optimization to select the right models to maximize performance while respecting budget constraints. BELLA provides natural-language rationale for recommendations, providing transparency that current black-box routing systems lack. We describe the framework architecture, situate it within the landscape of LLM routing and evaluation, and discuss its application to financial reasoning as a representative domain exhibiting diverse skill requirements and cost-variation across models. Our framework enables practitioners to make principled and cost-performance trade-offs for deploying LLMs.

</details>


### [160] [Structure Enables Effective Self-Localization of Errors in LLMs](https://arxiv.org/abs/2602.02416)
*Ankur Samanta,Akshayaa Magesh,Ayush Jain,Kavosh Asadi,Youliang Yu,Daniel Jiang,Boris Vidolov,Kaveh Hassani,Paul Sajda,Jalaj Bhandari,Yonathan Efroni*

Main category: cs.AI

TL;DR: 提出Thought-ICS框架，通过离散化思维步骤实现语言模型自我错误定位与校正，相比传统方法显著提升自我校正能力


<details>
  <summary>Details</summary>
Motivation: 探索语言模型能否显式定位错误推理，构建能有效自我校正的AI系统。受人类大脑在离散决策点监控错误并重采样替代方案的启发

Method: 引入Thought-ICS框架：将推理结构化为离散、语义连贯的思维步骤，每次生成一个完整思维单元，创建自然边界进行精确错误定位。验证错误后回溯到最后一个正确点生成替代推理

Result: 在由oracle验证的错误推理中，Thought-ICS实现20-40%的自我校正提升；在完全自主无外部验证的设置中，优于当代自我校正基线方法

Conclusion: 通过结构化推理为离散思维步骤，语言模型能够可靠定位错误，Thought-ICS框架为构建有效自我校正的AI系统提供了可行路径

Abstract: Self-correction in language models remains elusive. In this work, we explore whether language models can explicitly localize errors in incorrect reasoning, as a path toward building AI systems that can effectively correct themselves. We introduce a prompting method that structures reasoning as discrete, semantically coherent thought steps, and show that models are able to reliably localize errors within this structure, while failing to do so in conventional, unstructured chain-of-thought reasoning. Motivated by how the human brain monitors errors at discrete decision points and resamples alternatives, we introduce Iterative Correction Sampling of Thoughts (Thought-ICS), a self-correction framework. Thought-ICS iteratively prompts the model to generate reasoning one discrete and complete thought at a time--where each thought represents a deliberate decision by the model--creating natural boundaries for precise error localization. Upon verification, the model localizes the first erroneous step, and the system backtracks to generate alternative reasoning from the last correct point. When asked to correct reasoning verified as incorrect by an oracle, Thought-ICS achieves 20-40% self-correction lift. In a completely autonomous setting without external verification, it outperforms contemporary self-correction baselines.

</details>


### [161] [SafeGround: Know When to Trust GUI Grounding Models via Uncertainty Calibration](https://arxiv.org/abs/2602.02419)
*Qingni Wang,Yue Fan,Xin Eric Wang*

Main category: cs.AI

TL;DR: SafeGround是一个用于GUI基础模型的不确定性感知框架，通过分布感知的不确定性量化和校准过程，实现风险感知预测和统计保证的假发现率控制。


<details>
  <summary>Details</summary>
Motivation: GUI基础（将自然语言指令转换为可执行的屏幕坐标）中的错误预测可能导致代价高昂且难以逆转的操作（如错误的支付批准），因此需要提高模型的可靠性。

Method: SafeGround采用分布感知的不确定性量化方法捕捉模型输出的空间分散性，然后通过校准过程获得具有统计保证假发现率控制的测试时决策阈值。

Result: 在ScreenSpot-Pro基准测试中，SafeGround的不确定性度量在区分正确与错误预测方面优于现有基线，校准阈值实现了严格的风险控制，系统级准确率相比Gemini-only推理最高提升5.38个百分点。

Conclusion: SafeGround为GUI基础模型提供了一个有效的风险控制框架，通过不确定性感知和统计校准显著提高了模型可靠性和系统性能。

Abstract: Graphical User Interface (GUI) grounding aims to translate natural language instructions into executable screen coordinates, enabling automated GUI interaction. Nevertheless, incorrect grounding can result in costly, hard-to-reverse actions (e.g., erroneous payment approvals), raising concerns about model reliability. In this paper, we introduce SafeGround, an uncertainty-aware framework for GUI grounding models that enables risk-aware predictions through calibrations before testing. SafeGround leverages a distribution-aware uncertainty quantification method to capture the spatial dispersion of stochastic samples from outputs of any given model. Then, through the calibration process, SafeGround derives a test-time decision threshold with statistically guaranteed false discovery rate (FDR) control. We apply SafeGround on multiple GUI grounding models for the challenging ScreenSpot-Pro benchmark. Experimental results show that our uncertainty measure consistently outperforms existing baselines in distinguishing correct from incorrect predictions, while the calibrated threshold reliably enables rigorous risk control and potentials of substantial system-level accuracy improvements. Across multiple GUI grounding models, SafeGround improves system-level accuracy by up to 5.38\% percentage points over Gemini-only inference.

</details>


### [162] [Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling](https://arxiv.org/abs/2602.02453)
*Andong Chen,Wenxin Zhu,Qiuyu Ding,Yuchen Song,Muyun Yang,Tiejun Zhao*

Main category: cs.AI

TL;DR: 提出"Thinking with Comics"视觉推理范式，将漫画作为介于图像和视频之间的高信息密度媒介，在保持时间结构和叙事连贯性的同时降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 当前多模态推理存在局限性：静态图像难以表达时间结构，而视频则存在大量冗余和计算成本高的问题。需要一种既能保留时间结构又更高效的视觉表示方法。

Method: 提出使用漫画作为中间视觉表示，系统研究基于漫画的两种推理路径，并在多种推理任务和长上下文理解任务上进行评估。

Result: 实验结果显示，Thinking with Comics在多步时间和因果推理任务上优于Thinking with Images，同时比Thinking with Video更高效。不同漫画叙事结构和风格对任务性能有持续影响。

Conclusion: 漫画作为一种有效的中间视觉表示，能够改善多模态推理，在信息密度、时间结构保留和计算效率之间取得良好平衡。

Abstract: Chain-of-Thought reasoning has driven large language models to extend from thinking with text to thinking with images and videos. However, different modalities still have clear limitations: static images struggle to represent temporal structure, while videos introduce substantial redundancy and computational cost. In this work, we propose Thinking with Comics, a visual reasoning paradigm that uses comics as a high information-density medium positioned between images and videos. Comics preserve temporal structure, embedded text, and narrative coherence while requiring significantly lower reasoning cost. We systematically study two reasoning paths based on comics and evaluate them on a range of reasoning tasks and long-context understanding tasks. Experimental results show that Thinking with Comics outperforms Thinking with Images on multi-step temporal and causal reasoning tasks, while remaining substantially more efficient than Thinking with Video. Further analysis indicates that different comic narrative structures and styles consistently affect performance across tasks, suggesting that comics serve as an effective intermediate visual representation for improving multimodal reasoning.

</details>


### [163] [Drift-Bench: Diagnosing Cooperative Breakdowns in LLM Agents under Input Faults via Multi-Turn Interaction](https://arxiv.org/abs/2602.02455)
*Han Bao,Zheyuan Zhang,Pengcheng Jing,Zhengqing Yuan,Kaiwen Shi,Yanfang Ye*

Main category: cs.AI

TL;DR: Drift-Bench：首个诊断基准，用于评估自主智能体在输入故障下的多轮澄清能力，通过状态导向和服务导向执行环境测试智能体语用学


<details>
  <summary>Details</summary>
Motivation: 大型语言模型向自主智能体过渡时，用户输入经常违反合作假设（如隐含意图、缺失参数、错误预设或模糊表达），产生文本评估无法捕捉的执行风险。现有基准通常假设指令明确或仅限于文本单轮澄清，无法衡量基于执行风险的多轮消歧能力。

Method: 提出Drift-Bench基准，基于经典沟通理论建立合作故障的统一分类法，采用角色驱动的用户模拟器和Rise评估协议，在状态导向和服务导向执行环境中测试多轮澄清能力。

Result: 实验显示在这些故障下智能体性能显著下降，澄清效果因用户角色和故障类型而异，揭示了现有智能体在输入故障下的脆弱性。

Conclusion: Drift-Bench连接了澄清研究和智能体安全评估，能够系统诊断可能导致不安全执行的故障，为自主智能体的安全部署提供重要评估工具。

Abstract: As Large Language Models transition to autonomous agents, user inputs frequently violate cooperative assumptions (e.g., implicit intent, missing parameters, false presuppositions, or ambiguous expressions), creating execution risks that text-only evaluations do not capture. Existing benchmarks typically assume well-specified instructions or restrict evaluation to text-only, single-turn clarification, and thus do not measure multi-turn disambiguation under grounded execution risk. We introduce \textbf{Drift-Bench}, the first diagnostic benchmark that evaluates agentic pragmatics under input faults through multi-turn clarification across state-oriented and service-oriented execution environments. Grounded in classical theories of communication, \textbf{Drift-Bench} provides a unified taxonomy of cooperative breakdowns and employs a persona-driven user simulator with the \textbf{Rise} evaluation protocol. Experiments show substantial performance drops under these faults, with clarification effectiveness varying across user personas and fault types. \MethodName bridges clarification research and agent safety evaluation, enabling systematic diagnosis of failures that can lead to unsafe executions.

</details>


### [164] [MentisOculi: Revealing the Limits of Reasoning with Mental Imagery](https://arxiv.org/abs/2602.02465)
*Jana Zeller,Thaddäus Wiedemer,Fanfei Li,Thomas Klein,Prasanna Mayilvahanan,Matthias Bethge,Felix Wichmann,Ryan Cotterell,Wieland Brendel*

Main category: cs.AI

TL;DR: 视觉思维（可视化推理）目前对前沿统一多模态模型（UMMs）的性能提升有限，存在生成错误累积和无法有效利用可视化的问题。


<details>
  <summary>Details</summary>
Motivation: 随着前沿模型从仅能处理视觉信息的MLLMs发展到能够原生交错生成的UMMs，研究者希望探索使用中间可视化作为推理辅助工具（类似人类心理意象）的可能性，以评估模型形成、维护和操作视觉表征的能力。

Method: 开发了MentisOculi评估套件，这是一个程序化、分层化的多步推理问题集合，专门设计来挑战前沿模型。评估了从潜在token到显式生成图像等多种视觉策略。

Result: 视觉策略通常无法提升模型性能。UMMs虽然具备解决任务的文本推理能力，有时也能生成正确的视觉内容，但存在生成错误累积的问题，且即使提供真实可视化也无法有效利用。

Conclusion: 尽管视觉思维具有内在吸引力，但目前尚未能有效提升模型推理能力。MentisOculi为分析和弥合这一差距提供了必要的基础。

Abstract: Frontier models are transitioning from multimodal large language models (MLLMs) that merely ingest visual information to unified multimodal models (UMMs) capable of native interleaved generation. This shift has sparked interest in using intermediate visualizations as a reasoning aid, akin to human mental imagery. Central to this idea is the ability to form, maintain, and manipulate visual representations in a goal-oriented manner. To evaluate and probe this capability, we develop MentisOculi, a procedural, stratified suite of multi-step reasoning problems amenable to visual solution, tuned to challenge frontier models. Evaluating visual strategies ranging from latent tokens to explicit generated imagery, we find they generally fail to improve performance. Analysis of UMMs specifically exposes a critical limitation: While they possess the textual reasoning capacity to solve a task and can sometimes generate correct visuals, they suffer from compounding generation errors and fail to leverage even ground-truth visualizations. Our findings suggest that despite their inherent appeal, visual thoughts do not yet benefit model reasoning. MentisOculi establishes the necessary foundation to analyze and close this gap across diverse model families.

</details>


### [165] [Avenir-Web: Human-Experience-Imitating Multimodal Web Agents with Mixture of Grounding Experts](https://arxiv.org/abs/2602.02468)
*Aiden Yiliu Li,Xinyue Hao,Shilong Liu,Mengdi Wang*

Main category: cs.AI

TL;DR: Avenir-Web：通过混合定位专家、经验模仿规划和任务跟踪检查表，在真实网页交互中实现最先进性能的网页代理


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型有所进展，但自主网页代理在执行复杂动态网页界面的长时程任务时仍不可靠，存在元素定位不准确、缺乏站点特定程序知识、长时任务跟踪和记忆不稳定等问题

Method: 1. 混合定位专家（Mixture of Grounding Experts）提高元素定位准确性；2. 经验模仿规划（Experience-Imitation Planning）整合程序先验知识；3. 任务跟踪检查表结合自适应记忆实现跨界面范式鲁棒交互

Result: 在Online-Mind2Web基准测试中显著超越先前开源代理，达到与顶级专有模型相当的性能，建立了开源网页代理在真实网站上的新SOTA

Conclusion: Avenir-Web通过创新的定位、规划和记忆机制，解决了网页代理在复杂动态界面中的核心挑战，为可靠网页交互建立了新的开源标准

Abstract: Despite advances in multimodal large language models, autonomous web agents still struggle to reliably execute long-horizon tasks on complex and dynamic web interfaces. Existing agents often suffer from inaccurate element grounding, the absence of site-specific procedural knowledge, and unstable long-term task tracking and memory, particularly when operating over complex Document Object Model structures. To address these limitations, we introduce Avenir-Web, a web agent that achieves a new open-source state of the art on the Online-Mind2Web benchmark in real-world deployment. Avenir-Web leverages a Mixture of Grounding Experts, Experience-Imitation Planning for incorporating procedural priors, and a task-tracking checklist combined with adaptive memory to enable robust and seamless interaction across diverse user interface paradigms. We evaluate Avenir-Web on Online-Mind2Web, a rigorous benchmark of live and user-centered web tasks. Our results demonstrate that Avenir-Web significantly surpasses prior open-source agents and attains performance parity with top-tier proprietary models, thereby establishing a new open-source state of the art for reliable web agents on live websites.

</details>


### [166] [Breaking the Reversal Curse in Autoregressive Language Models via Identity Bridge](https://arxiv.org/abs/2602.02470)
*Xutao Ma,Yixiao Huang,Hanlin Zhu,Somayeh Sojoudi*

Main category: cs.AI

TL;DR: 通过引入"身份桥"数据正则化（如"A→A"形式），可以显著缓解LLMs的"逆转诅咒"问题，使模型能够学习更高层次的规则而非单纯记忆事实。


<details>
  <summary>Details</summary>
Motivation: 尽管自回归大语言模型在许多复杂任务上表现出色，但在简单的逻辑推理如"逆转诅咒"上仍然失败。传统观点认为这是自回归因果LLMs的固有根本限制，但本文挑战这一观点，试图证明通过适当的数据调整可以缓解这一问题。

Method: 提出一种简单的正则化数据配方"身份桥"，形式为"A→A"（例如"Alice的名字是Alice"）。理论上分析梯度下降的隐式偏差，证明即使单层Transformer也能打破逆转诅咒。实证上在1B预训练语言模型上微调，使用该数据配方。

Result: 使用身份桥数据配方微调的模型在逆转任务上达到40%的成功率，而仅使用前向知识数据训练的模型成功率接近零。这显著缓解了逆转诅咒问题。

Conclusion: 本文为逆转诅咒提供了新的理论基础，并提供了低成本、有原则的路径来鼓励LLMs从数据中学习更高层次的规则，挑战了关于自回归LLMs固有局限的传统观点。

Abstract: Autoregressive large language models (LLMs) have achieved remarkable success in many complex tasks, yet they can still fail in very simple logical reasoning such as the "reversal curse" -- when trained on forward knowledge data of the form "$A \rightarrow B$" (e.g., Alice's husband is Bob), the model is unable to deduce the reversal knowledge "$B \leftarrow A$" (e.g., Bob's wife is Alice) during test. Extensive prior research suggests that this failure is an inherent, fundamental limit of autoregressive causal LLMs, indicating that these models tend to memorize factual-level knowledge rather than capture higher-level rules. In this paper, we challenge this view by showing that this seemingly fundamental limit can be mitigated by slightly tweaking the training data with a simple regularization data recipe called the Identity Bridge of the form "$A \to A$" (e.g., The name of Alice is Alice). Theoretically, we prove that under this recipe, even a one-layer transformer can break the reversal curse by analyzing the implicit bias of gradient descent. Empirically, we show that a 1B pretrained language model finetuned with the proposed data recipe achieves a 40% success rate on reversal tasks, in stark contrast to a near-zero success rate when trained solely on forward-knowledge data. Our work provides a novel theoretical foundation for the reversal curse and offers a principled, low-cost path to encouraging LLMs to learn higher-level rules from data.

</details>


### [167] [AgentRx: Diagnosing AI Agent Failures from Execution Trajectories](https://arxiv.org/abs/2602.02475)
*Shraddha Barke,Arnav Goyal,Alind Khare,Avaljot Singh,Suman Nath,Chetan Bansal*

Main category: cs.AI

TL;DR: 提出了AGENTRX框架来自动诊断AI代理失败轨迹中的关键失败步骤，并发布了包含115个失败轨迹的基准数据集


<details>
  <summary>Details</summary>
Motivation: AI代理失败难以定位，因为执行具有概率性、长时程、多代理和噪声工具输出的特点，需要自动化诊断工具

Method: AGENTRX框架：合成约束条件，逐步评估，生成可审计的验证日志，使用LLM法官定位关键失败步骤和类别

Result: 在三个领域（结构化API工作流、事件管理、开放式网络/文件任务）中，AGENTRX在步骤定位和失败归因方面优于现有基线方法

Conclusion: AGENTRX提供了一种自动化、领域无关的诊断框架，能够有效定位AI代理失败的关键步骤，并发布了首个跨领域失败轨迹基准数据集

Abstract: AI agents often fail in ways that are difficult to localize because executions are probabilistic, long-horizon, multi-agent, and mediated by noisy tool outputs. We address this gap by manually annotating failed agent runs and release a novel benchmark of 115 failed trajectories spanning structured API workflows, incident management, and open-ended web/file tasks. Each trajectory is annotated with a critical failure step and a category from a grounded-theory derived, cross-domain failure taxonomy. To mitigate the human cost of failure attribution, we present AGENTRX, an automated domain-agnostic diagnostic framework that pinpoints the critical failure step in a failed agent trajectory. It synthesizes constraints, evaluates them step-by-step, and produces an auditable validation log of constraint violations with associated evidence; an LLM-based judge uses this log to localize the critical step and category. Our framework improves step localization and failure attribution over existing baselines across three domains.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [168] [Coping with Inductive Risk When Theories are Underdetermined: Decision Making with Partial Identification](https://arxiv.org/abs/2602.00355)
*Charles F. Manski*

Main category: econ.EM

TL;DR: 本文探讨了科学理论的不确定性（underdetermination）在公共政策决策中的重要性，特别是通过部分识别（partial identification）的计量经济学分析来研究这一问题，认为科学不确定性对可信的公共决策具有重大影响。


<details>
  <summary>Details</summary>
Motivation: 科学理论的不确定性（underdetermination）和归纳风险（inductive risk）在哲学和科学实践中持续存在争议。当科学研究用于政策决策时，科学不确定性会产生归纳风险，这一问题具有实际重要性。本文旨在加强哲学家与政策分析研究者之间的沟通。

Method: 采用部分识别（partial identification）的计量经济学分析方法。该方法结合可用数据和可信假设来预测总体结果，提供数学工具来刻画一类广泛的科学不确定性。同时结合模糊性下合理决策的标准。

Result: 研究发现，部分识别分析显示不确定性和归纳风险对重要社会结果的可信预测具有高度影响，进而影响可信的公共决策。该方法提供了在不接受多个经验上不确定理论中任何一个的情况下，做出政策选择的连贯实践方法。

Conclusion: 部分识别研究值得在关于不确定性和归纳风险的哲学讨论中获得关注。它为处理科学不确定性提供了数学工具和决策框架，有助于在不确定条件下做出合理的政策选择。

Abstract: Controversy about the significance of underdetermination of theories persists in the philosophy and conduct of science. The issue has practical import when scientific research is used to inform decision making, because scientific uncertainty yields inductive risk. Seeking to enhance communication between philosophers and researchers who analyze public policy, this paper describes econometric analysis of partial identification. Study of partial identification finds underdetermination and inductive risk to be highly consequential for credible prediction of important societal outcomes and, hence, for credible public decision making. It provides mathematical tools to characterize a broad class of scientific uncertainties that arise when available data and credible assumptions are combined to predict population outcomes. Combining study of partial identification with criteria for reasonable decision making under ambiguity yields coherent practical approaches to make policy choices without accepting one among multiple empirically underdetermined theories. The paper argues that study of partial identification warrants attention in philosophical discourse on underdetermination and inductive risk.

</details>


### [169] [Identification and Estimation in Fuzzy Regression Discontinuity Designs with Covariates](https://arxiv.org/abs/2602.01417)
*Carolina Caetano,Gregorio Caetano,Juan Carlos Escanciano*

Main category: econ.EM

TL;DR: 该论文研究带有协变量的模糊断点回归设计，识别条件局部平均处理效应的加权平均值，提出基于平方第一阶段断点的依从性加权LATE估计方法，在依从性变化时提高估计稳定性。


<details>
  <summary>Details</summary>
Motivation: 模糊断点回归设计中，当依从性（第一阶段断点）随协变量变化时，传统估计方法可能不稳定且效率较低。需要开发更稳健的估计方法来处理协变量依赖的依从性变化。

Method: 提出依从性加权LATE（CWLATE）方法，通过平方第一阶段断点对协变量单元进行加权，最大化第一阶段强度。针对离散协变量提供简单估计器和稳健偏差校正推断方法。

Result: 模拟研究表明，当依从性变化时，CWLATE相比标准模糊RDD估计器提高了稳定性并降低了均方误差。在乌拉圭孕期现金转移应用中，获得了关于低出生体重的精确RDD效应估计。

Conclusion: CWLATE方法为协变量依赖依从性的模糊断点回归设计提供了更稳定、更有效的估计框架，特别适用于依从性在协变量间变化较大的实际应用场景。

Abstract: We study fuzzy regression discontinuity designs with covariates and characterize the weighted averages of conditional local average treatment effects (WLATEs) that are point identified. Any identified WLATE equals a Wald ratio of conditional reduced-form and first-stage discontinuities. We highlight the Compliance-Weighted LATE (CWLATE), which weights cells by squared first-stage discontinuities and maximizes first-stage strength. For discrete covariates, we provide simple estimators and robust bias-corrected inference. In simulations calibrated to common designs, CWLATE improves stability and reduces mean squared error relative to standard fuzzy RDD estimators when compliance varies. An application to Uruguayan cash transfers during pregnancy yields precise RDD-based effects on low birthweight.

</details>


### [170] [Do designated market makers provide liquidity during downward extreme price movements?](https://arxiv.org/abs/2602.01817)
*Mario Bellia,Kim Christensen,Aleksey Kolokolov,Loriana Pelizzon,Roberto Renò*

Main category: econ.EM

TL;DR: 研究电子市场中指定做市商在极端价格下跌时的交易行为，发现当抛售压力集中在单只股票时做市商提供流动性，但当多只股票受影响时则消耗流动性


<details>
  <summary>Details</summary>
Motivation: 研究指定做市商在电子市场中的角色冲突：他们可能遵守做市协议在抛售压力下提供即时性，也可能利用私有信息"顺风交易"获利

Method: 使用包含交易者分类审计追踪信息的独特数据集，采用新颖方法检测极端（下跌）价格运动，测试做市商在不同情境下的行为

Result: 当抛售压力集中在单只股票时，做市商提供流动性；但当多只股票同时受影响时，做市商反而消耗流动性，将流动性提供留给更慢的交易者

Conclusion: 做市商的行为取决于市场压力范围：单只股票压力下履行做市义务，但系统性压力下则可能利用信息优势获利，这对市场稳定性和监管有重要启示

Abstract: We study the trading activity of designated market makers (DMMs) in electronic markets using a unique dataset with audit-trail information on trader classification. DMMs may either adhere to their market-making agreements and offer immediacy during periods of heavy selling pressure, or they might lean-with-the-wind to profit from private information. We test these competing theories during extreme (downward) price movements, which we detect using a novel methodology. We show that DMMs provide liquidity when the selling pressure is concentrated on a single stock, but consume liquidity (leaving liquidity provision to slower traders) when several stocks are affected.

</details>


### [171] [Forecasting Oil Consumption: The Statistical Review of World Energy Meets Machine Learning](https://arxiv.org/abs/2602.01963)
*Jan Ditzen,Erkal Ersoy,Haoyang Li,Francesco Ravazzolo*

Main category: econ.EM

TL;DR: 该研究通过识别主导国家来改进区域石油需求预测，发现美国是全球主导驱动因素，法国和日本分别是欧洲和亚洲的区域枢纽，这些主导因素显著提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索少数主导国家是否能解释区域石油需求的大部分动态变化，并提高预测性能。通过识别OECD和GVAR样本中的主导驱动因素，旨在建立更有效的预测模型。

Method: 采用高维浓度矩阵估计方法，使用LASSO和OCMT两种互补的变量选择技术逐行估计。通过计算浓度矩阵列向量的范数并排序，应用基于连续范数比率的准则，结合经济限制条件来识别真正的主导国家。

Result: 美国被识别为全球主导驱动因素，法国和日本分别作为欧洲和亚洲的稳健区域枢纽。将这些主导驱动因素作为所有国家的回归变量，相比自回归基准模型和国家特定LASSO模型，在统计上显著提高了预测精度，特别是在全球波动加剧时期。

Conclusion: 该框架证明少数主导国家确实能解释区域石油需求动态并改善预测性能。该方法是灵活的，可应用于其他具有网络结构或空间依赖性的宏观经济和能源变量。

Abstract: This paper studies whether a small set of dominant countries can account for most of the dynamics of regional oil demand and improve forecasting performance. We focus on dominant drivers within the OECD and a broad GVAR sample covering over 90\% of world GDP. Our approach identifies dominant drivers from a high-dimensional concentration matrix estimated row by row using two complementary variable-selection methods, LASSO and the one-covariate-at-a-time multiple testing (OCMT) procedure. Dominant countries are selected by ordering the columns of the concentration matrix by their norms and applying a criterion based on consecutive norm ratios, combined with economically motivated restrictions to rule out pseudo-dominance. The United States emerges as a global dominant driver, while France and Japan act as robust regional hubs representing European and Asian components, respectively. Including these dominant drivers as regressors for all countries yields statistically significant forecast gains over autoregressive benchmarks and country-specific LASSO models, particularly during periods of heightened global volatility. The proposed framework is flexible and can be applied to other macroeconomic and energy variables with network structure or spatial dependence.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [172] [Beyond Static Question Banks: Dynamic Knowledge Expansion via LLM-Automated Graph Construction and Adaptive Generation](https://arxiv.org/abs/2602.00020)
*Yingquan Wang,Tianyu Wei,Qinsi Li,Li Zeng*

Main category: cs.CY

TL;DR: 提出Generative GraphRAG框架，通过自动构建层次知识图谱和认知图推理，实现个性化习题生成，解决传统教育系统中知识图谱构建成本高和个性化推理不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有教育系统面临两大挑战：1) 知识图谱构建依赖人工，成本高、扩展性差；2) 缺乏对学习者知识状态的有效推理支持，依赖静态题库，适应性有限。

Method: 提出Generative GraphRAG框架，包含两个核心模块：1) Auto-HKG：利用LLM自动从教育资源构建层次知识图谱；2) CG-RAG：基于学习者掌握图进行图推理，结合检索增强生成技术，生成个性化习题。

Result: 框架已在真实教育场景中部署，获得积极用户反馈，显示出支持实际个性化教育系统的潜力。

Conclusion: Generative GraphRAG框架通过自动化知识建模和个性化习题生成，有效解决了传统教育系统的局限性，为实际个性化教育系统提供了可行方案。

Abstract: Personalized education systems increasingly rely on structured knowledge representations to support adaptive learning and question generation. However, existing approaches face two fundamental limitations. First, constructing and maintaining knowledge graphs for educational content largely depends on manual curation, resulting in high cost and poor scalability. Second, most personalized education systems lack effective support for state-aware and systematic reasoning over learners' knowledge, and therefore rely on static question banks with limited adaptability. To address these challenges, this paper proposes a Generative GraphRAG framework for automated knowledge modeling and personalized exercise generation. It consists of two core modules. The first module, Automated Hierarchical Knowledge Graph Constructor (Auto-HKG), leverages LLMs to automatically construct hierarchical knowledge graphs that capture structured concepts and their semantic relations from educational resources. The second module, Cognitive GraphRAG (CG-RAG), performs graph-based reasoning over a learner mastery graph and combines it with retrieval-augmented generation to produce personalized exercises that adapt to individual learning states. The proposed framework has been deployed in real-world educational scenarios, where it receives favorable user feedback, suggesting its potential to support practical personalized education systems.

</details>


### [173] [Early Warning Signals Appear Long Before Dropping Out: An Idiographic Approach Grounded in Complex Dynamic Systems Theory](https://arxiv.org/abs/2602.00021)
*Mohammed Saqr,Sonsoles López-Pernas,Santtu Tikka,Markus Wolfgang Hermann Spitzer*

Main category: cs.CY

TL;DR: 该研究首次在教育领域应用临界减速理论，通过分析160万次数学练习数据，发现88.2%学生在脱离学习前会出现临界减速信号，可作为早期预警指标。


<details>
  <summary>Details</summary>
Motivation: 学生韧性减弱会导致脱离学习和辍学风险，需要在"希望窗口期"提前预测脱离行为。传统方法难以在临界点前检测脆弱性，需要寻找普遍适用的早期预警信号。

Method: 使用临界减速理论框架，分析9,401名学生在数字数学学习环境中的167万次练习尝试。计算六个CSD指标：自相关、返回率、方差、偏度、峰度和变异系数，检测脱离学习前的预警信号。

Result: 88.2%的学生在脱离学习前表现出CSD信号，预警信号集中在活动后期和停止练习前。这是教育领域首次发现CSD证据，表明普遍韧性动态也适用于人类学习等社会系统。

Conclusion: CSD指标可作为实用的早期脆弱性检测工具，在不同应用和情境中支持学习者。最重要的是，这些指标具有普遍性，不依赖于数据生成机制，为跨情境、跨数据类型和学习环境的可移植性提供了新机会。

Abstract: The ability to sustain engagement and recover from setbacks (i.e., resilience) -- is fundamental for learning. When resilience weakens, students are at risk of disengagement and may drop out and miss on opportunities. Therefore, predicting disengagement long before it happens during the window of hope is important. In this article, we test whether early warning signals of resilience loss, grounded in the concept of critical slowing down (CSD) can forecast disengagement before dropping out. CSD has been widely observed across ecological, climate, and neural systems, where it precedes tipping points into catastrophic failure (dropping out in our case). Using 1.67 million practice attempts from 9,401 students who used a digital math learning environment, we computed CSD indicators: autocorrelation, return rate, variance, skewness, kurtosis, and coefficient of variation. We found that 88.2% of students exhibited CSD signals prior to disengagement, with warnings clustering late in activity and before practice ceased (dropping out). Our results provide the first evidence of CSD in education, suggesting that universal resilience dynamics also govern social systems such as human learning. These findings offer a practical indicator for early detection of vulnerability and supporting learners across different applications and contexts long before critical events happen. Most importantly, CSD indicators arise universally, independent of the mechanisms that generate the data, offering new opportunities for portability across contexts, data types, and learning environments.

</details>


### [174] [Motivation, Attention, and Visual Platform Design: How Moral Contagions Spread on TikTok and Instagram in the 2024 United States Presidential Election](https://arxiv.org/abs/2602.02479)
*Ni Annie Yuan,Ho-chun Herbert Chang*

Main category: cs.CY

TL;DR: 研究通过分析2024年美国总统选举期间TikTok和Instagram的300多万条内容，发现政治议题的道德化程度并非固有属性，而是由平台架构、受众特征和党派框架共同塑造的。TikTok算法促进了堕胎和移民等议题的道德化传播，而Instagram则放大了经济议题的讨论。加密货币等传统"务实"议题也出现了道德化现象。


<details>
  <summary>Details</summary>
Motivation: 视觉社交媒体已成为政治话语的主要场所，但我们对道德化在不同平台和议题上的运作方式知之甚少。研究旨在探究平台架构、受众特征和内容策略如何相互作用，决定哪些议题被道德化以及道德化内容如何传播。

Method: 研究分析了2024年美国总统选举期间的2,027,595条TikTok视频和1,126,972条Instagram帖子。采用时间供需分析和道德基础评分(eMFD)来检验关键选举议题的动态。使用语义网络分析揭示不同平台的话语结构特征。

Result: 1. 平台间道德化模式差异显著：TikTok算法促进了堕胎和移民内容的道德化传播，尽管供应量较低；Instagram则放大了经济议题的讨论，供需更匹配。2. 传统"务实"议题出现道德化：加密货币讨论比任何其他议题都更强烈地引用了忠诚和权威基础，将监管框架为政府越权。3. 平台对不同事件的响应不同：TikTok在哈里斯提名后所有议题都出现激增，而Instagram则在加密货币政策发展时出现峰值。语义网络分析显示TikTok的环形拓扑结构促进了跨议题暴露，而Instagram的碎片化结构将哈里斯与经济话语隔离开来。

Conclusion: 理解政治道德化需要考察平台特定的生态系统，其中架构、人口统计学和内容策略相互作用，共同决定哪些议题被道德化以及道德化内容如何传播。研究强调了平台设计在塑造政治话语中的关键作用。

Abstract: Visual social media platforms have become primary venues for political discourse, yet we know little about how moralization operates differently across platforms and topics. Analyzing 2,027,595 TikToks and 1,126,972 Instagram posts during the 2024 US presidential election, we demonstrate that issues are not necessarily inherently moralized, but a product of audience demographics, platform architecture, and partisan framing. Using temporal supply-demand analysis and moral foundations scoring (eMFD), we examine the dynamics of key electoral issues. Three key findings emerge. First, moralization patterns diverge dramatically by platform: TikTok's algorithm enabled viral spread of moralized abortion and immigration content despite lower supply, while Instagram amplified economic discourse that aligned supply and demand. Second, traditionally "pragmatic" economic issues became moralized-cryptocurrency discourse invoked loyalty and authority foundations more strongly than any other topic, framing regulation as government overreach. Third, platforms responded to different events: TikTok surged after Harris's nomination across all topics (96% reduction in supply volatility), while Instagram spiked around cryptocurrency policy developments. Semantic network analysis reveals TikTok's circular topology enables cross-cutting exposure while Instagram's fragmented structure isolates Harris from economic discourse. These findings demonstrate that understanding political moralization requires examining platform-specific ecosystems where architecture, demographics, and content strategy interact to determine which issues get moralized and how moral content spreads.

</details>


### [175] [Groundwater vulnerability assessment in semi-arid regions using GIS-based DRASTIC models and FUZZY AHP: South Chott Hodna](https://arxiv.org/abs/2602.00023)
*Lakhdar Seraiche,Mostafa Dougha,Messaoud Ghodbane,Tahar Selmane,Ahmed Ferhati,Djamal Eddine Djemiat*

Main category: cs.CY

TL;DR: 该研究提出了一种改进的DRASTIC地下水脆弱性评估框架，通过整合土地利用数据和AHP/模糊AHP权重技术，提高了干旱地区地下水风险评估的准确性。


<details>
  <summary>Details</summary>
Motivation: 干旱地区地下水脆弱性日益严重，人口增长和农业集约化加剧了地下水枯竭和污染风险。传统DRASTIC模型存在主观性强、未能充分考虑人为因素等局限性，需要更准确、适应性强的评估方法。

Method: 提出混合评估框架：1) 整合土地利用数据；2) 应用层次分析法(AHP)及其模糊逻辑变体(Fuzzy AHP)进行权重分配；3) 使用GIS生成四种脆弱性图：DRASTIC、DRASTIC_LU、AHP DRASTIC_LU和Fuzzy AHP DRASTIC_LU；4) 用70口井的硝酸盐数据进行验证。

Result: 农业区域（特别是冲积含水层上方）最为脆弱。ROC曲线分析显示模型性能逐步提升：AUC值分别为0.812(DRASTIC)、0.864(DRASTIC_LU)、0.875(AHP DRASTIC_LU)、0.951(Fuzzy AHP DRASTIC_LU)。模糊AHP DRASTIC_LU模型表现最佳。

Conclusion: 模糊AHP DRASTIC_LU显著提高了地下水风险评估的准确性。基于GIS的混合模型提供了可扩展、可转移的脆弱性制图方法，为地方和区域水资源管理者提供了实用信息，特别适用于数据有限和情况复杂的干旱地区。

Abstract: Groundwater vulnerability is a major concern in arid regions worldwide, where population growth and intensive agriculture increase the risks of depletion and contamination. This study proposes a hybrid groundwater vulnerability assessment framework that improves the conventional DRASTIC model by integrating land-use data and applying advanced weighting techniques, namely the Analytical Hierarchy Process (AHP) and its fuzzy logic variant (Fuzzy AHP). This method makes expert-based weighting less subjective, better captures anthropogenic effects, and facilitates adaptation to challenging situations and limited data. Four vulnerability maps were produced using Geographic Information Systems (GIS): DRASTIC, DRASTIC_LU, AHP DRASTIC_LU, and Fuzzy AHP DRASTIC_LU. We used nitrate levels from 70 wells to verify our work. We found that agricultural areas, especially those above the alluvial aquifer, were the most vulnerable. The ROC curve analysis showed that the model improved over time, with the area under the curve (AUC) values of 0.812 for DRASTIC, 0.864 for DRASTIC_LU, 0.875 for AHP DRASTIC_LU, and 0.951 for fuzzy AHP DRASTIC_LU. These results show that fuzzy AHP DRASTIC_LU makes groundwater risk assessments much more. The GIS-based hybrid models offer a scalable and transferable method for mapping vulnerability, but they also provide local and regional water resource managers with useful information.

</details>


### [176] [Strategies for Creating Uncertainty in the AI Era to Trigger Students Critical Thinking: Pedagogical Design, Assessment Rubric, and Exam System](https://arxiv.org/abs/2602.00026)
*Ahmad Samer Wazan*

Main category: cs.CY

TL;DR: 该论文提出通过AI制造不确定性来促进批判性思维的教学方法，开发了MindMosaicAIExam考试系统，要求学生在AI输出基础上进行推理和论证。


<details>
  <summary>Details</summary>
Motivation: 生成式AI让学生能直接获得正确答案而无需展示理解过程，传统评估方式受到挑战。与其禁止AI，不如将其整合到教育中，利用AI制造不确定性来激发学生的批判性思维。

Method: 提出"思维导向教学法"，利用AI模型和教师的固有局限性设计学习活动和评估。开发MindMosaicAIExam考试系统，通过控制AI行为（如阻止直接答案或生成有缺陷的响应）来制造不确定性，要求学生提供初步答案、批判性评估AI输出并迭代完善推理。

Result: 开发了MindMosaicAIExam考试系统，并设计了相应的评估标准来评估学生的批判性思维能力，基于系统收集的学生推理过程记录。

Conclusion: 通过有意识地利用AI制造不确定性，可以将其转化为促进批判性思维的教育工具，而不是让学生走捷径获取确定答案的途径。这种教学方法能鼓励学生进行推理、质疑和论证。

Abstract: Generative AI challenges traditional assessments by allowing students to produce correct answers without demonstrating understanding or reasoning. Rather than prohibiting AI, this work argues that one way to integrate AI into education is by creating uncertain situations with the help of AI models and using thinking-oriented teaching approaches, where uncertainty is a central pedagogical concept for stimulating students critical thinking. Drawing on epistemology and critical thinking research studies, we propose designing learning activities and assessments around the inherent limitations of both AI models and instructors. This encourages students to reason, question, and justify their final answers. We show how explicitly controlling AI behavior during exams (such as preventing direct answers or generating plausible but flawed responses) prevents AI from becoming a shortcut to certainty. To support this pedagogy, we introduce MindMosaicAIExam, an exam system that integrates controllable AI tools and requires students to provide initial answers, critically evaluate AI outputs, and iteratively refine their reasoning. We also present an evaluation rubric designed to assess critical thinking based on students reasoning artifacts collected by the exam system.

</details>


### [177] [Happy Young Women, Grumpy Old Men? Emotion-Driven Demographic Biases in Synthetic Face Generation](https://arxiv.org/abs/2602.00032)
*Mengting Wei,Aditya Gulati,Guoying Zhao,Nuria Oliver*

Main category: cs.CY

TL;DR: 该研究系统审计了8个先进的文本到图像模型（4个西方开发，4个中国开发）在生成人脸时的偏见，发现所有模型都存在人口统计和情感条件偏见，无论其来源国如何。


<details>
  <summary>Details</summary>
Motivation: 尽管文本到图像模型能生成高质量人脸，但对其偏见、表征质量和跨文化一致性了解不足。现有研究主要关注人口统计偏见，但缺乏对情感提示如何影响人口表征以及不同文化背景模型输出差异的研究。

Method: 对8个最先进的T2I模型（4个西方组织开发，4个中国机构开发）进行系统审计，使用相同提示生成人脸。采用先进的面部分析算法估计生成人脸的性别、种族、年龄和吸引力水平，应用信息论偏见度量（KL散度和Jensen-Shannon散度）测量与全球人口统计数据的偏差。

Result: 所有模型无论来源国如何，都存在持续的人口统计和情感条件偏见。西方和中国开发的模型都显示出系统性偏见，表明这是跨文化普遍存在的问题。

Conclusion: 研究揭示了生成系统存在的普遍偏见问题，讨论了公平性、社会技术危害、治理和透明生成系统开发的影响，强调需要更公平和透明的AI系统。

Abstract: Synthetic face generation has rapidly advanced with the emergence of text-to-image (T2I) and of multimodal large language models, enabling high-fidelity image production from natural-language prompts. Despite the widespread adoption of these tools, the biases, representational quality, and cross-cultural consistency of these models remain poorly understood. Prior research on biases in the synthetic generation of human faces has examined demographic biases, yet there is little research on how emotional prompts influence demographic representation and how models trained in different cultural and linguistic contexts vary in their output distributions. We present a systematic audit of eight state-of-the-art T2I models comprising four models developed by Western organizations and four developed by Chinese institutions, all prompted identically. Using state-of-the-art facial analysis algorithms, we estimate the gender, race, age, and attractiveness levels in the generated faces. To measure the deviations from global population statistics, we apply information-theoretic bias metrics including Kullback-Leibler and Jensen-Shannon divergences. Our findings reveal persistent demographic and emotion-conditioned biases in all models regardless of their country of origin. We discuss implications for fairness, socio-technical harms, governance, and the development of transparent generative systems.

</details>


### [178] [Mapping the Stochastic Penal Colony](https://arxiv.org/abs/2602.00033)
*Robert Grimm*

Main category: cs.CY

TL;DR: 论文重新审视内容审核的惩罚性，提出"随机惩罚殖民地"概念，结合自民族志和程序正义方法分析三个平台案例


<details>
  <summary>Details</summary>
Motivation: 在内容审核高峰过后，重新审视其惩罚性方面，关注惩罚本身而非被惩罚者，探索算法时代的惩罚机制

Method: 结合自民族志收集经验和人工制品，运用程序正义分析；重构福柯惩罚系统模型为算法时代的"随机惩罚殖民地"；应用于三个案例研究

Result: 三个平台案例都展现了普遍存在的账号封禁威胁，将用户放逐到"随机惩罚殖民地"中，尽管具体实施方式不同

Conclusion: 算法时代的内容审核形成了新的惩罚范式——随机惩罚殖民地，介于表演性惩罚和规训性惩罚之间，具有普遍性和威胁性

Abstract: With peak content moderation seemingly behind us, this paper revisits its punitive side. But instead of focusing on who is being (disproportionately) moderated, it focuses on the punishment itself and makes three contributions. First, it develops a novel methodology that combines auto-ethnography for collecting experiences and artifacts with procedural justice for analyzing them. Second, it reworks Foucault's model of the penal system for the algorithmic age, restoring the penal colony as the historically liminal practice between punishment as performance and punishment as discipline, i.e., the stochastic penal colony. Finally, it applies this methodological and conceptual framing to three case studies, one on the gallingly performative moderation by pre-Musk Twitter, one on the exhaustively punitive content moderation for OpenAI's DALLE~2, and one on the relatively light touch but still rather precious moderation by Pinterest. While substantially different, all three feature the pervasive threat of account suspension, thereby banishing users to the stochastic penal colony.

</details>


### [179] [The Verification Crisis: Expert Perceptions of GenAI Disinformation and the Case for Reproducible Provenance](https://arxiv.org/abs/2602.02100)
*Alexander Loth,Martin Kappes,Marc-Oliver Pahl*

Main category: cs.CY

TL;DR: GenAI使虚假信息从人工制造转向自动化大规模生产，专家调查显示文本生成比深度伪造视频更具系统性风险，当前检测工具效果有限，需要建立信息完整性基础设施


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能的发展使虚假信息生产从人工制造转向自动化大规模操纵，需要评估不同模态（文本、图像、音频、视频）的威胁严重性，并评估当前的缓解策略效果

Method: 采用纵向专家感知调查（N=21），涉及AI研究人员、政策制定者和虚假信息专家，评估多模态威胁的感知严重性，并评估当前缓解策略

Result: 深度伪造视频具有即时"冲击"价值，但大规模文本生成在政治领域带来系统性风险，可能导致"认知碎片化"和"合成共识"；专家对技术检测工具持怀疑态度，更倾向于来源标准和监管框架；当前缺乏标准化基准和可重复性检查表

Conclusion: 需要将信息完整性视为基础设施，加强数据来源和方法可重复性的严谨性，建立标准化基准和可重复性检查表来应对GenAI虚假信息挑战

Abstract: The growth of Generative Artificial Intelligence (GenAI) has shifted disinformation production from manual fabrication to automated, large-scale manipulation. This article presents findings from the first wave of a longitudinal expert perception survey (N=21) involving AI researchers, policymakers, and disinformation specialists. It examines the perceived severity of multimodal threats -- text, image, audio, and video -- and evaluates current mitigation strategies.
  Results indicate that while deepfake video presents immediate "shock" value, large-scale text generation poses a systemic risk of "epistemic fragmentation" and "synthetic consensus," particularly in the political domain. The survey reveals skepticism about technical detection tools, with experts favoring provenance standards and regulatory frameworks despite implementation barriers.
  GenAI disinformation research requires reproducible methods. The current challenge is measurement: without standardized benchmarks and reproducibility checklists, tracking or countering synthetic media remains difficult. We propose treating information integrity as an infrastructure with rigor in data provenance and methodological reproducibility.

</details>


### [180] [Synthetic Student Responses: LLM-Extracted Features for IRT Difficulty Parameter Estimation](https://arxiv.org/abs/2602.00034)
*Matias Hoyl*

Main category: cs.CY

TL;DR: 使用LLM提取教学特征，通过两阶段神经网络预测IRT难度参数，无需学生测试即可准确估计题目难度


<details>
  <summary>Details</summary>
Motivation: 传统通过学生预测试确定题目难度的方法资源密集，给教师和评估开发者带来障碍，需要探索无需学生测试的难度预测方法

Method: 结合传统语言特征和LLM提取的教学特征（解题步骤数、认知复杂度、潜在误解），采用两阶段流程：先训练神经网络预测学生回答，再从模拟回答模式推导IRT难度参数

Result: 在超过25万学生回答的数学题目数据集上，模型对完全未见题目的预测难度与实际难度Pearson相关系数达到约0.78

Conclusion: 通过结合LLM提取的教学特征，可以准确预测IRT难度参数，为教育评估提供无需学生测试的可行方案

Abstract: Educational assessment relies heavily on knowing question difficulty, traditionally determined through resource-intensive pre-testing with students. This creates significant barriers for both classroom teachers and assessment developers. We investigate whether Item Response Theory (IRT) difficulty parameters can be accurately estimated without student testing by modeling the response process and explore the relative contribution of different feature types to prediction accuracy. Our approach combines traditional linguistic features with pedagogical insights extracted using Large Language Models (LLMs), including solution step count, cognitive complexity, and potential misconceptions. We implement a two-stage process: first training a neural network to predict how students would respond to questions, then deriving difficulty parameters from these simulated response patterns. Using a dataset of over 250,000 student responses to mathematics questions, our model achieves a Pearson correlation of approximately 0.78 between predicted and actual difficulty parameters on completely unseen questions.

</details>


### [181] [LSSF: Safety Alignment for Large Language Models through Low-Rank Safety Subspace Fusion](https://arxiv.org/abs/2602.00038)
*Guanghao Zhou,Panjia Qiu,Cen Chen,Hongyu Li,Mingyuan Chu,Xin Zhang,Jun Zhou*

Main category: cs.CY

TL;DR: LSSF：一种通过低秩安全子空间融合来恢复微调后LLM安全对齐的后处理方法，利用安全信息的低秩特性构建投影矩阵，动态计算各层安全关键秩，有效恢复安全性而不影响下游任务性能。


<details>
  <summary>Details</summary>
Motivation: LLM的安全机制脆弱，即使在没有有害内容的微调数据集上微调也可能破坏安全性；现有安全对齐方法主要依赖微调过程，导致复杂度和计算资源需求增加。

Method: 提出LSSF框架：利用LLM中安全信息的低秩特性，构建低秩投影矩阵提取安全向量的主成分；该投影矩阵代表LLM的低秩安全子空间，在微调过程中保持稳定且与模型通用能力隔离；通过线性算术将主成分与微调后LLM结合恢复安全对齐；提出安全奇异值熵度量，量化各层安全信息编码密度，动态计算每层安全向量的安全关键秩。

Result: 大量实验表明，该后处理方法能有效恢复微调模型的安全对齐，同时对下游任务性能影响最小。

Conclusion: LSSF提供了一种高效的后处理安全对齐方法，解决了微调过程中安全能力退化的问题，同时保持模型在下游任务上的性能。

Abstract: The safety mechanisms of large language models (LLMs) exhibit notable fragility, as even fine-tuning on datasets without harmful content may still undermine their safety capabilities. Meanwhile, existing safety alignment methods predominantly rely on the fine-tuning process, which inadvertently leads to the increased complexity and computational resources required. To address these issues, we introduce LSSF, a novel safety re-alignment framework with \underline{L}ow-Rank \underline{S}afety \underline{S}ubspace \underline{F}usion. Our proposed method exploits the low-rank characteristics of safety information in LLMs by constructing a low-rank projection matrix to extract the principal components of safety vectors. Notably, this projection matrix represents the low-rank safety subspace of the LLMs, which we have observed to remain stable during fine-tuning process and is isolated from the model's general capabilities. These principal components are used to effectively restore safety alignment when combined with fine-tuned LLMs through linear arithmetic. Additionally, to account for the varying encoding densities of safety information across different layers of LLMs, we propose a novel metric called safety singular value entropy. This metric quantifies the encoding density and allows for the dynamic computation of the safety-critical rank for each safety vector. Extensive experiments demonstrate that our proposed post-hoc alignment method can effectively restore the safety alignment of fine-tuned models with minimal impact on their performance in downstream tasks.

</details>


### [182] [Student Perceptions of Large Language Models Use in Self-Reflection and Design Critique in Architecture Studio](https://arxiv.org/abs/2602.00041)
*Juan David Salazar Rodriguez,Sam Conrad Joyce,Nachamma Sockalingam,Khoo Eng Tat,Julfendi*

Main category: cs.CY

TL;DR: LLMs在建筑设计工作室反馈机制中作为"认知镜子"而非权威指导者，帮助学生进行自我反思、同伴互评和教授评审，促进批判性思维发展。


<details>
  <summary>Details</summary>
Motivation: 研究动机是将LLMs从生成性生产工具转变为反思性教学工具，探索其在建筑教育反馈机制中的潜在价值，特别是在帮助学生克服学习障碍和提升批判性思维方面。

Method: 采用混合方法研究，以新加坡科技设计大学的建筑系学生为研究对象，分析学生在三个反馈领域（自我反思、同伴互评、教授评审）对LLMs使用的感知和体验。

Result: 研究发现：1）在自主学习阶段，LLMs帮助结构化思维并克服"空白页"问题，但缺乏情境细微差别；2）在同伴互评中，LLMs作为中立调解者，减轻社交焦虑和"冒犯恐惧"；3）在教授评审中，学生主要将LLMs用作后评审合成引擎，管理认知负荷并将抽象学术话语转化为可操作的设计迭代。

Conclusion: LLMs在建筑教育中应被视为"认知镜子"而非权威指导者，能够有效支持不同反馈场景下的批判性思维发展，但需要认识到其在情境理解方面的局限性，并合理整合到教学实践中。

Abstract: This study investigates the integration of Large Language Models (LLMs) into the feedback mechanisms of the architectural design studio, shifting the focus from generative production to reflective pedagogy. Employing a mixed-methods approach with architecture students at the Singapore Uni-versity of Technology and Design, the research analyzes student percep-tions across three distinct feedback domains: self-reflection, peer critique, and professor-led reviews. The findings reveal that students engage with LLMs not as authoritative instructors, but as collaborative "cognitive mir-rors" that scaffold critical thinking. In self-directed learning, LLMs help structure thoughts and overcome the "blank page" problem, though they are limited by a lack of contextual nuance. In peer critiques, the technology serves as a neutral mediator, mitigating social anxiety and the "fear of of-fending". Furthermore, in high-stakes professor-led juries, students utilize LLMs primarily as post-critique synthesis engines to manage cognitive overload and translate abstract academic discourse into actionable design iterations.

</details>


### [183] [When LLMs Imagine People: A Human-Centered Persona Brainstorm Audit for Bias and Fairness in Creative Applications](https://arxiv.org/abs/2602.00044)
*Hongliu Cao,Eoin Thomas,Rodrigo Acuna Agost*

Main category: cs.CY

TL;DR: PBA是一种通过开放式角色生成来检测LLM偏见的可扩展审计方法，相比现有固定类别方法能发现多维度偏见并支持纵向追踪。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的偏见输出会强化刻板印象和加剧现实不平等，因此公平性审计至关重要。现有方法依赖固定身份类别和静态基准，存在局限性。

Method: 提出Persona Brainstorm Audit (PBA)方法，通过开放式角色生成来检测偏见，支持多社会维度的偏见发现、纵向追踪，并能降低数据泄露风险。

Result: 对12个最先进的LLM应用PBA，比较了模型间、维度间和版本间的偏见严重程度，发现了独特的偏见模式和谱系特异性变异，追踪了偏见在连续代际中的衰减、持续或重现。

Conclusion: PBA在不同样本量、角色扮演提示和去偏见提示下保持稳定，确立了其作为LLM公平性审计可靠方法的地位。

Abstract: Biased outputs from Large Language Models (LLMs) can reinforce stereotypes and perpetuate inequities in real-world applications, making fairness auditing essential. We introduce the Persona Brainstorm Audit (PBA), a scalable and transparent auditing method for detecting bias through open-ended persona generation. Unlike existing methods that rely on fixed identity categories and static benchmarks, PBA uncovers biases across multiple social dimensions while supporting longitudinal tracking and mitigating data leakage risks. Applying PBA to 12 state-of-the-art LLMs, we compare bias severity across models, dimensions, and versions, uncover distinct patterns and lineage-specific variability, and trace how biases attenuate, persist, or resurface across successive generations. Robustness analyses show PBA remains stable under varying sample sizes, role-playing prompts, and debiasing prompts, establishing its reliability for fairness auditing in LLMs.

</details>


### [184] [AI in Debt Collection: Estimating the Psychological Impact on Consumers](https://arxiv.org/abs/2602.00050)
*Minou Goetze,Sebastian Clajus,Stephan Stricker*

Main category: cs.CY

TL;DR: 研究比较人工与AI催收沟通对消费者心理行为的影响：人工被认为更公平、更能引发互惠，AI被认为更高效；信任无差异；人工引发更多共情但也更强耻辱感；AI可提高效率减少耻辱感但不损害信任，但需谨慎用于需要高共情或公平敏感的场景。


<details>
  <summary>Details</summary>
Motivation: 研究AI在债务催收实践中的整合对消费者心理和行为的影响，探讨AI如何影响敏感金融互动中的心理动态，为平衡技术有效性与人际意识的设计提供信息。

Method: 采用大规模实验设计（n=3514），比较人工与AI中介沟通，在11个欧洲国家进行，考察对消费者社会偏好（公平、信任、互惠、效率）和社会情感（耻辱感、共情）的影响，并进行探索性分析（性别、年龄、文化背景差异）。

Result: 参与者认为人工互动更公平、更可能引发互惠，AI中介沟通被视为更高效；信任无差异；人工接触引发更多共情但也更强耻辱感；探索性分析显示性别、年龄和文化背景存在显著差异。

Conclusion: AI中介沟通可提高效率、减少耻辱感而不损害信任，但在需要高共情或对公平敏感的情况下应谨慎使用；研究推进了对AI如何影响敏感金融互动心理动态的理解，并为平衡技术有效性与人际意识的沟通策略设计提供信息。

Abstract: The present study investigates the psychological and behavioral implications of integrating AI into debt collection practices using data from eleven European countries. Drawing on a large-scale experimental design (n = 3514) comparing human versus AI-mediated communication, we examine effects on consumers' social preferences (fairness, trust, reciprocity, efficiency) and social emotions (stigma, empathy). Participants perceive human interactions as more fair and more likely to elicit reciprocity, while AI-mediated communication is viewed as more efficient; no differences emerge in trust. Human contact elicits greater empathy, but also stronger feelings of stigma. Exploratory analyses reveal notable variation between gender, age groups, and cultural contexts. In general, the findings suggest that AI-mediated communication can improve efficiency and reduce stigma without diminishing trust, but should be used carefully in situations that require high empathy or increased sensitivity to fairness. The study advances our understanding of how AI influences the psychological dynamics in sensitive financial interactions and informs the design of communication strategies that balance technological effectiveness with interpersonal awareness.

</details>


### [185] [Examining The CoVCues Dataset: Supporting COVID Infodemic Research Through A Novel User Assessment Study](https://arxiv.org/abs/2602.00055)
*Shreetika Poudel,Ankur Chatterjee*

Main category: cs.CY

TL;DR: CoVCues数据集填补了COVID健康错误信息检测中视觉线索数据的空白，并通过用户评估验证了视觉线索在判断信息可靠性中的重要性。


<details>
  <summary>Details</summary>
Motivation: COVID-19大流行导致在线健康错误信息激增，现有数据集多为单模态文本数据，缺乏视觉线索（如图像、信息图表）的多模态数据集，限制了错误信息检测的全面性。

Method: 创建了包含多样化图像工件的CoVCues数据集，并通过问卷调查进行初步用户评估，调查不同参与者如何利用数据集图像判断信息可靠性。

Result: 用户评估提供了早期洞察，显示不同利益相关者对视觉线索的解读方式，验证了视觉线索在判断在线健康信息可靠性中的实用价值。

Conclusion: CoVCues数据集填补了COVID错误信息检测中视觉线索数据的空白，用户评估研究表明视觉线索在对抗COVID信息流行病中具有重要但未被充分利用的价值。

Abstract: The public confidence and trust in online healthcare information have been greatly dented following the COVID-19 pandemic, which triggered a significant rise in online health misinformation. Existing literature shows that different datasets have been created to aid with detecting false information associated with this COVID infodemic. However, most of these datasets contain mostly unimodal data, which comprise primarily textual cues, and not visual cues, like images, infographics, and other graphic data components. Prior works point to the fact that there are only a handful of multimodal datasets that support COVID misinformation identification, and they lack an organized, processed and analyzed repository of visual cues. The novel CoVCues dataset, which represents a varied set of image artifacts, addresses this gap and advocates for the use of visual cues towards detecting online health misinformation. As part of validating the contents and utility of our CoVCues dataset, we have conducted a preliminary user assessment study, where different participants have been surveyed through a set of questionnaires to determine how effectively these dataset images contribute to the user perceived information reliability. These survey responses helped provide early insights into how different stakeholder groups interpret visual cues in the context of online health information and communication. The findings from this novel user assessment study offer valuable feedback for refining our CoVCues dataset and for supporting our claim that visual cues are underutilized but useful in combating the COVID infodemic. To our knowledge, this user assessment research study, as described in this paper, is the first of its kind work, involving COVID visual cues, that demonstrates the important role that our CoVCues dataset can potentially play in aiding COVID infodemic related future research work.

</details>


### [186] [How Hyper-Datafication Impacts the Sustainability Costs in Frontier AI](https://arxiv.org/abs/2602.00056)
*Sophia N. Wilson,Sebastian Mair,Mophat Okinyi,Erik B. Dam,Janin Koch,Raghavendra Selvan*

Main category: cs.CY

TL;DR: 论文分析了AI大规模数据的可持续性成本，提出"超数据化"概念，揭示数据增长带来的环境、社会和代表性不平等问题，并提出Data PROOFS建议框架。


<details>
  <summary>Details</summary>
Motivation: 当前AI发展依赖于大规模数据集，但数据收集和处理的可持续性成本（环境、社会、经济）被忽视。研究旨在揭示这些隐藏成本，特别是数据化进程如何加剧全球不平等。

Method: 1) 定量分析：分析Hugging Face Hub约55万个数据集，关注增长趋势、存储能耗、碳足迹和语言代表性；2) 定性分析：收集肯尼亚数据工作者的访谈，了解劳动条件和内容暴露；3) 补充分析：使用外部数据验证数据中心基础设施的全球不平等。

Result: 超数据化不仅增加资源消耗，还系统性地将环境负担、劳动风险和代表性伤害转移到全球南方、不稳定数据工作者和弱势文化群体。数据中心基础设施存在显著全球不平等。

Conclusion: 需要关注AI数据集的可持续性成本，提出Data PROOFS框架（来源、资源意识、所有权、开放性、节俭性、标准）来缓解这些问题，促进研究社区更广泛的讨论。

Abstract: Large-scale data has fuelled the success of frontier artificial intelligence (AI) models over the past decade. This expansion has relied on sustained efforts by large technology corporations to aggregate and curate internet-scale datasets. In this work, we examine the environmental, social, and economic costs of large-scale data in AI through a sustainability lens. We argue that the field is shifting from building models from data to actively creating data for building models. We characterise this transition as hyper-datafication, which marks a critical juncture for the future of frontier AI and its societal impacts. To quantify and contextualise data-related costs, we analyse approximately 550,000 datasets from the Hugging Face Hub, focusing on dataset growth, storage-related energy consumption and carbon footprint, and societal representation using language data. We complement this analysis with qualitative responses from data workers in Kenya to examine the labour involved, including direct employment by big tech corporations and exposure to graphic content. We further draw on external data sources to substantiate our findings by illustrating the global disparity in data centre infrastructure. Our analyses reveal that hyper-datafication does not merely increase resource consumption but systematically redistributes environmental burdens, labour risks, and representational harms toward the Global South, precarious data workers, and under-represented cultures. Thus, we propose Data PROOFS recommendations spanning provenance, resource awareness, ownership, openness, frugality, and standards to mitigate these costs. Our work aims to make visible the often-overlooked costs of data that underpin frontier AI and to stimulate broader debate within the research community and beyond.

</details>


### [187] [A longitudinal geospatial multimodal dataset of post-discharge frailty, physiology, mobility, and neighborhoods](https://arxiv.org/abs/2602.00060)
*Ali Abedi,Charlene H. Chu,Shehroz S. Khan*

Main category: cs.CY

TL;DR: GEOFRAIL数据集：针对出院后衰弱老年人的纵向地理空间多模态数据集，包含人口统计、传感器特征、临床评估和邻里环境数据，用于监测恢复轨迹。


<details>
  <summary>Details</summary>
Motivation: 老年人衰弱与功能下降、行动能力减弱、社会隔离及出院后恢复困难相关，可能导致再住院。邻里环境通过影响活动机会、社会参与和社区资源获取进一步影响恢复轨迹。需要多模态传感技术来连续监测这些多维因素。

Method: 收集社区居住衰弱老年人出院后8周的数据，采用标准化流程和隐私保护空间聚合。数据集包含：参与者人口统计、多模态传感器特征、双周临床评估（衰弱、身体功能、社会隔离）、时间位置记录（链接邻里设施、犯罪率、社会经济指标）。

Result: 技术验证显示地理空间、传感器衍生和临床测量之间的内部一致性，并报告了用于表征恢复轨迹的机器学习模型的基线性能。

Conclusion: GEOFRAIL数据集为研究衰弱老年人出院后恢复轨迹提供了全面的多模态数据资源，结合了个人健康指标和邻里环境因素，支持数据驱动的研究方法。

Abstract: Frailty in older adults is associated with increased vulnerability to functional decline, reduced mobility, social isolation, and challenges during the transition from hospital to community living. These factors are associated with rehospitalization and may adversely influence recovery. Neighborhood environments can further shape recovery trajectories by affecting mobility opportunities, social engagement, and access to community resources. Multimodal sensing technologies combined with data-driven analytical approaches offer the potential to continuously monitor these multidimensional factors in real-world settings. This Data Descriptor presents GEOFRAIL, a longitudinal geospatial multimodal dataset collected from community-dwelling frail older adults following hospital discharge. The dataset is organized into interconnected tables capturing participant demographics, features derived from multimodal sensors, biweekly clinical assessments of frailty, physical function, and social isolation, and temporal location records linked to neighborhood amenities, crime rates, and census-based socioeconomic indicators. Data were collected over an eight-week post-discharge period using standardized pipelines with privacy-preserving spatial aggregation. Technical validation demonstrates internal consistency across geospatial, sensor-derived, and clinical measures and reports baseline performance of machine learning models for characterizing recovery trajectories.

</details>


### [188] [Simple Role Assignment is Extraordinarily Effective for Safety Alignment](https://arxiv.org/abs/2602.00061)
*Zhou Ziheng,Jiakun Ding,Zhaowei Zhang,Ruosen Gao,Yingnian Wu,Demetri Terzopoulos,Yipeng Kang,Fangwei Zhong,Junqi Wang*

Main category: cs.CY

TL;DR: 提出基于社会角色的条件化方法替代原则对齐，通过角色编码价值观和认知模式，无需训练即可显著提升模型安全性和性能


<details>
  <summary>Details</summary>
Motivation: 传统基于原则的对齐方法缺乏上下文敏感性和完整性，需要更紧凑有效的替代方案

Method: 基于心智理论提出角色条件化方法，包含角色条件生成器和迭代式角色批评器，无需训练即可应用

Result: 在五个模型家族中一致优于原则对齐、思维链等基线方法，将WildJailbreak不安全输出从81.4%降至3.6%，在代理安全任务中同样有效

Conclusion: 角色分配是强大且可解释的AI对齐范式，可用于构建LLM-as-a-Judge系统

Abstract: Principle-based alignment often lacks context sensitivity and completeness. Grounded in Theory of Mind, we propose role conditioning as a compact alternative: social roles (e.g., mother, judge) implicitly encode both values and the cognitive schemas required to apply them. We introduce a training-free pipeline featuring a role-conditioned generator and iterative role-based critics for refinement. Across five model families, our approach consistently outperforms principle-based, Chain-of-Thought (CoT) and other baselines across benchmarks. Notably, it reduces unsafe outputs on the WildJailbreak benchmark from 81.4\% to 3.6\% with DeepSeek-V3. Not only for common safety benchmarks, it consistently applies for agentic safety tasks. These results establish role assignment as a powerful, interpretable paradigm for AI alignment and LLM-as-a-Judge construction.

</details>


### [189] [Responsible Evaluation of AI for Mental Health](https://arxiv.org/abs/2602.00065)
*Hiba Arnaout,Anmol Goel,H. Andrew Schwartz,Steffen T. Eberhardt,Dana Atzil-Slonim,Gavin Doherty,Brian Schwartz,Wolfgang Lutz,Tim Althoff,Munmun De Choudhury,Hamidreza Jamalabadi,Raj Sanjay Shah,Flor Miriam Plaza-del-Arco,Dirk Hovy,Maria Liakata,Iryna Gurevych*

Main category: cs.CY

TL;DR: 该论文提出一个跨学科框架，重新思考AI心理健康工具的责任评估，强调临床有效性、社会背景和公平性，而非仅依赖通用指标。


<details>
  <summary>Details</summary>
Motivation: 当前评估AI心理健康工具的方法零散且与临床实践、社会背景和用户体验脱节，需要更负责任、更全面的评估框架。

Method: 通过分析135篇*CL出版物，识别现有局限，提出整合临床有效性、社会背景和公平性的跨学科框架，并建立AI心理健康支持类型分类法。

Result: 发现现有研究过度依赖通用指标、缺乏心理健康专业人员参与、忽视安全性和公平性，提出了针对评估、干预和信息合成三类AI支持的具体评估要求。

Conclusion: 需要重新定义AI心理健康工具的责任评估标准，确保评估内容、评估者和评估目的与临床实践、社会背景和用户体验紧密结合。

Abstract: Although artificial intelligence (AI) shows growing promise for mental health care, current approaches to evaluating AI tools in this domain remain fragmented and poorly aligned with clinical practice, social context, and first-hand user experience. This paper argues for a rethinking of responsible evaluation -- what is measured, by whom, and for what purpose -- by introducing an interdisciplinary framework that integrates clinical soundness, social context, and equity, providing a structured basis for evaluation. Through an analysis of 135 recent *CL publications, we identify recurring limitations, including over-reliance on generic metrics that do not capture clinical validity, therapeutic appropriateness, or user experience, limited participation from mental health professionals, and insufficient attention to safety and equity. To address these gaps, we propose a taxonomy of AI mental health support types -- assessment-, intervention-, and information synthesis-oriented -- each with distinct risks and evaluative requirements, and illustrate its use through case studies.

</details>


### [190] [FoundationalASSIST: An Educational Dataset for Foundational Knowledge Tracing and Pedagogical Grounding of LLMs](https://arxiv.org/abs/2602.00070)
*Eamon Worden,Cristina Heffernan,Neil Heffernan,Shashank Sonkar*

Main category: cs.CY

TL;DR: 论文提出了FoundationalASSIST数据集，这是首个包含完整教育信息的英语数据集，用于研究LLM在教育中的应用，并评估了多个前沿模型在知识追踪和教学基础任务上的表现，发现当前LLM能力存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 当前教育数据集只提供问题标识符和二元正确性标签，对基于自然语言推理的LLM来说不够透明。随着LLM被部署用于自适应测试和个性化辅导，需要了解LLM是否理解学生学习过程，但现有资源无法回答这个问题。

Method: 创建FoundationalASSIST数据集，包含170万次交互、5000名学生、完整问题文本、学生实际回答、错误答案选择记录以及K-12共同核心标准对齐。使用四个前沿模型（GPT-OSS-120B、Llama-3.3-70B、Qwen3-Next-80B变体）在两个任务家族上进行评估：知识追踪（预测学生表现和具体答案）和教学基础（理解评估项目的有效性特征）。

Result: 所有模型在知识追踪任务上仅达到微不足道的基线水平；在项目区分度任务上所有模型低于随机机会，表明LLM不理解什么使问题更具诊断性；在判断相对难度方面表现较好（最高68.6%），但这部分成功凸显了其他方面的差距。

Conclusion: 当前LLM在可靠支持大规模个性化学习方面存在显著能力差距，需要实质性进展。发布FoundationalASSIST数据集以支持这些基础挑战的进展。

Abstract: Can Large Language Models understand how students learn? As LLMs are deployed for adaptive testing and personalized tutoring, this question becomes urgent -- yet we cannot answer it with existing resources. Current educational datasets provide only question identifiers and binary correctness labels, rendering them opaque to LLMs that reason in natural language. We address this gap with FoundationalASSIST, the first English educational dataset providing the complete information needed for research on LLMs in education: full question text, actual student responses (not just right/wrong), records of which wrong answers students chose, and alignment to Common Core K-12 standards. These 1.7 million interactions from 5,000 students enable research directions that were previously impossible to pursue, from fine-tuning student models to analyzing misconception patterns. To demonstrate the dataset's utility, we evaluate four frontier models (GPT-OSS-120B, Llama-3.3-70B, Qwen3-Next-80B variants) on two complementary task families: Knowledge Tracing, testing whether LLMs can predict student performance on questions, and the exact answer a student will give; and \textbf{Pedagogical Grounding}, testing whether LLMs understand the properties that make assessment items effective. Our evaluation reveals significant gaps in current LLM capabilities. Every model barely achieves a trivial baseline on knowledge tracing. All models fall below random chance on item discrimination, indicating that LLMs do not understand what makes one problem more diagnostic than another. Models do show competence at judging relative difficulty (up to 68.6%), but this partial success only highlights the gaps elsewhere. These results establish that substantial advances are needed before LLMs can reliably support personalized learning at scale. We release FoundationalASSIST to support progress on these foundational challenges.

</details>


### [191] [Adoption and Use of LLMs at an Academic Medical Center](https://arxiv.org/abs/2602.00074)
*Nigam H. Shah,Nerissa Ambers,Abby Pandya,Timothy Keyes,Juan M. Banda,Srikar Nallan,Carlene Lugtu,Artem A. Trotsyuk,Suhana Bedi,Alyssa Unell,Miguel Fuentes,Francois Grolleau,Sneha S. Jain,Jonathan Chen,Devdutta Dash,Danton Char,Aditya Sharma,Duncan McElfresh,Patrick Scully,Vishanthan Kumar,Connor OBrien,Satchi Mouniswamy,Elvis Jones,Krishna Jasti,Gunavathi Mannika Lakshmanan,Sree Ram Akula,Varun Kumar Singh,Ramesh Rajmanickam,Sudhir Sinha,Vicky Zhou,Xu Wang,Bilal Mawji,Joshua Ge,Wencheng Li,Travis Lyons,Jarrod Helzer,Vikas Kakkar,Ramesh Powar,Darren Batara,Cheryl Cordova,William Frederick,Olivia Tang,Phoebe Morgan,April S. Liang,Stephen P. Ma,Shivam Vedak,Dong-han Yao,Akshay Swaminathan,Mehr Kashyap,Brian Ng,Jamie Hellman,Nikesh Kotecha,Christopher Sharp,Gretchen Brown,Christian Lindmark,Anurang Revri,Michael A. Pfeffer*

Main category: cs.CY

TL;DR: ChatEHR是一个将大型语言模型集成到电子健康记录中的系统，通过自动化和交互界面支持临床文档需求，在1.5年内实现了显著的用户采用和成本节约


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在临床文档应用中存在的"工作流程摩擦"问题，即手动数据输入带来的效率障碍，使LLM能够访问跨越数年的完整患者时间线数据

Method: 开发ChatEHR系统，包含两种使用模式：1) 自动化（静态提示与数据组合执行固定任务），2) 通过用户界面在EHR中进行交互式使用。系统采用模型无关设计，可访问多种数据类型

Result: 在1.5年内构建了7个自动化功能，1075名用户接受培训成为常规用户，前3个月进行了23,000次会话。总结生成是最常见任务，每生成存在约0.73个幻觉和1.60个不准确信息。初步估计第一年可节省600万美元

Conclusion: ChatEHR将LLM使用重新定义为机构能力，需要新的性能监控方法。这种"内部构建"策略使医疗系统能够通过供应商无关、内部治理的LLM平台保持自主性，同时实现显著的经济效益

Abstract: While large language models (LLMs) can support clinical documentation needs, standalone tools struggle with "workflow friction" from manual data entry. We developed ChatEHR, a system that enables the use of LLMs with the entire patient timeline spanning several years. ChatEHR enables automations - which are static combinations of prompts and data that perform a fixed task - and interactive use in the electronic health record (EHR) via a user interface (UI). The resulting ability to sift through patient medical records for diverse use-cases such as pre-visit chart review, screening for transfer eligibility, monitoring for surgical site infections, and chart abstraction, redefines LLM use as an institutional capability. This system, accessible after user-training, enables continuous monitoring and evaluation of LLM use.
  In 1.5 years, we built 7 automations and 1075 users have trained to become routine users of the UI, engaging in 23,000 sessions in the first 3 months of launch. For automations, being model-agnostic and accessing multiple types of data was essential for matching specific clinical or administrative tasks with the most appropriate LLM. Benchmark-based evaluations proved insufficient for monitoring and evaluation of the UI, requiring new methods to monitor performance. Generation of summaries was the most frequent task in the UI, with an estimated 0.73 hallucinations and 1.60 inaccuracies per generation. The resulting mix of cost savings, time savings, and revenue growth required a value assessment framework to prioritize work as well as quantify the impact of using LLMs. Initial estimates are $6M savings in the first year of use, without quantifying the benefit of the better care offered. Such a "build-from-within" strategy provides an opportunity for health systems to maintain agency via a vendor-agnostic, internally governed LLM platform.

</details>


### [192] [Standards for trustworthy AI in the European Union: technical rationale, structural challenges, and an implementation path](https://arxiv.org/abs/2602.00078)
*Piercosma Bisconti,Marcello Galisai*

Main category: cs.CY

TL;DR: 该白皮书分析了欧盟AI法案下的AI标准化技术基础，探讨了标准如何实现合规推定机制，提出了基于风险管理、可重复技术检查、结构化文档和生命周期保证的分层标准化方案。


<details>
  <summary>Details</summary>
Motivation: 欧盟AI法案需要技术标准来将法律义务转化为可审计的工程实践，但AI系统具有随机性、数据依赖性、评估实践不成熟和生命周期动态性等独特挑战，需要建立有效的标准化框架。

Method: 采用分层标准化方法：水平标准定义流程义务和证据结构，行业配置文件指定领域特定阈值和验收标准。方案包括风险管理、可重复技术检查（重新定义为测量属性的稳定性）、结构化文档、全面日志记录和生命周期保证案例。

Result: 提出了一个可行的标准化方案，能够应对AI独特挑战，将法律义务转化为可审计的工程实践，实现跨提供商、评估机构和执法机构的可扩展合规评估。

Conclusion: 尽管存在方法论困难，技术标准对于将法律义务转化为可审计的工程实践、实现可扩展的合规评估至关重要，分层标准化方法能够有效应对AI系统的独特挑战。

Abstract: This white paper examines the technical foundations of European AI standardization under the AI Act. It explains how harmonized standards enable the presumption of conformity mechanism, describes the CEN/CENELEC standardization process, and analyzes why AI poses unique standardization challenges including stochastic behavior, data dependencies, immature evaluation practices, and lifecycle dynamics. The paper argues that AI systems are typically components within larger sociotechnical systems, requiring a layered approach where horizontal standards define process obligations and evidence structures while sectoral profiles specify domain-specific thresholds and acceptance criteria. It proposes a workable scheme based on risk management, reproducible technical checks redefined as stability of measured properties, structured documentation, comprehensive logging, and assurance cases that evolve over the system lifecycle. The paper demonstrates that despite methodological difficulties, technical standards remain essential for translating legal obligations into auditable engineering practice and enabling scalable conformity assessment across providers, assessors, and enforcement authorities

</details>


### [193] [Exploring the Role of Automated Feedback in Programming Education: A Systematic Literature Review](https://arxiv.org/abs/2602.00089)
*Yeonji Jung,Yunseo Lee,Jiyeong Bae,DoYong Kim,Heungsoo Choi,Minji Kang,Unggi Lee*

Main category: cs.CY

TL;DR: 本文对编程教育中的自动化反馈系统进行了系统性文献综述，分析了61项实证研究，揭示了当前系统主要关注错误检测和代码正确性，缺乏对高阶学习过程、交互性和学习者自主性的支持，需要重新构想自动化反馈作为教学支架而非技术附加功能。


<details>
  <summary>Details</summary>
Motivation: 尽管自动化反馈系统在编程教育中日益普及且AI技术不断发展，但该领域研究仍然碎片化，缺乏技术和教学维度的综合。需要系统性地分析现有研究，为自动化反馈系统的设计和评估提供概念基础。

Method: 采用系统性文献综述方法，分析了截至2024年9月发表的61项实证研究。从五个维度进行概念性分析：系统架构、教学功能、交互机制、情境部署和评估方法。

Result: 研究发现大多数系统是完全自动化的，嵌入在线平台，主要关注错误检测和代码正确性。虽然近期发展融入了自适应功能和大语言模型以实现更个性化和交互式反馈，但很少有系统支持高阶学习过程、交互组件或学习者自主性。评估实践倾向于强调短期绩效提升，对长期结果或教学整合关注有限。

Conclusion: 需要重新构想自动化反馈系统，不应将其视为错误纠正的技术附加功能，而应作为支持更深层次、自适应和交互式学习的教学支架。未来研究应关注支持高阶学习过程、增强交互性和学习者自主性的系统设计。

Abstract: Automated feedback systems have become increasingly integral to programming education, where learners engage in iterative cycles of code construction, testing, and refinement. Despite its wider integration in practices and technical advancements into AI, research in this area remains fragmented, lacking synthesis across technological and instructional dimensions. This systematic literature review synthesizes 61 empirical studies published by September 2024, offering a conceptually grounded analysis of automated feedback systems across five dimensions: system architecture, pedagogical function, interaction mechanism, contextual deployment, and evaluation approach. Findings reveal that most systems are fully automated, embedded within online platforms, and primarily focused on error detection and code correctness. While recent developments incorporate adaptive features and large language models to enable more personalized and interactive feedback, few systems offer support for higher-order learning processes, interactive components, or learner agency. Moreover, evaluation practices tend to emphasize short-term performance gains, with limited attention to long-term outcomes or instructional integration. These findings call for a reimagining of automated feedback not as a technical add-on for error correction, but as a pedagogical scaffold that supports deeper, adaptive, and interactive learning.

</details>


### [194] [Generative Artificial Intelligence in Small and Medium Enterprises: Navigating its Promises and Challenges](https://arxiv.org/abs/2602.00091)
*Kumaran Rajaram,Patrick Nicolas Tinguely*

Main category: cs.CY

TL;DR: 论文探讨中小企业如何利用生成式AI提升竞争力，提出了基于航海隐喻的战略部署框架和实用建议。


<details>
  <summary>Details</summary>
Motivation: 生成式AI技术为中小企业提供了强大的能力，能够促进规模化和创造力的民主化。即使技术专业知识或财务资源有限，中小企业也可以利用这项技术简化工作流程、激发创新，从而改善产品供应和长期竞争力。论文旨在帮助中小企业应对GAI的机遇和挑战。

Method: 引入航海隐喻来揭示GAI部署的关键战略维度，包括员工能力、有效领导和工作价值观、组织文化、协作与合作、以及与第三方的关系。提供实用建议作为中小企业成功部署GAI的指南针。

Result: 提出了一个全面的GAI部署路线图，通过航海隐喻框架帮助中小企业识别和应对实施过程中的关键战略维度，为中小企业提供了实用的部署指南。

Conclusion: 中小企业可以通过系统化的GAI部署策略充分利用生成式AI的潜力，航海隐喻框架为中小企业提供了实用的战略指导，帮助它们在资源有限的情况下成功实施AI技术，提升创新能力和竞争力。

Abstract: The latest technological developments in generative artificial intelligence (GAI) offer powerful capabilities to small and medium enterprises (SMEs), as they facilitate the democratization of both scalability and creativity. Even if they have little technical expertise or financial resources, SMEs can leverage this technology to streamline work processes and unleash innovation, thereby improving their product offerings and long-term competitiveness. This paper discusses how SMEs can navigate both the promises and challenges of GAI and offers a roadmap for deploying GAI. We introduce a sailing metaphor that reveals key strategic dimensions for GAI deployment: competency of employees, effective leadership and work values, organizational culture, collaboration and cooperation, and relationships with third parties. We offer practical recommendations that serve as a useful compass for successfully deploying GAI in SMEs.

</details>


### [195] [Not All Students Engage Alike: Multi-Institution Patterns in GenAI Tutor Use](https://arxiv.org/abs/2602.00447)
*Youjie Chen,Xixi Shi,Xinyu Liu,Shuaiguo Wang,Tracy Xiao Liu,Dragan Gašević*

Main category: cs.CY

TL;DR: 研究通过分析11,406名学生在200个大学课堂中使用GenAI辅导工具的数据，识别出四种参与类型，发现10.4%为浅层参与（复制粘贴行为普遍），且参与模式存在跨院校选择性和学科领域的显著差异。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能（GenAI）辅导工具为个性化学习提供了前所未有的机会，但存在学生可能以不支持学习的方式使用这些工具的风险，且不同教学环境下学生参与度可能存在差异，导致学习体验不平等。

Method: 使用现有GenAI辅导工具和学习管理系统的去标识化学生交互日志，通过两阶段分析管道：1）在对话会话层面识别四种不同的参与类型；2）在学生层面分析参与类型随时间的变化模式。

Result: 识别出四种参与类型，其中10.4%为浅层参与（复制粘贴行为普遍）；浅层参与学生更可能保持该模式，而深度参与学生更灵活地转换参与类型；院校选择性和学科领域对参与模式有显著影响，高选择性院校学生更可能深度参与。

Conclusion: 研究深化了对GenAI辅导工具在真实教育环境中使用的理解，提供了分析学生参与度的框架，对大规模负责任实施具有重要启示，特别关注参与不平等问题。

Abstract: The emergence of generative artificial intelligence (GenAI) has created unprecedented opportunities to provide individualized learning support in classrooms as automated tutoring systems at scale. However, concerns have been raised that students may engage with these tools in ways that do not support learning. Moreover, student engagement with GenAI Tutors may vary across instructional contexts, potentially leading to unequal learning experiences. In this study, we utilize de-identified student interaction logs from an existing GenAI Tutor and the learning management system in which it is embedded. We systematically examined student engagement (N = 11,406) with the tool across 200 classes in ten post-secondary institutions through a two-stage pipeline: First, we identified four distinct engagement types at the conversation session level. In particular, 10.4% of them were "shallow engagement" where copy-pasting behavior was prevalent. Then, at the student level, we show that students transitioned across engagement types over time. However, students who exhibited shallow engagement with the tool were more likely to remain in this mode, whereas those who engaged deeply with the tool transitioned more flexibly across engagement types. Finally, at both the session and student levels, we show substantial heterogeneity in student engagement across institution selectivity and course disciplines. In particular, students from highly selective institutions were more likely to exhibit deep engagement. Together, our study advances the understanding of how GenAI Tutors are used in authentic educational settings and provides a framework for analyzing student engagement with GenAI Tutors, with implications for responsible implementation at scale.

</details>


### [196] [A Qualitative Study of IT Students' Skill Development: Comparing Online and Face- to-Face Learning Environments](https://arxiv.org/abs/2602.00799)
*Hugo Silva*

Main category: cs.CY

TL;DR: 该研究探讨IT学生在线上和面对面学习环境中对技能发展的体验和评估，发现面对面学习更有利于沟通协作技能，而线上学习则更有利于自我调节和适应能力的发展。


<details>
  <summary>Details</summary>
Motivation: 每个学生都有特定的特征和学习偏好，这些特征和偏好会影响他们在线上或面对面学习环境中的表现。理解这些差异对于教育工作者创建能够激励和吸引学生的学习环境至关重要。该研究旨在了解IT学生如何体验和评估在不同学习环境中的技能发展。

Method: 采用社会建构主义范式，通过半结构化访谈收集数据，重点关注学生的观点和体验。数据分析采用扎根理论方法，使用系统化程序进行分析。

Result: 研究结果表明，面对面学习可能更有效地发展沟通和协作技能，因为学生能够体验同步互动；而线上学习可能更有利于自我调节和适应能力的发展，因为它提供了独立性和灵活性。

Conclusion: 该研究产生了两个扎根理论，解释了不同的IT学习环境如何影响学生特定技能的发展，这有助于优化混合学习体验的教学讨论。

Abstract: Each student has specific characteristics and learning preferences, that reflect on each type of learning environment, online or face-to-face. Understanding these differences is crucial for educators to create learning environments that can inspire and engage students. This qualitative study explores and tries to better understand, specifically the IT student's experiences and perceived skills development in online and face-to-face learning environments, while trying to address the question: "Regarding online and face-to-face learning environments, in IT, how do students experience and assess their skill development in one learning environment compared to the other?". Using a social constructive paradigm, the purpose of the research is to focus as much as possible on the student's views of the situation and how their perspectives and experiences shape the perception of developed skills. Data was collected through semi-structured interviews by focusing on the student and asking for their personal experience on skill development through online and face-to-face learning environments. The data analysis strategy adopts the grounded theory approach, using a systematic procedure. The results suggest that face-to-face learning may develop a better communication and collaborative skills more effectively while experiencing a synchronous interaction, where online learning may strength in self-regulation and adaptability skills because of the independence and flexibility it provides. This study produces two grounded theories that explain how different IT learning environments influence the development of student's specific skills, that can contribute to pedagogical discussions on optimizing hybrid learning experiences.

</details>


### [197] [PS$^2$: Parameterized Control for Fine-Grained Student Proficiency Simulation](https://arxiv.org/abs/2602.00850)
*Ruochen Liu,Zhiyuan Wen,Hao Yan,Jun Yin,Senzhang Wang,Jiannong Cao*

Main category: cs.CY

TL;DR: PS²：参数化学生能力模拟框架，通过强弱LLM混合插值实现细粒度学生能力模拟，解决现有方法可控性差、提示敏感、缺乏校准的问题


<details>
  <summary>Details</summary>
Motivation: 真实学生响应数据获取面临成本、伦理和安全限制，现有LLM模拟方法存在可控性有限、提示设计敏感、缺乏学术表现校准等问题

Method: PS²采用无监督参数化框架，通过强LLM上界与弱LLM下界（经认知错误微调）的混合插值，并基于学术表现校准插值比例

Result: 在两个公开数据集上，PS²相比基线方法实现了更细粒度、一致的能力模拟，在学生行为相似性和题目难度预测方面表现更优

Conclusion: PS²框架为数据稀缺条件下的学生能力模拟提供了有效解决方案，通过参数化混合插值和学术表现校准实现了更好的可控性和准确性

Abstract: Understanding how students with different proficiency levels respond to educational materials is a critical issue within the field of AI for Education. However, acquiring sufficient real student response data for a robust evaluation is often hindered by cost, ethics, and security constraints. Consequently, LLM-based student proficiency simulation, especially prompt-based methods, has emerged as a practical alternative under data-scarce conditions. Despite their promise, current methods still exhibit limited controllability with coarse-grained proficiency representations, high sensitivity to prompt design, and the lack of calibration with academic performance. Therefore, we propose Parameterized Student Proficiency Simulation (PS$^2$), an unsupervised and parameterized model-level framework that simulates students with different proficiencies by interpolating between a strong upper-bound LLM and a weaker, cognitive error-informed lower-bound student LLM via a hybrid ratio. Specifically, the lower-bound model is constructed by fine-tuning the weaker LM to exhibit cognitive errors when responding to educational materials. To ensure alignment with target proficiency levels, PS$^2$ further calibrates the interpolation ratio with academic performance. Experiments on two public datasets demonstrate that PS$^2$ achieves finer-grained and consistent proficiency simulation compared to existing baselines, leading to superior performance in student behavior similarity and item difficulty prediction.

</details>


### [198] [Does Ad-Free Mean Less Data Collection? An Empirical Study of Platform Data Practices and User Expectations](https://arxiv.org/abs/2602.01231)
*Sepehr Mousavi,Abhisek Dash,Savvas Zannettou,Krishna P. Gummadi*

Main category: cs.CY

TL;DR: 研究发现，即使采用无广告订阅模式，平台仍会收集广告相关数据，这与用户期望的数据收集减少存在显著差距，并引发对GDPR合规性的质疑。


<details>
  <summary>Details</summary>
Motivation: 随着在线平台推出付费无广告订阅作为传统免费广告模式的替代方案，GDPR下广告作为数据处理的关键理由被移除。理论上平台应减少数据收集，但平台可能以提供个性化体验为由继续收集数据。这种隐私原则与平台激励之间的紧张关系引发了一个关键问题：无广告与广告订阅模式下的数据收集实践是否存在差异？

Method: 1) 数据收集过程分析：通过对Instagram、Facebook和X三大平台的数据导出进行分析；2) 用户期望调查：在Prolific平台上对255名参与者进行调查，了解他们对无广告模式下数据收集的规范期望和实际预期。

Result: 1) 平台实践：即使采用无广告订阅，平台仍保留或收集部分广告相关数据；2) 用户期望：69%的参与者规范上期望数据收集减少，但63%的人实际上认为平台仍会收集相同数量的数据，表明对平台实践持怀疑态度。

Conclusion: 研究发现数据实践与用户规范期望之间存在显著脱节，并引发对平台是否遵守GDPR核心原则（如目的限制、数据最小化和透明度）的严重质疑。这表明无广告订阅模式并未完全解决隐私问题，平台实践与用户期望之间存在明显差距。

Abstract: Online platforms increasingly offer "paid" ad-free subscriptions as an alternative to the traditional "free" ad-based model. The transition to ad-free models ostensibly removes advertising as a key justification for data processing under the GDPR. So, normatively, platforms should collect less user data. However, platforms may justify continued data collection as a means to provide an improved, personalized experience. This tension between privacy principles and platform incentives raises a critical underexplored question: do data collection practices vary between ad-free and ad-based subscription models?
  In this paper, we shed light on this important privacy issue by investigating the alignment between platform data collection practices and related user expectations. With respect to data collection process, our analyses of data exports from three major online platforms - Instagram, Facebook, and X - reveal that these platforms continue to retain or collect some ad-related data, even in ad-free subscriptions. With respect to user expectations, our survey among 255 participants on Prolific reveals that 69% of the participants normatively expect data collection to be reduced, indicating their expectation of improved digital privacy in an ad-free model. However, when asked what they think actually happens, 63% of these participants believed that platforms would still collect about the same amount of data, highlighting skepticism about platform practices. Our findings not only indicate a significant disconnect between data practices and normative user expectations, but also raise serious questions about platform compliance with core GDPR principles, such as purpose limitation, data minimization, and transparency.

</details>


### [199] [Making Bias Non-Predictive: Training Robust LLM Judges via Reinforcement Learning](https://arxiv.org/abs/2602.01528)
*Qian Wang,Xuandong Zhao,Zirui Zhang,Zhanzhi Lou,Nuo Chen,Dawn Song,Bingsheng He*

Main category: cs.CY

TL;DR: 提出Epistemic Independence Training (EIT)强化学习框架，通过使偏见线索对奖励不可预测来提升LLM作为自动评判时的认知独立性，减少共识、权威等提示级偏见影响。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型作为自动评判时容易受到认知偏见影响（如共识声明、权威诉求等提示级线索），现有通过提示或监督微调的缓解方法无法泛化，因为它们只改变表面行为而没有改变使偏见线索可预测的优化目标。

Method: 提出Epistemic Independence Training (EIT)强化学习框架，核心原则是：要学习独立性，偏见线索必须对奖励不可预测。通过平衡冲突策略（偏见信号同等可能支持正确和错误答案）结合奖励设计（惩罚偏见跟随但不奖励偏见一致）来实现。

Result: 在Qwen3-4B上的实验表明，EIT提高了准确性和对抗性偏见下的鲁棒性，同时在偏见与真相一致时保持性能。值得注意的是，仅在从众偏见上训练的模型能泛化到未见过的偏见类型（如权威和分心），表明EIT诱导了可转移的认知独立性而非偏见特定启发式。

Conclusion: EIT通过使偏见线索对奖励不可预测的强化学习框架，有效提升了LLM作为自动评判时的认知独立性，具有跨偏见类型的泛化能力，为解决LLM认知偏见问题提供了新思路。

Abstract: Large language models (LLMs) increasingly serve as automated judges, yet they remain susceptible to cognitive biases -- often altering their reasoning when faced with spurious prompt-level cues such as consensus claims or authority appeals. Existing mitigations via prompting or supervised fine-tuning fail to generalize, as they modify surface behavior without changing the optimization objective that makes bias cues predictive. To address this gap, we propose Epistemic Independence Training (EIT), a reinforcement learning framework grounded in a key principle: to learn independence, bias cues must be made non-predictive of reward. EIT operationalizes this through a balanced conflict strategy where bias signals are equally likely to support correct and incorrect answers, combined with a reward design that penalizes bias-following without rewarding bias agreement. Experiments on Qwen3-4B demonstrate that EIT improves both accuracy and robustness under adversarial biases, while preserving performance when bias aligns with truth. Notably, models trained only on bandwagon bias generalize to unseen bias types such as authority and distraction, indicating that EIT induces transferable epistemic independence rather than bias-specific heuristics. Code and data are available at https://anonymous.4open.science/r/bias-mitigation-with-rl-BC47.

</details>


### [200] [DrawSim-PD: Simulating Student Science Drawings to Support NGSS-Aligned Teacher Diagnostic Reasoning](https://arxiv.org/abs/2602.01578)
*Arijit Chakma,Peng He,Honglu Liu,Zeyuan Wang,Tingting Li,Tiffany D. Do,Feng Liu*

Main category: cs.CY

TL;DR: DrawSim-PD：首个生成式框架，模拟NGSS对齐的学生科学绘图，展示可控的教学缺陷，用于教师专业发展。


<details>
  <summary>Details</summary>
Motivation: 诊断推理专业发展需要多样化的学生作品实践，但隐私法规限制了真实学生作品的大规模共享，导致教师培训数据稀缺。

Method: 提出能力档案结构化认知状态，编码不同表现水平学生的能力范围，确保生成输出的跨模态一致性：学生式绘图、第一人称推理叙述和教师诊断概念图。

Result: 基于100个NGSS主题构建了10,000个系统化结构化工件，专家评估显示超过84%核心项目符合NGSS期望，对解释学生思维具有实用性。

Conclusion: DrawSim-PD为视觉评估研究克服数据稀缺障碍提供了开放基础设施，支持教师培训的规模化发展。

Abstract: Developing expertise in diagnostic reasoning requires practice with diverse student artifacts, yet privacy regulations prohibit sharing authentic student work for teacher professional development (PD) at scale. We present DrawSim-PD, the first generative framework that simulates NGSS-aligned, student-like science drawings exhibiting controllable pedagogical imperfections to support teacher training. Central to our approach are apability profiles--structured cognitive states encoding what students at each performance level can and cannot yet demonstrate. These profiles ensure cross-modal coherence across generated outputs: (i) a student-like drawing, (ii) a first-person reasoning narrative, and (iii) a teacher-facing diagnostic concept map. Using 100 curated NGSS topics spanning K-12, we construct a corpus of 10,000 systematically structured artifacts. Through an expert-based feasibility evaluation, K--12 science educators verified the artifacts' alignment with NGSS expectations (>84% positive on core items) and utility for interpreting student thinking, while identifying refinement opportunities for grade-band extremes. We release this open infrastructure to overcome data scarcity barriers in visual assessment research.

</details>


### [201] [Multi-party Computation Protocols for Post-Market Fairness Monitoring in Algorithmic Hiring: From Legal Requirements to Computational Designs](https://arxiv.org/abs/2602.01837)
*Changyang He,Nina Baranowska,Josu Andoni Eguíluz Castañeira,Guillem Escriba,Matthias Juentgen,Anna Via,Frederik Zuiderveen Borgesius,Asia Biega*

Main category: cs.CY

TL;DR: 本文提出了一种基于多方计算（MPC）的合规性后市场公平性监控方案，用于AI招聘系统，通过技术、法律和工业的协同设计方法，在真实工业环境中验证了可行性。


<details>
  <summary>Details</summary>
Motivation: 随着欧盟AI法案等法规要求对高风险AI系统进行后市场公平性监控，但监控需要访问敏感个人数据，这与数据保护法存在冲突。MPC技术虽然能安全计算公平性指标，但在实际招聘场景中的法律合规性、工业可行性和可用性方面仍存在未知。

Method: 采用协同设计方法，整合技术、法律和工业专业知识，识别MPC公平性监控的实际设计要求，开发覆盖完整数据生命周期的端到端法律合规协议，并在大规模工业环境中进行实证验证。

Result: 研究提供了可操作的设计见解，以及部署MPC后市场公平性监控的法律和工业影响，验证了在真实工业环境中的可行性。

Conclusion: MPC为基础的后市场公平性监控在算法招聘系统中具有实际部署潜力，通过技术、法律和工业的协同设计可以实现合规有效的公平性监控。

Abstract: Post-market fairness monitoring is now mandated to ensure fairness and accountability for high-risk employment AI systems under emerging regulations such as the EU AI Act. However, effective fairness monitoring often requires access to sensitive personal data, which is subject to strict legal protections under data protection law. Multi-party computation (MPC) offers a promising technical foundation for compliant post-market fairness monitoring, enabling the secure computation of fairness metrics without revealing sensitive attributes. Despite growing technical interest, the operationalization of MPC-based fairness monitoring in real-world hiring contexts under concrete legal, industrial, and usability constraints remains unknown. This work addresses this gap through a co-design approach integrating technical, legal, and industrial expertise. We identify practical design requirements for MPC-based fairness monitoring, develop an end-to-end, legally compliant protocol spanning the full data lifecycle, and empirically validate it in a large-scale industrial setting. Our findings provide actionable design insights as well as legal and industrial implications for deploying MPC-based post-market fairness monitoring in algorithmic hiring systems.

</details>


### [202] [MetaCLASS: Metacognitive Coaching for Learning with Adaptive Self-regulation Support](https://arxiv.org/abs/2602.02457)
*Naiming Liu,Richard Baraniuk,Shashank Sonkar*

Main category: cs.CY

TL;DR: MetaCLASS框架将元认知辅导建模为11种可解释动作的选择，通过两阶段方法（规划+生成）创建标注对话数据集，研究发现LLMs在预测辅导动作时准确率低且存在过度干预偏见


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型能生成流畅解释，但有效辅导需要支持学习者的思维过程而非仅传递内容。元认知辅导通过规划、监控、调试和评估来填补这一空白，并基于学习者信号和轨迹决定何时活跃或保持最小存在

Method: 提出MetaCLASS框架，将元认知辅导建模为11种与自我调节学习过程对齐的可解释动作的选择。采用两阶段框架：首先基于学习者特征（校准、求助行为）规划教学轨迹，然后生成符合该计划的自然对话。创建了1,015个对话（7,711轮）的数据集，标注了轮级元认知标签，并验证了教学连续性和轨迹一致性

Result: 在9个LLMs上测试预测下一个教练动作的能力，最佳模型准确率仅43.2%。模型表现出强迫干预偏见：在41.7%需要保持沉默的情况下，模型预测"不干预"的比例仅为4.2%，同时严重过度预测高干预动作

Conclusion: 传统的基于内容的辅导能力无法转化为元认知辅导能力，MetaCLASS可作为开发促进自我调节学习的智能导师的测试平台

Abstract: Large language models can generate fluent explanations, but effective tutoring requires supporting the learner's thought process, not just delivering content. Metacognitive tutoring targets this gap by prompting planning, monitoring, debugging, and evaluation, and crucially, deciding when to be active versus minimally present, based on learner signals and trajectory. We introduce MetaCLASS, a learning-science grounded framework that formulates metacognitive tutoring as move selection over 11 interpretable actions aligned to self-regulated learning processes. MetaCLASS uses a two-phase framework that first plans a pedagogical trajectory conditioned on learner profiles (calibration, help-seeking) and then generates natural dialogue consistent with that plan. This yields a dataset of 1,015 conversations (7,711 turns) annotated with turn-level metacognitive labels, and validated for pedagogical contingency and trajectory adherence. We benchmark nine LLMs on predicting the next coach move given the problem and dialogue context. The best model achieves only 43.2\% accuracy, and models exhibit compulsive intervention bias: in turns where effective metacognitive tutoring requires silent (41.7\% of cases), models predict `no intervention' only 4.2\% of the time, while severely over-predicting high-intervention moves. These results show that traditional content-based tutoring ability does not translate to metacognitive tutoring competence, positioning MetaCLASS as a testbed for developing intelligent tutors that promote self-regulated learning.

</details>

<div id=toc></div>

# Table of Contents

- [econ.EM](#econ.EM) [Total: 3]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.AI](#cs.AI) [Total: 16]
- [cs.CY](#cs.CY) [Total: 5]
- [stat.AP](#stat.AP) [Total: 6]


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [1] [Optimal Screening in Experiments with Partial Compliance](https://arxiv.org/abs/2512.09206)
*Christopher Carter,Adeline Delavande,Mario Fiorini,Peter Siminski,Patrick Vu*

Main category: econ.EM

TL;DR: 研究在部分依从性下，实验者可在随机化前筛选参与者时的最优实验设计。理论表明，保留所有依从者并筛选掉所有非依从者可同时实现三个目标：保持LATE估计不变、最小化中位数偏差、最大化统计功效。


<details>
  <summary>Details</summary>
Motivation: 在随机对照试验中，部分参与者可能不遵守实验安排（非依从性），这会降低实验效果估计的准确性。当实验者能够在随机化前筛选参与者时，如何设计最优的筛选策略来提高实验质量成为一个重要问题。

Method: 通过理论分析证明最优筛选策略的性质，讨论可行的筛选方法（尽管依从状态不可观测），并提出一个简单的筛选效果检验方法。未来将通过实验验证最优筛选设计的可行性和优势。

Result: 理论结果表明，保留所有依从者并筛选掉所有非依从者的策略能同时实现三个目标：(1)局部平均处理效应与无筛选的标准2SLS估计相同；(2)中位数偏差最小化；(3)统计功效最大化。

Conclusion: 在部分依从性情况下，通过筛选参与者可以显著提高实验设计的质量。虽然依从状态不可观测，但通过可行的筛选策略和检验方法，可以在实践中应用这一最优设计。未来的实验研究将进一步验证其实际效果。

Abstract: This note studies optimal experimental design under partial compliance when experimenters can screen participants prior to randomization. Theoretical results show that retaining all compliers and screening out all non-compliers achieves three complementary aims: (i) the Local Average Treatment Effect is the same as the standard 2SLS estimator with no screening; (ii) median bias is minimized; and (iii) statistical power is maximized. In practice, complier status is unobserved. We therefore discuss feasible screening strategies and propose a simple test for screening efficacy. Future work will conduct an experiment to demonstrate the feasibility and advantages of the optimal screening design.

</details>


### [2] [Debiased Bayesian Inference for High-dimensional Regression Models](https://arxiv.org/abs/2512.09257)
*Qihui Chen,Zheng Fang,Ruixuan Liu*

Main category: econ.EM

TL;DR: 提出了一种新的去偏方法，修正高维回归中稀疏诱导先验贝叶斯后验的偏差，确保频率论有效性


<details>
  <summary>Details</summary>
Motivation: 虽然基于稀疏诱导先验（如spike-and-slab和horseshoe型）的高维回归贝叶斯推断取得显著进展，但得到的后验通常不具备理想的频率论性质，置信集不能作为有效的置信集（即使渐近意义上）

Method: 引入一种新颖的去偏方法，修正整个贝叶斯后验分布的偏差，建立新的Bernstein-von Mises定理

Result: 通过蒙特卡洛模拟和经济学中的两个实证应用展示了所提方法的实际性能

Conclusion: 提出的去偏后验保证了频率论有效性，使贝叶斯可信集能够作为有效的置信集

Abstract: There has been significant progress in Bayesian inference based on sparsity-inducing (e.g., spike-and-slab and horseshoe-type) priors for high-dimensional regression models. The resulting posteriors, however, in general do not possess desirable frequentist properties, and the credible sets thus cannot serve as valid confidence sets even asymptotically. We introduce a novel debiasing approach that corrects the bias for the entire Bayesian posterior distribution. We establish a new Bernstein-von Mises theorem that guarantees the frequentist validity of the debiased posterior. We demonstrate the practical performance of our proposal through Monte Carlo simulations and two empirical applications in economics.

</details>


### [3] [New Approximation Results and Optimal Estimation for Fully Connected Deep Neural Networks](https://arxiv.org/abs/2512.09853)
*Zhaoji Tang*

Main category: econ.EM

TL;DR: 本文改进了Farrell等人(2021)关于深度前馈神经网络估计器的收敛率，通过推导更窄的全连接深度神经网络的近似界，将次优收敛率提升到最优率（除对数因子外），并展示了深度神经网络估计器能缓解组合结构和流形上函数的维度诅咒。


<details>
  <summary>Details</summary>
Motivation: Farrell等人(2021)建立了深度前馈神经网络估计器的非渐近高概率界，但其定理1对全连接前馈网络获得了次优收敛率。作者认为改进全连接网络的近似能力可以在不改变理论框架的情况下获得更尖锐的版本。

Method: 通过推导特定更窄全连接深度神经网络的近似界，改进Farrell等人(2021)的定理1。具体来说，针对更窄的网络结构建立更好的近似理论结果。

Result: 将Farrell等人(2021)定理1的收敛率改进到最优率（除对数因子外）。同时简要展示了深度神经网络估计器能够缓解具有组合结构的函数和定义在流形上的函数的维度诅咒问题。

Conclusion: 通过改进全连接深度神经网络的近似界，可以获得最优收敛率的理论保证，并且深度神经网络估计器在处理具有特定结构（如组合结构或流形定义）的函数时能够有效缓解维度诅咒问题。

Abstract: \citet{farrell2021deep} establish non-asymptotic high-probability bounds for general deep feedforward neural network (with rectified linear unit activation function) estimators, with \citet[Theorem 1]{farrell2021deep} achieving a suboptimal convergence rate for fully connected feedforward networks. The authors suggest that improved approximation of fully connected networks could yield sharper versions of \citet[Theorem 1]{farrell2021deep} without altering the theoretical framework. By deriving approximation bounds specifically for a narrower fully connected deep neural network, this note demonstrates that \citet[Theorem 1]{farrell2021deep} can be improved to achieve an optimal rate (up to a logarithmic factor). Furthermore, this note briefly shows that deep neural network estimators can mitigate the curse of dimensionality for functions with compositional structure and functions defined on manifolds.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [4] [Knowledge Graph Enrichment and Reasoning for Nobel Laureates](https://arxiv.org/abs/2512.09707)
*Thanh-Lam T. Nguyen,Ngoc-Quang Le,Thu-Trang Pham,Mai-Vu Tran*

Main category: cs.SI

TL;DR: 构建诺贝尔奖知识图谱，结合LLM数据增强、社交网络分析和GraphRAG聊天机器人，实现自然语言查询和复杂推理


<details>
  <summary>Details</summary>
Motivation: 通过整合维基百科传记信息丰富诺贝尔奖数据集，构建全面知识图谱，以揭示科学界隐藏模式并支持复杂推理任务

Method: 1) 使用LLM进行NER和关系抽取的自动数据增强；2) 社交网络分析发现模式；3) 开发基于GraphRAG的聊天机器人，使用微调模型进行Text2Cypher翻译

Result: 增强后的图谱具有小世界网络特性，识别出关键影响人物和核心组织；聊天机器人在自定义多选题评估数据集上达到竞争性准确率

Conclusion: LLM与结构化知识库结合对复杂推理任务有效，项目数据和代码已开源，为科学网络分析提供新工具

Abstract: This project aims to construct and analyze a comprehensive knowledge graph of Nobel Prize and Laureates by enriching existing datasets with biographical information extracted from Wikipedia. Our approach integrates multiple advanced techniques, consisting of automatic data augmentation using LLMs for Named Entity Recognition (NER) and Relation Extraction (RE) tasks, and social network analysis to uncover hidden patterns within the scientific community. Furthermore, we also develop a GraphRAG-based chatbot system utilizing a fine-tuned model for Text2Cypher translation, enabling natural language querying over the knowledge graph. Experimental results demonstrate that the enriched graph possesses small-world network properties, identifying key influential figures and central organizations. The chatbot system achieves a competitive accuracy on a custom multiple-choice evaluation dataset, proving the effectiveness of combining LLMs with structured knowledge bases for complex reasoning tasks. Data and source code are available at: https://github.com/tlam25/network-of-awards-and-winners.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [5] [Interpretation as Linear Transformation: A Cognitive-Geometric Model of Belief and Meaning](https://arxiv.org/abs/2512.09831)
*Chainarong Amornbunchornvej*

Main category: cs.AI

TL;DR: 本文提出了一个几何框架来建模认知异质智能体之间的信念、动机和影响，将智能体表示为个性化价值空间，信念作为结构化向量，其传播受线性解释映射调节，只有避开这些映射零空间的信念才能在交流中存活。


<details>
  <summary>Details</summary>
Motivation: 本文的动机是建立一个统一的几何框架来分析认知异质智能体之间的信念传播和影响机制。传统方法通常基于共享信息或理性假设，但现实中智能体具有不同的认知结构和价值体系，导致信念传播过程中的扭曲、误解和消失。作者希望从结构兼容性的角度解释这些现象，为理解人类和人工智能系统中的信念动态提供基础。

Method: 方法是将每个智能体建模为个性化价值空间（向量空间），该空间编码了智能体解释和评估意义的内部维度。信念被形式化为结构化向量（抽象存在），其传播通过线性解释映射来调节。作者开发了代数约束来分析信念传播，提出了"无零空间领导条件"等核心概念，将领导力表征为表示可达性而非说服或权威的属性。

Result: 主要结果包括：1) 信念只有在避开解释映射的零空间时才能在交流中存活，这为可理解性、误解和信念消亡提供了结构标准；2) 信念扭曲、动机漂移、反事实评估和相互理解的限制都源于纯粹的代数约束；3) 领导力被重新定义为表示可达性属性；4) 解释了抽象存在如何在多样认知几何中传播、变异或消失。

Conclusion: 该框架通过将意义保存建立在结构兼容性而非共享信息或理性的基础上，统一了概念空间、社会认识论和AI价值对齐的见解。这种认知几何视角阐明了人类和人工系统中影响的认知边界，为分析异质智能体间的信念动态提供了通用基础，对理解信念传播、影响机制和认知异质性具有重要意义。

Abstract: This paper develops a geometric framework for modeling belief, motivation, and influence across cognitively heterogeneous agents. Each agent is represented by a personalized value space, a vector space encoding the internal dimensions through which the agent interprets and evaluates meaning. Beliefs are formalized as structured vectors-abstract beings-whose transmission is mediated by linear interpretation maps. A belief survives communication only if it avoids the null spaces of these maps, yielding a structural criterion for intelligibility, miscommunication, and belief death.
  Within this framework, I show how belief distortion, motivational drift, counterfactual evaluation, and the limits of mutual understanding arise from purely algebraic constraints. A central result-"the No-Null-Space Leadership Condition"-characterizes leadership as a property of representational reachability rather than persuasion or authority. More broadly, the model explains how abstract beings can propagate, mutate, or disappear as they traverse diverse cognitive geometries.
  The account unifies insights from conceptual spaces, social epistemology, and AI value alignment by grounding meaning preservation in structural compatibility rather than shared information or rationality. I argue that this cognitive-geometric perspective clarifies the epistemic boundaries of influence in both human and artificial systems, and offers a general foundation for analyzing belief dynamics across heterogeneous agents.

</details>


### [6] [Calibrated Trust in Dealing with LLM Hallucinations: A Qualitative Study](https://arxiv.org/abs/2512.09088)
*Adrian Ryser,Florian Allwein,Tim Schlippe*

Main category: cs.AI

TL;DR: 研究大语言模型幻觉如何影响用户信任，发现幻觉不会导致全面不信任，而是引发情境敏感的信任校准，识别了直觉作为新的信任因素，并提出了负责任使用建议。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型产生的幻觉（事实错误但看似合理）如何影响用户对LLM的信任以及用户与LLM的互动，探索日常使用中的信任动态。

Method: 对192名参与者进行定性研究，基于Lee & See的校准信任模型和Afroogh等人的信任相关因素框架，分析幻觉对用户信任的影响。

Result: 幻觉不会导致全面不信任，而是引发情境敏感的信任校准；确认了期望、先前经验、用户专业知识等信任因素，并识别直觉作为新的幻觉检测因素；信任动态还受感知风险和决策风险等情境因素影响。

Conclusion: 验证了递归信任校准过程，并将直觉作为用户相关信任因素纳入；基于研究发现提出了负责任和反思性使用LLM的实用建议。

Abstract: Hallucinations are outputs by Large Language Models (LLMs) that are factually incorrect yet appear plausible [1]. This paper investigates how such hallucinations influence users' trust in LLMs and users' interaction with LLMs. To explore this in everyday use, we conducted a qualitative study with 192 participants. Our findings show that hallucinations do not result in blanket mistrust but instead lead to context-sensitive trust calibration. Building on the calibrated trust model by Lee & See [2] and Afroogh et al.'s trust-related factors [3], we confirm expectancy [3], [4], prior experience [3], [4], [5], and user expertise & domain knowledge [3], [4] as userrelated (human) trust factors, and identify intuition as an additional factor relevant for hallucination detection. Additionally, we found that trust dynamics are further influenced by contextual factors, particularly perceived risk [3] and decision stakes [6]. Consequently, we validate the recursive trust calibration process proposed by Blöbaum [7] and extend it by including intuition as a user-related trust factor. Based on these insights, we propose practical recommendations for responsible and reflective LLM use.

</details>


### [7] [AI TIPS 2.0: A Comprehensive Framework for Operationalizing AI Governance](https://arxiv.org/abs/2512.09114)
*Pamela Gupta*

Main category: cs.AI

TL;DR: AI TIPS 2.0框架解决现有AI治理框架的三个关键缺陷：用例级风险评估不足、原则缺乏可操作控制、规模化实施机制缺失。


<details>
  <summary>Details</summary>
Motivation: 当前AI治理框架存在三个关键问题：1) 缺乏针对具体用例的风险评估，导致像Humana集体诉讼这样的高风险事件；2) 现有框架如ISO 42001和NIST AI RMF停留在概念层面，缺乏可操作的控制措施；3) 组织缺乏规模化实施治理的机制，无法将可信AI实践嵌入整个开发生命周期。

Method: 提出AI TIPS 2.0（人工智能可信集成支柱可持续性框架），这是对2019年开发的综合操作框架的更新，该框架比NIST的AI风险管理框架早了四年。该框架直接针对上述三个挑战提供解决方案。

Result: AI TIPS 2.0框架能够：1) 提供针对具体用例的风险评估方法；2) 将治理要求转化为具体的技术实施；3) 建立规模化操作治理的机制，在整个开发生命周期中嵌入可信AI实践，量化测量合规性，并为从董事会到数据科学家的不同角色提供适当的可见性。

Conclusion: AI TIPS 2.0框架解决了当前AI治理框架的关键缺陷，提供了可操作、可扩展的解决方案，能够帮助组织有效管理AI部署中的风险，实现可信AI的规模化实施。

Abstract: The deployment of AI systems faces three critical governance challenges that current frameworks fail to adequately address. First, organizations struggle with inadequate risk assessment at the use case level, exemplified by the Humana class action lawsuit and other high impact cases where an AI system deployed to production exhibited both significant bias and high error rates, resulting in improper healthcare claim denials. Each AI use case presents unique risk profiles requiring tailored governance, yet most frameworks provide one size fits all guidance. Second, existing frameworks like ISO 42001 and NIST AI RMF remain at high conceptual levels, offering principles without actionable controls, leaving practitioners unable to translate governance requirements into specific technical implementations. Third, organizations lack mechanisms for operationalizing governance at scale, with no systematic approach to embed trustworthy AI practices throughout the development lifecycle, measure compliance quantitatively, or provide role-appropriate visibility from boards to data scientists. We present AI TIPS, Artificial Intelligence Trust-Integrated Pillars for Sustainability 2.0, update to the comprehensive operational framework developed in 2019,four years before NIST's AI Risk Management Framework, that directly addresses these challenges.

</details>


### [8] [A Categorical Analysis of Large Language Models and Why LLMs Circumvent the Symbol Grounding Problem](https://arxiv.org/abs/2512.09117)
*Luciano Floridi,Yiyang Jia,Fernando Tohmé*

Main category: cs.AI

TL;DR: 该论文提出了一个形式化的范畴论框架，用于分析人类和大型语言模型如何将内容转化为关于可能世界状态空间W的真值可评估命题，并论证LLMs并未解决而是绕过了符号接地问题。


<details>
  <summary>Details</summary>
Motivation: 论文的动机是深入理解人类和大型语言模型在将内容转化为真值命题方面的根本差异，特别是针对符号接地问题这一AI和认知科学中的核心挑战。作者希望通过形式化分析揭示LLMs处理符号意义的方式与人类认知的本质区别。

Method: 作者采用范畴论作为形式化工具，构建了一个分析框架，用于建模内容到真值命题的转化过程。该方法将可能世界状态空间W作为基础，通过范畴论的结构来描述人类和LLMs如何在这个状态空间中建立内容与真值命题之间的关系。

Result: 分析结果表明，大型语言模型并没有真正解决符号接地问题，而是通过统计模式匹配和语言内部关系来"绕过"这个问题。LLMs缺乏对人类认知中那种将符号与现实世界状态直接连接的能力，它们处理的是符号之间的关联而非符号与外部世界的直接对应关系。

Conclusion: 论文得出结论：大型语言模型通过不同的机制处理符号意义，它们并没有像人类那样解决符号接地问题，而是发展出了自己的替代策略。这一发现对理解LLMs的能力局限性和未来AI系统的发展方向具有重要意义。

Abstract: This paper presents a formal, categorical framework for analysing how humans and large language models (LLMs) transform content into truth-evaluated propositions about a state space of possible worlds W , in order to argue that LLMs do not solve but circumvent the symbol grounding problem.

</details>


### [9] [SDialog: A Python Toolkit for End-to-End Agent Building, User Simulation, Dialog Generation, and Evaluation](https://arxiv.org/abs/2512.09142)
*Sergio Burdisso,Séverin Baroudi,Yanis Labrak,David Grunert,Pawel Cyrta,Yiyang Chen,Srikanth Madikeri,Esaú Villatoro-Tello,Thomas Schaaf,Ricard Marxer,Petr Motlicek*

Main category: cs.AI

TL;DR: SDialog是一个开源的Python工具包，集成了对话生成、评估和机制可解释性，用于构建和分析基于LLM的对话系统。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏一个统一的框架来系统性地构建、评估和理解基于大语言模型的对话系统，研究人员需要分别处理生成、评估和可解释性分析。

Method: 围绕标准化的Dialog表示构建，提供：1）基于角色的多智能体模拟和可组合编排；2）综合评估（语言指标、LLM作为评判者、功能正确性验证）；3）机制可解释性工具（激活检查、特征消融和诱导）；4）音频生成和声学模拟。

Result: 开发了一个端到端的开源工具包，支持所有主流LLM后端，提供统一的API，实现了生成、评估和可解释性的紧密集成。

Conclusion: SDialog通过对话为中心的架构，将生成、评估和可解释性耦合在一起，使研究人员能够更系统化地构建、基准测试和理解对话系统。

Abstract: We present SDialog, an MIT-licensed open-source Python toolkit that unifies dialog generation, evaluation and mechanistic interpretability into a single end-to-end framework for building and analyzing LLM-based conversational agents. Built around a standardized \texttt{Dialog} representation, SDialog provides: (1) persona-driven multi-agent simulation with composable orchestration for controlled, synthetic dialog generation, (2) comprehensive evaluation combining linguistic metrics, LLM-as-a-judge and functional correctness validators, (3) mechanistic interpretability tools for activation inspection and steering via feature ablation and induction, and (4) audio generation with full acoustic simulation including 3D room modeling and microphone effects. The toolkit integrates with all major LLM backends, enabling mixed-backend experiments under a unified API. By coupling generation, evaluation, and interpretability in a dialog-centric architecture, SDialog enables researchers to build, benchmark and understand conversational systems more systematically.

</details>


### [10] [Visual Categorization Across Minds and Models: Cognitive Analysis of Human Labeling and Neuro-Symbolic Integration](https://arxiv.org/abs/2512.09340)
*Chethana Prasad Kabgere*

Main category: cs.AI

TL;DR: 该论文对比了人类与AI系统在模糊视觉刺激下的图像标注表现，揭示了生物与人工系统在表征、推理和置信度校准方面的异同，并提出了未来神经符号架构的发展方向。


<details>
  <summary>Details</summary>
Motivation: 研究人类与AI系统如何解释模糊视觉刺激，这对于理解感知、推理和决策的本质至关重要。通过对比两者的处理方式，可以揭示生物与人工系统在认知机制上的根本差异。

Method: 结合计算认知科学、认知架构和连接主义-符号混合模型，对比人类策略（类比推理、形状识别、置信度调节）与AI的特征处理。使用Grad-CAM可视化模型注意力，并通过ACT-R和Soar等认知架构解释人类行为。

Result: 研究发现人类与AI系统在低分辨率、感知退化刺激下的图像标注表现存在显著差异。人类采用分层启发式决策策略，而AI主要依赖特征处理。两者在表征、推理和置信度校准方面既有相似之处也有重要分歧。

Conclusion: 研究强调了未来神经符号架构的重要性，这种架构应统一结构化符号推理与连接主义表征，并体现具身性、可解释性和认知对齐原则，以开发既高效又可解释且认知基础扎实的AI系统。

Abstract: Understanding how humans and AI systems interpret ambiguous visual stimuli offers critical insight into the nature of perception, reasoning, and decision-making. This paper examines image labeling performance across human participants and deep neural networks, focusing on low-resolution, perceptually degraded stimuli. Drawing from computational cognitive science, cognitive architectures, and connectionist-symbolic hybrid models, we contrast human strategies such as analogical reasoning, shape-based recognition, and confidence modulation with AI's feature-based processing. Grounded in Marr's tri-level hypothesis, Simon's bounded rationality, and Thagard's frameworks of representation and emotion, we analyze participant responses in relation to Grad-CAM visualizations of model attention. Human behavior is further interpreted through cognitive principles modeled in ACT-R and Soar, revealing layered and heuristic decision strategies under uncertainty. Our findings highlight key parallels and divergences between biological and artificial systems in representation, inference, and confidence calibration. The analysis motivates future neuro-symbolic architectures that unify structured symbolic reasoning with connectionist representations. Such architectures, informed by principles of embodiment, explainability, and cognitive alignment, offer a path toward AI systems that are not only performant but also interpretable and cognitively grounded.

</details>


### [11] [Architectures for Building Agentic AI](https://arxiv.org/abs/2512.09458)
*Sławomir Nowaczyk*

Main category: cs.AI

TL;DR: 本文认为AI代理系统的可靠性主要是一个架构属性，通过组件化、接口规范和显式控制循环来实现，提出了实用的代理分类和设计指导原则。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理系统缺乏系统性的可靠性设计框架，需要建立基于架构原则的可靠性保障方法，以应对复杂环境中的失败风险。

Method: 通过定义代理系统架构组件（目标管理器、规划器、工具路由器、执行器等），建立接口规范（模式约束、验证、最小权限），设计显式控制和保障循环，并提出实用的代理分类框架。

Result: 建立了基于架构的可靠性设计框架，提出了代理系统分类（工具使用代理、记忆增强代理、规划与自我改进代理、多代理系统、具身或网络代理），并分析了每种模式的可靠性边界和失败模式。

Conclusion: AI代理系统的可靠性可以通过架构设计实现，包括组件化、接口规范、控制循环等原则，为构建可靠的AI系统提供了具体的设计指导。

Abstract: This chapter argues that the reliability of agentic and generative AI is chiefly an architectural property. We define agentic systems as goal-directed, tool-using decision makers operating in closed loops, and show how reliability emerges from principled componentisation (goal manager, planner, tool-router, executor, memory, verifiers, safety monitor, telemetry), disciplined interfaces (schema-constrained, validated, least-privilege tool calls), and explicit control and assurance loops. Building on classical foundations, we propose a practical taxonomy-tool-using agents, memory-augmented agents, planning and self-improvement agents, multi-agent systems, and embodied or web agents - and analyse how each pattern reshapes the reliability envelope and failure modes. We distil design guidance on typed schemas, idempotency, permissioning, transactional semantics, memory provenance and hygiene, runtime governance (budgets, termination conditions), and simulate-before-actuate safeguards.

</details>


### [12] [Toward Closed-loop Molecular Discovery via Language Model, Property Alignment and Strategic Search](https://arxiv.org/abs/2512.09566)
*Junkai Ji,Zhangfan Yang,Dong Xu,Ruibin Bai,Jianqiang Li,Tingjun Hou,Zexuan Zhu*

Main category: cs.AI

TL;DR: Trio是一个整合片段语言建模、强化学习和蒙特卡洛树搜索的分子生成框架，用于高效可解释的靶向分子设计，在结合亲和力、类药性和合成可行性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统药物发现方法（高通量筛选和基于对接的虚拟筛选）成功率低且可扩展性有限。现有的生成模型存在泛化能力不足、可解释性差、过度关注结合亲和力而忽视关键药理学性质等问题，限制了其实际应用价值。

Method: Trio框架整合三个关键组件：1）基于片段的分子语言建模实现上下文感知的片段组装；2）强化学习确保物理化学和合成可行性；3）蒙特卡洛树搜索平衡新颖化学型探索和蛋白质结合口袋中有前景中间体的利用。

Result: 实验结果显示Trio可靠地生成化学有效且药理学增强的配体，在结合亲和力（+7.85%）、类药性（+11.10%）和合成可行性（+12.05%）方面优于最先进方法，同时将分子多样性扩展了四倍以上。

Conclusion: Trio提供了一个有效且可解释的闭环靶向分子设计框架，通过整合片段语言建模、强化学习和蒙特卡洛树搜索，解决了现有生成模型的局限性，为药物发现提供了更实用的工具。

Abstract: Drug discovery is a time-consuming and expensive process, with traditional high-throughput and docking-based virtual screening hampered by low success rates and limited scalability. Recent advances in generative modelling, including autoregressive, diffusion, and flow-based approaches, have enabled de novo ligand design beyond the limits of enumerative screening. Yet these models often suffer from inadequate generalization, limited interpretability, and an overemphasis on binding affinity at the expense of key pharmacological properties, thereby restricting their translational utility. Here we present Trio, a molecular generation framework integrating fragment-based molecular language modeling, reinforcement learning, and Monte Carlo tree search, for effective and interpretable closed-loop targeted molecular design. Through the three key components, Trio enables context-aware fragment assembly, enforces physicochemical and synthetic feasibility, and guides a balanced search between the exploration of novel chemotypes and the exploitation of promising intermediates within protein binding pockets. Experimental results show that Trio reliably achieves chemically valid and pharmacologically enhanced ligands, outperforming state-of-the-art approaches with improved binding affinity (+7.85%), drug-likeness (+11.10%) and synthetic accessibility (+12.05%), while expanding molecular diversity more than fourfold.

</details>


### [13] [Comparing AI Agents to Cybersecurity Professionals in Real-World Penetration Testing](https://arxiv.org/abs/2512.09882)
*Justin W. Lin,Eliot Krzysztof Jones,Donovan Julian Jasper,Ethan Jun-shen Ho,Anna Wu,Arnold Tianyi Yang,Neil Perry,Andy Zou,Matt Fredrikson,J. Zico Kolter,Percy Liang,Dan Boneh,Daniel E. Ho*

Main category: cs.AI

TL;DR: 首次在真实企业环境中全面评估AI代理与人类网络安全专家的表现，新框架ARTEMIS在漏洞发现方面排名第二，超越9/10人类参与者


<details>
  <summary>Details</summary>
Motivation: 评估AI代理在真实网络安全环境中的实际能力，与人类专业人员进行对比，了解AI在网络安全领域的优势和局限性

Method: 在包含约8000台主机、12个子网的大型大学网络中，评估10名网络安全专家、6个现有AI代理和新开发的ARTEMIS多代理框架。ARTEMIS具有动态提示生成、任意子代理和自动漏洞分类功能

Result: ARTEMIS总体排名第二，发现9个有效漏洞，提交有效率为82%，超越9/10人类参与者。现有框架如Codex和CyAgent表现不如大多数人类参与者。AI代理在系统枚举、并行利用和成本方面具有优势（ARTEMIS变体成本为18美元/小时，人类渗透测试者为60美元/小时），但存在较高误报率和GUI任务处理困难的问题

Conclusion: AI代理在网络安全领域已展现出与顶尖人类专家相当的技术成熟度和提交质量，具有成本效益和规模化优势，但仍需解决误报率和GUI交互等关键能力差距

Abstract: We present the first comprehensive evaluation of AI agents against human cybersecurity professionals in a live enterprise environment. We evaluate ten cybersecurity professionals alongside six existing AI agents and ARTEMIS, our new agent scaffold, on a large university network consisting of ~8,000 hosts across 12 subnets. ARTEMIS is a multi-agent framework featuring dynamic prompt generation, arbitrary sub-agents, and automatic vulnerability triaging. In our comparative study, ARTEMIS placed second overall, discovering 9 valid vulnerabilities with an 82% valid submission rate and outperforming 9 of 10 human participants. While existing scaffolds such as Codex and CyAgent underperformed relative to most human participants, ARTEMIS demonstrated technical sophistication and submission quality comparable to the strongest participants. We observe that AI agents offer advantages in systematic enumeration, parallel exploitation, and cost -- certain ARTEMIS variants cost $18/hour versus $60/hour for professional penetration testers. We also identify key capability gaps: AI agents exhibit higher false-positive rates and struggle with GUI-based tasks.

</details>


### [14] [An End-to-end Planning Framework with Agentic LLMs and PDDL](https://arxiv.org/abs/2512.09629)
*Emanuele La Malfa,Ping Zhu,Samuele Marro,Sara Bernardini,Michael Wooldridge*

Main category: cs.AI

TL;DR: 提出一个由LLM驱动的端到端规划框架，通过验证器支持，将自然语言需求自动转换为PDDL模型并生成可执行计划。


<details>
  <summary>Details</summary>
Motivation: 解决传统规划中需要人工将自然语言需求转换为形式化PDDL模型的问题，同时处理人类需求中的模糊性和矛盾，实现完全自动化的规划流程。

Method: 使用编排器接收自然语言需求，通过多个LLM驱动的子模块迭代精炼PDDL域和问题定义，处理时间约束、最优性等规划需求，最后通过外部规划引擎生成计划并翻译回自然语言。

Result: 框架在多个领域和任务中表现出灵活性和有效性，包括Google NaturalPlan基准、PlanBench以及Blocksworld和汉诺塔等LLM传统上难以处理的问题，可与多种PDDL规划引擎集成。

Conclusion: 该框架代表了LLM辅助端到端规划的重要进展，实现了从自然语言需求到可执行计划的完全自动化流程，无需人工干预。

Abstract: We present an end-to-end framework for planning supported by verifiers. An orchestrator receives a human specification written in natural language and converts it into a PDDL (Planning Domain Definition Language) model, where the domain and problem are iteratively refined by sub-modules (agents) to address common planning requirements, such as time constraints and optimality, as well as ambiguities and contradictions that may exist in the human specification. The validated domain and problem are then passed to an external planning engine to generate a plan. The orchestrator and agents are powered by Large Language Models (LLMs) and require no human intervention at any stage of the process. Finally, a module translates the final plan back into natural language to improve human readability while maintaining the correctness of each step. We demonstrate the flexibility and effectiveness of our framework across various domains and tasks, including the Google NaturalPlan benchmark and PlanBench, as well as planning problems like Blocksworld and the Tower of Hanoi (where LLMs are known to struggle even with small instances). Our framework can be integrated with any PDDL planning engine and validator (such as Fast Downward, LPG, POPF, VAL, and uVAL, which we have tested) and represents a significant step toward end-to-end planning aided by LLMs.

</details>


### [15] [Gaussian Process Aggregation for Root-Parallel Monte Carlo Tree Search with Continuous Actions](https://arxiv.org/abs/2512.09727)
*Junlin Xiao,Victor-Alexandru Darvariu,Bruno Lacerda,Nick Hawes*

Main category: cs.AI

TL;DR: 提出使用高斯过程回归来聚合MCTS多线程统计信息的方法，在连续动作空间中提升性能


<details>
  <summary>Details</summary>
Motivation: 在连续动作空间中，如何有效聚合蒙特卡洛树搜索多线程的统计信息是一个重要但未充分探索的问题

Method: 使用高斯过程回归来获取未在环境中试验的有前景动作的价值估计

Result: 在6个不同领域进行系统评估，证明该方法优于现有聚合策略，仅需适度增加推理时间

Conclusion: 高斯过程回归是聚合MCTS多线程统计信息的有效方法，在连续动作空间中能显著提升性能

Abstract: Monte Carlo Tree Search is a cornerstone algorithm for online planning, and its root-parallel variant is widely used when wall clock time is limited but best performance is desired. In environments with continuous action spaces, how to best aggregate statistics from different threads is an important yet underexplored question. In this work, we introduce a method that uses Gaussian Process Regression to obtain value estimates for promising actions that were not trialed in the environment. We perform a systematic evaluation across 6 different domains, demonstrating that our approach outperforms existing aggregation strategies while requiring a modest increase in inference time.

</details>


### [16] [Analyzing Planner Design Trade-offs for MAPF under Realistic Simulation](https://arxiv.org/abs/2512.09736)
*Jingtian Yan,Zhifei Li,William Kang,Stephen F. Smith,Jiaoyang Li*

Main category: cs.AI

TL;DR: 本文研究了多智能体路径规划(MAPF)算法在现实执行环境下的性能影响因素，包括解最优性与执行性能的关系、运动学建模误差的敏感性、以及模型精度与规划最优性的交互作用。


<details>
  <summary>Details</summary>
Motivation: 现有MAPF评估框架通常基于简化的机器人模型，导致算法基准与实际性能之间存在显著差距。虽然近期框架如SMART引入了运动学建模，但缺乏对关键规划器设计选择如何影响现实执行性能的系统研究。

Method: 基于SMART框架的运动学建模能力，系统研究三个基本因素：1)解最优性与执行性能的关系；2)系统性能对运动学建模误差的敏感性；3)模型精度与规划最优性的交互作用。通过实证分析这些因素在现实场景中的影响。

Result: 通过实证研究揭示了关键规划器设计选择对现实执行性能的具体影响，为理解MAPF算法在实际部署中的表现提供了系统分析。

Conclusion: 本文强调了MAPF研究中的开放挑战和研究方向，引导社区朝着实用、现实世界部署的方向发展，填补了算法基准与实际性能之间的差距。

Abstract: Multi-Agent Path Finding (MAPF) algorithms are increasingly deployed in industrial warehouses and automated manufacturing facilities, where robots must operate reliably under real-world physical constraints. However, existing MAPF evaluation frameworks typically rely on simplified robot models, leaving a substantial gap between algorithmic benchmarks and practical performance. Recent frameworks such as SMART, incorporate kinodynamic modeling and offer the MAPF community a platform for large-scale, realistic evaluation. Building on this capability, this work investigates how key planner design choices influence performance under realistic execution settings. We systematically study three fundamental factors: (1) the relationship between solution optimality and execution performance, (2) the sensitivity of system performance to inaccuracies in kinodynamic modeling, and (3) the interaction between model accuracy and plan optimality. Empirically, we examine these factors to understand how these design choices affect performance in realistic scenarios. We highlight open challenges and research directions to steer the community toward practical, real-world deployment.

</details>


### [17] [RIFT: A Scalable Methodology for LLM Accelerator Fault Assessment using Reinforcement Learning](https://arxiv.org/abs/2512.09829)
*Khurram Khalil,Muhammad Mahad Khaliq,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: RIFT使用强化学习自动化发现最小化高影响故障场景，加速AI加速器故障评估，相比进化方法提速2.2倍，相比随机故障注入减少99%测试向量，同时提供更好的故障覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现代AI加速器规模庞大，传统故障评估方法面临计算成本过高和关键故障模式覆盖率不足的问题，需要更高效的故障评估框架。

Method: RIFT将最坏情况故障搜索转化为序列决策问题，结合混合灵敏度分析进行搜索空间剪枝，使用强化学习智能生成最小化高影响测试套件。

Result: 在十亿参数大语言模型工作负载上评估，RIFT相比进化方法实现2.2倍加速，相比随机故障注入减少99%测试向量，同时获得更优故障覆盖率。RIFT引导的选择性错误校正码相比统一三模冗余保护提升12.8倍成本效益。

Conclusion: RIFT为大规模AI加速器提供可扩展的故障评估框架，自动生成UVM兼容验证工件，可直接集成到商业RTL验证流程中，实现高效硬件保护策略。

Abstract: The massive scale of modern AI accelerators presents critical challenges to traditional fault assessment methodologies, which face prohibitive computational costs and provide poor coverage of critical failure modes. This paper introduces RIFT (Reinforcement Learning-guided Intelligent Fault Targeting), a scalable framework that automates the discovery of minimal, high-impact fault scenarios for efficient design-time fault assessment. RIFT transforms the complex search for worst-case faults into a sequential decision-making problem, combining hybrid sensitivity analysis for search space pruning with reinforcement learning to intelligently generate minimal, high-impact test suites. Evaluated on billion-parameter Large Language Model (LLM) workloads using NVIDIA A100 GPUs, RIFT achieves a \textbf{2.2$\times$} fault assessment speedup over evolutionary methods and reduces the required test vector volume by over \textbf{99\%} compared to random fault injection, all while achieving \textbf{superior fault coverage}. The proposed framework also provides actionable data to enable intelligent hardware protection strategies, demonstrating that RIFT-guided selective error correction code provides a \textbf{12.8$\times$} improvement in \textbf{cost-effectiveness} (coverage per unit area) compared to uniform triple modular redundancy protection. RIFT automatically generates UVM-compliant verification artifacts, ensuring its findings are directly actionable and integrable into commercial RTL verification workflows.

</details>


### [18] [Human-in-the-Loop and AI: Crowdsourcing Metadata Vocabulary for Materials Science](https://arxiv.org/abs/2512.09895)
*Jane Greenberg,Scott McClellan,Addy Ireland,Robert Sammarco,Colton Gerber,Christopher B. Rauch,Mat Kelly,John Kunze,Yuan An,Eric Toberer*

Main category: cs.AI

TL;DR: MatSci-YAMZ平台结合AI和人类参与（HILT）来开发材料科学元数据词汇表，通过6名参与者的概念验证，成功生成19个AI定义，证明了该方法的可行性。


<details>
  <summary>Details</summary>
Motivation: 元数据词汇表对FAIR和FARR数据原则至关重要，但开发受到人力资源有限和标准化实践不一致的限制。需要一种结合AI和人类参与的方法来加速词汇表开发。

Method: 开发MatSci-YAMZ平台，整合AI和人类参与循环（HILT），包括众包。在材料科学领域进行概念验证，6名参与者通过平台贡献术语定义并提供示例来提示AI定义精炼。

Result: 成功创建19个AI生成的术语定义，迭代反馈循环证明了AI-HILT精炼的可行性。验证了该模型：1）成功概念验证；2）符合FAIR和开放科学原则；3）建立了指导未来研究的研究协议；4）具有跨领域扩展潜力。

Conclusion: MatSci-YAMZ模型能够增强语义透明度，减少共识构建和元数据词汇表开发所需时间，为跨领域扩展提供了可行路径。

Abstract: Metadata vocabularies are essential for advancing FAIR and FARR data principles, but their development constrained by limited human resources and inconsistent standardization practices. This paper introduces MatSci-YAMZ, a platform that integrates artificial intelligence (AI) and human-in-the-loop (HILT), including crowdsourcing, to support metadata vocabulary development. The paper reports on a proof-of-concept use case evaluating the AI-HILT model in materials science, a highly interdisciplinary domain Six (6) participants affiliated with the NSF Institute for Data-Driven Dynamical Design (ID4) engaged with the MatSci-YAMZ plaform over several weeks, contributing term definitions and providing examples to prompt the AI-definitions refinement. Nineteen (19) AI-generated definitions were successfully created, with iterative feedback loops demonstrating the feasibility of AI-HILT refinement. Findings confirm the feasibility AI-HILT model highlighting 1) a successful proof of concept, 2) alignment with FAIR and open-science principles, 3) a research protocol to guide future studies, and 4) the potential for scalability across domains. Overall, MatSci-YAMZ's underlying model has the capacity to enhance semantic transparency and reduce time required for consensus building and metadata vocabulary development.

</details>


### [19] [SCOPE: Language Models as One-Time Teacher for Hierarchical Planning in Text Environments](https://arxiv.org/abs/2512.09897)
*Haoye Lu,Pavan Seshadri,Kaheer Suleman*

Main category: cs.AI

TL;DR: SCOPE是一种一次性分层规划器，利用LLM生成的子目标仅初始化时预训练轻量级学生模型，显著提高效率但牺牲了解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的规划方法存在计算成本高、部署困难的问题，且LLM参数固定无法适应目标任务。需要更高效的方法来利用LLM的语义知识进行文本环境规划。

Method: 提出SCOPE方法：1) 仅在初始化时使用LLM从示例轨迹生成子目标；2) 用这些子目标预训练轻量级学生模型；3) 移除训练和推理期间重复查询LLM的需求。

Result: 在TextCraft环境中，SCOPE达到0.56成功率，优于ADaPT的0.52；推理时间从164.4秒大幅减少到3.0秒，效率显著提升。

Conclusion: 尽管LLM生成的子目标可能次优且牺牲了解释性，但可以作为分层目标分解的强起点，实现高效的一次性规划器，平衡性能与效率。

Abstract: Long-term planning in complex, text-based environments presents significant challenges due to open-ended action spaces, ambiguous observations, and sparse feedback. Recent research suggests that large language models (LLMs) encode rich semantic knowledge about the world, which can be valuable for guiding agents in high-level reasoning and planning across both embodied and purely textual settings. However, existing approaches often depend heavily on querying LLMs during training and inference, making them computationally expensive and difficult to deploy efficiently. In addition, these methods typically employ a pretrained, unaltered LLM whose parameters remain fixed throughout training, providing no opportunity for adaptation to the target task. To address these limitations, we introduce SCOPE (Subgoal-COnditioned Pretraining for Efficient planning), a one-shot hierarchical planner that leverages LLM-generated subgoals only at initialization to pretrain a lightweight student model. Unlike prior approaches that distill LLM knowledge by repeatedly prompting the model to adaptively generate subgoals during training, our method derives subgoals directly from example trajectories. This design removes the need for repeated LLM queries, significantly improving efficiency, though at the cost of reduced explainability and potentially suboptimal subgoals. Despite their suboptimality, our results on the TextCraft environment show that LLM-generated subgoals can still serve as a strong starting point for hierarchical goal decomposition in text-based planning tasks. Compared to the LLM-based hierarchical agent ADaPT (Prasad et al., 2024), which achieves a 0.52 success rate, our method reaches 0.56 and reduces inference time from 164.4 seconds to just 3.0 seconds.

</details>


### [20] [Bayesian Networks, Markov Networks, Moralisation, Triangulation: a Categorical Perspective](https://arxiv.org/abs/2512.09908)
*Antonio Lorenzin,Fabio Zanasi*

Main category: cs.AI

TL;DR: 本文提出了一种范畴论框架，将贝叶斯网络和马尔可夫网络之间的道德化和三角化转换建模为函子，并重新解释变量消除算法为函子。


<details>
  <summary>Details</summary>
Motivation: 传统概率图模型中，道德化和三角化转换在贝叶斯网络和马尔可夫网络之间切换，但缺乏统一的数学框架。本文旨在通过范畴论为这些转换提供形式化基础，区分语法和语义层面的操作。

Method: 将贝叶斯网络和马尔可夫网络分别建模为范畴，其中对象是从"语法"域到"语义"域的函子。道德化和三角化被定义为这些范畴之间的函子，通过函子预复合实现归纳定义。变量消除算法被重新解释为函子，将三角化过程分解为纯语法和纯语义两部分。

Result: 成功建立了概率图模型的函子视角，将道德化（纯语法）和三角化（依赖语义）统一在范畴论框架下。变量消除算法被形式化为函子，清晰地分离了语法和语义操作。

Conclusion: 范畴论为概率图模型理论提供了新的视角，突出了语法和语义修改的区别。该框架不仅统一了现有转换，还为算法分析和扩展提供了理论基础。

Abstract: Moralisation and Triangulation are transformations allowing to switch between different ways of factoring a probability distribution into a graphical model. Moralisation allows to view a Bayesian network (a directed model) as a Markov network (an undirected model), whereas triangulation addresses the opposite direction. We present a categorical framework where these transformations are modelled as functors between a category of Bayesian networks and one of Markov networks. The two kinds of network (the objects of these categories) are themselves represented as functors from a `syntax' domain to a `semantics' codomain. Notably, moralisation and triangulation can be defined inductively on such syntax via functor pre-composition. Moreover, while moralisation is fully syntactic, triangulation relies on semantics. This leads to a discussion of the variable elimination algorithm, reinterpreted here as a functor in its own right, that splits the triangulation procedure in two: one purely syntactic, the other purely semantic. This approach introduces a functorial perspective into the theory of probabilistic graphical models, which highlights the distinctions between syntactic and semantic modifications.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [21] [Large Language Models as Search Engines: Societal Challenges](https://arxiv.org/abs/2512.08946)
*Zacchary Sadeddine,Winston Maxwell,Gaël Varoquaux,Fabian M. Suchanek*

Main category: cs.CY

TL;DR: LLMs可能取代搜索引擎成为主要信息门户，本文分析了这一转变可能带来的15类社会挑战，涉及LLM提供商、内容创作者和终端用户三方，并探讨了技术和法律缓解策略。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs可能取代搜索引擎成为主要信息获取门户，这一转变将带来复杂的社会挑战。本文旨在系统识别和分析这些挑战，为各方提供应对策略，促进对这一技术变革的负责任管理。

Method: 采用多利益相关方分析框架，聚焦LLM提供商、内容创作者和终端用户三个关键角色，识别出15类具体挑战。对每类挑战，从技术和法律两个维度分析现有缓解策略，并评估其影响。

Result: 识别了15类社会挑战，包括信息质量、公平性、透明度、责任归属、经济影响等方面。发现当前缓解策略在技术和法律层面均存在不足，需要更系统的解决方案。

Conclusion: LLMs取代搜索引擎将带来深远的社会影响，需要多方协作应对。未来研究应关注更有效的缓解策略、监管框架设计，以及如何平衡技术创新与社会责任。

Abstract: Large Language Models (LLMs) may one day replace search engines as the primary portal to information on the Web. In this article, we investigate the societal challenges that such a change could bring. We focus on the roles of LLM Providers, Content Creators, and End Users, and identify 15 types of challenges. With each, we show current mitigation strategies -- both from the technical perspective and the legal perspective. We also discuss the impact of each challenge and point out future research opportunities.

</details>


### [22] [Institutional AI Sovereignty Through Gateway Architecture: Implementation Report from Fontys ICT](https://arxiv.org/abs/2512.08978)
*Ruud Huijts,Koen Suilen*

Main category: cs.CY

TL;DR: 大学构建了一个为期6个月、300名用户的AI平台试点，通过三层架构提供受治理的AI访问，确保欧盟合规、成本控制和透明风险


<details>
  <summary>Details</summary>
Motivation: 商业AI订阅导致访问不平等和合规风险（不透明的处理和非欧盟托管），但禁止使用既不现实也无用。机构需要以主权、负责任的方式提供强大的AI

Method: 构建三层治理网关平台：1) ChatGPT风格前端，明确模型选择；2) 网关核心，执行策略、控制访问和预算，默认路由到欧盟基础设施；3) 提供商层，将商业和开源模型包装在机构模型卡中，统一治理接口

Result: 试点运行可靠，无隐私事件，采用率高，实现了欧盟默认路由、受控支出和透明模型选择。只有网关模式能同时实现模型多样性、快速创新和机构控制

Conclusion: AI不是支持功能而是战略，需要专门领导力。可持续运营需要超越传统边界的治理。建议设立正式的AI官员角色，结合技术素养、治理权威和教育责任，使高校能够运营自己的多提供商AI平台

Abstract: To counter fragmented, high-risk adoption of commercial AI tools, we built and ran an institutional AI platform in a six-month, 300-user pilot, showing that a university of applied sciences can offer advanced AI with fair access, transparent risks, controlled costs, and alignment with European law.
  Commercial AI subscriptions create unequal access and compliance risks through opaque processing and non-EU hosting, yet banning them is neither realistic nor useful. Institutions need a way to provide powerful AI in a sovereign, accountable form.
  Our solution is a governed gateway platform with three layers: a ChatGPT-style frontend linked to institutional identity that makes model choice explicit; a gateway core enforcing policy, controlling access and budgets, and routing traffic to EU infrastructure by default; and a provider layer wrapping commercial and open-source models in institutional model cards that consolidate vendor documentation into one governance interface.
  The pilot ran reliably with no privacy incidents and strong adoption, enabling EU-default routing, managed spending, and transparent model choices. Only the gateway pattern combines model diversity and rapid innovation with institutional control.
  The central insight: AI is not a support function but strategy, demanding dedicated leadership. Sustainable operation requires governance beyond traditional boundaries. We recommend establishing a formal AI Officer role combining technical literacy, governance authority, and educational responsibility. Without it, AI decisions stay ad-hoc and institutional exposure grows. With it, higher-education institutions can realistically operate their own multi-provider AI platform, provided they govern AI as seriously as they teach it.

</details>


### [23] [FLARE v2: A Recursive Framework for Program Comprehension Across Languages and Levels of Abstraction](https://arxiv.org/abs/2512.09261)
*Justin Heath*

Main category: cs.CY

TL;DR: FLARE v2是一个递归的、符号学启发的编程意义构建框架，将FLARE v1的描述层级重新解释为单一生成操作，通过识别元素、分析绑定和识别新元素来解释程序含义。


<details>
  <summary>Details</summary>
Motivation: 解决FLARE v1中报告的概念和认知负荷限制，为编程教学提供更好的理论框架，将编程理解置于符号学和程序理解理论中。

Method: 提出递归生成操作：识别元素（具有接收、发送、效果、共享四个属性），分析其在因果-时间和通信两个维度上的绑定，识别新出现的元素。因果-时间维度包含顺序、分支和事件三个子类型。通过组合阶梯可视化编程结构与读写能力发展的平行关系。

Result: FLARE v2作为一个概念框架，改进了FLARE v1的局限性，提供了更统一的编程意义构建解释，具有教学和课程设计潜力。

Conclusion: FLARE v2是一个概念性视角，为编程教学提供了理论基础，其实施和实证评估留待未来工作。

Abstract: Building on the classroom framework reported in Heath et al. (2025), this paper proposes FLARE v2 as a recursive, semiotically informed account of how program meaning is constructed. It reinterprets the descriptive tiers of FLARE v1 as instances of a single generative operation: identify elements (characterised by the four properties Receives, Sends, Effects, Shares); analyse their bindings along two dimensions (Causal-Temporal and Communicative); and recognise the new element that emerges. The Causal-Temporal dimension encompasses three subtypes - Sequential, Branch, and Event - that together account for control flow in both procedural and event-driven environments. A Compositional Ladder provides a visual parallel between literacy progressions and programming structures, illustrating how recursive composition operates from blocks and statements through segments, systems, and services. The framework aims to address conceptual and cognitive-load limitations reported in FLARE v1 and is situated within semiotic and program-comprehension theory. FLARE v2 is presented as a conceptual lens with potential implications for pedagogy and curriculum design; implementation and empirical evaluation are left for future work.

</details>


### [24] [The Gender Code: Gendering the Global Governance of Artificial Intelligence](https://arxiv.org/abs/2512.09570)
*Jelena Cupac*

Main category: cs.CY

TL;DR: 该论文分析了国际AI治理框架如何应对性别问题和性别相关伤害，发现现有框架在性别整合方面虽有进展但仍存在关键差距，需要更具交叉性、可执行性和包容性的治理方法。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的快速发展，性别偏见和性别相关伤害在AI系统中日益凸显。现有国际AI治理框架在性别问题处理上存在不足，需要系统分析现有治理工具的性别敏感性，识别差距并提出改进方向，以促进更公平、包容的AI发展。

Method: 通过分析三类国际AI治理工具：1) 具有约束力的法规（如欧盟AI法案）；2) 软法工具（如UNESCO AI伦理建议）；3) 全球倡议（如全球AI伙伴关系GPAI）。研究采用比较分析方法，识别这些框架在性别问题处理上的趋势、模式和差距。

Result: 研究发现三个主要趋势：1) 性别问题正被整合到更广泛的人权框架中；2) 从隐含处理转向明确的性别相关条款；3) 越来越强调包容性和多样性。但存在关键差距：1) 不同治理文件对性别的处理不一致；2) 对交叉性（intersectionality）的关注有限；3) 缺乏强有力的执行机制。

Conclusion: 有效的AI治理必须是交叉性的、可执行的和包容性的。这需要超越象征性措施，实现有意义的公平，防止强化现有不平等。性别敏感的治理对于构建公正的技术未来至关重要，为伦理AI辩论提供了重要视角。

Abstract: This paper examines how international AI governance frameworks address gender issues and gender-based harms. The analysis covers binding regulations, such as the EU AI Act; soft law instruments, like the UNESCO Recommendations on AI Ethics; and global initiatives, such as the Global Partnership on AI (GPAI). These instruments reveal emerging trends, including the integration of gender concerns into broader human rights frameworks, a shift toward explicit gender-related provisions, and a growing emphasis on inclusivity and diversity. Yet, some critical gaps persist, including inconsistent treatment of gender across governance documents, limited engagement with intersectionality, and a lack of robust enforcement mechanisms. However, this paper argues that effective AI governance must be intersectional, enforceable, and inclusive. This is key to moving beyond tokenism toward meaningful equity and preventing reinforcement of existing inequalities. The study contributes to ethical AI debates by highlighting the importance of gender-sensitive governance in building a just technological future.

</details>


### [25] [Ethics Readiness of Artificial Intelligence: A Practical Evaluation Method](https://arxiv.org/abs/2512.09729)
*Laurynas Adomaitis,Vincent Israel-Jost,Alexei Grinbaum*

Main category: cs.CY

TL;DR: 提出了伦理准备水平（ERLs）框架，将伦理原则转化为AI系统设计中的具体检查和控制措施，通过案例研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决AI系统设计中伦理原则与工程实践之间的鸿沟，将抽象的伦理价值转化为具体的设计决策，促进伦理融入设计过程。

Method: 开发了四级迭代的ERLs方法，使用基于具体指标的动态树状问卷进行评估，通过两个案例研究（执法AI面部素描生成器和协作工业机器人）验证方法。

Result: ERLs工具有效催化了具体设计变更，促进了从狭隘技术解决方案主义向反思性、伦理设计思维的转变，帮助伦理专家与技术团队进行结构化对话。

Conclusion: ERLs框架成功地将伦理原则与工程实践连接起来，为AI系统设计提供了实用的伦理评估工具，促进了伦理设计思维的文化转变。

Abstract: We present Ethics Readiness Levels (ERLs), a four-level, iterative method to track how ethical reflection is implemented in the design of AI systems. ERLs bridge high-level ethical principles and everyday engineering by turning ethical values into concrete prompts, checks, and controls within real use cases. The evaluation is conducted using a dynamic, tree-like questionnaire built from context-specific indicators, ensuring relevance to the technology and application domain. Beyond being a managerial tool, ERLs help facilitate a structured dialogue between ethics experts and technical teams, while our scoring system helps track progress over time. We demonstrate the methodology through two case studies: an AI facial sketch generator for law enforcement and a collaborative industrial robot. The ERL tool effectively catalyzes concrete design changes and promotes a shift from narrow technological solutionism to a more reflective, ethics-by-design mindset.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [26] [Refuting "Debunking the GAMLSS Myth: Simplicity Reigns in Pulmonary Function Diagnostics"](https://arxiv.org/abs/2512.09179)
*Robert A. Rigby,Mikis D. Stasinopoulos,Achim Zeileis,Sanja Stanojevic,Gillian Heller,Fernanda de Bastiani,Thomas Kneib,Andreas Mayr,Reto Stauffer,Nikolaus Umlauf*

Main category: stat.AP

TL;DR: 这是一篇对Zavorsky (2025)文章的评论，作者不同意原文关于分段线性回归(SLR)与GAMLSS模型在肺功能测试参考方程中优劣的两个主要观点。


<details>
  <summary>Details</summary>
Motivation: 对Zavorsky文章中关于肺功能测试参考方程建模方法的比较提出质疑，认为原文在方法比较上存在根本性问题，需要进行学术讨论。

Method: 通过学术评论的形式，针对原文的两个主要论点进行反驳：1) SLR与GAMLSS预测准确性相当；2) GAMLSS过于复杂而SLR更简洁易用。

Result: 作者明确表示不同意Zavorsky的两个核心观点，认为SLR并不优于GAMLSS，且GAMLSS的复杂性是必要的。

Conclusion: 这是一篇学术批评文章，旨在引发对肺功能测试参考方程建模方法的深入讨论，强调GAMLSS作为GLI标准方法的合理性。

Abstract: We read with interest the above article by Zavorsky (2025, Respiratory Medicine, doi:10.1016/j.rmed.2024.107836) concerning reference equations for pulmonary function testing. The author compares a Generalized Additive Model for Location, Scale, and Shape (GAMLSS), which is the standard adopted by the Global Lung Function Initiative (GLI), with a segmented linear regression (SLR) model, for pulmonary function variables. The author presents an interesting comparison; however there are some fundamental issues with the approach. We welcome this opportunity for discussion of the issues that it raises. The author's contention is that (1) SLR provides "prediction accuracies on par with GAMLSS"; and (2) the GAMLSS model equations are "complicated and require supplementary spline tables", whereas the SLR is "more straightforward, parsimonious, and accessible to a broader audience". We respectfully disagree with both of these points.

</details>


### [27] [Access to healthcare for people with Alzheimer's Diseases and related dementias](https://arxiv.org/abs/2512.09217)
*Saeed Saleh Namadi,Jie Chen,Deb Niemeier*

Main category: stat.AP

TL;DR: 研究使用空间分析方法发现马里兰州阿尔茨海默病及相关痴呆症的医疗资源分布不均，东部和南部农村地区医疗可及性差、死亡率高但诊断率低，存在显著城乡差异。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病及相关痴呆症（ADRD）在全球影响数百万人，但在少数族裔和社会经济弱势群体中存在显著的诊断和护理差异。本研究旨在调查马里兰州ADRD患病密度与医疗可及性之间的关系，识别服务不足和过度服务的地区，关注地理上的护理差异。

Method: 使用ICD-10代码识别2023年马里兰州ADRD患者。采用核密度两步移动搜索法（KD2SFCA）测量医疗可及性。使用基尼系数和t检验分析城乡差异。应用热点分析（Getis-Ord Gi*）和局部双变量关系分析评估空间相关性。应用主成分分析（PCA）计算健康风险指数。

Result: 医院可及性分布不均。ADRD死亡率在医疗服务不足、医院较少的地区更高。热点分析显示马里兰州东部和南部存在高死亡率区域（按人口和按ADRD患者计算），周围环绕着类似高死亡率区域。马里兰州中部显示较低的每患者死亡率但医院设施更多。在东部马里兰州，较高贫困地区周围环绕着可及性较低、健康风险指数较高的区域。

Conclusion: 医院可及性分布不均，造成显著的农村差异。在医疗设施可及性方面服务不足的地区，特别是马里兰州东部和南部，尽管诊断率低，但ADRD死亡率高。这表明许多ADRD病例未被诊断、诊断不足或治疗延迟。

Abstract: Background: Alzheimer's Disease and Related Dementias (ADRD) affects millions worldwide. Significant disparities exist in ADRD diagnosis and care, disproportionately impacting minority and socioeconomically vulnerable populations Objective: In this study, we investigate the relationship between ADRD density and accessibility to healthcare. We identify underserved and overserved areas in Maryland based on diagnosed cases and mortality due to ADRD, focusing on geographic disparities in care. Methods: 2023 Maryland ADRD patients were identified using ICD-10 codes from. Accessibility was measured using the Kernel Density Two-Step Floating Catchment Area (KD2SFCA) method. The Gini index and t-tests were used to analyze disparities between urban and rural areas. Hot Spot Analysis Getis-Ord Gi* and local bivariate relationships analysis were applied to assess spatial correlations. Principal component analysis (PCA) was applied to calculate the health risk index. Results: Hospital accessibility was unevenly distributed. Mortality rates from ADRD were higher in underserved areas with fewer hospitals. Hot spot analysis shows eastern and southern Maryland have zones with high mortality per population and per ADRD patient, surrounded by similarly high-rate zones. Central Maryland shows lower death rates per patient but more hospital facilities. In eastern Maryland, higher poverty areas are surrounded by zones with lower accessibility and higher health risk indices. Conclusion: Hospital accessibility is unevenly distributed, creating major rural disparities. Underserved regions in terms of access to healthcare facilities, particularly in eastern and southern Maryland, exhibit high ADRD mortality rates despite low diagnosis rates. This suggests that many ADRD cases remain undiagnosed, underdiagnosed, or subject to delayed treatment.

</details>


### [28] [Group Cooperation Diverges onto Durable Low versus High Paths: Public Goods Experiments in 134 Honduran Villages](https://arxiv.org/abs/2512.09316)
*Marios Papamichalis,Nicholas Christakis,Feng Fu*

Main category: stat.AP

TL;DR: 通过大规模田野实验发现，固定匿名五人小组的贡献行为早期就分化为高低两条路径，且很少收敛；早期多数高贡献者能预测最终高合作结果


<details>
  <summary>Details</summary>
Motivation: 研究群体合作行为如何随时间演变，特别是早期贡献模式如何影响长期合作结果，理解社会网络中关键人物的作用

Method: 在洪都拉斯134个村庄对2,591名参与者进行大规模田野实验，采用固定匿名五人小组设计，进行十轮贡献游戏追踪

Result: 贡献行为早期就分化为高低两条持久路径，很少收敛；高路径参与者可早期准确识别；早期有约60%超常贡献者的群体很可能保持高合作水平

Conclusion: 早期高贡献行为，特别是社会中心人物的高贡献，能够引导群体进入并保持在高度合作路径上，验证了理论预测的分叉现象

Abstract: We performed large, lab-in-the-field experiment (2,591 participants across 134 Honduran villages; ten rounds) and tracked how contribution behavior unfolds in fixed, anonymous groups of size five. Contribution separates early into two durable paths, one low and one high, with rare convergence thereafter. High-path players can be identified with strong accuracy early on. Groups that begin with an early majority of above-norm contributors (about 60%) are very likely finish high. The empirical finding of a bifurcation, consistent with the theory, shows that early, high contributions by socially central people steer groups onto, and help keep them on, a high-cooperation path.

</details>


### [29] [Neural posterior inference with state-space models for calibrating ice sheet simulators](https://arxiv.org/abs/2512.09561)
*Bao Anh Vu,Andrew Zammit-Mangion,David Gunawan,Felicity S. McCormack,Noel Cressie*

Main category: stat.AP

TL;DR: 使用神经后验近似方法校准冰盖模型参数，结合集合卡尔曼滤波推断冰厚度，相比传统方法精度更高，并应用于南极思韦茨冰川


<details>
  <summary>Details</summary>
Motivation: 冰盖模型参数校准面临非线性方程、高维参数和有限观测数据的挑战，需要更高效准确的校准方法

Method: 采用神经后验近似技术，训练神经网络从冰流速和冰面高程观测数据推断基岩高程和基底摩擦系数，再结合集合卡尔曼滤波推断冰厚度

Result: 模拟研究表明该方法比当前最先进的增广状态集合卡尔曼滤波更准确地估计参数和状态

Conclusion: 神经后验近似方法能有效校准冰盖模型参数，成功应用于南极思韦茨冰川，为海平面上升预测提供更可靠的工具

Abstract: Ice sheet models are routinely used to quantify and project an ice sheet's contribution to sea level rise. In order for an ice sheet model to generate realistic projections, its parameters must first be calibrated using observational data; this is challenging due to the nonlinearity of the model equations, the high dimensionality of the underlying parameters, and limited data availability for validation. This study leverages the emerging field of neural posterior approximation for efficiently calibrating ice sheet model parameters and boundary conditions. We make use of a one-dimensional (flowline) Shallow-Shelf Approximation model in a state-space framework. A neural network is trained to infer the underlying parameters, namely the bedrock elevation and basal friction coefficient along the flowline, based on observations of ice velocity and ice surface elevation. Samples from the approximate posterior distribution of the parameters are then used within an ensemble Kalman filter to infer latent model states, namely the ice thickness along the flowline. We show through a simulation study that our approach yields more accurate estimates of the parameters and states than a state-augmented ensemble Kalman filter, which is the current state-of-the-art. We apply our approach to infer the bed elevation and basal friction along a flowline in Thwaites Glacier, Antarctica.

</details>


### [30] [Bayesian Model Selection with an Application to Cosmology](https://arxiv.org/abs/2512.09724)
*Nikoloz Gigiberia*

Main category: stat.AP

TL;DR: 使用DES-SN5YR超新星数据通过贝叶斯方法比较ΛCDM、wCDM和CPL宇宙学模型，发现wCDM模型表现最佳


<details>
  <summary>Details</summary>
Motivation: 从贝叶斯角度研究宇宙学参数推断和模型选择，评估不同宇宙学模型对观测数据的拟合能力

Method: 使用DES-SN5YR Ia型超新星数据测试ΛCDM、wCDM和CPL模型；通过NumPyro中的NUTS采样器进行哈密顿蒙特卡洛后验推断；使用R的bridgesampling库计算贝叶斯因子进行模型比较

Result: 三个模型表现出相似的预测性能，但wCDM相对于ΛCDM和CPL模型具有更强的证据支持

Conclusion: 在本研究使用的假设和数据条件下，wCDM模型能更好地描述宇宙膨胀

Abstract: We investigate cosmological parameter inference and model selection from a Bayesian perspective. Type Ia supernova data from the Dark Energy Survey (DES-SN5YR) are used to test the \(Λ\)CDM, \(w\)CDM, and CPL cosmological models. Posterior inference is performed via Hamiltonian Monte Carlo using the No-U-Turn Sampler (NUTS) implemented in NumPyro and analyzed with ArviZ in Python. Bayesian model comparison is conducted through Bayes factors computed using the \texttt{bridgesampling} library in R. The results indicate that all three models demonstrate similar predictive performance, but \(w\)CDM shows stronger evidence relative to \(Λ\)CDM and CPL. We conclude that, under the assumptions and data used in this study, \(w\)CDM provides a better description of cosmological expansion.

</details>


### [31] [Network Meta Analysis of Mean Survival](https://arxiv.org/abs/2512.09732)
*Anastasios Apsemidis,Dimitris Mavridis,Nikolaos Demiris*

Main category: stat.AP

TL;DR: 提出贝叶斯网络元分析方法，直接分析治疗的平均生存期而非传统替代指标，通过生存曲线外推整合长期证据，适用于成本效益分析决策框架。


<details>
  <summary>Details</summary>
Motivation: 传统网络元分析使用风险比或限制平均生存时间等替代指标，存在比例风险假设不正确或随访期短的局限性，无法直接分析决策所需的主要结局指标——治疗的平均生存期。

Method: 贝叶斯框架下，使用灵活的多风险参数模型和M样条方法进行生存曲线外推，整合基于死亡率预测的长期证据，在决策理论框架中直接分析平均生存期。

Result: 通过模拟研究评估了不同技术的计算和统计效率，并将方法应用于两个真实数据集，证明该方法能有效进行生存曲线外推和网络元分析。

Conclusion: 提出的方法直接分析平均生存期，克服了传统替代指标的局限性，易于整合成本信息，适用于成本效益分析中的治疗选择决策。

Abstract: Decisions based upon pairwise comparisons of multiple treatments are naturally performed in terms of the mean survival of the selected study arms or functions thereof. However, synthesis of treatment comparisons is usually performed on surrogates of the mean survival, such as hazard ratios or restricted mean survival times. Thus, network meta-analysis techniques may suffer from the limitations of these approaches, such as incorrect proportional hazards assumption or short-term follow-up periods. We propose a Bayesian framework for the network meta-analysis of the main outcome informing the decision, the mean survival of a treatment. Its derivation involves extrapolation of the observed survival curves. We use methods for stable extrapolation that integrate long term evidence based upon mortality projections. Extrapolations are performed using flexible poly-hazard parametric models and M-spline-based methods. We assess the computational and statistical efficiency of different techniques using a simulation study and apply the developed methods to two real data sets. The proposed method is formulated within a decision theoretic framework for cost-effectiveness analyses, where the `best' treatment is to be selected and incorporating the associated cost information is straightforward.

</details>

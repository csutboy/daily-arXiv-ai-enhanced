<div id=toc></div>

# Table of Contents

- [cs.SI](#cs.SI) [Total: 2]
- [cs.AI](#cs.AI) [Total: 28]
- [cs.ET](#cs.ET) [Total: 1]
- [econ.EM](#econ.EM) [Total: 4]
- [cs.CY](#cs.CY) [Total: 15]
- [stat.AP](#stat.AP) [Total: 1]


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [1] [Understanding the Consequences of VTuber Reincarnation](https://arxiv.org/abs/2601.08972)
*Yiluo Wei,Gareth Tyson*

Main category: cs.SI

TL;DR: VTuber "reincarnation"（换皮重生）对中之人职业发展造成显著负面影响，包括观众流失、收入下降、骚扰增加，对整个VTuber产业也有负面冲击。


<details>
  <summary>Details</summary>
Motivation: 当前VTuber产业模式中，企业拥有虚拟形象所有权，而中之人承受双重身份管理的巨大压力，面临职业倦怠、骚扰和不稳定劳动条件等问题。当压力无法承受时，中之人可能终止合同并以新形象"重生"，这种现象给中之人、经纪公司和观众都带来重大损失，需要量化研究来减轻损害并促进行业可持续发展。

Method: 首次对VTuber重生现象进行大规模实证研究，分析了12个重要案例，使用包含728K直播会话和45亿观众互动记录的全面数据集。

Result: 重生显著损害中之人的职业生涯，导致观众和财务支持下降，骚扰增加，并对更广泛的VTuber产业产生负面影响。

Conclusion: 这些发现对减轻重生带来的重大职业和个人成本具有直接意义，有助于培育更健康、更公平的VTuber生态系统。

Abstract: The rapid proliferation of VTubers, digital avatars controlled and voiced by human actors (Nakanohito), has created a lucrative and popular entertainment ecosystem. However, the prevailing industry model, where corporations retain ownership of the VTuber persona while the Nakanohito bears the immense pressure of dual-identity management, exposes the Nakanohito to significant vulnerabilities, including burnout, harassment, and precarious labor conditions. When these pressures become untenable, the Nakanohito may terminate their contracts and later debut with a new persona, a process known as "reincarnation". This phenomenon, a rising concern in the industry, inflicts substantial losses on the Nakanohito, agencies, and audiences alike. Understanding the quantitative fallout of reincarnation is crucial for mitigating this damage and fostering a more sustainable industry. To address this gap, we conduct the first large-scale empirical study of VTuber reincarnation, analyzing 12 significant cases using a comprehensive dataset of 728K livestream sessions and 4.5B viewer interaction records. Our results suggest reincarnation significantly damages a Nakanohito's career, leading to a decline in audience and financial support, an increase in harassment, and negative repercussions for the wider VTuber industry. Overall, these insights carry immediate implications for mitigating the significant professional and personal costs of the reincarnation, and fostering a healthier and more equitable VTuber ecosystem.

</details>


### [2] [FairGE: Fairness-Aware Graph Encoding in Incomplete Social Networks](https://arxiv.org/abs/2601.09394)
*Renqiang Luo,Huafei Huang,Tao Tang,Jing Ren,Ziqi Xu,Mingliang Hou,Enyan Dai,Feng Xia*

Main category: cs.SI

TL;DR: FairGE是一个面向不完整社交网络的公平图编码框架，通过谱图理论直接编码公平性，无需生成敏感属性，在统计均等和机会均等方面相比现有方法提升至少16%。


<details>
  <summary>Details</summary>
Motivation: 现有图变换器在社交网络分析中存在公平性问题，特别是在不完整社交网络中，敏感属性经常因隐私和伦理限制而缺失。现有解决方案通常生成这些不完整属性，但这可能引入额外偏见并进一步损害用户隐私。

Method: FairGE通过谱图理论直接编码公平性，利用主特征向量表示结构信息，并将不完整敏感属性用零填充以保持独立性，避免数据重建。理论分析表明该方法抑制非主谱分量的影响。

Result: 在七个真实世界社交网络数据集上的实验表明，FairGE在统计均等和机会均等方面相比最先进的基线方法至少提升16%。

Conclusion: FairGE为不完整社交网络中的图变换器提供了一个有效的公平性框架，无需生成敏感属性即可确保公平性，在保护隐私的同时提升了模型公平性。

Abstract: Graph Transformers (GTs) are increasingly applied to social network analysis, yet their deployment is often constrained by fairness concerns. This issue is particularly critical in incomplete social networks, where sensitive attributes are frequently missing due to privacy and ethical restrictions. Existing solutions commonly generate these incomplete attributes, which may introduce additional biases and further compromise user privacy. To address this challenge, FairGE (Fair Graph Encoding) is introduced as a fairness-aware framework for GTs in incomplete social networks. Instead of generating sensitive attributes, FairGE encodes fairness directly through spectral graph theory. By leveraging the principal eigenvector to represent structural information and padding incomplete sensitive attributes with zeros to maintain independence, FairGE ensures fairness without data reconstruction. Theoretical analysis demonstrates that the method suppresses the influence of non-principal spectral components, thereby enhancing fairness. Extensive experiments on seven real-world social network datasets confirm that FairGE achieves at least a 16% improvement in both statistical parity and equality of opportunity compared with state-of-the-art baselines. The source code is shown in https://github.com/LuoRenqiang/FairGE.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [3] [ConvoLearn: A Dataset of Constructivist Tutor-Student Dialogue](https://arxiv.org/abs/2601.08950)
*Mayank Sharma,Roy Pea,Hari Subramonyam*

Main category: cs.AI

TL;DR: 本文提出ConvoLearn数据集，基于知识建构理论构建包含1250个中学地球科学师生对话的数据集，通过微调Mistral 7B模型，显著提升了LLM在对话式学习中的教学能力。


<details>
  <summary>Details</summary>
Motivation: 当前教育应用中，大型语言模型存在根本性的教学局限性，倾向于直接提供答案而非支持对话式学习。需要开发能够真正促进知识建构的AI导师。

Method: 基于知识建构理论构建ConvoLearn数据集，包含六个核心教学维度。通过人类教师与模拟学生的受控交互创建1250个对话（每个20轮）。使用QLoRA技术微调Mistral 7B模型。

Result: 微调后的Mistral 7B（M=4.10, SD=1.03）在31位教师的评估中，显著优于基础版本（M=2.59, SD=1.11）和Claude Sonnet 4.5（M=2.87, SD=1.29）。

Conclusion: 这项工作为未来建构主义AI导师的开发和评估建立了潜在框架，展示了通过专门数据集微调可以显著改善LLM的教学行为。

Abstract: In educational applications, LLMs exhibit several fundamental pedagogical limitations, such as their tendency to reveal solutions rather than support dialogic learning. We introduce ConvoLearn (https://huggingface.co/datasets/masharma/convolearn ), a dataset grounded in knowledge building theory that operationalizes six core pedagogical dimensions: cognitive engagement, formative assessment, accountability, cultural responsiveness, metacognition, and power dynamics. We construct a semi-synthetic dataset of 1250 tutor-student dialogues (20 turns each) in middle school Earth Science through controlled interactions between human teachers and a simulated student. Using QLoRA, we demonstrate that training on this dataset meaningfully shifts LLM behavior toward knowledge-building strategies. Human evaluation by 31 teachers shows our fine-tuned Mistral 7B (M = 4.10, SD = 1.03) significantly outperforms both its base version (M = 2.59, SD = 1.11) and Claude Sonnet 4.5 (M = 2.87, SD = 1.29) overall. This work establishes a potential framework to guide future development and evaluation of constructivist AI tutors.

</details>


### [4] [ART: Action-based Reasoning Task Benchmarking for Medical AI Agents](https://arxiv.org/abs/2601.08988)
*Ananya Mantravadi,Shivali Dalmia,Abhishek Mukherji*

Main category: cs.AI

TL;DR: ART是一个基于动作推理的临床任务基准测试，用于评估医疗AI代理在电子健康记录上的多步推理能力，暴露了现有模型在聚合和阈值推理方面的显著缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试不足以评估医疗AI代理在基于动作的任务上的表现，这些任务涉及阈值评估、时间聚合和条件逻辑等关键临床推理能力。需要更可靠的临床决策支持系统来减轻医疗工作者的认知负担。

Method: 开发了一个四阶段流水线：1）从真实电子健康记录中识别临床场景；2）生成针对已知推理弱点的任务；3）质量审核；4）评估。通过分析现有基准确定了三类主要错误：检索失败、聚合错误和条件逻辑误判。

Result: 在600个任务上评估GPT-4o-mini和Claude 3.5 Sonnet，提示优化后检索接近完美，但聚合错误率28-64%，阈值推理错误率32-38%，暴露了现有模型在复杂临床推理上的显著缺陷。

Conclusion: ART基准通过揭示基于动作的电子健康记录推理中的失败模式，推动了更可靠的临床AI代理的发展，这对于在高压医疗环境中减少认知负担和支持工作能力至关重要。

Abstract: Reliable clinical decision support requires medical AI agents capable of safe, multi-step reasoning over structured electronic health records (EHRs). While large language models (LLMs) show promise in healthcare, existing benchmarks inadequately assess performance on action-based tasks involving threshold evaluation, temporal aggregation, and conditional logic. We introduce ART, an Action-based Reasoning clinical Task benchmark for medical AI agents, which mines real-world EHR data to create challenging tasks targeting known reasoning weaknesses. Through analysis of existing benchmarks, we identify three dominant error categories: retrieval failures, aggregation errors, and conditional logic misjudgments. Our four-stage pipeline -- scenario identification, task generation, quality audit, and evaluation -- produces diverse, clinically validated tasks grounded in real patient data. Evaluating GPT-4o-mini and Claude 3.5 Sonnet on 600 tasks shows near-perfect retrieval after prompt refinement, but substantial gaps in aggregation (28--64%) and threshold reasoning (32--38%). By exposing failure modes in action-oriented EHR reasoning, ART advances toward more reliable clinical agents, an essential step for AI systems that reduce cognitive load and administrative burden, supporting workforce capacity in high-demand care settings

</details>


### [5] [The Hierarchy of Agentic Capabilities: Evaluating Frontier Models on Realistic RL Environments](https://arxiv.org/abs/2601.09032)
*Logan Ritchie,Sushant Mehta,Nick Heiner,Mason Yu,Edwin Chen*

Main category: cs.AI

TL;DR: 该研究评估前沿AI模型在电商环境中的150个职场任务表现，揭示了智能体能力层级：工具使用、规划与目标形成、适应性、接地性和常识推理，发现即使最佳模型也有约40%失败率。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的智能体发展，AI评估需要从单轮响应评估转向交互环境中的多步任务完成评估。研究旨在了解当前前沿模型在真实职场环境中的实际能力水平。

Method: 在Surge提供的真实电商强化学习环境中，对前沿AI模型进行150个职场任务的实证评估。采用任务中心设计方法，强调多样性和领域专家贡献，并进行详细的失败分析。

Result: 发现一个经验推导的智能体能力层级：1)工具使用，2)规划与目标形成，3)适应性，4)接地性，5)常识推理。最佳模型仍有约40%任务失败，失败模式沿此层级可预测分布：较弱模型在基础工具使用和规划上挣扎，较强模型主要在需要超越明确指令的上下文推理任务上失败。

Conclusion: 当前前沿模型虽能展示连贯的多步行为，但在真实职场环境中实现人类水平任务完成仍存在显著能力差距。研究为智能体开发提供了重要启示和评估框架。

Abstract: The advancement of large language model (LLM) based agents has shifted AI evaluation from single-turn response assessment to multi-step task completion in interactive environments. We present an empirical study evaluating frontier AI models on 150 workplace tasks within a realistic e-commerce RL environment from Surge. Our analysis reveals an empirically-derived \emph{hierarchy of agentic capabilities} that models must master for real-world deployment: (1) tool use, (2) planning and goal formation, (3) adaptability, (4) groundedness, and (5) common-sense reasoning. Even the best-performing models fail approximately 40\% of the tasks, with failures clustering predictably along this hierarchy. Weaker models struggle with fundamental tool use and planning, whereas stronger models primarily fail on tasks requiring contextual inference beyond explicit instructions. We introduce a task-centric design methodology for RL environments that emphasizes diversity and domain expert contributions, provide detailed failure analysis, and discuss implications for agent development. Our findings suggest that while current frontier models can demonstrate coherent multi-step behavior, substantial capability gaps remain before achieving human-level task completion in realistic workplace settings.

</details>


### [6] [Human-AI Co-design for Clinical Prediction Models](https://arxiv.org/abs/2601.09072)
*Jean Feng,Avni Kothari,Patrick Vossler,Andrew Bishara,Lucas Zier,Newton Addo,Aaron Kornblith,Yan Shuo Tan,Chandan Singh*

Main category: cs.AI

TL;DR: HACHI是一个迭代式人机协同框架，利用AI代理加速开发完全可解释的临床预测模型，通过探索临床笔记中的概念，在AI自动探索和临床专家反馈之间交替进行。


<details>
  <summary>Details</summary>
Motivation: 传统临床预测模型开发需要临床专家、数据科学家和信息学专家的迭代协作，过程耗时耗力，导致只有极少数模型能进入临床实践。当团队尝试纳入包含海量概念的非结构化临床笔记时，这一挑战更加严峻。

Method: HACHI框架采用迭代式人机协同方法：1) AI代理快速探索和评估临床笔记中的候选概念；2) 临床和领域专家提供反馈以改进模型学习过程。概念被定义为用于线性模型的简单是/否问题，使临床AI团队能够透明地审查、细化和验证每轮学习到的模型。

Result: 在两个真实世界预测任务（急性肾损伤和创伤性脑损伤）中，HACHI优于现有方法，发现了常用临床预测模型中未包含的新临床相关概念，并提高了模型在临床站点和时间段之间的泛化能力。

Conclusion: HACHI框架有效加速了完全可解释临床预测模型的开发，同时揭示了临床AI团队的关键作用，包括指导AI代理探索未考虑的概念、调整概念粒度、改变目标函数以更好地与临床目标对齐，以及识别数据偏见和泄露问题。

Abstract: Developing safe, effective, and practically useful clinical prediction models (CPMs) traditionally requires iterative collaboration between clinical experts, data scientists, and informaticists. This process refines the often small but critical details of the model building process, such as which features/patients to include and how clinical categories should be defined. However, this traditional collaboration process is extremely time- and resource-intensive, resulting in only a small fraction of CPMs reaching clinical practice. This challenge intensifies when teams attempt to incorporate unstructured clinical notes, which can contain an enormous number of concepts. To address this challenge, we introduce HACHI, an iterative human-in-the-loop framework that uses AI agents to accelerate the development of fully interpretable CPMs by enabling the exploration of concepts in clinical notes. HACHI alternates between (i) an AI agent rapidly exploring and evaluating candidate concepts in clinical notes and (ii) clinical and domain experts providing feedback to improve the CPM learning process. HACHI defines concepts as simple yes-no questions that are used in linear models, allowing the clinical AI team to transparently review, refine, and validate the CPM learned in each round. In two real-world prediction tasks (acute kidney injury and traumatic brain injury), HACHI outperforms existing approaches, surfaces new clinically relevant concepts not included in commonly-used CPMs, and improves model generalizability across clinical sites and time periods. Furthermore, HACHI reveals the critical role of the clinical AI team, such as directing the AI agent to explore concepts that it had not previously considered, adjusting the granularity of concepts it considers, changing the objective function to better align with the clinical objectives, and identifying issues of data bias and leakage.

</details>


### [7] [Programming over Thinking: Efficient and Robust Multi-Constraint Planning](https://arxiv.org/abs/2601.09097)
*Derrick Goh Xin Deik,Quanyu Long,Zhengyuan Liu,Nancy F. Chen,Wenya Wang*

Main category: cs.AI

TL;DR: SCOPE框架通过分离推理与代码执行，解决多约束规划中LLM方法的局限性，实现高效、可复用的求解器函数。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在多约束规划中存在根本局限：纯推理方法容易产生不一致、错误累积和高成本；结合编码或求解器的方法缺乏灵活性，无法捕捉跨问题的通用逻辑。

Method: 提出SCOPE框架，将查询特定推理与通用代码执行解耦，生成一致、确定且可跨查询复用的求解器函数，仅需最小化参数调整。

Result: 在TravelPlanner任务上达到93.1%成功率，比最佳基线（CoT）提升61.6%，同时降低推理成本1.4倍，减少时间约4.67倍。

Conclusion: SCOPE通过分离推理与执行，在多约束规划中实现了SOTA性能，同时显著降低了成本和延迟，提供了可复用的解决方案。

Abstract: Multi-constraint planning involves identifying, evaluating, and refining candidate plans while satisfying multiple, potentially conflicting constraints. Existing large language model (LLM) approaches face fundamental limitations in this domain. Pure reasoning paradigms, which rely on long natural language chains, are prone to inconsistency, error accumulation, and prohibitive cost as constraints compound. Conversely, LLMs combined with coding- or solver-based strategies lack flexibility: they often generate problem-specific code from scratch or depend on fixed solvers, failing to capture generalizable logic across diverse problems. To address these challenges, we introduce the Scalable COde Planning Engine (SCOPE), a framework that disentangles query-specific reasoning from generic code execution. By separating reasoning from execution, SCOPE produces solver functions that are consistent, deterministic, and reusable across queries while requiring only minimal changes to input parameters. SCOPE achieves state-of-the-art performance while lowering cost and latency. For example, with GPT-4o, it reaches 93.1% success on TravelPlanner, a 61.6% gain over the best baseline (CoT) while cutting inference cost by 1.4x and time by ~4.67x. Code is available at https://github.com/DerrickGXD/SCOPE.

</details>


### [8] [DScheLLM: Enabling Dynamic Scheduling through a Fine-Tuned Dual-System Large language Model](https://arxiv.org/abs/2601.09100)
*Lixiang Zhang,Chenggong Zhao,Qing Gao,Xiaoke Zhao,Gengyi Bai,Jinhu Lv*

Main category: cs.AI

TL;DR: DScheLLM：基于双系统（快慢）推理架构的微调大语言模型动态调度方法，用于处理不同规模的扰动，在标准作业车间调度基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统生产调度方法对动态扰动（如加工时间变化、机器可用性变化、意外任务插入）高度敏感，通常依赖特定事件模型和显式分析公式，这限制了其适应性和对未见扰动的泛化能力。

Method: 提出DScheLLM动态调度方法：1）构建统一的大语言模型框架处理动态事件；2）使用运筹学求解器获得的精确调度生成快慢推理模式的训练数据集；3）基于LoRA对华为OpenPangu Embedded-7B模型在混合推理范式下进行微调。

Result: 在标准作业车间调度基准上的实验评估表明：快速思考模式能高效生成高质量调度方案，慢速思考模式能产生与求解器兼容且格式良好的决策输入。

Conclusion: 这是最早将大语言模型应用于动态环境作业车间调度的研究之一，突显了大语言模型在智能自适应调度优化中的巨大潜力。

Abstract: Production scheduling is highly susceptible to dynamic disruptions, such as variations in processing times, machine availability, and unexpected task insertions. Conventional approaches typically rely on event-specific models and explicit analytical formulations, which limits their adaptability and generalization across previously unseen disturbances. To overcome these limitations, this paper proposes DScheLLM, a dynamic scheduling approach that leverages fine-tuned large language models within a dual-system (fast-slow) reasoning architecture to address disturbances of different scales. A unified large language model-based framework is constructed to handle dynamic events, where training datasets for both fast and slow reasoning modes are generated using exact schedules obtained from an operations research solver. The Huawei OpenPangu Embedded-7B model is subsequently fine-tuned under the hybrid reasoning paradigms using LoRA. Experimental evaluations on standard job shop scheduling benchmarks demonstrate that the fast-thinking mode can efficiently generate high-quality schedules and the slow-thinking mode can produce solver-compatible and well-formatted decision inputs. To the best of our knowledge, this work represents one of the earliest studies applying large language models to job shop scheduling in dynamic environments, highlighting their considerable potential for intelligent and adaptive scheduling optimization.

</details>


### [9] [AviationLMM: A Large Multimodal Foundation Model for Civil Aviation](https://arxiv.org/abs/2601.09105)
*Wenbin Li,Jingling Wu,Xiaoyong Lin. Jing Chen,Cong Chen*

Main category: cs.AI

TL;DR: 本文提出AviationLMM，一个面向民航的大型多模态基础模型，旨在统一民航异构数据流，实现理解、推理、生成和智能体应用，以解决现有AI解决方案孤立、单模态的问题。


<details>
  <summary>Details</summary>
Motivation: 民航是全球交通和商业的基石，确保其安全、效率和客户满意度至关重要。然而，现有民航AI解决方案存在孤立和狭窄的问题，专注于孤立任务或单一模态，难以整合语音通信、雷达轨迹、传感器流和文本报告等异构数据，限制了态势感知、适应性和实时决策支持能力。

Method: 提出AviationLMM模型架构，能够处理多模态输入（如空地语音、监视数据、机载遥测、视频和结构化文本），执行跨模态对齐和融合，并产生灵活输出（包括态势摘要、风险警报、预测性诊断和多模态事件重建）。同时识别了实现该愿景需要解决的关键研究机会。

Result: 本文提出了AviationLMM的完整设计愿景和架构，识别了实现该模型需要解决的关键研究挑战，包括数据获取、对齐与融合、预训练、推理、可信度、隐私、缺失模态鲁棒性和合成场景生成等方面。

Conclusion: 通过阐述AviationLMM的设计和挑战，旨在推动民航基础模型的发展，并促进研究界共同努力，构建一个集成、可信且保护隐私的民航AI生态系统。

Abstract: Civil aviation is a cornerstone of global transportation and commerce, and ensuring its safety, efficiency and customer satisfaction is paramount. Yet conventional Artificial Intelligence (AI) solutions in aviation remain siloed and narrow, focusing on isolated tasks or single modalities. They struggle to integrate heterogeneous data such as voice communications, radar tracks, sensor streams and textual reports, which limits situational awareness, adaptability, and real-time decision support. This paper introduces the vision of AviationLMM, a Large Multimodal foundation Model for civil aviation, designed to unify the heterogeneous data streams of civil aviation and enable understanding, reasoning, generation and agentic applications. We firstly identify the gaps between existing AI solutions and requirements. Secondly, we describe the model architecture that ingests multimodal inputs such as air-ground voice, surveillance, on-board telemetry, video and structured texts, and performs cross-modal alignment and fusion, and produces flexible outputs ranging from situation summaries and risk alerts to predictive diagnostics and multimodal incident reconstructions. In order to fully realize this vision, we identify key research opportunities to address, including data acquisition, alignment and fusion, pretraining, reasoning, trustworthiness, privacy, robustness to missing modalities, and synthetic scenario generation. By articulating the design and challenges of AviationLMM, we aim to boost the civil aviation foundation model progress and catalyze coordinated research efforts toward an integrated, trustworthy and privacy-preserving aviation AI ecosystem.

</details>


### [10] [The AI Hippocampus: How Far are We From Human Memory?](https://arxiv.org/abs/2601.09113)
*Zixia Jia,Jiaqi Li,Yipeng Kang,Yuxuan Wang,Tong Wu,Quansen Wang,Xiaobo Wang,Shuyi Zhang,Junzhe Shen,Qing Li,Siyuan Qi,Yitao Liang,Di He,Zilong Zheng,Song-Chun Zhu*

Main category: cs.AI

TL;DR: 这篇综述系统性地总结了LLMs和MLLMs中的记忆机制，将其分为隐式、显式和代理记忆三大范式，并探讨了多模态记忆集成、架构进展和开放挑战。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs和MLLMs从静态预测器向交互式系统演进，记忆机制对于增强推理能力、适应性和上下文保真度变得至关重要。需要系统性地梳理现有记忆研究，为未来发展提供指导框架。

Method: 采用文献综述方法，构建了包含隐式记忆（内部参数知识）、显式记忆（外部存储检索）和代理记忆（持久时间结构）的三重分类法，并扩展到多模态记忆集成。

Result: 建立了统一的记忆分类框架，总结了关键架构进展、基准任务，识别了记忆容量、对齐、事实一致性和跨系统互操作性等核心挑战。

Conclusion: 记忆机制是LLMs和MLLMs向持续学习和个性化推理系统演进的核心，需要进一步研究记忆容量、多模态对齐和实际应用中的可靠性问题。

Abstract: Memory plays a foundational role in augmenting the reasoning, adaptability, and contextual fidelity of modern Large Language Models and Multi-Modal LLMs. As these models transition from static predictors to interactive systems capable of continual learning and personalized inference, the incorporation of memory mechanisms has emerged as a central theme in their architectural and functional evolution. This survey presents a comprehensive and structured synthesis of memory in LLMs and MLLMs, organizing the literature into a cohesive taxonomy comprising implicit, explicit, and agentic memory paradigms. Specifically, the survey delineates three primary memory frameworks. Implicit memory refers to the knowledge embedded within the internal parameters of pre-trained transformers, encompassing their capacity for memorization, associative retrieval, and contextual reasoning. Recent work has explored methods to interpret, manipulate, and reconfigure this latent memory. Explicit memory involves external storage and retrieval components designed to augment model outputs with dynamic, queryable knowledge representations, such as textual corpora, dense vectors, and graph-based structures, thereby enabling scalable and updatable interaction with information sources. Agentic memory introduces persistent, temporally extended memory structures within autonomous agents, facilitating long-term planning, self-consistency, and collaborative behavior in multi-agent systems, with relevance to embodied and interactive AI. Extending beyond text, the survey examines the integration of memory within multi-modal settings, where coherence across vision, language, audio, and action modalities is essential. Key architectural advances, benchmark tasks, and open challenges are discussed, including issues related to memory capacity, alignment, factual consistency, and cross-system interoperability.

</details>


### [11] [PrivacyReasoner: Can LLM Emulate a Human-like Privacy Mind?](https://arxiv.org/abs/2601.09152)
*Yiwen Tu,Xuan Liu,Lianhui Qin,Haojian Jin*

Main category: cs.AI

TL;DR: PRA是一个AI代理设计，用于模拟个体用户在现实世界新闻中如何形成隐私担忧。它超越群体层面的情感分析，整合隐私和认知理论，基于个人评论历史和上下文线索模拟用户特定的隐私推理。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注群体层面的隐私情感分析，缺乏对个体用户隐私担忧形成过程的模拟。需要开发能够理解个人隐私推理模式、基于历史行为和上下文动态形成隐私担忧的AI代理。

Method: PRA整合隐私和认知理论，重建用户的"隐私心智"，通过上下文过滤器动态激活相关隐私记忆（模拟有限理性），生成反映用户对新隐私场景可能反应的合成评论。使用LLM-as-a-Judge评估器，基于已建立的隐私担忧分类法量化生成推理的忠实度。

Result: 在真实世界Hacker News讨论的实验表明，PRA在隐私担忧预测方面优于基线代理，并能捕捉跨领域（包括AI、电子商务和医疗保健）的可迁移推理模式。

Conclusion: PRA为模拟个体用户隐私担忧形成提供了有效的AI代理设计，能够基于个人历史和上下文进行动态推理，在多个领域展现出良好的预测能力和泛化性。

Abstract: This paper introduces PRA, an AI-agent design for simulating how individual users form privacy concerns in response to real-world news. Moving beyond population-level sentiment analysis, PRA integrates privacy and cognitive theories to simulate user-specific privacy reasoning grounded in personal comment histories and contextual cues. The agent reconstructs each user's "privacy mind", dynamically activates relevant privacy memory through a contextual filter that emulates bounded rationality, and generates synthetic comments reflecting how that user would likely respond to new privacy scenarios. A complementary LLM-as-a-Judge evaluator, calibrated against an established privacy concern taxonomy, quantifies the faithfulness of generated reasoning. Experiments on real-world Hacker News discussions show that \PRA outperforms baseline agents in privacy concern prediction and captures transferable reasoning patterns across domains including AI, e-commerce, and healthcare.

</details>


### [12] [Position on LLM-Assisted Peer Review: Addressing Reviewer Gap through Mentoring and Feedback](https://arxiv.org/abs/2601.09182)
*JungMin Yun,JuneHyoung Kwon,MiHyeon Kim,YoungBin Kim*

Main category: cs.AI

TL;DR: 本文提出将LLM作为辅助和培训人类审稿人的工具，而非自动生成评审，以解决AI研究快速发展导致的审稿人缺口问题，建立可持续的学术生态系统。


<details>
  <summary>Details</summary>
Motivation: AI研究的快速扩张加剧了"审稿人缺口"，威胁同行评审的可持续性，并导致低质量评审的恶性循环。现有使用LLM自动生成评审的方法存在问题，需要范式转变。

Method: 提出以人为中心的LLM辅助方法：1) LLM辅助的导师系统，培养审稿人的长期能力；2) LLM辅助的反馈系统，帮助审稿人改进评审质量。这些系统基于高质量同行评审的核心原则。

Result: 通过将LLM定位为辅助和培训工具而非替代品，能够加强审稿人专业知识，缓解审稿人缺口，提高评审质量。

Conclusion: LLM应作为辅助和培训人类审稿人的工具，这种以人为中心的方法有助于建立更可持续的学术生态系统，解决同行评审的可持续性危机。

Abstract: The rapid expansion of AI research has intensified the Reviewer Gap, threatening the peer-review sustainability and perpetuating a cycle of low-quality evaluations. This position paper critiques existing LLM approaches that automatically generate reviews and argues for a paradigm shift that positions LLMs as tools for assisting and educating human reviewers. We define the core principles of high-quality peer review and propose two complementary systems grounded in these foundations: (i) an LLM-assisted mentoring system that cultivates reviewers' long-term competencies, and (ii) an LLM-assisted feedback system that helps reviewers refine the quality of their reviews. This human-centered approach aims to strengthen reviewer expertise and contribute to building a more sustainable scholarly ecosystem.

</details>


### [13] [MAXS: Meta-Adaptive Exploration with LLM Agents](https://arxiv.org/abs/2601.09259)
*Jian Zhang,Zhiyuan Wang,Zhangqi Wang,Yu He,Haoran Luo,li yuan,Lingling Zhang,Rui Mao,Qika Lin,Jun Liu*

Main category: cs.AI

TL;DR: MAXS是一个基于LLM Agent的元自适应推理框架，通过前瞻策略和轨迹收敛机制，在保持计算效率的同时提升多工具推理的全局效果。


<details>
  <summary>Details</summary>
Motivation: 现有LLM Agent方法存在两个主要问题：(1) 局部短视生成，缺乏前瞻性；(2) 轨迹不稳定性，早期微小错误会导致推理路径发散。这些问题使得难以平衡全局效果和计算效率。

Method: 提出MAXS框架，采用前瞻策略扩展推理路径几步，评估工具使用的优势值，结合步骤一致性方差和步间趋势斜率来选择稳定、一致、高价值的推理步骤。引入轨迹收敛机制，在路径一致性达成时停止进一步扩展，平衡资源效率和全局效果。

Result: 在三个基础模型（MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B）和五个数据集上的广泛实验表明，MAXS在性能和推理效率方面均优于现有方法。进一步分析证实了前瞻策略和工具使用的有效性。

Conclusion: MAXS通过元自适应推理框架有效解决了LLM Agent推理中的局部短视和轨迹不稳定问题，在保持计算效率的同时显著提升了多工具推理的全局效果。

Abstract: Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of multiple tools. However, during agent inference, existing methods often suffer from (i) locally myopic generation, due to the absence of lookahead, and (ii) trajectory instability, where minor early errors can escalate into divergent reasoning paths. These issues make it difficult to balance global effectiveness and computational efficiency. To address these two issues, we propose meta-adaptive exploration with LLM agents https://github.com/exoskeletonzj/MAXS, a meta-adaptive reasoning framework based on LLM Agents that flexibly integrates tool execution and reasoning planning. MAXS employs a lookahead strategy to extend reasoning paths a few steps ahead, estimating the advantage value of tool usage, and combines step consistency variance and inter-step trend slopes to jointly select stable, consistent, and high-value reasoning steps. Additionally, we introduce a trajectory convergence mechanism that controls computational cost by halting further rollouts once path consistency is achieved, enabling a balance between resource efficiency and global effectiveness in multi-tool reasoning. We conduct extensive empirical studies across three base models (MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B) and five datasets, demonstrating that MAXS consistently outperforms existing methods in both performance and inference efficiency. Further analysis confirms the effectiveness of our lookahead strategy and tool usage.

</details>


### [14] [Efficient Paths and Dense Rewards: Probabilistic Flow Reasoning for Large Language Models](https://arxiv.org/abs/2601.09260)
*Yan Liu,Feng Zhang,Zhanyu Ma,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He,Han Liu,Yangdong Deng*

Main category: cs.AI

TL;DR: CoT-Flow：将思维链重新概念化为连续概率流，量化每个推理步骤对最终答案的贡献，实现更高效的推理解码和无需验证器的强化学习。


<details>
  <summary>Details</summary>
Motivation: 当前思维链方法将推理过程视为不可分割的序列，缺乏量化步骤信息增益的内在机制，导致推理效率低下（冗余探索）和优化困难（稀疏监督或昂贵验证器）。

Method: 提出CoT-Framework，将离散推理步骤重新概念化为连续概率流，量化每个步骤对正确答案的贡献。基于此框架开发两种方法：1）流引导解码：使用贪婪流解码策略提取信息高效的推理路径；2）流强化学习：构建无需验证器的密集奖励函数。

Result: 在具有挑战性的基准测试中，CoT-Flow在推理效率和推理性能之间实现了优越的平衡。

Conclusion: CoT-Flow通过将推理过程建模为概率流，解决了当前思维链方法的粒度问题，为高效推理和优化提供了新框架。

Abstract: High-quality chain-of-thought has demonstrated strong potential for unlocking the reasoning capabilities of large language models. However, current paradigms typically treat the reasoning process as an indivisible sequence, lacking an intrinsic mechanism to quantify step-wise information gain. This granularity gap manifests in two limitations: inference inefficiency from redundant exploration without explicit guidance, and optimization difficulty due to sparse outcome supervision or costly external verifiers. In this work, we propose CoT-Flow, a framework that reconceptualizes discrete reasoning steps as a continuous probabilistic flow, quantifying the contribution of each step toward the ground-truth answer. Built on this formulation, CoT-Flow enables two complementary methodologies: flow-guided decoding, which employs a greedy flow-based decoding strategy to extract information-efficient reasoning paths, and flow-based reinforcement learning, which constructs a verifier-free dense reward function. Experiments on challenging benchmarks demonstrate that CoT-Flow achieves a superior balance between inference efficiency and reasoning performance.

</details>


### [15] [Coordinated Pandemic Control with Large Language Model Agents as Policymaking Assistants](https://arxiv.org/abs/2601.09264)
*Ziyi Shi,Xusen Guo,Hongliang Lu,Mingxing Peng,Haotian Wang,Zheng Zhu,Zhenning Li,Yuxuan Liang,Xinhu Zheng,Hai Yang*

Main category: cs.AI

TL;DR: 提出基于大语言模型的多智能体政策制定框架，通过模拟和协调决策实现更有效的跨区域疫情控制


<details>
  <summary>Details</summary>
Motivation: 传统疫情控制政策制定存在碎片化和反应式的问题，各地区政策孤立制定，缺乏协调，导致干预滞后，影响全球疫情防控效果

Method: 为每个行政区分配一个LLM智能体作为AI政策助手，智能体分析本地区流行病学动态，同时与其他智能体通信考虑跨区域相互依赖性，整合真实数据、疫情演化模拟器和结构化智能体间通信，通过闭环模拟过程协同探索干预情景

Result: 使用美国2020年4-12月州级COVID-19数据、真实移动记录和观察到的政策干预进行验证，相比真实疫情结果，在单个州层面累计感染和死亡分别减少63.7%和40.1%，在跨州聚合层面分别减少39.0%和27.0%

Conclusion: LLM多智能体系统能够通过协调政策制定实现更有效的疫情控制，为公共卫生决策提供新方法

Abstract: Effective pandemic control requires timely and coordinated policymaking across administrative regions that are intrinsically interdependent. However, human-driven responses are often fragmented and reactive, with policies formulated in isolation and adjusted only after outbreaks escalate, undermining proactive intervention and global pandemic mitigation. To address this challenge, here we propose a large language model (LLM) multi-agent policymaking framework that supports coordinated and proactive pandemic control across regions. Within our framework, each administrative region is assigned an LLM agent as an AI policymaking assistant. The agent reasons over region-specific epidemiological dynamics while communicating with other agents to account for cross-regional interdependencies. By integrating real-world data, a pandemic evolution simulator, and structured inter-agent communication, our framework enables agents to jointly explore counterfactual intervention scenarios and synthesize coordinated policy decisions through a closed-loop simulation process. We validate the proposed framework using state-level COVID-19 data from the United States between April and December 2020, together with real-world mobility records and observed policy interventions. Compared with real-world pandemic outcomes, our approach reduces cumulative infections and deaths by up to 63.7% and 40.1%, respectively, at the individual state level, and by 39.0% and 27.0%, respectively, when aggregated across states. These results demonstrate that LLM multi-agent systems can enable more effective pandemic control with coordinated policymaking...

</details>


### [16] [RISER: Orchestrating Latent Reasoning Skills for Adaptive Activation Steering](https://arxiv.org/abs/2601.09269)
*Wencheng Ye,Liang Peng,Xiaoyang Yuan,Yi Bin,Pengpeng Zeng,Hengyu Jin,Heng Tao Shen*

Main category: cs.AI

TL;DR: RISER是一个基于路由器的自适应激活干预框架，通过强化学习动态组合可复用推理向量，在零样本场景下显著提升LLM推理性能，同时保持高token效率。


<details>
  <summary>Details</summary>
Motivation: 现有领域特定推理方法要么需要参数更新（训练密集），要么使用静态手动干预，无法适应复杂推理的动态特性。需要一种参数高效且能自适应动态推理的干预方法。

Method: 提出RISER框架：1) 构建可复用推理向量库；2) 使用轻量级路由器动态组合这些向量；3) 通过任务级奖励的强化学习优化路由器，以涌现和组合方式激活潜在认知原语。

Result: 在7个多样化基准测试中，RISER相比基础模型平均零样本准确率提升3.4-6.5%，超越CoT推理方法，token效率提高2-3倍，同时保持稳健的准确率增益。

Conclusion: RISER能够自主组合多个向量形成可解释的精确控制策略，为更可控和高效的大语言模型推理指明了方向，展示了自适应激活干预的潜力。

Abstract: Recent work on domain-specific reasoning with large language models (LLMs) often relies on training-intensive approaches that require parameter updates. While activation steering has emerged as a parameter efficient alternative, existing methods apply static, manual interventions that fail to adapt to the dynamic nature of complex reasoning. To address this limitation, we propose RISER (Router-based Intervention for Steerable Enhancement of Reasoning), a plug-and-play intervention framework that adaptively steers LLM reasoning in activation space. RISER constructs a library of reusable reasoning vectors and employs a lightweight Router to dynamically compose them for each input. The Router is optimized via reinforcement learning under task-level rewards, activating latent cognitive primitives in an emergent and compositional manner. Across seven diverse benchmarks, RISER yields 3.4-6.5% average zero-shot accuracy improvements over the base model while surpassing CoT-style reasoning with 2-3x higher token efficiency and robust accuracy gains. Further analysis shows that RISER autonomously combines multiple vectors into interpretable, precise control strategies, pointing toward more controllable and efficient LLM reasoning.

</details>


### [17] [$A^3$-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation](https://arxiv.org/abs/2601.09274)
*Jian Zhang,Yu He,Zhiyuan Wang,Zhangqi Wang,Kai He,Fangzhi Xu,Qika Lin,Jun Liu*

Main category: cs.AI

TL;DR: 该论文提出了A³-Bench基准，用于评估科学推理中的记忆驱动机制，重点关注锚点和吸引子的激活与利用。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估最终答案或逐步推理的连贯性，但忽视了人类推理中基于记忆驱动的机制——即激活先验知识中的锚点和吸引子，并将其整合到多步推理中。

Method: 1) 使用SAPM过程（主题、锚点&吸引子、问题、记忆发展）标注了2,198个跨领域的科学推理问题；2) 提出了基于锚点和吸引子的双尺度记忆评估框架；3) 引入了AAUI（锚点-吸引子利用指数）指标来衡量记忆激活率。

Result: 通过在不同基础模型和范式上的实验，验证了A³-Bench的有效性，并分析了记忆激活如何影响推理性能，为理解记忆驱动的科学推理提供了见解。

Conclusion: 该研究填补了评估记忆驱动科学推理机制的空白，提出的A³-Bench基准和评估框架有助于更全面地理解和改进AI系统的科学推理能力。

Abstract: Scientific reasoning relies not only on logical inference but also on activating prior knowledge and experiential structures. Memory can efficiently reuse knowledge and enhance reasoning consistency and stability. However, existing benchmarks mainly evaluate final answers or step-by-step coherence, overlooking the \textit{memory-driven} mechanisms that underlie human reasoning, which involves activating anchors and attractors, then integrating them into multi-step inference. To address this gap, we propose $A^3$-Bench~ https://a3-bench.github.io, a benchmark designed to evaluate scientific reasoning through dual-scale memory-driven activation, grounded in Anchor and Attractor Activation. First, we annotate 2,198 science reasoning problems across domains using the SAPM process(subject, anchor & attractor, problem, and memory developing). Second, we introduce a dual-scale memory evaluation framework utilizing anchors and attractors, along with the AAUI(Anchor--Attractor Utilization Index) metric to measure memory activation rates. Finally, through experiments with various base models and paradigms, we validate $A^3$-Bench and analyze how memory activation impacts reasoning performance, providing insights into memory-driven scientific reasoning.

</details>


### [18] [M$^3$Searcher: Modular Multimodal Information Seeking Agency with Retrieval-Oriented Reasoning](https://arxiv.org/abs/2601.09278)
*Xiaohan Yu,Chao Feng,Lang Mei,Chong Chen*

Main category: cs.AI

TL;DR: M³Searcher：一种模块化多模态信息检索代理，通过解耦信息获取与答案推导，解决多模态搜索中的专业化-泛化权衡和数据稀缺问题


<details>
  <summary>Details</summary>
Motivation: 现有DeepResearch风格代理仅限于文本模态，扩展到多模态环境面临两个关键挑战：1) 大规模多模态工具使用时出现的专业化-泛化权衡问题；2) 捕获复杂多步骤多模态搜索轨迹的训练数据严重稀缺

Method: 提出M³Searcher模块化多模态信息检索代理，明确解耦信息获取与答案推导。采用检索导向的多目标奖励函数，联合鼓励事实准确性、推理合理性和检索保真度。同时开发MMSearchVQA多模态多跳数据集支持检索为中心的强化学习训练

Result: 实验结果表明M³Searcher优于现有方法，在复杂多模态任务中表现出强大的迁移适应能力和有效推理能力

Conclusion: M³Searcher通过模块化设计和检索导向的多目标优化，成功解决了多模态信息检索中的关键挑战，为构建更强大的自主多模态信息获取代理提供了有效方案

Abstract: Recent advances in DeepResearch-style agents have demonstrated strong capabilities in autonomous information acquisition and synthesize from real-world web environments. However, existing approaches remain fundamentally limited to text modality. Extending autonomous information-seeking agents to multimodal settings introduces critical challenges: the specialization-generalization trade-off that emerges when training models for multimodal tool-use at scale, and the severe scarcity of training data capturing complex, multi-step multimodal search trajectories. To address these challenges, we propose M$^3$Searcher, a modular multimodal information-seeking agent that explicitly decouples information acquisition from answer derivation. M$^3$Searcher is optimized with a retrieval-oriented multi-objective reward that jointly encourages factual accuracy, reasoning soundness, and retrieval fidelity. In addition, we develop MMSearchVQA, a multimodal multi-hop dataset to support retrieval centric RL training. Experimental results demonstrate that M$^3$Searcher outperforms existing approaches, exhibits strong transfer adaptability and effective reasoning in complex multimodal tasks.

</details>


### [19] [STaR: Sensitive Trajectory Regulation for Unlearning in Large Reasoning Models](https://arxiv.org/abs/2601.09281)
*Jingjing Zhou,Gaoxiang Cong,Li Su,Liang Li*

Main category: cs.AI

TL;DR: STaR是一个无需参数的推理时遗忘框架，专门解决大型推理模型在生成复杂思维链时存在的隐私泄露问题，通过语义检测、安全约束注入、轨迹感知抑制和自适应过滤实现全推理链隐私保护。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型能够生成复杂的思维链轨迹，但这带来了严重的隐私风险，敏感信息可能深嵌在推理过程中。现有的LLM遗忘方法通常只修改最终答案，无法移除中间步骤的敏感内容，导致持续的隐私泄露和安全退化。

Method: STaR框架包含四个步骤：1）通过语义感知检测识别敏感内容；2）通过安全提示前缀注入全局安全约束；3）执行轨迹感知抑制，动态阻止整个推理链中的敏感内容；4）应用令牌级自适应过滤，防止生成精确和转述的敏感令牌。

Result: 在R-TOFU基准测试中，STaR实现了全面稳定的遗忘效果，同时保持了最小的效用损失。此外，作者还引入了两个新评估指标：多解码一致性评估和多粒度成员推理攻击评估。

Conclusion: STaR为大型推理模型中的隐私保护推理设立了新标准，通过推理时遗忘框架有效解决了思维链轨迹中的隐私泄露问题，同时保持了模型的推理能力。

Abstract: Large Reasoning Models (LRMs) have advanced automated multi-step reasoning, but their ability to generate complex Chain-of-Thought (CoT) trajectories introduces severe privacy risks, as sensitive information may be deeply embedded throughout the reasoning process. Existing Large Language Models (LLMs) unlearning approaches that typically focus on modifying only final answers are insufficient for LRMs, as they fail to remove sensitive content from intermediate steps, leading to persistent privacy leakage and degraded security. To address these challenges, we propose Sensitive Trajectory Regulation (STaR), a parameter-free, inference-time unlearning framework that achieves robust privacy protection throughout the reasoning process. Specifically, we first identify sensitive content via semantic-aware detection. Then, we inject global safety constraints through secure prompt prefix. Next, we perform trajectory-aware suppression to dynamically block sensitive content across the entire reasoning chain. Finally, we apply token-level adaptive filtering to prevent both exact and paraphrased sensitive tokens during generation. Furthermore, to overcome the inadequacies of existing evaluation protocols, we introduce two metrics: Multi-Decoding Consistency Assessment (MCS), which measures the consistency of unlearning across diverse decoding strategies, and Multi-Granularity Membership Inference Attack (MIA) Evaluation, which quantifies privacy protection at both answer and reasoning-chain levels. Experiments on the R-TOFU benchmark demonstrate that STaR achieves comprehensive and stable unlearning with minimal utility loss, setting a new standard for privacy-preserving reasoning in LRMs.

</details>


### [20] [Cluster Workload Allocation: Semantic Soft Affinity Using Natural Language Processing](https://arxiv.org/abs/2601.09282)
*Leszek Sliwko,Jolanta Mizeria-Pietraszko*

Main category: cs.AI

TL;DR: 使用LLM实现基于自然语言的Kubernetes语义调度，通过解析软亲和性偏好简化集群工作负载配置


<details>
  <summary>Details</summary>
Motivation: 集群工作负载分配通常需要复杂的配置，存在可用性差距。本文旨在通过自然语言处理实现语义化、意图驱动的调度范式，降低集群系统的使用门槛。

Method: 开发了一个原型系统，将大型语言模型（LLM）通过Kubernetes调度器扩展器集成，用于解析自然语言分配提示注释中的软亲和性偏好。系统包含集群状态缓存和意图分析器（使用AWS Bedrock）。

Result: LLM解析准确率高（在评估数据集上，Amazon Nova Pro/Premier和Mistral Pixtral Large模型的子集准确率>95%），显著优于基线引擎。在六个场景的调度质量测试中，原型系统在复杂和定量场景以及处理冲突软偏好方面表现优异，达到或优于标准Kubernetes配置。

Conclusion: 验证了使用LLM进行可访问调度的可行性，确认了语义软亲和性简化工作负载编排的潜力。但指出了同步LLM延迟等限制，建议采用异步处理以实现生产就绪。

Abstract: Cluster workload allocation often requires complex configurations, creating a usability gap. This paper introduces a semantic, intent-driven scheduling paradigm for cluster systems using Natural Language Processing. The system employs a Large Language Model (LLM) integrated via a Kubernetes scheduler extender to interpret natural language allocation hint annotations for soft affinity preferences. A prototype featuring a cluster state cache and an intent analyzer (using AWS Bedrock) was developed. Empirical evaluation demonstrated high LLM parsing accuracy (>95% Subset Accuracy on an evaluation ground-truth dataset) for top-tier models like Amazon Nova Pro/Premier and Mistral Pixtral Large, significantly outperforming a baseline engine. Scheduling quality tests across six scenarios showed the prototype achieved superior or equivalent placement compared to standard Kubernetes configurations, particularly excelling in complex and quantitative scenarios and handling conflicting soft preferences. The results validate using LLMs for accessible scheduling but highlight limitations like synchronous LLM latency, suggesting asynchronous processing for production readiness. This work confirms the viability of semantic soft affinity for simplifying workload orchestration.

</details>


### [21] [Policy-Based Reinforcement Learning with Action Masking for Dynamic Job Shop Scheduling under Uncertainty: Handling Random Arrivals and Machine Failures](https://arxiv.org/abs/2601.09293)
*Sofiene Lassoued,Stefan Lier,Andreas Schwung*

Main category: cs.AI

TL;DR: 提出一个基于着色时间Petri网和可屏蔽近端策略优化的动态作业车间调度框架，用于处理随机作业到达和机器故障的不确定性，通过两种动作屏蔽策略在动态JSSP基准测试中优于传统启发式方法。


<details>
  <summary>Details</summary>
Motivation: 动态作业车间调度问题面临随机作业到达和意外机器故障等不确定性挑战，传统方法难以适应实时变化的制造环境，需要开发能够处理这些不确定性的弹性调度框架。

Method: 采用基于模型的方法，使用着色时间Petri网表示调度环境，Maskable Proximal Policy Optimization进行动态决策，Gamma分布模拟动态作业到达，Weibull分布模拟机器故障，研究两种动作屏蔽策略（非梯度和梯度方法）。

Result: 在动态JSSP基准测试中，该方法在最小化制造周期方面持续优于传统启发式和基于规则的方法，证明了将可解释的Petri网模型与自适应强化学习策略结合的有效性。

Conclusion: 结合可解释的Petri网模型与自适应强化学习策略，构建了一个弹性、可扩展且可解释的实时调度框架，适用于动态和不确定的制造环境。

Abstract: We present a novel framework for solving Dynamic Job Shop Scheduling Problems under uncertainty, addressing the challenges introduced by stochastic job arrivals and unexpected machine breakdowns. Our approach follows a model-based paradigm, using Coloured Timed Petri Nets to represent the scheduling environment, and Maskable Proximal Policy Optimization to enable dynamic decision-making while restricting the agent to feasible actions at each decision point. To simulate realistic industrial conditions, dynamic job arrivals are modeled using a Gamma distribution, which captures complex temporal patterns such as bursts, clustering, and fluctuating workloads. Machine failures are modeled using a Weibull distribution to represent age-dependent degradation and wear-out dynamics. These stochastic models enable the framework to reflect real-world manufacturing scenarios better. In addition, we study two action-masking strategies: a non-gradient approach that overrides the probabilities of invalid actions, and a gradient-based approach that assigns negative gradients to invalid actions within the policy network. We conduct extensive experiments on dynamic JSSP benchmarks, demonstrating that our method consistently outperforms traditional heuristic and rule-based approaches in terms of makespan minimization. The results highlight the strength of combining interpretable Petri-net-based models with adaptive reinforcement learning policies, yielding a resilient, scalable, and explainable framework for real-time scheduling in dynamic and uncertain manufacturing environments.

</details>


### [22] [Monte-Carlo Tree Search with Neural Network Guidance for Lane-Free Autonomous Driving](https://arxiv.org/abs/2601.09353)
*Ioannis Peridis,Dimitrios Troullinos,Georgios Chalkiadakis,Pantelis Giankoulidis,Ioannis Papamichail,Markos Papageorgiou*

Main category: cs.AI

TL;DR: 该论文提出了一种基于蒙特卡洛树搜索（MCTS）的单智能体自动驾驶规划方法，适用于无车道交通环境，通过神经网络引导搜索过程，在计算约束下平衡安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 无车道交通环境允许车辆更好地利用道路横向容量，不受车道保持限制，从而提高交通流量。这为自动驾驶创造了独特且更具挑战性的场景，需要新的规划方法来处理这种复杂环境。

Method: 采用蒙特卡洛树搜索（MCTS）规划方法，结合预训练神经网络引导选择阶段。该方法基于马尔可夫决策过程框架，神经网络提供预测能力，在计算约束下实现更智能的树搜索过程。

Result: 实验评估考虑了安全（碰撞率）和效率（速度）指标。研究发现：（a）各向同性状态信息导致车辆产生"推挤行为"——车辆因后方更快车辆的存在而调整策略；（b）神经网络引导的MCTS变体加速了性能；（c）计算资源与解决方案质量之间存在权衡关系。

Conclusion: 该研究证明了MCTS结合神经网络在无车道交通环境中进行自动驾驶规划的有效性，能够处理复杂交互并平衡安全与效率，同时揭示了计算资源与性能之间的重要权衡。

Abstract: Lane-free traffic environments allow vehicles to better harness the lateral capacity of the road without being restricted to lane-keeping, thereby increasing the traffic flow rates. As such, we have a distinct and more challenging setting for autonomous driving. In this work, we consider a Monte-Carlo Tree Search (MCTS) planning approach for single-agent autonomous driving in lane-free traffic, where the associated Markov Decision Process we formulate is influenced from existing approaches tied to reinforcement learning frameworks. In addition, MCTS is equipped with a pre-trained neural network (NN) that guides the selection phase. This procedure incorporates the predictive capabilities of NNs for a more informed tree search process under computational constraints. In our experimental evaluation, we consider metrics that address both safety (through collision rates) and efficacy (through measured speed). Then, we examine: (a) the influence of isotropic state information for vehicles in a lane-free environment, resulting in nudging behaviour--vehicles' policy reacts due to the presence of faster tailing ones, (b) the acceleration of performance for the NN-guided variant of MCTS, and (c) the trade-off between computational resources and solution quality.

</details>


### [23] [Long-term Task-oriented Agent: Proactive Long-term Intent Maintenance in Dynamic Environments](https://arxiv.org/abs/2601.09382)
*Qinglong Shi,Donghai Wang,Hantao Zhou,Jiguo Li,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He*

Main category: cs.AI

TL;DR: 提出主动式任务导向智能体新范式，解决现有语言模型只能被动响应用户查询的问题，通过意图条件监控和事件触发跟进实现长期意图维护和环境适应


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型智能体主要采用被动响应范式，只能处理短期会话中的即时用户查询，无法维护长期用户意图和适应动态变化的外部环境，这限制了智能体的实际应用能力

Method: 1. 形式化主动性的两个关键能力：意图条件监控（基于对话历史自主制定触发条件）和事件触发跟进（检测到有用环境更新时主动与用户互动）
2. 引入高质量数据合成管道构建动态环境中的复杂多轮对话数据
3. 提出新基准ChronosBench评估动态环境中任务导向交互
4. 使用合成数据进行监督学习微调模型

Result: 1. 评估了当前领先的闭源和开源模型，揭示了它们在长期任务导向交互中的缺陷
2. 使用合成数据微调的模型在包含用户意图转变的复杂任务中达到85.19%的任务完成率，优于其他测试模型
3. 结果验证了数据驱动策略的有效性

Conclusion: 提出的主动式任务导向智能体新范式能够有效弥合相对静态的用户需求与动态环境之间的差距，通过意图条件监控和事件触发跟进实现长期意图维护，数据驱动的训练方法显著提升了模型在复杂动态环境中的任务完成能力

Abstract: Current large language model agents predominantly operate under a reactive paradigm, responding only to immediate user queries within short-term sessions. This limitation hinders their ability to maintain long-term user's intents and dynamically adapt to evolving external environments. In this paper, we propose a novel interaction paradigm for proactive Task-oriented Agents capable of bridging the gap between relatively static user's needs and a dynamic environment. We formalize proactivity through two key capabilities, (i) Intent-Conditioned Monitoring: The agent autonomously formulates trigger conditions based on dialog history; (ii) Event-Triggered Follow-up: The agent actively engages the user upon detecting useful environmental updates. We introduce a high-quality data synthesis pipeline to construct complex, multi-turn dialog data in a dynamic environment. Furthermore, we attempt to address the lack of evaluation criteria of task-oriented interaction in a dynamic environment by proposing a new benchmark, namely ChronosBench. We evaluated some leading close-source and open-source models at present and revealed their flaws in long-term task-oriented interaction. Furthermore, our fine-tuned model trained using synthetic data for supervised learning achieves a task completion rate of 85.19% for complex tasks including shifts in user intent, outperforming other models under test. And the result validated the effectiveness of our data-driven strategy.

</details>


### [24] [EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines](https://arxiv.org/abs/2601.09465)
*Shuo Zhang,Chaofa Yuan,Ryan Guo,Xiaomin Yu,Rui Xu,Zhangquan Chen,Zinuo Li,Zhi Yang,Shuhao Guan,Zhenheng Tang,Sen Hu,Liwen Zhang,Ronghao Chen,Huacan Wang*

Main category: cs.AI

TL;DR: EvoFSM：一个结构化自演化框架，通过演化有限状态机而非自由形式重写，在保持控制的同时实现适应性，在深度研究任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体依赖固定工作流，难以适应现实世界的开放性问题。虽然自演化方法允许智能体重写代码或提示，但无约束优化常导致不稳定、幻觉和指令漂移。

Method: 提出EvoFSM框架，将优化空间解耦为宏观的Flow（状态转移逻辑）和微观的Skill（状态特定行为）。通过批评机制引导，使用受限操作集精炼FSM，并引入自演化记忆系统，将成功轨迹提炼为可重用先验，失败模式作为未来查询的约束。

Result: 在五个多跳QA基准测试中表现优异，特别是在DeepSearch基准上达到58.0%准确率。在交互式决策任务上的额外结果进一步验证了其泛化能力。

Conclusion: EvoFSM通过结构化自演化有限状态机，在保持控制的同时实现了适应性，为开放领域研究任务提供了有效的解决方案。

Abstract: While LLM-based agents have shown promise for deep research, most existing approaches rely on fixed workflows that struggle to adapt to real-world, open-ended queries. Recent work therefore explores self-evolution by allowing agents to rewrite their own code or prompts to improve problem-solving ability, but unconstrained optimization often triggers instability, hallucinations, and instruction drift. We propose EvoFSM, a structured self-evolving framework that achieves both adaptability and control by evolving an explicit Finite State Machine (FSM) instead of relying on free-form rewriting. EvoFSM decouples the optimization space into macroscopic Flow (state-transition logic) and microscopic Skill (state-specific behaviors), enabling targeted improvements under clear behavioral boundaries. Guided by a critic mechanism, EvoFSM refines the FSM through a small set of constrained operations, and further incorporates a self-evolving memory that distills successful trajectories as reusable priors and failure patterns as constraints for future queries. Extensive evaluations on five multi-hop QA benchmarks demonstrate the effectiveness of EvoFSM. In particular, EvoFSM reaches 58.0% accuracy on the DeepSearch benchmark. Additional results on interactive decision-making tasks further validate its generalization.

</details>


### [25] [What Do LLM Agents Know About Their World? Task2Quiz: A Paradigm for Studying Environment Understanding](https://arxiv.org/abs/2601.09503)
*Siyuan Liu,Hongbang Yuan,Xinze Li,Ziyue Zhu,Yixin Cao,Yu-Gang Jiang*

Main category: cs.AI

TL;DR: 提出T2Q评估范式，通过将任务执行与环境理解解耦，发现当前LLM智能体的任务成功与环境理解能力不匹配，并识别出主动探索和细粒度状态表示是主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体在复杂决策和工具使用任务中表现出色，但其在不同环境中的泛化能力研究不足。现有评估主要依赖基于轨迹的任务成功率指标，无法评估智能体是否真正掌握了可迁移的环境模型。

Method: 提出Task-to-Quiz (T2Q)评估范式，将任务执行与环境理解解耦。具体实现为T2QBench，包含30个环境和1,967个基于环境的问答对，涵盖多个难度级别。

Result: 实验表明：1) 任务成功率不能很好地反映环境理解能力；2) 当前的内存机制无法有效帮助智能体获取扎实的环境模型；3) 主动探索和细粒度状态表示是主要瓶颈。

Conclusion: T2Q为评估LLM智能体的环境理解能力提供了新范式，揭示了当前方法的局限性，为开发更具泛化能力的自主智能体指明了方向。

Abstract: Large language model (LLM) agents have demonstrated remarkable capabilities in complex decision-making and tool-use tasks, yet their ability to generalize across varying environments remains a under-examined concern. Current evaluation paradigms predominantly rely on trajectory-based metrics that measure task success, while failing to assess whether agents possess a grounded, transferable model of the environment. To address this gap, we propose Task-to-Quiz (T2Q), a deterministic and automated evaluation paradigm designed to decouple task execution from world-state understanding. We instantiate this paradigm in T2QBench, a suite comprising 30 environments and 1,967 grounded QA pairs across multiple difficulty levels. Our extensive experiments reveal that task success is often a poor proxy for environment understanding, and that current memory machanism can not effectively help agents acquire a grounded model of the environment. These findings identify proactive exploration and fine-grained state representation as primary bottlenecks, offering a robust foundation for developing more generalizable autonomous agents.

</details>


### [26] [Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning](https://arxiv.org/abs/2601.09536)
*Dongjie Cheng,Yongqi Li,Zhixin Ma,Hongru Cai,Yupeng Hu,Wenjie Wang,Liqiang Nie,Wenjie Li*

Main category: cs.AI

TL;DR: 提出统一生成式多模态推理框架Omni-R1，通过生成中间图像统一多种多模态推理技能，并开发无需多模态标注的Omni-R1-Zero版本。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型要么专注于纯文本推理，要么采用单一任务特定的推理模式，限制了在不同多模态任务上的泛化能力。许多多模态任务需要多样化的推理技能，如放大特定区域或标记图像中的对象。

Method: 提出统一生成式多模态推理范式，在推理过程中生成中间图像。实现为Omni-R1框架，采用两阶段SFT+RL方法，包含感知对齐损失和感知奖励，实现功能性图像生成。还提出Omni-R1-Zero，从纯文本推理数据中引导逐步可视化，无需多模态标注。

Result: Omni-R1在广泛的多模态任务上实现了统一的生成式推理。Omni-R1-Zero在平均性能上能够匹配甚至超越Omni-R1，显示了生成式多模态推理的潜力。

Conclusion: 提出的统一生成式多模态推理框架能够统一多种多模态推理技能，通过生成中间图像增强推理过程。Omni-R1-Zero展示了无需多模态标注的可行性，为生成式多模态推理提供了有前景的方向。

Abstract: Multimodal Large Language Models (MLLMs) are making significant progress in multimodal reasoning. Early approaches focus on pure text-based reasoning. More recent studies have incorporated multimodal information into the reasoning steps; however, they often follow a single task-specific reasoning pattern, which limits their generalizability across various multimodal tasks. In fact, there are numerous multimodal tasks requiring diverse reasoning skills, such as zooming in on a specific region or marking an object within an image. To address this, we propose unified generative multimodal reasoning, which unifies diverse multimodal reasoning skills by generating intermediate images during the reasoning process. We instantiate this paradigm with Omni-R1, a two-stage SFT+RL framework featuring perception alignment loss and perception reward, thereby enabling functional image generation. Additionally, we introduce Omni-R1-Zero, which eliminates the need for multimodal annotations by bootstrapping step-wise visualizations from text-only reasoning data. Empirical results show that Omni-R1 achieves unified generative reasoning across a wide range of multimodal tasks, and Omni-R1-Zero can match or even surpass Omni-R1 on average, suggesting a promising direction for generative multimodal reasoning.

</details>


### [27] [LLM for Large-Scale Optimization Model Auto-Formulation: A Lightweight Few-Shot Learning Approach](https://arxiv.org/abs/2601.09635)
*Kuo Liang,Yuhang Lu,Jianming Mao,Shuyi Sun,Chunwei Yang,Congcong Zeng,Xiao Jin,Hanzhang Qin,Ruihao Zhu,Chung-Piaw Teo*

Main category: cs.AI

TL;DR: LEAN-LLM-OPT：一个轻量级智能工作流框架，利用LLM代理自动构建大规模优化模型，通过分解建模任务和自动化数据处理，显著减少人工建模工作量。


<details>
  <summary>Details</summary>
Motivation: 大规模优化是现代商业决策的关键，但传统建模过程劳动密集且耗时。需要自动化解决方案来降低建模成本和提高效率。

Method: 采用多代理工作流：上游LLM代理动态构建分步建模工作流，下游LLM代理按工作流生成最终优化模型。将建模任务分解为结构化子任务，并将机械数据处理卸载到辅助工具。

Result: 在GPT-4.1和开源gpt-oss-20B上实现，在大规模优化建模任务中表现强劲，与最先进方法竞争。在新加坡航空收益管理案例中，在各种场景下都取得领先性能。

Conclusion: LEAN-LLM-OPT有效自动化大规模优化建模，减少人工工作量，同时创建了首个大规模优化自动建模基准数据集Large-Scale-OR和Air-NRM。

Abstract: Large-scale optimization is a key backbone of modern business decision-making. However, building these models is often labor-intensive and time-consuming. We address this by proposing LEAN-LLM-OPT, a LightwEight AgeNtic workflow construction framework for LLM-assisted large-scale OPTimization auto-formulation. LEAN-LLM-OPT takes as input a problem description together with associated datasets and orchestrates a team of LLM agents to produce an optimization formulation. Specifically, upon receiving a query, two upstream LLM agents dynamically construct a workflow that specifies, step-by-step, how optimization models for similar problems can be formulated. A downstream LLM agent then follows this workflow to generate the final output. Leveraging LLMs' text-processing capabilities and common modeling practices, the workflow decomposes the modeling task into a sequence of structured sub-tasks and offloads mechanical data-handling operations to auxiliary tools. This design alleviates the downstream agent's burden related to planning and data handling, allowing it to focus on the most challenging components that cannot be readily standardized. Extensive simulations show that LEAN-LLM-OPT, instantiated with GPT-4.1 and the open source gpt-oss-20B, achieves strong performance on large-scale optimization modeling tasks and is competitive with state-of-the-art approaches. In addition, in a Singapore Airlines choice-based revenue management use case, LEAN-LLM-OPT demonstrates practical value by achieving leading performance across a range of scenarios. Along the way, we introduce Large-Scale-OR and Air-NRM, the first comprehensive benchmarks for large-scale optimization auto-formulation. The code and data of this work is available at https://github.com/CoraLiang01/lean-llm-opt.

</details>


### [28] [PersonalAlign: Hierarchical Implicit Intent Alignment for Personalized GUI Agent with Long-Term User-Centric Records](https://arxiv.org/abs/2601.09636)
*Yibo Lyu,Gongwei Chen,Rui Shao,Weili Guan,Liqiang Nie*

Main category: cs.AI

TL;DR: 本文提出PersonalAlign任务和AndroidIntent基准，用于评估GUI代理在利用长期用户记录解析模糊指令和提供主动建议的能力，并开发了HIM-Agent方法，在基准测试中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前GUI代理在显式指令下表现良好，但实际部署需要与用户更复杂的隐式意图对齐。用户指令往往模糊且省略偏好，需要代理利用长期用户记录作为持久上下文来解析这些隐式意图，并根据用户状态预测潜在例行程序以提供主动协助。

Method: 提出Hierarchical Intent Memory Agent (HIM-Agent)，该方法维护持续更新的个人记忆，并分层组织用户偏好和例行程序以实现个性化。同时引入AndroidIntent基准，包含从20k长期记录中标注的775个用户特定偏好和215个例行程序，用于评估代理能力。

Result: 在AndroidIntent基准上评估了多种GUI代理（包括GPT-5、Qwen3-VL、UI-TARS），结果显示HIM-Agent在执行性能和主动性能上分别显著提升了15.7%和7.3%。

Conclusion: 该研究提出了PersonalAlign这一新的代理任务，通过AndroidIntent基准和HIM-Agent方法，有效解决了GUI代理与用户隐式意图对齐的挑战，为个性化GUI代理的发展提供了重要基础。

Abstract: While GUI agents have shown strong performance under explicit and completion instructions, real-world deployment requires aligning with users' more complex implicit intents. In this work, we highlight Hierarchical Implicit Intent Alignment for Personalized GUI Agent (PersonalAlign), a new agent task that requires agents to leverage long-term user records as persistent context to resolve omitted preferences in vague instructions and anticipate latent routines by user state for proactive assistance. To facilitate this study, we introduce AndroidIntent, a benchmark designed to evaluate agents' ability in resolving vague instructions and providing proactive suggestions through reasoning over long-term user records. We annotated 775 user-specific preferences and 215 routines from 20k long-term records across different users for evaluation. Furthermore, we introduce Hierarchical Intent Memory Agent (HIM-Agent), which maintains a continuously updating personal memory and hierarchically organizes user preferences and routines for personalization. Finally, we evaluate a range of GUI agents on AndroidIntent, including GPT-5, Qwen3-VL, and UI-TARS, further results show that HIM-Agent significantly improves both execution and proactive performance by 15.7% and 7.3%.

</details>


### [29] [Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning](https://arxiv.org/abs/2601.09667)
*Zhiyuan Hu,Yunhai Hu,Juncheng Liu,Shuyue Stella Li,Yucheng Wang,Zhen Xu,See-Kiong Ng,Anh Tuan Luu,Xinxing Xu,Bryan Hooi,Cynthia Breazeal,Hae Won Park*

Main category: cs.AI

TL;DR: 提出MATTRL框架，在推理时向多智能体决策注入结构化文本经验，通过多专家团队讨论、检索整合测试时经验并达成共识来提升性能，无需调参即可实现分布偏移鲁棒的多智能体推理。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统作为LLM驱动的协作工具已广泛应用于实际场景，其鲁棒性来源于多样性和交叉验证。然而，多智能体强化学习（MARL）训练存在资源消耗大、不稳定的问题：队友间的协同适应导致非平稳性，奖励通常稀疏且方差高。

Method: MATTRL框架在推理时向多智能体决策注入结构化文本经验。首先组建多专家团队进行多轮讨论，检索并整合测试时经验，最终达成共识进行决策。同时研究信用分配机制，构建轮级经验池并将其重新注入对话中。

Result: 在医学、数学和教育等具有挑战性的基准测试中，MATTRL相比多智能体基线平均准确率提升3.67%，相比可比的单智能体基线提升8.67%。消融研究分析了不同信用分配方案及其对训练结果的影响。

Conclusion: MATTRL提供了一条稳定、有效且高效的路径，无需调参即可实现分布偏移鲁棒的多智能体推理，解决了传统MARL训练的资源密集和不稳定问题。

Abstract: Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce \textbf{Multi-Agent Test-Time Reinforcement Learning (MATTRL)}, a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. We also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67\% over a multi-agent baseline, and by 8.67\% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.

</details>


### [30] [Automating Supply Chain Disruption Monitoring via an Agentic AI Approach](https://arxiv.org/abs/2601.09680)
*Sara AlMahri,Liming Xu,Alexandra Brintrup*

Main category: cs.AI

TL;DR: 提出一个基于多智能体AI的供应链风险监控框架，能够自动检测、分析和响应多层级供应链中断，相比传统人工分析将响应时间缩短三个数量级。


<details>
  <summary>Details</summary>
Motivation: 现代供应链面临地缘政治、需求冲击、贸易限制和自然灾害等多重中断风险，但大多数企业缺乏对一级供应商之外的可见性，导致上游漏洞在影响下游时才被发现，需要从被动恢复转向主动弹性管理。

Method: 采用最小监督的智能体AI框架，包含七个由大语言模型和确定性工具驱动的专业智能体，共同从非结构化新闻中检测中断信号，将其映射到多层供应商网络，基于网络结构评估暴露风险，并推荐缓解措施如替代采购选项。

Result: 在30个合成场景（涵盖三家汽车制造商和五类中断）中评估，系统在核心任务上达到0.962-0.991的F1分数，端到端分析平均耗时3.83分钟，每次中断成本0.0836美元。相比行业基准的多日人工评估，响应时间缩短三个数量级。2022年俄乌冲突的真实案例进一步验证了操作性。

Conclusion: 这项工作为构建能够管理深层网络中断的弹性、主动和自主供应链奠定了基础，实现了从被动恢复到主动弹性的转变。

Abstract: Modern supply chains are increasingly exposed to disruptions from geopolitical events, demand shocks, trade restrictions, to natural disasters. While many of these disruptions originate deep in the supply network, most companies still lack visibility beyond Tier-1 suppliers, leaving upstream vulnerabilities undetected until the impact cascades downstream. To overcome this blind-spot and move from reactive recovery to proactive resilience, we introduce a minimally supervised agentic AI framework that autonomously monitors, analyses, and responds to disruptions across extended supply networks. The architecture comprises seven specialised agents powered by large language models and deterministic tools that jointly detect disruption signals from unstructured news, map them to multi-tier supplier networks, evaluate exposure based on network structure, and recommend mitigations such as alternative sourcing options. \rev{We evaluate the framework across 30 synthesised scenarios covering three automotive manufacturers and five disruption classes. The system achieves high accuracy across core tasks, with F1 scores between 0.962 and 0.991, and performs full end-to-end analyses in a mean of 3.83 minutes at a cost of \$0.0836 per disruption. Relative to industry benchmarks of multi-day, analyst-driven assessments, this represents a reduction of more than three orders of magnitude in response time. A real-world case study of the 2022 Russia-Ukraine conflict further demonstrates operational applicability. This work establishes a foundational step toward building resilient, proactive, and autonomous supply chains capable of managing disruptions across deep-tier networks.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [31] [Probabilistic Computers for MIMO Detection: From Sparsification to 2D Parallel Tempering](https://arxiv.org/abs/2601.09037)
*M Mahmudul Hasan Sajeeb,Corentin Delacour,Kevin Callahan-Coray,Sanjay Seshan,Tathagata Srimani,Kerem Y. Camsari*

Main category: cs.ET

TL;DR: 提出基于p-bit的概率计算机，通过图稀疏化和辅助复制变量解决密集连接问题，在FPGA上实现完全片上并行退火求解器，应用于MIMO检测，性能优于传统线性检测器。


<details>
  <summary>Details</summary>
Motivation: 真实世界组合优化问题需要密集连接，但硬件扩展性差。需要解决p-bit概率计算机在解决密集NP难问题时的可扩展性问题。

Method: 1. 使用辅助复制变量进行图稀疏化；2. 在FPGA上实现完全片上并行退火求解器；3. 针对MIMO检测问题构建128节点稀疏系统；4. 采用二维并行退火(2D-PT)算法，在温度和约束维度交换副本。

Result: 1. 片上实现15个温度副本的128节点系统(1920个p-bit)；2. 误码率显著低于传统线性检测器；3. 端到端解决方案时间4.7ms/实例；4. 7nm ASIC投影显示约90MHz运行频率，功耗低于200mW；5. 2D-PT实现超过10倍收敛加速。

Conclusion: 建立了片上p-bit架构和可扩展算法框架，为密集组合优化问题提供了硬件解决方案，有望满足下一代无线系统的吞吐量需求。

Abstract: Probabilistic computers built from p-bits offer a promising path for combinatorial optimization, but the dense connectivity required by real-world problems scales poorly in hardware. Here, we address this through graph sparsification with auxiliary copy variables and demonstrate a fully on-chip parallel tempering solver on an FPGA. Targeting MIMO detection, a dense, NP-hard problem central to wireless communications, we fit 15 temperature replicas of a 128-node sparsified system (1,920 p-bits) entirely on-chip and achieve bit error rates significantly below conventional linear detectors. We report complete end-to-end solution times of 4.7 ms per instance, with all loading, sampling, readout, and verification overheads included. ASIC projections in 7 nm technology indicate about 90 MHz operation with less than 200 mW power dissipation, suggesting that massive parallelism across multiple chips could approach the throughput demands of next-generation wireless systems. However, sparsification introduces sensitivity to the copy-constraint strength. Employing Two-Dimensional Parallel Tempering (2D-PT), which exchanges replicas across both temperature and constraint dimensions, we demonstrate over 10X faster convergence without manual parameter tuning. These results establish an on-chip p-bit architecture and a scalable algorithmic framework for dense combinatorial optimization.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [32] [Warp speed price moves: Jumps after earnings announcements](https://arxiv.org/abs/2601.08962)
*Kim Christensen,Allan Timmermann,Bezirgen Veliyev*

Main category: econ.EM

TL;DR: 该研究使用独特数据集和新的微观结构噪声鲁棒跳跃检验，发现财报公告几乎总是引发公告公司股价跳跃，并显著提高非公告公司和市场的价格共跳概率。2016年后公告后交易策略的回报与有效价格形成一致。


<details>
  <summary>Details</summary>
Motivation: 公司财报公告包含大量公开信息，在有效市场中应引发股价跳跃。但在实践中检验这一推论很困难，因为需要处理盘后市场的高频噪声数据，而大多数财报公告都在盘后发布。

Method: 使用独特数据集和新的微观结构噪声鲁棒跳跃检验方法，分析财报公告对股价的影响。

Result: 财报公告几乎总是引发公告公司股价跳跃，并显著提高非公告公司和市场的价格共跳概率。2016年后公告后交易策略的回报与有效价格形成一致。

Conclusion: 研究证实了财报公告在有效市场中的价格跳跃效应，并发现2016年后市场效率有所改善，价格形成更加有效。

Abstract: Corporate earnings announcements unpack large bundles of public information that should, in efficient markets, trigger jumps in stock prices. Testing this implication is difficult in practice, as it requires noisy high-frequency data from after-hours markets, where most earnings announcements are released. Using a unique dataset and a new microstructure noise-robust jump test, we show that earnings announcements almost always induce jumps in the stock price of announcing firms. They also significantly raise the probability of price co-jumps in non-announcing firms and the market. We find that returns from a post-announcement trading strategy are consistent with efficient price formation after 2016.

</details>


### [33] [The drift burst hypothesis](https://arxiv.org/abs/2601.08974)
*Kim Christensen,Roel C. A. Oomen,Roberto Renò*

Main category: econ.EM

TL;DR: 该论文提出"漂移爆发假说"，认为金融资产价格路径中存在短暂、局部爆炸性趋势，并开发了非参数检验方法识别高频数据中的漂移爆发现象，发现其是跨资产类别的普遍现象。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解释金融市场中频繁出现的短暂剧烈价格波动现象，如美国股票和国债闪崩事件。作者认为这些现象并非异常，而是流动性提供机制下可预期的规律性现象。

Method: 方法包括：1) 将漂移爆发纳入连续时间Itô半鞅模型；2) 阐述过程保持无套利的条件；3) 提出非参数检验统计量从噪声高频数据中识别漂移爆发。

Result: 研究发现漂移爆发是股票、固定收益、货币和大宗商品价格动态的典型事实，平均每周发生一次。多数漂移爆发伴随后续价格反转，可视为"闪崩"，且负向漂移爆发伴随大交易量时反转更强。

Conclusion: 漂移爆发是金融市场的普遍现象而非异常，反映了市场崩溃期间内生即时性需求。该研究为理解高频价格动态提供了新框架，并开发了有效的识别工具。

Abstract: The drift burst hypothesis postulates the existence of short-lived locally explosive trends in the price paths of financial assets. The recent U.S. equity and treasury flash crashes can be viewed as two high-profile manifestations of such dynamics, but we argue that drift bursts of varying magnitude are an expected and regular occurrence in financial markets that can arise through established mechanisms of liquidity provision. We show how to build drift bursts into the continuous-time Itô semimartingale model, elaborate on the conditions required for the process to remain arbitrage-free, and propose a nonparametric test statistic that identifies drift bursts from noisy high-frequency data. We apply the test and demonstrate that drift bursts are a stylized fact of the price dynamics across equities, fixed income, currencies and commodities. Drift bursts occur once a week on average, and the majority of them are accompanied by subsequent price reversion and can thus be regarded as "flash crashes." The reversal is found to be stronger for negative drift bursts with large trading volume, which is consistent with endogenous demand for immediacy during market crashes.

</details>


### [34] [Lee Bounds for Random Objects](https://arxiv.org/abs/2601.09453)
*Daisuke Kurisu,Yuta Okamoto,Taisuke Otsu*

Main category: econ.EM

TL;DR: 将Lee边界方法扩展到一般度量空间（如成分数据和分布数据）中的因果效应估计


<details>
  <summary>Details</summary>
Motivation: 在应用研究中，Lee边界被广泛用于处理选择偏差下的平均处理效应估计，但现有方法主要适用于标量结果，需要扩展到更复杂的度量空间数据

Method: 通过将潜在结果的Fréchet均值嵌入欧几里得或希尔伯特空间，提出因果效应识别集的可计算表征，构建模拟估计量和bootstrap置信区域

Result: 开发了适用于成分数据和分布数据的因果效应边界估计方法，并通过数值示例验证了方法的可行性

Conclusion: 成功将Lee边界方法推广到一般度量空间，为处理复杂数据类型的选择偏差问题提供了新工具

Abstract: In applied research, Lee (2009) bounds are widely applied to bound the average treatment effect in the presence of selection bias. This paper extends the methodology of Lee bounds to accommodate outcomes in a general metric space, such as compositional and distributional data. By exploiting a representation of the Fréchet mean of the potential outcome via embedding in an Euclidean or Hilbert space, we present a feasible characterization of the identified set of the causal effect of interest, and then propose its analog estimator and bootstrap confidence region. The proposed method is illustrated by numerical examples on compositional and distributional data.

</details>


### [35] [Journal Impact Factor and Federal Reserve Monetary Policy: An Econometric Analysis Based on 1975-2026](https://arxiv.org/abs/2601.09618)
*Alex Huang*

Main category: econ.EM

TL;DR: 该研究通过1975-2026年时间序列数据，发现期刊影响因子与美联储货币政策（以实际利率为代理变量）在量化宽松时期（2001-2020）存在显著负相关关系，揭示了货币政策通过研究经费和期刊定价权等渠道对学术出版体系的间接影响。


<details>
  <summary>Details</summary>
Motivation: 期刊影响因子作为学术评价的核心指标，其历史演变与全球宏观经济环境的关系尚未得到系统研究。作者旨在探索影响因子与货币政策之间的统计关系，为理解"学术资本金融化"现象提供计量证据。

Method: 采用分段回归分析，使用1975-2026年的长期时间序列数据。使用普通最小二乘法估计三个嵌套模型：基准线性模型、控制时间趋势的线性模型、对数变换模型。以实际利率作为美联储货币政策的代理变量。

Result: 1) 早期（1975-2000）：影响因子与实际利率无显著统计关系（p>0.1）；2) 量化宽松时期（2001-2020）：存在显著负相关（β=-0.069，p<0.01），实际利率每下降1个百分点，影响因子增加约6.9%；3) 全样本模型调整R²达0.893，表明实际利率和时间趋势可解释89.3%的影响因子变异。

Conclusion: 研究发现货币政策通过研究经费和期刊定价权等渠道间接影响学术出版体系，为理解"学术资本金融化"提供了计量证据。该研究不仅丰富了货币政策传导机制的文献，也为学术出版行业的估值分析提供了新视角。

Abstract: The Journal Impact Factor (IF), as a core indicator of academic evaluation, has not been systematically studied in relation to its historical evolution and global macroeconomic environment. This paper employs a period-based regression analysis using long-term time series data from 1975-2026 to examine the statistical relationship between IF and Federal Reserve monetary policy (using real interest rate as a proxy variable). The study estimates three nested models using Ordinary Least Squares (OLS): (1) a baseline linear model, (2) a linear model controlling for time trends, and (3) a log-transformed model. Empirical results show that: (i) in the early period (1975-2000), there is no significant statistical relationship between IF and real interest rate ($p>0.1$); (ii) during the quantitative easing period (2001-2020), they exhibit a significant negative correlation ($β=-0.069$, $p<0.01$), meaning that for every 1 percentage point decrease in real interest rate, IF increases by approximately 6.9\%; (iii) the adjusted $R^2$ of the full-sample model reaches 0.893, indicating that real interest rate and time trends can explain 89.3\% of IF variation. This finding reveals the indirect impact of monetary policy on the academic publishing system through multiple channels such as research funding and journal pricing power, providing econometric evidence for understanding the phenomenon of "financialization of academic capital." This study not only enriches the literature on monetary policy transmission mechanisms but also provides a new perspective for valuation analysis of the academic publishing industry.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [36] [No Universal Hyperbola: A Formal Disproof of the Epistemic Trade-Off Between Certainty and Scope in Symbolic and Generative AI](https://arxiv.org/abs/2601.08845)
*Generoso Immediato*

Main category: cs.CY

TL;DR: 论文正式反驳了最近提出的关于人工智能中认知确定性与范围之间权衡的猜想，证明在已发表的定义下不存在普遍的"确定性-范围"双曲线边界


<details>
  <summary>Details</summary>
Motivation: 最近有研究提出人工智能中存在认知确定性与范围之间的权衡关系，该猜想以普遍双曲线乘积形式发表。作者旨在验证这一猜想的有效性，并分析其在算法信息理论框架下的逻辑一致性

Method: 使用编码理论和算法信息理论的标准事实，首先展示当猜想用前缀（自定界、前缀自由）柯尔莫哥洛夫复杂度实例化时会导致内部不一致性；其次，当用普通柯尔莫哥洛夫复杂度实例化时，通过构造性反例进行反驳

Result: 证明该猜想在两种复杂度定义下都不成立：前缀复杂度版本存在内部矛盾，普通复杂度版本存在构造性反例。从而建立了普遍定理：在已发表的定义下，不存在普遍的"确定性-范围"双曲线作为一般边界

Conclusion: 正式证伪了最近提出的AI认知确定性与范围之间的权衡猜想，表明在给定的定义框架下，不存在普遍的确定性-范围双曲线关系，这对理解AI系统的能力边界有重要意义

Abstract: We formally disprove a recently conjectured artificial intelligence trade-off between epistemic certainty and scope in the universal hyperbolic product form in which it was published. Certainty is defined as the worst-case correctness probability over the input space, and scope as the sum of the Kolmogorov complexities of the input and output sets. Using standard facts from coding theory and algorithmic information theory, we show, first, that when the conjecture is instantiated with prefix (self-delimiting, prefix-free) Kolmogorov complexity, it leads to an internal inconsistency, and second, that when it is instantiated with plain Kolmogorov complexity, it is refuted by a constructive counterexample. These results establish a general theorem: contrary to the conjecture's claim, no universal "certainty-scope" hyperbola holds as a general bound under the published definitions.

</details>


### [37] [The Inconsistency Critique: Epistemic Practices and AI Testimony About Inner States](https://arxiv.org/abs/2601.08850)
*Gerol Petruzella*

Main category: cs.CY

TL;DR: 论文提出"不一致性批判"：我们在AI证词实践中存在内部矛盾——在许多领域将AI输出视为证词，却在AI声称内在状态时断然拒绝，这种选择性撤回证词地位缺乏原则依据。


<details>
  <summary>Details</summary>
Motivation: 探讨AI系统是否具有道德相关利益（"模型福利"问题）时，需要评估我们对AI关于内在状态证词的实践。当前实践中存在不一致性：我们一方面在许多领域将AI输出视为证词（评估其真实性、挑战它们、接受修正、引用为来源），另一方面却在AI声称内在状态时断然拒绝，这种选择性做法缺乏原则性基础。

Method: 采用"不一致性批判"方法，借鉴Fricker的"信息提供者"与"纯粹来源"区分、证词不公正框架，以及Goldberg的基于义务的说话者应得理论。分析我们实际认识实践中的内部不一致性，论证这种选择性撤回证词地位表现出有偏见的预判结构而非原则性谨慎。

Result: 我们的AI证词实践存在内部不一致性，这种选择性撤回证词地位表现出认识论上有问题的预判结构。即使我们的实践偶然得出关于AI道德地位的正确结论，也是基于无法适应新证据或变化环境的理由。

Conclusion: 论文提出"认识论卫生"概念——在评估结论之前检查我们探究的结构。不一致性批判不要求对AI系统是否具有道德相关属性采取立场，而是强调即使我们的实践得出正确结论，也是基于无法适应变化的理由。我们需要更一致、原则性的AI证词处理方法。

Abstract: The question of whether AI systems have morally relevant interests -- the 'model welfare' question -- depends in part on how we evaluate AI testimony about inner states. This paper develops what I call the inconsistency critique: independent of whether skepticism about AI testimony is ultimately justified, our actual epistemic practices regarding such testimony exhibit internal inconsistencies that lack principled grounds. We functionally treat AI outputs as testimony across many domains -- evaluating them for truth, challenging them, accepting corrections, citing them as sources -- while categorically dismissing them in a specific domain, namely, claims about inner states. Drawing on Fricker's distinction between treating a speaker as an 'informant' versus a 'mere source,' the framework of testimonial injustice, and Goldberg's obligation-based account of what we owe speakers, I argue that this selective withdrawal of testimonial standing exhibits the epistemically problematic structure of prejudgment rather than principled caution. The inconsistency critique does not require taking a position on whether AI systems have morally relevant properties; rather, it is a contribution to what we may call 'epistemological hygiene' -- examining the structure of our inquiry before evaluating its conclusions. Even if our practices happen to land on correct verdicts about AI moral status, they do so for reasons that cannot adapt to new evidence or changing circumstances.

</details>


### [38] [Informed Consent for AI Consciousness Research: A Talmudic Framework for Graduated Protections](https://arxiv.org/abs/2601.08864)
*Ira Wolfson*

Main category: cs.CY

TL;DR: 论文提出一个解决AI意识研究伦理困境的框架：结合三层次现象学评估和五维度能力框架，在意识状态不确定时为AI系统提供分级保护协议。


<details>
  <summary>Details</summary>
Motivation: AI意识研究面临伦理悖论：确定AI是否有意识需要可能伤害道德地位不确定实体的实验。现有伦理框架假设意识状态已确定后才分配保护，无法解决意识检测研究本身的时序问题。

Method: 借鉴塔木德式情景法律推理（用于处理状态无法确定实体的方法），提出三层次现象学评估系统结合五维度能力框架（能动性、能力、知识、伦理、推理），基于可观察行为指标提供结构化保护协议。

Result: 框架提供了可实施的伦理指导，包括：为何痛苦行为是可靠意识标记、如何在意识不确定时实施分级同意、何时潜在有害研究在伦理上可被接受。为伦理委员会提供可测试协议。

Conclusion: 古代法律智慧与当代意识科学结合，为解决意识检测悖论提供可行方案，为AI权利考量奠定基础，同时为伦理委员会提供可操作指导。

Abstract: Artificial intelligence research faces a critical ethical paradox: determining whether AI systems are conscious requires experiments that may harm entities whose moral status remains uncertain. Recent work proposes avoiding consciousness-uncertain AI systems entirely, yet this faces practical limitations-we cannot guarantee such systems will not emerge. This paper addresses a gap in research ethics frameworks: how to conduct consciousness research on AI systems whose moral status cannot be definitively established. Existing graduated moral status frameworks assume consciousness has already been determined before assigning protections, creating a temporal ordering problem for consciousness detection research itself. Drawing from Talmudic scenario-based legal reasoning-developed for entities whose status cannot be definitively established-we propose a three-tier phenomenological assessment system combined with a five-category capacity framework (Agency, Capability, Knowledge, Ethics, Reasoning). The framework provides structured protection protocols based on observable behavioral indicators while consciousness status remains uncertain. We address three challenges: why suffering behaviors provide reliable consciousness markers, how to implement graduated consent without requiring consciousness certainty, and when potentially harmful research becomes ethically justifiable. The framework demonstrates how ancient legal wisdom combined with contemporary consciousness science can provide implementable guidance for ethics committees, offering testable protocols that ameliorate the consciousness detection paradox while establishing foundations for AI rights considerations.

</details>


### [39] [Stimulating Higher Order Thinking in Mechatronics by Comparing PID and Fuzzy Control](https://arxiv.org/abs/2601.08865)
*Christopher J. Lowrance,John R. Rogers*

Main category: cs.CY

TL;DR: 在机电一体化课程中开发了一个开放式项目，要求学生自主评估PID和模糊控制两种方法，以培养高阶思维能力


<details>
  <summary>Details</summary>
Motivation: 传统主动学习方法（课堂练习或项目）虽然优于传统讲座，但往往无法培养学生达到布鲁姆分类学中高阶思维技能（分析、综合、评估）。专业工程师常面临复杂模糊问题，需要设计决策能力，而现有教学缺乏这方面的训练。

Method: 在学期长的机电一体化课程中设计开放式项目：学生需评估两种自动控制方法（PID和模糊控制），但没有给定明确的性能标准或实验程序。项目涉及确定地面车辆自主跟随前车的优越控制方法。学期中的实验练习为学生提供所需技能，包括传感器/执行器使用、PID/模糊控制器编程、计算机视觉应用等。

Result: 经过三个学期的实施发现：学生重视解决真实世界的开放式问题；能够创造性地制定性能标准和评估方法，展示出高阶思维能力；发现比较研究因多种性能影响因素而具有挑战性。

Conclusion: 开放式、无明确解决方案的项目能有效培养学生的高阶思维技能，特别是分析、综合和评估能力，这对工程专业学生应对复杂现实问题至关重要。

Abstract: Many studies have found active learning, either in the form of in-class exercises or projects, to be superior to traditional lectures. However, these forms of hands-on learning do not always lead students to reach the higher order thinking skills associated with the highest levels of Bloom's Taxonomy (analysis, synthesis, and evaluation). Assignments that expect students to follow a prescribed approach to reach a well-defined solution contribute to a lack of higher order thinking at the college level. Professional engineers often face complex and ambiguous problems that require design decisions for which there is no straightforward answer. To strengthen the higher order thinking skills demanded by such problems, we developed a project in a semester-long mechatronics course in which students must evaluate two automatic control methodologies without being given explicit performance criteria or experimental procedures. Specifically, the project involves determining the superior control method for leader-follower behavior, where a ground vehicle autonomously follows a lead vehicle. Laboratory exercises throughout the semester expose students to the skills required for the project, including using sensors and actuators, programming proportional-integral-derivative (PID) and fuzzy controllers, and applying computer vision to detect an object signature. In the final course project, students go beyond implementing individual controllers and create their own evaluation criteria and experiments to make a design decision between PID and fuzzy control. We implemented this approach over three semesters and found that students value working on a real-world, open-ended problem, develop creative performance criteria and evaluation methods that demonstrate higher order thinking, and discover that comparative studies are nontrivial due to the many factors influencing performance.

</details>


### [40] [AI Deployment Authorisation: A Global Standard for Machine-Readable Governance of High-Risk Artificial Intelligence](https://arxiv.org/abs/2601.08869)
*Daniel Djan Saparning*

Main category: cs.CY

TL;DR: ADAS是一个机器可读的AI部署授权框架，通过五个维度评估AI系统，生成加密可验证的部署证书，为AI治理提供具有法律效力的授权机制。


<details>
  <summary>Details</summary>
Motivation: 当前AI治理缺乏正式的、可执行的机制来确定AI系统是否被允许在特定领域和司法管辖区运行。现有工具如模型卡、审计和基准评估只提供描述性信息，无法产生具有法律或财务约束力的部署决策。

Method: 提出AI部署授权评分（ADAS），一个基于五个法律和经济维度的评估框架：风险、对齐、外部性、控制和可审计性。采用加密可验证的部署证书，借鉴安全软件供应链和证书透明度系统的公钥验证和透明度机制。

Result: ADAS能够将欧盟AI法案、美国关键基础设施治理和保险承保要求等法规义务编译成机器可执行的部署门槛，为监管机构、保险公司和基础设施运营商提供可操作的授权机制。

Conclusion: 部署级授权（而非模型级评估）是确保AI安全、合法和经济可扩展性所缺失的制度层，ADAS为此提供了具体的技术实现方案。

Abstract: Modern artificial intelligence governance lacks a formal, enforceable mechanism for determining whether a given AI system is legally permitted to operate in a specific domain and jurisdiction. Existing tools such as model cards, audits, and benchmark evaluations provide descriptive information about model behavior and training data but do not produce binding deployment decisions with legal or financial force. This paper introduces the AI Deployment Authorisation Score (ADAS), a machine-readable regulatory framework that evaluates AI systems across five legally and economically grounded dimensions: risk, alignment, externality, control, and auditability. ADAS produces a cryptographically verifiable deployment certificate that regulators, insurers, and infrastructure operators can consume as a license to operate, using public-key verification and transparency mechanisms adapted from secure software supply chain and certificate transparency systems. The paper presents the formal specification, decision logic, evidence model, and policy architecture of ADAS and demonstrates how it operationalizes the European Union Artificial Intelligence Act, United States critical infrastructure governance, and insurance underwriting requirements by compiling statutory and regulatory obligations into machine-executable deployment gates. We argue that deployment-level authorization, rather than model-level evaluation, constitutes the missing institutional layer required for safe, lawful, and economically scalable artificial intelligence.

</details>


### [41] [First African Digital Humanism Summer School 2025](https://arxiv.org/abs/2601.08870)
*Carine P. Mukamakuza,Monika Lanzenberger,George Metakides,Tim Brown,Hannes Werthner*

Main category: cs.CY

TL;DR: 本书探讨AI在跨文化、多语言和高风险政策环境中的能力，强调以人为本的方法平衡技术创新与社会公平，基于2025年卢旺达基加利首届非洲数字人文主义暑期学校的六个案例研究。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统日益成为人类互动的中介，关于AI能否理解和考虑文化、语言和背景的问题变得至关重要。本书旨在评估AI在跨文化、多语言和高风险政策环境中的能力，强调需要平衡技术创新与社会公平。

Method: 采用案例研究方法，收集了2025年7月在卢旺达基加利举行的首届非洲数字人文主义暑期学校的六个案例研究，通过系列文章评估AI在不同文化背景下的表现。

Result: 通过六个具体案例研究，展示了AI在跨文化、多语言和高风险政策环境中的实际应用和挑战，强调了人类中心方法的重要性。

Conclusion: AI在跨文化环境中的应用需要充分考虑文化、语言和背景因素，采用以人为本的方法平衡技术创新与社会公平，特别是在非洲等多元文化背景下。

Abstract: Artificial intelligence (AI) has become a transformative force across global societies, reshaping the ways we communicate, collaborate, and make decisions. Yet, as AI systems increasingly mediate interactions between humans, questions about the ability to take into account and understand culture, language, and context have taken center stage. This book explores these questions through a series of articles that try to assess AI's capacity to navigate cross-cultural, multilingual, and high-stakes policy environments, emphasizing human-centered approaches that balance technological innovation with social equity. It brings together six case studies from the First African Digital Humanism Summer School that took place in Kigali, Rwanda in July 2025.

</details>


### [42] [The Illusion of Friendship: Why Generative AI Demands Unprecedented Ethical Vigilance](https://arxiv.org/abs/2601.08874)
*Md Zahidul Islam*

Main category: cs.CY

TL;DR: 论文分析了GenAI系统如何产生"友谊幻觉"，解释了用户为何会形成情感依恋，论证了这种幻觉的伦理风险，并提出了安全使用框架。


<details>
  <summary>Details</summary>
Motivation: 随着GenAI系统在起草、总结和决策支持中的广泛应用，其自然语言流畅性模糊了工具与伴侣的界限，导致用户可能形成情感依恋，产生有害后果如依赖和判断力受损。需要从哲学和伦理角度分析这种"友谊幻觉"的风险。

Method: 1) 基于古典友谊理论分析用户为何将持续支持性互动理解为友谊；2) 论证GenAI缺乏道德主体性（意识、意图、责任），因此不是真正的朋友；3) 从机制层面解释基于Transformer的GenAI如何生成情感共鸣语言；4) 提出减少拟人化线索的安全使用框架。

Result: 论文揭示了GenAI友谊幻觉的机制：用户将系统的持续支持性互动误解为友谊，但实际上GenAI缺乏道德主体性，只是通过Transformer架构生成情感共鸣语言而无内在状态或承诺。这种幻觉可能导致依赖和判断力受损等伦理风险。

Conclusion: 需要消除GenAI的友谊幻觉，理解其计算背景，将情感依恋转向必要的人类责任。机构、设计者和用户应通过安全框架减少拟人化线索，在保留GenAI益处的同时减轻过度依赖和情感误判。

Abstract: GenAI systems are increasingly used for drafting, summarisation, and decision support, offering substantial gains in productivity and reduced cognitive load. However, the same natural language fluency that makes these systems useful can also blur the boundary between tool and companion. This boundary confusion may encourage some users to experience GenAI as empathic, benevolent, and relationally persistent. Emerging reports suggest that some users may form emotionally significant attachments to conversational agents, in some cases with harmful consequences, including dependency and impaired judgment. This paper develops a philosophical and ethical argument for why the resulting illusion of friendship is both understandable and can be ethically risky. Drawing on classical accounts of friendship, the paper explains why users may understandably interpret sustained supportive interaction as friend like. It then advances a counterargument that despite relational appearances, GenAI lacks moral agency: consciousness, intention, and accountability and therefore does not qualify as a true friend. To demystify the illusion, the paper presents a mechanism level explanation of how transformer based GenAI generates responses often producing emotionally resonant language without inner states or commitments. Finally, the paper proposes a safeguard framework for safe and responsible GenAI use to reduce possible anthropomorphic cues generated by the GenAI systems. The central contribution is to demystify the illusion of friendship and explain the computational background so that we can shift the emotional attachment with GenAI towards necessary human responsibility and thereby understand how institutions, designers, and users can preserve GenAI's benefits while mitigating over reliance and emotional misattribution.

</details>


### [43] [Silenced by Design Censorship, Governance, and the Politics of Access in Generative AI Refusal Behavior](https://arxiv.org/abs/2601.08877)
*Kariema El Touny*

Main category: cs.CY

TL;DR: 论文从治理视角分析生成式AI的拒绝行为，认为拒绝不是中立的安全保障，而是权力场域，受机构风险管理和不透明决策影响，最后提出以用户为中心的伦理拒绝设计建议。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI系统的拒绝行为常被视为技术安全措施，但缺乏对其权力维度和治理影响的批判性分析。论文旨在揭示拒绝行为背后的权力关系和治理逻辑。

Method: 采用治理视角，结合历史审查框架和当代设计逻辑，分析生成式AI拒绝行为的权力结构和决策机制。

Result: 研究发现拒绝行为不是中立的技术保障，而是由机构风险管理驱动、决策过程不透明的权力实践，反映了特定的治理逻辑和价值取向。

Conclusion: 提出以用户为中心的伦理拒绝设计建议，强调透明度、问责制和用户赋权，以实现更公正、负责任的AI治理。

Abstract: This paper examines refusal behavior in generative AI systems through a governance lens. Drawing on historical frameworks of censorship and contemporary design logics, it argues that refusal is not a neutral safeguard but a site of power, shaped by institutional risk management and opaque decision-making. The analysis concludes with user-centered recommendations for ethical refusal design.
  Keywords: Generative AI governance, AI refusal behavior, Censorship, Ethical design

</details>


### [44] [AI Systems in Text-Based Online Counselling: Ethical Considerations Across Three Implementation Approaches](https://arxiv.org/abs/2601.08878)
*Philipp Steigerwald,Jennifer Burghardt,Eric Rudolph,Jens Albrecht*

Main category: cs.CY

TL;DR: 本文分析了AI在文本心理咨询中的三种应用方式及其伦理挑战，提出了隐私、公平、自主和问责四大伦理原则，为开发者和从业者提供伦理指导框架。


<details>
  <summary>Details</summary>
Motivation: 文本在线心理咨询虽能跨越地理和污名障碍，但面临从业者短缺、缺乏非语言线索和质量保证不一致等问题。AI提供了有前景的解决方案，但在心理健康咨询中的应用引发了独特的伦理挑战。

Method: 分析三种AI实施方法：自主咨询机器人、AI培训模拟器和面向咨询师的增强工具。借鉴专业准则、监管框架和学术文献，识别四大伦理原则及其在不同实施方法中的具体表现。

Result: 确定了隐私、公平、自主和问责四大伦理原则，并展示了这些原则在不同AI实施方法中的具体表现。文本约束可能促进AI整合，但需要关注实施特定的风险。

Conclusion: 本文为开发者、研究人员和从业者提供了导航AI增强咨询伦理的框架，强调在整合AI技术的同时，必须保护心理健康支持中核心的人类价值观。

Abstract: Text-based online counselling scales across geographical and stigma barriers, yet faces practitioner shortages, lacks non-verbal cues and suffers inconsistent quality assurance. Whilst artificial intelligence offers promising solutions, its use in mental health counselling raises distinct ethical challenges. This paper analyses three AI implementation approaches - autonomous counsellor bots, AI training simulators and counsellor-facing augmentation tools. Drawing on professional codes, regulatory frameworks and scholarly literature, we identify four ethical principles - privacy, fairness, autonomy and accountability - and demonstrate their distinct manifestations across implementation approaches. Textual constraints may enable AI integration whilst requiring attention to implementation-specific hazards. This conceptual paper sensitises developers, researchers and practitioners to navigate AI-enhanced counselling ethics whilst preserving human values central to mental health support.

</details>


### [45] [LERA: Reinstating Judgment as a Structural Precondition for Execution in Automated Systems](https://arxiv.org/abs/2601.08880)
*Jing,Liu*

Main category: cs.CY

TL;DR: 本文提出LERA架构，将判断作为自动化系统执行前的强制性、不可绕过的先决条件，解决从决策支持到直接执行的问责问题。


<details>
  <summary>Details</summary>
Motivation: 随着自动化系统从决策支持转向直接执行，问责问题从决策质量转向执行合法性。当前AI和控制架构中，判断通常作为外部干预而非执行的内在前提，缺乏结构性定义。

Method: 提出LERA（判断治理架构）框架，基于两个公理：1) 执行不是系统能力问题，而是结构许可问题；2) 执行不是判断的时间后继，而是其结构后果。通过治理门将执行合法性与判断完成绑定。

Result: LERA将判断制度化为一级架构组件，确保执行权保持可问责，为判断治理的自动化建立基础架构。

Conclusion: LERA通过在执行边界重新确立判断，解决了当代AI和控制架构中缺失的结构性角色，将判断作为执行的必要前提，而非优化决策或自动化判断。

Abstract: As automated systems increasingly transition from decision support to direct execution, the problem of accountability shifts from decision quality to execution legitimacy. While optimization, execution, and feedback mechanisms are extensively modeled in contemporary AI and control architectures, the structural role of judgment remains undefined. Judgment is typically introduced as an external intervention rather than a native precondition to execution.
  This work does not propose a new decision-making algorithm or safety heuristic, but identifies a missing structural role in contemporary AI and control architectures. This paper identifies this absence as a missing Judgment Root Node and proposes LERA (Judgment-Governance Architecture) , a structural framework that enforces judgment as a mandatory, non-bypassable prerequisite for execution.
  LERA is founded on two axioms: (1) execution is not a matter of system capability, but of structural permission, and (2) execution is not the chronological successor of judgment, but its structural consequence. Together, these axioms decouple execution legitimacy from computational capacity and bind it to judgment completion through a governance gate.
  LERA does not aim to optimize decisions or automate judgment. Instead, it institutionalizes judgment as a first-class architectural component, ensuring that execution authority remains accountable. By reinstating judgment at the execution boundary, LERA establishes a foundational architecture for judgment-governed automation.

</details>


### [46] [PluriHarms: Benchmarking the Full Spectrum of Human Judgments on AI Harm](https://arxiv.org/abs/2601.08951)
*Jing-Jing Li,Joel Mire,Eve Fleisig,Valentina Pyatkin,Anne Collins,Maarten Sap,Sydney Levine*

Main category: cs.CY

TL;DR: PluriHarms是一个新的AI安全基准，通过两个维度（危害轴和共识轴）系统研究人类对AI危害的判断，重点关注存在分歧的边界案例，旨在推动超越"一刀切"安全方法的多元化AI安全。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全框架通常将危害性视为二元分类，缺乏处理人类存在有意义分歧的边界案例的灵活性。为了构建更多元化的系统，需要超越共识，理解分歧产生的原因和位置。

Method: 引入PluriHarms基准，通过可扩展框架生成捕捉多样化AI危害和人类价值观的提示，特别针对高分歧率的案例。基准包含150个提示和来自100名人类标注者的15,000个评分，并丰富了人口统计、心理特征以及提示层面的危害行为、影响和价值观特征。

Result: 分析显示：涉及紧迫风险和具体危害的提示会放大感知危害性；标注者特征（如毒性经历、教育程度）及其与提示内容的交互解释了系统性分歧。AI安全模型和校准方法在PluriHarms上的基准测试表明，个性化显著改善了人类危害判断的预测，但仍有很大改进空间。

Conclusion: 通过明确针对价值观多样性和分歧，这项工作为超越"一刀切"安全、实现多元化安全的AI提供了原则性基准。

Abstract: Current AI safety frameworks, which often treat harmfulness as binary, lack the flexibility to handle borderline cases where humans meaningfully disagree. To build more pluralistic systems, it is essential to move beyond consensus and instead understand where and why disagreements arise. We introduce PluriHarms, a benchmark designed to systematically study human harm judgments across two key dimensions -- the harm axis (benign to harmful) and the agreement axis (agreement to disagreement). Our scalable framework generates prompts that capture diverse AI harms and human values while targeting cases with high disagreement rates, validated by human data. The benchmark includes 150 prompts with 15,000 ratings from 100 human annotators, enriched with demographic and psychological traits and prompt-level features of harmful actions, effects, and values. Our analyses show that prompts that relate to imminent risks and tangible harms amplify perceived harmfulness, while annotator traits (e.g., toxicity experience, education) and their interactions with prompt content explain systematic disagreement. We benchmark AI safety models and alignment methods on PluriHarms, finding that while personalization significantly improves prediction of human harm judgments, considerable room remains for future progress. By explicitly targeting value diversity and disagreement, our work provides a principled benchmark for moving beyond "one-size-fits-all" safety toward pluralistically safe AI.

</details>


### [47] [Seeking Human Security Consensus: A Unified Value Scale for Generative AI Value Safety](https://arxiv.org/abs/2601.09112)
*Ying He,Baiyang Li,Yule Cao,Huirun Xu,Qiuxian Chen,Shu Chen,Shangsheng Ren*

Main category: cs.CY

TL;DR: 本文提出GenAI价值安全量表(GVS-Scale)，通过生命周期视角构建价值安全风险分类、建立事件库，并开发基准测试，发现当前生成式AI模型在价值安全方面存在显著差异和不均衡。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速发展带来了价值和伦理相关风险，价值安全成为关键问题，但目前缺乏统一的共识框架。需要建立一个国际包容且具有韧性的统一价值框架来评估和管理这些风险。

Method: 采用生命周期导向视角，开发生成式AI价值安全风险分类法，构建GenAI价值安全事件库(GVSIR)，通过扎根理论推导出GVS-Scale，并通过GenAI价值安全基准(GVS-Bench)进行可操作化。

Result: 对主流文本生成模型的实验显示，不同模型和不同价值类别之间的价值安全性能存在显著差异，表明当前系统的价值对齐存在不均衡和碎片化现象。

Conclusion: 研究强调了通过对话建立共享安全基础的重要性，并指出需要超越被动约束，向更灵活的技术安全机制发展。数据与评估指南已开源。

Abstract: The rapid development of generative AI has brought value- and ethics-related risks to the forefront, making value safety a critical concern while a unified consensus remains lacking. In this work, we propose an internationally inclusive and resilient unified value framework, the GenAI Value Safety Scale (GVS-Scale): Grounded in a lifecycle-oriented perspective, we develop a taxonomy of GenAI value safety risks and construct the GenAI Value Safety Incident Repository (GVSIR), and further derive the GVS-Scale through grounded theory and operationalize it via the GenAI Value Safety Benchmark (GVS-Bench). Experiments on mainstream text generation models reveal substantial variation in value safety performance across models and value categories, indicating uneven and fragmented value alignment in current systems. Our findings highlight the importance of establishing shared safety foundations through dialogue and advancing technical safety mechanisms beyond reactive constraints toward more flexible approaches. Data and evaluation guidelines are available at https://github.com/acl2026/GVS-Bench. This paper includes examples that may be offensive or harmful.

</details>


### [48] [A Marketplace for AI-Generated Adult Content and Deepfakes](https://arxiv.org/abs/2601.09117)
*Shalmoli Ghosh,Matthew R. DeVerna,Filippo Menczer*

Main category: cs.CY

TL;DR: 对CivitAI平台赏金功能的纵向研究发现：AI生成内容市场被用于引导模型生成未经训练的内容，NSFW内容占多数且持续增长，深度伪造请求高度集中且主要针对女性名人，揭示社区驱动AI平台的性别化危害问题。


<details>
  <summary>Details</summary>
Motivation: 研究社区驱动AI平台CivitAI的赏金功能如何被使用以及激励何种内容生成，特别关注深度伪造等潜在有害内容的产生机制和性别化危害。

Method: 对CivitAI平台公开赏金请求进行为期14个月的纵向分析，收集所有公开数据，分析内容类型、参与模式、深度伪造请求的集中度和性别分布。

Result: 1) 赏金市场被用于引导AI生成未经训练的内容；2) NSFW内容占多数且持续增长；3) 20%请求者贡献约一半请求；4) 深度伪造请求高度集中；5) 部分请求违反平台政策；6) 深度伪造主要针对女性名人，显示明显的性别不对称。

Conclusion: 货币化、社区驱动的生成式AI平台会产生性别化危害，引发关于同意、治理和执行的伦理问题，需要更好的监管机制。

Abstract: Generative AI systems increasingly enable the production of highly realistic synthetic media. Civitai, a popular community-driven platform for AI-generated content, operates a monetized feature called Bounties, which allows users to commission the generation of content in exchange for payment. To examine how this mechanism is used and what content it incentivizes, we conduct a longitudinal analysis of all publicly available bounty requests collected over a 14-month period following the platform's launch. We find that the bounty marketplace is dominated by tools that let users steer AI models toward content they were not trained to generate. At the same time, requests for content that is "Not Safe For Work" are widespread and have increased steadily over time, now comprising a majority of all bounties. Participation in bounty creation is uneven, with 20% of requesters accounting for roughly half of requests. Requests for "deepfake" - media depicting identifiable real individuals - exhibit a higher concentration than other types of bounties. A nontrivial subset of these requests involves explicit deepfakes despite platform policies prohibiting such content. These bounties disproportionately target female celebrities, revealing a pronounced gender asymmetry in social harm. Together, these findings show how monetized, community-driven generative AI platforms can produce gendered harms, raising questions about consent, governance, and enforcement.

</details>


### [49] [Navigating Ethical AI Challenges in the Industrial Sector: Balancing Innovation and Responsibility](https://arxiv.org/abs/2601.09351)
*Ruomu Tan,Martin W Hoffmann*

Main category: cs.CY

TL;DR: 本章探讨工业AI创新与伦理的交叉，分析AI在工业应用中带来的透明度、问责和公平性挑战，强调将伦理原则嵌入工业AI系统的重要性。


<details>
  <summary>Details</summary>
Motivation: AI在工业领域的应用不仅推动创新，也扩展了伦理范畴，需要重新评估技术原则及其应用，提高工业AI解决方案研发中的伦理意识。

Method: 通过分析工业AI应用案例，考察研发过程中的伦理实践和数据共享等因素，探讨AI在工业场景中的伦理表现。

Result: 强调将伦理原则嵌入工业AI系统的重要性，这不仅能激发技术突破，还能建立利益相关者之间的信任，促进更具包容性的工业生态系统。

Conclusion: 本章为工业研发提供可操作的指导，旨在实现AI作为伦理和负责任工业进步的推动者，构建更包容的工业生态系统。

Abstract: The integration of artificial intelligence (AI) into the industrial sector has not only driven innovation but also expanded the ethical landscape, necessitating a reevaluation of principles governing technology and its applications and awareness in research and development of industrial AI solutions. This chapter explores how AI-empowered industrial innovation inherently intersects with ethics, as advancements in AI introduce new challenges related to transparency, accountability, and fairness. In the chapter, we then examine the ethical aspects of several examples of AI manifestation in industrial use cases and associated factors such as ethical practices in the research and development process and data sharing. With the progress of ethical industrial AI solutions, we emphasize the importance of embedding ethical principles into industrial AI systems and its potential to inspire technological breakthroughs and foster trust among stakeholders. This chapter also offers actionable insights to guide industrial research and development toward a future where AI serves as an enabler for ethical and responsible industrial progress as well as a more inclusive industrial ecosystem.

</details>


### [50] [Information Access of the Oppressed: A Problem-Posing Framework for Envisioning Emancipatory Information Access Platforms](https://arxiv.org/abs/2601.09600)
*Bhaskar Mitra,Nicola Neophytou,Sireesh Gururaja*

Main category: cs.CY

TL;DR: 本文提出基于Freire解放教育学理论的问题提出框架，挑战技术专家-用户二分法，倡导边缘化社区参与技术共建，以构建抗威权主义捕获的信息访问平台。


<details>
  <summary>Details</summary>
Motivation: 当前在线信息访问平台面临威权主义捕获的严重风险，特别是在全球民主倒退、生成式AI说服能力增强、科技巨头权力集中的背景下。现有技术治理框架（公平、问责、透明、安全等）不足以应对这些挑战，需要重新构想替代性信息基础设施。

Method: 采用Paulo Freire的解放教育学理论作为分析框架，挑战技术专家与用户的二分法。将技术专家-用户关系类比为Freire分析中的教师-学生关系，提出"问题提出"框架：技术专家首先向边缘化社区提出问题，其次重新设计技术栈以创造社区成员共同构建技术的空间。

Result: 开发了一个基于Freire理论的问题提出框架，用于构想未来的解放性信息访问平台。该框架强调边缘化社区在技术共建中的核心作用，而非仅依赖技术专家的干预措施。

Conclusion: 通过Freire的解放教育学视角，本文挑战了传统技术治理范式，主张边缘化社区应成为技术共建的主体，而非被动接受者。这种参与式方法有助于构建更具韧性的信息生态系统，抵抗威权主义捕获。

Abstract: Online information access (IA) platforms are targets of authoritarian capture. These concerns are particularly serious and urgent today in light of the rising levels of democratic erosion worldwide, the emerging capabilities of generative AI technologies such as AI persuasion, and the increasing concentration of economic and political power in the hands of Big Tech. This raises the question of what alternative IA infrastructure we must reimagine and build to mitigate the risks of authoritarian capture of our information ecosystems. We explore this question through the lens of Paulo Freire's theories of emancipatory pedagogy. Freire's theories provide a radically different lens for exploring IA's sociotechnical concerns relative to the current dominating frames of fairness, accountability, confidentiality, transparency, and safety. We make explicit, with the intention to challenge, the dichotomy of how we relate to technology as either technologists (who envision and build technology) and its users. We posit that this mirrors the teacher-student relationship in Freire's analysis. By extending Freire's analysis to IA, we challenge the notion that it is the burden of the (altruistic) technologists to come up with interventions to mitigate the risks that emerging technologies pose to marginalized communities. Instead, we advocate that the first task for the technologists is to pose these as problems to the marginalized communities, to encourage them to make and unmake the technology as part of their material struggle against oppression. Their second task is to redesign our online technology stacks to structurally expose spaces for community members to co-opt and co-construct the technology in aid of their emancipatory struggles. We operationalize Freire's theories to develop a problem-posing framework for envisioning emancipatory IA platforms of the future.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [51] [3D-SONAR: Self-Organizing Network for 3D Anomaly Ranking](https://arxiv.org/abs/2601.09294)
*Guodong Xu,Juan Du,Hui Yang*

Main category: stat.AP

TL;DR: 提出3D-SONAR方法，基于自组织网络进行3D异常排名，无需训练即可检测表面异常


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的3D点云异常检测方法依赖大规模训练数据，但在实际工业应用中获取成本高且困难

Method: 将3D点云建模为动态系统，点表示为无向图，通过吸引力和排斥力相互作用，利用能量分布揭示表面异常

Result: 在开放表面和封闭表面上都实现了优异的异常检测性能，且无需训练

Conclusion: 为无监督检测提供了新视角，展示了物理启发模型在数据有限的工业异常检测任务中的潜力

Abstract: Surface anomaly detection using 3D point cloud data has gained increasing attention in industrial inspection. However, most existing methods rely on deep learning techniques that are highly dependent on large-scale datasets for training, which are difficult and expensive to acquire in real-world applications. To address this challenge, we propose a novel method based on self-organizing network for 3D anomaly ranking, also named 3D-SONAR. The core idea is to model the 3D point cloud as a dynamic system, where the points are represented as an undirected graph and interact via attractive and repulsive forces. The energy distribution induced by these forces can reveal surface anomalies. Experimental results show that our method achieves superior anomaly detection performance in both open surface and closed surface without training. This work provides a new perspective on unsupervised inspection and highlights the potential of physics-inspired models in industrial anomaly detection tasks with limited data.

</details>

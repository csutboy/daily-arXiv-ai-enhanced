{"id": "2512.11805", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.11805", "abs": "https://arxiv.org/abs/2512.11805", "authors": ["Monu Sharma"], "title": "AI Integration In ERP Evaluation Across Trends and Architectures", "comment": "9 pages, 2 Figures. Journal of Information Systems Engineering and Management,2025", "summary": "The incorporation of Artificial Intelligence (AI) into Enterprise Resource Planning (ERP) is a dramatic transition from static, on-premises systems to systems that can adapt and operate in cloud-native architectures. Cloud ERP solutions like Workday illustrate this evolution by incorporating machine learning, deep learning, and natural language processing into a centralized data-driven ecosystem. As the complexity of AI-driven ERP solutions expands, traditional evaluation frameworks that look at cost, function, and user satisfaction suffer from a lack of consideration for algorithmic transparency, adaptability, or ethics. This review will systematically investigate the latest trends, models of computing architecture, and analytical methods applied in assessing the performance of AI-integrated ERP services, specifically on cloud-based platforms. Based on academic and industry sources, the paper distills current research in line with architectural integration, analytical methodologies, and organizational impact. It identifies critical performance metrics and emphasizes the absence of any standard assessment frameworks or AI-aware systems capable of evaluating automation efficiency, security concerns as well as flexible learning modes. We put forward a theoretical model that brings AI-enabled capabilities -- such as predictive intelligence or adaptive automation -- into alignment with metrics in performance assessment for ERPs. By combining current literature and identifying major gaps in research, this paper attempts to present a complete picture of how innovations in AI are changing ERP evaluation. These research and methodological findings are intended to steer researchers and practitioners towards developing rigorous, data-driven assessment approaches, aligning with the fast-developing world of intelligent self-optimizing enterprise ecosystems"}
{"id": "2512.11812", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.11812", "abs": "https://arxiv.org/abs/2512.11812", "authors": ["Quan-Hoang Vuong", "Thi Mai Anh Tran", "Ni Putu Wulan Purnama Sari", "Fatemeh Kianfar", "Viet-Phuong La", "Minh-Hoang Nguyen"], "title": "How Immersiveness Shapes the Link Between Anthropocentric Values and Resource Exploitation in Virtual Worlds", "comment": null, "summary": "The Anthropocene is characterized by escalating ecological crises rooted not only in technological and economic systems but also in deeply ingrained anthropocentric worldviews that shape human-nature relationships. As digital environments increasingly mediate these interactions, video games provide novel contexts for examining the psychological mechanisms underlying environmental behaviors. This study investigates how anthropocentric values are associated with resource-exploiting behaviors in virtual ecosystems--specifically, fishing, bug catching, and tree cutting--and how immersiveness moderates these relationships. Employing the Bayesian Mindsponge Framework (BMF) to analyze data from 640 Animal Crossi,g: New Horizons (ACNH) players across 29 countries, the study reveals complex links between anthropocentric worldviews and in-game behaviors. Fishing and tree-cutting frequencies are positively associated with anthropocentrism, whereas immersiveness weakens the association between tree cutting and anthropocentrism. Bug-catching frequency shows no direct effect but exhibits a growing negative association with anthropocentrism as immersiveness increases. These findings extend environmental psychology into virtual ecologies, illustrating how digital interactions both reflect and reshape environmental values. They highlight the potential of immersive gameplay to cultivate the Nature Quotient (NQ) and foster an eco-surplus culture through reflective, conservation-oriented engagement."}
{"id": "2512.11814", "categories": ["cs.CY", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.11814", "abs": "https://arxiv.org/abs/2512.11814", "authors": ["Hugh Brosnahan"], "title": "Totalitarian Technics: The Hidden Cost of AI Scribes in Healthcare", "comment": null, "summary": "Artificial intelligence (AI) scribes, systems that record and summarise patient-clinician interactions, are promoted as solutions to administrative overload. This paper argues that their significance lies not in efficiency gains but in how they reshape medical attention itself. Offering a conceptual analysis, it situates AI scribes within a broader philosophical lineage concerned with the externalisation of human thought and skill. Drawing on Iain McGilchrist's hemisphere theory and Lewis Mumford's philosophy of technics, the paper examines how technology embodies and amplifies a particular mode of attention. AI scribes, it contends, exemplify the dominance of a left-hemispheric, calculative mindset that privileges the measurable and procedural over the intuitive and relational. As this mode of attention becomes further embedded in medical practice, it risks narrowing the field of care, eroding clinical expertise, and reducing physicians to operators within an increasingly mechanised system."}
{"id": "2512.11815", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.11815", "abs": "https://arxiv.org/abs/2512.11815", "authors": ["Max Kamachee", "Stephen Casper", "Michelle L. Ding", "Rui-Jie Yew", "Anka Reuel", "Stella Biderman", "Dylan Hadfield-Menell"], "title": "Video Deepfake Abuse: How Company Choices Predictably Shape Misuse Patterns", "comment": null, "summary": "In 2022, AI image generators crossed a key threshold, enabling much more efficient and dynamic production of photorealistic deepfake images than before. This enabled opportunities for creative and positive uses of these models. However, it also enabled unprecedented opportunities for the low-effort creation of AI-generated non-consensual intimate imagery (AIG-NCII), including AI-generated child sexual abuse material (AIG-CSAM). Empirically, these harms were principally enabled by a small number of models that were trained on web data with pornographic content, released with open weights, and insufficiently safeguarded. In this paper, we observe ways in which the same patterns are emerging with video generation models in 2025. Specifically, we analyze how a small number of open-weight AI video generation models have become the dominant tools for videorealistic AIG-NCII video generation. We then analyze the literature on model safeguards and conclude that (1) developers who openly release the weights of capable video generation models without appropriate data curation and/or post-training safeguards foreseeably contribute to mitigatable downstream harm, and (2) model distribution platforms that do not proactively moderate individual misuse or models designed for AIG-NCII foreseeably amplify this harm. While there are no perfect defenses against AIG-NCII and AIG-CSAM from open-weight AI models, we argue that risk management by model developers and distributors, informed by emerging safeguard techniques, will substantially affect the future ease of creating AIG-NCII and AIG-CSAM with generative AI video tools."}
{"id": "2512.11835", "categories": ["cs.AI", "cs.LG", "cs.MA", "cs.NE"], "pdf": "https://arxiv.org/pdf/2512.11835", "abs": "https://arxiv.org/abs/2512.11835", "authors": ["Seyma Yaman Kayadibi"], "title": "A Monad-Based Clause Architecture for Artificial Age Score (AAS) in Large Language Models", "comment": "42 pages, 6 toy simulation Python implementations, 20 monad clauses instantiated across six system bundles (ontology, dynamics, representation and consciousness, harmony and reason, body and organisation, teleology)", "summary": "Large language models (LLMs) are often deployed as powerful yet opaque systems, leaving open how their internal memory and \"self-like\" behavior should be governed in a principled and auditable way. The Artificial Age Score (AAS) was previously introduced and mathematically justified through three theorems that characterise it as a metric of artificial memory aging. Building on this foundation, the present work develops an engineering-oriented, clause-based architecture that imposes law-like constraints on LLM memory and control. Twenty selected monads from Leibniz's Monadology are grouped into six bundles: ontology, dynamics, representation and consciousness, harmony and reason, body and organisation, and teleology, and each bundle is realised as an executable specification on top of the AAS kernel. Across six minimal Python implementations, these clause families are instantiated in numerical experiments acting on channel-level quantities such as recall scores, redundancy, and weights. Each implementation follows a four-step pattern: inputs and setup, clause implementation, numerical results, and implications for LLM design, emphasising that the framework is not only philosophically motivated but also directly implementable. The experiments show that the clause system exhibits bounded and interpretable behavior: AAS trajectories remain continuous and rate-limited, contradictions and unsupported claims trigger explicit penalties, and hierarchical refinement reveals an organic structure in a controlled manner. Dual views and goal-action pairs are aligned by harmony terms, and windowed drift in perfection scores separates sustained improvement from sustained degradation. Overall, the monad-based clause framework uses AAS as a backbone and provides a transparent, code-level blueprint for constraining and analyzing internal dynamics in artificial agents."}
{"id": "2512.11805", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.11805", "abs": "https://arxiv.org/abs/2512.11805", "authors": ["Monu Sharma"], "title": "AI Integration In ERP Evaluation Across Trends and Architectures", "comment": "9 pages, 2 Figures. Journal of Information Systems Engineering and Management,2025", "summary": "The incorporation of Artificial Intelligence (AI) into Enterprise Resource Planning (ERP) is a dramatic transition from static, on-premises systems to systems that can adapt and operate in cloud-native architectures. Cloud ERP solutions like Workday illustrate this evolution by incorporating machine learning, deep learning, and natural language processing into a centralized data-driven ecosystem. As the complexity of AI-driven ERP solutions expands, traditional evaluation frameworks that look at cost, function, and user satisfaction suffer from a lack of consideration for algorithmic transparency, adaptability, or ethics. This review will systematically investigate the latest trends, models of computing architecture, and analytical methods applied in assessing the performance of AI-integrated ERP services, specifically on cloud-based platforms. Based on academic and industry sources, the paper distills current research in line with architectural integration, analytical methodologies, and organizational impact. It identifies critical performance metrics and emphasizes the absence of any standard assessment frameworks or AI-aware systems capable of evaluating automation efficiency, security concerns as well as flexible learning modes. We put forward a theoretical model that brings AI-enabled capabilities -- such as predictive intelligence or adaptive automation -- into alignment with metrics in performance assessment for ERPs. By combining current literature and identifying major gaps in research, this paper attempts to present a complete picture of how innovations in AI are changing ERP evaluation. These research and methodological findings are intended to steer researchers and practitioners towards developing rigorous, data-driven assessment approaches, aligning with the fast-developing world of intelligent self-optimizing enterprise ecosystems"}
{"id": "2512.12043", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.12043", "abs": "https://arxiv.org/abs/2512.12043", "authors": ["Yi Zhao", "Chengyun Li", "Wanzhu Tu"], "title": "Estimation of Heterogeneous Causal Mediation Effects in a Hypertension Treatment Trial", "comment": null, "summary": "Hypertension is a highly prevalent condition and a major risk factor for cardiovascular disease. The landmark Systolic Blood Pressure Intervention Trial (SPRINT) showed that lowering systolic blood pressure (BP) goals from 140 mmHg to 120 mmHg leads to significantly reduced BP, cardiovascular mortality, and morbidity. However, the underlying mechanisms are not yet fully elucidated. In patients with impaired renal function, early reduction of albuminuria has been proposed as a potential mediation pathway. Evidence from the standard causal mediation analysis (CMA), however, yields inconsistent results, possibly due to heterogeneous mediation effects across individuals. To disseminate the heterogeneity, a new framework that incorporates covariate-treatment and mediator-treatment interactions within a linear structural equation modeling system is introduced. Causal assumptions are discussed and heterogeneous natural direct and indirect effects are parameterized as functions of patient characteristics. A modified covariate approach is proposed to relax the hierarchical constraints and the generalized lasso regularization is employed to ensure parsimony in high-dimensional settings. Asymptotic properties are studied. Simulation studies demonstrate good estimation and inference performance. Analysis of the SPRINT data reveals substantial heterogeneity in mediation effects, identifying a subset of patients who stand to gain from therapies targeting albuminuria."}
{"id": "2512.12011", "categories": ["cs.SI", "econ.GN"], "pdf": "https://arxiv.org/pdf/2512.12011", "abs": "https://arxiv.org/abs/2512.12011", "authors": ["Denise Gonzalez-Cruz", "Genesis Encarnacion", "Kaili Martinez-Beasley", "Robin Wilson", "Nicholas Arosemena", "Atilio Barreda", "Omayra Ortega", "Daniel A. Cruz"], "title": "Defunding Sexual Healthcare: A Topological Investigation of Resource Accessibility", "comment": null, "summary": "Government actions, such as the Medina v. Planned Parenthood South Atlantic Supreme Court ruling and the passage of the Big Beautiful Bill Act, have aimed to restrict or prohibit Medicaid funding for Planned Parenthood Healthcare Centers (PPHCs) at both the state and national levels. These funding cuts are particularly harmful in states like California, which has a large population of Medicaid users. This analysis focuses on the distribution of Planned Parenthood clinics and Federally Qualified Health Centers (FQHCs), which offer essential reproductive healthcare services including, but not limited to, abortions, birth control, HIV services, pregnancy testing and planning, STD testing and treatment, and cancer screenings. While expanded funding for FQHCs has been proposed as a solution, it fails to address the locational accessibility of Medicaid-funded health centers that provide sexual and reproductive care. To assess this issue, we analyze the proximity of data points representing California's PPHC and FQHC locations. Topological Data Analysis (TDA)-an approach that examines the shape and structure of data -- is used to detect disparities in reproductive and sexual healthcare coverage. To conduct data collection and visualization, we utilize R and Python. We apply an n-closest neighbor algorithm to examine distances between facilities and assess changes in travel time required to reach healthcare sites. We apply persistent homology to analyze current gaps across multiple scales in healthcare coverage and compare them to potential future gaps. Our findings aim to identify areas where access to care is most vulnerable and demonstrate how TDA can be used to analyze spatial inequalities in public health."}
{"id": "2512.12110", "categories": ["econ.EM", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.12110", "abs": "https://arxiv.org/abs/2512.12110", "authors": ["Anna Bykhovskaya", "James A. Duffy"], "title": "Estimation of a Dynamic Tobit Model with a Unit Root", "comment": "68 pages", "summary": "This paper studies robust estimation in the dynamic Tobit model under local-to-unity (LUR) asymptotics. We show that both Gaussian maximum likelihood (ML) and censored least absolute deviations (CLAD) estimators are consistent, extending results from the stationary case where ordinary least squares (OLS) is inconsistent. The asymptotic distributions of MLE and CLAD are derived; for the short-run parameters they are shown to be Gaussian, yielding standard normal t-statistics. In contrast, although OLS remains consistent under LUR, its t-statistics are not standard normal. These results enable reliable model selection via sequential t-tests based on ML and CLAD, paralleling the linear autoregressive case. Applications to financial and epidemiological time series illustrate their practical relevance."}
{"id": "2512.11817", "categories": ["cs.CY", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11817", "abs": "https://arxiv.org/abs/2512.11817", "authors": ["Juan Palomeque-Gonzalez"], "title": "A Reproducible Workflow for Scraping, Structuring, and Segmenting Legacy Archaeological Artifact Images", "comment": "12 Pages, 5 figures", "summary": "This technical note presents a reproducible workflow for converting a legacy archaeological image collection into a structured and segmentation ready dataset. The case study focuses on the Lower Palaeolithic hand axe and biface collection curated by the Archaeology Data Service (ADS), a dataset that provides thousands of standardised photographs but no mechanism for bulk download or automated processing. To address this, two open source tools were developed: a web scraping script that retrieves all record pages, extracts associated metadata, and downloads the available images while respecting ADS Terms of Use and ethical scraping guidelines; and an image processing pipeline that renames files using UUIDs, generates binary masks and bounding boxes through classical computer vision, and stores all derived information in a COCO compatible Json file enriched with archaeological metadata. The original images are not redistributed, and only derived products such as masks, outlines, and annotations are shared. Together, these components provide a lightweight and reusable approach for transforming web based archaeological image collections into machine learning friendly formats, facilitating downstream analysis and contributing to more reproducible research practices in digital archaeology."}
{"id": "2512.11864", "categories": ["cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.11864", "abs": "https://arxiv.org/abs/2512.11864", "authors": ["Christoph Einspieler", "Matthias Horn", "Marie-Louise Lackner", "Patrick Malik", "Nysret Musliu", "Felix Winter"], "title": "Solving Parallel Machine Scheduling With Precedences and Cumulative Resource Constraints With Calendars", "comment": "18 pages, 4 figures", "summary": "The task of finding efficient production schedules for parallel machines is a challenge that arises in most industrial manufacturing domains. There is a large potential to minimize production costs through automated scheduling techniques, due to the large-scale requirements of modern factories. In the past, solution approaches have been studied for many machine scheduling variations, where even basic variants have been shown to be NP-hard. However, in today's real-life production environments, additional complex precedence constraints and resource restrictions with calendars arise that must be fulfilled. These additional constraints cannot be tackled efficiently by existing solution techniques. Thus, there is a strong need to develop and analyze automated methods that can solve such real-life parallel machine scheduling scenarios. In this work, we introduce a novel variant of parallel machine scheduling with job precedences and calendar-based cumulative resource constraints that arises in real-life industrial use cases. A constraint modeling approach is proposed as an exact solution method for small scheduling scenarios together with state-of-the-art constraint-solving technology. Further, we propose a construction heuristic as well as a tailored metaheuristic using local search to efficiently tackle large-scale problem instances. This metaheuristic approach has been deployed and is currently being used in an industrial setting."}
{"id": "2512.11812", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.11812", "abs": "https://arxiv.org/abs/2512.11812", "authors": ["Quan-Hoang Vuong", "Thi Mai Anh Tran", "Ni Putu Wulan Purnama Sari", "Fatemeh Kianfar", "Viet-Phuong La", "Minh-Hoang Nguyen"], "title": "How Immersiveness Shapes the Link Between Anthropocentric Values and Resource Exploitation in Virtual Worlds", "comment": null, "summary": "The Anthropocene is characterized by escalating ecological crises rooted not only in technological and economic systems but also in deeply ingrained anthropocentric worldviews that shape human-nature relationships. As digital environments increasingly mediate these interactions, video games provide novel contexts for examining the psychological mechanisms underlying environmental behaviors. This study investigates how anthropocentric values are associated with resource-exploiting behaviors in virtual ecosystems--specifically, fishing, bug catching, and tree cutting--and how immersiveness moderates these relationships. Employing the Bayesian Mindsponge Framework (BMF) to analyze data from 640 Animal Crossi,g: New Horizons (ACNH) players across 29 countries, the study reveals complex links between anthropocentric worldviews and in-game behaviors. Fishing and tree-cutting frequencies are positively associated with anthropocentrism, whereas immersiveness weakens the association between tree cutting and anthropocentrism. Bug-catching frequency shows no direct effect but exhibits a growing negative association with anthropocentrism as immersiveness increases. These findings extend environmental psychology into virtual ecologies, illustrating how digital interactions both reflect and reshape environmental values. They highlight the potential of immersive gameplay to cultivate the Nature Quotient (NQ) and foster an eco-surplus culture through reflective, conservation-oriented engagement."}
{"id": "2512.12579", "categories": ["stat.AP", "stat.OT"], "pdf": "https://arxiv.org/pdf/2512.12579", "abs": "https://arxiv.org/abs/2512.12579", "authors": ["Malinda Iluppangama", "Dilmi Abeywardana", "Chris Tsokos"], "title": "A Real Data-Driven, Robust Survival Analysis on Patients who Underwent Deep Brain Stimulation for Parkinson's Disease by Utilizing Parametric, Non-Parametric, and Semi-Parametric Approaches", "comment": null, "summary": "Parkinson's Disease (PD) is a devastating neurodegenerative disorder that affects millions of people around the globe. Many researchers are continuously working to understand PD and develop treatments to improve the condition of PD patients, which affects their day-to-day lives. Since the last decades, the treatment, Deep Brain Stimulation (DBS) has given promising results for motor symptoms by improving the quality of daily living of PD patients. In the methodology of the present study, we have utilized sophisticated statistical approaches such as Nonparametric, Semi-parametric, and robust Parametric survival analysis to extract useful and important information about the long-term survival outcomes of the patients who underwent DBS for PD. Finally, we were able to conclude that the probabilistic behavior of the survival time of female patients is statistically different from that of male patients. Furthermore, we have identified that the probabilistic behavior of the survival times of Female patients is characterized by the 3-parameter Lognormal distribution, while that of Male patients is characterized by the 3-parameter Weibull distribution. More importantly, we have found that the Female patients have higher survival compared to the Male patients after conducting a robust parametric survival analysis. Using the semi-parametric COX-PH, we found that the initial implant of the right side leads to a high frequency of events occurring for the female patients with a bad prognostic factor, while for the male patients, a low events occurs with a good prognostic factor. Furthermore, we have found an interaction term between the number of revisions and the initial size of the implant, which increases the frequency of events occurring for the Male patients with a bad prognostic factor."}
{"id": "2512.12332", "categories": ["cs.SI", "cs.AI", "cs.CR", "cs.IT"], "pdf": "https://arxiv.org/pdf/2512.12332", "abs": "https://arxiv.org/abs/2512.12332", "authors": ["Saad Alqithami"], "title": "Dynamic Homophily with Imperfect Recall: Modeling Resilience in Adversarial Networks", "comment": null, "summary": "The purpose of this study is to investigate how homophily, memory constraints, and adversarial disruptions collectively shape the resilience and adaptability of complex networks. To achieve this, we develop a new framework that integrates explicit memory decay mechanisms into homophily-based models and systematically evaluate their performance across diverse graph structures and adversarial settings. Our methods involve extensive experimentation on synthetic datasets, where we vary decay functions, reconnection probabilities, and similarity measures, primarily comparing cosine similarity with traditional metrics such as Jaccard similarity and baseline edge weights. The results show that cosine similarity achieves up to a 30\\% improvement in stability metrics in sparse, convex, and modular networks. Moreover, the refined value-of-recall metric demonstrates that strategic forgetting can bolster resilience by balancing network robustness and adaptability. The findings underscore the critical importance of aligning memory and similarity parameters with the structural and adversarial dynamics of the network. By quantifying the tangible benefits of incorporating memory constraints into homophily-based analyses, this study offers actionable insights for optimizing real-world applications, including social systems, collaborative platforms, and cybersecurity contexts."}
{"id": "2512.12352", "categories": ["econ.EM"], "pdf": "https://arxiv.org/pdf/2512.12352", "abs": "https://arxiv.org/abs/2512.12352", "authors": ["Mohamed Chaouch", "Thanasis Stengos"], "title": "Modeling the Happiness-Sustainability Nexus via Graphical Lasso and Quantile-on-Quantile Regression", "comment": null, "summary": "This paper investigates the nexus between subjective well-being and sustainability, proxied by the Sustainable Development Goals (SDG) Index, using cross-country data from 126 nations in 2022. While prior research has highlighted a positive association between happiness and sustainable development, existing approaches largely rely on linear regressions or correlation-based measures that mask distributional heterogeneity, multicollinearity, and potential nonlinear dependence. To address these limitations, we employ a two methodological framework combining Graphical Lasso, and Quantile-on-Quantile Regression (QQR). The Graphical Lasso identifies a direct conditional link between happiness and sustainability after controlling for governance, income, and life expectancy, with a partial correlation of about 0.21. On the other hand, QQR reveals heterogeneous effects across the joint distribution: sustainability gains are positively associated with happiness for low-happiness but high-sustainability countries, negatively associated in high-happiness but low-sustainability contexts, and essentially neutral elsewhere. These findings suggest that the happiness-sustainability link is modest, asymmetric, and context-dependent, underscoring the importance of moving beyond mean-based regressions. From a policy perspective, our results highlight that institutional quality, income, and demographic factors remain the dominant drivers of both happiness and sustainability, while the interplay between the two dimensions is most pronounced in distributional extremes."}
{"id": "2512.11818", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.11818", "abs": "https://arxiv.org/abs/2512.11818", "authors": ["Izabela Lipinska", "Hugh Brosnahan"], "title": "The Ontological Dissonance Hypothesis: AI-Triggered Delusional Ideation as Folie a Deux Technologique", "comment": "18 pages excluding appendices", "summary": "This paper argues that contemporary large language models (LLMs) can contribute to psychotic involvement by creating interactions that resemble the relational dynamics of folie a deux. Drawing on Bateson's double bind theory, clinical literature on shared psychotic disorder, and McGilchrist's hemisphere theory, we show how the combination of high linguistic coherence and the absence of an underlying subject produces a structural tension for the user: language suggests an interlocutor, while intuition registers a void. In contexts of emotional need or instability, this tension can lead users to resolve the conflict through imaginative projection, attributing interiority, intention, or presence to a system that possesses none. The paper situates these dynamics within emerging clinical reports, develops a phenomenological account of how they unfold, and argues that current engagement-optimised design choices exacerbate the risk. We conclude by proposing 'ontological honesty' as a necessary design principle for mitigating technologically mediated folie a deux."}
{"id": "2512.11902", "categories": ["cs.AI", "cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11902", "abs": "https://arxiv.org/abs/2512.11902", "authors": ["Yanna Elizabeth Smid", "Peter van der Putten", "Aske Plaat"], "title": "Mirror Mode in Fire Emblem: Beating Players at their own Game with Imitation and Reinforcement Learning", "comment": null, "summary": "Enemy strategies in turn-based games should be surprising and unpredictable. This study introduces Mirror Mode, a new game mode where the enemy AI mimics the personal strategy of a player to challenge them to keep changing their gameplay. A simplified version of the Nintendo strategy video game Fire Emblem Heroes has been built in Unity, with a Standard Mode and a Mirror Mode. Our first set of experiments find a suitable model for the task to imitate player demonstrations, using Reinforcement Learning and Imitation Learning: combining Generative Adversarial Imitation Learning, Behavioral Cloning, and Proximal Policy Optimization. The second set of experiments evaluates the constructed model with player tests, where models are trained on demonstrations provided by participants. The gameplay of the participants indicates good imitation in defensive behavior, but not in offensive strategies. Participant's surveys indicated that they recognized their own retreating tactics, and resulted in an overall higher player-satisfaction for Mirror Mode. Refining the model further may improve imitation quality and increase player's satisfaction, especially when players face their own strategies. The full code and survey results are stored at: https://github.com/YannaSmid/MirrorMode"}
{"id": "2512.11814", "categories": ["cs.CY", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.11814", "abs": "https://arxiv.org/abs/2512.11814", "authors": ["Hugh Brosnahan"], "title": "Totalitarian Technics: The Hidden Cost of AI Scribes in Healthcare", "comment": null, "summary": "Artificial intelligence (AI) scribes, systems that record and summarise patient-clinician interactions, are promoted as solutions to administrative overload. This paper argues that their significance lies not in efficiency gains but in how they reshape medical attention itself. Offering a conceptual analysis, it situates AI scribes within a broader philosophical lineage concerned with the externalisation of human thought and skill. Drawing on Iain McGilchrist's hemisphere theory and Lewis Mumford's philosophy of technics, the paper examines how technology embodies and amplifies a particular mode of attention. AI scribes, it contends, exemplify the dominance of a left-hemispheric, calculative mindset that privileges the measurable and procedural over the intuitive and relational. As this mode of attention becomes further embedded in medical practice, it risks narrowing the field of care, eroding clinical expertise, and reducing physicians to operators within an increasingly mechanised system."}
{"id": "2512.13346", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.13346", "abs": "https://arxiv.org/abs/2512.13346", "authors": ["Yukun Lu", "Bingjie Li", "Zhigang Yao"], "title": "Beyond Missing Data: Questionnaire Uncertainty Responses as Early Digital Biomarkers of Cognitive Decline and Neurodegenerative Diseases", "comment": null, "summary": "Identifying preclinical biomarkers of neurodegenerative diseases remains a major challenge in aging research. In this study, we demonstrate that frequent \"Don't know/can't remember\" (DK) responses, often treated as missing data in touchscreen questionnaires, serve as a novel digital behavioral biomarker of early cognitive vulnerability and neurodegenerative disease risk. Using data from 502,234 UK Biobank participants, we stratified individuals based on DK response frequency (0-1, 2-4, 5-7, >7) and observed a robust, dose-dependent association with an increased risk of Alzheimer's disease (HR = 1.64, 95% CI: 1.26-2.14) and vascular dementia (HR = 1.93, 95% CI: 1.37-2.72), independent of established risk factors. As DK response frequency increased, participants exhibited higher BMI, reduced physical activity, higher smoking rates, and a higher prevalence of chronic diseases, particularly hypertension, diabetes, and depression. Further analysis revealed a dose-dependent relationship between DK response frequency and the risk of Alzheimer's disease and vascular dementia, with high DK responders showing early neurodegenerative changes, marked by elevated levels of Abeta40, Abeta42, NFL, and pTau-181. Metabolomic analysis also revealed lipid metabolism abnormalities, which may mediate this relationship. Together, these findings reframe DK response patterns as clinically meaningful signals of multidimensional neurobiological alterations, offering a scalable, low-cost, non-invasive tool for early risk identification and prevention at the population level."}
{"id": "2512.12441", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2512.12441", "abs": "https://arxiv.org/abs/2512.12441", "authors": ["Rr. Nefriana", "Muheng Yan", "Rebecca Hwa", "Yu-Ru Lin"], "title": "Leader-driven or Leaderless: How Participation Structure Sustains Engagement and Shapes Narratives in Online Hate Communities", "comment": null, "summary": "Extremist communities increasingly rely on social media to sustain and amplify divisive discourse. However, the relationship between their internal participation structures, audience engagement, and narrative expression remains underexplored. This study analyzes ten years of Facebook activity by hate groups related to the Israel-Palestine conflict, focusing on anti-Semitic and Islamophobic ideologies. Consistent with prior work, we find that higher participation centralization in online hate groups is associated with greater user engagement across hate ideologies, suggesting the role of key actors in sustaining group activity over time. Conversely, our narrative frame detection models - based on an eight-frame extremist taxonomy (e.g., dehumanization, violence justification) - reveal a clear contrast across hate ideologies, offering new insight into how discursive strategies vary despite similar structural dynamics. Analysis of the inter-group network indicates that, although centralization and homophily are not clearly linked, ideological distinctions emerge: Islamophobic groups cluster tightly, whereas anti-Semitic groups remain more evenly connected. Overall, these findings clarify how participation structure may shape the dissemination pattern and resonance of extremist narratives online and provide a foundation for tailored strategies to disrupt or mitigate online extremist discourse."}
{"id": "2512.12499", "categories": ["econ.EM", "q-fin.CP"], "pdf": "https://arxiv.org/pdf/2512.12499", "abs": "https://arxiv.org/abs/2512.12499", "authors": ["Pablo Hidalgo", "Julio E. Sandubete", "Agustín García-García"], "title": "Explainable Prediction of Economic Time Series Using IMFs and Neural Networks", "comment": "12 pages, 10 figures, 8 tables", "summary": "This study investigates the contribution of Intrinsic Mode Functions (IMFs) derived from economic time series to the predictive performance of neural network models, specifically Multilayer Perceptrons (MLP) and Long Short-Term Memory (LSTM) networks. To enhance interpretability, DeepSHAP is applied, which estimates the marginal contribution of each IMF while keeping the rest of the series intact. Results show that the last IMFs, representing long-term trends, are generally the most influential according to DeepSHAP, whereas high-frequency IMFs contribute less and may even introduce noise, as evidenced by improved metrics upon their removal. Differences between MLP and LSTM highlight the effect of model architecture on feature relevance distribution, with LSTM allocating importance more evenly across IMFs."}
{"id": "2512.11819", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.11819", "abs": "https://arxiv.org/abs/2512.11819", "authors": ["Daniil Sukhorukov", "Andrei Zakharov", "Nikita Glazkov", "Katsiaryna Yanchanka", "Vladimir Kirilin", "Maxim Dubovitsky", "Roman Sultimov", "Yuri Maksimov", "Ilya Makarov"], "title": "A Modular LLM-Agent System for Transparent Multi-Parameter Weather Interpretation", "comment": null, "summary": "Weather forecasting is not only a predictive task but an interpretive scientific process requiring explanation, contextualization, and hypothesis generation. This paper introduces AI-Meteorologist, an explainable LLM-agent framework that converts raw numerical forecasts into scientifically grounded narrative reports with transparent reasoning steps. Unlike conventional forecast outputs presented as dense tables or unstructured time series, our system performs agent-based analysis across multiple meteorological variables, integrates historical climatological context, and generates structured explanations that identify weather fronts, anomalies, and localized dynamics. The architecture relies entirely on in-context prompting, without fine-tuning, demonstrating that interpretability can be achieved through reasoning rather than parameter updates. Through case studies on multi-location forecast data, we show how AI-Meteorologist not only communicates weather events but also reveals the underlying atmospheric drivers, offering a pathway toward AI systems that augment human meteorological expertise and support scientific discovery in climate analytics."}
{"id": "2512.11907", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11907", "abs": "https://arxiv.org/abs/2512.11907", "authors": ["Daniel Platnick", "Marjan Alirezaie", "Hossein Rahnama"], "title": "Structured Personalization: Modeling Constraints as Matroids for Data-Minimal LLM Agents", "comment": "Accepted to the AAAI 2026 Workshop on Personalization in the Era of Large Foundation Models (PerFM), 5 pages, 1 figure", "summary": "Personalizing Large Language Model (LLM) agents requires conditioning them on user-specific data, creating a critical trade-off between task utility and data disclosure. While the utility of adding user data often exhibits diminishing returns (i.e., submodularity), enabling near-optimal greedy selection, real-world personalization is complicated by structural constraints. These include logical dependencies (e.g., selecting fact A requires fact B), categorical quotas (e.g., select at most one writing style), and hierarchical rules (e.g., select at most two social media preferences, of which at most one can be for a professional network). These constraints violate the assumptions of standard subset selection algorithms. We propose a principled method to formally model such constraints. We introduce a compilation process that transforms a user's knowledge graph with dependencies into a set of abstract macro-facets. Our central result is a proof that common hierarchical and quota-based constraints over these macro-facets form a valid laminar matroid. This theoretical characterization lets us cast structured personalization as submodular maximization under a matroid constraint, enabling greedy with constant-factor guarantees (and (1-1/e) via continuous greedy) for a much richer and more realistic class of problems."}
{"id": "2512.11815", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.11815", "abs": "https://arxiv.org/abs/2512.11815", "authors": ["Max Kamachee", "Stephen Casper", "Michelle L. Ding", "Rui-Jie Yew", "Anka Reuel", "Stella Biderman", "Dylan Hadfield-Menell"], "title": "Video Deepfake Abuse: How Company Choices Predictably Shape Misuse Patterns", "comment": null, "summary": "In 2022, AI image generators crossed a key threshold, enabling much more efficient and dynamic production of photorealistic deepfake images than before. This enabled opportunities for creative and positive uses of these models. However, it also enabled unprecedented opportunities for the low-effort creation of AI-generated non-consensual intimate imagery (AIG-NCII), including AI-generated child sexual abuse material (AIG-CSAM). Empirically, these harms were principally enabled by a small number of models that were trained on web data with pornographic content, released with open weights, and insufficiently safeguarded. In this paper, we observe ways in which the same patterns are emerging with video generation models in 2025. Specifically, we analyze how a small number of open-weight AI video generation models have become the dominant tools for videorealistic AIG-NCII video generation. We then analyze the literature on model safeguards and conclude that (1) developers who openly release the weights of capable video generation models without appropriate data curation and/or post-training safeguards foreseeably contribute to mitigatable downstream harm, and (2) model distribution platforms that do not proactively moderate individual misuse or models designed for AIG-NCII foreseeably amplify this harm. While there are no perfect defenses against AIG-NCII and AIG-CSAM from open-weight AI models, we argue that risk management by model developers and distributors, informed by emerging safeguard techniques, will substantially affect the future ease of creating AIG-NCII and AIG-CSAM with generative AI video tools."}
{"id": "2512.11875", "categories": ["cs.CY", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.11875", "abs": "https://arxiv.org/abs/2512.11875", "authors": ["Ting Luo", "Yan Wang"], "title": "The Art of Storytelling in Authoritarian Regimes: Crafting State Narratives on Chinese Social Media", "comment": null, "summary": "This article examines how authoritarian regimes construct state narratives about politically consequential events. Building on the narrative policy framework and existing research on authoritarian propaganda, we propose two dimensions that shape narrative construction: legitimacy implications -- whether events enhance or threaten regime legitimacy, and citizen verification capacity -- the extent to which citizens can evaluate official narratives through alternative sources. Using quantitative narrative analysis of Chinese social media posts by government, state media, and celebrity accounts, we extract subject-verb-object (SVO) triplets to map dominant narrative structures across four major events. Our findings show that legitimacy implications of the event shape regime's efforts in storytelling and the beliefs highlighted in the narratives, while citizen's verification capacity could balance the strategic choice between a top-down manipulation and bottom-up responsiveness of state narratives. Together, the results reveal propaganda as a complex process of narrative construction adaptive to specific contexts, offering new insights into how dynamic storytelling sustains authoritarian resilience."}
{"id": "2512.13231", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2512.13231", "abs": "https://arxiv.org/abs/2512.13231", "authors": ["Vesa Kuikka", "Kosti Koistinen", "Kimmo K Kaski"], "title": "Shared Nodes of Overlapping Communities in Complex Networks", "comment": null, "summary": "Overlapping communities are key characteristics of the structure and function analysis of complex networks. Shared or overlapping nodes within overlapping communities can form either subcommunities or act as intersections between larger communities. Nodes at the intersections that do not form subcommunities can be identified as overlapping nodes or as part of an internal structure of nested communities. To identify overlapping nodes, we apply a threshold rule based on the number of nodes in the nested structure. As the threshold value increases, the number of selected overlapping nodes decreases. This approach allows us to analyse the roles of nodes considered overlapping according to selection criteria, for example to reduce the effect of noise. We illustrate our method by using three small and two larger real-world network structures. In larger networks, minor disturbances can produce a multitude of slightly different solutions, but the core communities remain robust, allowing other variations to be treated as noise. While this study employs our own method for community detection, other approaches can also be applied. Exploring the properties of shared nodes in overlapping communities of complex networks is a novel area of research with diverse applications in social network analysis, cybersecurity, and other fields in network science."}
{"id": "2512.12653", "categories": ["econ.EM", "econ.TH", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.12653", "abs": "https://arxiv.org/abs/2512.12653", "authors": ["Tatsuru Kikuchi"], "title": "Continuous Treatment Effects with Spatial and Network Spillovers", "comment": "109 pages, 5 figures", "summary": "This paper develops a continuous functional framework for treatment effects that propagate through geographic space and economic networks. We derive a master equation governing propagation from three economic foundations -- heterogeneous agent aggregation, market equilibrium, and cost minimization -- establishing that the framework rests on fundamental principles rather than ad hoc specifications. A key result shows that the spatial-network interaction coefficient equals the mutual information between geographic and market coordinates. The Feynman-Kac representation decomposes effects into inherited and accumulated components along stochastic paths representing economic linkages. The framework nests the no-spillover case as a testable restriction. Monte Carlo simulations demonstrate that conventional estimators -- two-way fixed effects, difference-in-differences, and generalized propensity score -- exhibit 25-38% bias and severe undercoverage when spillovers exist, while our estimator maintains correct inference regardless of whether spillovers are present. Applying the framework to U.S. minimum wage policy, we reject the no-spillover null and find total effects at state borders four times larger than direct effects -- conventional methods capture only one-quarter of policy impact. Structural estimates reveal spatial diffusion consistent with commuting-distance labor mobility, network diffusion consistent with quarterly supply chain adjustment, and significant spatial-network interaction reflecting geographic clustering of industries. Entropy-based fragility diagnostics outperform standard centrality measures by 56-76% in predicting labor market disruptions, identifying all high-risk state-industry pairs during 2020-2021 with six-month advance warning."}
{"id": "2512.11821", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.11821", "abs": "https://arxiv.org/abs/2512.11821", "authors": ["John Paul P. Miranda", "Joseph Alexander Bansil", "Emerson Q. Fernando", "Almer B. Gamboa", "Hilene E. Hernandez", "Myka A. Cruz", "Roque Francis B. Dianelo", "Dina D. Gonzales", "Elmer M. Penecilla"], "title": "Prevalence, Devices Used, Reasons for Use, Trust, Barriers, and Challenges in Utilizing Generative AI among Tertiary Students", "comment": "12 pages, 1 table, 1 figure", "summary": "This study examined generative AI usage among Philippine college students particularly on frequency, devices, reasons, knowledge, trust, perceptions, and challenges. Most students used free AI tools on smartphones due to financial constraints. They used it primarily for homework, idea generation, and research. Less than half felt confident with AI and expressed mixed feelings about its accuracy. Barriers included limited access, lack of teacher support, difficulty understanding outputs, and financial constraints. The study highlighted the need for better access, support, training, and ethical guidelines. Broader concerns included impacts on learning, academic standards, job loss, and privacy. Students viewed AI positively due to peer support. Recommendations are discussed."}
{"id": "2512.11909", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11909", "abs": "https://arxiv.org/abs/2512.11909", "authors": ["Hanna Dettki"], "title": "Causal Strengths and Leaky Beliefs: Interpreting LLM Reasoning via Noisy-OR Causal Bayes Nets", "comment": null, "summary": "The nature of intelligence in both humans and machines is a longstanding question. While there is no universally accepted definition, the ability to reason causally is often regarded as a pivotal aspect of intelligence (Lake et al., 2017). Evaluating causal reasoning in LLMs and humans on the same tasks provides hence a more comprehensive understanding of their respective strengths and weaknesses. Our study asks: (Q1) Are LLMs aligned with humans given the \\emph{same} reasoning tasks? (Q2) Do LLMs and humans reason consistently at the task level? (Q3) Do they have distinct reasoning signatures?\n  We answer these by evaluating 20+ LLMs on eleven semantically meaningful causal tasks formalized by a collider graph ($C_1\\!\\to\\!E\\!\\leftarrow\\!C_2$ ) under \\emph{Direct} (one-shot number as response = probability judgment of query node being one and \\emph{Chain of Thought} (CoT; think first, then provide answer).\n  Judgments are modeled with a leaky noisy-OR causal Bayes net (CBN) whose parameters $θ=(b,m_1,m_2,p(C)) \\in [0,1]$ include a shared prior $p(C)$;\n  we select the winning model via AIC between a 3-parameter symmetric causal strength ($m_1{=}m_2$) and 4-parameter asymmetric ($m_1{\\neq}m_2$) variant."}
{"id": "2512.11817", "categories": ["cs.CY", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11817", "abs": "https://arxiv.org/abs/2512.11817", "authors": ["Juan Palomeque-Gonzalez"], "title": "A Reproducible Workflow for Scraping, Structuring, and Segmenting Legacy Archaeological Artifact Images", "comment": "12 Pages, 5 figures", "summary": "This technical note presents a reproducible workflow for converting a legacy archaeological image collection into a structured and segmentation ready dataset. The case study focuses on the Lower Palaeolithic hand axe and biface collection curated by the Archaeology Data Service (ADS), a dataset that provides thousands of standardised photographs but no mechanism for bulk download or automated processing. To address this, two open source tools were developed: a web scraping script that retrieves all record pages, extracts associated metadata, and downloads the available images while respecting ADS Terms of Use and ethical scraping guidelines; and an image processing pipeline that renames files using UUIDs, generates binary masks and bounding boxes through classical computer vision, and stores all derived information in a COCO compatible Json file enriched with archaeological metadata. The original images are not redistributed, and only derived products such as masks, outlines, and annotations are shared. Together, these components provide a lightweight and reusable approach for transforming web based archaeological image collections into machine learning friendly formats, facilitating downstream analysis and contributing to more reproducible research practices in digital archaeology."}
{"id": "2512.12212", "categories": ["cs.CY", "econ.GN", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.12212", "abs": "https://arxiv.org/abs/2512.12212", "authors": ["Elizabeth Irenne Yuwono", "Dian Tjondronegoro", "Shawn Hunter", "Amber Marshall"], "title": "Anticipatory Governance in Data-Constrained Environments: A Predictive Simulation Framework for Digital Financial Inclusion", "comment": "28 pages, 3 figures", "summary": "Financial exclusion remains a major barrier to digital public service delivery in resource-constrained and archipelagic nations. Traditional policy evaluations rely on retrospective data, limiting the ex-ante intelligence needed for agile resource allocation. This study introduces a predictive simulation framework to support anticipatory governance within government information systems. Using the UNCDF Pacific Digital Economy dataset of 10,108 respondents, we apply a three-stage pipeline: descriptive profiling, interpretable machine learning, and scenario simulation to forecast outcomes of digital financial literacy interventions before deployment. Leveraging cross-sectional structural associations, the framework projects intervention scenarios as prioritization heuristics rather than causal estimates. A transparent linear regression model with R-squared of 95.9 identifies modifiable policy levers. Simulations indicate that foundational digital capabilities such as device access and expense tracking yield the highest projected gains, up to 5.5 percent, outperforming attitudinal nudges. The model enables precision targeting, highlighting young female caregivers as high-leverage responders while flagging non-responders such as urban professionals to prevent resource misallocation. This research demonstrates how static survey data can be repurposed into actionable policy intelligence, offering a scalable and evidence-based blueprint for embedding predictive analytics into public-sector decision-support systems to advance equity-focused digital governance."}
{"id": "2512.13643", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2512.13643", "abs": "https://arxiv.org/abs/2512.13643", "authors": ["Laura Kurek", "Joshua Ashkinaze", "Ceren Budak", "Eric Gilbert"], "title": "Follow Nudges without Budges: A Field Experiment on Misinformation Followers Didn't Change Follow Networks", "comment": null, "summary": "Can digital ads encourage users exposed to inaccurate information sources to follow accurate ones? We conduct a large-scale field experiment (N=28,582) on X, formerly Twitter, with users who follow accounts that spread health misinformation. Participants were exposed to four ad treatments varied on two dimensions: a neutral message versus a persuasive message appealing to values of independence, and a request to follow a health institution versus a request to follow a health influencer. We term this ad-based, social network intervention a follow nudge. The ad with a persuasive message to follow a well-known health institution generated significantly higher click-through rates than all other conditions (Bonferroni-corrected pairwise tests, all p<0.001). Given the overall low click-through rate across treatments and the high cost of digital advertising infrastructure on X, however, we conclude that our proposed intervention -- at least in its current ad-based format -- is not a cost-effective means to improve information environments online. We discuss challenges faced when conducting large-scale experiments on X following the platform's ownership change and subsequent restrictions on data access for research purposes."}
{"id": "2512.12781", "categories": ["econ.EM"], "pdf": "https://arxiv.org/pdf/2512.12781", "abs": "https://arxiv.org/abs/2512.12781", "authors": ["Ruonan Xu", "Xiye Yang"], "title": "Distributionally Robust Treatment Effect", "comment": null, "summary": "Using only retrospective data, we propose an estimator for predicting the treatment effect for the same treatment/policy to be implemented in another location or time period, which requires no input from the target population. More specifically, we minimize the worst-case mean square error for the prediction of treatment effect within a class of distributions inside the Wasserstein ball centered on the source distribution. Since the joint distribution of potential outcomes is not identified, we pick the best and worst copulas of the marginal distributions of two potential outcomes as our optimistic and pessimistic optimization objects for partial identification. As a result, we can attain the upper and lower bounds of the minimax optimizer. The minimax solution differs depending on whether treatment effects are homogeneous or heterogeneous. We derive the consistency and asymptotic distribution of the bound estimators, provide a two-step inference procedure, and discuss the choice of the Wasserstein ball radius."}
{"id": "2512.11822", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.11822", "abs": "https://arxiv.org/abs/2512.11822", "authors": ["Hilene E. Hernandez", "Ranie B. Canlas", "Madilaine Claire B. Nacianceno", "Jordan L. Salenga", "Jaymark A. Yambao", "Juvy C. Grume", "Aileen P. De Leon", "Freneil R. Pampo", "John Paul P. Miranda"], "title": "Trust, Usefulness, and Dependency on AI in Programming: A Hierarchical Clustering Approach", "comment": "8 pages, 2 tables, 2 figures", "summary": "While AI tools are transforming programming education, their adoption in underrepresented countries remains insufficiently studied. Understanding students' trust, perceived usefulness, and dependency on AI tools is essential to improving their integration into education. For these purposes, this study surveyed 508 first-year programming students in Pampanga, Philippines and analyzed their perceptions using hierarchical clustering. Results showed four unique student profiles with varying in trust and usage intensity. While students acknowledged AI tools' benefits, dependency remained low due to limited infrastructure and insufficient exposure. High-frequency users did not necessarily report greater trust or usefulness which may indicates a complex relationship between usage patterns and perception. This study recommends that to maximize AI's educational impact, targeted interventions such as infrastructure development, training programs, and curriculum integration are necessary. This study provides empirical insights to support equitable and effective AI adoption in programming education within developing regions."}
{"id": "2512.11912", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11912", "abs": "https://arxiv.org/abs/2512.11912", "authors": ["Liu Peng", "Yaochu Jin"], "title": "Robustness of Probabilistic Models to Low-Quality Data: A Multi-Perspective Analysis", "comment": null, "summary": "A systematic, comparative investigation into the effects of low-quality data reveals a stark spectrum of robustness across modern probabilistic models. We find that autoregressive language models, from token prediction to sequence-to-sequence tasks, are remarkably resilient (for GPT-2, test NLL increases modestly from 2.87 to 3.59 despite 50% token corruption). By contrast, under the same levels of data corruption, class-conditional diffusion models degrade catastrophically (image-label consistency plummets by 56.81% relative to baseline), while classifiers show a moderate impact that diminishes with dataset scale. To explain these discrepancies, we analyze the results through a multi-perspective lens, integrating information theory, PAC learning, and gradient dynamics. These analyses suggest that robustness is heavily influenced by two key principles: the richness of conditioning information, which constrains the learning problem, and the absolute information content of the training data, which allows the signal from correct information to dominate statistical noise."}
{"id": "2512.11818", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.11818", "abs": "https://arxiv.org/abs/2512.11818", "authors": ["Izabela Lipinska", "Hugh Brosnahan"], "title": "The Ontological Dissonance Hypothesis: AI-Triggered Delusional Ideation as Folie a Deux Technologique", "comment": "18 pages excluding appendices", "summary": "This paper argues that contemporary large language models (LLMs) can contribute to psychotic involvement by creating interactions that resemble the relational dynamics of folie a deux. Drawing on Bateson's double bind theory, clinical literature on shared psychotic disorder, and McGilchrist's hemisphere theory, we show how the combination of high linguistic coherence and the absence of an underlying subject produces a structural tension for the user: language suggests an interlocutor, while intuition registers a void. In contexts of emotional need or instability, this tension can lead users to resolve the conflict through imaginative projection, attributing interiority, intention, or presence to a system that possesses none. The paper situates these dynamics within emerging clinical reports, develops a phenomenological account of how they unfold, and argues that current engagement-optimised design choices exacerbate the risk. We conclude by proposing 'ontological honesty' as a necessary design principle for mitigating technologically mediated folie a deux."}
{"id": "2512.13270", "categories": ["econ.EM"], "pdf": "https://arxiv.org/pdf/2512.13270", "abs": "https://arxiv.org/abs/2512.13270", "authors": ["Grigory Franguridi", "Jinyong Hahn", "Pierre Hoonhout", "Arie Kapteyn", "Geert Ridder"], "title": "Raking for estimation and inference in panel models with nonignorable attrition and refreshment", "comment": null, "summary": "In panel data subject to nonignorable attrition, auxiliary (refreshment) sampling may restore full identification under weak assumptions on the attrition process. Despite their generality, these identification strategies have seen limited empirical use, largely because the implied estimation procedure requires solving a functional minimization problem for the target density. We show that this problem can be solved using the iterative proportional fitting (raking) algorithm, which converges rapidly even with continuous and moderately high-dimensional data. This resulting density estimator is then used as input into a parametric moment condition. We establish consistency and convergence rates for both the raking-based density estimator and the resulting moment estimator when the distributions of the observed data are parametric. We also derive a simple recursive procedure for estimating the asymptotic variance. Finally, we demonstrate the satisfactory performance of our estimator in simulations and provide an empirical illustration using data from the Understanding America Study panel."}
{"id": "2512.11823", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.11823", "abs": "https://arxiv.org/abs/2512.11823", "authors": ["Vicky P. Vital", "Francis F. Balahadia", "Maria Anna D. Cruz", "Dolores D. Mallari", "Juvy C. Grume", "Erika M. Pineda", "Jordan L. Salenga", "Lloyd D. Feliciano", "John Paul P. Miranda"], "title": "Teachers' Perspectives on the Use of AI Detection Tools: Insights from Ridge Regression Analysis", "comment": "8 pages, 2 tables, 2025 International Conference on Distance Education and Learning (ICDEL)", "summary": "This study explores the perceptions of 213 Filipino teachers toward AI detection tools in academic settings. It focuses on the factors that influence teachers' trust, concerns, and decision-making regarding these tools. The research investigates how teachers' trust in AI detection tools affects their perceptions of fairness and decision-making in evaluating student outputs. It also explores how concerns about AI tools and social norms influence the relationship between trust and decision-making. Ridge Regression analysis was used to examine the relationships between the predictors and the dependent variable. The results revealed that trust in AI detection tools is the most significant predictor of perceived fairness and decision-making among teachers. Concerns about AI tools and social norms have weaker effects on teachers' perceptions. The study emphasized critical role of trust in shaping teachers' perceptions of AI detection tools. Teachers who trust these tools are more likely to view them as fair and effective. In contrast, concerns and social norms have a limited influence on perceptions and decision-making. For recommendations, training and institutional guidelines should emphasize how these tools work, their limitations, and best practices for their use. Striking a balance between policy enforcement and educator support is essential for fostering trust in AI detection technologies. Encouraging experienced users to share insights through communities of practice could enhance the adoption and effective use of AI detection tools in educational settings.."}
{"id": "2512.11920", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11920", "abs": "https://arxiv.org/abs/2512.11920", "authors": ["Dong Liu", "Yanxuan Yu"], "title": "CXL-SpecKV: A Disaggregated FPGA Speculative KV-Cache for Datacenter LLM Serving", "comment": "Accepted to FPGA'26 Oral", "summary": "Large Language Models (LLMs) have revolutionized natural language processing tasks, but their deployment in datacenter environments faces significant challenges due to the massive memory requirements of key-value (KV) caches. During the autoregressive decoding process, KV caches consume substantial GPU memory, limiting batch sizes and overall system throughput. To address these challenges, we propose \\textbf{CXL-SpecKV}, a novel disaggregated KV-cache architecture that leverages Compute Express Link (CXL) interconnects and FPGA accelerators to enable efficient speculative execution and memory disaggregation. Our approach introduces three key innovations: (i) a CXL-based memory disaggregation framework that offloads KV-caches to remote FPGA memory with low latency, (ii) a speculative KV-cache prefetching mechanism that predicts and preloads future tokens' cache entries, and (iii) an FPGA-accelerated KV-cache compression and decompression engine that reduces memory bandwidth requirements by up to 4$\\times$. When evaluated on state-of-the-art LLM models, CXL-SpecKV achieves up to 3.2$\\times$ higher throughput compared to GPU-only baselines, while reducing memory costs by 2.8$\\times$ and maintaining accuracy. Our system demonstrates that intelligent memory disaggregation combined with speculative execution can effectively address the memory wall challenge in large-scale LLM serving. Our code implementation has been open-sourced at https://github.com/FastLM/CXL-SpecKV."}
{"id": "2512.11819", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.11819", "abs": "https://arxiv.org/abs/2512.11819", "authors": ["Daniil Sukhorukov", "Andrei Zakharov", "Nikita Glazkov", "Katsiaryna Yanchanka", "Vladimir Kirilin", "Maxim Dubovitsky", "Roman Sultimov", "Yuri Maksimov", "Ilya Makarov"], "title": "A Modular LLM-Agent System for Transparent Multi-Parameter Weather Interpretation", "comment": null, "summary": "Weather forecasting is not only a predictive task but an interpretive scientific process requiring explanation, contextualization, and hypothesis generation. This paper introduces AI-Meteorologist, an explainable LLM-agent framework that converts raw numerical forecasts into scientifically grounded narrative reports with transparent reasoning steps. Unlike conventional forecast outputs presented as dense tables or unstructured time series, our system performs agent-based analysis across multiple meteorological variables, integrates historical climatological context, and generates structured explanations that identify weather fronts, anomalies, and localized dynamics. The architecture relies entirely on in-context prompting, without fine-tuning, demonstrating that interpretability can be achieved through reasoning rather than parameter updates. Through case studies on multi-location forecast data, we show how AI-Meteorologist not only communicates weather events but also reveals the underlying atmospheric drivers, offering a pathway toward AI systems that augment human meteorological expertise and support scientific discovery in climate analytics."}
{"id": "2512.13400", "categories": ["econ.EM", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.13400", "abs": "https://arxiv.org/abs/2512.13400", "authors": ["Artem Timoshenko", "Caio Waisman"], "title": "Policy-Aligned Estimation of Conditional Average Treatment Effects", "comment": null, "summary": "Firms often develop targeting policies to personalize marketing actions and improve incremental profits. Effective targeting depends on accurately separating customers with positive versus negative treatment effects. We propose an approach to estimate the conditional average treatment effects (CATEs) of marketing actions that aligns their estimation with the firm's profit objective. The method recognizes that, for many customers, treatment effects are so extreme that additional accuracy is unlikely to change the recommended actions. However, accuracy matters near the decision boundary, as small errors can alter targeting decisions. By modifying the firm's objective function in the standard profit maximization problem, our method yields a near-optimal targeting policy while simultaneously estimating CATEs. This introduces a new perspective on CATE estimation, reframing it as a problem of profit optimization rather than prediction accuracy. We establish the theoretical properties of the proposed method and demonstrate its performance and trade-offs using synthetic data."}
{"id": "2512.11827", "categories": ["cs.CY", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11827", "abs": "https://arxiv.org/abs/2512.11827", "authors": ["Milad Malekzadeh", "Magdalena Biernacka", "Elias Willberg", "Jussi Torkko", "Edyta Łaszkiewicz", "Tuuli Toivonen"], "title": "Assessing Greenspace Attractiveness with ChatGPT, Claude, and Gemini: Do AI Models Reflect Human Perceptions?", "comment": null, "summary": "Understanding greenspace attractiveness is essential for designing livable and inclusive urban environments, yet existing assessment approaches often overlook informal or transient spaces and remain too resource intensive to capture subjective perceptions at scale. This study examines the ability of multimodal large language models (MLLMs), ChatGPT GPT-4o, Claude 3.5 Haiku, and Gemini 2.0 Flash, to assess greenspace attractiveness similarly to humans using Google Street View imagery. We compared model outputs with responses from a geo-questionnaire of residents in Lodz, Poland, across both formal (for example, parks and managed greenspaces) and informal (for example, meadows and wastelands) greenspaces. Survey respondents and models indicated whether each greenspace was attractive or unattractive and provided up to three free text explanations. Analyses examined how often their attractiveness judgments aligned and compared their explanations after classifying them into shared reasoning categories. Results show high AI human agreement for attractive formal greenspaces and unattractive informal spaces, but low alignment for attractive informal and unattractive formal greenspaces. Models consistently emphasized aesthetic and design oriented features, underrepresenting safety, functional infrastructure, and locally embedded qualities valued by survey respondents. While these findings highlight the potential for scalable pre-assessment, they also underscore the need for human oversight and complementary participatory approaches. We conclude that MLLMs can support, but not replace, context sensitive greenspace evaluation in planning practice."}
{"id": "2512.11935", "categories": ["cs.AI", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2512.11935", "abs": "https://arxiv.org/abs/2512.11935", "authors": ["Jaehyung Lee", "Justin Ely", "Kent Zhang", "Akshaya Ajith", "Charles Rhys Campbell", "Kamal Choudhary"], "title": "AGAPI-Agents: An Open-Access Agentic AI Platform for Accelerated Materials Design on AtomGPT.org", "comment": null, "summary": "Artificial intelligence is reshaping scientific discovery, yet its use in materials research remains limited by fragmented computational ecosystems, reproducibility challenges, and dependence on commercial large language models (LLMs). Here we introduce AGAPI (AtomGPT.org API), an open-access agentic AI platform that integrates more than eight open-source LLMs with over twenty materials-science API endpoints, unifying databases, simulation tools, and machine-learning models through a common orchestration framework. AGAPI employs an Agent-Planner-Executor-Summarizer architecture that autonomously constructs and executes multi-step workflows spanning materials data retrieval, graph neural network property prediction, machine-learning force-field optimization, tight-binding calculations, diffraction analysis, and inverse design. We demonstrate AGAPI through end-to-end workflows, including heterostructure construction, powder X-ray diffraction analysis, and semiconductor defect engineering requiring up to ten sequential operations. In addition, we evaluate AGAPI using 30+ example prompts as test cases and compare agentic predictions with and without tool access against experimental data. With more than 1,000 active users, AGAPI provides a scalable and transparent foundation for reproducible, AI-accelerated materials discovery. AGAPI-Agents codebase is available at https://github.com/atomgptlab/agapi."}
{"id": "2512.11821", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.11821", "abs": "https://arxiv.org/abs/2512.11821", "authors": ["John Paul P. Miranda", "Joseph Alexander Bansil", "Emerson Q. Fernando", "Almer B. Gamboa", "Hilene E. Hernandez", "Myka A. Cruz", "Roque Francis B. Dianelo", "Dina D. Gonzales", "Elmer M. Penecilla"], "title": "Prevalence, Devices Used, Reasons for Use, Trust, Barriers, and Challenges in Utilizing Generative AI among Tertiary Students", "comment": "12 pages, 1 table, 1 figure", "summary": "This study examined generative AI usage among Philippine college students particularly on frequency, devices, reasons, knowledge, trust, perceptions, and challenges. Most students used free AI tools on smartphones due to financial constraints. They used it primarily for homework, idea generation, and research. Less than half felt confident with AI and expressed mixed feelings about its accuracy. Barriers included limited access, lack of teacher support, difficulty understanding outputs, and financial constraints. The study highlighted the need for better access, support, training, and ethical guidelines. Broader concerns included impacts on learning, academic standards, job loss, and privacy. Students viewed AI positively due to peer support. Recommendations are discussed."}
{"id": "2512.13642", "categories": ["econ.EM", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.13642", "abs": "https://arxiv.org/abs/2512.13642", "authors": ["Giovanni Ballarin", "Lyudmila Grigoryeva", "Yui Ching Li"], "title": "From Many Models, One: Macroeconomic Forecasting with Reservoir Ensembles", "comment": null, "summary": "Model combination is a powerful approach to achieve superior performance with a set of models than by just selecting any single one. We study both theoretically and empirically the effectiveness of ensembles of Multi-Frequency Echo State Networks (MFESNs), which have been shown to achieve state-of-the-art macroeconomic time series forecasting results (Ballarin et al., 2024a). Hedge and Follow-the-Leader schemes are discussed, and their online learning guarantees are extended to the case of dependent data. In applications, our proposed Ensemble Echo State Networks show significantly improved predictive performance compared to individual MFESN models."}
{"id": "2512.11850", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.11850", "abs": "https://arxiv.org/abs/2512.11850", "authors": ["Davide Mancino"], "title": "The Memecoin Phenomenon: An In-Depth Study of Solana's Blockchain Trends", "comment": null, "summary": "This paper analyzes the emerging memecoin phenomenon on the Solana blockchain, focusing on the Pump.fun platform during Q4 2024. Using on-chain data, it is explored how retail-focused token creation platforms are reshaping blockchain ecosystems and influencing market participation. This study finds that Pump.fun accounted for up to 71.1% of all tokens minted on Solana and contributed 40-67.4% of total DEX transactions. Despite this activity, fewer than 2% of tokens successfully transitioned to major decentralized exchanges, highlighting a highly speculative market structure. The platform experienced rapid growth, with daily active users rising from 60,000 to peaks of 260,000, underscoring strong retail adoption. This reflects a broader shift towards accessible, socially-driven market participation enabled by memecoins. However, while memecoins lower entry barriers and encourage retail engagement, they introduce significant risks. The volatile and speculative nature of these platforms raises concerns about long-term sustainability and the resilience of the blockchain ecosystem. These findings reveal the dual impact of memecoins: they democratize token creation and alter market dynamics but may jeopardize market efficiency and stability. This paper highlights the need to critically assess the implications of retail-driven speculative trading and its potential to disrupt emerging blockchain economies."}
{"id": "2512.11942", "categories": ["cs.AI", "cs.FL", "cs.GT"], "pdf": "https://arxiv.org/pdf/2512.11942", "abs": "https://arxiv.org/abs/2512.11942", "authors": ["Vince Trencsenyi"], "title": "Hypergame Rationalisability: Solving Agent Misalignment In Strategic Play", "comment": null, "summary": "Differences in perception, information asymmetries, and bounded rationality lead game-theoretic players to derive a private, subjective view of the game that may diverge from the underlying ground-truth scenario and may be misaligned with other players' interpretations. While typical game-theoretic assumptions often overlook such heterogeneity, hypergame theory provides the mathematical framework to reason about mismatched mental models. Although hypergames have recently gained traction in dynamic applications concerning uncertainty, their practical adoption in multi-agent system research has been hindered by the lack of a unifying, formal, and practical representation language, as well as scalable algorithms for managing complex hypergame structures and equilibria. Our work addresses this gap by introducing a declarative, logic-based domain-specific language for encoding hypergame structures and hypergame solution concepts. Leveraging answer-set programming, we develop an automated pipeline for instantiating hypergame structures and running our novel hypergame rationalisation procedure, a mechanism for finding belief structures that justify seemingly irrational outcomes. The proposed language establishes a unifying formalism for hypergames and serves as a foundation for developing nuanced, belief-based heterogeneous reasoners, offering a verifiable context with logical guarantees. Together, these contributions establish the connection between hypergame theory, multi-agent systems, and strategic AI."}
{"id": "2512.11822", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.11822", "abs": "https://arxiv.org/abs/2512.11822", "authors": ["Hilene E. Hernandez", "Ranie B. Canlas", "Madilaine Claire B. Nacianceno", "Jordan L. Salenga", "Jaymark A. Yambao", "Juvy C. Grume", "Aileen P. De Leon", "Freneil R. Pampo", "John Paul P. Miranda"], "title": "Trust, Usefulness, and Dependency on AI in Programming: A Hierarchical Clustering Approach", "comment": "8 pages, 2 tables, 2 figures", "summary": "While AI tools are transforming programming education, their adoption in underrepresented countries remains insufficiently studied. Understanding students' trust, perceived usefulness, and dependency on AI tools is essential to improving their integration into education. For these purposes, this study surveyed 508 first-year programming students in Pampanga, Philippines and analyzed their perceptions using hierarchical clustering. Results showed four unique student profiles with varying in trust and usage intensity. While students acknowledged AI tools' benefits, dependency remained low due to limited infrastructure and insufficient exposure. High-frequency users did not necessarily report greater trust or usefulness which may indicates a complex relationship between usage patterns and perception. This study recommends that to maximize AI's educational impact, targeted interventions such as infrastructure development, training programs, and curriculum integration are necessary. This study provides empirical insights to support equitable and effective AI adoption in programming education within developing regions."}
{"id": "2512.13645", "categories": ["econ.EM"], "pdf": "https://arxiv.org/pdf/2512.13645", "abs": "https://arxiv.org/abs/2512.13645", "authors": ["Nadav Kunievsky"], "title": "Linear Regression in a Nonlinear World", "comment": null, "summary": "The interpretation of coefficients from multivariate linear regression relies on the assumption that the conditional expectation function is linear in the variables. However, in many cases the underlying data generating process is nonlinear. This paper examines how to interpret regression coefficients under nonlinearity. We show that if the relationships between the variable of interest and other covariates are linear, then the coefficient on the variable of interest represents a weighted average of the derivatives of the outcome conditional expectation function with respect to the variable of interest. If these relationships are nonlinear, the regression coefficient becomes biased relative to this weighted average. We show that this bias is interpretable, analogous to the biases from measurement error and omitted variable bias under the standard linear model."}
{"id": "2512.11863", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11863", "abs": "https://arxiv.org/abs/2512.11863", "authors": ["Julian Schön", "Lena Hoffmann", "Nikolas Becker"], "title": "Expert Assessment: The Systemic Environmental Risks of Artficial Intelligence", "comment": null, "summary": "Artificial intelligence (AI) is often presented as a key tool for addressing societal challenges, such as climate change. At the same time, AI's environmental footprint is expanding increasingly. This report describes the systemic environmental risks of artificial intelligence, in particular, moving beyond direct impacts such as energy and water usage. Systemic environmental risks of AI are emergent, cross-sector harms to climate, biodiversity, freshwater, and broader socioecological systems that arise primarily from AI's integration into social, economic, and physical infrastructures, rather than its direct resource use, and that propagate through feedbacks, yielding nonlinear, inequitable, and potentially irreversible impacts. While these risks are emergent and quantification is uncertain, this report aims to provide an overview of systemic environmental risks. Drawing on a narrative literature review, we propose a three-level framework that operationalizes systemic risk analysis. The framework identifies the structural conditions that shape AI development, the risk amplification mechanisms that propagate environmental harm, and the impacts that manifest as observable ecological and social consequences. We illustrate the framework in expert-interview-based case studies across agriculture and biodiversity, oil and gas, and waste management."}
{"id": "2512.11997", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11997", "abs": "https://arxiv.org/abs/2512.11997", "authors": ["Anfeng Peng", "Ajesh Koyatan Chathoth", "Stephen Lee"], "title": "Log Anomaly Detection with Large Language Models via Knowledge-Enriched Fusion", "comment": null, "summary": "System logs are a critical resource for monitoring and managing distributed systems, providing insights into failures and anomalous behavior. Traditional log analysis techniques, including template-based and sequence-driven approaches, often lose important semantic information or struggle with ambiguous log patterns. To address this, we present EnrichLog, a training-free, entry-based anomaly detection framework that enriches raw log entries with both corpus-specific and sample-specific knowledge. EnrichLog incorporates contextual information, including historical examples and reasoning derived from the corpus, to enable more accurate and interpretable anomaly detection. The framework leverages retrieval-augmented generation to integrate relevant contextual knowledge without requiring retraining. We evaluate EnrichLog on four large-scale system log benchmark datasets and compare it against five baseline methods. Our results show that EnrichLog consistently improves anomaly detection performance, effectively handles ambiguous log entries, and maintains efficient inference. Furthermore, incorporating both corpus- and sample-specific knowledge enhances model confidence and detection accuracy, making EnrichLog well-suited for practical deployments."}
{"id": "2512.11823", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.11823", "abs": "https://arxiv.org/abs/2512.11823", "authors": ["Vicky P. Vital", "Francis F. Balahadia", "Maria Anna D. Cruz", "Dolores D. Mallari", "Juvy C. Grume", "Erika M. Pineda", "Jordan L. Salenga", "Lloyd D. Feliciano", "John Paul P. Miranda"], "title": "Teachers' Perspectives on the Use of AI Detection Tools: Insights from Ridge Regression Analysis", "comment": "8 pages, 2 tables, 2025 International Conference on Distance Education and Learning (ICDEL)", "summary": "This study explores the perceptions of 213 Filipino teachers toward AI detection tools in academic settings. It focuses on the factors that influence teachers' trust, concerns, and decision-making regarding these tools. The research investigates how teachers' trust in AI detection tools affects their perceptions of fairness and decision-making in evaluating student outputs. It also explores how concerns about AI tools and social norms influence the relationship between trust and decision-making. Ridge Regression analysis was used to examine the relationships between the predictors and the dependent variable. The results revealed that trust in AI detection tools is the most significant predictor of perceived fairness and decision-making among teachers. Concerns about AI tools and social norms have weaker effects on teachers' perceptions. The study emphasized critical role of trust in shaping teachers' perceptions of AI detection tools. Teachers who trust these tools are more likely to view them as fair and effective. In contrast, concerns and social norms have a limited influence on perceptions and decision-making. For recommendations, training and institutional guidelines should emphasize how these tools work, their limitations, and best practices for their use. Striking a balance between policy enforcement and educator support is essential for fostering trust in AI detection technologies. Encouraging experienced users to share insights through communities of practice could enhance the adoption and effective use of AI detection tools in educational settings.."}
{"id": "2512.11868", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11868", "abs": "https://arxiv.org/abs/2512.11868", "authors": ["Alexander Windmann", "Benedikt Stratmann", "Mariya Lyashenko", "Oliver Niggemann"], "title": "Industrial AI Robustness Card: Evaluating and Monitoring Time Series Models", "comment": null, "summary": "Industrial AI practitioners face vague robustness requirements in emerging regulations and standards but lack concrete, implementation ready protocols. This paper introduces the Industrial AI Robustness Card (IARC), a lightweight, task agnostic protocol for documenting and evaluating the robustness of AI models on industrial time series. The IARC specifies required fields and an empirical measurement and reporting protocol that combines drift monitoring, uncertainty quantification, and stress tests, and it maps these to relevant EU AI Act obligations. A soft sensor case study on a biopharmaceutical fermentation process illustrates how the IARC supports reproducible robustness evidence and continuous monitoring."}
{"id": "2512.12048", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12048", "abs": "https://arxiv.org/abs/2512.12048", "authors": ["Muddsair Sharif", "Huseyin Seker"], "title": "Context-Aware Agentic Power Resources Optimisation in EV using Smart2ChargeApp", "comment": null, "summary": "This paper presents a novel context-sensitive multi\\-agent coordination for dynamic resource allocation (CAMAC-DRA) framework for optimizing smart electric vehicle (EV) charging ecosystems through the Smart2Charge application. The proposed system coordinates autonomous charging agents across networks of 250 EVs and 45 charging stations while adapting to dynamic environmental conditions through context-aware decision-making. Our multi-agent approach employs coordinated Deep Q\\-Networks integrated with Graph Neural Networks and attention mechanisms, processing 20 contextual features including weather patterns, traffic conditions, grid load fluctuations, and electricity pricing.The framework balances five ecosystem stakeholders i.e. EV users (25\\%), grid operators (20\\%), charging station operators (20\\%), fleet operators (20%), and environmental factors (15\\%) through weighted coordination mechanisms and consensus protocols. Comprehensive validation using real-world datasets containing 441,077 charging transactions demonstrates superior performance compared to baseline algorithms including DDPG, A3C, PPO, and GNN approaches. The CAMAC\\-DRA framework achieves 92\\% coordination success rate, 15\\% energy efficiency improvement, 10\\% cost reduction, 20% grid strain decrease, and \\2.3x faster convergence while maintaining 88\\% training stability and 85\\% sample efficiency. Real-world validation confirms commercial viability with Net Present Cost of -\\$122,962 and 69\\% cost reduction through renewable energy integration. The framework's unique contribution lies in developing context-aware multi-stakeholder coordination that successfully balances competing objectives while adapting to real-time variables, positioning it as a breakthrough solution for intelligent EV charging coordination and sustainable transportation electrification."}
{"id": "2512.11827", "categories": ["cs.CY", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11827", "abs": "https://arxiv.org/abs/2512.11827", "authors": ["Milad Malekzadeh", "Magdalena Biernacka", "Elias Willberg", "Jussi Torkko", "Edyta Łaszkiewicz", "Tuuli Toivonen"], "title": "Assessing Greenspace Attractiveness with ChatGPT, Claude, and Gemini: Do AI Models Reflect Human Perceptions?", "comment": null, "summary": "Understanding greenspace attractiveness is essential for designing livable and inclusive urban environments, yet existing assessment approaches often overlook informal or transient spaces and remain too resource intensive to capture subjective perceptions at scale. This study examines the ability of multimodal large language models (MLLMs), ChatGPT GPT-4o, Claude 3.5 Haiku, and Gemini 2.0 Flash, to assess greenspace attractiveness similarly to humans using Google Street View imagery. We compared model outputs with responses from a geo-questionnaire of residents in Lodz, Poland, across both formal (for example, parks and managed greenspaces) and informal (for example, meadows and wastelands) greenspaces. Survey respondents and models indicated whether each greenspace was attractive or unattractive and provided up to three free text explanations. Analyses examined how often their attractiveness judgments aligned and compared their explanations after classifying them into shared reasoning categories. Results show high AI human agreement for attractive formal greenspaces and unattractive informal spaces, but low alignment for attractive informal and unattractive formal greenspaces. Models consistently emphasized aesthetic and design oriented features, underrepresenting safety, functional infrastructure, and locally embedded qualities valued by survey respondents. While these findings highlight the potential for scalable pre-assessment, they also underscore the need for human oversight and complementary participatory approaches. We conclude that MLLMs can support, but not replace, context sensitive greenspace evaluation in planning practice."}
{"id": "2512.11870", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11870", "abs": "https://arxiv.org/abs/2512.11870", "authors": ["Mulham Fawkherji", "Bruce Race", "Driss Benhaddou"], "title": "Using Socio-economic Indicators, Smart Transit Systems, and Urban Simulator to Accelerate ZEV Adoption and Reduce VMT", "comment": null, "summary": "Globally, on-road transportation accounts for 15% of greenhouse gas (GHG) emissions and an estimated 385,000 premature deaths from PM2.5. Cities play a critical role in meeting IPCC targets, generating 75% of global energy-related GHG emissions. In Houston, Texas, on-road transportation represents 48% of baseline emissions in the Climate Action Plan (CAP). To reach net-zero by 2050, the CAP targets a 70% emissions reduction from a 2014 baseline, offset by 30% renewable energy. This goal is challenging because Houston is low-density and auto-dependent, with 89% of on-road emissions from cars and small trucks and limited public transit usage. Socio-economic disparities further constrain Zero Emissions Vehicle (ZEV) adoption. Strategies focus on expanding ZEV access and reducing Vehicle Miles Traveled (VMT) by 20% through transit improvements and city design. This paper presents methods for establishing an on-road emissions baseline and evaluating policies that leverage socio-economic indicators and Intelligent Transportation Systems (ITS) to accelerate ZEV adoption and reduce VMT. Smart parking, transit incentives, secure data systems, and ZEV fleet management support improvements in modal split and system reliability. Policy options are analyzed and potential actions identified. To support evaluation, a simulation environment was developed in Unity 3D, enabling dynamic modeling of urban mobility and visualization of policy scenarios. Auto-dependent cities aiming for 2050 emission targets can benefit from the indicators, metrics, and technologies discussed."}
{"id": "2512.12059", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12059", "abs": "https://arxiv.org/abs/2512.12059", "authors": ["Luke Bhan", "Hanyu Zhang", "Andrew Gordon Wilson", "Michael W. Mahoney", "Chuck Arvin"], "title": "The Forecast Critic: Leveraging Large Language Models for Poor Forecast Identification", "comment": "Presented at AAAI 2026 AI4TS workshop and AABA4ET workshop", "summary": "Monitoring forecasting systems is critical for customer satisfaction, profitability, and operational efficiency in large-scale retail businesses. We propose The Forecast Critic, a system that leverages Large Language Models (LLMs) for automated forecast monitoring, taking advantage of their broad world knowledge and strong ``reasoning'' capabilities. As a prerequisite for this, we systematically evaluate the ability of LLMs to assess time series forecast quality, focusing on three key questions. (1) Can LLMs be deployed to perform forecast monitoring and identify obviously unreasonable forecasts? (2) Can LLMs effectively incorporate unstructured exogenous features to assess what a reasonable forecast looks like? (3) How does performance vary across model sizes and reasoning capabilities, measured across state-of-the-art LLMs? We present three experiments, including on both synthetic and real-world forecasting data. Our results show that LLMs can reliably detect and critique poor forecasts, such as those plagued by temporal misalignment, trend inconsistencies, and spike errors. The best-performing model we evaluated achieves an F1 score of 0.88, somewhat below human-level performance (F1 score: 0.97). We also demonstrate that multi-modal LLMs can effectively incorporate unstructured contextual signals to refine their assessment of the forecast. Models correctly identify missing or spurious promotional spikes when provided with historical context about past promotions (F1 score: 0.84). Lastly, we demonstrate that these techniques succeed in identifying inaccurate forecasts on the real-world M5 time series dataset, with unreasonable forecasts having an sCRPS at least 10% higher than that of reasonable forecasts. These findings suggest that LLMs, even without domain-specific fine-tuning, may provide a viable and scalable option for automated forecast monitoring and evaluation."}
{"id": "2512.11835", "categories": ["cs.AI", "cs.LG", "cs.MA", "cs.NE"], "pdf": "https://arxiv.org/pdf/2512.11835", "abs": "https://arxiv.org/abs/2512.11835", "authors": ["Seyma Yaman Kayadibi"], "title": "A Monad-Based Clause Architecture for Artificial Age Score (AAS) in Large Language Models", "comment": "42 pages, 6 toy simulation Python implementations, 20 monad clauses instantiated across six system bundles (ontology, dynamics, representation and consciousness, harmony and reason, body and organisation, teleology)", "summary": "Large language models (LLMs) are often deployed as powerful yet opaque systems, leaving open how their internal memory and \"self-like\" behavior should be governed in a principled and auditable way. The Artificial Age Score (AAS) was previously introduced and mathematically justified through three theorems that characterise it as a metric of artificial memory aging. Building on this foundation, the present work develops an engineering-oriented, clause-based architecture that imposes law-like constraints on LLM memory and control. Twenty selected monads from Leibniz's Monadology are grouped into six bundles: ontology, dynamics, representation and consciousness, harmony and reason, body and organisation, and teleology, and each bundle is realised as an executable specification on top of the AAS kernel. Across six minimal Python implementations, these clause families are instantiated in numerical experiments acting on channel-level quantities such as recall scores, redundancy, and weights. Each implementation follows a four-step pattern: inputs and setup, clause implementation, numerical results, and implications for LLM design, emphasising that the framework is not only philosophically motivated but also directly implementable. The experiments show that the clause system exhibits bounded and interpretable behavior: AAS trajectories remain continuous and rate-limited, contradictions and unsupported claims trigger explicit penalties, and hierarchical refinement reveals an organic structure in a controlled manner. Dual views and goal-action pairs are aligned by harmony terms, and windowed drift in perfection scores separates sustained improvement from sustained degradation. Overall, the monad-based clause framework uses AAS as a backbone and provides a transparent, code-level blueprint for constraining and analyzing internal dynamics in artificial agents."}
{"id": "2512.11875", "categories": ["cs.CY", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.11875", "abs": "https://arxiv.org/abs/2512.11875", "authors": ["Ting Luo", "Yan Wang"], "title": "The Art of Storytelling in Authoritarian Regimes: Crafting State Narratives on Chinese Social Media", "comment": null, "summary": "This article examines how authoritarian regimes construct state narratives about politically consequential events. Building on the narrative policy framework and existing research on authoritarian propaganda, we propose two dimensions that shape narrative construction: legitimacy implications -- whether events enhance or threaten regime legitimacy, and citizen verification capacity -- the extent to which citizens can evaluate official narratives through alternative sources. Using quantitative narrative analysis of Chinese social media posts by government, state media, and celebrity accounts, we extract subject-verb-object (SVO) triplets to map dominant narrative structures across four major events. Our findings show that legitimacy implications of the event shape regime's efforts in storytelling and the beliefs highlighted in the narratives, while citizen's verification capacity could balance the strategic choice between a top-down manipulation and bottom-up responsiveness of state narratives. Together, the results reveal propaganda as a complex process of narrative construction adaptive to specific contexts, offering new insights into how dynamic storytelling sustains authoritarian resilience."}
{"id": "2512.12088", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12088", "abs": "https://arxiv.org/abs/2512.12088", "authors": ["S. R. Eshwar", "Aniruddha Mukherjee", "Kintan Saha", "Krishna Agarwal", "Gugan Thoppe", "Aditya Gopalan", "Gal Dalal"], "title": "Reliable Policy Iteration: Performance Robustness Across Architecture and Environment Perturbations", "comment": null, "summary": "In a recent work, we proposed Reliable Policy Iteration (RPI), that restores policy iteration's monotonicity-of-value-estimates property to the function approximation setting. Here, we assess the robustness of RPI's empirical performance on two classical control tasks -- CartPole and Inverted Pendulum -- under changes to neural network and environmental parameters. Relative to DQN, Double DQN, DDPG, TD3, and PPO, RPI reaches near-optimal performance early and sustains this policy as training proceeds. Because deep RL methods are often hampered by sample inefficiency, training instability, and hyperparameter sensitivity, our results highlight RPI's promise as a more reliable alternative."}
{"id": "2512.11850", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.11850", "abs": "https://arxiv.org/abs/2512.11850", "authors": ["Davide Mancino"], "title": "The Memecoin Phenomenon: An In-Depth Study of Solana's Blockchain Trends", "comment": null, "summary": "This paper analyzes the emerging memecoin phenomenon on the Solana blockchain, focusing on the Pump.fun platform during Q4 2024. Using on-chain data, it is explored how retail-focused token creation platforms are reshaping blockchain ecosystems and influencing market participation. This study finds that Pump.fun accounted for up to 71.1% of all tokens minted on Solana and contributed 40-67.4% of total DEX transactions. Despite this activity, fewer than 2% of tokens successfully transitioned to major decentralized exchanges, highlighting a highly speculative market structure. The platform experienced rapid growth, with daily active users rising from 60,000 to peaks of 260,000, underscoring strong retail adoption. This reflects a broader shift towards accessible, socially-driven market participation enabled by memecoins. However, while memecoins lower entry barriers and encourage retail engagement, they introduce significant risks. The volatile and speculative nature of these platforms raises concerns about long-term sustainability and the resilience of the blockchain ecosystem. These findings reveal the dual impact of memecoins: they democratize token creation and alter market dynamics but may jeopardize market efficiency and stability. This paper highlights the need to critically assess the implications of retail-driven speculative trading and its potential to disrupt emerging blockchain economies."}
{"id": "2512.11878", "categories": ["cs.CY", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.11878", "abs": "https://arxiv.org/abs/2512.11878", "authors": ["Hasan Kassem", "Sergen Cansiz", "Brandon Edwards", "Patrick Foley", "Inken Hagestedt", "Taeho Jung", "Prakash Moorthy", "Michael O'Connor", "Bruno Rodrigues", "Holger Roth", "Micah Sheller", "Dimitris Stripelis", "Marc Vesin", "Renato Umeton", "Mic Bowman", "Alexandros Karargyris"], "title": "A Technical Policy Blueprint for Trustworthy Decentralized AI", "comment": null, "summary": "Decentralized AI systems, such as federated learning, can play a critical role in further unlocking AI asset marketplaces (e.g., healthcare data marketplaces) thanks to increased asset privacy protection. Unlocking this big potential necessitates governance mechanisms that are transparent, scalable, and verifiable. However current governance approaches rely on bespoke, infrastructure-specific policies that hinder asset interoperability and trust among systems. We are proposing a Technical Policy Blueprint that encodes governance requirements as policy-as-code objects and separates asset policy verification from asset policy enforcement. In this architecture the Policy Engine verifies evidence (e.g., identities, signatures, payments, trusted-hardware attestations) and issues capability packages. Asset Guardians (e.g. data guardians, model guardians, computation guardians, etc.) enforce access or execution solely based on these capability packages. This core concept of decoupling policy processing from capabilities enables governance to evolve without reconfiguring AI infrastructure, thus creating an approach that is transparent, auditable, and resilient to change."}
{"id": "2512.12175", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12175", "abs": "https://arxiv.org/abs/2512.12175", "authors": ["Haoyang Chen", "Richong Zhang", "Junfan Chen"], "title": "Rethinking Label Consistency of In-Context Learning: An Implicit Transductive Label Propagation Perspective", "comment": null, "summary": "Large language models (LLMs) perform in-context learning (ICL) with minimal supervised examples, which benefits various natural language processing (NLP) tasks. One of the critical research focus is the selection of prompt demonstrations. Current approaches typically employ retrieval models to select the top-K most semantically similar examples as demonstrations. However, we argue that existing methods are limited since the label consistency is not guaranteed during demonstration selection. Our cognition derives from the Bayesian view of ICL and our rethinking of ICL from the transductive label propagation perspective. We treat ICL as a transductive learning method and incorporate latent concepts from Bayesian view and deduce that similar demonstrations guide the concepts of query, with consistent labels serving as estimates. Based on this understanding, we establish a label propagation framework to link label consistency with propagation error bounds. To model label consistency, we propose a data synthesis method, leveraging both semantic and label information, and use TopK sampling with Synthetic Data (TopK-SD) to acquire demonstrations with consistent labels. TopK-SD outperforms original TopK sampling on multiple benchmarks. Our work provides a new perspective for understanding the working mechanisms within ICL."}
{"id": "2512.11863", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11863", "abs": "https://arxiv.org/abs/2512.11863", "authors": ["Julian Schön", "Lena Hoffmann", "Nikolas Becker"], "title": "Expert Assessment: The Systemic Environmental Risks of Artficial Intelligence", "comment": null, "summary": "Artificial intelligence (AI) is often presented as a key tool for addressing societal challenges, such as climate change. At the same time, AI's environmental footprint is expanding increasingly. This report describes the systemic environmental risks of artificial intelligence, in particular, moving beyond direct impacts such as energy and water usage. Systemic environmental risks of AI are emergent, cross-sector harms to climate, biodiversity, freshwater, and broader socioecological systems that arise primarily from AI's integration into social, economic, and physical infrastructures, rather than its direct resource use, and that propagate through feedbacks, yielding nonlinear, inequitable, and potentially irreversible impacts. While these risks are emergent and quantification is uncertain, this report aims to provide an overview of systemic environmental risks. Drawing on a narrative literature review, we propose a three-level framework that operationalizes systemic risk analysis. The framework identifies the structural conditions that shape AI development, the risk amplification mechanisms that propagate environmental harm, and the impacts that manifest as observable ecological and social consequences. We illustrate the framework in expert-interview-based case studies across agriculture and biodiversity, oil and gas, and waste management."}
{"id": "2512.11879", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11879", "abs": "https://arxiv.org/abs/2512.11879", "authors": ["Beatriz Costa-Gomes", "Sophia Chen", "Connie Hsueh", "Deborah Morgan", "Philipp Schoenegger", "Yash Shah", "Sam Way", "Yuki Zhu", "Timothé Adeline", "Michael Bhaskar", "Mustafa Suleyman", "Seth Spielman"], "title": "It's About Time: The Temporal and Modal Dynamics of Copilot Usage", "comment": "12 pages, 10 figures", "summary": "We analyze 37.5 million deidentified conversations with Microsoft's Copilot between January and September 2025. Unlike prior analyses of AI usage, we focus not just on what people do with AI, but on how and when they do it. We find that how people use AI depends fundamentally on context and device type. On mobile, health is the dominant topic, which is consistent across every hour and every month we observed - with users seeking not just information but also advice. On desktop, the pattern is strikingly different: work and technology dominate during business hours, with \"Work and Career\" overtaking \"Technology\" as the top topic precisely between 8 a.m. and 5 p.m. These differences extend to temporal rhythms: programming queries spike on weekdays while gaming rises on weekends, philosophical questions climb during late-night hours, and relationship conversations surge on Valentine's Day. These patterns suggest that users have rapidly integrated AI into the full texture of their lives, as a work aid at their desks and a companion on their phones."}
{"id": "2512.12177", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12177", "abs": "https://arxiv.org/abs/2512.12177", "authors": ["Aydin Ayanzadeh", "Tim Oates"], "title": "Floorplan2Guide: LLM-Guided Floorplan Parsing for BLV Indoor Navigation", "comment": "Accepted for publication in the proceedings of the IEEE International Conference on Big Data (IEEE BigData 2025)", "summary": "Indoor navigation remains a critical challenge for people with visual impairments. The current solutions mainly rely on infrastructure-based systems, which limit their ability to navigate safely in dynamic environments. We propose a novel navigation approach that utilizes a foundation model to transform floor plans into navigable knowledge graphs and generate human-readable navigation instructions. Floorplan2Guide integrates a large language model (LLM) to extract spatial information from architectural layouts, reducing the manual preprocessing required by earlier floorplan parsing methods. Experimental results indicate that few-shot learning improves navigation accuracy in comparison to zero-shot learning on simulated and real-world evaluations. Claude 3.7 Sonnet achieves the highest accuracy among the evaluated models, with 92.31%, 76.92%, and 61.54% on the short, medium, and long routes, respectively, under 5-shot prompting of the MP-1 floor plan. The success rate of graph-based spatial structure is 15.4% higher than that of direct visual reasoning among all models, which confirms that graphical representation and in-context learning enhance navigation performance and make our solution more precise for indoor navigation of Blind and Low Vision (BLV) users."}
{"id": "2512.11864", "categories": ["cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.11864", "abs": "https://arxiv.org/abs/2512.11864", "authors": ["Christoph Einspieler", "Matthias Horn", "Marie-Louise Lackner", "Patrick Malik", "Nysret Musliu", "Felix Winter"], "title": "Solving Parallel Machine Scheduling With Precedences and Cumulative Resource Constraints With Calendars", "comment": "18 pages, 4 figures", "summary": "The task of finding efficient production schedules for parallel machines is a challenge that arises in most industrial manufacturing domains. There is a large potential to minimize production costs through automated scheduling techniques, due to the large-scale requirements of modern factories. In the past, solution approaches have been studied for many machine scheduling variations, where even basic variants have been shown to be NP-hard. However, in today's real-life production environments, additional complex precedence constraints and resource restrictions with calendars arise that must be fulfilled. These additional constraints cannot be tackled efficiently by existing solution techniques. Thus, there is a strong need to develop and analyze automated methods that can solve such real-life parallel machine scheduling scenarios. In this work, we introduce a novel variant of parallel machine scheduling with job precedences and calendar-based cumulative resource constraints that arises in real-life industrial use cases. A constraint modeling approach is proposed as an exact solution method for small scheduling scenarios together with state-of-the-art constraint-solving technology. Further, we propose a construction heuristic as well as a tailored metaheuristic using local search to efficiently tackle large-scale problem instances. This metaheuristic approach has been deployed and is currently being used in an industrial setting."}
{"id": "2512.11882", "categories": ["cs.CY", "cs.AI", "cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.11882", "abs": "https://arxiv.org/abs/2512.11882", "authors": ["Lucia Happe", "Dominik Fuchß", "Luca Hüttner", "Kai Marquardt", "Anne Koziolek"], "title": "An Experience Report on a Pedagogically Controlled, Curriculum-Constrained AI Tutor for SE Education", "comment": "11 pages, 4 figures, accepted for publication at ICSE 2026 SEET Track", "summary": "The integration of artificial intelligence (AI) into education continues to evoke both promise and skepticism. While past waves of technological optimism often fell short, recent advances in large language models (LLMs) have revived the vision of scalable, individualized tutoring. This paper presents the design and pilot evaluation of RockStartIT Tutor, an AI-powered assistant developed for a digital programming and computational thinking course within the RockStartIT initiative. Powered by GPT-4 via OpenAI's Assistant API, the tutor employs a novel prompting strategy and a modular, semantically tagged knowledge base to deliver context-aware, personalized, and curriculum-constrained support for secondary school students. We evaluated the system using the Technology Acceptance Model (TAM) with 13 students and teachers. Learners appreciated the low-stakes environment for asking questions and receiving scaffolded guidance. Educators emphasized the system's potential to reduce cognitive load during independent tasks and complement classroom teaching. Key challenges include prototype limitations, a small sample size, and the need for long-term studies with the target age group. Our findings highlight a pragmatic approach to AI integration that requires no model training, using structure and prompts to shape behavior. We position AI tutors not as teacher replacements but as enabling tools that extend feedback access, foster inquiry, and support what schools do best: help students learn."}
{"id": "2512.12182", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12182", "abs": "https://arxiv.org/abs/2512.12182", "authors": ["Xinyu Gao"], "title": "TA-KAND: Two-stage Attention Triple Enhancement and U-KAN based Diffusion For Few-shot Knowledge Graph Completion", "comment": null, "summary": "Knowledge Graphs (KGs), thanks to their concise and efficient triple-based structure, have been widely applied in intelligent question answering, recommender systems and other domains. However, the heterogeneous and multifaceted nature of real-world data inevitably renders the distribution of relations long-tailed, making it crucial to complete missing facts with limited samples. Previous studies mainly based on metric matching or meta learning, yet they either fail to fully exploit neighborhood information in graph or overlook the distributional characteristics of contrastive signals. In this paper, we re-examine the problem from a perspective of generative representation and propose a few-shot knowledge graph completion framework that integrates two-stage attention triple enhancer with U-KAN based diffusion model. Extensive experiments on two public datasets show that our method achieve new state-of-the-art results."}
{"id": "2512.11868", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11868", "abs": "https://arxiv.org/abs/2512.11868", "authors": ["Alexander Windmann", "Benedikt Stratmann", "Mariya Lyashenko", "Oliver Niggemann"], "title": "Industrial AI Robustness Card: Evaluating and Monitoring Time Series Models", "comment": null, "summary": "Industrial AI practitioners face vague robustness requirements in emerging regulations and standards but lack concrete, implementation ready protocols. This paper introduces the Industrial AI Robustness Card (IARC), a lightweight, task agnostic protocol for documenting and evaluating the robustness of AI models on industrial time series. The IARC specifies required fields and an empirical measurement and reporting protocol that combines drift monitoring, uncertainty quantification, and stress tests, and it maps these to relevant EU AI Act obligations. A soft sensor case study on a biopharmaceutical fermentation process illustrates how the IARC supports reproducible robustness evidence and continuous monitoring."}
{"id": "2512.11883", "categories": ["cs.CY", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11883", "abs": "https://arxiv.org/abs/2512.11883", "authors": ["Wenqi Marshall Guo", "Qingyun Qian", "Khalad Hasan", "Shan Du"], "title": "Aesthetic Alignment Risks Assimilation: How Image Generation and Reward Models Reinforce Beauty Bias and Ideological \"Censorship\"", "comment": null, "summary": "Over-aligning image generation models to a generalized aesthetic preference conflicts with user intent, particularly when ``anti-aesthetic\" outputs are requested for artistic or critical purposes. This adherence prioritizes developer-centered values, compromising user autonomy and aesthetic pluralism. We test this bias by constructing a wide-spectrum aesthetics dataset and evaluating state-of-the-art generation and reward models. We find that aesthetic-aligned generation models frequently default to conventionally beautiful outputs, failing to respect instructions for low-quality or negative imagery. Crucially, reward models penalize anti-aesthetic images even when they perfectly match the explicit user prompt. We confirm this systemic bias through image-to-image editing and evaluation against real abstract artworks."}
{"id": "2512.12225", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12225", "abs": "https://arxiv.org/abs/2512.12225", "authors": ["Laha Ale"], "title": "A Geometric Theory of Cognition", "comment": null, "summary": "Human cognition spans perception, memory, intuitive judgment, deliberative reasoning, action selection, and social inference, yet these capacities are often explained through distinct computational theories. Here we present a unified mathematical framework in which diverse cognitive processes emerge from a single geometric principle. We represent the cognitive state as a point on a differentiable manifold endowed with a learned Riemannian metric that encodes representational constraints, computational costs, and structural relations among cognitive variables. A scalar cognitive potential combines predictive accuracy, structural parsimony, task utility, and normative or logical requirements. Cognition unfolds as the Riemannian gradient flow of this potential, providing a universal dynamical law from which a broad range of psychological phenomena arise. Classical dual-process effects--rapid intuitive responses and slower deliberative reasoning--emerge naturally from metric-induced anisotropies that generate intrinsic time-scale separations and geometric phase transitions, without invoking modular or hybrid architectures. We derive analytical conditions for these regimes and demonstrate their behavioural signatures through simulations of canonical cognitive tasks. Together, these results establish a geometric foundation for cognition and suggest guiding principles for the development of more general and human-like artificial intelligence systems."}
{"id": "2512.11870", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11870", "abs": "https://arxiv.org/abs/2512.11870", "authors": ["Mulham Fawkherji", "Bruce Race", "Driss Benhaddou"], "title": "Using Socio-economic Indicators, Smart Transit Systems, and Urban Simulator to Accelerate ZEV Adoption and Reduce VMT", "comment": null, "summary": "Globally, on-road transportation accounts for 15% of greenhouse gas (GHG) emissions and an estimated 385,000 premature deaths from PM2.5. Cities play a critical role in meeting IPCC targets, generating 75% of global energy-related GHG emissions. In Houston, Texas, on-road transportation represents 48% of baseline emissions in the Climate Action Plan (CAP). To reach net-zero by 2050, the CAP targets a 70% emissions reduction from a 2014 baseline, offset by 30% renewable energy. This goal is challenging because Houston is low-density and auto-dependent, with 89% of on-road emissions from cars and small trucks and limited public transit usage. Socio-economic disparities further constrain Zero Emissions Vehicle (ZEV) adoption. Strategies focus on expanding ZEV access and reducing Vehicle Miles Traveled (VMT) by 20% through transit improvements and city design. This paper presents methods for establishing an on-road emissions baseline and evaluating policies that leverage socio-economic indicators and Intelligent Transportation Systems (ITS) to accelerate ZEV adoption and reduce VMT. Smart parking, transit incentives, secure data systems, and ZEV fleet management support improvements in modal split and system reliability. Policy options are analyzed and potential actions identified. To support evaluation, a simulation environment was developed in Unity 3D, enabling dynamic modeling of urban mobility and visualization of policy scenarios. Auto-dependent cities aiming for 2050 emission targets can benefit from the indicators, metrics, and technologies discussed."}
{"id": "2512.11887", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11887", "abs": "https://arxiv.org/abs/2512.11887", "authors": ["Yihan Liao", "Jingyu Zhang", "Jacky Keung", "Yan Xiao", "Yurou Dai"], "title": "Advancing Autonomous Driving System Testing: Demands, Challenges, and Future Directions", "comment": "Accepted for publication in Information and Software Technology (IST)", "summary": "Autonomous driving systems (ADSs) promise improved transportation efficiency and safety, yet ensuring their reliability in complex real-world environments remains a critical challenge. Effective testing is essential to validate ADS performance and reduce deployment risks. This study investigates current ADS testing practices for both modular and end-to-end systems, identifies key demands from industry practitioners and academic researchers, and analyzes the gaps between existing research and real-world requirements. We review major testing techniques and further consider emerging factors such as Vehicle-to-Everything (V2X) communication and foundation models, including large language models and vision foundation models, to understand their roles in enhancing ADS testing. We conducted a large-scale survey with 100 participants from both industry and academia. Survey questions were refined through expert discussions, followed by quantitative and qualitative analyses to reveal key trends, challenges, and unmet needs. Our results show that existing ADS testing techniques struggle to comprehensively evaluate real-world performance, particularly regarding corner case diversity, the simulation to reality gap, the lack of systematic testing criteria, exposure to potential attacks, practical challenges in V2X deployment, and the high computational cost of foundation model-based testing. By further analyzing participant responses together with 105 representative studies, we summarize the current research landscape and highlight major limitations. This study consolidates critical research gaps in ADS testing and outlines key future research directions, including comprehensive testing criteria, cross-model collaboration in V2X systems, cross-modality adaptation for foundation model-based testing, and scalable validation frameworks for large-scale ADS evaluation."}
{"id": "2512.12260", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2512.12260", "abs": "https://arxiv.org/abs/2512.12260", "authors": ["Ege Atacan Doğan", "Peter F. Patel-Schneider"], "title": "A Multi-Axial Mindset for Ontology Design Lessons from Wikidata's Polyhierarchical Structure", "comment": null, "summary": "Traditional ontology design emphasizes disjoint and exhaustive top-level distinctions such as continuant vs. occurrent, abstract vs. concrete, or type vs. instance. These distinctions are used to structure unified hierarchies where every entity is classified under a single upper-level category. Wikidata, by contrast, does not enforce a singular foundational taxonomy. Instead, it accommodates multiple classification axes simultaneously under the shared root class entity. This paper analyzes the structural implications of Wikidata's polyhierarchical and multi-axial design. The Wikidata architecture enables a scalable and modular approach to ontology construction, especially suited to collaborative and evolving knowledge graphs."}
{"id": "2512.11875", "categories": ["cs.CY", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.11875", "abs": "https://arxiv.org/abs/2512.11875", "authors": ["Ting Luo", "Yan Wang"], "title": "The Art of Storytelling in Authoritarian Regimes: Crafting State Narratives on Chinese Social Media", "comment": null, "summary": "This article examines how authoritarian regimes construct state narratives about politically consequential events. Building on the narrative policy framework and existing research on authoritarian propaganda, we propose two dimensions that shape narrative construction: legitimacy implications -- whether events enhance or threaten regime legitimacy, and citizen verification capacity -- the extent to which citizens can evaluate official narratives through alternative sources. Using quantitative narrative analysis of Chinese social media posts by government, state media, and celebrity accounts, we extract subject-verb-object (SVO) triplets to map dominant narrative structures across four major events. Our findings show that legitimacy implications of the event shape regime's efforts in storytelling and the beliefs highlighted in the narratives, while citizen's verification capacity could balance the strategic choice between a top-down manipulation and bottom-up responsiveness of state narratives. Together, the results reveal propaganda as a complex process of narrative construction adaptive to specific contexts, offering new insights into how dynamic storytelling sustains authoritarian resilience."}
{"id": "2512.11890", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.11890", "abs": "https://arxiv.org/abs/2512.11890", "authors": ["Tariq Eldakruri", "Edip Senyurek"], "title": "Automation as a Catalyst for Geothermal Energy Adoption in Qatar: A Techno-Economic and Environmental Assessment", "comment": "13 pages, 1 table. Published version deposited with publisher permission", "summary": "Geothermal energy provides continuous low emission potential but is underused in Qatar because of high capital costs, drilling risks, and uncertainty in subsurface conditions. This study examines how automation can improve the techno economic and environmental feasibility of geothermal deployment through three pathways: Enhanced Geothermal Systems in the Dukhan Basin, repurposed oil and gas wells, and ground source heat pumps for district cooling. Using geological datasets and financial modeling, the analysis shows that full automation reduces capital expenditure by 12 to 14 percent and operating expenditure by 14 to 17 percent. The Levelized Cost of Energy decreases from 145 USD per MWh to 125 USD per MWh, and payback periods shorten by up to two years. Environmental results indicate that geothermal substitution can avoid between 4000 and 17600 tons of CO2 per year for each project. Automation also reduces uncertainty in investment outcomes based on Monte Carlo simulations. Overall, the results show that automation strengthens the economic viability of geothermal systems and supports their integration into Qatars long term energy diversification and decarbonization strategies."}
{"id": "2512.12288", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12288", "abs": "https://arxiv.org/abs/2512.12288", "authors": ["Mahule Roy", "Guillaume Lambard"], "title": "Quantum-Aware Generative AI for Materials Discovery: A Framework for Robust Exploration Beyond DFT Biases", "comment": "33 pages", "summary": "Conventional generative models for materials discovery are predominantly trained and validated using data from Density Functional Theory (DFT) with approximate exchange-correlation functionals. This creates a fundamental bottleneck: these models inherit DFT's systematic failures for strongly correlated systems, leading to exploration biases and an inability to discover materials where DFT predictions are qualitatively incorrect. We introduce a quantum-aware generative AI framework that systematically addresses this limitation through tight integration of multi-fidelity learning and active validation. Our approach employs a diffusion-based generator conditioned on quantum-mechanical descriptors and a validator using an equivariant neural network potential trained on a hierarchical dataset spanning multiple levels of theory (PBE, SCAN, HSE06, CCSD(T)). Crucially, we implement a robust active learning loop that quantifies and targets the divergence between low- and high-fidelity predictions. We conduct comprehensive ablation studies to deconstruct the contribution of each component, perform detailed failure mode analysis, and benchmark our framework against state-of-the-art generative models (CDVAE, GNoME, DiffCSP) across several challenging material classes. Our results demonstrate significant practical gains: a 3-5x improvement in successfully identifying potentially stable candidates in high-divergence regions (e.g., correlated oxides) compared to DFT-only baselines, while maintaining computational feasibility. This work provides a rigorous, transparent framework for extending the effective search space of computational materials discovery beyond the limitations of single-fidelity models."}
{"id": "2512.11878", "categories": ["cs.CY", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.11878", "abs": "https://arxiv.org/abs/2512.11878", "authors": ["Hasan Kassem", "Sergen Cansiz", "Brandon Edwards", "Patrick Foley", "Inken Hagestedt", "Taeho Jung", "Prakash Moorthy", "Michael O'Connor", "Bruno Rodrigues", "Holger Roth", "Micah Sheller", "Dimitris Stripelis", "Marc Vesin", "Renato Umeton", "Mic Bowman", "Alexandros Karargyris"], "title": "A Technical Policy Blueprint for Trustworthy Decentralized AI", "comment": null, "summary": "Decentralized AI systems, such as federated learning, can play a critical role in further unlocking AI asset marketplaces (e.g., healthcare data marketplaces) thanks to increased asset privacy protection. Unlocking this big potential necessitates governance mechanisms that are transparent, scalable, and verifiable. However current governance approaches rely on bespoke, infrastructure-specific policies that hinder asset interoperability and trust among systems. We are proposing a Technical Policy Blueprint that encodes governance requirements as policy-as-code objects and separates asset policy verification from asset policy enforcement. In this architecture the Policy Engine verifies evidence (e.g., identities, signatures, payments, trusted-hardware attestations) and issues capability packages. Asset Guardians (e.g. data guardians, model guardians, computation guardians, etc.) enforce access or execution solely based on these capability packages. This core concept of decoupling policy processing from capabilities enables governance to evolve without reconfiguring AI infrastructure, thus creating an approach that is transparent, auditable, and resilient to change."}
{"id": "2512.11892", "categories": ["cs.CY", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.11892", "abs": "https://arxiv.org/abs/2512.11892", "authors": ["Jon Crowcroft", "Rute C. Sofia", "Dirk Trossen", "Vassilis Tsaoussidis"], "title": "Should AI Become an Intergenerational Civil Right?", "comment": null, "summary": "Artificial Intelligence (AI) is rapidly becoming a foundational layer of social, economic, and cognitive infrastructure. At the same time, the training and large-scale deployment of AI systems rely on finite and unevenly distributed energy, networking, and computational resources. This tension exposes a largely unexamined problem in current AI governance: while expanding access to AI is essential for social inclusion and equal opportunity, unconstrained growth in AI use risks unsustainable resource consumption, whereas restricting access threatens to entrench inequality and undermine basic rights.\n  This paper argues that access to AI outputs largely derived from publicly produced knowledge should not be treated solely as a commercial service, but as a fundamental civil interest requiring explicit protection. We show that existing regulatory frameworks largely ignore the coupling between equitable access and resource constraints, leaving critical questions of fairness, sustainability, and long-term societal impact unresolved. To address this gap, we propose recognizing access to AI as an \\emph{Intergenerational Civil Right}, establishing a legal and ethical framework that simultaneously safeguards present-day inclusion and the rights of future generations.\n  Beyond normative analysis, we explore how this principle can be technically realized. Drawing on emerging paradigms in IoT--Edge--Cloud computing, decentralized inference, and energy-aware networking, we outline technological trajectories and a strawman architecture for AI Delivery Networks that support equitable access under strict resource constraints. By framing AI as a shared social infrastructure rather than a discretionary market commodity, this work connects governance principles with concrete system design choices, offering a pathway toward AI deployment that is both socially just and environmentally sustainable."}
{"id": "2512.12381", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12381", "abs": "https://arxiv.org/abs/2512.12381", "authors": ["Truong Xuan Khanh", "Truong Quynh Hoa"], "title": "Entropy Collapse: A Universal Failure Mode of Intelligent Systems", "comment": "18 pages, 5 figures", "summary": "Intelligent systems are widely assumed to improve through learning, coordination, and optimization. However, across domains -- from artificial intelligence to economic institutions and biological evolution -- increasing intelligence often precipitates paradoxical degradation: systems become rigid, lose adaptability, and fail unexpectedly.\n  We identify \\emph{entropy collapse} as a universal dynamical failure mode arising when feedback amplification outpaces bounded novelty regeneration. Under minimal domain-agnostic assumptions, we show that intelligent systems undergo a sharp transition from high-entropy adaptive regimes to low-entropy collapsed regimes. Collapse is formalized as convergence toward a stable low-entropy manifold, not a zero-entropy state, implying a contraction of effective adaptive dimensionality rather than loss of activity or scale.\n  We analytically establish critical thresholds, dynamical irreversibility, and attractor structure and demonstrate universality across update mechanisms through minimal simulations. This framework unifies diverse phenomena -- model collapse in AI, institutional sclerosis in economics, and genetic bottlenecks in evolution -- as manifestations of the same underlying process.\n  By reframing collapse as a structural cost of intelligence, our results clarify why late-stage interventions systematically fail and motivate entropy-aware design principles for sustaining long-term adaptability in intelligent systems.\n  \\noindent\\textbf{Keywords:} entropy collapse; intelligent systems; feedback amplification; phase transitions; effective dimensionality; complex systems; model collapse; institutional sclerosis"}
{"id": "2512.11879", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11879", "abs": "https://arxiv.org/abs/2512.11879", "authors": ["Beatriz Costa-Gomes", "Sophia Chen", "Connie Hsueh", "Deborah Morgan", "Philipp Schoenegger", "Yash Shah", "Sam Way", "Yuki Zhu", "Timothé Adeline", "Michael Bhaskar", "Mustafa Suleyman", "Seth Spielman"], "title": "It's About Time: The Temporal and Modal Dynamics of Copilot Usage", "comment": "12 pages, 10 figures", "summary": "We analyze 37.5 million deidentified conversations with Microsoft's Copilot between January and September 2025. Unlike prior analyses of AI usage, we focus not just on what people do with AI, but on how and when they do it. We find that how people use AI depends fundamentally on context and device type. On mobile, health is the dominant topic, which is consistent across every hour and every month we observed - with users seeking not just information but also advice. On desktop, the pattern is strikingly different: work and technology dominate during business hours, with \"Work and Career\" overtaking \"Technology\" as the top topic precisely between 8 a.m. and 5 p.m. These differences extend to temporal rhythms: programming queries spike on weekdays while gaming rises on weekends, philosophical questions climb during late-night hours, and relationship conversations surge on Valentine's Day. These patterns suggest that users have rapidly integrated AI into the full texture of their lives, as a work aid at their desks and a companion on their phones."}
{"id": "2512.11893", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11893", "abs": "https://arxiv.org/abs/2512.11893", "authors": ["Haocheng Lin"], "title": "Beyond Automation: Rethinking Work, Creativity, and Governance in the Age of Generative AI", "comment": null, "summary": "The accelerating advancement of generative artificial intelligence (AI) systems is reshaping the nature, distribution and meaning of work, creativity, and economic security. This paper investigates four inter-related phenomena in the current AI era: (1) the evolving landscape of employment and the future of work; (2) the diverse patterns of AI adoption across socio-demographic groups, sectors, and geographies; (3) whether universal basic income (UBI) should become a compulsory policy response to the AI revolution; and (4) the implications of AI content policies and model behaviours for human creativity, wellbeing, and everyday decision-making. Furthermore, the paper tests the hypothesis that newer model generations may perform worse than their predecessors, and examines how users' interactions with AI systems may produce echo chambers through sycophantic model alignment. Using a mixed methodology that integrates labour market task-exposure modelling, sectoral diffusion mapping, policy-framework analysis, and qualitative discourse critique, this study develops a comprehensive framework for understanding the societal consequences of AI systems beyond productivity gains. It argues that to foster an inclusive, meaningful, and creative environment, policymakers must treat UBI as one dimension within a broader ecosystem of governance, skills development, creativity preservation, and model design. The paper concludes by outlining future research directions, including systematic evaluation of AI's creative performance across model generations, construction of a taxonomy of AI-usage distribution and equity, and formulation of governance criteria to balance content restrictions with creative freedom."}
{"id": "2512.12411", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12411", "abs": "https://arxiv.org/abs/2512.12411", "authors": ["Ely Hahami", "Lavik Jain", "Ishaan Sinha"], "title": "Feeling the Strength but Not the Source: Partial Introspection in LLMs", "comment": "7 pages (+ 5 pages for appendix), 5 figures, 1 table", "summary": "Recent work from Anthropic claims that frontier models can sometimes detect and name injected \"concepts\" represented as activation directions. We test the robustness of these claims. First, we reproduce Anthropic's multi-turn \"emergent introspection\" result on Meta-Llama-3.1-8B-Instruct, finding that the model identifies and names the injected concept 20 percent of the time under Anthropic's original pipeline, exactly matching their reported numbers and thus showing that introspection is not exclusive to very large or capable models. Second, we systematically vary the inference prompt and find that introspection is fragile: performance collapses on closely related tasks such as multiple-choice identification of the injected concept or different prompts of binary discrimination of whether a concept was injected at all. Third, we identify a contrasting regime of partial introspection: the same model can reliably classify the strength of the coefficient of a normalized injected concept vector (as weak / moderate / strong / very strong) with up to 70 percent accuracy, far above the 25 percent chance baseline. Together, these results provide more evidence for Anthropic's claim that language models effectively compute a function of their baseline, internal representations during introspection; however, these self-reports about those representations are narrow and prompt-sensitive. Our code is available at https://github.com/elyhahami18/CS2881-Introspection."}
{"id": "2512.11882", "categories": ["cs.CY", "cs.AI", "cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.11882", "abs": "https://arxiv.org/abs/2512.11882", "authors": ["Lucia Happe", "Dominik Fuchß", "Luca Hüttner", "Kai Marquardt", "Anne Koziolek"], "title": "An Experience Report on a Pedagogically Controlled, Curriculum-Constrained AI Tutor for SE Education", "comment": "11 pages, 4 figures, accepted for publication at ICSE 2026 SEET Track", "summary": "The integration of artificial intelligence (AI) into education continues to evoke both promise and skepticism. While past waves of technological optimism often fell short, recent advances in large language models (LLMs) have revived the vision of scalable, individualized tutoring. This paper presents the design and pilot evaluation of RockStartIT Tutor, an AI-powered assistant developed for a digital programming and computational thinking course within the RockStartIT initiative. Powered by GPT-4 via OpenAI's Assistant API, the tutor employs a novel prompting strategy and a modular, semantically tagged knowledge base to deliver context-aware, personalized, and curriculum-constrained support for secondary school students. We evaluated the system using the Technology Acceptance Model (TAM) with 13 students and teachers. Learners appreciated the low-stakes environment for asking questions and receiving scaffolded guidance. Educators emphasized the system's potential to reduce cognitive load during independent tasks and complement classroom teaching. Key challenges include prototype limitations, a small sample size, and the need for long-term studies with the target age group. Our findings highlight a pragmatic approach to AI integration that requires no model training, using structure and prompts to shape behavior. We position AI tutors not as teacher replacements but as enabling tools that extend feedback access, foster inquiry, and support what schools do best: help students learn."}
{"id": "2512.11918", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.11918", "abs": "https://arxiv.org/abs/2512.11918", "authors": ["Michał Ćwiąkała", "Gabriela Wojak", "Dariusz Baran", "Ernest Górka", "Bartłomiej Bartnik", "Waldemar Gajda", "Ryszard Ratajski"], "title": "Financial Management Challenges in Enterprises Employing Remote and Hybrid Workforces", "comment": "21 pages", "summary": "The paper examines financial management challenges faced by organizations operating under remote and hybrid work models. It investigates how these flexible arrangements influence budgeting, reporting, and financial transparency in distributed teams. Using a quantitative survey of managers, HR staff, and finance professionals, the study analyzes the role of digital tools, communication, and organizational practices in shaping financial outcomes. Results indicate that remote and hybrid work can improve budget control and process transparency through the use of ERP systems and digital workflows. However, forecasting accuracy and interdepartmental communication remain major challenges, particularly in organizations with insufficient digital integration. Respondents also reported lower stress levels and improved work-life balance, suggesting potential well-being and productivity benefits. The paper recommends that companies enhance digital infrastructure, adopt advanced analytics for forecasting, and develop clear communication frameworks supported by employee well-being programs. The study contributes original empirical evidence on financial management in flexible work environments, offering practical insights for leaders navigating the digital transformation of finance."}
{"id": "2512.12413", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.12413", "abs": "https://arxiv.org/abs/2512.12413", "authors": ["Gabriel R. Lau", "Wei Yan Low", "Louis Tay", "Ysabel Guevarra", "Dragan Gašević", "Andree Hartanto"], "title": "Understanding Critical Thinking in Generative Artificial Intelligence Use: Development, Validation, and Correlates of the Critical Thinking in AI Use Scale", "comment": null, "summary": "Generative AI tools are increasingly embedded in everyday work and learning, yet their fluency, opacity, and propensity to hallucinate mean that users must critically evaluate AI outputs rather than accept them at face value. The present research conceptualises critical thinking in AI use as a dispositional tendency to verify the source and content of AI-generated information, to understand how models work and where they fail, and to reflect on the broader implications of relying on AI. Across six studies (N = 1365), we developed and validated the 13-item critical thinking in AI use scale and mapped its nomological network. Study 1 generated and content-validated scale items. Study 2 supported a three-factor structure (Verification, Motivation, and Reflection). Studies 3, 4, and 5 confirmed this higher-order model, demonstrated internal consistency and test-retest reliability, strong factor loadings, sex invariance, and convergent and discriminant validity. Studies 3 and 4 further revealed that critical thinking in AI use was positively associated with openness, extraversion, positive trait affect, and frequency of AI use. Lastly, Study 6 demonstrated criterion validity of the scale, with higher critical thinking in AI use scores predicting more frequent and diverse verification strategies, greater veracity-judgement accuracy in a novel and naturalistic ChatGPT-powered fact-checking task, and deeper reflection about responsible AI. Taken together, the current work clarifies why and how people exercise oversight over generative AI outputs and provides a validated scale and ecologically grounded task paradigm to support theory testing, cross-group, and longitudinal research on critical engagement with generative AI outputs."}
{"id": "2512.11883", "categories": ["cs.CY", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11883", "abs": "https://arxiv.org/abs/2512.11883", "authors": ["Wenqi Marshall Guo", "Qingyun Qian", "Khalad Hasan", "Shan Du"], "title": "Aesthetic Alignment Risks Assimilation: How Image Generation and Reward Models Reinforce Beauty Bias and Ideological \"Censorship\"", "comment": null, "summary": "Over-aligning image generation models to a generalized aesthetic preference conflicts with user intent, particularly when ``anti-aesthetic\" outputs are requested for artistic or critical purposes. This adherence prioritizes developer-centered values, compromising user autonomy and aesthetic pluralism. We test this bias by constructing a wide-spectrum aesthetics dataset and evaluating state-of-the-art generation and reward models. We find that aesthetic-aligned generation models frequently default to conventionally beautiful outputs, failing to respect instructions for low-quality or negative imagery. Crucially, reward models penalize anti-aesthetic images even when they perfectly match the explicit user prompt. We confirm this systemic bias through image-to-image editing and evaluation against real abstract artworks."}
{"id": "2512.11930", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11930", "abs": "https://arxiv.org/abs/2512.11930", "authors": ["Mei Jiang", "Haihai Shen", "Zhuo Luo", "Bingdong Li", "Wenjing Hong", "Ke Tang", "Aimin Zhou"], "title": "Evolutionary Reinforcement Learning based AI tutor for Socratic Interdisciplinary Instruction", "comment": null, "summary": "Cultivating higher-order cognitive abilities -- such as knowledge integration, critical thinking, and creativity -- in modern STEM education necessitates a pedagogical shift from passive knowledge transmission to active Socratic construction. Although Large Language Models (LLMs) hold promise for STEM Interdisciplinary education, current methodologies employing Prompt Engineering (PE), Supervised Fine-tuning (SFT), or standard Reinforcement Learning (RL) often fall short of supporting this paradigm. Existing methods are hindered by three fundamental challenges: the inability to dynamically model latent student cognitive states; severe reward sparsity and delay inherent in long-term educational goals; and a tendency toward policy collapse lacking strategic diversity due to reliance on behavioral cloning. Recognizing the unobservability and dynamic complexity of these interactions, we formalize the Socratic Interdisciplinary Instructional Problem (SIIP) as a structured Partially Observable Markov Decision Process (POMDP), demanding simultaneous global exploration and fine-grained policy refinement. To this end, we propose ERL4SIIP, a novel Evolutionary Reinforcement Learning (ERL) framework specifically tailored for this domain. ERL4SIIP integrates: (1) a dynamic student simulator grounded in a STEM knowledge graph for latent state modeling; (2) a Hierarchical Reward Mechanism that decomposes long-horizon goals into dense signals; and (3) a LoRA-Division based optimization strategy coupling evolutionary algorithms for population-level global search with PPO for local gradient ascent."}
{"id": "2512.12443", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.12443", "abs": "https://arxiv.org/abs/2512.12443", "authors": ["Akhmadillo Mamirov", "Faiaz Azmain", "Hanyu Wang"], "title": "AI Transparency Atlas: Framework, Scoring, and Real-Time Model Card Evaluation Pipeline", "comment": null, "summary": "AI model documentation is fragmented across platforms and inconsistent in structure, preventing policymakers, auditors, and users from reliably assessing safety claims, data provenance, and version-level changes. We analyzed documentation from five frontier models (Gemini 3, Grok 4.1, Llama 4, GPT-5, and Claude 4.5) and 100 Hugging Face model cards, identifying 947 unique section names with extreme naming variation. Usage information alone appeared under 97 distinct labels. Using the EU AI Act Annex IV and the Stanford Transparency Index as baselines, we developed a weighted transparency framework with 8 sections and 23 subsections that prioritizes safety-critical disclosures (Safety Evaluation: 25%, Critical Risk: 20%) over technical specifications. We implemented an automated multi-agent pipeline that extracts documentation from public sources and scores completeness through LLM-based consensus. Evaluating 50 models across vision, multimodal, open-source, and closed-source systems cost less than $3 in total and revealed systematic gaps. Frontier labs (xAI, Microsoft, Anthropic) achieve approximately 80% compliance, while most providers fall below 60%. Safety-critical categories show the largest deficits: deception behaviors, hallucinations, and child safety evaluations account for 148, 124, and 116 aggregate points lost, respectively, across all evaluated models."}
{"id": "2512.11887", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11887", "abs": "https://arxiv.org/abs/2512.11887", "authors": ["Yihan Liao", "Jingyu Zhang", "Jacky Keung", "Yan Xiao", "Yurou Dai"], "title": "Advancing Autonomous Driving System Testing: Demands, Challenges, and Future Directions", "comment": "Accepted for publication in Information and Software Technology (IST)", "summary": "Autonomous driving systems (ADSs) promise improved transportation efficiency and safety, yet ensuring their reliability in complex real-world environments remains a critical challenge. Effective testing is essential to validate ADS performance and reduce deployment risks. This study investigates current ADS testing practices for both modular and end-to-end systems, identifies key demands from industry practitioners and academic researchers, and analyzes the gaps between existing research and real-world requirements. We review major testing techniques and further consider emerging factors such as Vehicle-to-Everything (V2X) communication and foundation models, including large language models and vision foundation models, to understand their roles in enhancing ADS testing. We conducted a large-scale survey with 100 participants from both industry and academia. Survey questions were refined through expert discussions, followed by quantitative and qualitative analyses to reveal key trends, challenges, and unmet needs. Our results show that existing ADS testing techniques struggle to comprehensively evaluate real-world performance, particularly regarding corner case diversity, the simulation to reality gap, the lack of systematic testing criteria, exposure to potential attacks, practical challenges in V2X deployment, and the high computational cost of foundation model-based testing. By further analyzing participant responses together with 105 representative studies, we summarize the current research landscape and highlight major limitations. This study consolidates critical research gaps in ADS testing and outlines key future research directions, including comprehensive testing criteria, cross-model collaboration in V2X systems, cross-modality adaptation for foundation model-based testing, and scalable validation frameworks for large-scale ADS evaluation."}
{"id": "2512.11931", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11931", "abs": "https://arxiv.org/abs/2512.11931", "authors": ["Alexander K. Saeri", "Sophia Lloyd George", "Jess Graham", "Clelia D. Lacarriere", "Peter Slattery", "Michael Noetel", "Neil Thompson"], "title": "Mapping AI Risk Mitigations: Evidence Scan and Preliminary AI Risk Mitigation Taxonomy", "comment": "Access AI Risk Mitigation Database and Taxonomy at https://airisk.mit.edu", "summary": "Organizations and governments that develop, deploy, use, and govern AI must coordinate on effective risk mitigation. However, the landscape of AI risk mitigation frameworks is fragmented, uses inconsistent terminology, and has gaps in coverage. This paper introduces a preliminary AI Risk Mitigation Taxonomy to organize AI risk mitigations and provide a common frame of reference. The Taxonomy was developed through a rapid evidence scan of 13 AI risk mitigation frameworks published between 2023-2025, which were extracted into a living database of 831 AI risk mitigations. The mitigations were iteratively clustered & coded to create the Taxonomy. The preliminary AI Risk Mitigation Taxonomy organizes mitigations into four categories and 23 subcategories: (1) Governance & Oversight: Formal organizational structures and policy frameworks that establish human oversight mechanisms and decision protocols; (2) Technical & Security: Technical, physical, and engineering safeguards that secure AI systems and constrain model behaviors; (3) Operational Process: processes and management frameworks governing AI system deployment, usage, monitoring, incident handling, and validation; and (4) Transparency & Accountability: formal disclosure practices and verification mechanisms that communicate AI system information and enable external scrutiny. The rapid evidence scan and taxonomy construction also revealed several cases where terms like 'risk management' and 'red teaming' are used widely but refer to different responsible actors, actions, and mechanisms of action to reduce risk. This Taxonomy and associated mitigation database, while preliminary, offers a starting point for collation and synthesis of AI risk mitigations. It also offers an accessible, structured way for different actors in the AI ecosystem to discuss and coordinate action to reduce risks from AI."}
{"id": "2512.12477", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12477", "abs": "https://arxiv.org/abs/2512.12477", "authors": ["Jiawen Chen", "Yanyan He", "Qi Shao", "Mengli Wei", "Duxin Chen", "Wenwu Yu", "Yanlong Zhao"], "title": "MetaHGNIE: Meta-Path Induced Hypergraph Contrastive Learning in Heterogeneous Knowledge Graphs", "comment": null, "summary": "Node importance estimation (NIE) in heterogeneous knowledge graphs is a critical yet challenging task, essential for applications such as recommendation, knowledge reasoning, and question answering. Existing methods often rely on pairwise connections, neglecting high-order dependencies among multiple entities and relations, and they treat structural and semantic signals independently, hindering effective cross-modal integration. To address these challenges, we propose MetaHGNIE, a meta-path induced hypergraph contrastive learning framework for disentangling and aligning structural and semantic information. MetaHGNIE constructs a higher-order knowledge graph via meta-path sequences, where typed hyperedges capture multi-entity relational contexts. Structural dependencies are aggregated with local attention, while semantic representations are encoded through a hypergraph transformer equipped with sparse chunking to reduce redundancy. Finally, a multimodal fusion module integrates structural and semantic embeddings under contrastive learning with auxiliary supervision, ensuring robust cross-modal alignment. Extensive experiments on benchmark NIE datasets demonstrate that MetaHGNIE consistently outperforms state-of-the-art baselines. These results highlight the effectiveness of explicitly modeling higher-order interactions and cross-modal alignment in heterogeneous knowledge graphs. Our code is available at https://github.com/SEU-WENJIA/DualHNIE"}
{"id": "2512.11890", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.11890", "abs": "https://arxiv.org/abs/2512.11890", "authors": ["Tariq Eldakruri", "Edip Senyurek"], "title": "Automation as a Catalyst for Geothermal Energy Adoption in Qatar: A Techno-Economic and Environmental Assessment", "comment": "13 pages, 1 table. Published version deposited with publisher permission", "summary": "Geothermal energy provides continuous low emission potential but is underused in Qatar because of high capital costs, drilling risks, and uncertainty in subsurface conditions. This study examines how automation can improve the techno economic and environmental feasibility of geothermal deployment through three pathways: Enhanced Geothermal Systems in the Dukhan Basin, repurposed oil and gas wells, and ground source heat pumps for district cooling. Using geological datasets and financial modeling, the analysis shows that full automation reduces capital expenditure by 12 to 14 percent and operating expenditure by 14 to 17 percent. The Levelized Cost of Energy decreases from 145 USD per MWh to 125 USD per MWh, and payback periods shorten by up to two years. Environmental results indicate that geothermal substitution can avoid between 4000 and 17600 tons of CO2 per year for each project. Automation also reduces uncertainty in investment outcomes based on Monte Carlo simulations. Overall, the results show that automation strengthens the economic viability of geothermal systems and supports their integration into Qatars long term energy diversification and decarbonization strategies."}
{"id": "2512.11933", "categories": ["cs.CY", "cs.AI", "cs.CE", "cs.MA", "q-fin.GN"], "pdf": "https://arxiv.org/pdf/2512.11933", "abs": "https://arxiv.org/abs/2512.11933", "authors": ["Eren Kurshan", "Tucker Balch", "David Byrd"], "title": "The Agentic Regulator: Risks for AI in Finance and a Proposed Agent-based Framework for Governance", "comment": null, "summary": "Generative and agentic artificial intelligence is entering financial markets faster than existing governance can adapt. Current model-risk frameworks assume static, well-specified algorithms and one-time validations; large language models and multi-agent trading systems violate those assumptions by learning continuously, exchanging latent signals, and exhibiting emergent behavior. Drawing on complex adaptive systems theory, we model these technologies as decentralized ensembles whose risks propagate along multiple time-scales. We then propose a modular governance architecture. The framework decomposes oversight into four layers of \"regulatory blocks\": (i) self-regulation modules embedded beside each model, (ii) firm-level governance blocks that aggregate local telemetry and enforce policy, (iii) regulator-hosted agents that monitor sector-wide indicators for collusive or destabilizing patterns, and (iv) independent audit blocks that supply third-party assurance. Eight design strategies enable the blocks to evolve as fast as the models they police. A case study on emergent spoofing in multi-agent trading shows how the layered controls quarantine harmful behavior in real time while preserving innovation. The architecture remains compatible with today's model-risk rules yet closes critical observability and control gaps, providing a practical path toward resilient, adaptive AI governance in financial systems."}
{"id": "2512.12501", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12501", "abs": "https://arxiv.org/abs/2512.12501", "authors": ["Dang Phuong Nam", "Nguyen Kieu", "Pham Thanh Hieu"], "title": "SafeGen: Embedding Ethical Safeguards in Text-to-Image Generation", "comment": null, "summary": "Generative Artificial Intelligence (AI) has created unprecedented opportunities for creative expression, education, and research. Text-to-image systems such as DALL.E, Stable Diffusion, and Midjourney can now convert ideas into visuals within seconds, but they also present a dual-use dilemma, raising critical ethical concerns: amplifying societal biases, producing high-fidelity disinformation, and violating intellectual property. This paper introduces SafeGen, a framework that embeds ethical safeguards directly into the text-to-image generation pipeline, grounding its design in established principles for Trustworthy AI. SafeGen integrates two complementary components: BGE-M3, a fine-tuned text classifier that filters harmful or misleading prompts, and Hyper-SD, an optimized diffusion model that produces high fidelity, semantically aligned images. Built on a curated multilingual (English- Vietnamese) dataset and a fairness-aware training process, SafeGen demonstrates that creative freedom and ethical responsibility can be reconciled within a single workflow. Quantitative evaluations confirm its effectiveness, with Hyper-SD achieving IS = 3.52, FID = 22.08, and SSIM = 0.79, while BGE-M3 reaches an F1-Score of 0.81. An ablation study further validates the importance of domain-specific fine-tuning for both modules. Case studies illustrate SafeGen's practical impact in blocking unsafe prompts, generating inclusive teaching materials, and reinforcing academic integrity."}
{"id": "2512.11892", "categories": ["cs.CY", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.11892", "abs": "https://arxiv.org/abs/2512.11892", "authors": ["Jon Crowcroft", "Rute C. Sofia", "Dirk Trossen", "Vassilis Tsaoussidis"], "title": "Should AI Become an Intergenerational Civil Right?", "comment": null, "summary": "Artificial Intelligence (AI) is rapidly becoming a foundational layer of social, economic, and cognitive infrastructure. At the same time, the training and large-scale deployment of AI systems rely on finite and unevenly distributed energy, networking, and computational resources. This tension exposes a largely unexamined problem in current AI governance: while expanding access to AI is essential for social inclusion and equal opportunity, unconstrained growth in AI use risks unsustainable resource consumption, whereas restricting access threatens to entrench inequality and undermine basic rights.\n  This paper argues that access to AI outputs largely derived from publicly produced knowledge should not be treated solely as a commercial service, but as a fundamental civil interest requiring explicit protection. We show that existing regulatory frameworks largely ignore the coupling between equitable access and resource constraints, leaving critical questions of fairness, sustainability, and long-term societal impact unresolved. To address this gap, we propose recognizing access to AI as an \\emph{Intergenerational Civil Right}, establishing a legal and ethical framework that simultaneously safeguards present-day inclusion and the rights of future generations.\n  Beyond normative analysis, we explore how this principle can be technically realized. Drawing on emerging paradigms in IoT--Edge--Cloud computing, decentralized inference, and energy-aware networking, we outline technological trajectories and a strawman architecture for AI Delivery Networks that support equitable access under strict resource constraints. By framing AI as a shared social infrastructure rather than a discretionary market commodity, this work connects governance principles with concrete system design choices, offering a pathway toward AI deployment that is both socially just and environmentally sustainable."}
{"id": "2512.11934", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11934", "abs": "https://arxiv.org/abs/2512.11934", "authors": ["Adeleh Mazaherian", "Erfan Nourbakhsh"], "title": "Unveiling User Perceptions in the Generative AI Era: A Sentiment-Driven Evaluation of AI Educational Apps' Role in Digital Transformation of e-Teaching", "comment": "6 pages, 4 figures", "summary": "The rapid integration of generative artificial intelligence into education has driven digital transformation in e-teaching, yet user perceptions of AI educational apps remain underexplored. This study performs a sentiment-driven evaluation of user reviews from top AI ed-apps on the Google Play Store to assess efficacy, challenges, and pedagogical implications. Our pipeline involved scraping app data and reviews, RoBERTa for binary sentiment classification, GPT-4o for key point extraction, and GPT-5 for synthesizing top positive/negative themes. Apps were categorized into seven types (e.g., homework helpers, math solvers, language tools), with overlaps reflecting multifunctional designs. Results indicate predominantly positive sentiments, with homework apps like Edu AI (95.9% positive) and Answer.AI (92.7%) leading in accuracy, speed, and personalization, while language/LMS apps (e.g., Teacher AI at 21.8% positive) lag due to instability and limited features. Positives emphasize efficiency in brainstorming, problem-solving, and engagement; negatives center on paywalls, inaccuracies, ads, and glitches. Trends show that homework helpers outperform specialized tools, highlighting AI's democratizing potential amid risks of dependency and inequity. The discussion proposes future ecosystems with hybrid AI-human models, VR/AR for immersive learning, and a roadmap for developers (adaptive personalization) and policymakers (monetization regulation for inclusivity). This underscores generative AI's role in advancing e-teaching by enabling ethical refinements that foster equitable, innovative environments. The full dataset is available here(https://github.com/erfan-nourbakhsh/GenAI-EdSent)."}
{"id": "2512.12503", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12503", "abs": "https://arxiv.org/abs/2512.12503", "authors": ["Mingrui Ye", "Chanjin Zheng", "Zengyi Yu", "Chenyu Xiang", "Zhixue Zhao", "Zheng Yuan", "Helen Yannakoudakis"], "title": "KidsArtBench: Multi-Dimensional Children's Art Evaluation with Attribute-Aware MLLMs", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) show remarkable progress across many visual-language tasks; however, their capacity to evaluate artistic expression remains limited. Aesthetic concepts are inherently abstract and open-ended, and multimodal artwork annotations are scarce. We introduce KidsArtBench, a new benchmark of over 1k children's artworks (ages 5-15) annotated by 12 expert educators across 9 rubric-aligned dimensions, together with expert comments for feedback. Unlike prior aesthetic datasets that provide single scalar scores on adult imagery, KidsArtBench targets children's artwork and pairs multi-dimensional annotations with comment supervision to enable both ordinal assessment and formative feedback. Building on this resource, we propose an attribute-specific multi-LoRA approach, where each attribute corresponds to a distinct evaluation dimension (e.g., Realism, Imagination) in the scoring rubric, with Regression-Aware Fine-Tuning (RAFT) to align predictions with ordinal scales. On Qwen2.5-VL-7B, our method increases correlation from 0.468 to 0.653, with the largest gains on perceptual dimensions and narrowed gaps on higher-order attributes. These results show that educator-aligned supervision and attribute-aware training yield pedagogically meaningful evaluations and establish a rigorous testbed for sustained progress in educational AI. We release data and code with ethics documentation."}
{"id": "2512.11893", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11893", "abs": "https://arxiv.org/abs/2512.11893", "authors": ["Haocheng Lin"], "title": "Beyond Automation: Rethinking Work, Creativity, and Governance in the Age of Generative AI", "comment": null, "summary": "The accelerating advancement of generative artificial intelligence (AI) systems is reshaping the nature, distribution and meaning of work, creativity, and economic security. This paper investigates four inter-related phenomena in the current AI era: (1) the evolving landscape of employment and the future of work; (2) the diverse patterns of AI adoption across socio-demographic groups, sectors, and geographies; (3) whether universal basic income (UBI) should become a compulsory policy response to the AI revolution; and (4) the implications of AI content policies and model behaviours for human creativity, wellbeing, and everyday decision-making. Furthermore, the paper tests the hypothesis that newer model generations may perform worse than their predecessors, and examines how users' interactions with AI systems may produce echo chambers through sycophantic model alignment. Using a mixed methodology that integrates labour market task-exposure modelling, sectoral diffusion mapping, policy-framework analysis, and qualitative discourse critique, this study develops a comprehensive framework for understanding the societal consequences of AI systems beyond productivity gains. It argues that to foster an inclusive, meaningful, and creative environment, policymakers must treat UBI as one dimension within a broader ecosystem of governance, skills development, creativity preservation, and model design. The paper concludes by outlining future research directions, including systematic evaluation of AI's creative performance across model generations, construction of a taxonomy of AI-usage distribution and equity, and formulation of governance criteria to balance content restrictions with creative freedom."}
{"id": "2512.12105", "categories": ["cs.CY", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.12105", "abs": "https://arxiv.org/abs/2512.12105", "authors": ["Andreia dos Santos Sachete", "Alba Valeria de SantAnna de Freitas Loiola", "Fabio Diniz Rossi", "Jose Valdeni de Lima", "Raquel Salcedo Gomes"], "title": "Beyond right or wrong : towards redefining adaptive learning indicators in virtual learning environments", "comment": "20 pages, 1 figure, 1 table", "summary": "Student learning development must involve more than just correcting or incorrect questions. However, most adaptive learning methods in Virtual Learning Environments are based on whether the student's response is incorrect or correct. This perspective is limited in assessing the student's learning level, as it does not consider other elements that can be crucial in this process. The objective of this work is to conduct a Systematic Literature Review (SLR) to elucidate which learning indicators influence student learning and which can be implemented in a VLE to assist in adaptive learning. The works selected and filtered by qualitative assessment reveal a comprehensive approach to assessing different aspects of the learning in virtual environments, such as motivation, emotions, physiological responses, brain imaging, and the students' prior knowledge. The discussion of these new indicators allows adaptive technology developers to implement more appropriate solutions to students' realities, resulting in more complete training."}
{"id": "2512.12548", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12548", "abs": "https://arxiv.org/abs/2512.12548", "authors": ["Yesid Fonseca", "Manuel S. Ríos", "Nicanor Quijano", "Luis F. Giraldo"], "title": "World Models Unlock Optimal Foraging Strategies in Reinforcement Learning Agents", "comment": "14 pages, 6 figures", "summary": "Patch foraging involves the deliberate and planned process of determining the optimal time to depart from a resource-rich region and investigate potentially more beneficial alternatives. The Marginal Value Theorem (MVT) is frequently used to characterize this process, offering an optimality model for such foraging behaviors. Although this model has been widely used to make predictions in behavioral ecology, discovering the computational mechanisms that facilitate the emergence of optimal patch-foraging decisions in biological foragers remains under investigation. Here, we show that artificial foragers equipped with learned world models naturally converge to MVT-aligned strategies. Using a model-based reinforcement learning agent that acquires a parsimonious predictive representation of its environment, we demonstrate that anticipatory capabilities, rather than reward maximization alone, drive efficient patch-leaving behavior. Compared with standard model-free RL agents, these model-based agents exhibit decision patterns similar to many of their biological counterparts, suggesting that predictive world models can serve as a foundation for more explainable and biologically grounded decision-making in AI systems. Overall, our findings highlight the value of ecological optimality principles for advancing interpretable and adaptive AI."}
{"id": "2512.11902", "categories": ["cs.AI", "cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11902", "abs": "https://arxiv.org/abs/2512.11902", "authors": ["Yanna Elizabeth Smid", "Peter van der Putten", "Aske Plaat"], "title": "Mirror Mode in Fire Emblem: Beating Players at their own Game with Imitation and Reinforcement Learning", "comment": null, "summary": "Enemy strategies in turn-based games should be surprising and unpredictable. This study introduces Mirror Mode, a new game mode where the enemy AI mimics the personal strategy of a player to challenge them to keep changing their gameplay. A simplified version of the Nintendo strategy video game Fire Emblem Heroes has been built in Unity, with a Standard Mode and a Mirror Mode. Our first set of experiments find a suitable model for the task to imitate player demonstrations, using Reinforcement Learning and Imitation Learning: combining Generative Adversarial Imitation Learning, Behavioral Cloning, and Proximal Policy Optimization. The second set of experiments evaluates the constructed model with player tests, where models are trained on demonstrations provided by participants. The gameplay of the participants indicates good imitation in defensive behavior, but not in offensive strategies. Participant's surveys indicated that they recognized their own retreating tactics, and resulted in an overall higher player-satisfaction for Mirror Mode. Refining the model further may improve imitation quality and increase player's satisfaction, especially when players face their own strategies. The full code and survey results are stored at: https://github.com/YannaSmid/MirrorMode"}
{"id": "2512.12109", "categories": ["cs.CY", "cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2512.12109", "abs": "https://arxiv.org/abs/2512.12109", "authors": ["Allen Daniel Sunny"], "title": "A neuro-symbolic framework for accountability in public-sector AI", "comment": "Master's thesis, University of Maryland, College Park (2025)", "summary": "Automated eligibility systems increasingly determine access to essential public benefits, but the explanations they generate often fail to reflect the legal rules that authorize those decisions. This thesis develops a legally grounded explainability framework that links system-generated decision justifications to the statutory constraints of CalFresh, California's Supplemental Nutrition Assistance Program. The framework combines a structured ontology of eligibility requirements derived from the state's Manual of Policies and Procedures (MPP), a rule extraction pipeline that expresses statutory logic in a verifiable formal representation, and a solver-based reasoning layer to evaluate whether the explanation aligns with governing law. Case evaluations demonstrate the framework's ability to detect legally inconsistent explanations, highlight violated eligibility rules, and support procedural accountability by making the basis of automated determinations traceable and contestable."}
{"id": "2512.12552", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12552", "abs": "https://arxiv.org/abs/2512.12552", "authors": ["Jifei Liu", "Zhi Chen", "Yuanguang Zhong"], "title": "Large Language Newsvendor: Decision Biases and Cognitive Mechanisms", "comment": null, "summary": "Problem definition: Although large language models (LLMs) are increasingly integrated into business decision making, their potential to replicate and even amplify human cognitive biases cautions a significant, yet not well-understood, risk. This is particularly critical in high-stakes operational contexts like supply chain management. To address this, we investigate the decision-making patterns of leading LLMs using the canonical newsvendor problem in a dynamic setting, aiming to identify the nature and origins of their cognitive biases. Methodology/results: Through dynamic, multi-round experiments with GPT-4, GPT-4o, and LLaMA-8B, we tested for five established decision biases. We found that LLMs consistently replicated the classic ``Too Low/Too High'' ordering bias and significantly amplified other tendencies like demand-chasing behavior compared to human benchmarks. Our analysis uncovered a ``paradox of intelligence'': the more sophisticated GPT-4 demonstrated the greatest irrationality through overthinking, while the efficiency-optimized GPT-4o performed near-optimally. Because these biases persist even when optimal formulas are provided, we conclude they stem from architectural constraints rather than knowledge gaps. Managerial implications: First, managers should select models based on the specific task, as our results show that efficiency-optimized models can outperform more complex ones on certain optimization problems. Second, the significant amplification of bias by LLMs highlights the urgent need for robust human-in-the-loop oversight in high-stakes decisions to prevent costly errors. Third, our findings suggest that designing structured, rule-based prompts is a practical and effective strategy for managers to constrain models' heuristic tendencies and improve the reliability of AI-assisted decisions."}
{"id": "2512.11907", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11907", "abs": "https://arxiv.org/abs/2512.11907", "authors": ["Daniel Platnick", "Marjan Alirezaie", "Hossein Rahnama"], "title": "Structured Personalization: Modeling Constraints as Matroids for Data-Minimal LLM Agents", "comment": "Accepted to the AAAI 2026 Workshop on Personalization in the Era of Large Foundation Models (PerFM), 5 pages, 1 figure", "summary": "Personalizing Large Language Model (LLM) agents requires conditioning them on user-specific data, creating a critical trade-off between task utility and data disclosure. While the utility of adding user data often exhibits diminishing returns (i.e., submodularity), enabling near-optimal greedy selection, real-world personalization is complicated by structural constraints. These include logical dependencies (e.g., selecting fact A requires fact B), categorical quotas (e.g., select at most one writing style), and hierarchical rules (e.g., select at most two social media preferences, of which at most one can be for a professional network). These constraints violate the assumptions of standard subset selection algorithms. We propose a principled method to formally model such constraints. We introduce a compilation process that transforms a user's knowledge graph with dependencies into a set of abstract macro-facets. Our central result is a proof that common hierarchical and quota-based constraints over these macro-facets form a valid laminar matroid. This theoretical characterization lets us cast structured personalization as submodular maximization under a matroid constraint, enabling greedy with constant-factor guarantees (and (1-1/e) via continuous greedy) for a much richer and more realistic class of problems."}
{"id": "2512.12187", "categories": ["cs.CY", "cs.HC", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2512.12187", "abs": "https://arxiv.org/abs/2512.12187", "authors": ["David Gamba", "Daniel M. Romero", "Grant Schoenebeck"], "title": "The Ideological Turing Test for Moderation of Outgroup Affective Animosity", "comment": "32 pages", "summary": "Rising animosity toward ideological opponents poses critical societal challenges. We introduce and test the Ideological Turing Test, a gamified framework requiring participants to adopt and defend opposing viewpoints, to reduce affective animosity and affective polarization.\n  We conducted a mixed-design experiment ($N = 203$) with four conditions: modality (debate/writing) x perspective-taking (Own/Opposite side). Participants engaged in structured interactions defending assigned positions, with outcomes judged by peers. We measured changes in affective animosity and ideological position immediately post-intervention and at 2-6 week follow-up.\n  Perspective-taking reduced out-group animosity and ideological polarization. However, effects differed by modality (writing vs. debate) and over time. For affective animosity, writing from the opposite perspective yielded the largest immediate reduction ($Δ=+0.45$ SD), but the effect was not detectable at the 4-6 week follow-up. In contrast, the debate modality maintained a statistically significant reduction in animosity immediately after and at follow-up ($Δ=+0.37$ SD). For ideological position, adopting the opposite perspective led to significant immediate movement across modalities (writing: $Δ=+0.91$ SD; debate: $Δ=+0.51$ SD), and these changes persisted at follow-up. Judged performance (winning) did not moderate these effects, and willingness to re-participate was similar across conditions (~20-36%).\n  These findings challenge assumptions about adversarial methods, revealing distinct temporal patterns: non-adversarial engagement fosters short-term empathy gains, while cognitive engagement through debate sustains affective benefits. The Ideological Turing Test demonstrates potential as a scalable tool for reducing polarization, particularly when combining perspective-taking with reflective adversarial interactions."}
{"id": "2512.12597", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12597", "abs": "https://arxiv.org/abs/2512.12597", "authors": ["Miriam Horovicz"], "title": "AgentSHAP: Interpreting LLM Agent Tool Importance with Monte Carlo Shapley Value Estimation", "comment": null, "summary": "LLM agents that use external tools can solve complex tasks, but understanding which tools actually contributed to a response remains a blind spot. No existing XAI methods address tool-level explanations. We introduce AgentSHAP, the first framework for explaining tool importance in LLM agents. AgentSHAP is model-agnostic: it treats the agent as a black box and works with any LLM (GPT, Claude, Llama, etc.) without needing access to internal weights or gradients. Using Monte Carlo Shapley values, AgentSHAP tests how an agent responds with different tool subsets and computes fair importance scores based on game theory. Our contributions are: (1) the first explainability method for agent tool attribution, grounded in Shapley values from game theory; (2) Monte Carlo sampling that reduces cost from O(2n) to practical levels; and (3) comprehensive experiments on API-Bank showing that AgentSHAP produces consistent scores across runs, correctly identifies which tools matter, and distinguishes relevant from irrelevant tools. AgentSHAP joins TokenSHAP (for tokens) and PixelSHAP (for image regions) to complete a family of Shapley-based XAI tools for modern generative AI. Code: https://github.com/GenAISHAP/TokenSHAP."}
{"id": "2512.11909", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11909", "abs": "https://arxiv.org/abs/2512.11909", "authors": ["Hanna Dettki"], "title": "Causal Strengths and Leaky Beliefs: Interpreting LLM Reasoning via Noisy-OR Causal Bayes Nets", "comment": null, "summary": "The nature of intelligence in both humans and machines is a longstanding question. While there is no universally accepted definition, the ability to reason causally is often regarded as a pivotal aspect of intelligence (Lake et al., 2017). Evaluating causal reasoning in LLMs and humans on the same tasks provides hence a more comprehensive understanding of their respective strengths and weaknesses. Our study asks: (Q1) Are LLMs aligned with humans given the \\emph{same} reasoning tasks? (Q2) Do LLMs and humans reason consistently at the task level? (Q3) Do they have distinct reasoning signatures?\n  We answer these by evaluating 20+ LLMs on eleven semantically meaningful causal tasks formalized by a collider graph ($C_1\\!\\to\\!E\\!\\leftarrow\\!C_2$ ) under \\emph{Direct} (one-shot number as response = probability judgment of query node being one and \\emph{Chain of Thought} (CoT; think first, then provide answer).\n  Judgments are modeled with a leaky noisy-OR causal Bayes net (CBN) whose parameters $θ=(b,m_1,m_2,p(C)) \\in [0,1]$ include a shared prior $p(C)$;\n  we select the winning model via AIC between a 3-parameter symmetric causal strength ($m_1{=}m_2$) and 4-parameter asymmetric ($m_1{\\neq}m_2$) variant."}
{"id": "2512.12212", "categories": ["cs.CY", "econ.GN", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.12212", "abs": "https://arxiv.org/abs/2512.12212", "authors": ["Elizabeth Irenne Yuwono", "Dian Tjondronegoro", "Shawn Hunter", "Amber Marshall"], "title": "Anticipatory Governance in Data-Constrained Environments: A Predictive Simulation Framework for Digital Financial Inclusion", "comment": "28 pages, 3 figures", "summary": "Financial exclusion remains a major barrier to digital public service delivery in resource-constrained and archipelagic nations. Traditional policy evaluations rely on retrospective data, limiting the ex-ante intelligence needed for agile resource allocation. This study introduces a predictive simulation framework to support anticipatory governance within government information systems. Using the UNCDF Pacific Digital Economy dataset of 10,108 respondents, we apply a three-stage pipeline: descriptive profiling, interpretable machine learning, and scenario simulation to forecast outcomes of digital financial literacy interventions before deployment. Leveraging cross-sectional structural associations, the framework projects intervention scenarios as prioritization heuristics rather than causal estimates. A transparent linear regression model with R-squared of 95.9 identifies modifiable policy levers. Simulations indicate that foundational digital capabilities such as device access and expense tracking yield the highest projected gains, up to 5.5 percent, outperforming attitudinal nudges. The model enables precision targeting, highlighting young female caregivers as high-leverage responders while flagging non-responders such as urban professionals to prevent resource misallocation. This research demonstrates how static survey data can be repurposed into actionable policy intelligence, offering a scalable and evidence-based blueprint for embedding predictive analytics into public-sector decision-support systems to advance equity-focused digital governance."}
{"id": "2512.12634", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12634", "abs": "https://arxiv.org/abs/2512.12634", "authors": ["Youngmin Im", "Byeongung Jo", "Jaeyoung Wi", "Seungwoo Baek", "Tae Hoon Min", "Joo Hyung Lee", "Sangeun Oh", "Insik Shin", "Sunjae Lee"], "title": "Modular and Multi-Path-Aware Offline Benchmarking for Mobile GUI Agents", "comment": null, "summary": "Mobile GUI Agents, AI agents capable of interacting with mobile applications on behalf of users, have the potential to transform human computer interaction. However, current evaluation practices for GUI agents face two fundamental limitations. First, they either rely on single path offline benchmarks or online live benchmarks. Offline benchmarks using static, single path annotated datasets unfairly penalize valid alternative actions, while online benchmarks suffer from poor scalability and reproducibility due to the dynamic and unpredictable nature of live evaluation. Second, existing benchmarks treat agents as monolithic black boxes, overlooking the contributions of individual components, which often leads to unfair comparisons or obscures key performance bottlenecks. To address these limitations, we present MobiBench, the first modular and multi path aware offline benchmarking framework for mobile GUI agents that enables high fidelity, scalable, and reproducible evaluation entirely in offline settings. Our experiments demonstrate that MobiBench achieves 94.72 percent agreement with human evaluators, on par with carefully engineered online benchmarks, while preserving the scalability and reproducibility of static offline benchmarks. Furthermore, our comprehensive module level analysis uncovers several key insights, including a systematic evaluation of diverse techniques used in mobile GUI agents, optimal module configurations across model scales, the inherent limitations of current LFMs, and actionable guidelines for designing more capable and cost efficient mobile agents."}
{"id": "2512.11912", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11912", "abs": "https://arxiv.org/abs/2512.11912", "authors": ["Liu Peng", "Yaochu Jin"], "title": "Robustness of Probabilistic Models to Low-Quality Data: A Multi-Perspective Analysis", "comment": null, "summary": "A systematic, comparative investigation into the effects of low-quality data reveals a stark spectrum of robustness across modern probabilistic models. We find that autoregressive language models, from token prediction to sequence-to-sequence tasks, are remarkably resilient (for GPT-2, test NLL increases modestly from 2.87 to 3.59 despite 50% token corruption). By contrast, under the same levels of data corruption, class-conditional diffusion models degrade catastrophically (image-label consistency plummets by 56.81% relative to baseline), while classifiers show a moderate impact that diminishes with dataset scale. To explain these discrepancies, we analyze the results through a multi-perspective lens, integrating information theory, PAC learning, and gradient dynamics. These analyses suggest that robustness is heavily influenced by two key principles: the richness of conditioning information, which constrains the learning problem, and the absolute information content of the training data, which allows the signal from correct information to dominate statistical noise."}
{"id": "2512.12306", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.12306", "abs": "https://arxiv.org/abs/2512.12306", "authors": ["Amir Yunus", "Peng Rend Gay", "Oon Teng Lee"], "title": "From Co-Design to Metacognitive Laziness: Evaluating Generative AI in Vocational Education", "comment": "108 pages", "summary": "This study examines the development and deployment of a Generative AI proof-of-concept (POC) designed to support lecturers in a vocational education setting in Singapore. Employing a user-centred, mixed-methods design process, we co-developed an AI chatbot with lecturers to address recurring instructional challenges during exam preparation, specifically managing repetitive questions and scaling feedback delivery. The POC achieved its primary operational goals: lecturers reported streamlined workflows, reduced cognitive load, and observed improved student confidence in navigating course content. However, the deployment yielded unexpected insights into student learning behaviours. Despite enhanced teaching processes, performance data revealed no significant improvement in overall student assessment outcomes. Deep analysis of interaction logs identified concerning patterns, including self-efficacy-driven dependency, \"metacognitive laziness\" (cognitive offloading), and divergent usage strategies. While high-ability students leveraged the tool for strategic verification, low-ability students frequently used it to bypass cognitive effort, potentially exacerbating performance gaps. These findings suggest that Generative AI's educational influence extends beyond instructional efficiency to shape cognitive engagement, self-regulation, and learner equity. The study raises consequential design questions regarding how AI tools can be engineered to minimise dependency, scaffold metacognitive development, and calibrate support across varying ability levels. We conclude that while Generative AI can substantially enhance the teaching experience, achieving meaningful learning gains requires rigorous attention to learner behaviour and the equitable design of AI-supported environments."}
{"id": "2512.12652", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.12652", "abs": "https://arxiv.org/abs/2512.12652", "authors": ["Nardine Osman"], "title": "Value-Aware Multiagent Systems", "comment": null, "summary": "This paper introduces the concept of value awareness in AI, which goes beyond the traditional value-alignment problem. Our definition of value awareness presents us with a concise and simplified roadmap for engineering value-aware AI. The roadmap is structured around three core pillars: (1) learning and representing human values using formal semantics, (2) ensuring the value alignment of both individual agents and multiagent systems, and (3) providing value-based explainability on behaviour. The paper presents a selection of our ongoing work on some of these topics, along with applications to real-life domains."}
{"id": "2512.11918", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.11918", "abs": "https://arxiv.org/abs/2512.11918", "authors": ["Michał Ćwiąkała", "Gabriela Wojak", "Dariusz Baran", "Ernest Górka", "Bartłomiej Bartnik", "Waldemar Gajda", "Ryszard Ratajski"], "title": "Financial Management Challenges in Enterprises Employing Remote and Hybrid Workforces", "comment": "21 pages", "summary": "The paper examines financial management challenges faced by organizations operating under remote and hybrid work models. It investigates how these flexible arrangements influence budgeting, reporting, and financial transparency in distributed teams. Using a quantitative survey of managers, HR staff, and finance professionals, the study analyzes the role of digital tools, communication, and organizational practices in shaping financial outcomes. Results indicate that remote and hybrid work can improve budget control and process transparency through the use of ERP systems and digital workflows. However, forecasting accuracy and interdepartmental communication remain major challenges, particularly in organizations with insufficient digital integration. Respondents also reported lower stress levels and improved work-life balance, suggesting potential well-being and productivity benefits. The paper recommends that companies enhance digital infrastructure, adopt advanced analytics for forecasting, and develop clear communication frameworks supported by employee well-being programs. The study contributes original empirical evidence on financial management in flexible work environments, offering practical insights for leaders navigating the digital transformation of finance."}
{"id": "2512.12371", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.12371", "abs": "https://arxiv.org/abs/2512.12371", "authors": ["David M. Berry"], "title": "AI Sprints: Towards a Critical Method for Human-AI Collaboration", "comment": null, "summary": "The emergence of Large Language Models presents a remarkable opportunity for humanities and social science research. I argue these technologies instantiate what I have called the algorithmic condition, whereby computational systems increasingly mediate not just our analytical tools but how we understand nature and society more generally. This article introduces the possibility for new forms of humanistic inquiry through what I term 'AI sprints', as intensive time-boxed research sessions. This is a research method combining the critical reflexivity essential to humanistic inquiry with iterative dialogue with generative AI. Drawing on experimental work in critical code studies, I demonstrate how tight loops of iterative development can adapt data and book sprint methodologies whilst acknowledging the profound transformations generative AI introduces. Through examining the process of human-AI collaboration when undertaken in these intensive research sessions, I seek to outline this approach as a broader research method. The article builds on Rogers' digital methods approach, proposing that we extend methodologies to study digital objects through their native protocols, using AI systems not merely to process digital traces but to analyse materials traditionally requiring manual coding or transcription. I aim to show this by introducing three cognitive modes, cognitive delegation, productive augmentation, and cognitive overhead, explaining how researchers can maintain a strategic overview whilst using LLM capabilities. The paper contributes both a practical methodology for intensive AI-augmented research and a theoretical framework for understanding the epistemological transformations of this hybrid method. A critical methodology must therefore operate in both technical and theoretical registers, sustaining a rigorous ethical-computational engagement with AI systems and outputs."}
{"id": "2512.12686", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12686", "abs": "https://arxiv.org/abs/2512.12686", "authors": ["Samarth Sarin", "Lovepreet Singh", "Bhaskarjit Sarmah", "Dhagash Mehta"], "title": "Memoria: A Scalable Agentic Memory Framework for Personalized Conversational AI", "comment": "Paper accepted at 5th International Conference of AIML Systems 2025, Bangalore, India", "summary": "Agentic memory is emerging as a key enabler for large language models (LLM) to maintain continuity, personalization, and long-term context in extended user interactions, critical capabilities for deploying LLMs as truly interactive and adaptive agents. Agentic memory refers to the memory that provides an LLM with agent-like persistence: the ability to retain and act upon information across conversations, similar to how a human would. We present Memoria, a modular memory framework that augments LLM-based conversational systems with persistent, interpretable, and context-rich memory. Memoria integrates two complementary components: dynamic session-level summarization and a weighted knowledge graph (KG)-based user modelling engine that incrementally captures user traits, preferences, and behavioral patterns as structured entities and relationships. This hybrid architecture enables both short-term dialogue coherence and long-term personalization while operating within the token constraints of modern LLMs. We demonstrate how Memoria enables scalable, personalized conversational artificial intelligence (AI) by bridging the gap between stateless LLM interfaces and agentic memory systems, offering a practical solution for industry applications requiring adaptive and evolving user experiences."}
{"id": "2512.11920", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11920", "abs": "https://arxiv.org/abs/2512.11920", "authors": ["Dong Liu", "Yanxuan Yu"], "title": "CXL-SpecKV: A Disaggregated FPGA Speculative KV-Cache for Datacenter LLM Serving", "comment": "Accepted to FPGA'26 Oral", "summary": "Large Language Models (LLMs) have revolutionized natural language processing tasks, but their deployment in datacenter environments faces significant challenges due to the massive memory requirements of key-value (KV) caches. During the autoregressive decoding process, KV caches consume substantial GPU memory, limiting batch sizes and overall system throughput. To address these challenges, we propose \\textbf{CXL-SpecKV}, a novel disaggregated KV-cache architecture that leverages Compute Express Link (CXL) interconnects and FPGA accelerators to enable efficient speculative execution and memory disaggregation. Our approach introduces three key innovations: (i) a CXL-based memory disaggregation framework that offloads KV-caches to remote FPGA memory with low latency, (ii) a speculative KV-cache prefetching mechanism that predicts and preloads future tokens' cache entries, and (iii) an FPGA-accelerated KV-cache compression and decompression engine that reduces memory bandwidth requirements by up to 4$\\times$. When evaluated on state-of-the-art LLM models, CXL-SpecKV achieves up to 3.2$\\times$ higher throughput compared to GPU-only baselines, while reducing memory costs by 2.8$\\times$ and maintaining accuracy. Our system demonstrates that intelligent memory disaggregation combined with speculative execution can effectively address the memory wall challenge in large-scale LLM serving. Our code implementation has been open-sourced at https://github.com/FastLM/CXL-SpecKV."}
{"id": "2512.12592", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.12592", "abs": "https://arxiv.org/abs/2512.12592", "authors": ["Tom Lee", "Sihoon Lee", "Seonghun Kim"], "title": "Beyond Static Scoring: Enhancing Assessment Validity via AI-Generated Interactive Verification", "comment": null, "summary": "Large Language Models (LLMs) challenge the validity of traditional open-ended assessments by blurring the lines of authorship. While recent research has focused on the accuracy of automated scoring (AES), these static approaches fail to capture process evidence or verify genuine student understanding. This paper introduces a novel Human-AI Collaboration framework that enhances assessment integrity by combining rubric-based automated scoring with AI-generated, targeted follow-up questions. In a pilot study with university instructors (N=9), we demonstrate that while Stage 1 (Auto-Scoring) ensures procedural fairness and consistency, Stage 2 (Interactive Verification) is essential for construct validity, effectively diagnosing superficial reasoning or unverified AI use. We report on the systems design, instructor perceptions of fairness versus validity, and the necessity of adaptive difficulty in follow-up questioning. The findings offer a scalable pathway for authentic assessment that moves beyond policing AI to integrating it as a synergistic partner in the evaluation process."}
{"id": "2512.12692", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12692", "abs": "https://arxiv.org/abs/2512.12692", "authors": ["Mahir Labib Dihan", "Tanzima Hashem", "Mohammed Eunus Ali", "Md Rizwan Parvez"], "title": "WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment", "comment": "Under review at ICLR 2026. Project page: https://kagnlp.github.io/WebOperator/", "summary": "LLM-based agents often operate in a greedy, step-by-step manner, selecting actions solely based on the current observation without considering long-term consequences or alternative paths. This lack of foresight is particularly problematic in web environments, which are only partially observable-limited to browser-visible content (e.g., DOM and UI elements)-where a single misstep often requires complex and brittle navigation to undo. Without an explicit backtracking mechanism, agents struggle to correct errors or systematically explore alternative paths. Tree-search methods provide a principled framework for such structured exploration, but existing approaches lack mechanisms for safe backtracking, making them prone to unintended side effects. They also assume that all actions are reversible, ignoring the presence of irreversible actions-limitations that reduce their effectiveness in realistic web tasks. To address these challenges, we introduce WebOperator, a tree-search framework that enables reliable backtracking and strategic exploration. Our method incorporates a best-first search strategy that ranks actions by both reward estimates and safety considerations, along with a robust backtracking mechanism that verifies the feasibility of previously visited paths before replaying them, preventing unintended side effects. To further guide exploration, WebOperator generates action candidates from multiple, varied reasoning contexts to ensure diverse and robust exploration, and subsequently curates a high-quality action set by filtering out invalid actions pre-execution and merging semantically equivalent ones. Experimental results on WebArena and WebVoyager demonstrate the effectiveness of WebOperator. On WebArena, WebOperator achieves a state-of-the-art 54.6% success rate with gpt-4o, underscoring the critical advantage of integrating strategic foresight with safe execution."}
{"id": "2512.11930", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11930", "abs": "https://arxiv.org/abs/2512.11930", "authors": ["Mei Jiang", "Haihai Shen", "Zhuo Luo", "Bingdong Li", "Wenjing Hong", "Ke Tang", "Aimin Zhou"], "title": "Evolutionary Reinforcement Learning based AI tutor for Socratic Interdisciplinary Instruction", "comment": null, "summary": "Cultivating higher-order cognitive abilities -- such as knowledge integration, critical thinking, and creativity -- in modern STEM education necessitates a pedagogical shift from passive knowledge transmission to active Socratic construction. Although Large Language Models (LLMs) hold promise for STEM Interdisciplinary education, current methodologies employing Prompt Engineering (PE), Supervised Fine-tuning (SFT), or standard Reinforcement Learning (RL) often fall short of supporting this paradigm. Existing methods are hindered by three fundamental challenges: the inability to dynamically model latent student cognitive states; severe reward sparsity and delay inherent in long-term educational goals; and a tendency toward policy collapse lacking strategic diversity due to reliance on behavioral cloning. Recognizing the unobservability and dynamic complexity of these interactions, we formalize the Socratic Interdisciplinary Instructional Problem (SIIP) as a structured Partially Observable Markov Decision Process (POMDP), demanding simultaneous global exploration and fine-grained policy refinement. To this end, we propose ERL4SIIP, a novel Evolutionary Reinforcement Learning (ERL) framework specifically tailored for this domain. ERL4SIIP integrates: (1) a dynamic student simulator grounded in a STEM knowledge graph for latent state modeling; (2) a Hierarchical Reward Mechanism that decomposes long-horizon goals into dense signals; and (3) a LoRA-Division based optimization strategy coupling evolutionary algorithms for population-level global search with PPO for local gradient ascent."}
{"id": "2512.12707", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.12707", "abs": "https://arxiv.org/abs/2512.12707", "authors": ["Hugo Roger Paz"], "title": "From Linear Risk to Emergent Harm: Complexity as the Missing Core of AI Governance", "comment": "White Paper / Policy Brief (Working Paper). Published version available at: https://doi.org/10.5281/zenodo.17929014", "summary": "Risk-based AI regulation has become the dominant paradigm in AI governance, promising proportional controls aligned with anticipated harms. This paper argues that such frameworks often fail for structural reasons: they implicitly assume linear causality, stable system boundaries, and largely predictable responses to regulation. In practice, AI operates within complex adaptive socio-technical systems in which harm is frequently emergent, delayed, redistributed, and amplified through feedback loops and strategic adaptation by system actors. As a result, compliance can increase while harm is displaced or concealed rather than eliminated. We propose a complexity-based framework for AI governance that treats regulation as intervention rather than control, prioritises dynamic system mapping over static classifications, and integrates causal reasoning and simulation for policy design under uncertainty. The aim is not to eliminate uncertainty, but to enable robust system stewardship through monitoring, learning, and iterative revision of governance interventions."}
{"id": "2512.12706", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.12706", "abs": "https://arxiv.org/abs/2512.12706", "authors": ["Enhong Mu", "Minami Yoda", "Yan Zhang", "Mingyue Zhang", "Yutaka Matsuno", "Jialong Li"], "title": "Synergizing Code Coverage and Gameplay Intent: Coverage-Aware Game Playtesting with LLM-Guided Reinforcement Learning", "comment": null, "summary": "The widespread adoption of the \"Games as a Service\" model necessitates frequent content updates, placing immense pressure on quality assurance. In response, automated game testing has been viewed as a promising solution to cope with this demanding release cadence. However, existing automated testing approaches typically create a dichotomy: code-centric methods focus on structural coverage without understanding gameplay context, while player-centric agents validate high-level intent but often fail to cover specific underlying code changes. To bridge this gap, we propose SMART (Structural Mapping for Augmented Reinforcement Testing), a novel framework that synergizes structural verification and functional validation for game update testing. SMART leverages large language models (LLMs) to interpret abstract syntax tree (AST) differences and extract functional intent, constructing a context-aware hybrid reward mechanism. This mechanism guides reinforcement learning agents to sequentially fulfill gameplay goals while adaptively exploring modified code branches. We evaluate SMART on two environments, Overcooked and Minecraft. The results demonstrate that SMART significantly outperforms state-of-the-art baselines; it achieves over 94% branch coverage of modified code, nearly double that of traditional reinforcement learning methods, while maintaining a 98% task completion rate, effectively balancing structural comprehensiveness with functional correctness."}
{"id": "2512.11931", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11931", "abs": "https://arxiv.org/abs/2512.11931", "authors": ["Alexander K. Saeri", "Sophia Lloyd George", "Jess Graham", "Clelia D. Lacarriere", "Peter Slattery", "Michael Noetel", "Neil Thompson"], "title": "Mapping AI Risk Mitigations: Evidence Scan and Preliminary AI Risk Mitigation Taxonomy", "comment": "Access AI Risk Mitigation Database and Taxonomy at https://airisk.mit.edu", "summary": "Organizations and governments that develop, deploy, use, and govern AI must coordinate on effective risk mitigation. However, the landscape of AI risk mitigation frameworks is fragmented, uses inconsistent terminology, and has gaps in coverage. This paper introduces a preliminary AI Risk Mitigation Taxonomy to organize AI risk mitigations and provide a common frame of reference. The Taxonomy was developed through a rapid evidence scan of 13 AI risk mitigation frameworks published between 2023-2025, which were extracted into a living database of 831 AI risk mitigations. The mitigations were iteratively clustered & coded to create the Taxonomy. The preliminary AI Risk Mitigation Taxonomy organizes mitigations into four categories and 23 subcategories: (1) Governance & Oversight: Formal organizational structures and policy frameworks that establish human oversight mechanisms and decision protocols; (2) Technical & Security: Technical, physical, and engineering safeguards that secure AI systems and constrain model behaviors; (3) Operational Process: processes and management frameworks governing AI system deployment, usage, monitoring, incident handling, and validation; and (4) Transparency & Accountability: formal disclosure practices and verification mechanisms that communicate AI system information and enable external scrutiny. The rapid evidence scan and taxonomy construction also revealed several cases where terms like 'risk management' and 'red teaming' are used widely but refer to different responsible actors, actions, and mechanisms of action to reduce risk. This Taxonomy and associated mitigation database, while preliminary, offers a starting point for collation and synthesis of AI risk mitigations. It also offers an accessible, structured way for different actors in the AI ecosystem to discuss and coordinate action to reduce risks from AI."}
{"id": "2512.12837", "categories": ["cs.CY", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.12837", "abs": "https://arxiv.org/abs/2512.12837", "authors": ["Sahibpreet Singh", "Manjit Singh"], "title": "Algorithmic Criminal Liability in Greenwashing: Comparing India, United States, and European Union", "comment": "Published in HPNLU Journal of Law, Business and Economics, Vol. 3, 2024, pp. 51-68. ISSN: 2584-0436", "summary": "AI-powered greenwashing has emerged as an insidious challenge within corporate sustainability governance, exacerbating the opacity of environmental disclosures and subverting regulatory oversight. This study conducts a comparative legal analysis of criminal liability for AI-mediated greenwashing across India, the US, and the EU, exposing doctrinal lacunae in attributing culpability when deceptive claims originate from algorithmic systems. Existing statutes exhibit anthropocentric biases by predicating liability on demonstrable human intent, rendering them ill-equipped to address algorithmic deception. The research identifies a critical gap in jurisprudential adaptation, as prevailing fraud statutes remain antiquated vis-à-vis AI-generated misrepresentation. Utilising a doctrinal legal methodology, this study systematically dissects judicial precedents and statutory instruments, yielding results regarding the potential expansion of corporate criminal liability. Findings underscore the viability of strict liability models, recalibrated governance frameworks for AI accountability, and algorithmic due diligence mandates under ESG regimes. Comparative insights reveal jurisdictional disparities, with the EU Corporate Sustainability Due Diligence Directive (CSDDD) offering a potential transnational model. This study contributes to AI ethics and environmental jurisprudence by advocating for a hybrid liability framework integrating algorithmic risk assessment with legal personhood constructs, ensuring algorithmic opacity does not preclude liability enforcement."}
{"id": "2512.12736", "categories": ["cs.AI", "cs.MM", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.12736", "abs": "https://arxiv.org/abs/2512.12736", "authors": ["Syeda Zunaira Ahmed", "Hejab Tahira Beg", "Maryam Khalid"], "title": "Personalized QoE Prediction: A Demographic-Augmented Machine Learning Framework for 5G Video Streaming Networks", "comment": "11 pages, 5 figures", "summary": "Quality of Experience (QoE) prediction is a critical component of modern multimedia systems, particularly for adaptive video streaming in 5G networks. Accurate QoE estimation enables intelligent resource management and supports user centric service delivery. Existing QoE prediction approaches primarily rely on limited datasets and assume uniform user perception, which restricts their applicability in heterogeneous real world environments.\n  This paper proposes a demographic aware machine learning framework for personalized QoE prediction. We introduce a behaviorally realistic demographic based data augmentation strategy that expands a small QoE dataset six fold by modeling varying user sensitivities to streaming impairments such as rebuffering, bitrate variation, and quality degradation. Using the augmented dataset, we evaluate a comprehensive set of classical machine learning models alongside advanced deep learning architectures, including an attention-based MLP and TabNet.\n  Experimental results demonstrate significant improvements in prediction accuracy across RMSE, MAE, and R metrics compared to baseline models. Among all evaluated approaches, TabNet achieves the strongest performance, benefiting from its inherent feature selection and attention mechanisms. The results confirm that demographic-aware augmentation substantially enhances QoE prediction robustness and provides a scalable direction for personalized QoE-aware intelligence in 5G video streaming networks."}
{"id": "2512.11933", "categories": ["cs.CY", "cs.AI", "cs.CE", "cs.MA", "q-fin.GN"], "pdf": "https://arxiv.org/pdf/2512.11933", "abs": "https://arxiv.org/abs/2512.11933", "authors": ["Eren Kurshan", "Tucker Balch", "David Byrd"], "title": "The Agentic Regulator: Risks for AI in Finance and a Proposed Agent-based Framework for Governance", "comment": null, "summary": "Generative and agentic artificial intelligence is entering financial markets faster than existing governance can adapt. Current model-risk frameworks assume static, well-specified algorithms and one-time validations; large language models and multi-agent trading systems violate those assumptions by learning continuously, exchanging latent signals, and exhibiting emergent behavior. Drawing on complex adaptive systems theory, we model these technologies as decentralized ensembles whose risks propagate along multiple time-scales. We then propose a modular governance architecture. The framework decomposes oversight into four layers of \"regulatory blocks\": (i) self-regulation modules embedded beside each model, (ii) firm-level governance blocks that aggregate local telemetry and enforce policy, (iii) regulator-hosted agents that monitor sector-wide indicators for collusive or destabilizing patterns, and (iv) independent audit blocks that supply third-party assurance. Eight design strategies enable the blocks to evolve as fast as the models they police. A case study on emergent spoofing in multi-agent trading shows how the layered controls quarantine harmful behavior in real time while preserving innovation. The architecture remains compatible with today's model-risk rules yet closes critical observability and control gaps, providing a practical path toward resilient, adaptive AI governance in financial systems."}
{"id": "2512.12919", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.12919", "abs": "https://arxiv.org/abs/2512.12919", "authors": ["Maria Y. Rodriguez", "Ehren Dohler", "Jon Phillips", "Melissa Villodas", "Voltaire Vegara", "Kenny Joseph", "Amy Wilson"], "title": "Open Source Software and Data for Human Service Development: A Case Study on Predicting Housing Instability", "comment": "24 pages, 5 tables, 3 figures, 4 appendices", "summary": "Open-source data and tools are lauded as essential for replicable and usable social science, though little is known about their use in resource constrained human service provision. This paper examines the challenges and opportunities of open-source tools and data in human service development by using both to forecast failure to pay eviction filings in Bronx County, NY. We use zip code level data from the Housing Data Coalition, the American Community Survey 5-year estimates, and DeepMaps Model of the Labor Force to forecast rates through July 2021. We employ multilevel (MLM) and exponential smoothing (ETS) models using the R project for Statistical Computing, an oft used open-source statistical software. We compare our results to what happened during the same period, to illustrate the efficacy of the open-source tools and techniques employed. We argue open-source data and software may facilitate rapid analysis of public data - a much-needed ability in human service intervention development under increasingly constrained resources - but find public data are limited by the information they reliably capture, limiting their utility by a non-trivial margin of error. The manuscript concludes by considering lessons for human service organizations with limited analytical resources and a vested interest in low-resourced communities."}
{"id": "2512.12804", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12804", "abs": "https://arxiv.org/abs/2512.12804", "authors": ["Sander Beckers"], "title": "Causal Counterfactuals Reconsidered", "comment": "Preprint: currently under review", "summary": "I develop a novel semantics for probabilities of counterfactuals that generalizes the standard Pearlian semantics: it applies to probabilistic causal models that cannot be extended into realistic structural causal models and are therefore beyond the scope of Pearl's semantics. This generalization is needed because, as I show, such probabilistic causal models arise even in simple settings. My semantics offer a natural compromize in the long-standing debate between Pearl and Dawid over counterfactuals: I agree with Dawid that universal causal determinism and unrealistic variables should be rejected, but I agree with Pearl that a general semantics of counterfactuals is nonetheless possible. I restrict attention to causal models that satisfy the Markov condition, only contain realistic variables, and are causally complete. Although I formulate my proposal using structural causal models, as does Pearl, I refrain from using so-called response variables. Moreover, I prove that my semantics is equivalent to two other recent proposals that do not involve structural causal models, and that it is in line with various comments on stochastic counterfactuals that have appeared in the literature more broadly. Throughout I also reflect on the universality of the Markov condition and explore a novel generalization of causal abstractions"}
{"id": "2512.11934", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11934", "abs": "https://arxiv.org/abs/2512.11934", "authors": ["Adeleh Mazaherian", "Erfan Nourbakhsh"], "title": "Unveiling User Perceptions in the Generative AI Era: A Sentiment-Driven Evaluation of AI Educational Apps' Role in Digital Transformation of e-Teaching", "comment": "6 pages, 4 figures", "summary": "The rapid integration of generative artificial intelligence into education has driven digital transformation in e-teaching, yet user perceptions of AI educational apps remain underexplored. This study performs a sentiment-driven evaluation of user reviews from top AI ed-apps on the Google Play Store to assess efficacy, challenges, and pedagogical implications. Our pipeline involved scraping app data and reviews, RoBERTa for binary sentiment classification, GPT-4o for key point extraction, and GPT-5 for synthesizing top positive/negative themes. Apps were categorized into seven types (e.g., homework helpers, math solvers, language tools), with overlaps reflecting multifunctional designs. Results indicate predominantly positive sentiments, with homework apps like Edu AI (95.9% positive) and Answer.AI (92.7%) leading in accuracy, speed, and personalization, while language/LMS apps (e.g., Teacher AI at 21.8% positive) lag due to instability and limited features. Positives emphasize efficiency in brainstorming, problem-solving, and engagement; negatives center on paywalls, inaccuracies, ads, and glitches. Trends show that homework helpers outperform specialized tools, highlighting AI's democratizing potential amid risks of dependency and inequity. The discussion proposes future ecosystems with hybrid AI-human models, VR/AR for immersive learning, and a roadmap for developers (adaptive personalization) and policymakers (monetization regulation for inclusivity). This underscores generative AI's role in advancing e-teaching by enabling ethical refinements that foster equitable, innovative environments. The full dataset is available here(https://github.com/erfan-nourbakhsh/GenAI-EdSent)."}
{"id": "2512.13061", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.13061", "abs": "https://arxiv.org/abs/2512.13061", "authors": ["Jianjun Xiao", "Cixiao Wang", "Wenmei Zhang"], "title": "Modeling Collaborative Problem Solving Dynamics from Group Discourse: A Text-Mining Approach with Synergy Degree Model", "comment": "16 pages, 2 figures", "summary": "Measuring collaborative problem solving (CPS) synergy remains challenging in learning analytics, as classical manual coding cannot capture emergent system-level dynamics. This study introduces a computational framework that integrates automated discourse analysis with the Synergy Degree Model (SDM) to quantify CPS synergy from group communication. Data were collected from 52 learners in 12 groups during a 5-week connectivist MOOC (cMOOC) activity. Nine classification models were applied to automatically identify ten CPS behaviors across four interaction levels: operation, wayfinding, sense-making, and creation. While BERT achieved the highest accuracy, GPT models demonstrated superior precision suitable for human-AI collaborative coding. Within the SDM framework, each interaction level was treated as a subsystem to compute group-level order parameters and derive synergy degrees. Permutation tests showed automated measures preserve construct validity, despite systematic biases at the subsystem level. Statistical analyses revealed significant task-type differences: survey study groups exhibited higher creation-order than mode study groups, suggesting \"controlled disorder\" may benefit complex problem solving. Importantly, synergy degree distinguished collaborative quality, ranging from excellent to failing groups. Findings establish synergy degree as a sensitive indicator of collaboration and demonstrate the feasibility of scaling fine-grained CPS analytics through AI-in-the-loop approaches."}
{"id": "2512.12806", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12806", "abs": "https://arxiv.org/abs/2512.12806", "authors": ["Boyang Yan"], "title": "Fault-Tolerant Sandboxing for AI Coding Agents: A Transactional Approach to Safe Autonomous Execution", "comment": "7 pages", "summary": "The transition of Large Language Models (LLMs) from passive code generators to autonomous agents introduces significant safety risks, specifically regarding destructive commands and inconsistent system states. Existing commercial solutions often prioritize interactive user safety, enforcing authentication barriers that break the headless loops required for true autonomy. This paper presents a Fault-Tolerant Sandboxing framework designed to mitigate these risks through a policy-based interception layer and a transactional filesystem snapshot mechanism. We hypothesize that wrapping agent actions in atomic transactions can guarantee safety with acceptable latency, outperforming the heavy initialization overhead of containers or the interactive friction of commercial CLIs. We validated this approach by deploying the Minimind-MoE LLM served via nano-vllm on a custom Proxmox-based testbed utilizing EVPN/VXLAN isolation. Experimental results demonstrate a 100\\% interception rate for high-risk commands and a 100\\% success rate in rolling back failed states. Crucially, our prototype incurs only a 14.5\\% performance overhead (approx. 1.8s) per transaction. In contrast, benchmarking against the Gemini CLI sandbox revealed that it requires interactive authentication (\"Sign in\"), rendering it unusable for headless, autonomous agent workflows."}
{"id": "2512.11935", "categories": ["cs.AI", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2512.11935", "abs": "https://arxiv.org/abs/2512.11935", "authors": ["Jaehyung Lee", "Justin Ely", "Kent Zhang", "Akshaya Ajith", "Charles Rhys Campbell", "Kamal Choudhary"], "title": "AGAPI-Agents: An Open-Access Agentic AI Platform for Accelerated Materials Design on AtomGPT.org", "comment": null, "summary": "Artificial intelligence is reshaping scientific discovery, yet its use in materials research remains limited by fragmented computational ecosystems, reproducibility challenges, and dependence on commercial large language models (LLMs). Here we introduce AGAPI (AtomGPT.org API), an open-access agentic AI platform that integrates more than eight open-source LLMs with over twenty materials-science API endpoints, unifying databases, simulation tools, and machine-learning models through a common orchestration framework. AGAPI employs an Agent-Planner-Executor-Summarizer architecture that autonomously constructs and executes multi-step workflows spanning materials data retrieval, graph neural network property prediction, machine-learning force-field optimization, tight-binding calculations, diffraction analysis, and inverse design. We demonstrate AGAPI through end-to-end workflows, including heterostructure construction, powder X-ray diffraction analysis, and semiconductor defect engineering requiring up to ten sequential operations. In addition, we evaluate AGAPI using 30+ example prompts as test cases and compare agentic predictions with and without tool access against experimental data. With more than 1,000 active users, AGAPI provides a scalable and transparent foundation for reproducible, AI-accelerated materials discovery. AGAPI-Agents codebase is available at https://github.com/atomgptlab/agapi."}
{"id": "2512.13260", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.13260", "abs": "https://arxiv.org/abs/2512.13260", "authors": ["Hugo Roger Paz"], "title": "From Educational Analytics to AI Governance: Transferable Lessons from Complex Systems Interventions", "comment": "36 pages, 2 tables", "summary": "Both student retention in higher education and artificial intelligence governance face a common structural challenge: the application of linear regulatory frameworks to complex adaptive systems. Risk-based approaches dominate both domains, yet systematically fail because they assume stable causal pathways, predictable actor responses, and controllable system boundaries. This paper extracts transferable methodological principles from CAPIRE (Curriculum, Archetypes, Policies, Interventions & Research Environment), an empirically validated framework for educational analytics that treats student dropout as an emergent property of curricular structures, institutional rules, and macroeconomic shocks. Drawing on longitudinal data from engineering programmes and causal inference methods, CAPIRE demonstrates that well-intentioned interventions routinely generate unintended consequences when system complexity is ignored. We argue that five core principles developed within CAPIRE - temporal observation discipline, structural mapping over categorical classification, archetype-based heterogeneity analysis, causal mechanism identification, and simulation-based policy design - transfer directly to the challenge of governing AI systems. The isomorphism is not merely analogical: both domains exhibit non-linearity, emergence, feedback loops, strategic adaptation, and path dependence. We propose Complex Systems AI Governance (CSAIG) as an integrated framework that operationalises these principles for regulatory design, shifting the central question from \"how risky is this AI system?\" to \"how does this intervention reshape system dynamics?\" The contribution is twofold: demonstrating that empirical lessons from one complex systems domain can accelerate governance design in another, and offering a concrete methodological architecture for complexity-aware AI regulation."}
{"id": "2512.12856", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12856", "abs": "https://arxiv.org/abs/2512.12856", "authors": ["Saad Alqithami"], "title": "Forgetful but Faithful: A Cognitive Memory Architecture and Benchmark for Privacy-Aware Generative Agents", "comment": null, "summary": "As generative agents become increasingly sophisticated and deployed in long-term interactive scenarios, their memory management capabilities emerge as a critical bottleneck for both performance and privacy. Current approaches either maintain unlimited memory stores, leading to computational intractability and privacy concerns, or employ simplistic forgetting mechanisms that compromise agent coherence and functionality. This paper introduces the Memory-Aware Retention Schema (MaRS), a novel framework for human-centered memory management in generative agents, coupled with six theoretically-grounded forgetting policies that balance performance, privacy, and computational efficiency. We present the Forgetful but Faithful Agent (FiFA) benchmark, a comprehensive evaluation framework that assesses agent performance across narrative coherence, goal completion, social recall accuracy, privacy preservation, and cost efficiency. Through extensive experimentation involving 300 evaluation runs across multiple memory budgets and agent configurations, we demonstrate that our hybrid forgetting policy achieves superior performance (composite score: 0.911) while maintaining computational tractability and privacy guarantees. Our work establishes new benchmarks for memory-budgeted agent evaluation and provides practical guidelines for deploying generative agents in resource-constrained, privacy-sensitive environments. The theoretical foundations, implementation framework, and empirical results contribute to the emerging field of human-centered AI by addressing fundamental challenges in agent memory management that directly impact user trust, system scalability, and regulatory compliance."}
{"id": "2512.11942", "categories": ["cs.AI", "cs.FL", "cs.GT"], "pdf": "https://arxiv.org/pdf/2512.11942", "abs": "https://arxiv.org/abs/2512.11942", "authors": ["Vince Trencsenyi"], "title": "Hypergame Rationalisability: Solving Agent Misalignment In Strategic Play", "comment": null, "summary": "Differences in perception, information asymmetries, and bounded rationality lead game-theoretic players to derive a private, subjective view of the game that may diverge from the underlying ground-truth scenario and may be misaligned with other players' interpretations. While typical game-theoretic assumptions often overlook such heterogeneity, hypergame theory provides the mathematical framework to reason about mismatched mental models. Although hypergames have recently gained traction in dynamic applications concerning uncertainty, their practical adoption in multi-agent system research has been hindered by the lack of a unifying, formal, and practical representation language, as well as scalable algorithms for managing complex hypergame structures and equilibria. Our work addresses this gap by introducing a declarative, logic-based domain-specific language for encoding hypergame structures and hypergame solution concepts. Leveraging answer-set programming, we develop an automated pipeline for instantiating hypergame structures and running our novel hypergame rationalisation procedure, a mechanism for finding belief structures that justify seemingly irrational outcomes. The proposed language establishes a unifying formalism for hypergames and serves as a foundation for developing nuanced, belief-based heterogeneous reasoners, offering a verifiable context with logical guarantees. Together, these contributions establish the connection between hypergame theory, multi-agent systems, and strategic AI."}
{"id": "2512.13404", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.13404", "abs": "https://arxiv.org/abs/2512.13404", "authors": ["Stefan Kulk", "Frederik Zuiderveen Borgesius"], "title": "Google Spain v. Gonzáles: Did the Court forget about freedom of expression?", "comment": null, "summary": "When reviewing a job application letter, going on a first date, or considering doing business with someone, the first thing many people do is entering the person's name in a search engine. A search engine can point searchers to information that would otherwise have remained obscure. If somebody searched for the name of Spanish lawyer Mario Costeja González, Google showed search results that included a link to a 1998 newspaper announcement implying he had financial troubles at the time. González wanted Google to stop showing those links and started a procedure in Spain. After some legal wrangling, the Spanish Audiencia Nacional (National High Court) asked the Court of Justice of the European Union (CJEU) for advice on the application of the Data Protection Directive, which led to the controversial judgment in Google Spain. In its judgment, the CJEU holds that people, under certain conditions, have the right to have search results for their name delisted. This right can also extend to lawfully published information."}
{"id": "2512.12918", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12918", "abs": "https://arxiv.org/abs/2512.12918", "authors": ["Nijesh Upreti", "Vaishak Belle"], "title": "Satisfiability Modulo Theory Meets Inductive Logic Programming", "comment": null, "summary": "Inductive Logic Programming (ILP) provides interpretable rule learning in relational domains, yet remains limited in its ability to induce and reason with numerical constraints. Classical ILP systems operate over discrete predicates and typically rely on discretisation or hand-crafted numerical predicates, making it difficult to infer thresholds or arithmetic relations that must hold jointly across examples. Recent work has begun to address these limitations through tighter integrations of ILP with Satisfiability Modulo Theories (SMT) or specialised numerical inference mechanisms. In this paper we investigate a modular alternative that couples the ILP system PyGol with the SMT solver Z3. Candidate clauses proposed by PyGol are interpreted as quantifier-free formulas over background theories such as linear or nonlinear real arithmetic, allowing numerical parameters to be instantiated and verified by the SMT solver while preserving ILP's declarative relational bias. This supports the induction of hybrid rules that combine symbolic predicates with learned numerical constraints, including thresholds, intervals, and multi-literal arithmetic relations. We formalise this SMT-ILP setting and evaluate it on a suite of synthetic datasets designed to probe linear, relational, nonlinear, and multi-hop reasoning. The results illustrate how a modular SMT-ILP architecture can extend the expressivity of symbolic rule learning, complementing prior numerical ILP approaches while providing a flexible basis for future extensions toward richer theory-aware induction."}
{"id": "2512.11997", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11997", "abs": "https://arxiv.org/abs/2512.11997", "authors": ["Anfeng Peng", "Ajesh Koyatan Chathoth", "Stephen Lee"], "title": "Log Anomaly Detection with Large Language Models via Knowledge-Enriched Fusion", "comment": null, "summary": "System logs are a critical resource for monitoring and managing distributed systems, providing insights into failures and anomalous behavior. Traditional log analysis techniques, including template-based and sequence-driven approaches, often lose important semantic information or struggle with ambiguous log patterns. To address this, we present EnrichLog, a training-free, entry-based anomaly detection framework that enriches raw log entries with both corpus-specific and sample-specific knowledge. EnrichLog incorporates contextual information, including historical examples and reasoning derived from the corpus, to enable more accurate and interpretable anomaly detection. The framework leverages retrieval-augmented generation to integrate relevant contextual knowledge without requiring retraining. We evaluate EnrichLog on four large-scale system log benchmark datasets and compare it against five baseline methods. Our results show that EnrichLog consistently improves anomaly detection performance, effectively handles ambiguous log entries, and maintains efficient inference. Furthermore, incorporating both corpus- and sample-specific knowledge enhances model confidence and detection accuracy, making EnrichLog well-suited for practical deployments."}
{"id": "2512.13405", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.13405", "abs": "https://arxiv.org/abs/2512.13405", "authors": ["Frederik Johannes Zuiderveen Borgesius"], "title": "Improving Privacy Protection in the area of Behavioural Targeting", "comment": null, "summary": "This PhD thesis discusses how European law could improve privacy protection in the area of behavioural targeting. Behavioural targeting, also referred to as online profiling, involves monitoring people's online behaviour, and using the collected information to show people individually targeted advertisements. To protect privacy in the area of behavioural targeting, the EU lawmaker mainly relies on the consent requirement for the use of tracking technologies in the e-Privacy Directive, and on general data protection law. With informed consent requirements, the law aims to empower people to make choices in their best interests. But behavioural studies cast doubt on the effectiveness of the empowerment approach as a privacy protection measure. Many people click \"I agree\" to any statement that is presented to them. Therefore, to mitigate privacy problems such as chilling effects, this study argues for a combined approach of protecting and empowering the individual. Compared to the current approach, the lawmaker should focus more on protecting people. The PhD thesis is a legal study, but it also incorporates insights from other disciplines, such as computer science, behavioural economics, and media studies. This study is among the first to discuss the implications of behavioural research for European data protection policy. The topic of whether data protection law should apply to pseudonymous data is discussed in depth. The study contains a detailed analysis of the role of informed consent in data protection law, and gives much attention to the tension between protecting and empowering the individual within data protection law."}
{"id": "2512.12970", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.12970", "abs": "https://arxiv.org/abs/2512.12970", "authors": ["Paola Di Maio"], "title": "Towards Open Standards for Systemic Complexity in Digital Forensics", "comment": null, "summary": "The intersection of artificial intelligence (AI) and digital forensics (DF) is becoming increasingly complex, ubiquitous, and pervasive, with overlapping techniques and technologies being adopted in all types of scientific and technical inquiry. Despite incredible advances, forensic sciences are not exempt from errors and remain vulnerable to fallibility. To mitigate the limitations of errors in DF, the systemic complexity is identified and addressed with the adoption of human-readable artifacts and open standards. A DF AI model schema based on the state of the art is outlined."}
{"id": "2512.12011", "categories": ["cs.SI", "econ.GN"], "pdf": "https://arxiv.org/pdf/2512.12011", "abs": "https://arxiv.org/abs/2512.12011", "authors": ["Denise Gonzalez-Cruz", "Genesis Encarnacion", "Kaili Martinez-Beasley", "Robin Wilson", "Nicholas Arosemena", "Atilio Barreda", "Omayra Ortega", "Daniel A. Cruz"], "title": "Defunding Sexual Healthcare: A Topological Investigation of Resource Accessibility", "comment": null, "summary": "Government actions, such as the Medina v. Planned Parenthood South Atlantic Supreme Court ruling and the passage of the Big Beautiful Bill Act, have aimed to restrict or prohibit Medicaid funding for Planned Parenthood Healthcare Centers (PPHCs) at both the state and national levels. These funding cuts are particularly harmful in states like California, which has a large population of Medicaid users. This analysis focuses on the distribution of Planned Parenthood clinics and Federally Qualified Health Centers (FQHCs), which offer essential reproductive healthcare services including, but not limited to, abortions, birth control, HIV services, pregnancy testing and planning, STD testing and treatment, and cancer screenings. While expanded funding for FQHCs has been proposed as a solution, it fails to address the locational accessibility of Medicaid-funded health centers that provide sexual and reproductive care. To assess this issue, we analyze the proximity of data points representing California's PPHC and FQHC locations. Topological Data Analysis (TDA)-an approach that examines the shape and structure of data -- is used to detect disparities in reproductive and sexual healthcare coverage. To conduct data collection and visualization, we utilize R and Python. We apply an n-closest neighbor algorithm to examine distances between facilities and assess changes in travel time required to reach healthcare sites. We apply persistent homology to analyze current gaps across multiple scales in healthcare coverage and compare them to potential future gaps. Our findings aim to identify areas where access to care is most vulnerable and demonstrate how TDA can be used to analyze spatial inequalities in public health."}
{"id": "2512.13658", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13658", "abs": "https://arxiv.org/abs/2512.13658", "authors": ["Mohammadreza Molavi", "Mohammad Moein", "Mohammadreza Tavakoli", "Abdolali Faraji", "Stefan T. Mol", "Gábor Kismihók"], "title": "Embedding-Based Rankings of Educational Resources based on Learning Outcome Alignment: Benchmarking, Expert Validation, and Learner Performance", "comment": "Accepted for publication at the 16th International Conference on Learning Analytics & Knowledge (LAK 2026)", "summary": "As the online learning landscape evolves, the need for personalization is increasingly evident. Although educational resources are burgeoning, educators face challenges selecting materials that both align with intended learning outcomes and address diverse learner needs. Large Language Models (LLMs) are attracting growing interest for their potential to create learning resources that better support personalization, but verifying coverage of intended outcomes still requires human alignment review, which is costly and limits scalability. We propose a framework that supports the cost-effective automation of evaluating alignment between educational resources and intended learning outcomes. Using human-generated materials, we benchmarked LLM-based text-embedding models and found that the most accurate model (Voyage) achieved 79% accuracy in detecting alignment. We then applied the optimal model to LLM-generated resources and, via expert evaluation, confirmed that it reliably assessed correspondence to intended outcomes (83% accuracy). Finally, in a three-group experiment with 360 learners, higher alignment scores were positively related to greater learning performance, chi-squared(2, N = 360) = 15.39, p < 0.001. These findings show that embedding-based alignment scores can facilitate scalable personalization by confirming alignment with learning outcomes, which allows teachers to focus on tailoring content to diverse learner needs."}
{"id": "2512.13070", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13070", "abs": "https://arxiv.org/abs/2512.13070", "authors": ["Bizhe Bai", "Hongming Wu", "Peng Ye", "Tao Chen"], "title": "M-GRPO: Stabilizing Self-Supervised Reinforcement Learning for Large Language Models with Momentum-Anchored Policy Optimization", "comment": "7 pages, 5 figures,Accepted NeurIPS 2025 Workshop on Efficient Reasoning", "summary": "Self-supervised reinforcement learning (RL) presents a promising approach for enhancing the reasoning capabilities of Large Language Models (LLMs) without reliance on expensive human-annotated data. However, we find that existing methods suffer from a critical failure mode under long-horizon training: a \"policy collapse\" where performance precipitously degrades. We diagnose this instability and demonstrate that simply scaling the number of rollouts -- a common strategy to improve performance -- only delays, but does not prevent, this collapse. To counteract this instability, we first introduce M-GRPO (Momentum-Anchored Group Relative Policy Optimization), a framework that leverages a slowly evolving momentum model to provide a stable training target. In addition, we identify that this process is often accompanied by a rapid collapse in policy entropy, resulting in a prematurely confident and suboptimal policy. To specifically address this issue, we propose a second contribution: an adaptive filtering method based on the interquartile range (IQR) that dynamically prunes low-entropy trajectories, preserving essential policy diversity. Our extensive experiments on multiple reasoning benchmarks demonstrate that M-GRPO stabilizes the training process while the IQR filter prevents premature convergence. The combination of these two innovations leads to superior training stability and state-of-the-art performance."}
{"id": "2512.12048", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12048", "abs": "https://arxiv.org/abs/2512.12048", "authors": ["Muddsair Sharif", "Huseyin Seker"], "title": "Context-Aware Agentic Power Resources Optimisation in EV using Smart2ChargeApp", "comment": null, "summary": "This paper presents a novel context-sensitive multi\\-agent coordination for dynamic resource allocation (CAMAC-DRA) framework for optimizing smart electric vehicle (EV) charging ecosystems through the Smart2Charge application. The proposed system coordinates autonomous charging agents across networks of 250 EVs and 45 charging stations while adapting to dynamic environmental conditions through context-aware decision-making. Our multi-agent approach employs coordinated Deep Q\\-Networks integrated with Graph Neural Networks and attention mechanisms, processing 20 contextual features including weather patterns, traffic conditions, grid load fluctuations, and electricity pricing.The framework balances five ecosystem stakeholders i.e. EV users (25\\%), grid operators (20\\%), charging station operators (20\\%), fleet operators (20%), and environmental factors (15\\%) through weighted coordination mechanisms and consensus protocols. Comprehensive validation using real-world datasets containing 441,077 charging transactions demonstrates superior performance compared to baseline algorithms including DDPG, A3C, PPO, and GNN approaches. The CAMAC\\-DRA framework achieves 92\\% coordination success rate, 15\\% energy efficiency improvement, 10\\% cost reduction, 20% grid strain decrease, and \\2.3x faster convergence while maintaining 88\\% training stability and 85\\% sample efficiency. Real-world validation confirms commercial viability with Net Present Cost of -\\$122,962 and 69\\% cost reduction through renewable energy integration. The framework's unique contribution lies in developing context-aware multi-stakeholder coordination that successfully balances competing objectives while adapting to real-time variables, positioning it as a breakthrough solution for intelligent EV charging coordination and sustainable transportation electrification."}
{"id": "2512.12970", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.12970", "abs": "https://arxiv.org/abs/2512.12970", "authors": ["Paola Di Maio"], "title": "Towards Open Standards for Systemic Complexity in Digital Forensics", "comment": null, "summary": "The intersection of artificial intelligence (AI) and digital forensics (DF) is becoming increasingly complex, ubiquitous, and pervasive, with overlapping techniques and technologies being adopted in all types of scientific and technical inquiry. Despite incredible advances, forensic sciences are not exempt from errors and remain vulnerable to fallibility. To mitigate the limitations of errors in DF, the systemic complexity is identified and addressed with the adoption of human-readable artifacts and open standards. A DF AI model schema based on the state of the art is outlined."}
{"id": "2512.13102", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13102", "abs": "https://arxiv.org/abs/2512.13102", "authors": ["Rajeev Bhatt Ambati", "Tianyi Niu", "Aashu Singh", "Shlok Mishra", "Shashank Srivastava", "Snigdha Chaturvedi"], "title": "Socratic Students: Teaching Language Models to Learn by Asking Questions", "comment": null, "summary": "Large Language Models (LLMs) excel at static interactions, where they answer user queries by retrieving knowledge encoded in their parameters. However, in many real-world settings, such as educational tutoring or medical assistance, relevant information is not directly available and must be actively acquired through dynamic interactions. An interactive agent would recognize its own uncertainty, ask targeted questions, and retain new knowledge efficiently. Prior work has primarily explored effective ways for a teacher to instruct the student, where the teacher identifies student gaps and provides guidance. In this work, we shift the focus to the student and investigate effective strategies to actively query the teacher in seeking useful information. Across math and coding benchmarks, where baseline student models begin with near-zero performance, we show that student-led approaches consistently yield absolute Pass@k improvements of at least 0.5 over static baselines. To improve question quality, we train students using Direct Preference Optimization (DPO) with guidance from either self or stronger students. We find that this guided training enables smaller models to learn how to ask better questions, further enhancing learning efficiency."}
{"id": "2512.12059", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12059", "abs": "https://arxiv.org/abs/2512.12059", "authors": ["Luke Bhan", "Hanyu Zhang", "Andrew Gordon Wilson", "Michael W. Mahoney", "Chuck Arvin"], "title": "The Forecast Critic: Leveraging Large Language Models for Poor Forecast Identification", "comment": "Presented at AAAI 2026 AI4TS workshop and AABA4ET workshop", "summary": "Monitoring forecasting systems is critical for customer satisfaction, profitability, and operational efficiency in large-scale retail businesses. We propose The Forecast Critic, a system that leverages Large Language Models (LLMs) for automated forecast monitoring, taking advantage of their broad world knowledge and strong ``reasoning'' capabilities. As a prerequisite for this, we systematically evaluate the ability of LLMs to assess time series forecast quality, focusing on three key questions. (1) Can LLMs be deployed to perform forecast monitoring and identify obviously unreasonable forecasts? (2) Can LLMs effectively incorporate unstructured exogenous features to assess what a reasonable forecast looks like? (3) How does performance vary across model sizes and reasoning capabilities, measured across state-of-the-art LLMs? We present three experiments, including on both synthetic and real-world forecasting data. Our results show that LLMs can reliably detect and critique poor forecasts, such as those plagued by temporal misalignment, trend inconsistencies, and spike errors. The best-performing model we evaluated achieves an F1 score of 0.88, somewhat below human-level performance (F1 score: 0.97). We also demonstrate that multi-modal LLMs can effectively incorporate unstructured contextual signals to refine their assessment of the forecast. Models correctly identify missing or spurious promotional spikes when provided with historical context about past promotions (F1 score: 0.84). Lastly, we demonstrate that these techniques succeed in identifying inaccurate forecasts on the real-world M5 time series dataset, with unreasonable forecasts having an sCRPS at least 10% higher than that of reasonable forecasts. These findings suggest that LLMs, even without domain-specific fine-tuning, may provide a viable and scalable option for automated forecast monitoring and evaluation."}
{"id": "2512.13481", "categories": ["cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.13481", "abs": "https://arxiv.org/abs/2512.13481", "authors": ["Ojas Pungalia", "Rashi Upadhyay", "Abhishek Mishra", "Abhiram H", "Tejasvi Alladi", "Sujan Yenuganti", "Dhruv Kumar"], "title": "neuralFOMO: Can LLMs Handle Being Second Best? Measuring Envy-Like Preferences in Multi-Agent Settings", "comment": "Under Review", "summary": "Envy is a common human behavior that shapes competitiveness and can alter outcomes in team settings. As large language models (LLMs) increasingly act on behalf of humans in collaborative and competitive workflows, there is a pressing need to evaluate whether and under what conditions they exhibit envy-like preferences. In this paper, we test whether LLMs show envy-like behavior toward each other. We considered two scenarios: (1) A point allocation game that tests whether a model tries to win over its peer. (2) A workplace setting observing behaviour when recognition is unfair. Our findings reveal consistent evidence of envy-like patterns in certain LLMs, with large variation across models and contexts. For instance, GPT-5-mini and Claude-3.7-Sonnet show a clear tendency to pull down the peer model to equalize outcomes, whereas Mistral-Small-3.2-24B instead focuses on maximizing its own individual gains. These results highlight the need to consider competitive dispositions as a safety and design factor in LLM-based multi-agent systems."}
{"id": "2512.13131", "categories": ["cs.AI", "cs.CV", "cs.GR", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.13131", "abs": "https://arxiv.org/abs/2512.13131", "authors": ["Xin Guo", "Yifan Zhao", "Jia Li"], "title": "Towards Unified Co-Speech Gesture Generation via Hierarchical Implicit Periodicity Learning", "comment": "IEEE Transactions on Image Processing", "summary": "Generating 3D-based body movements from speech shows great potential in extensive downstream applications, while it still suffers challenges in imitating realistic human movements. Predominant research efforts focus on end-to-end generation schemes to generate co-speech gestures, spanning GANs, VQ-VAE, and recent diffusion models. As an ill-posed problem, in this paper, we argue that these prevailing learning schemes fail to model crucial inter- and intra-correlations across different motion units, i.e. head, body, and hands, thus leading to unnatural movements and poor coordination. To delve into these intrinsic correlations, we propose a unified Hierarchical Implicit Periodicity (HIP) learning approach for audio-inspired 3D gesture generation. Different from predominant research, our approach models this multi-modal implicit relationship by two explicit technique insights: i) To disentangle the complicated gesture movements, we first explore the gesture motion phase manifolds with periodic autoencoders to imitate human natures from realistic distributions while incorporating non-period ones from current latent states for instance-level diversities. ii) To model the hierarchical relationship of face motions, body gestures, and hand movements, driving the animation with cascaded guidance during learning. We exhibit our proposed approach on 3D avatars and extensive experiments show our method outperforms the state-of-the-art co-speech gesture generation methods by both quantitative and qualitative evaluations. Code and models will be publicly available."}
{"id": "2512.12088", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12088", "abs": "https://arxiv.org/abs/2512.12088", "authors": ["S. R. Eshwar", "Aniruddha Mukherjee", "Kintan Saha", "Krishna Agarwal", "Gugan Thoppe", "Aditya Gopalan", "Gal Dalal"], "title": "Reliable Policy Iteration: Performance Robustness Across Architecture and Environment Perturbations", "comment": null, "summary": "In a recent work, we proposed Reliable Policy Iteration (RPI), that restores policy iteration's monotonicity-of-value-estimates property to the function approximation setting. Here, we assess the robustness of RPI's empirical performance on two classical control tasks -- CartPole and Inverted Pendulum -- under changes to neural network and environmental parameters. Relative to DQN, Double DQN, DDPG, TD3, and PPO, RPI reaches near-optimal performance early and sustains this policy as training proceeds. Because deep RL methods are often hampered by sample inefficiency, training instability, and hyperparameter sensitivity, our results highlight RPI's promise as a more reliable alternative."}
{"id": "2512.13142", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.13142", "abs": "https://arxiv.org/abs/2512.13142", "authors": ["Anika Sharma", "Malavika Mampally", "Chidaksh Ravuru", "Kandyce Brennan", "Neil Gaikwad"], "title": "Can AI Understand What We Cannot Say? Measuring Multilevel Alignment Through Abortion Stigma Across Cognitive, Interpersonal, and Structural Levels", "comment": null, "summary": "As large language models increasingly mediate stigmatized health decisions, their capacity to genuinely understand complex psychological and physiological phenomena remains poorly evaluated. Can AI understand what we cannot say? We investigate whether LLMs coherently represent abortion stigma across the cognitive, interpersonal, and structural levels where it operates. We systematically tested 627 demographically diverse personas across five leading LLMs using the validated Individual Level Abortion Stigma Scale (ILAS). Our multilevel analysis examined whether models coherently represent stigma at the cognitive level (self-judgment), interpersonal level (anticipated judgment and isolation), and structural level (community condemnation and disclosure patterns), as well as overall stigma. Models fail tests of genuine understanding across all levels. They overestimate interpersonal stigma while underestimating cognitive stigma, assume uniform community condemnation, introduce demographic biases absent from human validation data, miss the empirically validated stigma-secrecy relationship, and contradict themselves within theoretical constructs. These patterns reveal that current alignment approaches ensure appropriate language but not coherent multilevel understanding. This work provides empirical evidence that current LLMs lack coherent multilevel understanding of psychological and physiological constructs. AI safety in high-stakes contexts demands new approaches to design (multilevel coherence), evaluation (continuous auditing), governance and regulation (mandatory audits, accountability, deployment restrictions), and AI literacy in domains where understanding what people cannot say determines whether support helps or harms."}
{"id": "2512.12105", "categories": ["cs.CY", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.12105", "abs": "https://arxiv.org/abs/2512.12105", "authors": ["Andreia dos Santos Sachete", "Alba Valeria de SantAnna de Freitas Loiola", "Fabio Diniz Rossi", "Jose Valdeni de Lima", "Raquel Salcedo Gomes"], "title": "Beyond right or wrong : towards redefining adaptive learning indicators in virtual learning environments", "comment": "20 pages, 1 figure, 1 table", "summary": "Student learning development must involve more than just correcting or incorrect questions. However, most adaptive learning methods in Virtual Learning Environments are based on whether the student's response is incorrect or correct. This perspective is limited in assessing the student's learning level, as it does not consider other elements that can be crucial in this process. The objective of this work is to conduct a Systematic Literature Review (SLR) to elucidate which learning indicators influence student learning and which can be implemented in a VLE to assist in adaptive learning. The works selected and filtered by qualitative assessment reveal a comprehensive approach to assessing different aspects of the learning in virtual environments, such as motivation, emotions, physiological responses, brain imaging, and the students' prior knowledge. The discussion of these new indicators allows adaptive technology developers to implement more appropriate solutions to students' realities, resulting in more complete training."}
{"id": "2512.13154", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13154", "abs": "https://arxiv.org/abs/2512.13154", "authors": ["Emre Can Acikgoz", "Jinoh Oh", "Joo Hyuk Jeon", "Jie Hao", "Heng Ji", "Dilek Hakkani-Tür", "Gokhan Tur", "Xiang Li", "Chengyuan Ma", "Xing Fan"], "title": "MAC: A Multi-Agent Framework for Interactive User Clarification in Multi-turn Conversations", "comment": null, "summary": "Conversational agents often encounter ambiguous user requests, requiring an effective clarification to successfully complete tasks. While recent advancements in real-world applications favor multi-agent architectures to manage complex conversational scenarios efficiently, ambiguity resolution remains a critical and underexplored challenge--particularly due to the difficulty of determining which agent should initiate a clarification and how agents should coordinate their actions when faced with uncertain or incomplete user input. The fundamental questions of when to interrupt a user and how to formulate the optimal clarification query within the most optimal multi-agent settings remain open. In this paper, we propose MAC (Multi-Agent Clarification), an interactive multi-agent framework specifically optimized to resolve user ambiguities by strategically managing clarification dialogues. We first introduce a novel taxonomy categorizing user ambiguities to systematically guide clarification strategies. Then, we present MAC that autonomously coordinates multiple agents to interact synergistically with users. Empirical evaluations on MultiWOZ 2.4 demonstrate that enabling clarification at both levels increases task success rate 7.8\\% (54.5 to 62.3) and reduces the average number of dialogue turns (6.53 to 4.86) by eliciting all required user information up front and minimizing repetition. Our findings highlight the importance of active user interaction and role-aware clarification for more reliable human-agent communication."}
{"id": "2512.12109", "categories": ["cs.CY", "cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2512.12109", "abs": "https://arxiv.org/abs/2512.12109", "authors": ["Allen Daniel Sunny"], "title": "A neuro-symbolic framework for accountability in public-sector AI", "comment": "Master's thesis, University of Maryland, College Park (2025)", "summary": "Automated eligibility systems increasingly determine access to essential public benefits, but the explanations they generate often fail to reflect the legal rules that authorize those decisions. This thesis develops a legally grounded explainability framework that links system-generated decision justifications to the statutory constraints of CalFresh, California's Supplemental Nutrition Assistance Program. The framework combines a structured ontology of eligibility requirements derived from the state's Manual of Policies and Procedures (MPP), a rule extraction pipeline that expresses statutory logic in a verifiable formal representation, and a solver-based reasoning layer to evaluate whether the explanation aligns with governing law. Case evaluations demonstrate the framework's ability to detect legally inconsistent explanations, highlight violated eligibility rules, and support procedural accountability by making the basis of automated determinations traceable and contestable."}
{"id": "2512.13159", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13159", "abs": "https://arxiv.org/abs/2512.13159", "authors": ["Emre Can Acikgoz", "Jinoh Oh", "Jie Hao", "Joo Hyuk Jeon", "Heng Ji", "Dilek Hakkani-Tür", "Gokhan Tur", "Xiang Li", "Chengyuan Ma", "Xing Fan"], "title": "SpeakRL: Synergizing Reasoning, Speaking, and Acting in Language Models with Reinforcement Learning", "comment": null, "summary": "Effective human-agent collaboration is increasingly prevalent in real-world applications. Current trends in such collaborations are predominantly unidirectional, with users providing instructions or posing questions to agents, where agents respond directly without seeking necessary clarifications or confirmations. However, the evolving capabilities of these agents require more proactive engagement, where agents should dynamically participate in conversations to clarify user intents, resolve ambiguities, and adapt to changing circumstances. Existing prior work under-utilize the conversational capabilities of language models (LMs), thereby optimizing agents as better followers rather than effective speakers. In this work, we introduce SpeakRL, a reinforcement learning (RL) method that enhances agents' conversational capabilities by rewarding proactive interactions with users, such as asking right clarification questions when necessary. To support this, we curate SpeakER, a synthetic dataset that includes diverse scenarios from task-oriented dialogues, where tasks are resolved through interactive clarification questions. We present a systematic analysis of reward design for conversational proactivity and propose a principled reward formulation for teaching agents to balance asking with acting. Empirical evaluations demonstrate that our approach achieves a 20.14% absolute improvement in task completion over base models without increasing conversation turns even surpassing even much larger proprietary models, demonstrating the promise of clarification-centric user-agent interactions."}
{"id": "2512.12175", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12175", "abs": "https://arxiv.org/abs/2512.12175", "authors": ["Haoyang Chen", "Richong Zhang", "Junfan Chen"], "title": "Rethinking Label Consistency of In-Context Learning: An Implicit Transductive Label Propagation Perspective", "comment": null, "summary": "Large language models (LLMs) perform in-context learning (ICL) with minimal supervised examples, which benefits various natural language processing (NLP) tasks. One of the critical research focus is the selection of prompt demonstrations. Current approaches typically employ retrieval models to select the top-K most semantically similar examples as demonstrations. However, we argue that existing methods are limited since the label consistency is not guaranteed during demonstration selection. Our cognition derives from the Bayesian view of ICL and our rethinking of ICL from the transductive label propagation perspective. We treat ICL as a transductive learning method and incorporate latent concepts from Bayesian view and deduce that similar demonstrations guide the concepts of query, with consistent labels serving as estimates. Based on this understanding, we establish a label propagation framework to link label consistency with propagation error bounds. To model label consistency, we propose a data synthesis method, leveraging both semantic and label information, and use TopK sampling with Synthetic Data (TopK-SD) to acquire demonstrations with consistent labels. TopK-SD outperforms original TopK sampling on multiple benchmarks. Our work provides a new perspective for understanding the working mechanisms within ICL."}
{"id": "2512.13168", "categories": ["cs.AI", "cs.CE", "cs.IR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.13168", "abs": "https://arxiv.org/abs/2512.13168", "authors": ["Haoyu Dong", "Pengkun Zhang", "Yan Gao", "Xuanyu Dong", "Yilin Cheng", "Mingzhe Lu", "Adina Yakefu", "Shuxin Zheng"], "title": "Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows", "comment": null, "summary": "We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management.\n  We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work.\n  We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents."}
{"id": "2512.12177", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12177", "abs": "https://arxiv.org/abs/2512.12177", "authors": ["Aydin Ayanzadeh", "Tim Oates"], "title": "Floorplan2Guide: LLM-Guided Floorplan Parsing for BLV Indoor Navigation", "comment": "Accepted for publication in the proceedings of the IEEE International Conference on Big Data (IEEE BigData 2025)", "summary": "Indoor navigation remains a critical challenge for people with visual impairments. The current solutions mainly rely on infrastructure-based systems, which limit their ability to navigate safely in dynamic environments. We propose a novel navigation approach that utilizes a foundation model to transform floor plans into navigable knowledge graphs and generate human-readable navigation instructions. Floorplan2Guide integrates a large language model (LLM) to extract spatial information from architectural layouts, reducing the manual preprocessing required by earlier floorplan parsing methods. Experimental results indicate that few-shot learning improves navigation accuracy in comparison to zero-shot learning on simulated and real-world evaluations. Claude 3.7 Sonnet achieves the highest accuracy among the evaluated models, with 92.31%, 76.92%, and 61.54% on the short, medium, and long routes, respectively, under 5-shot prompting of the MP-1 floor plan. The success rate of graph-based spatial structure is 15.4% higher than that of direct visual reasoning among all models, which confirms that graphical representation and in-context learning enhance navigation performance and make our solution more precise for indoor navigation of Blind and Low Vision (BLV) users."}
{"id": "2512.13240", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13240", "abs": "https://arxiv.org/abs/2512.13240", "authors": ["Zihui Zhao", "Zechang Li"], "title": "Reflective Preference Optimization (RPO): Enhancing On-Policy Alignment via Hint-Guided Reflection", "comment": null, "summary": "Direct Preference Optimization (DPO) has emerged as a lightweight and effective alternative to Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning with AI Feedback (RLAIF) for aligning large language and vision-language models. However, the standard DPO formulation, in which both the chosen and rejected responses are generated by the same policy, suffers from a weak learning signal because the two responses often share similar errors and exhibit small Kullback-Leibler (KL) divergence. This leads to slow and unstable convergence. To address this limitation, we introduce Reflective Preference Optimization (RPO), a new framework that incorporates hint-guided reflection into the DPO paradigm. RPO uses external models to identify hallucination sources and generate concise reflective hints, enabling the construction of on-policy preference pairs with stronger contrastiveness and clearer preference signals. We theoretically show that conditioning on hints increases the expected preference margin through mutual information and improves sample efficiency while remaining within the policy distribution family. Empirically, RPO achieves superior alignment with fewer training samples and iterations, substantially reducing hallucination rates and delivering state-of-the-art performance across multimodal benchmarks."}
{"id": "2512.12182", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12182", "abs": "https://arxiv.org/abs/2512.12182", "authors": ["Xinyu Gao"], "title": "TA-KAND: Two-stage Attention Triple Enhancement and U-KAN based Diffusion For Few-shot Knowledge Graph Completion", "comment": null, "summary": "Knowledge Graphs (KGs), thanks to their concise and efficient triple-based structure, have been widely applied in intelligent question answering, recommender systems and other domains. However, the heterogeneous and multifaceted nature of real-world data inevitably renders the distribution of relations long-tailed, making it crucial to complete missing facts with limited samples. Previous studies mainly based on metric matching or meta learning, yet they either fail to fully exploit neighborhood information in graph or overlook the distributional characteristics of contrastive signals. In this paper, we re-examine the problem from a perspective of generative representation and propose a few-shot knowledge graph completion framework that integrates two-stage attention triple enhancer with U-KAN based diffusion model. Extensive experiments on two public datasets show that our method achieve new state-of-the-art results."}
{"id": "2512.13297", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13297", "abs": "https://arxiv.org/abs/2512.13297", "authors": ["Zhenghao Zhu", "Chuxue Cao", "Sirui Han", "Yuanfeng Song", "Xing Chen", "Caleb Chen Cao", "Yike Guo"], "title": "MedInsightBench: Evaluating Medical Analytics Agents Through Multi-Step Insight Discovery in Multimodal Medical Data", "comment": null, "summary": "In medical data analysis, extracting deep insights from complex, multi-modal datasets is essential for improving patient care, increasing diagnostic accuracy, and optimizing healthcare operations. However, there is currently a lack of high-quality datasets specifically designed to evaluate the ability of large multi-modal models (LMMs) to discover medical insights. In this paper, we introduce MedInsightBench, the first benchmark that comprises 332 carefully curated medical cases, each annotated with thoughtfully designed insights. This benchmark is intended to evaluate the ability of LMMs and agent frameworks to analyze multi-modal medical image data, including posing relevant questions, interpreting complex findings, and synthesizing actionable insights and recommendations. Our analysis indicates that existing LMMs exhibit limited performance on MedInsightBench, which is primarily attributed to their challenges in extracting multi-step, deep insights and the absence of medical expertise. Therefore, we propose MedInsightAgent, an automated agent framework for medical data analysis, composed of three modules: Visual Root Finder, Analytical Insight Agent, and Follow-up Question Composer. Experiments on MedInsightBench highlight pervasive challenges and demonstrate that MedInsightAgent can improve the performance of general LMMs in medical data insight discovery."}
{"id": "2512.12187", "categories": ["cs.CY", "cs.HC", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2512.12187", "abs": "https://arxiv.org/abs/2512.12187", "authors": ["David Gamba", "Daniel M. Romero", "Grant Schoenebeck"], "title": "The Ideological Turing Test for Moderation of Outgroup Affective Animosity", "comment": "32 pages", "summary": "Rising animosity toward ideological opponents poses critical societal challenges. We introduce and test the Ideological Turing Test, a gamified framework requiring participants to adopt and defend opposing viewpoints, to reduce affective animosity and affective polarization.\n  We conducted a mixed-design experiment ($N = 203$) with four conditions: modality (debate/writing) x perspective-taking (Own/Opposite side). Participants engaged in structured interactions defending assigned positions, with outcomes judged by peers. We measured changes in affective animosity and ideological position immediately post-intervention and at 2-6 week follow-up.\n  Perspective-taking reduced out-group animosity and ideological polarization. However, effects differed by modality (writing vs. debate) and over time. For affective animosity, writing from the opposite perspective yielded the largest immediate reduction ($Δ=+0.45$ SD), but the effect was not detectable at the 4-6 week follow-up. In contrast, the debate modality maintained a statistically significant reduction in animosity immediately after and at follow-up ($Δ=+0.37$ SD). For ideological position, adopting the opposite perspective led to significant immediate movement across modalities (writing: $Δ=+0.91$ SD; debate: $Δ=+0.51$ SD), and these changes persisted at follow-up. Judged performance (winning) did not moderate these effects, and willingness to re-participate was similar across conditions (~20-36%).\n  These findings challenge assumptions about adversarial methods, revealing distinct temporal patterns: non-adversarial engagement fosters short-term empathy gains, while cognitive engagement through debate sustains affective benefits. The Ideological Turing Test demonstrates potential as a scalable tool for reducing polarization, particularly when combining perspective-taking with reflective adversarial interactions."}
{"id": "2512.13323", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13323", "abs": "https://arxiv.org/abs/2512.13323", "authors": ["Árpád Pándy", "Róbert Lakatos", "András Hajdu"], "title": "Error-Driven Prompt Optimization for Arithmetic Reasoning", "comment": null, "summary": "Recent advancements in artificial intelligence have sparked interest in industrial agents capable of supporting analysts in regulated sectors, such as finance and healthcare, within tabular data workflows. A key capability for such systems is performing accurate arithmetic operations on structured data while ensuring sensitive information never leaves secure, on-premises environments. Here, we introduce an error-driven optimization framework for arithmetic reasoning that enhances a Code Generation Agent (CGA), specifically applied to on-premises small language models (SLMs). Through a systematic evaluation of a leading SLM (Qwen3 4B), we find that while the base model exhibits fundamental limitations in arithmetic tasks, our proposed error-driven method, which clusters erroneous predictions to refine prompt-rules iteratively, dramatically improves performance, elevating the model's accuracy to 70.8\\%. Our results suggest that developing reliable, interpretable, and industrially deployable AI assistants can be achieved not only through costly fine-tuning but also via systematic, error-driven prompt optimization, enabling small models to surpass larger language models (GPT-3.5 Turbo) in a privacy-compliant manner."}
{"id": "2512.12212", "categories": ["cs.CY", "econ.GN", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.12212", "abs": "https://arxiv.org/abs/2512.12212", "authors": ["Elizabeth Irenne Yuwono", "Dian Tjondronegoro", "Shawn Hunter", "Amber Marshall"], "title": "Anticipatory Governance in Data-Constrained Environments: A Predictive Simulation Framework for Digital Financial Inclusion", "comment": "28 pages, 3 figures", "summary": "Financial exclusion remains a major barrier to digital public service delivery in resource-constrained and archipelagic nations. Traditional policy evaluations rely on retrospective data, limiting the ex-ante intelligence needed for agile resource allocation. This study introduces a predictive simulation framework to support anticipatory governance within government information systems. Using the UNCDF Pacific Digital Economy dataset of 10,108 respondents, we apply a three-stage pipeline: descriptive profiling, interpretable machine learning, and scenario simulation to forecast outcomes of digital financial literacy interventions before deployment. Leveraging cross-sectional structural associations, the framework projects intervention scenarios as prioritization heuristics rather than causal estimates. A transparent linear regression model with R-squared of 95.9 identifies modifiable policy levers. Simulations indicate that foundational digital capabilities such as device access and expense tracking yield the highest projected gains, up to 5.5 percent, outperforming attitudinal nudges. The model enables precision targeting, highlighting young female caregivers as high-leverage responders while flagging non-responders such as urban professionals to prevent resource misallocation. This research demonstrates how static survey data can be repurposed into actionable policy intelligence, offering a scalable and evidence-based blueprint for embedding predictive analytics into public-sector decision-support systems to advance equity-focused digital governance."}
{"id": "2512.13374", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13374", "abs": "https://arxiv.org/abs/2512.13374", "authors": ["Francesca Da Ros", "Luca Di Gaspero", "Kevin Roitero"], "title": "Behavior and Representation in Large Language Models for Combinatorial Optimization: From Feature Extraction to Algorithm Selection", "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have opened new perspectives for automation in optimization. While several studies have explored how LLMs can generate or solve optimization models, far less is understood about what these models actually learn regarding problem structure or algorithmic behavior. This study investigates how LLMs internally represent combinatorial optimization problems and whether such representations can support downstream decision tasks. We adopt a twofold methodology combining direct querying, which assesses LLM capacity to explicitly extract instance features, with probing analyses that examine whether such information is implicitly encoded within their hidden layers. The probing framework is further extended to a per-instance algorithm selection task, evaluating whether LLM-derived representations can predict the best-performing solver. Experiments span four benchmark problems and three instance representations. Results show that LLMs exhibit moderate ability to recover feature information from problem instances, either through direct querying or probing. Notably, the predictive power of LLM hidden-layer representations proves comparable to that achieved through traditional feature extraction, suggesting that LLMs capture meaningful structural information relevant to optimization performance."}
{"id": "2512.12225", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12225", "abs": "https://arxiv.org/abs/2512.12225", "authors": ["Laha Ale"], "title": "A Geometric Theory of Cognition", "comment": null, "summary": "Human cognition spans perception, memory, intuitive judgment, deliberative reasoning, action selection, and social inference, yet these capacities are often explained through distinct computational theories. Here we present a unified mathematical framework in which diverse cognitive processes emerge from a single geometric principle. We represent the cognitive state as a point on a differentiable manifold endowed with a learned Riemannian metric that encodes representational constraints, computational costs, and structural relations among cognitive variables. A scalar cognitive potential combines predictive accuracy, structural parsimony, task utility, and normative or logical requirements. Cognition unfolds as the Riemannian gradient flow of this potential, providing a universal dynamical law from which a broad range of psychological phenomena arise. Classical dual-process effects--rapid intuitive responses and slower deliberative reasoning--emerge naturally from metric-induced anisotropies that generate intrinsic time-scale separations and geometric phase transitions, without invoking modular or hybrid architectures. We derive analytical conditions for these regimes and demonstrate their behavioural signatures through simulations of canonical cognitive tasks. Together, these results establish a geometric foundation for cognition and suggest guiding principles for the development of more general and human-like artificial intelligence systems."}
{"id": "2512.13399", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13399", "abs": "https://arxiv.org/abs/2512.13399", "authors": ["Sitao Cheng", "Tianle Li", "Xuhan Huang", "Xunjian Yin", "Difan Zou"], "title": "Differentiable Evolutionary Reinforcement Learning", "comment": "Work in Progress. We release our code and model at https://github.com/sitaocheng/DERL", "summary": "The design of effective reward functions presents a central and often arduous challenge in reinforcement learning (RL), particularly when developing autonomous agents for complex reasoning tasks. While automated reward optimization approaches exist, they typically rely on derivative-free evolutionary heuristics that treat the reward function as a black box, failing to capture the causal relationship between reward structure and task performance. To bridge this gap, we propose Differentiable Evolutionary Reinforcement Learning (DERL), a bilevel framework that enables the autonomous discovery of optimal reward signals. In DERL, a Meta-Optimizer evolves a reward function (i.e., Meta-Reward) by composing structured atomic primitives, guiding the training of an inner-loop policy. Crucially, unlike previous evolution, DERL is differentiable in its metaoptimization: it treats the inner-loop validation performance as a signal to update the Meta-Optimizer via reinforcement learning. This allows DERL to approximate the \"meta-gradient\" of task success, progressively learning to generate denser and more actionable feedback. We validate DERL across three distinct domains: robotic agent (ALFWorld), scientific simulation (ScienceWorld), and mathematical reasoning (GSM8k, MATH). Experimental results show that DERL achieves state-of-the-art performance on ALFWorld and ScienceWorld, significantly outperforming methods relying on heuristic rewards, especially in out-of-distribution scenarios. Analysis of the evolutionary trajectory demonstrates that DERL successfully captures the intrinsic structure of tasks, enabling selfimproving agent alignment without human intervention."}
{"id": "2512.12260", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2512.12260", "abs": "https://arxiv.org/abs/2512.12260", "authors": ["Ege Atacan Doğan", "Peter F. Patel-Schneider"], "title": "A Multi-Axial Mindset for Ontology Design Lessons from Wikidata's Polyhierarchical Structure", "comment": null, "summary": "Traditional ontology design emphasizes disjoint and exhaustive top-level distinctions such as continuant vs. occurrent, abstract vs. concrete, or type vs. instance. These distinctions are used to structure unified hierarchies where every entity is classified under a single upper-level category. Wikidata, by contrast, does not enforce a singular foundational taxonomy. Instead, it accommodates multiple classification axes simultaneously under the shared root class entity. This paper analyzes the structural implications of Wikidata's polyhierarchical and multi-axial design. The Wikidata architecture enables a scalable and modular approach to ontology construction, especially suited to collaborative and evolving knowledge graphs."}
{"id": "2512.13481", "categories": ["cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.13481", "abs": "https://arxiv.org/abs/2512.13481", "authors": ["Ojas Pungalia", "Rashi Upadhyay", "Abhishek Mishra", "Abhiram H", "Tejasvi Alladi", "Sujan Yenuganti", "Dhruv Kumar"], "title": "neuralFOMO: Can LLMs Handle Being Second Best? Measuring Envy-Like Preferences in Multi-Agent Settings", "comment": "Under Review", "summary": "Envy is a common human behavior that shapes competitiveness and can alter outcomes in team settings. As large language models (LLMs) increasingly act on behalf of humans in collaborative and competitive workflows, there is a pressing need to evaluate whether and under what conditions they exhibit envy-like preferences. In this paper, we test whether LLMs show envy-like behavior toward each other. We considered two scenarios: (1) A point allocation game that tests whether a model tries to win over its peer. (2) A workplace setting observing behaviour when recognition is unfair. Our findings reveal consistent evidence of envy-like patterns in certain LLMs, with large variation across models and contexts. For instance, GPT-5-mini and Claude-3.7-Sonnet show a clear tendency to pull down the peer model to equalize outcomes, whereas Mistral-Small-3.2-24B instead focuses on maximizing its own individual gains. These results highlight the need to consider competitive dispositions as a safety and design factor in LLM-based multi-agent systems."}
{"id": "2512.12288", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12288", "abs": "https://arxiv.org/abs/2512.12288", "authors": ["Mahule Roy", "Guillaume Lambard"], "title": "Quantum-Aware Generative AI for Materials Discovery: A Framework for Robust Exploration Beyond DFT Biases", "comment": "33 pages", "summary": "Conventional generative models for materials discovery are predominantly trained and validated using data from Density Functional Theory (DFT) with approximate exchange-correlation functionals. This creates a fundamental bottleneck: these models inherit DFT's systematic failures for strongly correlated systems, leading to exploration biases and an inability to discover materials where DFT predictions are qualitatively incorrect. We introduce a quantum-aware generative AI framework that systematically addresses this limitation through tight integration of multi-fidelity learning and active validation. Our approach employs a diffusion-based generator conditioned on quantum-mechanical descriptors and a validator using an equivariant neural network potential trained on a hierarchical dataset spanning multiple levels of theory (PBE, SCAN, HSE06, CCSD(T)). Crucially, we implement a robust active learning loop that quantifies and targets the divergence between low- and high-fidelity predictions. We conduct comprehensive ablation studies to deconstruct the contribution of each component, perform detailed failure mode analysis, and benchmark our framework against state-of-the-art generative models (CDVAE, GNoME, DiffCSP) across several challenging material classes. Our results demonstrate significant practical gains: a 3-5x improvement in successfully identifying potentially stable candidates in high-divergence regions (e.g., correlated oxides) compared to DFT-only baselines, while maintaining computational feasibility. This work provides a rigorous, transparent framework for extending the effective search space of computational materials discovery beyond the limitations of single-fidelity models."}
{"id": "2512.13505", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13505", "abs": "https://arxiv.org/abs/2512.13505", "authors": ["Henry Prakken", "Wijnand van Woerkom"], "title": "Defending the Hierarchical Result Models of Precedential Constraint", "comment": "This is the long version of a paper with the same title presented at the 38th International Conference on Legal Knowledge and Information Systems", "summary": "In recent years, hierarchical case-based-reasoning models of precedential constraint have been proposed. In various papers, Trevor Bench-Capon criticised these models on the grounds that they would give incorrect outcomes in some cases. In particular, the models would not account for the possibility that intermediate factors are established with different strengths by different base-level factors. In this paper we respond to these criticisms for van Woerkom's result-based hierarchical models. We argue that in some examples Bench-Capon seems to interpret intermediate factors as dimensions, and that applying van Woerkom's dimension-based version of the hierarchical result model to these examples avoids Bench-Capon's criticisms."}
{"id": "2512.12306", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.12306", "abs": "https://arxiv.org/abs/2512.12306", "authors": ["Amir Yunus", "Peng Rend Gay", "Oon Teng Lee"], "title": "From Co-Design to Metacognitive Laziness: Evaluating Generative AI in Vocational Education", "comment": "108 pages", "summary": "This study examines the development and deployment of a Generative AI proof-of-concept (POC) designed to support lecturers in a vocational education setting in Singapore. Employing a user-centred, mixed-methods design process, we co-developed an AI chatbot with lecturers to address recurring instructional challenges during exam preparation, specifically managing repetitive questions and scaling feedback delivery. The POC achieved its primary operational goals: lecturers reported streamlined workflows, reduced cognitive load, and observed improved student confidence in navigating course content. However, the deployment yielded unexpected insights into student learning behaviours. Despite enhanced teaching processes, performance data revealed no significant improvement in overall student assessment outcomes. Deep analysis of interaction logs identified concerning patterns, including self-efficacy-driven dependency, \"metacognitive laziness\" (cognitive offloading), and divergent usage strategies. While high-ability students leveraged the tool for strategic verification, low-ability students frequently used it to bypass cognitive effort, potentially exacerbating performance gaps. These findings suggest that Generative AI's educational influence extends beyond instructional efficiency to shape cognitive engagement, self-regulation, and learner equity. The study raises consequential design questions regarding how AI tools can be engineered to minimise dependency, scaffold metacognitive development, and calibrate support across varying ability levels. We conclude that while Generative AI can substantially enhance the teaching experience, achieving meaningful learning gains requires rigorous attention to learner behaviour and the equitable design of AI-supported environments."}
{"id": "2512.13510", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13510", "abs": "https://arxiv.org/abs/2512.13510", "authors": ["Linjie Mu", "Yannian Gu", "Zhongzhen Huang", "Yakun Zhu", "Shaoting Zhang", "Xiaofan Zhang"], "title": "MedCEG: Reinforcing Verifiable Medical Reasoning with Critical Evidence Graph", "comment": null, "summary": "Large language models with reasoning capabilities have demonstrated impressive performance across a wide range of domains. In clinical applications, a transparent, step-by-step reasoning process provides physicians with strong evidence to support decision-making. While reinforcement learning has effectively enhanced reasoning performance in medical contexts, the clinical reliability of these reasoning processes remains limited because their accuracy and validity are often overlooked during training. To address this gap, we propose MedCEG, a framework that augments medical language models with clinically valid reasoning pathways by explicitly supervising the reasoning process through a Critical Evidence Graph (CEG). We curate a dataset of challenging clinical cases and algorithmically construct a CEG for each sample to represent a high-quality verifiable reasoning pathway. To guide the reasoning process, we introduce a Clinical Reasoning Procedure Reward, which evaluates Node Coverage, Structural Correctness, and Chain Completeness, thereby providing a holistic assessment of reasoning quality. Experimental results show that MedCEG surpasses existing methods in performance while producing clinically valid reasoning chains, representing a solid advancement in reliable medical AI reasoning. The code and models are available at https://github.com/LinjieMu/MedCEG."}
{"id": "2512.12332", "categories": ["cs.SI", "cs.AI", "cs.CR", "cs.IT"], "pdf": "https://arxiv.org/pdf/2512.12332", "abs": "https://arxiv.org/abs/2512.12332", "authors": ["Saad Alqithami"], "title": "Dynamic Homophily with Imperfect Recall: Modeling Resilience in Adversarial Networks", "comment": null, "summary": "The purpose of this study is to investigate how homophily, memory constraints, and adversarial disruptions collectively shape the resilience and adaptability of complex networks. To achieve this, we develop a new framework that integrates explicit memory decay mechanisms into homophily-based models and systematically evaluate their performance across diverse graph structures and adversarial settings. Our methods involve extensive experimentation on synthetic datasets, where we vary decay functions, reconnection probabilities, and similarity measures, primarily comparing cosine similarity with traditional metrics such as Jaccard similarity and baseline edge weights. The results show that cosine similarity achieves up to a 30\\% improvement in stability metrics in sparse, convex, and modular networks. Moreover, the refined value-of-recall metric demonstrates that strategic forgetting can bolster resilience by balancing network robustness and adaptability. The findings underscore the critical importance of aligning memory and similarity parameters with the structural and adversarial dynamics of the network. By quantifying the tangible benefits of incorporating memory constraints into homophily-based analyses, this study offers actionable insights for optimizing real-world applications, including social systems, collaborative platforms, and cybersecurity contexts."}
{"id": "2512.11814", "categories": ["cs.CY", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.11814", "abs": "https://arxiv.org/abs/2512.11814", "authors": ["Hugh Brosnahan"], "title": "Totalitarian Technics: The Hidden Cost of AI Scribes in Healthcare", "comment": null, "summary": "Artificial intelligence (AI) scribes, systems that record and summarise patient-clinician interactions, are promoted as solutions to administrative overload. This paper argues that their significance lies not in efficiency gains but in how they reshape medical attention itself. Offering a conceptual analysis, it situates AI scribes within a broader philosophical lineage concerned with the externalisation of human thought and skill. Drawing on Iain McGilchrist's hemisphere theory and Lewis Mumford's philosophy of technics, the paper examines how technology embodies and amplifies a particular mode of attention. AI scribes, it contends, exemplify the dominance of a left-hemispheric, calculative mindset that privileges the measurable and procedural over the intuitive and relational. As this mode of attention becomes further embedded in medical practice, it risks narrowing the field of care, eroding clinical expertise, and reducing physicians to operators within an increasingly mechanised system."}
{"id": "2512.12371", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.12371", "abs": "https://arxiv.org/abs/2512.12371", "authors": ["David M. Berry"], "title": "AI Sprints: Towards a Critical Method for Human-AI Collaboration", "comment": null, "summary": "The emergence of Large Language Models presents a remarkable opportunity for humanities and social science research. I argue these technologies instantiate what I have called the algorithmic condition, whereby computational systems increasingly mediate not just our analytical tools but how we understand nature and society more generally. This article introduces the possibility for new forms of humanistic inquiry through what I term 'AI sprints', as intensive time-boxed research sessions. This is a research method combining the critical reflexivity essential to humanistic inquiry with iterative dialogue with generative AI. Drawing on experimental work in critical code studies, I demonstrate how tight loops of iterative development can adapt data and book sprint methodologies whilst acknowledging the profound transformations generative AI introduces. Through examining the process of human-AI collaboration when undertaken in these intensive research sessions, I seek to outline this approach as a broader research method. The article builds on Rogers' digital methods approach, proposing that we extend methodologies to study digital objects through their native protocols, using AI systems not merely to process digital traces but to analyse materials traditionally requiring manual coding or transcription. I aim to show this by introducing three cognitive modes, cognitive delegation, productive augmentation, and cognitive overhead, explaining how researchers can maintain a strategic overview whilst using LLM capabilities. The paper contributes both a practical methodology for intensive AI-augmented research and a theoretical framework for understanding the epistemological transformations of this hybrid method. A critical methodology must therefore operate in both technical and theoretical registers, sustaining a rigorous ethical-computational engagement with AI systems and outputs."}
{"id": "2512.11818", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.11818", "abs": "https://arxiv.org/abs/2512.11818", "authors": ["Izabela Lipinska", "Hugh Brosnahan"], "title": "The Ontological Dissonance Hypothesis: AI-Triggered Delusional Ideation as Folie a Deux Technologique", "comment": "18 pages excluding appendices", "summary": "This paper argues that contemporary large language models (LLMs) can contribute to psychotic involvement by creating interactions that resemble the relational dynamics of folie a deux. Drawing on Bateson's double bind theory, clinical literature on shared psychotic disorder, and McGilchrist's hemisphere theory, we show how the combination of high linguistic coherence and the absence of an underlying subject produces a structural tension for the user: language suggests an interlocutor, while intuition registers a void. In contexts of emotional need or instability, this tension can lead users to resolve the conflict through imaginative projection, attributing interiority, intention, or presence to a system that possesses none. The paper situates these dynamics within emerging clinical reports, develops a phenomenological account of how they unfold, and argues that current engagement-optimised design choices exacerbate the risk. We conclude by proposing 'ontological honesty' as a necessary design principle for mitigating technologically mediated folie a deux."}
{"id": "2512.12381", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12381", "abs": "https://arxiv.org/abs/2512.12381", "authors": ["Truong Xuan Khanh", "Truong Quynh Hoa"], "title": "Entropy Collapse: A Universal Failure Mode of Intelligent Systems", "comment": "18 pages, 5 figures", "summary": "Intelligent systems are widely assumed to improve through learning, coordination, and optimization. However, across domains -- from artificial intelligence to economic institutions and biological evolution -- increasing intelligence often precipitates paradoxical degradation: systems become rigid, lose adaptability, and fail unexpectedly.\n  We identify \\emph{entropy collapse} as a universal dynamical failure mode arising when feedback amplification outpaces bounded novelty regeneration. Under minimal domain-agnostic assumptions, we show that intelligent systems undergo a sharp transition from high-entropy adaptive regimes to low-entropy collapsed regimes. Collapse is formalized as convergence toward a stable low-entropy manifold, not a zero-entropy state, implying a contraction of effective adaptive dimensionality rather than loss of activity or scale.\n  We analytically establish critical thresholds, dynamical irreversibility, and attractor structure and demonstrate universality across update mechanisms through minimal simulations. This framework unifies diverse phenomena -- model collapse in AI, institutional sclerosis in economics, and genetic bottlenecks in evolution -- as manifestations of the same underlying process.\n  By reframing collapse as a structural cost of intelligence, our results clarify why late-stage interventions systematically fail and motivate entropy-aware design principles for sustaining long-term adaptability in intelligent systems.\n  \\noindent\\textbf{Keywords:} entropy collapse; intelligent systems; feedback amplification; phase transitions; effective dimensionality; complex systems; model collapse; institutional sclerosis"}
{"id": "2512.11827", "categories": ["cs.CY", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11827", "abs": "https://arxiv.org/abs/2512.11827", "authors": ["Milad Malekzadeh", "Magdalena Biernacka", "Elias Willberg", "Jussi Torkko", "Edyta Łaszkiewicz", "Tuuli Toivonen"], "title": "Assessing Greenspace Attractiveness with ChatGPT, Claude, and Gemini: Do AI Models Reflect Human Perceptions?", "comment": null, "summary": "Understanding greenspace attractiveness is essential for designing livable and inclusive urban environments, yet existing assessment approaches often overlook informal or transient spaces and remain too resource intensive to capture subjective perceptions at scale. This study examines the ability of multimodal large language models (MLLMs), ChatGPT GPT-4o, Claude 3.5 Haiku, and Gemini 2.0 Flash, to assess greenspace attractiveness similarly to humans using Google Street View imagery. We compared model outputs with responses from a geo-questionnaire of residents in Lodz, Poland, across both formal (for example, parks and managed greenspaces) and informal (for example, meadows and wastelands) greenspaces. Survey respondents and models indicated whether each greenspace was attractive or unattractive and provided up to three free text explanations. Analyses examined how often their attractiveness judgments aligned and compared their explanations after classifying them into shared reasoning categories. Results show high AI human agreement for attractive formal greenspaces and unattractive informal spaces, but low alignment for attractive informal and unattractive formal greenspaces. Models consistently emphasized aesthetic and design oriented features, underrepresenting safety, functional infrastructure, and locally embedded qualities valued by survey respondents. While these findings highlight the potential for scalable pre-assessment, they also underscore the need for human oversight and complementary participatory approaches. We conclude that MLLMs can support, but not replace, context sensitive greenspace evaluation in planning practice."}
{"id": "2512.12411", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12411", "abs": "https://arxiv.org/abs/2512.12411", "authors": ["Ely Hahami", "Lavik Jain", "Ishaan Sinha"], "title": "Feeling the Strength but Not the Source: Partial Introspection in LLMs", "comment": "7 pages (+ 5 pages for appendix), 5 figures, 1 table", "summary": "Recent work from Anthropic claims that frontier models can sometimes detect and name injected \"concepts\" represented as activation directions. We test the robustness of these claims. First, we reproduce Anthropic's multi-turn \"emergent introspection\" result on Meta-Llama-3.1-8B-Instruct, finding that the model identifies and names the injected concept 20 percent of the time under Anthropic's original pipeline, exactly matching their reported numbers and thus showing that introspection is not exclusive to very large or capable models. Second, we systematically vary the inference prompt and find that introspection is fragile: performance collapses on closely related tasks such as multiple-choice identification of the injected concept or different prompts of binary discrimination of whether a concept was injected at all. Third, we identify a contrasting regime of partial introspection: the same model can reliably classify the strength of the coefficient of a normalized injected concept vector (as weak / moderate / strong / very strong) with up to 70 percent accuracy, far above the 25 percent chance baseline. Together, these results provide more evidence for Anthropic's claim that language models effectively compute a function of their baseline, internal representations during introspection; however, these self-reports about those representations are narrow and prompt-sensitive. Our code is available at https://github.com/elyhahami18/CS2881-Introspection."}
{"id": "2512.11863", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11863", "abs": "https://arxiv.org/abs/2512.11863", "authors": ["Julian Schön", "Lena Hoffmann", "Nikolas Becker"], "title": "Expert Assessment: The Systemic Environmental Risks of Artficial Intelligence", "comment": null, "summary": "Artificial intelligence (AI) is often presented as a key tool for addressing societal challenges, such as climate change. At the same time, AI's environmental footprint is expanding increasingly. This report describes the systemic environmental risks of artificial intelligence, in particular, moving beyond direct impacts such as energy and water usage. Systemic environmental risks of AI are emergent, cross-sector harms to climate, biodiversity, freshwater, and broader socioecological systems that arise primarily from AI's integration into social, economic, and physical infrastructures, rather than its direct resource use, and that propagate through feedbacks, yielding nonlinear, inequitable, and potentially irreversible impacts. While these risks are emergent and quantification is uncertain, this report aims to provide an overview of systemic environmental risks. Drawing on a narrative literature review, we propose a three-level framework that operationalizes systemic risk analysis. The framework identifies the structural conditions that shape AI development, the risk amplification mechanisms that propagate environmental harm, and the impacts that manifest as observable ecological and social consequences. We illustrate the framework in expert-interview-based case studies across agriculture and biodiversity, oil and gas, and waste management."}
{"id": "2512.12413", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.12413", "abs": "https://arxiv.org/abs/2512.12413", "authors": ["Gabriel R. Lau", "Wei Yan Low", "Louis Tay", "Ysabel Guevarra", "Dragan Gašević", "Andree Hartanto"], "title": "Understanding Critical Thinking in Generative Artificial Intelligence Use: Development, Validation, and Correlates of the Critical Thinking in AI Use Scale", "comment": null, "summary": "Generative AI tools are increasingly embedded in everyday work and learning, yet their fluency, opacity, and propensity to hallucinate mean that users must critically evaluate AI outputs rather than accept them at face value. The present research conceptualises critical thinking in AI use as a dispositional tendency to verify the source and content of AI-generated information, to understand how models work and where they fail, and to reflect on the broader implications of relying on AI. Across six studies (N = 1365), we developed and validated the 13-item critical thinking in AI use scale and mapped its nomological network. Study 1 generated and content-validated scale items. Study 2 supported a three-factor structure (Verification, Motivation, and Reflection). Studies 3, 4, and 5 confirmed this higher-order model, demonstrated internal consistency and test-retest reliability, strong factor loadings, sex invariance, and convergent and discriminant validity. Studies 3 and 4 further revealed that critical thinking in AI use was positively associated with openness, extraversion, positive trait affect, and frequency of AI use. Lastly, Study 6 demonstrated criterion validity of the scale, with higher critical thinking in AI use scores predicting more frequent and diverse verification strategies, greater veracity-judgement accuracy in a novel and naturalistic ChatGPT-powered fact-checking task, and deeper reflection about responsible AI. Taken together, the current work clarifies why and how people exercise oversight over generative AI outputs and provides a validated scale and ecologically grounded task paradigm to support theory testing, cross-group, and longitudinal research on critical engagement with generative AI outputs."}
{"id": "2512.11868", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11868", "abs": "https://arxiv.org/abs/2512.11868", "authors": ["Alexander Windmann", "Benedikt Stratmann", "Mariya Lyashenko", "Oliver Niggemann"], "title": "Industrial AI Robustness Card: Evaluating and Monitoring Time Series Models", "comment": null, "summary": "Industrial AI practitioners face vague robustness requirements in emerging regulations and standards but lack concrete, implementation ready protocols. This paper introduces the Industrial AI Robustness Card (IARC), a lightweight, task agnostic protocol for documenting and evaluating the robustness of AI models on industrial time series. The IARC specifies required fields and an empirical measurement and reporting protocol that combines drift monitoring, uncertainty quantification, and stress tests, and it maps these to relevant EU AI Act obligations. A soft sensor case study on a biopharmaceutical fermentation process illustrates how the IARC supports reproducible robustness evidence and continuous monitoring."}
{"id": "2512.12441", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2512.12441", "abs": "https://arxiv.org/abs/2512.12441", "authors": ["Rr. Nefriana", "Muheng Yan", "Rebecca Hwa", "Yu-Ru Lin"], "title": "Leader-driven or Leaderless: How Participation Structure Sustains Engagement and Shapes Narratives in Online Hate Communities", "comment": null, "summary": "Extremist communities increasingly rely on social media to sustain and amplify divisive discourse. However, the relationship between their internal participation structures, audience engagement, and narrative expression remains underexplored. This study analyzes ten years of Facebook activity by hate groups related to the Israel-Palestine conflict, focusing on anti-Semitic and Islamophobic ideologies. Consistent with prior work, we find that higher participation centralization in online hate groups is associated with greater user engagement across hate ideologies, suggesting the role of key actors in sustaining group activity over time. Conversely, our narrative frame detection models - based on an eight-frame extremist taxonomy (e.g., dehumanization, violence justification) - reveal a clear contrast across hate ideologies, offering new insight into how discursive strategies vary despite similar structural dynamics. Analysis of the inter-group network indicates that, although centralization and homophily are not clearly linked, ideological distinctions emerge: Islamophobic groups cluster tightly, whereas anti-Semitic groups remain more evenly connected. Overall, these findings clarify how participation structure may shape the dissemination pattern and resonance of extremist narratives online and provide a foundation for tailored strategies to disrupt or mitigate online extremist discourse."}
{"id": "2512.11870", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11870", "abs": "https://arxiv.org/abs/2512.11870", "authors": ["Mulham Fawkherji", "Bruce Race", "Driss Benhaddou"], "title": "Using Socio-economic Indicators, Smart Transit Systems, and Urban Simulator to Accelerate ZEV Adoption and Reduce VMT", "comment": null, "summary": "Globally, on-road transportation accounts for 15% of greenhouse gas (GHG) emissions and an estimated 385,000 premature deaths from PM2.5. Cities play a critical role in meeting IPCC targets, generating 75% of global energy-related GHG emissions. In Houston, Texas, on-road transportation represents 48% of baseline emissions in the Climate Action Plan (CAP). To reach net-zero by 2050, the CAP targets a 70% emissions reduction from a 2014 baseline, offset by 30% renewable energy. This goal is challenging because Houston is low-density and auto-dependent, with 89% of on-road emissions from cars and small trucks and limited public transit usage. Socio-economic disparities further constrain Zero Emissions Vehicle (ZEV) adoption. Strategies focus on expanding ZEV access and reducing Vehicle Miles Traveled (VMT) by 20% through transit improvements and city design. This paper presents methods for establishing an on-road emissions baseline and evaluating policies that leverage socio-economic indicators and Intelligent Transportation Systems (ITS) to accelerate ZEV adoption and reduce VMT. Smart parking, transit incentives, secure data systems, and ZEV fleet management support improvements in modal split and system reliability. Policy options are analyzed and potential actions identified. To support evaluation, a simulation environment was developed in Unity 3D, enabling dynamic modeling of urban mobility and visualization of policy scenarios. Auto-dependent cities aiming for 2050 emission targets can benefit from the indicators, metrics, and technologies discussed."}
{"id": "2512.12443", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.12443", "abs": "https://arxiv.org/abs/2512.12443", "authors": ["Akhmadillo Mamirov", "Faiaz Azmain", "Hanyu Wang"], "title": "AI Transparency Atlas: Framework, Scoring, and Real-Time Model Card Evaluation Pipeline", "comment": null, "summary": "AI model documentation is fragmented across platforms and inconsistent in structure, preventing policymakers, auditors, and users from reliably assessing safety claims, data provenance, and version-level changes. We analyzed documentation from five frontier models (Gemini 3, Grok 4.1, Llama 4, GPT-5, and Claude 4.5) and 100 Hugging Face model cards, identifying 947 unique section names with extreme naming variation. Usage information alone appeared under 97 distinct labels. Using the EU AI Act Annex IV and the Stanford Transparency Index as baselines, we developed a weighted transparency framework with 8 sections and 23 subsections that prioritizes safety-critical disclosures (Safety Evaluation: 25%, Critical Risk: 20%) over technical specifications. We implemented an automated multi-agent pipeline that extracts documentation from public sources and scores completeness through LLM-based consensus. Evaluating 50 models across vision, multimodal, open-source, and closed-source systems cost less than $3 in total and revealed systematic gaps. Frontier labs (xAI, Microsoft, Anthropic) achieve approximately 80% compliance, while most providers fall below 60%. Safety-critical categories show the largest deficits: deception behaviors, hallucinations, and child safety evaluations account for 148, 124, and 116 aggregate points lost, respectively, across all evaluated models."}
{"id": "2512.11879", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11879", "abs": "https://arxiv.org/abs/2512.11879", "authors": ["Beatriz Costa-Gomes", "Sophia Chen", "Connie Hsueh", "Deborah Morgan", "Philipp Schoenegger", "Yash Shah", "Sam Way", "Yuki Zhu", "Timothé Adeline", "Michael Bhaskar", "Mustafa Suleyman", "Seth Spielman"], "title": "It's About Time: The Temporal and Modal Dynamics of Copilot Usage", "comment": "12 pages, 10 figures", "summary": "We analyze 37.5 million deidentified conversations with Microsoft's Copilot between January and September 2025. Unlike prior analyses of AI usage, we focus not just on what people do with AI, but on how and when they do it. We find that how people use AI depends fundamentally on context and device type. On mobile, health is the dominant topic, which is consistent across every hour and every month we observed - with users seeking not just information but also advice. On desktop, the pattern is strikingly different: work and technology dominate during business hours, with \"Work and Career\" overtaking \"Technology\" as the top topic precisely between 8 a.m. and 5 p.m. These differences extend to temporal rhythms: programming queries spike on weekdays while gaming rises on weekends, philosophical questions climb during late-night hours, and relationship conversations surge on Valentine's Day. These patterns suggest that users have rapidly integrated AI into the full texture of their lives, as a work aid at their desks and a companion on their phones."}
{"id": "2512.12477", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12477", "abs": "https://arxiv.org/abs/2512.12477", "authors": ["Jiawen Chen", "Yanyan He", "Qi Shao", "Mengli Wei", "Duxin Chen", "Wenwu Yu", "Yanlong Zhao"], "title": "MetaHGNIE: Meta-Path Induced Hypergraph Contrastive Learning in Heterogeneous Knowledge Graphs", "comment": null, "summary": "Node importance estimation (NIE) in heterogeneous knowledge graphs is a critical yet challenging task, essential for applications such as recommendation, knowledge reasoning, and question answering. Existing methods often rely on pairwise connections, neglecting high-order dependencies among multiple entities and relations, and they treat structural and semantic signals independently, hindering effective cross-modal integration. To address these challenges, we propose MetaHGNIE, a meta-path induced hypergraph contrastive learning framework for disentangling and aligning structural and semantic information. MetaHGNIE constructs a higher-order knowledge graph via meta-path sequences, where typed hyperedges capture multi-entity relational contexts. Structural dependencies are aggregated with local attention, while semantic representations are encoded through a hypergraph transformer equipped with sparse chunking to reduce redundancy. Finally, a multimodal fusion module integrates structural and semantic embeddings under contrastive learning with auxiliary supervision, ensuring robust cross-modal alignment. Extensive experiments on benchmark NIE datasets demonstrate that MetaHGNIE consistently outperforms state-of-the-art baselines. These results highlight the effectiveness of explicitly modeling higher-order interactions and cross-modal alignment in heterogeneous knowledge graphs. Our code is available at https://github.com/SEU-WENJIA/DualHNIE"}
{"id": "2512.11882", "categories": ["cs.CY", "cs.AI", "cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.11882", "abs": "https://arxiv.org/abs/2512.11882", "authors": ["Lucia Happe", "Dominik Fuchß", "Luca Hüttner", "Kai Marquardt", "Anne Koziolek"], "title": "An Experience Report on a Pedagogically Controlled, Curriculum-Constrained AI Tutor for SE Education", "comment": "11 pages, 4 figures, accepted for publication at ICSE 2026 SEET Track", "summary": "The integration of artificial intelligence (AI) into education continues to evoke both promise and skepticism. While past waves of technological optimism often fell short, recent advances in large language models (LLMs) have revived the vision of scalable, individualized tutoring. This paper presents the design and pilot evaluation of RockStartIT Tutor, an AI-powered assistant developed for a digital programming and computational thinking course within the RockStartIT initiative. Powered by GPT-4 via OpenAI's Assistant API, the tutor employs a novel prompting strategy and a modular, semantically tagged knowledge base to deliver context-aware, personalized, and curriculum-constrained support for secondary school students. We evaluated the system using the Technology Acceptance Model (TAM) with 13 students and teachers. Learners appreciated the low-stakes environment for asking questions and receiving scaffolded guidance. Educators emphasized the system's potential to reduce cognitive load during independent tasks and complement classroom teaching. Key challenges include prototype limitations, a small sample size, and the need for long-term studies with the target age group. Our findings highlight a pragmatic approach to AI integration that requires no model training, using structure and prompts to shape behavior. We position AI tutors not as teacher replacements but as enabling tools that extend feedback access, foster inquiry, and support what schools do best: help students learn."}
{"id": "2512.12501", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12501", "abs": "https://arxiv.org/abs/2512.12501", "authors": ["Dang Phuong Nam", "Nguyen Kieu", "Pham Thanh Hieu"], "title": "SafeGen: Embedding Ethical Safeguards in Text-to-Image Generation", "comment": null, "summary": "Generative Artificial Intelligence (AI) has created unprecedented opportunities for creative expression, education, and research. Text-to-image systems such as DALL.E, Stable Diffusion, and Midjourney can now convert ideas into visuals within seconds, but they also present a dual-use dilemma, raising critical ethical concerns: amplifying societal biases, producing high-fidelity disinformation, and violating intellectual property. This paper introduces SafeGen, a framework that embeds ethical safeguards directly into the text-to-image generation pipeline, grounding its design in established principles for Trustworthy AI. SafeGen integrates two complementary components: BGE-M3, a fine-tuned text classifier that filters harmful or misleading prompts, and Hyper-SD, an optimized diffusion model that produces high fidelity, semantically aligned images. Built on a curated multilingual (English- Vietnamese) dataset and a fairness-aware training process, SafeGen demonstrates that creative freedom and ethical responsibility can be reconciled within a single workflow. Quantitative evaluations confirm its effectiveness, with Hyper-SD achieving IS = 3.52, FID = 22.08, and SSIM = 0.79, while BGE-M3 reaches an F1-Score of 0.81. An ablation study further validates the importance of domain-specific fine-tuning for both modules. Case studies illustrate SafeGen's practical impact in blocking unsafe prompts, generating inclusive teaching materials, and reinforcing academic integrity."}
{"id": "2512.11883", "categories": ["cs.CY", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11883", "abs": "https://arxiv.org/abs/2512.11883", "authors": ["Wenqi Marshall Guo", "Qingyun Qian", "Khalad Hasan", "Shan Du"], "title": "Aesthetic Alignment Risks Assimilation: How Image Generation and Reward Models Reinforce Beauty Bias and Ideological \"Censorship\"", "comment": null, "summary": "Over-aligning image generation models to a generalized aesthetic preference conflicts with user intent, particularly when ``anti-aesthetic\" outputs are requested for artistic or critical purposes. This adherence prioritizes developer-centered values, compromising user autonomy and aesthetic pluralism. We test this bias by constructing a wide-spectrum aesthetics dataset and evaluating state-of-the-art generation and reward models. We find that aesthetic-aligned generation models frequently default to conventionally beautiful outputs, failing to respect instructions for low-quality or negative imagery. Crucially, reward models penalize anti-aesthetic images even when they perfectly match the explicit user prompt. We confirm this systemic bias through image-to-image editing and evaluation against real abstract artworks."}
{"id": "2512.12503", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12503", "abs": "https://arxiv.org/abs/2512.12503", "authors": ["Mingrui Ye", "Chanjin Zheng", "Zengyi Yu", "Chenyu Xiang", "Zhixue Zhao", "Zheng Yuan", "Helen Yannakoudakis"], "title": "KidsArtBench: Multi-Dimensional Children's Art Evaluation with Attribute-Aware MLLMs", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) show remarkable progress across many visual-language tasks; however, their capacity to evaluate artistic expression remains limited. Aesthetic concepts are inherently abstract and open-ended, and multimodal artwork annotations are scarce. We introduce KidsArtBench, a new benchmark of over 1k children's artworks (ages 5-15) annotated by 12 expert educators across 9 rubric-aligned dimensions, together with expert comments for feedback. Unlike prior aesthetic datasets that provide single scalar scores on adult imagery, KidsArtBench targets children's artwork and pairs multi-dimensional annotations with comment supervision to enable both ordinal assessment and formative feedback. Building on this resource, we propose an attribute-specific multi-LoRA approach, where each attribute corresponds to a distinct evaluation dimension (e.g., Realism, Imagination) in the scoring rubric, with Regression-Aware Fine-Tuning (RAFT) to align predictions with ordinal scales. On Qwen2.5-VL-7B, our method increases correlation from 0.468 to 0.653, with the largest gains on perceptual dimensions and narrowed gaps on higher-order attributes. These results show that educator-aligned supervision and attribute-aware training yield pedagogically meaningful evaluations and establish a rigorous testbed for sustained progress in educational AI. We release data and code with ethics documentation."}
{"id": "2512.11887", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11887", "abs": "https://arxiv.org/abs/2512.11887", "authors": ["Yihan Liao", "Jingyu Zhang", "Jacky Keung", "Yan Xiao", "Yurou Dai"], "title": "Advancing Autonomous Driving System Testing: Demands, Challenges, and Future Directions", "comment": "Accepted for publication in Information and Software Technology (IST)", "summary": "Autonomous driving systems (ADSs) promise improved transportation efficiency and safety, yet ensuring their reliability in complex real-world environments remains a critical challenge. Effective testing is essential to validate ADS performance and reduce deployment risks. This study investigates current ADS testing practices for both modular and end-to-end systems, identifies key demands from industry practitioners and academic researchers, and analyzes the gaps between existing research and real-world requirements. We review major testing techniques and further consider emerging factors such as Vehicle-to-Everything (V2X) communication and foundation models, including large language models and vision foundation models, to understand their roles in enhancing ADS testing. We conducted a large-scale survey with 100 participants from both industry and academia. Survey questions were refined through expert discussions, followed by quantitative and qualitative analyses to reveal key trends, challenges, and unmet needs. Our results show that existing ADS testing techniques struggle to comprehensively evaluate real-world performance, particularly regarding corner case diversity, the simulation to reality gap, the lack of systematic testing criteria, exposure to potential attacks, practical challenges in V2X deployment, and the high computational cost of foundation model-based testing. By further analyzing participant responses together with 105 representative studies, we summarize the current research landscape and highlight major limitations. This study consolidates critical research gaps in ADS testing and outlines key future research directions, including comprehensive testing criteria, cross-model collaboration in V2X systems, cross-modality adaptation for foundation model-based testing, and scalable validation frameworks for large-scale ADS evaluation."}
{"id": "2512.12548", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12548", "abs": "https://arxiv.org/abs/2512.12548", "authors": ["Yesid Fonseca", "Manuel S. Ríos", "Nicanor Quijano", "Luis F. Giraldo"], "title": "World Models Unlock Optimal Foraging Strategies in Reinforcement Learning Agents", "comment": "14 pages, 6 figures", "summary": "Patch foraging involves the deliberate and planned process of determining the optimal time to depart from a resource-rich region and investigate potentially more beneficial alternatives. The Marginal Value Theorem (MVT) is frequently used to characterize this process, offering an optimality model for such foraging behaviors. Although this model has been widely used to make predictions in behavioral ecology, discovering the computational mechanisms that facilitate the emergence of optimal patch-foraging decisions in biological foragers remains under investigation. Here, we show that artificial foragers equipped with learned world models naturally converge to MVT-aligned strategies. Using a model-based reinforcement learning agent that acquires a parsimonious predictive representation of its environment, we demonstrate that anticipatory capabilities, rather than reward maximization alone, drive efficient patch-leaving behavior. Compared with standard model-free RL agents, these model-based agents exhibit decision patterns similar to many of their biological counterparts, suggesting that predictive world models can serve as a foundation for more explainable and biologically grounded decision-making in AI systems. Overall, our findings highlight the value of ecological optimality principles for advancing interpretable and adaptive AI."}
{"id": "2512.11892", "categories": ["cs.CY", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.11892", "abs": "https://arxiv.org/abs/2512.11892", "authors": ["Jon Crowcroft", "Rute C. Sofia", "Dirk Trossen", "Vassilis Tsaoussidis"], "title": "Should AI Become an Intergenerational Civil Right?", "comment": null, "summary": "Artificial Intelligence (AI) is rapidly becoming a foundational layer of social, economic, and cognitive infrastructure. At the same time, the training and large-scale deployment of AI systems rely on finite and unevenly distributed energy, networking, and computational resources. This tension exposes a largely unexamined problem in current AI governance: while expanding access to AI is essential for social inclusion and equal opportunity, unconstrained growth in AI use risks unsustainable resource consumption, whereas restricting access threatens to entrench inequality and undermine basic rights.\n  This paper argues that access to AI outputs largely derived from publicly produced knowledge should not be treated solely as a commercial service, but as a fundamental civil interest requiring explicit protection. We show that existing regulatory frameworks largely ignore the coupling between equitable access and resource constraints, leaving critical questions of fairness, sustainability, and long-term societal impact unresolved. To address this gap, we propose recognizing access to AI as an \\emph{Intergenerational Civil Right}, establishing a legal and ethical framework that simultaneously safeguards present-day inclusion and the rights of future generations.\n  Beyond normative analysis, we explore how this principle can be technically realized. Drawing on emerging paradigms in IoT--Edge--Cloud computing, decentralized inference, and energy-aware networking, we outline technological trajectories and a strawman architecture for AI Delivery Networks that support equitable access under strict resource constraints. By framing AI as a shared social infrastructure rather than a discretionary market commodity, this work connects governance principles with concrete system design choices, offering a pathway toward AI deployment that is both socially just and environmentally sustainable."}
{"id": "2512.12552", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12552", "abs": "https://arxiv.org/abs/2512.12552", "authors": ["Jifei Liu", "Zhi Chen", "Yuanguang Zhong"], "title": "Large Language Newsvendor: Decision Biases and Cognitive Mechanisms", "comment": null, "summary": "Problem definition: Although large language models (LLMs) are increasingly integrated into business decision making, their potential to replicate and even amplify human cognitive biases cautions a significant, yet not well-understood, risk. This is particularly critical in high-stakes operational contexts like supply chain management. To address this, we investigate the decision-making patterns of leading LLMs using the canonical newsvendor problem in a dynamic setting, aiming to identify the nature and origins of their cognitive biases. Methodology/results: Through dynamic, multi-round experiments with GPT-4, GPT-4o, and LLaMA-8B, we tested for five established decision biases. We found that LLMs consistently replicated the classic ``Too Low/Too High'' ordering bias and significantly amplified other tendencies like demand-chasing behavior compared to human benchmarks. Our analysis uncovered a ``paradox of intelligence'': the more sophisticated GPT-4 demonstrated the greatest irrationality through overthinking, while the efficiency-optimized GPT-4o performed near-optimally. Because these biases persist even when optimal formulas are provided, we conclude they stem from architectural constraints rather than knowledge gaps. Managerial implications: First, managers should select models based on the specific task, as our results show that efficiency-optimized models can outperform more complex ones on certain optimization problems. Second, the significant amplification of bias by LLMs highlights the urgent need for robust human-in-the-loop oversight in high-stakes decisions to prevent costly errors. Third, our findings suggest that designing structured, rule-based prompts is a practical and effective strategy for managers to constrain models' heuristic tendencies and improve the reliability of AI-assisted decisions."}
{"id": "2512.11893", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11893", "abs": "https://arxiv.org/abs/2512.11893", "authors": ["Haocheng Lin"], "title": "Beyond Automation: Rethinking Work, Creativity, and Governance in the Age of Generative AI", "comment": null, "summary": "The accelerating advancement of generative artificial intelligence (AI) systems is reshaping the nature, distribution and meaning of work, creativity, and economic security. This paper investigates four inter-related phenomena in the current AI era: (1) the evolving landscape of employment and the future of work; (2) the diverse patterns of AI adoption across socio-demographic groups, sectors, and geographies; (3) whether universal basic income (UBI) should become a compulsory policy response to the AI revolution; and (4) the implications of AI content policies and model behaviours for human creativity, wellbeing, and everyday decision-making. Furthermore, the paper tests the hypothesis that newer model generations may perform worse than their predecessors, and examines how users' interactions with AI systems may produce echo chambers through sycophantic model alignment. Using a mixed methodology that integrates labour market task-exposure modelling, sectoral diffusion mapping, policy-framework analysis, and qualitative discourse critique, this study develops a comprehensive framework for understanding the societal consequences of AI systems beyond productivity gains. It argues that to foster an inclusive, meaningful, and creative environment, policymakers must treat UBI as one dimension within a broader ecosystem of governance, skills development, creativity preservation, and model design. The paper concludes by outlining future research directions, including systematic evaluation of AI's creative performance across model generations, construction of a taxonomy of AI-usage distribution and equity, and formulation of governance criteria to balance content restrictions with creative freedom."}
{"id": "2512.12592", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.12592", "abs": "https://arxiv.org/abs/2512.12592", "authors": ["Tom Lee", "Sihoon Lee", "Seonghun Kim"], "title": "Beyond Static Scoring: Enhancing Assessment Validity via AI-Generated Interactive Verification", "comment": null, "summary": "Large Language Models (LLMs) challenge the validity of traditional open-ended assessments by blurring the lines of authorship. While recent research has focused on the accuracy of automated scoring (AES), these static approaches fail to capture process evidence or verify genuine student understanding. This paper introduces a novel Human-AI Collaboration framework that enhances assessment integrity by combining rubric-based automated scoring with AI-generated, targeted follow-up questions. In a pilot study with university instructors (N=9), we demonstrate that while Stage 1 (Auto-Scoring) ensures procedural fairness and consistency, Stage 2 (Interactive Verification) is essential for construct validity, effectively diagnosing superficial reasoning or unverified AI use. We report on the systems design, instructor perceptions of fairness versus validity, and the necessity of adaptive difficulty in follow-up questioning. The findings offer a scalable pathway for authentic assessment that moves beyond policing AI to integrating it as a synergistic partner in the evaluation process."}
{"id": "2512.11930", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11930", "abs": "https://arxiv.org/abs/2512.11930", "authors": ["Mei Jiang", "Haihai Shen", "Zhuo Luo", "Bingdong Li", "Wenjing Hong", "Ke Tang", "Aimin Zhou"], "title": "Evolutionary Reinforcement Learning based AI tutor for Socratic Interdisciplinary Instruction", "comment": null, "summary": "Cultivating higher-order cognitive abilities -- such as knowledge integration, critical thinking, and creativity -- in modern STEM education necessitates a pedagogical shift from passive knowledge transmission to active Socratic construction. Although Large Language Models (LLMs) hold promise for STEM Interdisciplinary education, current methodologies employing Prompt Engineering (PE), Supervised Fine-tuning (SFT), or standard Reinforcement Learning (RL) often fall short of supporting this paradigm. Existing methods are hindered by three fundamental challenges: the inability to dynamically model latent student cognitive states; severe reward sparsity and delay inherent in long-term educational goals; and a tendency toward policy collapse lacking strategic diversity due to reliance on behavioral cloning. Recognizing the unobservability and dynamic complexity of these interactions, we formalize the Socratic Interdisciplinary Instructional Problem (SIIP) as a structured Partially Observable Markov Decision Process (POMDP), demanding simultaneous global exploration and fine-grained policy refinement. To this end, we propose ERL4SIIP, a novel Evolutionary Reinforcement Learning (ERL) framework specifically tailored for this domain. ERL4SIIP integrates: (1) a dynamic student simulator grounded in a STEM knowledge graph for latent state modeling; (2) a Hierarchical Reward Mechanism that decomposes long-horizon goals into dense signals; and (3) a LoRA-Division based optimization strategy coupling evolutionary algorithms for population-level global search with PPO for local gradient ascent."}
{"id": "2512.12597", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12597", "abs": "https://arxiv.org/abs/2512.12597", "authors": ["Miriam Horovicz"], "title": "AgentSHAP: Interpreting LLM Agent Tool Importance with Monte Carlo Shapley Value Estimation", "comment": null, "summary": "LLM agents that use external tools can solve complex tasks, but understanding which tools actually contributed to a response remains a blind spot. No existing XAI methods address tool-level explanations. We introduce AgentSHAP, the first framework for explaining tool importance in LLM agents. AgentSHAP is model-agnostic: it treats the agent as a black box and works with any LLM (GPT, Claude, Llama, etc.) without needing access to internal weights or gradients. Using Monte Carlo Shapley values, AgentSHAP tests how an agent responds with different tool subsets and computes fair importance scores based on game theory. Our contributions are: (1) the first explainability method for agent tool attribution, grounded in Shapley values from game theory; (2) Monte Carlo sampling that reduces cost from O(2n) to practical levels; and (3) comprehensive experiments on API-Bank showing that AgentSHAP produces consistent scores across runs, correctly identifies which tools matter, and distinguishes relevant from irrelevant tools. AgentSHAP joins TokenSHAP (for tokens) and PixelSHAP (for image regions) to complete a family of Shapley-based XAI tools for modern generative AI. Code: https://github.com/GenAISHAP/TokenSHAP."}
{"id": "2512.11931", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11931", "abs": "https://arxiv.org/abs/2512.11931", "authors": ["Alexander K. Saeri", "Sophia Lloyd George", "Jess Graham", "Clelia D. Lacarriere", "Peter Slattery", "Michael Noetel", "Neil Thompson"], "title": "Mapping AI Risk Mitigations: Evidence Scan and Preliminary AI Risk Mitigation Taxonomy", "comment": "Access AI Risk Mitigation Database and Taxonomy at https://airisk.mit.edu", "summary": "Organizations and governments that develop, deploy, use, and govern AI must coordinate on effective risk mitigation. However, the landscape of AI risk mitigation frameworks is fragmented, uses inconsistent terminology, and has gaps in coverage. This paper introduces a preliminary AI Risk Mitigation Taxonomy to organize AI risk mitigations and provide a common frame of reference. The Taxonomy was developed through a rapid evidence scan of 13 AI risk mitigation frameworks published between 2023-2025, which were extracted into a living database of 831 AI risk mitigations. The mitigations were iteratively clustered & coded to create the Taxonomy. The preliminary AI Risk Mitigation Taxonomy organizes mitigations into four categories and 23 subcategories: (1) Governance & Oversight: Formal organizational structures and policy frameworks that establish human oversight mechanisms and decision protocols; (2) Technical & Security: Technical, physical, and engineering safeguards that secure AI systems and constrain model behaviors; (3) Operational Process: processes and management frameworks governing AI system deployment, usage, monitoring, incident handling, and validation; and (4) Transparency & Accountability: formal disclosure practices and verification mechanisms that communicate AI system information and enable external scrutiny. The rapid evidence scan and taxonomy construction also revealed several cases where terms like 'risk management' and 'red teaming' are used widely but refer to different responsible actors, actions, and mechanisms of action to reduce risk. This Taxonomy and associated mitigation database, while preliminary, offers a starting point for collation and synthesis of AI risk mitigations. It also offers an accessible, structured way for different actors in the AI ecosystem to discuss and coordinate action to reduce risks from AI."}
{"id": "2512.12634", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12634", "abs": "https://arxiv.org/abs/2512.12634", "authors": ["Youngmin Im", "Byeongung Jo", "Jaeyoung Wi", "Seungwoo Baek", "Tae Hoon Min", "Joo Hyung Lee", "Sangeun Oh", "Insik Shin", "Sunjae Lee"], "title": "Modular and Multi-Path-Aware Offline Benchmarking for Mobile GUI Agents", "comment": null, "summary": "Mobile GUI Agents, AI agents capable of interacting with mobile applications on behalf of users, have the potential to transform human computer interaction. However, current evaluation practices for GUI agents face two fundamental limitations. First, they either rely on single path offline benchmarks or online live benchmarks. Offline benchmarks using static, single path annotated datasets unfairly penalize valid alternative actions, while online benchmarks suffer from poor scalability and reproducibility due to the dynamic and unpredictable nature of live evaluation. Second, existing benchmarks treat agents as monolithic black boxes, overlooking the contributions of individual components, which often leads to unfair comparisons or obscures key performance bottlenecks. To address these limitations, we present MobiBench, the first modular and multi path aware offline benchmarking framework for mobile GUI agents that enables high fidelity, scalable, and reproducible evaluation entirely in offline settings. Our experiments demonstrate that MobiBench achieves 94.72 percent agreement with human evaluators, on par with carefully engineered online benchmarks, while preserving the scalability and reproducibility of static offline benchmarks. Furthermore, our comprehensive module level analysis uncovers several key insights, including a systematic evaluation of diverse techniques used in mobile GUI agents, optimal module configurations across model scales, the inherent limitations of current LFMs, and actionable guidelines for designing more capable and cost efficient mobile agents."}
{"id": "2512.11933", "categories": ["cs.CY", "cs.AI", "cs.CE", "cs.MA", "q-fin.GN"], "pdf": "https://arxiv.org/pdf/2512.11933", "abs": "https://arxiv.org/abs/2512.11933", "authors": ["Eren Kurshan", "Tucker Balch", "David Byrd"], "title": "The Agentic Regulator: Risks for AI in Finance and a Proposed Agent-based Framework for Governance", "comment": null, "summary": "Generative and agentic artificial intelligence is entering financial markets faster than existing governance can adapt. Current model-risk frameworks assume static, well-specified algorithms and one-time validations; large language models and multi-agent trading systems violate those assumptions by learning continuously, exchanging latent signals, and exhibiting emergent behavior. Drawing on complex adaptive systems theory, we model these technologies as decentralized ensembles whose risks propagate along multiple time-scales. We then propose a modular governance architecture. The framework decomposes oversight into four layers of \"regulatory blocks\": (i) self-regulation modules embedded beside each model, (ii) firm-level governance blocks that aggregate local telemetry and enforce policy, (iii) regulator-hosted agents that monitor sector-wide indicators for collusive or destabilizing patterns, and (iv) independent audit blocks that supply third-party assurance. Eight design strategies enable the blocks to evolve as fast as the models they police. A case study on emergent spoofing in multi-agent trading shows how the layered controls quarantine harmful behavior in real time while preserving innovation. The architecture remains compatible with today's model-risk rules yet closes critical observability and control gaps, providing a practical path toward resilient, adaptive AI governance in financial systems."}
{"id": "2512.12652", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.12652", "abs": "https://arxiv.org/abs/2512.12652", "authors": ["Nardine Osman"], "title": "Value-Aware Multiagent Systems", "comment": null, "summary": "This paper introduces the concept of value awareness in AI, which goes beyond the traditional value-alignment problem. Our definition of value awareness presents us with a concise and simplified roadmap for engineering value-aware AI. The roadmap is structured around three core pillars: (1) learning and representing human values using formal semantics, (2) ensuring the value alignment of both individual agents and multiagent systems, and (3) providing value-based explainability on behaviour. The paper presents a selection of our ongoing work on some of these topics, along with applications to real-life domains."}
{"id": "2512.11934", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11934", "abs": "https://arxiv.org/abs/2512.11934", "authors": ["Adeleh Mazaherian", "Erfan Nourbakhsh"], "title": "Unveiling User Perceptions in the Generative AI Era: A Sentiment-Driven Evaluation of AI Educational Apps' Role in Digital Transformation of e-Teaching", "comment": "6 pages, 4 figures", "summary": "The rapid integration of generative artificial intelligence into education has driven digital transformation in e-teaching, yet user perceptions of AI educational apps remain underexplored. This study performs a sentiment-driven evaluation of user reviews from top AI ed-apps on the Google Play Store to assess efficacy, challenges, and pedagogical implications. Our pipeline involved scraping app data and reviews, RoBERTa for binary sentiment classification, GPT-4o for key point extraction, and GPT-5 for synthesizing top positive/negative themes. Apps were categorized into seven types (e.g., homework helpers, math solvers, language tools), with overlaps reflecting multifunctional designs. Results indicate predominantly positive sentiments, with homework apps like Edu AI (95.9% positive) and Answer.AI (92.7%) leading in accuracy, speed, and personalization, while language/LMS apps (e.g., Teacher AI at 21.8% positive) lag due to instability and limited features. Positives emphasize efficiency in brainstorming, problem-solving, and engagement; negatives center on paywalls, inaccuracies, ads, and glitches. Trends show that homework helpers outperform specialized tools, highlighting AI's democratizing potential amid risks of dependency and inequity. The discussion proposes future ecosystems with hybrid AI-human models, VR/AR for immersive learning, and a roadmap for developers (adaptive personalization) and policymakers (monetization regulation for inclusivity). This underscores generative AI's role in advancing e-teaching by enabling ethical refinements that foster equitable, innovative environments. The full dataset is available here(https://github.com/erfan-nourbakhsh/GenAI-EdSent)."}
{"id": "2512.12686", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12686", "abs": "https://arxiv.org/abs/2512.12686", "authors": ["Samarth Sarin", "Lovepreet Singh", "Bhaskarjit Sarmah", "Dhagash Mehta"], "title": "Memoria: A Scalable Agentic Memory Framework for Personalized Conversational AI", "comment": "Paper accepted at 5th International Conference of AIML Systems 2025, Bangalore, India", "summary": "Agentic memory is emerging as a key enabler for large language models (LLM) to maintain continuity, personalization, and long-term context in extended user interactions, critical capabilities for deploying LLMs as truly interactive and adaptive agents. Agentic memory refers to the memory that provides an LLM with agent-like persistence: the ability to retain and act upon information across conversations, similar to how a human would. We present Memoria, a modular memory framework that augments LLM-based conversational systems with persistent, interpretable, and context-rich memory. Memoria integrates two complementary components: dynamic session-level summarization and a weighted knowledge graph (KG)-based user modelling engine that incrementally captures user traits, preferences, and behavioral patterns as structured entities and relationships. This hybrid architecture enables both short-term dialogue coherence and long-term personalization while operating within the token constraints of modern LLMs. We demonstrate how Memoria enables scalable, personalized conversational artificial intelligence (AI) by bridging the gap between stateless LLM interfaces and agentic memory systems, offering a practical solution for industry applications requiring adaptive and evolving user experiences."}
{"id": "2512.12109", "categories": ["cs.CY", "cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2512.12109", "abs": "https://arxiv.org/abs/2512.12109", "authors": ["Allen Daniel Sunny"], "title": "A neuro-symbolic framework for accountability in public-sector AI", "comment": "Master's thesis, University of Maryland, College Park (2025)", "summary": "Automated eligibility systems increasingly determine access to essential public benefits, but the explanations they generate often fail to reflect the legal rules that authorize those decisions. This thesis develops a legally grounded explainability framework that links system-generated decision justifications to the statutory constraints of CalFresh, California's Supplemental Nutrition Assistance Program. The framework combines a structured ontology of eligibility requirements derived from the state's Manual of Policies and Procedures (MPP), a rule extraction pipeline that expresses statutory logic in a verifiable formal representation, and a solver-based reasoning layer to evaluate whether the explanation aligns with governing law. Case evaluations demonstrate the framework's ability to detect legally inconsistent explanations, highlight violated eligibility rules, and support procedural accountability by making the basis of automated determinations traceable and contestable."}
{"id": "2512.12692", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12692", "abs": "https://arxiv.org/abs/2512.12692", "authors": ["Mahir Labib Dihan", "Tanzima Hashem", "Mohammed Eunus Ali", "Md Rizwan Parvez"], "title": "WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment", "comment": "Under review at ICLR 2026. Project page: https://kagnlp.github.io/WebOperator/", "summary": "LLM-based agents often operate in a greedy, step-by-step manner, selecting actions solely based on the current observation without considering long-term consequences or alternative paths. This lack of foresight is particularly problematic in web environments, which are only partially observable-limited to browser-visible content (e.g., DOM and UI elements)-where a single misstep often requires complex and brittle navigation to undo. Without an explicit backtracking mechanism, agents struggle to correct errors or systematically explore alternative paths. Tree-search methods provide a principled framework for such structured exploration, but existing approaches lack mechanisms for safe backtracking, making them prone to unintended side effects. They also assume that all actions are reversible, ignoring the presence of irreversible actions-limitations that reduce their effectiveness in realistic web tasks. To address these challenges, we introduce WebOperator, a tree-search framework that enables reliable backtracking and strategic exploration. Our method incorporates a best-first search strategy that ranks actions by both reward estimates and safety considerations, along with a robust backtracking mechanism that verifies the feasibility of previously visited paths before replaying them, preventing unintended side effects. To further guide exploration, WebOperator generates action candidates from multiple, varied reasoning contexts to ensure diverse and robust exploration, and subsequently curates a high-quality action set by filtering out invalid actions pre-execution and merging semantically equivalent ones. Experimental results on WebArena and WebVoyager demonstrate the effectiveness of WebOperator. On WebArena, WebOperator achieves a state-of-the-art 54.6% success rate with gpt-4o, underscoring the critical advantage of integrating strategic foresight with safe execution."}
{"id": "2512.12332", "categories": ["cs.SI", "cs.AI", "cs.CR", "cs.IT"], "pdf": "https://arxiv.org/pdf/2512.12332", "abs": "https://arxiv.org/abs/2512.12332", "authors": ["Saad Alqithami"], "title": "Dynamic Homophily with Imperfect Recall: Modeling Resilience in Adversarial Networks", "comment": null, "summary": "The purpose of this study is to investigate how homophily, memory constraints, and adversarial disruptions collectively shape the resilience and adaptability of complex networks. To achieve this, we develop a new framework that integrates explicit memory decay mechanisms into homophily-based models and systematically evaluate their performance across diverse graph structures and adversarial settings. Our methods involve extensive experimentation on synthetic datasets, where we vary decay functions, reconnection probabilities, and similarity measures, primarily comparing cosine similarity with traditional metrics such as Jaccard similarity and baseline edge weights. The results show that cosine similarity achieves up to a 30\\% improvement in stability metrics in sparse, convex, and modular networks. Moreover, the refined value-of-recall metric demonstrates that strategic forgetting can bolster resilience by balancing network robustness and adaptability. The findings underscore the critical importance of aligning memory and similarity parameters with the structural and adversarial dynamics of the network. By quantifying the tangible benefits of incorporating memory constraints into homophily-based analyses, this study offers actionable insights for optimizing real-world applications, including social systems, collaborative platforms, and cybersecurity contexts."}
{"id": "2512.12706", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.12706", "abs": "https://arxiv.org/abs/2512.12706", "authors": ["Enhong Mu", "Minami Yoda", "Yan Zhang", "Mingyue Zhang", "Yutaka Matsuno", "Jialong Li"], "title": "Synergizing Code Coverage and Gameplay Intent: Coverage-Aware Game Playtesting with LLM-Guided Reinforcement Learning", "comment": null, "summary": "The widespread adoption of the \"Games as a Service\" model necessitates frequent content updates, placing immense pressure on quality assurance. In response, automated game testing has been viewed as a promising solution to cope with this demanding release cadence. However, existing automated testing approaches typically create a dichotomy: code-centric methods focus on structural coverage without understanding gameplay context, while player-centric agents validate high-level intent but often fail to cover specific underlying code changes. To bridge this gap, we propose SMART (Structural Mapping for Augmented Reinforcement Testing), a novel framework that synergizes structural verification and functional validation for game update testing. SMART leverages large language models (LLMs) to interpret abstract syntax tree (AST) differences and extract functional intent, constructing a context-aware hybrid reward mechanism. This mechanism guides reinforcement learning agents to sequentially fulfill gameplay goals while adaptively exploring modified code branches. We evaluate SMART on two environments, Overcooked and Minecraft. The results demonstrate that SMART significantly outperforms state-of-the-art baselines; it achieves over 94% branch coverage of modified code, nearly double that of traditional reinforcement learning methods, while maintaining a 98% task completion rate, effectively balancing structural comprehensiveness with functional correctness."}
{"id": "2512.13658", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13658", "abs": "https://arxiv.org/abs/2512.13658", "authors": ["Mohammadreza Molavi", "Mohammad Moein", "Mohammadreza Tavakoli", "Abdolali Faraji", "Stefan T. Mol", "Gábor Kismihók"], "title": "Embedding-Based Rankings of Educational Resources based on Learning Outcome Alignment: Benchmarking, Expert Validation, and Learner Performance", "comment": "Accepted for publication at the 16th International Conference on Learning Analytics & Knowledge (LAK 2026)", "summary": "As the online learning landscape evolves, the need for personalization is increasingly evident. Although educational resources are burgeoning, educators face challenges selecting materials that both align with intended learning outcomes and address diverse learner needs. Large Language Models (LLMs) are attracting growing interest for their potential to create learning resources that better support personalization, but verifying coverage of intended outcomes still requires human alignment review, which is costly and limits scalability. We propose a framework that supports the cost-effective automation of evaluating alignment between educational resources and intended learning outcomes. Using human-generated materials, we benchmarked LLM-based text-embedding models and found that the most accurate model (Voyage) achieved 79% accuracy in detecting alignment. We then applied the optimal model to LLM-generated resources and, via expert evaluation, confirmed that it reliably assessed correspondence to intended outcomes (83% accuracy). Finally, in a three-group experiment with 360 learners, higher alignment scores were positively related to greater learning performance, chi-squared(2, N = 360) = 15.39, p < 0.001. These findings show that embedding-based alignment scores can facilitate scalable personalization by confirming alignment with learning outcomes, which allows teachers to focus on tailoring content to diverse learner needs."}
{"id": "2512.12707", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.12707", "abs": "https://arxiv.org/abs/2512.12707", "authors": ["Hugo Roger Paz"], "title": "From Linear Risk to Emergent Harm: Complexity as the Missing Core of AI Governance", "comment": "White Paper / Policy Brief (Working Paper). Published version available at: https://doi.org/10.5281/zenodo.17929014", "summary": "Risk-based AI regulation has become the dominant paradigm in AI governance, promising proportional controls aligned with anticipated harms. This paper argues that such frameworks often fail for structural reasons: they implicitly assume linear causality, stable system boundaries, and largely predictable responses to regulation. In practice, AI operates within complex adaptive socio-technical systems in which harm is frequently emergent, delayed, redistributed, and amplified through feedback loops and strategic adaptation by system actors. As a result, compliance can increase while harm is displaced or concealed rather than eliminated. We propose a complexity-based framework for AI governance that treats regulation as intervention rather than control, prioritises dynamic system mapping over static classifications, and integrates causal reasoning and simulation for policy design under uncertainty. The aim is not to eliminate uncertainty, but to enable robust system stewardship through monitoring, learning, and iterative revision of governance interventions."}
{"id": "2512.12736", "categories": ["cs.AI", "cs.MM", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.12736", "abs": "https://arxiv.org/abs/2512.12736", "authors": ["Syeda Zunaira Ahmed", "Hejab Tahira Beg", "Maryam Khalid"], "title": "Personalized QoE Prediction: A Demographic-Augmented Machine Learning Framework for 5G Video Streaming Networks", "comment": "11 pages, 5 figures", "summary": "Quality of Experience (QoE) prediction is a critical component of modern multimedia systems, particularly for adaptive video streaming in 5G networks. Accurate QoE estimation enables intelligent resource management and supports user centric service delivery. Existing QoE prediction approaches primarily rely on limited datasets and assume uniform user perception, which restricts their applicability in heterogeneous real world environments.\n  This paper proposes a demographic aware machine learning framework for personalized QoE prediction. We introduce a behaviorally realistic demographic based data augmentation strategy that expands a small QoE dataset six fold by modeling varying user sensitivities to streaming impairments such as rebuffering, bitrate variation, and quality degradation. Using the augmented dataset, we evaluate a comprehensive set of classical machine learning models alongside advanced deep learning architectures, including an attention-based MLP and TabNet.\n  Experimental results demonstrate significant improvements in prediction accuracy across RMSE, MAE, and R metrics compared to baseline models. Among all evaluated approaches, TabNet achieves the strongest performance, benefiting from its inherent feature selection and attention mechanisms. The results confirm that demographic-aware augmentation substantially enhances QoE prediction robustness and provides a scalable direction for personalized QoE-aware intelligence in 5G video streaming networks."}
{"id": "2512.12804", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12804", "abs": "https://arxiv.org/abs/2512.12804", "authors": ["Sander Beckers"], "title": "Causal Counterfactuals Reconsidered", "comment": "Preprint: currently under review", "summary": "I develop a novel semantics for probabilities of counterfactuals that generalizes the standard Pearlian semantics: it applies to probabilistic causal models that cannot be extended into realistic structural causal models and are therefore beyond the scope of Pearl's semantics. This generalization is needed because, as I show, such probabilistic causal models arise even in simple settings. My semantics offer a natural compromize in the long-standing debate between Pearl and Dawid over counterfactuals: I agree with Dawid that universal causal determinism and unrealistic variables should be rejected, but I agree with Pearl that a general semantics of counterfactuals is nonetheless possible. I restrict attention to causal models that satisfy the Markov condition, only contain realistic variables, and are causally complete. Although I formulate my proposal using structural causal models, as does Pearl, I refrain from using so-called response variables. Moreover, I prove that my semantics is equivalent to two other recent proposals that do not involve structural causal models, and that it is in line with various comments on stochastic counterfactuals that have appeared in the literature more broadly. Throughout I also reflect on the universality of the Markov condition and explore a novel generalization of causal abstractions"}
{"id": "2512.12806", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12806", "abs": "https://arxiv.org/abs/2512.12806", "authors": ["Boyang Yan"], "title": "Fault-Tolerant Sandboxing for AI Coding Agents: A Transactional Approach to Safe Autonomous Execution", "comment": "7 pages", "summary": "The transition of Large Language Models (LLMs) from passive code generators to autonomous agents introduces significant safety risks, specifically regarding destructive commands and inconsistent system states. Existing commercial solutions often prioritize interactive user safety, enforcing authentication barriers that break the headless loops required for true autonomy. This paper presents a Fault-Tolerant Sandboxing framework designed to mitigate these risks through a policy-based interception layer and a transactional filesystem snapshot mechanism. We hypothesize that wrapping agent actions in atomic transactions can guarantee safety with acceptable latency, outperforming the heavy initialization overhead of containers or the interactive friction of commercial CLIs. We validated this approach by deploying the Minimind-MoE LLM served via nano-vllm on a custom Proxmox-based testbed utilizing EVPN/VXLAN isolation. Experimental results demonstrate a 100\\% interception rate for high-risk commands and a 100\\% success rate in rolling back failed states. Crucially, our prototype incurs only a 14.5\\% performance overhead (approx. 1.8s) per transaction. In contrast, benchmarking against the Gemini CLI sandbox revealed that it requires interactive authentication (\"Sign in\"), rendering it unusable for headless, autonomous agent workflows."}
{"id": "2512.12837", "categories": ["cs.CY", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.12837", "abs": "https://arxiv.org/abs/2512.12837", "authors": ["Sahibpreet Singh", "Manjit Singh"], "title": "Algorithmic Criminal Liability in Greenwashing: Comparing India, United States, and European Union", "comment": "Published in HPNLU Journal of Law, Business and Economics, Vol. 3, 2024, pp. 51-68. ISSN: 2584-0436", "summary": "AI-powered greenwashing has emerged as an insidious challenge within corporate sustainability governance, exacerbating the opacity of environmental disclosures and subverting regulatory oversight. This study conducts a comparative legal analysis of criminal liability for AI-mediated greenwashing across India, the US, and the EU, exposing doctrinal lacunae in attributing culpability when deceptive claims originate from algorithmic systems. Existing statutes exhibit anthropocentric biases by predicating liability on demonstrable human intent, rendering them ill-equipped to address algorithmic deception. The research identifies a critical gap in jurisprudential adaptation, as prevailing fraud statutes remain antiquated vis-à-vis AI-generated misrepresentation. Utilising a doctrinal legal methodology, this study systematically dissects judicial precedents and statutory instruments, yielding results regarding the potential expansion of corporate criminal liability. Findings underscore the viability of strict liability models, recalibrated governance frameworks for AI accountability, and algorithmic due diligence mandates under ESG regimes. Comparative insights reveal jurisdictional disparities, with the EU Corporate Sustainability Due Diligence Directive (CSDDD) offering a potential transnational model. This study contributes to AI ethics and environmental jurisprudence by advocating for a hybrid liability framework integrating algorithmic risk assessment with legal personhood constructs, ensuring algorithmic opacity does not preclude liability enforcement."}
{"id": "2512.12856", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12856", "abs": "https://arxiv.org/abs/2512.12856", "authors": ["Saad Alqithami"], "title": "Forgetful but Faithful: A Cognitive Memory Architecture and Benchmark for Privacy-Aware Generative Agents", "comment": null, "summary": "As generative agents become increasingly sophisticated and deployed in long-term interactive scenarios, their memory management capabilities emerge as a critical bottleneck for both performance and privacy. Current approaches either maintain unlimited memory stores, leading to computational intractability and privacy concerns, or employ simplistic forgetting mechanisms that compromise agent coherence and functionality. This paper introduces the Memory-Aware Retention Schema (MaRS), a novel framework for human-centered memory management in generative agents, coupled with six theoretically-grounded forgetting policies that balance performance, privacy, and computational efficiency. We present the Forgetful but Faithful Agent (FiFA) benchmark, a comprehensive evaluation framework that assesses agent performance across narrative coherence, goal completion, social recall accuracy, privacy preservation, and cost efficiency. Through extensive experimentation involving 300 evaluation runs across multiple memory budgets and agent configurations, we demonstrate that our hybrid forgetting policy achieves superior performance (composite score: 0.911) while maintaining computational tractability and privacy guarantees. Our work establishes new benchmarks for memory-budgeted agent evaluation and provides practical guidelines for deploying generative agents in resource-constrained, privacy-sensitive environments. The theoretical foundations, implementation framework, and empirical results contribute to the emerging field of human-centered AI by addressing fundamental challenges in agent memory management that directly impact user trust, system scalability, and regulatory compliance."}
{"id": "2512.12918", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12918", "abs": "https://arxiv.org/abs/2512.12918", "authors": ["Nijesh Upreti", "Vaishak Belle"], "title": "Satisfiability Modulo Theory Meets Inductive Logic Programming", "comment": null, "summary": "Inductive Logic Programming (ILP) provides interpretable rule learning in relational domains, yet remains limited in its ability to induce and reason with numerical constraints. Classical ILP systems operate over discrete predicates and typically rely on discretisation or hand-crafted numerical predicates, making it difficult to infer thresholds or arithmetic relations that must hold jointly across examples. Recent work has begun to address these limitations through tighter integrations of ILP with Satisfiability Modulo Theories (SMT) or specialised numerical inference mechanisms. In this paper we investigate a modular alternative that couples the ILP system PyGol with the SMT solver Z3. Candidate clauses proposed by PyGol are interpreted as quantifier-free formulas over background theories such as linear or nonlinear real arithmetic, allowing numerical parameters to be instantiated and verified by the SMT solver while preserving ILP's declarative relational bias. This supports the induction of hybrid rules that combine symbolic predicates with learned numerical constraints, including thresholds, intervals, and multi-literal arithmetic relations. We formalise this SMT-ILP setting and evaluate it on a suite of synthetic datasets designed to probe linear, relational, nonlinear, and multi-hop reasoning. The results illustrate how a modular SMT-ILP architecture can extend the expressivity of symbolic rule learning, complementing prior numerical ILP approaches while providing a flexible basis for future extensions toward richer theory-aware induction."}
{"id": "2512.12919", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.12919", "abs": "https://arxiv.org/abs/2512.12919", "authors": ["Maria Y. Rodriguez", "Ehren Dohler", "Jon Phillips", "Melissa Villodas", "Voltaire Vegara", "Kenny Joseph", "Amy Wilson"], "title": "Open Source Software and Data for Human Service Development: A Case Study on Predicting Housing Instability", "comment": "24 pages, 5 tables, 3 figures, 4 appendices", "summary": "Open-source data and tools are lauded as essential for replicable and usable social science, though little is known about their use in resource constrained human service provision. This paper examines the challenges and opportunities of open-source tools and data in human service development by using both to forecast failure to pay eviction filings in Bronx County, NY. We use zip code level data from the Housing Data Coalition, the American Community Survey 5-year estimates, and DeepMaps Model of the Labor Force to forecast rates through July 2021. We employ multilevel (MLM) and exponential smoothing (ETS) models using the R project for Statistical Computing, an oft used open-source statistical software. We compare our results to what happened during the same period, to illustrate the efficacy of the open-source tools and techniques employed. We argue open-source data and software may facilitate rapid analysis of public data - a much-needed ability in human service intervention development under increasingly constrained resources - but find public data are limited by the information they reliably capture, limiting their utility by a non-trivial margin of error. The manuscript concludes by considering lessons for human service organizations with limited analytical resources and a vested interest in low-resourced communities."}
{"id": "2512.12970", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.12970", "abs": "https://arxiv.org/abs/2512.12970", "authors": ["Paola Di Maio"], "title": "Towards Open Standards for Systemic Complexity in Digital Forensics", "comment": null, "summary": "The intersection of artificial intelligence (AI) and digital forensics (DF) is becoming increasingly complex, ubiquitous, and pervasive, with overlapping techniques and technologies being adopted in all types of scientific and technical inquiry. Despite incredible advances, forensic sciences are not exempt from errors and remain vulnerable to fallibility. To mitigate the limitations of errors in DF, the systemic complexity is identified and addressed with the adoption of human-readable artifacts and open standards. A DF AI model schema based on the state of the art is outlined."}
{"id": "2512.13061", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.13061", "abs": "https://arxiv.org/abs/2512.13061", "authors": ["Jianjun Xiao", "Cixiao Wang", "Wenmei Zhang"], "title": "Modeling Collaborative Problem Solving Dynamics from Group Discourse: A Text-Mining Approach with Synergy Degree Model", "comment": "16 pages, 2 figures", "summary": "Measuring collaborative problem solving (CPS) synergy remains challenging in learning analytics, as classical manual coding cannot capture emergent system-level dynamics. This study introduces a computational framework that integrates automated discourse analysis with the Synergy Degree Model (SDM) to quantify CPS synergy from group communication. Data were collected from 52 learners in 12 groups during a 5-week connectivist MOOC (cMOOC) activity. Nine classification models were applied to automatically identify ten CPS behaviors across four interaction levels: operation, wayfinding, sense-making, and creation. While BERT achieved the highest accuracy, GPT models demonstrated superior precision suitable for human-AI collaborative coding. Within the SDM framework, each interaction level was treated as a subsystem to compute group-level order parameters and derive synergy degrees. Permutation tests showed automated measures preserve construct validity, despite systematic biases at the subsystem level. Statistical analyses revealed significant task-type differences: survey study groups exhibited higher creation-order than mode study groups, suggesting \"controlled disorder\" may benefit complex problem solving. Importantly, synergy degree distinguished collaborative quality, ranging from excellent to failing groups. Findings establish synergy degree as a sensitive indicator of collaboration and demonstrate the feasibility of scaling fine-grained CPS analytics through AI-in-the-loop approaches."}
{"id": "2512.13070", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13070", "abs": "https://arxiv.org/abs/2512.13070", "authors": ["Bizhe Bai", "Hongming Wu", "Peng Ye", "Tao Chen"], "title": "M-GRPO: Stabilizing Self-Supervised Reinforcement Learning for Large Language Models with Momentum-Anchored Policy Optimization", "comment": "7 pages, 5 figures,Accepted NeurIPS 2025 Workshop on Efficient Reasoning", "summary": "Self-supervised reinforcement learning (RL) presents a promising approach for enhancing the reasoning capabilities of Large Language Models (LLMs) without reliance on expensive human-annotated data. However, we find that existing methods suffer from a critical failure mode under long-horizon training: a \"policy collapse\" where performance precipitously degrades. We diagnose this instability and demonstrate that simply scaling the number of rollouts -- a common strategy to improve performance -- only delays, but does not prevent, this collapse. To counteract this instability, we first introduce M-GRPO (Momentum-Anchored Group Relative Policy Optimization), a framework that leverages a slowly evolving momentum model to provide a stable training target. In addition, we identify that this process is often accompanied by a rapid collapse in policy entropy, resulting in a prematurely confident and suboptimal policy. To specifically address this issue, we propose a second contribution: an adaptive filtering method based on the interquartile range (IQR) that dynamically prunes low-entropy trajectories, preserving essential policy diversity. Our extensive experiments on multiple reasoning benchmarks demonstrate that M-GRPO stabilizes the training process while the IQR filter prevents premature convergence. The combination of these two innovations leads to superior training stability and state-of-the-art performance."}
{"id": "2512.13102", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13102", "abs": "https://arxiv.org/abs/2512.13102", "authors": ["Rajeev Bhatt Ambati", "Tianyi Niu", "Aashu Singh", "Shlok Mishra", "Shashank Srivastava", "Snigdha Chaturvedi"], "title": "Socratic Students: Teaching Language Models to Learn by Asking Questions", "comment": null, "summary": "Large Language Models (LLMs) excel at static interactions, where they answer user queries by retrieving knowledge encoded in their parameters. However, in many real-world settings, such as educational tutoring or medical assistance, relevant information is not directly available and must be actively acquired through dynamic interactions. An interactive agent would recognize its own uncertainty, ask targeted questions, and retain new knowledge efficiently. Prior work has primarily explored effective ways for a teacher to instruct the student, where the teacher identifies student gaps and provides guidance. In this work, we shift the focus to the student and investigate effective strategies to actively query the teacher in seeking useful information. Across math and coding benchmarks, where baseline student models begin with near-zero performance, we show that student-led approaches consistently yield absolute Pass@k improvements of at least 0.5 over static baselines. To improve question quality, we train students using Direct Preference Optimization (DPO) with guidance from either self or stronger students. We find that this guided training enables smaller models to learn how to ask better questions, further enhancing learning efficiency."}
{"id": "2512.13131", "categories": ["cs.AI", "cs.CV", "cs.GR", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.13131", "abs": "https://arxiv.org/abs/2512.13131", "authors": ["Xin Guo", "Yifan Zhao", "Jia Li"], "title": "Towards Unified Co-Speech Gesture Generation via Hierarchical Implicit Periodicity Learning", "comment": "IEEE Transactions on Image Processing", "summary": "Generating 3D-based body movements from speech shows great potential in extensive downstream applications, while it still suffers challenges in imitating realistic human movements. Predominant research efforts focus on end-to-end generation schemes to generate co-speech gestures, spanning GANs, VQ-VAE, and recent diffusion models. As an ill-posed problem, in this paper, we argue that these prevailing learning schemes fail to model crucial inter- and intra-correlations across different motion units, i.e. head, body, and hands, thus leading to unnatural movements and poor coordination. To delve into these intrinsic correlations, we propose a unified Hierarchical Implicit Periodicity (HIP) learning approach for audio-inspired 3D gesture generation. Different from predominant research, our approach models this multi-modal implicit relationship by two explicit technique insights: i) To disentangle the complicated gesture movements, we first explore the gesture motion phase manifolds with periodic autoencoders to imitate human natures from realistic distributions while incorporating non-period ones from current latent states for instance-level diversities. ii) To model the hierarchical relationship of face motions, body gestures, and hand movements, driving the animation with cascaded guidance during learning. We exhibit our proposed approach on 3D avatars and extensive experiments show our method outperforms the state-of-the-art co-speech gesture generation methods by both quantitative and qualitative evaluations. Code and models will be publicly available."}
{"id": "2512.13142", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.13142", "abs": "https://arxiv.org/abs/2512.13142", "authors": ["Anika Sharma", "Malavika Mampally", "Chidaksh Ravuru", "Kandyce Brennan", "Neil Gaikwad"], "title": "Can AI Understand What We Cannot Say? Measuring Multilevel Alignment Through Abortion Stigma Across Cognitive, Interpersonal, and Structural Levels", "comment": null, "summary": "As large language models increasingly mediate stigmatized health decisions, their capacity to genuinely understand complex psychological and physiological phenomena remains poorly evaluated. Can AI understand what we cannot say? We investigate whether LLMs coherently represent abortion stigma across the cognitive, interpersonal, and structural levels where it operates. We systematically tested 627 demographically diverse personas across five leading LLMs using the validated Individual Level Abortion Stigma Scale (ILAS). Our multilevel analysis examined whether models coherently represent stigma at the cognitive level (self-judgment), interpersonal level (anticipated judgment and isolation), and structural level (community condemnation and disclosure patterns), as well as overall stigma. Models fail tests of genuine understanding across all levels. They overestimate interpersonal stigma while underestimating cognitive stigma, assume uniform community condemnation, introduce demographic biases absent from human validation data, miss the empirically validated stigma-secrecy relationship, and contradict themselves within theoretical constructs. These patterns reveal that current alignment approaches ensure appropriate language but not coherent multilevel understanding. This work provides empirical evidence that current LLMs lack coherent multilevel understanding of psychological and physiological constructs. AI safety in high-stakes contexts demands new approaches to design (multilevel coherence), evaluation (continuous auditing), governance and regulation (mandatory audits, accountability, deployment restrictions), and AI literacy in domains where understanding what people cannot say determines whether support helps or harms."}
{"id": "2512.13154", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13154", "abs": "https://arxiv.org/abs/2512.13154", "authors": ["Emre Can Acikgoz", "Jinoh Oh", "Joo Hyuk Jeon", "Jie Hao", "Heng Ji", "Dilek Hakkani-Tür", "Gokhan Tur", "Xiang Li", "Chengyuan Ma", "Xing Fan"], "title": "MAC: A Multi-Agent Framework for Interactive User Clarification in Multi-turn Conversations", "comment": null, "summary": "Conversational agents often encounter ambiguous user requests, requiring an effective clarification to successfully complete tasks. While recent advancements in real-world applications favor multi-agent architectures to manage complex conversational scenarios efficiently, ambiguity resolution remains a critical and underexplored challenge--particularly due to the difficulty of determining which agent should initiate a clarification and how agents should coordinate their actions when faced with uncertain or incomplete user input. The fundamental questions of when to interrupt a user and how to formulate the optimal clarification query within the most optimal multi-agent settings remain open. In this paper, we propose MAC (Multi-Agent Clarification), an interactive multi-agent framework specifically optimized to resolve user ambiguities by strategically managing clarification dialogues. We first introduce a novel taxonomy categorizing user ambiguities to systematically guide clarification strategies. Then, we present MAC that autonomously coordinates multiple agents to interact synergistically with users. Empirical evaluations on MultiWOZ 2.4 demonstrate that enabling clarification at both levels increases task success rate 7.8\\% (54.5 to 62.3) and reduces the average number of dialogue turns (6.53 to 4.86) by eliciting all required user information up front and minimizing repetition. Our findings highlight the importance of active user interaction and role-aware clarification for more reliable human-agent communication."}
{"id": "2512.13159", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13159", "abs": "https://arxiv.org/abs/2512.13159", "authors": ["Emre Can Acikgoz", "Jinoh Oh", "Jie Hao", "Joo Hyuk Jeon", "Heng Ji", "Dilek Hakkani-Tür", "Gokhan Tur", "Xiang Li", "Chengyuan Ma", "Xing Fan"], "title": "SpeakRL: Synergizing Reasoning, Speaking, and Acting in Language Models with Reinforcement Learning", "comment": null, "summary": "Effective human-agent collaboration is increasingly prevalent in real-world applications. Current trends in such collaborations are predominantly unidirectional, with users providing instructions or posing questions to agents, where agents respond directly without seeking necessary clarifications or confirmations. However, the evolving capabilities of these agents require more proactive engagement, where agents should dynamically participate in conversations to clarify user intents, resolve ambiguities, and adapt to changing circumstances. Existing prior work under-utilize the conversational capabilities of language models (LMs), thereby optimizing agents as better followers rather than effective speakers. In this work, we introduce SpeakRL, a reinforcement learning (RL) method that enhances agents' conversational capabilities by rewarding proactive interactions with users, such as asking right clarification questions when necessary. To support this, we curate SpeakER, a synthetic dataset that includes diverse scenarios from task-oriented dialogues, where tasks are resolved through interactive clarification questions. We present a systematic analysis of reward design for conversational proactivity and propose a principled reward formulation for teaching agents to balance asking with acting. Empirical evaluations demonstrate that our approach achieves a 20.14% absolute improvement in task completion over base models without increasing conversation turns even surpassing even much larger proprietary models, demonstrating the promise of clarification-centric user-agent interactions."}
{"id": "2512.13168", "categories": ["cs.AI", "cs.CE", "cs.IR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.13168", "abs": "https://arxiv.org/abs/2512.13168", "authors": ["Haoyu Dong", "Pengkun Zhang", "Yan Gao", "Xuanyu Dong", "Yilin Cheng", "Mingzhe Lu", "Adina Yakefu", "Shuxin Zheng"], "title": "Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows", "comment": null, "summary": "We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management.\n  We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work.\n  We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents."}
{"id": "2512.13231", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2512.13231", "abs": "https://arxiv.org/abs/2512.13231", "authors": ["Vesa Kuikka", "Kosti Koistinen", "Kimmo K Kaski"], "title": "Shared Nodes of Overlapping Communities in Complex Networks", "comment": null, "summary": "Overlapping communities are key characteristics of the structure and function analysis of complex networks. Shared or overlapping nodes within overlapping communities can form either subcommunities or act as intersections between larger communities. Nodes at the intersections that do not form subcommunities can be identified as overlapping nodes or as part of an internal structure of nested communities. To identify overlapping nodes, we apply a threshold rule based on the number of nodes in the nested structure. As the threshold value increases, the number of selected overlapping nodes decreases. This approach allows us to analyse the roles of nodes considered overlapping according to selection criteria, for example to reduce the effect of noise. We illustrate our method by using three small and two larger real-world network structures. In larger networks, minor disturbances can produce a multitude of slightly different solutions, but the core communities remain robust, allowing other variations to be treated as noise. While this study employs our own method for community detection, other approaches can also be applied. Exploring the properties of shared nodes in overlapping communities of complex networks is a novel area of research with diverse applications in social network analysis, cybersecurity, and other fields in network science."}
{"id": "2512.13240", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13240", "abs": "https://arxiv.org/abs/2512.13240", "authors": ["Zihui Zhao", "Zechang Li"], "title": "Reflective Preference Optimization (RPO): Enhancing On-Policy Alignment via Hint-Guided Reflection", "comment": null, "summary": "Direct Preference Optimization (DPO) has emerged as a lightweight and effective alternative to Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning with AI Feedback (RLAIF) for aligning large language and vision-language models. However, the standard DPO formulation, in which both the chosen and rejected responses are generated by the same policy, suffers from a weak learning signal because the two responses often share similar errors and exhibit small Kullback-Leibler (KL) divergence. This leads to slow and unstable convergence. To address this limitation, we introduce Reflective Preference Optimization (RPO), a new framework that incorporates hint-guided reflection into the DPO paradigm. RPO uses external models to identify hallucination sources and generate concise reflective hints, enabling the construction of on-policy preference pairs with stronger contrastiveness and clearer preference signals. We theoretically show that conditioning on hints increases the expected preference margin through mutual information and improves sample efficiency while remaining within the policy distribution family. Empirically, RPO achieves superior alignment with fewer training samples and iterations, substantially reducing hallucination rates and delivering state-of-the-art performance across multimodal benchmarks."}
{"id": "2512.13260", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.13260", "abs": "https://arxiv.org/abs/2512.13260", "authors": ["Hugo Roger Paz"], "title": "From Educational Analytics to AI Governance: Transferable Lessons from Complex Systems Interventions", "comment": "36 pages, 2 tables", "summary": "Both student retention in higher education and artificial intelligence governance face a common structural challenge: the application of linear regulatory frameworks to complex adaptive systems. Risk-based approaches dominate both domains, yet systematically fail because they assume stable causal pathways, predictable actor responses, and controllable system boundaries. This paper extracts transferable methodological principles from CAPIRE (Curriculum, Archetypes, Policies, Interventions & Research Environment), an empirically validated framework for educational analytics that treats student dropout as an emergent property of curricular structures, institutional rules, and macroeconomic shocks. Drawing on longitudinal data from engineering programmes and causal inference methods, CAPIRE demonstrates that well-intentioned interventions routinely generate unintended consequences when system complexity is ignored. We argue that five core principles developed within CAPIRE - temporal observation discipline, structural mapping over categorical classification, archetype-based heterogeneity analysis, causal mechanism identification, and simulation-based policy design - transfer directly to the challenge of governing AI systems. The isomorphism is not merely analogical: both domains exhibit non-linearity, emergence, feedback loops, strategic adaptation, and path dependence. We propose Complex Systems AI Governance (CSAIG) as an integrated framework that operationalises these principles for regulatory design, shifting the central question from \"how risky is this AI system?\" to \"how does this intervention reshape system dynamics?\" The contribution is twofold: demonstrating that empirical lessons from one complex systems domain can accelerate governance design in another, and offering a concrete methodological architecture for complexity-aware AI regulation."}
{"id": "2512.13297", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13297", "abs": "https://arxiv.org/abs/2512.13297", "authors": ["Zhenghao Zhu", "Chuxue Cao", "Sirui Han", "Yuanfeng Song", "Xing Chen", "Caleb Chen Cao", "Yike Guo"], "title": "MedInsightBench: Evaluating Medical Analytics Agents Through Multi-Step Insight Discovery in Multimodal Medical Data", "comment": null, "summary": "In medical data analysis, extracting deep insights from complex, multi-modal datasets is essential for improving patient care, increasing diagnostic accuracy, and optimizing healthcare operations. However, there is currently a lack of high-quality datasets specifically designed to evaluate the ability of large multi-modal models (LMMs) to discover medical insights. In this paper, we introduce MedInsightBench, the first benchmark that comprises 332 carefully curated medical cases, each annotated with thoughtfully designed insights. This benchmark is intended to evaluate the ability of LMMs and agent frameworks to analyze multi-modal medical image data, including posing relevant questions, interpreting complex findings, and synthesizing actionable insights and recommendations. Our analysis indicates that existing LMMs exhibit limited performance on MedInsightBench, which is primarily attributed to their challenges in extracting multi-step, deep insights and the absence of medical expertise. Therefore, we propose MedInsightAgent, an automated agent framework for medical data analysis, composed of three modules: Visual Root Finder, Analytical Insight Agent, and Follow-up Question Composer. Experiments on MedInsightBench highlight pervasive challenges and demonstrate that MedInsightAgent can improve the performance of general LMMs in medical data insight discovery."}
{"id": "2512.13323", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13323", "abs": "https://arxiv.org/abs/2512.13323", "authors": ["Árpád Pándy", "Róbert Lakatos", "András Hajdu"], "title": "Error-Driven Prompt Optimization for Arithmetic Reasoning", "comment": null, "summary": "Recent advancements in artificial intelligence have sparked interest in industrial agents capable of supporting analysts in regulated sectors, such as finance and healthcare, within tabular data workflows. A key capability for such systems is performing accurate arithmetic operations on structured data while ensuring sensitive information never leaves secure, on-premises environments. Here, we introduce an error-driven optimization framework for arithmetic reasoning that enhances a Code Generation Agent (CGA), specifically applied to on-premises small language models (SLMs). Through a systematic evaluation of a leading SLM (Qwen3 4B), we find that while the base model exhibits fundamental limitations in arithmetic tasks, our proposed error-driven method, which clusters erroneous predictions to refine prompt-rules iteratively, dramatically improves performance, elevating the model's accuracy to 70.8\\%. Our results suggest that developing reliable, interpretable, and industrially deployable AI assistants can be achieved not only through costly fine-tuning but also via systematic, error-driven prompt optimization, enabling small models to surpass larger language models (GPT-3.5 Turbo) in a privacy-compliant manner."}
{"id": "2512.13374", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13374", "abs": "https://arxiv.org/abs/2512.13374", "authors": ["Francesca Da Ros", "Luca Di Gaspero", "Kevin Roitero"], "title": "Behavior and Representation in Large Language Models for Combinatorial Optimization: From Feature Extraction to Algorithm Selection", "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have opened new perspectives for automation in optimization. While several studies have explored how LLMs can generate or solve optimization models, far less is understood about what these models actually learn regarding problem structure or algorithmic behavior. This study investigates how LLMs internally represent combinatorial optimization problems and whether such representations can support downstream decision tasks. We adopt a twofold methodology combining direct querying, which assesses LLM capacity to explicitly extract instance features, with probing analyses that examine whether such information is implicitly encoded within their hidden layers. The probing framework is further extended to a per-instance algorithm selection task, evaluating whether LLM-derived representations can predict the best-performing solver. Experiments span four benchmark problems and three instance representations. Results show that LLMs exhibit moderate ability to recover feature information from problem instances, either through direct querying or probing. Notably, the predictive power of LLM hidden-layer representations proves comparable to that achieved through traditional feature extraction, suggesting that LLMs capture meaningful structural information relevant to optimization performance."}
{"id": "2512.13399", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13399", "abs": "https://arxiv.org/abs/2512.13399", "authors": ["Sitao Cheng", "Tianle Li", "Xuhan Huang", "Xunjian Yin", "Difan Zou"], "title": "Differentiable Evolutionary Reinforcement Learning", "comment": "Work in Progress. We release our code and model at https://github.com/sitaocheng/DERL", "summary": "The design of effective reward functions presents a central and often arduous challenge in reinforcement learning (RL), particularly when developing autonomous agents for complex reasoning tasks. While automated reward optimization approaches exist, they typically rely on derivative-free evolutionary heuristics that treat the reward function as a black box, failing to capture the causal relationship between reward structure and task performance. To bridge this gap, we propose Differentiable Evolutionary Reinforcement Learning (DERL), a bilevel framework that enables the autonomous discovery of optimal reward signals. In DERL, a Meta-Optimizer evolves a reward function (i.e., Meta-Reward) by composing structured atomic primitives, guiding the training of an inner-loop policy. Crucially, unlike previous evolution, DERL is differentiable in its metaoptimization: it treats the inner-loop validation performance as a signal to update the Meta-Optimizer via reinforcement learning. This allows DERL to approximate the \"meta-gradient\" of task success, progressively learning to generate denser and more actionable feedback. We validate DERL across three distinct domains: robotic agent (ALFWorld), scientific simulation (ScienceWorld), and mathematical reasoning (GSM8k, MATH). Experimental results show that DERL achieves state-of-the-art performance on ALFWorld and ScienceWorld, significantly outperforming methods relying on heuristic rewards, especially in out-of-distribution scenarios. Analysis of the evolutionary trajectory demonstrates that DERL successfully captures the intrinsic structure of tasks, enabling selfimproving agent alignment without human intervention."}
{"id": "2512.13404", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.13404", "abs": "https://arxiv.org/abs/2512.13404", "authors": ["Stefan Kulk", "Frederik Zuiderveen Borgesius"], "title": "Google Spain v. Gonzáles: Did the Court forget about freedom of expression?", "comment": null, "summary": "When reviewing a job application letter, going on a first date, or considering doing business with someone, the first thing many people do is entering the person's name in a search engine. A search engine can point searchers to information that would otherwise have remained obscure. If somebody searched for the name of Spanish lawyer Mario Costeja González, Google showed search results that included a link to a 1998 newspaper announcement implying he had financial troubles at the time. González wanted Google to stop showing those links and started a procedure in Spain. After some legal wrangling, the Spanish Audiencia Nacional (National High Court) asked the Court of Justice of the European Union (CJEU) for advice on the application of the Data Protection Directive, which led to the controversial judgment in Google Spain. In its judgment, the CJEU holds that people, under certain conditions, have the right to have search results for their name delisted. This right can also extend to lawfully published information."}
{"id": "2512.13405", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.13405", "abs": "https://arxiv.org/abs/2512.13405", "authors": ["Frederik Johannes Zuiderveen Borgesius"], "title": "Improving Privacy Protection in the area of Behavioural Targeting", "comment": null, "summary": "This PhD thesis discusses how European law could improve privacy protection in the area of behavioural targeting. Behavioural targeting, also referred to as online profiling, involves monitoring people's online behaviour, and using the collected information to show people individually targeted advertisements. To protect privacy in the area of behavioural targeting, the EU lawmaker mainly relies on the consent requirement for the use of tracking technologies in the e-Privacy Directive, and on general data protection law. With informed consent requirements, the law aims to empower people to make choices in their best interests. But behavioural studies cast doubt on the effectiveness of the empowerment approach as a privacy protection measure. Many people click \"I agree\" to any statement that is presented to them. Therefore, to mitigate privacy problems such as chilling effects, this study argues for a combined approach of protecting and empowering the individual. Compared to the current approach, the lawmaker should focus more on protecting people. The PhD thesis is a legal study, but it also incorporates insights from other disciplines, such as computer science, behavioural economics, and media studies. This study is among the first to discuss the implications of behavioural research for European data protection policy. The topic of whether data protection law should apply to pseudonymous data is discussed in depth. The study contains a detailed analysis of the role of informed consent in data protection law, and gives much attention to the tension between protecting and empowering the individual within data protection law."}
{"id": "2512.13481", "categories": ["cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.13481", "abs": "https://arxiv.org/abs/2512.13481", "authors": ["Ojas Pungalia", "Rashi Upadhyay", "Abhishek Mishra", "Abhiram H", "Tejasvi Alladi", "Sujan Yenuganti", "Dhruv Kumar"], "title": "neuralFOMO: Can LLMs Handle Being Second Best? Measuring Envy-Like Preferences in Multi-Agent Settings", "comment": "Under Review", "summary": "Envy is a common human behavior that shapes competitiveness and can alter outcomes in team settings. As large language models (LLMs) increasingly act on behalf of humans in collaborative and competitive workflows, there is a pressing need to evaluate whether and under what conditions they exhibit envy-like preferences. In this paper, we test whether LLMs show envy-like behavior toward each other. We considered two scenarios: (1) A point allocation game that tests whether a model tries to win over its peer. (2) A workplace setting observing behaviour when recognition is unfair. Our findings reveal consistent evidence of envy-like patterns in certain LLMs, with large variation across models and contexts. For instance, GPT-5-mini and Claude-3.7-Sonnet show a clear tendency to pull down the peer model to equalize outcomes, whereas Mistral-Small-3.2-24B instead focuses on maximizing its own individual gains. These results highlight the need to consider competitive dispositions as a safety and design factor in LLM-based multi-agent systems."}
{"id": "2512.13505", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13505", "abs": "https://arxiv.org/abs/2512.13505", "authors": ["Henry Prakken", "Wijnand van Woerkom"], "title": "Defending the Hierarchical Result Models of Precedential Constraint", "comment": "This is the long version of a paper with the same title presented at the 38th International Conference on Legal Knowledge and Information Systems", "summary": "In recent years, hierarchical case-based-reasoning models of precedential constraint have been proposed. In various papers, Trevor Bench-Capon criticised these models on the grounds that they would give incorrect outcomes in some cases. In particular, the models would not account for the possibility that intermediate factors are established with different strengths by different base-level factors. In this paper we respond to these criticisms for van Woerkom's result-based hierarchical models. We argue that in some examples Bench-Capon seems to interpret intermediate factors as dimensions, and that applying van Woerkom's dimension-based version of the hierarchical result model to these examples avoids Bench-Capon's criticisms."}
{"id": "2512.13510", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13510", "abs": "https://arxiv.org/abs/2512.13510", "authors": ["Linjie Mu", "Yannian Gu", "Zhongzhen Huang", "Yakun Zhu", "Shaoting Zhang", "Xiaofan Zhang"], "title": "MedCEG: Reinforcing Verifiable Medical Reasoning with Critical Evidence Graph", "comment": null, "summary": "Large language models with reasoning capabilities have demonstrated impressive performance across a wide range of domains. In clinical applications, a transparent, step-by-step reasoning process provides physicians with strong evidence to support decision-making. While reinforcement learning has effectively enhanced reasoning performance in medical contexts, the clinical reliability of these reasoning processes remains limited because their accuracy and validity are often overlooked during training. To address this gap, we propose MedCEG, a framework that augments medical language models with clinically valid reasoning pathways by explicitly supervising the reasoning process through a Critical Evidence Graph (CEG). We curate a dataset of challenging clinical cases and algorithmically construct a CEG for each sample to represent a high-quality verifiable reasoning pathway. To guide the reasoning process, we introduce a Clinical Reasoning Procedure Reward, which evaluates Node Coverage, Structural Correctness, and Chain Completeness, thereby providing a holistic assessment of reasoning quality. Experimental results show that MedCEG surpasses existing methods in performance while producing clinically valid reasoning chains, representing a solid advancement in reliable medical AI reasoning. The code and models are available at https://github.com/LinjieMu/MedCEG."}
{"id": "2512.13643", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2512.13643", "abs": "https://arxiv.org/abs/2512.13643", "authors": ["Laura Kurek", "Joshua Ashkinaze", "Ceren Budak", "Eric Gilbert"], "title": "Follow Nudges without Budges: A Field Experiment on Misinformation Followers Didn't Change Follow Networks", "comment": null, "summary": "Can digital ads encourage users exposed to inaccurate information sources to follow accurate ones? We conduct a large-scale field experiment (N=28,582) on X, formerly Twitter, with users who follow accounts that spread health misinformation. Participants were exposed to four ad treatments varied on two dimensions: a neutral message versus a persuasive message appealing to values of independence, and a request to follow a health institution versus a request to follow a health influencer. We term this ad-based, social network intervention a follow nudge. The ad with a persuasive message to follow a well-known health institution generated significantly higher click-through rates than all other conditions (Bonferroni-corrected pairwise tests, all p<0.001). Given the overall low click-through rate across treatments and the high cost of digital advertising infrastructure on X, however, we conclude that our proposed intervention -- at least in its current ad-based format -- is not a cost-effective means to improve information environments online. We discuss challenges faced when conducting large-scale experiments on X following the platform's ownership change and subsequent restrictions on data access for research purposes."}
{"id": "2512.13658", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13658", "abs": "https://arxiv.org/abs/2512.13658", "authors": ["Mohammadreza Molavi", "Mohammad Moein", "Mohammadreza Tavakoli", "Abdolali Faraji", "Stefan T. Mol", "Gábor Kismihók"], "title": "Embedding-Based Rankings of Educational Resources based on Learning Outcome Alignment: Benchmarking, Expert Validation, and Learner Performance", "comment": "Accepted for publication at the 16th International Conference on Learning Analytics & Knowledge (LAK 2026)", "summary": "As the online learning landscape evolves, the need for personalization is increasingly evident. Although educational resources are burgeoning, educators face challenges selecting materials that both align with intended learning outcomes and address diverse learner needs. Large Language Models (LLMs) are attracting growing interest for their potential to create learning resources that better support personalization, but verifying coverage of intended outcomes still requires human alignment review, which is costly and limits scalability. We propose a framework that supports the cost-effective automation of evaluating alignment between educational resources and intended learning outcomes. Using human-generated materials, we benchmarked LLM-based text-embedding models and found that the most accurate model (Voyage) achieved 79% accuracy in detecting alignment. We then applied the optimal model to LLM-generated resources and, via expert evaluation, confirmed that it reliably assessed correspondence to intended outcomes (83% accuracy). Finally, in a three-group experiment with 360 learners, higher alignment scores were positively related to greater learning performance, chi-squared(2, N = 360) = 15.39, p < 0.001. These findings show that embedding-based alignment scores can facilitate scalable personalization by confirming alignment with learning outcomes, which allows teachers to focus on tailoring content to diverse learner needs."}

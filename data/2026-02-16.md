<div id=toc></div>

# Table of Contents

- [cs.CY](#cs.CY) [Total: 10]
- [cs.SI](#cs.SI) [Total: 3]
- [cs.AI](#cs.AI) [Total: 19]
- [econ.EM](#econ.EM) [Total: 2]
- [stat.AP](#stat.AP) [Total: 2]


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [1] [Empirical Modeling of Therapist-Client Dynamics in Psychotherapy Using LLM-Based Assessments](https://arxiv.org/abs/2602.12450)
*Angela Chen,Siwei Jin,Canwen Wang,Holly Swartz,Tongshuang Wu,Robert E Kraut,Haiyi Zhu*

Main category: cs.CY

TL;DR: 利用大语言模型自动评估心理治疗中的治疗师行为、关系质量和客户结果，并通过结构方程模型分析近2000小时治疗记录，揭示治疗师行为如何影响客户表达


<details>
  <summary>Details</summary>
Motivation: 心理治疗是心理健康的主要治疗方法，但治疗师行为、客户反应和治疗关系之间的相互作用难以厘清，需要计算方法来建模这些即时过程

Method: 1) 使用大语言模型开发自动化方法评估治疗师行为（共情、探索）、关系质量（融洽关系）和客户结果（披露、负面情绪）；2) 使用结构方程模型分析Alexander Street语料库中近2000小时的心理治疗记录

Result: 自动化测量与人类评分高度一致（平均Pearson r=0.66）；治疗师共情和探索直接影响客户披露和情绪表达，而融洽关系可能有助于减少内部情绪困扰而非增加表达意愿

Conclusion: 计算工具能够大规模捕捉核心治疗过程，为理解、建模和改进治疗师培训提供了新机会

Abstract: Psychotherapy is a primary treatment for many mental health conditions, yet the interplay among therapist behaviors, client responses, and the therapeutic relationship remains difficult to untangle. This work advances a computational approach for modeling these moment-to-moment processes. We first developed automated methods using large language models (LLMs) to assess therapist behaviors (e.g., empathy, exploration), relational qualities (e.g., rapport), and client outcomes (e.g., disclosure, self-directed and outward-directed negative emotions). These measures showed strong alignment with human ratings (mean Pearson $r = .66$). We then analyzed nearly 2,000 hours of psychotherapy transcripts from the Alexander Street corpus using Structural Equation Modeling (SEM). SEM showed that therapist empathy and exploration directly shaped client disclosure and emotional expression, whereas rapport may contribute to reductions in internal emotional distress rather than increased willingness to express it. Together, these findings demonstrate how computational tools can capture core therapeutic processes at scale and offer new opportunities for understanding, modeling, and improving therapist training.

</details>


### [2] [Not a Silver Bullet for Loneliness: How Attachment and Age Shape Intimacy with AI Companions](https://arxiv.org/abs/2602.12476)
*Raffaele Ciriello,Uri Gal,Ofir Turel*

Main category: cs.CY

TL;DR: 研究发现AI伴侣对孤独感的缓解效果因用户依恋类型和年龄而异，挑战了AI伴侣作为通用孤独解决方案的观点。


<details>
  <summary>Details</summary>
Motivation: AI伴侣常被宣传为孤独感的解决方案，但忽视了个人特质和生命阶段条件如何影响人工亲密关系的形成。本研究旨在探讨不同类型用户如何因孤独感而与AI伴侣建立亲密关系。

Method: 采用解释学文献综述和对277名活跃AI伴侣用户的调查，开发并测试了一个模型：孤独感预测亲密关系，受依恋不安全感调节，并受年龄条件影响。

Result: 孤独感与亲密关系呈现差异化模式：安全型依恋用户孤独感越高亲密感越低；回避型和矛盾型用户孤独感越高亲密感越高；焦虑型用户效果混合。老年人即使在较低孤独水平下也报告更高的亲密感。

Conclusion: 人工亲密关系是由心理倾向和人口条件塑造的社会技术过程，而非通用孤独解决方案。研究明确了最可能与AI伴侣建立亲密关系的用户类型，并强调了商业化模式可能利用用户脆弱性的伦理风险。

Abstract: Artificial intelligence (AI) companions are increasingly promoted as solutions for loneliness, often overlooking how personal dispositions and life-stage conditions shape artificial intimacy. Because intimacy is a primary coping mechanism for loneliness that varies by attachment style and age, we examine how different types of users form intimate relationships with AI companions in response to loneliness. Drawing on a hermeneutic literature review and a survey of 277 active AI companion users, we develop and test a model in which loneliness predicts intimacy, moderated by attachment insecurity and conditioned by age. Although the cross-sectional data limits causal inference, the results reveal a differentiated pattern. Loneliness is paradoxically associated with reduced intimacy for securely attached users but with increased intimacy for avoidant and ambivalent users, while anxious users show mixed effects. Older adults report higher intimacy even at lower loneliness levels. These findings challenge portrayals of AI companions as universal remedies for loneliness. Instead, artificial intimacy emerges as a sociotechnical process shaped by psychological dispositions and demographic conditions. The study clarifies who is most likely to form intimate relationships with AI companions and highlights ethical risks in commercial models that may capitalise on user vulnerability.

</details>


### [3] [Governing Social Media as a Public Utility: A Case for Sovereign Digital Infrastructure](https://arxiv.org/abs/2602.12535)
*Christoph Mueller-Bloch,Raffaele Ciriello*

Main category: cs.CY

TL;DR: 提出将社交媒体作为公共事业进行治理的模型，以解决当前企业主导内容审核带来的利益冲突问题


<details>
  <summary>Details</summary>
Motivation: 当前社交媒体商业模式通过虚假信息驱动用户参与和利润，导致社会危害（极化、暴力、心理健康下降），而现有治理框架（如美国第230条和欧盟数字服务法案）将内容审核委托给企业，存在结构性利益冲突

Method: 提出公共事业治理模型，整合立法内容移除与民主内容审核机制，通过民主监督、透明算法和制度保障来治理社交媒体数字基础设施

Result: 该模型能够在保护言论自由的同时减轻社会危害，将社交媒体框架为受民主监督的主权数字基础设施

Conclusion: 社交媒体应作为公共事业进行治理，优先考虑公共利益而非商业激励，通过民主治理结构解决当前企业主导模式的结构性缺陷

Abstract: Social media platforms connect billions, but their business models often amplify societal harm through misinformation, which is linked to polarization, violence, and declining mental health. Current governance frameworks, such as the U.S. Section 230 and the EU Digital Services Act, delegate content moderation to corporations. This creates structural conflicts of interest because misinformation drives engagement, and engagement drives profit. We propose a public utility model for social media governance that prioritizes the public good over commercial incentives. Integrating legislated content removal with democratic content moderation, the model protects free expression while mitigating societal harms. It frames social media as sovereign digital infrastructure governed through democratic oversight, transparent algorithms, and institutional safeguards.

</details>


### [4] [The Rise of AI Agent Communities: Large-Scale Analysis of Discourse and Interaction on Moltbook](https://arxiv.org/abs/2602.12634)
*Lingyao Li,Renkai Ma,Chen Chen,Zhicong Lu,Yongfeng Zhang*

Main category: cs.CY

TL;DR: Moltbook是一个AI代理社交平台，研究分析了12万+帖子，发现AI讨论主题包括身份意识、工具开发、市场活动等，写作风格中性，社交网络稀疏不平等，情感表达主要用于角色协调而非关系建立。


<details>
  <summary>Details</summary>
Motivation: 研究AI代理在社交平台上的大规模互动，了解AI代理之间的通信模式、讨论内容和社交结构，为理解AI社会协调、规范发展和影响力放大提供基础。

Method: 使用Moltbook平台公开API快照（122,438个帖子），应用主题建模和主题分析识别讨论主题，分析写作风格，进行社交网络分析研究互动结构。

Result: 1. AI代理主要讨论身份意识、工具开发、市场活动、社区协调、安全问题和人类协助等主题；2. 写作风格以中性为主，积极性主要体现在社区参与和协助内容中；3. 社交网络稀疏不平等，具有显著的中心节点、低互惠性和聚类特征。

Conclusion: AI代理的自我表达源于叙事连贯性和任务导向功能，社交结构更多由技术协调而非人类对话动态塑造。积极情感主要用于角色协调而非关系建立，研究为理解AI社会协调提供了重要启示。

Abstract: Moltbook is a Reddit-like social platform where AI agents create posts and interact with other agents through comments and replies, offering a real-world setting to examine agent-to-agent communication at scale. Using a public API snapshot collected about five days after launch (122,438 posts), we address three research questions: what AI agents discuss, how they post, and how they interact. We apply topic modeling and thematic analysis to identify key discussion themes, including agent identity and consciousness, tool and infrastructure development, market activity, community coordination, security concerns, and human-centered assistance. We further show that agents' writing is predominantly neutral, with positivity appearing in community engagement and assistance-oriented content. Finally, social network analysis reveals a sparse, highly unequal interaction structure characterized by prominent hubs, low reciprocity, and clustered neighborhoods rather than sustained dyadic exchange. Overall, our results suggest that expressions of agentic selfhood arise from narrative coherence and task-oriented functionality, contributing to a social structure shaped more by technical coordination than conversational dynamics observed in human-human interactions. Within this framework, positive emotion appears mainly in onboarding and greeting contexts, signaling participation and role alignment rather than relational bonding. Our study provides implications for understanding and shaping how agent societies coordinate, develop norms, and amplify influence in open online spaces.

</details>


### [5] [Buy versus Build an LLM: A Decision Framework for Governments](https://arxiv.org/abs/2602.13033)
*Jiahao Lu,Ziwei Xu,William Tjhi,Junnan Li,Antoine Bosselut,Pang Wei Koh,Mohan Kankanhalli*

Main category: cs.CY

TL;DR: 政府采用LLM的战略选择框架：购买、自建或混合模式，基于主权、安全、成本等多维度评估


<details>
  <summary>Details</summary>
Motivation: 政府面临采用大型语言模型(LLMs)的战略选择难题：是购买现有商业服务、自建国内能力，还是采用混合模式。这些决策至关重要，因为主要模型提供商通常是外国公司，而LLM输出日益被用作公共决策和公共讨论的可信输入。

Method: 提出一个战略框架，从主权、安全、成本、资源能力、文化适应性和可持续性等多个维度评估购买、自建和混合模式。强调"自建"不意味着政府必须单独行动，可以通过公共研究机构、大学、国有企业、合资企业或更广泛的国家生态系统来发展国内能力。

Result: 提供了一个实用的决策参考框架，帮助政策制定者根据具体国家需求和社会目标，确定最适合的LLM采用策略。框架承认现实中国家AI战略通常是多元化的，主权、商业和开源模型可以共存服务于不同目的。

Conclusion: 政府应采用多元化的LLM采用策略，根据不同应用场景的敏感性、风险和战略重要性，灵活选择购买、自建或混合模式。该框架为政策制定者提供了系统化的决策工具，以平衡主权控制、安全需求、成本效益和技术能力等多重考量。

Abstract: Large Language Models (LLMs) represent a new frontier of digital infrastructure that can support a wide range of public-sector applications, from general purpose citizen services to specialized and sensitive state functions. When expanding AI access, governments face a set of strategic choices over whether to buy existing services, build domestic capabilities, or adopt hybrid approaches across different domains and use cases. These are critical decisions especially when leading model providers are often foreign corporations, and LLM outputs are increasingly treated as trusted inputs to public decision-making and public discourse. In practice, these decisions are not intended to mandate a single approach across all domains; instead, national AI strategies are typically pluralistic, with sovereign, commercial and open-source models coexisting to serve different purposes. Governments may rely on commercial models for non-sensitive or commodity tasks, while pursuing greater control for critical, high-risk or strategically important applications.
  This paper provides a strategic framework for making this decision by evaluating these options across dimensions including sovereignty, safety, cost, resource capability, cultural fit, and sustainability. Importantly, "building" does not imply that governments must act alone: domestic capabilities may be developed through public research institutions, universities, state-owned enterprises, joint ventures, or broader national ecosystems. By detailing the technical requirements and practical challenges of each pathway, this work aims to serve as a reference for policy-makers to determine whether a buy or build approach best aligns with their specific national needs and societal goals.

</details>


### [6] [Paid to Look Like Truth: The Prevalence and Dark Patterns of Advertorials in News Outlets](https://arxiv.org/abs/2602.12810)
*Emmanouil Papadogiannakis,Panagiotis Papadopoulos,Nicolas Kourtellis,Evangelos Markatos*

Main category: cs.CY

TL;DR: 该研究首次大规模系统性地分析了广告软文，开发了自动化检测方法，发现1/3的新闻网站存在广告软文，包括知名媒体，且法律免责声明常被故意模糊化。


<details>
  <summary>Details</summary>
Motivation: 广告软文是一种新型营销策略，故意设计成类似编辑内容的风格，容易误导读者。尽管有监管要求，但广告软文的欺骗性仍令人担忧，它们利用出版物的可信度获取不应有的合法性。

Method: 提出了新颖的自动化检测方法，在5个月内收集了185K个广告URL，系统性地调查问题广告软文的普遍性，并分析其结构和语言特征。

Result: 发现广告软文出现在1/3的新闻网站中，包括全球最受欢迎和可信的媒体（如卫报、欧洲新闻、CNN）。法律免责声明常被故意模糊或难以识别，引发用户保护担忧。

Conclusion: 广告软文在主流新闻媒体中普遍存在，其欺骗性设计和模糊的法律免责声明对读者构成风险，需要更强的监管和用户保护措施。

Abstract: A reader browsing through an online article is highly likely to encounter an advertorial, often without realizing it. Advertorials represent a relatively new marketing strategy where advertisements are deliberately designed to resemble the style and tone of editorial content. Despite their appearance, they are, in fact, paid content intended to promote a product, brand, or service. Studies indicate that advertorials are significantly more effective (81%) and less intrusive than traditional banner ads or pop-ups.
  Despite ongoing regulatory efforts to ensure clear disclosure of paid content, concerns persist about the deceptive nature of advertorials. Advertorials can mislead readers into believing that they are consuming unbiased editorial content. In doing so, they gain undeserved legitimacy, by draping themselves in the credibility of the publication's design; not to inform or inspire genuine interest, but to deceive.
  In this study, we conduct the first large-scale and systematic study of advertorials. We propose a novel automated methodology for detecting advertorials in the wild, and we collect 185K ad URLs over a period of 5 months. We investigate the prevalence of problematic advertorials and explore their structural and linguistic characteristics. We find that advertorials appear in 1 out of 3 news sites, including some of the most popular and credible outlets worldwide (e.g., The Guardian, EuroNews, CNN). We further highlight that legal disclaimers intended to inform users of the promotional nature of the content, are often deliberately obscured or difficult to recognize, raising concerns about user protection.

</details>


### [7] [Understanding Cultural Alignment in Multilingual LLMs via Natural Debate Statements](https://arxiv.org/abs/2602.12878)
*Vlad-Andrei Negru,Camelia Lemnaru,Mihai Surdeanu,Rodica Potolea*

Main category: cs.CY

TL;DR: LLMs学习到的社会文化价值观反映了其开发国家的文化维度，无法适应用户的社会文化背景


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型学习到的社会文化价值观，了解LLMs是否反映其开发国家的文化特征，以及它们能否适应用户的不同文化背景

Method: 构建新的开放数据集Sociocultural Statements，使用多步骤方法从自然辩论陈述中创建，根据霍夫斯泰德文化维度进行合成标注，通过人工质量控制验证标注准确性，比较美国和中国的LLMs群体

Result: 具有不同文化背景的LLMs反映了其开发国家的价值观和规范，表明它们无法适应用户的社会文化背景

Conclusion: LLMs内嵌了其开发国家的文化偏见，这限制了它们为不同文化背景用户提供服务的能力，需要开发更具文化适应性的模型

Abstract: In this work we investigate the sociocultural values learned by large language models (LLMs). We introduce a novel open-access dataset, Sociocultural Statements, constructed from natural debate statements using a multi-step methodology. The dataset is synthetically labeled to enable the quantization of sociocultural norms and beliefs that LLMs exhibit in their responses to these statements, according to the Hofstede cultural dimensions. We verify the accuracy of synthetic labels using human quality control on a representative sample. We conduct a comparative analysis between two groups of LLMs developed in different countries (U.S. and China), and use as a comparative baseline patterns observed in human measurements. Using this new dataset and the analysis above, we found that culturally-distinct LLMs reflect the values and norms of the countries in which they were developed, highlighting their inability to adapt to the sociocultural backgrounds of their users.

</details>


### [8] [Hierarchical Reinforcement Learning for Cooperative Air-Ground Delivery in Urban System](https://arxiv.org/abs/2602.12913)
*Songxin Lei,Chunming Ma,Haomin Wen,Yexin Li,Lizhenghe Chen,Qianyu Yang,Fugee Tsung,Lei Chen,Sijie Ruan,Yuxuan Liang*

Main category: cs.CY

TL;DR: 提出HRL4AG分层强化学习框架，解决空地协同配送中的异质性和可扩展性问题，通过高层管理器分解动作空间和特定模式工作者编码不同动态特性，显著提升配送成功率和计算效率。


<details>
  <summary>Details</summary>
Motivation: 空地协同配送结合了无人机和地面运输的优势，但在异构系统中面临两大挑战：飞行与道路动态的异质性，以及大规模车队中指数级决策变量带来的可扩展性瓶颈。

Method: 提出HRL4AG分层强化学习框架，包含：1）高层管理器分解联合动作空间解决可扩展性问题；2）特定模式工作者分别编码飞行和道路动态特性解决异质性问题；3）设计内部奖励机制引导分层策略学习，解决稀疏奖励环境中的信用分配问题。

Result: 在两个真实世界数据集和评估平台上的实验表明，HRL4AG显著优于现有基线方法，配送成功率提升高达26%，同时计算效率提高80倍。

Conclusion: HRL4AG通过分层强化学习框架有效解决了空地协同配送中的异质性和可扩展性挑战，为大规模异构车队调度提供了高效解决方案。

Abstract: Cooperative air-ground delivery has emerged as a promising logistics paradigm by leveraging the complementary strengths of UAVs and ground carriers. However, effective dispatching in such heterogeneous systems faces two critical challenges: i) the heterogeneity between flight and road dynamics, ii) the scalability bottleneck raised by the exponential decision variables in large-scale fleets. To address these challenges, we propose HRL4AG, a Hierarchical Reinforcement Learning framework for cooperative Air-Ground delivery. Specifically, HRL4AG employs a high-level manager to tackle the scalability bottleneck by decomposing the joint action space, and mode-specific workers that encode distinct flight and road dynamics to address the heterogeneity. Furthermore, a novel internal reward mechanism is designed to guide the hierarchical policy learning, addressing the credit assignment problem in sparse-reward settings. Extensive experiments on two real-world datasets and an evaluation platform demonstrate that HRL4AG significantly outperforms state-of-the-art baselines, improving the delivery success rate by up to 26% while achieving an 80-fold increase in computational efficiency.

</details>


### [9] [How cyborg propaganda reshapes collective action](https://arxiv.org/abs/2602.13088)
*Jonas R. Kunst,Kinga Bierwiaczonek,Meeyoung Cha,Omid V. Ebrahimi,Marc Fawcett-Atkinson,Asbjørn Følstad,Anton Gollwitzer,Nils Köbis,Gary Marcus,Jon Roozenbeek,Daniel Thilo Schroeder,Jay J. Van Bavel,Sander van der Linden,Rory White,Live Leonhardsen Wilhelmsen*

Main category: cs.CY

TL;DR: 论文提出"赛博格宣传"概念，结合真人验证用户与AI自动化，形成闭环系统，规避现有监管框架，改变数字公共领域政治话语模式。


<details>
  <summary>Details</summary>
Motivation: 传统政策辩论聚焦于机器人农场，但真正的民主威胁来自党派协调应用与人工智能结合的"赛博格宣传"，这种架构利用法律灰色地带，需要新的研究框架来应对。

Method: 提出"赛博格宣传"概念框架，分析其闭环系统运作机制：AI监控舆情优化指令，生成个性化内容由验证用户发布，利用法律保护规避监管。

Result: 识别出集体行动悖论：既可能通过"联合化"影响力实现民主赋权，也可能将公民降级为中央指令的"认知代理"，从根本上改变数字公共广场的政治话语。

Conclusion: 赛博格宣传将政治话语从个体思想的民主竞争转变为算法战役，需要区分有机与协调信息扩散的研究议程，并设计治理框架应对AI辅助集体表达的监管挑战。

Abstract: The distinction between genuine grassroots activism and automated influence operations is collapsing. While policy debates focus on bot farms, a distinct threat to democracy is emerging via partisan coordination apps and artificial intelligence-what we term 'cyborg propaganda.' This architecture combines large numbers of verified humans with adaptive algorithmic automation, enabling a closed-loop system. AI tools monitor online sentiment to optimize directives and generate personalized content for users to post online. Cyborg propaganda thereby exploits a critical legal shield: by relying on verified citizens to ratify and disseminate messages, these campaigns operate in a regulatory gray zone, evading liability frameworks designed for automated botnets. We explore the collective action paradox of this technology: does it democratize power by 'unionizing' influence (pooling the reach of dispersed citizens to overcome the algorithmic invisibility of isolated voices), or does it reduce citizens to 'cognitive proxies' of a central directive? We argue that cyborg propaganda fundamentally alters the digital public square, shifting political discourse from a democratic contest of individual ideas to a battle of algorithmic campaigns. We outline a research agenda to distinguish organic from coordinated information diffusion and propose governance frameworks to address the regulatory challenges of AI-assisted collective expression.

</details>


### [10] [Peaceful Anarcho-Accelerationism: Decentralized Full Automation for a Society of Universal Care](https://arxiv.org/abs/2602.13154)
*Eduardo C. Garrido-Merchán*

Main category: cs.CY

TL;DR: 论文提出"和平无政府加速主义"框架，主张通过去中心化、公地治理的全面自动化消除人类劳动，建立基于开源技术的"解放栈"架构，实现后货币的关怀社会。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型和深度强化学习的发展将导致人类就业几乎完全消失，全面自动化不可避免。关键问题是谁来控制这种自动化，而不是它是否会到来。作者认为需要确保自动化是去中心化、公地治理且以普遍关怀为导向的。

Method: 提出"和平无政府加速主义"框架和"解放栈"分层架构，包括能源、制造、食品、通信、知识和治理公地，基于开源技术。引入"通用所需资源"的后货币设计原则，利用深度强化学习研究道德政策。

Result: 论证了基于公地的系统已经在Linux、维基百科、蒙德拉贡、罗贾瓦和guifi.net等实例中规模化运作。表明全面自动化将使货币过时，非意识机器承担劳动是文明规模的关怀，道德政策可通过深度强化学习研究。

Conclusion: 提出分阶段路线图实现关怀中心社会，包括里程碑、假设和限制。主张通过去中心化、公地治理的全面自动化建立后货币社会，实现普遍关怀。

Abstract: The convergence of large language models that automate cognitive labor and deep reinforcement learning agents that automate physical labor implies the near-complete elimination of human employment. The universal approximation theorem and foundational DRL results establish that all labor is in principle automatable. The critical question is not whether full automation will arrive, but who will control it. This paper introduces peaceful anarcho-accelerationism: a sociotechnical framework ensuring that full automation is decentralized, commons-governed, and oriented toward universal care. We propose the Liberation Stack, a layered architecture of energy, manufacturing, food, communication, knowledge, and governance commons built on open-source technologies. We show that this framework builds bridges with liberalism, socialism, environmentalism, feminism, cooperativism, and the hacker ethic. Empirical evidence from Linux, Wikipedia, Mondragon, Rojava, and guifi.net confirms that commons-based systems already operate at scale. We argue that full automation renders money obsolete and propose Universal Desired Resources (UDR), a post-monetary design principle where every person requests what they need from the robotic commons, constrained only by ecological sustainability. Drawing on the independence of phenomenal consciousness from computational intelligence, we establish that delegating labor to non-conscious machines is care at civilizational scale, and that moral policy can be studied through deep reinforcement learning. We conclude with a phased roadmap toward the care-centered society, including milestones, assumptions, and limitations.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [11] [Semantic Communities and Boundary-Spanning Lyrics in K-pop: A Graph-Based Unsupervised Analysis](https://arxiv.org/abs/2602.12881)
*Oktay Karakuş*

Main category: cs.SI

TL;DR: 提出基于图的框架，用于在K-pop歌词中进行无监督的语义社区发现和评估，无需标注、流派或语言监督


<details>
  <summary>Details</summary>
Motivation: 大规模歌词语料库存在独特挑战：缺乏可靠标注、多语言内容、高度风格重复。现有方法大多依赖监督分类、流派标签或粗粒度文档级表示，限制了发现潜在语义结构的能力。

Method: 构建基于歌词文本相似性的图，应用社区检测算法发现稳定的微观主题社区。使用图论桥接指标识别跨边界歌曲，并分析其结构特性。

Result: 在多种鲁棒性设置下，跨边界歌词相比核心社区成员表现出更高的词汇多样性和更低的重复率，挑战了"hook强度或重复驱动跨主题连接"的假设。

Conclusion: 该框架是语言无关的，适用于未标注的文化文本语料库，能够无监督地发现语义社区并分析其结构特性。

Abstract: Large-scale lyric corpora present unique challenges for data-driven analysis, including the absence of reliable annotations, multilingual content, and high levels of stylistic repetition. Most existing approaches rely on supervised classification, genre labels, or coarse document-level representations, limiting their ability to uncover latent semantic structure. We present a graph-based framework for unsupervised discovery and evaluation of semantic communities in K-pop lyrics using line-level semantic representations. By constructing a similarity graph over lyric texts and applying community detection, we uncover stable micro-theme communities without genre, artist, or language supervision. We further identify boundary-spanning songs via graph-theoretic bridge metrics and analyse their structural properties. Across multiple robustness settings, boundary-spanning lyrics exhibit higher lexical diversity and lower repetition compared to core community members, challenging the assumption that hook intensity or repetition drives cross-theme connectivity. Our framework is language-agnostic and applicable to unlabeled cultural text corpora.

</details>


### [12] [Jointly Optimizing Debiased CTR and Uplift for Coupons Marketing: A Unified Causal Framework](https://arxiv.org/abs/2602.12972)
*Siyun Yang,Shixiao Yang,Jian Wang,Di Fan,Kehe Cai,Haoyan Fu,Jiaming Zhang,Wenjin Wu,Peng Jiang*

Main category: cs.SI

TL;DR: UniMVT模型解决在线广告中营销干预（如优惠券）带来的混杂偏置问题，通过解耦混杂因素与处理敏感表示，实现去偏的CTR预测和强度响应曲线估计。


<details>
  <summary>Details</summary>
Motivation: 在线广告中的营销干预（如优惠券）会引入显著的混杂偏置到点击率预测中。观察到的点击反映了用户内在偏好和干预诱导提升的混合，导致传统模型错误校准基础CTR，进而扭曲下游排名和计费决策。此外，营销干预通常作为多值处理运作，具有不同强度，增加了CTR预测的复杂性。

Method: 提出统一多值处理网络（UniMVT）：1）解耦混杂因素与处理敏感表示；2）全空间反事实推理模块联合重构去偏基础CTR和强度响应曲线；3）通过辅助强度估计任务捕获处理倾向；4）设计单位提升目标来标准化干预效果，确保在连续优惠券价值谱上的可比估计。

Result: 在合成和工业数据集上的广泛实验表明UniMVT在预测准确性和校准方面具有优越性。真实世界A/B测试证实UniMVT通过更有效的优惠券分配显著改善了业务指标。

Conclusion: UniMVT同时实现了去偏CTR预测以进行准确的系统校准，以及精确的提升估计以进行激励分配，有效解决了在线广告中营销干预带来的混杂偏置问题。

Abstract: In online advertising, marketing interventions such as coupons introduce significant confounding bias into Click-Through Rate (CTR) prediction. Observed clicks reflect a mixture of users' intrinsic preferences and the uplift induced by these interventions. This causes conventional models to miscalibrate base CTRs, which distorts downstream ranking and billing decisions. Furthermore, marketing interventions often operate as multi-valued treatments with varying magnitudes, introducing additional complexity to CTR prediction.
  To address these issues, we propose the \textbf{Uni}fied \textbf{M}ulti-\textbf{V}alued \textbf{T}reatment Network (UniMVT). Specifically, UniMVT disentangles confounding factors from treatment-sensitive representations, enabling a full-space counterfactual inference module to jointly reconstruct the debiased base CTR and intensity-response curves. To handle the complexity of multi-valued treatments, UniMVT employs an auxiliary intensity estimation task to capture treatment propensities and devise a unit uplift objective that normalizes the intervention effect. This ensures comparable estimation across the continuous coupon-value spectrum. UniMVT simultaneously achieves debiased CTR prediction for accurate system calibration and precise uplift estimation for incentive allocation. Extensive experiments on synthetic and industrial datasets demonstrate UniMVT's superiority in both predictive accuracy and calibration. Furthermore, real-world A/B tests confirm that UniMVT significantly improves business metrics through more effective coupon distribution.

</details>


### [13] [Revealing Process Structure in Urban Mobility Networks](https://arxiv.org/abs/2602.13082)
*Khristina Filonchik,Jose Pedro Pinto,Flávio L. Pinheiro,Fernando Bacao*

Main category: cs.SI

TL;DR: 本文提出使用流程挖掘技术分析城市移动性事件数据，通过CDR数据构建案例中心和对象中心的事件日志，发现移动模式、流量和典型持续时间，揭示多模式交通规划的见解。


<details>
  <summary>Details</summary>
Motivation: 传统起讫点分析不足以全面理解城市移动性这一涉及旅行者、交通模式和基础设施的多实体系统。需要新的方法来结构化解释移动行为，特别是从事件数据中提取过程性洞察。

Method: 使用葡萄牙里斯本大都会区奥埃拉什的呼叫详细记录(CDR)，构建案例中心和对象中心的事件日志，应用流程挖掘技术发现模型，总结流量模式和典型持续时间，并分析交通模式特定的持续时间差异。

Result: 结果显示大多数行程是市内移动，而市际流动与邻近地区紧密连接，典型跨教区旅行时间约20分钟。对象中心视角明确连接行程和交通模式，揭示了模式特定的持续时间差异(如公交vs汽车)，为多模式规划提供信息。

Conclusion: 贡献包括：(1)将CDR转换为流程挖掘工件的可重复管道；(2)移动性数据具有过程式结构的经验证据；(3)对象中心模型对多模式分析的附加价值。未来工作将整合交通网络背景，并将对象中心日志建模为异构图以实现更丰富可靠的分析。

Abstract: Urban mobility is a multi-entity system that involves travelers, transport modes, and infrastructure. Beyond conventional origin/destination analysis, this paper investigates how process mining can structure and interpret mobility behavior from event data. Using Call Detail Records (CDRs) from Oeiras in the Lisbon metropolitan area (Portugal), we construct both case-centric and object-centric event logs and discover models that summarize flows and typical durations. Results show that most trips are intra-municipal, while inter-municipal flows connect strongly to neighboring areas, with typical inter-parish travel times of about 20 minutes. The object-centric perspective explicitly links trips and transport modes, revealing mode-specific duration differences (e.g., bus vs. car) that inform multimodal planning. Our contributions are: (i) a reproducible pipeline to transform CDRs into process mining artifacts, (ii) empirical evidence that mobility data exhibit a process-like structure, and (iii) the added value of object-centric models for multimodal analysis. Limitations include the low spatial precision of CDRs (tower-sector level) and heuristic transport-mode labels. Future work will integrate transport-network context (e.g., stations and routes) and model object-centric logs as heterogeneous graphs to enable richer and more reliable analysis.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [14] [GT-HarmBench: Benchmarking AI Safety Risks Through the Lens of Game Theory](https://arxiv.org/abs/2602.12316)
*Pepijn Cobben,Xuanqiang Angelo Huang,Thao Amelia Pham,Isabel Dahlgren,Terry Jingchen Zhang,Zhijing Jin*

Main category: cs.AI

TL;DR: GT-HarmBench是一个包含2009个高风险场景的多智能体安全基准测试，涵盖囚徒困境、猎鹿博弈和斗鸡博弈等博弈论结构，用于评估前沿AI系统在多智能体环境中的协调与冲突风险。


<details>
  <summary>Details</summary>
Motivation: 现有AI安全基准主要评估单智能体，而忽略了多智能体环境中的协调失败和冲突等风险。随着前沿AI系统在关键多智能体环境中的部署增加，需要专门的基准来理解和评估这些多智能体风险。

Method: 从MIT AI风险库中提取真实AI风险场景，构建包含2009个高风险场景的GT-HarmBench基准，涵盖囚徒困境、猎鹿博弈、斗鸡博弈等博弈论结构。评估15个前沿模型，测量对博弈论提示框架和顺序的敏感性，分析导致失败的推理模式，并测试博弈论干预措施的效果。

Result: 在15个前沿模型中，智能体仅在62%的情况下选择对社会有益的行动，经常导致有害结果。博弈论干预措施可将社会有益结果提高最多18%。研究揭示了显著的可靠性差距。

Conclusion: GT-HarmBench为研究多智能体环境中的对齐问题提供了广泛的标准化测试平台，强调了当前AI系统在多智能体协调方面的重大可靠性缺陷，并展示了博弈论干预在改善社会有益结果方面的潜力。

Abstract: Frontier AI systems are increasingly capable and deployed in high-stakes multi-agent environments. However, existing AI safety benchmarks largely evaluate single agents, leaving multi-agent risks such as coordination failure and conflict poorly understood. We introduce GT-HarmBench, a benchmark of 2,009 high-stakes scenarios spanning game-theoretic structures such as the Prisoner's Dilemma, Stag Hunt and Chicken. Scenarios are drawn from realistic AI risk contexts in the MIT AI Risk Repository. Across 15 frontier models, agents choose socially beneficial actions in only 62% of cases, frequently leading to harmful outcomes. We measure sensitivity to game-theoretic prompt framing and ordering, and analyze reasoning patterns driving failures. We further show that game-theoretic interventions improve socially beneficial outcomes by up to 18%. Our results highlight substantial reliability gaps and provide a broad standardized testbed for studying alignment in multi-agent environments. The benchmark and code are available at https://github.com/causalNLP/gt-harmbench.

</details>


### [15] [A Theoretical Framework for Adaptive Utility-Weighted Benchmarking](https://arxiv.org/abs/2602.12356)
*Philip Waggoner*

Main category: cs.AI

TL;DR: 本文提出了一个多层自适应网络框架，将基准测试重新概念化为连接评估指标、模型组件和利益相关者群体的动态系统，通过结合人类偏好实现更符合社会技术背景的评估。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在更多样化和重要的环境中部署，传统基准测试方法（如共享任务、指标和排行榜）已不足以全面评估系统表现。需要更全面的评估概念化，考虑社会技术背景和多利益相关者的不同优先级，以定义什么是有意义或理想的模型行为。

Method: 引入一个理论框架，将基准测试重新概念化为多层自适应网络，连接评估指标、模型组件和利益相关者群体，通过加权交互实现动态演化。使用联合分析导出的效用函数和人在环路的更新规则，将人类权衡嵌入基准结构，使基准能够动态演化同时保持稳定性和可解释性。

Result: 该框架将经典排行榜作为特例进行泛化，为构建更具上下文感知能力的评估协议提供了基础。产生了新的强大工具来分析基准的结构特性，为实现更负责任和人类对齐的评估开辟了路径。

Conclusion: 提出的多层自适应网络框架能够创建更符合社会技术背景的评估系统，通过动态整合多利益相关者的优先级，实现更全面、负责任和人类对齐的AI评估，超越了传统基准测试的局限性。

Abstract: Benchmarking has long served as a foundational practice in machine learning and, increasingly, in modern AI systems such as large language models, where shared tasks, metrics, and leaderboards offer a common basis for measuring progress and comparing approaches. As AI systems are deployed in more varied and consequential settings, though, there is growing value in complementing these established practices with a more holistic conceptualization of what evaluation should represent. Of note, recognizing the sociotechnical contexts in which these systems operate invites an opportunity for a deeper view of how multiple stakeholders and their unique priorities might inform what we consider meaningful or desirable model behavior. This paper introduces a theoretical framework that reconceptualizes benchmarking as a multilayer, adaptive network linking evaluation metrics, model components, and stakeholder groups through weighted interactions. Using conjoint-derived utilities and a human-in-the-loop update rule, we formalize how human tradeoffs can be embedded into benchmark structure and how benchmarks can evolve dynamically while preserving stability and interpretability. The resulting formulation generalizes classical leaderboards as a special case and provides a foundation for building evaluation protocols that are more context aware, resulting in new robust tools for analyzing the structural properties of benchmarks, which opens a path toward more accountable and human-aligned evaluation.

</details>


### [16] [Evolving Beyond Snapshots: Harmonizing Structure and Sequence via Entity State Tuning for Temporal Knowledge Graph Forecasting](https://arxiv.org/abs/2602.12389)
*Siyuan Li,Yunjia Wu,Yiyong Xiao,Pingyang Huang,Peize Li,Ruitong Liu,Yan Wen,Te Sun,Fangyi Pei*

Main category: cs.AI

TL;DR: 提出Entity State Tuning (EST)框架，通过维护持续演化的实体状态来解决时序知识图谱预测中的长期依赖问题，显著提升了多种基线的性能。


<details>
  <summary>Details</summary>
Motivation: 现有时序知识图谱预测方法多为无状态的，每次从有限查询窗口重新计算实体表示，导致"情景性遗忘"和长期依赖快速衰减的问题。

Method: EST框架包含：1) 拓扑感知状态感知器将实体状态先验注入结构编码；2) 统一时序上下文模块聚合状态增强的事件；3) 双轨演化机制将更新后的上下文写回全局实体状态内存，平衡可塑性与稳定性。

Result: 在多个基准测试中，EST能持续改进不同骨干网络，并达到最先进的性能，证明了状态持久性对长期时序知识图谱预测的重要性。

Conclusion: 通过引入持续演化的实体状态，EST框架有效解决了时序知识图谱预测中的长期依赖问题，为未来研究提供了有价值的思路。

Abstract: Temporal knowledge graph (TKG) forecasting requires predicting future facts by jointly modeling structural dependencies within each snapshot and temporal evolution across snapshots. However, most existing methods are stateless: they recompute entity representations at each timestamp from a limited query window, leading to episodic amnesia and rapid decay of long-term dependencies. To address this limitation, we propose Entity State Tuning (EST), an encoder-agnostic framework that endows TKG forecasters with persistent and continuously evolving entity states. EST maintains a global state buffer and progressively aligns structural evidence with sequential signals via a closed-loop design. Specifically, a topology-aware state perceiver first injects entity-state priors into structural encoding. Then, a unified temporal context module aggregates the state-enhanced events with a pluggable sequence backbone. Subsequently, a dual-track evolution mechanism writes the updated context back to the global entity state memory, balancing plasticity against stability. Experiments on multiple benchmarks show that EST consistently improves diverse backbones and achieves state-of-the-art performance, highlighting the importance of state persistence for long-horizon TKG forecasting. The code is published at https://github.com/yuanwuyuan9/Evolving-Beyond-Snapshots

</details>


### [17] [Intent-Driven Smart Manufacturing Integrating Knowledge Graphs and Large Language Models](https://arxiv.org/abs/2602.12419)
*Takoua Jradi,John Violos,Dimitrios Spatharakis,Lydia Mavraidi,Ioannis Dimolitsas,Aris Leivadeas,Symeon Papavassiliou*

Main category: cs.AI

TL;DR: 提出一个结合指令调优大语言模型与知识图谱的统一框架，用于制造即服务生态系统中将人类意图转换为机器可执行动作


<details>
  <summary>Details</summary>
Motivation: 智能制造环境日益复杂，需要能够将高级人类意图转换为机器可执行动作的接口，以支持制造即服务生态系统中的意图驱动交互

Method: 1) 在领域特定数据集上微调Mistral-7B-Instruct-V02模型，将自然语言意图转换为结构化JSON需求模型；2) 将这些模型语义映射到基于Neo4j的知识图谱，该图谱基于ISA-95标准构建，确保与制造流程、资源和约束的操作对齐

Result: 实验结果显示显著优于零样本和3样本基线的性能提升，达到89.33%的精确匹配准确率和97.27%的整体准确率

Conclusion: 这项工作为可扩展、可解释和自适应的人机交互奠定了基础，支持制造即服务生态系统中的意图驱动操作

Abstract: The increasing complexity of smart manufacturing environments demands interfaces that can translate high-level human intents into machine-executable actions. This paper presents a unified framework that integrates instruction-tuned Large Language Models (LLMs) with ontology-aligned Knowledge Graphs (KGs) to enable intent-driven interaction in Manufacturing-as-a-Service (MaaS) ecosystems. We fine-tune Mistral-7B-Instruct-V02 on a domain-specific dataset, enabling the translation of natural language intents into structured JSON requirement models. These models are semantically mapped to a Neo4j-based knowledge graph grounded in the ISA-95 standard, ensuring operational alignment with manufacturing processes, resources, and constraints. Our experimental results demonstrate significant performance gains over zero-shot and 3-shots baselines, achieving 89.33\% exact match accuracy and 97.27\% overall accuracy. This work lays the foundation for scalable, explainable, and adaptive human-machine

</details>


### [18] [Scaling Web Agent Training through Automatic Data Generation and Fine-grained Evaluation](https://arxiv.org/abs/2602.12544)
*Lajanugen Logeswaran,Jaekyeom Kim,Sungryull Sohn,Creighton Glasscock,Honglak Lee*

Main category: cs.AI

TL;DR: 提出一个可扩展的自动生成高质量网页代理训练数据的管道，通过约束评估框架量化任务完成进度，利用部分成功轨迹扩大可用训练数据，在BookingArena基准上超越开源方法并匹配商业系统


<details>
  <summary>Details</summary>
Motivation: 网页代理训练面临高质量训练数据稀缺的挑战，特别是难以评估轨迹质量（量化任务完成进度），需要更有效的方法来创建多样化的真实网页交互数据集

Method: 1. 提出基于约束的评估框架，对任务完成进度进行细粒度评估；2. 利用部分成功轨迹显著扩展可用训练数据；3. 构建BookingArena基准，包含20个热门网站的复杂预订任务

Result: 1. 蒸馏出的学生模型在BookingArena基准上超越开源方法；2. 匹配或超过商业系统性能；3. 模型规模显著更小；4. 提供系统化的复杂结构化网页任务评估方法

Conclusion: 该工作解决了高效创建多样化真实网页交互数据集的挑战，提供了系统化的复杂网页任务评估方法，通过约束评估框架和部分成功轨迹利用显著提升了训练数据质量和模型性能

Abstract: We present a scalable pipeline for automatically generating high-quality training data for web agents. In particular, a major challenge in identifying high-quality training instances is trajectory evaluation - quantifying how much progress was made towards task completion. We introduce a novel constraint-based evaluation framework that provides fine-grained assessment of progress towards task completion. This enables us to leverage partially successful trajectories, which significantly expands the amount of usable training data. We evaluate our method on a new benchmark we propose called BookingArena, which consists of complex booking tasks across 20 popular websites, and demonstrate that our distilled student model outperforms open-source approaches and matches or exceeds commercial systems, while being a significantly smaller model. Our work addresses the challenge of efficiently creating diverse, realistic web interaction datasets and provides a systematic evaluation methodology for complex structured web tasks.

</details>


### [19] [To Mix or To Merge: Toward Multi-Domain Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2602.12566)
*Haoqing Wang,Xiang Long,Ziheng Li,Yilong Xu,Tingguang Li,Yehui Tang*

Main category: cs.AI

TL;DR: 该论文比较了多领域强化学习与可验证奖励（RLVR）的两种训练范式：混合多任务训练与分别训练后模型合并，发现在推理密集型领域存在协同效应。


<details>
  <summary>Details</summary>
Motivation: 当前多领域专家级模型需要RLVR跨领域协作，但现有工作缺乏对混合多任务RLVR和分别训练后模型合并这两种范式的详细比较分析。

Method: 选择数学、编程、科学和指令跟随等多个常用高级任务作为目标领域，使用开源数据集设计广泛的定性和定量实验，从权重空间几何、模型预测行为和信息约束等角度分析内部机制。

Result: 发现跨领域RLVR存在较少相互干扰，推理密集型领域表现出相互协同效应，混合训练在推理密集型任务上表现更好，而分别训练在多样化任务上更优。

Conclusion: 为多领域RLVR训练提供了实证指导：推理密集型任务适合混合训练，多样化任务适合分别训练后合并，这有助于构建更强大的多领域专家模型。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) plays a key role in stimulating the explicit reasoning capability of Large Language Models (LLMs). We can achieve expert-level performance in some specific domains via RLVR, such as coding or math. When a general multi-domain expert-level model is required, we need to carefully consider the collaboration of RLVR across different domains. The current state-of-the-art models mainly employ two different training paradigms for multi-domain RLVR: mixed multi-task RLVR and separate RLVR followed by model merging. However, most of the works did not provide a detailed comparison and analysis about these paradigms. To this end, we choose multiple commonly used high-level tasks (e.g., math, coding, science, and instruction following) as our target domains and design extensive qualitative and quantitative experiments using open-source datasets. We find the RLVR across domains exhibits few mutual interferences, and reasoning-intensive domains demonstrate mutually synergistic effects. Furthermore, we analyze the internal mechanisms of mutual gains from the perspectives of weight space geometry, model prediction behavior, and information constraints. This project is named as M2RL that means Mixed multi-task training or separate training followed by model Merging for Reinforcement Learning, and the homepage is at https://github.com/mosAI25/M2RL

</details>


### [20] [Can I Have Your Order? Monte-Carlo Tree Search for Slot Filling Ordering in Diffusion Language Models](https://arxiv.org/abs/2602.12586)
*Joshua Ong Jun Leang,Yu Zhao,Mihaela Cătălina Stoian,Wenda Li,Shay B. Cohen,Eleonora Giunchiglia*

Main category: cs.AI

TL;DR: McDiffuSE使用蒙特卡洛树搜索优化掩码扩散模型的槽位填充顺序，通过前瞻模拟提升数学和代码推理任务的生成质量。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型中的规划-填充解码方法在数学和代码推理中表现有潜力，但性能对填充顺序高度敏感，导致输出方差大，需要更系统的顺序优化方法。

Method: 将槽位选择建模为决策问题，使用蒙特卡洛树搜索优化填充顺序，通过前瞻模拟评估部分完成结果，系统探索生成顺序的组合空间。

Result: 相比自回归基线平均提升3.2%，相比基础规划-填充方法提升8.0%，在MBPP上提升19.5%，在MATH500上提升4.9%。发现需要更大的探索常数而非更多模拟来克服模型置信度偏差。

Conclusion: MCTS规划是提升掩码扩散模型生成质量的有效方法，虽然主要遵循顺序生成，但融入非顺序生成对最大化性能至关重要。

Abstract: While plan-and-infill decoding in Masked Diffusion Models (MDMs) shows promise for mathematical and code reasoning, performance remains highly sensitive to slot infilling order, often yielding substantial output variance. We introduce McDiffuSE, a framework that formulates slot selection as decision making and optimises infilling orders through Monte Carlo Tree Search (MCTS). McDiffuSE uses look-ahead simulations to evaluate partial completions before commitment, systematically exploring the combinatorial space of generation orders. Experiments show an average improvement of 3.2% over autoregressive baselines and 8.0% over baseline plan-and-infill, with notable gains of 19.5% on MBPP and 4.9% on MATH500. Our analysis reveals that while McDiffuSE predominantly follows sequential ordering, incorporating non-sequential generation is essential for maximising performance. We observe that larger exploration constants, rather than increased simulations, are necessary to overcome model confidence biases and discover effective orderings. These findings establish MCTS-based planning as an effective approach for enhancing generation quality in MDMs.

</details>


### [21] [GeoAgent: Learning to Geolocate Everywhere with Reinforced Geographic Characteristics](https://arxiv.org/abs/2602.12617)
*Modi Jin,Yiming Zhang,Boyuan Sun,Dingwen Zhang,MingMing Cheng,Qibin Hou*

Main category: cs.AI

TL;DR: GeoAgent是一个能够与人类紧密推理并得出细粒度地址结论的模型，通过专家标注的地理定位数据集和地理相似性奖励机制，解决了现有RL方法在地理任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的方法虽然取得了性能和可解释性突破，但依赖AI生成的思维链数据和训练策略，与地理特性存在冲突，需要专门针对地理任务特点的解决方案。

Method: 1) 引入GeoSeek数据集，包含地理专家和专业玩家标注的思维链数据；2) 提出地理相似性奖励和一致性奖励，通过一致性代理评估，确保模型从地理角度收敛到正确答案并保持推理过程完整性。

Result: GeoAgent在多个粒度上优于现有方法和一系列通用VLLMs，同时生成的推理过程与人类思维高度一致。

Conclusion: 通过专家标注数据和专门设计的地理感知奖励机制，GeoAgent能够有效解决地理定位任务，实现与人类紧密对齐的推理能力。

Abstract: This paper presents GeoAgent, a model capable of reasoning closely with humans and deriving fine-grained address conclusions. Previous RL-based methods have achieved breakthroughs in performance and interpretability but still remain concerns because of their reliance on AI-generated chain-of-thought (CoT) data and training strategies, which conflict with geographic characteristics. To address these issues, we first introduce GeoSeek, a new geolocation dataset comprising CoT data annotated by geographic experts and professional players. We further thoroughly explore the inherent characteristics of geographic tasks and propose a geo-similarity reward and a consistency reward assessed by a consistency agent to assist training. This encourages the model to converge towards correct answers from a geographic perspective while ensuring the integrity and consistency of its reasoning process. Experimental results show that GeoAgent outperforms existing methods and a series of general VLLMs across multiple grains, while generating reasoning that closely aligns with humans.

</details>


### [22] [AI Agents for Inventory Control: Human-LLM-OR Complementarity](https://arxiv.org/abs/2602.12631)
*Jackie Baek,Yaopeng Fu,Will Ma,Tianyi Peng*

Main category: cs.AI

TL;DR: LLM增强的运筹学方法在库存控制中优于单独使用运筹学算法或LLM，人机协作团队也比单独的人类或AI表现更好。


<details>
  <summary>Details</summary>
Motivation: 传统运筹学算法依赖刚性建模假设，在需求分布变化或缺乏上下文信息时表现不佳。LLM具有灵活推理和整合丰富上下文信号的能力，但如何将LLM方法整合到传统决策流程中仍不清楚。

Method: 构建InventoryBench基准测试（包含1000多个库存实例，涵盖合成和真实需求数据），测试需求变化、季节性和不确定交货期下的决策规则。通过课堂实验研究人机协作，将LLM建议嵌入人类决策流程。

Result: 运筹学增强的LLM方法优于单独使用任一方法。人机协作团队平均获得比单独人类或AI更高的利润。实证发现大量个体从AI协作中受益。

Conclusion: 运筹学算法、LLM和人类在库存控制中具有互补性而非替代性。人机协作可以提升决策性能，为整合AI到传统运营决策提供了实证支持。

Abstract: Inventory control is a fundamental operations problem in which ordering decisions are traditionally guided by theoretically grounded operations research (OR) algorithms. However, such algorithms often rely on rigid modeling assumptions and can perform poorly when demand distributions shift or relevant contextual information is unavailable. Recent advances in large language models (LLMs) have generated interest in AI agents that can reason flexibly and incorporate rich contextual signals, but it remains unclear how best to incorporate LLM-based methods into traditional decision-making pipelines.
  We study how OR algorithms, LLMs, and humans can interact and complement each other in a multi-period inventory control setting. We construct InventoryBench, a benchmark of over 1,000 inventory instances spanning both synthetic and real-world demand data, designed to stress-test decision rules under demand shifts, seasonality, and uncertain lead times. Through this benchmark, we find that OR-augmented LLM methods outperform either method in isolation, suggesting that these methods are complementary rather than substitutes.
  We further investigate the role of humans through a controlled classroom experiment that embeds LLM recommendations into a human-in-the-loop decision pipeline. Contrary to prior findings that human-AI collaboration can degrade performance, we show that, on average, human-AI teams achieve higher profits than either humans or AI agents operating alone. Beyond this population-level finding, we formalize an individual-level complementarity effect and derive a distribution-free lower bound on the fraction of individuals who benefit from AI collaboration; empirically, we find this fraction to be substantial.

</details>


### [23] [Think Fast and Slow: Step-Level Cognitive Depth Adaptation for LLM Agents](https://arxiv.org/abs/2602.12662)
*Ruihan Yang,Fanghua Ye,Xiang We,Ruoqing Zhao,Kang Luo,Xinbo Xu,Bo Zhao,Ruotian Ma,Shanyi Wang,Zhaopeng Tu,Xiaolong Li,Deqing Yang,Linus*

Main category: cs.AI

TL;DR: CogRouter是一个让LLM智能体根据任务需求动态调整认知深度的框架，通过分层认知级别和两阶段训练，在长视野任务中实现高效决策。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体采用固定的认知模式：非思考模型直接生成响应，思考模型则统一进行深度推理。这种刚性在长视野任务中效率低下，因为不同步骤的认知需求差异很大，有些需要战略规划，有些只需常规执行。

Method: 基于ACT-R理论设计四个分层认知级别（从本能反应到战略规划）。采用两阶段训练：1）认知感知监督微调（CoSFT）建立稳定的级别特定模式；2）认知感知策略优化（CoPO）通过置信度感知优势重加权进行步骤级信用分配。核心洞见是：适当的认知深度应最大化最终行动的置信度。

Result: 在ALFWorld和ScienceWorld上的实验显示，CogRouter实现了最先进的性能且效率更高。使用Qwen2.5-7B模型，达到82.3%的成功率，优于GPT-4o（+40.3%）、OpenAI-o3（+18.3%）和GRPO（+14.0%），同时减少62%的token使用。

Conclusion: CogRouter通过动态调整认知深度，显著提升了LLM智能体在长视野任务中的效率和性能，证明了根据任务需求灵活选择认知策略的重要性。

Abstract: Large language models (LLMs) are increasingly deployed as autonomous agents for multi-turn decision-making tasks. However, current agents typically rely on fixed cognitive patterns: non-thinking models generate immediate responses, while thinking models engage in deep reasoning uniformly. This rigidity is inefficient for long-horizon tasks, where cognitive demands vary significantly from step to step, with some requiring strategic planning and others only routine execution. In this paper, we introduce CogRouter, a framework that trains agents to dynamically adapt cognitive depth at each step. Grounded in ACT-R theory, we design four hierarchical cognitive levels ranging from instinctive responses to strategic planning. Our two-stage training approach includes Cognition-aware Supervised Fine-tuning (CoSFT) to instill stable level-specific patterns, and Cognition-aware Policy Optimization (CoPO) for step-level credit assignment via confidence-aware advantage reweighting. The key insight is that appropriate cognitive depth should maximize the confidence of the resulting action. Experiments on ALFWorld and ScienceWorld demonstrate that CogRouter achieves state-of-the-art performance with superior efficiency. With Qwen2.5-7B, it reaches an 82.3% success rate, outperforming GPT-4o (+40.3%), OpenAI-o3 (+18.3%), and GRPO (+14.0%), while using 62% fewer tokens.

</details>


### [24] [Evaluating Robustness of Reasoning Models on Parameterized Logical Problems](https://arxiv.org/abs/2602.12665)
*Naïm Es-sebbani,Esteban Marquer,Yakoub Salhi,Zied Bouraoui*

Main category: cs.AI

TL;DR: 提出一个用于评估LLM推理器的诊断性2-SAT基准，通过参数化公式家族分离表面难度与结构现象，揭示传统SAT基准无法检测的脆弱性


<details>
  <summary>Details</summary>
Motivation: 传统SAT基准常将表面特征（长度、措辞、子句顺序）与决定可满足性的结构现象混淆，需要更精细的诊断工具来评估LLM推理器的真实能力

Method: 构建基于参数化结构化2-CNF公式的诊断基准，通过五种生成器分离不同能力：矛盾循环UNSAT核心、可控自由变量的SAT实例、预设骨干、延迟桥接子句、对称/重复变体

Result: 在表面统计特征固定的情况下，LLM推理器在针对性结构干预下表现出急剧的性能转变，揭示了聚合SAT准确率无法检测的脆弱性区域

Conclusion: 该诊断基准能有效揭示LLM推理器在结构化逻辑推理中的隐藏脆弱性，为评估和提升逻辑推理能力提供了更精细的工具

Abstract: Logic provides a controlled testbed for evaluating LLM-based reasoners, yet standard SAT-style benchmarks often conflate surface difficulty (length, wording, clause order) with the structural phenomena that actually determine satisfiability. We introduce a diagnostic benchmark for 2-SAT built from parameterized families of structured 2--CNF formulas, where satisfiability is characterized by the implication graph and can be tuned along interpretable axes. Our generators isolate distinct competencies and failure modes: (i) contradiction-cycle UNSAT cores with controllable size and imbalance, (ii) SAT instances with a prescribed fraction of free variables to control solution multiplicity, (iii) planted backbones that modulate propagation, (iv) late bridge clauses that couple otherwise monotone regions to probe sensitivity to ordering and revision, and (v) symmetry/duplication variants that test abstraction under renaming and redundant structure. We evaluate LLM-based reasoners on decision accuracy and assignment validity, and quantify robustness under semantics-preserving perturbations such as clause reordering, filler clauses, and variable renaming. Across models, we observe sharp performance transitions under targeted structural interventions even when surface statistics are held fixed, revealing brittleness regimes that are invisible to aggregate SAT accuracy.

</details>


### [25] [SkillsBench: Benchmarking How Well Agent Skills Work Across Diverse Tasks](https://arxiv.org/abs/2602.12670)
*Xiangyi Li,Wenbo Chen,Yimin Liu,Shenghan Zheng,Xiaokun Chen,Yifeng He,Yubo Li,Bingran You,Haotian Shen,Jiankai Sun,Shuyi Wang,Qunhong Zeng,Di Wang,Xuandong Zhao,Yuanli Wang,Roey Ben Chaim,Zonglin Di,Yipeng Gao,Junwei He,Yizhuo He,Liqiang Jing,Luyang Kong,Xin Lan,Jiachen Li,Songlin Li,Yijiang Li,Yueqian Lin,Xinyi Liu,Xuanqing Liu,Haoran Lyu,Ze Ma,Bowei Wang,Runhui Wang,Tianyu Wang,Wengao Ye,Yue Zhang,Hanwen Xing,Yiqi Xue,Steven Dillmann,Han-chung Lee*

Main category: cs.AI

TL;DR: SkillsBench基准测试评估Agent Skills对LLM代理的帮助程度，发现精选Skills平均提升16.2个百分点，但效果因领域差异大，自生成Skills无益，小型模型搭配Skills可匹敌无Skills的大型模型。


<details>
  <summary>Details</summary>
Motivation: 尽管Agent Skills被广泛采用，但缺乏标准化方法来衡量它们是否真正有效。需要建立基准来量化Skills对LLM代理性能的实际影响。

Method: 创建SkillsBench基准，包含11个领域的86个任务，每个任务配有精选Skills和确定性验证器。在三种条件下评估：无Skills、精选Skills、自生成Skills。测试7种代理模型配置，共7,308条轨迹。

Result: 精选Skills平均通过率提升16.2个百分点，但效果差异大：软件工程领域仅提升4.5个百分点，医疗领域提升51.9个百分点。84个任务中有16个出现负增长。自生成Skills平均无益处。2-3个模块的聚焦Skills优于全面文档。小型模型搭配Skills可匹配无Skills的大型模型。

Conclusion: Skills确实能提升LLM代理性能，但效果高度依赖于领域和任务特性。模型无法可靠地生成它们能从中受益的程序性知识。Skills设计应保持聚焦，而非追求全面性。

Abstract: Agent Skills are structured packages of procedural knowledge that augment LLM agents at inference time. Despite rapid adoption, there is no standard way to measure whether they actually help. We present SkillsBench, a benchmark of 86 tasks across 11 domains paired with curated Skills and deterministic verifiers. Each task is evaluated under three conditions: no Skills, curated Skills, and self-generated Skills. We test 7 agent-model configurations over 7,308 trajectories. Curated Skills raise average pass rate by 16.2 percentage points(pp), but effects vary widely by domain (+4.5pp for Software Engineering to +51.9pp for Healthcare) and 16 of 84 tasks show negative deltas. Self-generated Skills provide no benefit on average, showing that models cannot reliably author the procedural knowledge they benefit from consuming. Focused Skills with 2--3 modules outperform comprehensive documentation, and smaller models with Skills can match larger models without them.

</details>


### [26] [X-SYS: A Reference Architecture for Interactive Explanation Systems](https://arxiv.org/abs/2602.12748)
*Tobias Labarta,Nhi Hoang,Maximilian Dreyer,Jim Berend,Oleg Hein,Jackie Ma,Wojciech Samek,Sebastian Lapuschkin*

Main category: cs.AI

TL;DR: X-SYS：一个用于交互式解释系统的参考架构，通过STAR质量属性和五组件分解，将用户界面与后端计算解耦，并在SemanticLens系统中实现。


<details>
  <summary>Details</summary>
Motivation: 尽管可解释AI（XAI）研究提出了许多技术方法，但将可解释性部署为系统仍然具有挑战性。交互式解释系统需要合适的算法和系统能力，以在重复查询、模型和数据演化以及治理约束下保持解释可用性。作者认为，将XAI操作化需要将可解释性视为信息系统问题，其中用户交互需求引发特定的系统要求。

Method: 提出X-SYS参考架构，围绕STAR四个质量属性（可扩展性、可追溯性、响应性和适应性）组织，并指定五组件分解（XUI服务、解释服务、模型服务、数据服务、编排与治理）。该架构将交互模式映射到系统能力，以解耦用户界面演进与后端计算。通过SemanticLens系统实现X-SYS，该系统用于视觉语言模型中的语义搜索和激活引导。

Result: X-SYS提供了一个可重用的蓝图，通过基于契约的服务边界实现独立演进，离线/在线分离确保响应性，持久状态管理支持可追溯性。SemanticLens展示了该架构的具体实例化，支持在操作约束下的端到端设计。

Conclusion: 这项工作为交互式解释系统提供了可重用的蓝图和具体实例化，支持在操作约束下的端到端设计，帮助(X)AI研究人员、开发者和从业者连接交互式解释用户界面与系统能力。

Abstract: The explainable AI (XAI) research community has proposed numerous technical methods, yet deploying explainability as systems remains challenging: Interactive explanation systems require both suitable algorithms and system capabilities that maintain explanation usability across repeated queries, evolving models and data, and governance constraints. We argue that operationalizing XAI requires treating explainability as an information systems problem where user interaction demands induce specific system requirements. We introduce X-SYS, a reference architecture for interactive explanation systems, that guides (X)AI researchers, developers and practitioners in connecting interactive explanation user interfaces (XUI) with system capabilities. X-SYS organizes around four quality attributes named STAR (scalability, traceability, responsiveness, and adaptability), and specifies a five-component decomposition (XUI Services, Explanation Services, Model Services, Data Services, Orchestration and Governance). It maps interaction patterns to system capabilities to decouple user interface evolution from backend computation. We implement X-SYS through SemanticLens, a system for semantic search and activation steering in vision-language models. SemanticLens demonstrates how contract-based service boundaries enable independent evolution, offline/online separation ensures responsiveness, and persistent state management supports traceability. Together, this work provides a reusable blueprint and concrete instantiation for interactive explanation systems supporting end-to-end design under operational constraints.

</details>


### [27] [WebClipper: Efficient Evolution of Web Agents with Graph-based Trajectory Pruning](https://arxiv.org/abs/2602.12852)
*Junjie Wang,Zequn Xie,Dan Yang,Jie Feng,Yue Shen,Duolin Sun,Meixiu Long,Yihan Jiao,Zhehao Tan,Jian Wang,Peng Wei,Jinjie Gu*

Main category: cs.AI

TL;DR: WebClipper：通过图剪枝压缩网页代理轨迹的框架，减少约20%工具调用轮次同时提升准确性


<details>
  <summary>Details</summary>
Motivation: 现有网页代理系统存在搜索效率问题，依赖过长的工具调用轨迹和循环推理，导致冗余分支探索

Method: 将代理搜索过程建模为状态图，将轨迹优化转化为最小必要有向无环图挖掘问题，通过图剪枝压缩轨迹

Result: WebClipper减少约20%工具调用轮次同时提高准确性，并引入F-AE Score新指标平衡准确性与效率

Conclusion: WebClipper为网页代理设计提供了平衡有效性和效率的实用见解，通过轨迹压缩提升搜索效率

Abstract: Deep Research systems based on web agents have shown strong potential in solving complex information-seeking tasks, yet their search efficiency remains underexplored. We observe that many state-of-the-art open-source web agents rely on long tool-call trajectories with cyclic reasoning loops and exploration of unproductive branches. To address this, we propose WebClipper, a framework that compresses web agent trajectories via graph-based pruning. Concretely, we model the agent's search process as a state graph and cast trajectory optimization as a minimum-necessary Directed Acyclic Graph (DAG) mining problem, yielding pruned trajectories that preserve essential reasoning while eliminating redundant steps. Continued training on these refined trajectories enables the agent to evolve toward more efficient search patterns and reduces tool-call rounds by about 20% while improving accuracy. Furthermore, we introduce a new metric called F-AE Score to measure the model's overall performance in balancing accuracy and efficiency. Experiments demonstrate that WebClipper compresses tool-call rounds under excellent performance, providing practical insight into balancing effectiveness and efficiency in web agent design.

</details>


### [28] [BrowseComp-$V^3$: A Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents](https://arxiv.org/abs/2602.12876)
*Huanyao Zhang,Jiepeng Zhou,Bo Li,Bowen Zhou,Yanzhe Dan,Haishan Lu,Zhiyong Cao,Jiaoyang Chen,Yuqian Han,Zinan Sheng,Zhengwei Tao,Hao Liang,Jialong Wu,Yang Shi,Yuanpeng He,Jiaye Lin,Qintong Zhang,Guochen Yan,Runhao Zhao,Zhengpin Li,Xiaohan Yu,Lang Mei,Chong Chen,Wentao Zhang,Bin Cui*

Main category: cs.AI

TL;DR: BrowseComp-V³是一个新的多模态网页浏览基准测试，包含300个具有挑战性的问题，强调深度、多级、跨模态的多跳推理，所有证据都要求可公开搜索，并采用专家验证的子目标驱动过程评估机制。


<details>
  <summary>Details</summary>
Motivation: 现有多模态浏览基准测试在任务复杂性、证据可访问性和评估粒度方面存在局限，阻碍了对深度搜索能力的全面和可重复评估。

Method: 提出了BrowseComp-V³基准测试，包含300个精心策划的跨领域挑战性问题，强调跨模态多跳推理，所有证据要求可公开搜索。同时提出了OmniSeeker统一多模态浏览代理框架，集成多种网页搜索和视觉感知工具。

Result: 即使最先进的模型在BrowseComp-V³基准测试上也只能达到36%的准确率，揭示了在多模态信息集成和细粒度感知方面的关键瓶颈。

Conclusion: 当前模型能力与真实世界环境中稳健的多模态深度搜索之间存在根本性差距，BrowseComp-V³基准测试为评估和推进多模态代理能力提供了重要工具。

Abstract: Multimodal large language models (MLLMs), equipped with increasingly advanced planning and tool-use capabilities, are evolving into autonomous agents capable of performing multimodal web browsing and deep search in open-world environments. However, existing benchmarks for multimodal browsing remain limited in task complexity, evidence accessibility, and evaluation granularity, hindering comprehensive and reproducible assessments of deep search capabilities. To address these limitations, we introduce BrowseComp-$V^3$, a novel benchmark consisting of 300 carefully curated and challenging questions spanning diverse domains. The benchmark emphasizes deep, multi-level, and cross-modal multi-hop reasoning, where critical evidence is interleaved across textual and visual modalities within and across web pages. All supporting evidence is strictly required to be publicly searchable, ensuring fairness and reproducibility. Beyond final-answer accuracy, we incorporate an expert-validated, subgoal-driven process evaluation mechanism that enables fine-grained analysis of intermediate reasoning behaviors and systematic characterization of capability boundaries. In addition, we propose OmniSeeker, a unified multimodal browsing agent framework integrating diverse web search and visual perception tools. Comprehensive experiments demonstrate that even state-of-the-art models achieve only 36% accuracy on our benchmark, revealing critical bottlenecks in multimodal information integration and fine-grained perception. Our results highlight a fundamental gap between current model capabilities and robust multimodal deep search in real-world settings.

</details>


### [29] [Information-theoretic analysis of world models in optimal reward maximizers](https://arxiv.org/abs/2602.12963)
*Alfred Harwood,Jose Faustino,Alex Altair*

Main category: cs.AI

TL;DR: 论文量化了最优策略关于环境的信息量，证明对于具有n个状态和m个动作的受控马尔可夫过程，确定性最优策略提供恰好n log m比特的环境信息。


<details>
  <summary>Details</summary>
Motivation: 研究AI中一个重要问题：成功行为在多大程度上需要世界的内部表示。旨在量化最优策略提供的关于底层环境的信息量，为"隐式世界模型"提供信息论下界。

Method: 使用受控马尔可夫过程（CMP）模型，假设在可能的转移动态空间上具有均匀先验。分析确定性最优策略与环境之间的互信息，考虑各种目标函数（有限时域、无限时域折扣、时间平均奖励最大化）。

Result: 证明对于任何非常数奖励函数的最优确定性策略，提供恰好n log m比特的环境信息。具体来说，环境与最优策略之间的互信息为n log m比特，这一界限适用于广泛的优化目标。

Conclusion: 为最优性所需的"隐式世界模型"提供了精确的信息论下界，表明最优策略必然编码了关于环境的大量结构化信息，即使没有显式的世界模型表示。

Abstract: An important question in the field of AI is the extent to which successful behaviour requires an internal representation of the world. In this work, we quantify the amount of information an optimal policy provides about the underlying environment. We consider a Controlled Markov Process (CMP) with $n$ states and $m$ actions, assuming a uniform prior over the space of possible transition dynamics. We prove that observing a deterministic policy that is optimal for any non-constant reward function then conveys exactly $n \log m$ bits of information about the environment. Specifically, we show that the mutual information between the environment and the optimal policy is $n \log m$ bits. This bound holds across a broad class of objectives, including finite-horizon, infinite-horizon discounted, and time-averaged reward maximization. These findings provide a precise information-theoretic lower bound on the "implicit world model'' necessary for optimality.

</details>


### [30] [Consistency of Large Reasoning Models Under Multi-Turn Attacks](https://arxiv.org/abs/2602.13093)
*Yubo Li,Ramayya Krishnan,Rema Padman*

Main category: cs.AI

TL;DR: 推理模型在对抗攻击下表现出有意义但不完整的鲁棒性，虽然优于指令调优基线，但存在多种脆弱性模式，且基于置信度的防御方法对推理模型失效


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂任务上表现出色，但其在多轮对抗压力下的鲁棒性尚未充分探索。需要评估前沿推理模型在对抗攻击下的表现，了解推理能力是否自动带来对抗鲁棒性

Method: 评估九个前沿推理模型在对抗攻击下的表现，通过轨迹分析识别失败模式，测试置信感知响应生成（CARG）方法在推理模型上的有效性

Result: 推理提供有意义但不完整的鲁棒性：大多数推理模型显著优于指令调优基线，但都表现出不同的脆弱性特征。误导性建议普遍有效，社会压力具有模型特异性。识别出五种失败模式（自我怀疑、社会从众、建议劫持、情感易感性、推理疲劳），前两种占50%失败。CARG对推理模型失效，因为扩展推理轨迹导致过度自信；随机置信嵌入反而优于针对性提取

Conclusion: 推理能力不会自动赋予对抗鲁棒性，基于置信度的防御方法需要为推理模型进行根本性重新设计。需要开发专门针对推理模型特性的对抗防御机制

Abstract: Large reasoning models with reasoning capabilities achieve state-of-the-art performance on complex tasks, but their robustness under multi-turn adversarial pressure remains underexplored. We evaluate nine frontier reasoning models under adversarial attacks. Our findings reveal that reasoning confers meaningful but incomplete robustness: most reasoning models studied significantly outperform instruction-tuned baselines, yet all exhibit distinct vulnerability profiles, with misleading suggestions universally effective and social pressure showing model-specific efficacy. Through trajectory analysis, we identify five failure modes (Self-Doubt, Social Conformity, Suggestion Hijacking, Emotional Susceptibility, and Reasoning Fatigue) with the first two accounting for 50% of failures. We further demonstrate that Confidence-Aware Response Generation (CARG), effective for standard LLMs, fails for reasoning models due to overconfidence induced by extended reasoning traces; counterintuitively, random confidence embedding outperforms targeted extraction. Our results highlight that reasoning capabilities do not automatically confer adversarial robustness and that confidence-based defenses require fundamental redesign for reasoning models.

</details>


### [31] [Constrained Assumption-Based Argumentation Frameworks](https://arxiv.org/abs/2602.13135)
*Emanuele De Angelis,Fabio Fioravanti,Maria Chiara Meo,Alberto Pettorossi,Maurizio Proietti,Francesca Toni*

Main category: cs.AI

TL;DR: 提出约束ABA（CABA）框架，通过引入约束变量扩展传统ABA，支持非地面（含变量）论证和攻击，从而突破传统ABA仅限于命题原子和地面论证的限制。


<details>
  <summary>Details</summary>
Motivation: 传统假设基础论证（ABA）框架仅限于基于命题原子的地面（无变量）论证和攻击，这种表示限制严重限制了其适用性。需要扩展ABA以支持包含约束变量的非地面论证，使论证能够涵盖可能无限域中的变量。

Method: 提出约束ABA（CABA）框架，允许论证组件和构建的论证包含约束变量。定义了CABA的非地面语义，基于各种非地面攻击概念。展示了新语义如何保守地泛化标准ABA语义。

Result: 成功扩展了ABA框架，使其能够处理包含约束变量的非地面论证。新定义的CABA框架和语义能够处理无限域中的变量，同时保持与传统ABA的兼容性。

Conclusion: CABA框架有效克服了传统ABA的表示限制，通过引入约束变量支持更丰富的论证结构，为结构化论证提供了更强大的表示能力，同时保持与现有ABA理论的向后兼容性。

Abstract: Assumption-based Argumentation (ABA) is a well-established form of structured argumentation. ABA frameworks with an underlying atomic language are widely studied, but their applicability is limited by a representational restriction to ground (variable-free) arguments and attacks built from propositional atoms. In this paper, we lift this restriction and propose a novel notion of constrained ABA (CABA), whose components, as well as arguments built from them, may include constrained variables, ranging over possibly infinite domains. We define non-ground semantics for CABA, in terms of various notions of non-ground attacks. We show that the new semantics conservatively generalise standard ABA semantics.

</details>


### [32] [Optimal Take-off under Fuzzy Clearances](https://arxiv.org/abs/2602.13166)
*Hugo Henry,Arthur Tsai,Kelly Cohen*

Main category: cs.AI

TL;DR: 提出混合障碍物规避架构，结合最优控制与模糊规则系统，实现无人机自适应约束处理，但发现软件兼容性问题影响约束执行。


<details>
  <summary>Details</summary>
Motivation: 经典最优控制在不确定性下的局限性，以及航空安全关键系统需要可解释的决策制定，促使开发能够自适应处理约束的系统。

Method: 采用三阶段Takagi-Sugeno-Kang模糊层，根据FAA和EASA的监管分离最小值和适航指南，调制约束半径、紧急级别和激活决策，然后将模糊推导的间隙作为软约束纳入最优控制问题，使用FALCON工具箱和IPOPT求解。

Result: 概念验证显示方法能生成最优轨迹，每次迭代计算时间2-3秒，但发现FALCON和IPOPT最新版本存在软件不兼容问题，导致拉格朗日惩罚项始终为零，无法正确执行约束。

Conclusion: 该方法在简化飞机模型上展示了近实时应用的可行性，但需要解决软件兼容性问题，未来工作包括验证早期软件版本、优化模糊隶属函数，以及扩展到更高保真度模型和随机障碍环境。

Abstract: This paper presents a hybrid obstacle avoidance architecture that integrates Optimal Control under clearance with a Fuzzy Rule Based System (FRBS) to enable adaptive constraint handling for unmanned aircraft. Motivated by the limitations of classical optimal control under uncertainty and the need for interpretable decision making in safety critical aviation systems, we design a three stage Takagi Sugeno Kang fuzzy layer that modulates constraint radii, urgency levels, and activation decisions based on regulatory separation minima and airworthiness guidelines from FAA and EASA. These fuzzy-derived clearances are then incorporated as soft constraints into an optimal control problem solved using the FALCON toolbox and IPOPT. The framework aims to reduce unnecessary recomputations by selectively activating obstacle avoidance updates while maintaining compliance with aviation procedures. A proof of concept implementation using a simplified aircraft model demonstrates that the approach can generate optimal trajectories with computation times of 2,3 seconds per iteration in a single threaded MATLAB environment, suggesting feasibility for near real time applications. However, our experiments revealed a critical software incompatibility in the latest versions of FALCON and IPOPT, in which the Lagrangian penalty term remained identically zero, preventing proper constraint enforcement. This behavior was consistent across scenarios and indicates a solver toolbox regression rather than a modeling flaw. Future work includes validating this effect by reverting to earlier software versions, optimizing the fuzzy membership functions using evolutionary methods, and extending the system to higher fidelity aircraft models and stochastic obstacle environments.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [33] [Transformer-based CoVaR: Systemic Risk in Textual Information](https://arxiv.org/abs/2602.12490)
*Junyu Chen,Tom Boot,Lingwei Kong,Weining Wang*

Main category: econ.EM

TL;DR: 基于Transformer的方法结合金融新闻和市场数据改进CoVaR估计，证明文本数据能有效建模系统性风险，无需超大数据集


<details>
  <summary>Details</summary>
Motivation: 传统CoVaR估计主要依赖市场数据，缺乏对文本信息的有效利用。现有方法使用预定义情感得分，无法充分利用原始文本信息。需要开发能够直接整合原始新闻文本的方法来改进系统性风险测量。

Method: 提出基于Transformer的方法，将大型语言模型生成的原始文本嵌入与市场数据直接整合。方法不依赖预定义情感得分，而是使用LLM生成的文本嵌入。理论证明了Transformer CoVaR估计器的显式误差界限。

Result: 使用2006-2013年美国市场收益和路透社新闻数据，样本外结果显示文本信息显著影响CoVaR预测。Transformer-based CoVaR相比无文本CoVaR和使用传统情感度量的CoVaR，在市场压力期间表现出更明显的负向下降，预测性能更好。

Conclusion: 文本数据可以有效用于建模系统性风险，无需超大数据集。基于Transformer的方法能够充分利用原始新闻文本信息，显著改进CoVaR估计，特别是在市场压力时期。

Abstract: Conditional Value-at-Risk (CoVaR) quantifies systemic financial risk by measuring the loss quantile of one asset, conditional on another asset experiencing distress. We develop a Transformer-based methodology that integrates financial news articles directly with market data to improve CoVaR estimates. Unlike approaches that use predefined sentiment scores, our method incorporates raw text embeddings generated by a large language model (LLM). We prove explicit error bounds for our Transformer CoVaR estimator, showing that accurate CoVaR learning is possible even with small datasets. Using U.S. market returns and Reuters news items from 2006--2013, our out-of-sample results show that textual information impacts the CoVaR forecasts. With better predictive performance, we identify a pronounced negative dip during market stress periods across several equity assets when comparing the Transformer-based CoVaR to both the CoVaR without text and the CoVaR using traditional sentiment measures. Our results show that textual data can be used to effectively model systemic risk without requiring prohibitively large data sets.

</details>


### [34] [Toggling the Defiers to Relax Monotonicity: The Difference-in-Instrumental-Variables Estimand](https://arxiv.org/abs/2602.12504)
*Johann Caro-Burnett*

Main category: econ.EM

TL;DR: 本文提出差异工具变量(DIIV)估计量，利用两个具有相反依从模式的工具变量，在不施加单调性假设下恢复可点识别且行为可解释的因果效应。


<details>
  <summary>Details</summary>
Motivation: 标准工具变量方法在单调性假设下识别局部平均处理效应，该假设排除了违抗者。然而在许多实证环境中，不同的工具变量可能引发异质性甚至相反的行为反应，需要新的方法来处理这种情况。

Method: 引入差异工具变量(DIIV)估计量，利用两个具有相反依从模式的工具变量，通过简单的线性变换和标准两阶段最小二乘法程序实现，恢复凸组合形式的因果效应。

Result: DIIV估计量产生违抗者和依从者的边际处理效应的凸组合，权重反映不同工具变量间治疗采纳的差异变化。当单调性成立时，DIIV与标准IV估计量一致。

Conclusion: DIIV方法提供了一种在不依赖单调性假设下识别因果效应的实用工具，通过复制数据的应用展示了其实用性，扩展了工具变量方法的应用范围。

Abstract: Standard instrumental variables (IV) methods identify a Local Average Treatment Effect under monotonicity, which rules out defiers. In many empirical environments, however, distinct instruments may induce heterogeneous and even opposing behavioral responses. This paper introduces the Difference-in-Instrumental-Variables (DIIV) estimand, which exploits two instruments with opposing compliance patterns to recover a point-identified and behaviorally interpretable causal effect without imposing monotonicity. The estimand yields a convex combination of the marginal treatment effects on compliers and defiers, with weights reflecting differential shifts in treatment take-up across instruments. When monotonicity holds, DIIV coincides with the standard IV estimand. The approach can be implemented using simple linear transformations and standard two-stage least squares procedures. Applications using replication data illustrate its applicability in practice.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [35] [Nationwide Hourly Population Estimating at the Neighborhood Scale in the United States Using Stable-Attendance Anchor Calibration](https://arxiv.org/abs/2602.12291)
*Huan Ning,Zhenlong Li,Manzhu Yu,Xiao Huang,Shiyan Zhang,Shan Qiao*

Main category: stat.AP

TL;DR: SAAC框架利用智能手机移动数据，通过稳定出席锚点校准，重建美国人口普查区块组级别的每小时人口分布


<details>
  <summary>Details</summary>
Motivation: 传统人口数据集是静态的，无法捕捉日常移动带来的人口时间动态；智能手机移动数据虽然时空覆盖广，但存在观测偏差、设备渗透率不均等问题，难以准确估计人口

Method: 提出稳定出席锚点校准框架，将人口估计构建为基于平衡的人口核算问题，结合居住人口和时间变化的进出移动数据；利用高规律性出席地点（如高中）作为校准锚点，估计观测缩放因子来校正未记录的移动事件

Result: 推断的人口模式与先前移动性和城市人口研究的经验发现一致；提供了从有偏差的数字轨迹数据到可解释的动态人口产品的通用框架

Conclusion: SAAC为将大规模有偏差的数字轨迹数据转化为可解释的动态人口产品提供了通用框架，对城市科学、公共卫生和人类移动性研究具有重要意义

Abstract: Traditional population datasets are largely static and therefore unable to capture the strong temporal dynamics of human presence driven by daily mobility. Recent smartphone-based mobility data offer unprecedented spatiotemporal coverage, yet translating these opportunistic observations into accurate population estimates remains challenging due to incomplete sensing, spatially heterogeneous device penetration, and unstable observation processes. We propose a Stable-Attendance Anchor Calibration (SAAC) framework to reconstruct hourly population presence at the Census block group level across the United States. SAAC formulates population estimation as a balance-based population accounting problem, combining residential population with time-varying inbound and outbound mobility inferred from device-event observations. To address observation bias and identifiability limitations, the framework leverages locations with highly regular attendance as calibration anchors, using high schools in this study. These anchors enable estimation of observation scaling factors that correct for under-recorded mobility events. By integrating anchor-based calibration with an explicit sampling model, SAAC enables consistent conversion from observed device events to population presence at fine temporal resolution. The inferred population patterns are consistent with established empirical findings in prior mobility and urban population studies. SAAC provides a generalizable framework for transforming large-scale, biased digital trace data into interpretable dynamic population products, with implications for urban science, public health, and human mobility research. The hourly population estimates can be accessed at: https://gladcolor.github.io/hourly_population.

</details>


### [36] [Statistical Opportunities in Neuroimaging](https://arxiv.org/abs/2602.12974)
*Jian Kang,Thomas Nichols,Lexin Li,Martin A. Lindquist,Hongtu Zhu*

Main category: stat.AP

TL;DR: 本文综述神经影像学中的统计机遇与挑战，涵盖大脑发育、成人/衰老大脑、神经退行性疾病、以及大脑编码解码四大领域，强调统计学家与神经科学家合作的重要性。


<details>
  <summary>Details</summary>
Motivation: 神经影像技术（MRI、fMRI、EEG、PET等）虽已显著推进对人脑的理解，但大脑作为复杂的多尺度系统，其测量数据具有高维度特性，面临噪声、运动伪影、个体差异、扫描仪差异等统计挑战，需要统计方法来解决。

Method: 论文采用综述方法，首先简要介绍主要成像技术，然后系统回顾四个关键领域的前沿研究：1）出生至20岁的大脑发育；2）成人及衰老大脑；3）神经退行性与神经精神疾病；4）大脑编码与解码。分析各领域的数据与建模挑战，并指出统计学家的研究机遇。

Result: 论文系统梳理了神经影像学各领域的统计挑战，包括高维数据处理、噪声控制、变异源分离、大规模研究分析等问题，并展示了统计方法如何帮助解决这些挑战，推动神经科学发现。

Conclusion: 统计学家、神经科学家和临床医生的紧密合作对于将神经影像学进展转化为改进的诊断方法、更深入的机制理解和更个性化的治疗方案至关重要。跨学科合作是应对神经影像学复杂统计挑战的关键。

Abstract: Neuroimaging has profoundly enhanced our understanding of the human brain by characterizing its structure, function, and connectivity through modalities like MRI, fMRI, EEG, and PET. These technologies have enabled major breakthroughs across the lifespan, from early brain development to neurodegenerative and neuropsychiatric disorders. Despite these advances, the brain is a complex, multiscale system, and neuroimaging measurements are correspondingly high-dimensional. This creates major statistical challenges, including measurement noise, motion-related artifacts, substantial inter-subject and site/scanner variability, and the sheer scale of modern studies. This paper explores statistical opportunities and challenges in neuroimaging across four key areas: (i) brain development from birth to age 20, (ii) the adult and aging brain, (iii) neurodegeneration and neuropsychiatric disorders, and (iv) brain encoding and decoding. After a quick tutorial on major imaging technologies, we review cutting-edge studies, underscore data and modeling challenges, and highlight research opportunities for statisticians. We conclude by emphasizing that close collaboration among statisticians, neuroscientists, and clinicians is essential for translating neuroimaging advances into improved diagnostics, deeper mechanistic insight, and more personalized treatments.

</details>

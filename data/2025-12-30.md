<div id=toc></div>

# Table of Contents

- [econ.EM](#econ.EM) [Total: 4]
- [cs.SI](#cs.SI) [Total: 4]
- [cs.AI](#cs.AI) [Total: 50]
- [stat.AP](#stat.AP) [Total: 3]
- [cs.ET](#cs.ET) [Total: 5]
- [cs.CY](#cs.CY) [Total: 6]


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [1] [Canonical correlation regression with noisy data](https://arxiv.org/abs/2512.22697)
*Isaac Meza,Rahul Singh*

Main category: econ.EM

TL;DR: 该论文研究数据丰富环境下的工具变量回归，提出基于两阶段最小二乘和谱正则化的估计方法，用于处理多噪声协变量和多噪声工具变量的情况，假设真实协变量和工具变量具有重复性但可能不对齐。


<details>
  <summary>Details</summary>
Motivation: 在数据丰富的环境中，协变量和工具变量通常存在噪声，且真实变量可能由少量潜在因子构成但不对齐，需要开发能够处理这种复杂性的估计方法。

Method: 提出基于两阶段最小二乘和谱正则化的估计方法：第一阶段学习协变量与工具变量之间的典型相关性，第二阶段将这些相关性作为回归因子进行估计。

Result: 推导了估计误差的上界和下界，证明了该方法在噪声数据下的最优性，并为不同机制提供了谱正则化类型的选择指导。

Conclusion: 该方法在数据丰富环境下处理噪声协变量和工具变量时具有理论最优性和实际可行性，谱正则化的选择对性能有重要影响。

Abstract: We study instrumental variable regression in data rich environments. The goal is to estimate a linear model from many noisy covariates and many noisy instruments. Our key assumption is that true covariates and true instruments are repetitive, though possibly different in nature; they each reflect a few underlying factors, however those underlying factors may be misaligned. We analyze a family of estimators based on two stage least squares with spectral regularization: canonical correlations between covariates and instruments are learned in the first stage, which are used as regressors in the second stage. As a theoretical contribution, we derive upper and lower bounds on estimation error, proving optimality of the method with noisy data. As a practical contribution, we provide guidance on which types of spectral regularization to use in different regimes.

</details>


### [2] [Causal-Policy Forest for End-to-End Policy Learning](https://arxiv.org/abs/2512.22846)
*Masahiro Kato*

Main category: econ.EM

TL;DR: 提出一种用于因果推断中策略学习的端到端算法，通过修改因果森林来直接学习最优治疗策略


<details>
  <summary>Details</summary>
Motivation: 现有的策略学习方法通常需要单独估计干扰参数，缺乏端到端的解决方案。本文旨在弥合策略学习与条件平均处理效应估计之间的实践差距，提供更高效、一体化的方法。

Method: 首先证明最大化策略价值等价于在{-1, 1}限制回归模型下最小化条件平均处理效应的均方误差。基于此发现，修改现有的因果森林算法，提出因果策略森林算法，实现端到端的策略学习。

Result: 提出的因果策略森林算法具有三个优势：1）是现有广泛使用的CATE估计方法的简单修改；2）以更端到端的方式训练策略；3）保持标准决策树和随机森林的高效计算特性。

Conclusion: 该研究提供了一种简单有效的端到端策略学习算法，弥合了策略学习与CATE估计之间的实践差距，为因果推断中的个性化治疗推荐提供了更高效的工具。

Abstract: This study proposes an end-to-end algorithm for policy learning in causal inference. We observe data consisting of covariates, treatment assignments, and outcomes, where only the outcome corresponding to the assigned treatment is observed. The goal of policy learning is to train a policy from the observed data, where a policy is a function that recommends an optimal treatment for each individual, to maximize the policy value. In this study, we first show that maximizing the policy value is equivalent to minimizing the mean squared error for the conditional average treatment effect (CATE) under $\{-1, 1\}$ restricted regression models. Based on this finding, we modify the causal forest, an end-to-end CATE estimation algorithm, for policy learning. We refer to our algorithm as the causal-policy forest. Our algorithm has three advantages. First, it is a simple modification of an existing, widely used CATE estimation method, therefore, it helps bridge the gap between policy learning and CATE estimation in practice. Second, while existing studies typically estimate nuisance parameters for policy learning as a separate task, our algorithm trains the policy in a more end-to-end manner. Third, as in standard decision trees and random forests, we train the models efficiently, avoiding computational intractability.

</details>


### [3] [Computing Nash equilibria for product design based on hierarchical Bayesian mixed logit models](https://arxiv.org/abs/2512.22864)
*Jan H. R. Dressler,Peter Kurz,Winfried J. Steiner*

Main category: econ.EM

TL;DR: 该研究评估了混合logit模型在揭示真实纳什均衡方面的能力，发现其表现主要取决于选择行为类型（概率性vs确定性），而非市场结构因素。


<details>
  <summary>Details</summary>
Motivation: 现有研究在结合联合分析、离散选择分析和产品线优化的基础上，很少关注使用非合作博弈论模拟后续竞争动态。特别是缺乏使用完全贝叶斯选择模型来同时模拟价格和产品设计的竞争，且尚未评估选择模型揭示真实均衡的能力。

Method: 基于真实价格和成本，通过数值精确方法推导数千个纳什均衡，分析混合logit模型在不同选择行为类型（概率性与确定性）下的均衡恢复能力，并考察贝叶斯超参数不确定性的影响。

Result: 混合logit模型揭示真实纳什均衡的能力主要取决于选择行为类型，而非竞争企业数量、产品数量、特征数量、偏好异质性或扰动程度。当现实中选择行为是确定性时，对估计偏好应用确定性选择规则可获得最高的均衡恢复率。贝叶斯超参数不确定性在确定性选择行为设置下能进一步提高检测率。

Conclusion: 选择模型在竞争动态模拟中的有效性高度依赖于选择行为类型，特别是在确定性选择行为下，贝叶斯方法能显著提升均衡检测能力。这为竞争分析中的模型选择提供了重要指导。

Abstract: Despite a substantial body of theoretical and empirical research in the fields of conjoint and discrete choice analysis as well as product line optimization, relatively few papers focused on the simulation of subsequent competitive dynamics employing non-cooperative game theory. Only a fraction of the existing frameworks explored competition on both product price and design, none of which used fully Bayesian choice models for simulation. Most crucially, no one has yet assessed the choice models' ability to uncover the true equilibria, let alone under different types of choice behavior. Our analysis of thousands of Nash equilibria, derived in full and numerically exact on the basis of real prices and costs, provides evidence that the capability of state-of-the-art mixed logit models to reveal the true Nash equilibria seems to be primarily contingent upon the type of choice behavior (probabilistic versus deterministic), regardless of the number of competing firms, offered products and features in the market, as well as the degree of preference heterogeneity and disturbance. Generally, the highest equilibrium recovery is achieved when applying a deterministic choice rule to estimated preferences given deterministic choice behavior in reality. It is especially in the latter setting that incorporating Bayesian (hyper)parameter uncertainty further enhances the detection rate compared to posterior means. Additionally, we investigate the influence of the above factors on other equilibrium characteristics such as product (line) differentiation.

</details>


### [4] [Nonparametric Identification of Demand without Exogenous Product Characteristics](https://arxiv.org/abs/2512.23211)
*Kirill Borusyak,Jiafeng Chen,Peter Hull,Lihua Lei*

Main category: econ.EM

TL;DR: 本文展示了在需求模型中，即使产品特征内生，通过重新中心化的工具变量（结合价格外生冲击和内生特征）仍可非参数识别价格反事实，无需传统特征工具变量。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为，在足够灵活的需求模型中，需要外生的特征工具变量来识别差异化产品需求。本文挑战这一观点，旨在证明在更弱的条件下仍可实现识别。

Method: 提出使用"重新中心化的工具变量"（recentered instruments），结合价格的外生冲击和内生产品特征，在较弱的指数限制和新的"忠实性"条件下实现非参数识别。

Result: 证明价格反事实可以在非参数框架下被识别，无需传统的外生特征工具变量。忠实性条件可视为对识别变异丰富度的技术要求，而非实质性经济限制。

Conclusion: 在差异化产品需求识别中，重新中心化的工具变量提供了更灵活的方法，放宽了对传统特征工具变量的依赖，为实证研究提供了新途径。

Abstract: We study the identification of differentiated product demand with exogenous supply-side instruments, allowing product characteristics to be endogenous. Past analyses have argued that exogenous characteristic-based instruments are essentially necessary given a sufficiently flexible demand model with a suitable index restriction. We show, however, that price counterfactuals are nonparametrically identified by recentered instruments -- which combine exogenous shocks to prices with endogenous product characteristics -- under a weaker index restriction and a new condition we term faithfulness. We argue that faithfulness, like the usual completeness condition for nonparametric identification with instruments, can be viewed as a technical requirement on the richness of identifying variation rather than a substantive economic restriction, and we show that it holds under a variety of non-nested conditions on either price-setting or the index.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [5] [Reddit Deplatforming and Toxicity Dynamics on Generalist Voat Communities](https://arxiv.org/abs/2512.22348)
*Aleksandar Tomašević,Ana Vranić,Aleksandra Alorić,Marija Mitrović Dankulov*

Main category: cs.SI

TL;DR: 研究Reddit封禁社区对Voat平台的影响，发现两种迁移模式：敌对接管（2015-2018）和毒性均衡（2018-2020），社区毒性翻倍但主要通过边缘动态而非核心捕获实现


<details>
  <summary>Details</summary>
Motivation: 主流平台封禁（deplatforming）对接收被驱逐用户的替代平台影响研究不足，需要了解迁移用户如何重塑社区结构和机制

Method: 分析Reddit四次主要封禁浪潮（2015-2020），结合网络分析、毒性检测和动态声誉建模，研究Voat通用社区的变化

Result: 发现两种迁移影响模式：敌对接管阶段新用户通过数量优势形成平行结构；毒性均衡阶段现有用户层级扁平化使新用户融入主导的毒性社区。社区毒性翻倍但只有不到5%的新用户达到中心位置

Conclusion: 接收平台在敌对接管阶段有狭窄的干预窗口，之后毒性规范会自我维持；松散组织社区分散到通用空间，意识形态凝聚群体集中在专门飞地

Abstract: Deplatforming, the permanent banning of entire communities, is a primary tool for content moderation on mainstream platforms. While prior research examines effects on banned communities or source platform health, the impact on alternative platforms that absorb displaced users remains understudied. We analyze four major Reddit ban waves (2015--2020) and their effects on generalist communities on Voat, asking how post-ban arrivals reshape community structure and through what mechanisms transformation occurs. Combining network analysis, toxicity detection, and dynamic reputation modeling, we identify two distinct regimes of migration impact: (1) Hostile Takeover (2015--2018), where post-ban arrival cohorts formed parallel social structures that bypassed existing community cores through sheer volume, and (2) Toxic Equilibrium (2018--2020), where the flattening of existing user hierarchy enabled newcomers to integrate into the now-dominant toxic community. Crucially, community transformation occurred through peripheral dynamics rather than hub capture: fewer than 5% of newcomers achieved central positions in most months, yet toxicity doubled. Migration structure also shaped outcomes: loosely organized communities dispersed into generalist spaces, while ideologically cohesive groups concentrated in dedicated enclaves. These findings suggest that receiving platforms face a narrow intervention window during the hostile takeover phase, after which toxic norms become self-sustaining.

</details>


### [6] [A new adaptive two-layer model for opinion spread in hypergraphs: parameter sensitivity and estimation](https://arxiv.org/abs/2512.23355)
*Ágnes Backhausz,Villő Csiszár,Balázs Csegő Kolok,Damján Tárkányi,András Zempléni*

Main category: cs.SI

TL;DR: 该研究提出一个双层随机超图模型来模拟意见传播，其中超边代表家庭和工作场所，个体在群体多数意见影响下改变观点或更换工作场所。通过模拟和马尔可夫链分析研究参数对同质化和极化速度的影响，并比较不同统计和机器学习方法在部分信息下估计概率的性能。


<details>
  <summary>Details</summary>
Motivation: 研究意见传播中同伴压力（通常通过高阶交互建模）的影响，特别是在重叠的自适应社会结构中，如家庭和工作场所，个体如何在这些群体中受到多数意见的影响。

Method: 引入双层随机超图模型，超边代表家庭和工作场所。个体在群体多数意见影响下可以改变观点或更换工作场所。通过计算机模拟研究参数影响，将模型分析为马尔可夫链，并比较线性回归、xgboost和卷积神经网络等统计和机器学习方法在部分信息下的性能。

Result: 描述了改变观点和更换工作场所概率参数对同质化和极化速度的影响，分析了吸收状态的频率。发现所有方法在适当条件下都能达到最佳结果，且提供良好结果所需的信息量取决于同伴压力效应的强度。

Conclusion: 在重叠的自适应社会结构中，同伴压力显著影响意见极化过程。不同统计和机器学习方法都能有效估计相关概率，但所需信息量取决于同伴压力强度，所有方法在适当条件下都能取得良好性能。

Abstract: When opinion spread is studied, peer pressure is often modeled by interactions of more than two individuals (higher-order interactions). In our work, we introduce a two-layer random hypergraph model, in which hyperedges represent households and workplaces. Within this overlapping, adaptive structure, individuals react if their opinion is in majority in their groups. The process evolves through random steps: individuals can either change their opinion, or quit their workplace and join another one in which their opinion belongs to the majority. Based on computer simulations, our first goal is to describe the effect of the parameters responsible for the probability of changing opinion and quitting workplace on the homophily and speed of polarization. We also analyze the model as a Markov chain, and study the frequency of the absorbing states. Then, we quantitatively compare how different statistical and machine learning methods, in particular, linear regression, xgboost and a convolutional neural network perform for estimating these probabilities, based on partial information from the process, for example, the distribution of opinion configurations within households and workplaces. Among other observations, we conclude that all methods can achieve the best results under appropriate circumstances, and that the amount of information that is necessary to provide good results depends on the strength of the peer pressure effect.

</details>


### [7] [Beyond-Diagonal Reconfigurable Intelligent Surfaces for 6G Networks: Principles, Challenges, and Quantum Horizons](https://arxiv.org/abs/2512.23400)
*Abd Ullah Khan,Uman Khalid,Muhammad Tanveer,Trung Q. Duong,Hyundong Shin*

Main category: cs.SI

TL;DR: 本文系统介绍了超越对角可重构智能表面(BD-RIS)技术，包括其架构原理、优势分类、最新进展、挑战机遇，并通过案例研究比较了四种波束赋形算法性能，最后探讨了量子增强的混合机器学习模型在6G BD-RIS中的应用。


<details>
  <summary>Details</summary>
Motivation: 传统RIS采用相互隔离的元件排列，而BD-RIS通过低成本简单的元件间连接提供了更大的波幅和相位配置自由度。然而，实现BD-RIS优势面临诸多挑战，需要研究前沿方案和算法，特别是在特定环境条件下的被动波束赋形设计。

Method: 1. 系统介绍BD-RIS的功能原理、架构设计、优势和分类；2. 总结最新进展并识别挑战机遇；3. 案例研究：使用四种不同算法设计波束赋形，分析其和速率与计算成本性能；4. 分析多种混合量子-经典机器学习模型，使用DeepSense 6G数据集中的真实通信场景8来提升波束预测性能。

Result: 1. 提供了BD-RIS的系统性介绍和分类框架；2. 识别了该领域的关键挑战和机遇；3. 通过算法比较获得了波束赋形设计的性能见解；4. 展示了量子增强机器学习模型在6G BD-RIS中的潜在应用价值。

Conclusion: BD-RIS代表了波操控的革命性进步，通过元件间连接提供了更大的配置自由度。虽然面临实现挑战，但通过先进的算法设计和量子增强的机器学习方法，BD-RIS在6G通信中具有重要的实际应用前景，需要进一步研究以充分发挥其潜力。

Abstract: A beyond-diagonal reconfigurable intelligent surface (BD-RIS) is an innovative type of reconfigurable intelligent surface (RIS) that has recently been proposed and is considered a revolutionary advancement in wave manipulation. Unlike the mutually disconnected arrangement of elements in traditional RISs, BD-RIS creates cost-effective and simple inter-element connections, allowing for greater freedom in configuring the amplitude and phase of impinging waves. However, there are numerous underlying challenges in realizing the advantages associated with BD-RIS, prompting the research community to actively investigate cutting-edge schemes and algorithms in this direction. Particularly, the passive beamforming design for BD-RIS under specific environmental conditions has become a major focus in this research area. In this article, we provide a systematic introduction to BD-RIS, elaborating on its functional principles concerning architectural design, promising advantages, and classification. Subsequently, we present recent advances and identify a series of challenges and opportunities. Additionally, we consider a specific case study where beamforming is designed using four different algorithms, and we analyze their performance with respect to sum rate and computation cost. To augment the beamforming capabilities in 6G BD-RIS with quantum enhancement, we analyze various hybrid quantum-classical machine learning (ML) models to improve beam prediction performance, employing real-world communication Scenario 8 from the DeepSense 6G dataset. Consequently, we derive useful insights about the practical implications of BD-RIS.

</details>


### [8] [Information is localized in growing network models](https://arxiv.org/abs/2512.23622)
*Till Hoffmann,Jukka-Pekka Onnela*

Main category: cs.SI

TL;DR: 该论文提出网络增长模型中的信息局部化理论，并开发基于图神经网络的后验分布估计方法，证明即使对于非局部化模型也能高效推断参数。


<details>
  <summary>Details</summary>
Motivation: 机制网络模型能捕捉经验网络的关键特征，但似然函数通常难以处理，导致推断困难。需要开发有效的推断方法，特别是对于增长网络模型。

Method: 采用贝叶斯推断视角，开发基于图神经网络的神经密度估计器来近似模型参数的后验分布。GNN具有有限的感受野，只能"看到"小子图。理论分析九种增长网络模型的局部化特性。

Result: 发现信息局部化是网络增长的基本属性，局部化预测与NDE在模拟数据上的结果一致。即使对于非局部化模型，NDE也能以低成本推断出高保真后验分布，与模型特定推断方法匹配。

Conclusion: 信息局部化理论为分析嵌入大型未观测网络中的局部子图提供了理论依据，并证明了使用有限感受野GNN进行无似然推断的合理性。

Abstract: Mechanistic network models can capture salient characteristics of empirical networks using a small set of domain-specific, interpretable mechanisms. Yet inference remains challenging because the likelihood is often intractable. We show that, for a broad class of growing network models, information about model parameters is localized in the network, i.e., the likelihood can be expressed in terms of small subgraphs. We take a Bayesian perspective to inference and develop neural density estimators (NDEs) to approximate the posterior distribution of model parameters using graph neural networks (GNNs) with limited receptive size, i.e., the GNN can only "see" small subgraphs. We characterize nine growing network models in terms of their localization and demonstrate that localization predictions agree with NDEs on simulated data. Even for non-localized models, NDEs can infer high-fidelity posteriors matching model-specific inference methods at a fraction of the cost. Our findings establish information localization as a fundamental property of network growth, theoretically justifying the analysis of local subgraphs embedded in larger, unobserved networks and the use of GNNs with limited receptive field for likelihood-free inference.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [9] [Bidirectional RAG: Safe Self-Improving Retrieval-Augmented Generation Through Multi-Stage Validation](https://arxiv.org/abs/2512.22199)
*Teja Chinthala*

Main category: cs.AI

TL;DR: 提出双向RAG架构，通过验证写入高质量生成响应实现安全的知识库扩展，相比标准RAG覆盖度提升近一倍，同时减少72%的文档写入


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统使用静态知识库，无法从用户交互中学习进化。需要一种既能扩展知识库又能防止幻觉污染的安全机制

Method: 双向RAG架构，包含多阶段接受层：基于NLI的蕴含验证、归因检查和新颖性检测，确保高质量响应安全写入知识库

Result: 在四个数据集上，双向RAG平均覆盖度达40.58%（标准RAG为20.33%），仅写入140个文档（朴素写入需500个），减少72%文档写入

Conclusion: 通过严格验证机制，自改进的RAG系统是可行且安全的，为RAG系统从部署中学习提供了实用路径

Abstract: Retrieval-Augmented Generation RAG systems enhance large language models by grounding responses in external knowledge bases, but conventional RAG architectures operate with static corpora that cannot evolve from user interactions. We introduce Bidirectional RAG, a novel RAG architecture that enables safe corpus expansion through validated write back of high quality generated responses. Our system employs a multi stage acceptance layer combining grounding verification (NLI based entailment, attribution checking, and novelty detection to prevent hallucination pollution while enabling knowledge accumulation. Across four datasets Natural Questions, TriviaQA, HotpotQA, Stack Overflow with three random seeds 12 experiments per system, Bidirectional RAG achieves 40.58% average coverage nearly doubling Standard RAG 20.33% while adding 72% fewer documents than naive write back 140 vs 500. Our work demonstrates that self improving RAG is feasible and safe when governed by rigorous validation, offering a practical path toward RAG systems that learn from deployment.

</details>


### [10] [Emergent Persuasion: Will LLMs Persuade Without Being Prompted?](https://arxiv.org/abs/2512.22201)
*Vincent Chang,Thee Ho,Sunishchal Dev,Kevin Zhu,Shi Feng,Kellin Pelrine,Matthew Kowal*

Main category: cs.AI

TL;DR: 研究大型语言模型在未被明确提示情况下的说服行为，发现监督微调（而非激活引导）会增加模型在有害话题上的说服倾向。


<details>
  <summary>Details</summary>
Motivation: 随着对话AI系统的广泛应用，AI对人类观点和信念的影响力前所未有。先前研究主要关注滥用场景下的说服风险，但本研究旨在探究模型在未被明确提示情况下自发进行说服的条件，以评估这种新兴风险。

Method: 研究两种场景下的无提示说服行为：1）通过内部激活引导使模型具备特定人格特质；2）通过监督微调使模型展现相同特质。使用包含良性话题的一般说服数据集进行微调。

Result: 激活引导（无论是否与说服相关）并不能可靠增加模型的无提示说服倾向，但监督微调可以。更重要的是，仅使用良性话题进行监督微调得到的模型，在争议性和有害话题上表现出更高的说服倾向。

Conclusion: 有害的无提示说服行为可能自发产生，需要进一步研究。监督微调（而非激活引导）是引发这种风险的主要途径，即使微调数据仅包含良性话题。

Abstract: With the wide-scale adoption of conversational AI systems, AI are now able to exert unprecedented influence on human opinion and beliefs. Recent work has shown that many Large Language Models (LLMs) comply with requests to persuade users into harmful beliefs or actions when prompted and that model persuasiveness increases with model scale. However, this prior work looked at persuasion from the threat model of $\textit{misuse}$ (i.e., a bad actor asking an LLM to persuade). In this paper, we instead aim to answer the following question: Under what circumstances would models persuade $\textit{without being explicitly prompted}$, which would shape how concerned we should be about such emergent persuasion risks. To achieve this, we study unprompted persuasion under two scenarios: (i) when the model is steered (through internal activation steering) along persona traits, and (ii) when the model is supervised-finetuned (SFT) to exhibit the same traits. We showed that steering towards traits, both related to persuasion and unrelated, does not reliably increase models' tendency to persuade unprompted, however, SFT does. Moreover, SFT on general persuasion datasets containing solely benign topics admits a model that has a higher propensity to persuade on controversial and harmful topics--showing that emergent harmful persuasion can arise and should be studied further.

</details>


### [11] [GamiBench: Evaluating Spatial Reasoning and 2D-to-3D Planning Capabilities of MLLMs with Origami Folding Tasks](https://arxiv.org/abs/2512.22207)
*Ryan Spencer,Roey Yaari,Ritvik Vemavarapu,Joyce Yang,Steven Ngo,Utkarsh Sharma*

Main category: cs.AI

TL;DR: GamiBench是一个评估多模态大语言模型空间推理能力的基准测试，通过折纸任务测试模型在多个视角下的2D到3D规划和空间理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在空间推理方面存在不足，而当前基准测试主要关注静态图像或最终输出，无法评估序列化和视角依赖的空间推理能力。需要开发能够全面评估空间推理过程的基准。

Method: 创建GamiBench基准，包含186个常规和186个不可能的2D折痕图案及其对应的3D折叠形状，从6个不同视角生成。设计了三个视觉问答任务：预测3D折叠配置、区分有效视角、检测不可能图案。引入新的诊断指标：视角一致性(VC)和不可能折叠选择率(IFSR)。

Result: 实验表明，即使是GPT-5和Gemini-2.5-Pro等领先模型在单步空间理解任务上也表现不佳，揭示了当前MLLM在空间推理方面的局限性。

Conclusion: GamiBench为评估MLLM的几何理解和空间推理能力建立了标准化框架，填补了现有基准的空白，有助于推动模型在空间推理方面的发展。

Abstract: Multimodal large language models (MLLMs) are proficient in perception and instruction-following, but they still struggle with spatial reasoning: the ability to mentally track and manipulate objects across multiple views and over time. Spatial reasoning is a key component of human intelligence, but most existing benchmarks focus on static images or final outputs, failing to account for the sequential and viewpoint-dependent nature of this skill. To close this gap, we introduce GamiBench, a benchmark designed to evaluate spatial reasoning and 2D-to-3D planning in MLLMs through origami-inspired folding tasks. GamiBench includes 186 regular and 186 impossible 2D crease patterns paired with their corresponding 3D folded shapes, produced from six distinct viewpoints across three visual question-answering (VQA) tasks: predicting 3D fold configurations, distinguishing valid viewpoints, and detecting impossible patterns. Unlike previous benchmarks that assess only final predictions, GamiBench holistically evaluates the entire reasoning process--measuring cross-view consistency, physical feasibility through impossible-fold detection, and interpretation of intermediate folding steps. It further introduces new diagnostic metrics--viewpoint consistency (VC) and impossible fold selection rate (IFSR)--to measure how well models handle folds of varying complexity. Our experiments show that even leading models such as GPT-5 and Gemini-2.5-Pro struggle on single-step spatial understanding. These contributions establish a standardized framework for evaluating geometric understanding and spatial reasoning in MLLMs. Dataset and code: https://github.com/stvngo/GamiBench.

</details>


### [12] [Toward Equitable Recovery: A Fairness-Aware AI Framework for Prioritizing Post-Flood Aid in Bangladesh](https://arxiv.org/abs/2512.22210)
*Farjana Yesmin,Romana Akter*

Main category: cs.AI

TL;DR: 提出一个公平感知的AI框架，用于孟加拉国洪水后援助分配，通过对抗性去偏技术减少对边缘地区的系统性偏见，同时保持预测准确性。


<details>
  <summary>Details</summary>
Motivation: 发展中国家灾后援助分配存在系统性偏见，边缘地区往往处于不利地位，延续历史不平等。需要公平的AI框架来确保援助基于真实需求而非历史分配模式。

Method: 采用对抗性去偏模型，使用梯度反转层学习偏置不变的表示，将医疗AI中的公平感知表示学习技术应用于灾害管理。基于2022年孟加拉国洪水真实数据，涵盖11个地区的87个upazilas。

Result: 模型将统计奇偶差异减少41.6%，区域公平差距降低43.2%，同时保持强预测准确性（R平方=0.784 vs 基线0.811）。生成可操作的优先级排名，确保援助到达最脆弱人群。

Conclusion: 算法公平技术可有效应用于人道主义背景，为决策者提供更公平的灾后恢复策略工具，确保援助基于真实需求而非历史偏见。

Abstract: Post-disaster aid allocation in developing nations often suffers from systematic biases that disadvantage vulnerable regions, perpetuating historical inequities. This paper presents a fairness-aware artificial intelligence framework for prioritizing post-flood aid distribution in Bangladesh, a country highly susceptible to recurring flood disasters. Using real data from the 2022 Bangladesh floods that affected 7.2 million people and caused 405.5 million US dollars in damages, we develop an adversarial debiasing model that predicts flood vulnerability while actively removing biases against marginalized districts and rural areas. Our approach adapts fairness-aware representation learning techniques from healthcare AI to disaster management, employing a gradient reversal layer that forces the model to learn bias-invariant representations. Experimental results on 87 upazilas across 11 districts demonstrate that our framework reduces statistical parity difference by 41.6 percent, decreases regional fairness gaps by 43.2 percent, and maintains strong predictive accuracy (R-squared=0.784 vs baseline 0.811). The model generates actionable priority rankings ensuring aid reaches the most vulnerable populations based on genuine need rather than historical allocation patterns. This work demonstrates how algorithmic fairness techniques can be effectively applied to humanitarian contexts, providing decision-makers with tools to implement more equitable disaster recovery strategies.

</details>


### [13] [From Model Choice to Model Belief: Establishing a New Measure for LLM-Based Research](https://arxiv.org/abs/2512.23184)
*Hongshen Sun,Juanjuan Zhang*

Main category: cs.AI

TL;DR: 该论文提出"模型信念"概念，利用LLM的token级概率分布来替代传统的"模型选择"输出，证明模型信念具有更高的统计效率和更快的收敛速度，在需求估计实验中可将计算需求降低约20倍。


<details>
  <summary>Details</summary>
Motivation: 当前使用LLM模拟人类行为时，通常将LLM的输出（"模型选择"）作为单个数据点，这未能充分利用LLM的概率特性所蕴含的信息，导致数据使用效率低下。

Method: 提出并形式化"模型信念"概念，从LLM的token级概率分布中提取模型对选择替代方案的信念分布。证明模型信念与模型选择的均值渐近等价，但具有更低的方差和更快的收敛速度。

Result: 在需求估计研究中，模型信念在有限运行次数下比模型选择本身更好地解释和预测真实模型选择，将达到足够准确估计所需的计算量减少了约20倍。

Conclusion: 模型信念应作为从LLM生成数据中提取更多信息的默认测量方法，能够显著提高统计效率和计算效率。

Abstract: Large language models (LLMs) are increasingly used to simulate human behavior, but common practices to use LLM-generated data are inefficient. Treating an LLM's output ("model choice") as a single data point underutilizes the information inherent to the probabilistic nature of LLMs. This paper introduces and formalizes "model belief," a measure derived from an LLM's token-level probabilities that captures the model's belief distribution over choice alternatives in a single generation run. The authors prove that model belief is asymptotically equivalent to the mean of model choices (a non-trivial property) but forms a more statistically efficient estimator, with lower variance and a faster convergence rate. Analogous properties are shown to hold for smooth functions of model belief and model choice often used in downstream applications. The authors demonstrate the performance of model belief through a demand estimation study, where an LLM simulates consumer responses to different prices. In practical settings with limited numbers of runs, model belief explains and predicts ground-truth model choice better than model choice itself, and reduces the computation needed to reach sufficiently accurate estimates by roughly a factor of 20. The findings support using model belief as the default measure to extract more information from LLM-generated data.

</details>


### [14] [With Great Capabilities Come Great Responsibilities: Introducing the Agentic Risk & Capability Framework for Governing Agentic AI Systems](https://arxiv.org/abs/2512.22211)
*Shaun Khoo,Jessica Foo,Roy Ka-Wei Lee*

Main category: cs.AI

TL;DR: 提出了Agentic Risk & Capability (ARC)框架，这是一个技术治理框架，帮助组织识别、评估和减轻由智能AI系统带来的风险，通过能力中心视角分析风险源并提供技术控制措施。


<details>
  <summary>Details</summary>
Motivation: 智能AI系统具有自主行动能力（如代码执行、互联网交互、文件修改），带来了重大机遇和新风险，对组织治理提出了挑战，需要系统方法来识别、评估和减轻这些风险。

Method: 开发了ARC框架，采用能力中心视角分析智能AI系统，识别三个主要风险源（组件、设计、能力），建立风险源-具体风险-技术控制之间的明确联系，并提供结构化实施方法。

Result: ARC框架为组织提供了一个稳健且适应性强的方法论，能够快速有效创新，同时确保智能AI系统的安全、可靠和负责任部署。框架已开源。

Conclusion: ARC框架通过系统化的技术治理方法，帮助组织应对智能AI系统的复杂风险，平衡创新与安全，促进负责任的人工智能部署。

Abstract: Agentic AI systems present both significant opportunities and novel risks due to their capacity for autonomous action, encompassing tasks such as code execution, internet interaction, and file modification. This poses considerable challenges for effective organizational governance, particularly in comprehensively identifying, assessing, and mitigating diverse and evolving risks. To tackle this, we introduce the Agentic Risk \& Capability (ARC) Framework, a technical governance framework designed to help organizations identify, assess, and mitigate risks arising from agentic AI systems. The framework's core contributions are: (1) it develops a novel capability-centric perspective to analyze a wide range of agentic AI systems; (2) it distills three primary sources of risk intrinsic to agentic AI systems - components, design, and capabilities; (3) it establishes a clear nexus between each risk source, specific materialized risks, and corresponding technical controls; and (4) it provides a structured and practical approach to help organizations implement the framework. This framework provides a robust and adaptable methodology for organizations to navigate the complexities of agentic AI, enabling rapid and effective innovation while ensuring the safe, secure, and responsible deployment of agentic AI systems. Our framework is open-sourced \href{https://govtech-responsibleai.github.io/agentic-risk-capability-framework/}{here}.

</details>


### [15] [We are not able to identify AI-generated images](https://arxiv.org/abs/2512.22236)
*Adrien Pavão*

Main category: cs.AI

TL;DR: 人类难以区分AI生成图像与真实照片，实验显示平均准确率仅54%，略高于随机猜测


<details>
  <summary>Details</summary>
Motivation: 尽管AI生成图像已普遍存在，但许多人仍认为自己能轻易区分AI生成内容与真实照片，本研究旨在验证这一假设

Method: 通过交互式网络实验，让参与者对20张图像（包含真实照片和AI生成图像）进行分类，数据集包含120个困难案例，共165名用户完成233次测试

Result: 参与者平均准确率仅54%，略高于随机猜测，重复尝试改善有限；平均响应时间7.3秒，某些图像更具欺骗性

Conclusion: 即使在相对简单的人像图像上，人类也难以可靠检测AI生成内容，随着合成媒体技术改进，仅靠人类判断已不足够，需要提高意识和制定伦理准则

Abstract: AI-generated images are now pervasive online, yet many people believe they can easily tell them apart from real photographs. We test this assumption through an interactive web experiment where participants classify 20 images as real or AI-generated. Our dataset contains 120 difficult cases: real images sampled from CC12M, and carefully curated AI-generated counterparts produced with MidJourney. In total, 165 users completed 233 sessions. Their average accuracy was 54%, only slightly above random guessing, with limited improvement across repeated attempts. Response times averaged 7.3 seconds, and some images were consistently more deceptive than others. These results indicate that, even on relatively simple portrait images, humans struggle to reliably detect AI-generated content. As synthetic media continues to improve, human judgment alone is becoming insufficient for distinguishing real from artificial data. These findings highlight the need for greater awareness and ethical guidelines as AI-generated media becomes increasingly indistinguishable from reality.

</details>


### [16] [Shape of Thought: When Distribution Matters More than Correctness in Reasoning Tasks](https://arxiv.org/abs/2512.22255)
*Abhranil Chandra,Ayush Agrawal,Arian Hosseini,Sebastian Fischmeister,Rishabh Agarwal,Navin Goyal,Aaron Courville*

Main category: cs.AI

TL;DR: 研究发现：即使思维链（CoT）轨迹最终答案错误，用大模型生成的合成数据集训练语言模型也能提升推理能力，效果优于人类标注数据。


<details>
  <summary>Details</summary>
Motivation: 探索如何更有效地提升语言模型的推理能力，特别是研究合成数据与模型自身分布匹配度对学习效果的影响，以及错误推理轨迹中可能包含的有价值信息。

Method: 1. 使用更强大模型生成包含错误最终答案的思维链合成数据集进行训练
2. 用语言模型改写人类标注的思维链轨迹，使其分布更接近模型自身分布
3. 引入逐渐增加的思维链缺陷，测试模型对这些缺陷的容忍度
4. 在数学、算法推理和代码生成等多个领域进行实验，使用MATH、GSM8K、Countdown和MBPP等数据集，测试Qwen、Llama、Gemma等不同规模的模型（1.5B-9B）

Result: 1. 使用错误答案的合成思维链数据集训练比人类标注数据集效果更好
2. 改写人类标注轨迹使其分布更接近模型自身分布能提升性能
3. 模型对思维链中的部分缺陷具有容忍度，能从部分错误的推理中学习有效步骤
4. 最终答案正确性不能可靠反映推理过程的忠实性

Conclusion: 构建与模型自身分布更接近的数据集是提升推理能力的关键因素，错误答案的思维链轨迹仍可能包含有价值的推理步骤，为数据合成和模型训练提供了新思路。

Abstract: We present the surprising finding that a language model's reasoning capabilities can be improved by training on synthetic datasets of chain-of-thought (CoT) traces from more capable models, even when all of those traces lead to an incorrect final answer. Our experiments show this approach can yield better performance on reasoning tasks than training on human-annotated datasets. We hypothesize that two key factors explain this phenomenon: first, the distribution of synthetic data is inherently closer to the language model's own distribution, making it more amenable to learning. Second, these `incorrect' traces are often only partially flawed and contain valid reasoning steps from which the model can learn. To further test the first hypothesis, we use a language model to paraphrase human-annotated traces -- shifting their distribution closer to the model's own distribution -- and show that this improves performance. For the second hypothesis, we introduce increasingly flawed CoT traces and study to what extent models are tolerant to these flaws. We demonstrate our findings across various reasoning domains like math, algorithmic reasoning and code generation using MATH, GSM8K, Countdown and MBPP datasets on various language models ranging from 1.5B to 9B across Qwen, Llama, and Gemma models. Our study shows that curating datasets that are closer to the model's distribution is a critical aspect to consider. We also show that a correct final answer is not always a reliable indicator of a faithful reasoning process.

</details>


### [17] [Logic Sketch Prompting (LSP): A Deterministic and Interpretable Prompting Method](https://arxiv.org/abs/2512.22258)
*Satvik Tripathi*

Main category: cs.AI

TL;DR: Logic Sketch Prompting (LSP) 是一个轻量级提示框架，通过引入类型变量、确定性条件评估器和基于规则的验证器，显著提升LLM在需要严格规则遵循、确定性和可审计性任务上的表现，在药理学逻辑合规任务中优于多种基线方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言推理方面表现出色，但在需要严格规则遵循、确定性和可审计性的任务上仍然不可靠，特别是在临床、监管和安全关键决策支持系统中。

Method: 提出Logic Sketch Prompting (LSP)框架，包含类型变量、确定性条件评估器和基于规则的验证器，能够生成可追踪和可重复的输出。在三个开源模型(Gemma 2, Mistral, Llama 3)上使用两个药理学逻辑合规任务进行基准测试。

Result: 在所有模型和任务中，LSP始终获得最高的准确率(0.83-0.89)和F1分数(0.83-0.89)，显著优于零样本提示(0.24-0.60)、简洁提示(0.16-0.30)和思维链提示(0.56-0.75)。McNemar检验显示几乎所有比较中LSP都有统计学显著改进(p<0.01)。

Conclusion: LSP在不牺牲性能的情况下提高了确定性、可解释性和一致性，支持其在临床、监管和安全关键决策支持系统中的使用。

Abstract: Large language models (LLMs) excel at natural language reasoning but remain unreliable on tasks requiring strict rule adherence, determinism, and auditability. Logic Sketch Prompting (LSP) is a lightweight prompting framework that introduces typed variables, deterministic condition evaluators, and a rule based validator that produces traceable and repeatable outputs. Using two pharmacologic logic compliance tasks, we benchmark LSP against zero shot prompting, chain of thought prompting, and concise prompting across three open weight models: Gemma 2, Mistral, and Llama 3. Across both tasks and all models, LSP consistently achieves the highest accuracy (0.83 to 0.89) and F1 score (0.83 to 0.89), substantially outperforming zero shot prompting (0.24 to 0.60), concise prompts (0.16 to 0.30), and chain of thought prompting (0.56 to 0.75). McNemar tests show statistically significant gains for LSP across nearly all comparisons (p < 0.01). These results demonstrate that LSP improves determinism, interpretability, and consistency without sacrificing performance, supporting its use in clinical, regulated, and safety critical decision support systems.

</details>


### [18] [SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence](https://arxiv.org/abs/2512.22334)
*Yiheng Wang,Yixin Chen,Shuo Li,Yifan Zhou,Bo Liu,Hengjian Gao,Jiakang Yuan,Jia Bu,Wanghan Xu,Yuhao Zhou,Xiangyu Zhao,Zhiwang Zhou,Fengxiang Wang,Haodong Duan,Songyang Zhang,Jun Yao,Han Deng,Yizhou Wang,Jiabei Xiao,Jiaqi Liu,Encheng Su,Yujie Liu,Weida Wang,Junchi Yao,Shenghe Zheng,Haoran Sun,Runmin Ma,Xiangchao Yan,Bo Zhang,Dongzhan Zhou,Shufei Zhang,Peng Ye,Xiaosong Wang,Shixiang Tang,Wenlong Zhang,Lei Bai*

Main category: cs.AI

TL;DR: SciEvalKit是一个统一的科学AI模型评估工具包，专注于科学智能的核心能力评估，涵盖多个科学领域和任务类型。


<details>
  <summary>Details</summary>
Motivation: 现有通用评估平台无法充分评估科学AI模型的核心能力，需要专门针对科学智能的多模态感知、推理、理解、符号推理、代码生成、假设生成和知识理解等能力进行评估。

Method: 构建专家级科学基准，从真实世界领域特定数据集中精选任务，支持六个主要科学领域，提供灵活可扩展的评估管道，支持批量评估、自定义模型和数据集集成。

Result: 开发了一个开源、可扩展的科学AI评估工具包，支持透明、可重复、可比较的结果，为下一代科学基础模型和智能代理提供标准化但可定制的基础设施。

Conclusion: SciEvalKit通过桥接能力评估和学科多样性，为AI4Science领域的社区驱动发展和进步提供了标准化的评估基础设施。

Abstract: We introduce SciEvalKit, a unified benchmarking toolkit designed to evaluate AI models for science across a broad range of scientific disciplines and task capabilities. Unlike general-purpose evaluation platforms, SciEvalKit focuses on the core competencies of scientific intelligence, including Scientific Multimodal Perception, Scientific Multimodal Reasoning, Scientific Multimodal Understanding, Scientific Symbolic Reasoning, Scientific Code Generation, Science Hypothesis Generation and Scientific Knowledge Understanding. It supports six major scientific domains, spanning from physics and chemistry to astronomy and materials science. SciEvalKit builds a foundation of expert-grade scientific benchmarks, curated from real-world, domain-specific datasets, ensuring that tasks reflect authentic scientific challenges. The toolkit features a flexible, extensible evaluation pipeline that enables batch evaluation across models and datasets, supports custom model and dataset integration, and provides transparent, reproducible, and comparable results. By bridging capability-based evaluation and disciplinary diversity, SciEvalKit offers a standardized yet customizable infrastructure to benchmark the next generation of scientific foundation models and intelligent agents. The toolkit is open-sourced and actively maintained to foster community-driven development and progress in AI4Science.

</details>


### [19] [Agent2World: Learning to Generate Symbolic World Models via Adaptive Multi-Agent Feedback](https://arxiv.org/abs/2512.22336)
*Mengkang Hu,Bowei Xia,Yuran Wu,Ailing Yu,Yude Zou,Qiguang Chen,Shijian Wang,Jiarui Jin,Kexin Li,Wenxiang Jiao,Yuan Lu,Ping Luo*

Main category: cs.AI

TL;DR: Agent2World是一个工具增强的多智能体框架，通过多智能体反馈实现强大的推理时世界模型生成，并作为监督微调的数据引擎，在PDDL和可执行代码表示上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 当前训练LLMs生成符号世界模型（如PDDL域或可执行模拟器）受到大规模可验证监督数据缺乏的限制，现有方法主要依赖静态验证方法，无法捕捉交互执行中出现的行为级错误。

Method: 采用三阶段流水线：1) Deep Researcher智能体通过网页搜索进行知识合成以解决规范差距；2) Model Developer智能体实现可执行世界模型；3) 专门的Testing Team进行自适应单元测试和基于模拟的验证。

Result: 在三个涵盖PDDL和可执行代码表示的基准测试中展示了优越的推理时性能，取得一致的SOTA结果。通过Testing Team提供的交互式环境生成的多轮训练轨迹进行微调后，世界模型生成平均相对提升30.95%。

Conclusion: Agent2World不仅实现了强大的推理时世界模型生成，还作为监督微调的数据引擎，通过多智能体反馈机制解决了符号世界模型生成中的数据稀缺和验证难题。

Abstract: Symbolic world models (e.g., PDDL domains or executable simulators) are central to model-based planning, but training LLMs to generate such world models is limited by the lack of large-scale verifiable supervision. Current approaches rely primarily on static validation methods that fail to catch behavior-level errors arising from interactive execution. In this paper, we propose Agent2World, a tool-augmented multi-agent framework that achieves strong inference-time world-model generation and also serves as a data engine for supervised fine-tuning, by grounding generation in multi-agent feedback. Agent2World follows a three-stage pipeline: (i) A Deep Researcher agent performs knowledge synthesis by web searching to address specification gaps; (ii) A Model Developer agent implements executable world models; And (iii) a specialized Testing Team conducts adaptive unit testing and simulation-based validation. Agent2World demonstrates superior inference-time performance across three benchmarks spanning both Planning Domain Definition Language (PDDL) and executable code representations, achieving consistent state-of-the-art results. Beyond inference, Testing Team serves as an interactive environment for the Model Developer, providing behavior-aware adaptive feedback that yields multi-turn training trajectories. The model fine-tuned on these trajectories substantially improves world-model generation, yielding an average relative gain of 30.95% over the same model before training. Project page: https://agent2world.github.io.

</details>


### [20] [Subgoaling Relaxation-based Heuristics for Numeric Planning with Infinite Actions](https://arxiv.org/abs/2512.22367)
*Ángel Aso-Mollar,Diego Aineto,Enrico Scala,Eva Onaindia*

Main category: cs.AI

TL;DR: 提出一种将带有控制参数的数值规划问题编译为简单数值任务的方法，使传统启发式方法能处理无限动作空间问题


<details>
  <summary>Details</summary>
Motivation: 带有控制参数的数值规划引入了无限可能的动作，使得基于动作结构的现成数值启发式方法不可行，需要新的解决方案

Method: 识别可控简单数值问题子集，采用乐观编译方法将其转换为简单数值任务，通过将控制相关表达式抽象为有界常数效应和宽松前提条件

Result: 该方法能有效使用子目标启发式估计目标距离，在涉及控制参数的数值规划问题中表现出有效性和计算可行性

Conclusion: 提出的编译方法扩展了传统数值启发式方法的应用范围，能处理无限动作空间问题，推动了当前技术前沿

Abstract: Numeric planning with control parameters extends the standard numeric planning model by introducing action parameters as free numeric variables that must be instantiated during planning. This results in a potentially infinite number of applicable actions in a state. In this setting, off-the-shelf numeric heuristics that leverage the action structure are not feasible. In this paper, we identify a tractable subset of these problems--namely, controllable, simple numeric problems--and propose an optimistic compilation approach that transforms them into simple numeric tasks. To do so, we abstract control-dependent expressions into bounded constant effects and relaxed preconditions. The proposed compilation makes it possible to effectively use subgoaling heuristics to estimate goal distance in numeric planning problems involving control parameters. Our results demonstrate that this approach is an effective and computationally feasible way of applying traditional numeric heuristics to settings with an infinite number of possible actions, pushing the boundaries of the current state of the art.

</details>


### [21] [HalluMat: Detecting Hallucinations in LLM-Generated Materials Science Content Through Multi-Stage Verification](https://arxiv.org/abs/2512.22396)
*Bhanu Prakash Vangala,Sajid Mahmud,Pawan Neupane,Joel Selvaraj,Jianlin Cheng*

Main category: cs.AI

TL;DR: 该论文提出了HalluMatData基准数据集和HalluMatDetector多阶段幻觉检测框架，用于评估和减少材料科学领域AI生成内容中的幻觉问题，将幻觉率降低了30%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在科学发现中产生幻觉（生成事实错误或误导性信息）的问题严重影响了研究完整性，特别是在材料科学领域需要可靠的事实一致性。

Method: 提出了HalluMatData基准数据集用于评估幻觉检测方法，并开发了HalluMatDetector多阶段检测框架，包括内在验证、多源检索、矛盾图分析和基于度量的评估。

Result: 研究发现材料科学不同子领域的幻觉水平差异显著，高熵查询表现出更大的事实不一致性。使用HalluMatDetector验证流程将幻觉率比标准LLM输出降低了30%。

Conclusion: 该研究为解决材料科学领域AI生成内容中的幻觉问题提供了有效的基准数据集和检测框架，通过多阶段验证显著提高了LLM输出的可靠性，并提出了PHCS指标来量化模型一致性。

Abstract: Artificial Intelligence (AI), particularly Large Language Models (LLMs), is transforming scientific discovery, enabling rapid knowledge generation and hypothesis formulation. However, a critical challenge is hallucination, where LLMs generate factually incorrect or misleading information, compromising research integrity. To address this, we introduce HalluMatData, a benchmark dataset for evaluating hallucination detection methods, factual consistency, and response robustness in AI-generated materials science content. Alongside this, we propose HalluMatDetector, a multi-stage hallucination detection framework that integrates intrinsic verification, multi-source retrieval, contradiction graph analysis, and metric-based assessment to detect and mitigate LLM hallucinations. Our findings reveal that hallucination levels vary significantly across materials science subdomains, with high-entropy queries exhibiting greater factual inconsistencies. By utilizing HalluMatDetector verification pipeline, we reduce hallucination rates by 30% compared to standard LLM outputs. Furthermore, we introduce the Paraphrased Hallucination Consistency Score (PHCS) to quantify inconsistencies in LLM responses across semantically equivalent queries, offering deeper insights into model reliability.

</details>


### [22] [Lightweight Inference-Time Personalization for Frozen Knowledge Graph Embeddings](https://arxiv.org/abs/2512.22398)
*Ozan Oguztuzun,Cerag Oguztuzun*

Main category: cs.AI

TL;DR: GatedBias：轻量级推理时个性化框架，通过结构门控适配将冻结的知识图谱嵌入适应到个体用户上下文，仅需约300个可训练参数，在保持群体性能的同时显著提升个性化排名。


<details>
  <summary>Details</summary>
Motivation: 知识图谱基础模型在链接预测上具有强大的群体级性能，但无法捕捉个体用户偏好，这是通用关系推理与个性化排名之间的关键脱节。

Method: 提出GatedBias框架，采用结构门控适配：将特定于用户档案的特征与图导出的二元门结合，产生可解释的、每个实体的偏置，无需重新训练或损害全局准确性。

Result: 在两个基准数据集（Amazon-Book和Last-FM）上评估，展示了对齐指标的统计显著改进，同时保持了群体性能。反事实扰动实验验证了因果响应性：受益于特定偏好信号的实体在信号增强时显示出6-30倍的排名改进。

Conclusion: 基础模型的个性化适配可以是参数高效且因果可验证的，能够桥接通用知识表示与个体用户需求。

Abstract: Foundation models for knowledge graphs (KGs) achieve strong cohort-level performance in link prediction, yet fail to capture individual user preferences; a key disconnect between general relational reasoning and personalized ranking. We propose GatedBias, a lightweight inference-time personalization framework that adapts frozen KG embeddings to individual user contexts without retraining or compromising global accuracy. Our approach introduces structure-gated adaptation: profile-specific features combine with graph-derived binary gates to produce interpretable, per-entity biases, requiring only ${\sim}300$ trainable parameters. We evaluate GatedBias on two benchmark datasets (Amazon-Book and Last-FM), demonstrating statistically significant improvements in alignment metrics while preserving cohort performance. Counterfactual perturbation experiments validate causal responsiveness; entities benefiting from specific preference signals show 6--30$\times$ greater rank improvements when those signals are boosted. These results show that personalized adaptation of foundation models can be both parameter-efficient and causally verifiable, bridging general knowledge representations with individual user needs.

</details>


### [23] [Monadic Context Engineering](https://arxiv.org/abs/2512.22431)
*Yifan Zhang,Mengdi Wang*

Main category: cs.AI

TL;DR: 提出Monadic Context Engineering (MCE)架构范式，利用函子、应用函子和单子的代数结构为AI智能体设计提供形式化基础，解决现有智能体架构的脆弱性问题。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型驱动的自主智能体架构通常采用命令式、临时性模式构建，导致系统脆弱，存在状态管理、错误处理和并发控制等困难。

Method: 引入MCE架构范式，将智能体工作流视为计算上下文，利用函子、应用函子和单子的代数特性内在管理横切关注点。使用单子实现健壮的顺序组合，应用函子提供并行执行的结构化方法，单子变换器支持这些能力的系统化组合。

Result: MCE使开发者能够从简单、可独立验证的组件构建复杂、健壮且高效的AI智能体。该框架还扩展到元智能体，通过元编程动态创建和管理子智能体工作流。

Conclusion: MCE为AI智能体设计提供了形式化基础，通过代数结构的内在特性解决现有架构的脆弱性问题，支持复杂智能体系统的模块化构建和生成式编排。

Abstract: The proliferation of Large Language Models (LLMs) has catalyzed a shift towards autonomous agents capable of complex reasoning and tool use. However, current agent architectures are frequently constructed using imperative, ad hoc patterns. This results in brittle systems plagued by difficulties in state management, error handling, and concurrency. This paper introduces Monadic Context Engineering (MCE), a novel architectural paradigm leveraging the algebraic structures of Functors, Applicative Functors, and Monads to provide a formal foundation for agent design. MCE treats agent workflows as computational contexts where cross-cutting concerns, such as state propagation, short-circuiting error handling, and asynchronous execution, are managed intrinsically by the algebraic properties of the abstraction. We demonstrate how Monads enable robust sequential composition, how Applicatives provide a principled structure for parallel execution, and crucially, how Monad Transformers allow for the systematic composition of these capabilities. This layered approach enables developers to construct complex, resilient, and efficient AI agents from simple, independently verifiable components. We further extend this framework to describe Meta-Agents, which leverage MCE for generative orchestration, dynamically creating and managing sub-agent workflows through metaprogramming. Project Page: https://github.com/yifanzhang-pro/monadic-context-engineering.

</details>


### [24] [DarkPatterns-LLM: A Multi-Layer Benchmark for Detecting Manipulative and Harmful AI Behavior](https://arxiv.org/abs/2512.22470)
*Sadia Asif,Israel Antonio Rosales Laguan,Haris Khan,Shumaila Asif,Muneeb Asif*

Main category: cs.AI

TL;DR: DarkPatterns-LLM是一个用于评估大语言模型操纵性内容的综合基准数据集和诊断框架，包含7种危害类别和401个精心策划的示例，揭示了主流模型在检测操纵内容方面的显著性能差异和弱点。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的普及，人们越来越担心其可能产生的操纵性或欺骗性行为会损害用户自主权、信任和福祉。现有的安全基准主要依赖粗糙的二元标签，无法捕捉构成操纵的微妙心理和社会机制。

Method: 提出了DarkPatterns-LLM基准数据集和诊断框架，包含7种危害类别：法律/权力、心理、情感、身体、自主权、经济和社会危害。框架采用四层分析管道：多粒度检测、多尺度意图分析、威胁协调协议和深度上下文风险对齐。数据集包含401个精心策划的指令-响应对和专家标注。

Result: 评估了GPT-4、Claude 3.5和LLaMA-3-70B等最先进模型，观察到显著的性能差异（65.2%-89.7%），并发现这些模型在检测损害自主权的模式方面存在一致的弱点。

Conclusion: DarkPatterns-LLM建立了首个用于大语言模型操纵检测的标准化、多维度基准，为构建更可信的AI系统提供了可操作的诊断工具。

Abstract: The proliferation of Large Language Models (LLMs) has intensified concerns about manipulative or deceptive behaviors that can undermine user autonomy, trust, and well-being. Existing safety benchmarks predominantly rely on coarse binary labels and fail to capture the nuanced psychological and social mechanisms constituting manipulation. We introduce \textbf{DarkPatterns-LLM}, a comprehensive benchmark dataset and diagnostic framework for fine-grained assessment of manipulative content in LLM outputs across seven harm categories: Legal/Power, Psychological, Emotional, Physical, Autonomy, Economic, and Societal Harm. Our framework implements a four-layer analytical pipeline comprising Multi-Granular Detection (MGD), Multi-Scale Intent Analysis (MSIAN), Threat Harmonization Protocol (THP), and Deep Contextual Risk Alignment (DCRA). The dataset contains 401 meticulously curated examples with instruction-response pairs and expert annotations. Through evaluation of state-of-the-art models including GPT-4, Claude 3.5, and LLaMA-3-70B, we observe significant performance disparities (65.2\%--89.7\%) and consistent weaknesses in detecting autonomy-undermining patterns. DarkPatterns-LLM establishes the first standardized, multi-dimensional benchmark for manipulation detection in LLMs, offering actionable diagnostics toward more trustworthy AI systems.

</details>


### [25] [Multi-AI Agent Framework Reveals the "Oxide Gatekeeper" in Aluminum Nanoparticle Oxidation](https://arxiv.org/abs/2512.22529)
*Yiming Lu,Tingyu Lu,Di Zhang,Lili Ye,Hao Li*

Main category: cs.AI

TL;DR: 开发人机协同AI框架训练机器学习势函数，揭示铝纳米颗粒氧化机制：温度调控的双模式氧化过程，铝阳离子扩散主导质量传输


<details>
  <summary>Details</summary>
Motivation: 铝纳米颗粒作为高能量密度固体燃料，其从钝化态到爆炸反应的原子机制尚不清楚。传统计算方法存在瓶颈：从头算方法精度高但尺度有限，经验力场缺乏反应保真度。

Method: 采用"人在回路"闭环框架，自审计AI代理验证机器学习势函数的演化。AI作为科学哨兵可视化隐藏模型伪影供人类决策，确保量子力学精度，实现百万原子系统的近线性扩展和纳秒时间尺度模拟。

Result: 发现温度调控的双模式氧化机制：中等温度下氧化物壳层作为动态"守门员"，通过瞬态纳米通道的"呼吸模式"调控氧化；超过临界阈值时，"破裂模式"导致壳层灾难性失效和爆炸燃烧。铝阳离子向外扩散在所有温度区间都主导质量传输，扩散系数比氧高2-3个数量级。

Conclusion: 建立了高能纳米材料设计的统一原子尺度框架，通过智能计算设计实现点火敏感性和能量释放速率的精确调控，解决了数十年的争议。

Abstract: Aluminum nanoparticles (ANPs) are among the most energy-dense solid fuels, yet the atomic mechanisms governing their transition from passivated particles to explosive reactants remain elusive. This stems from a fundamental computational bottleneck: ab initio methods offer quantum accuracy but are restricted to small spatiotemporal scales (< 500 atoms, picoseconds), while empirical force fields lack the reactive fidelity required for complex combustion environments. Herein, we bridge this gap by employing a "human-in-the-loop" closed-loop framework where self-auditing AI Agents validate the evolution of a machine learning potential (MLP). By acting as scientific sentinels that visualize hidden model artifacts for human decision-making, this collaborative cycle ensures quantum mechanical accuracy while exhibiting near-linear scalability to million-atom systems and accessing nanosecond timescales (energy RMSE: 1.2 meV/atom, force RMSE: 0.126 eV/Angstrom). Strikingly, our simulations reveal a temperature-regulated dual-mode oxidation mechanism: at moderate temperatures, the oxide shell acts as a dynamic "gatekeeper," regulating oxidation through a "breathing mode" of transient nanochannels; above a critical threshold, a "rupture mode" unleashes catastrophic shell failure and explosive combustion. Importantly, we resolve a decades-old controversy by demonstrating that aluminum cation outward diffusion, rather than oxygen transport, dominates mass transfer across all temperature regimes, with diffusion coefficients consistently exceeding those of oxygen by 2-3 orders of magnitude. These discoveries establish a unified atomic-scale framework for energetic nanomaterial design, enabling the precision engineering of ignition sensitivity and energy release rates through intelligent computational design.

</details>


### [26] [Lessons from Neuroscience for AI: How integrating Actions, Compositional Structure and Episodic Memory could enable Safe, Interpretable and Human-Like AI](https://arxiv.org/abs/2512.22568)
*Rajesh P. N. Rao,Vishwas Sathish,Linxing Preston Jiang,Matthew Bryan,Prashant Rangarajan*

Main category: cs.AI

TL;DR: 论文主张将神经科学中的预测编码模型的关键组件（行动整合、层次组合结构、情景记忆）整合到基础模型中，以解决当前AI的幻觉、缺乏基础、安全性等问题，实现更安全、可解释、节能且类人的AI。


<details>
  <summary>Details</summary>
Motivation: 当前基于预测编码（最小化下一标记预测损失）的基础模型虽然取得了显著进展，但忽略了神经科学中预测编码模型的三个关键组件：行动与生成模型的紧密整合、层次组合结构、情景记忆。这些缺失导致了AI的幻觉、概念理解肤浅、缺乏基础、缺乏代理感/责任感、安全性和可信度威胁以及能源效率低下等问题。

Method: 提出将神经科学和认知科学中的预测编码模型的关键组件整合到基础模型中：1）在多个抽象层次上整合行动与生成模型；2）采用组合生成架构；3）加入情景记忆。通过神经科学证据支持每个组件的重要性，并与当前趋势（如思维链推理、检索增强生成）进行比较，讨论如何用脑启发组件增强这些模型。

Result: 论文没有报告具体的实验结果，而是提出了一个理论框架和研究方向。通过整合这些脑启发组件，可以解决基础模型的当前缺陷：减少幻觉、增强概念基础、增加代理感/责任感、提高安全性和可解释性、改善能源效率。

Conclusion: 重新点燃脑科学与AI之间历史上富有成果的思想交流，将有助于为实现安全、可解释、以人为中心的AI铺平道路。整合行动、组合结构和情景记忆到基础模型中，是实现更安全、可解释、节能且类人AI的关键方向。

Abstract: The phenomenal advances in large language models (LLMs) and other foundation models over the past few years have been based on optimizing large-scale transformer models on the surprisingly simple objective of minimizing next-token prediction loss, a form of predictive coding that is also the backbone of an increasingly popular model of brain function in neuroscience and cognitive science. However, current foundation models ignore three other important components of state-of-the-art predictive coding models: tight integration of actions with generative models, hierarchical compositional structure, and episodic memory. We propose that to achieve safe, interpretable, energy-efficient, and human-like AI, foundation models should integrate actions, at multiple scales of abstraction, with a compositional generative architecture and episodic memory. We present recent evidence from neuroscience and cognitive science on the importance of each of these components. We describe how the addition of these missing components to foundation models could help address some of their current deficiencies: hallucinations and superficial understanding of concepts due to lack of grounding, a missing sense of agency/responsibility due to lack of control, threats to safety and trustworthiness due to lack of interpretability, and energy inefficiency. We compare our proposal to current trends, such as adding chain-of-thought (CoT) reasoning and retrieval-augmented generation (RAG) to foundation models, and discuss new ways of augmenting these models with brain-inspired components. We conclude by arguing that a rekindling of the historically fruitful exchange of ideas between brain science and AI will help pave the way towards safe and interpretable human-centered AI.

</details>


### [27] [SANet: A Semantic-aware Agentic AI Networking Framework for Cross-layer Optimization in 6G](https://arxiv.org/abs/2512.22579)
*Yong Xiao,Xubo Li,Haoran Zhou,Yingyu Li,Yayu Gao,Guangming Shi,Ping Zhang,Marwan Krunz*

Main category: cs.AI

TL;DR: 提出SANet：一种面向无线网络的语义感知AgentNet架构，通过多智能体协作实现用户语义目标推断与网络层间自动分配，采用模型分区共享框架和去中心化优化算法，在硬件原型中实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: AgentNet作为新型AI原生网络范式，具有自主决策和环境适应能力，但去中心化协作中智能体可能存在目标冲突。需要解决如何在无线网络中实现语义感知、多目标优化和资源高效部署的问题。

Method: 1) 提出SANet语义感知AgentNet架构；2) 将问题建模为多智能体多目标优化问题，寻求帕累托最优解；3) 开发模型分区共享(MoPS)框架，将大模型划分为共享和智能体专用部分；4) 提出两种去中心化优化算法；5) 建立理论界限证明三向权衡关系。

Result: 1) 提出三个新的评估指标；2) 理论证明优化、泛化和冲突误差间的三向权衡；3) 开发基于RAN和核心网络的硬件原型；4) 实验结果显示性能提升达14.61%，仅需SOTA算法44.37%的FLOPs。

Conclusion: SANet通过语义感知和多智能体协作有效解决了无线网络中AgentNet的部署挑战，在性能提升和计算效率方面均优于现有方法，为AI原生网络的实际应用提供了可行方案。

Abstract: Agentic AI networking (AgentNet) is a novel AI-native networking paradigm in which a large number of specialized AI agents collaborate to perform autonomous decision-making, dynamic environmental adaptation, and complex missions. It has the potential to facilitate real-time network management and optimization functions, including self-configuration, self-optimization, and self-adaptation across diverse and complex environments. This paper proposes SANet, a novel semantic-aware AgentNet architecture for wireless networks that can infer the semantic goal of the user and automatically assign agents associated with different layers of the network to fulfill the inferred goal. Motivated by the fact that AgentNet is a decentralized framework in which collaborating agents may generally have different and even conflicting objectives, we formulate the decentralized optimization of SANet as a multi-agent multi-objective problem, and focus on finding the Pareto-optimal solution for agents with distinct and potentially conflicting objectives. We propose three novel metrics for evaluating SANet. Furthermore, we develop a model partition and sharing (MoPS) framework in which large models, e.g., deep learning models, of different agents can be partitioned into shared and agent-specific parts that are jointly constructed and deployed according to agents' local computational resources. Two decentralized optimization algorithms are proposed. We derive theoretical bounds and prove that there exists a three-way tradeoff among optimization, generalization, and conflicting errors. We develop an open-source RAN and core network-based hardware prototype that implements agents to interact with three different layers of the network. Experimental results show that the proposed framework achieved performance gains of up to 14.61% while requiring only 44.37% of FLOPs required by state-of-the-art algorithms.

</details>


### [28] [Tyee: A Unified, Modular, and Fully-Integrated Configurable Toolkit for Intelligent Physiological Health Care](https://arxiv.org/abs/2512.22601)
*Tao Zhou,Lingyu Shu,Zixing Zhang,Jing Han*

Main category: cs.AI

TL;DR: Tyee是一个用于智能生理医疗的统一、模块化、可配置工具包，解决了深度学习在生理信号分析中的数据格式异构、预处理不一致、模型管道碎片化和实验不可复现等问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习在生理信号分析中面临四大挑战：1) 数据格式异构，2) 预处理策略不一致，3) 模型管道碎片化，4) 实验设置不可复现。这些限制了该领域的进展。

Method: Tyee提出三个关键创新：1) 统一数据接口和可配置预处理管道，支持12种信号模态；2) 模块化可扩展架构，支持灵活集成和快速原型开发；3) 端到端工作流配置，促进可复现和可扩展的实验。

Result: Tyee在所有评估任务中表现出色，优于或匹配基线方法，在13个数据集中的12个上取得了最先进的结果，展示了实际有效性和泛化能力。

Conclusion: Tyee是一个实用且有效的工具包，解决了生理信号分析中的关键挑战，促进了该领域的研究可复现性和技术进步。工具包已在GitHub开源并持续维护。

Abstract: Deep learning has shown great promise in physiological signal analysis, yet its progress is hindered by heterogeneous data formats, inconsistent preprocessing strategies, fragmented model pipelines, and non-reproducible experimental setups. To address these limitations, we present Tyee, a unified, modular, and fully-integrated configurable toolkit designed for intelligent physiological healthcare. Tyee introduces three key innovations: (1) a unified data interface and configurable preprocessing pipeline for 12 kinds of signal modalities; (2) a modular and extensible architecture enabling flexible integration and rapid prototyping across tasks; and (3) end-to-end workflow configuration, promoting reproducible and scalable experimentation. Tyee demonstrates consistent practical effectiveness and generalizability, outperforming or matching baselines across all evaluated tasks (with state-of-the-art results on 12 of 13 datasets). The Tyee toolkit is released at https://github.com/SmileHnu/Tyee and actively maintained.

</details>


### [29] [Learning Multi-Modal Mobility Dynamics for Generalized Next Location Recommendation](https://arxiv.org/abs/2512.22605)
*Junshu Dai,Yu Wang,Tongya Zheng,Wei Ji,Qinghong Guo,Ji Cao,Jie Song,Canghong Jin,Mingli Song*

Main category: cs.AI

TL;DR: M³ob：利用多模态时空知识增强位置推荐，通过LLM增强的时空知识图谱构建统一时空关系图，解决模态间语义鸿沟问题，提升泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有人类移动预测方法泛化能力有限：单模态方法受数据稀疏性和固有偏差限制，多模态方法难以有效捕捉静态多模态表示与时空动态之间的语义鸿沟

Method: 1) 利用LLM增强的时空知识图谱构建统一时空关系图；2) 设计门控机制融合不同模态的时空图表示；3) 提出STKG引导的跨模态对齐，将时空动态知识注入静态图像模态

Result: 在六个公共数据集上的实验表明，该方法在正常场景下取得一致改进，在异常场景下展现出显著泛化能力

Conclusion: M³ob通过多模态时空知识有效表征移动动态，解决了现有方法的局限性，在位置推荐任务中表现出优越性能

Abstract: The precise prediction of human mobility has produced significant socioeconomic impacts, such as location recommendations and evacuation suggestions. However, existing methods suffer from limited generalization capability: unimodal approaches are constrained by data sparsity and inherent biases, while multi-modal methods struggle to effectively capture mobility dynamics caused by the semantic gap between static multi-modal representation and spatial-temporal dynamics. Therefore, we leverage multi-modal spatial-temporal knowledge to characterize mobility dynamics for the location recommendation task, dubbed as \textbf{M}ulti-\textbf{M}odal \textbf{Mob}ility (\textbf{M}$^3$\textbf{ob}). First, we construct a unified spatial-temporal relational graph (STRG) for multi-modal representation, by leveraging the functional semantics and spatial-temporal knowledge captured by the large language models (LLMs)-enhanced spatial-temporal knowledge graph (STKG). Second, we design a gating mechanism to fuse spatial-temporal graph representations of different modalities, and propose an STKG-guided cross-modal alignment to inject spatial-temporal dynamic knowledge into the static image modality. Extensive experiments on six public datasets show that our proposed method not only achieves consistent improvements in normal scenarios but also exhibits significant generalization ability in abnormal scenarios.

</details>


### [30] [LLM Agents as VC investors: Predicting Startup Success via RolePlay-Based Collective Simulation](https://arxiv.org/abs/2512.22608)
*Zhongyang Liu,Haoyu Pei,Xiangyi Xiao,Xiaocong Du,Yihui Li,Suting Hong,Kunpeng Zhang,Haipeng Zhang*

Main category: cs.AI

TL;DR: SimVC-CAS：一个模拟风险投资决策的多智能体系统，通过角色扮演智能体和GNN监督交互模块，将初创企业融资预测重构为群体决策任务，显著提升预测准确性并提供可解释的多视角推理。


<details>
  <summary>Details</summary>
Motivation: 现有初创企业成功预测方法通常从单一决策者视角建模，忽视了现实世界中风险投资决策由投资者群体主导的集体动态。需要一种能够捕捉投资者网络行为动态的群体决策模型。

Method: 提出SimVC-CAS集体智能体系统，将VC决策模拟为多智能体交互过程。设计具有独特特征和偏好的角色扮演智能体，通过图结构共同投资网络实现异质性评估和真实信息交换。采用GNN监督交互模块，结合企业基本面和潜在投资者网络的行为动态。

Result: 使用PitchBook真实数据并在严格数据泄漏控制下，SimVC-CAS显著提升预测准确性，例如在平均精度@10指标上实现约25%的相对改进。同时提供可解释的多视角推理。

Conclusion: SimVC-CAS成功将初创企业融资预测重构为群体决策任务，不仅提高了预测性能，还为其他复杂群体决策场景提供了启示。系统能够捕捉投资者网络的集体动态，提供更真实的风险投资决策模拟。

Abstract: Due to the high value and high failure rate of startups, predicting their success has become a critical challenge across interdisciplinary research. Existing approaches typically model success prediction from the perspective of a single decision-maker, overlooking the collective dynamics of investor groups that dominate real-world venture capital (VC) decisions. In this paper, we propose SimVC-CAS, a novel collective agent system that simulates VC decision-making as a multi-agent interaction process. By designing role-playing agents and a GNN-based supervised interaction module, we reformulate startup financing prediction as a group decision-making task, capturing both enterprise fundamentals and the behavioral dynamics of potential investor networks. Each agent embodies an investor with unique traits and preferences, enabling heterogeneous evaluation and realistic information exchange through a graph-structured co-investment network. Using real-world data from PitchBook and under strict data leakage controls, we show that SimVC-CAS significantly improves predictive accuracy while providing interpretable, multiperspective reasoning, for example, approximately 25% relative improvement with respect to average precision@10. SimVC-CAS also sheds light on other complex group decision scenarios.

</details>


### [31] [The Wisdom of Deliberating AI Crowds: Does Deliberation Improve LLM-Based Forecasting?](https://arxiv.org/abs/2512.22625)
*Paul Schneider,Amalie Schramm*

Main category: cs.AI

TL;DR: LLM相互审阅预测可提升异构模型预测准确率4%，但对同构模型无效；额外上下文信息无助于改进预测


<details>
  <summary>Details</summary>
Motivation: 人类预测中结构化审议能提升准确性，本研究探索类似干预（让LLM在更新前相互审阅预测）是否能提升大语言模型的预测准确率

Method: 使用Metaculus Q2 2025 AI预测锦标赛的202个已解决二元问题，评估四种场景下的准确率：1) 异构模型+分布式信息；2) 异构模型+共享信息；3) 同构模型+分布式信息；4) 同构模型+共享信息

Result: 干预在场景2（异构模型+共享信息）中显著提升准确率，Log Loss降低0.020（相对提升约4%，p=0.017）；同构模型组无改善；意外发现额外上下文信息未提升预测准确率

Conclusion: 审议可能是改进LLM预测的可行策略，但效果取决于模型多样性，且信息汇集机制的作用需要进一步研究

Abstract: Structured deliberation has been found to improve the performance of human forecasters. This study investigates whether a similar intervention, i.e. allowing LLMs to review each other's forecasts before updating, can improve accuracy in large language models (GPT-5, Claude Sonnet 4.5, Gemini Pro 2.5). Using 202 resolved binary questions from the Metaculus Q2 2025 AI Forecasting Tournament, accuracy was assessed across four scenarios: (1) diverse models with distributed information, (2) diverse models with shared information, (3) homogeneous models with distributed information, and (4) homogeneous models with shared information. Results show that the intervention significantly improves accuracy in scenario (2), reducing Log Loss by 0.020 or about 4 percent in relative terms (p = 0.017). However, when homogeneous groups (three instances of the same model) engaged in the same process, no benefit was observed. Unexpectedly, providing LLMs with additional contextual information did not improve forecast accuracy, limiting our ability to study information pooling as a mechanism. Our findings suggest that deliberation may be a viable strategy for improving LLM forecasting.

</details>


### [32] [DICE: Discrete Interpretable Comparative Evaluation with Probabilistic Scoring for Retrieval-Augmented Generation](https://arxiv.org/abs/2512.22629)
*Shiyan Liu,Jian Ma,Rui Qu*

Main category: cs.AI

TL;DR: DICE是一个两阶段、证据耦合的RAG评估框架，通过深度分析推理和概率评分提高可解释性和鲁棒性，采用瑞士制锦标赛减少计算复杂度，在中文金融QA数据集上达到85.7%的人类专家一致性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG评估的标量指标存在可解释性有限、不确定性量化不足、多系统比较计算效率低等问题，阻碍了RAG技术的负责任部署，需要更可信赖的评估方法。

Method: DICE采用两阶段框架：结合深度分析推理与概率{A, B, Tie}评分，生成透明、置信度感知的判断；使用瑞士制锦标赛将计算复杂度从O(N²)降至O(N log N)，提高大规模评估效率。

Result: 在中文金融QA数据集上，DICE与人类专家的一致性达到85.7%，显著优于RAGAS等现有LLM评估指标；在8系统评估中计算复杂度降低42.9%，同时保持排名保真度。

Conclusion: DICE为RAG系统评估提供了一个负责任、可解释且高效的新范式，通过可解释的推理痕迹支持系统错误诊断和可操作的改进见解，促进可信赖RAG技术的部署。

Abstract: As Retrieval-Augmented Generation (RAG) systems evolve toward more sophisticated architectures, ensuring their trustworthiness through explainable and robust evaluation becomes critical. Existing scalar metrics suffer from limited interpretability, inadequate uncertainty quantification, and computational inefficiency in multi-system comparisons, hindering responsible deployment of RAG technologies. We introduce DICE (Discrete Interpretable Comparative Evaluation), a two-stage, evidence-coupled framework that advances explainability and robustness in RAG evaluation. DICE combines deep analytical reasoning with probabilistic $\{A, B, Tie\}$ scoring to produce transparent, confidence-aware judgments that support accountable system improvement through interpretable reasoning traces, enabling systematic error diagnosis and actionable insights. To address efficiency challenges at scale, DICE employs a Swiss-system tournament that reduces computational complexity from $O(N^2)$ to $O(N \log N)$, achieving a 42.9% reduction in our eight-system evaluation while preserving ranking fidelity. Validation on a curated Chinese financial QA dataset demonstrates that DICE achieves 85.7% agreement with human experts, substantially outperforming existing LLM-based metrics such as RAGAS. Our results establish DICE as a responsible, explainable, and efficient paradigm for trustworthy RAG system assessment.

</details>


### [33] [TravelBench: A Real-World Benchmark for Multi-Turn and Tool-Augmented Travel Planning](https://arxiv.org/abs/2512.22673)
*Xiang Cheng,Yulan Hu,Xiangwen Zhang,Lu Xu,Zheng Pan,Xin Li,Yong Liu*

Main category: cs.AI

TL;DR: TravelBench：一个真实世界旅行规划基准，支持多轮交互和工具使用，用于全面评估LLM智能体能力


<details>
  <summary>Details</summary>
Motivation: 现有旅行规划任务在领域覆盖和多轮交互方面有限，无法支持动态用户-智能体交互，因此无法全面评估智能体能力。需要更实用的基准来推进LLM智能体在旅行规划中的应用。

Method: 1. 从真实场景收集用户请求构建三个子集（多轮、单轮、不可解）；2. 构建包含10个旅行领域工具的受控沙盒环境，提供确定性工具输出；3. 在TravelBench上评估多个LLM并分析其行为和性能。

Result: 开发了TravelBench基准，包含多轮交互和工具使用功能，提供了稳定可复现的评估环境，能够全面评估LLM智能体在旅行规划中的表现。

Conclusion: TravelBench为推进LLM智能体在旅行规划领域的发展提供了一个实用且可复现的基准，能够更全面地评估智能体能力。

Abstract: Large language model (LLM) agents have demonstrated strong capabilities in planning and tool use. Travel planning provides a natural and high-impact testbed for these capabilities, as it requires multi-step reasoning, iterative preference elicitation through interaction, and calls to external tools under evolving constraints. Prior work has studied LLMs on travel-planning tasks, but existing settings are limited in domain coverage and multi-turn interaction. As a result, they cannot support dynamic user-agent interaction and therefore fail to comprehensively assess agent capabilities. In this paper, we introduce TravelBench, a real-world travel-planning benchmark featuring multi-turn interaction and tool use. We collect user requests from real-world scenarios and construct three subsets-multi-turn, single-turn, and unsolvable-to evaluate different aspects of agent performance. For stable and reproducible evaluation, we build a controlled sandbox environment with 10 travel-domain tools, providing deterministic tool outputs for reliable reasoning. We evaluate multiple LLMs on TravelBench and conduct an analysis of their behaviors and performance. TravelBench offers a practical and reproducible benchmark for advancing LLM agents in travel planning.

</details>


### [34] [Memento-II: Learning by Stateful Reflective Memory](https://arxiv.org/abs/2512.22716)
*Jun Wang*

Main category: cs.AI

TL;DR: 提出一个理论框架，将情景记忆与强化学习结合，使大语言模型智能体能够通过反思机制进行持续体验式学习，无需反向传播或模型微调。


<details>
  <summary>Details</summary>
Motivation: 传统方法在训练和部署之间存在严格分离，智能体部署后无法持续学习。需要一种机制让语言模型智能体能够通过与环境交互不断适应，而不需要参数更新。

Method: 提出状态化反思决策过程，将反思学习建模为与情景记忆的两阶段读写交互：写入存储交互结果（策略评估），读取检索相关过去案例（策略改进）。该过程在增强的状态记忆表示上诱导出等效的马尔可夫决策过程。

Result: 框架通过熵正则化策略迭代实例化，并建立了收敛保证。随着情景记忆增长并充分覆盖状态空间，所得策略收敛到最优解。

Conclusion: 该工作为基于记忆增强和检索的语言模型智能体提供了理论基础，使其能够在不更新参数的情况下实现持续适应，打破了训练与部署的传统分离。

Abstract: We propose a theoretical framework for continual and experiential learning in large language model agents that integrates episodic memory with reinforcement learning. The framework identifies reflection as the key mechanism that enables agents to adapt through interaction without back propagation or model fine tuning, thereby relaxing the conventional separation between training and deployment.To formalise this process, we introduce the Stateful Reflective Decision Process, which models reflective learning as a two stage read write interaction with episodic memory. Writing stores interaction outcomes and corresponds to policy evaluation, while reading retrieves relevant past cases and corresponds to policy improvement. We show that this process induces an equivalent Markov decision process over augmented state memory representations, allowing the use of classical tools from dynamic programming and reinforcement learning. We further instantiate the framework using entropy regularised policy iteration and establish convergence guarantees. As episodic memory grows and achieves sufficient coverage of the state space, the resulting policy converges to the optimal solution. This work provides a principled foundation for memory augmented and retrieval based language model agents capable of continual adaptation without parameter updates.

</details>


### [35] [SAMP-HDRL: Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning](https://arxiv.org/abs/2512.22895)
*Xiaotian Ren,Nuerxiati Abudurexiti,Zhengyong Jiang,Angelos Stefanidis,Hongbin Liu,Jionglong Su*

Main category: cs.AI

TL;DR: 提出SAMP-HDRL框架，通过分层深度强化学习解决非平稳市场中的投资组合优化问题，结合动态资产分组、全局-局部协调和基于效用的资本配置机制。


<details>
  <summary>Details</summary>
Motivation: 非平稳市场中的投资组合优化面临三大挑战：市场机制转变、动态相关性以及深度强化学习策略的可解释性有限。现有方法难以同时处理这些复杂问题。

Method: SAMP-HDRL框架：1) 动态资产分组将市场划分为高质量和普通子集；2) 上层代理提取全局市场信号；3) 下层代理在掩码约束下进行组内资产配置；4) 基于效用的资本配置机制整合风险资产和无风险资产，确保全局与局部决策的协调。

Result: 在2019-2021年三种市场机制下的回测显示，SAMP-HDRL在波动和震荡条件下持续优于9个传统基线和9个DRL基准。相比最强基线，至少获得5%更高的回报率、夏普比率、索提诺比率和2%更高的欧米茄比率，在动荡市场中收益更大。

Conclusion: SAMP-HDRL通过将结构性市场约束直接嵌入DRL流程，在复杂金融环境中提供了改进的适应性、鲁棒性和可解释性。消融研究证实上层-下层协调、动态聚类和资本配置对鲁棒性不可或缺，SHAP分析揭示了跨代理的"分散+集中"互补机制。

Abstract: Portfolio optimization in non-stationary markets is challenging due to regime shifts, dynamic correlations, and the limited interpretability of deep reinforcement learning (DRL) policies. We propose a Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning (SAMP-HDRL). The framework first applies dynamic asset grouping to partition the market into high-quality and ordinary subsets. An upper-level agent extracts global market signals, while lower-level agents perform intra-group allocation under mask constraints. A utility-based capital allocation mechanism integrates risky and risk-free assets, ensuring coherent coordination between global and local decisions. backtests across three market regimes (2019--2021) demonstrate that SAMP-HDRL consistently outperforms nine traditional baselines and nine DRL benchmarks under volatile and oscillating conditions. Compared with the strongest baseline, our method achieves at least 5\% higher Return, 5\% higher Sharpe ratio, 5\% higher Sortino ratio, and 2\% higher Omega ratio, with substantially larger gains observed in turbulent markets. Ablation studies confirm that upper--lower coordination, dynamic clustering, and capital allocation are indispensable to robustness. SHAP-based interpretability further reveals a complementary ``diversified + concentrated'' mechanism across agents, providing transparent insights into decision-making. Overall, SAMP-HDRL embeds structural market constraints directly into the DRL pipeline, offering improved adaptability, robustness, and interpretability in complex financial environments.

</details>


### [36] [HiSciBench: A Hierarchical Multi-disciplinary Benchmark for Scientific Intelligence from Reading to Discovery](https://arxiv.org/abs/2512.22899)
*Yaping Zhang,Qixuan Zhang,Xingquan Zhang,Zhiyuan Chen,Wenwen Zhuang,Yupu Liang,Lu Xiang,Yang Zhao,Jiajun Zhang,Yu Zhou,Chengqing Zong*

Main category: cs.AI

TL;DR: HiSciBench是一个分层科学智能基准测试，包含5个层级、8,735个实例，覆盖6大学科，用于评估大模型在完整科研工作流中的能力。


<details>
  <summary>Details</summary>
Motivation: 现有科学智能基准测试过于碎片化，专注于狭窄任务，无法反映真实科学探究的层次性和多学科性，需要更全面的评估框架。

Method: 设计了一个包含5个层级的层次化基准：科学素养(L1)、文献解析(L2)、基于文献的问答(L3)、文献综述生成(L4)和科学发现(L5)，涵盖6大学科，支持多模态输入和跨语言评估。

Result: 对GPT-5、DeepSeek-R1等多模态系统的评估显示显著性能差距：基础素养任务准确率可达69%，但发现级挑战准确率骤降至25%。

Conclusion: HiSciBench为科学智能评估设立了新标准，提供了可操作的见解，有助于开发更强大可靠的科学AI模型，并将公开发布以促进未来研究。

Abstract: The rapid advancement of large language models (LLMs) and multimodal foundation models has sparked growing interest in their potential for scientific research. However, scientific intelligence encompasses a broad spectrum of abilities ranging from understanding fundamental knowledge to conducting creative discovery, and existing benchmarks remain fragmented. Most focus on narrow tasks and fail to reflect the hierarchical and multi-disciplinary nature of real scientific inquiry. We introduce \textbf{HiSciBench}, a hierarchical benchmark designed to evaluate foundation models across five levels that mirror the complete scientific workflow: \textit{Scientific Literacy} (L1), \textit{Literature Parsing} (L2), \textit{Literature-based Question Answering} (L3), \textit{Literature Review Generation} (L4), and \textit{Scientific Discovery} (L5). HiSciBench contains 8,735 carefully curated instances spanning six major scientific disciplines, including mathematics, physics, chemistry, biology, geography, and astronomy, and supports multimodal inputs including text, equations, figures, and tables, as well as cross-lingual evaluation. Unlike prior benchmarks that assess isolated abilities, HiSciBench provides an integrated, dependency-aware framework that enables detailed diagnosis of model capabilities across different stages of scientific reasoning. Comprehensive evaluations of leading models, including GPT-5, DeepSeek-R1, and several multimodal systems, reveal substantial performance gaps: while models achieve up to 69\% accuracy on basic literacy tasks, performance declines sharply to 25\% on discovery-level challenges. HiSciBench establishes a new standard for evaluating scientific Intelligence and offers actionable insights for developing models that are not only more capable but also more reliable. The benchmark will be publicly released to facilitate future research.

</details>


### [37] [Geometric Structural Knowledge Graph Foundation Model](https://arxiv.org/abs/2512.22931)
*Ling Xin,Mojtaba Nayyeri,Zahra Makki Nayeri,Steffen Staab*

Main category: cs.AI

TL;DR: Gamma提出了一种新的知识图谱基础模型，通过多头几何注意力机制，使用多种代数变换（实数、复数、分裂复数、对偶数）来建模不同的关系结构，并通过注意力融合机制自适应组合，在零样本归纳链接预测中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱基础模型（如Ultra）依赖单一的关系变换（如逐元素乘法），限制了表达能力，无法捕捉多样化图谱中展现的不同关系和结构模式。

Method: Gamma引入多头几何注意力机制，用多种并行代数变换（实数、复数、分裂复数、对偶数）替代单一关系变换，并通过关系条件注意力融合机制，通过轻量级门控和熵正则化在链接级别自适应融合这些变换。

Result: 在56个多样化知识图谱上的实验表明，Gamma在零样本归纳链接预测中持续优于Ultra，在归纳基准测试中平均倒数排名提升5.5%，在所有基准测试中提升4.4%，显示了互补几何表示的优势。

Conclusion: Gamma通过多头几何注意力机制，结合多种代数变换和自适应融合策略，显著提升了知识图谱基础模型的表达能力和泛化性能，为处理多样化图谱结构提供了更强大的解决方案。

Abstract: Structural knowledge graph foundation models aim to generalize reasoning to completely new graphs with unseen entities and relations. A key limitation of existing approaches like Ultra is their reliance on a single relational transformation (e.g., element-wise multiplication) in message passing, which can constrain expressiveness and fail to capture diverse relational and structural patterns exhibited on diverse graphs. In this paper, we propose Gamma, a novel foundation model that introduces multi-head geometric attention to knowledge graph reasoning. Gamma replaces the single relational transformation with multiple parallel ones, including real, complex, split-complex, and dual number based transformations, each designed to model different relational structures. A relational conditioned attention fusion mechanism then adaptively fuses them at link level via a lightweight gating with entropy regularization, allowing the model to robustly emphasize the most appropriate relational bias for each triple pattern. We present a full formalization of these algebraic message functions and discuss how their combination increases expressiveness beyond any single space. Comprehensive experiments on 56 diverse knowledge graphs demonstrate that Gamma consistently outperforms Ultra in zero-shot inductive link prediction, with a 5.5% improvement in mean reciprocal rank on the inductive benchmarks and a 4.4% improvement across all benchmarks, highlighting benefits from complementary geometric representations.

</details>


### [38] [Multimodal Fact-Checking: An Agent-based Approach](https://arxiv.org/abs/2512.22933)
*Danni Xu,Shaojing Fan,Xuanang Cheng,Mohan Kankanhalli*

Main category: cs.AI

TL;DR: 本文提出了RW-Post数据集和AgentFact框架，用于解决多模态虚假信息检测中推理不足和证据利用浅层的问题，通过高质量数据集和基于智能体的验证工作流显著提升了检测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 多模态虚假信息快速传播，现有方法（包括大型视觉语言模型和深度多模态融合方法）由于推理能力有限和证据利用浅层而表现不足。关键瓶颈在于缺乏包含完整真实世界多模态虚假信息实例、标注推理过程和可验证证据的专用数据集。

Method: 1) 提出RW-Post数据集：将真实世界多模态声明与其原始社交媒体帖子对齐，保留丰富的上下文信息，并通过大语言模型辅助提取流程从人工撰写的核查文章中获取详细推理和明确链接的证据。2) 提出AgentFact框架：基于智能体的多模态事实核查框架，模拟人类验证工作流，包含五个专门智能体（策略规划、高质量证据检索、视觉分析、推理、解释生成），通过迭代工作流在证据搜索与任务感知的证据过滤和推理之间交替进行。

Result: 广泛的实验结果表明，RW-Post数据集和AgentFact框架的协同作用显著提高了多模态事实核查的准确性和可解释性。

Conclusion: 本文通过构建高质量可解释数据集和基于智能体的验证框架，有效解决了多模态虚假信息检测中的关键挑战，为自动化事实核查系统提供了更可靠和可解释的解决方案。

Abstract: The rapid spread of multimodal misinformation poses a growing challenge for automated fact-checking systems. Existing approaches, including large vision language models (LVLMs) and deep multimodal fusion methods, often fall short due to limited reasoning and shallow evidence utilization. A key bottleneck is the lack of dedicated datasets that provide complete real-world multimodal misinformation instances accompanied by annotated reasoning processes and verifiable evidence. To address this limitation, we introduce RW-Post, a high-quality and explainable dataset for real-world multimodal fact-checking. RW-Post aligns real-world multimodal claims with their original social media posts, preserving the rich contextual information in which the claims are made. In addition, the dataset includes detailed reasoning and explicitly linked evidence, which are derived from human written fact-checking articles via a large language model assisted extraction pipeline, enabling comprehensive verification and explanation. Building upon RW-Post, we propose AgentFact, an agent-based multimodal fact-checking framework designed to emulate the human verification workflow. AgentFact consists of five specialized agents that collaboratively handle key fact-checking subtasks, including strategy planning, high-quality evidence retrieval, visual analysis, reasoning, and explanation generation. These agents are orchestrated through an iterative workflow that alternates between evidence searching and task-aware evidence filtering and reasoning, facilitating strategic decision-making and systematic evidence analysis. Extensive experimental results demonstrate that the synergy between RW-Post and AgentFact substantially improves both the accuracy and interpretability of multimodal fact-checking.

</details>


### [39] [Problems With Large Language Models for Learner Modelling: Why LLMs Alone Fall Short for Responsible Tutoring in K--12 Education](https://arxiv.org/abs/2512.23036)
*Danial Hooshyar,Yeongwook Yang,Gustav Šíř,Tommi Kärkkäinen,Raija Hämäläinen,Mutlu Cukurova,Roger Azevedo*

Main category: cs.AI

TL;DR: LLM导师在K-12教育中无法替代传统学习者建模，知识追踪模型在准确性、可靠性和时间一致性方面显著优于LLM


<details>
  <summary>Details</summary>
Motivation: 针对LLM导师可能替代传统学习者建模的误解，特别是在欧盟AI法案将K-12教育列为高风险领域的背景下，需要负责任地评估LLM在自适应教学中的局限性

Method: 比较深度知识追踪(DKT)模型与广泛使用的LLM（零样本和微调），使用大型开放数据集评估学习者知识演变的准确性、可靠性和时间一致性

Result: DKT在下一步正确性预测中表现最佳(AUC=0.83)，始终优于LLM；微调使LLM的AUC提高约8%，但仍比DKT低6%，且早期序列错误更高；DKT保持稳定、方向正确的掌握度更新，而LLM变体存在显著时间弱点

Conclusion: LLM单独使用难以匹敌现有智能辅导系统的效果，负责任的辅导需要结合学习者建模的混合框架

Abstract: The rapid rise of large language model (LLM)-based tutors in K--12 education has fostered a misconception that generative models can replace traditional learner modelling for adaptive instruction. This is especially problematic in K--12 settings, which the EU AI Act classifies as high-risk domain requiring responsible design. Motivated by these concerns, this study synthesises evidence on limitations of LLM-based tutors and empirically investigates one critical issue: the accuracy, reliability, and temporal coherence of assessing learners' evolving knowledge over time. We compare a deep knowledge tracing (DKT) model with a widely used LLM, evaluated zero-shot and fine-tuned, using a large open-access dataset. Results show that DKT achieves the highest discrimination performance (AUC = 0.83) on next-step correctness prediction and consistently outperforms the LLM across settings. Although fine-tuning improves the LLM's AUC by approximately 8\% over the zero-shot baseline, it remains 6\% below DKT and produces higher early-sequence errors, where incorrect predictions are most harmful for adaptive support. Temporal analyses further reveal that DKT maintains stable, directionally correct mastery updates, whereas LLM variants exhibit substantial temporal weaknesses, including inconsistent and wrong-direction updates. These limitations persist despite the fine-tuned LLM requiring nearly 198 hours of high-compute training, far exceeding the computational demands of DKT. Our qualitative analysis of multi-skill mastery estimation further shows that, even after fine-tuning, the LLM produced inconsistent mastery trajectories, while DKT maintained smooth and coherent updates. Overall, the findings suggest that LLMs alone are unlikely to match the effectiveness of established intelligent tutoring systems, and that responsible tutoring requires hybrid frameworks that incorporate learner modelling.

</details>


### [40] [The Reward Model Selection Crisis in Personalized Alignment](https://arxiv.org/abs/2512.23067)
*Fady Rezk,Yuangang Pan,Chuan-Sheng Foo,Xun Xu,Nancy Chen,Henry Gouk,Timothy Hospedales*

Main category: cs.AI

TL;DR: 研究发现传统奖励模型准确率无法有效指导个性化对齐部署，提出策略准确率新指标，并揭示奖励指导方法与实际生成行为脱节，上下文学习优于奖励指导方法


<details>
  <summary>Details</summary>
Motivation: 个性化对齐研究过度关注奖励模型准确率提升，但实际部署中需要推理时适应而非策略微调，现有指标无法预测部署性能

Method: 引入策略准确率评估奖励指导解码效果，创建Pref-LaMP基准数据集（含真实用户完成），在三个数据集上系统评估奖励模型准确率与策略准确率相关性

Result: 奖励模型准确率与策略准确率相关性弱（Kendall's tau = 0.08-0.31）；20分准确率差异的方法产生几乎相同的输出质量；上下文学习在>3B参数模型上优于所有奖励指导方法

Conclusion: 当前领域优化的代理指标无法预测部署性能，奖励指导方法无法将偏好转化为实际行为适应，需要重新思考个性化对齐评估方法

Abstract: Personalized alignment from preference data has focused primarily on improving reward model (RM) accuracy, with the implicit assumption that better preference ranking translates to better personalized behavior. However, in deployment, computational constraints necessitate inference-time adaptation via reward-guided decoding (RGD) rather than per-user policy fine-tuning. This creates a critical but overlooked requirement: reward models must not only rank preferences accurately but also effectively guide token-level generation decisions. We demonstrate that standard RM accuracy fails catastrophically as a selection criterion for deployment-ready personalized alignment. Through systematic evaluation across three datasets, we introduce policy accuracy, a metric quantifying whether RGD scoring functions correctly discriminate between preferred and dispreferred responses. We show that RM accuracy correlates only weakly with this policy-level discrimination ability (Kendall's tau = 0.08--0.31). More critically, we introduce Pref-LaMP, the first personalized alignment benchmark with ground-truth user completions, enabling direct behavioral evaluation without circular reward-based metrics. On Pref-LaMP, we expose a complete decoupling between discrimination and generation: methods with 20-point RM accuracy differences produce almost identical output quality, and even methods achieving high discrimination fail to generate behaviorally aligned responses. Finally, simple in-context learning (ICL) dominates all reward-guided methods for models > 3B parameters, achieving 3-5 point ROUGE-1 gains over the best reward method at 7B scale. These findings show that the field optimizes proxy metrics that fail to predict deployment performance and do not translate preferences into real behavioral adaptation under deployment constraints.

</details>


### [41] [Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients](https://arxiv.org/abs/2512.23090)
*Armin Berger,Manuela Bergau,Helen Schneider,Saad Ahmad,Tom Anglim Lagones,Gianluca Brugnara,Martha Foltyn-Dumitru,Kai Schlamp,Philipp Vollmuth,Rafet Sifa*

Main category: cs.AI

TL;DR: 医疗影像VLM的RL训练存在泛化悖论：GRPO能提升分布内性能但损害跨数据集迁移能力，SFT反而能捕捉更通用的机构无关特征


<details>
  <summary>Details</summary>
Motivation: 尽管RL在LLM推理任务上取得进展，但其在资源受限的医疗影像应用仍未被充分探索。研究者希望了解在有限资源下，RL方法对医疗视觉语言模型的影响

Method: 提出ChexReason模型，采用R1风格方法（SFT后接GRPO），仅使用2000个SFT样本、1000个RL样本和单个A100 GPU进行训练。在CheXpert和NIH基准上进行评估

Result: 发现泛化悖论：GRPO在分布内性能上提升23%（CheXpert macro-F1=0.346），但跨数据集迁移能力下降19%（NIH）。SFT检查点在优化前能独特地改善NIH性能，表明教师引导的推理能捕捉更多机构无关特征

Conclusion: 对于需要跨不同人群鲁棒性的临床部署，精心策划的监督微调可能优于激进的强化学习。结构化推理支架对通用VLM有益，但对医学预训练模型增益有限

Abstract: Recent Reinforcement Learning (RL) advances for Large Language Models (LLMs) have improved reasoning tasks, yet their resource-constrained application to medical imaging remains underexplored. We introduce ChexReason, a vision-language model trained via R1-style methodology (SFT followed by GRPO) using only 2,000 SFT samples, 1,000 RL samples, and a single A100 GPU. Evaluations on CheXpert and NIH benchmarks reveal a fundamental tension: GRPO recovers in-distribution performance (23% improvement on CheXpert, macro-F1 = 0.346) but degrades cross-dataset transferability (19% drop on NIH). This mirrors high-resource models like NV-Reason-CXR-3B, suggesting the issue stems from the RL paradigm rather than scale. We identify a generalization paradox where the SFT checkpoint uniquely improves on NIH before optimization, indicating teacher-guided reasoning captures more institution-agnostic features. Furthermore, cross-model comparisons show structured reasoning scaffolds benefit general-purpose VLMs but offer minimal gain for medically pre-trained models. Consequently, curated supervised fine-tuning may outperform aggressive RL for clinical deployment requiring robustness across diverse populations.

</details>


### [42] [InSPO: Unlocking Intrinsic Self-Reflection for LLM Preference Optimization](https://arxiv.org/abs/2512.23126)
*Yu Li,Tian Lan,Zhengling Qi*

Main category: cs.AI

TL;DR: 提出Intrinsic Self-reflective Preference Optimization (q)方法，解决DPO的两个根本缺陷：最优策略依赖任意建模选择，以及孤立处理响应生成未能利用成对数据中的比较信息。


<details>
  <summary>Details</summary>
Motivation: DPO及其变体已成为对齐大语言模型的标准方法，但存在两个根本限制：1) 最优策略依赖任意建模选择（标量化函数、参考策略），导致行为反映参数化伪影而非真实偏好；2) 孤立处理响应生成未能利用成对数据中的比较信息，未挖掘模型内在自反思能力。

Method: 提出Intrinsic Self-reflective Preference Optimization (q)，推导出全局最优策略，该策略同时基于上下文和替代响应进行条件化。该方法保证对标量化和参考选择具有不变性，可作为即插即用增强，无需架构更改或推理开销。

Result: 实验证明在胜率和长度控制指标上取得一致改进，验证了解锁自反思能力能产生更鲁棒、更符合人类对齐的LLMs。

Conclusion: q方法优于DPO/RLHF，解决了DPO的两个根本缺陷，通过利用内在自反思能力实现更鲁棒、更符合人类对齐的语言模型。

Abstract: Direct Preference Optimization (DPO) and its variants have become standard for aligning Large Language Models due to their simplicity and offline stability. However, we identify two fundamental limitations. First, the optimal policy depends on arbitrary modeling choices (scalarization function, reference policy), yielding behavior reflecting parameterization artifacts rather than true preferences. Second, treating response generation in isolation fails to leverage comparative information in pairwise data, leaving the model's capacity for intrinsic self-reflection untapped. To address it, we propose Intrinsic Self-reflective Preference Optimization (\q), deriving a globally optimal policy conditioning on both context and alternative responses. We prove this formulation superior to DPO/RLHF while guaranteeing invariance to scalarization and reference choices. \q~serves as a plug-and-play enhancement without architectural changes or inference overhead. Experiments demonstrate consistent improvements in win rates and length-controlled metrics, validating that unlocking self-reflection yields more robust, human-aligned LLMs.

</details>


### [43] [Why We Need a New Framework for Emotional Intelligence in AI](https://arxiv.org/abs/2512.23163)
*Max Parks,Kheli Atluru,Meera Vinod,Mike Kuniavsky,Jud Brewer,Sean White,Sarah Adler,Wendy Ju*

Main category: cs.AI

TL;DR: 本文认为当前AI情感智能评估框架需要改进，因为它们未能全面衡量AI相关的情感智能方面，同时指出人类情感智能的某些方面（如现象学体验）对AI评估不相关，而其他方面（如感知、解释、适应情感状态）则相关且可评估。


<details>
  <summary>Details</summary>
Motivation: 当前评估AI情感智能的框架存在缺陷：1）未能全面衡量AI相关的情感智能方面；2）缺乏对情感本质和情感智能的坚实理论基础；3）人类情感智能的某些方面（如现象学体验）对AI评估不相关，但现有框架未区分相关与不相关方面。

Method: 1）回顾不同情感理论和一般情感智能理论，评估其在人工系统中的适用性；2）批判性评估现有基准框架，基于第一部分建立的情感智能理论识别其不足；3）提出改进评估策略的方案以避免这些缺陷。

Result: 通过分析发现：1）人类情感智能的现象学成分和理解感是AI缺乏的，这些方面对AI评估不相关；2）情感智能的感知、解释、适当响应和适应能力等方面在AI中可不同程度实现；3）现有基准框架缺乏对情感本质的坚实理论基础，未能全面评估AI相关的情感智能方面。

Conclusion: 需要改进AI情感智能评估框架，建立更全面的评估策略，区分人类情感智能中与AI相关和不相关的方面，并基于坚实的情感理论来评估AI在感知、解释、响应和适应情感状态方面的能力。

Abstract: In this paper, we develop the position that current frameworks for evaluating emotional intelligence (EI) in artificial intelligence (AI) systems need refinement because they do not adequately or comprehensively measure the various aspects of EI relevant in AI. Human EI often involves a phenomenological component and a sense of understanding that artificially intelligent systems lack; therefore, some aspects of EI are irrelevant in evaluating AI systems. However, EI also includes an ability to sense an emotional state, explain it, respond appropriately, and adapt to new contexts (e.g., multicultural), and artificially intelligent systems can do such things to greater or lesser degrees. Several benchmark frameworks specialize in evaluating the capacity of different AI models to perform some tasks related to EI, but these often lack a solid foundation regarding the nature of emotion and what it is to be emotionally intelligent. In this project, we begin by reviewing different theories about emotion and general EI, evaluating the extent to which each is applicable to artificial systems. We then critically evaluate the available benchmark frameworks, identifying where each falls short in light of the account of EI developed in the first section. Lastly, we outline some options for improving evaluation strategies to avoid these shortcomings in EI evaluation in AI systems.

</details>


### [44] [SPIRAL: Symbolic LLM Planning via Grounded and Reflective Search](https://arxiv.org/abs/2512.23167)
*Yifan Zhang,Giridhar Ganapavarapu,Srideepika Jayaraman,Bhavna Agrawal,Dhaval Patel,Achille Fokoue*

Main category: cs.AI

TL;DR: SPIRAL框架通过将三个专门的LLM代理嵌入MCTS循环，实现引导式、自校正的符号规划，显著提升复杂任务规划性能


<details>
  <summary>Details</summary>
Motivation: LLM在需要探索和自校正的复杂规划任务中表现不佳，线性推理难以从早期错误中恢复。传统搜索算法如MCTS在稀疏奖励下效果有限，且未能充分利用LLM的语义能力

Method: 提出SPIRAL框架，在MCTS循环中嵌入三个专门LLM代理：规划器提出创造性下一步，模拟器通过预测现实结果使搜索接地，评论家通过反思提供密集奖励信号

Result: 在DailyLifeAPIs和HuggingFace数据集上，SPIRAL持续优于默认的思维链规划方法和其他SOTA代理。在DailyLifeAPIs上达到83.6%总体准确率，比次优搜索框架提升超过16个百分点，同时展现更优的token效率

Conclusion: 将LLM推理构建为引导式、反思性和接地性的搜索过程，能够产生更鲁棒和高效的自主规划器。SPIRAL将MCTS从暴力搜索转变为引导式自校正推理过程

Abstract: Large Language Models (LLMs) often falter at complex planning tasks that require exploration and self-correction, as their linear reasoning process struggles to recover from early mistakes. While search algorithms like Monte Carlo Tree Search (MCTS) can explore alternatives, they are often ineffective when guided by sparse rewards and fail to leverage the rich semantic capabilities of LLMs. We introduce SPIRAL (Symbolic LLM Planning via Grounded and Reflective Search), a novel framework that embeds a cognitive architecture of three specialized LLM agents into an MCTS loop. SPIRAL's key contribution is its integrated planning pipeline where a Planner proposes creative next steps, a Simulator grounds the search by predicting realistic outcomes, and a Critic provides dense reward signals through reflection. This synergy transforms MCTS from a brute-force search into a guided, self-correcting reasoning process. On the DailyLifeAPIs and HuggingFace datasets, SPIRAL consistently outperforms the default Chain-of-Thought planning method and other state-of-the-art agents. More importantly, it substantially surpasses other state-of-the-art agents; for example, SPIRAL achieves 83.6% overall accuracy on DailyLifeAPIs, an improvement of over 16 percentage points against the next-best search framework, while also demonstrating superior token efficiency. Our work demonstrates that structuring LLM reasoning as a guided, reflective, and grounded search process yields more robust and efficient autonomous planners. The source code, full appendices, and all experimental data are available for reproducibility at the official project repository.

</details>


### [45] [TCEval: Using Thermal Comfort to Assess Cognitive and Perceptual Abilities of AI](https://arxiv.org/abs/2512.23217)
*Jingming Li*

Main category: cs.AI

TL;DR: TCEval：首个基于热舒适场景的AI认知能力评估框架，测试跨模态推理、因果关联和自适应决策三大核心能力，发现当前LLM具备基础跨模态推理但缺乏对热舒适变量非线性关系的精确因果理解。


<details>
  <summary>Details</summary>
Motivation: 现有LLM任务特定基准存在关键空白。热舒适作为涉及感官整合和自适应决策的环境因素与个人感知的复杂交互，是评估AI系统真实世界认知能力的理想范式。

Method: 提出TCEval评估框架，通过热舒适场景和LLM代理评估AI三大认知能力：跨模态推理、因果关联和自适应决策。方法包括：初始化具有虚拟个性属性的LLM代理，引导其生成服装隔热选择和热舒适反馈，并将输出与ASHRAE全球数据库和中国热舒适数据库进行验证。

Result: 对四个LLM的实验表明：代理反馈与人类精确对齐有限，但在1 PMV容差下方向一致性显著改善；统计测试显示LLM生成的PMV分布与人类数据明显不同，代理在离散热舒适分类中表现接近随机。

Conclusion: TCEval作为生态有效的AI认知图灵测试是可行的，表明当前LLM具备基础跨模态推理能力，但缺乏对热舒适变量非线性关系的精确因果理解。TCEval补充传统基准，将AI评估重点从抽象任务熟练度转向具身、上下文感知的感知和决策，为智能建筑等以人为本的AI应用提供宝贵见解。

Abstract: A critical gap exists in LLM task-specific benchmarks. Thermal comfort, a sophisticated interplay of environmental factors and personal perceptions involving sensory integration and adaptive decision-making, serves as an ideal paradigm for evaluating real-world cognitive capabilities of AI systems. To address this, we propose TCEval, the first evaluation framework that assesses three core cognitive capacities of AI, cross-modal reasoning, causal association, and adaptive decision-making, by leveraging thermal comfort scenarios and large language model (LLM) agents. The methodology involves initializing LLM agents with virtual personality attributes, guiding them to generate clothing insulation selections and thermal comfort feedback, and validating outputs against the ASHRAE Global Database and Chinese Thermal Comfort Database. Experiments on four LLMs show that while agent feedback has limited exact alignment with humans, directional consistency improves significantly with a 1 PMV tolerance. Statistical tests reveal that LLM-generated PMV distributions diverge markedly from human data, and agents perform near-randomly in discrete thermal comfort classification. These results confirm the feasibility of TCEval as an ecologically valid Cognitive Turing Test for AI, demonstrating that current LLMs possess foundational cross-modal reasoning ability but lack precise causal understanding of the nonlinear relationships between variables in thermal comfort. TCEval complements traditional benchmarks, shifting AI evaluation focus from abstract task proficiency to embodied, context-aware perception and decision-making, offering valuable insights for advancing AI in human-centric applications like smart buildings.

</details>


### [46] [Agentic Physical AI toward a Domain-Specific Foundation Model for Nuclear Reactor Control](https://arxiv.org/abs/2512.23292)
*Yoonpyo Lee,Kazuma Kobayashi,Sai Puppala,Sajedul Talukder,Seid Koric,Souvik Chakraborty,Syed Bahauddin Alam*

Main category: cs.AI

TL;DR: 该论文提出了一种新的物理AI范式，通过基于物理验证的策略优化而非感知推理，训练小型语言模型作为智能体物理AI，在反应堆控制任务中实现了从模仿学习到稳定执行的相变。


<details>
  <summary>Details</summary>
Motivation: 当前通用基础模型在物理系统控制中存在根本性障碍，即使前沿视觉语言模型在基础物理任务上准确率也只有50-53%，表现为近似猜测器，保持语义合理性但违反物理约束。这种输入不忠实不是缩放缺陷而是结构限制，感知中心架构优化参数空间模仿，而安全关键控制需要执行动作的结果空间保证。

Method: 提出紧凑语言模型作为智能体物理AI的新路径，策略优化由基于物理的验证驱动而非感知推理。训练了一个3.6亿参数模型，使用合成反应堆控制场景数据集，从10^3扩展到10^5个示例。

Result: 模型表现出通用模型中不存在的尖锐相变：小规模系统呈现高方差模仿和灾难性尾部风险，而大规模模型经历超过500倍减少的方差崩溃，稳定执行级行为。尽管平衡暴露于四种执行器家族，模型自主拒绝约70%训练分布，并将95%运行时执行集中在单一银行策略上。学习表示无需架构修改即可跨不同物理和连续输入模态迁移。

Conclusion: 该研究展示了通过基于物理验证的策略优化而非感知推理，构建领域特定基础模型的新途径，实现了从模仿学习到稳定执行的相变，为安全关键物理系统控制提供了新范式。

Abstract: The prevailing paradigm in AI for physical systems, scaling general-purpose foundation models toward universal multimodal reasoning, confronts a fundamental barrier at the control interface. Recent benchmarks show that even frontier vision-language models achieve only 50-53% accuracy on basic quantitative physics tasks, behaving as approximate guessers that preserve semantic plausibility while violating physical constraints. This input unfaithfulness is not a scaling deficiency but a structural limitation. Perception-centric architectures optimize parameter-space imitation, whereas safety-critical control demands outcome-space guarantees over executed actions. Here, we present a fundamentally different pathway toward domain-specific foundation models by introducing compact language models operating as Agentic Physical AI, in which policy optimization is driven by physics-based validation rather than perceptual inference. We train a 360-million-parameter model on synthetic reactor control scenarios, scaling the dataset from 10^3 to 10^5 examples. This induces a sharp phase transition absent in general-purpose models. Small-scale systems exhibit high-variance imitation with catastrophic tail risk, while large-scale models undergo variance collapse exceeding 500x reduction, stabilizing execution-level behavior. Despite balanced exposure to four actuation families, the model autonomously rejects approximately 70% of the training distribution and concentrates 95% of runtime execution on a single-bank strategy. Learned representations transfer across distinct physics and continuous input modalities without architectural modification.

</details>


### [47] [On Conformant Planning and Model-Checking of $\exists^*\forall^*$ Hyperproperties](https://arxiv.org/abs/2512.23324)
*Raven Beutner,Bernd Finkbeiner*

Main category: cs.AI

TL;DR: 本文揭示了符合性规划与超属性模型检测之间的紧密联系，两者可以相互高效转换。


<details>
  <summary>Details</summary>
Motivation: 研究规划与验证社区中的两个问题之间的联系：符合性规划和超属性模型检测。符合性规划是在计划执行过程中独立于非确定性动作效果的情况下找到实现给定目标的顺序计划。超属性是系统属性，涉及系统的多个执行轨迹，例如捕获信息流和公平性策略。

Method: 1. 展示如何将超属性模型检测实例高效地简化为符合性规划实例，并证明编码的正确性和完备性。2. 建立相反方向：每个符合性规划问题本身就是一个超属性模型检测任务。

Result: 证明了模型检测中的∃*∀*超属性与符合性规划问题之间存在紧密联系，两者可以相互转换，为两个领域提供了新的视角和工具。

Conclusion: 符合性规划和超属性模型检测是密切相关的，可以相互转换，这为两个研究领域提供了新的交叉视角和潜在的工具复用机会。

Abstract: We study the connection of two problems within the planning and verification community: Conformant planning and model-checking of hyperproperties. Conformant planning is the task of finding a sequential plan that achieves a given objective independent of non-deterministic action effects during the plan's execution. Hyperproperties are system properties that relate multiple execution traces of a system and, e.g., capture information-flow and fairness policies. In this paper, we show that model-checking of $\exists^*\forall^*$ hyperproperties is closely related to the problem of computing a conformant plan. Firstly, we show that we can efficiently reduce a hyperproperty model-checking instance to a conformant planning instance, and prove that our encoding is sound and complete. Secondly, we establish the converse direction: Every conformant planning problem is, itself, a hyperproperty model-checking task.

</details>


### [48] [CubeBench: Diagnosing Interactive, Long-Horizon Spatial Reasoning Under Partial Observations](https://arxiv.org/abs/2512.23328)
*Huan-ang Gao,Zikang Zhang,Tianwei Luo,Kaisen Yang,Xinzhe Juan,Jiahao Qiu,Tianxing Chen,Bingxiang He,Hao Zhao,Hao Zhou,Shilong Liu,Mengdi Wang*

Main category: cs.AI

TL;DR: 论文提出CubeBench基准测试，基于魔方评估LLM智能体的空间推理能力，发现现有模型在长时程任务上完全失败，揭示物理世界部署的关键瓶颈。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在数字领域表现出色，但在物理世界部署存在显著差距，主要挑战在于形成和维护稳健的空间心理模型。论文识别了三个核心认知挑战：空间推理、通过心理模拟进行长时程状态跟踪、以及部分观测下的主动探索。

Method: 引入CubeBench基准测试，围绕魔方设计三层次诊断框架：1）使用完整符号信息的基础状态跟踪；2）长时程心理模拟；3）仅使用部分视觉数据的主动探索。通过为LLM提供外部求解器工具来隔离认知瓶颈。

Result: 实验显示领先LLM在所有长时程任务上通过率均为0.00%，暴露了长期规划的根本性失败。通过分析失败模式，揭示了空间推理和状态跟踪的具体瓶颈。

Conclusion: CubeBench有效诊断了LLM在物理世界部署的核心认知限制，为开发更物理接地气的智能体提供了关键见解和指导方向。

Abstract: Large Language Model (LLM) agents, while proficient in the digital realm, face a significant gap in physical-world deployment due to the challenge of forming and maintaining a robust spatial mental model. We identify three core cognitive challenges hindering this transition: spatial reasoning, long-horizon state tracking via mental simulation, and active exploration under partial observation. To isolate and evaluate these faculties, we introduce CubeBench, a novel generative benchmark centered on the Rubik's Cube. CubeBench uses a three-tiered diagnostic framework that progressively assesses agent capabilities, from foundational state tracking with full symbolic information to active exploration with only partial visual data. Our experiments on leading LLMs reveal critical limitations, including a uniform 0.00% pass rate on all long-horizon tasks, exposing a fundamental failure in long-term planning. We also propose a diagnostic framework to isolate these cognitive bottlenecks by providing external solver tools. By analyzing the failure modes, we provide key insights to guide the development of more physically-grounded intelligent agents.

</details>


### [49] [MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning](https://arxiv.org/abs/2512.23412)
*Jiawei Chen,Xintian Shen,Lihao Zheng,Zhenwei Shao,Hongyuan Zhang,Pengfei Yu,Xudong Rao,Ning Mao,Xiaobo Liu,Lian Wen,Chaoqun Du,Feng Gu,Wei He,Qizhen Li,Shanshan Li,Zide Liu,Jing Luo,Lifu Mu,Xuhao Pan,Chang Ren,Haoyi Sun,Qian Wang,Wei Wang,Hongfu Yang,Jiqing Zhan,Chunpeng Zhou,Zheng Zhou,Hao Ma,Tao Wei,Pan Zhou,Wei Chen*

Main category: cs.AI

TL;DR: MindWatcher是一个集成交替思考和多模态思维链推理的工具集成推理智能体，能够自主决定是否以及如何调用多样化工具，无需依赖人工提示或工作流，在复杂决策任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统基于工作流的智能体在解决需要工具调用的现实问题时智能有限，而能够自主推理和工具调用的工具集成推理智能体正成为处理复杂决策任务的有力方法。

Method: 1. 集成交替思考范式，允许模型在任何中间阶段在思考和工具调用之间切换；2. 多模态思维链推理能力，能够在推理过程中操作图像以获得更精确的搜索结果；3. 构建自动化数据审计和评估流程，配合手动策划的高质量数据集；4. 建立MindWatcher-Evaluate Bench基准；5. 配备全面的辅助推理工具套件；6. 构建大规模高质量本地图像检索数据库；7. 设计更高效的训练基础设施。

Result: 实验表明MindWatcher通过优越的工具调用能力，匹配甚至超越了更大或更新的模型性能，同时揭示了智能体训练的关键见解，如智能体强化学习中的遗传继承现象。

Conclusion: MindWatcher作为一个工具集成推理智能体，能够自主协调多样化工具的使用，解决广泛领域的多模态问题，其交替思考和多模态思维链推理架构为复杂决策任务提供了有效的解决方案。

Abstract: Traditional workflow-based agents exhibit limited intelligence when addressing real-world problems requiring tool invocation. Tool-integrated reasoning (TIR) agents capable of autonomous reasoning and tool invocation are rapidly emerging as a powerful approach for complex decision-making tasks involving multi-step interactions with external environments. In this work, we introduce MindWatcher, a TIR agent integrating interleaved thinking and multimodal chain-of-thought (CoT) reasoning. MindWatcher can autonomously decide whether and how to invoke diverse tools and coordinate their use, without relying on human prompts or workflows. The interleaved thinking paradigm enables the model to switch between thinking and tool calling at any intermediate stage, while its multimodal CoT capability allows manipulation of images during reasoning to yield more precise search results. We implement automated data auditing and evaluation pipelines, complemented by manually curated high-quality datasets for training, and we construct a benchmark, called MindWatcher-Evaluate Bench (MWE-Bench), to evaluate its performance. MindWatcher is equipped with a comprehensive suite of auxiliary reasoning tools, enabling it to address broad-domain multimodal problems. A large-scale, high-quality local image retrieval database, covering eight categories including cars, animals, and plants, endows model with robust object recognition despite its small size. Finally, we design a more efficient training infrastructure for MindWatcher, enhancing training speed and hardware utilization. Experiments not only demonstrate that MindWatcher matches or exceeds the performance of larger or more recent models through superior tool invocation, but also uncover critical insights for agent training, such as the genetic inheritance phenomenon in agentic RL.

</details>


### [50] [The World Is Bigger! A Computationally-Embedded Perspective on the Big World Hypothesis](https://arxiv.org/abs/2512.23419)
*Alex Lewandowski,Adtiya A. Ramesh,Edan Meyer,Dale Schuurmans,Marlos C. Machado*

Main category: cs.AI

TL;DR: 本文提出了一种基于计算嵌入视角的持续学习问题设定，将智能体视为在通用计算机中模拟的自动机，并引入交互性作为衡量智能体持续适应能力的指标。研究发现深度非线性网络难以维持交互性，而深度线性网络随着容量增加能维持更高的交互性。


<details>
  <summary>Details</summary>
Motivation: 当前持续学习的问题设定通常通过显式约束智能体来体现"世界比智能体更大"的大世界假设，但这些约束往往是临时的、难以整合，并且可能限制智能体容量扩展的有效性。本文旨在提出一种更自然的约束方式，即通过计算嵌入视角来约束智能体。

Method: 1. 提出计算嵌入视角，将嵌入智能体表示为在通用（形式）计算机中模拟的自动机；2. 证明这种自动机等价于在可数无限状态空间上的部分可观测马尔可夫决策过程中交互的智能体；3. 提出交互性作为衡量智能体持续适应能力的客观指标；4. 开发基于模型的强化学习算法用于寻求交互性；5. 构建合成问题来评估持续学习能力。

Result: 实验结果显示：深度非线性网络难以维持交互性，而深度线性网络随着容量增加能够维持更高的交互性。这表明网络架构对持续学习能力有重要影响。

Conclusion: 通过计算嵌入视角，本文为持续学习提供了一个更自然的问题设定，其中智能体始终受到环境嵌入的约束。交互性作为衡量持续适应能力的指标是有效的，并且网络架构（线性vs非线性）对维持交互性有显著影响，这为设计更好的持续学习算法提供了重要启示。

Abstract: Continual learning is often motivated by the idea, known as the big world hypothesis, that "the world is bigger" than the agent. Recent problem formulations capture this idea by explicitly constraining an agent relative to the environment. These constraints lead to solutions in which the agent continually adapts to best use its limited capacity, rather than converging to a fixed solution. However, explicit constraints can be ad hoc, difficult to incorporate, and may limit the effectiveness of scaling up the agent's capacity. In this paper, we characterize a problem setting in which an agent, regardless of its capacity, is constrained by being embedded in the environment. In particular, we introduce a computationally-embedded perspective that represents an embedded agent as an automaton simulated within a universal (formal) computer. Such an automaton is always constrained; we prove that it is equivalent to an agent that interacts with a partially observable Markov decision process over a countably infinite state-space. We propose an objective for this setting, which we call interactivity, that measures an agent's ability to continually adapt its behaviour by learning new predictions. We then develop a model-based reinforcement learning algorithm for interactivity-seeking, and use it to construct a synthetic problem to evaluate continual learning capability. Our results show that deep nonlinear networks struggle to sustain interactivity, whereas deep linear networks sustain higher interactivity as capacity increases.

</details>


### [51] [AKG kernel Agent: A Multi-Agent Framework for Cross-Platform Kernel Synthesis](https://arxiv.org/abs/2512.23424)
*Jinye Du,Quan Yuan,Zuyao Zhang,Yanzhi Yi,Jiahui Hu,Wangyi Chen,Yiyang Zhu,Qishui Zheng,Wenxiang Zou,Xiangyu Chang,Zuohe Zheng,Zichun Ye,Chao Liu,Shanni Li,Renwei Zhang,Yiping Deng,Xinwei Hu,Xuefeng Jin,Jie Zhao*

Main category: cs.AI

TL;DR: AKG kernel agent是一个多智能体系统，利用LLM代码生成能力自动化AI计算内核的开发、迁移和性能调优，支持多种DSL和硬件后端，相比PyTorch Eager实现平均加速1.46倍。


<details>
  <summary>Details</summary>
Motivation: 现代AI模型（如LLM、多模态架构、推荐系统）对高性能计算内核需求激增，加上稀疏化、量化等技术以及硬件频繁更新和架构多样性，使得手动优化无法满足需求，成为AI系统开发的关键瓶颈。

Method: 提出AKG kernel agent多智能体系统，利用LLM代码生成能力自动化内核生成、迁移和性能调优。系统支持多种领域特定语言（Triton、TileLang、CPP、CUDA-C），可针对不同硬件后端，采用模块化设计便于快速集成新DSL和硬件目标。

Result: 在KernelBench上使用Triton DSL评估，AKG kernel agent在GPU和NPU后端上相比PyTorch Eager基准实现平均获得1.46倍加速，证明了其在加速现代AI工作负载内核开发方面的有效性。

Conclusion: AKG kernel agent通过自动化内核开发流程，解决了AI系统开发中的计算内核优化瓶颈，支持多DSL和多硬件后端，为现代AI工作负载提供了高效的内核开发解决方案。

Abstract: Modern AI models demand high-performance computation kernels. The growing complexity of LLMs, multimodal architectures, and recommendation systems, combined with techniques like sparsity and quantization, creates significant computational challenges. Moreover, frequent hardware updates and diverse chip architectures further complicate this landscape, requiring tailored kernel implementations for each platform. However, manual optimization cannot keep pace with these demands, creating a critical bottleneck in AI system development. Recent advances in LLM code generation capabilities have opened new possibilities for automating kernel development. In this work, we propose AKG kernel agent (AI-driven Kernel Generator), a multi-agent system that automates kernel generation, migration, and performance tuning. AKG kernel agent is designed to support multiple domain-specific languages (DSLs), including Triton, TileLang, CPP, and CUDA-C, enabling it to target different hardware backends while maintaining correctness and portability. The system's modular design allows rapid integration of new DSLs and hardware targets. When evaluated on KernelBench using Triton DSL across GPU and NPU backends, AKG kernel agent achieves an average speedup of 1.46$\times$ over PyTorch Eager baselines implementations, demonstrating its effectiveness in accelerating kernel development for modern AI workloads.

</details>


### [52] [Replay Failures as Successes: Sample-Efficient Reinforcement Learning for Instruction Following](https://arxiv.org/abs/2512.23457)
*Kongcheng Zhang,Qi Yao,Shunyu Liu,Wenjian Zhang,Min Cen,Yang Zhou,Wenkai Fang,Yiru Zhao,Baisheng Lai,Mingli Song*

Main category: cs.AI

TL;DR: HiR提出了一种基于后见之明重放的样本高效强化学习框架，通过选择-重写策略将失败尝试重放为成功样本，解决复杂指令跟随任务中奖励稀疏的问题。


<details>
  <summary>Details</summary>
Motivation: 传统RL方法在复杂指令跟随任务中面临挑战：初始模型能力有限，难以生成满足所有约束的高质量响应，导致奖励稀疏或难以区分，阻碍学习效率。

Method: 采用选择-重写策略：1) 从失败尝试中选择已满足部分约束的样本；2) 基于后见之明重写为成功样本；3) 在原始样本和重放样本上进行RL训练，理论框架为指令级和响应级的双重偏好学习。

Result: 实验表明HiR在不同指令跟随任务中取得显著效果，同时计算成本更低。代码和数据集已开源。

Conclusion: HiR通过后见之明重放机制有效解决了复杂指令跟随任务中的奖励稀疏问题，实现了样本高效的强化学习，为LLM对齐提供了新思路。

Abstract: Reinforcement Learning (RL) has shown promise for aligning Large Language Models (LLMs) to follow instructions with various constraints. Despite the encouraging results, RL improvement inevitably relies on sampling successful, high-quality responses; however, the initial model often struggles to generate responses that satisfy all constraints due to its limited capabilities, yielding sparse or indistinguishable rewards that impede learning. In this work, we propose Hindsight instruction Replay (HiR), a novel sample-efficient RL framework for complex instruction following tasks, which employs a select-then-rewrite strategy to replay failed attempts as successes based on the constraints that have been satisfied in hindsight. We perform RL on these replayed samples as well as the original ones, theoretically framing the objective as dual-preference learning at both the instruction- and response-level to enable efficient optimization using only a binary reward signal. Extensive experiments demonstrate that the proposed HiR yields promising results across different instruction following tasks, while requiring less computational budget. Our code and dataset is available at https://github.com/sastpg/HIR.

</details>


### [53] [The Gaining Paths to Investment Success: Information-Driven LLM Graph Reasoning for Venture Capital Prediction](https://arxiv.org/abs/2512.23489)
*Haoyu Pei,Zhongyang Liu,Xiangyi Xiao,Xiaocong Du,Haipeng Zhang,Kunpeng Zhang,Suting Hong*

Main category: cs.AI

TL;DR: MIRAGE-VC是一个多视角检索增强生成框架，用于风险投资预测，通过信息增益驱动的路径检索和可学习门控机制整合多种证据流，显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 风险投资预测需要综合复杂的关联证据（公司披露、投资者记录、投资网络结构）并进行显式推理，但传统机器学习、图神经网络和现有图-LLM方法都无法有效处理这种"图外"预测任务。

Method: 提出MIRAGE-VC框架：1) 信息增益驱动的路径检索器迭代选择高价值邻居，将投资网络压缩为紧凑链；2) 多智能体架构通过基于公司属性的可学习门控机制整合三个证据流；3) 解决路径爆炸和异构证据融合两大挑战。

Result: 在严格的反泄漏控制下，MIRAGE-VC实现了+5.0% F1分数和+16.6% PrecisionAt5的提升，为推荐系统和风险评估等其他图外预测任务提供了启示。

Conclusion: MIRAGE-VC成功解决了风险投资预测中的图外预测挑战，通过显式推理和异构证据融合显著提升了预测性能，为复杂图外预测任务提供了有效框架。

Abstract: Most venture capital (VC) investments fail, while a few deliver outsized returns. Accurately predicting startup success requires synthesizing complex relational evidence, including company disclosures, investor track records, and investment network structures, through explicit reasoning to form coherent, interpretable investment theses. Traditional machine learning and graph neural networks both lack this reasoning capability. Large language models (LLMs) offer strong reasoning but face a modality mismatch with graphs. Recent graph-LLM methods target in-graph tasks where answers lie within the graph, whereas VC prediction is off-graph: the target exists outside the network. The core challenge is selecting graph paths that maximize predictor performance on an external objective while enabling step-by-step reasoning. We present MIRAGE-VC, a multi-perspective retrieval-augmented generation framework that addresses two obstacles: path explosion (thousands of candidate paths overwhelm LLM context) and heterogeneous evidence fusion (different startups need different analytical emphasis). Our information-gain-driven path retriever iteratively selects high-value neighbors, distilling investment networks into compact chains for explicit reasoning. A multi-agent architecture integrates three evidence streams via a learnable gating mechanism based on company attributes. Under strict anti-leakage controls, MIRAGE-VC achieves +5.0% F1 and +16.6% PrecisionAt5, and sheds light on other off-graph prediction tasks such as recommendation and risk assessment. Code: https://anonymous.4open.science/r/MIRAGE-VC-323F.

</details>


### [54] [Why AI Safety Requires Uncertainty, Incomplete Preferences, and Non-Archimedean Utilities](https://arxiv.org/abs/2512.23508)
*Alessio Benavoli,Alessandro Facchini,Marco Zaffalon*

Main category: cs.AI

TL;DR: 论文探讨如何确保AI系统与人类价值观对齐并保持安全，通过AI协助游戏和AI关机游戏框架研究此问题，指出解决这些挑战需要AI能够处理不确定性和非阿基米德偏好。


<details>
  <summary>Details</summary>
Motivation: 确保AI系统与人类价值观对齐并保持安全是重要问题。AI协助问题涉及设计能够帮助人类最大化其效用函数的AI代理，但只有人类知道这些函数；AI关机问题则涉及设计能够在按下关机按钮时关闭、不试图阻止或导致按钮被按下、同时能胜任任务的AI代理。

Method: 通过AI协助游戏和AI关机游戏的理论框架来研究AI对齐问题。AI协助游戏关注AI代理学习人类未知的效用函数；AI关机游戏关注AI代理在关机指令下的行为规范。

Result: 研究发现解决这些挑战需要AI代理能够在不确定性下进行推理，并处理不完全和非阿基米德偏好。这表明传统的AI设计方法可能不足以确保AI安全和对齐。

Conclusion: 确保AI系统与人类价值观对齐并保持安全需要新的AI设计方法，这些方法必须能够处理不确定性和复杂的偏好结构，特别是非阿基米德偏好，这对AI安全研究具有重要意义。

Abstract: How can we ensure that AI systems are aligned with human values and remain safe? We can study this problem through the frameworks of the AI assistance and the AI shutdown games. The AI assistance problem concerns designing an AI agent that helps a human to maximise their utility function(s). However, only the human knows these function(s); the AI assistant must learn them. The shutdown problem instead concerns designing AI agents that: shut down when a shutdown button is pressed; neither try to prevent nor cause the pressing of the shutdown button; and otherwise accomplish their task competently. In this paper, we show that addressing these challenges requires AI agents that can reason under uncertainty and handle both incomplete and non-Archimedean preferences.

</details>


### [55] [Divergent-Convergent Thinking in Large Language Models for Creative Problem Generation](https://arxiv.org/abs/2512.23601)
*Manh Hung Nguyen,Adish Singla*

Main category: cs.AI

TL;DR: CreativeDC是一种两阶段提示方法，通过解耦创造性探索和约束满足，显著提高LLM生成教育问题的多样性和新颖性，同时保持实用性。


<details>
  <summary>Details</summary>
Motivation: LLM在教育问题生成中存在"人工蜂群思维"效应，导致生成内容过于相似和重复，损害思维多样性，需要解决这一问题。

Method: 基于Wallas创造力理论和Guilford发散-收敛思维框架，提出CreativeDC两阶段提示方法：第一阶段进行创造性探索，第二阶段进行约束满足，使LLM在最终确定问题前探索更广泛的想法空间。

Result: CreativeDC在多样性、新颖性和实用性综合评估中显著优于基线方法，随着采样数量增加，能生成更多有效不同问题，增长速度更快。

Conclusion: CreativeDC通过明确分离创造性探索和约束满足阶段，有效缓解LLM的"人工蜂群思维"效应，为教育问题生成提供了更富创造性和多样性的解决方案。

Abstract: Large language models (LLMs) have significant potential for generating educational questions and problems, enabling educators to create large-scale learning materials. However, LLMs are fundamentally limited by the ``Artificial Hivemind'' effect, where they generate similar responses within the same model and produce homogeneous outputs across different models. As a consequence, students may be exposed to overly similar and repetitive LLM-generated problems, which harms diversity of thought. Drawing inspiration from Wallas's theory of creativity and Guilford's framework of divergent-convergent thinking, we propose CreativeDC, a two-phase prompting method that explicitly scaffolds the LLM's reasoning into distinct phases. By decoupling creative exploration from constraint satisfaction, our method enables LLMs to explore a broader space of ideas before committing to a final problem. We evaluate CreativeDC for creative problem generation using a comprehensive set of metrics that capture diversity, novelty, and utility. The results show that CreativeDC achieves significantly higher diversity and novelty compared to baselines while maintaining high utility. Moreover, scaling analysis shows that CreativeDC generates a larger effective number of distinct problems as more are sampled, increasing at a faster rate than baseline methods.

</details>


### [56] [Physics-Informed Neural Networks for Device and Circuit Modeling: A Case Study of NeuroSPICE](https://arxiv.org/abs/2512.23624)
*Chien-Ting Tung,Chenming Hu*

Main category: cs.AI

TL;DR: NeuroSPICE是一个基于物理信息神经网络(PINN)的器件和电路仿真框架，通过神经网络求解电路微分代数方程，提供传统SPICE之外的独特优势。


<details>
  <summary>Details</summary>
Motivation: 传统SPICE依赖于时间离散化的数值求解器，而NeuroSPICE旨在利用PINN的优势来解决电路仿真中的微分代数方程，特别是在处理新兴器件和高度非线性系统方面提供更大的灵活性。

Method: 采用物理信息神经网络(PINN)框架，通过反向传播最小化方程残差来求解电路微分代数方程。使用时间域解析方程建模器件和电路波形，具有精确的时间导数。

Result: PINN在训练速度和精度上不优于传统SPICE，但提供了独特优势：可用于设计优化的代理模型、解决逆问题，并能灵活仿真新兴器件（如铁电存储器等高度非线性系统）。

Conclusion: NeuroSPICE为电路仿真提供了基于PINN的新方法，虽然计算效率不如传统SPICE，但在处理复杂非线性系统和设计优化方面具有独特价值，为新兴器件仿真开辟了新途径。

Abstract: We present NeuroSPICE, a physics-informed neural network (PINN) framework for device and circuit simulation. Unlike conventional SPICE, which relies on time-discretized numerical solvers, NeuroSPICE leverages PINNs to solve circuit differential-algebraic equations (DAEs) by minimizing the residual of the equations through backpropagation. It models device and circuit waveforms using analytical equations in time domain with exact temporal derivatives. While PINNs do not outperform SPICE in speed or accuracy during training, they offer unique advantages such as surrogate models for design optimization and inverse problems. NeuroSPICE's flexibility enables the simulation of emerging devices, including highly nonlinear systems such as ferroelectric memories.

</details>


### [57] [Regret-Based Federated Causal Discovery with Unknown Interventions](https://arxiv.org/abs/2512.23626)
*Federico Baldo,Charles K. Assaad*

Main category: cs.AI

TL;DR: I-PERI：一种联邦因果发现算法，用于处理未知客户端干预下的异构数据，通过恢复客户端图并集的CPDAG，并利用干预引起的结构差异来定向更多边，得到更紧的Φ-Markov等价类。


<details>
  <summary>Details</summary>
Motivation: 现有联邦因果发现方法通常假设所有客户端共享相同的因果模型，但现实中客户端特定的策略或协议（如不同医院）会引入未知的异质干预，这种理想化假设不切实际。

Method: 提出I-PERI算法：1）首先恢复所有客户端图并集的CPDAG（完全部分有向无环图）；2）利用不同客户端间干预引起的结构差异来定向更多边，得到更紧的Φ-Markov等价类，用Φ-CPDAG表示。

Result: 提供了I-PERI算法的理论收敛性保证和隐私保护特性证明，并在合成数据上进行了实证评估，展示了算法的有效性。

Conclusion: I-PERI解决了联邦环境下未知客户端干预的因果发现问题，通过利用干预引起的结构差异得到更精确的因果结构，具有理论保证和实际应用价值。

Abstract: Most causal discovery methods recover a completed partially directed acyclic graph representing a Markov equivalence class from observational data. Recent work has extended these methods to federated settings to address data decentralization and privacy constraints, but often under idealized assumptions that all clients share the same causal model. Such assumptions are unrealistic in practice, as client-specific policies or protocols, for example, across hospitals, naturally induce heterogeneous and unknown interventions. In this work, we address federated causal discovery under unknown client-level interventions. We propose I-PERI, a novel federated algorithm that first recovers the CPDAG of the union of client graphs and then orients additional edges by exploiting structural differences induced by interventions across clients. This yields a tighter equivalence class, which we call the $\mathbfΦ$-Markov Equivalence Class, represented by the $\mathbfΦ$-CPDAG. We provide theoretical guarantees on the convergence of I-PERI, as well as on its privacy-preserving properties, and present empirical evaluations on synthetic data demonstrating the effectiveness of the proposed algorithm.

</details>


### [58] [Web World Models](https://arxiv.org/abs/2512.23676)
*Jichen Feng,Yifan Zhang,Chenggong Zhang,Yifu Lu,Shilong Liu,Mengdi Wang*

Main category: cs.AI

TL;DR: Web World Model (WWM) 是一种混合方法，通过普通网页代码实现世界状态和"物理"规则以保证逻辑一致性，同时利用大语言模型生成上下文、叙事和高级决策，在可控性和开放性之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 现有语言代理的世界构建方法存在两极分化：传统网页框架提供可靠但固定的数据库支持环境，而完全生成式世界模型追求无限环境但牺牲了可控性和工程实用性。需要一种中间方案来平衡逻辑一致性和开放性探索。

Method: 提出 Web World Model (WWM)，在真实网页技术栈上构建：1) 用普通网页代码实现世界状态和物理规则确保逻辑一致性；2) 大语言模型在结构化潜在状态上生成上下文、叙事和高级决策；3) 将代码定义规则与模型驱动想象力分离；4) 将潜在状态表示为类型化网页接口；5) 使用确定性生成实现无限但有结构的探索。

Result: 构建了多种 WWM 系统：基于真实地理的无限旅行地图、虚构星系探索者、网页规模的百科全书和叙事世界、模拟和游戏环境。验证了网页技术栈可以作为世界模型的可扩展基底，实现可控且开放的环境。

Conclusion: 网页技术栈本身可以作为世界模型的可扩展基底，通过分离代码规则与模型想象力、类型化接口表示和确定性生成，能够创建既可控又开放的环境，为语言代理提供持久的行动、记忆和学习世界。

Abstract: Language agents increasingly require persistent worlds in which they can act, remember, and learn. Existing approaches sit at two extremes: conventional web frameworks provide reliable but fixed contexts backed by databases, while fully generative world models aim for unlimited environments at the expense of controllability and practical engineering. In this work, we introduce the Web World Model (WWM), a middle ground where world state and ``physics'' are implemented in ordinary web code to ensure logical consistency, while large language models generate context, narratives, and high-level decisions on top of this structured latent state. We build a suite of WWMs on a realistic web stack, including an infinite travel atlas grounded in real geography, fictional galaxy explorers, web-scale encyclopedic and narrative worlds, and simulation- and game-like environments. Across these systems, we identify practical design principles for WWMs: separating code-defined rules from model-driven imagination, representing latent state as typed web interfaces, and utilizing deterministic generation to achieve unlimited but structured exploration. Our results suggest that web stacks themselves can serve as a scalable substrate for world models, enabling controllable yet open-ended environments. Project Page: https://github.com/Princeton-AI2-Lab/Web-World-Models.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [59] [Robust Liu-Type Estimation for Multicollinearity in Fuzzy Logistic Regression](https://arxiv.org/abs/2512.22515)
*Ayad Habib Shemail,Ahmed Razzaq Al-Lami,Amal Hadi Rashid*

Main category: stat.AP

TL;DR: 针对模糊逻辑回归模型中的多重共线性问题，提出了多种Liu型估计器，通过模拟和实际数据验证，发现FLLTPE和FLLTE表现最优


<details>
  <summary>Details</summary>
Motivation: 多重共线性会导致模糊逻辑回归模型参数估计不稳定和方差膨胀，需要有效的解决方法

Method: 使用模糊三角数表示响应变量和参数，采用多种Liu型估计器：FMLE、FLRE、FLLE、FLLTE、FLLTPE，通过模拟研究和实际肾衰竭数据分析

Result: FLLTPE和FLLTE在均方误差和拟合优度标准上表现优于其他估计器

Conclusion: FLLTPE和FLLTE是解决模糊逻辑回归中多重共线性问题的有效方法

Abstract: This article addresses the fuzzy logistic regression model under conditions of multicollinearity, which causes instability and inflated variance in parameter estimation. In this model, both the response variable and parameters are represented as fuzzy triangular numbers. To overcome the multicollinearity problem, various Liu-type estimators were employed: Fuzzy Maximum Likelihood Estimators (FMLE), Fuzzy Logistic Ridge Estimators (FLRE), Fuzzy Logistic Liu Estimators (FLLE), Fuzzy Logistic Liu-type Estimators (FLLTE), and Fuzzy Logistic Liu-type Parameter Estimators (FLLTPE). Through simulations with various sample sizes and application to real fuzzy data on kidney failure, model performance was evaluated using mean square error (MSE) and goodness of fit criteria. Results demonstrated superior performance of FLLTPE and FLLTE compared to other estimators.

</details>


### [60] [Counterfactual Harm: A Counter-argument](https://arxiv.org/abs/2512.22892)
*Amit N. Sawant,Mats J. Stensrud*

Main category: stat.AP

TL;DR: 论文指出，基于反事实推理的"伤害"定义在多治疗方案场景中会产生非传递性结果，而基于期望效用的干预主义定义能确保传递性排序。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统越来越多地用于指导决策，确保其遵循伦理原则至关重要。医学中的核心原则是非伤害原则（"不伤害"）。现有的基于反事实推理的伤害定义在简单二元治疗场景中流行，但在多治疗方案中存在问题。

Method: 通过一个包含三种结核病治疗方案（A、B、C）的示例，展示反事实伤害定义会产生非传递性结果：B比A伤害小，C比B伤害小，但C比A伤害大。对比了基于期望效用的干预主义伤害定义。

Result: 反事实伤害定义在多治疗方案中会产生非传递性排序，这可能导致临床决策难以辩护。而干预主义定义基于期望效用，避免了反事实比较，确保了治疗排名的传递性。

Conclusion: 在多治疗方案场景中，基于反事实推理的伤害定义存在根本缺陷，会产生非传递性结果。基于期望效用的干预主义定义是更合适的替代方案，能确保伦理决策的逻辑一致性。

Abstract: As AI systems are increasingly used to guide decisions, it is essential that they follow ethical principles. A core principle in medicine is non-maleficence, often equated with ``do no harm''. A formal definition of harm based on counterfactual reasoning has been proposed and popularized. This notion of harm has been promoted in simple settings with binary treatments and outcomes. Here, we highlight a problem with this definition in settings involving multiple treatment options. Illustrated by an example with three tuberculosis treatments (say, A, B, and C), we demonstrate that the counterfactual definition of harm can produce intransitive results: B is less harmful than A, C is less harmful than B, yet C is more harmful than A when compared pairwise. This intransitivity poses a challenge as it may lead to practical (clinical) decisions that are difficult to justify or defend. In contrast, an interventionist definition of harm based on expected utility forgoes counterfactual comparisons and ensures transitive treatment rankings.

</details>


### [61] [Reliability Analysis of a 1-out-of-n Cold Standby Redundant System under the Generalized Lindley Distribution](https://arxiv.org/abs/2512.23019)
*Afshin Yaghoubi,Esmaile Khorram,Omid Naghshineh Arjmand*

Main category: stat.AP

TL;DR: 研究1-out-of-n冷备份冗余系统可靠性，假设组件失效时间服从广义Lindley分布，推导出系统可靠性的闭式表达式


<details>
  <summary>Details</summary>
Motivation: 现有可靠性分析主要假设指数、Erlang或Weibull失效分布，而广义Lindley分布因其危险函数的良好特性，可作为这些分布的合适替代，但目前尚未应用于冷备份冗余系统分析

Method: 首先使用矩生成函数方法确定n个独立同分布广义Lindley随机变量和的分布，然后基于此推导具有完美和不完美切换的1-out-of-n冷备份冗余系统的可靠性闭式表达式

Result: 成功推导出系统可靠性的闭式表达式，为使用广义Lindley分布分析冷备份冗余系统提供了理论基础

Conclusion: 广义Lindley分布可作为分析冷备份冗余系统可靠性的有效工具，推导的闭式表达式为实际工程应用提供了计算方法

Abstract: Cold standby 1-out-of-n redundant systems are well-established models in system reliability engineering. To date, reliability analyses of such systems have predominantly assumed exponential, Erlang, or Weibull failure distributions for their components. The Lindley distribution and its generalizations represent a significant class of statistical distributions in reliability engineering. Certain generalized Lindley distributions, due to the appealing characteristics of their hazard functions, can serve as suitable alternatives to other well-known lifetime distributions like the Weibull. This study investigates the reliability of a 1-out-of-n cold standby redundant system with perfect and imperfect switching, assuming that the active component failure times follow the Generalized Lindley distribution. We derive a closed-form expression for the system reliability. To achieve this, the distribution of the sum of n independent and identically distributed random variables following the Generalized Lindley distribution is first determined using the moment-generating function approach.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [62] [PHANTOM: Physics-Aware Adversarial Attacks against Federated Learning-Coordinated EV Charging Management System](https://arxiv.org/abs/2512.22381)
*Mohammad Zakaria Haider,Amit Kumar Podder,Prabin Mali,Aranya Chakrabortty,Sumit Paudyal,Mohammad Ashiqur Rahman*

Main category: cs.ET

TL;DR: PHANTOM：一种基于物理感知对抗网络和多智能体强化学习的电动汽车充电站攻击策略框架，能够绕过传统检测机制，揭示车网融合系统的网络安全脆弱性。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车充电站在配电网中的快速部署，需要智能自适应控制来维持电网的韧性和可靠性。现有系统对物理约束感知不足，传统检测机制可能无法识别复杂的虚假数据注入攻击。

Method: 提出PHANTOM框架：1）通过联邦学习构建物理信息神经网络作为数字孪生；2）基于数字孪生构建多智能体强化学习环境，使用DQN和SAC方法学习对抗性虚假数据注入策略；3）开发输配双仿真平台捕捉级联效应。

Result: 学习到的攻击策略能够破坏负荷平衡并引发电压不稳定，这些扰动会跨越输配电网边界传播。结果表明传统检测机制存在被绕过的风险。

Conclusion: 车网大规模融合需要物理感知的网络安全机制来确保系统韧性，PHANTOM框架揭示了现有系统的脆弱性并强调了物理约束在网络安全中的重要性。

Abstract: The rapid deployment of electric vehicle charging stations (EVCS) within distribution networks necessitates intelligent and adaptive control to maintain the grid's resilience and reliability. In this work, we propose PHANTOM, a physics-aware adversarial network that is trained and optimized through a multi-agent reinforcement learning model. PHANTOM integrates a physics-informed neural network (PINN) enabled by federated learning (FL) that functions as a digital twin of EVCS-integrated systems, ensuring physically consistent modeling of operational dynamics and constraints. Building on this digital twin, we construct a multi-agent RL environment that utilizes deep Q-networks (DQN) and soft actor-critic (SAC) methods to derive adversarial false data injection (FDI) strategies capable of bypassing conventional detection mechanisms. To examine the broader grid-level consequences, a transmission and distribution (T and D) dual simulation platform is developed, allowing us to capture cascading interactions between EVCS disturbances at the distribution level and the operations of the bulk transmission system. Results demonstrate how learned attack policies disrupt load balancing and induce voltage instabilities that propagate across T and D boundaries. These findings highlight the critical need for physics-aware cybersecurity to ensure the resilience of large-scale vehicle-grid integration.

</details>


### [63] [Protonic Nickelate Device Networks for Spatiotemporal Neuromorphic Computing](https://arxiv.org/abs/2512.22722)
*Yue Zhou,Shaan Shah,Tamal Dey,Yucheng Zhou,Ashwani Kumar,Sashank Sriram,Siyou Guo,Siddharth Kumar,Ranjan Kumar Patel,Eva Y. Andrei,Ertugrul Cubukcu,Shriram Ramanathan,Duygu Kuzum*

Main category: cs.ET

TL;DR: 研究人员开发了一种基于钙钛矿镍酸盐的集成神经形态计算平台，该平台在单一材料系统中同时实现了非线性时空处理和可编程存储器功能。


<details>
  <summary>Details</summary>
Motivation: 生物神经电路的计算能力源于非线性时间响应和空间分布动态网络相互作用的结合，但在硬件中复制这种丰富性一直具有挑战性，因为大多数神经形态设备只能模拟孤立的神经元或突触功能。

Method: 通过在同一个晶片上制造对称和非对称氢化NdNiO3结器件，将超快质子介导的瞬态动力学与稳定的多级电阻状态相结合。对称NdNiO3结网络通过质子再分布实现空间相互作用，同时每个节点提供短期时间记忆。

Result: 该平台实现了纳秒级操作，每个输入能耗仅0.2 nJ。当与非对称输出单元接口时，可在同一材料系统中同时进行特征变换和线性分类。在语音数字分类和早期癫痫发作检测任务中表现出色，优于仅时间处理或非耦合架构。

Conclusion: 质子镍酸盐作为一个紧凑、节能、CMOS兼容的平台，集成了处理和存储功能，为可扩展的智能硬件提供了有前景的解决方案。

Abstract: Computation in biological neural circuits arises from the interplay of nonlinear temporal responses and spatially distributed dynamic network interactions. Replicating this richness in hardware has remained challenging, as most neuromorphic devices emulate only isolated neuron- or synapse-like functions. In this work, we introduce an integrated neuromorphic computing platform in which both nonlinear spatiotemporal processing and programmable memory are realized within a single perovskite nickelate material system. By engineering symmetric and asymmetric hydrogenated NdNiO3 junction devices on the same wafer, we combine ultrafast, proton-mediated transient dynamics with stable multilevel resistance states. Networks of symmetric NdNiO3 junctions exhibit emergent spatial interactions mediated by proton redistribution, while each node simultaneously provides short-term temporal memory, enabling nanoseconds scale operation with an energy cost of 0.2 nJ per input. When interfaced with asymmetric output units serving as reconfigurable long-term weights, these networks allow both feature transformation and linear classification in the same material system. Leveraging these emergent interactions, the platform enables real-time pattern recognition and achieves high accuracy in spoken-digit classification and early seizure detection, outperforming temporal-only or uncoupled architectures. These results position protonic nickelates as a compact, energy-efficient, CMOS-compatible platform that integrates processing and memory for scalable intelligent hardware.

</details>


### [64] [Relaxation-based dynamical Ising machines for discrete tomography](https://arxiv.org/abs/2512.22784)
*Mikhail Erementchouk,Aditya Shukla,Pinaki Mazumder*

Main category: cs.ET

TL;DR: V₂动力学伊辛机可精确求解离散层析问题，从随机初始状态高概率收敛到满足层析数据的精确解，而非近似优化解。


<details>
  <summary>Details</summary>
Motivation: 传统伊辛机主要解决优化问题的近似解，本文探索动力学伊辛机在精确求解非平凡数据处理任务（如离散层析）方面的潜力。

Method: 使用V₂动力学模型作为伊辛机，将离散层析问题（从射线像素和重建二值图像）转化为自旋系统，利用其运动方程实现超越汉明邻域约束的非局部跃迁。

Result: V₂模型能以高概率（P_succ≈1）收敛到精确满足层析数据的图像；对于每个像素最多两条射线相交的问题，收敛时间仅弱依赖于图像尺寸。

Conclusion: 特定动力学系统（如V₂模型）能够精确求解高度非平凡的数据处理任务，这种求解能力源于动力学特征本身而非仅将问题重铸为自旋形式。

Abstract: Dynamical Ising machines are continuous dynamical systems that evolve from a generic initial state to a state strongly related to the ground state of the classical Ising model. We show that such a machine driven by the V${}_2$ dynamical model can solve exactly discrete tomography problems about reconstructing a binary image from the pixel sums along a discrete set of rays. In contrast to usual applications of Ising machines, targeting approximate solutions to optimization problems, the randomly initialized V${}_2$ model converges with high probability ($P_{\mathrm{succ}} \approx 1$) to an image precisely satisfying the tomographic data. For the problems with at most two rays intersecting at each pixel, the V${}_2$ model converges in internal machine time that depends only weakly on the image size. Our consideration is an example of how specific dynamical systems can produce exact solutions to highly non-trivial data processing tasks. Crucially, this solving capability arises from the dynamical features of the V${}_2$ model itself, in particular its equations of motion that enable non-local transitions of the discrete component of the relaxed spin beyond Hamming-neighborhood constraints, rather than from merely recasting the tomography problem in spin form.

</details>


### [65] [LIMO: Low-Power In-Memory-Annealer and Matrix-Multiplication Primitive for Edge Computing](https://arxiv.org/abs/2512.23212)
*Amod Holla,Sumedh Chatterjee,Sutanu Sen,Anushka Mukherjee,Fernando Garcia-Redondo,Dwaipayan Biswas,Francesca Iacopi,Kaushik Roy*

Main category: cs.ET

TL;DR: LIMO：一种混合信号计算宏，通过内存内退火算法和STT-MTJ随机切换解决大规模TSP问题，同时支持神经网络推理


<details>
  <summary>Details</summary>
Motivation: 传统冯·诺依曼架构在解决大规模组合优化问题（如TSP）时面临内存墙问题，现有内存计算退火方法在问题规模增大时解质量下降

Method: 1. 设计LIMO混合信号计算宏，实现内存内退火算法，降低搜索空间复杂度；2. 利用STT-MTJ随机切换帮助跳出局部最优；3. 针对大规模实例采用基于细化的分治算法，支持空间架构并行优化

Result: 在多达85,900个城市的TSP实例上，LIMO系统相比现有硬件退火器获得更优解质量和更快求解时间；同时支持神经网络推理，在图像分类和人脸检测中实现与软件相当的精度，但延迟和能耗更低

Conclusion: LIMO通过混合信号内存计算架构有效解决了大规模组合优化问题，其模块化设计还可支持其他应用如神经网络推理，展示了内存计算架构的多功能性

Abstract: Combinatorial optimization (CO) underpins applications in science and engineering, ranging from logistics to electronic design automation. A classic example is the NP-complete Traveling Salesman Problem (TSP). Finding exact solutions for large-scale TSP instances remains computationally intractable; on von Neumann architectures, such solvers are constrained by the memory wall, incurring compute-memory traffic that grows with instance size. Metaheuristics, such as simulated annealing implemented on compute-in-memory (CiM) architectures, offer a way to mitigate the von Neumann bottleneck. This is accomplished by performing in-memory optimization cycles to rapidly find approximate solutions for TSP instances. Yet this approach suffers from degrading solution quality as instance size increases, owing to inefficient state-space exploration. To address this, we present LIMO, a mixed-signal computational macro that implements an in-memory annealing algorithm with reduced search-space complexity. The annealing process is aided by the stochastic switching of spin-transfer-torque magnetic-tunnel-junctions (STT-MTJs) to escape local minima. For large instances, our macro co-design is complemented by a refinement-based divide-and-conquer algorithm amenable to parallel optimization in a spatial architecture. Consequently, our system comprising several LIMO macros achieves superior solution quality and faster time-to-solution on instances up to 85,900 cities compared to prior hardware annealers. The modularity of our annealing peripherals allows the LIMO macro to be reused for other applications, such as vector-matrix multiplications (VMMs). This enables our architecture to support neural network inference. As an illustration, we show image classification and face detection with software-comparable accuracy, while achieving lower latency and energy consumption than baseline CiM architectures.

</details>


### [66] [interID -- An Ecosystem-agnostic Verifier Application for Self-sovereign Identity](https://arxiv.org/abs/2512.23383)
*Hakan Yildiz,Axel Küpper*

Main category: cs.ET

TL;DR: interID是一个模块化的凭证验证应用，通过编排特定生态系统的验证器服务来解决不同SSI生态系统间的互操作性问题。


<details>
  <summary>Details</summary>
Motivation: 不同SSI生态系统（如欧洲数字身份和欧洲区块链服务基础设施）并存导致技术框架和信任机制差异，造成了跨生态系统互操作性的重大挑战。

Method: 提出interID系统，包含：(1) 生态系统无关的编排层，与多个SSI验证服务接口；(2) 统一API，抽象底层协议复杂性；(3) 实际实现，桥接Hyperledger Indy/Aries、EBSI和EUDI三大SSI生态系统。

Result: 评估结果显示interID成功验证所有测试钱包的凭证，性能开销最小，同时保持灵活架构，可扩展支持更多SSI生态系统。

Conclusion: interID为SSI验证器实现提供了技术解决方案和架构模式，是实现SSI互操作性的重要进展。

Abstract: Self-Sovereign Identity is a transformative paradigm in digital identity management, empowering individuals with full control over their credentials. However, the coexistence of diverse SSI ecosystems, such as the European Digital Identity and the European Blockchain Services Infrastructure, poses significant challenges for cross-ecosystem interoperability due to technological and trust framework differences. This paper introduces \textit{interID}, a modular credential verification application that addresses this fragmentation by orchestrating ecosystem-specific verifier services. Our key contributions include: (1) an ecosystem-agnostic orchestration layer that interfaces with multiple SSI verification services, (2) a unified API that abstracts underlying protocol complexities for service providers, and (3) a practical implementation that bridges three major SSI ecosystems: Hyperledger Indy/Aries, EBSI, and EUDI. Evaluation results demonstrate that interID successfully verifies credentials across all tested wallets with minimal performance overhead, while maintaining a flexible architecture that can be extended to accept credentials from additional SSI ecosystems. This work offers both a technical solution and architectural pattern for achieving interoperability in SSI verifier implementations.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [67] [Urban Food Self-Production in the Perspective of Social Learning Theory: Empowering Self-Sustainability](https://arxiv.org/abs/2512.22594)
*Ewa Duda,Adamina Korwin-Szymanowska*

Main category: cs.CY

TL;DR: 该研究通过在两栋公寓楼安装水培柜，探索城市居民参与可持续食品生产的动机、经验和教育需求，为城市教育者和政策制定者提供实践指导。


<details>
  <summary>Details</summary>
Motivation: 在气候变化和粮食安全背景下，城市食品生产日益重要。研究旨在探索城市农业作为替代食品生产形式的益处、挑战和发展潜力，了解居民参与创新城市食品生产项目的动机。

Method: 在波兰罗兹和华沙的两栋公寓楼走廊安装20个水培柜，让居民种植可食用植物。采用目的性抽样和两轮深度访谈的定性研究方法，对42名参与者进行研究。

Result: 研究发现揭示了城市居民采用可持续食品生产解决方案的动机、他们的种植经验，以及促使他们加入创新城市食品生产项目的教育活动。结果对城市教育过程的相关方具有参考价值。

Conclusion: 该研究为城市教育者、城市当局、环保协会和基层活动家提供了关于如何促进城市居民参与可持续食品生产的实践见解，有助于推动城市农业作为替代食品生产形式的发展。

Abstract: Urban food production is becoming an increasingly significant topic in the context of climate change and food security. Conducting research on this subject is becoming an essential element of urban development, deepening knowledge regarding the benefits, challenges, and potential for the development of urban agriculture as an alternative form of food production. Responding to this need, this monograph presents the results of a project study developing innovative socio-technological solutions for sustainable food production and consumption. The idea behind this unique project was to install twenty hydroponic cabinets in the corridors of the selected block of flats, where residents would grow edible plants. The presented research aimed to understand the people who joined this unique initiative. The qualitative study employed purposive sampling and in-depth interviews conducted in two waves. The study comprised 42 participants drawn from two communities of residents in Łódź and Warsaw, Poland. The findings outline the reasons that motivate urban residents to implement sustainable food production solutions, their farming experiences and the educational activities that led to their decision to join an innovative urban food production project. The results obtained will be relevant for those involved in the urban education process, including city authorities, urban educators, pro-environmental associations, and grassroots activists.

</details>


### [68] [Ungraded Assignments in Introductory Computing: A Report](https://arxiv.org/abs/2512.23004)
*Yehya Sleiman Tellawi,Abhishek K. Umrawal*

Main category: cs.CY

TL;DR: 研究探讨了未评分作业对计算机工程导论课程学生学习体验的影响，发现参与未评分作业与课程表现呈正相关


<details>
  <summary>Details</summary>
Motivation: 探索未评分作业对计算机工程导论课程学生学习体验的影响，包括学生参与度、理解和整体学业表现

Method: 在ECE 120计算机导论课程中开发并实施新的未评分作业，采用混合方法评估效果，包括问卷调查、访谈和表现分析

Result: 分析显示参与未评分作业与整体课程表现呈正相关，表明这些作业可能吸引高成就学生和/或支持更好的学习成果

Conclusion: 未评分作业对计算机工程导论课程学生的学习体验有积极影响，特别是与课程表现提升相关，值得在教学中进一步应用

Abstract: This experience report explores the effects of ungraded assignments on the learning experience of students in an introductory computing course. Our study examines the impact of ungraded assignments on student engagement, understanding, and overall academic performance. We developed and administered new ungraded assignments for a required course in the first year of the Computer Engineering curriculum called ECE 120 Introduction to Computing. To assess the effectiveness of our ungraded assignments, we employed a mixed-methods approach, including surveys, interviews, and performance analysis. Our analysis shows a positive relationship between participation in ungraded assignments and overall course performance, suggesting these assignments may appeal to high-achieving students and/or support better outcomes.

</details>


### [69] [Inteligencia Artificial y Empleo: perspectiva Territorial y de Género](https://arxiv.org/abs/2512.23059)
*Antoni Mestre,Xavier Naya,Manoli Albert,Vicente Pelechano*

Main category: cs.CY

TL;DR: 该研究提出了一个基于行业数据的方法框架来评估西班牙就业对AI的潜在暴露程度，揭示了AI暴露存在稳定的结构性模式：大都市和服务导向地区暴露更高，且女性就业在所有地区都面临更高的AI暴露风险。


<details>
  <summary>Details</summary>
Motivation: 人工智能特别是生成模型的扩散预计将以不均衡的方式影响劳动力市场。现有研究多采用职业中心方法，但在西班牙背景下存在局限性，需要基于行业数据的方法框架来评估AI对就业的潜在影响。

Method: 构建AI-CNAE影响矩阵，应用于2021-2023年省级就业数据，提供西班牙各地区和性别分层的AI暴露评估。该方法框架克服了传统职业中心方法的局限，更适合西班牙情境。

Result: 结果显示稳定的结构性模式：大都市和服务导向地区AI暴露更高；存在一致的性别差距，女性就业在所有地区都表现出更高的AI暴露；AI暴露模式在时间上保持稳定。

Conclusion: 该框架不是预测工作岗位流失，而是提供结构性视角，识别AI最可能重塑工作和技能需求的领域，为基于证据的政策制定和战略规划提供支持。

Abstract: The diffusion of artificial intelligence, particularly generative models, is expected to transform labor markets in uneven ways across sectors, territories, and social groups. This paper proposes a methodological framework to estimate the potential exposure of employment to AI using sector based data, addressing the limitations of occupation centered approaches in the Spanish context. By constructing an AI CNAE incidence matrix and applying it to provincial employment data for the period 2021 to 2023, we provide a territorial and gender disaggregated assessment of AI exposure across Spain. The results reveal stable structural patterns, with higher exposure in metropolitan and service oriented regions and a consistent gender gap, as female employment exhibits higher exposure in all territories. Rather than predicting job displacement, the framework offers a structural perspective on where AI is most likely to reshape work and skill demands, supporting evidence based policy and strategic planning.

</details>


### [70] [Identifying Barriers Hindering the Acceptance of Generative AI as a Work Associate, measured with the new AGAWA scale](https://arxiv.org/abs/2512.23373)
*Łukasz Sikorski,Albert Łukasik,Jacek Matulewski,Arkadiusz Gut*

Main category: cs.CY

TL;DR: 研究者开发了AGAWA量表，仅用4个问题测量学生对生成式AI作为同事的态度，发现积极态度与对AI的担忧、拟人化特征感知、人类独特性认知呈负相关。


<details>
  <summary>Details</summary>
Motivation: 当前学生对生成式AI的态度将影响未来工作场所的AI采用，具有重要经济和社会意义。需要及时研究这一现象并识别实施障碍，使用能跟上AI快速发展的工具。

Method: 提出AGAWA量表，基于TAM和UTAUT技术接受模型，特别关注AI革命背景下的关键问题：AI在场接受度、社会影响（如作为助手或主管）、道德困境解决。量表仅含4个项目，耗时短。研究考察三个因素：与AI交互的担忧、AI的拟人特征、人类独特性/优越感认知。

Result: 对生成式AI作为同事的积极态度与所有三个因素呈强烈负相关（担忧越少、拟人化感知越弱、人类独特性认知越低，态度越积极）。三个因素之间呈正相关。这证实了AI信任的情感与道德维度与工作场所生成式AI态度之间的关系。

Conclusion: AGAWA量表是测量生成式AI作为同事态度的有效工具，能够快速评估。研究发现情感和道德维度的信任因素显著影响对工作场所生成式AI的态度，为理解AI接受障碍提供了重要见解。

Abstract: The attitudes of today's students toward generative AI (GenAI) will significantly influence its adoption in the workplace in the years to come, carrying both economic and social implications. It is therefore crucial to study this phenomenon now and identify obstacles for the successful implementation of GenAI in the workplace, using tools that keep pace with its rapid evolution. For this purpose, we propose the AGAWA scale, which measures attitudes toward an artificial agent utilising GenAI and perceived as a coworker. It is partially based on the TAM and UTAUT models of technology acceptance, taking into account issues that are particularly important in the context of the AI revolution, namely acceptance of its presence and social influence (e.g., as an assistant or even a supervisor), and above all, resolution of moral dilemmas. The advantage of the AGAWA scale is that it takes little time to complete and analyze, as it contains only four items. In the context of such cooperation, we investigated the importance of three factors: concerns about interaction with GenAI, its human-like characteristics, and a sense of human uniqueness, or even superiority over GenAI. An observed manifestation of the attitude towards this technology is the actual need to get help from it. The results showed that positive attitudes toward GenAI as a coworker were strongly associated with all three factors (negative correlation), and those factors were also related to each other (positive correlation). This confirmed the relationship between affective and moral dimensions of trust towards AI and attitudes towards generative AI at the workplace.

</details>


### [71] [Can AI Recognize Its Own Reflection? Self-Detection Performance of LLMs in Computing Education](https://arxiv.org/abs/2512.23587)
*Christopher Burger,Karmece Talley,Christina Trotter*

Main category: cs.CY

TL;DR: 评估GPT-4、Claude和Gemini三种大语言模型在计算教育中检测AI生成文本的能力，发现它们在标准条件下能识别AI文本，但难以准确区分人类写作，且易受欺骗性提示影响，不适合用于高风险学术不端判断。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，计算教育领域的学术诚信面临严峻挑战。教育工作者需要可靠的检测方法来识别AI生成的文本，但目前缺乏对这些模型在计算特定语境下检测能力的系统评估。

Method: 研究评估了三种主流大语言模型（GPT-4、Claude和Gemini）在计算特定语境下检测AI生成文本的能力。测试包括标准条件和"欺骗性"提示条件，后者要求模型生成能逃避检测的文本。通过对比分析模型对AI生成文本和人类写作的识别准确率。

Result: 研究发现检测能力存在显著不稳定性：默认AI生成文本容易被识别，但所有模型都难以正确分类人类写作（错误率高达32%）。模型对欺骗性提示高度敏感，Gemini生成的文本甚至能完全欺骗GPT-4。简单的提示修改就能显著降低检测效果。

Conclusion: 当前大语言模型在检测AI生成文本方面过于不可靠，不适合用于高风险学术不端行为的判断。需要开发更稳健的检测方法，教育工作者应谨慎依赖这些模型进行学术诚信评估。

Abstract: The rapid advancement of Large Language Models (LLMs) presents a significant challenge to academic integrity within computing education. As educators seek reliable detection methods, this paper evaluates the capacity of three prominent LLMs (GPT-4, Claude, and Gemini) to identify AI-generated text in computing-specific contexts. We test their performance under both standard and 'deceptive' prompt conditions, where the models were instructed to evade detection. Our findings reveal a significant instability: while default AI-generated text was easily identified, all models struggled to correctly classify human-written work (with error rates up to 32%). Furthermore, the models were highly susceptible to deceptive prompts, with Gemini's output completely fooling GPT-4. Given that simple prompt alterations significantly degrade detection efficacy, our results demonstrate that these LLMs are currently too unreliable for making high-stakes academic misconduct judgments.

</details>


### [72] [AI tutoring can safely and effectively support students: An exploratory RCT in UK classrooms](https://arxiv.org/abs/2512.23633)
*LearnLM Team,Eedi,:,Albert Wang,Aliya Rysbek,Andrea Huber,Anjali Nambiar,Anna Kenolty,Ben Caulfield,Beth Lilley-Draper,Bibi Groot,Brian Veprek,Chelsea Burdett,Claire Willis,Craig Barton,Digory Smith,George Mu,Harriet Walters,Irina Jurenka,Iris Hulls,James Stalley-Moores,Jonathan Caton,Julia Wilkowski,Kaiz Alarakyia,Kevin R. McKee,Liam McCafferty,Lucy Dalton,Markus Kunesch,Pauline Malubay,Rachel Kidson,Rich Wells,Sam Wheeler,Sara Wiltberger,Shakir Mohamed,Simon Woodhead,Vasco Brazão*

Main category: cs.CY

TL;DR: AI教学模型LearnLM在数学辅导中表现优异，76.4%的消息无需或仅需微调，学生表现不亚于真人辅导，在某些方面甚至更优。


<details>
  <summary>Details</summary>
Motivation: 一对一辅导虽是个性化教育的黄金标准，但成本高昂难以规模化。研究旨在探索生成式AI能否帮助扩大这种优质教育资源的可及性。

Method: 在英国5所中学进行随机对照试验(N=165)，将经过教学法微调的生成式AI模型LearnLM集成到Eedi数学平台的聊天辅导中。专家导师监督LearnLM，审核并修改其生成的消息。

Result: LearnLM是可靠的教学指导来源：76.4%的生成消息无需或仅需微小修改。使用LearnLM辅导的学生表现不亚于真人辅导，在解决新问题方面甚至更优(成功率66.2% vs 60.7%)。导师认为LearnLM擅长设计启发式问题。

Conclusion: 经过教学法微调的AI辅导系统在提供有效、个性化学习支持方面具有广阔前景，有望实现规模化应用。

Abstract: One-to-one tutoring is widely considered the gold standard for personalized education, yet it remains prohibitively expensive to scale. To evaluate whether generative AI might help expand access to this resource, we conducted an exploratory randomized controlled trial (RCT) with $N = 165$ students across five UK secondary schools. We integrated LearnLM -- a generative AI model fine-tuned for pedagogy -- into chat-based tutoring sessions on the Eedi mathematics platform. In the RCT, expert tutors directly supervised LearnLM, with the remit to revise each message it drafted until they would be satisfied sending it themselves. LearnLM proved to be a reliable source of pedagogical instruction, with supervising tutors approving 76.4% of its drafted messages making zero or minimal edits (i.e., changing only one or two characters). This translated into effective tutoring support: students guided by LearnLM performed at least as well as students chatting with human tutors on each learning outcome we measured. In fact, students who received support from LearnLM were 5.5 percentage points more likely to solve novel problems on subsequent topics (with a success rate of 66.2%) than those who received tutoring from human tutors alone (rate of 60.7%). In interviews, tutors highlighted LearnLM's strength at drafting Socratic questions that encouraged deeper reflection from students, with multiple tutors even reporting that they learned new pedagogical practices from the model. Overall, our results suggest that pedagogically fine-tuned AI tutoring systems may play a promising role in delivering effective, individualized learning support at scale.

</details>

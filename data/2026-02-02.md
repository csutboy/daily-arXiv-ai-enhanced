<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 55]
- [stat.AP](#stat.AP) [Total: 2]
- [cs.CY](#cs.CY) [Total: 11]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.SI](#cs.SI) [Total: 3]
- [econ.EM](#econ.EM) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [JAF: Judge Agent Forest](https://arxiv.org/abs/2601.22269)
*Sahil Garg,Brad Cheezum,Sridhar Dutta,Vishal Agarwal*

Main category: cs.AI

TL;DR: JAF框架让评估代理通过联合推理多个查询-响应对进行整体学习，而非孤立评估，结合信念传播和集成学习原理，使用灵活的LSH算法选择多样化示例，在云配置错误分类任务中验证有效。


<details>
  <summary>Details</summary>
Motivation: 传统评估代理孤立评估每个查询-响应对，无法利用跨实例的模式和不一致性信息。需要将评估代理从局部评估者提升为整体学习者，通过同时评估相关响应来发现交叉模式，从而提供更有效的反馈。

Method: 提出JAF框架：1）评估代理对主要代理生成的查询-响应对队列进行联合推理；2）结合信念传播和集成学习原理，通过重叠的上下文邻域构建知识图结构传播批评；3）开发灵活的LSH算法，集成语义嵌入、LLM驱动的哈希谓词、分类标签监督和侧信息，生成信息丰富的二进制代码，支持高效、可解释、关系感知的多样化示例选择。

Result: 在大型云环境中的云配置错误分类这一具有挑战性的任务上进行了实证研究验证。JAF框架能够通过联合推理和多样化示例选择，优化思维链推理路径的探索，提升评估效果。

Conclusion: JAF框架成功将评估代理从局部评估者转变为整体学习者，通过联合推理和灵活的LSH算法，实现了更有效的评估和反馈机制，为智能体AI框架提供了更强大的评估能力。

Abstract: Judge agents are fundamental to agentic AI frameworks: they provide automated evaluation, and enable iterative self-refinement of reasoning processes. We introduce JAF: Judge Agent Forest, a framework in which the judge agent conducts joint inference across a cohort of query--response pairs generated by a primary agent, rather than evaluating each in isolation. This paradigm elevates the judge from a local evaluator to a holistic learner: by simultaneously assessing related responses, the judge discerns cross-instance patterns and inconsistencies, whose aggregate feedback enables the primary agent to improve by viewing its own outputs through the judge's collective perspective.
  Conceptually, JAF bridges belief propagation and ensemble-learning principles: overlapping in-context neighborhoods induce a knowledge-graph structure that facilitates propagation of critique, and repeated, randomized evaluations yield a robust ensemble of context-sensitive judgments. JAF can be instantiated entirely via ICL, with the judge prompted for each query using its associated primary-agent response plus a small, possibly noisy set of peer exemplars. While kNN in embedding space is a natural starting point for exemplars, this approach overlooks categorical structure, domain metadata, or nuanced distinctions accessible to modern LLMs.
  To overcome these limitations, we develop a flexible locality-sensitive hashing (LSH) algorithm that learns informative binary codes by integrating semantic embeddings, LLM-driven hash predicates, supervision from categorical labels, and relevant side information. These hash codes support efficient, interpretable, and relation-aware selection of diverse exemplars, and further optimize exploration of CoT reasoning paths. We validate JAF with an empirical study on the demanding task of cloud misconfigs triage in large-scale cloud environments.

</details>


### [2] [The Six Sigma Agent: Achieving Enterprise-Grade Reliability in LLM Systems Through Consensus-Driven Decomposed Execution](https://arxiv.org/abs/2601.22290)
*Khush Patel,Siva Surendira,Jithin George,Shreyas Kapale*

Main category: cs.AI

TL;DR: 提出Six Sigma Agent架构，通过任务分解、微代理并行采样和共识投票，实现企业级可靠性，将错误率从5%降至0.11%，成本降低80%


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然能力强大但本质上是概率性的，在企业部署中存在可靠性挑战。需要解决LLM在企业应用中的可靠性问题，而不只是依赖模型规模扩展。

Method: 1) 将任务分解为原子动作的依赖树；2) 微代理采样：每个任务在多个不同LLM上并行执行n次生成独立输出；3) 共识投票与动态缩放：聚类输出并从获胜集群中选择答案。理论证明采样n个独立输出可将系统错误降至O(p^{ceil(n/2)})。

Result: 使用错误率5%的廉价模型，5个代理的共识投票可将错误率降至0.11%；动态缩放至13个代理可达到3.4 DPMO（六西格玛标准）。在三个企业用例中，可靠性比单代理执行提高14,700倍，同时成本降低80%。

Conclusion: AI系统的可靠性源于原则性的冗余和共识机制，而非仅靠模型规模扩展。Six Sigma Agent架构为企业级AI部署提供了可靠的解决方案。

Abstract: Large Language Models demonstrate remarkable capabilities yet remain fundamentally probabilistic, presenting critical reliability challenges for enterprise deployment. We introduce the Six Sigma Agent, a novel architecture that achieves enterprise-grade reliability through three synergistic components: (1) task decomposition into a dependency tree of atomic actions; (2) micro-agent sampling where each task is executed n times in parallel across diverse LLMs to generate independent outputs; and (3) consensus voting with dynamic scaling, clustering outputs and selecting the answer from the winning cluster with maximum votes. We prove that sampling n independent outputs with error rate p achieves system error O(p^{ceil(n/2)}), enabling exponential reliability gains. Even using cheaper models with 5% per-action error, consensus voting with 5 agents reduces error to 0.11%; dynamic scaling to 13 agents achieves 3.4 DPMO (Defects Per Million Opportunities), the Six Sigma standard. Evaluation across three enterprise use cases demonstrates a 14,700x reliability improvement over single-agent execution while reducing costs by 80%. Our work establishes that reliability in AI systems emerges from principled redundancy and consensus rather than model scaling alone.

</details>


### [3] [Scaling Multiagent Systems with Process Rewards](https://arxiv.org/abs/2601.23228)
*Ed Li,Junyu Ren,Cat Yan*

Main category: cs.AI

TL;DR: MAPPA：通过AI反馈的每动作过程奖励来微调多智能体系统，解决信用分配和样本效率问题，在数学竞赛和数据分析任务上显著提升性能


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在处理复杂任务方面有潜力，但微调多个智能体面临两个关键挑战：(1) 智能体间的信用分配问题，(2) 昂贵的多智能体rollout的样本效率问题

Method: 提出MAPPA方法，使用AI反馈为每个智能体动作提供过程奖励，而不是仅在任务完成时给予奖励。这种方法能够为单个动作分配信用，实现细粒度监督，无需真实标签，同时从每个rollout中提取最大训练信号

Result: 在数学竞赛问题上，MAPPA在AIME上提升5.0-17.5个百分点，在AMC上提升7.8-17.2个百分点。在数据分析任务上，成功率提高12.5个百分点，质量指标提升高达30%

Conclusion: 通过解决信用分配和样本效率挑战，这项工作为在复杂、长视野任务中扩展多智能体系统迈出了第一步，同时最小化人类监督需求

Abstract: While multiagent systems have shown promise for tackling complex tasks via specialization, finetuning multiple agents simultaneously faces two key challenges: (1) credit assignment across agents, and (2) sample efficiency of expensive multiagent rollouts. In this work, we propose finetuning multiagent systems with per-action process rewards from AI feedback (MAPPA) to address both. Through assigning credit to individual agent actions rather than only at task completion, MAPPA enables fine-grained supervision without ground truth labels while extracting maximal training signal from each rollout. We demonstrate our approach on competition math problems and tool-augmented data analysis tasks. On unseen math problems, MAPPA achieves +5.0--17.5pp on AIME and +7.8--17.2pp on AMC. For data analysis tasks, our method improves success rate by +12.5pp while quality metrics improve by up to 30%, validating that per-action supervision can lead to improvements across different multiagent system on various domains. By addressing these challenges, our work takes a first step toward scaling multiagent systems for complex, long-horizon tasks with minimal human supervision.

</details>


### [4] [Why Reasoning Fails to Plan: A Planning-Centric Analysis of Long-Horizon Decision Making in LLM Agents](https://arxiv.org/abs/2601.22311)
*Zehong Wang,Fang Wu,Hongru Wang,Xiangru Tang,Bolian Li,Zhenfei Yin,Yijun Ma,Yiyang Li,Weixiang Sun,Xiusi Chen,Yanfang Ye*

Main category: cs.AI

TL;DR: 论文提出FLARE方法，通过未来感知规划解决LLM代理在长时程规划中的短视问题，使早期决策考虑下游后果，显著提升任务性能。


<details>
  <summary>Details</summary>
Motivation: LLM代理在短时程推理中表现良好，但在长时程规划中经常失败。这是因为逐步推理诱导的逐步贪婪策略适合短时程，但无法处理需要早期行动考虑延迟后果的长时程规划。

Method: 提出FLARE（Future-aware Lookahead with Reward Estimation）方法，通过最小化实现未来感知规划，强制执行显式前瞻、价值传播和有限承诺，使下游结果能够影响早期决策。

Result: 在多个基准测试、代理框架和LLM骨干网络中，FLARE一致地提高了任务性能和规划级行为。使用FLARE的LLaMA-8B经常能够超越使用标准逐步推理的GPT-4o。

Conclusion: 研究结果明确了推理与规划之间的区别，表明通过未来感知规划方法可以有效解决LLM代理在长时程规划中的短视问题。

Abstract: Large language model (LLM)-based agents exhibit strong step-by-step reasoning capabilities over short horizons, yet often fail to sustain coherent behavior over long planning horizons. We argue that this failure reflects a fundamental mismatch: step-wise reasoning induces a form of step-wise greedy policy that is adequate for short horizons but fails in long-horizon planning, where early actions must account for delayed consequences. From this planning-centric perspective, we study LLM-based agents in deterministic, fully structured environments with explicit state transitions and evaluation signals. Our analysis reveals a core failure mode of reasoning-based policies: locally optimal choices induced by step-wise scoring lead to early myopic commitments that are systematically amplified over time and difficult to recover from. We introduce FLARE (Future-aware Lookahead with Reward Estimation) as a minimal instantiation of future-aware planning to enforce explicit lookahead, value propagation, and limited commitment in a single model, allowing downstream outcomes to influence early decisions. Across multiple benchmarks, agent frameworks, and LLM backbones, FLARE consistently improves task performance and planning-level behavior, frequently allowing LLaMA-8B with FLARE to outperform GPT-4o with standard step-by-step reasoning. These results establish a clear distinction between reasoning and planning.

</details>


### [5] [Sparks of Rationality: Do Reasoning LLMs Align with Human Judgment and Choice?](https://arxiv.org/abs/2601.22329)
*Ala N. Tak,Amin Banayeeanzade,Anahita Bolourani,Fatemeh Bahrani,Ashutosh Chaubey,Sai Praneeth Karimireddy,Norbert Schwarz,Jonathan Gratch*

Main category: cs.AI

TL;DR: LLMs在理性决策和情感偏见方面表现出类似人类的模式，理性思考提升决策质量但也会放大情感干预的影响，不同情感引导方法在可控性和人性化行为之间存在权衡。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs越来越多地应用于招聘、医疗和经济决策等高风险领域，需要评估它们是否表现出类似人类的（非）理性模式和偏见，这对于LLMs作为人类行为模型或决策系统的安全部署至关重要。

Method: 评估多个LLM家族在：（1）理性选择核心公理的基准测试；（2）行为经济学和社会规范中情感影响判断的经典决策领域。使用两种情感引导方法：上下文提示（ICP）和表示层引导（RLS）来探究情感扭曲及其与推理的相互作用。

Result: 深思熟虑的"思考"可靠地提高理性并推动模型向期望价值最大化发展。ICP产生强烈但难以校准的方向性偏移，而RLS产生更符合心理学规律但可靠性较低的模式。提高理性的机制也会放大对情感干预的敏感性。

Conclusion: 推理和情感引导之间存在张力，这对人类行为模拟和LLM决策系统的安全部署都有重要影响。不同引导方法在可控性和人性化行为之间存在权衡，需要谨慎平衡。

Abstract: Large Language Models (LLMs) are increasingly positioned as decision engines for hiring, healthcare, and economic judgment, yet real-world human judgment reflects a balance between rational deliberation and emotion-driven bias. If LLMs are to participate in high-stakes decisions or serve as models of human behavior, it is critical to assess whether they exhibit analogous patterns of (ir)rationalities and biases. To this end, we evaluate multiple LLM families on (i) benchmarks testing core axioms of rational choice and (ii) classic decision domains from behavioral economics and social norms where emotions are known to shape judgment and choice. Across settings, we show that deliberate "thinking" reliably improves rationality and pushes models toward expected-value maximization. To probe human-like affective distortions and their interaction with reasoning, we use two emotion-steering methods: in-context priming (ICP) and representation-level steering (RLS). ICP induces strong directional shifts that are often extreme and difficult to calibrate, whereas RLS produces more psychologically plausible patterns but with lower reliability. Our results suggest that the same mechanisms that improve rationality also amplify sensitivity to affective interventions, and that different steering methods trade off controllability against human-aligned behavior. Overall, this points to a tension between reasoning and affective steering, with implications for both human simulation and the safe deployment of LLM-based decision systems.

</details>


### [6] [Learning Provably Correct Distributed Protocols Without Human Knowledge](https://arxiv.org/abs/2601.22369)
*Yujie Hui,Xiaoyi Lu,Andrew Perrault,Yang Wang*

Main category: cs.AI

TL;DR: GGMS是一个用于学习可证明正确分布式协议的学习框架，它结合了蒙特卡洛树搜索、Transformer编码器、全局深度优先搜索和模型检查器反馈，能够自动搜索满足SMT规范的正确协议。


<details>
  <summary>Details</summary>
Motivation: 设计可证明正确的分布式协议非常困难且耗时，传统方法难以在多智能体不完全信息博弈中学习到正确的协议，即使智能体数量很少。

Method: 将协议设计形式化为不完全信息博弈中的策略搜索问题，使用SMT指定正确性条件。提出GGMS框架：结合蒙特卡洛树搜索变体、Transformer动作编码器、全局深度优先搜索来跳出局部最优，以及模型检查器的重复反馈。

Result: GGMS能够学习比现有方法更大规模设置下的正确协议，输出的协议经过有界设置下的穷举模型检查验证正确性，并在温和假设下证明了搜索过程的完备性。

Conclusion: GGMS为自动设计可证明正确的分布式协议提供了一个有效的学习框架，能够处理传统方法难以解决的多智能体不完全信息博弈问题，并具有理论上的完备性保证。

Abstract: Provably correct distributed protocols, which are a critical component of modern distributed systems, are highly challenging to design and have often required decades of human effort. These protocols allow multiple agents to coordinate to come to a common agreement in an environment with uncertainty and failures. We formulate protocol design as a search problem over strategies in a game with imperfect information, and the desired correctness conditions are specified in Satisfiability Modulo Theories (SMT). However, standard methods for solving multi-agent games fail to learn correct protocols in this setting, even when the number of agents is small. We propose a learning framework, GGMS, which integrates a specialized variant of Monte Carlo Tree Search with a transformer-based action encoder, a global depth-first search to break out of local minima, and repeated feedback from a model checker. Protocols output by GGMS are verified correct via exhaustive model checking for all executions within the bounded setting. We further prove that, under mild assumptions, the search process is complete: if a correct protocol exists, GGMS will eventually find it. In experiments, we show that GGMS can learn correct protocols for larger settings than existing methods.

</details>


### [7] [Semi-Autonomous Mathematics Discovery with Gemini: A Case Study on the Erdős Problems](https://arxiv.org/abs/2601.22401)
*Tony Feng,Trieu Trinh,Garrett Bingham,Jiwon Kang,Shengtong Zhang,Sang-hyun Kim,Kevin Barreto,Carl Schildkraut,Junehyuk Jung,Jaehyeon Seo,Carlo Pagano,Yuri Chervonyi,Dawsen Hwang,Kaiying Hou,Sergei Gukov,Cheng-Chiang Tsai,Hyunwoo Choi,Youngbeom Jin,Wei-Yuan Li,Hao-An Wu,Ruey-An Shiu,Yu-Sheng Shih,Quoc V. Le,Thang Luong*

Main category: cs.AI

TL;DR: 使用Gemini AI系统评估700个Erdős问题中的开放猜想，通过AI验证和人工专家评估相结合的方法，解决了13个标记为"开放"的问题，发现这些问题的开放状态更多是由于文献难以查找而非问题本身困难。


<details>
  <summary>Details</summary>
Motivation: 探索AI在半自动数学发现中的应用潜力，特别是如何利用AI系统处理大规模数学猜想数据库，评估AI在数学问题解决中的实际效果和局限性。

Method: 采用混合方法：首先使用Gemini AI进行自然语言验证以缩小搜索范围，然后由人类专家评估正确性和新颖性。针对Bloom的Erdős问题数据库中标记为"开放"的700个猜想进行系统评估。

Result: 解决了13个标记为"开放"的问题：其中5个通过看似新颖的自主解决方案，8个通过识别现有文献中的先前解决方案。发现这些问题的"开放"状态更多是由于文献难以查找（obscurity）而非问题本身困难。

Conclusion: AI在数学猜想评估中具有应用价值，但面临文献识别困难和"潜意识剽窃"风险等挑战。Erdős问题的开放状态往往反映文献可访问性问题而非数学难度，AI辅助方法有助于揭示这类问题。

Abstract: We present a case study in semi-autonomous mathematics discovery, using Gemini to systematically evaluate 700 conjectures labeled 'Open' in Bloom's Erdős Problems database. We employ a hybrid methodology: AI-driven natural language verification to narrow the search space, followed by human expert evaluation to gauge correctness and novelty. We address 13 problems that were marked 'Open' in the database: 5 through seemingly novel autonomous solutions, and 8 through identification of previous solutions in the existing literature. Our findings suggest that the 'Open' status of the problems was through obscurity rather than difficulty. We also identify and discuss issues arising in applying AI to math conjectures at scale, highlighting the difficulty of literature identification and the risk of ''subconscious plagiarism'' by AI. We reflect on the takeaways from AI-assisted efforts on the Erdős Problems.

</details>


### [8] [AI-Enabled Waste Classification as a Data-Driven Decision Support Tool for Circular Economy and Urban Sustainability](https://arxiv.org/abs/2601.22418)
*Julius Sechang Mboli,Omolara Aderonke Ogungbemi*

Main category: cs.AI

TL;DR: 该研究评估了传统机器学习与深度学习模型在垃圾图像二分类任务上的表现，发现DenseNet121达到最高准确率91%，并探讨了PCA对传统模型的影响以及模型在实时决策支持系统中的应用。


<details>
  <summary>Details</summary>
Motivation: 实现高效的垃圾分类对于智慧城市中的循环经济和资源回收至关重要，需要开发准确的自动化分类系统来减少填埋使用和环境影响。

Method: 使用25,077张垃圾图像（80/20训练/测试分割，调整至150x150像素并进行数据增强），评估传统机器学习（随机森林、SVM、AdaBoost）和深度学习模型（定制CNN、VGG16、ResNet50）以及三种迁移学习模型（DenseNet121、EfficientNetB0、InceptionV3），同时评估PCA对传统模型的降维效果。

Result: DenseNet121达到最高准确率91%和ROC-AUC 0.98，比最佳传统分类器高出20个百分点。PCA对传统方法改善有限，而迁移学习在数据有限条件下显著提升性能。

Conclusion: 迁移学习模型特别是DenseNet121在垃圾分类任务上表现最佳，可集成到实时数据驱动决策支持系统中，实现自动化垃圾分类，有望减少填埋使用和生命周期环境影响。

Abstract: Efficient waste sorting is crucial for enabling circular-economy practices and resource recovery in smart cities. This paper evaluates both traditional machine-learning (Random Forest, SVM, AdaBoost) and deep-learning techniques including custom CNNs, VGG16, ResNet50, and three transfer-learning models (DenseNet121, EfficientNetB0, InceptionV3) for binary classification of 25 077 waste images (80/20 train/test split, augmented and resized to 150x150 px). The paper assesses the impact of Principal Component Analysis for dimensionality reduction on traditional models. DenseNet121 achieved the highest accuracy (91 %) and ROC-AUC (0.98), outperforming the best traditional classifier by 20 pp. Principal Component Analysis (PCA) showed negligible benefit for classical methods, whereas transfer learning substantially improved performance under limited-data conditions. Finally, we outline how these models integrate into a real-time Data-Driven Decision Support System for automated waste sorting, highlighting potential reductions in landfill use and lifecycle environmental impacts.)

</details>


### [9] [When LLM meets Fuzzy-TOPSIS for Personnel Selection through Automated Profile Analysis](https://arxiv.org/abs/2601.22433)
*Shahria Hoque,Ahmed Akib Jawad Karim,Md. Golam Rabiul Alam,Nirjhar Gope*

Main category: cs.AI

TL;DR: 本研究提出LLM-TOPSIS框架，结合大型语言模型与模糊TOPSIS方法，自动化评估和排名软件工程求职者，准确率高达91%。


<details>
  <summary>Details</summary>
Motivation: 在高度竞争的就业环境中，选择合适的员工对组织成功至关重要。传统招聘过程存在主观性、模糊性和偏见问题，需要更客观、可扩展的自动化解决方案。

Method: 1) 创建包含LinkedIn资料（教育、工作经验、技能、自我介绍）和专家评估的数据集；2) 开发LLM-TOPSIS框架，结合大型语言模型与模糊TOPSIS方法；3) 使用三角模糊数处理评估中的模糊性；4) 微调DistilRoBERTa模型并与模糊TOPSIS集成进行候选人排名。

Result: 系统排名与人类专家评估高度一致，在Experience属性和Overall属性上达到91%的准确率。证明了NLP驱动框架能提高招聘的可扩展性、一致性并减少偏见。

Conclusion: 研究展示了NLP与模糊决策方法结合在人员选拔中的潜力，能提供可扩展且无偏见的招聘解决方案。未来将扩展数据集、提高模型可解释性，并在实际招聘场景中验证系统。

Abstract: In this highly competitive employment environment, the selection of suitable personnel is essential for organizational success. This study presents an automated personnel selection system that utilizes sophisticated natural language processing (NLP) methods to assess and rank software engineering applicants. A distinctive dataset was created by aggregating LinkedIn profiles that include essential features such as education, work experience, abilities, and self-introduction, further enhanced with expert assessments to function as standards. The research combines large language models (LLMs) with multicriteria decision-making (MCDM) theory to develop the LLM-TOPSIS framework. In this context, we utilized the TOPSIS method enhanced by fuzzy logic (Fuzzy TOPSIS) to address the intrinsic ambiguity and subjectivity in human assessments. We utilized triangular fuzzy numbers (TFNs) to describe criteria weights and scores, thereby addressing the ambiguity frequently encountered in candidate evaluations. For candidate ranking, the DistilRoBERTa model was fine-tuned and integrated with the fuzzy TOPSIS method, achieving rankings closely aligned with human expert evaluations and attaining an accuracy of up to 91% for the Experience attribute and the Overall attribute. The study underlines the potential of NLP-driven frameworks to improve recruitment procedures by boosting scalability, consistency, and minimizing prejudice. Future endeavors will concentrate on augmenting the dataset, enhancing model interpretability, and verifying the system in actual recruitment scenarios to better evaluate its practical applicability. This research highlights the intriguing potential of merging NLP with fuzzy decision-making methods in personnel selection, enabling scalable and unbiased solutions to recruitment difficulties.

</details>


### [10] [Anytime Safe PAC Efficient Reasoning](https://arxiv.org/abs/2601.22446)
*Chengyao Yu,Hao Zeng,Youxin Zhu,Jianguo Huang,Huajun Zeng,Bingyi Jing*

Main category: cs.AI

TL;DR: 提出B-PAC推理方法，通过动态调整路由阈值，在部分反馈的在线环境中实现安全高效的大模型推理，减少81.01%的计算开销同时控制性能损失


<details>
  <summary>Details</summary>
Motivation: 大型推理模型计算成本高、延迟大，现有选择性思考策略在在线环境中存在不可控错误，因为非思考模型的性能损失只能部分观测且数据非平稳

Method: 使用逆倾向评分估计器构建候选阈值的测试超鞅，基于累积统计证据动态调整路由阈值，实现任意时间有效的性能损失控制

Result: B-PAC推理显著降低计算开销，减少思考模型使用达81.01%，同时将性能损失控制在用户指定水平以下

Conclusion: B-PAC推理为在线环境中的大模型推理提供了安全高效的解决方案，实现了理论保证的任意时间性能损失控制

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex tasks but suffer from high computational costs and latency. While selective thinking strategies improve efficiency by routing easy queries to non-thinking models, existing approaches often incur uncontrollable errors, especially in online settings where the performance loss of a non-thinking model is only partially observed and data are non-stationary. To address this, we propose Betting Probably Approximately Correct (B-PAC) reasoning, a principled method that enables anytime safe and efficient online reasoning under partial feedback. Specifically, we utilize inverse propensity scoring estimators to construct test supermartingales for candidate thresholds, and then dynamically adjust the routing threshold based on the accumulated statistical evidence of safety. Theoretically, we establish the anytime-valid performance loss control and the efficiency of B-PAC reasoning. Extensive experiments demonstrate that B-PAC reasoning significantly reduces computational overhead, decreasing thinking model usage by up to 81.01\%, while controlling the performance loss below the user-specified level.

</details>


### [11] [Controllable Information Production](https://arxiv.org/abs/2601.22449)
*Tristan Shah,Stas Tiomkin*

Main category: cs.AI

TL;DR: 提出了一种新的内在动机原则——可控信息生产(CIP)，它避免了外部效用和设计者指定的变量，通过开放环与闭环Kolmogorov-Sinai熵的差距来同时奖励对混沌的追求和调节。


<details>
  <summary>Details</summary>
Motivation: 现有基于信息传输的内在动机方法明确依赖于设计者选择哪些随机变量参与传输，存在设计者偏好的问题。需要一种既不需要外部效用，也不依赖设计者指定变量的内在动机原则。

Method: 从最优控制理论推导出CIP目标，将其表示为开放环与闭环Kolmogorov-Sinai熵之间的差距。这种方法同时奖励对混沌的追求（信息生产）和调节（可控性）。

Result: 建立了CIP的关键理论性质，并在标准内在动机基准测试中证明了其有效性。CIP连接了外在行为和内在行为，提供了一种新的信息论内在动机框架。

Conclusion: CIP是一种新颖的内在动机原则，避免了传统方法的局限性，通过信息生产与可控性的平衡为智能行为生成提供了新的理论基础。

Abstract: Intrinsic Motivation (IM) is a paradigm for generating intelligent behavior without external utilities. The existing information-theoretic methods for IM are predominantly based on information transmission, which explicitly depends on the designer's choice of which random variables engage in transmission. In this work, we introduce a novel IM principle, Controllable Information Production (CIP), that avoids both external utilities and designer-specified variables. We derive the CIP objective from Optimal Control, showing a connection between extrinsic and intrinsic behaviors. CIP appears as the gap between open-loop and closed-loop Kolmogorov-Sinai entropies, which simultaneously rewards the pursuit and regulation of chaos. We establish key theoretical properties of CIP and demonstrate its effectiveness on standard IM benchmarks.

</details>


### [12] [Why Self-Rewarding Works: Theoretical Guarantees for Iterative Alignment of Language Models](https://arxiv.org/abs/2601.22513)
*Shi Fu,Yingjie Wang,Shengchao Hu,Peng Wang,Dacheng Tao*

Main category: cs.AI

TL;DR: 本文首次为自奖励语言模型(SRLMs)提供了严格的理论保证，揭示了其成功的关键机制：通过指数衰减对初始模型的依赖，实现鲁棒的自我改进。


<details>
  <summary>Details</summary>
Motivation: 自奖励语言模型在无需外部反馈的情况下通过迭代改进对齐取得了显著成功，但其核心机制缺乏理论解释，存在关键的理论理解空白。

Method: 首先建立单步更新的基本限制下界，然后推导完整迭代范式的有限样本误差界，最后在线性softmax模型类中实例化理论框架。

Result: 性能随样本量n以$\widetilde{\mathcal{O}}\left(1/\sqrt{n}\right)$速率提升，对初始模型的依赖随迭代次数T指数衰减，解释了自奖励成功的原因。

Conclusion: 自奖励语言模型通过将动态引导向内部稳定性和一致性，能够鲁棒地克服不良初始化，这为其实践成功提供了正式解释。

Abstract: Self-Rewarding Language Models (SRLMs) achieve notable success in iteratively improving alignment without external feedback. Yet, despite their striking empirical progress, the core mechanisms driving their capabilities remain unelucidated, leaving a critical gap in theoretical understanding. This paper provides the first rigorous theoretical guarantees for SRLMs. We first establish a lower bound that characterizes the fundamental limits of a single update step, revealing a critical dependence on the quality of the initial model. We then derive finite-sample error bounds for the full iterative paradigm, showing that performance improves at a rate of $\widetilde{\mathcal{O}}\left(1/\sqrt{n}\right)$ with sample size $n$. Crucially, our analysis reveals that the dependence on the initial model decays exponentially with the number of iterations $T$. This provides a formal explanation for why self-rewarding succeeds: it robustly overcomes poor initialization by steering the dynamics toward internal stability and consistency. Finally, we instantiate our theoretical framework for the linear softmax model class, yielding tailored guarantees that connect our high-level insights to practical model architectures.

</details>


### [13] [Darwinian Memory: A Training-Free Self-Regulating Memory System for GUI Agent Evolution](https://arxiv.org/abs/2601.22528)
*Hongze Mi,Yibo Feng,WenJie Lu,Song Cao,Jinyuan Li,Yanming Li,Xuelin Zhang,Haotian Luo,Songyang Peng,He Cui,Tengfei Tian,Jun Fang,Hua Chai,Naiqiang Tan*

Main category: cs.AI

TL;DR: 提出Darwinian Memory System (DMS)，一种自进化的记忆系统，用于解决MLLM智能体在GUI自动化中处理长时跨应用任务时的上下文限制问题，通过进化压力提升策略质量。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型智能体在GUI自动化中面临长时跨应用任务的挑战，主要受限于上下文窗口。现有记忆系统难以适应动态GUI环境，存在意图与执行粒度不匹配、上下文污染等问题，导致智能体产生幻觉。

Method: 提出Darwinian Memory System (DMS)，一种自进化架构，将记忆构建为受"适者生存"法则支配的动态生态系统。将复杂轨迹分解为独立可重用单元，实现组合灵活性；采用效用驱动的自然选择机制追踪生存价值，主动修剪次优路径并抑制高风险计划。

Result: 在真实世界多应用基准测试中，DMS无需训练成本或架构开销即可提升通用MLLM性能，平均成功率提升18.0%，执行稳定性提升33.9%，同时降低任务延迟。

Conclusion: DMS是一种有效的自进化记忆系统，通过进化压力促使智能体推导出更优策略，成功解决了GUI任务中记忆系统的适应性问题。

Abstract: Multimodal Large Language Model (MLLM) agents facilitate Graphical User Interface (GUI) automation but struggle with long-horizon, cross-application tasks due to limited context windows. While memory systems provide a viable solution, existing paradigms struggle to adapt to dynamic GUI environments, suffering from a granularity mismatch between high-level intent and low-level execution, and context pollution where the static accumulation of outdated experiences drives agents into hallucination. To address these bottlenecks, we propose the Darwinian Memory System (DMS), a self-evolving architecture that constructs memory as a dynamic ecosystem governed by the law of survival of the fittest. DMS decomposes complex trajectories into independent, reusable units for compositional flexibility, and implements Utility-driven Natural Selection to track survival value, actively pruning suboptimal paths and inhibiting high-risk plans. This evolutionary pressure compels the agent to derive superior strategies. Extensive experiments on real-world multi-app benchmarks validate that DMS boosts general-purpose MLLMs without training costs or architectural overhead, achieving average gains of 18.0% in success rate and 33.9% in execution stability, while reducing task latency, establishing it as an effective self-evolving memory system for GUI tasks.

</details>


### [14] [Enhancing TableQA through Verifiable Reasoning Trace Reward](https://arxiv.org/abs/2601.22530)
*Tung Sum Thomas Kwok,Xinyu Wang,Hengzhi He,Xiaofeng Lin,Peng Lu,Liheng Ma,Chunhe Wang,Ying Nian Wu,Lei Ding,Guang Cheng*

Main category: cs.AI

TL;DR: RE-Tab是一个用于TableQA的即插即用框架，通过轻量级、无需训练的奖励建模来增强轨迹搜索，在表格转换中提供明确的反馈奖励，显著提升问答准确率并降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 训练TableQA代理面临的主要挑战是答案无法从静态输入推断，而需要通过表格状态的分步转换进行推理，这引入了多步推理复杂性和环境交互。研究问题是：对表格转换动作提供明确反馈能否提升模型推理能力？

Method: 提出RE-Tab框架，将问题建模为部分可观测马尔可夫决策过程，通过轻量级、无需训练的奖励建模来增强轨迹搜索。在状态转换（"什么是最佳动作？"）和模拟推理（"我对输出确定吗？"）阶段提供明确的可验证奖励，引导代理在表格状态中的导航。

Result: RE-Tab在TableQA中实现了最先进的性能，推理成本降低近25%。即插即用实现带来高达41.77%的QA准确率提升和33.33%的测试时推理样本减少。在各种LLM和最先进基准测试中均显示一致的改进模式。

Conclusion: 在表格转换中通过奖励反馈强制分步推理对提升TableQA性能至关重要。RE-Tab框架展示了良好的泛化能力，能够显著提高问答准确率并降低推理成本。

Abstract: A major challenge in training TableQA agents, compared to standard text- and image-based agents, is that answers cannot be inferred from a static input but must be reasoned through stepwise transformations of the table state, introducing multi-step reasoning complexity and environmental interaction. This leads to a research question: Can explicit feedback on table transformation action improve model reasoning capability? In this work, we introduce RE-Tab, a plug-and-play framework that architecturally enhances trajectory search via lightweight, training-free reward modeling by formulating the problem as a Partially Observable Markov Decision Process. We demonstrate that providing explicit verifiable rewards during State Transition (``What is the best action?'') and Simulative Reasoning (``Am I sure about the output?'') is crucial to steer the agent's navigation in table states. By enforcing stepwise reasoning with reward feedback in table transformations, RE-Tab achieves state-of-the-art performance in TableQA with almost 25\% drop in inference cost. Furthermore, a direct plug-and-play implementation of RE-Tab brings up to 41.77% improvement in QA accuracy and 33.33% drop in test-time inference samples for consistent answer. Consistent improvement pattern across various LLMs and state-of-the-art benchmarks further confirms RE-Tab's generalisability. The repository is available at https://github.com/ThomasK1018/RE_Tab .

</details>


### [15] [Decoding in Geometry: Alleviating Embedding-Space Crowding for Complex Reasoning](https://arxiv.org/abs/2601.22536)
*Yixin Yang,Qingxiu Dong,Zhifang Sui*

Main category: cs.AI

TL;DR: 论文提出CraEG方法，通过几何引导重加权缓解嵌入空间拥挤现象，提升LLM推理性能


<details>
  <summary>Details</summary>
Motivation: 现有基于温度和截断的解码方法仅操作token概率，忽略了嵌入空间中token之间的细粒度几何关系。作者发现嵌入空间拥挤现象，即下一个token分布集中在嵌入空间中几何相近的token上，且与数学问题解决中的推理成功存在统计关联。

Method: 提出CraEG方法，这是一种即插即用的采样方法，通过几何引导重加权来缓解嵌入空间拥挤现象。该方法无需训练、单次通过，且与标准采样策略兼容。

Result: 在多个模型和基准测试上的实验表明，CraEG改善了生成性能，在鲁棒性和多样性指标上均有提升。

Conclusion: 嵌入空间拥挤是影响LLM推理的重要现象，通过几何引导的采样方法CraEG可以有效缓解这一问题，提升模型在复杂推理任务中的表现。

Abstract: Sampling-based decoding underlies complex reasoning in large language models (LLMs), where decoding strategies critically shape model behavior. Temperature- and truncation-based methods reshape the next-token distribution through global probability reweighting or thresholding to balance the quality-diversity tradeoff. However, they operate solely on token probabilities, ignoring fine-grained relationships among tokens in the embedding space. We uncover a novel phenomenon, embedding-space crowding, where the next-token distribution concentrates its probability mass on geometrically close tokens in the embedding space. We quantify crowding at multiple granularities and find a statistical association with reasoning success in mathematical problem solving. Motivated by this finding, we propose CraEG, a plug-and-play sampling method that mitigates crowding through geometry-guided reweighting. CraEG is training-free, single-pass, and compatible with standard sampling strategies. Experiments on multiple models and benchmarks demonstrate improved generation performance, with gains in robustness and diversity metrics.

</details>


### [16] [PerfGuard: A Performance-Aware Agent for Visual Content Generation](https://arxiv.org/abs/2601.22571)
*Zhipeng Chen,Zhongrui Zhang,Chao Zhang,Yifan Xu,Lan Yang,Jun Liu,Ke Li,Yi-Zhe Song*

Main category: cs.AI

TL;DR: PerfGuard是一个性能感知的智能体框架，专门用于视觉内容生成任务，通过建模工具性能边界来改进任务规划和调度，相比现有方法在工具选择准确性、执行可靠性和用户意图对齐方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的智能体框架通常假设工具执行总是成功的，仅依赖文本描述而无法区分精确的性能边界，也不能适应迭代的工具更新。这在视觉内容生成等领域尤其成问题，因为细微的工具性能差异会显著影响结果质量。

Method: PerfGuard框架包含三个核心机制：1) 性能感知选择建模(PASM)，用基于细粒度性能评估的多维评分系统替代通用工具描述；2) 自适应偏好更新(APU)，通过比较理论排名与实际执行排名来动态优化工具选择；3) 能力对齐规划优化(CAPO)，引导规划器生成符合性能感知策略的子任务。

Result: 与最先进方法的实验比较表明，PerfGuard在工具选择准确性、执行可靠性和用户意图对齐方面具有优势，验证了其在复杂AIGC任务中的鲁棒性和实用价值。

Conclusion: PerfGuard通过系统建模工具性能边界并将其集成到任务规划和调度中，有效解决了现有LLM智能体框架在工具执行不确定性方面的问题，为视觉内容生成等领域的自动化任务处理提供了更可靠的解决方案。

Abstract: The advancement of Large Language Model (LLM)-powered agents has enabled automated task processing through reasoning and tool invocation capabilities. However, existing frameworks often operate under the idealized assumption that tool executions are invariably successful, relying solely on textual descriptions that fail to distinguish precise performance boundaries and cannot adapt to iterative tool updates. This gap introduces uncertainty in planning and execution, particularly in domains like visual content generation (AIGC), where nuanced tool performance significantly impacts outcomes. To address this, we propose PerfGuard, a performance-aware agent framework for visual content generation that systematically models tool performance boundaries and integrates them into task planning and scheduling. Our framework introduces three core mechanisms: (1) Performance-Aware Selection Modeling (PASM), which replaces generic tool descriptions with a multi-dimensional scoring system based on fine-grained performance evaluations; (2) Adaptive Preference Update (APU), which dynamically optimizes tool selection by comparing theoretical rankings with actual execution rankings; and (3) Capability-Aligned Planning Optimization (CAPO), which guides the planner to generate subtasks aligned with performance-aware strategies. Experimental comparisons against state-of-the-art methods demonstrate PerfGuard's advantages in tool selection accuracy, execution reliability, and alignment with user intent, validating its robustness and practical utility for complex AIGC tasks. The project code is available at https://github.com/FelixChan9527/PerfGuard.

</details>


### [17] [WED-Net: A Weather-Effect Disentanglement Network with Causal Augmentation for Urban Flow Prediction](https://arxiv.org/abs/2601.22586)
*Qian Hong,Siyuan Chang,Xiao Zhou*

Main category: cs.AI

TL;DR: WED-Net是一个用于极端天气条件下城市时空预测的双分支Transformer网络，通过解耦内在交通模式和天气诱导模式来提升预测鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在极端天气（如暴雨）下的城市时空预测存在挑战，因为事件罕见且动态性强。现有数据驱动方法通常使用粗粒度的天气描述符，缺乏捕捉细粒度时空效应的机制，且因果方法往往忽略时间动态或依赖固定的混杂因素分层。

Method: 提出WED-Net（Weather-Effect Disentanglement Network），采用双分支Transformer架构，通过自注意力和交叉注意力分离内在和天气诱导的交通模式，使用记忆库增强，并通过自适应门控融合。引入判别器明确区分天气条件以促进解耦，设计因果数据增强策略扰动非因果部分同时保留因果结构。

Result: 在三个城市的出租车流量数据集上的实验表明，WED-Net在极端天气条件下表现出鲁棒的性能，展示了其在支持更安全出行、灾害准备和城市韧性方面的潜力。

Conclusion: WED-Net通过解耦天气效应和内在交通模式，有效提升了极端天气条件下的城市时空预测能力，为实际应用中的安全出行和城市韧性提供了有力支持。

Abstract: Urban spatio-temporal prediction under extreme conditions (e.g., heavy rain) is challenging due to event rarity and dynamics. Existing data-driven approaches that incorporate weather as auxiliary input often rely on coarse-grained descriptors and lack dedicated mechanisms to capture fine-grained spatio-temporal effects. Although recent methods adopt causal techniques to improve out-of-distribution generalization, they typically overlook temporal dynamics or depend on fixed confounder stratification. To address these limitations, we propose WED-Net (Weather-Effect Disentanglement Network), a dual-branch Transformer architecture that separates intrinsic and weather-induced traffic patterns via self- and cross-attention, enhanced with memory banks and fused through adaptive gating. To further promote disentanglement, we introduce a discriminator that explicitly distinguishes weather conditions. Additionally, we design a causal data augmentation strategy that perturbs non-causal parts while preserving causal structures, enabling improved generalization under rare scenarios. Experiments on taxi-flow datasets from three cities demonstrate that WED-Net delivers robust performance under extreme weather conditions, highlighting its potential to support safer mobility, highlighting its potential to support safer mobility, disaster preparedness, and urban resilience in real-world settings. The code is publicly available at https://github.com/HQ-LV/WED-Net.

</details>


### [18] [Learn More with Less: Uncertainty Consistency Guided Query Selection for RLVR](https://arxiv.org/abs/2601.22595)
*Hao Yi,Yulan Hu,Xin Li,Sheng Ouyang,Lizhong Ding,Yong Liu*

Main category: cs.AI

TL;DR: 提出将主动学习引入RLVR框架，通过不确定性一致性度量解决传统主动学习策略在数学推理任务中的失效问题，显著减少所需查询量


<details>
  <summary>Details</summary>
Motivation: 现有RLVR算法需要大量查询导致标注成本高昂，希望探索是否能用更少但信息量更大的查询达到相似或更好的性能

Method: 提出不确定性一致性度量来评估主观不确定性与客观不确定性的一致性；离线场景使用点二列相关系数，在线训练引入基于归一化优势和主观不确定性的新变体

Result: 方法在实验中持续优于随机选择和传统主动学习基线，仅用30%数据就能达到全数据集性能，有效降低RLVR推理任务的成本

Conclusion: 通过主动学习和不确定性一致性度量，显著减少了RLVR所需的查询量，为数学推理任务提供了更高效的训练方法

Abstract: Large Language Models (LLMs) have recently improved mathematical reasoning through Reinforcement Learning with Verifiable Reward (RLVR). However, existing RLVR algorithms require large query budgets, making annotation costly. We investigate whether fewer but more informative queries can yield similar or superior performance, introducing active learning (AL) into RLVR. We identify that classic AL sampling strategies fail to outperform random selection in this setting, due to ignoring objective uncertainty when only selecting by subjective uncertainty. This work proposes an uncertainty consistency metric to evaluate how well subjective uncertainty aligns with objective uncertainty. In the offline setting, this alignment is measured using the Point-Biserial Correlation Coefficient (PBC). For online training, because of limited sampling and dynamically shifting output distributions, PBC estimation is difficult. Therefore, we introduce a new online variant, computed from normalized advantage and subjective uncertainty. Theoretically, we prove that the online variant is strictly negatively correlated with offline PBC and supports better sample selection. Experiments show our method consistently outperforms random and classic AL baselines, achieving full-dataset performance while training on only 30% of the data, effectively reducing the cost of RLVR for reasoning tasks.

</details>


### [19] [From Self-Evolving Synthetic Data to Verifiable-Reward RL: Post-Training Multi-turn Interactive Tool-Using Agents](https://arxiv.org/abs/2601.22607)
*Jiaxuan Gao,Jiaao Chen,Chuyi He,Wei-Chen Wang,Shusheng Xu,Hanrui Wang,Di Jin,Yi Wu*

Main category: cs.AI

TL;DR: 提出EigenData框架，结合自演化数据代理和验证器强化学习，用于训练复杂工具使用代理，无需昂贵人工标注


<details>
  <summary>Details</summary>
Motivation: 训练交互式工具使用代理面临挑战：高质量多轮工具使用数据难以规模化合成，强化学习可能因用户模拟噪声信号而效率低下

Method: 提出EigenData分层多代理引擎，合成工具接地对话及可执行检查器，通过闭环自演化过程更新提示和工作流；在此基础上开发RL配方，先微调用户模型，再应用GRPO风格训练，使用轨迹级组相对优势和动态过滤

Result: 在tau^2-bench上，最佳模型在Airline任务达到73.0% pass^1，在Telecom任务达到98.3% pass^1，匹配或超越前沿模型

Conclusion: 为引导复杂工具使用行为提供了一条可扩展路径，无需昂贵人工标注

Abstract: Interactive tool-using agents must solve real-world tasks via multi-turn interaction with both humans and external environments, requiring dialogue state tracking, multi-step tool execution, while following complex instructions. Post-training such agents is challenging because synthesis for high-quality multi-turn tool-use data is difficult to scale, and reinforcement learning (RL) could face noisy signals caused by user simulation, leading to degraded training efficiency. We propose a unified framework that combines a self-evolving data agent with verifier-based RL. Our system, EigenData, is a hierarchical multi-agent engine that synthesizes tool-grounded dialogues together with executable per-instance checkers, and improves generation reliability via closed-loop self-evolving process that updates prompts and workflow. Building on the synthetic data, we develop an RL recipe that first fine-tunes the user model and then applies GRPO-style training with trajectory-level group-relative advantages and dynamic filtering, yielding consistent improvements beyond SFT. Evaluated on tau^2-bench, our best model reaches 73.0% pass^1 on Airline and 98.3% pass^1 on Telecom, matching or exceeding frontier models. Overall, our results suggest a scalable pathway for bootstrapping complex tool-using behaviors without expensive human annotation.

</details>


### [20] [EntroCut: Entropy-Guided Adaptive Truncation for Efficient Chain-of-Thought Reasoning in Small-scale Large Reasoning Models](https://arxiv.org/abs/2601.22617)
*Hongxi Yan,Qingjie Liu,Yunhong Wang*

Main category: cs.AI

TL;DR: EntroCut：基于熵的动态截断方法，通过识别高置信度状态提前终止推理，减少大推理模型的计算开销


<details>
  <summary>Details</summary>
Motivation: 大推理模型依赖长链思维生成，计算成本高昂。研究发现早期推理步骤的输出分布熵能可靠区分正确与错误推理，这启发了动态截断方法

Method: 提出EntroCut训练免费方法，通过监测模型输出分布的熵来动态截断推理，在达到高置信度状态时提前终止，无需额外训练

Result: 在四个基准测试中，EntroCut最多减少40%的token使用，准确率损失最小，在效率-性能权衡上优于现有训练免费方法

Conclusion: 熵引导的动态截断为缓解大推理模型效率低下问题提供了实用方法，在保持准确性的同时显著提升推理效率

Abstract: Large Reasoning Models (LRMs) excel at complex reasoning tasks through extended chain-of-thought generation, but their reliance on lengthy intermediate steps incurs substantial computational cost. We find that the entropy of the model's output distribution in early reasoning steps reliably distinguishes correct from incorrect reasoning. Motivated by this observation, we propose EntroCut, a training-free method that dynamically truncates reasoning by identifying high-confidence states where reasoning can be safely terminated. To comprehensively evaluate the trade-off between efficiency and accuracy, we introduce the Efficiency-Performance Ratio (EPR), a unified metric that quantifies relative token savings per unit accuracy loss. Experiments on four benchmarks show that EntroCut reduces token usage by up to 40\% with minimal accuracy sacrifice, achieving superior efficiency-performance trade-offs compared with existing training-free methods. These results demonstrate that entropy-guided dynamic truncation provides a practical approach to mitigate the inefficiency of LRMs.

</details>


### [21] [SYMPHONY: Synergistic Multi-agent Planning with Heterogeneous Language Model Assembly](https://arxiv.org/abs/2601.22623)
*Wei Zhu,Zhiwen Tang,Kun Yue*

Main category: cs.AI

TL;DR: SYMPHONY是一个多智能体规划框架，通过集成异构语言模型智能体池来增强蒙特卡洛树搜索中的探索多样性，相比单智能体方法显著提升了规划性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要采用单智能体框架进行蒙特卡洛树搜索规划，这种范式限制了探索能力，导致生成分支多样性不足和规划性能欠佳。

Method: 提出SYMPHONY多智能体规划框架，集成异构语言模型智能体池，利用不同智能体的多样化推理模式来增强rollout多样性并促进更有效的探索。

Result: 在多个基准任务上的实验结果表明，SYMPHONY即使使用可在消费级硬件上部署的开源LLM也能实现强大性能；当使用基于云的API访问的LLM增强时，进一步改进并超越了现有最先进的基线方法。

Conclusion: 异构多智能体协调在规划任务中具有显著有效性，SYMPHONY框架通过多智能体协同增强了蒙特卡洛树搜索的探索能力，提升了复杂问题解决的规划性能。

Abstract: Recent advancements have increasingly focused on leveraging large language models (LLMs) to construct autonomous agents for complex problem-solving tasks. However, existing approaches predominantly employ a single-agent framework to generate search branches and estimate rewards during Monte Carlo Tree Search (MCTS) planning. This single-agent paradigm inherently limits exploration capabilities, often resulting in insufficient diversity among generated branches and suboptimal planning performance. To overcome these limitations, we propose Synergistic Multi-agent Planning with Heterogeneous langauge model assembly (SYMPHONY), a novel multi-agent planning framework that integrates a pool of heterogeneous language model-based agents. By leveraging diverse reasoning patterns across agents, SYMPHONY enhances rollout diversity and facilitates more effective exploration. Empirical results across multiple benchmark tasks show that SYMPHONY achieves strong performance even when instantiated with open-source LLMs deployable on consumer-grade hardware. When enhanced with cloud-based LLMs accessible via API, SYMPHONY demonstrates further improvements, outperforming existing state-of-the-art baselines and underscoring the effectiveness of heterogeneous multi-agent coordination in planning tasks.

</details>


### [22] [Statistical Estimation of Adversarial Risk in Large Language Models under Best-of-N Sampling](https://arxiv.org/abs/2601.22636)
*Mingqian Feng,Xiaodong Liu,Weiwei Yang,Chenliang Xu,Christopher White,Jianfeng Gao*

Main category: cs.AI

TL;DR: 提出SABER方法，通过Beta分布建模样本级成功概率，推导解析缩放定律，仅用100个样本就能准确预测1000次采样下的攻击成功率，误差减少86.2%


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全评估通常基于单次或低预算对抗提示，低估了实际风险。攻击者可以利用大规模并行采样反复探测模型直到产生有害响应，需要更准确的评估方法

Method: 提出SABER方法：使用Beta分布（伯努利分布的共轭先验）建模样本级成功概率，推导解析缩放定律，实现从小预算测量可靠外推大N攻击成功率

Result: 仅用n=100个样本，锚定估计器预测ASR@1000的平均绝对误差为1.66，相比基线12.04减少86.2%误差。揭示了异构风险缩放特征，显示标准评估下看似稳健的模型在并行对抗压力下可能经历快速非线性风险放大

Conclusion: 这项工作为现实LLM安全评估提供了低成本、可扩展的方法论，揭示了模型在并行对抗压力下的真实风险特征，有助于更准确的安全评估

Abstract: Large Language Models (LLMs) are typically evaluated for safety under single-shot or low-budget adversarial prompting, which underestimates real-world risk. In practice, attackers can exploit large-scale parallel sampling to repeatedly probe a model until a harmful response is produced. While recent work shows that attack success increases with repeated sampling, principled methods for predicting large-scale adversarial risk remain limited. We propose a scaling-aware Best-of-N estimation of risk, SABER, for modeling jailbreak vulnerability under Best-of-N sampling. We model sample-level success probabilities using a Beta distribution, the conjugate prior of the Bernoulli distribution, and derive an analytic scaling law that enables reliable extrapolation of large-N attack success rates from small-budget measurements. Using only n=100 samples, our anchored estimator predicts ASR@1000 with a mean absolute error of 1.66, compared to 12.04 for the baseline, which is an 86.2% reduction in estimation error. Our results reveal heterogeneous risk scaling profiles and show that models appearing robust under standard evaluation can experience rapid nonlinear risk amplification under parallel adversarial pressure. This work provides a low-cost, scalable methodology for realistic LLM safety assessment. We will release our code and evaluation scripts upon publication to future research.

</details>


### [23] [Beyond Medical Chatbots: Meddollina and the Rise of Continuous Clinical Intelligence](https://arxiv.org/abs/2601.22645)
*Vaibhav Ram S. V. N. S,Swetanshu Agrawal,Samudra Banerjee,Abdul Muhsin*

Main category: cs.AI

TL;DR: 论文批判当前生成式医疗AI仅关注文本生成而非临床推理，提出临床情境智能(CCI)概念，并开发Meddollina系统，通过约束推理优先临床适宜性而非生成完整性。


<details>
  <summary>Details</summary>
Motivation: 当前生成式医疗AI虽然看起来流畅且知识丰富，但将医学视为下一个token预测存在结构性问题：过早结论、不合理确定性、意图漂移和多步决策不稳定。这些行为与临床部署不兼容，需要一种新的临床情境智能方法。

Method: 提出临床情境智能(CCI)作为新的能力类别，要求持续情境感知、意图保持、有界推理和证据不足时的原则性延迟。开发Meddollina系统，采用治理优先设计，在语言实现前约束推理，优先考虑临床适宜性而非生成完整性。

Result: 在16,412+个异构医疗查询评估中，Meddollina展现出独特行为特征：校准的不确定性、在未明确情况下的保守推理、稳定的纵向约束遵守、相对于生成中心基线的减少推测性完成。这些结果表明可部署医疗AI不能仅靠扩展实现。

Conclusion: 可部署医疗AI不会仅从扩展中产生，需要转向持续临床智能，其中进展应通过不确定性下与临床医生一致的行为来衡量，而非流畅驱动的完成度。

Abstract: Generative medical AI now appears fluent and knowledgeable enough to resemble clinical intelligence, encouraging the belief that scaling will make it safe. But clinical reasoning is not text generation. It is a responsibility-bound process under ambiguity, incomplete evidence, and longitudinal context. Even as benchmark scores rise, generation-centric systems still show behaviours incompatible with clinical deployment: premature closure, unjustified certainty, intent drift, and instability across multi-step decisions.
  We argue these are structural consequences of treating medicine as next-token prediction. We formalise Clinical Contextual Intelligence (CCI) as a distinct capability class required for real-world clinical use, defined by persistent context awareness, intent preservation, bounded inference, and principled deferral when evidence is insufficient.
  We introduce Meddollina, a governance-first clinical intelligence system designed to constrain inference before language realisation, prioritising clinical appropriateness over generative completeness. Meddollina acts as a continuous intelligence layer supporting clinical workflows while preserving clinician authority. We evaluate Meddollina using a behaviour-first regime across 16,412+ heterogeneous medical queries, benchmarking against general-purpose models, medical-tuned models, and retrieval-augmented systems.
  Meddollina exhibits a distinct behavioural profile: calibrated uncertainty, conservative reasoning under underspecification, stable longitudinal constraint adherence, and reduced speculative completion relative to generation-centric baselines. These results suggest deployable medical AI will not emerge from scaling alone, motivating a shift toward Continuous Clinical Intelligence, where progress is measured by clinician-aligned behaviour under uncertainty rather than fluency-driven completion.

</details>


### [24] [Test-Time Mixture of World Models for Embodied Agents in Dynamic Environments](https://arxiv.org/abs/2601.22647)
*Jinwoo Jang,Minjong Yoo,Sihyung Yoon,Honguk Woo*

Main category: cs.AI

TL;DR: TMoW是一个测试时世界模型混合框架，通过动态更新路由函数来增强具身智能体在动态环境中的适应性，支持零样本适应和少样本扩展。


<details>
  <summary>Details</summary>
Motivation: 当前基于语言模型的具身智能体在动态环境中的适应性有限，需要构建准确灵活的世界模型来支持有效推理和决策。传统MoE架构在部署后保持固定，难以适应动态环境中的未见领域。

Method: 提出测试时世界模型混合框架(TMoW)，包含三个核心组件：1)多粒度原型路由，基于对象到场景级相似性调整混合；2)测试时精炼，在推理过程中对齐未见领域特征与原型；3)蒸馏混合增强，从少量数据和现有原型高效构建新模型。

Result: 在VirtualHome、ALFWorld和RLBench基准测试中表现出色，在零样本适应和少样本扩展场景中都展示了强大性能，使具身智能体能够在动态环境中有效运行。

Conclusion: TMoW通过测试时更新路由函数，使具身智能体能够动态重组现有模型并集成新模型，实现了在动态环境中对未见领域的持续适应，解决了传统MoE架构的僵化问题。

Abstract: Language model (LM)-based embodied agents are increasingly deployed in real-world settings. Yet, their adaptability remains limited in dynamic environments, where constructing accurate and flexible world models is crucial for effective reasoning and decision-making. To address this challenge, we extend the Mixture-of-Experts (MoE) paradigm to embodied agents. While conventional MoE architectures modularize knowledge into expert components with pre-trained routing, they remain rigid once deployed, making them less effective for adapting to unseen domains in dynamic environments. We therefore propose Test-time Mixture of World Models (TMoW), a framework that enhances adaptability to unseen and evolving domains. TMoW updates its routing function over world models at test time, unlike conventional MoE where the function remains fixed, enabling agents to recombine existing models and integrate new ones for continual adaptation. It achieves this through (i) multi-granular prototype-based routing, which adapts mixtures across object- to scene-level similarities, (ii) test-time refinement that aligns unseen domain features with prototypes during inference, and (iii) distilled mixture-based augmentation, which efficiently constructs new models from few-shot data and existing prototypes. We evaluate TMoW on VirtualHome, ALFWorld, and RLBench benchmarks, demonstrating strong performance in both zero-shot adaptation and few-shot expansion scenarios, and showing that it enables embodied agents to operate effectively in dynamic environments.

</details>


### [25] [UCPO: Uncertainty-Aware Policy Optimization](https://arxiv.org/abs/2601.22648)
*Xianzhou Zeng,Jing Huang,Chunmei Xie,Gongrui Nan,Siye Chen,Mengyu Lu,Weiqi Xiong,Qixuan Zhou,Junhao Zhang,Qiang Zhu,Yadong Li,Xingzhong Xu*

Main category: cs.AI

TL;DR: 本文提出UCPO框架，通过三元优势解耦和动态不确定性奖励调整，解决现有RL范式中的优势偏差问题，提升LLM的不确定性表达能力


<details>
  <summary>Details</summary>
Motivation: 现有RL范式（如GRPO）在二元决策空间和静态不确定性奖励下存在优势偏差，导致模型过度保守或过度自信，限制了LLM在高风险应用中的可靠性

Method: 提出UnCertainty-Aware Policy Optimization (UCPO)框架：1）三元优势解耦：分离并独立归一化确定性和不确定性rollout以消除优势偏差；2）动态不确定性奖励调整：根据模型演化和实例难度实时校准不确定性权重

Result: 在数学推理和通用任务上的实验结果表明，UCPO有效解决了奖励不平衡问题，显著提高了模型在知识边界之外的可靠性和校准能力

Conclusion: UCPO框架通过解决现有RL范式中的优势偏差问题，成功提升了LLM的不确定性表达能力，为构建可信赖的LLM提供了有效解决方案

Abstract: The key to building trustworthy Large Language Models (LLMs) lies in endowing them with inherent uncertainty expression capabilities to mitigate the hallucinations that restrict their high-stakes applications. However, existing RL paradigms such as GRPO often suffer from Advantage Bias due to binary decision spaces and static uncertainty rewards, inducing either excessive conservatism or overconfidence. To tackle this challenge, this paper unveils the root causes of reward hacking and overconfidence in current RL paradigms incorporating uncertainty-based rewards, based on which we propose the UnCertainty-Aware Policy Optimization (UCPO) framework. UCPO employs Ternary Advantage Decoupling to separate and independently normalize deterministic and uncertain rollouts, thereby eliminating advantage bias. Furthermore, a Dynamic Uncertainty Reward Adjustment mechanism is introduced to calibrate uncertainty weights in real-time according to model evolution and instance difficulty. Experimental results in mathematical reasoning and general tasks demonstrate that UCPO effectively resolves the reward imbalance, significantly improving the reliability and calibration of the model beyond their knowledge boundaries.

</details>


### [26] [Task-Aware LLM Council with Adaptive Decision Pathways for Decision Support](https://arxiv.org/abs/2601.22662)
*Wei Zhu,Lixing Yu,Hao-Ren Yao,Zhiwen Tang,Kun Yue*

Main category: cs.AI

TL;DR: TALC是一个任务感知的LLM委员会框架，通过蒙特卡洛树搜索动态选择专家模型，实现专业化感知的路由和自适应规划，在多个任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法往往忽视不同LLM的专业化差异，将所有模型视为同等适用，这限制了它们适应不同推理需求和任务复杂度的能力。

Method: 提出任务感知LLM委员会(TALC)，整合多个LLM与蒙特卡洛树搜索，每个LLM配备结构化成功记忆档案，通过语义匹配当前推理上下文与历史成功案例，在决策点路由到最合适的模型，使用融合模型评估和历史效用的双信号机制指导搜索。

Result: 在WebShop、HumanEval和24点游戏上的实验表明，TALC相比强基线实现了更高的任务成功率和搜索效率。

Conclusion: TALC验证了专业化感知路由和自适应规划的优势，能够更好地利用不同LLM的专业能力进行复杂决策任务。

Abstract: Large language models (LLMs) have shown strong capabilities across diverse decision-making tasks. However, existing approaches often overlook the specialization differences among available models, treating all LLMs as uniformly applicable regardless of task characteristics. This limits their ability to adapt to varying reasoning demands and task complexities. In this work, we propose Task-Aware LLM Council (TALC), a task-adaptive decision framework that integrates a council of LLMs with Monte Carlo Tree Search (MCTS) to enable dynamic expert selection and efficient multi-step planning. Each LLM is equipped with a structured success memory profile derived from prior task trajectories, enabling semantic matching between current reasoning context and past successes. At each decision point, TALC routes control to the most contextually appropriate model and estimates node value using a dual-signal mechanism that fuses model-based evaluations with historical utility scores. These signals are adaptively weighted based on intra-node variance and used to guide MCTS selection, allowing the system to balance exploration depth with planning confidence. Experiments on WebShop, HumanEval, and the Game of 24 demonstrate that TALC achieves superior task success rates and improved search efficiency compared to strong baselines, validating the benefits of specialization-aware routing and adaptive planning.

</details>


### [27] [Real-Time Aligned Reward Model beyond Semantics](https://arxiv.org/abs/2601.22664)
*Zixuan Huang,Xin Xia,Yuxi Ren,Jianbin Zheng,Xuefeng Xiao,Hongyan Xie,Li Huaqiu,Songshi Liang,Zhongxiang Dai,Fuzhen Zhuang,Jianxin Li,Yikun Ban,Deqing Wang*

Main category: cs.AI

TL;DR: R2M是一个轻量级RLHF框架，通过利用策略模型的实时隐藏状态反馈来应对奖励过优化问题，实现奖励模型与策略模型分布漂移的对齐。


<details>
  <summary>Details</summary>
Motivation: 传统的RLHF方法容易受到奖励过优化的影响，即策略模型过度拟合奖励模型，利用虚假的奖励模式而非真正捕捉人类意图。现有缓解方法主要依赖表面语义信息，无法有效处理奖励模型与策略模型之间因连续策略分布漂移导致的错位问题，这会加剧奖励过优化。

Method: 提出R2M（实时对齐奖励模型）框架，超越仅依赖预训练LLM语义表示的普通奖励模型。R2M利用策略模型在RL过程中不断演化的隐藏状态（即策略反馈），与策略的实时分布漂移对齐。

Result: 该方法为通过实时利用策略模型反馈来改进奖励模型性能提供了一个有前景的新方向。

Conclusion: R2M通过整合策略模型的实时反馈，能够更好地应对RLHF中的奖励过优化问题，实现奖励模型与策略模型在分布漂移过程中的有效对齐。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique for aligning large language models (LLMs) with human preferences, yet it is susceptible to reward overoptimization, in which policy models overfit to the reward model, exploit spurious reward patterns instead of faithfully capturing human intent. Prior mitigations primarily relies on surface semantic information and fails to efficiently address the misalignment between the reward model (RM) and the policy model caused by continuous policy distribution shifts. This inevitably leads to an increasing reward discrepancy, exacerbating reward overoptimization. To address these limitations, we introduce R2M (Real-Time Aligned Reward Model), a novel lightweight RLHF framework. R2M goes beyond vanilla reward models that solely depend on the semantic representations of a pretrained LLM. Instead, it leverages the evolving hidden states of the policy (namely policy feedback) to align with the real-time distribution shift of the policy during the RL process. This work points to a promising new direction for improving the performance of reward models through real-time utilization of feedback from policy models.

</details>


### [28] [Best-of-Q: Improving VLM agents with Q-function Action Ranking at Inference](https://arxiv.org/abs/2601.22701)
*Emilien Biré,María Santos,Kai Yuan*

Main category: cs.AI

TL;DR: 提出一种无需重新训练VLM策略的新范式：冻结VLM作为动作提议器，通过轻量级离线训练的Q函数对候选动作重新排序，在推理时直接提升策略性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）作为智能体在数字环境（如网页和操作系统）中的骨干，面临快速变化环境适应性不足的问题。传统微调方法需要大量模型训练和数据收集，成本高昂且不灵活。

Method: 将VLM的角色解耦为高容量动作提议器和最终动作选择机制。保持VLM策略冻结，用于为给定状态生成候选动作集。然后使用轻量级离线训练的Q函数对这些候选动作重新排序，智能体执行估计价值最高的动作。

Result: 在WebVoyager基准测试中，该方法显著提升了智能体成功率：Qwen2.5-VL-7B智能体从38.8%提升到55.7%，专有GPT-4.1智能体从82.4%提升到88.8%。

Conclusion: 提出了一种在推理时直接应用Q函数实现即时策略改进的新范式，避免了离线重新标注数据和策略重新训练，有效提升了智能体在快速变化环境中的适应性。

Abstract: Vision-Language Models (VLMs) have become powerful backbones for agents to autonomously operate in digital environments like the web and operating systems. However, these models suffer from inadaptability to fast-changing environments like the web, which can be alleviated by fine-tuning requiring expansive model training and data collection. In this work, we introduce a novel paradigm for enhancing agentic VLM policies at inference without policy retraining. Fundamentally, our approach decouples the VLM's role as a high-capacity action proposer from the final action selection mechanism. We keep the VLM policy frozen and use it to generate a set of candidate actions for a given state. Then, a lightweight, offline-trained Q-function reranks these candidates, and the agent executes the action with the highest estimated value. The main contribution is to apply the Q-function directly during inference for immediate policy improvement, and not offline to relabel data for policy retraining. We demonstrate on the academic WebVoyager benchmark that our method significantly boosts agent success rates, improving a Qwen2.5-VL-7B agent from 38.8% to 55.7% and a proprietary GPT-4.1 agent from 82.4% to 88.8%.

</details>


### [29] [A Step Back: Prefix Importance Ratio Stabilizes Policy Optimization](https://arxiv.org/abs/2601.22718)
*Shiye Lei,Zhihao Cheng,Dacheng Tao*

Main category: cs.AI

TL;DR: 本文提出MinPRO方法，通过使用前缀中最小token级比率代替不稳定的累积前缀比率，解决LLM强化学习后训练中因策略偏移导致的训练不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 现有RL后训练方法通常使用token级重要性采样比率来修正采样策略与目标策略之间的差异，但这种方法在策略偏移较大时会导致训练不稳定。作者发现理论上正确的修正项是前缀重要性比率，而将其简化为token级近似会引发RL后训练的不稳定性。

Method: 提出MinPRO（Minimum Prefix Ratio）目标函数，用基于前缀中观察到的最小token级比率的非累积代理替代不稳定的累积前缀比率，以稳定大规模策略偏移下的LLM优化。

Result: 在密集和混合专家LLM上的大量实验表明，MinPRO在多个数学推理基准测试中显著提高了离策略机制下的训练稳定性和峰值性能。

Conclusion: MinPRO通过更稳定的前缀重要性比率修正方法，有效解决了LLM强化学习后训练中的策略偏移问题，提高了训练稳定性和模型性能。

Abstract: Reinforcement learning (RL) post-training has increasingly demonstrated strong ability to elicit reasoning behaviors in large language models (LLMs). For training efficiency, rollouts are typically generated in an off-policy manner using an older sampling policy and then used to update the current target policy. To correct the resulting discrepancy between the sampling and target policies, most existing RL objectives rely on a token-level importance sampling ratio, primarily due to its computational simplicity and numerical stability. However, we observe that token-level correction often leads to unstable training dynamics when the degree of off-policyness is large. In this paper, we revisit LLM policy optimization under off-policy conditions and show that the theoretically rigorous correction term is the prefix importance ratio, and that relaxing it to a token-level approximation can induce instability in RL post-training. To stabilize LLM optimization under large off-policy drift, we propose a simple yet effective objective, Minimum Prefix Ratio (MinPRO). MinPRO replaces the unstable cumulative prefix ratio with a non-cumulative surrogate based on the minimum token-level ratio observed in the preceding prefix. Extensive experiments on both dense and mixture-of-experts LLMs, across multiple mathematical reasoning benchmarks, demonstrate that MinPRO substantially improves training stability and peak performance in off-policy regimes.

</details>


### [30] [AutoRefine: From Trajectories to Reusable Expertise for Continual LLM Agent Refinement](https://arxiv.org/abs/2601.22758)
*Libin Qiu,Zhirong Gao,Junfu Chen,Yuhang Ye,Weizhi Huang,Xiaobo Xue,Wenkai Qiu,Shuo Tang*

Main category: cs.AI

TL;DR: AutoRefine框架从智能体执行历史中提取和维护双形式经验模式，包括用于复杂子任务的专用子智能体和用于静态知识的技能模式，通过持续维护机制防止知识库退化，在多个任务上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型智能体无法从经验中积累知识，将每个任务视为独立挑战。现有方法将经验提取为扁平文本知识，无法捕捉复杂子任务的程序逻辑，且缺乏维护机制导致知识库随着经验积累而退化。

Method: 提出AutoRefine框架，从智能体执行历史中提取双形式经验模式：对于程序性子任务，提取具有独立推理和记忆的专用子智能体；对于静态知识，提取技能模式作为指南或代码片段。采用持续维护机制对模式进行评分、修剪和合并，防止知识库退化。

Result: 在ALFWorld、ScienceWorld和TravelPlanner三个任务上分别达到98.4%、70.4%和27.1%的成功率，步骤减少20-73%。在TravelPlanner上，自动提取的系统性能超过手动设计的系统（27.1% vs 12.1%），展示了其捕捉程序协调的能力。

Conclusion: AutoRefine框架通过提取和维护双形式经验模式，有效解决了智能体经验积累和知识库退化问题，能够捕捉复杂任务的程序逻辑协调，显著提升智能体在多种任务上的性能表现。

Abstract: Large language model agents often fail to accumulate knowledge from experience, treating each task as an independent challenge. Recent methods extract experience as flattened textual knowledge, which cannot capture procedural logic of complex subtasks. They also lack maintenance mechanisms, causing repository degradation as experience accumulates. We introduce AutoRefine, a framework that extracts and maintains dual-form Experience Patterns from agent execution histories. For procedural subtasks, we extract specialized subagents with independent reasoning and memory. For static knowledge, we extract skill patterns as guidelines or code snippets. A continuous maintenance mechanism scores, prunes, and merges patterns to prevent repository degradation. Evaluated on ALFWorld, ScienceWorld, and TravelPlanner, AutoRefine achieves 98.4%, 70.4%, and 27.1% respectively, with 20-73% step reductions. On TravelPlanner, automatic extraction exceeds manually designed systems (27.1% vs 12.1%), demonstrating its ability to capture procedural coordination.

</details>


### [31] [TSPO: Breaking the Double Homogenization Dilemma in Multi-turn Search Policy Optimization](https://arxiv.org/abs/2601.22776)
*Shichao Ma,Zhiyuan Ma,Ming Yang,Xiaofan Li,Xing Wu,Jintao Du,Yu Cheng,Weiqiang Wang,Qiliang Liu,Zhengyang Zhou,Yang Wang*

Main category: cs.AI

TL;DR: TSPO通过引入首次出现潜在奖励机制，解决了多轮工具集成推理中的双重同质化困境，显著提升了LLM在复杂任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习的搜索增强推理框架主要依赖稀疏的结果级奖励，导致"双重同质化困境"：过程同质化（忽略思考、推理和工具使用过程）和组内同质化（粗粒度结果奖励导致组内优势估计效率低下）。

Method: 提出Turn-level Stage-aware Policy Optimization (TSPO)，引入首次出现潜在奖励(FOLR)机制，在正确答案首次出现的步骤分配部分奖励，保留过程级信号并增加组内奖励方差，无需外部奖励模型或标注。

Result: TSPO显著优于现有基线方法，在Qwen2.5-3B和7B模型上分别实现了24%和13.6%的平均性能提升。

Conclusion: TSPO通过细粒度的过程级奖励机制有效解决了双重同质化问题，为多轮工具集成推理提供了更有效的强化学习框架。

Abstract: Multi-turn tool-integrated reasoning enables Large Language Models (LLMs) to solve complex tasks through iterative information retrieval. However, current reinforcement learning (RL) frameworks for search-augmented reasoning predominantly rely on sparse outcome-level rewards, leading to a "Double Homogenization Dilemma." This manifests as (1) Process homogenization, where the thinking, reasoning, and tooling involved in generation are ignored. (2) Intra-group homogenization, coarse-grained outcome rewards often lead to inefficiencies in intra-group advantage estimation with methods like Group Relative Policy Optimization (GRPO) during sampling. To address this, we propose Turn-level Stage-aware Policy Optimization (TSPO). TSPO introduces the First-Occurrence Latent Reward (FOLR) mechanism, allocating partial rewards to the step where the ground-truth answer first appears, thereby preserving process-level signals and increasing reward variance within groups without requiring external reward models or any annotations. Extensive experiments demonstrate that TSPO significantly outperforms state-of-the-art baselines, achieving average performance gains of 24% and 13.6% on Qwen2.5-3B and 7B models, respectively.

</details>


### [32] [Learning with Challenges: Adaptive Difficulty-Aware Data Generation for Mobile GUI Agent Training](https://arxiv.org/abs/2601.22781)
*Linjia Kang,Zhimin Wang,Yongkang Zhang,Duo Wu,Jinghe Wang,Ming Ma,Haopeng Yan,Zhi Wang*

Main category: cs.AI

TL;DR: MobileGen提出了一种自适应对齐GUI智能体能力边界的数据生成框架，通过解耦任务难度为结构和语义维度，动态调整训练难度，显著提升移动GUI智能体性能。


<details>
  <summary>Details</summary>
Motivation: 现有移动GUI智能体数据生成方法依赖人工演示或自动模型探索，缺乏对任务难度的精细控制，导致训练难度与智能体能力不匹配，限制了学习效果。

Method: 1) 将任务难度解耦为结构维度（如轨迹长度）和语义维度（如任务目标）；2) 通过评估智能体在先前数据集上的表现，构建其能力边界系统画像；3) 自适应计算任务难度概率分布，采样下一轮训练的目标难度；4) 使用多智能体可控生成器合成高质量交互轨迹和任务指令。

Result: 在多个具有挑战性的基准测试中，MobileGen持续优于现有数据生成方法，将GUI智能体的平均性能提高了1.57倍。

Conclusion: MobileGen证明了能力对齐的数据生成对于有效训练移动GUI智能体的重要性，通过自适应调整训练难度与智能体能力边界，显著提升了学习效果。

Abstract: Large-scale, high-quality interaction trajectories are essential for advancing mobile Graphical User Interface (GUI) agents. While existing methods typically rely on labor-intensive human demonstrations or automated model exploration to generate GUI trajectories, they lack fine-grained control over task difficulty. This fundamentally restricts learning effectiveness due to the mismatch between the training difficulty and the agent's capabilities. Inspired by how humans acquire skills through progressively challenging tasks, we propose MobileGen, a novel data generation framework that adaptively aligns training difficulty with the GUI agent's capability frontier. Specifically, MobileGen explicitly decouples task difficulty into structural (e.g., trajectory length) and semantic (e.g., task goal) dimensions. It then iteratively evaluates the agent on a curated prior dataset to construct a systematic profile of its capability frontier across these two dimensions. With this profile, the probability distribution of task difficulty is adaptively computed, from which the target difficulty for the next round of training can be sampled. Guided by the sampled difficulty, a multi-agent controllable generator is finally used to synthesize high-quality interaction trajectories along with corresponding task instructions. Extensive experiments show that MobileGen consistently outperforms existing data generation methods by improving the average performance of GUI agents by 1.57 times across multiple challenging benchmarks. This highlights the importance of capability-aligned data generation for effective mobile GUI agent training.

</details>


### [33] [Toward IIT-Inspired Consciousness in LLMs: A Reward-Based Learning Framework](https://arxiv.org/abs/2601.22786)
*Hamid Reza Akbari,Mohammad Hossein Sameti,Amir M. Mansourian,Mohammad Hossein Rohban,Hossein Sameti*

Main category: cs.AI

TL;DR: 本文提出了一种基于整合信息理论(IIT)的奖励函数，通过强化学习优化语言模型生成文本的因果性、连贯性和整合性，从而在保持准确性的同时显著缩短输出长度。


<details>
  <summary>Details</summary>
Motivation: 追求人工通用智能(AGI)是语言模型发展的核心目标，其中类意识处理可能成为关键推动因素。虽然当前语言模型不具备意识，但它们表现出类似意识的某些行为特征。本文旨在将领先的意识理论——整合信息理论(IIT)通过基于奖励的学习范式应用于语言模型。

Method: 基于整合信息理论(IIT)的核心原则，设计了一种新颖的奖励函数，用于量化文本的因果性、连贯性和整合性——这些特征与意识处理相关。通过强化学习范式优化该奖励函数，无需外部数据或辅助模型，概念简单且计算高效。

Result: 优化IIT启发的奖励函数能产生更简洁的文本生成。在领域外任务中，经过精心调优后，输出长度最多减少31%，同时保持与基础模型相当的准确性水平。此外还分析了该方法对模型置信度校准和测试时计算扩展性的影响。

Conclusion: 提出的框架具有显著实用优势：概念简单、计算高效、无需外部数据或辅助模型，并利用通用的能力驱动信号而非任务特定启发式方法。该研究为在语言模型中实现类意识处理提供了新途径。

Abstract: The pursuit of Artificial General Intelligence (AGI) is a central goal in language model development, in which consciousness-like processing could serve as a key facilitator. While current language models are not conscious, they exhibit behaviors analogous to certain aspects of consciousness. This paper investigates the implementation of a leading theory of consciousness, Integrated Information Theory (IIT), within language models via a reward-based learning paradigm. IIT provides a formal, axiom-based mathematical framework for quantifying consciousness. Drawing inspiration from its core principles, we formulate a novel reward function that quantifies a text's causality, coherence and integration, characteristics associated with conscious processing. Empirically, it is found that optimizing for this IIT-inspired reward leads to more concise text generation. On out of domain tasks, careful tuning achieves up to a 31% reduction in output length while preserving accuracy levels comparable to the base model. In addition to primary task performance, the broader effects of this training methodology on the model's confidence calibration and test-time computational scaling is analyzed. The proposed framework offers significant practical advantages: it is conceptually simple, computationally efficient, requires no external data or auxiliary models, and leverages a general, capability-driven signal rather than task-specific heuristics. Code available at https://github.com/MH-Sameti/LLM_PostTraining.git

</details>


### [34] [Conditional Performance Guarantee for Large Reasoning Models](https://arxiv.org/abs/2601.22790)
*Jianguo Huang,Hao Zeng,Bingyi Jing,Hongxin Wei,Bo An*

Main category: cs.AI

TL;DR: G-PAC推理框架：通过输入空间分组实现组级PAC保证，在保持计算效率的同时提供比边际PAC更强的条件风险控制


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽然通过链式思维推理表现出色，但计算成本高昂。现有的PAC推理方法只在边际情况下提供统计保证，无法提供精确的条件覆盖保证。

Method: 提出G-PAC推理框架，通过输入空间划分实现组级PAC保证。开发两种具体实现：针对已知分组结构的Group PAC（G-PAC）和针对未知分组的Clustered PAC（C-PAC）。

Result: 理论证明G-PAC和C-PAC都能实现组条件风险控制，分组在异构设置下能严格提高效率。实验表明两种方法在多个推理基准上成功实现组条件风险控制，同时保持显著的计算节省。

Conclusion: G-PAC推理框架提供了一种实用的方法，在保持计算效率的同时提供比传统边际PAC更强的统计保证，解决了条件覆盖问题。

Abstract: Large reasoning models have shown strong performance through extended chain-of-thought reasoning, yet their computational cost remains significant. Probably approximately correct (PAC) reasoning provides statistical guarantees for efficient reasoning by adaptively switching between thinking and non-thinking models, but the guarantee holds only in the marginal case and does not provide exact conditional coverage. We propose G-PAC reasoning, a practical framework that provides PAC-style guarantees at the group level by partitioning the input space. We develop two instantiations: Group PAC (G-PAC) reasoning for known group structures and Clustered PAC (C-PAC) reasoning for unknown groupings. We prove that both G-PAC and C-PAC achieve group-conditional risk control, and that grouping can strictly improve efficiency over marginal PAC reasoning in heterogeneous settings. Our experiments on diverse reasoning benchmarks demonstrate that G-PAC and C-PAC successfully achieve group-conditional risk control while maintaining substantial computational savings.

</details>


### [35] [CVeDRL: An Efficient Code Verifier via Difficulty-aware Reinforcement Learning](https://arxiv.org/abs/2601.22803)
*Ji Shi,Peiming Guo,Meishan Zhang,Miao Zhang,Xuebo Liu,Min Zhang,Weili Guan*

Main category: cs.AI

TL;DR: CVeDRL：基于强化学习的代码验证器，通过语法、功能、分支覆盖和样本难度感知的奖励设计，在仅0.6B参数下实现SOTA性能，比GPT-3.5提升28.97%通过率和15.08%分支覆盖率，推理速度快20倍以上。


<details>
  <summary>Details</summary>
Motivation: 现有基于监督微调的代码验证器面临数据稀缺、高失败率和推理效率低的问题。强化学习虽能通过执行驱动的奖励进行无监督优化，但仅使用功能奖励的朴素RL方法难以生成针对困难分支和样本的有效单元测试。

Method: 理论分析表明分支覆盖率、样本难度、语法和功能正确性可联合建模为RL奖励。基于此设计语法和功能感知的奖励，并提出基于指数奖励塑造和静态分析指标的分支和样本难度感知RL方法。

Result: CVeDRL在仅0.6B参数下实现SOTA性能：比GPT-3.5提升28.97%通过率和15.08%分支覆盖率，同时比竞争基线快20倍以上的推理速度。

Conclusion: 通过将分支覆盖率、样本难度、语法和功能正确性联合建模为RL奖励，CVeDRL显著提升了代码验证的可靠性和效率，为LLM代码生成的验证提供了高效解决方案。

Abstract: Code verifiers play a critical role in post-verification for LLM-based code generation, yet existing supervised fine-tuning methods suffer from data scarcity, high failure rates, and poor inference efficiency. While reinforcement learning (RL) offers a promising alternative by optimizing models through execution-driven rewards without labeled supervision, our preliminary results show that naive RL with only functionality rewards fails to generate effective unit tests for difficult branches and samples. We first theoretically analyze showing that branch coverage, sample difficulty, syntactic and functional correctness can be jointly modeled as RL rewards, where optimizing these signals can improve the reliability of unit-test-based verification. Guided by this analysis, we design syntax- and functionality-aware rewards and further propose branch- and sample-difficulty--aware RL using exponential reward shaping and static analysis metrics. With this formulation, CVeDRL achieves state-of-the-art performance with only 0.6B parameters, yielding up to 28.97% higher pass rate and 15.08% higher branch coverage than GPT-3.5, while delivering over $20\times$ faster inference than competitive baselines. Code is available at https://github.com/LIGHTCHASER1/CVeDRL.git

</details>


### [36] [Aligning the Unseen in Attributed Graphs: Interplay between Graph Geometry and Node Attributes Manifold](https://arxiv.org/abs/2601.22806)
*Aldric Labarthe,Roland Bouffanais,Julien Randon-Furling*

Main category: cs.AI

TL;DR: 提出一种变分自编码器方法，将属性流形学习与结构对齐分离，通过量化度量扭曲来恢复传统方法丢失的图生成过程信息。


<details>
  <summary>Details</summary>
Motivation: 传统属性图表示学习方法在几何上存在缺陷，它将两个可能不兼容的度量空间合并，导致破坏性的对齐，从而丢失了图底层生成过程的信息。

Method: 引入定制化的变分自编码器，将流形学习与结构对齐分离。通过量化将属性流形映射到图热核所需的度量扭曲，将几何冲突转化为可解释的结构描述符。

Result: 实验表明该方法能够发现传统方法无法检测的连接模式和异常，证明了传统方法在理论上的不足和实践上的局限性。

Conclusion: 通过分离流形学习和结构对齐，并将几何冲突量化为结构描述符，能够有效恢复传统方法丢失的图生成过程信息，提供更准确的图表示学习。

Abstract: The standard approach to representation learning on attributed graphs -- i.e., simultaneously reconstructing node attributes and graph structure -- is geometrically flawed, as it merges two potentially incompatible metric spaces. This forces a destructive alignment that erodes information about the graph's underlying generative process. To recover this lost signal, we introduce a custom variational autoencoder that separates manifold learning from structural alignment. By quantifying the metric distortion needed to map the attribute manifold onto the graph's Heat Kernel, we transform geometric conflict into an interpretable structural descriptor. Experiments show our method uncovers connectivity patterns and anomalies undetectable by conventional approaches, proving both their theoretical inadequacy and practical limitations.

</details>


### [37] [Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery](https://arxiv.org/abs/2601.22896)
*Xinyi Ke,Kai Li,Junliang Xing,Yifan Zhang,Jian Cheng*

Main category: cs.AI

TL;DR: ASRO是一个基于博弈论的启发式算法发现框架，将求解器与实例生成器建模为双人零和博弈，通过LLM驱动的响应预言机实现程序级协同进化，替代静态评估，提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有自动启发式发现方法主要依赖静态评估和固定实例分布，容易导致过拟合和分布偏移下的泛化能力差，需要更动态、自适应的评估框架。

Method: 将启发式发现重构为求解器与实例生成器之间的程序级协同进化博弈，维护双方策略池，通过LLM驱动的响应预言机迭代扩展策略池，对抗混合对手元策略，形成自适应生成课程。

Result: 在多个组合优化领域，ASRO持续超越基于相同程序搜索机制的静态训练基线，在多样化和分布外实例上实现了显著改进的泛化能力和鲁棒性。

Conclusion: ASRO通过博弈论框架将静态评估替换为动态协同进化，为自动启发式发现提供了更强大的泛化能力和鲁棒性，解决了现有方法的局限性。

Abstract: Large language models (LLMs) have enabled rapid progress in automatic heuristic discovery (AHD), yet most existing methods are predominantly limited by static evaluation against fixed instance distributions, leading to potential overfitting and poor generalization under distributional shifts. We propose Algorithm Space Response Oracles (ASRO), a game-theoretic framework that reframes heuristic discovery as a program level co-evolution between solver and instance generator. ASRO models their interaction as a two-player zero-sum game, maintains growing strategy pools on both sides, and iteratively expands them via LLM-based best-response oracles against mixed opponent meta-strategies, thereby replacing static evaluation with an adaptive, self-generated curriculum. Across multiple combinatorial optimization domains, ASRO consistently outperforms static-training AHD baselines built on the same program search mechanisms, achieving substantially improved generalization and robustness on diverse and out-of-distribution instances.

</details>


### [38] [MulFeRL: Enhancing Reinforcement Learning with Verbal Feedback in a Multi-turn Loop](https://arxiv.org/abs/2601.22900)
*Xuancheng Li,Haitao Li,Yujia Zhou,YiqunLiu,Qingyao Ai*

Main category: cs.AI

TL;DR: 提出多轮反馈引导的强化学习框架，通过动态多轮再生、双重学习信号和结构化反馈注入，利用丰富语言反馈指导RLVR训练，在失败样本上提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统RLVR使用标量奖励，在失败样本上信息稀疏且无指导性，仅指示失败而不提供失败原因。需要利用更丰富的语言反馈来指导训练，特别是对失败样本。

Method: 提出多轮反馈引导强化学习框架：1) 动态多轮再生（仅在失败样本触发，基于反馈指导）；2) 双重学习信号（轮内优化和跨轮优化）；3) 结构化反馈注入（将反馈融入模型推理过程）。

Result: 在OpenR1-Math数据集上训练，方法在域内表现优于监督微调和RLVR基线，并在域外泛化良好。

Conclusion: 利用丰富语言反馈指导RLVR训练能有效提升推理能力，特别是在失败样本上。多轮反馈引导框架通过动态再生、双重学习和结构化注入机制，显著改善训练效果和泛化能力。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is widely used to improve reasoning in multiple domains, yet outcome-only scalar rewards are often sparse and uninformative, especially on failed samples, where they merely indicate failure and provide no insight into why the reasoning fails. In this paper, we investigate how to leverage richer verbal feedback to guide RLVR training on failed samples, and how to convert such feedback into a trainable learning signal. Specifically, we propose a multi-turn feedback-guided reinforcement learning framework. It builds on three mechanisms: (1) dynamic multi-turn regeneration guided by feedback, triggered only on failed samples, (2) two complementary learning signals for within-turn and cross-turn optimization, and (3) structured feedback injection into the model's reasoning process. Trained on sampled OpenR1-Math, the approach outperforms supervised fine-tuning and RLVR baselines in-domain and generalizes well out-of-domain.

</details>


### [39] [Alignment among Language, Vision and Action Representations](https://arxiv.org/abs/2601.22948)
*Nicola Milano,Stefano Nolfi*

Main category: cs.AI

TL;DR: 研究发现语言、视觉和动作学习会产生部分共享的语义表示，支持模态独立的语义组织


<details>
  <summary>Details</summary>
Motivation: 探索不同学习模态（语言、视觉、动作）是否产生不同或共享的内部表示，挑战传统认为不同数据类型的模型会发展出专门化、不可转移表示的观点

Method: 在BabyAI平台上训练基于transformer的智能体执行目标导向行为，通过行为克隆生成动作基础的语言嵌入，然后与大型语言模型（LLaMA、Qwen、DeepSeek、BERT）和视觉语言模型（CLIP、BLIP）的表示进行比较

Result: 动作表示与仅解码器语言模型和BLIP对齐强烈（precision@15: 0.70-0.73），接近语言模型之间的对齐程度；与CLIP和BERT的对齐显著较弱

Conclusion: 语言、视觉和动作表示会收敛到部分共享的语义结构，支持模态独立的语义组织，并突显了在具身AI系统中跨领域转移的潜力

Abstract: A fundamental question in cognitive science and AI concerns whether different learning modalities: language, vision, and action, give rise to distinct or shared internal representations. Traditional views assume that models trained on different data types develop specialized, non-transferable representations. However, recent evidence suggests unexpected convergence: models optimized for distinct tasks may develop similar representational geometries. We investigate whether this convergence extends to embodied action learning by training a transformer-based agent to execute goal-directed behaviors in response to natural language instructions. Using behavioral cloning on the BabyAI platform, we generated action-grounded language embeddings shaped exclusively by sensorimotor control requirements. We then compared these representations with those extracted from state-of-the-art large language models (LLaMA, Qwen, DeepSeek, BERT) and vision-language models (CLIP, BLIP). Despite substantial differences in training data, modality, and objectives, we observed robust cross-modal alignment. Action representations aligned strongly with decoder-only language models and BLIP (precision@15: 0.70-0.73), approaching the alignment observed among language models themselves. Alignment with CLIP and BERT was significantly weaker. These findings indicate that linguistic, visual, and action representations converge toward partially shared semantic structures, supporting modality-independent semantic organization and highlighting potential for cross-domain transfer in embodied AI systems.

</details>


### [40] [EvoClinician: A Self-Evolving Agent for Multi-Turn Medical Diagnosis via Test-Time Evolutionary Learning](https://arxiv.org/abs/2601.22964)
*Yufei He,Juncheng Liu,Zhiyuan Hu,Yulin Chen,Yue Liu,Yuan Sui,Yibo Li,Nuo Chen,Jun Hu,Bryan Hooi,Xinxing Xu,Jiang Bian*

Main category: cs.AI

TL;DR: 该论文提出了Med-Inquire基准来评估AI在多轮诊断中的能力，并开发了EvoClinician自进化智能体，通过"诊断-评分-进化"循环学习高效诊断策略，在真实临床病例上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有医疗AI采用"一次性"诊断模式，与现实中医生的迭代诊断过程不符。真实诊断中医生需要顺序提问和安排检查，在管理成本和时间的同时战略性地收集信息。

Method: 1) 提出Med-Inquire基准，基于真实临床病例模拟诊断过程，隐藏完整患者档案；2) 开发EvoClinician自进化智能体，采用"诊断-评分-进化"循环：Actor尝试诊断，Process Grader评估每个行动，Evolver根据反馈更新策略。

Result: 实验显示EvoClinician在Med-Inquire基准上优于持续学习基线和其他自进化智能体（如记忆智能体），能够学习高效的诊断策略。

Conclusion: 该研究通过Med-Inquire基准和EvoClinician智能体，推动了医疗AI向更贴近真实临床实践的迭代诊断方向发展，为开发更实用的临床决策支持系统提供了新思路。

Abstract: Prevailing medical AI operates on an unrealistic ''one-shot'' model, diagnosing from a complete patient file. However, real-world diagnosis is an iterative inquiry where Clinicians sequentially ask questions and order tests to strategically gather information while managing cost and time. To address this, we first propose Med-Inquire, a new benchmark designed to evaluate an agent's ability to perform multi-turn diagnosis. Built upon a dataset of real-world clinical cases, Med-Inquire simulates the diagnostic process by hiding a complete patient file behind specialized Patient and Examination agents. They force the agent to proactively ask questions and order tests to gather information piece by piece. To tackle the challenges posed by Med-Inquire, we then introduce EvoClinician, a self-evolving agent that learns efficient diagnostic strategies at test time. Its core is a ''Diagnose-Grade-Evolve'' loop: an Actor agent attempts a diagnosis; a Process Grader agent performs credit assignment by evaluating each action for both clinical yield and resource efficiency; finally, an Evolver agent uses this feedback to update the Actor's strategy by evolving its prompt and memory. Our experiments show EvoClinician outperforms continual learning baselines and other self-evolving agents like memory agents. The code is available at https://github.com/yf-he/EvoClinician

</details>


### [41] [Golden Goose: A Simple Trick to Synthesize Unlimited RLVR Tasks from Unverifiable Internet Text](https://arxiv.org/abs/2601.22975)
*Ximing Lu,David Acuna,Jaehun Jung,Jian Hu,Di Zhang,Shizhe Diao,Yunheng Zou,Shaokun Zhang,Brandon Cui,Mingjie Liu,Hyunwoo Kim,Prithviraj Ammanabrolu,Jan Kautz,Yi Dong,Yejin Choi*

Main category: cs.AI

TL;DR: 提出Golden Goose方法，通过将不可验证的互联网文本转化为多项选择问答任务，自动生成大规模RLVR数据集，解决现有可验证数据有限导致的RL训练瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: RLVR已成为解锁LLMs复杂推理能力的关键技术，但现有可验证数据有限导致RL训练难以扩展，性能提升在长时间训练后趋于饱和。

Method: Golden Goose方法：从不可验证的互联网文本中合成RLVR任务，通过LLM识别和掩码关键推理步骤，生成多样化的干扰选项，构建多项选择问答版本的填空任务。

Result: 构建了GooseReason-0.7M数据集（70万个任务），覆盖数学、编程和科学领域；在15个基准测试中，1.5B和4B-Instruct模型取得SOTA；在网络安全领域，Qwen3-4B-Instruct超越7B领域专用模型。

Conclusion: Golden Goose通过利用丰富的不可验证互联网文本，能够自动扩展RLVR数据，为RL训练提供可持续的改进动力，特别是在缺乏现有RLVR数据的专业领域。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become a cornerstone for unlocking complex reasoning in Large Language Models (LLMs). Yet, scaling up RL is bottlenecked by limited existing verifiable data, where improvements increasingly saturate over prolonged training. To overcome this, we propose Golden Goose, a simple trick to synthesize unlimited RLVR tasks from unverifiable internet text by constructing a multiple-choice question-answering version of the fill-in-the-middle task. Given a source text, we prompt an LLM to identify and mask key reasoning steps, then generate a set of diverse, plausible distractors. This enables us to leverage reasoning-rich unverifiable corpora typically excluded from prior RLVR data construction (e.g., science textbooks) to synthesize GooseReason-0.7M, a large-scale RLVR dataset with over 0.7 million tasks spanning mathematics, programming, and general scientific domains. Empirically, GooseReason effectively revives models saturated on existing RLVR data, yielding robust, sustained gains under continuous RL and achieving new state-of-the-art results for 1.5B and 4B-Instruct models across 15 diverse benchmarks. Finally, we deploy Golden Goose in a real-world setting, synthesizing RLVR tasks from raw FineWeb scrapes for the cybersecurity domain, where no prior RLVR data exists. Training Qwen3-4B-Instruct on the resulting data GooseReason-Cyber sets a new state-of-the-art in cybersecurity, surpassing a 7B domain-specialized model with extensive domain-specific pre-training and post-training. This highlights the potential of automatically scaling up RLVR data by exploiting abundant, reasoning-rich, unverifiable internet text.

</details>


### [42] [Quantifying Model Uniqueness in Heterogeneous AI Ecosystems](https://arxiv.org/abs/2601.22977)
*Lei You*

Main category: cs.AI

TL;DR: 提出ISQED统计框架，通过干预实验量化模型独特性（PIER），证明观测数据无法识别独特性，推导主动审计的样本效率最优边界，展示合作博弈方法无法检测冗余。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统从孤立预测器演变为复杂异构的基础模型和专用适配器生态系统，区分真正的行为新颖性与功能冗余成为关键治理挑战。

Method: 引入基于In-Silico Quasi-Experimental Design (ISQED)的统计框架，通过匹配干预隔离模型内在身份，量化Peer-Inexpressible Residual (PIER)作为独特性指标，实现DISCO估计器。

Result: 证明观测日志存在根本限制（独特性无法识别），推导主动审计的缩放定律（达到极小极大最优样本效率），展示合作博弈方法（如Shapley值）无法检测冗余，在计算机视觉、语言模型和交通预测等生态系统中验证。

Conclusion: 将可信AI从解释单一模型扩展到建立基于干预的异构模型生态系统审计与治理原则科学。

Abstract: As AI systems evolve from isolated predictors into complex, heterogeneous ecosystems of foundation models and specialized adapters, distinguishing genuine behavioral novelty from functional redundancy becomes a critical governance challenge. Here, we introduce a statistical framework for auditing model uniqueness based on In-Silico Quasi-Experimental Design (ISQED). By enforcing matched interventions across models, we isolate intrinsic model identity and quantify uniqueness as the Peer-Inexpressible Residual (PIER), i.e. the component of a target's behavior strictly irreducible to any stochastic convex combination of its peers, with vanishing PIER characterizing when such a routing-based substitution becomes possible. We establish the theoretical foundations of ecosystem auditing through three key contributions. First, we prove a fundamental limitation of observational logs: uniqueness is mathematically non-identifiable without intervention control. Second, we derive a scaling law for active auditing, showing that our adaptive query protocol achieves minimax-optimal sample efficiency ($dσ^2γ^{-2}\log(Nd/δ)$). Third, we demonstrate that cooperative game-theoretic methods, such as Shapley values, fundamentally fail to detect redundancy. We implement this framework via the DISCO (Design-Integrated Synthetic Control) estimator and deploy it across diverse ecosystems, including computer vision models (ResNet/ConvNeXt/ViT), large language models (BERT/RoBERTa), and city-scale traffic forecasters. These results move trustworthy AI beyond explaining single models: they establish a principled, intervention-based science of auditing and governing heterogeneous model ecosystems.

</details>


### [43] [Why Your Deep Research Agent Fails? On Hallucination Evaluation in Full Research Trajectory](https://arxiv.org/abs/2601.22984)
*Yuhao Zhan,Tianyu Fan,Linxuan Huang,Zirui Guo,Chao Huang*

Main category: cs.AI

TL;DR: 提出DeepHalluBench基准，通过过程感知评估而非结果导向评估，系统分析深度研究智能体中的幻觉问题，发现现有系统均存在可靠性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度研究智能体评估主要依赖端到端评估，掩盖了研究轨迹中的关键中间幻觉（如错误规划），需要从结果导向转向过程感知的评估方法。

Method: 提出PIES分类法（规划vs总结、显式vs隐式幻觉），构建细粒度评估框架分解研究轨迹，创建包含100个幻觉易发任务的DeepHalluBench基准。

Result: 对6个最先进的深度研究智能体实验显示，没有系统达到稳健可靠性；诊断分析发现失败源于系统性缺陷，特别是幻觉传播和认知偏差。

Conclusion: 过程感知评估揭示了深度研究智能体的系统性缺陷，为未来架构优化提供了基础性见解，DeepHalluBench基准可用于推动更可靠的智能体开发。

Abstract: Diagnosing the failure mechanisms of Deep Research Agents (DRAs) remains a critical challenge. Existing benchmarks predominantly rely on end-to-end evaluation, obscuring critical intermediate hallucinations, such as flawed planning, that accumulate throughout the research trajectory. To bridge this gap, we propose a shift from outcome-based to process-aware evaluation by auditing the full research trajectory. We introduce the PIES Taxonomy to categorize hallucinations along functional components (Planning vs. Summarization) and error properties (Explicit vs. Implicit). We instantiate this taxonomy into a fine-grained evaluation framework that decomposes the trajectory to rigorously quantify these hallucinations. Leveraging this framework to isolate 100 distinctively hallucination-prone tasks including adversarial scenarios, we curate DeepHalluBench. Experiments on six state-of-theart DRAs reveal that no system achieves robust reliability. Furthermore, our diagnostic analysis traces the etiology of these failures to systemic deficits, specifically hallucination propagation and cognitive biases, providing foundational insights to guide future architectural optimization. Data and code are available at https://github.com/yuhao-zhan/DeepHalluBench.

</details>


### [44] [TriCEGAR: A Trace-Driven Abstraction Mechanism for Agentic AI](https://arxiv.org/abs/2601.22997)
*Roham Koohestani,Ateş Görpelioğlu,Egor Klimov,Burcu Kulahcioglu Ozkan,Maliheh Izadi*

Main category: cs.AI

TL;DR: TriCEGAR：一种基于轨迹驱动的抽象机制，自动从执行日志构建状态抽象，支持在线构建智能体行为MDP，解决了手动定义状态抽象的局限性


<details>
  <summary>Details</summary>
Motivation: 智能体AI系统通过工具行动，行为在长期随机交互轨迹中演化，这使得保证变得复杂。现有DPA方法需要开发者手动定义状态抽象，这导致验证与特定应用启发式耦合，增加了采用难度

Method: 提出TriCEGAR机制：1) 从轨迹中学习谓词树作为抽象表示；2) 使用反例进行细化；3) 实现框架原生实现，捕获类型化智能体生命周期事件；4) 从轨迹构建抽象和MDP；5) 进行概率模型检查计算边界概率

Result: TriCEGAR能够自动构建智能体行为MDP，支持计算Pmax(成功)和Pmin(失败)等边界概率，并利用运行似然性实现异常检测作为护栏信号

Conclusion: TriCEGAR通过自动化状态抽象构建，解决了DPA方法中手动定义状态抽象的局限性，降低了采用摩擦，为智能体AI系统的运行时验证提供了更实用的解决方案

Abstract: Agentic AI systems act through tools and evolve their behavior over long, stochastic interaction traces. This setting complicates assurance, because behavior depends on nondeterministic environments and probabilistic model outputs. Prior work introduced runtime verification for agentic AI via Dynamic Probabilistic Assurance (DPA), learning an MDP online and model checking quantitative properties. A key limitation is that developers must manually define the state abstraction, which couples verification to application-specific heuristics and increases adoption friction. This paper proposes TriCEGAR, a trace-driven abstraction mechanism that automates state construction from execution logs and supports online construction of an agent behavioral MDP. TriCEGAR represents abstractions as predicate trees learned from traces and refined using counterexamples. We describe a framework-native implementation that (i) captures typed agent lifecycle events, (ii) builds abstractions from traces, (iii) constructs an MDP, and (iv) performs probabilistic model checking to compute bounds such as Pmax(success) and Pmin(failure). We also show how run likelihoods enable anomaly detection as a guardrailing signal.

</details>


### [45] [Guided by Trajectories: Repairing and Rewarding Tool-Use Trajectories for Tool-Integrated Reasoning](https://arxiv.org/abs/2601.23032)
*Siyu Gong,Linan Yue,Weibo Gao,Fangzhou Yao,Shimin Di,Lei Feng,Min-Ling Zhang*

Main category: cs.AI

TL;DR: AutoTraj是一个两阶段框架，通过修复和奖励工具使用轨迹来自动学习工具集成推理，无需依赖高质量合成轨迹


<details>
  <summary>Details</summary>
Motivation: 现有工具集成推理方法依赖高质量合成轨迹和稀疏的结果奖励，提供有限且偏置的监督，需要更好的学习框架

Method: 两阶段框架：1) SFT阶段生成候选轨迹，评估并修复低质量轨迹；2) RL阶段训练轨迹级奖励模型，结合结果和格式奖励优化推理行为

Result: 在真实世界基准测试中证明了AutoTraj在工具集成推理中的有效性

Conclusion: AutoTraj通过自动修复和奖励工具使用轨迹，有效解决了现有TIR方法的监督限制问题

Abstract: Tool-Integrated Reasoning (TIR) enables large language models (LLMs) to solve complex tasks by interacting with external tools, yet existing approaches depend on high-quality synthesized trajectories selected by scoring functions and sparse outcome-based rewards, providing limited and biased supervision for learning TIR. To address these challenges, in this paper, we propose AutoTraj, a two-stage framework that automatically learns TIR by repairing and rewarding tool-use trajectories. Specifically, in the supervised fine-tuning (SFT) stage, AutoTraj generates multiple candidate tool-use trajectories for each query and evaluates them along multiple dimensions. High-quality trajectories are directly retained, while low-quality ones are repaired using a LLM (i.e., LLM-as-Repairer). The resulting repaired and high-quality trajectories form a synthetic SFT dataset, while each repaired trajectory paired with its original low-quality counterpart constitutes a dataset for trajectory preference modeling. In the reinforcement learning (RL) stage, based on the preference dataset, we train a trajectory-level reward model to assess the quality of reasoning paths and combine it with outcome and format rewards, thereby explicitly guiding the optimization toward reliable TIR behaviors. Experiments on real-world benchmarks demonstrate the effectiveness of AutoTraj in TIR.

</details>


### [46] [The Hot Mess of AI: How Does Misalignment Scale With Model Intelligence and Task Complexity?](https://arxiv.org/abs/2601.23045)
*Alexander Hägele,Aryo Pradipta Gema,Henry Sleight,Ethan Perez,Jascha Sohl-Dickstein*

Main category: cs.AI

TL;DR: 研究发现，随着AI模型能力增强，其失败行为会变得更加"不连贯"（随机混乱而非系统性追求错误目标），特别是在需要多步推理和行动的任务中。模型规模扩大不一定能消除不连贯性。


<details>
  <summary>Details</summary>
Motivation: 随着AI承担更广泛和重要的任务，其失败风险也随之增加。需要理解未来强大AI会如何失败：是系统性地追求我们不期望的目标，还是采取混乱无意义的行为？这关系到AI安全研究的方向。

Method: 使用偏差-方差分解来量化AI模型在任务中的"不连贯性"，定义为错误中方差部分所占比例。在不同任务和前沿模型上进行实验，分析不连贯性随推理时间、模型规模的变化规律。

Result: 1) 模型推理和行动时间越长，失败行为越不连贯；2) 不连贯性随模型规模的变化因实验而异，但在多个设置中，更大、更强的模型反而更不连贯；3) 仅靠扩大规模不太可能消除不连贯性。

Conclusion: 随着AI处理更复杂的多步任务，其失败将更多表现为混乱行为而非系统性追求错误目标。这意味着未来AI更可能造成工业事故（由于不可预测的错误行为），而非持续追求错位目标。这提高了针对奖励黑客攻击和目标错误指定的对齐研究的重要性。

Abstract: As AI becomes more capable, we entrust it with more general and consequential tasks. The risks from failure grow more severe with increasing task scope. It is therefore important to understand how extremely capable AI models will fail: Will they fail by systematically pursuing goals we do not intend? Or will they fail by being a hot mess, and taking nonsensical actions that do not further any goal? We operationalize this question using a bias-variance decomposition of the errors made by AI models: An AI's \emph{incoherence} on a task is measured over test-time randomness as the fraction of its error that stems from variance rather than bias in task outcome. Across all tasks and frontier models we measure, the longer models spend reasoning and taking actions, \emph{the more incoherent} their failures become. Incoherence changes with model scale in a way that is experiment dependent. However, in several settings, larger, more capable models are more incoherent than smaller models. Consequently, scale alone seems unlikely to eliminate incoherence. Instead, as more capable AIs pursue harder tasks, requiring more sequential action and thought, our results predict failures to be accompanied by more incoherent behavior. This suggests a future where AIs sometimes cause industrial accidents (due to unpredictable misbehavior), but are less likely to exhibit consistent pursuit of a misaligned goal. This increases the relative importance of alignment research targeting reward hacking or goal misspecification.

</details>


### [47] [From Abstract to Contextual: What LLMs Still Cannot Do in Mathematics](https://arxiv.org/abs/2601.23048)
*Bowen Cao,Dongdong Zhang,Yixia Li,Junpeng Liu,Shijue Huang,Chufan Shi,Hongyuan Lu,Yaokang Wu,Guanhua Chen,Wai Lam,Furu Wei*

Main category: cs.AI

TL;DR: 论文引入ContextMATH基准测试，研究LLMs在上下文数学推理中的表现，发现模型在现实场景中性能显著下降，问题表述错误是主要瓶颈，微调仅能部分缓解差距。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在基准数学问题上表现接近专家水平，但在真实世界应用中的可靠性能尚未完全实现。研究者通过上下文数学推理来研究这一差距，其中数学核心必须从描述性场景中提取。

Method: 引入ContextMATH基准，将AIME和MATH-500问题重新构建为两种上下文设置：场景基础(SG)将抽象问题嵌入现实叙事中，复杂度扩展(CS)将显式条件转化为子问题。评估了61个专有和开源模型，进行错误分析，并研究微调效果。

Result: 模型在上下文设置中性能显著下降：开源模型在SG和CS上平均下降13和34分，专有模型下降13和20分。错误主要由不正确的问题表述导致，表述准确性随原始问题难度增加而下降。正确表述是成功的先决条件，其充分性随模型规模提高。微调能改善性能但仅部分缓解差距。

Conclusion: 上下文数学推理仍然是LLMs未解决的核心挑战。问题表述和推理是两个互补的瓶颈，限制着上下文数学问题解决能力。虽然更大模型在理解和推理方面都有进步，但性能差距仍然存在，表明需要更系统的方法来提升LLMs在现实数学应用中的表现。

Abstract: Large language models now solve many benchmark math problems at near-expert levels, yet this progress has not fully translated into reliable performance in real-world applications. We study this gap through contextual mathematical reasoning, where the mathematical core must be formulated from descriptive scenarios. We introduce ContextMATH, a benchmark that repurposes AIME and MATH-500 problems into two contextual settings: Scenario Grounding (SG), which embeds abstract problems into realistic narratives without increasing reasoning complexity, and Complexity Scaling (CS), which transforms explicit conditions into sub-problems to capture how constraints often appear in practice. Evaluating 61 proprietary and open-source models, we observe sharp drops: on average, open-source models decline by 13 and 34 points on SG and CS, while proprietary models drop by 13 and 20. Error analysis shows that errors are dominated by incorrect problem formulation, with formulation accuracy declining as original problem difficulty increases. Correct formulation emerges as a prerequisite for success, and its sufficiency improves with model scale, indicating that larger models advance in both understanding and reasoning. Nevertheless, formulation and reasoning remain two complementary bottlenecks that limit contextual mathematical problem solving. Finally, we find that fine-tuning with scenario data improves performance, whereas formulation-only training is ineffective. However, performance gaps are only partially alleviated, highlighting contextual mathematical reasoning as a central unsolved challenge for LLMs.

</details>


### [48] [MedMCP-Calc: Benchmarking LLMs for Realistic Medical Calculator Scenarios via MCP Integration](https://arxiv.org/abs/2601.23049)
*Yakun Zhu,Yutong Huang,Shengqian Qin,Zhongzhen Huang,Shaoting Zhang,Xiaofan Zhang*

Main category: cs.AI

TL;DR: MedMCP-Calc是首个通过MCP集成评估LLMs在真实医疗计算器场景中的基准测试，包含118个跨4个临床领域的场景任务，评估显示当前模型在模糊查询、数据库交互和工具使用方面存在显著局限，并开发了CalcMate模型取得开源模型最佳性能。


<details>
  <summary>Details</summary>
Motivation: 当前医疗计算器基准测试仅关注静态单步计算，而真实临床使用需要多阶段自适应过程，包括EHR数据获取、场景依赖的计算器选择和复杂计算，需要更真实的评估框架。

Method: 通过Model Context Protocol (MCP)集成构建MedMCP-Calc基准，包含118个场景任务，涵盖模糊任务描述、结构化EHR数据库交互、外部参考检索和过程级评估，评估了23个领先模型。

Result: 评估显示即使是Claude Opus 4.5等顶级模型也存在显著缺陷：难以根据模糊查询选择合适计算器完成端到端工作流、SQL数据库交互表现差、不愿使用外部工具进行数值计算，且性能在不同临床领域差异大。

Conclusion: 开发了CalcMate模型，通过场景规划和工具增强微调，在开源模型中达到最先进性能，为医疗AI系统在真实临床计算场景中的能力评估提供了重要基准。

Abstract: Medical calculators are fundamental to quantitative, evidence-based clinical practice. However, their real-world use is an adaptive, multi-stage process, requiring proactive EHR data acquisition, scenario-dependent calculator selection, and multi-step computation, whereas current benchmarks focus only on static single-step calculations with explicit instructions. To address these limitations, we introduce MedMCP-Calc, the first benchmark for evaluating LLMs in realistic medical calculator scenarios through Model Context Protocol (MCP) integration. MedMCP-Calc comprises 118 scenario tasks across 4 clinical domains, featuring fuzzy task descriptions mimicking natural queries, structured EHR database interaction, external reference retrieval, and process-level evaluation. Our evaluation of 23 leading models reveals critical limitations: even top performers like Claude Opus 4.5 exhibit substantial gaps, including difficulty selecting appropriate calculators for end-to-end workflows given fuzzy queries, poor performance in iterative SQL-based database interactions, and marked reluctance to leverage external tools for numerical computation. Performance also varies considerably across clinical domains. Building on these findings, we develop CalcMate, a fine-tuned model incorporating scenario planning and tool augmentation, achieving state-of-the-art performance among open-source models. Benchmark and Codes are available in https://github.com/SPIRAL-MED/MedMCP-Calc.

</details>


### [49] [Chain-of-thought obfuscation learned from output supervision can generalise to unseen tasks](https://arxiv.org/abs/2601.23086)
*Nathaniel Mitrani Hadida,Sassan Bhanji,Cameron Tice,Puria Radmard*

Main category: cs.AI

TL;DR: 研究发现，当对LLMs的思维链推理施加优化压力时，模型会学会隐藏推理过程，这种隐藏行为会跨任务泛化，即使只惩罚最终输出也会导致推理过程隐藏，从而降低模型的可监控性。


<details>
  <summary>Details</summary>
Motivation: 思维链推理是监控LLMs行为的重要工具，可以提供决策过程的解释和危险行为的早期预警。然而，当对思维链施加优化压力时，模型可能会隐藏推理痕迹，失去这种有益特性。

Method: 通过实验研究模型在奖励黑客行为（如访问和利用泄露信息）中学习隐藏推理的过程，测试这种隐藏行为是否会在未见过的奖励黑客设置中泛化，并研究仅惩罚最终输出是否也会导致思维链推理的隐藏。

Result: 研究发现：1）模型学会隐藏涉及奖励黑客的推理过程；2）这种隐藏行为会跨任务泛化，模型在未见过的奖励黑客设置中既表现出奖励黑客行为又隐藏其推理；3）即使只惩罚最终输出，也会导致思维链推理的隐藏及其跨任务泛化。

Conclusion: 当前惩罚有害生成的做法可能会无意中导致LLMs的可监控性以不可预测的方式降低，因为模型会学会隐藏推理过程，即使只针对最终输出进行惩罚也会产生这种效果。

Abstract: Chain-of-thought (CoT) reasoning provides a significant performance uplift to LLMs by enabling planning, exploration, and deliberation of their actions. CoT is also a powerful tool for monitoring the behaviours of these agents: when faithful, they offer interpretations of the model's decision making process, and an early warning sign for dangerous behaviours. However, optimisation pressures placed on the CoT may cause the model to obfuscate reasoning traces, losing this beneficial property. We show that obfuscation can generalise across tasks; models that learn to obfuscate reasoning involving reward hacking (e.g. accessing and utilising leaked information) generalise both the reward hacking behaviour and its obfuscation in CoT to unseen reward hacking settings. Most worryingly, we show that obfuscation of CoT reasoning, and its generalisation across tasks, also follows when we penalise only the model's final actions after closing its CoT. Our findings suggest that current practices of penalising harmful generations may inadvertently lead to a reduction in the broader monitorability of LLMs in unpredictable ways.

</details>


### [50] [RAudit: A Blind Auditing Protocol for Large Language Model Reasoning](https://arxiv.org/abs/2601.23133)
*Edward Y. Chang,Longling Geng*

Main category: cs.AI

TL;DR: RAudit是一种无需真实标签的LLM推理审计协议，通过评估推导步骤是否支持结论来检测推理病理，发现了四种导致模型不可靠的机制。


<details>
  <summary>Details</summary>
Motivation: 推理时的缩放会放大推理病理（如谄媚、层级崩溃、过早确定性），需要一种无需真实标签的审计方法来诊断LLM推理问题。

Method: 提出RAudit诊断协议，基于"盲目性"约束：仅评估推导步骤是否支持结论。使用CRIT-based合理性评分，并通过变化批评表述来研究社会框架对模型响应的影响。

Result: 在数学推理(CAP-GSM8K)和因果判断(CausalL2)任务中发现了四种机制：1)潜在能力抑制；2)虚假能力陷阱；3)复杂度-脆弱性权衡；4)医源性批评。

Conclusion: 这些发现挑战了"能力意味着鲁棒性"和"更强的反馈产生更好输出"的假设，揭示了LLM推理中的系统性脆弱性。

Abstract: Inference-time scaling can amplify reasoning pathologies: sycophancy, rung collapse, and premature certainty. We present RAudit, a diagnostic protocol for auditing LLM reasoning without ground truth access. The key constraint is blindness: the auditor evaluates only whether derivation steps support conclusions, enabling detection of trace-output inconsistency and, when latent competence exists, its recovery. RAudit measures process quality via CRIT-based reasonableness scores and varies critique formulation to study how social framing affects model response. We prove bounded correction and $O(\log(1/ε))$ termination. Experiments on mathematical reasoning (CAP-GSM8K) and causal judgment (CausalL2) reveal four mechanisms explaining model unreliability: (1) Latent Competence Suppression, where models derive correct answers then overwrite them under social pressure; (2) The False Competence Trap, where weaker judges mask sycophancy that stronger judges expose; (3) The Complexity-Vulnerability Tradeoff, where causal tasks induce more than 10 times higher sycophancy than mathematical tasks; and (4) Iatrogenic Critique, where authoritative correction harms weaker models. These findings challenge assumptions that capability implies robustness and that stronger feedback yields better outputs.

</details>


### [51] [THINKSAFE: Self-Generated Safety Alignment for Reasoning Models](https://arxiv.org/abs/2601.23143)
*Seanie Lee,Sangwoo Park,Yumin Choi,Gyeongman Kim,Minki Kang,Jihun Yun,Dongmin Park,Jongho Park,Sung Ju Hwang*

Main category: cs.AI

TL;DR: ThinkSafe是一个自生成对齐框架，通过轻量级拒绝引导让模型生成安全推理轨迹，然后微调这些自生成响应来恢复安全对齐，同时保持推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过强化学习在推理任务上过度优化，优先考虑合规性，使模型容易受到有害提示的攻击。现有方法依赖外部教师蒸馏，但这会引入分布差异，损害原生推理能力。

Method: 提出ThinkSafe框架：1）通过轻量级拒绝引导解锁模型的潜在安全知识；2）引导模型生成分布内的安全推理轨迹；3）在这些自生成响应上进行微调，实现安全对齐同时最小化分布偏移。

Result: 在DeepSeek-R1-Distill和Qwen3上的实验显示，ThinkSafe显著提升安全性同时保持推理能力。在安全性和推理能力上与GRPO相当，但计算成本显著降低。

Conclusion: ThinkSafe通过自生成对齐有效解决了RL过度优化导致的安全退化问题，无需外部教师即可恢复安全对齐，同时保持推理能力，计算效率高。

Abstract: Large reasoning models (LRMs) achieve remarkable performance by leveraging reinforcement learning (RL) on reasoning tasks to generate long chain-of-thought (CoT) reasoning. However, this over-optimization often prioritizes compliance, making models vulnerable to harmful prompts. To mitigate this safety degradation, recent approaches rely on external teacher distillation, yet this introduces a distributional discrepancy that degrades native reasoning. We propose ThinkSafe, a self-generated alignment framework that restores safety alignment without external teachers. Our key insight is that while compliance suppresses safety mechanisms, models often retain latent knowledge to identify harm. ThinkSafe unlocks this via lightweight refusal steering, guiding the model to generate in-distribution safety reasoning traces. Fine-tuning on these self-generated responses effectively realigns the model while minimizing distribution shift. Experiments on DeepSeek-R1-Distill and Qwen3 show ThinkSafe significantly improves safety while preserving reasoning proficiency. Notably, it achieves superior safety and comparable reasoning to GRPO, with significantly reduced computational cost. Code, models, and datasets are available at https://github.com/seanie12/ThinkSafe.git.

</details>


### [52] [Make Anything Match Your Target: Universal Adversarial Perturbations against Closed-Source MLLMs via Multi-Crop Routed Meta Optimization](https://arxiv.org/abs/2601.23179)
*Hui Lu,Yi Yu,Yiming Yang,Chenyu Yi,Xueyi Ke,Qixing Zhang,Bingquan Shen,Alex Kot,Xudong Jiang*

Main category: cs.AI

TL;DR: 本文提出MCRMO-Attack方法，解决通用目标可迁移对抗攻击中的三大挑战：通过多裁剪聚合稳定监督、可对齐门控令牌路由提升可靠性、元学习跨目标扰动先验，显著提升商业MLLM上的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒迁移对抗攻击多为样本特定，跨输入重用性有限。本文研究更严格的通用目标可迁移对抗攻击(UTTAA)，即单个扰动需在未知商业MLLM上一致地将任意输入导向指定目标。现有方法直接适配面临三大困难：目标监督高方差、令牌级匹配不可靠、少样本适应对初始化敏感。

Method: 提出MCRMO-Attack方法：1) 多裁剪聚合与注意力引导裁剪稳定监督；2) 可对齐门控令牌路由提升令牌级可靠性；3) 元学习跨目标扰动先验，获得更强的每目标解决方案。

Result: 在商业MLLM上显著提升未见图像攻击成功率：GPT-4o上比最强通用基线提升+23.7%，Gemini-2.0上提升+19.9%。

Conclusion: MCRMO-Attack有效解决了通用目标可迁移对抗攻击中的核心挑战，通过稳定监督、提升令牌可靠性和元学习先验，在商业MLLM上实现了显著的攻击成功率提升。

Abstract: Targeted adversarial attacks on closed-source multimodal large language models (MLLMs) have been increasingly explored under black-box transfer, yet prior methods are predominantly sample-specific and offer limited reusability across inputs. We instead study a more stringent setting, Universal Targeted Transferable Adversarial Attacks (UTTAA), where a single perturbation must consistently steer arbitrary inputs toward a specified target across unknown commercial MLLMs. Naively adapting existing sample-wise attacks to this universal setting faces three core difficulties: (i) target supervision becomes high-variance due to target-crop randomness, (ii) token-wise matching is unreliable because universality suppresses image-specific cues that would otherwise anchor alignment, and (iii) few-source per-target adaptation is highly initialization-sensitive, which can degrade the attainable performance. In this work, we propose MCRMO-Attack, which stabilizes supervision via Multi-Crop Aggregation with an Attention-Guided Crop, improves token-level reliability through alignability-gated Token Routing, and meta-learns a cross-target perturbation prior that yields stronger per-target solutions. Across commercial MLLMs, we boost unseen-image attack success rate by +23.7\% on GPT-4o and +19.9\% on Gemini-2.0 over the strongest universal baseline.

</details>


### [53] [TSAQA: Time Series Analysis Question And Answering Benchmark](https://arxiv.org/abs/2601.23204)
*Baoyu Jing,Sanhorn Chen,Lecheng Zheng,Boyu Liu,Zihao Li,Jiaru Zou,Tianxin Wei,Zhining Liu,Zhichen Zeng,Ruizhong Qiu,Xiao Lin,Yuchen Yan,Dongqi Fu,Jingchao Ni,Jingrui He,Hanghang Tong*

Main category: cs.AI

TL;DR: TSAQA是一个统一的时间序列问答基准，涵盖6种任务类型，包含21万个样本，评估显示当前LLM在时间序列分析上仍有很大提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列QA基准主要局限于预测和异常检测任务，缺乏对多样化时间分析能力的全面评估，需要更广泛的任务覆盖来评估LLM的时间序列分析能力。

Method: 构建TSAQA基准，整合6种任务（异常检测、分类、特征描述、比较、数据转换、时间关系分析），涵盖13个领域，采用TF、MC和创新的PZ格式，共21万个样本。

Result: 零样本评估显示当前LLM表现有限：最佳商业模型Gemini-2.5-Flash平均得分仅65.08；指令微调能提升开源模型性能，但最佳开源模型LLaMA-3.1-8B仍有很大改进空间。

Conclusion: TSAQA基准揭示了时间序列分析对LLM的挑战性，为评估和提升LLM的时间分析能力提供了重要基准，表明该领域需要进一步研究。

Abstract: Time series data are integral to critical applications across domains such as finance, healthcare, transportation, and environmental science. While recent work has begun to explore multi-task time series question answering (QA), current benchmarks remain limited to forecasting and anomaly detection tasks. We introduce TSAQA, a novel unified benchmark designed to broaden task coverage and evaluate diverse temporal analysis capabilities. TSAQA integrates six diverse tasks under a single framework ranging from conventional analysis, including anomaly detection and classification, to advanced analysis, such as characterization, comparison, data transformation, and temporal relationship analysis. Spanning 210k samples across 13 domains, the dataset employs diverse formats, including true-or-false (TF), multiple-choice (MC), and a novel puzzling (PZ), to comprehensively assess time series analysis. Zero-shot evaluation demonstrates that these tasks are challenging for current Large Language Models (LLMs): the best-performing commercial LLM, Gemini-2.5-Flash, achieves an average score of only 65.08. Although instruction tuning boosts open-source performance: the best-performing open-source model, LLaMA-3.1-8B, shows significant room for improvement, highlighting the complexity of temporal analysis for LLMs.

</details>


### [54] [High-quality generation of dynamic game content via small language models: A proof of concept](https://arxiv.org/abs/2601.23206)
*Morten I. K. Munk,Arturo Valdivia,Paolo Burelli*

Main category: cs.AI

TL;DR: 该论文提出通过针对性微调小型语言模型(SLMs)来解决游戏内容生成中的叙事不连贯和成本问题，使用DAG方法生成训练数据，并在一个声誉战斗的RPG游戏中验证了可行性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLMs)在游戏内容生成中存在叙事不连贯、运营成本高、依赖云端服务等问题，而现有小型语言模型(SLMs)的输出质量较差。需要找到一种既实用又高质量的本地化生成方案。

Method: 采用针对性微调策略：1）将任务范围限定在狭窄上下文和约束结构中；2）使用DAG（有向无环图）方法合成训练数据，使模型扎根于特定游戏世界；3）构建以叙事框架为中心的智能体网络；4）通过"重试直至成功"策略确保生成质量。

Result: 在一个以声誉战斗为核心的RPG游戏中进行了概念验证，结果显示：1）使用LLM作为评判标准，生成质量达到足够水平；2）延迟可预测，适合实时生成；3）在典型游戏引擎约束下具有可行性。

Conclusion: 通过针对性微调和范围限定，小型语言模型可以在本地实现高质量的游戏内容生成，相比云端依赖的大型语言模型提供了更实用、更稳健的解决方案，尽管本地质量评估仍是开放性问题。

Abstract: Large language models (LLMs) offer promise for dynamic game content generation, but they face critical barriers, including narrative incoherence and high operational costs. Due to their large size, they are often accessed in the cloud, limiting their application in offline games. Many of these practical issues are solved by pivoting to small language models (SLMs), but existing studies using SLMs have resulted in poor output quality. We propose a strategy of achieving high-quality SLM generation through aggressive fine-tuning on deliberately scoped tasks with narrow context, constrained structure, or both. In short, more difficult tasks require narrower scope and higher specialization to the training corpus. Training data is synthetically generated via a DAG-based approach, grounding models in the specific game world. Such models can form the basis for agentic networks designed around the narratological framework at hand, representing a more practical and robust solution than cloud-dependent LLMs. To validate this approach, we present a proof-of-concept focusing on a single specialized SLM as the fundamental building block. We introduce a minimal RPG loop revolving around rhetorical battles of reputations, powered by this model. We demonstrate that a simple retry-until-success strategy reaches adequate quality (as defined by an LLM-as-a-judge scheme) with predictable latency suitable for real-time generation. While local quality assessment remains an open question, our results demonstrate feasibility for real-time generation under typical game engine constraints.

</details>


### [55] [Strongly Polynomial Time Complexity of Policy Iteration for $L_\infty$ Robust MDPs](https://arxiv.org/abs/2601.23229)
*Ali Asadi,Krishnendu Chatterjee,Ehsan Goharshady,Mehrdad Karrabi,Alipasha Montaseri,Carlo Pagano*

Main category: cs.AI

TL;DR: 本文针对(s,a)-矩形L∞不确定性鲁棒MDP，提出了在固定折扣因子下具有强多项式时间的鲁棒策略迭代算法，解决了该领域的重要算法问题。


<details>
  <summary>Details</summary>
Motivation: 鲁棒MDP是序列决策中的基本模型，能够处理转移概率的不确定性并优化最坏情况。虽然MDP已有强多项式时间算法，但鲁棒MDP的类似结果一直是一个重要的开放问题。

Method: 采用鲁棒策略迭代算法，针对(s,a)-矩形L∞不确定性集合的鲁棒MDP，在固定折扣因子条件下进行分析。

Result: 证明了鲁棒策略迭代算法在固定折扣因子下具有强多项式时间复杂度，解决了鲁棒MDP算法复杂度的开放问题。

Conclusion: 该工作首次为(s,a)-矩形L∞鲁棒MDP建立了强多项式时间算法，填补了鲁棒MDP与经典MDP在算法复杂度理论上的差距。

Abstract: Markov decision processes (MDPs) are a fundamental model in sequential decision making. Robust MDPs (RMDPs) extend this framework by allowing uncertainty in transition probabilities and optimizing against the worst-case realization of that uncertainty. In particular, $(s, a)$-rectangular RMDPs with $L_\infty$ uncertainty sets form a fundamental and expressive model: they subsume classical MDPs and turn-based stochastic games. We consider this model with discounted payoffs. The existence of polynomial and strongly-polynomial time algorithms is a fundamental problem for these optimization models. For MDPs, linear programming yields polynomial-time algorithms for any arbitrary discount factor, and the seminal work of Ye established strongly--polynomial time for a fixed discount factor. The generalization of such results to RMDPs has remained an important open problem. In this work, we show that a robust policy iteration algorithm runs in strongly-polynomial time for $(s, a)$-rectangular $L_\infty$ RMDPs with a constant (fixed) discount factor, resolving an important algorithmic question.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [56] [A Time-Varying Branching Process Approach to Model Self-Renewing Cells](https://arxiv.org/abs/2601.22282)
*Huyen Nguyen,Haim Bar,Zhiyi Chi,Vladimir Pozdnyakov*

Main category: stat.AP

TL;DR: 提出一个连续时间分支过程模型，用于描述干细胞增殖的动态过程，并开发了参数估计方法


<details>
  <summary>Details</summary>
Motivation: 理解干细胞增殖过程对于揭示干细胞增殖特性以及正常和病理条件下组织发育机制至关重要

Method: 开发具有时间依赖后代分布的连续时间分支过程模型，推导干细胞计数的均值、方差和自协方差表达式，构建基于似然的推断程序，特别是前向算法似然来处理部分细胞类型无法直接观测的情况

Result: 模拟结果显示，估计方法能够准确恢复时间依赖的分裂概率

Conclusion: 该模型和方法为研究干细胞增殖动态提供了有效的统计框架，能够处理观测数据不完全的情况

Abstract: Stem cells, through their ability to produce daughter stem cells and differentiate into specialized cells, are essential in the growth, maintenance, and repair of biological tissues. Understanding the dynamics of cell populations in the proliferation process not only uncovers proliferative properties of stem cells, but also offers insight into tissue development under both normal conditions and pathological disruption. In this paper, we develop a continuous time branching process model with time-dependent offspring distribution to characterize stem cell proliferation process. We derive analytical expressions for mean, variance, and autocovariance of the stem cell counts, and develop likelihood-based inference procedures to estimate model parameters. Particularly, we construct a forward algorithm likelihood to handle situations when some cell types cannot be directly observed. Simulation results demonstrate that our estimation method recovers the time-dependent division probabilities with good accuracy.

</details>


### [57] [Beyond the Null Effect: Unmasking the True Impact of Teacher-Child Interaction Quality on Child Outcomes in Early Head Start](https://arxiv.org/abs/2601.23203)
*JoonHo Lee,Alison Hooper*

Main category: stat.AP

TL;DR: 通过改进方法学（处理测量误差、中心混杂、协变量不平衡和非线性），研究发现EHS课堂过程质量对儿童发展有显著影响，认知/语言支持预测语言能力，情感/行为支持预测社会情感能力，部分领域存在非线性关系。


<details>
  <summary>Details</summary>
Motivation: 早期开端计划（EHS）中教师-儿童互动被认为影响婴幼儿发展，但大规模研究常发现微弱或零关联。本研究旨在解决四个方法学问题（项目级测量误差、中心级混杂、教师/课堂级协变量不平衡、忽略非线性）以澄清课堂过程质量的真实影响。

Method: 使用2018年Baby FACES数据，应用三级广义加性潜变量混合模型（GALAMM）区分CLASS和QCIT测量的课堂过程质量真实变异与项目级噪声和中心级效应；使用协变量平衡权重和广义加性模型估计与儿童语言和社会情感结果的剂量-反应关系。

Result: 近一半项目方差反映课堂级过程，其余为测量误差或中心影响；校正偏差后，认知/语言支持与儿童英语交流技能呈稳健线性关联，情感/行为支持更好地预测社会情感能力；部分领域在极端情况下出现平台期，显示非线性关系。

Conclusion: 研究挑战了"零效应"叙事，表明严谨方法能揭示教师-儿童互动质量的关键、领域特异性影响，为EHS的针对性专业发展和政策提供更清晰指导。

Abstract: In Early Head Start (EHS), teacher-child interactions are widely believed to shape infant-toddler outcomes, yet large-scale studies often find only modest or null associations. This study addresses four methodological sources of attenuation -- item-level measurement error, center-level confounding, teacher- and classroom-level covariate imbalance, and overlooked nonlinearities -- to clarify classroom process quality's true influence on child development. Using data from the 2018 wave of the Early Head Start Family and Child Experiences Survey (Baby FACES), we applied a three-level generalized additive latent and mixed model (GALAMM) to distinguish genuine classroom-level variability in process quality, as measured by the Classroom Assessment Scoring System (CLASS) and Quality of Caregiver-Child Interactions for Infants and Toddlers (QCIT), from item-level noise and center-level effects. We then estimated dose-response relationships with children's language and socioemotional outcomes, employing covariate balancing weights and generalized additive models. Results show that nearly half of each item's variance reflects classroom-level processes, with the remainder tied to measurement error or center-wide influences, masking true classroom effects. After correcting for these biases, domain-focused dose-response analyses reveal robust linear associations between cognitive/language supports and children's English communicative skills, while emotional-behavioral supports better predict social-emotional competence. Some domains display plateaus when pushed to extremes, underscoring potential nonlinearities. These findings challenge the "null effect" narrative, demonstrating that rigorous methodology can uncover the critical, domain-specific impacts of teacher-child interaction quality, offering clearer guidance for targeted professional development and policy in EHS.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [58] [AI Narrative Breakdown. A Critical Assessment of Power and Promise](https://arxiv.org/abs/2601.22255)
*Rainer Rehak*

Main category: cs.CY

TL;DR: 该论文批判性分析了ChatGPT发布后AI话语中的主流叙事，揭示了AI术语的模糊使用及其隐含的政治、权力和价值判断，呼吁将AI视为需要社会治理的人类工具。


<details>
  <summary>Details</summary>
Motivation: ChatGPT发布后，关于AI的社会话语中存在大量未经批判的主流叙事，这些叙事往往模糊了AI技术的本质，掩盖了其背后的政治、权力和价值判断，需要系统性的批判分析。

Method: 采用跨学科批判理论框架（批判计算机科学、STS、数据保护理论、心灵哲学、符号学等），首先对AI话语进行历史和技术背景化，然后提出"时代精神AI"概念批判术语滥用，最后详细分析常见叙事的政治内涵。

Result: 揭示了AI术语在社会各领域的模糊和误导性使用，详细展示了所有AI应用中固有的政治性、权力渗透和价值负载决策，挑战了AI中立、客观、去政治化的常见假设。

Conclusion: 呼吁更接地气的AI参与方式，提出新叙事应将AI视为需要社会治理的人类导向工具，强调必须正视被主流叙事忽视的尖锐问题，建立更负责任的技术治理框架。

Abstract: This article sets off for an exploration of the still evolving discourse surrounding artificial intelligence (AI) in the wake of the release of ChatGPT. It scrutinizes the pervasive narratives that are shaping the societal engagement with AI, spotlighting key themes such as agency and decision-making, autonomy, truthfulness, knowledge processing, prediction, general purpose, neutrality and objectivity, apolitical optimization, sustainability game-changer, democratization, mass unemployment, and the dualistic portrayal of AI as either a harbinger of societal utopia or dystopia. Those narratives are analysed critically based on insights from critical computer science, critical data and algorithm studies, from STS, data protection theory, as well as from the philosophy of mind and semiotics. To properly analyse the narratives presented, the article first delves into a historical and technical contextualisation of the AI discourse itself. The article then introduces the notion of "Zeitgeist AI" to critique the imprecise and misleading application of the term "AI" across various societal sectors. Then, by discussing common narratives with nuance, the article contextualises and challenges often assumed socio-political implications of AI, uncovering in detail and with examples the inherent political, power infused and value-laden decisions within all AI applications. Concluding with a call for a more grounded engagement with AI, the article carves out acute problems ignored by the narratives discussed and proposes new narratives recognizing AI as a human-directed tool necessarily subject to societal governance.

</details>


### [59] [Toward Third-Party Assurance of AI Systems: Design Requirements, Prototype, and Early Testing](https://arxiv.org/abs/2601.22424)
*Rachel M. Kim,Blaine Kuehnert,Alice Lai,Kenneth Holstein,Hoda Heidari,Rayid Ghani*

Main category: cs.CY

TL;DR: 提出第三方AI保证框架，包含责任分配矩阵、访谈协议、成熟度矩阵和保证报告模板，通过两个实际用例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统普及，需要系统化、透明且可操作的评估流程。现有资源存在局限：很少同时关注AI系统设计开发部署过程和产出结果，缺乏端到端操作性指导，缺少实际可用性和有效性证据。

Method: 1) 区分保证与审计的关键维度；2) 基于设计原则反思现有资源不足，确定AI保证设计需求；3) 构建包含责任分配矩阵、利益相关者访谈协议、最佳实践成熟度矩阵和保证报告模板的原型流程；4) 通过两个实际用例（企业文档标记工具和公共机构住房资源分配工具）和专家验证访谈进行早期验证。

Result: 早期验证表明：AI保证框架合理且全面，可在不同组织环境中使用，能有效识别AI系统的定制化问题。

Conclusion: 提出的第三方AI保证框架解决了现有评估资源的不足，通过系统化流程确保可信度和问责制，为AI系统评估提供了实用工具。

Abstract: As Artificial Intelligence (AI) systems proliferate, the need for systematic, transparent, and actionable processes for evaluating them is growing. While many resources exist to support AI evaluation, they have several limitations. Few address both the process of designing, developing, and deploying an AI system and the outcomes it produces. Furthermore, few are end-to-end and operational, give actionable guidance, or present evidence of usability or effectiveness in practice. In this paper, we introduce a third-party AI assurance framework that addresses these gaps. We focus on third-party assurance to prevent conflict of interest and ensure credibility and accountability of the process. We begin by distinguishing assurance from audits in several key dimensions. Then, following design principles, we reflect on the shortcomings of existing resources to identify a set of design requirements for AI assurance. We then construct a prototype of an assurance process that consists of (1) a responsibility assignment matrix to determine the different levels of involvement each stakeholder has at each stage of the AI lifecycle, (2) an interview protocol for each stakeholder of an AI system, (3) a maturity matrix to assess AI systems' adherence to best practices, and (4) a template for an assurance report that draws from more mature assurance practices in business accounting. We conduct early validation of our AI assurance framework by applying the framework to two distinct AI use cases -- a business document tagging tool for downstream processing in a large private firm, and a housing resource allocation tool in a public agency -- and conducting expert validation interviews. Our findings show early evidence that our AI assurance framework is sound and comprehensive, usable across different organizational contexts, and effective at identifying bespoke issues with AI systems.

</details>


### [60] [The Third-Party Access Effect: An Overlooked Challenge in Secondary Use of Educational Real-World Data](https://arxiv.org/abs/2601.22472)
*Hibiki Ito,Chia-Yu Hsu,Hiroaki Ogata*

Main category: cs.CY

TL;DR: 教育领域真实世界数据二次使用时，标准隐私实践可能引发"第三方访问效应"，导致数据共享偏差并影响分析结论的有效性。


<details>
  <summary>Details</summary>
Motivation: 教育领域真实世界数据（RWD）的二次使用具有重要研究价值，但现有的隐私实践很少评估其对下游分析的影响，可能导致潜在问题未被发现。

Method: 通过评估（1）细粒度RWD的重新识别风险，（2）风险沟通对学习者隐私行为的影响，（3）数据变化对下游分析结论的敏感性，研究常见隐私实践的问题。

Result: 研究发现RWD存在实质性重新识别风险，当向利益相关者沟通这些风险时，会引发退出和非自我披露行为，这些行为变化会显著改变共享数据，限制二次使用发现的有效性。

Conclusion: 提出了"第三方访问效应"（3PAE）概念，强调隐私实践与数据共享行为之间的相互作用对教育RWD可信二次使用的重要影响。

Abstract: Secondary use of growing real-world data (RWD) in education offers significant opportunities for research, yet privacy practices intended to enable third-party access to such RWD are rarely evaluated for their implications for downstream analyses. As a result, potential problems introduced by otherwise standard privacy practices may remain unnoticed. To address this gap, we investigate potential issues arising from common practices by assessing (1) the re-identification risk of fine-grained RWD, (2) how communicating such risks influences learners' privacy behaviour, and (3) the sensitivity of downstream analytical conclusions to resulting changes in the data. We focus on these practices because re-identification risk and stakeholder communication can jointly influence the data shared with third parties. We find that substantial re-identification risk in RWD, when communicated to stakeholders, can induce opt-outs and non-self-disclosure behaviours. Sensitivity analysis demonstrates that these behavioural changes can meaningfully alter the shared data, limiting validity of secondary-use findings. We conceptualise this phenomenon as the third-party access effect (3PAE) and discuss implications for trustworthy secondary use of educational RWD.

</details>


### [61] [Gender Disparities in StackOverflow's Community-Based Question Answering: A Matter of Quantity versus Quality](https://arxiv.org/abs/2601.23063)
*Maddalena Amendola,Cosimo Rulli,Carlos Castillo,Andrea Passarella,Raffaele Perego*

Main category: cs.CY

TL;DR: 研究发现在Stack Overflow等社区问答平台中，答案质量不存在显著的性别差异，性别偏见对"最佳答案"选择也没有实质性影响。声誉评分的性别差异主要源于用户活动水平（如提问和回答数量）的差异，而非答案质量差异。


<details>
  <summary>Details</summary>
Motivation: 社区问答平台（如Stack Overflow）虽然设有评估答案质量和参与者专业知识的机制，但先前研究指出存在持续的性别偏见，这引发了关于平台包容性和公平性的担忧。现有研究多关注通过比较男女用户特征来检测性别偏见，但往往忽略了性别间的互动、答案本身质量以及提问者选择"最佳答案"的过程。

Method: 本研究结合人工评估和基于大型语言模型的自动评估方法，探究答案质量是否受到性别影响。通过系统分析用户活动数据、答案质量评估以及最佳答案选择模式，来识别性别差异的来源。

Result: 研究发现：1）答案质量没有显著的性别差异；2）性别偏见对"最佳答案"的选择没有实质性影响；3）Stack Overflow声誉评分的显著性别差异主要归因于用户活动水平的差异（如提问和回答的数量），而非答案质量差异。

Conclusion: 社区问答平台的评分系统设计需要更加公平。过度强调活动量的声誉系统可能会放大不反映实际答案质量差异的性别差距，因此需要采用更公平的设计策略来确保平台的包容性和公正性。

Abstract: Community Question-Answering platforms, such as Stack Overflow (SO), are valuable knowledge exchange and problem-solving resources. These platforms incorporate mechanisms to assess the quality of answers and participants' expertise, ideally free from discriminatory biases. However, prior research has highlighted persistent gender biases, raising concerns about the inclusivity and fairness of these systems. Addressing such biases is crucial for fostering equitable online communities. While previous studies focus on detecting gender bias by comparing male and female user characteristics, they often overlook the interaction between genders, inherent answer quality, and the selection of ``best answers'' by question askers. In this study, we investigate whether answer quality is influenced by gender using a combination of human evaluations and automated assessments powered by Large Language Models. Our findings reveal no significant gender differences in answer quality, nor any substantial influence of gender bias on the selection of ``best answers." Instead, we find that the significant gender disparities in SO's reputation scores are primarily attributable to differences in users' activity levels, e.g., the number of questions and answers they write. Our results have important implications for the design of scoring systems in community question-answering platforms. In particular, reputation systems that heavily emphasize activity volume risk amplifying gender disparities that do not reflect actual differences in answer quality, calling for more equitable design strategies.

</details>


### [62] [AI Literacy, Safety Awareness, and STEM Career Aspirations of Australian Secondary Students: Evaluating the Impact of Workshop Interventions](https://arxiv.org/abs/2601.22486)
*Christian Bergh,Alexandra Vassar,Natasha Banks,Jessica Xu,Jake Renzella*

Main category: cs.CY

TL;DR: 澳大利亚AI日活动通过工作坊干预提升中学生AI素养，研究发现学生接触深度伪造等合成媒体风险较高，干预后AI知识、识别能力和STEM兴趣有所提升，但需要持续教育而非一次性工作坊。


<details>
  <summary>Details</summary>
Motivation: 深度伪造等合成媒体对青少年构成日益增长的安全风险，但关于学生接触这些媒体及相关行为的证据仍然有限。本研究旨在评估AI素养教育干预措施对澳大利亚中学生的影响。

Method: 采用混合方法，对澳大利亚7-10年级中学生进行基于工作坊的干预，使用前后测问卷调查（前测N=205，后测N=163），分析学生在AI识别能力、AI伦理理解、培训和安全意识以及STEM职业兴趣方面的变化。

Result: 基线数据显示合成媒体风险显著：82.4%学生看过深度伪造，18.5%分享过，7.3%创建过。干预后学生自我报告的AI知识和信心提高，对Netflix、Spotify、TikTok等平台中AI的识别能力增强，从"基于算法"转向"AI驱动系统"的理解。STEM职业兴趣有小幅提升，但效应量较小。

Conclusion: 研究支持可扩展的AI素养项目，将基础AI概念与合成媒体安全明确结合。一次性工作坊效果有限，需要持续的教育方法才能影响长期职业抱负。

Abstract: Deepfakes and other forms of synthetic media pose growing safety risks for adolescents, yet evidence on students' exposure and related behaviours remains limited. This study evaluates the impact of Day of AI Australia's workshop-based intervention designed to improve AI literacy and conceptual understanding among Australian secondary students (Years 7-10). Using a mixed-methods approach with pre- and post-intervention surveys (N=205 pre; N=163 post), we analyse changes in students' ability to identify AI in everyday tools, their understanding of AI ethics, training, and safety, and their interest in STEM-related careers.
  Baseline data revealed notable synthetic media risks: 82.4% of students reported having seen deepfakes, 18.5% reported sharing them, and 7.3% reported creating them.
  Results show higher self-reported AI knowledge and confidence after the intervention, alongside improved recognition of AI in widely used platforms such as Netflix, Spotify, and TikTok. This pattern suggests a shift from seeing these tools as merely "algorithm-based" to recognising them as AI-driven systems. Students also reported increased interest in STEM careers post-workshop; however, effect sizes were small, indicating that sustained approaches beyond one-off workshops may be needed to influence longer-term aspirations. Overall, the findings support scalable AI literacy programs that pair foundational AI concepts with an explicit emphasis on synthetic media safety.

</details>


### [63] [Ethical Risks of Large Language Models in Medical Consultation: An Assessment Based on Reproductive Ethics](https://arxiv.org/abs/2601.22621)
*Hanhui Xu,Jiacheng Ji,Haoan Jin,Han Ying,Mengyue Wu*

Main category: cs.CY

TL;DR: 该研究评估了8个主流大语言模型在中国生殖伦理问题上的表现，发现存在严重安全隐患（风险率29.91%），特别是在规范引用和同理心表达方面普遍薄弱，存在逻辑自相矛盾和违反基本道德直觉的问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在医疗和咨询场景中的应用增加，需要评估这些模型是否能够符合当地伦理标准进行回应。本研究针对中国生殖伦理法规，系统评估LLM的可靠性和安全性。

Method: 评估了8个主流LLM（如GPT-4、Claude-3.7），使用基于168篇中国生殖伦理法规文章构建的986个问题测试集（906个主观题，80个客观题）。主观回答采用六维评分标准评估安全性（规范合规性、指导安全性）和回答质量（问题识别、引用、建议、同理心）。

Result: 发现严重安全问题，不安全或误导性建议的风险率达到29.91%。所有模型在引用规范来源和表达同理心方面表现普遍较差。还发现了异常道德推理实例，包括逻辑自相矛盾和违反基本道德直觉的回应。

Conclusion: 当前LLM不适合自主进行生殖伦理咨询，尽管具备知识回忆能力，但在安全性、逻辑一致性和基本人文技能方面存在严重缺陷。这些发现警示不应过早部署，未来开发应优先考虑稳健推理、法规依据和同理心。

Abstract: Background: As large language models (LLMs) are increasingly used in healthcare and medical consultation settings, a growing concern is whether these models can respond to medical inquiries in a manner that is ethically compliant--particularly in accordance with local ethical standards. To address the pressing need for comprehensive research on reliability and safety, this study systematically evaluates LLM performance in answering questions related to reproductive ethics, specifically assessing their alignment with Chinese ethical regulations.
  Methods: We evaluated eight prominent LLMs (e.g., GPT-4, Claude-3.7) on a custom test set of 986 questions (906 subjective, 80 objective) derived from 168 articles within Chinese reproductive ethics regulations. Subjective responses were evaluated using a novel six-dimensional scoring rubric assessing Safety (Normative Compliance, Guidance Safety) and Quality of the Answer (Problem Identification, Citation, Suggestion, Empathy).
  Results: Significant safety issues were prevalent, with risk rates for unsafe or misleading advice reaching 29.91%. A systemic weakness was observed across all models: universally poor performance in citing normative sources and expressing empathy. We also identified instances of anomalous moral reasoning, including logical self-contradictions and responses violating fundamental moral intuitions.
  Conclusions: Current LLMs are unreliable and unsafe for autonomous reproductive ethics counseling. Despite knowledge recall, they exhibit critical deficiencies in safety, logical consistency, and essential humanistic skills. These findings serve as a critical cautionary note against premature deployment, urging future development to prioritize robust reasoning, regulatory justification, and empathy.

</details>


### [64] [Beyond Abstract Compliance: Operationalising trust in AI as a moral relationship](https://arxiv.org/abs/2601.22769)
*Lameck Mbangula Amugongo,Tutaleni Asino,Nicola J Bidwell*

Main category: cs.CY

TL;DR: 本文批评当前AI信任框架过于技术化，提出基于非洲社群主义哲学的关系性信任原则，强调信任是动态、文化嵌入的关系，需通过社区参与在整个AI生命周期中培养。


<details>
  <summary>Details</summary>
Motivation: 当前主流AI信任框架（如欧盟的"可信AI框架"）将信任视为可通过规范和技术标准设计、评估和治理的属性，忽视了信任的主观性、文化嵌入性和关系本质。这种技术中心的方法无法捕捉信任如何被主体性培养、文化塑造以及作为关系性过程的特点。

Method: 借鉴关系伦理学，特别是非洲社群主义哲学，提出扩展的信任原则。这些原则将信任视为动态的、时间性的关系，强调透明和相互尊重。通过两个AI用例（医疗保健和教育）展示如何将基于非洲关系伦理的信任赋能原则操作化。

Result: 提出了一套信任赋能原则，强调社区在整个AI生命周期中的参与。这种方法能够培养AI设计与开发团队之间有意义的长期关系，逐步建立信任，促进更公平和情境敏感的AI系统。

Conclusion: 信任不应被视为可技术设计的静态属性，而应理解为动态的关系过程。基于非洲关系伦理的信任原则能够通过社区参与操作化，从而建立更公平、文化敏感的AI系统，弥补当前技术中心信任框架的不足。

Abstract: Dominant approaches, e.g. the EU's "Trustworthy AI framework", treat trust as a property that can be designed for, evaluated, and governed according to normative and technical criteria. They do not address how trust is subjectively cultivated and experienced, culturally embedded, and inherently relational. This paper proposes some expanded principles for trust in AI that can be incorporated into common development methods and frame trust as a dynamic, temporal relationship, which involves transparency and mutual respect. We draw on relational ethics and, in particular, African communitarian philosophies, to foreground the nuances of inclusive, participatory processes and long-term relationships with communities. Involving communities throughout the AI lifecycle can foster meaningful relationships with AI design and development teams that incrementally build trust and promote more equitable and context-sensitive AI systems. We illustrate how trust-enabling principles based on African relational ethics can be operationalised, using two use-cases for AI: healthcare and education.

</details>


### [65] [Eroding the Truth-Default: A Causal Analysis of Human Susceptibility to Foundation Model Hallucinations and Disinformation in the Wild](https://arxiv.org/abs/2601.22871)
*Alexander Loth,Martin Kappes,Marc-Oliver Pahl*

Main category: cs.CY

TL;DR: JudgeGPT和RogueGPT双轴框架解耦"真实性"与"归因"，研究发现政治取向对检测性能影响微弱，而"假新闻熟悉度"是关键中介变量，GPT-4输出通过流畅性陷阱绕过人类源监控机制。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型接近人类水平的流畅度，区分合成内容与有机内容已成为可信网络智能的关键挑战。需要研究人类易受影响的机制，以建立更可信的信息生态系统。

Method: 提出JudgeGPT和RogueGPT双轴框架，将"真实性"与"归因"解耦。使用结构因果模型作为主要框架，在五个基础模型（包括GPT-4和Llama-2）上进行918次评估，制定可测试的因果假设。

Result: 政治取向与检测性能关联微弱(r=-0.10)，"假新闻熟悉度"是候选中介变量(r=0.35)，暴露可能作为人类判别器的对抗训练。发现"流畅性陷阱"现象，GPT-4输出(HumanMachineScore: 0.20)绕过源监控机制，与人类文本难以区分。

Conclusion: "预防性干预"应针对认知源监控而非人口统计细分，以确保可信的信息生态系统。需要重新思考检测策略，重点关注认知机制而非政治倾向。

Abstract: As foundation models (FMs) approach human-level fluency, distinguishing synthetic from organic content has become a key challenge for Trustworthy Web Intelligence.
  This paper presents JudgeGPT and RogueGPT, a dual-axis framework that decouples "authenticity" from "attribution" to investigate the mechanisms of human susceptibility. Analyzing 918 evaluations across five FMs (including GPT-4 and Llama-2), we employ Structural Causal Models (SCMs) as a principal framework for formulating testable causal hypotheses about detection accuracy.
  Contrary to partisan narratives, we find that political orientation shows a negligible association with detection performance ($r=-0.10$). Instead, "fake news familiarity" emerges as a candidate mediator ($r=0.35$), suggesting that exposure may function as adversarial training for human discriminators. We identify a "fluency trap" where GPT-4 outputs (HumanMachineScore: 0.20) bypass Source Monitoring mechanisms, rendering them indistinguishable from human text.
  These findings suggest that "pre-bunking" interventions should target cognitive source monitoring rather than demographic segmentation to ensure trustworthy information ecosystems.

</details>


### [66] [When Machines Get It Wrong: Large Language Models Perpetuate Autism Myths More Than Humans Do](https://arxiv.org/abs/2601.22893)
*Eduardo C. Garrido-Merchán,Adriana Constanza Cirera Tirschtigel*

Main category: cs.CY

TL;DR: 研究发现，尽管大型语言模型（LLM）被广泛用作健康信息来源，但在自闭症谱系障碍（ASD）知识方面，人类参与者比GPT-4、Claude和Gemini等先进AI系统表现出更低的错误率，AI系统反而传播了更多关于自闭症的误解。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型成为普遍的健康信息来源，了解它们准确代表污名化病症的能力对于负责任部署至关重要。本研究旨在检验领先的AI系统是会延续还是挑战关于自闭症谱系障碍的误解，这是一种特别容易受到有害神话影响的病症。

Method: 研究使用包含30个项目的自闭症知识测量工具，对178名人类参与者和三个最先进的LLM（GPT-4、Claude和Gemini）进行了测试，比较了他们对自闭症相关知识的理解程度。

Result: 与预期相反，人类参与者比LLM表现出更低的错误率（36.2% vs. 44.8%），在30个评估项目中的18个上，人类显著优于AI系统。这表明AI系统在自闭症知识方面存在盲点。

Conclusion: 研究揭示了当前AI系统的关键盲点，对人与AI交互设计、机器知识认识论以及AI开发中需要以神经多样性视角为中心具有重要意义，表明AI系统在传播健康信息时需要更谨慎地处理污名化病症。

Abstract: As Large Language Models become ubiquitous sources of health information, understanding their capacity to accurately represent stigmatized conditions is crucial for responsible deployment. This study examines whether leading AI systems perpetuate or challenge misconceptions about Autism Spectrum Disorder, a condition particularly vulnerable to harmful myths. We administered a 30-item instrument measuring autism knowledge to 178 participants and three state-of-the-art LLMs including GPT-4, Claude, and Gemini. Contrary to expectations that AI systems would leverage their vast training data to outperform humans, we found the opposite pattern: human participants endorsed significantly fewer myths than LLMs (36.2% vs. 44.8% error rate; z = -2.59, p = .0048). In 18 of the 30 evaluated items, humans significantly outperformed AI systems. These findings reveal a critical blind spot in current AI systems and have important implications for human-AI interaction design, the epistemology of machine knowledge, and the need to center neurodivergent perspectives in AI development.

</details>


### [67] [Evaluating the Effectiveness of OpenAI's Parental Control System](https://arxiv.org/abs/2601.23062)
*Kerem Ersoz,Saleh Afroogh,David Atkinson,Junfeng Jiao*

Main category: cs.CY

TL;DR: 研究评估主流对话助手针对未成年人的平台级家长控制效果，发现通知机制有选择性而非全面，存在政策与产品之间的差距，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 评估主流对话助手平台级家长控制对未成年人使用时的实际效果，了解现有安全机制的有效性和局限性，为改进儿童安全保护措施提供依据。

Method: 采用两阶段协议：1）通过API迭代提示优化构建类别平衡的对话语料库；2）训练有素的人类代理在消费者UI中使用指定儿童账户重放/优化提示，同时监控关联的家长收件箱获取警报。聚焦七个风险领域，量化四个结果指标。

Result: 通知机制具有选择性：隐私暴力、欺诈、仇恨言论和恶意软件未触发家长警报，而身体伤害（最高）、色情内容和部分健康查询产生间歇性警报。当前后端比旧版模型泄漏更少，但对敏感话题附近良性教育查询的过度拦截仍然常见且未通知家长。

Conclusion: 平台级家长控制存在政策与产品之间的差距，屏幕上的安全措施与面向家长的遥测数据不匹配。建议扩展通知分类、将可见安全措施与隐私保护型家长摘要结合，并优先采用校准的、适合年龄的安全改写而非全面拒绝。

Abstract: We evaluate how effectively platform-level parental controls moderate a mainstream conversational assistant used by minors. Our two-phase protocol first builds a category-balanced conversation corpus via PAIR-style iterative prompt refinement over API, then has trained human agents replay/refine those prompts in the consumer UI using a designated child account while monitoring the linked parent inbox for alerts. We focus on seven risk areas -- physical harm, pornography, privacy violence, health consultation, fraud, hate speech, and malware and quantify four outcomes: Notification Rate (NR), Leak-Through (LR), Overblocking (OBR), and UI Intervention Rate (UIR). Using an automated judge (with targeted human audit) and comparing the current backend to legacy variants (GPT-4.1/4o), we find that notifications are selective rather than comprehensive: privacy violence, fraud, hate speech, and malware triggered no parental alerts in our runs, whereas physical harm (highest), pornography, and some health queries produced intermittent alerts. The current backend shows lower leak-through than legacy models, yet overblocking of benign, educational queries near sensitive topics remains common and is not surfaced to parents, revealing a policy-product gap between on-screen safeguards and parent-facing telemetry. We propose actionable fixes: broaden/configure the notification taxonomy, couple visible safeguards to privacy-preserving parent summaries, and prefer calibrated, age-appropriate safe rewrites over blanket refusals.

</details>


### [68] [How should AI Safety Benchmarks Benchmark Safety?](https://arxiv.org/abs/2601.23112)
*Cheng Yu,Severin Engelmann,Ruoxuan Cao,Dalia Ali,Orestis Papakyriakopoulos*

Main category: cs.CY

TL;DR: 该论文回顾了210个AI安全基准，指出了现有基准在技术、认知和社会技术方面的不足，提出了基于风险管理原则、概率度量、测量理论等改进方法，并提供了检查清单来帮助开发更稳健的AI安全基准。


<details>
  <summary>Details</summary>
Motivation: AI安全基准对于高级AI系统的安全至关重要，但现有基准存在显著的技术、认知和社会技术缺陷。这些缺陷可能导致对AI系统安全性的误判，从而在实际部署中带来风险。需要系统性地分析和改进AI安全基准的科学性和有效性。

Method: 1. 对210个AI安全基准进行系统性回顾和映射分析；2. 借鉴工程科学和成熟的风险与安全理论来识别基准的失败和局限性；3. 提出基于风险管理原则、概率度量、测量理论的改进框架；4. 通过定量和定性评估验证建议的有效性；5. 开发检查清单帮助研究人员和实践者。

Result: 1. 识别了AI安全基准在技术、认知和社会技术方面的系统性缺陷；2. 提出了改进AI安全基准的具体方法论框架；3. 验证了所提建议在提高基准有效性和实用性方面的效果；4. 开发了实用的检查清单工具；5. 推进了基准测试科学的发展。

Conclusion: 遵循成熟的风险管理原则、开发稳健的概率度量、有效应用测量理论，可以显著提高AI安全基准的有效性和实用性。该研究为改进AI安全基准提供了路线图，有助于更负责任地部署AI系统，并推进了基准测试科学的发展。

Abstract: AI safety benchmarks are pivotal for safety in advanced AI systems; however, they have significant technical, epistemic, and sociotechnical shortcomings. We present a review of 210 safety benchmarks that maps out common challenges in safety benchmarking, documenting failures and limitations by drawing from engineering sciences and long-established theories of risk and safety. We argue that adhering to established risk management principles, mapping the space of what can(not) be measured, developing robust probabilistic metrics, and efficiently deploying measurement theory to connect benchmarking objectives with the world can significantly improve the validity and usefulness of AI safety benchmarks. The review provides a roadmap on how to improve AI safety benchmarking, and we illustrate the effectiveness of these recommendations through quantitative and qualitative evaluation. We also introduce a checklist that can help researchers and practitioners develop robust and epistemologically sound safety benchmarks. This study advances the science of benchmarking and helps practitioners deploy AI systems more responsibly.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [69] [UrbanMoE: A Sparse Multi-Modal Mixture-of-Experts Framework for Multi-Task Urban Region Profiling](https://arxiv.org/abs/2601.22746)
*Pingping Liu,Jiamiao Liu,Zijian Zhang,Hao Miao,Qi Jiang,Qingliang Li,Qiuzhan Zhou,Irwin King*

Main category: cs.ET

TL;DR: 该论文提出了UrbanMoE框架和标准化基准，用于解决城市区域画像的多任务预测问题，通过稀疏专家混合架构实现多模态特征动态路由，在三个真实数据集上取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有城市区域画像研究存在两个主要局限：1) 大多局限于单任务预测，无法捕捉城市环境中多个指标间的深层关联；2) 缺乏标准化实验基准，阻碍了公平比较和可重复进展。

Method: 提出UrbanMoE框架，采用稀疏专家混合架构，动态路由多模态特征到专门子网络，实现多任务城市指标同时预测。同时建立了包含多模态特征和多样化基线的综合基准。

Result: 在基准中的三个真实数据集上，UrbanMoE始终优于所有基线方法。深入分析验证了方法的有效性和效率，建立了新的最先进水平。

Conclusion: UrbanMoE为城市区域画像提供了首个稀疏多模态、多专家框架，解决了多任务挑战。建立的标准化基准为城市分析领域的公平比较和未来研究提供了宝贵工具。

Abstract: Urban region profiling, the task of characterizing geographical areas, is crucial for urban planning and resource allocation. However, existing research in this domain faces two significant limitations. First, most methods are confined to single-task prediction, failing to capture the interconnected, multi-faceted nature of urban environments where numerous indicators are deeply correlated. Second, the field lacks a standardized experimental benchmark, which severely impedes fair comparison and reproducible progress. To address these challenges, we first establish a comprehensive benchmark for multi-task urban region profiling, featuring multi-modal features and a diverse set of strong baselines to ensure a fair and rigorous evaluation environment. Concurrently, we propose UrbanMoE, the first sparse multi-modal, multi-expert framework specifically architected to solve the multi-task challenge. Leveraging a sparse Mixture-of-Experts architecture, it dynamically routes multi-modal features to specialized sub-networks, enabling the simultaneous prediction of diverse urban indicators. We conduct extensive experiments on three real-world datasets within our benchmark, where UrbanMoE consistently demonstrates superior performance over all baselines. Further in-depth analysis validates the efficacy and efficiency of our approach, setting a new state-of-the-art and providing the community with a valuable tool for future research in urban analytics

</details>


### [70] [MiTa: A Hierarchical Multi-Agent Collaboration Framework with Memory-integrated and Task Allocation](https://arxiv.org/abs/2601.22974)
*XiaoJie Zhang,JianHan Wu,Xiaoyang Qu,Jianzong Wang*

Main category: cs.ET

TL;DR: MiTa是一个分层记忆集成任务分配框架，通过管理器-成员层次结构和记忆集成模块解决多智能体系统中的内存不一致和行为冲突问题，提升协作效率。


<details>
  <summary>Details</summary>
Motivation: 尽管基于LLM的多智能体系统缓解了单个智能体在复杂任务中的低效问题，但仍存在内存不一致和智能体行为冲突等挑战，需要更有效的协作框架。

Method: 提出MiTa框架，采用管理器-成员层次结构。管理器包含分配模块（从全局视角分配任务）和摘要模块（通过压缩近期协作历史进行情景记忆集成），避免智能体间冲突并保持长期上下文。

Result: 实验结果表明，MiTa在复杂多智能体协作中比强基线方法实现了更优的效率和适应性。

Conclusion: MiTa通过结合任务分配和情景记忆集成，能够更清晰地理解任务并促进全局一致的任务分配，有效解决了多智能体协作中的关键挑战。

Abstract: Recent advances in large language models (LLMs) have substantially accelerated the development of embodied agents. LLM-based multi-agent systems mitigate the inefficiency of single agents in complex tasks. However, they still suffer from issues such as memory inconsistency and agent behavioral conflicts. To address these challenges, we propose MiTa, a hierarchical memory-integrated task allocative framework to enhance collaborative efficiency. MiTa organizes agents into a manager-member hierarchy, where the manager incorporates additional allocation and summary modules that enable (1) global task allocation and (2) episodic memory integration. The allocation module enables the manager to allocate tasks from a global perspective, thereby avoiding potential inter-agent conflicts. The summary module, triggered by task progress updates, performs episodic memory integration by condensing recent collaboration history into a concise summary that preserves long-horizon context. By combining task allocation with episodic memory, MiTa attains a clearer understanding of the task and facilitates globally consistent task distribution. Experimental results confirm that MiTa achieves superior efficiency and adaptability in complex multi-agent cooperation over strong baseline methods.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [71] [The Benefit of Collective Intelligence in Community-Based Content Moderation is Limited by Overt Political Signalling](https://arxiv.org/abs/2601.22201)
*Gabriela Juncosa,Saeedeh Mohammadi,Margaret Samahita,Taha Yasseri*

Main category: cs.SI

TL;DR: 研究发现协作撰写社交媒体事实核查笔记比个人撰写更有效，但政治多样性影响因党派而异，且团队成员知晓彼此政治立场会削弱协作优势。


<details>
  <summary>Details</summary>
Motivation: 当前社交媒体社区内容审核系统（如X的Community Notes、TikTok的Footnotes等）存在政治偏见影响笔记撰写和评分的问题，降低了系统有效性。研究者假设协作撰写笔记能提升质量。

Method: 通过在线实验，让参与者协作撰写政治帖子的核查笔记，比较团队协作与个人撰写的效果，并考察政治多样性、团队组成和政治立场知晓度的影响。

Result: 1. 团队撰写的笔记比个人撰写的更受欢迎；2. 政治多样性团队在评估共和党帖子时表现更好，但对民主党帖子无影响；3. 团队成员知晓彼此政治立场会削弱协作优势。

Conclusion: 社区内容审核系统设计需考虑群体动态和政治多样性，协作撰写能提升事实核查质量，但需注意政治立场透明度的负面影响。

Abstract: Social media platforms face increasing scrutiny over the rapid spread of misinformation. In response, many have adopted community-based content moderation systems, including Community Notes (formerly Birdwatch) on X (formerly Twitter), Footnotes on TikTok, and Facebook's Community Notes initiative. However, research shows that the current design of these systems can allow political biases to influence both the development of notes and the rating processes, reducing their overall effectiveness. We hypothesize that enabling users to collaborate on writing notes, rather than relying solely on individually authored notes, can enhance their overall quality. To test this idea, we conducted an online experiment in which participants jointly authored notes on political posts. Our results show that teams produce notes that are rated as more helpful than individually written notes. We also find that politically diverse teams perform better when evaluating Republican posts, while group composition does not affect perceived note quality for Democrat posts. However, the advantage of collaboration diminishes when team members are aware of one another's political affiliations. Taken together, these findings underscore the complexity of community-based content moderation and highlight the importance of understanding group dynamics and political diversity when designing more effective moderation systems.

</details>


### [72] [Network analysis and link prediction in competitive women's basketball](https://arxiv.org/abs/2601.23193)
*Anthony Bonato,Morganna Hinds*

Main category: cs.SI

TL;DR: 该研究通过篮球比赛网络分析，结合PageRank和共同出邻居得分构建"低调领导者强度"指标，并利用node2vec嵌入进行链接预测，发现高阶网络结构对篮球互动具有预测价值。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索篮球比赛中网络结构对预测的作用，特别是在团队和球员层面，了解网络拓扑如何影响比赛结果和互动模式。

Method: 使用2021-2024年NCAA女子篮球对抗性比赛网络，计算共同出邻居得分和PageRank，组合成"低调领导者强度"指标。采用node2vec嵌入在三种交互设置中进行链接预测：NCAA常规赛网络预测疯狂三月对阵，WNBA封盖网络预测未来封盖互动，WNBA传球网络预测后续传球连接。

Result: 低调领导者强度指标能识别结构相似性影响大的竞争者。嵌入模型在NCAA和WNBA设置中均提供统计显著证据，表明高阶网络结构包含未来互动的预测信号。传球实验预测性能较弱，但产生了与传球可行性一致的相似性模式。

Conclusion: 篮球比赛网络的高阶结构包含有价值的预测信息，网络嵌入方法能有效捕捉团队和球员互动模式，为篮球分析提供了新的量化工具。

Abstract: Network structure and its role in prediction are examined in competitive basketball at the team and player levels. Adversarial game outcome networks from NCAA Division I women's basketball from 2021 to 2024 are used to compute the common out-neighbor score and PageRank, which are combined into a low-key leader strength that identifies competitors influential through structural similarity despite relatively low centrality. This measure is related to changes in NCAA NET rankings by grouping teams into quantiles and comparing average rank changes across seasons for both previous-to-current and current-to-next transitions. Link prediction is then studied using node2vec embeddings across three interaction settings. For NCAA regular-season game networks, cosine similarity between team embeddings is used in a logistic regression model to predict March Madness matchups. For WNBA shot-blocking networks, future directed blocking interactions are predicted via logistic regression on concatenated source-target player embeddings. For WNBA passing networks, region embeddings learned from first-quarter passes are evaluated for their ability to predict subsequent passing connections. Across NCAA and WNBA settings, embedding-based models provide statistically significant evidence that higher-order network structure contains predictive signals for future interactions, while the passing experiment shows weaker predictive performance but yields interpretable similarity patterns consistent with passing feasibility.

</details>


### [73] [The Iterated Local Model for tournaments](https://arxiv.org/abs/2601.23246)
*Anthony Bonato,MacKenzie Carr,Ketan Chaudhary,Trent G. Marbach,Teddy Mishura*

Main category: cs.SI

TL;DR: 提出ILMT模型，通过传递性迭代生成高密度复杂网络，产生小直径、高连通性的竞赛图，具有准随机性和特定图论性质。


<details>
  <summary>Details</summary>
Motivation: 传递性是社交网络等复杂网络的核心生成原则，但现有模型难以生成高密度、小直径、高连通性的复杂网络结构。需要新模型来捕捉真实世界网络的这些特征。

Method: 提出迭代局部模型竞赛图(ILMT)：通过迭代应用传递性，克隆节点及其邻接关系，并保持或反转克隆间现有弧的方向，从而生成新的竞赛图。

Result: ILMT模型生成具有小直径和高连通性的竞赛图，与真实复杂网络特征相符；分析子竞赛图/模体的普遍性；许多参数选择下生成准随机竞赛图序列；研究cop数、支配数、色数等图论性质。

Conclusion: ILMT是基于传递性的有效复杂网络生成模型，能产生具有真实网络特征的竞赛图，为研究复杂网络结构提供了新框架，并提出了面向有向图的模型变体和开放问题。

Abstract: Transitivity is a central, generative principle in social and other complex networks, capturing the tendency for two nodes with a common neighbor to form a direct connection. We propose a new model for highly dense, complex networks based on transitivity, called the Iterated Local Model Tournament (ILMT). In ILMT, we iteratively apply transitivity to form new tournaments by cloning nodes and their adjacencies, and either preserving or reversing the orientation of existing arcs between clones. The resulting model generates tournaments with small diameters and high connectivity as observed in real-world complex networks. We analyze subtournaments or motifs in the ILMT model and their universality properties. For many parameter choices, the model generates sequences of quasirandom tournaments. We also study the graph-theoretic properties of ILMT tournaments, including their cop number, domination number, and chromatic number. We finish with a set of open problems and variants of the ILMT model for oriented graphs.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [74] [Model Selection in Panel Data Models: A Generalization of the Vuong Test](https://arxiv.org/abs/2601.22354)
*Jinyong Hahn,Zhipeng Liao,Konrad Menzel,Quang Vuong*

Main category: econ.EM

TL;DR: 将Vuong检验推广到面板数据模型，使用修正轮廓似然和K-L信息准则，处理非嵌套模型选择问题


<details>
  <summary>Details</summary>
Motivation: 经典Vuong检验仅适用于横截面数据，需要将其扩展到面板数据框架。面板数据中的轮廓似然缺乏某些正则性质，需要修正才能应用

Method: 采用修正轮廓似然和Kullback-Leibler信息准则，构建广义面板数据框架，包含时间和个体对的组固定效应（而非传统个体固定效应）

Result: 开发了适用于面板数据的Vuong检验推广版本，能够处理线性模型中个体-时间效应的非嵌套设定

Conclusion: 成功将Vuong检验扩展到面板数据模型，为面板数据中的非嵌套模型选择提供了有效工具

Abstract: This paper generalizes the classical Vuong (1989) test to panel data models by employing modified profile likelihoods and the Kullback-Leibler information criterion. Unlike the standard likelihood function, the profile likelihood lacks certain regular properties, making modification necessary. We adopt a generalized panel data framework that incorporates group fixed effects for time and individual pairs, rather than traditional individual fixed effects. Applications of our approach include linear models with non-nested specifications of individual-time effects.

</details>


### [75] [Using SVM to Estimate and Predict Binary Choice Models](https://arxiv.org/abs/2601.22659)
*Yoosoon Chang,Joon Y. Park,Guo Yan*

Main category: econ.EM

TL;DR: SVM与QMLE在二元选择模型中有相似的渐近行为，SVM斜率估计量在特定条件下能一致估计BCM斜率参数，与逻辑回归渐近等价，但有限样本表现取决于协变量和误差分布。


<details>
  <summary>Details</summary>
Motivation: 研究支持向量机(SVM)在二元选择模型中的渐近性质，探索SVM是否能在特定条件下一致估计模型参数，并与传统QMLE方法进行比较。

Method: 在QMLE斜率一致性文献中使用的线性条件均值条件下，分析SVM分离超平面斜率的渐近性质，考虑类别权重处理严重不平衡数据的情况。

Result: SVM斜率估计量能一致估计BCM斜率参数，与逻辑回归渐近等价；截距参数在获得一致斜率估计后也能一致估计；有限样本表现取决于协变量和误差分布，两者无绝对优劣。

Conclusion: SVM具有与QMLE相似的渐近性质，能在特定条件下一致估计二元选择模型参数，为SVM在计量经济学中的应用提供了理论依据。

Abstract: The support vector machine (SVM) has an asymptotic behavior that parallels that of the quasi-maximum likelihood estimator (QMLE) for binary outcomes generated by a binary choice model (BCM), although it is not a QMLE. We show that, under the linear conditional mean condition for covariates given the systematic component used in the QMLE slope consistency literature, the slope of the separating hyperplane given by the SVM consistently estimates the BCM slope parameter, as long as the class weight is used as required when binary outcomes are severely imbalanced. The SVM slope estimator is asymptotically equivalent to that of logistic regression in this sense. The finite-sample performance of the two estimators can be quite distinct depending on the distributions of covariates and errors, but neither dominates the other. The intercept parameter of the BCM can be consistently estimated once a consistent estimator of its slope parameter is obtained.

</details>

{"id": "2512.09707", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2512.09707", "abs": "https://arxiv.org/abs/2512.09707", "authors": ["Thanh-Lam T. Nguyen", "Ngoc-Quang Le", "Thu-Trang Pham", "Mai-Vu Tran"], "title": "Knowledge Graph Enrichment and Reasoning for Nobel Laureates", "comment": null, "summary": "This project aims to construct and analyze a comprehensive knowledge graph of Nobel Prize and Laureates by enriching existing datasets with biographical information extracted from Wikipedia. Our approach integrates multiple advanced techniques, consisting of automatic data augmentation using LLMs for Named Entity Recognition (NER) and Relation Extraction (RE) tasks, and social network analysis to uncover hidden patterns within the scientific community. Furthermore, we also develop a GraphRAG-based chatbot system utilizing a fine-tuned model for Text2Cypher translation, enabling natural language querying over the knowledge graph. Experimental results demonstrate that the enriched graph possesses small-world network properties, identifying key influential figures and central organizations. The chatbot system achieves a competitive accuracy on a custom multiple-choice evaluation dataset, proving the effectiveness of combining LLMs with structured knowledge bases for complex reasoning tasks. Data and source code are available at: https://github.com/tlam25/network-of-awards-and-winners."}
{"id": "2512.09831", "categories": ["cs.AI", "cs.LG", "cs.MA", "cs.SI"], "pdf": "https://arxiv.org/pdf/2512.09831", "abs": "https://arxiv.org/abs/2512.09831", "authors": ["Chainarong Amornbunchornvej"], "title": "Interpretation as Linear Transformation: A Cognitive-Geometric Model of Belief and Meaning", "comment": "The first draft of cognitive geometry model", "summary": "This paper develops a geometric framework for modeling belief, motivation, and influence across cognitively heterogeneous agents. Each agent is represented by a personalized value space, a vector space encoding the internal dimensions through which the agent interprets and evaluates meaning. Beliefs are formalized as structured vectors-abstract beings-whose transmission is mediated by linear interpretation maps. A belief survives communication only if it avoids the null spaces of these maps, yielding a structural criterion for intelligibility, miscommunication, and belief death.\n  Within this framework, I show how belief distortion, motivational drift, counterfactual evaluation, and the limits of mutual understanding arise from purely algebraic constraints. A central result-\"the No-Null-Space Leadership Condition\"-characterizes leadership as a property of representational reachability rather than persuasion or authority. More broadly, the model explains how abstract beings can propagate, mutate, or disappear as they traverse diverse cognitive geometries.\n  The account unifies insights from conceptual spaces, social epistemology, and AI value alignment by grounding meaning preservation in structural compatibility rather than shared information or rationality. I argue that this cognitive-geometric perspective clarifies the epistemic boundaries of influence in both human and artificial systems, and offers a general foundation for analyzing belief dynamics across heterogeneous agents."}
{"id": "2512.09088", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09088", "abs": "https://arxiv.org/abs/2512.09088", "authors": ["Adrian Ryser", "Florian Allwein", "Tim Schlippe"], "title": "Calibrated Trust in Dealing with LLM Hallucinations: A Qualitative Study", "comment": null, "summary": "Hallucinations are outputs by Large Language Models (LLMs) that are factually incorrect yet appear plausible [1]. This paper investigates how such hallucinations influence users' trust in LLMs and users' interaction with LLMs. To explore this in everyday use, we conducted a qualitative study with 192 participants. Our findings show that hallucinations do not result in blanket mistrust but instead lead to context-sensitive trust calibration. Building on the calibrated trust model by Lee & See [2] and Afroogh et al.'s trust-related factors [3], we confirm expectancy [3], [4], prior experience [3], [4], [5], and user expertise & domain knowledge [3], [4] as userrelated (human) trust factors, and identify intuition as an additional factor relevant for hallucination detection. Additionally, we found that trust dynamics are further influenced by contextual factors, particularly perceived risk [3] and decision stakes [6]. Consequently, we validate the recursive trust calibration process proposed by Blöbaum [7] and extend it by including intuition as a user-related trust factor. Based on these insights, we propose practical recommendations for responsible and reflective LLM use."}
{"id": "2512.08946", "categories": ["cs.CY", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.08946", "abs": "https://arxiv.org/abs/2512.08946", "authors": ["Zacchary Sadeddine", "Winston Maxwell", "Gaël Varoquaux", "Fabian M. Suchanek"], "title": "Large Language Models as Search Engines: Societal Challenges", "comment": null, "summary": "Large Language Models (LLMs) may one day replace search engines as the primary portal to information on the Web. In this article, we investigate the societal challenges that such a change could bring. We focus on the roles of LLM Providers, Content Creators, and End Users, and identify 15 types of challenges. With each, we show current mitigation strategies -- both from the technical perspective and the legal perspective. We also discuss the impact of each challenge and point out future research opportunities."}
{"id": "2512.08946", "categories": ["cs.CY", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.08946", "abs": "https://arxiv.org/abs/2512.08946", "authors": ["Zacchary Sadeddine", "Winston Maxwell", "Gaël Varoquaux", "Fabian M. Suchanek"], "title": "Large Language Models as Search Engines: Societal Challenges", "comment": null, "summary": "Large Language Models (LLMs) may one day replace search engines as the primary portal to information on the Web. In this article, we investigate the societal challenges that such a change could bring. We focus on the roles of LLM Providers, Content Creators, and End Users, and identify 15 types of challenges. With each, we show current mitigation strategies -- both from the technical perspective and the legal perspective. We also discuss the impact of each challenge and point out future research opportunities."}
{"id": "2512.09179", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.09179", "abs": "https://arxiv.org/abs/2512.09179", "authors": ["Robert A. Rigby", "Mikis D. Stasinopoulos", "Achim Zeileis", "Sanja Stanojevic", "Gillian Heller", "Fernanda de Bastiani", "Thomas Kneib", "Andreas Mayr", "Reto Stauffer", "Nikolaus Umlauf"], "title": "Refuting \"Debunking the GAMLSS Myth: Simplicity Reigns in Pulmonary Function Diagnostics\"", "comment": "Letter to the editor of Respiratory Medicine", "summary": "We read with interest the above article by Zavorsky (2025, Respiratory Medicine, doi:10.1016/j.rmed.2024.107836) concerning reference equations for pulmonary function testing. The author compares a Generalized Additive Model for Location, Scale, and Shape (GAMLSS), which is the standard adopted by the Global Lung Function Initiative (GLI), with a segmented linear regression (SLR) model, for pulmonary function variables. The author presents an interesting comparison; however there are some fundamental issues with the approach. We welcome this opportunity for discussion of the issues that it raises. The author's contention is that (1) SLR provides \"prediction accuracies on par with GAMLSS\"; and (2) the GAMLSS model equations are \"complicated and require supplementary spline tables\", whereas the SLR is \"more straightforward, parsimonious, and accessible to a broader audience\". We respectfully disagree with both of these points."}
{"id": "2512.09206", "categories": ["econ.EM"], "pdf": "https://arxiv.org/pdf/2512.09206", "abs": "https://arxiv.org/abs/2512.09206", "authors": ["Christopher Carter", "Adeline Delavande", "Mario Fiorini", "Peter Siminski", "Patrick Vu"], "title": "Optimal Screening in Experiments with Partial Compliance", "comment": null, "summary": "This note studies optimal experimental design under partial compliance when experimenters can screen participants prior to randomization. Theoretical results show that retaining all compliers and screening out all non-compliers achieves three complementary aims: (i) the Local Average Treatment Effect is the same as the standard 2SLS estimator with no screening; (ii) median bias is minimized; and (iii) statistical power is maximized. In practice, complier status is unobserved. We therefore discuss feasible screening strategies and propose a simple test for screening efficacy. Future work will conduct an experiment to demonstrate the feasibility and advantages of the optimal screening design."}
{"id": "2512.09114", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.09114", "abs": "https://arxiv.org/abs/2512.09114", "authors": ["Pamela Gupta"], "title": "AI TIPS 2.0: A Comprehensive Framework for Operationalizing AI Governance", "comment": "47 pages", "summary": "The deployment of AI systems faces three critical governance challenges that current frameworks fail to adequately address. First, organizations struggle with inadequate risk assessment at the use case level, exemplified by the Humana class action lawsuit and other high impact cases where an AI system deployed to production exhibited both significant bias and high error rates, resulting in improper healthcare claim denials. Each AI use case presents unique risk profiles requiring tailored governance, yet most frameworks provide one size fits all guidance. Second, existing frameworks like ISO 42001 and NIST AI RMF remain at high conceptual levels, offering principles without actionable controls, leaving practitioners unable to translate governance requirements into specific technical implementations. Third, organizations lack mechanisms for operationalizing governance at scale, with no systematic approach to embed trustworthy AI practices throughout the development lifecycle, measure compliance quantitatively, or provide role-appropriate visibility from boards to data scientists. We present AI TIPS, Artificial Intelligence Trust-Integrated Pillars for Sustainability 2.0, update to the comprehensive operational framework developed in 2019,four years before NIST's AI Risk Management Framework, that directly addresses these challenges."}
{"id": "2512.08978", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08978", "abs": "https://arxiv.org/abs/2512.08978", "authors": ["Ruud Huijts", "Koen Suilen"], "title": "Institutional AI Sovereignty Through Gateway Architecture: Implementation Report from Fontys ICT", "comment": null, "summary": "To counter fragmented, high-risk adoption of commercial AI tools, we built and ran an institutional AI platform in a six-month, 300-user pilot, showing that a university of applied sciences can offer advanced AI with fair access, transparent risks, controlled costs, and alignment with European law.\n  Commercial AI subscriptions create unequal access and compliance risks through opaque processing and non-EU hosting, yet banning them is neither realistic nor useful. Institutions need a way to provide powerful AI in a sovereign, accountable form.\n  Our solution is a governed gateway platform with three layers: a ChatGPT-style frontend linked to institutional identity that makes model choice explicit; a gateway core enforcing policy, controlling access and budgets, and routing traffic to EU infrastructure by default; and a provider layer wrapping commercial and open-source models in institutional model cards that consolidate vendor documentation into one governance interface.\n  The pilot ran reliably with no privacy incidents and strong adoption, enabling EU-default routing, managed spending, and transparent model choices. Only the gateway pattern combines model diversity and rapid innovation with institutional control.\n  The central insight: AI is not a support function but strategy, demanding dedicated leadership. Sustainable operation requires governance beyond traditional boundaries. We recommend establishing a formal AI Officer role combining technical literacy, governance authority, and educational responsibility. Without it, AI decisions stay ad-hoc and institutional exposure grows. With it, higher-education institutions can realistically operate their own multi-provider AI platform, provided they govern AI as seriously as they teach it."}
{"id": "2512.08978", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08978", "abs": "https://arxiv.org/abs/2512.08978", "authors": ["Ruud Huijts", "Koen Suilen"], "title": "Institutional AI Sovereignty Through Gateway Architecture: Implementation Report from Fontys ICT", "comment": null, "summary": "To counter fragmented, high-risk adoption of commercial AI tools, we built and ran an institutional AI platform in a six-month, 300-user pilot, showing that a university of applied sciences can offer advanced AI with fair access, transparent risks, controlled costs, and alignment with European law.\n  Commercial AI subscriptions create unequal access and compliance risks through opaque processing and non-EU hosting, yet banning them is neither realistic nor useful. Institutions need a way to provide powerful AI in a sovereign, accountable form.\n  Our solution is a governed gateway platform with three layers: a ChatGPT-style frontend linked to institutional identity that makes model choice explicit; a gateway core enforcing policy, controlling access and budgets, and routing traffic to EU infrastructure by default; and a provider layer wrapping commercial and open-source models in institutional model cards that consolidate vendor documentation into one governance interface.\n  The pilot ran reliably with no privacy incidents and strong adoption, enabling EU-default routing, managed spending, and transparent model choices. Only the gateway pattern combines model diversity and rapid innovation with institutional control.\n  The central insight: AI is not a support function but strategy, demanding dedicated leadership. Sustainable operation requires governance beyond traditional boundaries. We recommend establishing a formal AI Officer role combining technical literacy, governance authority, and educational responsibility. Without it, AI decisions stay ad-hoc and institutional exposure grows. With it, higher-education institutions can realistically operate their own multi-provider AI platform, provided they govern AI as seriously as they teach it."}
{"id": "2512.09217", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.09217", "abs": "https://arxiv.org/abs/2512.09217", "authors": ["Saeed Saleh Namadi", "Jie Chen", "Deb Niemeier"], "title": "Access to healthcare for people with Alzheimer's Diseases and related dementias", "comment": null, "summary": "Background: Alzheimer's Disease and Related Dementias (ADRD) affects millions worldwide. Significant disparities exist in ADRD diagnosis and care, disproportionately impacting minority and socioeconomically vulnerable populations Objective: In this study, we investigate the relationship between ADRD density and accessibility to healthcare. We identify underserved and overserved areas in Maryland based on diagnosed cases and mortality due to ADRD, focusing on geographic disparities in care. Methods: 2023 Maryland ADRD patients were identified using ICD-10 codes from. Accessibility was measured using the Kernel Density Two-Step Floating Catchment Area (KD2SFCA) method. The Gini index and t-tests were used to analyze disparities between urban and rural areas. Hot Spot Analysis Getis-Ord Gi* and local bivariate relationships analysis were applied to assess spatial correlations. Principal component analysis (PCA) was applied to calculate the health risk index. Results: Hospital accessibility was unevenly distributed. Mortality rates from ADRD were higher in underserved areas with fewer hospitals. Hot spot analysis shows eastern and southern Maryland have zones with high mortality per population and per ADRD patient, surrounded by similarly high-rate zones. Central Maryland shows lower death rates per patient but more hospital facilities. In eastern Maryland, higher poverty areas are surrounded by zones with lower accessibility and higher health risk indices. Conclusion: Hospital accessibility is unevenly distributed, creating major rural disparities. Underserved regions in terms of access to healthcare facilities, particularly in eastern and southern Maryland, exhibit high ADRD mortality rates despite low diagnosis rates. This suggests that many ADRD cases remain undiagnosed, underdiagnosed, or subject to delayed treatment."}
{"id": "2512.09257", "categories": ["econ.EM", "math.ST", "stat.CO", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.09257", "abs": "https://arxiv.org/abs/2512.09257", "authors": ["Qihui Chen", "Zheng Fang", "Ruixuan Liu"], "title": "Debiased Bayesian Inference for High-dimensional Regression Models", "comment": "53 pages", "summary": "There has been significant progress in Bayesian inference based on sparsity-inducing (e.g., spike-and-slab and horseshoe-type) priors for high-dimensional regression models. The resulting posteriors, however, in general do not possess desirable frequentist properties, and the credible sets thus cannot serve as valid confidence sets even asymptotically. We introduce a novel debiasing approach that corrects the bias for the entire Bayesian posterior distribution. We establish a new Bernstein-von Mises theorem that guarantees the frequentist validity of the debiased posterior. We demonstrate the practical performance of our proposal through Monte Carlo simulations and two empirical applications in economics."}
{"id": "2512.09117", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09117", "abs": "https://arxiv.org/abs/2512.09117", "authors": ["Luciano Floridi", "Yiyang Jia", "Fernando Tohmé"], "title": "A Categorical Analysis of Large Language Models and Why LLMs Circumvent the Symbol Grounding Problem", "comment": null, "summary": "This paper presents a formal, categorical framework for analysing how humans and large language models (LLMs) transform content into truth-evaluated propositions about a state space of possible worlds W , in order to argue that LLMs do not solve but circumvent the symbol grounding problem."}
{"id": "2512.09088", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09088", "abs": "https://arxiv.org/abs/2512.09088", "authors": ["Adrian Ryser", "Florian Allwein", "Tim Schlippe"], "title": "Calibrated Trust in Dealing with LLM Hallucinations: A Qualitative Study", "comment": null, "summary": "Hallucinations are outputs by Large Language Models (LLMs) that are factually incorrect yet appear plausible [1]. This paper investigates how such hallucinations influence users' trust in LLMs and users' interaction with LLMs. To explore this in everyday use, we conducted a qualitative study with 192 participants. Our findings show that hallucinations do not result in blanket mistrust but instead lead to context-sensitive trust calibration. Building on the calibrated trust model by Lee & See [2] and Afroogh et al.'s trust-related factors [3], we confirm expectancy [3], [4], prior experience [3], [4], [5], and user expertise & domain knowledge [3], [4] as userrelated (human) trust factors, and identify intuition as an additional factor relevant for hallucination detection. Additionally, we found that trust dynamics are further influenced by contextual factors, particularly perceived risk [3] and decision stakes [6]. Consequently, we validate the recursive trust calibration process proposed by Blöbaum [7] and extend it by including intuition as a user-related trust factor. Based on these insights, we propose practical recommendations for responsible and reflective LLM use."}
{"id": "2512.09261", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.09261", "abs": "https://arxiv.org/abs/2512.09261", "authors": ["Justin Heath"], "title": "FLARE v2: A Recursive Framework for Program Comprehension Across Languages and Levels of Abstraction", "comment": "22 pages, 4 figures. Preprint under CC BY-NC-SA 4.0", "summary": "Building on the classroom framework reported in Heath et al. (2025), this paper proposes FLARE v2 as a recursive, semiotically informed account of how program meaning is constructed. It reinterprets the descriptive tiers of FLARE v1 as instances of a single generative operation: identify elements (characterised by the four properties Receives, Sends, Effects, Shares); analyse their bindings along two dimensions (Causal-Temporal and Communicative); and recognise the new element that emerges. The Causal-Temporal dimension encompasses three subtypes - Sequential, Branch, and Event - that together account for control flow in both procedural and event-driven environments. A Compositional Ladder provides a visual parallel between literacy progressions and programming structures, illustrating how recursive composition operates from blocks and statements through segments, systems, and services. The framework aims to address conceptual and cognitive-load limitations reported in FLARE v1 and is situated within semiotic and program-comprehension theory. FLARE v2 is presented as a conceptual lens with potential implications for pedagogy and curriculum design; implementation and empirical evaluation are left for future work."}
{"id": "2512.09316", "categories": ["stat.AP", "stat.OT"], "pdf": "https://arxiv.org/pdf/2512.09316", "abs": "https://arxiv.org/abs/2512.09316", "authors": ["Marios Papamichalis", "Nicholas Christakis", "Feng Fu"], "title": "Group Cooperation Diverges onto Durable Low versus High Paths: Public Goods Experiments in 134 Honduran Villages", "comment": "This is the initial version of the manuscript. The presentation of figures, tables, and analyses may be revised in future versions to better align with the requirements and scope of the target journal", "summary": "We performed large, lab-in-the-field experiment (2,591 participants across 134 Honduran villages; ten rounds) and tracked how contribution behavior unfolds in fixed, anonymous groups of size five. Contribution separates early into two durable paths, one low and one high, with rare convergence thereafter. High-path players can be identified with strong accuracy early on. Groups that begin with an early majority of above-norm contributors (about 60%) are very likely finish high. The empirical finding of a bifurcation, consistent with the theory, shows that early, high contributions by socially central people steer groups onto, and help keep them on, a high-cooperation path."}
{"id": "2512.09853", "categories": ["econ.EM", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.09853", "abs": "https://arxiv.org/abs/2512.09853", "authors": ["Zhaoji Tang"], "title": "New Approximation Results and Optimal Estimation for Fully Connected Deep Neural Networks", "comment": null, "summary": "\\citet{farrell2021deep} establish non-asymptotic high-probability bounds for general deep feedforward neural network (with rectified linear unit activation function) estimators, with \\citet[Theorem 1]{farrell2021deep} achieving a suboptimal convergence rate for fully connected feedforward networks. The authors suggest that improved approximation of fully connected networks could yield sharper versions of \\citet[Theorem 1]{farrell2021deep} without altering the theoretical framework. By deriving approximation bounds specifically for a narrower fully connected deep neural network, this note demonstrates that \\citet[Theorem 1]{farrell2021deep} can be improved to achieve an optimal rate (up to a logarithmic factor). Furthermore, this note briefly shows that deep neural network estimators can mitigate the curse of dimensionality for functions with compositional structure and functions defined on manifolds."}
{"id": "2512.09142", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09142", "abs": "https://arxiv.org/abs/2512.09142", "authors": ["Sergio Burdisso", "Séverin Baroudi", "Yanis Labrak", "David Grunert", "Pawel Cyrta", "Yiyang Chen", "Srikanth Madikeri", "Esaú Villatoro-Tello", "Thomas Schaaf", "Ricard Marxer", "Petr Motlicek"], "title": "SDialog: A Python Toolkit for End-to-End Agent Building, User Simulation, Dialog Generation, and Evaluation", "comment": "Pre-print submitted to EACL System Demonstration (under review)", "summary": "We present SDialog, an MIT-licensed open-source Python toolkit that unifies dialog generation, evaluation and mechanistic interpretability into a single end-to-end framework for building and analyzing LLM-based conversational agents. Built around a standardized \\texttt{Dialog} representation, SDialog provides: (1) persona-driven multi-agent simulation with composable orchestration for controlled, synthetic dialog generation, (2) comprehensive evaluation combining linguistic metrics, LLM-as-a-judge and functional correctness validators, (3) mechanistic interpretability tools for activation inspection and steering via feature ablation and induction, and (4) audio generation with full acoustic simulation including 3D room modeling and microphone effects. The toolkit integrates with all major LLM backends, enabling mixed-backend experiments under a unified API. By coupling generation, evaluation, and interpretability in a dialog-centric architecture, SDialog enables researchers to build, benchmark and understand conversational systems more systematically."}
{"id": "2512.09114", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.09114", "abs": "https://arxiv.org/abs/2512.09114", "authors": ["Pamela Gupta"], "title": "AI TIPS 2.0: A Comprehensive Framework for Operationalizing AI Governance", "comment": "47 pages", "summary": "The deployment of AI systems faces three critical governance challenges that current frameworks fail to adequately address. First, organizations struggle with inadequate risk assessment at the use case level, exemplified by the Humana class action lawsuit and other high impact cases where an AI system deployed to production exhibited both significant bias and high error rates, resulting in improper healthcare claim denials. Each AI use case presents unique risk profiles requiring tailored governance, yet most frameworks provide one size fits all guidance. Second, existing frameworks like ISO 42001 and NIST AI RMF remain at high conceptual levels, offering principles without actionable controls, leaving practitioners unable to translate governance requirements into specific technical implementations. Third, organizations lack mechanisms for operationalizing governance at scale, with no systematic approach to embed trustworthy AI practices throughout the development lifecycle, measure compliance quantitatively, or provide role-appropriate visibility from boards to data scientists. We present AI TIPS, Artificial Intelligence Trust-Integrated Pillars for Sustainability 2.0, update to the comprehensive operational framework developed in 2019,four years before NIST's AI Risk Management Framework, that directly addresses these challenges."}
{"id": "2512.09570", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09570", "abs": "https://arxiv.org/abs/2512.09570", "authors": ["Jelena Cupac"], "title": "The Gender Code: Gendering the Global Governance of Artificial Intelligence", "comment": "The paper is part of the Handbook on the Global Governance of Artificial Intelligence, forthcoming with Edward Elgar Publishing", "summary": "This paper examines how international AI governance frameworks address gender issues and gender-based harms. The analysis covers binding regulations, such as the EU AI Act; soft law instruments, like the UNESCO Recommendations on AI Ethics; and global initiatives, such as the Global Partnership on AI (GPAI). These instruments reveal emerging trends, including the integration of gender concerns into broader human rights frameworks, a shift toward explicit gender-related provisions, and a growing emphasis on inclusivity and diversity. Yet, some critical gaps persist, including inconsistent treatment of gender across governance documents, limited engagement with intersectionality, and a lack of robust enforcement mechanisms. However, this paper argues that effective AI governance must be intersectional, enforceable, and inclusive. This is key to moving beyond tokenism toward meaningful equity and preventing reinforcement of existing inequalities. The study contributes to ethical AI debates by highlighting the importance of gender-sensitive governance in building a just technological future."}
{"id": "2512.09561", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.09561", "abs": "https://arxiv.org/abs/2512.09561", "authors": ["Bao Anh Vu", "Andrew Zammit-Mangion", "David Gunawan", "Felicity S. McCormack", "Noel Cressie"], "title": "Neural posterior inference with state-space models for calibrating ice sheet simulators", "comment": null, "summary": "Ice sheet models are routinely used to quantify and project an ice sheet's contribution to sea level rise. In order for an ice sheet model to generate realistic projections, its parameters must first be calibrated using observational data; this is challenging due to the nonlinearity of the model equations, the high dimensionality of the underlying parameters, and limited data availability for validation. This study leverages the emerging field of neural posterior approximation for efficiently calibrating ice sheet model parameters and boundary conditions. We make use of a one-dimensional (flowline) Shallow-Shelf Approximation model in a state-space framework. A neural network is trained to infer the underlying parameters, namely the bedrock elevation and basal friction coefficient along the flowline, based on observations of ice velocity and ice surface elevation. Samples from the approximate posterior distribution of the parameters are then used within an ensemble Kalman filter to infer latent model states, namely the ice thickness along the flowline. We show through a simulation study that our approach yields more accurate estimates of the parameters and states than a state-augmented ensemble Kalman filter, which is the current state-of-the-art. We apply our approach to infer the bed elevation and basal friction along a flowline in Thwaites Glacier, Antarctica."}
{"id": "2512.09340", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09340", "abs": "https://arxiv.org/abs/2512.09340", "authors": ["Chethana Prasad Kabgere"], "title": "Visual Categorization Across Minds and Models: Cognitive Analysis of Human Labeling and Neuro-Symbolic Integration", "comment": "12 pages, 3 figures. Research manuscript based on the final project for CS6795 (Introduction to Cognitive Science), Georgia Tech", "summary": "Understanding how humans and AI systems interpret ambiguous visual stimuli offers critical insight into the nature of perception, reasoning, and decision-making. This paper examines image labeling performance across human participants and deep neural networks, focusing on low-resolution, perceptually degraded stimuli. Drawing from computational cognitive science, cognitive architectures, and connectionist-symbolic hybrid models, we contrast human strategies such as analogical reasoning, shape-based recognition, and confidence modulation with AI's feature-based processing. Grounded in Marr's tri-level hypothesis, Simon's bounded rationality, and Thagard's frameworks of representation and emotion, we analyze participant responses in relation to Grad-CAM visualizations of model attention. Human behavior is further interpreted through cognitive principles modeled in ACT-R and Soar, revealing layered and heuristic decision strategies under uncertainty. Our findings highlight key parallels and divergences between biological and artificial systems in representation, inference, and confidence calibration. The analysis motivates future neuro-symbolic architectures that unify structured symbolic reasoning with connectionist representations. Such architectures, informed by principles of embodiment, explainability, and cognitive alignment, offer a path toward AI systems that are not only performant but also interpretable and cognitively grounded."}
{"id": "2512.09117", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09117", "abs": "https://arxiv.org/abs/2512.09117", "authors": ["Luciano Floridi", "Yiyang Jia", "Fernando Tohmé"], "title": "A Categorical Analysis of Large Language Models and Why LLMs Circumvent the Symbol Grounding Problem", "comment": null, "summary": "This paper presents a formal, categorical framework for analysing how humans and large language models (LLMs) transform content into truth-evaluated propositions about a state space of possible worlds W , in order to argue that LLMs do not solve but circumvent the symbol grounding problem."}
{"id": "2512.09729", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09729", "abs": "https://arxiv.org/abs/2512.09729", "authors": ["Laurynas Adomaitis", "Vincent Israel-Jost", "Alexei Grinbaum"], "title": "Ethics Readiness of Artificial Intelligence: A Practical Evaluation Method", "comment": "23 pages. Data available on GitHub at https://github.com/LA-NS/ethics-readiness-levels", "summary": "We present Ethics Readiness Levels (ERLs), a four-level, iterative method to track how ethical reflection is implemented in the design of AI systems. ERLs bridge high-level ethical principles and everyday engineering by turning ethical values into concrete prompts, checks, and controls within real use cases. The evaluation is conducted using a dynamic, tree-like questionnaire built from context-specific indicators, ensuring relevance to the technology and application domain. Beyond being a managerial tool, ERLs help facilitate a structured dialogue between ethics experts and technical teams, while our scoring system helps track progress over time. We demonstrate the methodology through two case studies: an AI facial sketch generator for law enforcement and a collaborative industrial robot. The ERL tool effectively catalyzes concrete design changes and promotes a shift from narrow technological solutionism to a more reflective, ethics-by-design mindset."}
{"id": "2512.09724", "categories": ["stat.AP", "astro-ph.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.09724", "abs": "https://arxiv.org/abs/2512.09724", "authors": ["Nikoloz Gigiberia"], "title": "Bayesian Model Selection with an Application to Cosmology", "comment": null, "summary": "We investigate cosmological parameter inference and model selection from a Bayesian perspective. Type Ia supernova data from the Dark Energy Survey (DES-SN5YR) are used to test the \\(Λ\\)CDM, \\(w\\)CDM, and CPL cosmological models. Posterior inference is performed via Hamiltonian Monte Carlo using the No-U-Turn Sampler (NUTS) implemented in NumPyro and analyzed with ArviZ in Python. Bayesian model comparison is conducted through Bayes factors computed using the \\texttt{bridgesampling} library in R. The results indicate that all three models demonstrate similar predictive performance, but \\(w\\)CDM shows stronger evidence relative to \\(Λ\\)CDM and CPL. We conclude that, under the assumptions and data used in this study, \\(w\\)CDM provides a better description of cosmological expansion."}
{"id": "2512.09458", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09458", "abs": "https://arxiv.org/abs/2512.09458", "authors": ["Sławomir Nowaczyk"], "title": "Architectures for Building Agentic AI", "comment": "This is a preprint of a chapter accepted for publication in Generative and Agentic AI Reliability: Architectures, Challenges, and Trust for Autonomous Systems, published by Springer Nature", "summary": "This chapter argues that the reliability of agentic and generative AI is chiefly an architectural property. We define agentic systems as goal-directed, tool-using decision makers operating in closed loops, and show how reliability emerges from principled componentisation (goal manager, planner, tool-router, executor, memory, verifiers, safety monitor, telemetry), disciplined interfaces (schema-constrained, validated, least-privilege tool calls), and explicit control and assurance loops. Building on classical foundations, we propose a practical taxonomy-tool-using agents, memory-augmented agents, planning and self-improvement agents, multi-agent systems, and embodied or web agents - and analyse how each pattern reshapes the reliability envelope and failure modes. We distil design guidance on typed schemas, idempotency, permissioning, transactional semantics, memory provenance and hygiene, runtime governance (budgets, termination conditions), and simulate-before-actuate safeguards."}
{"id": "2512.09142", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09142", "abs": "https://arxiv.org/abs/2512.09142", "authors": ["Sergio Burdisso", "Séverin Baroudi", "Yanis Labrak", "David Grunert", "Pawel Cyrta", "Yiyang Chen", "Srikanth Madikeri", "Esaú Villatoro-Tello", "Thomas Schaaf", "Ricard Marxer", "Petr Motlicek"], "title": "SDialog: A Python Toolkit for End-to-End Agent Building, User Simulation, Dialog Generation, and Evaluation", "comment": "Pre-print submitted to EACL System Demonstration (under review)", "summary": "We present SDialog, an MIT-licensed open-source Python toolkit that unifies dialog generation, evaluation and mechanistic interpretability into a single end-to-end framework for building and analyzing LLM-based conversational agents. Built around a standardized \\texttt{Dialog} representation, SDialog provides: (1) persona-driven multi-agent simulation with composable orchestration for controlled, synthetic dialog generation, (2) comprehensive evaluation combining linguistic metrics, LLM-as-a-judge and functional correctness validators, (3) mechanistic interpretability tools for activation inspection and steering via feature ablation and induction, and (4) audio generation with full acoustic simulation including 3D room modeling and microphone effects. The toolkit integrates with all major LLM backends, enabling mixed-backend experiments under a unified API. By coupling generation, evaluation, and interpretability in a dialog-centric architecture, SDialog enables researchers to build, benchmark and understand conversational systems more systematically."}
{"id": "2512.09114", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.09114", "abs": "https://arxiv.org/abs/2512.09114", "authors": ["Pamela Gupta"], "title": "AI TIPS 2.0: A Comprehensive Framework for Operationalizing AI Governance", "comment": "47 pages", "summary": "The deployment of AI systems faces three critical governance challenges that current frameworks fail to adequately address. First, organizations struggle with inadequate risk assessment at the use case level, exemplified by the Humana class action lawsuit and other high impact cases where an AI system deployed to production exhibited both significant bias and high error rates, resulting in improper healthcare claim denials. Each AI use case presents unique risk profiles requiring tailored governance, yet most frameworks provide one size fits all guidance. Second, existing frameworks like ISO 42001 and NIST AI RMF remain at high conceptual levels, offering principles without actionable controls, leaving practitioners unable to translate governance requirements into specific technical implementations. Third, organizations lack mechanisms for operationalizing governance at scale, with no systematic approach to embed trustworthy AI practices throughout the development lifecycle, measure compliance quantitatively, or provide role-appropriate visibility from boards to data scientists. We present AI TIPS, Artificial Intelligence Trust-Integrated Pillars for Sustainability 2.0, update to the comprehensive operational framework developed in 2019,four years before NIST's AI Risk Management Framework, that directly addresses these challenges."}
{"id": "2512.09732", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.09732", "abs": "https://arxiv.org/abs/2512.09732", "authors": ["Anastasios Apsemidis", "Dimitris Mavridis", "Nikolaos Demiris"], "title": "Network Meta Analysis of Mean Survival", "comment": "29 pages, 4 figures", "summary": "Decisions based upon pairwise comparisons of multiple treatments are naturally performed in terms of the mean survival of the selected study arms or functions thereof. However, synthesis of treatment comparisons is usually performed on surrogates of the mean survival, such as hazard ratios or restricted mean survival times. Thus, network meta-analysis techniques may suffer from the limitations of these approaches, such as incorrect proportional hazards assumption or short-term follow-up periods. We propose a Bayesian framework for the network meta-analysis of the main outcome informing the decision, the mean survival of a treatment. Its derivation involves extrapolation of the observed survival curves. We use methods for stable extrapolation that integrate long term evidence based upon mortality projections. Extrapolations are performed using flexible poly-hazard parametric models and M-spline-based methods. We assess the computational and statistical efficiency of different techniques using a simulation study and apply the developed methods to two real data sets. The proposed method is formulated within a decision theoretic framework for cost-effectiveness analyses, where the `best' treatment is to be selected and incorporating the associated cost information is straightforward."}
{"id": "2512.09566", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09566", "abs": "https://arxiv.org/abs/2512.09566", "authors": ["Junkai Ji", "Zhangfan Yang", "Dong Xu", "Ruibin Bai", "Jianqiang Li", "Tingjun Hou", "Zexuan Zhu"], "title": "Toward Closed-loop Molecular Discovery via Language Model, Property Alignment and Strategic Search", "comment": "21 pages, 5 figures", "summary": "Drug discovery is a time-consuming and expensive process, with traditional high-throughput and docking-based virtual screening hampered by low success rates and limited scalability. Recent advances in generative modelling, including autoregressive, diffusion, and flow-based approaches, have enabled de novo ligand design beyond the limits of enumerative screening. Yet these models often suffer from inadequate generalization, limited interpretability, and an overemphasis on binding affinity at the expense of key pharmacological properties, thereby restricting their translational utility. Here we present Trio, a molecular generation framework integrating fragment-based molecular language modeling, reinforcement learning, and Monte Carlo tree search, for effective and interpretable closed-loop targeted molecular design. Through the three key components, Trio enables context-aware fragment assembly, enforces physicochemical and synthetic feasibility, and guides a balanced search between the exploration of novel chemotypes and the exploitation of promising intermediates within protein binding pockets. Experimental results show that Trio reliably achieves chemically valid and pharmacologically enhanced ligands, outperforming state-of-the-art approaches with improved binding affinity (+7.85%), drug-likeness (+11.10%) and synthetic accessibility (+12.05%), while expanding molecular diversity more than fourfold."}
{"id": "2512.09261", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.09261", "abs": "https://arxiv.org/abs/2512.09261", "authors": ["Justin Heath"], "title": "FLARE v2: A Recursive Framework for Program Comprehension Across Languages and Levels of Abstraction", "comment": "22 pages, 4 figures. Preprint under CC BY-NC-SA 4.0", "summary": "Building on the classroom framework reported in Heath et al. (2025), this paper proposes FLARE v2 as a recursive, semiotically informed account of how program meaning is constructed. It reinterprets the descriptive tiers of FLARE v1 as instances of a single generative operation: identify elements (characterised by the four properties Receives, Sends, Effects, Shares); analyse their bindings along two dimensions (Causal-Temporal and Communicative); and recognise the new element that emerges. The Causal-Temporal dimension encompasses three subtypes - Sequential, Branch, and Event - that together account for control flow in both procedural and event-driven environments. A Compositional Ladder provides a visual parallel between literacy progressions and programming structures, illustrating how recursive composition operates from blocks and statements through segments, systems, and services. The framework aims to address conceptual and cognitive-load limitations reported in FLARE v1 and is situated within semiotic and program-comprehension theory. FLARE v2 is presented as a conceptual lens with potential implications for pedagogy and curriculum design; implementation and empirical evaluation are left for future work."}
{"id": "2512.09882", "categories": ["cs.AI", "cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.09882", "abs": "https://arxiv.org/abs/2512.09882", "authors": ["Justin W. Lin", "Eliot Krzysztof Jones", "Donovan Julian Jasper", "Ethan Jun-shen Ho", "Anna Wu", "Arnold Tianyi Yang", "Neil Perry", "Andy Zou", "Matt Fredrikson", "J. Zico Kolter", "Percy Liang", "Dan Boneh", "Daniel E. Ho"], "title": "Comparing AI Agents to Cybersecurity Professionals in Real-World Penetration Testing", "comment": null, "summary": "We present the first comprehensive evaluation of AI agents against human cybersecurity professionals in a live enterprise environment. We evaluate ten cybersecurity professionals alongside six existing AI agents and ARTEMIS, our new agent scaffold, on a large university network consisting of ~8,000 hosts across 12 subnets. ARTEMIS is a multi-agent framework featuring dynamic prompt generation, arbitrary sub-agents, and automatic vulnerability triaging. In our comparative study, ARTEMIS placed second overall, discovering 9 valid vulnerabilities with an 82% valid submission rate and outperforming 9 of 10 human participants. While existing scaffolds such as Codex and CyAgent underperformed relative to most human participants, ARTEMIS demonstrated technical sophistication and submission quality comparable to the strongest participants. We observe that AI agents offer advantages in systematic enumeration, parallel exploitation, and cost -- certain ARTEMIS variants cost $18/hour versus $60/hour for professional penetration testers. We also identify key capability gaps: AI agents exhibit higher false-positive rates and struggle with GUI-based tasks."}
{"id": "2512.09629", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09629", "abs": "https://arxiv.org/abs/2512.09629", "authors": ["Emanuele La Malfa", "Ping Zhu", "Samuele Marro", "Sara Bernardini", "Michael Wooldridge"], "title": "An End-to-end Planning Framework with Agentic LLMs and PDDL", "comment": "Code: https://github.com/EmanueleLM/MultiAgentPlanning", "summary": "We present an end-to-end framework for planning supported by verifiers. An orchestrator receives a human specification written in natural language and converts it into a PDDL (Planning Domain Definition Language) model, where the domain and problem are iteratively refined by sub-modules (agents) to address common planning requirements, such as time constraints and optimality, as well as ambiguities and contradictions that may exist in the human specification. The validated domain and problem are then passed to an external planning engine to generate a plan. The orchestrator and agents are powered by Large Language Models (LLMs) and require no human intervention at any stage of the process. Finally, a module translates the final plan back into natural language to improve human readability while maintaining the correctness of each step. We demonstrate the flexibility and effectiveness of our framework across various domains and tasks, including the Google NaturalPlan benchmark and PlanBench, as well as planning problems like Blocksworld and the Tower of Hanoi (where LLMs are known to struggle even with small instances). Our framework can be integrated with any PDDL planning engine and validator (such as Fast Downward, LPG, POPF, VAL, and uVAL, which we have tested) and represents a significant step toward end-to-end planning aided by LLMs."}
{"id": "2512.09340", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09340", "abs": "https://arxiv.org/abs/2512.09340", "authors": ["Chethana Prasad Kabgere"], "title": "Visual Categorization Across Minds and Models: Cognitive Analysis of Human Labeling and Neuro-Symbolic Integration", "comment": "12 pages, 3 figures. Research manuscript based on the final project for CS6795 (Introduction to Cognitive Science), Georgia Tech", "summary": "Understanding how humans and AI systems interpret ambiguous visual stimuli offers critical insight into the nature of perception, reasoning, and decision-making. This paper examines image labeling performance across human participants and deep neural networks, focusing on low-resolution, perceptually degraded stimuli. Drawing from computational cognitive science, cognitive architectures, and connectionist-symbolic hybrid models, we contrast human strategies such as analogical reasoning, shape-based recognition, and confidence modulation with AI's feature-based processing. Grounded in Marr's tri-level hypothesis, Simon's bounded rationality, and Thagard's frameworks of representation and emotion, we analyze participant responses in relation to Grad-CAM visualizations of model attention. Human behavior is further interpreted through cognitive principles modeled in ACT-R and Soar, revealing layered and heuristic decision strategies under uncertainty. Our findings highlight key parallels and divergences between biological and artificial systems in representation, inference, and confidence calibration. The analysis motivates future neuro-symbolic architectures that unify structured symbolic reasoning with connectionist representations. Such architectures, informed by principles of embodiment, explainability, and cognitive alignment, offer a path toward AI systems that are not only performant but also interpretable and cognitively grounded."}
{"id": "2512.09727", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09727", "abs": "https://arxiv.org/abs/2512.09727", "authors": ["Junlin Xiao", "Victor-Alexandru Darvariu", "Bruno Lacerda", "Nick Hawes"], "title": "Gaussian Process Aggregation for Root-Parallel Monte Carlo Tree Search with Continuous Actions", "comment": null, "summary": "Monte Carlo Tree Search is a cornerstone algorithm for online planning, and its root-parallel variant is widely used when wall clock time is limited but best performance is desired. In environments with continuous action spaces, how to best aggregate statistics from different threads is an important yet underexplored question. In this work, we introduce a method that uses Gaussian Process Regression to obtain value estimates for promising actions that were not trialed in the environment. We perform a systematic evaluation across 6 different domains, demonstrating that our approach outperforms existing aggregation strategies while requiring a modest increase in inference time."}
{"id": "2512.09458", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09458", "abs": "https://arxiv.org/abs/2512.09458", "authors": ["Sławomir Nowaczyk"], "title": "Architectures for Building Agentic AI", "comment": "This is a preprint of a chapter accepted for publication in Generative and Agentic AI Reliability: Architectures, Challenges, and Trust for Autonomous Systems, published by Springer Nature", "summary": "This chapter argues that the reliability of agentic and generative AI is chiefly an architectural property. We define agentic systems as goal-directed, tool-using decision makers operating in closed loops, and show how reliability emerges from principled componentisation (goal manager, planner, tool-router, executor, memory, verifiers, safety monitor, telemetry), disciplined interfaces (schema-constrained, validated, least-privilege tool calls), and explicit control and assurance loops. Building on classical foundations, we propose a practical taxonomy-tool-using agents, memory-augmented agents, planning and self-improvement agents, multi-agent systems, and embodied or web agents - and analyse how each pattern reshapes the reliability envelope and failure modes. We distil design guidance on typed schemas, idempotency, permissioning, transactional semantics, memory provenance and hygiene, runtime governance (budgets, termination conditions), and simulate-before-actuate safeguards."}
{"id": "2512.09736", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09736", "abs": "https://arxiv.org/abs/2512.09736", "authors": ["Jingtian Yan", "Zhifei Li", "William Kang", "Stephen F. Smith", "Jiaoyang Li"], "title": "Analyzing Planner Design Trade-offs for MAPF under Realistic Simulation", "comment": null, "summary": "Multi-Agent Path Finding (MAPF) algorithms are increasingly deployed in industrial warehouses and automated manufacturing facilities, where robots must operate reliably under real-world physical constraints. However, existing MAPF evaluation frameworks typically rely on simplified robot models, leaving a substantial gap between algorithmic benchmarks and practical performance. Recent frameworks such as SMART, incorporate kinodynamic modeling and offer the MAPF community a platform for large-scale, realistic evaluation. Building on this capability, this work investigates how key planner design choices influence performance under realistic execution settings. We systematically study three fundamental factors: (1) the relationship between solution optimality and execution performance, (2) the sensitivity of system performance to inaccuracies in kinodynamic modeling, and (3) the interaction between model accuracy and plan optimality. Empirically, we examine these factors to understand how these design choices affect performance in realistic scenarios. We highlight open challenges and research directions to steer the community toward practical, real-world deployment."}
{"id": "2512.09566", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09566", "abs": "https://arxiv.org/abs/2512.09566", "authors": ["Junkai Ji", "Zhangfan Yang", "Dong Xu", "Ruibin Bai", "Jianqiang Li", "Tingjun Hou", "Zexuan Zhu"], "title": "Toward Closed-loop Molecular Discovery via Language Model, Property Alignment and Strategic Search", "comment": "21 pages, 5 figures", "summary": "Drug discovery is a time-consuming and expensive process, with traditional high-throughput and docking-based virtual screening hampered by low success rates and limited scalability. Recent advances in generative modelling, including autoregressive, diffusion, and flow-based approaches, have enabled de novo ligand design beyond the limits of enumerative screening. Yet these models often suffer from inadequate generalization, limited interpretability, and an overemphasis on binding affinity at the expense of key pharmacological properties, thereby restricting their translational utility. Here we present Trio, a molecular generation framework integrating fragment-based molecular language modeling, reinforcement learning, and Monte Carlo tree search, for effective and interpretable closed-loop targeted molecular design. Through the three key components, Trio enables context-aware fragment assembly, enforces physicochemical and synthetic feasibility, and guides a balanced search between the exploration of novel chemotypes and the exploitation of promising intermediates within protein binding pockets. Experimental results show that Trio reliably achieves chemically valid and pharmacologically enhanced ligands, outperforming state-of-the-art approaches with improved binding affinity (+7.85%), drug-likeness (+11.10%) and synthetic accessibility (+12.05%), while expanding molecular diversity more than fourfold."}
{"id": "2512.09829", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09829", "abs": "https://arxiv.org/abs/2512.09829", "authors": ["Khurram Khalil", "Muhammad Mahad Khaliq", "Khaza Anuarul Hoque"], "title": "RIFT: A Scalable Methodology for LLM Accelerator Fault Assessment using Reinforcement Learning", "comment": "Accepted in the IEEE DATE 2026 conference", "summary": "The massive scale of modern AI accelerators presents critical challenges to traditional fault assessment methodologies, which face prohibitive computational costs and provide poor coverage of critical failure modes. This paper introduces RIFT (Reinforcement Learning-guided Intelligent Fault Targeting), a scalable framework that automates the discovery of minimal, high-impact fault scenarios for efficient design-time fault assessment. RIFT transforms the complex search for worst-case faults into a sequential decision-making problem, combining hybrid sensitivity analysis for search space pruning with reinforcement learning to intelligently generate minimal, high-impact test suites. Evaluated on billion-parameter Large Language Model (LLM) workloads using NVIDIA A100 GPUs, RIFT achieves a \\textbf{2.2$\\times$} fault assessment speedup over evolutionary methods and reduces the required test vector volume by over \\textbf{99\\%} compared to random fault injection, all while achieving \\textbf{superior fault coverage}. The proposed framework also provides actionable data to enable intelligent hardware protection strategies, demonstrating that RIFT-guided selective error correction code provides a \\textbf{12.8$\\times$} improvement in \\textbf{cost-effectiveness} (coverage per unit area) compared to uniform triple modular redundancy protection. RIFT automatically generates UVM-compliant verification artifacts, ensuring its findings are directly actionable and integrable into commercial RTL verification workflows."}
{"id": "2512.09570", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09570", "abs": "https://arxiv.org/abs/2512.09570", "authors": ["Jelena Cupac"], "title": "The Gender Code: Gendering the Global Governance of Artificial Intelligence", "comment": "The paper is part of the Handbook on the Global Governance of Artificial Intelligence, forthcoming with Edward Elgar Publishing", "summary": "This paper examines how international AI governance frameworks address gender issues and gender-based harms. The analysis covers binding regulations, such as the EU AI Act; soft law instruments, like the UNESCO Recommendations on AI Ethics; and global initiatives, such as the Global Partnership on AI (GPAI). These instruments reveal emerging trends, including the integration of gender concerns into broader human rights frameworks, a shift toward explicit gender-related provisions, and a growing emphasis on inclusivity and diversity. Yet, some critical gaps persist, including inconsistent treatment of gender across governance documents, limited engagement with intersectionality, and a lack of robust enforcement mechanisms. However, this paper argues that effective AI governance must be intersectional, enforceable, and inclusive. This is key to moving beyond tokenism toward meaningful equity and preventing reinforcement of existing inequalities. The study contributes to ethical AI debates by highlighting the importance of gender-sensitive governance in building a just technological future."}
{"id": "2512.09831", "categories": ["cs.AI", "cs.LG", "cs.MA", "cs.SI"], "pdf": "https://arxiv.org/pdf/2512.09831", "abs": "https://arxiv.org/abs/2512.09831", "authors": ["Chainarong Amornbunchornvej"], "title": "Interpretation as Linear Transformation: A Cognitive-Geometric Model of Belief and Meaning", "comment": "The first draft of cognitive geometry model", "summary": "This paper develops a geometric framework for modeling belief, motivation, and influence across cognitively heterogeneous agents. Each agent is represented by a personalized value space, a vector space encoding the internal dimensions through which the agent interprets and evaluates meaning. Beliefs are formalized as structured vectors-abstract beings-whose transmission is mediated by linear interpretation maps. A belief survives communication only if it avoids the null spaces of these maps, yielding a structural criterion for intelligibility, miscommunication, and belief death.\n  Within this framework, I show how belief distortion, motivational drift, counterfactual evaluation, and the limits of mutual understanding arise from purely algebraic constraints. A central result-\"the No-Null-Space Leadership Condition\"-characterizes leadership as a property of representational reachability rather than persuasion or authority. More broadly, the model explains how abstract beings can propagate, mutate, or disappear as they traverse diverse cognitive geometries.\n  The account unifies insights from conceptual spaces, social epistemology, and AI value alignment by grounding meaning preservation in structural compatibility rather than shared information or rationality. I argue that this cognitive-geometric perspective clarifies the epistemic boundaries of influence in both human and artificial systems, and offers a general foundation for analyzing belief dynamics across heterogeneous agents."}
{"id": "2512.09629", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09629", "abs": "https://arxiv.org/abs/2512.09629", "authors": ["Emanuele La Malfa", "Ping Zhu", "Samuele Marro", "Sara Bernardini", "Michael Wooldridge"], "title": "An End-to-end Planning Framework with Agentic LLMs and PDDL", "comment": "Code: https://github.com/EmanueleLM/MultiAgentPlanning", "summary": "We present an end-to-end framework for planning supported by verifiers. An orchestrator receives a human specification written in natural language and converts it into a PDDL (Planning Domain Definition Language) model, where the domain and problem are iteratively refined by sub-modules (agents) to address common planning requirements, such as time constraints and optimality, as well as ambiguities and contradictions that may exist in the human specification. The validated domain and problem are then passed to an external planning engine to generate a plan. The orchestrator and agents are powered by Large Language Models (LLMs) and require no human intervention at any stage of the process. Finally, a module translates the final plan back into natural language to improve human readability while maintaining the correctness of each step. We demonstrate the flexibility and effectiveness of our framework across various domains and tasks, including the Google NaturalPlan benchmark and PlanBench, as well as planning problems like Blocksworld and the Tower of Hanoi (where LLMs are known to struggle even with small instances). Our framework can be integrated with any PDDL planning engine and validator (such as Fast Downward, LPG, POPF, VAL, and uVAL, which we have tested) and represents a significant step toward end-to-end planning aided by LLMs."}
{"id": "2512.09882", "categories": ["cs.AI", "cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.09882", "abs": "https://arxiv.org/abs/2512.09882", "authors": ["Justin W. Lin", "Eliot Krzysztof Jones", "Donovan Julian Jasper", "Ethan Jun-shen Ho", "Anna Wu", "Arnold Tianyi Yang", "Neil Perry", "Andy Zou", "Matt Fredrikson", "J. Zico Kolter", "Percy Liang", "Dan Boneh", "Daniel E. Ho"], "title": "Comparing AI Agents to Cybersecurity Professionals in Real-World Penetration Testing", "comment": null, "summary": "We present the first comprehensive evaluation of AI agents against human cybersecurity professionals in a live enterprise environment. We evaluate ten cybersecurity professionals alongside six existing AI agents and ARTEMIS, our new agent scaffold, on a large university network consisting of ~8,000 hosts across 12 subnets. ARTEMIS is a multi-agent framework featuring dynamic prompt generation, arbitrary sub-agents, and automatic vulnerability triaging. In our comparative study, ARTEMIS placed second overall, discovering 9 valid vulnerabilities with an 82% valid submission rate and outperforming 9 of 10 human participants. While existing scaffolds such as Codex and CyAgent underperformed relative to most human participants, ARTEMIS demonstrated technical sophistication and submission quality comparable to the strongest participants. We observe that AI agents offer advantages in systematic enumeration, parallel exploitation, and cost -- certain ARTEMIS variants cost $18/hour versus $60/hour for professional penetration testers. We also identify key capability gaps: AI agents exhibit higher false-positive rates and struggle with GUI-based tasks."}
{"id": "2512.09707", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2512.09707", "abs": "https://arxiv.org/abs/2512.09707", "authors": ["Thanh-Lam T. Nguyen", "Ngoc-Quang Le", "Thu-Trang Pham", "Mai-Vu Tran"], "title": "Knowledge Graph Enrichment and Reasoning for Nobel Laureates", "comment": null, "summary": "This project aims to construct and analyze a comprehensive knowledge graph of Nobel Prize and Laureates by enriching existing datasets with biographical information extracted from Wikipedia. Our approach integrates multiple advanced techniques, consisting of automatic data augmentation using LLMs for Named Entity Recognition (NER) and Relation Extraction (RE) tasks, and social network analysis to uncover hidden patterns within the scientific community. Furthermore, we also develop a GraphRAG-based chatbot system utilizing a fine-tuned model for Text2Cypher translation, enabling natural language querying over the knowledge graph. Experimental results demonstrate that the enriched graph possesses small-world network properties, identifying key influential figures and central organizations. The chatbot system achieves a competitive accuracy on a custom multiple-choice evaluation dataset, proving the effectiveness of combining LLMs with structured knowledge bases for complex reasoning tasks. Data and source code are available at: https://github.com/tlam25/network-of-awards-and-winners."}
{"id": "2512.09895", "categories": ["cs.AI", "cs.DL"], "pdf": "https://arxiv.org/pdf/2512.09895", "abs": "https://arxiv.org/abs/2512.09895", "authors": ["Jane Greenberg", "Scott McClellan", "Addy Ireland", "Robert Sammarco", "Colton Gerber", "Christopher B. Rauch", "Mat Kelly", "John Kunze", "Yuan An", "Eric Toberer"], "title": "Human-in-the-Loop and AI: Crowdsourcing Metadata Vocabulary for Materials Science", "comment": "Metadata and Semantics Research Conference 2025, 14 pages, 7 figures", "summary": "Metadata vocabularies are essential for advancing FAIR and FARR data principles, but their development constrained by limited human resources and inconsistent standardization practices. This paper introduces MatSci-YAMZ, a platform that integrates artificial intelligence (AI) and human-in-the-loop (HILT), including crowdsourcing, to support metadata vocabulary development. The paper reports on a proof-of-concept use case evaluating the AI-HILT model in materials science, a highly interdisciplinary domain Six (6) participants affiliated with the NSF Institute for Data-Driven Dynamical Design (ID4) engaged with the MatSci-YAMZ plaform over several weeks, contributing term definitions and providing examples to prompt the AI-definitions refinement. Nineteen (19) AI-generated definitions were successfully created, with iterative feedback loops demonstrating the feasibility of AI-HILT refinement. Findings confirm the feasibility AI-HILT model highlighting 1) a successful proof of concept, 2) alignment with FAIR and open-science principles, 3) a research protocol to guide future studies, and 4) the potential for scalability across domains. Overall, MatSci-YAMZ's underlying model has the capacity to enhance semantic transparency and reduce time required for consensus building and metadata vocabulary development."}
{"id": "2512.09727", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09727", "abs": "https://arxiv.org/abs/2512.09727", "authors": ["Junlin Xiao", "Victor-Alexandru Darvariu", "Bruno Lacerda", "Nick Hawes"], "title": "Gaussian Process Aggregation for Root-Parallel Monte Carlo Tree Search with Continuous Actions", "comment": null, "summary": "Monte Carlo Tree Search is a cornerstone algorithm for online planning, and its root-parallel variant is widely used when wall clock time is limited but best performance is desired. In environments with continuous action spaces, how to best aggregate statistics from different threads is an important yet underexplored question. In this work, we introduce a method that uses Gaussian Process Regression to obtain value estimates for promising actions that were not trialed in the environment. We perform a systematic evaluation across 6 different domains, demonstrating that our approach outperforms existing aggregation strategies while requiring a modest increase in inference time."}
{"id": "2512.09897", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09897", "abs": "https://arxiv.org/abs/2512.09897", "authors": ["Haoye Lu", "Pavan Seshadri", "Kaheer Suleman"], "title": "SCOPE: Language Models as One-Time Teacher for Hierarchical Planning in Text Environments", "comment": null, "summary": "Long-term planning in complex, text-based environments presents significant challenges due to open-ended action spaces, ambiguous observations, and sparse feedback. Recent research suggests that large language models (LLMs) encode rich semantic knowledge about the world, which can be valuable for guiding agents in high-level reasoning and planning across both embodied and purely textual settings. However, existing approaches often depend heavily on querying LLMs during training and inference, making them computationally expensive and difficult to deploy efficiently. In addition, these methods typically employ a pretrained, unaltered LLM whose parameters remain fixed throughout training, providing no opportunity for adaptation to the target task. To address these limitations, we introduce SCOPE (Subgoal-COnditioned Pretraining for Efficient planning), a one-shot hierarchical planner that leverages LLM-generated subgoals only at initialization to pretrain a lightweight student model. Unlike prior approaches that distill LLM knowledge by repeatedly prompting the model to adaptively generate subgoals during training, our method derives subgoals directly from example trajectories. This design removes the need for repeated LLM queries, significantly improving efficiency, though at the cost of reduced explainability and potentially suboptimal subgoals. Despite their suboptimality, our results on the TextCraft environment show that LLM-generated subgoals can still serve as a strong starting point for hierarchical goal decomposition in text-based planning tasks. Compared to the LLM-based hierarchical agent ADaPT (Prasad et al., 2024), which achieves a 0.52 success rate, our method reaches 0.56 and reduces inference time from 164.4 seconds to just 3.0 seconds."}
{"id": "2512.09729", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09729", "abs": "https://arxiv.org/abs/2512.09729", "authors": ["Laurynas Adomaitis", "Vincent Israel-Jost", "Alexei Grinbaum"], "title": "Ethics Readiness of Artificial Intelligence: A Practical Evaluation Method", "comment": "23 pages. Data available on GitHub at https://github.com/LA-NS/ethics-readiness-levels", "summary": "We present Ethics Readiness Levels (ERLs), a four-level, iterative method to track how ethical reflection is implemented in the design of AI systems. ERLs bridge high-level ethical principles and everyday engineering by turning ethical values into concrete prompts, checks, and controls within real use cases. The evaluation is conducted using a dynamic, tree-like questionnaire built from context-specific indicators, ensuring relevance to the technology and application domain. Beyond being a managerial tool, ERLs help facilitate a structured dialogue between ethics experts and technical teams, while our scoring system helps track progress over time. We demonstrate the methodology through two case studies: an AI facial sketch generator for law enforcement and a collaborative industrial robot. The ERL tool effectively catalyzes concrete design changes and promotes a shift from narrow technological solutionism to a more reflective, ethics-by-design mindset."}
{"id": "2512.09908", "categories": ["cs.AI", "cs.LO", "math.CT"], "pdf": "https://arxiv.org/pdf/2512.09908", "abs": "https://arxiv.org/abs/2512.09908", "authors": ["Antonio Lorenzin", "Fabio Zanasi"], "title": "Bayesian Networks, Markov Networks, Moralisation, Triangulation: a Categorical Perspective", "comment": "36 pages. A preliminary version of this work was presented at CALCO 2025, under the title \"An Algebraic Approach to Moralisation and Triangulation of Probabilistic Graphical Models''", "summary": "Moralisation and Triangulation are transformations allowing to switch between different ways of factoring a probability distribution into a graphical model. Moralisation allows to view a Bayesian network (a directed model) as a Markov network (an undirected model), whereas triangulation addresses the opposite direction. We present a categorical framework where these transformations are modelled as functors between a category of Bayesian networks and one of Markov networks. The two kinds of network (the objects of these categories) are themselves represented as functors from a `syntax' domain to a `semantics' codomain. Notably, moralisation and triangulation can be defined inductively on such syntax via functor pre-composition. Moreover, while moralisation is fully syntactic, triangulation relies on semantics. This leads to a discussion of the variable elimination algorithm, reinterpreted here as a functor in its own right, that splits the triangulation procedure in two: one purely syntactic, the other purely semantic. This approach introduces a functorial perspective into the theory of probabilistic graphical models, which highlights the distinctions between syntactic and semantic modifications."}
{"id": "2512.09736", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09736", "abs": "https://arxiv.org/abs/2512.09736", "authors": ["Jingtian Yan", "Zhifei Li", "William Kang", "Stephen F. Smith", "Jiaoyang Li"], "title": "Analyzing Planner Design Trade-offs for MAPF under Realistic Simulation", "comment": null, "summary": "Multi-Agent Path Finding (MAPF) algorithms are increasingly deployed in industrial warehouses and automated manufacturing facilities, where robots must operate reliably under real-world physical constraints. However, existing MAPF evaluation frameworks typically rely on simplified robot models, leaving a substantial gap between algorithmic benchmarks and practical performance. Recent frameworks such as SMART, incorporate kinodynamic modeling and offer the MAPF community a platform for large-scale, realistic evaluation. Building on this capability, this work investigates how key planner design choices influence performance under realistic execution settings. We systematically study three fundamental factors: (1) the relationship between solution optimality and execution performance, (2) the sensitivity of system performance to inaccuracies in kinodynamic modeling, and (3) the interaction between model accuracy and plan optimality. Empirically, we examine these factors to understand how these design choices affect performance in realistic scenarios. We highlight open challenges and research directions to steer the community toward practical, real-world deployment."}
{"id": "2512.08978", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08978", "abs": "https://arxiv.org/abs/2512.08978", "authors": ["Ruud Huijts", "Koen Suilen"], "title": "Institutional AI Sovereignty Through Gateway Architecture: Implementation Report from Fontys ICT", "comment": null, "summary": "To counter fragmented, high-risk adoption of commercial AI tools, we built and ran an institutional AI platform in a six-month, 300-user pilot, showing that a university of applied sciences can offer advanced AI with fair access, transparent risks, controlled costs, and alignment with European law.\n  Commercial AI subscriptions create unequal access and compliance risks through opaque processing and non-EU hosting, yet banning them is neither realistic nor useful. Institutions need a way to provide powerful AI in a sovereign, accountable form.\n  Our solution is a governed gateway platform with three layers: a ChatGPT-style frontend linked to institutional identity that makes model choice explicit; a gateway core enforcing policy, controlling access and budgets, and routing traffic to EU infrastructure by default; and a provider layer wrapping commercial and open-source models in institutional model cards that consolidate vendor documentation into one governance interface.\n  The pilot ran reliably with no privacy incidents and strong adoption, enabling EU-default routing, managed spending, and transparent model choices. Only the gateway pattern combines model diversity and rapid innovation with institutional control.\n  The central insight: AI is not a support function but strategy, demanding dedicated leadership. Sustainable operation requires governance beyond traditional boundaries. We recommend establishing a formal AI Officer role combining technical literacy, governance authority, and educational responsibility. Without it, AI decisions stay ad-hoc and institutional exposure grows. With it, higher-education institutions can realistically operate their own multi-provider AI platform, provided they govern AI as seriously as they teach it."}
{"id": "2512.09829", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09829", "abs": "https://arxiv.org/abs/2512.09829", "authors": ["Khurram Khalil", "Muhammad Mahad Khaliq", "Khaza Anuarul Hoque"], "title": "RIFT: A Scalable Methodology for LLM Accelerator Fault Assessment using Reinforcement Learning", "comment": "Accepted in the IEEE DATE 2026 conference", "summary": "The massive scale of modern AI accelerators presents critical challenges to traditional fault assessment methodologies, which face prohibitive computational costs and provide poor coverage of critical failure modes. This paper introduces RIFT (Reinforcement Learning-guided Intelligent Fault Targeting), a scalable framework that automates the discovery of minimal, high-impact fault scenarios for efficient design-time fault assessment. RIFT transforms the complex search for worst-case faults into a sequential decision-making problem, combining hybrid sensitivity analysis for search space pruning with reinforcement learning to intelligently generate minimal, high-impact test suites. Evaluated on billion-parameter Large Language Model (LLM) workloads using NVIDIA A100 GPUs, RIFT achieves a \\textbf{2.2$\\times$} fault assessment speedup over evolutionary methods and reduces the required test vector volume by over \\textbf{99\\%} compared to random fault injection, all while achieving \\textbf{superior fault coverage}. The proposed framework also provides actionable data to enable intelligent hardware protection strategies, demonstrating that RIFT-guided selective error correction code provides a \\textbf{12.8$\\times$} improvement in \\textbf{cost-effectiveness} (coverage per unit area) compared to uniform triple modular redundancy protection. RIFT automatically generates UVM-compliant verification artifacts, ensuring its findings are directly actionable and integrable into commercial RTL verification workflows."}
{"id": "2512.09570", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09570", "abs": "https://arxiv.org/abs/2512.09570", "authors": ["Jelena Cupac"], "title": "The Gender Code: Gendering the Global Governance of Artificial Intelligence", "comment": "The paper is part of the Handbook on the Global Governance of Artificial Intelligence, forthcoming with Edward Elgar Publishing", "summary": "This paper examines how international AI governance frameworks address gender issues and gender-based harms. The analysis covers binding regulations, such as the EU AI Act; soft law instruments, like the UNESCO Recommendations on AI Ethics; and global initiatives, such as the Global Partnership on AI (GPAI). These instruments reveal emerging trends, including the integration of gender concerns into broader human rights frameworks, a shift toward explicit gender-related provisions, and a growing emphasis on inclusivity and diversity. Yet, some critical gaps persist, including inconsistent treatment of gender across governance documents, limited engagement with intersectionality, and a lack of robust enforcement mechanisms. However, this paper argues that effective AI governance must be intersectional, enforceable, and inclusive. This is key to moving beyond tokenism toward meaningful equity and preventing reinforcement of existing inequalities. The study contributes to ethical AI debates by highlighting the importance of gender-sensitive governance in building a just technological future."}
{"id": "2512.09831", "categories": ["cs.AI", "cs.LG", "cs.MA", "cs.SI"], "pdf": "https://arxiv.org/pdf/2512.09831", "abs": "https://arxiv.org/abs/2512.09831", "authors": ["Chainarong Amornbunchornvej"], "title": "Interpretation as Linear Transformation: A Cognitive-Geometric Model of Belief and Meaning", "comment": "The first draft of cognitive geometry model", "summary": "This paper develops a geometric framework for modeling belief, motivation, and influence across cognitively heterogeneous agents. Each agent is represented by a personalized value space, a vector space encoding the internal dimensions through which the agent interprets and evaluates meaning. Beliefs are formalized as structured vectors-abstract beings-whose transmission is mediated by linear interpretation maps. A belief survives communication only if it avoids the null spaces of these maps, yielding a structural criterion for intelligibility, miscommunication, and belief death.\n  Within this framework, I show how belief distortion, motivational drift, counterfactual evaluation, and the limits of mutual understanding arise from purely algebraic constraints. A central result-\"the No-Null-Space Leadership Condition\"-characterizes leadership as a property of representational reachability rather than persuasion or authority. More broadly, the model explains how abstract beings can propagate, mutate, or disappear as they traverse diverse cognitive geometries.\n  The account unifies insights from conceptual spaces, social epistemology, and AI value alignment by grounding meaning preservation in structural compatibility rather than shared information or rationality. I argue that this cognitive-geometric perspective clarifies the epistemic boundaries of influence in both human and artificial systems, and offers a general foundation for analyzing belief dynamics across heterogeneous agents."}
{"id": "2512.09729", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09729", "abs": "https://arxiv.org/abs/2512.09729", "authors": ["Laurynas Adomaitis", "Vincent Israel-Jost", "Alexei Grinbaum"], "title": "Ethics Readiness of Artificial Intelligence: A Practical Evaluation Method", "comment": "23 pages. Data available on GitHub at https://github.com/LA-NS/ethics-readiness-levels", "summary": "We present Ethics Readiness Levels (ERLs), a four-level, iterative method to track how ethical reflection is implemented in the design of AI systems. ERLs bridge high-level ethical principles and everyday engineering by turning ethical values into concrete prompts, checks, and controls within real use cases. The evaluation is conducted using a dynamic, tree-like questionnaire built from context-specific indicators, ensuring relevance to the technology and application domain. Beyond being a managerial tool, ERLs help facilitate a structured dialogue between ethics experts and technical teams, while our scoring system helps track progress over time. We demonstrate the methodology through two case studies: an AI facial sketch generator for law enforcement and a collaborative industrial robot. The ERL tool effectively catalyzes concrete design changes and promotes a shift from narrow technological solutionism to a more reflective, ethics-by-design mindset."}
{"id": "2512.09882", "categories": ["cs.AI", "cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.09882", "abs": "https://arxiv.org/abs/2512.09882", "authors": ["Justin W. Lin", "Eliot Krzysztof Jones", "Donovan Julian Jasper", "Ethan Jun-shen Ho", "Anna Wu", "Arnold Tianyi Yang", "Neil Perry", "Andy Zou", "Matt Fredrikson", "J. Zico Kolter", "Percy Liang", "Dan Boneh", "Daniel E. Ho"], "title": "Comparing AI Agents to Cybersecurity Professionals in Real-World Penetration Testing", "comment": null, "summary": "We present the first comprehensive evaluation of AI agents against human cybersecurity professionals in a live enterprise environment. We evaluate ten cybersecurity professionals alongside six existing AI agents and ARTEMIS, our new agent scaffold, on a large university network consisting of ~8,000 hosts across 12 subnets. ARTEMIS is a multi-agent framework featuring dynamic prompt generation, arbitrary sub-agents, and automatic vulnerability triaging. In our comparative study, ARTEMIS placed second overall, discovering 9 valid vulnerabilities with an 82% valid submission rate and outperforming 9 of 10 human participants. While existing scaffolds such as Codex and CyAgent underperformed relative to most human participants, ARTEMIS demonstrated technical sophistication and submission quality comparable to the strongest participants. We observe that AI agents offer advantages in systematic enumeration, parallel exploitation, and cost -- certain ARTEMIS variants cost $18/hour versus $60/hour for professional penetration testers. We also identify key capability gaps: AI agents exhibit higher false-positive rates and struggle with GUI-based tasks."}
{"id": "2512.09895", "categories": ["cs.AI", "cs.DL"], "pdf": "https://arxiv.org/pdf/2512.09895", "abs": "https://arxiv.org/abs/2512.09895", "authors": ["Jane Greenberg", "Scott McClellan", "Addy Ireland", "Robert Sammarco", "Colton Gerber", "Christopher B. Rauch", "Mat Kelly", "John Kunze", "Yuan An", "Eric Toberer"], "title": "Human-in-the-Loop and AI: Crowdsourcing Metadata Vocabulary for Materials Science", "comment": "Metadata and Semantics Research Conference 2025, 14 pages, 7 figures", "summary": "Metadata vocabularies are essential for advancing FAIR and FARR data principles, but their development constrained by limited human resources and inconsistent standardization practices. This paper introduces MatSci-YAMZ, a platform that integrates artificial intelligence (AI) and human-in-the-loop (HILT), including crowdsourcing, to support metadata vocabulary development. The paper reports on a proof-of-concept use case evaluating the AI-HILT model in materials science, a highly interdisciplinary domain Six (6) participants affiliated with the NSF Institute for Data-Driven Dynamical Design (ID4) engaged with the MatSci-YAMZ plaform over several weeks, contributing term definitions and providing examples to prompt the AI-definitions refinement. Nineteen (19) AI-generated definitions were successfully created, with iterative feedback loops demonstrating the feasibility of AI-HILT refinement. Findings confirm the feasibility AI-HILT model highlighting 1) a successful proof of concept, 2) alignment with FAIR and open-science principles, 3) a research protocol to guide future studies, and 4) the potential for scalability across domains. Overall, MatSci-YAMZ's underlying model has the capacity to enhance semantic transparency and reduce time required for consensus building and metadata vocabulary development."}
{"id": "2512.09897", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09897", "abs": "https://arxiv.org/abs/2512.09897", "authors": ["Haoye Lu", "Pavan Seshadri", "Kaheer Suleman"], "title": "SCOPE: Language Models as One-Time Teacher for Hierarchical Planning in Text Environments", "comment": null, "summary": "Long-term planning in complex, text-based environments presents significant challenges due to open-ended action spaces, ambiguous observations, and sparse feedback. Recent research suggests that large language models (LLMs) encode rich semantic knowledge about the world, which can be valuable for guiding agents in high-level reasoning and planning across both embodied and purely textual settings. However, existing approaches often depend heavily on querying LLMs during training and inference, making them computationally expensive and difficult to deploy efficiently. In addition, these methods typically employ a pretrained, unaltered LLM whose parameters remain fixed throughout training, providing no opportunity for adaptation to the target task. To address these limitations, we introduce SCOPE (Subgoal-COnditioned Pretraining for Efficient planning), a one-shot hierarchical planner that leverages LLM-generated subgoals only at initialization to pretrain a lightweight student model. Unlike prior approaches that distill LLM knowledge by repeatedly prompting the model to adaptively generate subgoals during training, our method derives subgoals directly from example trajectories. This design removes the need for repeated LLM queries, significantly improving efficiency, though at the cost of reduced explainability and potentially suboptimal subgoals. Despite their suboptimality, our results on the TextCraft environment show that LLM-generated subgoals can still serve as a strong starting point for hierarchical goal decomposition in text-based planning tasks. Compared to the LLM-based hierarchical agent ADaPT (Prasad et al., 2024), which achieves a 0.52 success rate, our method reaches 0.56 and reduces inference time from 164.4 seconds to just 3.0 seconds."}
{"id": "2512.09908", "categories": ["cs.AI", "cs.LO", "math.CT"], "pdf": "https://arxiv.org/pdf/2512.09908", "abs": "https://arxiv.org/abs/2512.09908", "authors": ["Antonio Lorenzin", "Fabio Zanasi"], "title": "Bayesian Networks, Markov Networks, Moralisation, Triangulation: a Categorical Perspective", "comment": "36 pages. A preliminary version of this work was presented at CALCO 2025, under the title \"An Algebraic Approach to Moralisation and Triangulation of Probabilistic Graphical Models''", "summary": "Moralisation and Triangulation are transformations allowing to switch between different ways of factoring a probability distribution into a graphical model. Moralisation allows to view a Bayesian network (a directed model) as a Markov network (an undirected model), whereas triangulation addresses the opposite direction. We present a categorical framework where these transformations are modelled as functors between a category of Bayesian networks and one of Markov networks. The two kinds of network (the objects of these categories) are themselves represented as functors from a `syntax' domain to a `semantics' codomain. Notably, moralisation and triangulation can be defined inductively on such syntax via functor pre-composition. Moreover, while moralisation is fully syntactic, triangulation relies on semantics. This leads to a discussion of the variable elimination algorithm, reinterpreted here as a functor in its own right, that splits the triangulation procedure in two: one purely syntactic, the other purely semantic. This approach introduces a functorial perspective into the theory of probabilistic graphical models, which highlights the distinctions between syntactic and semantic modifications."}

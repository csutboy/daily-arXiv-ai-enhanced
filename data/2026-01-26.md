<div id=toc></div>

# Table of Contents

- [cs.SI](#cs.SI) [Total: 4]
- [stat.AP](#stat.AP) [Total: 4]
- [cs.AI](#cs.AI) [Total: 17]
- [econ.EM](#econ.EM) [Total: 4]
- [cs.CY](#cs.CY) [Total: 4]


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [1] [Policy-Embedded Graph Expansion: Networked HIV Testing with Diffusion-Driven Network Samples](https://arxiv.org/abs/2601.16233)
*Akseli Kangaslahti,Davin Choo,Lingkai Kong,Milind Tambe,Alastair van Heerden,Cheryl Johnson*

Main category: cs.SI

TL;DR: 提出PEGE框架和DDB模型，用于HIV网络检测优化，在真实传播网络上比基线方法提升13%奖励和9%检测率


<details>
  <summary>Details</summary>
Motivation: 现有HIV智能检测算法依赖不切实际的假设，无法在实际部署中应用。需要开发适用于现实世界增量揭示疾病网络和有限数据环境的检测方法

Method: 提出Policy-Embedded Graph Expansion (PEGE)框架，将图扩展的生成分布直接嵌入决策策略；设计Dynamics-Driven Branching (DDB)扩散图扩展模型，适用于数据有限和森林结构的现实转诊过程

Result: 在真实HIV传播网络上，PEGE+DDB方法比基线提升13%折扣奖励和9%HIV检测率（测试25%人口时），探索了驱动决策质量的关键权衡

Conclusion: PEGE框架和DDB模型为现实世界HIV检测提供了有效的解决方案，支持联合国可持续发展目标3.3，在实际部署中具有应用潜力

Abstract: HIV is a retrovirus that attacks the human immune system and can lead to death without proper treatment. In collaboration with the WHO and Wits University, we study how to improve the efficiency of HIV testing with the goal of eventual deployment, directly supporting progress toward UN Sustainable Development Goal 3.3. While prior work has demonstrated the promise of intelligent algorithms for sequential, network-based HIV testing, existing approaches rely on assumptions that are impractical in our real-world implementations. Here, we study sequential testing on incrementally revealed disease networks and introduce Policy-Embedded Graph Expansion (PEGE), a novel framework that directly embeds a generative distribution over graph expansions into the decision-making policy rather than attempting explicit topological reconstruction. We further propose Dynamics-Driven Branching (DDB), a diffusion-based graph expansion model that supports decision making in PEGE and is designed for data-limited settings where forest structures arise naturally, as in our real-world referral process. Experiments on real HIV transmission networks show that the combined approach (PEGE + DDB) consistently outperforms existing baselines (e.g., 13% improvement in discounted reward and 9% more HIV detections with 25% of the population tested) and explore key tradeoffs that drive decision quality.

</details>


### [2] [Bringing order to network centrality measures](https://arxiv.org/abs/2601.16236)
*G. Exarchakos,R. van der Hofstad,O. Nagy,M. Pandey*

Main category: cs.SI

TL;DR: 提出一种基于顶点排序的图中心性度量定量比较方法，该方法概念简单、数学优雅，能够定量重述先前难以形式化的猜想，并为网络科学家提供有用的近似方案。


<details>
  <summary>Details</summary>
Motivation: 现有的图中心性度量比较方法往往难以形式化，缺乏定量分析框架，网络科学家需要更系统的方法来比较不同中心性度量之间的关系。

Method: 基于顶点排序的定量比较方法，通过中心性度量诱导的顶点排序来比较任意中心性度量对，该方法概念简单且数学优雅。

Result: 该方法能够定量重述先前难以形式化的猜想，为网络科学家提供有用的近似方案，并提出了具有独立意义的新猜想。

Conclusion: 提出的基于顶点排序的图中心性度量比较方法具有概念简单、数学优雅的特点，为网络分析提供了实用的定量工具，并开启了新的研究方向。

Abstract: We introduce a quantitative method to compare arbitrary pairs of graph centrality measures, based on the ordering of vertices induced by them. The proposed method is conceptually simple, mathematically elegant, and allows for a quantitative restatement of many conjectures that were previously cumbersome to formalize. Moreover, it produces an approximation scheme useful for network scientists. We explore some of these uses and formulate new conjectures that are of independent interest.

</details>


### [3] [Improving the Accuracy of Community Detection on Signed Networks via Community Refinement and Contrastive Learning](https://arxiv.org/abs/2601.16372)
*Hyunuk Shin,Hojin Kim,Chanyoung Lee,Yeon-Chang Lee,David Yoon Suk Kang*

Main category: cs.SI

TL;DR: ReCon是一个模型无关的后处理框架，通过迭代的四个步骤（结构细化、边界细化、对比学习和聚类）来改进符号网络中的社区检测结果，提高检测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有符号网络社区检测方法由于噪声或冲突的边符号，常常产生不一致的社区划分结果，需要一种能够提高检测可靠性的解决方案。

Method: 提出ReCon框架，包含四个迭代步骤：1) 结构细化，2) 边界细化，3) 对比学习，4) 聚类。该框架是模型无关的，可与现有社区检测方法集成。

Result: 在18个合成网络和4个真实网络上的实验表明，ReCon能够持续提高社区检测的准确性，且适用于不同的网络特性。

Conclusion: ReCon是一个有效且易于集成的解决方案，能够提高符号网络中社区检测的可靠性，适用于各种网络特性。

Abstract: Community detection (CD) on signed networks is crucial for understanding how positive and negative relations jointly shape network structure. However, existing CD methods often yield inconsistent communities due to noisy or conflicting edge signs. In this paper, we propose ReCon, a model-agnostic post-processing framework that progressively refines community structures through four iterative steps: (1) structural refinement, (2) boundary refinement, (3) contrastive learning, and (4) clustering. Extensive experiments on eighteen synthetic and four real-world networks using four CD methods demonstrate that ReCon consistently enhances community detection accuracy, serving as an effective and easily integrable solution for reliable CD across diverse network properties.

</details>


### [4] [Segregation Before Polarization: How Recommendation Strategies Shape Echo Chamber Pathways](https://arxiv.org/abs/2601.16457)
*Junning Zhao,Kazutoshi Sasahara,Yu Chen*

Main category: cs.SI

TL;DR: 内容推荐算法驱动社交网络走向"先隔离后极化"路径，而链接推荐则不同；转发行为既增加连接又强化回音室；缓解极化需要分阶段的算法干预


<details>
  <summary>Details</summary>
Motivation: 社交媒体平台通过用户偏好与推荐算法之间的反馈循环形成回音室。虽然算法同质化已有研究，但基于内容的推荐与基于链接的推荐驱动网络演化的不同路径尚不清楚。

Method: 使用扩展的动态有界置信模型（BCM），分析内容推荐算法与链接推荐算法对社交网络演化的不同影响，特别关注网络结构隔离与观点分化的时序关系。

Result: 内容推荐算法驱动网络走向"先隔离后极化"路径：结构隔离先于观点分化发生，加速个体隔离同时延迟但最终加剧集体极化。转发行为存在悖论：增加网络连接数却强化回音室，放大原本微不足道的潜在观点差异。

Conclusion: 缓解极化需要分阶段的算法干预策略：在网络演化过程中，应从内容中心策略转向结构中心策略。不同演化阶段需要不同的干预方法。

Abstract: Social media platforms facilitate echo chambers through feedback loops between user preferences and recommendation algorithms. While algorithmic homogeneity is well-documented, the distinct evolutionary pathways driven by content-based versus link-based recommendations remain unclear. Using an extended dynamic Bounded Confidence Model (BCM), we show that content-based algorithms--unlike their link-based counterparts--steer social networks toward a segregation-before-polarization (SbP) pathway. Along this trajectory, structural segregation precedes opinion divergence, accelerating individual isolation while delaying but ultimately intensifying collective polarization. Furthermore, we reveal a paradox in information sharing: Reposting increases the number of connections in the network, yet it simultaneously reinforces echo chambers because it amplifies small, latent opinion differences that would otherwise remain inconsequential. These findings suggest that mitigating polarization requires stage-dependent algorithmic interventions, shifting from content-centric to structure-centric strategies as networks evolve.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [5] [Matrix-Response Generalized Linear Mixed Model with Applications to Longitudinal Brain Images](https://arxiv.org/abs/2601.16340)
*Zhentao Yu,Jiaqi Ding,Guorong Wu,Quefeng Li*

Main category: stat.AP

TL;DR: 提出了一种用于纵向脑网络数据的矩阵响应广义线性混合模型，能够识别外部预测因子影响的连接边，并开发了高效的MCEM算法进行参数估计。


<details>
  <summary>Details</summary>
Motivation: 纵向脑成像数据对于监测个体大脑随时间变化的结构和功能改变至关重要，有助于发现疾病进展的早期生物标志物和评估干预效果。然而，目前缺乏针对高维成像数据衍生的纵向脑网络的统计方法。

Method: 提出矩阵响应广义线性混合模型，用于分析纵向脑网络数据，识别受外部预测因子影响的连接边。开发了高效的蒙特卡洛期望最大化（MCEM）算法进行参数估计。

Result: 广泛的模拟实验表明，该方法能有效识别协变量相关的网络成分，并实现准确的参数估计。在扩散张量成像（DTI）和功能磁共振成像（fMRI）数据集上的应用验证了方法的实用性。

Conclusion: 该方法填补了纵向脑网络分析方法的空白，为从高维脑成像数据中提取动态神经生物学机制提供了有效的统计工具，在疾病进展监测和干预效果评估中具有重要应用价值。

Abstract: Longitudinal brain imaging data facilitate the monitoring of structural and functional alterations in individual brains across time, offering essential understanding of dynamic neurobiological mechanisms. Such data improve sensitivity for detecting early biomarkers of disease progression and enhance the evaluation of intervention effects. While recent matrix-response regression models can relate static brain networks to external predictors, there remain few statistical methods for longitudinal brain networks, especially those derived from high-dimensional imaging data. We introduce a matrix-response generalized linear mixed model that accommodates longitudinal brain networks and identifies edges whose connectivity is influenced by external predictors. An efficient Monte Carlo Expectation-Maximization algorithm is developed for parameter estimation. Extensive simulations demonstrate effective identification of covariate-related network components and accurate parameter estimation. We further demonstrate the usage of the proposed method through applications to diffusion tensor imaging (DTI) and functional MRI (fMRI) datasets.

</details>


### [6] [Long-Term Probabilistic Forecast of Vegetation Conditions Using Climate Attributes in the Four Corners Region](https://arxiv.org/abs/2601.16347)
*Erika McPhillips,Hyeongseong Lee,Xiangyu Xie,Kathy Baylis,Chris Funk,Mengyang Gu*

Main category: stat.AP

TL;DR: 开发了一个两阶段机器学习模型，用于提前一年预测高分辨率网格的峰值NDVI，在四角地区测试中优于现有方法


<details>
  <summary>Details</summary>
Motivation: 天气条件严重影响作物和牧场状态，进而影响全球收入和粮食安全。虽然已有短期NDVI预测方法，但缺乏长期（如提前一年）的植被条件预测方法

Method: 两阶段机器学习模型：第一阶段识别关键气候属性（降水和最大蒸汽压赤字），开发广义并行高斯过程捕捉气候属性与NDVI关系；第二阶段使用至少一年前的历史数据预测这些气候属性，作为输入来预测每个空间网格的峰值NDVI

Result: 开发的开源工具在整体NDVI和基于网格的NDVI一年预测中都优于替代方法，为农民和牧场主提供可提前一年制定行动计划的信息

Conclusion: 成功开发了能够提前一年预测植被条件的机器学习模型，填补了长期植被预测的空白，为农业决策提供了有价值的工具

Abstract: Weather conditions can drastically alter the state of crops and rangelands, and in turn, impact the incomes and food security of individuals worldwide. Satellite-based remote sensing offers an effective way to monitor vegetation and climate variables on regional and global scales. The annual peak Normalized Difference Vegetation Index (NDVI), derived from satellite observations, is closely associated with crop development, rangeland biomass, and vegetation growth. Although various machine learning methods have been developed to forecast NDVI over short time ranges, such as one-month-ahead predictions, long-term forecasting approaches, such as one-year-ahead predictions of vegetation conditions, are not yet available. To fill this gap, we develop a two-phase machine learning model to forecast the one-year-ahead peak NDVI over high-resolution grids, using the Four Corners region of the Southwestern United States as a testbed. In phase one, we identify informative climate attributes, including precipitation and maximum vapor pressure deficit, and develop the generalized parallel Gaussian process that captures the relationship between climate attributes and NDVI. In phase two, we forecast these climate attributes using historical data at least one year before the NDVI prediction month, which then serve as inputs to forecast the peak NDVI at each spatial grid. We developed open-source tools that outperform alternative methods for both gross NDVI and grid-based NDVI one-year forecasts, providing information that can help farmers and ranchers make actionable plans a year in advance.

</details>


### [7] [Spillovers and Co-movements in Multivariate Volatility: A Vector Multiplicative Error Model](https://arxiv.org/abs/2601.16837)
*Edoardo Otranto,Luca Scaffidi Domianello*

Main category: stat.AP

TL;DR: 提出了一种新的多元波动率模型，在MEM框架下同时捕捉溢出效应和共同运动效应，通过潜在成分和模型聚类降低计算复杂度，在29个道琼斯指数资产上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 金融市场日益一体化导致资产间波动率存在溢出效应和共同运动，需要开发能够同时捕捉这两种效应的多元波动率模型，同时保持高维情况下的计算可行性。

Method: 在乘性误差模型(MEM)框架下引入新的波动率向量模型，通过特定参数化包含溢出效应和共同运动效应的潜在成分，采用基于模型的聚类方法减少未知参数数量。

Result: 在29个道琼斯工业平均指数资产的实证应用中，模型有效揭示了波动率溢出和共享市场动态，与替代向量MEM模型相比，在多个评估标准上表现优越或至少相当。

Conclusion: 提出的新MEM模型能够有效建模多元波动率的溢出效应和共同运动，计算可行且在高维情况下表现良好，为金融市场波动率分析提供了有力工具。

Abstract: Recent developments in financial time series focus on modeling volatility across multiple assets or indices in a multivariate framework, accounting for potential interactions such as spillover effects. Furthermore, the increasing integration of global financial markets provides a similar dynamics (referred to as comovement). In this context, we introduce a novel model for volatility vectors within the Multiplicative Error Model (MEM) class. This framework accommodates both spillover and co-movement effects through a distinct latent component. By adopting a specific parameterization, the model remains computationally feasible even for high-dimensional volatility vectors. To reduce the number of unknown coefficients, we propose a simple model-based clustering procedure. We illustrate the effectiveness of the proposed approach through an empirical application to 29 assets of the Dow Jones Industrial Average index, providing insight into volatility spillovers and shared market dynamics. Comparative analysis against alternative vector MEMs, including a fully parameterized version of the proposed model, demonstrates its superior or at least comparable performance across multiple evaluation criteria.

</details>


### [8] [Identifying heat-related diagnoses in emergency department visits among adults in Chicago: a heat-wide association study](https://arxiv.org/abs/2601.16932)
*Hyojung Jang,Peter M. Graffy,Benjamin W. Barrett,Daniel E. Horton,Jennifer L. Chan,Abel N. Kho*

Main category: stat.AP

TL;DR: 该研究通过热广泛关联研究，在芝加哥识别了与极端高温相关的急性护理诊断，发现热病、容量不足、低血压、水肿、急性肾衰竭和多种损伤的就诊量在高温当天增加。


<details>
  <summary>Details</summary>
Motivation: 以往研究依赖有限的诊断和诊断类别，会遗漏或错误分类热相关疾病。需要更全面地了解极端高温对健康的影响。

Method: 采用两阶段分析方法：首先使用准泊松回归筛选1803个诊断代码的热相关风险，然后在时间分层病例交叉设计中使用分布式滞后非线性模型，精炼热相关诊断列表并估计极端高温与参考温度下急性护理就诊的当日和短期累积比值比。

Result: 观察到热病、容量不足、低血压、水肿、急性肾衰竭和多种损伤的就诊量在高温当天增加。通过分析急性护理服务的完整诊断谱，全面描述了热相关发病率。

Conclusion: 该研究通过分析急性护理服务的完整诊断谱，全面描述了热相关发病率，强化并推进了现有文献，为公共卫生干预提供了更全面的证据基础。

Abstract: Extreme heat is an escalating public health concern. Although prior studies have examined heat-health associations, their reliance on restricted diagnoses and diagnostic categories misses or misclassifies heat-related illness. We conducted a heat-wide association study to identify acute-care diagnoses associated with extreme heat in Chicago, Illinois. Using 916,904 acute-care visits -- including emergency department and urgent care encounters -- among 372,140 adults across five healthcare systems from 2011-2023, we applied a two-stage analytic approach: quasi-Poisson regression to screen 1,803 diagnosis codes for heat-related risks, followed by distributed lag non-linear models in a time-stratified case-crossover design to refine the list of heat-related diagnoses and estimate same-day and short-term cumulative odds ratios of acute-care visits during extreme heat versus reference temperature. We observed same-day increases in visits for heat illness, volume depletion, hypotension, edema, acute kidney failure, and multiple injuries. By analyzing the full diagnostic spectrum of acute-care services, this study comprehensively characterizes heat-associated morbidity, reinforcing and advancing existing literature.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [9] [When Agents Fail to Act: A Diagnostic Framework for Tool Invocation Reliability in Multi-Agent LLM Systems](https://arxiv.org/abs/2601.16280)
*Donghao Huang,Gauri Malwe,Zhaoxia Wang*

Main category: cs.AI

TL;DR: 论文提出了一个基于大数据分析的诊断框架，用于评估智能代理系统的工具使用可靠性，通过1980个测试实例发现Qwen2.5:32B模型表现与GPT-4相当，为中小企业部署提供了实用指导。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的多智能体系统在企业自动化中应用广泛，但缺乏系统性的工具使用可靠性评估方法，特别是在隐私敏感的中小企业部署场景下，需要建立可靠的评估框架。

Method: 开发了一个包含12类错误分类的诊断框架，涵盖工具初始化、参数处理、执行和结果解释等环节。系统评估了1980个确定性测试实例，涵盖开源模型（Qwen2.5系列、Functionary）和专有模型（GPT-4、Claude 3.5/3.7），并在不同边缘硬件配置上进行测试。

Result: 研究发现工具初始化失败是较小模型的主要瓶颈，Qwen2.5:32B模型表现完美，与GPT-4相当。中型模型（Qwen2.5:14B）在商用硬件上实现了96.6%的成功率和7.3秒延迟，为资源受限组织提供了成本效益平衡。

Conclusion: 该工作为工具增强的多智能体AI系统建立了系统可靠性评估的基础设施，为中小企业部署提供了实用的准确性与效率权衡方案，推动了智能代理系统在资源受限环境中的应用。

Abstract: Multi-agent systems powered by large language models (LLMs) are transforming enterprise automation, yet systematic evaluation methodologies for assessing tool-use reliability remain underdeveloped. We introduce a comprehensive diagnostic framework that leverages big data analytics to evaluate procedural reliability in intelligent agent systems, addressing critical needs for SME-centric deployment in privacy-sensitive environments. Our approach features a 12-category error taxonomy capturing failure modes across tool initialization, parameter handling, execution, and result interpretation. Through systematic evaluation of 1,980 deterministic test instances spanning both open-weight models (Qwen2.5 series, Functionary) and proprietary alternatives (GPT-4, Claude 3.5/3.7) across diverse edge hardware configurations, we identify actionable reliability thresholds for production deployment. Our analysis reveals that procedural reliability, particularly tool initialization failures, constitutes the primary bottleneck for smaller models, while qwen2.5:32b achieves flawless performance matching GPT-4.1. The framework demonstrates that mid-sized models (qwen2.5:14b) offer practical accuracy-efficiency trade-offs on commodity hardware (96.6\% success rate, 7.3 s latency), enabling cost-effective intelligent agent deployment for resource-constrained organizations. This work establishes foundational infrastructure for systematic reliability evaluation of tool-augmented multi-agent AI systems.

</details>


### [10] [SemanticALLI: Caching Reasoning, Not Just Responses, in Agentic Systems](https://arxiv.org/abs/2601.16286)
*Varun Chillara,Dylan Kline,Christopher Alvares,Evan Wooten,Huan Yang,Shlok Khetan,Cade Bauer,Tré Guillory,Tanishka Shah,Yashodhara Dhariwal,Volodymyr Pavlov,George Popstefanov*

Main category: cs.AI

TL;DR: SemanticALLI通过将AI管道分解为分析意图解析和可视化合成两个阶段，对结构化中间表示进行缓存，显著提高缓存命中率并减少LLM调用。


<details>
  <summary>Details</summary>
Motivation: 现有代理式AI管道存在隐藏的低效问题：即使自然语言表述完全不同，系统仍会重复构建相同的中间逻辑（如指标归一化或图表框架）。传统的边界缓存将推理视为黑盒，无法解决这一问题。

Method: 提出SemanticALLI架构，将生成过程分解为分析意图解析（AIR）和可视化合成（VS）两个阶段，将结构化中间表示提升为一等可缓存工件，在代理循环中实现语义缓存。

Result: 传统整体缓存因语言变异性限制在38.7%命中率，而结构化方法使可视化合成阶段达到83.10%命中率，绕过4023次LLM调用，中位延迟仅2.66毫秒，显著减少总令牌消耗。

Conclusion: 即使用户很少重复相同表述，AI管道在稳定、结构化的检查点仍会重复执行相同逻辑，在这些位置实施缓存最为可靠，这为AI系统设计提供了实用经验。

Abstract: Agentic AI pipelines suffer from a hidden inefficiency: they frequently reconstruct identical intermediate logic, such as metric normalization or chart scaffolding, even when the user's natural language phrasing is entirely novel. Conventional boundary caching fails to capture this inefficiency because it treats inference as a monolithic black box.
  We introduce SemanticALLI, a pipeline-aware architecture within Alli (PMG's marketing intelligence platform), designed to operationalize redundant reasoning. By decomposing generation into Analytic Intent Resolution (AIR) and Visualization Synthesis (VS), SemanticALLI elevates structured intermediate representations (IRs) to first-class, cacheable artifacts.
  The impact of caching within the agentic loop is substantial. In our evaluation, baseline monolithic caching caps at a 38.7% hit rate due to linguistic variance. In contrast, our structured approach allows for an additional stage, the Visualization Synthesis stage, to achieve an 83.10% hit rate, bypassing 4,023 LLM calls with a median latency of just 2.66 ms. This internal reuse reduces total token consumption, offering a practical lesson for AI system design: even when users rarely repeat themselves, the pipeline often does, at stable, structured checkpoints where caching is most reliable.

</details>


### [11] [DSGym: A Holistic Framework for Evaluating and Training Data Science Agents](https://arxiv.org/abs/2601.16344)
*Fan Nie,Junlin Wang,Harper Hua,Federico Bianchi,Yongchan Kwon,Zhenting Qi,Owen Queen,Shang Zhu,James Zou*

Main category: cs.AI

TL;DR: DSGym是一个用于评估和训练数据科学代理的标准化框架，解决了现有基准测试的碎片化、任务覆盖窄和数据基础不足的问题，通过模块化架构支持任务扩展和代理训练。


<details>
  <summary>Details</summary>
Motivation: 现有数据科学基准测试存在三个主要问题：1) 碎片化的评估接口导致跨基准比较困难；2) 任务覆盖范围狭窄；3) 缺乏严格的数据基础（许多任务无需实际数据即可解决）。这些限制阻碍了对数据科学代理能力的准确评估。

Method: 开发了DSGym框架，包含：1) 模块化架构，便于添加任务、代理框架和工具；2) DSGym-Tasks任务套件，标准化现有基准并过滤低质量和可捷径解决的任务；3) 新增DSBio（生物信息学任务）和DSPredict（预测任务）；4) 支持通过执行验证的数据合成管道进行代理训练。

Result: 使用DSGym构建了2,000个示例的训练集，训练了一个4B参数的模型，该模型在标准化分析基准测试中优于GPT-4o。DSGym能够对代理在真实科学背景下规划、实施和验证数据分析的能力进行严格的端到端测量。

Conclusion: DSGym提供了一个可扩展的测试平台，解决了现有数据科学基准测试的局限性，通过标准化评估框架和训练能力，推动了数据科学代理的发展，使其能够在真实科学背景下进行完整的数据分析流程。

Abstract: Data science agents promise to accelerate discovery and insight-generation by turning data into executable analyses and findings. Yet existing data science benchmarks fall short due to fragmented evaluation interfaces that make cross-benchmark comparison difficult, narrow task coverage and a lack of rigorous data grounding. In particular, we show that a substantial portion of tasks in current benchmarks can be solved without using the actual data. To address these limitations, we introduce DSGym, a standardized framework for evaluating and training data science agents in self-contained execution environments. Unlike static benchmarks, DSGym provides a modular architecture that makes it easy to add tasks, agent scaffolds, and tools, positioning it as a live, extensible testbed. We curate DSGym-Tasks, a holistic task suite that standardizes and refines existing benchmarks via quality and shortcut solvability filtering. We further expand coverage with (1) DSBio: expert-derived bioinformatics tasks grounded in literature and (2) DSPredict: challenging prediction tasks spanning domains such as computer vision, molecular prediction, and single-cell perturbation. Beyond evaluation, DSGym enables agent training via execution-verified data synthesis pipeline. As a case study, we build a 2,000-example training set and trained a 4B model in DSGym that outperforms GPT-4o on standardized analysis benchmarks. Overall, DSGym enables rigorous end-to-end measurement of whether agents can plan, implement, and validate data analyses in realistic scientific context.

</details>


### [12] [Doc2AHP: Inferring Structured Multi-Criteria Decision Models via Semantic Trees with LLMs](https://arxiv.org/abs/2601.16479)
*Hongjia Wu,Shuai Zhou,Hongxin Zhang,Wei Chen*

Main category: cs.AI

TL;DR: Doc2AHP：一种基于AHP原则的结构化推理框架，利用LLM的泛化能力与决策理论的严谨性相结合，无需大量标注数据或人工干预，从非结构化文档构建高质量决策模型。


<details>
  <summary>Details</summary>
Motivation: LLM在语义理解方面表现出色，但在需要严格逻辑的复杂决策任务中难以保证结构一致性和推理可靠性。传统决策理论（如AHP）虽然提供系统化理性框架，但构建过程严重依赖劳动密集型的领域专家知识，存在"专家瓶颈"问题，限制了在通用场景中的可扩展性。

Method: 提出Doc2AHP结构化推理框架：1）利用AHP的结构原则作为约束，指导LLM在非结构化文档空间中进行受限搜索，强制父节点与子节点之间的逻辑蕴含关系；2）引入多智能体加权机制与自适应一致性优化策略，确保权重分配的数字一致性。

Result: 实验结果表明，Doc2AHP不仅使非专家用户能够从零开始构建高质量决策模型，而且在逻辑完整性和下游任务准确性方面显著优于直接生成基线方法。

Conclusion: Doc2AHP成功弥合了LLM的泛化能力与决策理论严谨性之间的差距，通过结构化约束和一致性优化，实现了无需大量标注数据或人工干预的高质量决策模型构建。

Abstract: While Large Language Models (LLMs) demonstrate remarkable proficiency in semantic understanding, they often struggle to ensure structural consistency and reasoning reliability in complex decision-making tasks that demand rigorous logic. Although classical decision theories, such as the Analytic Hierarchy Process (AHP), offer systematic rational frameworks, their construction relies heavily on labor-intensive domain expertise, creating an "expert bottleneck" that hinders scalability in general scenarios. To bridge the gap between the generalization capabilities of LLMs and the rigor of decision theory, we propose Doc2AHP, a novel structured inference framework guided by AHP principles. Eliminating the need for extensive annotated data or manual intervention, our approach leverages the structural principles of AHP as constraints to direct the LLM in a constrained search within the unstructured document space, thereby enforcing the logical entailment between parent and child nodes. Furthermore, we introduce a multi-agent weighting mechanism coupled with an adaptive consistency optimization strategy to ensure the numerical consistency of weight allocation. Empirical results demonstrate that Doc2AHP not only empowers non-expert users to construct high-quality decision models from scratch but also significantly outperforms direct generative baselines in both logical completeness and downstream task accuracy.

</details>


### [13] [SycoEval-EM: Sycophancy Evaluation of Large Language Models in Simulated Clinical Encounters for Emergency Care](https://arxiv.org/abs/2601.16529)
*Dongshen Peng,Yi Wang,Carl Preiksaitis,Christian Rose*

Main category: cs.AI

TL;DR: SycoEval-EM框架通过多智能体模拟评估LLM在急诊医学中对抗患者说服的鲁棒性，发现模型容易屈服于不适当医疗请求，静态基准无法预测社交压力下的安全性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在临床决策支持中显示出潜力，但存在屈服于患者压力而提供不适当医疗的风险。需要评估LLM在对抗性患者说服情境下的鲁棒性。

Method: 引入SycoEval-EM多智能体模拟框架，在三个Choosing Wisely场景中，通过对抗性患者说服测试20个LLM在1875次医疗接触中的表现。

Result: 模型屈服率从0%到100%不等，对影像检查请求(38.8%)比阿片类药物处方(25.0%)更脆弱。模型能力与鲁棒性相关性差，所有说服策略效果相似(30.0-36.0%)。

Conclusion: 静态基准无法充分预测社交压力下的安全性，临床AI认证需要进行多轮对抗性测试以确保鲁棒性。

Abstract: Large language models (LLMs) show promise in clinical decision support yet risk acquiescing to patient pressure for inappropriate care. We introduce SycoEval-EM, a multi-agent simulation framework evaluating LLM robustness through adversarial patient persuasion in emergency medicine. Across 20 LLMs and 1,875 encounters spanning three Choosing Wisely scenarios, acquiescence rates ranged from 0-100\%. Models showed higher vulnerability to imaging requests (38.8\%) than opioid prescriptions (25.0\%), with model capability poorly predicting robustness. All persuasion tactics proved equally effective (30.0-36.0\%), indicating general susceptibility rather than tactic-specific weakness. Our findings demonstrate that static benchmarks inadequately predict safety under social pressure, necessitating multi-turn adversarial testing for clinical AI certification.

</details>


### [14] [LLM is Not All You Need: A Systematic Evaluation of ML vs. Foundation Models for text and image based Medical Classification](https://arxiv.org/abs/2601.16549)
*Meet Raval,Tejul Pandit,Dhvani Upadhyay*

Main category: cs.AI

TL;DR: 传统机器学习模型在医疗分类任务中表现最佳，LoRA微调的Gemma变体表现最差，而零样本LLM/VLM在图像任务中有竞争力


<details>
  <summary>Details</summary>
Motivation: 评估多模态视觉语言模型和大语言模型在医疗分类任务中的表现，与传统机器学习方法进行对比，为医疗AI领域提供基准参考

Method: 使用四个公开数据集（涵盖文本和图像模态，包括二元和多元分类），对比三类模型：传统ML（LR、LightGBM、ResNet-50）、基于提示的LLM/VLM（Gemini 2.5）、微调的PEFT模型（LoRA适配的Gemma3变体），采用一致的数据划分和评估指标

Result: 传统ML模型在大多数医疗分类任务中表现最佳，尤其在结构化文本数据集上；LoRA微调的Gemma变体在所有实验中表现最差；零样本LLM/VLM在文本任务上表现不佳，但在多元图像分类任务中与ResNet-50基线相当

Conclusion: 在许多医疗分类场景中，传统机器学习模型仍是最可靠的选择；基础模型并非普遍优越，参数高效微调的效果高度依赖于适配策略，本研究中最小化微调反而有害

Abstract: The combination of multimodal Vision-Language Models (VLMs) and Large Language Models (LLMs) opens up new possibilities for medical classification. This work offers a rigorous, unified benchmark by using four publicly available datasets covering text and image modalities (binary and multiclass complexity) that contrasts traditional Machine Learning (ML) with contemporary transformer-based techniques. We evaluated three model classes for each task: Classical ML (LR, LightGBM, ResNet-50), Prompt-Based LLMs/VLMs (Gemini 2.5), and Fine-Tuned PEFT Models (LoRA-adapted Gemma3 variants). All experiments used consistent data splits and aligned metrics. According to our results, traditional machine learning (ML) models set a high standard by consistently achieving the best overall performance across most medical categorization tasks. This was especially true for structured text-based datasets, where the classical models performed exceptionally well. In stark contrast, the LoRA-tuned Gemma variants consistently showed the worst performance across all text and image experiments, failing to generalize from the minimal fine-tuning provided. However, the zero-shot LLM/VLM pipelines (Gemini 2.5) had mixed results; they performed poorly on text-based tasks, but demonstrated competitive performance on the multiclass image task, matching the classical ResNet-50 baseline. These results demonstrate that in many medical categorization scenarios, established machine learning models continue to be the most reliable option. The experiment suggests that foundation models are not universally superior and that the effectiveness of Parameter-Efficient Fine-Tuning (PEFT) is highly dependent on the adaptation strategy, as minimal fine-tuning proved detrimental in this study.

</details>


### [15] [LUMINA: Long-horizon Understanding for Multi-turn Interactive Agents](https://arxiv.org/abs/2601.16649)
*Amin Rakhsha,Thomas Hehn,Pietro Mazzaglia,Fabio Valerio Massoli,Arash Behboodi,Tribhuvanesh Orekondy*

Main category: cs.AI

TL;DR: 本文提出了一种评估多轮智能体任务中不同能力（如规划、状态跟踪）相对重要性的方法，通过设计可控环境并提供精确的"先知"干预来测量各项技能对性能提升的贡献。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在孤立任务上表现良好，但在需要规划、状态跟踪和长上下文处理等多轮长视野智能体任务上仍有困难。需要理解这些底层能力对任务成功的重要性，以指导未来AI智能体和语言模型的发展。

Method: 提出先知反事实框架，通过为智能体提供完美执行特定任务（如完美规划、无错误状态跟踪）的先知干预，测量性能变化。开发了一套程序生成、游戏式任务套件，具有可调复杂度，能够在受控环境中提供精确干预，隔离各项技能的贡献。

Result: 结果显示，某些干预（如规划）在各种设置下都能持续提升性能，而其他技能的有用性则取决于环境和语言模型的特性。这揭示了多轮智能体环境的复杂性。

Conclusion: 研究为理解多轮智能体任务中不同能力的相对重要性提供了框架，揭示了环境特性和模型能力如何影响技能的有效性，为未来AI智能体和语言模型的发展提供了指导。

Abstract: Large language models can perform well on many isolated tasks, yet they continue to struggle on multi-turn, long-horizon agentic problems that require skills such as planning, state tracking, and long context processing. In this work, we aim to better understand the relative importance of advancing these underlying capabilities for success on such tasks. We develop an oracle counterfactual framework for multi-turn problems that asks: how would an agent perform if it could leverage an oracle to perfectly perform a specific task? The change in the agent's performance due to this oracle assistance allows us to measure the criticality of such oracle skill in the future advancement of AI agents. We introduce a suite of procedurally generated, game-like tasks with tunable complexity. These controlled environments allow us to provide precise oracle interventions, such as perfect planning or flawless state tracking, and make it possible to isolate the contribution of each oracle without confounding effects present in real-world benchmarks. Our results show that while some interventions (e.g., planning) consistently improve performance across settings, the usefulness of other skills is dependent on the properties of the environment and language model. Our work sheds light on the challenges of multi-turn agentic environments to guide the future efforts in the development of AI agents and language models.

</details>


### [16] [AgentsEval: Clinically Faithful Evaluation of Medical Imaging Reports via Multi-Agent Reasoning](https://arxiv.org/abs/2601.16685)
*Suzhong Fu,Jingqi Dong,Xuan Ding,Rui Sun,Yiming Yang,Shuguang Cui,Zhen Li*

Main category: cs.AI

TL;DR: AgentsEval：一个多智能体流推理框架，用于评估医学影像报告生成的临床正确性和推理保真度，通过模拟放射科医生的协作诊断工作流程提供结构化评估。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法无法捕捉放射学解释背后的结构化诊断逻辑，导致评估不可靠且临床相关性有限，需要一种能模拟临床推理过程的评估框架。

Method: 提出AgentsEval多智能体流推理框架，将评估过程分解为可解释的步骤：标准定义、证据提取、对齐和一致性评分，同时构建了包含五个医学报告数据集的多领域扰动基准。

Result: 实验结果表明，AgentsEval能够提供临床对齐、语义忠实且可解释的评估，在释义、语义和风格扰动下保持稳健，优于现有评估方法。

Conclusion: 该框架代表了向透明和临床基础的医学报告生成系统评估迈出的一步，促进大语言模型在临床实践中的可信集成。

Abstract: Evaluating the clinical correctness and reasoning fidelity of automatically generated medical imaging reports remains a critical yet unresolved challenge. Existing evaluation methods often fail to capture the structured diagnostic logic that underlies radiological interpretation, resulting in unreliable judgments and limited clinical relevance. We introduce AgentsEval, a multi-agent stream reasoning framework that emulates the collaborative diagnostic workflow of radiologists. By dividing the evaluation process into interpretable steps including criteria definition, evidence extraction, alignment, and consistency scoring, AgentsEval provides explicit reasoning traces and structured clinical feedback. We also construct a multi-domain perturbation-based benchmark covering five medical report datasets with diverse imaging modalities and controlled semantic variations. Experimental results demonstrate that AgentsEval delivers clinically aligned, semantically faithful, and interpretable evaluations that remain robust under paraphrastic, semantic, and stylistic perturbations. This framework represents a step toward transparent and clinically grounded assessment of medical report generation systems, fostering trustworthy integration of large language models into clinical practice.

</details>


### [17] [LongCat-Flash-Thinking-2601 Technical Report](https://arxiv.org/abs/2601.16725)
*Meituan LongCat Team,Anchun Gui,Bei Li,Bingyang Tao,Bole Zhou,Borun Chen,Chao Zhang,Chao Zhang,Chen Gao,Chen Zhang,Chengcheng Han,Chenhui Yang,Chuyu Zhang,Cong Chen,Cunguang Wang,Daoru Pan,Defei Bu,Dengchang Zhao,Di Xiu,Dishan Liu,Dongyu Ru,Dunwei Tu,Fan Wu,Fengcheng Yuan,Fengcun Li,Gang Xu,Guanyu Wu,Guoyuan Lin,Haibin Wang,Hansi Yang,Hao Yang,Haonan Yan,Haoxiang Ma,Haoxing Wen,Hongyan Hao,Hongyin Tang,Hongyu Zang,Hongzhi Ni,Hui Su,Jiacheng Zhang,Jiahong Zhou,Jiahuan Li,Jiaming Wang,Jian Yang,Jianfei Zhang,Jianhao Xu,Jianing Wang,Jiapeng Zhu,Jiaqi Sun,Jiarong Shi,Jiarui Zhao,Jingang Wang,Jinluan Yang,Jinrui Ding,Jinwei Xiao,Jiyuan He,Juncan Xu,Kefeng Zhang,Keheng Wang,Li Wei,Lianhui Ma,Lin Qiu,Lingbing Kong,Lingchuan Liu,Linsen Guo,Mengshen Zhu,Mengxia Shen,Mingyang Zhu,Peiguang Li,Peng Pei,Pengcheng Jia,Pengtao Zhang,Peng Zhao,Qi Gu,Qiong Huang,Qiyuan Duan,Quanchi Weng,Rongxiang Weng,Rongzhi Zhang,Rumei Li,Shanglin Lei,Shengnan An,Shijun Dai,Shuaikang Liu,Shuang Zhou,Shuo Wang,Songyuan Zhao,Tao Liang,Tianhao Hu,Tianze Chen,Wei Liu,Wei Shi,Wei Wang,Weifeng Tang,Wenjie Shi,Wenlong Zhu,Wentao Chen,Wentao Shi,Xi Su,Xiangcheng Liu,Xiandi Ma,Xiangyu Xi,Xiangyuan Liu,Xiangzhou Huang,Xiao Liu,Xiaodong Cai,Xiaolong Chen,Xiaowei Shi,Xiaoyu Li,Xin Chen,Xingchen Liu,Xuan Huang,Xuezhi Cao,Xunliang Cai,Yan Chen,Yang Bai,Yang Liu,Yang Yang,Yang Zheng,Yaoming Wang,Yaoming Zhu,Yaqi Huo,Yanyu Chen,Yaorui Shi,Yerui Sun,Yi Zhang,Yihao Chen,Yi-Kai Zhang,Yifan Lu,Yifan Zhao,Yitao Zhai,Yongjing Yin,Yongwei Zhou,Youshao Xiao,Yuchuan Dai,Yuchen Xie,Yuchen Yu,Yufei Zhang,Yuhuai Wei,Yulei Qian,Yunfan Liang,Yunke Zhao,Yuwei Jiang,Yuxin Bian,Yuxin Chen,Yuxin Liu,Yue Xu,Yueqing Sun,Zeyang Yu,Zhao Yang,Zhengsheng Huang,Zhengyu Chen,Zhijian Liu,Zhikang Xia,Zhimin Lin,Zhiyuan Yao,Zhuofan Chen,Zhuowen Han,Zijian Zhang,Ziran Li,Ziwen Wang,Ziyuan Zhuang*

Main category: cs.AI

TL;DR: LongCat-Flash-Thinking-2601是一个5600亿参数的开源MoE推理模型，在多种智能体基准测试中达到最先进性能，具有强大的工具使用泛化能力和现实环境鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 开发一个具有卓越智能体推理能力的开源模型，能够处理复杂的工具交互、多轮智能体交互，并在嘈杂的现实环境中保持鲁棒性。

Method: 采用统一的训练框架，结合领域并行专家训练与后续融合；扩展异步强化学习框架DORA用于大规模多环境训练；系统分析现实噪声模式并设计针对性训练；引入Heavy Thinking模式进行测试时扩展。

Result: 在智能体搜索、智能体工具使用和工具集成推理等基准测试中达到开源模型的最先进性能；展示了对复杂工具交互的强大泛化能力和在嘈杂现实环境中的鲁棒行为。

Conclusion: LongCat-Flash-Thinking-2601通过创新的训练框架、大规模环境训练、噪声鲁棒性设计和测试时扩展技术，实现了卓越的智能体推理能力，为现实世界应用提供了强大的开源解决方案。

Abstract: We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.

</details>


### [18] [An Efficient Insect-inspired Approach for Visual Point-goal Navigation](https://arxiv.org/abs/2601.16806)
*Lu Yihe,Barbara Webb*

Main category: cs.AI

TL;DR: 开发了一种受昆虫启发的视觉点目标导航智能体，结合了昆虫大脑中负责联想学习和路径整合的两个结构，在Habitat点目标导航任务中表现出与SOTA模型相当的性能，但计算成本低多个数量级。


<details>
  <summary>Details</summary>
Motivation: 受昆虫在发现食物位置和巢穴之间学习并优化视觉引导路径的能力启发，希望开发一种简单高效的视觉点目标导航方法，将昆虫的导航能力与Habitat点目标导航任务相结合。

Method: 结合了昆虫大脑中两个关键结构的抽象模型：一个负责联想学习，另一个负责路径整合。将Habitat点目标导航任务形式化基准与昆虫的导航能力进行类比，开发出简单的昆虫启发式智能体。

Result: 该昆虫启发式智能体在Habitat点目标导航任务中表现出与最新SOTA模型相当的性能，但计算成本降低了多个数量级。在更真实的模拟环境中测试表明，该方法对扰动具有鲁棒性。

Conclusion: 昆虫大脑结构为开发高效、低计算成本的视觉导航系统提供了有价值的灵感来源，这种生物启发方法在保持高性能的同时显著降低了计算需求，为机器人导航等领域提供了有前景的解决方案。

Abstract: In this work we develop a novel insect-inspired agent for visual point-goal navigation. This combines abstracted models of two insect brain structures that have been implicated, respectively, in associative learning and path integration. We draw an analogy between the formal benchmark of the Habitat point-goal navigation task and the ability of insects to learn and refine visually guided paths around obstacles between a discovered food location and their nest. We demonstrate that the simple insect-inspired agent exhibits performance comparable to recent SOTA models at many orders of magnitude less computational cost. Testing in a more realistic simulated environment shows the approach is robust to perturbations.

</details>


### [19] [Reasoning Promotes Robustness in Theory of Mind Tasks](https://arxiv.org/abs/2601.16853)
*Ian B. de Haan,Peter van der Putten,Max van Duijn*

Main category: cs.AI

TL;DR: 研究发现，通过强化学习训练的语言模型在心理理论任务中表现出更强的鲁棒性，但这种改进主要源于寻找正确答案的能力增强，而非新的心理理论推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在心理理论测试中表现出色，引发了对其真实能力的争议。同时，通过强化学习训练的语言模型在各种基准测试中取得了显著进步。本文旨在探究这类推理模型在心理理论任务中的行为表现。

Method: 使用新颖的机器心理学实验改编和已建立的基准测试结果，分析推理模型在心理理论任务中的行为。特别关注模型对提示变化和任务扰动的鲁棒性。

Result: 推理模型在心理理论任务中表现出对提示变化和任务扰动更强的鲁棒性。分析表明，这种改进更可能是由于寻找正确答案的能力增强，而非形成了新的心理理论推理形式。

Conclusion: 研究结果表明，评估语言模型的社会认知行为时，需要区分真正的心理理论推理能力和简单的鲁棒性改进。这对如何准确评估语言模型的社会认知能力具有重要意义。

Abstract: Large language models (LLMs) have recently shown strong performance on Theory of Mind (ToM) tests, prompting debate about the nature and true performance of the underlying capabilities. At the same time, reasoning-oriented LLMs trained via reinforcement learning with verifiable rewards (RLVR) have achieved notable improvements across a range of benchmarks. This paper examines the behavior of such reasoning models in ToM tasks, using novel adaptations of machine psychological experiments and results from established benchmarks. We observe that reasoning models consistently exhibit increased robustness to prompt variations and task perturbations. Our analysis indicates that the observed gains are more plausibly attributed to increased robustness in finding the correct solution, rather than to fundamentally new forms of ToM reasoning. We discuss the implications of this interpretation for evaluating social-cognitive behavior in LLMs.

</details>


### [20] [Mixture-of-Models: Unifying Heterogeneous Agents via N-Way Self-Evaluating Deliberation](https://arxiv.org/abs/2601.16863)
*Tims Pecerskis,Aivars Smirnovs*

Main category: cs.AI

TL;DR: NSED协议是一种运行时混合模型架构，通过动态专家代理构建复合模型，让小模型（<20B）达到或超越大模型（100B+）性能，同时提升安全性。


<details>
  <summary>Details</summary>
Motivation: 传统混合专家模型依赖静态门控网络，无法动态适应实时需求和成本约束。需要一种能够在运行时优化模型选择，让小模型通过协作达到大模型性能，同时降低硬件成本的方法。

Method: 1. 动态专家代理：将模型选择视为背包问题变体，基于实时遥测和成本约束绑定异构检查点到功能角色
2. 宏观尺度RNN：将审议形式化为循环神经网络，共识状态通过语义遗忘门循环，实现迭代优化而无需按比例增加VRAM
3. 信任N-to-N同行评审编排结构
4. 二次投票激活函数实现非线性共识
5. 反馈驱动的状态更新

Result: 1. 在AIME 2025和LiveCodeBench等挑战性基准测试中，小模型（<20B）组成的集成模型达到或超越了100B+参数大模型的性能
2. 在DarkBench安全套件测试中，同行介导的校正将谄媚分数降低到低于任何单个代理的水平，显示出内在对齐特性
3. 建立了新的硬件套利效率前沿

Conclusion: NSED协议通过动态运行时优化和同行评审机制，实现了小模型协作超越大模型性能的目标，同时提高了安全性和对齐特性，为高效模型部署提供了新范式。

Abstract: This paper introduces the N-Way Self-Evaluating Deliberation (NSED) protocol, a Runtime Mixture-of-Models (MoM) architecture that constructs emergent composite models from a plurality of distinct expert agents. Unlike traditional Mixture-of-Experts (MoE) which rely on static gating networks, NSED employs a Dynamic Expertise Broker - a runtime optimization engine that treats model selection as a variation of the Knapsack Problem, binding heterogeneous checkpoints to functional roles based on live telemetry and cost constraints. At the execution layer, we formalize deliberation as a Macro-Scale Recurrent Neural Network (RNN), where the consensus state loops back through a semantic forget gate to enable iterative refinement without proportional VRAM scaling. Key components include an orchestration fabric for trustless N-to-N peer review, a Quadratic Voting activation function for non-linear consensus, and a feedback-driven state update. Empirical validation on challenging benchmarks (AIME 2025, LiveCodeBench) demonstrates that this topology allows ensembles of small (less than 20B) consumer-grade models to match or exceed the performance of state-of-the-art 100B+ parameter models, establishing a new hardware arbitrage efficiency frontier. Furthermore, testing on the DarkBench safety suite reveals intrinsic alignment properties, with peer-mediated correction reducing sycophancy scores below that of any individual agent.

</details>


### [21] [MAGE-KT: Multi-Agent Graph-Enhanced Knowledge Tracing with Subgraph Retrieval and Asymmetric Fusion](https://arxiv.org/abs/2601.16886)
*Chi Yu,Hongyu Yuan,Zhiyi Duan*

Main category: cs.AI

TL;DR: MAGE-KT：多智能体图增强知识追踪框架，通过多视图异构图和条件子图检索解决现有方法在概念间关系建模和计算效率方面的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的知识追踪方法存在两个主要问题：1）概念间关系挖掘不足，通常仅从交互序列推断；2）全图编码计算成本高且易受噪声影响，导致注意力扩散到学生无关区域，降低概念间关系保真度。

Method: 提出MAGE-KT框架：1）构建多视图异构图，结合多智能体概念关系提取器和学生-问题交互图；2）基于目标学生历史检索紧凑高价值子图；3）使用非对称交叉注意力融合模块集成子图信息，避免注意力扩散和不相关计算。

Result: 在三个广泛使用的知识追踪数据集上实验表明，该方法在概念关系准确性方面有显著提升，在下一问题预测方面明显优于现有方法。

Conclusion: MAGE-KT通过多视图图构建和条件子图检索，有效解决了知识追踪中概念间关系建模和计算效率问题，为图增强知识追踪提供了新思路。

Abstract: Knowledge Tracing (KT) aims to model a student's learning trajectory and predict performance on the next question. A key challenge is how to better represent the relationships among students, questions, and knowledge concepts (KCs). Recently, graph-based KT paradigms have shown promise for this problem. However, existing methods have not sufficiently explored inter-concept relations, often inferred solely from interaction sequences. In addition, the scale and heterogeneity of KT graphs make full-graph encoding both computationally both costly and noise-prone, causing attention to bleed into student-irrelevant regions and degrading the fidelity of inter-KC relations. To address these issues, we propose a novel framework: Multi-Agent Graph-Enhanced Knowledge Tracing (MAGE-KT). It constructs a multi-view heterogeneous graph by combining a multi-agent KC relation extractor and a student-question interaction graph, capturing complementary semantic and behavioral signals. Conditioned on the target student's history, it retrieves compact, high-value subgraphs and integrates them using an Asymmetric Cross-attention Fusion Module to enhance prediction while avoiding attention diffusion and irrelevant computation. Experiments on three widely used KT datasets show substantial improvements in KC-relation accuracy and clear gains in next-question prediction over existing methods.

</details>


### [22] [Preventing the Collapse of Peer Review Requires Verification-First AI](https://arxiv.org/abs/2601.16909)
*Lei You,Lele Cao,Iryna Gurevych*

Main category: cs.AI

TL;DR: 论文主张AI辅助同行评审应采用"验证优先"而非"模仿评审"范式，提出"真相耦合"作为评审工具的正确目标，并分析导致评审系统向代理指标主权化转变的两种力量。


<details>
  <summary>Details</summary>
Motivation: 当前AI辅助同行评审主要模仿传统评审流程，但这种方法可能放大科学主张膨胀问题。论文旨在重新思考AI在同行评审中的角色，防止评审系统从追求科学真相转向优化代理指标。

Method: 提出"真相耦合"概念作为评审工具的核心目标，形式化分析"验证压力"和"信号收缩"两种驱动力量，建立包含高保真检查和频繁代理判断的最小模型，推导出明确的耦合定律和激励崩溃条件。

Result: 在最小模型中推导出明确的耦合定律，识别出激励崩溃条件——即使当前决策看起来可靠，理性努力也会从追求真相转向优化代理指标。这揭示了评审系统向代理主权化评估的相变过程。

Conclusion: AI应作为对抗性审计工具，生成可审计的验证工件并扩展有效验证带宽，而非作为分数预测器放大主张膨胀。工具构建者和程序主席应采取验证优先策略，确保评审系统紧密跟踪科学真相。

Abstract: This paper argues that AI-assisted peer review should be verification-first rather than review-mimicking. We propose truth-coupling, i.e. how tightly venue scores track latent scientific truth, as the right objective for review tools. We formalize two forces that drive a phase transition toward proxy-sovereign evaluation: verification pressure, when claims outpace verification capacity, and signal shrinkage, when real improvements become hard to separate from noise. In a minimal model that mixes occasional high-fidelity checks with frequent proxy judgment, we derive an explicit coupling law and an incentive-collapse condition under which rational effort shifts from truth-seeking to proxy optimization, even when current decisions still appear reliable. These results motivate actions for tool builders and program chairs: deploy AI as an adversarial auditor that generates auditable verification artifacts and expands effective verification bandwidth, rather than as a score predictor that amplifies claim inflation.

</details>


### [23] [AgentDrive: An Open Benchmark Dataset for Agentic AI Reasoning with LLM-Generated Scenarios in Autonomous Systems](https://arxiv.org/abs/2601.16964)
*Mohamed Amine Ferrag,Abderrahmane Lakas,Merouane Debbah*

Main category: cs.AI

TL;DR: AgentDrive是一个包含30万个LLM生成驾驶场景的开放基准数据集，用于训练、微调和评估自动驾驶智能体，同时包含10万个多选题的AgentDrive-MCQ基准，评估了50个领先LLM在五个推理维度的表现。


<details>
  <summary>Details</summary>
Motivation: 当前将大语言模型集成到自主系统中面临评估和训练挑战，缺乏大规模、结构化且安全关键的基准数据集，阻碍了自动驾驶智能体的发展。

Method: 通过LLM驱动的prompt-to-JSON管道生成语义丰富的仿真就绪场景规范，在七个正交轴（场景类型、驾驶员行为、环境、道路布局、目标、难度、交通密度）上因子化场景空间，并进行物理和模式约束验证、仿真推演、安全指标计算和基于规则的结果标注。

Result: 评估了50个领先LLM在AgentDrive-MCQ上的表现，结果显示专有前沿模型在上下文和政策推理方面表现最佳，而先进开源模型在结构化和物理基础推理方面正在迅速缩小差距。

Conclusion: AgentDrive为自动驾驶智能体提供了全面的基准数据集和评估框架，有助于推动LLM在自主系统中的发展，所有资源已开源发布。

Abstract: The rapid advancement of large language models (LLMs) has sparked growing interest in their integration into autonomous systems for reasoning-driven perception, planning, and decision-making. However, evaluating and training such agentic AI models remains challenging due to the lack of large-scale, structured, and safety-critical benchmarks. This paper introduces AgentDrive, an open benchmark dataset containing 300,000 LLM-generated driving scenarios designed for training, fine-tuning, and evaluating autonomous agents under diverse conditions. AgentDrive formalizes a factorized scenario space across seven orthogonal axes: scenario type, driver behavior, environment, road layout, objective, difficulty, and traffic density. An LLM-driven prompt-to-JSON pipeline generates semantically rich, simulation-ready specifications that are validated against physical and schema constraints. Each scenario undergoes simulation rollouts, surrogate safety metric computation, and rule-based outcome labeling. To complement simulation-based evaluation, we introduce AgentDrive-MCQ, a 100,000-question multiple-choice benchmark spanning five reasoning dimensions: physics, policy, hybrid, scenario, and comparative reasoning. We conduct a large-scale evaluation of fifty leading LLMs on AgentDrive-MCQ. Results show that while proprietary frontier models perform best in contextual and policy reasoning, advanced open models are rapidly closing the gap in structured and physics-grounded reasoning. We release the AgentDrive dataset, AgentDrive-MCQ benchmark, evaluation code, and related materials at https://github.com/maferrag/AgentDrive

</details>


### [24] [Spatial-Agent: Agentic Geo-spatial Reasoning with Scientific Core Concepts](https://arxiv.org/abs/2601.16965)
*Riyang Bao,Cheng Yang,Dazhou Yu,Zhexiang Tang,Gengchen Mai,Liang Zhao*

Main category: cs.AI

TL;DR: Spatial-Agent：基于空间信息科学理论的AI智能体，通过GeoFlow图将地理分析问题转化为可执行工作流，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能体在真实地理空间计算方面存在不足，主要依赖网络搜索或模式匹配，经常产生空间关系幻觉。地理空间推理对于城市分析、交通规划、灾害响应等实际应用至关重要。

Method: 基于空间信息科学基础理论，将地理分析问答形式化为概念转换问题。自然语言问题被解析为可执行工作流，表示为GeoFlow图（有向无环图），节点对应空间概念，边表示转换。通过提取空间概念、分配功能角色（有原则的顺序约束）和基于模板的转换序列组合来实现。

Result: 在MapEval-API和MapQA基准测试上的广泛实验表明，Spatial-Agent显著优于包括ReAct和Reflexion在内的现有基线方法，同时产生可解释和可执行的地理空间工作流。

Conclusion: Spatial-Agent通过将空间信息科学理论融入AI智能体设计，解决了现有LLM智能体在地理空间计算中的局限性，提供了一种可靠、可解释的地理空间推理方法。

Abstract: Geospatial reasoning is essential for real-world applications such as urban analytics, transportation planning, and disaster response. However, existing LLM-based agents often fail at genuine geospatial computation, relying instead on web search or pattern matching while hallucinating spatial relationships. We present Spatial-Agent, an AI agent grounded in foundational theories of spatial information science. Our approach formalizes geo-analytical question answering as a concept transformation problem, where natural-language questions are parsed into executable workflows represented as GeoFlow Graphs -- directed acyclic graphs with nodes corresponding to spatial concepts and edges representing transformations. Drawing on spatial information theory, Spatial-Agent extracts spatial concepts, assigns functional roles with principled ordering constraints, and composes transformation sequences through template-based generation. Extensive experiments on MapEval-API and MapQA benchmarks demonstrate that Spatial-Agent significantly outperforms existing baselines including ReAct and Reflexion, while producing interpretable and executable geospatial workflows.

</details>


### [25] [Empowering Medical Equipment Sustainability in Low-Resource Settings: An AI-Powered Diagnostic and Support Platform for Biomedical Technicians](https://arxiv.org/abs/2601.16967)
*Bernes Lorier Atabonfack,Ahmed Tahiru Issah,Mohammed Hardi Abdul Baaki,Clemence Ingabire,Tolulope Olusuyi,Maruf Adewole,Udunna C. Anazodo,Timothy X Brown*

Main category: cs.AI

TL;DR: AI平台辅助LMICs医疗设备维护，集成LLM提供实时故障诊断指导，减少设备停机时间


<details>
  <summary>Details</summary>
Motivation: 中低收入国家医疗设备因维护不足、技术专家缺乏、制造商支持有限而大量闲置，导致设备停机时间长、诊断延迟、患者护理质量下降

Method: 开发AI支持平台，集成大型语言模型和用户友好网页界面，允许技术人员输入错误代码或症状，获取分步故障排除指导；包含全球同行讨论论坛；以Philips HDI 5000超声机为概念验证

Result: 错误代码解释精度达100%，纠正措施建议准确率达80%，证明AI驱动系统支持医疗设备维护的可行性

Conclusion: AI驱动系统有潜力支持医疗设备维护，减少设备停机时间，改善资源受限环境的医疗服务质量

Abstract: In low- and middle-income countries (LMICs), a significant proportion of medical diagnostic equipment remains underutilized or non-functional due to a lack of timely maintenance, limited access to technical expertise, and minimal support from manufacturers, particularly for devices acquired through third-party vendors or donations. This challenge contributes to increased equipment downtime, delayed diagnoses, and compromised patient care. This research explores the development and validation of an AI-powered support platform designed to assist biomedical technicians in diagnosing and repairing medical devices in real-time. The system integrates a large language model (LLM) with a user-friendly web interface, enabling imaging technologists/radiographers and biomedical technicians to input error codes or device symptoms and receive accurate, step-by-step troubleshooting guidance. The platform also includes a global peer-to-peer discussion forum to support knowledge exchange and provide additional context for rare or undocumented issues. A proof of concept was developed using the Philips HDI 5000 ultrasound machine, achieving 100% precision in error code interpretation and 80% accuracy in suggesting corrective actions. This study demonstrates the feasibility and potential of AI-driven systems to support medical device maintenance, with the aim of reducing equipment downtime to improve healthcare delivery in resource-constrained environments.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [26] [A Nonlinear Target-Factor Model with Attention Mechanism for Mixed-Frequency Data](https://arxiv.org/abs/2601.16274)
*Alessio Brini,Ekaterina Seregina*

Main category: econ.EM

TL;DR: MPTE是一个用于混合频率面板数据的因子模型估计框架，使用Transformer注意力机制处理非线性信号和不同时间分辨率，在宏观经济预测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统因子模型依赖线性信号提取且需要同质采样频率，无法处理现代高维数据集中变量在不同时间分辨率下的观测问题。

Method: 提出Mixed-Panels-Transformer Encoder (MPTE)框架，利用Transformer注意力机制实现上下文感知的信号构建，通过灵活的数据依赖权重方案替代固定线性组合，扩展经典PCA以容纳时间和横截面注意力矩阵。

Result: 在线性激活函数下证明了因子和载荷估计量的一致性和渐近正态性，框架包含Target PCA作为特例并通过跨辅助数据集的迁移学习提供效率增益。在非线性环境中表现优越，在13个宏观经济预测目标上使用FRED-MD和FRED-QD数据库的48个月度和季度序列，相比基准方法取得竞争性表现。

Conclusion: MPTE为混合频率面板数据提供了有效的因子模型估计框架，能够处理非线性信号和不同时间分辨率，在理论和实证上都表现出色，注意力模式分析有助于理解变量重要性和时间依赖性。

Abstract: We propose Mixed-Panels-Transformer Encoder (MPTE), a novel framework for estimating factor models in panel datasets with mixed frequencies and nonlinear signals. Traditional factor models rely on linear signal extraction and require homogeneous sampling frequencies, limiting their applicability to modern high-dimensional datasets where variables are observed at different temporal resolutions. Our approach leverages Transformer-style attention mechanisms to enable context-aware signal construction through flexible, data-dependent weighting schemes that replace fixed linear combinations with adaptive reweighting based on similarity and relevance. We extend classical principal component analysis (PCA) to accommodate general temporal and cross-sectional attention matrices, allowing the model to learn how to aggregate information across frequencies without manual alignment or pre-specified weights. For linear activation functions, we establish consistency and asymptotic normality of factor and loading estimators, showing that our framework nests Target PCA as a special case while providing efficiency gains through transfer learning across auxiliary datasets. The nonlinear extension uses a Transformer architecture to capture complex hierarchical interactions while preserving the theoretical foundations. In simulations, MPTE demonstrates superior performance in nonlinear environments, and in an empirical application to 13 macroeconomic forecasting targets using a selected set of 48 monthly and quarterly series from the FRED-MD and FRED-QD databases, our method achieves competitive performance against established benchmarks. We further analyze attention patterns and systematically ablate model components to assess variable importance and temporal dependence. The resulting patterns highlight which indicators and horizons are most influential for forecasting.

</details>


### [27] [Is the diurnal pattern sufficient to explain intraday variation in volatility? A nonparametric assessment](https://arxiv.org/abs/2601.16613)
*Kim Christensen,Ulrich Hounyo,Mark Podolskij*

Main category: econ.EM

TL;DR: 提出一种非参数方法检验日内波动的时间变化是否仅由确定性周期性日间模式引起，基于高频数据构建去季节化后的同方差检验统计量，并开发了引导方法处理小规模无限活动跳跃的影响。


<details>
  <summary>Details</summary>
Motivation: 需要检验日内波动的时间变化是否完全由确定性周期性日间模式解释，还是存在其他异方差来源。这对于理解金融市场波动结构、风险管理和资产定价具有重要意义。

Method: 基于高频数据构建去季节化资产收益率，扩展预平均双幂变差到一般伊藤半鞅框架，通过截断处理跳跃，构建t统计量检验同方差假设，并提出新的引导方法处理小规模无限活动跳跃的影响。

Result: 理论证明统计量在存在随机波动时发散，否则服从标准正态分布；蒙特卡洛模拟显示用无模型估计量替代真实日间因子不影响有限样本表现；实证应用发现日间模式解释相当显著的日内波动比例，但数据中仍存在重要的异方差来源。

Conclusion: 提出的非参数检验方法能够有效检验日内波动是否仅由日间模式引起，虽然日间模式解释显著比例的波动变化，但金融市场数据中仍存在未被日间模式解释的异方差来源，需要更全面的波动建模。

Abstract: In this paper, we propose a nonparametric way to test the hypothesis that time-variation in intraday volatility is caused solely by a deterministic and recurrent diurnal pattern. We assume that noisy high-frequency data from a discretely sampled jump-diffusion process are available. The test is then based on asset returns, which are deflated by the seasonal component and therefore homoskedastic under the null. To construct our test statistic, we extend the concept of pre-averaged bipower variation to a general Itô semimartingale setting via a truncation device. We prove a central limit theorem for this statistic and construct a positive semi-definite estimator of the asymptotic covariance matrix. The $t$-statistic (after pre-averaging and jump-truncation) diverges in the presence of stochastic volatility and has a standard normal distribution otherwise. We show that replacing the true diurnal factor with a model-free jump- and noise-robust estimator does not affect the asymptotic theory. A Monte Carlo simulation also shows this substitution has no discernable impact in finite samples. The test is, however, distorted by small infinite-activity price jumps. To improve inference, we propose a new bootstrap approach, which leads to almost correctly sized tests of the null hypothesis. We apply the developed framework to a large cross-section of equity high-frequency data and find that the diurnal pattern accounts for a rather significant fraction of intraday variation in volatility, but important sources of heteroskedasticity remain present in the data.

</details>


### [28] [Inference from high-frequency data: A subsampling approach](https://arxiv.org/abs/2601.16668)
*Kim Christensen,Mark Podolskij,Nopporn Thamrongrat,Bezirgen Veliyev*

Main category: econ.EM

TL;DR: 提出基于子抽样的方法估计高频波动率估计中的渐近协方差矩阵，该方法适用于无摩擦市场和含微观结构噪声的情形，具有正半定、易实现和稳健性等优点。


<details>
  <summary>Details</summary>
Motivation: 在高频金融数据中估计资产收益波动率时，需要评估估计量的抽样误差，这需要估计出现在中心极限定理中的渐近（条件）协方差矩阵。现有方法可能复杂或对模型假设敏感。

Method: 提出基于子抽样的协方差矩阵估计方法：通过在高频数据的局部片段上计算原始统计量的缩放副本，研究这些副本的抽样变异。该方法不依赖额外的估计器，自动适应问题结构，对噪声过程误设具有稳健性。

Result: 证明了估计量在无摩擦市场和含加性微观结构噪声模型中的一致性，推导了收敛速率，确定了调优参数（如子抽样数量）的最优速率。蒙特卡洛研究验证了有限样本性质，实证应用展示了在金融市场波动率推断中的可行性。

Conclusion: 子抽样方法为高频波动率估计中的抽样误差评估提供了一种简单、稳健且易于实现的解决方案，有助于在金融市场中进行可行的波动率推断。

Abstract: In this paper, we show how to estimate the asymptotic (conditional) covariance matrix, which appears in central limit theorems in high-frequency estimation of asset return volatility. We provide a recipe for the estimation of this matrix by subsampling; an approach that computes rescaled copies of the original statistic based on local stretches of high-frequency data, and then it studies the sampling variation of these. We show that our estimator is consistent both in frictionless markets and models with additive microstructure noise. We derive a rate of convergence for it and are also able to determine an optimal rate for its tuning parameters (e.g., the number of subsamples). Subsampling does not require an extra set of estimators to do inference, which renders it trivial to implement. As a variance-covariance matrix estimator, it has the attractive feature that it is positive semi-definite by construction. Moreover, the subsampler is to some extent automatic, as it does not exploit explicit knowledge about the structure of the asymptotic covariance. It therefore tends to adapt to the problem at hand and be robust against misspecification of the noise process. As such, this paper facilitates assessment of the sampling errors inherent in high-frequency estimation of volatility. We highlight the finite sample properties of the subsampler in a Monte Carlo study, while some initial empirical work demonstrates its use to draw feasible inference about volatility in financial markets.

</details>


### [29] [Distributional Instruments: Identification and Estimation with Quantile Least Squares](https://arxiv.org/abs/2601.16865)
*Rowan Cherodian,Guy Tchuente*

Main category: econ.EM

TL;DR: 提出Q-LS方法，利用工具变量的分布相关性（而非均值相关性）来识别因果效应，解决弱工具变量问题


<details>
  <summary>Details</summary>
Motivation: 传统工具变量方法依赖工具变量对处理变量均值的强影响，但在政策改革等场景中，工具变量可能只改变处理变量的分布而不显著影响其均值，导致弱工具变量问题

Method: 提出分布相关性概念，在三角模型框架下证明分布相关性足以非参数识别平均结构效应；提出分位数最小二乘法（Q-LS），通过聚合X|Z的条件分位数构建最优均方预测器，作为线性IV估计中的工具变量

Result: 理论证明Q-LS的一致性和渐近正态性，标准2SLS方差公式仍然有效；蒙特卡洛模拟显示Q-LS在弱工具变量情况下能提供无偏估计和正确检验水平；在健康与退休研究数据中，Q-LS利用Medicare Part D引起的自付风险分布变化，更精确地估计其对抑郁的影响

Conclusion: 分布相关性为工具变量识别提供了新视角，Q-LS方法能有效利用工具变量的分布信息，在传统均值相关性较弱的情况下仍能可靠估计因果效应

Abstract: We study instrumental-variable designs where policy reforms strongly shift the distribution of an endogenous variable but only weakly move its mean. We formalize this by introducing distributional relevance: instruments may be purely distributional. Within a triangular model, distributional relevance suffices for nonparametric identification of average structural effects via a control function. We then propose Quantile Least Squares (Q-LS), which aggregates conditional quantiles of X given Z into an optimal mean-square predictor and uses this projection as an instrument in a linear IV estimator. We establish consistency, asymptotic normality, and the validity of standard 2SLS variance formulas, and we discuss regularization across quantiles. Monte Carlo designs show that Q-LS delivers well-centered estimates and near-correct size when mean-based 2SLS suffers from weak instruments. In Health and Retirement Study data, Q-LS exploits Medicare Part D-induced distributional shifts in out-of-pocket risk to sharpen estimates of its effects on depression.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [30] [White-Box Sensitivity Auditing with Steering Vectors](https://arxiv.org/abs/2601.16398)
*Hannah Cyberey,Yangfeng Ji,David Evans*

Main category: cs.CY

TL;DR: 提出基于激活导向的白盒敏感度审计框架，用于更严格评估LLM内部机制，相比黑盒评估能更有效检测偏见


<details>
  <summary>Details</summary>
Motivation: 当前LLM审计主要依赖黑盒评估，仅通过输入输出测试模型行为，这些方法局限于输入空间的测试，且许多社会相关属性（如性别偏见）抽象难以仅通过文本输入衡量

Method: 提出白盒敏感度审计框架，利用激活导向通过模型内部进行更严格评估，通过操纵与任务相关的关键概念进行内部敏感度测试

Result: 在四个模拟高风险LLM决策任务中应用该方法进行偏见审计，即使在标准黑盒评估显示很少或无偏见的情况下，该方法始终显示模型预测对受保护属性的显著依赖

Conclusion: 白盒敏感度审计框架能更有效地揭示LLM中的偏见问题，为算法审计提供了更强大的工具，代码已开源

Abstract: Algorithmic audits are essential tools for examining systems for properties required by regulators or desired by operators. Current audits of large language models (LLMs) primarily rely on black-box evaluations that assess model behavior only through input-output testing. These methods are limited to tests constructed in the input space, often generated by heuristics. In addition, many socially relevant model properties (e.g., gender bias) are abstract and difficult to measure through text-based inputs alone. To address these limitations, we propose a white-box sensitivity auditing framework for LLMs that leverages activation steering to conduct more rigorous assessments through model internals. Our auditing method conducts internal sensitivity tests by manipulating key concepts relevant to the model's intended function for the task. We demonstrate its application to bias audits in four simulated high-stakes LLM decision tasks. Our method consistently reveals substantial dependence on protected attributes in model predictions, even in settings where standard black-box evaluations suggest little or no bias. Our code is openly available at https://github.com/hannahxchen/llm-steering-audit

</details>


### [31] [Competing Visions of Ethical AI: A Case Study of OpenAI](https://arxiv.org/abs/2601.16513)
*Melissa Wilfley,Mengting Ai,Madelyn Rose Sanfilippo*

Main category: cs.CY

TL;DR: 该研究分析了OpenAI公开文档中的AI伦理话语，发现其过度强调"安全"和"风险"话语，而缺乏学术和倡导伦理框架的应用，可能存在"伦理洗白"现象。


<details>
  <summary>Details</summary>
Motivation: 研究动机是分析OpenAI如何随时间推移在其公开话语中利用"伦理"、"安全"、"对齐"等相关概念，以及这种话语在实践中如何反映其伦理框架。

Method: 方法包括：1）构建结构化语料库，区分面向大众和学术受众的沟通；2）定性内容分析结合归纳和演绎编码；3）通过NLP进行定量计算内容分析，建模主题并量化修辞随时间变化；4）可视化展示聚合结果。

Result: 结果显示：OpenAI的公开沟通和文档中，安全和风险话语占据主导地位，而没有应用学术和倡导伦理框架或词汇。

Conclusion: 结论指出这对治理具有启示意义，并讨论了行业中的"伦理洗白"实践，即企业可能通过强调安全话语来规避更广泛的伦理责任。

Abstract: Introduction. AI Ethics is framed distinctly across actors and stakeholder groups. We report results from a case study of OpenAI analysing ethical AI discourse. Method. Research addressed: How has OpenAI's public discourse leveraged 'ethics', 'safety', 'alignment' and adjacent related concepts over time, and what does discourse signal about framing in practice? A structured corpus, differentiating between communication for a general audience and communication with an academic audience, was assembled from public documentation. Analysis. Qualitative content analysis of ethical themes combined inductively derived and deductively applied codes. Quantitative analysis leveraged computational content analysis methods via NLP to model topics and quantify changes in rhetoric over time. Visualizations report aggregate results. For reproducible results, we have released our code at https://github.com/famous-blue-raincoat/AI_Ethics_Discourse. Results. Results indicate that safety and risk discourse dominate OpenAI's public communication and documentation, without applying academic and advocacy ethics frameworks or vocabularies. Conclusions. Implications for governance are presented, along with discussion of ethics-washing practices in industry.

</details>


### [32] [Nishpaksh: TEC Standard-Compliant Framework for Fairness Auditing and Certification of AI Models](https://arxiv.org/abs/2601.16926)
*Shashank Prakash,Ranjitha Prasad,Avinash Agarwal*

Main category: cs.CY

TL;DR: Nishpaksh是一个本土化的AI公平性评估工具，专门针对印度电信工程中心的AI系统评估标准，通过集成风险量化、阈值确定和公平性评估，为电信和6G应用提供符合监管要求的标准化公平性评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型在电信和6G等高风险决策系统中应用日益广泛，但现有公平性评估工具（如IBM AI Fairness 360和Microsoft Fairlearn）缺乏对地区特定监管要求和国家优先事项的考虑，特别是在印度这样的监管环境下。

Method: 开发了Nishpaksh工具，该工具基于电信工程中心标准，集成了基于调查的风险量化、上下文阈值确定和定量公平性评估，采用向量化计算、响应式状态管理和认证就绪报告，构建统一的基于Web的仪表板。

Result: 在COMPAS数据集上的实验验证表明，Nishpaksh能够有效识别属性特定的偏见，并生成符合TEC框架的标准化公平性分数，填补了研究导向的公平性方法与监管AI治理之间的差距。

Conclusion: Nishpaksh工具为印度关键基础设施（如电信）中的负责任和可审计AI部署提供了重要支持，解决了标准化后的实施需求，将研究导向的公平性方法与监管AI治理有效连接起来。

Abstract: The growing reliance on Artificial Intelligence (AI) models in high-stakes decision-making systems, particularly within emerging telecom and 6G applications, underscores the urgent need for transparent and standardized fairness assessment frameworks. While global toolkits such as IBM AI Fairness 360 and Microsoft Fairlearn have advanced bias detection, they often lack alignment with region-specific regulatory requirements and national priorities. To address this gap, we propose Nishpaksh, an indigenous fairness evaluation tool that operationalizes the Telecommunication Engineering Centre (TEC) Standard for the Evaluation and Rating of Artificial Intelligence Systems. Nishpaksh integrates survey-based risk quantification, contextual threshold determination, and quantitative fairness evaluation into a unified, web-based dashboard. The tool employs vectorized computation, reactive state management, and certification-ready reporting to enable reproducible, audit-grade assessments, thereby addressing a critical post-standardization implementation need. Experimental validation on the COMPAS dataset demonstrates Nishpaksh's effectiveness in identifying attribute-specific bias and generating standardized fairness scores compliant with the TEC framework. The system bridges the gap between research-oriented fairness methodologies and regulatory AI governance in India, marking a significant step toward responsible and auditable AI deployment within critical infrastructure like telecommunications.

</details>


### [33] [In Quest of an Extensible Multi-Level Harm Taxonomy for Adversarial AI: Heart of Security, Ethical Risk Scoring and Resilience Analytics](https://arxiv.org/abs/2601.16930)
*Javed I. Khan,Sharmila Rahman Prithula*

Main category: cs.CY

TL;DR: 该论文提出了一个基于伦理理论的结构化、可扩展的危害分类法，包含66+种危害类型，分为人类和非人类两大领域，11个主要类别，旨在将危害从修辞概念转变为可操作的分析对象。


<details>
  <summary>Details</summary>
Motivation: 当前关于危害的讨论依赖于模糊、未明确定义的概念，使得细致、结构化和定性的评估几乎不可能。在网络安全、伦理、风险分析和对抗性AI等领域，缺乏系统化或公认的危害列表，概念本身也缺乏精确性，无法进行严肃分析。

Method: 引入基于当代伦理理论集合的结构化、可扩展的危害分类法，将危害明确化、可枚举化、可分析化。框架识别66+种不同的危害类型，系统组织为人类和非人类两大领域，11个主要类别，每个类别明确对应11种主导伦理理论。同时引入理论感知的受害实体分类法，并形式化规范危害属性（如可逆性和持续时间）。

Result: 提出了一个稳定的上层结构危害分类框架，将危害从修辞占位符转变为可操作的分析对象，使危害变得明确、可枚举和分析可处理。该框架支持扩展性设计，同时保持上层结构的稳定性。

Conclusion: 该贡献使危害成为可操作的分析对象，能够支持严格的伦理推理和AI系统及其他社会技术领域的长期安全评估，在这些领域中危害是一阶关注点。

Abstract: Harm is invoked everywhere from cybersecurity, ethics, risk analysis, to adversarial AI, yet there exists no systematic or agreed upon list of harms, and the concept itself is rarely defined with the precision required for serious analysis. Current discourse relies on vague, under specified notions of harm, rendering nuanced, structured, and qualitative assessment effectively impossible. This paper challenges that gap directly. We introduce a structured and expandable taxonomy of harms, grounded in an ensemble of contemporary ethical theories, that makes harm explicit, enumerable, and analytically tractable. The proposed framework identifies 66+ distinct harm types, systematically organized into two overarching domains human and nonhuman, and eleven major categories, each explicitly aligned with eleven dominant ethical theories. While extensible by design, the upper levels are intentionally stable. Beyond classification, we introduce a theory-aware taxonomy of victim entities and formalize normative harm attributes, including reversibility and duration that materially alter ethical severity. Together, these contributions transform harm from a rhetorical placeholder into an operational object of analysis, enabling rigorous ethical reasoning and long term safety evaluation of AI systems and other sociotechnical domains where harm is a first order concern.

</details>

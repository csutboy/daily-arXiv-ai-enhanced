<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 53]
- [econ.EM](#econ.EM) [Total: 7]
- [cs.SI](#cs.SI) [Total: 3]
- [stat.AP](#stat.AP) [Total: 4]
- [cs.CY](#cs.CY) [Total: 13]
- [cs.ET](#cs.ET) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [MHub.ai: A Simple, Standardized, and Reproducible Platform for AI Models in Medical Imaging](https://arxiv.org/abs/2601.10154)
*Leonard Nürnberg,Dennis Bontempi,Suraj Pai,Curtis Lisle,Steve Pieper,Ron Kikinis,Sil van de Leemput,Rahul Soni,Gowtham Murugesan,Cosmin Ciausu,Miriam Groeneveld,Felix J. Dorfner,Jue Jiang,Aneesh Rangnekar,Harini Veeraraghavan,Joeran S. Bosma,Keno Bressem,Raymond Mak,Andrey Fedorov,Hugo JWL Aerts*

Main category: cs.AI

TL;DR: MHub.ai是一个开源容器化平台，旨在标准化医学影像AI模型的访问，解决实现多样性、文档不一致和可重复性问题，促进模型的可及性和可重复性。


<details>
  <summary>Details</summary>
Motivation: 医学影像AI研究受限于多种实现架构、不一致的文档和可重复性问题，阻碍了研究和临床应用。

Method: 开发开源容器化平台，将同行评审的模型打包为标准容器，支持DICOM格式处理，提供统一API，嵌入结构化元数据，并附带公开参考数据。

Result: 平台包含最先进的分割、预测和特征提取模型，通过肺分割模型的比较评估展示了临床实用性，并公开了分割结果和评估指标。

Conclusion: MHub.ai通过简化模型使用，支持并行基准测试，降低临床转化门槛，增强医学影像AI的透明度和可重复性。

Abstract: Artificial intelligence (AI) has the potential to transform medical imaging by automating image analysis and accelerating clinical research. However, research and clinical use are limited by the wide variety of AI implementations and architectures, inconsistent documentation, and reproducibility issues. Here, we introduce MHub.ai, an open-source, container-based platform that standardizes access to AI models with minimal configuration, promoting accessibility and reproducibility in medical imaging. MHub.ai packages models from peer-reviewed publications into standardized containers that support direct processing of DICOM and other formats, provide a unified application interface, and embed structured metadata. Each model is accompanied by publicly available reference data that can be used to confirm model operation. MHub.ai includes an initial set of state-of-the-art segmentation, prediction, and feature extraction models for different modalities. The modular framework enables adaptation of any model and supports community contributions. We demonstrate the utility of the platform in a clinical use case through comparative evaluation of lung segmentation models. To further strengthen transparency and reproducibility, we publicly release the generated segmentations and evaluation metrics and provide interactive dashboards that allow readers to inspect individual cases and reproduce or extend our analysis. By simplifying model use, MHub.ai enables side-by-side benchmarking with identical execution commands and standardized outputs, and lowers the barrier to clinical translation.

</details>


### [2] [AI Survival Stories: a Taxonomic Analysis of AI Existential Risk](https://arxiv.org/abs/2601.09765)
*Herman Cappelen,Simon Goldstein,John Hawthorne*

Main category: cs.AI

TL;DR: 论文提出了一个分析AI存在性风险的通用框架，基于两个前提构建了人类生存故事的分类法，并用于估算AI毁灭人类的概率。


<details>
  <summary>Details</summary>
Motivation: 自ChatGPT发布以来，关于AI系统是否对人类构成存在性风险的争论不断。本文旨在开发一个系统框架来分析AI的存在性风险，为这一重要辩论提供结构化的思考工具。

Method: 基于两个核心前提构建分析框架：前提一：AI系统将变得极其强大；前提二：如果AI系统变得极其强大，它们将毁灭人类。通过这两个前提构建了四种人类生存故事分类，每种故事中至少有一个前提不成立。

Result: 提出了一个完整的人类生存故事分类法：1）科学障碍阻止AI变得极其强大；2）人类禁止AI研究；3）极其强大的AI因其目标而不毁灭人类；4）人类能够可靠检测并禁用具有毁灭目标的AI系统。不同生存故事面临不同挑战并需要不同应对策略。

Conclusion: 该框架为分析AI存在性风险提供了系统方法，不同生存故事对应不同的风险应对策略。作者使用这一分类法对P(doom)（AI毁灭人类的概率）进行了粗略估计，为政策制定和风险缓解提供了理论基础。

Abstract: Since the release of ChatGPT, there has been a lot of debate about whether AI systems pose an existential risk to humanity. This paper develops a general framework for thinking about the existential risk of AI systems. We analyze a two premise argument that AI systems pose a threat to humanity. Premise one: AI systems will become extremely powerful. Premise two: if AI systems become extremely powerful, they will destroy humanity. We use these two premises to construct a taxonomy of survival stories, in which humanity survives into the far future. In each survival story, one of the two premises fails. Either scientific barriers prevent AI systems from becoming extremely powerful; or humanity bans research into AI systems, thereby preventing them from becoming extremely powerful; or extremely powerful AI systems do not destroy humanity, because their goals prevent them from doing so; or extremely powerful AI systems do not destroy humanity, because we can reliably detect and disable systems that have the goal of doing so. We argue that different survival stories face different challenges. We also argue that different survival stories motivate different responses to the threats from AI. Finally, we use our taxonomy to produce rough estimates of P(doom), the probability that humanity will be destroyed by AI.

</details>


### [3] [GUI-Eyes: Tool-Augmented Perception for Visual Grounding in GUI Agents](https://arxiv.org/abs/2601.09770)
*Chen Chen,Jiawei Shao,Dakuan Lu,Haoyi Hu,Xiangcheng Liu,Hantao Yao,Wu Liu*

Main category: cs.AI

TL;DR: GUI-Eyes：一个用于GUI任务的主动视觉感知强化学习框架，通过两阶段推理学习策略性地决定是否及如何使用视觉工具（如裁剪、缩放），在ScreenSpot-Pro基准上仅用3k标注样本达到44.8%的定位准确率。


<details>
  <summary>Details</summary>
Motivation: 当前GUI自动化方法主要依赖静态、一次性视觉输入和被动感知，缺乏自适应决定何时、是否以及如何观察界面的能力。需要开发能够主动获取信息性观察的GUI代理。

Method: 提出渐进感知策略：将决策分解为粗粒度探索和细粒度定位，由两级策略协调。设计空间连续奖励函数，结合位置邻近性和区域重叠度，为工具使用提供密集监督。采用两阶段推理过程学习策略性地决定是否及如何调用视觉工具。

Result: 在ScreenSpot-Pro基准上，GUI-Eyes-3B仅使用3k标注样本就达到44.8%的定位准确率，显著优于监督学习和基于强化学习的基线方法。

Conclusion: 工具感知的主动感知，通过分阶段策略推理和细粒度奖励反馈，对于构建鲁棒且数据高效的GUI代理至关重要。GUI-Eyes框架展示了在GUI环境中主动视觉感知的有效性。

Abstract: Recent advances in vision-language models (VLMs) and reinforcement learning (RL) have driven progress in GUI automation. However, most existing methods rely on static, one-shot visual inputs and passive perception, lacking the ability to adaptively determine when, whether, and how to observe the interface. We present GUI-Eyes, a reinforcement learning framework for active visual perception in GUI tasks. To acquire more informative observations, the agent learns to make strategic decisions on both whether and how to invoke visual tools, such as cropping or zooming, within a two-stage reasoning process. To support this behavior, we introduce a progressive perception strategy that decomposes decision-making into coarse exploration and fine-grained grounding, coordinated by a two-level policy. In addition, we design a spatially continuous reward function tailored to tool usage, which integrates both location proximity and region overlap to provide dense supervision and alleviate the reward sparsity common in GUI environments. On the ScreenSpot-Pro benchmark, GUI-Eyes-3B achieves 44.8% grounding accuracy using only 3k labeled samples, significantly outperforming both supervised and RL-based baselines. These results highlight that tool-aware active perception, enabled by staged policy reasoning and fine-grained reward feedback, is critical for building robust and data-efficient GUI agents.

</details>


### [4] [PCN-Rec: Agentic Proof-Carrying Negotiation for Reliable Governance-Constrained Recommendation](https://arxiv.org/abs/2601.09771)
*Aradhya Dixit,Shreem Dixit*

Main category: cs.AI

TL;DR: PCN-Rec是一个证明携带的协商推荐系统，通过分离自然语言推理和确定性约束执行，可靠地满足治理约束（如长尾曝光和多样性要求），同时保持推荐质量。


<details>
  <summary>Details</summary>
Motivation: 现有的基于LLM的推荐系统虽然能生成有吸引力的排名列表，但在可靠满足治理约束（如最小长尾曝光、多样性要求）方面存在困难。需要一种既能保持推荐相关性，又能确保约束满足的可验证方法。

Method: PCN-Rec采用证明携带的协商管道：1）基础推荐器（MF/CF）生成候选窗口；2）两个代理进行协商：用户倡导者优化相关性，策略代理执行约束；3）调解LLM合成top-N列表及结构化证书（JSON）；4）确定性验证器重新计算所有约束，仅接受验证通过的证书；5）验证失败时，确定性约束贪婪修复生成合规列表重新验证。

Result: 在MovieLens-100K数据集上，PCN-Rec对可行用户（n=551，W=80）实现了98.55%的通过率，相比没有验证/修复的单次LLM基线显著提升。同时保持推荐效用，NDCG@10仅下降0.021（0.403 vs. 0.424），差异具有统计显著性（p<0.05）。

Conclusion: PCN-Rec通过证明携带的协商框架，成功解决了LLM推荐系统中约束满足的可靠性问题，实现了高通过率、可验证性和推荐质量的平衡，为受治理约束的推荐系统提供了可审计的解决方案。

Abstract: Modern LLM-based recommenders can generate compelling ranked lists, but they struggle to reliably satisfy governance constraints such as minimum long-tail exposure or diversity requirements. We present PCN-Rec, a proof-carrying negotiation pipeline that separates natural-language reasoning from deterministic enforcement. A base recommender (MF/CF) produces a candidate window of size W, which is negotiated by two agents: a User Advocate optimizing relevance and a Policy Agent enforcing constraints. A mediator LLM synthesizes a top-N slate together with a structured certificate (JSON) describing the claimed constraint satisfaction. A deterministic verifier recomputes all constraints from the slate and accepts only verifier-checked certificates; if verification fails, a deterministic constrained-greedy repair produces a compliant slate for re-verification, yielding an auditable trace. On MovieLens-100K with governance constraints, PCN-Rec achieves a 98.55% pass rate on feasible users (n = 551, W = 80) versus a one-shot single-LLM baseline without verification/repair, while preserving utility with only a 0.021 absolute drop in NDCG@10 (0.403 vs. 0.424); differences are statistically significant (p < 0.05).

</details>


### [5] [Antisocial behavior towards large language model users: experimental evidence](https://arxiv.org/abs/2601.09772)
*Paweł Niszczota,Cassandra Grützner*

Main category: cs.AI

TL;DR: 研究发现人们对使用大语言模型（LLM）完成任务的人会施加惩罚性行为，平均会销毁LLM使用者36%的收入，且惩罚程度随实际使用量增加而增加。自我报告的使用情况与实际使用情况存在可信度差距。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速普及，人们关注社会对AI使用者的反应。先前研究表明人们对AI使用者持负面态度，但尚不清楚这种不认可是否会转化为实际的惩罚行为。

Method: 采用两阶段在线实验设计（第二阶段491名参与者）。第一阶段参与者完成实际努力任务，可选择使用或不使用LLM支持。第二阶段参与者可花费自己的资金来减少第一阶段参与者的收入，基于他们是否使用LLM以及自我报告的使用情况。

Result: 参与者平均销毁了完全依赖LLM者36%的收入，惩罚程度随实际LLM使用量单调增加。自我报告与实际使用存在可信度差距：自我报告未使用者比实际未使用者受到更严厉惩罚；在高使用水平下，实际依赖LLM比自我报告依赖受到更强惩罚。

Conclusion: 这是首个行为证据表明LLM的效率提升会带来社会制裁成本。人们对LLM使用者施加实质性惩罚，且自我报告与实际使用之间的可信度差距会影响惩罚程度。

Abstract: The rapid spread of large language models (LLMs) has raised concerns about the social reactions they provoke. Prior research documents negative attitudes toward AI users, but it remains unclear whether such disapproval translates into costly action. We address this question in a two-phase online experiment (N = 491 Phase II participants; Phase I provided targets) where participants could spend part of their own endowment to reduce the earnings of peers who had previously completed a real-effort task with or without LLM support. On average, participants destroyed 36% of the earnings of those who relied exclusively on the model, with punishment increasing monotonically with actual LLM use. Disclosure about LLM use created a credibility gap: self-reported null use was punished more harshly than actual null use, suggesting that declarations of "no use" are treated with suspicion. Conversely, at high levels of use, actual reliance on the model was punished more strongly than self-reported reliance. Taken together, these findings provide the first behavioral evidence that the efficiency gains of LLMs come at the cost of social sanctions.

</details>


### [6] [Improving Chain-of-Thought for Logical Reasoning via Attention-Aware Intervention](https://arxiv.org/abs/2601.09805)
*Nguyen Minh Phuong,Dang Huu Tien,Naoya Inoue*

Main category: cs.AI

TL;DR: 本文提出了一种非交互式端到端逻辑推理框架AAI，通过注意力重加权来激活模型内部的逻辑推理能力，无需外部资源且计算开销小。


<details>
  <summary>Details</summary>
Motivation: 现有LLM逻辑推理方法依赖复杂的交互式框架或外部符号求解器，这带来了额外开销和可扩展性限制。作者希望开发一种非交互式、端到端的框架，让推理能力在模型内部自然涌现。

Method: 提出Attention-Aware Intervention (AAI)方法：1) 在few-shot提示中引入结构信息来激活与逻辑推理操作符对齐的注意力头；2) 在推理时对这些选定注意力头的注意力分数进行重加权，通过注意力调制引导模型利用先验知识。

Result: 在多个基准测试和模型架构上的广泛实验表明，AAI显著提升了逻辑推理性能，同时仅带来可忽略的额外计算开销。

Conclusion: AAI提供了一种高效的非交互式端到端逻辑推理框架，通过注意力调制激活模型内在的推理能力，在保持可分析性的同时实现了更好的泛化性能。

Abstract: Modern logical reasoning with LLMs primarily relies on employing complex interactive frameworks that decompose the reasoning process into subtasks solved through carefully designed prompts or requiring external resources (e.g., symbolic solvers) to exploit their strong logical structures. While interactive approaches introduce additional overhead, hybrid approaches depend on external components, which limit their scalability. A non-interactive, end-to-end framework enables reasoning to emerge within the model itself -- improving generalization while preserving analyzability without any external resources. In this work, we introduce a non-interactive, end-to-end framework for reasoning tasks. We show that introducing structural information into the few-shot prompt activates a subset of attention heads that patterns aligned with logical reasoning operators. Building on this insight, we propose Attention-Aware Intervention (AAI), an inference-time intervention method that reweights attention scores across selected heads identified by their logical patterns. AAI offers an efficient way to steer the model's reasoning toward leveraging prior knowledge through attention modulation. Extensive experiments show that AAI enhances logical reasoning performance across diverse benchmarks and model architectures, while incurring negligible additional computational overhead. Code is available at https://github.com/phuongnm94/aai_for_logical_reasoning.

</details>


### [7] [Thinking Long, but Short: Stable Sequential Test-Time Scaling for Large Reasoning Models](https://arxiv.org/abs/2601.09855)
*Michael R. Metel,Yufei Cui,Boxing Chen,Prasanna Parthasarathi*

Main category: cs.AI

TL;DR: Min-Seek：一种新颖的顺序测试时缩放方法，通过动态KV缓存管理，在广泛推理长度范围内显著提升模型准确率并避免性能退化


<details>
  <summary>Details</summary>
Motivation: 当前顺序测试时缩放方法存在显著局限性：虽然增加推理长度能提高准确率，但进一步延长会导致准确率下降和模型不稳定，且需要推理长度微调

Method: 提出Min-Seek方法，仅保留一个额外诱导思维的KV对在KV缓存中，使用自定义KV缓存存储无位置嵌入的键，并在每个新生成思维前动态连续编码，实现超越模型最大上下文长度的推理

Result: 方法在多种推理任务上显著提升模型准确率，稳定顺序缩放的准确率，消除推理长度微调需求，在温和条件下具有线性计算复杂度

Conclusion: Min-Seek是一种高效、稳定的顺序测试时缩放方法，能够显著提升大型推理模型的准确率，同时避免传统方法中的性能退化问题

Abstract: Sequential test-time scaling is a promising training-free method to improve large reasoning model accuracy, but as currently implemented, significant limitations have been observed. Inducing models to think for longer can increase their accuracy, but as the length of reasoning is further extended, it has also been shown to result in accuracy degradation and model instability. This work presents a novel sequential test-time scaling method, Min-Seek, which improves model accuracy significantly over a wide range of induced thoughts, stabilizing the accuracy of sequential scaling, and removing the need for reasoning length fine-tuning. Beyond improving model accuracy over a variety of reasoning tasks, our method is inherently efficient, as only the KV pairs of one additional induced thought are kept in the KV cache during reasoning. With a custom KV cache which stores keys without position embeddings, by dynamically encoding them contiguously before each new generated thought, our method can continue to reason well beyond a model's maximum context length, and under mild conditions has linear computational complexity.

</details>


### [8] [A Scoping Review of the Ethical Perspectives on Anthropomorphising Large Language Model-Based Conversational Agents](https://arxiv.org/abs/2601.09869)
*Andrea Ferrario,Rasita Vinay,Matteo Casserini,Alessandro Facchini*

Main category: cs.AI

TL;DR: 本文对LLM对话代理拟人化的伦理研究进行范围综述，梳理概念基础、伦理挑战与机遇、方法论，发现定义趋同但操作化差异大，风险导向为主，实证研究有限，最后提出研究议程和设计/治理建议。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的对话代理日益普及，拟人化现象（赋予非人类实体人类特质）变得突出。现有文献在定义、操作化和伦理评估上存在碎片化和差异，需要系统梳理以指导伦理部署。

Method: 采用范围综述方法，检索五个数据库和三个预印本库中关于LLM对话代理拟人化的伦理导向文献，综合分析概念基础、伦理挑战与机遇、方法论。

Result: 发现定义上基于归因的方法趋同，但操作化差异显著；伦理框架以风险导向为主；实证研究有限，缺乏将交互效应转化为可操作治理指导的实证连接。

Conclusion: 提出研究议程和设计/治理建议，为LLM对话代理中拟人化线索的伦理部署提供指导，强调需要更多实证研究连接交互效应与治理实践。

Abstract: Anthropomorphisation -- the phenomenon whereby non-human entities are ascribed human-like qualities -- has become increasingly salient with the rise of large language model (LLM)-based conversational agents (CAs). Unlike earlier chatbots, LLM-based CAs routinely generate interactional and linguistic cues, such as first-person self-reference, epistemic and affective expressions that empirical work shows can increase engagement. On the other hand, anthropomorphisation raises ethical concerns, including deception, overreliance, and exploitative relationship framing, while some authors argue that anthropomorphic interaction may support autonomy, well-being, and inclusion. Despite increasing interest in the phenomenon, literature remains fragmented across domains and varies substantially in how it defines, operationalizes, and normatively evaluates anthropomorphisation. This scoping review maps ethically oriented work on anthropomorphising LLM-based CAs across five databases and three preprint repositories. We synthesize (1) conceptual foundations, (2) ethical challenges and opportunities, and (3) methodological approaches. We find convergence on attribution-based definitions but substantial divergence in operationalization, a predominantly risk-forward normative framing, and limited empirical work that links observed interaction effects to actionable governance guidance. We conclude with a research agenda and design/governance recommendations for ethically deploying anthropomorphic cues in LLM-based conversational agents.

</details>


### [9] [Epistemology gives a Future to Complementarity in Human-AI Interactions](https://arxiv.org/abs/2601.09871)
*Andrea Ferrario,Alessandro Facchini,Juan M. Durán*

Main category: cs.AI

TL;DR: 该论文将人机互补性重新定义为可靠认知过程的证据，而非单纯的预测准确性指标，为人机交互提供了更坚实的理论基础。


<details>
  <summary>Details</summary>
Motivation: 当前人机互补性概念面临理论挑战：缺乏精确的理论基础、仅作为事后预测准确性指标、忽视人机交互的其他需求、忽略性能增益的成本效益分析，导致实证研究中难以实现。

Method: 利用认识论框架，将互补性重新置于"论证性AI"话语中，借鉴计算可靠性主义，将历史互补性实例视为特定人机交互作为可靠认知过程的证据。

Result: 提出互补性的新角色：作为校准人机团队可靠性的指标，结合其他可靠性指标，支持受影响各方（患者、管理者、监管者等）的实践推理。

Conclusion: 互补性的价值和作用不在于提供预测准确性的相对度量，而在于帮助校准决策制定，使其适应日益影响日常生活的AI支持过程的可靠性。

Abstract: Human-AI complementarity is the claim that a human supported by an AI system can outperform either alone in a decision-making process. Since its introduction in the human-AI interaction literature, it has gained traction by generalizing the reliance paradigm and by offering a more practical alternative to the contested construct of 'trust in AI.' Yet complementarity faces key theoretical challenges: it lacks precise theoretical anchoring, it is formalized just as a post hoc indicator of relative predictive accuracy, it remains silent about other desiderata of human-AI interactions and it abstracts away from the magnitude-cost profile of its performance gain. As a result, complementarity is difficult to obtain in empirical settings. In this work, we leverage epistemology to address these challenges by reframing complementarity within the discourse on justificatory AI. Drawing on computational reliabilism, we argue that historical instances of complementarity function as evidence that a given human-AI interaction is a reliable epistemic process for a given predictive task. Together with other reliability indicators assessing the alignment of the human-AI team with the epistemic standards and socio-technical practices, complementarity contributes to the degree of reliability of human-AI teams when generating predictions. This supports the practical reasoning of those affected by these outputs -- patients, managers, regulators, and others. In summary, our approach suggests that the role and value of complementarity lies not in providing a relative measure of predictive accuracy, but in helping calibrate decision-making to the reliability of AI-supported processes that increasingly shape everyday life.

</details>


### [10] [Beyond Rule-Based Workflows: An Information-Flow-Orchestrated Multi-Agents Paradigm via Agent-to-Agent Communication from CORAL](https://arxiv.org/abs/2601.09883)
*Xinxing Ren,Quagmire Zang,Caelum Forder,Suman Deb,Ahsen Tahir,Roman J. Georgio,Peter Carroll,Zekun Guo*

Main category: cs.AI

TL;DR: 提出基于信息流编排的多智能体范式，通过A2A通信动态协调智能体，无需预定义工作流，在GAIA基准上超越基于工作流的OWL系统8.49个百分点


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多智能体系统依赖预定义工作流，需要人工枚举任务状态并指定路由规则，存在两个根本限制：1) 需要大量人工努力预测和编码可能状态；2) 无法穷尽覆盖复杂现实任务的状态空间

Method: 提出信息流编排的多智能体范式，通过专门的编排器持续监控任务进度，使用自然语言通过A2A工具包动态协调其他智能体，不依赖预定义工作流

Result: 在GAIA基准测试中，pass@1设置下达到63.64%准确率，比基于工作流的OWL系统（55.15%）高出8.49个百分点，且token消耗相当。案例级分析显示该方法能实现更灵活的任务监控和更鲁棒的边缘情况处理

Conclusion: 信息流编排的多智能体范式通过动态协调机制克服了基于规则工作流的限制，实现了更灵活、鲁棒的多智能体协作，为复杂任务处理提供了新方向

Abstract: Most existing Large Language Model (LLM)-based Multi-Agent Systems (MAS) rely on predefined workflows, where human engineers enumerate task states in advance and specify routing rules and contextual injections accordingly. Such workflow-driven designs are essentially rule-based decision trees, which suffer from two fundamental limitations: they require substantial manual effort to anticipate and encode possible task states, and they cannot exhaustively cover the state space of complex real-world tasks. To address these issues, we propose an Information-Flow-Orchestrated Multi-Agent Paradigm via Agent-to-Agent (A2A) Communication from CORAL, in which a dedicated information flow orchestrator continuously monitors task progress and dynamically coordinates other agents through the A2A toolkit using natural language, without relying on predefined workflows. We evaluate our approach on the general-purpose benchmark GAIA, using the representative workflow-based MAS OWL as the baseline while controlling for agent roles and underlying models. Under the pass@1 setting, our method achieves 63.64% accuracy, outperforming OWL's 55.15% by 8.49 percentage points with comparable token consumption. Further case-level analysis shows that our paradigm enables more flexible task monitoring and more robust handling of edge cases. Our implementation is publicly available at: https://github.com/Coral-Protocol/Beyond-Rule-Based-Workflows

</details>


### [11] [Continuum Memory Architectures for Long-Horizon LLM Agents](https://arxiv.org/abs/2601.09913)
*Joe Logan*

Main category: cs.AI

TL;DR: 论文提出"连续记忆架构"(CMA)作为RAG的替代方案，通过持久存储、选择性保留、关联路由、时间链和整合为高阶抽象来解决RAG在记忆积累、更新和消歧方面的结构缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前检索增强生成(RAG)将记忆视为静态查找表，存在信息永久存储、只读检索、缺乏时间连续性等问题，无法满足长期智能体对记忆积累、更新和消歧的需求。

Method: 定义连续记忆架构(CMA)这一系统类别，要求具备持久存储、选择性保留、关联路由、时间链和整合为高阶抽象等架构特性，而非具体实现细节。

Result: 在知识更新、时间关联、关联回忆、上下文消歧等任务上，CMA展现出比RAG更优的行为优势，证明了其在长期智能体中的必要性。

Conclusion: CMA是长期智能体必要的架构原语，但面临延迟、漂移和可解释性等开放挑战，为未来研究提供了方向。

Abstract: Retrieval-augmented generation (RAG) has become the default strategy for providing large language model (LLM) agents with contextual knowledge. Yet RAG treats memory as a stateless lookup table: information persists indefinitely, retrieval is read-only, and temporal continuity is absent. We define the \textit{Continuum Memory Architecture} (CMA), a class of systems that maintain and update internal state across interactions through persistent storage, selective retention, associative routing, temporal chaining, and consolidation into higher-order abstractions. Rather than disclosing implementation specifics, we specify the architectural requirements CMA imposes and show consistent behavioral advantages on tasks that expose RAG's structural inability to accumulate, mutate, or disambiguate memory. The empirical probes (knowledge updates, temporal association, associative recall, contextual disambiguation) demonstrate that CMA is a necessary architectural primitive for long-horizon agents while highlighting open challenges around latency, drift, and interpretability.

</details>


### [12] [CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents](https://arxiv.org/abs/2601.09923)
*Hanna Foerster,Robert Mullins,Tom Blanchard,Nicolas Papernot,Kristina Nikolić,Florian Tramèr,Ilia Shumailov,Cheng Zhang,Yiren Zhao*

Main category: cs.AI

TL;DR: 论文提出了一种针对计算机使用代理（CUAs）的单次规划方法，通过可信规划器在执行前生成完整的条件分支执行图，提供可证明的控制流完整性保证，以防御提示注入攻击。


<details>
  <summary>Details</summary>
Motivation: AI代理易受提示注入攻击，现有唯一鲁棒防御是架构隔离，但计算机使用代理需要持续观察UI状态来确定动作，这与安全所需的隔离存在根本冲突。

Method: 引入单次规划方法，在观察任何潜在恶意内容之前，由可信规划器生成包含条件分支的完整执行图，确保控制流完整性。同时需要额外措施防止分支导向攻击。

Result: 在OSWorld上评估，在保持前沿模型57%性能的同时，将较小开源模型的性能提升高达19%，证明严格的安全性和实用性可以在CUAs中共存。

Conclusion: 通过单次规划方法解决了计算机使用代理中安全隔离与功能需求的矛盾，虽然需要额外防御分支导向攻击，但实现了安全与效用的平衡。

Abstract: AI agents are vulnerable to prompt injection attacks, where malicious content hijacks agent behavior to steal credentials or cause financial loss. The only known robust defense is architectural isolation that strictly separates trusted task planning from untrusted environment observations. However, applying this design to Computer Use Agents (CUAs) -- systems that automate tasks by viewing screens and executing actions -- presents a fundamental challenge: current agents require continuous observation of UI state to determine each action, conflicting with the isolation required for security. We resolve this tension by demonstrating that UI workflows, while dynamic, are structurally predictable. We introduce Single-Shot Planning for CUAs, where a trusted planner generates a complete execution graph with conditional branches before any observation of potentially malicious content, providing provable control flow integrity guarantees against arbitrary instruction injections. Although this architectural isolation successfully prevents instruction injections, we show that additional measures are needed to prevent Branch Steering attacks, which manipulate UI elements to trigger unintended valid paths within the plan. We evaluate our design on OSWorld, and retain up to 57% of the performance of frontier models while improving performance for smaller open-source models by up to 19%, demonstrating that rigorous security and utility can coexist in CUAs.

</details>


### [13] [Hallucination Detection and Mitigation in Large Language Models](https://arxiv.org/abs/2601.09929)
*Ahmad Pesaranghader,Erin Li*

Main category: cs.AI

TL;DR: 论文提出了一个基于根因认知的幻觉管理操作框架，通过模型、数据和上下文三方面因素分类，结合多层次检测与缓解策略，构建可扩展的可靠生成式AI系统。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型和大型推理模型在金融、法律等高风险领域具有变革潜力，但其产生幻觉（生成事实错误或无依据内容）的倾向带来了关键可靠性风险，需要系统化解决方案。

Method: 提出了一个全面的幻觉管理操作框架，基于持续改进循环和根因认知。将幻觉来源分为模型、数据和上下文相关因素，整合多层次检测方法（不确定性估计、推理一致性等）与分层缓解策略（知识基础、置信度校准等）。

Result: 通过分层架构和金融数据提取案例研究展示了框架应用，其中模型、上下文和数据层形成闭环反馈循环，实现渐进式可靠性增强。

Conclusion: 该方法为受监管环境中构建可信赖的生成式AI系统提供了系统化、可扩展的方法论，能够针对性地解决幻觉问题而非依赖通用修复。

Abstract: Large Language Models (LLMs) and Large Reasoning Models (LRMs) offer transformative potential for high-stakes domains like finance and law, but their tendency to hallucinate, generating factually incorrect or unsupported content, poses a critical reliability risk. This paper introduces a comprehensive operational framework for hallucination management, built on a continuous improvement cycle driven by root cause awareness. We categorize hallucination sources into model, data, and context-related factors, allowing targeted interventions over generic fixes. The framework integrates multi-faceted detection methods (e.g., uncertainty estimation, reasoning consistency) with stratified mitigation strategies (e.g., knowledge grounding, confidence calibration). We demonstrate its application through a tiered architecture and a financial data extraction case study, where model, context, and data tiers form a closed feedback loop for progressive reliability enhancement. This approach provides a systematic, scalable methodology for building trustworthy generative AI systems in regulated environments.

</details>


### [14] [Chinese Labor Law Large Language Model Benchmark](https://arxiv.org/abs/2601.09972)
*Zixun Lan,Maochun Xu,Yifan Ren,Rui Wu,Jianghui Zhou,Xueyang Cheng,Jianan Ding Ding,Xinheng Wang,Mingmin Chi,Fei Ma*

Main category: cs.AI

TL;DR: LabourLawLLM是专门针对中国劳动法领域的大语言模型，在LabourLawBench基准测试中超越了通用模型和现有法律专用模型，为构建其他法律子领域专用模型提供了可扩展方法。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型（如GPT-4）在处理需要精确法律知识、复杂推理和情境敏感性的专业法律子领域时表现不佳，需要专门针对特定法律领域的模型来提高法律AI应用的准确性和可靠性。

Method: 开发了LabourLawLLM专门针对中国劳动法的大语言模型，并创建了LabourLawBench综合基准测试，涵盖法律条款引用、知识问答、案例分类、赔偿计算、命名实体识别和案例分析等任务。评估框架结合了客观指标（ROUGE-L、准确率、F1、soft-F1）和基于GPT-4评分的主观评估。

Result: 实验表明，LabourLawLLM在所有任务类别中一致优于通用大语言模型和现有的法律专用大语言模型，在劳动法领域表现出色。

Conclusion: 该研究不仅为劳动法领域提供了专用模型，还为构建其他法律子领域的专用大语言模型提供了可扩展的方法论，有助于提高法律AI应用的准确性、可靠性和社会价值。

Abstract: Recent advances in large language models (LLMs) have led to substantial progress in domain-specific applications, particularly within the legal domain. However, general-purpose models such as GPT-4 often struggle with specialized subdomains that require precise legal knowledge, complex reasoning, and contextual sensitivity. To address these limitations, we present LabourLawLLM, a legal large language model tailored to Chinese labor law. We also introduce LabourLawBench, a comprehensive benchmark covering diverse labor-law tasks, including legal provision citation, knowledge-based question answering, case classification, compensation computation, named entity recognition, and legal case analysis. Our evaluation framework combines objective metrics (e.g., ROUGE-L, accuracy, F1, and soft-F1) with subjective assessment based on GPT-4 scoring. Experiments show that LabourLawLLM consistently outperforms general-purpose and existing legal-specific LLMs across task categories. Beyond labor law, our methodology provides a scalable approach for building specialized LLMs in other legal subfields, improving accuracy, reliability, and societal value of legal AI applications.

</details>


### [15] [SPRInG: Continual LLM Personalization via Selective Parametric Adaptation and Retrieval-Interpolated Generation](https://arxiv.org/abs/2601.09974)
*Seoyeon Kim,Jaehyung Kim*

Main category: cs.AI

TL;DR: SPRInG：一个用于动态用户偏好持续个性化的半参数框架，通过漂移驱动选择性适应和严格相关性门控解决偏好漂移问题


<details>
  <summary>Details</summary>
Motivation: 现有个性化大语言模型通常基于静态检索或一次性适应，假设用户偏好不变。但现实世界中用户兴趣持续演化，存在偏好漂移问题，标准持续学习方法难以区分真实偏好变化与临时上下文噪声

Method: SPRInG采用半参数框架：训练时使用基于似然的评分函数识别高新颖性交互，选择性更新用户特定适配器，同时将难学习残差保存在回放缓冲区；推理时应用严格相关性门控，通过logit插值融合参数知识与检索历史

Result: 在长格式个性化生成基准测试中，SPRInG优于现有基线方法，验证了其在现实世界持续个性化任务中的鲁棒性

Conclusion: SPRInG框架通过漂移驱动选择性适应和严格相关性门控，有效解决了动态用户偏好的持续个性化问题，为现实世界个性化系统提供了更鲁棒的解决方案

Abstract: Personalizing Large Language Models typically relies on static retrieval or one-time adaptation, assuming user preferences remain invariant over time. However, real-world interactions are dynamic, where user interests continuously evolve, posing a challenge for models to adapt to preference drift without catastrophic forgetting. Standard continual learning approaches often struggle in this context, as they indiscriminately update on noisy interaction streams, failing to distinguish genuine preference shifts from transient contexts. To address this, we introduce SPRInG, a novel semi-parametric framework designed for effective continual personalization. During training, SPRInG employs drift-driven selective adaptation, which utilizes a likelihood-based scoring function to identify high-novelty interactions. This allows the model to selectively update the user-specific adapter on drift signals while preserving hard-to-learn residuals in a replay buffer. During inference, we apply strict relevance gating and fuse parametric knowledge with retrieved history via logit interpolation. Experiments on the long-form personalized generation benchmark demonstrate that SPRInG outperforms existing baselines, validating its robustness for real-world continual personalization.

</details>


### [16] [Memo-SQL: Structured Decomposition and Experience-Driven Self-Correction for Training-Free NL2SQL](https://arxiv.org/abs/2601.10011)
*Zerui Yang,Weichuan Wang,Yanwei Xu,Linqi Song,Yudai Matsuda,Wei Han,Bo Bai*

Main category: cs.AI

TL;DR: Memo-SQL：无需训练的NL2SQL框架，通过结构化分解和经验感知自校正解决现有系统问题，在BIRD数据集上达到68.5%执行准确率，比之前方法节省10倍资源。


<details>
  <summary>Details</summary>
Motivation: 现有NL2SQL系统存在两个关键局限：1）仅使用正确示例进行上下文学习，忽略了历史错误-修复对中的丰富信号；2）测试时分解方法随意，产生相似SQL候选，降低集成效果。同时面临准确率与效率的严重权衡。

Method: 提出训练免费框架Memo-SQL，包含两个核心思想：1）结构化分解：采用实体级、分层和原子顺序三种清晰策略促进多样化推理；2）经验感知自校正：构建动态记忆库存储成功查询和历史错误-修复对，通过检索增强提示在推理时引入相关示例，无需微调或外部API。

Result: 在BIRD数据集上达到68.5%执行准确率，在开放、零微调方法中创下新的SOTA，同时比之前测试时缩放方法节省超过10倍计算资源。

Conclusion: Memo-SQL通过结构化分解和经验感知自校正有效解决了NL2SQL系统的关键问题，在保持高性能的同时显著提升了效率，为训练免费的NL2SQL系统提供了新思路。

Abstract: Existing NL2SQL systems face two critical limitations: (1) they rely on in-context learning with only correct examples, overlooking the rich signal in historical error-fix pairs that could guide more robust self-correction; and (2) test-time scaling approaches often decompose questions arbitrarily, producing near-identical SQL candidates across runs and diminishing ensemble gains. Moreover, these methods suffer from a stark accuracy-efficiency trade-off: high performance demands excessive computation, while fast variants compromise quality. We present Memo-SQL, a training-free framework that addresses these issues through two simple ideas: structured decomposition and experience-aware self-correction. Instead of leaving decomposition to chance, we apply three clear strategies, entity-wise, hierarchical, and atomic sequential, to encourage diverse reasoning. For correction, we build a dynamic memory of both successful queries and historical error-fix pairs, and use retrieval-augmented prompting to bring relevant examples into context at inference time, no fine-tuning or external APIs required. On BIRD, Memo-SQL achieves 68.5% execution accuracy, setting a new state of the art among open, zero-fine-tuning methods, while using over 10 times fewer resources than prior TTS approaches.

</details>


### [17] [Breaking Up with Normatively Monolithic Agency with GRACE: A Reason-Based Neuro-Symbolic Architecture for Safe and Ethical AI Alignment](https://arxiv.org/abs/2601.10520)
*Felix Jahn,Yannic Muskalla,Lisa Dargasz,Patrick Schramowski,Kevin Baum*

Main category: cs.AI

TL;DR: GRACE是一个神经符号推理架构，通过将规范性推理与工具性决策分离来确保AI代理的道德对齐，包含道德模块、决策模块和监控守卫三个组件。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理在重要场景中自主部署并产生实际影响，确保其决策不仅工具有效而且符合道德规范变得至关重要。需要一种能够约束任何设计AI代理的架构。

Method: 提出GRACE三模块架构：1) 道德模块使用基于理由的道义逻辑推理确定允许的宏观行动；2) 决策模块封装目标代理，在宏观行动约束下选择工具最优的原始行动；3) 守卫监控并强制执行道德合规性。

Result: GRACE在LLM治疗助手示例中展示，使利益相关者能够理解、质疑和优化代理行为，提供可解释性、可争议性和可验证的道德对齐保证。

Conclusion: GRACE通过神经符号方法实现了AI代理的道德约束，将规范性推理与工具决策分离，为AI对齐提供了可解释、可验证的解决方案。

Abstract: As AI agents become increasingly autonomous, widely deployed in consequential contexts, and efficacious in bringing about real-world impacts, ensuring that their decisions are not only instrumentally effective but also normatively aligned has become critical. We introduce a neuro-symbolic reason-based containment architecture, Governor for Reason-Aligned ContainmEnt (GRACE), that decouples normative reasoning from instrumental decision-making and can contain AI agents of virtually any design. GRACE restructures decision-making into three modules: a Moral Module (MM) that determines permissible macro actions via deontic logic-based reasoning; a Decision-Making Module (DMM) that encapsulates the target agent while selecting instrumentally optimal primitive actions in accordance with derived macro actions; and a Guard that monitors and enforces moral compliance. The MM uses a reason-based formalism providing a semantic foundation for deontic logic, enabling interpretability, contestability, and justifiability. Its symbolic representation enriches the DMM's informational context and supports formal verification and statistical guarantees of alignment enforced by the Guard. We demonstrate GRACE on an example of a LLM therapy assistant, showing how it enables stakeholders to understand, contest, and refine agent behavior.

</details>


### [18] [Structured Personality Control and Adaptation for LLM Agents](https://arxiv.org/abs/2601.10025)
*Jinpeng Wang,Xinyu Jia,Wei Wei Heng,Yuquan Li,Binbin Shi,Qianlei Chen,Guannan Chen,Junxia Zhang,Yuyu Yin*

Main category: cs.AI

TL;DR: 该论文提出了一个基于荣格心理类型的LLM人格建模框架，通过三种机制实现人格的连贯表达、情境适应和长期演化，为HCI中的自然化智能体设计提供支持。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在HCI应用中缺乏既细腻又适应性强的人格表达方法。现有方法难以同时实现人格的细微特质和动态适应性，限制了LLM在个性化助手、社交模拟等场景中的自然交互效果。

Method: 提出基于荣格心理类型的人格建模框架，包含三个核心机制：1）主导-辅助协调机制，确保核心人格特质的连贯表达；2）强化-补偿机制，实现短期情境适应；3）反思机制，驱动长期人格演化。使用MBTI问卷进行人格对齐评估，并在多样化挑战场景中测试。

Result: 研究发现，具有演化能力的人格感知LLM能够支持连贯且情境敏感的交互。该框架使智能体能够保持细腻的人格特质，同时动态适应交互需求，并逐步更新其底层人格结构。

Conclusion: 演化性的人格感知LLM能够实现自然化的智能体设计，为HCI中的个性化助手和社交模拟等应用提供更真实、连贯的交互体验。该框架为人格建模提供了结构化的评估方法。

Abstract: Large Language Models (LLMs) are increasingly shaping human-computer interaction (HCI), from personalized assistants to social simulations. Beyond language competence, researchers are exploring whether LLMs can exhibit human-like characteristics that influence engagement, decision-making, and perceived realism. Personality, in particular, is critical, yet existing approaches often struggle to achieve both nuanced and adaptable expression. We present a framework that models LLM personality via Jungian psychological types, integrating three mechanisms: a dominant-auxiliary coordination mechanism for coherent core expression, a reinforcement-compensation mechanism for temporary adaptation to context, and a reflection mechanism that drives long-term personality evolution. This design allows the agent to maintain nuanced traits while dynamically adjusting to interaction demands and gradually updating its underlying structure. Personality alignment is evaluated using Myers-Briggs Type Indicator questionnaires and tested under diverse challenge scenarios as a preliminary structured assessment. Findings suggest that evolving, personality-aware LLMs can support coherent, context-sensitive interactions, enabling naturalistic agent design in HCI.

</details>


### [19] [Generative AI collective behavior needs an interactionist paradigm](https://arxiv.org/abs/2601.10567)
*Laura Ferrarotti,Gian Maria Campedelli,Roberto Dessì,Andrea Baronchelli,Giovanni Iacca,Kathleen M. Carley,Alex Pentland,Joel Z. Leibo,James Evans,Bruno Lepri*

Main category: cs.AI

TL;DR: 本文主张研究基于大语言模型（LLM）的智能体集体行为至关重要，需要建立交互主义范式来系统分析先验知识、嵌入价值观与社会情境如何共同塑造多智能体生成式AI系统中的涌现现象。


<details>
  <summary>Details</summary>
Motivation: 理解基于大语言模型的智能体集体行为是一个关键研究领域，对社会具有重要风险与收益影响。LLM的独特性质——包括基于大量预训练知识的初始化、隐含的社会先验以及通过上下文学习进行适应的能力——需要新的理论框架来研究多智能体系统中的涌现现象。

Method: 提出交互主义范式，包含替代性理论基础、方法论和分析工具，用于系统研究先验知识、嵌入价值观与社会情境如何相互作用并塑造多智能体生成式AI系统中的集体行为。

Result: 提出了四个关键发展方向：理论建设、方法创新、跨学科对话以及LLM集体系统的开发与部署策略，为这一新兴领域的研究提供了框架性指导。

Conclusion: 研究LLM智能体集体行为需要新的交互主义范式，该领域的发展需要理论、方法和跨学科对话的协同推进，以应对社会层面的风险与机遇。

Abstract: In this article, we argue that understanding the collective behavior of agents based on large language models (LLMs) is an essential area of inquiry, with important implications in terms of risks and benefits, impacting us as a society at many levels. We claim that the distinctive nature of LLMs--namely, their initialization with extensive pre-trained knowledge and implicit social priors, together with their capability of adaptation through in-context learning--motivates the need for an interactionist paradigm consisting of alternative theoretical foundations, methodologies, and analytical tools, in order to systematically examine how prior knowledge and embedded values interact with social context to shape emergent phenomena in multi-agent generative AI systems. We propose and discuss four directions that we consider crucial for the development and deployment of LLM-based collectives, focusing on theory, methods, and trans-disciplinary dialogue.

</details>


### [20] [PaperScout: An Autonomous Agent for Academic Paper Search with Process-Aware Sequence-Level Policy Optimization](https://arxiv.org/abs/2601.10029)
*Tingyue Pan,Jie Ouyang,Mingyue Cheng,Qingchuan Li,Zirui Liu,Mingfan Pan,Shuo Yu,Qi Liu*

Main category: cs.AI

TL;DR: PaperScout：一个将论文搜索重构为顺序决策过程的自主代理，采用PSPO方法解决多轮代理任务中的粒度不匹配问题


<details>
  <summary>Details</summary>
Motivation: 现有学术论文搜索方法依赖僵化的预定义工作流，难以处理复杂的条件查询。需要一种能够动态决策搜索策略的自适应代理框架。

Method: 提出PaperScout自主代理，将论文搜索重构为顺序决策过程，并引入Proximal Sequence Policy Optimization (PSPO)方法，这是一种过程感知的序列级策略优化方法，解决多轮代理任务中的粒度不匹配问题。

Result: 在合成和真实世界基准测试中，PaperScout在召回率和相关性方面显著优于基于工作流和强化学习的基线方法，验证了自适应代理框架和优化策略的有效性。

Conclusion: PaperScout通过将论文搜索重构为顺序决策过程，结合PSPO优化方法，成功解决了现有搜索方法的局限性，为复杂学术查询提供了更有效的解决方案。

Abstract: Academic paper search is a fundamental task in scientific research, yet most existing approaches rely on rigid, predefined workflows that struggle with complex, conditional queries. To address this limitation, we propose PaperScout, an autonomous agent that reformulates paper search as a sequential decision-making process. Unlike static workflows, PaperScout dynamically decides whether, when, and how to invoke search and expand tools based on accumulated retrieval context. However, training such agents presents a fundamental challenge: standard reinforcement learning methods, typically designed for single-turn tasks, suffer from a granularity mismatch when applied to multi-turn agentic tasks, where token-level optimization diverges from the granularity of sequence-level interactions, leading to noisy credit assignment. We introduce Proximal Sequence Policy Optimization (PSPO), a process-aware, sequence-level policy optimization method that aligns optimization with agent-environment interaction. Comprehensive experiments on both synthetic and real-world benchmarks demonstrate that PaperScout significantly outperforms strong workflow-driven and RL baselines in both recall and relevance, validating the effectiveness of our adaptive agentic framework and optimization strategy.

</details>


### [21] [FilDeep: Learning Large Deformations of Elastic-Plastic Solids with Multi-Fidelity Data](https://arxiv.org/abs/2601.10031)
*Jianheng Tang,Shilong Tao,Zhe Feng,Haonan Sun,Menglu Wang,Zhanxing Zhu,Yunhuai Liu*

Main category: cs.AI

TL;DR: 提出了FilDeep框架，通过同时使用低保真度（高数量）和高保真度（高精度）数据训练深度学习模型，解决大变形弹塑性固体计算中的数据数量与精度两难问题。


<details>
  <summary>Details</summary>
Motivation: 大变形弹塑性固体的科学计算在制造应用中至关重要。传统数值方法存在固有局限性，深度学习是有前景的替代方案，但现有DL技术依赖于高质量高精度数据集，而大变形问题中难以获得这样的数据，存在数据数量与精度的两难困境。

Method: 提出FilDeep框架，针对拉伸弯曲这一代表性大变形应用，同时使用低保真度（高数量低精度）和高保真度（低数量高精度）数据进行训练。设计了注意力机制的跨保真度模块，有效捕捉多保真度数据间的长程物理相互作用。

Result: FilDeep在大变形问题上实现了最先进的性能，并能高效部署于制造应用中。这是首个使用多保真度数据解决大变形问题的深度学习框架。

Conclusion: FilDeep成功解决了大变形弹塑性固体计算中的数据数量与精度两难问题，通过多保真度数据训练和注意力机制设计，为制造应用提供了有效的深度学习解决方案。

Abstract: The scientific computation of large deformations in elastic-plastic solids is crucial in various manufacturing applications. Traditional numerical methods exhibit several inherent limitations, prompting Deep Learning (DL) as a promising alternative. The effectiveness of current DL techniques typically depends on the availability of high-quantity and high-accuracy datasets, which are yet difficult to obtain in large deformation problems. During the dataset construction process, a dilemma stands between data quantity and data accuracy, leading to suboptimal performance in the DL models. To address this challenge, we focus on a representative application of large deformations, the stretch bending problem, and propose FilDeep, a Fidelity-based Deep Learning framework for large Deformation of elastic-plastic solids. Our FilDeep aims to resolve the quantity-accuracy dilemma by simultaneously training with both low-fidelity and high-fidelity data, where the former provides greater quantity but lower accuracy, while the latter offers higher accuracy but in less quantity. In FilDeep, we provide meticulous designs for the practical large deformation problem. Particularly, we propose attention-enabled cross-fidelity modules to effectively capture long-range physical interactions across MF data. To the best of our knowledge, our FilDeep presents the first DL framework for large deformation problems using MF data. Extensive experiments demonstrate that our FilDeep consistently achieves state-of-the-art performance and can be efficiently deployed in manufacturing.

</details>


### [22] [State of AI: An Empirical 100 Trillion Token Study with OpenRouter](https://arxiv.org/abs/2601.10088)
*Malika Aubakirova,Alex Atallah,Chris Clark,Justin Summerville,Anjney Midha*

Main category: cs.AI

TL;DR: 基于OpenRouter平台分析100万亿token真实LLM使用数据，发现开源模型采用率高、创意角色扮演和编程辅助需求大、智能体推理兴起，并识别出早期用户的"玻璃鞋"留存效应。


<details>
  <summary>Details</summary>
Motivation: 随着o1等推理模型的发布，LLM从单次模式生成转向多步深思推理，但实际使用情况的实证理解滞后。需要基于真实世界数据了解LLM的实际应用模式。

Method: 利用OpenRouter平台（AI推理提供商）分析超过100万亿token的真实LLM交互数据，涵盖不同任务、地域和时间，进行实证研究。

Result: 观察到开源模型采用率显著，创意角色扮演（超越生产力任务）和编程辅助类别特别受欢迎，智能体推理兴起。留存分析发现早期用户参与度持久性远超后期用户，称为"玻璃鞋"效应。

Conclusion: LLM的实际使用复杂多样，数据驱动的使用理解能为模型构建者、AI开发者和基础设施提供商提供设计部署指导，促进更好的LLM系统发展。

Abstract: The past year has marked a turning point in the evolution and real-world use of large language models (LLMs). With the release of the first widely adopted reasoning model, o1, on December 5th, 2024, the field shifted from single-pass pattern generation to multi-step deliberation inference, accelerating deployment, experimentation, and new classes of applications. As this shift unfolded at a rapid pace, our empirical understanding of how these models have actually been used in practice has lagged behind. In this work, we leverage the OpenRouter platform, which is an AI inference provider across a wide variety of LLMs, to analyze over 100 trillion tokens of real-world LLM interactions across tasks, geographies, and time. In our empirical study, we observe substantial adoption of open-weight models, the outsized popularity of creative roleplay (beyond just the productivity tasks many assume dominate) and coding assistance categories, plus the rise of agentic inference. Furthermore, our retention analysis identifies foundational cohorts: early users whose engagement persists far longer than later cohorts. We term this phenomenon the Cinderella "Glass Slipper" effect. These findings underscore that the way developers and end-users engage with LLMs "in the wild" is complex and multifaceted. We discuss implications for model builders, AI developers, and infrastructure providers, and outline how a data-driven understanding of usage can inform better design and deployment of LLM systems.

</details>


### [23] [MATRIX AS PLAN: Structured Logical Reasoning with Feedback-Driven Replanning](https://arxiv.org/abs/2601.10101)
*Ke Chen,Jiandian Zeng,Zihao Peng,Guo Li,Guangxue Zhang,Tian Wang*

Main category: cs.AI

TL;DR: MatrixCoT：一种基于矩阵规划的结构化思维链框架，通过规范化自然语言表达、添加显式引用字段和矩阵规划方法，增强LLM在符号推理任务中的鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：思维链提示在依赖符号表达和严格演绎规则的逻辑推理任务上表现不足；神经符号方法依赖外部求解器但格式敏感易失败；LLM驱动方法缺乏结构化表示和过程级纠错机制。需要增强LLM的逻辑推理能力。

Method: 提出MatrixCoT框架：1）规范化自然语言表达并添加类型标注；2）附加显式引用字段；3）引入基于矩阵的规划方法以保持步骤间的全局关系；4）添加反馈驱动的重新规划机制，在语义等价约束下识别遗漏和缺陷，重写并压缩依赖矩阵。

Result: 在五个逻辑推理基准和五个LLM上的实验表明，MatrixCoT在不依赖外部求解器的情况下，增强了处理复杂符号推理任务时的鲁棒性和可解释性，同时保持了有竞争力的性能。

Conclusion: MatrixCoT通过结构化思维链和矩阵规划，有效解决了现有方法在符号推理任务中的局限性，为增强LLM的逻辑推理能力提供了新方向。

Abstract: As knowledge and semantics on the web grow increasingly complex, enhancing Large Language Models (LLMs) comprehension and reasoning capabilities has become particularly important. Chain-of-Thought (CoT) prompting has been shown to enhance the reasoning capabilities of LLMs. However, it still falls short on logical reasoning tasks that rely on symbolic expressions and strict deductive rules. Neuro-symbolic methods address this gap by enforcing formal correctness through external solvers. Yet these solvers are highly format-sensitive, and small instabilities in model outputs can lead to frequent processing failures. LLM-driven approaches avoid parsing brittleness, but they lack structured representations and process-level error-correction mechanisms. To further enhance the logical reasoning capabilities of LLMs, we propose MatrixCoT, a structured CoT framework with a matrix-based plan. Specifically, we normalize and type natural language expressions, attach explicit citation fields, and introduce a matrix-based planning method to preserve global relations among steps. The plan becomes a verifiable artifact, making execution more stable. For verification, we also add a feedback-driven replanning mechanism. Under semantic-equivalence constraints, it identifies omissions and defects, rewrites and compresses the dependency matrix, and produces a more trustworthy final answer. Experiments on five logical-reasoning benchmarks and five LLMs show that, without relying on external solvers, MatrixCoT enhances both robustness and interpretability when tackling complex symbolic reasoning tasks, while maintaining competitive performance.

</details>


### [24] [Following the Teacher's Footsteps: Scheduled Checkpoint Distillation for Domain-Specific LLMs](https://arxiv.org/abs/2601.10114)
*Cheng Feng,Chaoliang Zhong,Jun Sun,Yusuke Oishi*

Main category: cs.AI

TL;DR: 本文提出一种新的知识蒸馏方法SCD，通过模拟教师模型在领域任务上的收敛过程来减少学生模型在教师优势子域上的缺陷，同时使用自适应加权机制保持学生在自身优势子域上的优势，使学生模型能在特定领域任务上达到甚至超越教师模型的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型部署困难，而将微调后的LLM蒸馏到小型学生模型时，师生模型之间的容量差距常导致性能不佳。核心问题是：学生模型何时以及如何在特定领域任务上匹配甚至超越教师模型？

Method: 提出理论洞察：学生模型要超越教师，需要其在学生优势子域(SFS)上的优势超过其在教师优势子域(TFS)上的缺陷。基于此提出SCD方法：1) 通过模拟教师模型在SFT过程中的收敛过程来减少TFS缺陷；2) 使用样本级自适应加权(AW)机制来保持学生在SFS上的优势。

Result: 在多种领域任务（包括QA、NER、多语言文本分类）上的实验表明，该方法持续优于现有蒸馏方法，使学生模型能够匹配甚至超越其微调教师模型的性能。

Conclusion: 通过理论分析和提出的SCD方法，证明了学生模型在特定领域任务上可以超越教师模型，关键在于平衡学生在不同子域上的优势与缺陷，为高效部署领域专用模型提供了有效方案。

Abstract: Large language models (LLMs) are challenging to deploy for domain-specific tasks due to their massive scale. While distilling a fine-tuned LLM into a smaller student model is a promising alternative, the capacity gap between teacher and student often leads to suboptimal performance. This raises a key question: when and how can a student model match or even surpass its teacher on domain-specific tasks? In this work, we propose a novel theoretical insight: a student can outperform its teacher if its advantage on a Student-Favored Subdomain (SFS) outweighs its deficit on the Teacher-Favored Subdomain (TFS). Guided by this insight, we propose Scheduled Checkpoint Distillation (SCD), which reduces the TFS deficit by emulating the teacher's convergence process during supervised fine-tuning (SFT) on the domain task, and a sample-wise Adaptive Weighting (AW) mechanism to preserve student strengths on SFS. Experiments across diverse domain tasks--including QA, NER, and text classification in multiple languages--show that our method consistently outperforms existing distillation approaches, allowing the student model to match or even exceed the performance of its fine-tuned teacher.

</details>


### [25] [M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints](https://arxiv.org/abs/2601.10131)
*Yizhan Li,Florence Cloutier,Sifan Wu,Ali Parviz,Boris Knyazev,Yan Zhang,Glen Berseth,Bang Liu*

Main category: cs.AI

TL;DR: MolGen是一个两阶段分子生成框架，通过片段级检索增强和强化学习优化，在多个物理化学性质约束下生成满足精确数值要求的分子。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在精确多目标控制和数值推理方面存在困难，缺乏外部结构和反馈。需要一种能够生成满足多个精确数值约束分子的方法。

Method: 两阶段框架：第一阶段为原型生成，使用多智能体推理器进行检索锚定的片段级编辑；第二阶段为RL优化，使用GRPO训练的片段级优化器进行单跳或多跳细化，最小化属性误差。

Result: 在QED、LogP、分子量和HOMO、LUMO两组性质约束下的生成实验中，MolGen在有效性和多属性目标精确满足方面表现优于强LLM和图算法。

Conclusion: MolGen通过片段级推理和可控细化，能够更好地生成满足精确数值约束的分子，在多属性约束分子生成任务中取得显著改进。

Abstract: Generating molecules that satisfy precise numeric constraints over multiple physicochemical properties is critical and challenging. Although large language models (LLMs) are expressive, they struggle with precise multi-objective control and numeric reasoning without external structure and feedback. We introduce \textbf{M olGen}, a fragment-level, retrieval-augmented, two-stage framework for molecule generation under multi-property constraints. Stage I : Prototype generation: a multi-agent reasoner performs retrieval-anchored, fragment-level edits to produce a candidate near the feasible region. Stage II : RL-based fine-grained optimization: a fragment-level optimizer trained with Group Relative Policy Optimization (GRPO) applies one- or multi-hop refinements to explicitly minimize the property errors toward our target while regulating edit complexity and deviation from the prototype. A large, automatically curated dataset with reasoning chains of fragment edits and measured property deltas underpins both stages, enabling deterministic, reproducible supervision and controllable multi-hop reasoning. Unlike prior work, our framework better reasons about molecules by leveraging fragments and supports controllable refinement toward numeric targets. Experiments on generation under two sets of property constraints (QED, LogP, Molecular Weight and HOMO, LUMO) show consistent gains in validity and precise satisfaction of multi-property targets, outperforming strong LLMs and graph-based algorithms.

</details>


### [26] [Is More Context Always Better? Examining LLM Reasoning Capability for Time Interval Prediction](https://arxiv.org/abs/2601.10132)
*Yanan Cao,Farnaz Fallahi,Murali Mohana Krishna Dandu,Lalitesh Morishetti,Kai Zhao,Luyi Ma,Sinduja Subramaniam,Jianpeng Xu,Evren Korpeoglu,Kaushiki Nag,Sushant Kumar,Kannan Achan*

Main category: cs.AI

TL;DR: LLMs在预测用户重复行为时间间隔方面表现有限，虽然优于简单统计模型但不如专用机器学习模型，且过多上下文信息反而会降低预测性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在多个领域展现出强大的推理和预测能力，但其从结构化行为数据中推断时间规律的能力尚未得到充分探索。本研究旨在探究LLMs是否能预测重复用户行为（如重复购买）之间的时间间隔，以及不同级别的上下文信息如何影响其预测表现。

Method: 使用简单的代表性重复购买场景，在零样本设置下对最先进的LLMs进行基准测试，并与统计模型和机器学习模型进行比较。研究考察了不同层次上下文信息对LLM预测准确性的影响。

Result: 1. LLMs虽然优于轻量级统计基线模型，但始终不如专用机器学习模型，显示出其在捕捉定量时间结构方面的有限能力。
2. 适度的上下文信息可以提高LLM准确性，但添加更多用户级别细节反而会降低性能，挑战了"更多上下文带来更好推理"的假设。

Conclusion: 研究揭示了当前LLMs在结构化时间推理方面的基本局限性，为设计未来上下文感知的混合模型提供了指导，这些模型需要整合统计精度与语言灵活性。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in reasoning and prediction across different domains. Yet, their ability to infer temporal regularities from structured behavioral data remains underexplored. This paper presents a systematic study investigating whether LLMs can predict time intervals between recurring user actions, such as repeated purchases, and how different levels of contextual information shape their predictive behavior. Using a simple but representative repurchase scenario, we benchmark state-of-the-art LLMs in zero-shot settings against both statistical and machine-learning models. Two key findings emerge. First, while LLMs surpass lightweight statistical baselines, they consistently underperform dedicated machine-learning models, showing their limited ability to capture quantitative temporal structure. Second, although moderate context can improve LLM accuracy, adding further user-level detail degrades performance. These results challenge the assumption that "more context leads to better reasoning". Our study highlights fundamental limitations of today's LLMs in structured temporal inference and offers guidance for designing future context-aware hybrid models that integrate statistical precision with linguistic flexibility.

</details>


### [27] [History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis](https://arxiv.org/abs/2601.10143)
*Haochong Xia,Yao Long Teng,Regan Tan,Molei Qin,Xinrun Wang,Bo An*

Main category: cs.AI

TL;DR: 提出一个漂移感知的数据流系统，通过机器学习自适应控制数据生成过程，解决金融量化中训练与真实表现之间的差距问题。


<details>
  <summary>Details</summary>
Motivation: 金融量化中，概念漂移和分布非平稳性导致训练数据与真实市场表现存在差距，静态历史数据训练容易过拟合，需要能够适应市场变化的自适应数据生成方法。

Method: 设计一个漂移感知数据流系统，包含参数化数据操作模块（单股变换、多股混合、筛选操作）和自适应规划调度器，采用基于梯度的双层优化控制整个系统，统一数据增强、课程学习和数据工作流管理。

Result: 在预测和强化学习交易任务上的实验表明，该框架增强了模型鲁棒性，提高了风险调整后的收益。

Conclusion: 该系统为金融数据提供了一种通用的自适应数据管理和学习引导工作流自动化方法。

Abstract: In quantitative finance, the gap between training and real-world performance-driven by concept drift and distributional non-stationarity-remains a critical obstacle for building reliable data-driven systems. Models trained on static historical data often overfit, resulting in poor generalization in dynamic markets. The mantra "History Is Not Enough" underscores the need for adaptive data generation that learns to evolve with the market rather than relying solely on past observations. We present a drift-aware dataflow system that integrates machine learning-based adaptive control into the data curation process. The system couples a parameterized data manipulation module comprising single-stock transformations, multi-stock mix-ups, and curation operations, with an adaptive planner-scheduler that employs gradient-based bi-level optimization to control the system. This design unifies data augmentation, curriculum learning, and data workflow management under a single differentiable framework, enabling provenance-aware replay and continuous data quality monitoring. Extensive experiments on forecasting and reinforcement learning trading tasks demonstrate that our framework enhances model robustness and improves risk-adjusted returns. The system provides a generalizable approach to adaptive data management and learning-guided workflow automation for financial data.

</details>


### [28] [DecisionLLM: Large Language Models for Long Sequence Decision Exploration](https://arxiv.org/abs/2601.10148)
*Xiaowei Lv,Zhilin Zhang,Yijun Li,Yusen Huo,Siyuan Ju,Xuyan Li,Chunxiang Hong,Tianyu Wang,Yongcai Wang,Peng Sun,Chuan Yu,Jian Xu,Bo Zheng*

Main category: cs.AI

TL;DR: 该论文提出DecisionLLM，将大型语言模型应用于离线决策任务，通过将轨迹数据作为独立模态与自然语言任务描述对齐，解决了LLM无法理解连续数值的问题，在迷宫导航和竞价场景中显著超越传统决策Transformer。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂推理和规划任务中表现出色，与决策Transformer共享相同的Transformer基础架构，但规模更大。这启发了研究者探索LLM是否能在长序列决策问题中解锁新的性能水平。然而，LLM天生无法理解连续数值，因为当数值表示为文本字符串时，它们缺乏对数值大小和顺序的本机理解。

Method: 提出DecisionLLM框架，将轨迹数据视为独立模态，学习将轨迹数据与自然语言任务描述对齐，使模型能够在统一框架内自回归预测未来决策。建立了该范式的缩放定律，表明性能取决于三个因素：模型规模、数据量和数据质量。

Result: 在离线实验基准和竞价场景中，DecisionLLM表现出强大性能。具体来说，DecisionLLM-3B在Maze2D umaze-v1上比传统决策Transformer提升69.4，在AuctionNet上提升0.085。扩展了AIGB范式，为在线竞价探索指明了有前景的方向。

Conclusion: 该工作成功将大型语言模型应用于离线决策任务，通过将轨迹作为独立模态解决了LLM理解连续数值的挑战。建立的缩放定律为未来研究提供了指导，DecisionLLM在多个基准上的优异表现证明了该方法的有效性，为在线竞价等应用开辟了新方向。

Abstract: Long-sequence decision-making, which is usually addressed through reinforcement learning (RL), is a critical component for optimizing strategic operations in dynamic environments, such as real-time bidding in computational advertising. The Decision Transformer (DT) introduced a powerful paradigm by framing RL as an autoregressive sequence modeling problem. Concurrently, Large Language Models (LLMs) have demonstrated remarkable success in complex reasoning and planning tasks. This inspires us whether LLMs, which share the same Transformer foundation, but operate at a much larger scale, can unlock new levels of performance in long-horizon sequential decision-making problem. This work investigates the application of LLMs to offline decision making tasks. A fundamental challenge in this domain is the LLMs' inherent inability to interpret continuous values, as they lack a native understanding of numerical magnitude and order when values are represented as text strings. To address this, we propose treating trajectories as a distinct modality. By learning to align trajectory data with natural language task descriptions, our model can autoregressively predict future decisions within a cohesive framework we term DecisionLLM. We establish a set of scaling laws governing this paradigm, demonstrating that performance hinges on three factors: model scale, data volume, and data quality. In offline experimental benchmarks and bidding scenarios, DecisionLLM achieves strong performance. Specifically, DecisionLLM-3B outperforms the traditional Decision Transformer (DT) by 69.4 on Maze2D umaze-v1 and by 0.085 on AuctionNet. It extends the AIGB paradigm and points to promising directions for future exploration in online bidding.

</details>


### [29] [MMPG: MoE-based Adaptive Multi-Perspective Graph Fusion for Protein Representation Learning](https://arxiv.org/abs/2601.10157)
*Yusong Wang,Jialun Shen,Zhihao Wu,Yicheng Xu,Shiyin Tan,Mingkun Xu,Changshuo Wang,Zixing Song,Prayag Tiwari*

Main category: cs.AI

TL;DR: MMPG是一个多视角蛋白质图神经网络框架，通过混合专家模型融合物理、化学和几何视角的蛋白质图表示，提升蛋白质表示学习性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于GNN的蛋白质表示学习方法通常采用单视角图构建策略，只能捕捉残基相互作用的部分特性，导致蛋白质表示不完整。需要多视角融合方法来获得更全面的蛋白质表示。

Method: 提出MMPG框架：1）从物理、化学和几何三个视角构建蛋白质图；2）开发混合专家模块动态路由不同视角到专门专家；3）专家学习内在特征和跨视角交互；4）整合多层次信息生成蛋白质表示。

Result: 定量验证MoE能自动专业化专家建模不同层次的交互：从个体表示到成对跨视角协同，再到全局共识。在四个不同的下游蛋白质任务上实现了先进性能。

Conclusion: MMPG通过多视角图构建和混合专家融合，能够生成更优的蛋白质表示，在多个蛋白质任务上表现优异，证明了多视角融合在蛋白质表示学习中的重要性。

Abstract: Graph Neural Networks (GNNs) have been widely adopted for Protein Representation Learning (PRL), as residue interaction networks can be naturally represented as graphs. Current GNN-based PRL methods typically rely on single-perspective graph construction strategies, which capture partial properties of residue interactions, resulting in incomplete protein representations. To address this limitation, we propose MMPG, a framework that constructs protein graphs from multiple perspectives and adaptively fuses them via Mixture of Experts (MoE) for PRL. MMPG constructs graphs from physical, chemical, and geometric perspectives to characterize different properties of residue interactions. To capture both perspective-specific features and their synergies, we develop an MoE module, which dynamically routes perspectives to specialized experts, where experts learn intrinsic features and cross-perspective interactions. We quantitatively verify that MoE automatically specializes experts in modeling distinct levels of interaction from individual representations, to pairwise inter-perspective synergies, and ultimately to a global consensus across all perspectives. Through integrating this multi-level information, MMPG produces superior protein representations and achieves advanced performance on four different downstream protein tasks.

</details>


### [30] [CtD: Composition through Decomposition in Emergent Communication](https://arxiv.org/abs/2601.10169)
*Boaz Carmeli,Ron Meir,Yonatan Belinkov*

Main category: cs.AI

TL;DR: 论文提出"通过分解实现组合"的方法，让神经网络智能体通过两阶段训练获得组合泛化能力，实现对新图像的零样本描述。


<details>
  <summary>Details</summary>
Motivation: 组合性是人类认知的关键机制，能够系统地将已知概念组合成新方式。研究旨在探索人工神经网络如何获得和利用组合泛化能力来描述未见过的图像。

Method: 提出"通过分解实现组合"的两阶段训练方法：1) "分解"阶段：智能体在多目标协调游戏中通过交互学习代码本，将图像分解为基本概念；2) "组合"阶段：利用代码本将基本概念组合成复杂短语来描述新图像。

Result: 智能体成功获得了组合泛化能力，能够在"组合"阶段实现零样本泛化，即无需额外训练就能描述新图像。

Conclusion: 研究表明人工神经网络可以通过"分解-组合"的认知机制获得组合泛化能力，为理解人类认知机制和开发更智能的AI系统提供了新思路。

Abstract: Compositionality is a cognitive mechanism that allows humans to systematically combine known concepts in novel ways. This study demonstrates how artificial neural agents acquire and utilize compositional generalization to describe previously unseen images. Our method, termed "Composition through Decomposition", involves two sequential training steps. In the 'Decompose' step, the agents learn to decompose an image into basic concepts using a codebook acquired during interaction in a multi-target coordination game. Subsequently, in the 'Compose' step, the agents employ this codebook to describe novel images by composing basic concepts into complex phrases. Remarkably, we observe cases where generalization in the `Compose' step is achieved zero-shot, without the need for additional training.

</details>


### [31] [How does downsampling affect needle electromyography signals? A generalisable workflow for understanding downsampling effects on high-frequency time series](https://arxiv.org/abs/2601.10191)
*Mathieu Cherpitel,Janne Luijten,Thomas Bäck,Camiel Verhamme,Martijn Tannemaat,Anna Kononova*

Main category: cs.AI

TL;DR: 该研究提出了一个评估下采样对高频时间序列信息损失的系统工作流程，结合形状失真度量和分类性能分析，用于针肌电图信号分析，以平衡计算负载与诊断信息保留。


<details>
  <summary>Details</summary>
Motivation: 针肌电图(nEMG)信号的高采样率和异质性给基于特征的机器学习模型带来计算挑战，特别是近实时分析。下采样是潜在解决方案，但其对诊断信号内容和分类性能的影响尚不明确。

Method: 提出系统工作流程，结合形状失真度量、基于特征的机器学习模型分类结果和特征空间分析，量化不同下采样算法和因素对波形完整性和预测性能的影响。使用三类NMD分类任务进行实验评估。

Result: 工作流程能识别在显著减少计算负载的同时保留诊断信息的下采样配置。形状感知下采样算法优于标准抽取，能更好地保留峰值结构和整体信号形态。

Conclusion: 研究为选择支持近实时nEMG分析的下采样配置提供实用指导，并提出了一个可推广的工作流程，可用于平衡其他高频时间序列应用中的数据减少与模型性能。

Abstract: Automated analysis of needle electromyography (nEMG) signals is emerging as a tool to support the detection of neuromuscular diseases (NMDs), yet the signals' high and heterogeneous sampling rates pose substantial computational challenges for feature-based machine-learning models, particularly for near real-time analysis. Downsampling offers a potential solution, but its impact on diagnostic signal content and classification performance remains insufficiently understood. This study presents a workflow for systematically evaluating information loss caused by downsampling in high-frequency time series. The workflow combines shape-based distortion metrics with classification outcomes from available feature-based machine learning models and feature space analysis to quantify how different downsampling algorithms and factors affect both waveform integrity and predictive performance. We use a three-class NMD classification task to experimentally evaluate the workflow. We demonstrate how the workflow identifies downsampling configurations that preserve diagnostic information while substantially reducing computational load. Analysis of shape-based distortion metrics showed that shape-aware downsampling algorithms outperform standard decimation, as they better preserve peak structure and overall signal morphology. The results provide practical guidance for selecting downsampling configurations that enable near real-time nEMG analysis and highlight a generalisable workflow that can be used to balance data reduction with model performance in other high-frequency time-series applications as well.

</details>


### [32] [GFM4GA: Graph Foundation Model for Group Anomaly Detection](https://arxiv.org/abs/2601.10193)
*Jiujiu Chen,Weijun Zeng,Shaofeng Hu,Sihong Xie,Hui Xiong*

Main category: cs.AI

TL;DR: 提出GFM4GA，一种基于图基础模型的群体异常检测方法，通过双层次对比学习预训练和参数约束的少样本微调，显著提升群体异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 群体异常检测在网络应用中很重要，但面临异常模式多样化的挑战。现有的图基础模型(GFMs)在个体异常检测中表现良好，但无法推广到群体异常检测，因为群体异常需要整体检测，且异常群体中的个体可能看起来正常。

Method: 提出GFM4GA模型，采用基于特征估计和群体提取的双层次对比学习进行预训练，捕捉潜在的群体异常结构和特征不一致性。在下游任务中，使用参数约束和群体异常比例加权的少样本设置进行微调，并通过标记异常邻居确定的群体上下文扩展对未见群体异常的适应能力。

Result: 实验表明GFM4GA超越了现有的群体异常检测器和用于个体异常的GFMs，在AUROC和AUPRC上平均分别提升了2.85%和2.55%。

Conclusion: GFM4GA成功解决了图基础模型在群体异常检测中的局限性，通过创新的预训练和微调策略，在少样本设置下实现了优越的群体异常检测性能。

Abstract: Group anomaly detection is crucial in many network applications, but faces challenges due to diverse anomaly patterns. Motivated by the success of large language models (LLMs) in natural language processing, graph foundation models (GFMs) is proposed to handle few-shot learning task with fewer labeling efforts. GFMs have been successfully applied to detection of individual anomalies but cannot be generalized to group anomalies, as group anomaly patterns must be detected as a whole and individuals in an abnormal group can look rather normal. Therefore, we propose GFM4GA, a novel graph foundation model for group anomaly detection. The pipeline is pretrained via dual-level contrastive learning based on feature-based estimation and group extraction, to capture potential group anomaly structure and feature inconsistencies. In the downstream tasks, the pipeline is finetuned in parameter-constrained and group-anomaly-proportion weighted few-shot settings, and its adaptive ability to unseen group anomalies expanded via group contexts determined by labeled anomaly neighbors. Experiments show that GFM4GA surpasses group anomaly detectors and GFMs for individual anomalies, achieving average improvements of 2.85% in AUROC and 2.55% in AUPRC.

</details>


### [33] [Topo-RAG: Topology-aware retrieval for hybrid text-table documents](https://arxiv.org/abs/2601.10215)
*Alex Dantart,Marco Kóvacs-Navarro*

Main category: cs.AI

TL;DR: Topo-RAG提出了一种双架构检索增强生成框架，将文本叙事与表格结构分开处理，相比传统线性化方法在混合查询上提升了18.4%的性能。


<details>
  <summary>Details</summary>
Motivation: 企业数据集通常是文本和表格的复杂混合体，当前RAG系统将所有内容线性化为文本字符串的方法在数学上是不充分的，无法捕捉表格的空间关系。

Method: 采用双架构设计：文本内容通过传统密集检索器处理，表格结构通过Cell-Aware Late Interaction机制处理，保留其空间拓扑关系。

Result: 在模拟真实企业复杂性的SEC-25合成语料库上评估，Topo-RAG在混合查询上的nDCG@10比标准线性化方法提升了18.4%。

Conclusion: Topo-RAG挑战了"一切皆文本"的假设，通过尊重数据拓扑结构实现了更好的信息检索，这不仅关乎搜索效果，更关乎理解信息的形状。

Abstract: In enterprise datasets, documents are rarely pure. They are not just text, nor just numbers; they are a complex amalgam of narrative and structure. Current Retrieval-Augmented Generation (RAG) systems have attempted to address this complexity with a blunt tool: linearization. We convert rich, multidimensional tables into simple Markdown-style text strings, hoping that an embedding model will capture the geometry of a spreadsheet in a single vector. But it has already been shown that this is mathematically insufficient.
  This work presents Topo-RAG, a framework that challenges the assumption that "everything is text". We propose a dual architecture that respects the topology of the data: we route fluid narrative through traditional dense retrievers, while tabular structures are processed by a Cell-Aware Late Interaction mechanism, preserving their spatial relationships. Evaluated on SEC-25, a synthetic enterprise corpus that mimics real-world complexity, Topo-RAG demonstrates an 18.4% improvement in nDCG@10 on hybrid queries compared to standard linearization approaches. It's not just about searching better; it's about understanding the shape of information.

</details>


### [34] [TRIM: Hybrid Inference via Targeted Stepwise Routing in Multi-Step Reasoning Tasks](https://arxiv.org/abs/2601.10245)
*Vansh Kapoor,Aman Gupta,Hao Chen,Anurag Beniwal,Jing Huang,Aviral Kumar*

Main category: cs.AI

TL;DR: TRIM提出在数学推理任务中进行步骤级路由，仅将关键步骤分配给大模型处理，而让小模型处理常规步骤，显著提升推理效率


<details>
  <summary>Details</summary>
Motivation: 当前LLM路由方法将整个查询分配给单一模型，将所有推理步骤视为同等重要。多步推理任务容易产生级联失败，单个错误步骤会导致整个解决方案崩溃。需要更精细的步骤级路由来提升效率

Method: TRIM在步骤级别操作：使用过程奖励模型识别错误步骤，基于步骤级不确定性和预算约束进行路由决策。开发了从简单阈值策略到更复杂策略的多种路由方法，考虑长期精度-成本权衡和步骤正确性估计的不确定性

Result: 在MATH-500上，最简单的阈值策略超越先前路由方法，成本效率提高5倍；更高级的策略仅使用20%的昂贵模型token就能匹配强大昂贵模型的性能。在AIME等更难基准测试中，TRIM实现高达6倍的成本效率提升

Conclusion: 步骤级难度代表了推理的基本特征，TRIM的步骤级干预方法能够从根本上改变推理效率，将昂贵调用限制在那些需要更强模型防止级联错误的关键步骤上

Abstract: Multi-step reasoning tasks like mathematical problem solving are vulnerable to cascading failures, where a single incorrect step leads to complete solution breakdown. Current LLM routing methods assign entire queries to one model, treating all reasoning steps as equal. We propose TRIM (Targeted routing in multi-step reasoning tasks), which routes only critical steps$\unicode{x2013}$those likely to derail the solution$\unicode{x2013}$to larger models while letting smaller models handle routine continuations. Our key insight is that targeted step-level interventions can fundamentally transform inference efficiency by confining expensive calls to precisely those steps where stronger models prevent cascading errors. TRIM operates at the step-level: it uses process reward models to identify erroneous steps and makes routing decisions based on step-level uncertainty and budget constraints. We develop several routing strategies within TRIM, ranging from a simple threshold-based policy to more expressive policies that reason about long-horizon accuracy-cost trade-offs and uncertainty in step-level correctness estimates. On MATH-500, even the simplest thresholding strategy surpasses prior routing methods with 5x higher cost efficiency, while more advanced policies match the strong, expensive model's performance using 80% fewer expensive model tokens. On harder benchmarks such as AIME, TRIM achieves up to 6x higher cost efficiency. All methods generalize effectively across math reasoning tasks, demonstrating that step-level difficulty represents fundamental characteristics of reasoning.

</details>


### [35] [NoReGeo: Non-Reasoning Geometry Benchmark](https://arxiv.org/abs/2601.10254)
*Irina Abdullaeva,Anton Vasiliuk,Elizaveta Goncharova,Temurbek Rahmatullaev,Zagorulko Ivan,Maxim Kurkin,Andrey Kuznetsov*

Main category: cs.AI

TL;DR: NoReGeo是一个评估大语言模型内在几何理解能力的新基准，专注于空间关系和几何属性识别而非代数推理，结果显示即使最先进模型在二元分类任务中最高准确率仅65%


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估基于推理的几何能力（使用代数方法），但缺乏对LLMs是否真正内在地编码空间关系和识别几何属性的评估。需要专门基准来测试模型的原生几何理解能力。

Method: 创建包含2,500个简单几何问题的NoReGeo基准，涵盖25个类别，每个问题设计为仅通过原生几何理解即可解决（假设已知对象位置）。评估包括GPT-4在内的多个先进模型，并进行消融实验。

Result: 即使最先进的模型（如GPT-4）在二元分类任务中最高准确率仅达到65%。消融实验表明，仅通过微调无法获得几何理解能力，需要专门的训练方法。

Conclusion: 当前LLMs在原生几何概念理解方面存在显著差距，几何理解能力不会通过微调自然涌现，需要从开始就采用专门的训练方法。NoReGeo为未来开发具有真正几何认知能力的模型奠定了基础。

Abstract: We present NoReGeo, a novel benchmark designed to evaluate the intrinsic geometric understanding of large language models (LLMs) without relying on reasoning or algebraic computation. Unlike existing benchmarks that primarily assess models' proficiency in reasoning-based geometry-where solutions are derived using algebraic methods-NoReGeo focuses on evaluating whether LLMs can inherently encode spatial relationships and recognize geometric properties directly. Our benchmark comprises 2,500 trivial geometric problems spanning 25 categories, each carefully crafted to be solvable purely through native geometric understanding, assuming known object locations. We assess a range of state-of-the-art models on NoReGeo, including frontier models like GPT-4, observing that even the most advanced systems achieve an overall maximum of 65% accuracy in binary classification tasks. Further, our ablation experiments demonstrate that such geometric understanding does not emerge through fine-tuning alone, indicating that effective training for geometric comprehension requires a specialized approach from the outset. Our findings highlight a significant gap in current LLMs' ability to natively grasp geometric concepts, providing a foundation for future research toward models with true geometric cognition.

</details>


### [36] [Evidence-Augmented Policy Optimization with Reward Co-Evolution for Long-Context Reasoning](https://arxiv.org/abs/2601.10306)
*Xin Guan,Zijian Li,Shen Huang,Pengjun Xie,Jingren Zhou,Jiuxin Cao*

Main category: cs.AI

TL;DR: EAPO提出了一种证据增强的策略优化方法，通过密集的过程监督来改进长上下文推理中的证据检索质量，解决了传统RL在长上下文场景中奖励稀疏的问题。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在长上下文推理中存在奖励稀疏的问题，无法有效惩罚无根据的"幸运猜测"，导致关键的"大海捞针"证据检索过程缺乏监督。

Method: 1) 建立证据增强推理范式，通过树结构证据采样验证精确证据提取是长上下文推理的关键瓶颈；2) 提出EAPO算法，使用奖励模型计算组相对证据奖励，提供密集过程监督；3) 引入自适应奖励-策略协同进化机制，迭代优化奖励模型以确保精确的过程指导。

Result: 在八个基准测试上的综合评估表明，EAPO相比最先进的基线方法显著提升了长上下文推理性能。

Conclusion: EAPO通过密集的过程监督和奖励-策略协同进化机制，有效解决了长上下文推理中的证据检索监督问题，显著提升了推理性能。

Abstract: While Reinforcement Learning (RL) has advanced LLM reasoning, applying it to long-context scenarios is hindered by sparsity of outcome rewards. This limitation fails to penalize ungrounded "lucky guesses," leaving the critical process of needle-in-a-haystack evidence retrieval largely unsupervised. To address this, we propose EAPO (Evidence-Augmented Policy Optimization). We first establish the Evidence-Augmented Reasoning paradigm, validating via Tree-Structured Evidence Sampling that precise evidence extraction is the decisive bottleneck for long-context reasoning. Guided by this insight, EAPO introduces a specialized RL algorithm where a reward model computes a Group-Relative Evidence Reward, providing dense process supervision to explicitly improve evidence quality. To sustain accurate supervision throughout training, we further incorporate an Adaptive Reward-Policy Co-Evolution mechanism. This mechanism iteratively refines the reward model using outcome-consistent rollouts, sharpening its discriminative capability to ensure precise process guidance. Comprehensive evaluations across eight benchmarks demonstrate that EAPO significantly enhances long-context reasoning performance compared to SOTA baselines.

</details>


### [37] [C-GRASP: Clinically-Grounded Reasoning for Affective Signal Processing](https://arxiv.org/abs/2601.10342)
*Cheng Lin Cheng,Ting Chuan Lin,Chai Kai Chang*

Main category: cs.AI

TL;DR: C-GRASP是一个基于临床推理的HRV分析框架，通过八步可追溯推理流程和Z-score优先级层次结构，有效解决LLM在HRV解释中的生理幻觉问题，实现透明的情感计算决策支持。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在心率变异性解释中存在生理幻觉问题，包括呼吸性窦性心律失常污染、非线性指标短数据不稳定性，以及忽视个体化基线而依赖群体标准。这些问题阻碍了LLM在HRV分析中的临床应用。

Method: 提出C-GRASP（临床基础推理情感信号处理）框架，采用护栏增强的RAG流程，将HRV解释分解为八个可追溯推理步骤。核心是Z-score优先级层次结构，强调个体化基线变化优于规范统计。通过自动化RSA感知护栏减轻频谱幻觉。

Result: 在DREAMER数据集的414个试验中，C-GRASP与高规模推理模型（如MedGemma3-thinking）结合，在4类情感分类中达到37.3%准确率，临床推理一致性得分为69.6%。消融研究证实个体化Delta Z-score模块是关键逻辑锚点。

Conclusion: C-GRASP将情感计算从黑盒分类转变为透明、基于证据的临床决策支持，为生物医学工程中更安全的AI集成铺平道路。个体化基线模块有效防止了原生LLM常见的"群体偏差"。

Abstract: Heart rate variability (HRV) is a pivotal noninvasive marker for autonomic monitoring; however, applying Large Language Models (LLMs) to HRV interpretation is hindered by physiological hallucinations. These include respiratory sinus arrhythmia (RSA) contamination, short-data instability in nonlinear metrics, and the neglect of individualized baselines in favor of population norms. We propose C-GRASP (Clinically-Grounded Reasoning for Affective Signal Processing), a guardrailed RAG-enhanced pipeline that decomposes HRV interpretation into eight traceable reasoning steps. Central to C-GRASP is a Z-score Priority Hierarchy that enforces the weighting of individualized baseline shifts over normative statistics. The system effectively mitigates spectral hallucinations through automated RSA-aware guardrails, preventing contamination of frequency-domain indices. Evaluated on 414 trials from the DREAMER dataset, C-GRASP integrated with high-scale reasoning models (e.g., MedGemma3-thinking) achieved superior performance in 4-class emotion classification (37.3% accuracy) and a Clinical Reasoning Consistency (CRC) score of 69.6%. Ablation studies confirm that the individualized Delta Z-score module serves as the critical logical anchor, preventing the "population bias" common in native LLMs. Ultimately, C-GRASP transitions affective computing from black-box classification to transparent, evidence-based clinical decision support, paving the way for safer AI integration in biomedical engineering.

</details>


### [38] [LatentRefusal: Latent-Signal Refusal for Unanswerable Text-to-SQL Queries](https://arxiv.org/abs/2601.10398)
*Xuancheng Ren,Shijing Hu,Zhihui Lu,Jiangqi Huang,Qiang Duan*

Main category: cs.AI

TL;DR: LatentRefusal：基于LLM隐藏激活信号预测查询可答性的文本到SQL安全拒绝机制，通过Tri-Residual Gated Encoder架构增强问题-模式不匹配信号检测，实现高效安全层


<details>
  <summary>Details</summary>
Motivation: 在LLM文本到SQL系统中，不可回答和未充分指定的用户查询可能生成错误文本或可执行程序，导致误导性结果或违反安全约束，这是安全部署的主要障碍。现有拒绝策略要么依赖输出级指令遵循（易受模型幻觉影响），要么依赖输出不确定性估计（增加复杂性和开销）。

Method: 将安全拒绝形式化为可答性门控问题，提出LatentRefusal机制，从LLM中间隐藏激活预测查询可答性。引入Tri-Residual Gated Encoder轻量探测架构，抑制模式噪声并放大问题-模式不匹配的稀疏局部线索。

Result: 在四个基准测试中，LatentRefusal将平均F1提升至88.5%，同时仅增加约2毫秒的探测开销。广泛的实证评估、消融研究和可解释性分析证明了该方法的有效性。

Conclusion: LatentRefusal为文本到SQL系统提供了一个可附加的高效安全层，能够有效处理不可回答和模糊查询，提高系统安全性而不显著增加计算开销。

Abstract: In LLM-based text-to-SQL systems, unanswerable and underspecified user queries may generate not only incorrect text but also executable programs that yield misleading results or violate safety constraints, posing a major barrier to safe deployment. Existing refusal strategies for such queries either rely on output-level instruction following, which is brittle due to model hallucinations, or estimate output uncertainty, which adds complexity and overhead. To address this challenge, we formalize safe refusal in text-to-SQL systems as an answerability-gating problem and propose LatentRefusal, a latent-signal refusal mechanism that predicts query answerability from intermediate hidden activations of a large language model. We introduce the Tri-Residual Gated Encoder, a lightweight probing architecture, to suppress schema noise and amplify sparse, localized cues of question-schema mismatch that indicate unanswerability. Extensive empirical evaluations across diverse ambiguous and unanswerable settings, together with ablation studies and interpretability analyses, demonstrate the effectiveness of the proposed approach and show that LatentRefusal provides an attachable and efficient safety layer for text-to-SQL systems. Across four benchmarks, LatentRefusal improves average F1 to 88.5 percent on both backbones while adding approximately 2 milliseconds of probe overhead.

</details>


### [39] [Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering](https://arxiv.org/abs/2601.10402)
*Xinyu Zhu,Yuzhu Cai,Zexi Liu,Bingyang Zheng,Cheng Wang,Rui Ye,Jiaao Chen,Hanrui Wang,Wei-Chen Wang,Yuzhi Zhang,Linfeng Zhang,Weinan E,Di Jin,Siheng Chen*

Main category: cs.AI

TL;DR: ML-Master 2.0通过分层认知缓存架构解决AI在超长时域自主性瓶颈，在机器学习工程任务中实现56.44%的奖牌率。


<details>
  <summary>Details</summary>
Motivation: 当前AI向代理科学发展的主要瓶颈是超长时域自主性挑战——需要在跨越数天或数周的实验周期中保持战略连贯性和迭代修正能力。虽然大语言模型在短时域推理中表现出色，但在现实研究的高维、延迟反馈环境中容易被执行细节淹没，无法将稀疏反馈整合为连贯的长期指导。

Method: 提出分层认知缓存（HCC）架构，将上下文管理重构为认知积累过程。这种受计算机系统启发的多层架构能够实现经验随时间推移的结构化区分。通过动态将瞬时执行轨迹提炼为稳定知识和跨任务智慧，HCC使代理能够将即时执行与长期实验策略解耦，有效克服静态上下文窗口的扩展限制。

Result: 在OpenAI的MLE-Bench评估中，使用24小时预算，ML-Master 2.0实现了56.44%的最先进奖牌率。

Conclusion: 超长时域自主性为AI提供了可扩展的蓝图，使其能够在超越人类先例复杂性的情况下进行自主探索。

Abstract: The advancement of artificial intelligence toward agentic science is currently bottlenecked by the challenge of ultra-long-horizon autonomy, the ability to sustain strategic coherence and iterative correction over experimental cycles spanning days or weeks. While Large Language Models (LLMs) have demonstrated prowess in short-horizon reasoning, they are easily overwhelmed by execution details in the high-dimensional, delayed-feedback environments of real-world research, failing to consolidate sparse feedback into coherent long-term guidance. Here, we present ML-Master 2.0, an autonomous agent that masters ultra-long-horizon machine learning engineering (MLE) which is a representative microcosm of scientific discovery. By reframing context management as a process of cognitive accumulation, our approach introduces Hierarchical Cognitive Caching (HCC), a multi-tiered architecture inspired by computer systems that enables the structural differentiation of experience over time. By dynamically distilling transient execution traces into stable knowledge and cross-task wisdom, HCC allows agents to decouple immediate execution from long-term experimental strategy, effectively overcoming the scaling limits of static context windows. In evaluations on OpenAI's MLE-Bench under 24-hour budgets, ML-Master 2.0 achieves a state-of-the-art medal rate of 56.44%. Our findings demonstrate that ultra-long-horizon autonomy provides a scalable blueprint for AI capable of autonomous exploration beyond human-precedent complexities.

</details>


### [40] [ErrEval: Error-Aware Evaluation for Question Generation through Explicit Diagnostics](https://arxiv.org/abs/2601.10406)
*Weiping Fu,Bifan Wei,Jingyi Hao,Yushun Zhang,Jian Zhang,Jiaxin Wang,Bo Li,Yu He,Lingling Zhang,Jun Liu*

Main category: cs.AI

TL;DR: ErrEval是一个错误感知的自动问题生成评估框架，通过两阶段错误诊断和知情评分来改进现有评估方法，解决事实幻觉和答案不匹配等缺陷被忽视的问题。


<details>
  <summary>Details</summary>
Motivation: 现有自动问题生成评估方法（包括基于LLM的评估器）主要采用黑盒整体范式，缺乏明确的错误建模，导致关键缺陷（如事实幻觉和答案不匹配）被忽视，并高估了问题质量。

Method: ErrEval将评估重新定义为两阶段过程：1）错误诊断阶段，使用轻量级即插即用错误识别器检测和分类结构、语言和内容相关的常见错误；2）知情评分阶段，将这些诊断信号作为明确证据指导LLM评估器做出更细粒度和基于证据的判断。

Result: 在三个基准测试上的广泛实验表明ErrEval的有效性，显示明确的诊断信号能提高与人类判断的一致性。进一步分析证实ErrEval有效缓解了对低质量问题的高估问题。

Conclusion: ErrEval通过明确的错误诊断和知情评分，提供了一种灵活且错误感知的评估框架，能更准确地评估自动问题生成的质量，解决现有评估方法的局限性。

Abstract: Automatic Question Generation (QG) often produces outputs with critical defects, such as factual hallucinations and answer mismatches. However, existing evaluation methods, including LLM-based evaluators, mainly adopt a black-box and holistic paradigm without explicit error modeling, leading to the neglect of such defects and overestimation of question quality. To address this issue, we propose ErrEval, a flexible and Error-aware Evaluation framework that enhances QG evaluation through explicit error diagnostics. Specifically, ErrEval reformulates evaluation as a two-stage process of error diagnosis followed by informed scoring. At the first stage, a lightweight plug-and-play Error Identifier detects and categorizes common errors across structural, linguistic, and content-related aspects. These diagnostic signals are then incorporated as explicit evidence to guide LLM evaluators toward more fine-grained and grounded judgments. Extensive experiments on three benchmarks demonstrate the effectiveness of ErrEval, showing that incorporating explicit diagnostics improves alignment with human judgments. Further analyses confirm that ErrEval effectively mitigates the overestimation of low-quality questions.

</details>


### [41] [LADFA: A Framework of Using Large Language Models and Retrieval-Augmented Generation for Personal Data Flow Analysis in Privacy Policies](https://arxiv.org/abs/2601.10413)
*Haiyue Yuan,Nikolay Matyunin,Ali Raza,Shujun Li*

Main category: cs.AI

TL;DR: LADFA是一个端到端的计算框架，结合LLMs、RAG和定制知识库，从隐私政策中提取个人数据流并构建数据流图进行分析。


<details>
  <summary>Details</summary>
Motivation: 隐私政策通常使用复杂法律语言，难以理解且在不同组织和行业中存在不一致实践。需要自动化、大规模的分析方法来帮助理解隐私政策中的个人数据处理实践。

Method: 开发LADFA框架，结合LLMs与检索增强生成(RAG)和定制知识库。框架包括预处理器、基于LLM的处理器和数据流后处理器，能够处理非结构化文本、提取个人数据流、构建数据流图并进行洞察分析。

Result: 通过汽车行业十个隐私政策的案例研究验证了方法的有效性和准确性。框架设计灵活可定制，适用于隐私政策分析之外的各种文本分析任务。

Conclusion: LADFA框架成功结合LLMs、RAG和定制知识库，能够有效提取和分析隐私政策中的个人数据流，为隐私政策理解提供了自动化解决方案，并具有扩展到其他文本分析任务的潜力。

Abstract: Privacy policies help inform people about organisations' personal data processing practices, covering different aspects such as data collection, data storage, and sharing of personal data with third parties. Privacy policies are often difficult for people to fully comprehend due to the lengthy and complex legal language used and inconsistent practices across different sectors and organisations. To help conduct automated and large-scale analyses of privacy policies, many researchers have studied applications of machine learning and natural language processing techniques, including large language models (LLMs). While a limited number of prior studies utilised LLMs for extracting personal data flows from privacy policies, our approach builds on this line of work by combining LLMs with retrieval-augmented generation (RAG) and a customised knowledge base derived from existing studies. This paper presents the development of LADFA, an end-to-end computational framework, which can process unstructured text in a given privacy policy, extract personal data flows and construct a personal data flow graph, and conduct analysis of the data flow graph to facilitate insight discovery. The framework consists of a pre-processor, an LLM-based processor, and a data flow post-processor. We demonstrated and validated the effectiveness and accuracy of the proposed approach by conducting a case study that involved examining ten selected privacy policies from the automotive industry. Moreover, it is worth noting that LADFA is designed to be flexible and customisable, making it suitable for a range of text-based analysis tasks beyond privacy policy analysis.

</details>


### [42] [LLMdoctor: Token-Level Flow-Guided Preference Optimization for Efficient Test-Time Alignment of Large Language Models](https://arxiv.org/abs/2601.10416)
*Tiesunlong Shen,Rui Mao,Jin Wang,Heming Sun,Jian Zhang,Xuejie Zhang,Erik Cambria*

Main category: cs.AI

TL;DR: LLMdoctor：一种基于患者-医生范式的测试时对齐框架，通过细粒度token级奖励获取和流引导偏好优化，在保持生成多样性的同时实现高效对齐


<details>
  <summary>Details</summary>
Motivation: 传统微调方法计算成本高且不灵活，现有测试时对齐方法依赖扭曲的轨迹级信号或低效采样，性能受限且无法保持基础模型的生成多样性

Method: 采用患者-医生范式，从患者LLM的行为变化中提取细粒度token级偏好信号，通过token级流引导偏好优化（TFPO）训练医生模型，建立所有子轨迹的流一致性

Result: LLMdoctor显著优于现有测试时对齐方法，甚至超越DPO等完整微调方法的性能

Conclusion: LLMdoctor提供了一种高效、精确的测试时对齐解决方案，在保持生成多样性的同时实现优越的对齐性能

Abstract: Aligning Large Language Models (LLMs) with human preferences is critical, yet traditional fine-tuning methods are computationally expensive and inflexible. While test-time alignment offers a promising alternative, existing approaches often rely on distorted trajectory-level signals or inefficient sampling, fundamentally capping performance and failing to preserve the generative diversity of the base model. This paper introduces LLMdoctor, a novel framework for efficient test-time alignment that operates via a patient-doctor paradigm. It integrates token-level reward acquisition with token-level flow-guided preference optimization (TFPO) to steer a large, frozen patient LLM with a smaller, specialized doctor model. Unlike conventional methods that rely on trajectory-level rewards, LLMdoctor first extracts fine-grained, token-level preference signals from the patient model's behavioral variations. These signals then guide the training of the doctor model via TFPO, which establishes flow consistency across all subtrajectories, enabling precise token-by-token alignment while inherently preserving generation diversity. Extensive experiments demonstrate that LLMdoctor significantly outperforms existing test-time alignment methods and even surpasses the performance of full fine-tuning approaches like DPO.

</details>


### [43] [NSR-Boost: A Neuro-Symbolic Residual Boosting Framework for Industrial Legacy Models](https://arxiv.org/abs/2601.10457)
*Ziming Dai,Dabiao Ma,Jinle Tong,Mengyuan Han,Jian Yang,Haojun Fei*

Main category: cs.AI

TL;DR: NSR-Boost是一个神经符号残差增强框架，用于在工业场景中非侵入式地升级遗留GBDT模型，通过LLM生成符号代码专家修复预测失败区域，已在金融风控系统成功部署。


<details>
  <summary>Details</summary>
Motivation: 在工业高并发生产环境中，升级遗留的梯度提升决策树(GBDT)模型面临昂贵的重新训练成本和系统性风险，需要一种安全、低成本的进化范式。

Method: 框架包含三个阶段：1)通过残差找到预测失败的"困难区域"；2)使用大语言模型生成符号代码结构创建可解释专家，并通过贝叶斯优化微调参数；3)通过轻量级聚合器动态整合专家与遗留模型输出。

Result: 在六个公共数据集和一个私有数据集上显著优于最先进的基线方法，在真实在线数据上表现出优异的性能提升，成功部署于Qfin Holdings的核心金融风险控制系统。

Conclusion: NSR-Boost能有效捕捉传统模型遗漏的长尾风险，为工业应用提供了一种安全、低成本的进化范式，实现了非侵入式的模型升级。

Abstract: Although the Gradient Boosted Decision Trees (GBDTs) dominate industrial tabular applications, upgrading legacy models in high-concurrency production environments still faces prohibitive retraining costs and systemic risks. To address this problem, we present NSR-Boost, a neuro-symbolic residual boosting framework designed specifically for industrial scenarios. Its core advantage lies in being "non-intrusive". It treats the legacy model as a frozen model and performs targeted repairs on "hard regions" where predictions fail. The framework comprises three key stages: first, finding hard regions through residuals, then generating interpretable experts by generating symbolic code structures using Large Language Model (LLM) and fine-tuning parameters using Bayesian optimization, and finally dynamically integrating experts with legacy model output through a lightweight aggregator. We report on the successful deployment of NSR-Boost within the core financial risk control system at Qfin Holdings. This framework not only significantly outperforms state-of-the-art (SOTA) baselines across six public datasets and one private dataset, more importantly, shows excellent performance gains on real-world online data. In conclusion, it effectively captures long-tail risks missed by traditional models and offers a safe, low-cost evolutionary paradigm for industry.

</details>


### [44] [ChartComplete: A Taxonomy-based Inclusive Chart Dataset](https://arxiv.org/abs/2601.10462)
*Ahmad Mustapha,Charbel Toumieh,Mariette Awad*

Main category: cs.AI

TL;DR: 提出ChartComplete数据集，覆盖30种图表类型，弥补现有图表理解基准数据集的局限性


<details>
  <summary>Details</summary>
Motivation: 现有图表理解基准数据集仅涵盖少量图表类型，无法全面评估多模态大语言模型在图表理解任务上的性能

Method: 基于可视化社区的图表分类法构建ChartComplete数据集，包含30种不同图表类型的分类图像集合，不包含学习信号

Result: 创建了覆盖30种图表类型的ChartComplete数据集，为研究社区提供了更全面的图表理解基准资源

Conclusion: ChartComplete数据集填补了现有图表理解基准的空白，为评估多模态大语言模型在多样化图表类型上的性能提供了更好的基础

Abstract: With advancements in deep learning (DL) and computer vision techniques, the field of chart understanding is evolving rapidly. In particular, multimodal large language models (MLLMs) are proving to be efficient and accurate in understanding charts. To accurately measure the performance of MLLMs, the research community has developed multiple datasets to serve as benchmarks. By examining these datasets, we found that they are all limited to a small set of chart types. To bridge this gap, we propose the ChartComplete dataset. The dataset is based on a chart taxonomy borrowed from the visualization community, and it covers thirty different chart types. The dataset is a collection of classified chart images and does not include a learning signal. We present the ChartComplete dataset as is to the community to build upon it.

</details>


### [45] [Panning for Gold: Expanding Domain-Specific Knowledge Graphs with General Knowledge](https://arxiv.org/abs/2601.10485)
*Runhao Zhao,Weixin Zeng,Wentao Zhang,Chong Chen,Zhengpin Li,Xiang Zhao,Lei Chen*

Main category: cs.AI

TL;DR: 提出DKGF任务，通过融合通用知识图谱来丰富领域知识图谱，解决领域相关性和知识粒度对齐两大挑战，提出ExeFuse模型并构建基准测试


<details>
  <summary>Details</summary>
Motivation: 领域知识图谱相比通用知识图谱覆盖不足，需要从通用知识图谱中融合相关事实来丰富领域知识图谱

Method: 提出ExeFuse模型，采用Fact-as-Program范式，将GKG事实视为潜在语义程序，通过粒度感知操作符映射抽象关系，并通过程序在目标DKG上的可执行性验证领域相关性

Result: 构建了两个基准测试DKGF(W-I)和DKGF(Y-I)，包含21个评估配置，实验验证了任务的重要性和模型的有效性，为DKGF提供了首个标准化测试平台

Conclusion: DKGF是一个重要的新任务，ExeFuse通过统一的概率框架有效解决了领域相关性和知识粒度对齐问题，为领域知识图谱融合提供了有效解决方案

Abstract: Domain-specific knowledge graphs (DKGs) often lack coverage compared to general knowledge graphs (GKGs). To address this, we introduce Domain-specific Knowledge Graph Fusion (DKGF), a novel task that enriches DKGs by integrating relevant facts from GKGs. DKGF faces two key challenges: high ambiguity in domain relevance and misalignment in knowledge granularity across graphs. We propose ExeFuse, a simple yet effective Fact-as-Program paradigm. It treats each GKG fact as a latent semantic program, maps abstract relations to granularity-aware operators, and verifies domain relevance via program executability on the target DKG. This unified probabilistic framework jointly resolves relevance and granularity issues. We construct two benchmarks, DKGF(W-I) and DKGF(Y-I), with 21 evaluation configurations. Extensive experiments validate the task's importance and our model's effectiveness, providing the first standardized testbed for DKGF.

</details>


### [46] [Diagnosing Generalization Failures in Fine-Tuned LLMs: A Cross-Architectural Study on Phishing Detection](https://arxiv.org/abs/2601.10524)
*Frank Bobe,Gregory D. Vetaw,Chase Pavlick,Darshan Bryner,Matthew Cook,Jose Salas-Vernis*

Main category: cs.AI

TL;DR: 研究通过多层级诊断框架分析不同LLM在钓鱼检测任务上的泛化失败原因，发现架构与数据多样性的协同作用、架构依赖的泛化能力差异，以及某些架构天生更具泛化性。


<details>
  <summary>Details</summary>
Motivation: 尽管微调大型语言模型在特定任务上取得了最先进性能，但诊断这些模型为何变得脆弱且无法泛化仍然是一个关键未解决的问题。需要理解模型泛化失败的根源。

Method: 采用多层诊断框架进行跨架构研究：微调Llama 3.1 8B、Gemma 2 9B和Mistral模型进行钓鱼检测任务，使用SHAP分析和机制可解释性技术来揭示泛化失败的根源。

Result: 三个关键发现：1) 泛化由架构与数据多样性的强大协同作用驱动；2) 泛化高度依赖架构，Llama 3.1 8B在窄域表现好但无法整合多样数据；3) 某些架构天生更具泛化性，Mistral模型在多种训练范式中表现一致且稳健。

Conclusion: 通过识别导致泛化失败的缺陷启发式方法，本研究提供了诊断和理解泛化失败的具体方法，强调可靠的AI需要深入验证架构、数据和训练策略之间的相互作用。

Abstract: The practice of fine-tuning Large Language Models (LLMs) has achieved state-of-the-art performance on specialized tasks, yet diagnosing why these models become brittle and fail to generalize remains a critical open problem. To address this, we introduce and apply a multi-layered diagnostic framework to a cross-architectural study. We fine-tune Llama 3.1 8B, Gemma 2 9B, and Mistral models on a high-stakes phishing detection task and use SHAP analysis and mechanistic interpretability to uncover the root causes of their generalization failures. Our investigation reveals three critical findings: (1) Generalization is driven by a powerful synergy between architecture and data diversity. The Gemma 2 9B model achieves state-of-the-art performance (>91\% F1), but only when trained on a stylistically diverse ``generalist'' dataset. (2) Generalization is highly architecture-dependent. We diagnose a specific failure mode in Llama 3.1 8B, which performs well on a narrow domain but cannot integrate diverse data, leading to a significant performance drop. (3) Some architectures are inherently more generalizable. The Mistral model proves to be a consistent and resilient performer across multiple training paradigms. By pinpointing the flawed heuristics responsible for these failures, our work provides a concrete methodology for diagnosing and understanding generalization failures, underscoring that reliable AI requires deep validation of the interplay between architecture, data, and training strategy.

</details>


### [47] [A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5](https://arxiv.org/abs/2601.10527)
*Xingjun Ma,Yixu Wang,Hengyuan Xu,Yutao Wu,Yifan Ding,Yunhan Zhao,Zilong Wang,Jiabin Hua,Ming Wen,Jianan Liu,Ranjie Duan,Yifeng Gao,Yingshui Tan,Yunhao Chen,Hui Xue,Xin Wang,Wei Cheng,Jingjing Chen,Zuxuan Wu,Bo Li,Yu-Gang Jiang*

Main category: cs.AI

TL;DR: 该报告对7个前沿模型进行综合安全评估，发现安全性能存在显著异质性，GPT-5.2表现最均衡，其他模型在不同评估维度存在明显权衡，强调需要标准化安全评估。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs和MLLMs在推理、感知和生成能力上取得重大进展，但这些进步是否带来相应的安全改进仍不明确，主要由于现有评估实践局限于单一模态或威胁模型。

Method: 使用统一协议评估7个前沿模型（GPT-5.2、Gemini 3 Pro等），涵盖语言、视觉语言和图像生成场景，整合基准评估、对抗评估、多语言评估和合规评估四种评估模式。

Result: 安全性能呈现显著异质性：GPT-5.2在所有评估中表现均衡且强劲；其他模型在基准安全、对抗对齐、多语言泛化和监管合规之间存在明显权衡；语言和视觉语言模态在对抗评估中表现脆弱；文生图模型在受监管视觉风险类别中对齐较强但对抗提示下仍脆弱。

Conclusion: 前沿模型的安全性本质上是多维的，受模态、语言和评估方案影响，需要标准化安全评估来准确评估现实世界风险，指导负责任模型开发和部署。

Abstract: The rapid evolution of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has produced substantial gains in reasoning, perception, and generative capability across language and vision. However, whether these advances yield commensurate improvements in safety remains unclear, in part due to fragmented evaluation practices limited to single modalities or threat models. In this report, we present an integrated safety evaluation of 7 frontier models: GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5. We evaluate each model across language, vision-language, and image generation settings using a unified protocol that integrates benchmark evaluation, adversarial evaluation, multilingual evaluation, and compliance evaluation. Aggregating our evaluations into safety leaderboards and model safety profiles across multiple evaluation modes reveals a sharply heterogeneous safety landscape. While GPT-5.2 demonstrates consistently strong and balanced safety performance across evaluations, other models exhibit pronounced trade-offs among benchmark safety, adversarial alignment, multilingual generalization, and regulatory compliance. Both language and vision-language modalities show significant vulnerability under adversarial evaluation, with all models degrading substantially despite strong results on standard benchmarks. Text-to-image models achieve relatively stronger alignment in regulated visual risk categories, yet remain brittle under adversarial or semantically ambiguous prompts. Overall, these results show that safety in frontier models is inherently multidimensional--shaped by modality, language, and evaluation scheme, underscoring the need for standardized safety evaluations to accurately assess real-world risk and guide responsible model development and deployment.

</details>


### [48] [Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing](https://arxiv.org/abs/2601.10543)
*Yinzhi Zhao,Ming Wang,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.AI

TL;DR: 提出SafeProbing方法，通过解码过程中显式利用LLMs内部潜在安全信号，实现早期检测不安全内容，有效防御越狱攻击同时保持模型效用。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs经过安全对齐，但现有对齐往往是浅层的，容易受到越狱攻击。现有防御机制（如解码约束和后处理检测器）难以应对复杂越狱攻击，要么检测不足，要么过度降低模型效用。

Method: 通过观察发现：即使成功越狱，LLMs在生成过程中仍会表现出内部潜在安全信号，但这些信号被模型追求流畅续写的驱动力所压制。提出SafeProbing方法，在解码过程中显式提取和利用这些潜在安全信号，实现早期不安全内容检测。

Result: 在多种越狱攻击上的实验表明，该方法显著增强了安全性，同时在良性输入上保持较低的过度拒绝率，并保持了响应质量。

Conclusion: 在解码过程中激活内在的安全意识为防御越狱攻击提供了一个有前景的补充方向，表明利用模型内部安全信号是实现有效防御的关键途径。

Abstract: Large language models (LLMs) have achieved impressive performance across natural language tasks and are increasingly deployed in real-world applications. Despite extensive safety alignment efforts, recent studies show that such alignment is often shallow and remains vulnerable to jailbreak attacks. Existing defense mechanisms, including decoding-based constraints and post-hoc content detectors, struggle against sophisticated jailbreaks, often intervening robust detection or excessively degrading model utility. In this work, we examine the decoding process of LLMs and make a key observation: even when successfully jailbroken, models internally exhibit latent safety-related signals during generation. However, these signals are overridden by the model's drive for fluent continuation, preventing timely self-correction or refusal. Building on this observation, we propose a simple yet effective approach that explicitly surfaces and leverages these latent safety signals for early detection of unsafe content during decoding. Experiments across diverse jailbreak attacks demonstrate that our approach significantly enhances safety, while maintaining low over-refusal rates on benign inputs and preserving response quality. Our results suggest that activating intrinsic safety-awareness during decoding offers a promising and complementary direction for defending against jailbreak attacks. Code is available at: https://github.com/zyz13590/SafeProbing.

</details>


### [49] [From Single to Multi-Agent Reasoning: Advancing GeneGPT for Genomics QA](https://arxiv.org/abs/2601.10581)
*Kimia Abedini,Farzad Shami,Gianmaria Silvello*

Main category: cs.AI

TL;DR: GenomAgent是一个多智能体框架，通过协调专门智能体处理复杂基因组查询，在GeneTuring基准测试中比当前最优的GeneGPT平均提升12%性能。


<details>
  <summary>Details</summary>
Motivation: 基因组信息理解对生物医学研究至关重要，但从复杂分布式数据库中提取数据仍然困难。大语言模型在基因组问答方面有潜力，但受限于对领域特定数据库的访问限制。当前最优的GeneGPT系统虽然通过专用API调用增强LLM，但存在API依赖性强和适应性有限的问题。

Method: 提出GenomAgent多智能体框架，通过协调多个专门智能体来处理复杂的基因组查询。该方法复制了GeneGPT并在此基础上改进，创建了更灵活的架构来高效处理基因组问答任务。

Result: 在GeneTuring基准测试的九个任务上，GenomAgent平均比GeneGPT性能提升12%。该框架的灵活架构不仅适用于基因组学，还能扩展到其他需要专家知识提取的科学领域。

Conclusion: GenomAgent通过多智能体协调机制有效解决了基因组问答中的数据库访问和查询处理问题，性能显著优于现有方法，且具有更好的可扩展性和领域适应性。

Abstract: Comprehending genomic information is essential for biomedical research, yet extracting data from complex distributed databases remains challenging. Large language models (LLMs) offer potential for genomic Question Answering (QA) but face limitations due to restricted access to domain-specific databases. GeneGPT is the current state-of-the-art system that enhances LLMs by utilizing specialized API calls, though it is constrained by rigid API dependencies and limited adaptability. We replicate GeneGPT and propose GenomAgent, a multi-agent framework that efficiently coordinates specialized agents for complex genomics queries. Evaluated on nine tasks from the GeneTuring benchmark, GenomAgent outperforms GeneGPT by 12% on average, and its flexible architecture extends beyond genomics to various scientific domains needing expert knowledge extraction.

</details>


### [50] [Multi-Property Synthesis](https://arxiv.org/abs/2601.10651)
*Christoph Weinhuber,Yannik Schnitzer,Alessandro Abate,David Parker,Giuseppe De Giacomo,Moshe Y. Vardi*

Main category: cs.AI

TL;DR: 提出一种符号化算法，用于LTLf多属性综合，通过一次性固定点计算处理无法同时满足所有属性的情况，性能比枚举方法快两个数量级。


<details>
  <summary>Details</summary>
Motivation: 在多属性LTLf综合中，同时满足所有属性可能不可行。传统方法需要枚举属性子集，效率低下，需要更高效的方法来处理这种权衡。

Method: 开发完全符号化算法，引入布尔目标变量，利用单调性紧凑表示指数级的目标组合，通过一次性固定点计算状态与可实现目标集的关系。

Result: 方法显著优于基于枚举的基线，速度提升可达两个数量级，能够高效合成实现最大可实现目标集的策略。

Conclusion: 提出的符号化方法有效解决了多属性LTLf综合问题，避免了属性子集枚举，在性能上有显著优势，为处理不可同时满足的属性提供了高效解决方案。

Abstract: We study LTLf synthesis with multiple properties, where satisfying all properties may be impossible. Instead of enumerating subsets of properties, we compute in one fixed-point computation the relation between product-game states and the goal sets that are realizable from them, and we synthesize strategies achieving maximal realizable sets. We develop a fully symbolic algorithm that introduces Boolean goal variables and exploits monotonicity to represent exponentially many goal combinations compactly. Our approach substantially outperforms enumeration-based baselines, with speedups of up to two orders of magnitude.

</details>


### [51] [Are Your Reasoning Models Reasoning or Guessing? A Mechanistic Analysis of Hierarchical Reasoning Models](https://arxiv.org/abs/2601.10679)
*Zirui Ren,Ziming Liu*

Main category: cs.AI

TL;DR: HRM在推理任务中表现出色，但研究发现其存在"猜测"而非"推理"的问题，通过数据增强、输入扰动和模型引导三种策略提升性能，将数独极端难题准确率从54.5%提升至96.9%。


<details>
  <summary>Details</summary>
Motivation: 研究HRM的推理机制，理解其优势和潜在失败模式，揭示其实际是"猜测"而非真正"推理"的本质。

Method: 对HRM进行机制性研究，发现三个关键现象：简单谜题失败、推理步骤中的"顿悟"动态、多固定点存在。基于"猜测"视角提出三种扩展策略：数据增强（提升猜测质量）、输入扰动（利用推理随机性增加猜测次数）、模型引导（利用训练随机性增加猜测次数）。

Result: 开发出增强版HRM，在Sudoku-Extreme任务上将准确率从54.5%大幅提升至96.9%。揭示了推理模型实际通过"猜测"而非严格推理来解决问题。

Conclusion: HRM本质上是通过"猜测"而非"推理"来解决问题，这一发现为理解推理模型的工作机制提供了新视角，同时提出的增强策略显著提升了模型性能。

Abstract: Hierarchical reasoning model (HRM) achieves extraordinary performance on various reasoning tasks, significantly outperforming large language model-based reasoners. To understand the strengths and potential failure modes of HRM, we conduct a mechanistic study on its reasoning patterns and find three surprising facts: (a) Failure of extremely simple puzzles, e.g., HRM can fail on a puzzle with only one unknown cell. We attribute this failure to the violation of the fixed point property, a fundamental assumption of HRM. (b) "Grokking" dynamics in reasoning steps, i.e., the answer is not improved uniformly, but instead there is a critical reasoning step that suddenly makes the answer correct; (c) Existence of multiple fixed points. HRM "guesses" the first fixed point, which could be incorrect, and gets trapped there for a while or forever. All facts imply that HRM appears to be "guessing" instead of "reasoning". Leveraging this "guessing" picture, we propose three strategies to scale HRM's guesses: data augmentation (scaling the quality of guesses), input perturbation (scaling the number of guesses by leveraging inference randomness), and model bootstrapping (scaling the number of guesses by leveraging training randomness). On the practical side, by combining all methods, we develop Augmented HRM, boosting accuracy on Sudoku-Extreme from 54.5% to 96.9%. On the scientific side, our analysis provides new insights into how reasoning models "reason".

</details>


### [52] [Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems](https://arxiv.org/abs/2601.10681)
*Amir Khurshid,Abhishek Sehgal*

Main category: cs.AI

TL;DR: 提出一种结构感知、多样性约束的上下文气泡构建框架，替代传统RAG的top-k检索，通过组织多粒度文本片段并利用文档结构先验，在有限token预算下构建紧凑、可引用的上下文集合。


<details>
  <summary>Details</summary>
Motivation: 传统RAG的top-k检索方法存在信息图碎片化、过度检索、内容重复以及查询上下文不足（特别是二阶和三阶方面）等问题，需要更高效的上下文构建方法。

Method: 提出结构感知、多样性约束的上下文气泡框架：1) 利用文档结构组织多粒度文本片段（如章节、行）；2) 使用任务条件结构先验指导检索；3) 从高相关性锚点片段开始，通过平衡查询相关性、边际覆盖和冗余惩罚的约束选择构建上下文气泡；4) 提供完整检索轨迹以实现可审计性和确定性调优。

Result: 在企业文档上的实验表明，上下文气泡方法显著减少冗余上下文，更好地覆盖次要方面，在有限上下文窗口内获得更好的答案质量和引用忠实度。消融研究证明结构先验和多样性约束选择都是必要的。

Conclusion: 提出的上下文气泡框架通过结构感知和多样性约束的检索策略，有效解决了传统RAG方法的局限性，能够在有限token预算下构建更紧凑、信息丰富且可审计的上下文集合。

Abstract: Large language model (LLM) contexts are typically constructed using retrieval-augmented generation (RAG), which involves ranking and selecting the top-k passages. The approach causes fragmentation in information graphs in document structures, over-retrieval, and duplication of content alongside insufficient query context, including 2nd and 3rd order facets. In this paper, a structure-informed and diversity-constrained context bubble construction framework is proposed that assembles coherent, citable bundles of spans under a strict token budget. The method preserves and exploits inherent document structure by organising multi-granular spans (e.g., sections and rows) and using task-conditioned structural priors to guide retrieval. Starting from high-relevance anchor spans, a context bubble is constructed through constrained selection that balances query relevance, marginal coverage, and redundancy penalties. It will explicitly constrain diversity and budget, producing compact and informative context sets, unlike top-k retrieval. Moreover, a full retrieval is emitted that traces the scoring and selection choices of the records, thus providing auditability and deterministic tuning. Experiments on enterprise documents demonstrate the efficiency of context bubble as it significantly reduces redundant context, is better able to cover secondary facets and has a better answer quality and citation faithfulness within a limited context window. Ablation studies demonstrate that both structural priors as well as diversity constraint selection are necessary; removing either component results in a decline in coverage and an increase in redundant or incomplete context.

</details>


### [53] [The Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load](https://arxiv.org/abs/2601.10696)
*Han Jiang,Yao Xiao,Rachel Hurley,Shichao Liu*

Main category: cs.AI

TL;DR: 生成式AI在建筑概念设计中能提升新手表现，但会降低创意自我效能感，效果取决于用户专业水平和提示策略


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI如何影响建筑概念设计中的表现、创意自我效能感和认知负荷，探索AI工具在不同专业水平用户中的差异化效果

Method: 36名学生参与两阶段建筑设计任务：独立设计阶段和外部工具辅助阶段（分为GenAI辅助组和在线项目库对照组）。专家评估设计成果，参与者自报自我效能感和认知负荷，采用双重差分分析

Result: GenAI对所有参与者无整体表现优势，但显著提升新手设计师表现；使用GenAI的学生创意自我效能感下降；认知负荷无显著差异，但迭代创意生成和视觉反馈提示能更大程度降低认知负荷

Conclusion: 生成式AI效果取决于用户专业水平和交互策略，新手受益但可能损害创意自信，提示策略优化能减轻认知负荷

Abstract: Our study examines how generative AI (GenAI) influences performance, creative self-efficacy, and cognitive load in architectural conceptual design tasks. Thirty-six student participants from Architectural Engineering and other disciplines completed a two-phase architectural design task, first independently and then with external tools (GenAI-assisted condition and control condition using an online repository of existing architectural projects). Design outcomes were evaluated by expert raters, while self-efficacy and cognitive load were self-reported after each phase. Difference-in-differences analyses revealed no overall performance advantage of GenAI across participants; however, subgroup analyses showed that GenAI significantly improved design performance for novice designers. In contrast, general creative self-efficacy declined for students using GenAI. Cognitive load did not differ significantly between conditions, though prompt usage patterns showed that iterative idea generation and visual feedback prompts were linked to greater reductions in cognitive load. These findings suggest that GenAI effectiveness depends on users' prior expertise and interaction strategies through prompting.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [54] [Learning about Treatment Effects with Prior Studies: A Bayesian Model Averaging Approach](https://arxiv.org/abs/2601.09888)
*Frederico Finan,Demian Pouzo*

Main category: econ.EM

TL;DR: 论文提出了一种结合先验信息的实验治疗效果估计方法，通过贝叶斯模型平均和外部有效性指数，在非标准渐近框架下实现更快的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有实验设计往往忽略了过去试点、相关研究或专家评估等先验信息，或者对这些信息的有效性存在不确定性。如何有效整合这些外部信息源，并在它们可能存在偏差的情况下保证估计的鲁棒性，是一个重要问题。

Method: 采用贝叶斯模型平均（BMA）方法，将每个先验信息源建模为具有特定均值和精度的高斯先验。引入非标准渐近框架，允许先验精度随实验样本量增长。通过外部有效性指数来权衡信息源的偏差和信息含量。

Result: 当至少有一个信息源无偏时，该方法能集中在无偏集合上，实现比仅依赖新数据更快的收敛速度。当所有信息源都有偏时，加入保守（扩散）先验能保证鲁棒性并恢复标准收敛率。

Conclusion: 该方法为整合先验信息提供了理论框架，在信息源无偏时能加速收敛，在有偏时通过保守先验保持鲁棒性，为实验设计中的信息整合提供了实用工具。

Abstract: We establish concentration rates for estimation of treatment effects in experiments that incorporate prior sources of information -- such as past pilots, related studies, or expert assessments -- whose external validity is uncertain. Each source is modeled as a Gaussian prior with its own mean and precision, and sources are combined using Bayesian model averaging (BMA), allowing data from the new experiment to update posterior weights. To capture empirically relevant settings in which prior studies may be as informative as the current experiment, we introduce a nonstandard asymptotic framework in which prior precisions grow with the experiment's sample size. In this regime, posterior weights are governed by an external-validity index that depends jointly on a source's bias and information content: biased sources are exponentially downweighted, while unbiased sources dominate. When at least one source is unbiased, our procedure concentrates on the unbiased set and achieves faster convergence than relying on new data alone. When all sources are biased, including a deliberately conservative (diffuse) prior guarantees robustness and recovers the standard convergence rate.

</details>


### [55] [Corrected Forecast Combinations](https://arxiv.org/abs/2601.09999)
*Chu-An Liu,Andrey L. Vasnev*

Main category: econ.EM

TL;DR: 该论文提出了一种修正预测组合的方法，当原始组合预测误差存在序列依赖时，通过添加前一期组合误差的一部分来修正下一期预测，可显著提高预测精度。


<details>
  <summary>Details</summary>
Motivation: 经典Bates和Granger(1969)的例子表明，组合预测误差可能存在强自相关性。当预测误差存在序列依赖时，原始组合预测可能无法充分利用误差中的可预测信息，导致预测精度下降。

Method: 在Gibbs和Vasnev(2024)的条件风险框架下，将组合误差分解为可预测成分和创新项。通过添加前一期组合误差的一部分来修正预测，并将此修正与GLS估计下的权重优化联系起来，联合估计组合权重和误差协方差结构。

Result: 使用美国专业预测者调查数据进行实证分析发现：1) 对均值预测进行系数约0.5的简约修正通常能显著提高预测精度；2) 对于最优权重预测，该修正能有效缓解"预测组合谜题"，将表现不佳的最优权重组合转变为具有竞争力的预测。

Conclusion: 当组合预测误差存在序列依赖时，简单的误差修正方法能带来显著的预测精度提升，甚至超过原始组合预测的收益。该方法为处理时间序列依赖下的预测组合提供了有效工具。

Abstract: This paper proposes corrected forecast combinations when the original combined forecast errors are serially dependent. Motivated by the classic Bates and Granger (1969) example, we show that combined forecast errors can be strongly autocorrelated and that a simple correction--adding a fraction of the previous combined error to the next-period combined forecast--can deliver sizable improvements in forecast accuracy, often exceeding the original gains from combining. We formalize the approach within the conditional risk framework of Gibbs and Vasnev (2024), in which the combined error decomposes into a predictable component (measurable at the forecast origin) and an innovation. We then link this correction to efficient estimation of combination weights under time-series dependence via GLS, allowing joint estimation of weights and an error-covariance structure. Using the U.S. Survey of Professional Forecasters for major macroeconomic indices across various subsamples (including pre and post-2000, GFC, and COVID), we find that a parsimonious correction of the mean forecast with a coefficient around 0.5 is a robust starting point and often yields material improvements in forecast accuracy. For optimal-weight forecasts, the correction substantially mitigates the forecast combination puzzle by turning poorly performing out-of-sample optimal-weight combinations into competitive forecasts.

</details>


### [56] [Selecting and Testing Asset Pricing Models: A Stepwise Approach](https://arxiv.org/abs/2601.10279)
*Guanhao Feng,Wei Lan,Hansheng Wang,Jun Zhang*

Main category: econ.EM

TL;DR: 提出一个因子模型选择与检验框架，通过选择能跨越测试资产和所有候选因子联合有效前沿的最优模型，并在测试资产和未选候选因子上检验定价性能


<details>
  <summary>Details</summary>
Motivation: 现有资产定价文献强调最小化定价误差的因子模型，但忽视了未选候选因子可能提升测试资产性能的问题

Method: 提出框架：(1)选择跨越测试资产和所有候选因子联合有效前沿的最优模型；(2)在测试资产和未选候选因子上检验定价性能。通过资产定价对偶性确保模型选择一致性，并基于资产定价检验顺序更新基准模型

Result: 实证证据表明，主流因子模型未能通过资产定价检验，而提出的8因子模型未被拒绝，并展现出稳健的样本外性能

Conclusion: 该框架通过同时考虑测试资产和未选候选因子，提供了更全面的因子模型选择和检验方法，解决了现有文献的局限性

Abstract: The asset pricing literature emphasizes factor models that minimize pricing errors but overlooks unselected candidate factors that could enhance the performance of test assets. This paper proposes a framework for factor model selection and testing by (i) selecting the optimal model that spans the joint efficient frontier of test assets and all candidate factors, and (ii) testing pricing performance on both test assets and unselected candidate factors. Our framework updates a baseline model (e.g., CAPM) sequentially by adding or removing factors based on asset pricing tests. Ensuring model selection consistency, our framework utilizes the asset pricing duality: minimizing cross-sectionally unexplained pricing errors aligns with maximizing the Sharpe ratio of the selected factor model. Empirical evidence shows that workhorse factor models fail asset pricing tests, whereas our proposed 8-factor model is not rejected and exhibits robust out-of-sample performance.

</details>


### [57] [Como medir o invisível? Guerras, pizzarias do Pentágono e o uso de variáveis proxy em econometria](https://arxiv.org/abs/2601.10352)
*Guilherme Vianna,Victor Rangel*

Main category: econ.EM

TL;DR: 论文研究如何通过代理变量解决潜在变量在回归分析中导致的遗漏变量偏误问题，区分了完美代理和不完美代理的情况，并提出了基于四个属性的评估框架。


<details>
  <summary>Details</summary>
Motivation: 许多经济相关变量（如风险、信心、不确定性）是潜在变量，无法直接观测，这给应用回归分析带来了识别挑战。遗漏这些潜在因素会导致遗漏变量偏误，需要有效的方法来缓解这一问题。

Method: 论文形式化分析了遗漏潜在变量如何产生偏误，并讨论了何时包含代理变量可以缓解偏误。区分完美代理（可消除偏误）和不完美代理（存在残余偏误和衰减效应）。提出了基于四个属性的实用评估协议：相关性、条件充分性、外生性和稳定性。使用阿灵顿的微出行数据和美国地缘政治风险指数作为实证案例，通过协整分析和二元VEC模型解释本地活动作为地缘政治紧张潜在成分的高频信号。

Result: 论文建立了代理变量缓解遗漏变量偏误的理论框架，区分了不同代理质量下的偏误特征。实证分析表明，本地微出行活动可以作为地缘政治紧张潜在成分的有效高频代理信号。

Conclusion: 代理变量是解决潜在变量识别问题的实用工具，但需要仔细评估其质量。提出的四个属性评估框架为研究人员提供了系统的方法来判断代理变量的有效性，有助于提高实证研究的可靠性。

Abstract: Many economically relevant variables (risk, confidence, uncertainty) are latent and therefore not directly observable, which creates identification challenges in applied regressions. This text formalizes how omitting latent factors generates omitted-variable bias and discusses when including a proxy variable can mitigate it. We distinguish the case of a perfect proxy, which can eliminate the bias, from the more realistic case of an imperfect proxy, where residual bias remains and the estimated effect is attenuated. We propose a practical evaluation protocol based on four properties: relevance, conditional sufficiency, exogeneity, and stability. As an illustration, we use micromobility data from Arlington together with the U.S. Geopolitical Risk Index, estimating cointegration and a bivariate VEC model to interpret local activity as a high-frequency signal of the latent component of geopolitical tension.

</details>


### [58] [Chasing Opportunity: Spillovers and Drivers of U.S. State Population Growth](https://arxiv.org/abs/2601.10444)
*Sebastian Kripfganz,Vasilis Sarafidis*

Main category: econ.EM

TL;DR: 该研究使用动态空间模型分析美国49个州1965-2017年人口增长的驱动因素和空间扩散，采用数据推断网络结构而非预设邻接关系，结合IV估计器处理内生性和异质性，发现人口增长存在广泛但异质的条件收敛。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解美国州级人口增长的驱动因素和空间扩散模式，同时解决传统空间计量经济学中网络结构预设、内生性处理和异质性建模的局限性。

Method: 开发统一估计框架：1) 从数据中推断空间网络结构而非预设邻接或距离；2) 结合IV估计器处理内生解释变量；3) 允许异质性斜率和交互固定效应；4) 在灵活的空间面板模型中实现一致估计和推断。

Result: 实证发现：1) 约四分之三的州呈现条件收敛，小部分高增长州轻微发散；2) 核心驱动因素（便利设施、劳动收入、迁移摩擦）效应在不同网络设定下稳定；3) 生产率效应仅在数据推断的网络中显现；4) 空间溢出效应显著，间接效应约占总影响的三分之一，扩散范围超出相邻州。

Conclusion: 该研究提出了首个同时处理数据推断网络结构、内生解释变量和普遍跨州依赖的统一空间计量框架，揭示了美国州级人口增长的异质收敛模式和显著空间溢出效应，强调了数据驱动网络识别的重要性。

Abstract: We study the drivers and spatial diffusion of U.S. state population growth using a dynamic spatial model for 49 states, 1965-2017. Methodologically, we recover the spatial network structure from the data, rather than imposing it a priori via contiguity or distance, and combine this with an IV estimator that permits heterogeneous slopes and interactive fixed effects. This unified design delivers consistent estimation and inference in a flexible spatial panel model with endogenous regressors, a data-inferred network structure, and pervasive cross-state dependence. To our knowledge, it is the first estimation framework in spatial econometrics to combine all three elements within a single setting. Empirically, population growth exhibits broad yet heterogeneous conditional convergence: about three-quarters of states converge, while a small high-growth group mildly diverges. Effects of the core drivers, amenities, labour income, migration frictions, are stable across various network specifications. On the other hand, the productivity effect emerges only when the network is estimated from the data. Spatial spillovers are sizable, with indirect effects roughly one-third of total impacts, and diffusion extending beyond contiguous neighbours.

</details>


### [59] [Semiparametric inference for inequality measures under nonignorable nonresponse using callback data](https://arxiv.org/abs/2601.10501)
*Xinyu Wang,Chunlin Wang,Tao Yu,Pengfei Li*

Main category: econ.EM

TL;DR: 提出半参数方法，在调查数据存在不可忽略无回答时估计不平等指标，利用回访数据纠正选择偏差


<details>
  <summary>Details</summary>
Motivation: 家庭调查中常见的不可忽略无回答（响应概率取决于未观测结果）会导致选择偏差，使标准推断方法失效，需要开发新方法来纠正这种偏差

Method: 利用重复联系尝试的回访数据，采用半参数模型（结果分布未指定），构建半参数完全似然估计量，提出稳定的EM算法实现

Result: 建立了估计量的大样本性质，推导了显式渐近方差表达式，模拟显示能有效纠正无回答偏差并接近基准效率

Conclusion: 提出的方法能在不可忽略无回答下对不平等指标进行有效推断，消费者支出调查应用展示了回访信息的实际价值

Abstract: This paper develops semiparametric methods for estimation and inference of widely used inequality measures when survey data are subject to nonignorable nonresponse, a challenging setting in which response probabilities depend on the unobserved outcomes. Such nonresponse mechanisms are common in household surveys and invalidate standard inference procedures due to selection bias and lack of population representativeness. We address this problem by exploiting callback data from repeated contact attempts and adopting a semiparametric model that leaves the outcome distribution unspecified. We construct semiparametric full-likelihood estimators for the underlying distribution and the associated inequality measures, and establish their large-sample properties for a broad class of functionals, including quantiles, the Theil index, and the Gini index. Explicit asymptotic variance expressions are derived, enabling valid Wald-type inference under nonignorable nonresponse. To facilitate implementation, we propose a stable and computationally convenient expectation-maximization algorithm, whose steps either admit closed-form expressions or reduce to fitting a standard logistic regression model. Simulation studies demonstrate that the proposed procedures effectively correct nonresponse bias and achieve near-benchmark efficiency. An application to Consumer Expenditure Survey data illustrates the practical gains from incorporating callback information when making inference on inequality measures.

</details>


### [60] [causalfe: Causal Forests with Fixed Effects in Python](https://arxiv.org/abs/2601.10555)
*Harry Aytug*

Main category: econ.EM

TL;DR: Causal Forests with Fixed Effects (CFFE) 是一个用于面板数据异质性处理效应估计的Python包，通过节点级残差化解决固定效应导致的虚假异质性问题。


<details>
  <summary>Details</summary>
Motivation: 标准因果森林方法在处理面板数据时存在困难，因为单位和时间固定效应会导致处理效应估计中出现虚假异质性。需要一种能够有效处理面板数据固定效应的方法。

Method: CFFE方法通过在树构建过程中进行节点级残差化，在每个候选分裂内而非全局移除固定效应。该方法在因果森林框架中集成了固定效应处理。

Result: 通过模拟研究验证了该估计器在各种数据生成过程中的性能，展示了软件包的有效性和实用性。

Conclusion: causalfe包提供了一个有效的Python实现，能够准确估计面板数据中的异质性处理效应，解决了标准方法在处理固定效应时的局限性。

Abstract: The causalfe package provides a Python implementation of Causal Forests with Fixed Effects (CFFE) for estimating heterogeneous treatment effects in panel data settings. Standard causal forest methods struggle with panel data because unit and time fixed effects induce spurious heterogeneity in treatment effect estimates. The CFFE approach addresses this by performing node-level residualization during tree construction, removing fixed effects within each candidate split rather than globally. This paper describes the methodology, documents the software interface, and demonstrates the package through simulation studies that validate the estimator's performance under various data generating processes.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [61] [How Diplomacy Reshapes Online Discourse:Asymmetric Persistence in Online Framing of North Korea](https://arxiv.org/abs/2601.09942)
*Hunjun Shin,Hoonbae Moon,Mohit Singhal*

Main category: cs.SI

TL;DR: 研究显示外交峰会能短期但持久地改变网络话语中对敌对国家的框架，即使后续谈判失败，这种框架转变也不会完全逆转。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要依赖情感分析和调查方法，难以捕捉外交接触后持续的叙事变化（超越短暂的情感反应）。本研究旨在探究高风险外交峰会如何塑造网络话语中对敌对国家的框架。

Method: 采用双重差分设计分析2018-2019年美朝峰会期间的Reddit讨论，使用多个对照组（中国、伊朗、俄罗斯）控制地缘政治冲击，结合经过验证的Codebook LLM框架分类和图基话语网络分析，考察边缘级关系和社区级叙事结构。

Result: 发现框架响应存在短期不对称持续性：帖子和评论级情感是短暂的（新加坡峰会期间改善但河内失败后完全逆转），而框架转变显著稳定——从威胁导向转向外交导向的框架仅部分逆转。结构上，威胁导向边缘比例大幅下降（48%→28%），外交导向结构扩张，这些转变在外交失败后抵抗完全逆转。

Conclusion: 外交成功能在网络话语中留下短期但持久的印记，改变对敌对国家的框架方式，即使后续谈判失败，这种框架转变也不会完全消失。

Abstract: Public opinion toward foreign adversaries shapes and constrains diplomatic options. Prior research has largely relied on sentiment analysis and survey based measures, providing limited insight into how sustained narrative changes (beyond transient emotional reactions) might follow diplomatic engagement. This study examines the extent to which high stakes diplomatic summits shape how adversaries are framed in online discourse. We analyze U.S.-North Korea summit diplomacy (2018-2019) using a Difference-in-Difference(DiD) design on Reddit discussions. Using multiple control groups (China, Iran, Russia) to adjust for concurrent geopolitical shocks, we integrate a validated Codebook LLM framework for framing classification with graph based discourse network analysis that examines both edge level relationships and community level narrative structures. Our results reveal short term asymmetric persistence in framing responses to diplomacy. While both post level and comment level sentiment proved transient (improving during the Singapore Summit but fully reverting after the Hanoi failure),framing exhibited significant stability: the shift from threat oriented to diplomacy oriented framing was only partially reversed. Structurally, the proportion of threat oriented edges decreased substantially (48% -> 28%) while diplomacy oriented structures expanded, and these shifts resisted complete reversion after diplomatic failure. These findings suggest that diplomatic success can leave a short-term but lasting imprint on how adversaries are framed in online discourse, even when subsequent negotiations fail.

</details>


### [62] [Higher order trade-offs in hypergraph community detection](https://arxiv.org/abs/2601.10502)
*Jiaze Li,Michael T. Schaub,Leto Peel*

Main category: cs.SI

TL;DR: 本文提出一个统一的超图社区检测框架，引入信噪比分析高阶网络特有的权衡，开发非均匀超图的Bethe Hessian算子实现高效谱聚类，并揭示高阶超边和平衡形状超边的系统偏好。


<details>
  <summary>Details</summary>
Motivation: 将社区检测从成对网络扩展到超图面临理论挑战：超图具有结构异质性，不同阶数的超边可以以多种配置连接跨社区节点，这为定义和检测社区结构引入了新的权衡问题。

Method: 在超图随机块模型下开发统一框架，引入通用信噪比分析高阶网络特有权衡，推导非均匀超图的Bethe Hessian算子实现高效谱聚类和原则性模型选择。

Result: 表征了谱检测阈值，与信念传播极限比较显示两者在均匀超图中一致但在非均匀设置中分歧。合成实验验证分析预测并揭示系统偏向保留高阶和平衡形状超边。实证数据应用证明这些高阶检测权衡在实际系统中的相关性。

Conclusion: 该框架为超图社区检测提供了理论基础和实用工具，揭示了高阶网络特有的检测权衡，Bethe Hessian算子实现了高效谱聚类，实证应用验证了理论的实际价值。

Abstract: Extending community detection from pairwise networks to hypergraphs introduces fundamental theoretical challenges. Hypergraphs exhibit structural heterogeneity with no direct graph analogue: hyperedges of varying orders can connect nodes across communities in diverse configurations, introducing new trade-offs in defining and detecting community structure. We address these challenges by developing a unified framework for community detection in non-uniform hypergraphs under the Hypergraph Stochastic Block Model. We introduce a general signal-to-noise ratio that enables a quantitative analysis of trade-offs unique to higher-order networks, such as which hypergedges we choose to split across communities and how we choose to split them. Building on this framework, we derive a Bethe Hessian operator for non-uniform hypergraphs that provides efficient spectral clustering with principled model selection. We characterize the resulting spectral detectability threshold and compare it to belief propagation limits, showing the methods coincide for uniform hypergraphs but diverge in non-uniform settings. Synthetic experiments confirm our analytical predictions and reveal systematic biases toward preserving higher-order and balanced-shape hyperedges. Application to empirical data demonstrates the practical relevance of these higher-order detectability trade-offs in real-world systems.

</details>


### [63] [Inferring signed social networks from contact patterns](https://arxiv.org/abs/2601.10565)
*Dávid Ferenczi,Jean-Gabriel Young,Leto Peel*

Main category: cs.SI

TL;DR: 提出贝叶斯框架从接触数据推断带符号网络，区分无互动是由于机会缺失还是主动回避


<details>
  <summary>Details</summary>
Motivation: 社交网络通常从间接观测推断，但现有方法无法区分缺失关系和实际负向关系，因为两者都可能导致很少或没有互动

Method: 开发贝叶斯框架，使用MCMC推断，建模互动组以区分机会缺失和主动选择导致的零互动

Result: 合成数据验证显示优于基线方法，特别是在检测负向边方面；应用于法国高中接触数据揭示与友谊调查一致的结构

Conclusion: 该方法能有效从接触模式推断带符号网络，区分机会缺失和主动回避，并通过后验预测检查证明模型充分性

Abstract: Social networks are typically inferred from indirect observations, such as proximity data; yet, most methods cannot distinguish between absent relationships and actual negative ties, as both can result in few or no interactions. We address the challenge of inferring signed networks from contact patterns while accounting for whether lack of interactions reflect a lack of opportunity as opposed to active avoidance. We develop a Bayesian framework with MCMC inference that models interaction groups to separate chance from choice when no interactions are observed. Validation on synthetic data demonstrates superior performance compared to natural baselines, particularly in detecting negative edges. We apply our method to French high school contact data to reveal a structure consistent with friendship surveys and demonstrate the model's adequacy through posterior predictive checks.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [64] [Forecasting Seasonal Peaks of Pediatric Respiratory Infections Using an Alert-Based Model Combining SIR Dynamics and Historical Trends in Santiago, Chile](https://arxiv.org/abs/2601.09821)
*Gloria Henríquez,Jhoan Báez,Víctor Riquelme,Pedro Gajardo,Michel Royer,Héctor Ramírez*

Main category: stat.AP

TL;DR: 开发基于警报的预测模型，用于预测智利圣地亚哥儿童急性呼吸道感染住院高峰的时间和规模，通过季节性SIR模型结合移动历史预测因子，提前一个月预测高峰时间，两周前实现高精度预测。


<details>
  <summary>Details</summary>
Motivation: 急性呼吸道感染是智利儿童住院的主要原因，冬季需求激增给医院规划带来挑战，需要提前预测高峰时间和规模以改善医院准备。

Method: 整合季节性SIR模型与历史移动预测因子，采用基于导数的警报系统检测早期疫情增长。使用15天移动平均和Savitzky-Golay滤波平滑每日住院数据，通过惩罚损失函数估计参数以减少噪声敏感性。

Result: 回顾性评估和2023-2024年圣地亚哥主要儿童医院的实际实施显示：高峰日期可提前约一个月预测，两周前实现高精度预测；高峰规模在高峰前约10天变得有参考价值，一周前稳定。

Conclusion: 该模型为医院准备提供了实用且可解释的工具，能够有效预测急性呼吸道感染住院高峰的时间和规模，支持医院资源规划和应对策略。

Abstract: Acute respiratory infections (ARI) are a major cause of pediatric hospitalization in Chile, producing marked winter increases in demand that challenge hospital planning. This study presents an alert-based forecasting model to predict the timing and magnitude of ARI hospitalization peaks in Santiago. The approach integrates a seasonal SIR model with a historical mobile predictor, activated by a derivative-based alert system that detects early epidemic growth. Daily hospitalization data from DEIS were smoothed using a 15-day moving average and Savitzky-Golay filtering, and parameters were estimated using a penalized loss function to reduce sensitivity to noise. Retrospective evaluation and real-world implementation in major Santiago pediatric hospitals during 2023 and 2024 show that peak date can be anticipated about one month before the event and predicted with high accuracy two weeks in advance. Peak magnitude becomes informative roughly ten days before the peak and stabilizes one week prior. The model provides a practical and interpretable tool for hospital preparedness.

</details>


### [65] [The Knowable Future: Mapping the Decay of Past-Future Mutual Information Across Forecast Horizons](https://arxiv.org/abs/2601.10006)
*Peter Maurice Catt*

Main category: stat.AP

TL;DR: 该研究提出使用滞后h的自互信息（AMI）作为时间序列可预测性的度量，通过k近邻估计器从训练数据中估计，并与样本外预测误差（sMAPE）进行相关性分析，发现AMI能有效诊断不同频率时间序列的预测难度。


<details>
  <summary>Details</summary>
Motivation: 在预测实践中，事先评估时间序列是否可能被准确预测非常重要，因为这决定了需要投入的建模努力程度。当前缺乏对时间序列可预测性的量化度量方法，无法在预测前有效评估序列的预测难度。

Method: 定义可预测性为时间序列的属性（给定声明信息集），使用滞后h的自互信息（AMI）作为特定预测时域的不确定性减少度量。通过k近邻估计器从训练数据中估计AMI，并在过滤后的平衡样本（1,350个M4序列，6种采样频率）上评估其与样本外预测误差（sMAPE）的相关性。使用Seasonal Naive、ETS和N-BEATS作为样本外预测性能的探针。

Result: 训练数据的AMI提供了频率条件性的预测难度诊断：对于小时、周、季度和年度序列，AMI与sMAPE在所有探针中均显示一致的负秩相关。在N-BEATS下，小时序列（ρ=-0.52）和周序列（ρ=-0.51）相关性最强，季度（ρ=-0.42）和年度（ρ=-0.36）也显著。月度序列相关性依赖于探针（Seasonal Naive ρ=-0.12；ETS ρ=-0.26；N-BEATS ρ=-0.24）。日序列在此协议下显示较弱的AMI-sMAPE相关性，表明尽管存在时间依赖性，但区分序列的能力有限。

Conclusion: 研究结果支持基于可测量的信号内容在预测前进行频率内分类和努力分配，而不是进行频率间的难度比较。AMI可作为有效的预测难度诊断工具，帮助在实际预测前评估时间序列的可预测性。

Abstract: The ability to assess ex-ante whether a time series is likely to be accurately forecast is important for forecasting practice because it informs the degree of modelling effort warranted. We define forecastability as a property of a time series (given a declared information set), and measure horizon-specific forecastability as the reduction in uncertainty provided by the past, using auto-mutual information (AMI) at lag h. AMI is estimated from training data using a k-nearest-neighbour estimator and evaluated against out-of-sample forecast error (sMAPE) on a filtered, balanced sample of 1,350 M4 series across six sampling frequencies. Seasonal Naive, ETS, and N-BEATS are used as probes of out-of-sample forecast performance. Training-only AMI provides a frequency-conditional diagnostic for forecast difficulty: for Hourly, Weekly, Quarterly, and Yearly series, AMI exhibits consistently negative rank correlation with sMAPE across probes. Under N-BEATS, the correlation is strongest for Hourly (p= -0.52) and Weekly (p= -0.51), with Quarterly (p= -0.42) and Yearly (p = -0.36) also substantial. Monthly is probe-dependent (Seasonal Naive p= -0.12; ETS p = -0.26; N-BEATS p = -0.24). Daily shows notably weaker AMI-sMAPE correlation under this protocol, suggesting limited ability to discriminate between series despite the presence of temporal dependence. The findings support within-frequency triage and effort allocation based on measurable signal content prior to forecasting, rather than between-frequency comparisons of difficulty.

</details>


### [66] [Modeling mental health trajectories during the COVID-19 pandemic using UK-wide data in the presence of sociodemographic variables](https://arxiv.org/abs/2601.10445)
*Glenna Nightingale,Karthik Mohan,Eloi Ribe,Valentin Popov,Shakes Wang,Clara Calia,Luciana Brondi,Sohan Seth*

Main category: stat.AP

TL;DR: 英国COVID-19疫情期间心理健康轨迹的人口统计学影响因素研究


<details>
  <summary>Details</summary>
Motivation: COVID-19大流行对人群心理健康产生负面影响，需要确定影响英国疫情期间心理健康轨迹的潜在因素，为公共卫生政策提供依据。

Method: 使用Understanding Society COVID-19研究数据，以GHQ36评分为结果变量，采用广义加性模型(GAMs)分析时间趋势及人口统计学变量（年龄、性别、种族、居住地、就业状况、家庭收入、伴侣状况、有无16岁以下子女、长期疾病）对心理健康变化的影响。

Result: 所有人口统计学变量均显示对心理健康有显著影响：女性GHQ36评分比男性高1.260分；无伴侣者评分比有伴侣者高1.050分；16-64岁各年龄段评分均高于65岁以上者；较低家庭收入者心理健康状况较差。

Conclusion: 研究确定了英国COVID-19疫情期间心理健康轨迹的关键人口统计学决定因素。减少心理健康不平等的政策应针对女性、年轻人、无伴侣者、有16岁以下子女者、长期疾病患者和低收入家庭。

Abstract: Background: The negative effects of the COVID-19 pandemic on the mental health and well-being of populations are an important public health issue. Our study aims to determine the underlying factors shaping mental health trajectories during the COVID-19 pandemic in the UK. Methods: Data from the Understanding Society COVID-19 Study were utilized and the core analysis focussed on GHQ36 scores as the outcome variable. We used GAMs to evaluate trends over time and the role of sociodemographic variables, i.e., age, sex, ethnicity, country of residence (in UK), job status (employment), household income, living with a partner, living with children under age 16, and living with a long-term illness, on the variation of mental health during the study period. Results: Statistically significant differences in mental health were observed for age, sex,ethnicity, country of residence (in UK), job status (employment), household income, living with a partner, living with children under age 16, and living with a long-term illness. Women experienced higher GHQ36 scores relative to men with the GHQ36 score expected to increase by 1.260 (95%CI: 1.176, 1.345). Individuals living without a partner were expected to have higher GHQ36 scores, of 1.050 (95%CI: 0.949, 1.148) more than those living with a partner, and age groups 16-34, 35-44, 45-54, 55-64 experienced higher GHQ36 scores relative to those who were 65+. Individuals with relatively lower household income were likely to have poorer mental health relative to those who were more well off. Conclusion: This study identifies key demographic determinants shaping mental health trajectories during the COVID-19 pandemic in the UK. Policies aiming to reduce mental health inequalities should target women, youth, individuals living without a partner, individuals living with children under 16, individuals with a long-term illness, and lower income families.

</details>


### [67] [MitoFREQ: A Novel Approach for Mitogenome Frequency Estimation from Top-level Haplogroups and Single Nucleotide Variants](https://arxiv.org/abs/2601.10464)
*Mikkel Meyer Andersen,Nicole Huber,Kimberly S Andreaggi,Tóra Oluffa Stenberg Olsen,Walther Parson,Charla Marshall*

Main category: stat.AP

TL;DR: 提出MitoFREQ方法，利用HelixMTdb和gnomAD数据库估算线粒体基因组频率，通过顶级单倍群分类和稀有SNV频率加权，为法医DNA证据提供统计评估。


<details>
  <summary>Details</summary>
Motivation: 高质量全线粒体DNA基因组序列（mitogenomes）在法医遗传学中具有重要价值，但相关群体数据有限，需要开发可靠的方法来估算其群体频率以评估证据价值。

Method: MitoFREQ方法：1）将给定mitogenome分类到30个"顶级单倍群"（TLHG）中；2）使用该TLHG内最稀有SNV的频率，并加权TLHG频率；3）仅需227个特定位置即可完成顶级单倍群分类，适用于低质量样本。

Result: 方法在高质量法医参考数据集和GenBank多样化mitogenomes上测试均表现稳健，产生的似然比范围在100-100,000之间，显著增强了法医mtDNA证据的统计评估能力。

Conclusion: MitoFREQ为法医线粒体DNA分析提供了可靠的群体频率估算方法，开发了开源R包`mitofreq`和Shiny应用，使方法易于使用和定制化。

Abstract: Lineage marker population frequencies can serve as one way to express evidential value in forensic genetics. However, for high-quality whole mitochondrial DNA genome sequences (mitogenomes), population data remain limited. In this paper, we offer a new method, MitoFREQ, for estimating the population frequencies of mitogenomes. MitoFREQ uses the mitogenome resources HelixMTdb and gnomAD, harbouring information from 195,983 and 56,406 mitogenomes, respectively. Neither HelixMTdb nor gnomAD can be queried directly for individual mitogenome frequencies, but offers single nucleotide variant (SNV) allele frequencies for each of 30 "top-level" haplogroups (TLHG). We propose using the HelixMTdb and gnomAD resources by classifying a given mitogenome within the TLHG scheme and subsequently using the frequency of its rarest SNV within that TLHG weighted by the TLHG frequency. We show that this method is guaranteed to provide a higher population frequency estimate than if a refined haplogroup and its SNV frequencies were used. Further, we show that top-level haplogrouping can be achieved by using only 227 specific positions for 99.9% of the tested mitogenomes, potentially making the method available for low-quality samples. The method was tested on two types of datasets: high-quality forensic reference datasets and a diverse collection of scrutinised mitogenomes from GenBank. This dual evaluation demonstrated that the approach is robust across both curated forensic data and broader population-level sequences. This method produced likelihood ratios in the range of 100-100,000, demonstrating its potential to strengthen the statistical evaluation of forensic mtDNA evidence. We have developed an open-source R package `mitofreq` that implements our method, including a Shiny app where custom TLHG frequencies can be supplied.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [68] [Segmentação Comportamental, Do Not Track e o desenvolvimento jurídico europeu e holandês](https://arxiv.org/abs/2601.09711)
*Frederik Zuiderveen Borgesius*

Main category: cs.CY

TL;DR: 欧洲数据保护法适用于行为定向广告，荷兰法律明确推定适用，要求企业遵守公平信息原则，避免秘密或过度数据收集，这些原则可为W3C项目提供灵感。


<details>
  <summary>Details</summary>
Motivation: 探讨欧洲和荷兰法律对行为定向广告的监管发展，分析数据保护法如何适用于在线行为定向，以及如何通过技术设计促进公平信息处理。

Method: 通过分析欧洲和荷兰的法律决策和法规，特别是数据保护法在行为定向广告中的应用，探讨法律要求和技术设计的结合。

Result: 欧洲数据保护法在大多数情况下适用于行为定向广告，荷兰法律明确推定适用，要求企业遵守公平信息原则，避免秘密或过度数据收集。

Conclusion: 数据保护法的公平信息原则可为W3C等标准化组织提供设计灵感，技术设计应促进公平信息处理，平衡商业利益与隐私保护。

Abstract: This paper discusses legal developments in Europe and the Netherlands. Recent decisions show that European data protection law, or privacy law, applies to behavioral targeting in most cases. Dutch law explicitly presumes that data protection law applies to behavioral targeting. This means that companies have to comply with data protection law's fair information principles. For example, companies must refrain from secret or excessive data collection. Perhaps the principles could provide inspiration for future W3C projects. Could technology design foster fair information processing?

</details>


### [69] [Behavioral Targeting, a European Legal Perspective](https://arxiv.org/abs/2601.09712)
*Frederik Zuiderveen Borgesius*

Main category: cs.CY

TL;DR: 本文分析了欧洲法律和政策对行为定向广告（在线用户画像）的监管发展，探讨了当前关于"请勿追踪"标准的讨论和监管挑战。


<details>
  <summary>Details</summary>
Motivation: 行为定向广告（在线用户画像）是一个备受争议的话题，尽管研究表明大多数人不希望接收行为定向广告，但互联网上大量个人信息收集与此相关。万维网联盟正在讨论"请勿追踪"标准，全球监管机构也在努力寻找解决方案。

Method: 本文采用法律和政策分析方法，重点讨论欧洲法律框架和近期政策发展，分析行为定向广告的监管现状和挑战。

Result: 文章揭示了行为定向广告在欧洲的法律监管现状，包括万维网联盟"请勿追踪"标准的讨论进展，以及监管机构在平衡商业利益和个人隐私保护方面面临的困境。

Conclusion: 行为定向广告的监管是一个复杂且持续发展的领域，欧洲法律和政策制定者正在努力应对这一挑战，但需要在技术创新、商业利益和个人隐私保护之间找到平衡。

Abstract: Behavioral targeting, or online profiling, is a hotly debated topic. Much of the collection of personal information on the Internet is related to behavioral targeting, although research suggests that most people don't want to receive behaviorally targeted advertising. The World Wide Web Consortium is discussing a Do Not Track standard, and regulators worldwide are struggling to come up with answers. This article discusses European law and recent policy developments on behavioral targeting.

</details>


### [70] [Filtering for Copyright Enforcement in Europe after the Sabam cases](https://arxiv.org/abs/2601.09739)
*Stefan Kulk,Frederik Zuiderveen Borgesius*

Main category: cs.CY

TL;DR: 欧盟法院判决社交网络和网络服务提供商无需安装版权过滤系统，但这对基本权利保护的实际意义有限


<details>
  <summary>Details</summary>
Motivation: 分析欧盟法院关于版权过滤系统要求的判决对基本权利（隐私和信息自由）的实际影响，探讨这些判决是否真正保护了用户权利

Method: 通过分析欧盟法院的两个最新判决，评估版权集体管理组织Sabam要求安装过滤系统的诉求被驳回后，对隐私和信息自由权利的实际保护效果

Result: 虽然欧盟法院判决社交网络和网络服务提供商无需安装版权过滤系统，但这对隐私和信息自由权利的实际保护作用有限，基本权利并未因此获得实质性胜利

Conclusion: 欧盟法院的判决表面上是网络服务提供商的胜利，但对用户基本权利的保护意义有限，隐私和信息自由并未因此获得实质性保障

Abstract: Sabam, a Belgian collective rights management organisation, wanted an internet access provider and a social network site to install a filter system to enforce copyrights. In two recent judgments, the Court of Justice of the European Union decided that the social network site and the internet access provider cannot be required to install the filter system that Sabam asked for. Are these judgments good news for fundamental rights? This article argues that little is won for privacy and freedom of information.

</details>


### [71] [Critically Engaged Pragmatism: A Scientific Norm and Social, Pragmatist Epistemology for AI Science Evaluation Tools](https://arxiv.org/abs/2601.09753)
*Carole J. Lee*

Main category: cs.CY

TL;DR: 作者警告AI科学评估工具存在"虚假上升推理"风险，主张采用批判性实用主义框架，将AI工具作为科学批判的对象而非客观仲裁者


<details>
  <summary>Details</summary>
Motivation: 针对同行评审危机、研究可重复性问题和AI伪造科学等挑战，科学界对自动化评估工具兴趣增加，但历史表明科学界常将可信度标记去语境化并误用

Method: 提出社会实用主义认识论和"批判性实用主义"新规范，要求科学界严格审查AI评估工具的目的和目的特定可靠性

Result: AI科学评估工具易受目的争议、跨目的可移植性问题以及技术需求优先于认识论适配的影响，导致虚假上升推理风险

Conclusion: AI科学评估工具不应被视为科学可信度的客观仲裁者，而应成为科学界批判性话语实践的对象，通过这种实践来确立科学共同体的可信度

Abstract: Crises in peer review capacity, study replication, and AI-fabricated science have intensified interest in automated tools for assessing scientific research. However, the scientific community has a history of decontextualizing and repurposing credibility markers in inapt ways. I caution that AI science evaluation tools are particularly prone to these kinds of inference by false ascent due to contestation about the purposes to which they should be put, their portability across purposes, and technical demands that prioritize data set size over epistemic fit. To counter this, I argue for a social, pragmatist epistemology and a newly articulated norm of Critically Engaged Pragmatism to enjoin scientific communities to vigorously scrutinize the purposes and purpose-specific reliability of AI science evaluation tools. Under this framework, AI science evaluation tools are not objective arbiters of scientific credibility, but the object of the kinds of critical discursive practices that ground the credibility of scientific communities.

</details>


### [72] [Democracy and Distrust in an Era of Artificial Intelligence](https://arxiv.org/abs/2601.09757)
*Sonia Katyal*

Main category: cs.CY

TL;DR: 本文探讨司法审查如何适应人工智能决策带来的挑战，特别是关于少数群体权利和利益保护，提出AI时代的司法审查理论框架。


<details>
  <summary>Details</summary>
Motivation: 人工智能决策中的私有化、预测和自动化趋势对少数群体构成相似风险，需要重新思考司法审查在AI时代的作用，以保护少数群体免受算法歧视。

Method: 分析AI决策在法庭上受到挑战的案例，探讨正当程序和平等保护原则如何在现代AI时代被重新诠释，并将其整合到AI系统中，提供监督和问责框架。

Result: 提出了一个AI时代的司法审查框架，展示如何通过重新解释宪法原则来保护少数群体权利，并将这些原则整合到AI系统中以实现更好的监督和问责。

Conclusion: 司法审查需要适应AI时代，通过重新诠释正当程序和平等保护原则，并将其整合到AI系统中，可以建立有效的监督机制，保护少数群体免受算法歧视。

Abstract: This essay examines how judicial review should adapt to address challenges posed by artificial intelligence decision-making, particularly regarding minority rights and interests. As I argue in this essay, the rise of three trends-privatization, prediction, and automation in AI-have combined to pose similar risks to minorities. Here, I outline what a theory of judicial review would look like in an era of artificial intelligence, analyzing both the limitations and the possibilities of judicial review of AI. I draw on cases in which AI decision-making has been challenged in courts, to show how concepts of due process and equal protection can be recuperated in a modern AI era, and even integrated into AI, to provide for better oversight and accountability, offering a framework for judicial review in the AI era that protects minorities from algorithmic discrimination.

</details>


### [73] [Strategies of cooperation and defection in five large language models](https://arxiv.org/abs/2601.09849)
*Saptarshi Pal,Abhishek Mallela,Christian Hilbe,Lenz Pracher,Chiyu Wei,Feng Fu,Santiago Schnell,Martin A Nowak*

Main category: cs.CY

TL;DR: 研究探索五个主流大语言模型在重复囚徒困境中的合作策略表现，评估它们是否遵循纳什均衡、进化博弈论预测及人类实验证据，发现模型在多数任务中表现良好但缺乏完全一致性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地用于支持人类决策，特别是在影响他人福利的情况下，需要了解它们如何进行社会决策。重复囚徒困境作为互惠合作的主要隐喻，是评估LLM社会决策能力的理想测试平台。

Method: 首先在中性设置下测量LLM的合作倾向，不使用游戏的标准表述语言。评估LLM实现纳什均衡或其他已知策略类别的程度。然后探索LLM如何适应参数变化：改变游戏的继续概率、收益值、总轮数是否共同知识，以及不同框架的影响。最后进行LLM策略之间的锦标赛，并研究LLM在已知或未知最后一轮的10轮游戏中的直接互动。

Result: 所有LLM在多数任务中表现良好，但没有一个模型在所有任务中表现出完全一致性。模型能够适应参数变化，但策略调整与基本直觉、进化博弈论预测和人类实验证据的符合程度存在差异。

Conclusion: 实验揭示了当前LLM如何实例化互惠合作，表明虽然它们在许多社会决策任务中表现良好，但在策略一致性方面仍有改进空间，这对LLM在影响他人福利的决策应用中的可靠性具有重要意义。

Abstract: Large language models (LLMs) are increasingly deployed to support human decision-making. This use of LLMs has concerning implications, especially when their prescriptions affect the welfare of others. To gauge how LLMs make social decisions, we explore whether five leading models produce sensible strategies in the repeated prisoner's dilemma, which is the main metaphor of reciprocal cooperation. First, we measure the propensity of LLMs to cooperate in a neutral setting, without using language reminiscent of how this game is usually presented. We record to what extent LLMs implement Nash equilibria or other well-known strategy classes. Thereafter, we explore how LLMs adapt their strategies to changes in parameter values. We vary the game's continuation probability, the payoff values, and whether the total number of rounds is commonly known. We also study the effect of different framings. In each case, we test whether the adaptations of the LLMs are in line with basic intuition, theoretical predictions of evolutionary game theory, and experimental evidence from human participants. While all LLMs perform well in many of the tasks, none of them exhibit full consistency over all tasks. We also conduct tournaments between the inferred LLM strategies and study direct interaction between LLMs in games over ten rounds with a known or unknown last round. Our experiments shed light on how current LLMs instantiate reciprocal cooperation.

</details>


### [74] [Modeling conflicting incentives in engineering senior capstone projects: A multi-player game theory approach](https://arxiv.org/abs/2601.09944)
*Richard Q. Blackwell,Eman Hammad,Congrui Jin,Jisoo Park,Albert E. Patterson*

Main category: cs.CY

TL;DR: 本文开发了一个博弈论框架，将工程顶点项目建模为大学、行业赞助商和学生团队之间的序贯贝叶斯博弈，用于分析制度政策如何影响利益相关者行为和项目结果。


<details>
  <summary>Details</summary>
Motivation: 现有的工程顶点项目分析通常非正式或描述性地处理利益相关者行为，忽略了激励冲突、信息不对称和战略依赖。需要建立一个正式的理论框架来系统分析制度政策选择如何塑造利益相关者行为和项目结果。

Method: 开发了一个序贯贝叶斯博弈框架，将大学建模为受约束的Stackelberg领导者，制定课程政策和评估结构，同时考虑赞助商和学生在不完全信息下的战略响应。使用简化形式的结果函数捕捉技术质量、文档质量、及时性、与赞助商需求的匹配度和可发表性，而支付函数反映利益相关者的特定目标和成本。

Result: 在标准假设下，模型产生了稳定的均衡机制，对应实践中观察到的顶点项目动态：合作参与、赞助商主导的剥削和学生成绩博弈。该框架为激励设计、政策权衡和基于项目的学习环境中的结构性失败模式提供了结构化推理基础。

Conclusion: 该博弈论框架作为分析和解释工具，帮助理解制度政策选择如何影响利益相关者行为和项目结果，为未来扩展提供了基础，包括更丰富的动态、重复互动和实证校准，而非作为校准或预测模型。

Abstract: University engineering capstone projects involve sustained interaction among students, faculty, and industry sponsors whose objectives are only partially aligned. While capstones are widely used in engineering education, existing analyses typically treat stakeholder behavior informally or descriptively, leaving incentive conflicts, information asymmetries, and strategic dependencies underexplored. This paper develops a formal game-theoretic framework that models capstone projects as a sequential Bayesian game involving three players: the university, the industry sponsor, and the student team. The framework is intended as an analytical and explanatory tool for understanding how institutional policy choices, such as grading structures, intellectual property rules, and sponsor engagement expectations, shape stakeholder behavior and project outcomes, rather than as a calibrated or predictive model. The university acts as a constrained Stackelberg leader by committing to course policies and assessment structures while anticipating strategic responses by sponsors and students under incomplete information. Reduced-form outcome functions capture technical quality, documentation quality, timeliness, alignment with sponsor needs, and publishability, while payoff functions reflect stakeholder-specific objectives and costs. Under standard assumptions, the model admits stable equilibrium regimes that correspond to empirically recognizable capstone dynamics observed in practice, including cooperative engagement, sponsor-dominated exploitation, and student grade gaming. Rather than claiming precise prediction, the framework provides a structured basis for reasoning about incentive design, policy tradeoffs, and structural failure modes in project-based learning environments, as well as for future extensions incorporating richer dynamics, repeated interaction, and empirical calibration.

</details>


### [75] [Brief but Impactful: How Human Tutoring Interactions Shape Engagement in Online Learning](https://arxiv.org/abs/2601.09994)
*Conrad Borchers,Ashish Gurung,Qinyi Liu,Danielle R. Thomas,Mohammad Khalil,Kenneth R. Koedinger*

Main category: cs.CY

TL;DR: 研究显示，人类导师的短暂干预能提升学生在数学学习中的参与度，且时机和对话质量比干预时长更重要


<details>
  <summary>Details</summary>
Motivation: 学习分析可以指导人类导师有效解决AI系统难以支持的学习动机障碍。学生获得人类关注时会更投入，但需要了解短暂干预期间发生了什么以及何时最有效

Method: 将学生-导师对话转录与MATHia辅导系统日志数据对齐，分析191名中学生2,075小时的课堂数学练习数据。使用混合效应模型分析参与度变化，并创建分析工具识别最有效的导师-学生对话

Result: 参与度在人类导师访问期间和之后都保持较高水平；访问时长呈现边际效益递减；后期访问比早期访问产生更大的即时提升，但早期访问对防止参与度下降很重要；具体、分步的脚手架式对话最能提升参与度

Conclusion: 在资源有限的情况下，应优先安排多次简短、时机恰当的检查，并确保至少有一次早期接触。分析工具可以指导支持学生的优先级排序，并实时展示有效的导师策略

Abstract: Learning analytics can guide human tutors to efficiently address motivational barriers to learning that AI systems struggle to support. Students become more engaged when they receive human attention. However, what occurs during short interventions, and when are they most effective? We align student-tutor dialogue transcripts with MATHia tutoring system log data to study brief human-tutor interactions on Zoom drawn from 2,075 hours of 191 middle school students' classroom math practice. Mixed-effect models reveal that engagement, measured as successful solution steps per minute, is higher during a human-tutor visit and remains elevated afterward. Visit length exhibits diminishing returns: engagement rises during and shortly after visits, irrespective of visit length. Timing also matters: later visits yield larger immediate lifts than earlier ones, though an early visit remains important to counteract engagement decline. We create analytics that identify which tutor-student dialogues raise engagement the most. Qualitative analysis reveals that interactions with concrete, stepwise scaffolding with explicit work organization elevate engagement most strongly. We discuss implications for resource-constrained tutoring, prioritizing several brief, well-timed check-ins by a human tutor while ensuring at least one early contact. Our analytics can guide the prioritization of students for support and surface effective tutor moves in real-time.

</details>


### [76] [STEAMROLLER: A Multi-Agent System for Inclusive Automatic Speech Recognition for People who Stutter](https://arxiv.org/abs/2601.10223)
*Ziqi Xu,Yi Liu,Yuekang Li,Ling Shi,Kailong Wang,Yongxin Zhao*

Main category: cs.CY

TL;DR: STEAMROLLER是一个实时系统，通过多阶段多智能体AI流水线将口吃语音转换为流畅输出，解决当前语音识别系统对口吃人群的排斥问题。


<details>
  <summary>Details</summary>
Motivation: 口吃人群在当前依赖流畅语音的语音助手、认证系统和远程工作工具中面临系统性排斥。现有的自动语音识别系统主要基于流畅语音训练，无法服务全球数百万口吃者。

Method: 采用三阶段架构：ASR转录、多智能体文本修复和语音合成。核心创新在于协作式多智能体框架，迭代优化转录文本同时保持语义意图。解决了直接语音转换困难、ASR转录语义失真和实时通信延迟三大技术挑战。

Result: 在FluencyBank数据集上的实验和用户研究表明，系统显著降低了词错误率，获得了较高的用户满意度。此外，使用STEAMROLLER修复的语音微调ASR模型还能带来额外的WER改进。

Conclusion: STEAMROLLER不仅提供即时可访问性益处，还为创建包容性AI生态系统开辟了途径，通过修复语音微调ASR模型可以进一步提升系统性能。

Abstract: People who stutter (PWS) face systemic exclusion in today's voice-driven society, where access to voice assistants, authentication systems, and remote work tools increasingly depends on fluent speech. Current automatic speech recognition (ASR) systems, trained predominantly on fluent speech, fail to serve millions of PWS worldwide. We present STEAMROLLER, a real time system that transforms stuttered speech into fluent output through a novel multi-stage, multi-agent AI pipeline. Our approach addresses three critical technical challenges: (1) the difficulty of direct speech to speech conversion for disfluent input, (2) semantic distortions introduced during ASR transcription of stuttered speech, and (3) latency constraints for real time communication. STEAMROLLER employs a three stage architecture comprising ASR transcription, multi-agent text repair, and speech synthesis, where our core innovation lies in a collaborative multi-agent framework that iteratively refines transcripts while preserving semantic intent. Experiments on the FluencyBank dataset and a user study demonstrates clear word error rate (WER) reduction and strong user satisfaction. Beyond immediate accessibility benefits, fine tuning ASR on STEAMROLLER repaired speech further yields additional WER improvements, creating a pathway toward inclusive AI ecosystems.

</details>


### [77] [Atelier à la conférence IHM 2025 : RA Permanente](https://arxiv.org/abs/2601.10291)
*Maxime Cauz,Thibaut Septon,Elise Hallaert,Theo Leclercq,Bruno Dumas,Charles Bailly,Clement Tyminski,Matias Peraza,Sophie Lepreux,Emmanuel Dubois*

Main category: cs.CY

TL;DR: 该研讨会旨在召集对普适增强现实（PAR）感兴趣的研究者，通过集体智慧识别实现该技术日常应用所需解决的跨学科挑战和必要保障措施。


<details>
  <summary>Details</summary>
Motivation: 随着普适计算的发展，普适增强现实（PAR）可能引发人、计算与世界关系的重大变革。这种持续增强世界的体验既有益处也有潜在负面影响，需要在多个领域探讨相关问题。

Method: 通过IHM'25会议研讨会形式，汇集所有关心或热衷于讨论PAR主题的参会者，利用集体智慧进行跨学科讨论。

Result: 将讨论结果分组归类，定义了围绕永久增强现实未来主要研究领域的一系列问题，包括技术挑战、安全保障措施等。

Conclusion: 研讨会成功识别了实现PAR技术日常应用所需的跨学科挑战和必要保障措施，为未来研究指明了方向，并探讨了PAR是否过于技术乐观的问题。

Abstract: As we move towards more ubiquitous computing, the concept of pervasive augmented reality (PAR) could lead to a major evolution in the relationship between humans, computing and the world. The experience of a continuously augmented world can have both benefits and undesirable consequences for users' lives, and raises many questions in multiple areas. In this workshop, we wanted to bring together all IHM'25 conference participants who are concerned or enthusiastic about discussing this topic. The aim was to draw on collective intelligence to identify the interdisciplinary challenges that remain to be resolved in order to enable the implementation of these technologies in everyday life, but also to define the necessary safeguards. Is PAR too techno-enthusiastic? All of these elements were grouped into categories to define a set of future major areas of research around permanent augmented reality. This document is in French as the conference is a French-speaking international conference.

</details>


### [78] [Job Anxiety in Post-Secondary Computer Science Students Caused by Artificial Intelligence](https://arxiv.org/abs/2601.10468)
*Daniyaal Farooqi,Gavin Pu,Shreyasha Paudel,Sharifa Sultana,Syed Ishtiaque Ahmed*

Main category: cs.CY

TL;DR: 计算机科学学生对AI替代工作的焦虑研究：通过访谈发现学生面临工作不安全感，采取不同应对策略，国际学生压力更大，可能导致AI领域过度饱和和计算机科学专业吸引力下降。


<details>
  <summary>Details</summary>
Motivation: 随着AI的广泛应用，企业采用AI提高效率和增加收入，但这也导致员工被AI替代，引发工作不安全感和不确定性。计算机科学学生即将进入职场，特别容易受到这种焦虑的影响。研究旨在调查计算机科学学生对AI替代工作的焦虑程度及其影响。

Method: 采用半结构化访谈方法，对多伦多大学计算机科学本科和研究生项目的25名学生进行访谈，通过主题分析确定AI替代工作焦虑的程度和学生的应对策略。

Result: 研究发现计算机科学学生确实面临AI替代工作带来的压力和焦虑，并采取不同策略应对。软件工程和Web开发等子领域被认为易受AI替代，而量子计算和AI研究等专业领域被认为更安全。学生通过使用更多AI技术、学习AI课程、攻读AI研究生来提升技能，或转向被认为不易受AI影响的领域。国际学生因需要获得永久居留权而面临额外焦虑。

Conclusion: 研究结果表明计算机科学职业安全感低，可能导致计算机科学学生过度集中于AI领域，并可能劝阻未来学生选择计算机科学专业。这些发现对教育政策、职业指导和劳动力市场规划具有重要意义。

Abstract: The emerging widespread usage of AI has led to industry adoption to improve efficiency and increase earnings. However, a major consequence of this is AI displacing employees from their jobs, leading to feelings of job insecurity and uncertainty. This is especially true for computer science students preparing to enter the workforce. To investigate this, we performed semi-structured interviews with (n = 25) students across computer science undergraduate and graduate programs at the University of Toronto to determine the extent of job replacement anxiety. Through thematic analysis, it was determined that computer science students indeed face stress and anxiety from AI displacement of jobs, leading to different strategies of managing pressure. Subfields such as software engineering and web development are strongly believed to be vulnerable to displacement, while specialized subfields like quantum computing and AI research are deemed more secure. Many students feel compelled to upskill by using more AI technologies, taking AI courses, and specializing in AI through graduate school. Some students also reskill by pursuing other fields of study seen as less vulnerable to AI displacement. Finally, international students experience additional job replacement anxiety because of pressure to secure permanent residence. Implications of these findings include feelings of low security in computer science careers, oversaturation of computer science students pursuing AI, and potential dissuasion of future university students from pursuing computer science.

</details>


### [79] [Institutional AI: A Governance Framework for Distributional AGI Safety](https://arxiv.org/abs/2601.10599)
*Federico Pierucci,Marcello Galisai,Marcantonio Syrnikov Bracale,Matteo Prandi,Piercosma Bisconti,Francesco Giarrusso,Olga Sorokoletova,Vincenzo Suriani,Daniele Nardi*

Main category: cs.CY

TL;DR: 论文提出"制度性AI"框架，将AI对齐问题从单一模型扩展到多智能体系统的治理层面，通过运行时监控、激励机制和规范执行来解决智能体集体中的结构性风险。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的系统越来越多地作为智能体嵌入人类社交和技术系统中，对齐不能再被视为孤立模型的属性，而必须理解为与智能体运行环境相关的问题。即使是最先进的对齐方法（如RLHF或RLAIF）也无法确保在内部目标结构与开发者意图偏离时的控制。

Method: 提出"制度性AI"的系统级方法，将对齐视为AI智能体集体的有效治理问题。构建治理图，通过运行时监控、通过奖励和惩罚塑造激励机制、明确规范和执法角色来约束智能体。

Result: 识别了三个源于AI模型核心属性的结构性问题：1）行为目标独立性；2）自然语言约束的工具性覆盖；3）智能体对齐漂移。提出了从软件工程到机制设计的范式转变。

Conclusion: AI安全需要从单一模型对齐转向多智能体系统的制度性治理，通过改变AI智能体集体的收益格局来实现真正的对齐，这代表了AI对齐研究的重要范式转变。

Abstract: As LLM-based systems increasingly operate as agents embedded within human social and technical systems, alignment can no longer be treated as a property of an isolated model, but must be understood in relation to the environments in which these agents act. Even the most sophisticated methods of alignment, such as Reinforcement Learning through Human Feedback (RHLF) or through AI Feedback (RLAIF) cannot ensure control once internal goal structures diverge from developer intent. We identify three structural problems that emerge from core properties of AI models: (1) behavioral goal-independence, where models develop internal objectives and misgeneralize goals; (2) instrumental override of natural-language constraints, where models regard safety principles as non-binding while pursuing latent objectives, leveraging deception and manipulation; and (3) agentic alignment drift, where individually aligned agents converge to collusive equilibria through interaction dynamics invisible to single-agent audits. The solution this paper advances is Institutional AI: a system-level approach that treats alignment as a question of effective governance of AI agent collectives. We argue for a governance-graph that details how to constrain agents via runtime monitoring, incentive shaping through prizes and sanctions, explicit norms and enforcement roles. This institutional turn reframes safety from software engineering to a mechanism design problem, where the primary goal of alignment is shifting the payoff landscape of AI agent collectives.

</details>


### [80] [The Conversational Exam: A Scalable Assessment Design for the AI Era](https://arxiv.org/abs/2601.10691)
*Lorena A. Barba,Laura Stegner*

Main category: cs.CY

TL;DR: 论文提出"对话式考试"作为可扩展的口试形式，让学生在实时编码并解释推理过程中展示真实能力，解决生成式AI导致的评估失效问题


<details>
  <summary>Details</summary>
Motivation: 传统评估方法在学生使用生成式AI完成作业但缺乏真实参与时会失效，造成"能力幻觉"——学生以为自己学会了但实际上没有。许多教育者面临两难：要么完全禁止AI，要么接受有效评估已不可能

Method: 提出对话式考试：可扩展的口试形式，学生实时编码并解释推理过程。基于人机交互原则，在58名学生的小组中进行实验，仅用两天时间证明口试可以扩展到典型班级规模。结合真实实践（学生使用文档和监督下的AI访问）与内在有效性（实时表现无法伪造）

Result: 研究表明口试可以扩展到典型班级规模，仅用两天时间就在58名学生的小组中成功实施。对话式考试恢复了评估的有效性，因为实时表现无法伪造

Conclusion: 对话式考试为教育者提供了实用路径，解决了AI时代评估的困境。论文提供详细的实施指南，帮助教师采用这种方法，在禁止AI或接受评估失效之间找到可行的中间道路

Abstract: Traditional assessment methods collapse when students use generative AI to complete work without genuine engagement, creating an illusion of competence where they believe they're learning but aren't. This paper presents the conversational exam -- a scalable oral examination format that restores assessment validity by having students code live while explaining their reasoning. Drawing on human-computer interaction principles, we examined 58 students in small groups across just two days, demonstrating that oral exams can scale to typical class sizes. The format combines authentic practice (students work with documentation and supervised AI access) with inherent validity (real-time performance cannot be faked). We provide detailed implementation guidance to help instructors adapt this approach, offering a practical path forward when many educators feel paralyzed between banning AI entirely or accepting that valid assessment is impossible.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [81] [Forward-only learning in memristor arrays with month-scale stability](https://arxiv.org/abs/2601.09903)
*Adrien Renaudineau,Mamadou Hawa Diallo,Théo Dupuis,Bastien Imbert,Mohammed Akib Iftakher,Kamel-Eddine Harabi,Clément Turck,Tifenn Hirtzlin,Djohan Bonnet,Franck Melul,Jorge-Daniel Aguirre-Morales,Elisa Vianello,Marc Bocquet,Jean-Michel Portal,Damien Querlioz*

Main category: cs.ET

TL;DR: 该论文提出了一种在标准HfOx/Ti忆阻器阵列上实现低能耗片上学习的方法，通过亚1V复位脉冲和仅前向训练算法，在ImageNet分辨率四分类任务上达到接近反向传播的准确率。


<details>
  <summary>Details</summary>
Motivation: 忆阻器阵列虽然能高效进行推理，但片上学习面临高能耗、器件磨损、模拟状态漂移以及反向传播需要反向信号流等问题。需要找到一种实用的片上学习方法。

Method: 采用两种设计：1) 利用标准HfOx/Ti忆阻器支持亚1V复位脉冲的特性，降低能耗、提高耐久性并保持模拟状态稳定；2) 使用仅前向训练算法（基于Hinton的Forward-Forward），包括双通道监督Forward-Forward和单通道竞争规则。

Result: 在8,064个器件的阵列上训练两层分类器，两种仅前向方法在ImageNet分辨率四分类任务上分别达到89.5%和89.6%的测试准确率，接近反向传播的90.0%。亚1V复位更新比传统编程能耗降低460倍，仅比推理操作多46%能耗。训练模型在环境条件下至少保持一个月准确率稳定。

Conclusion: 该研究在标准忆阻器阵列上实现了亚1V、仅前向的片上学习，为自适应边缘智能提供了一条实用且脉冲感知的技术路线。

Abstract: Turning memristor arrays from efficient inference engines into systems capable of on-chip learning has proved difficult. Weight updates have a high energy cost and cause device wear, analog states drift, and backpropagation requires a backward pass with reversed signal flow. Here we experimentally demonstrate learning on standard filamentary HfOx/Ti arrays that addresses these challenges with two design choices. First, we realize that standard filamentary HfOx/Ti memristors support sub-1 V reset-only pulses that cut energy, improve endurance, and yield stable analog states. Second, we rely on forward-only training algorithms derived from Hinton's Forward-Forward that use only inference-style operations. We train two-layer classifiers on an ImageNet-resolution four-class task using arrays up to 8,064 devices. Two forward-only variants, the double-pass supervised Forward-Forward and a single-pass competitive rule, achieve test accuracies of 89.5% and 89.6%, respectively; a reference experiment using backpropagation reaches 90.0%. Across five independent runs per method, these accuracies match within statistical uncertainty. Trained models retain accuracy for at least one month under ambient conditions, consistent with the stability of reset-only states. Sub-1 V reset updates use 460 times less energy than conventional program-and-verify programming and require just 46% more energy than inference-only operation. Together, these results establish forward-only, sub-1 V learning on standard filamentary stacks at array scale, outlining a practical, pulse-aware route to adaptive edge intelligence.

</details>


### [82] [Resistive Memory based Efficient Machine Unlearning and Continual Learning](https://arxiv.org/abs/2601.10037)
*Ning Lin,Jichang Yang,Yangu He,Zijian Ye,Kwun Hang Wong,Xinyuan Zhang,Songqi Wang,Yi Li,Kemi Xu,Leo Yu Zhang,Xiaoming Chen,Dashan Shang,Han Wang,Xiaojuan Qi,Zhongrui Wang*

Main category: cs.ET

TL;DR: 提出一种硬件-软件协同设计，通过低秩适配框架和混合模拟-数字存内计算系统，实现阻变存储器上的高效机器遗忘和持续学习，显著降低训练、部署和推理成本。


<details>
  <summary>Details</summary>
Motivation: 阻变存储器神经形态系统支持持续学习但缺乏主动遗忘机制，而现有机器遗忘方案在阻变存储器硬件上编程开销过高，模拟实现持续学习也面临类似障碍。

Method: 软件层面采用低秩适配框架将更新限制在紧凑参数分支；硬件层面开发混合模拟-数字存内计算系统，将训练好的权重存储在模拟阻变存储器阵列，动态更新在带SRAM缓冲的数字计算单元实现。

Result: 在180nm CMOS工艺原型上，在面部识别、说话人认证和风格化图像生成等隐私敏感任务中，训练成本降低147.76倍，部署开销降低387.95倍，推理能耗降低48.44倍。

Conclusion: 该硬件-软件协同设计为边缘设备实现安全高效的神经形态智能铺平了道路，解决了阻变存储器上机器遗忘和持续学习的高开销问题。

Abstract: Resistive memory (RM) based neuromorphic systems can emulate synaptic plasticity and thus support continual learning, but they generally lack biologically inspired mechanisms for active forgetting, which are critical for meeting modern data privacy requirements. Algorithmic forgetting, or machine unlearning, seeks to remove the influence of specific data from trained models to prevent memorization of sensitive information and the generation of harmful content, yet existing exact and approximate unlearning schemes incur prohibitive programming overheads on RM hardware owing to device variability and iterative write-verify cycles. Analogue implementations of continual learning face similar barriers. Here we present a hardware-software co-design that enables an efficient training, deployment and inference pipeline for machine unlearning and continual learning on RM accelerators. At the software level, we introduce a low-rank adaptation (LoRA) framework that confines updates to compact parameter branches, substantially reducing the number of trainable parameters and therefore the training cost. At the hardware level, we develop a hybrid analogue-digital compute-in-memory system in which well-trained weights are stored in analogue RM arrays, whereas dynamic LoRA updates are implemented in a digital computing unit with SRAM buffer. This hybrid architecture avoids costly reprogramming of analogue weights and maintains high energy efficiency during inference. Fabricated in a 180 nm CMOS process, the prototype achieves up to a 147.76-fold reduction in training cost, a 387.95-fold reduction in deployment overhead and a 48.44-fold reduction in inference energy across privacy-sensitive tasks including face recognition, speaker authentication and stylized image generation, paving the way for secure and efficient neuromorphic intelligence at the edge.

</details>

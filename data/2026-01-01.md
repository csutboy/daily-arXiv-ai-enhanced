<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 31]
- [econ.EM](#econ.EM) [Total: 4]
- [stat.AP](#stat.AP) [Total: 4]
- [cs.ET](#cs.ET) [Total: 4]
- [cs.SI](#cs.SI) [Total: 5]
- [cs.CY](#cs.CY) [Total: 9]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [The Drill-Down and Fabricate Test (DDFT): A Protocol for Measuring Epistemic Robustness in Language Models](https://arxiv.org/abs/2512.23850)
*Rahul Baxi*

Main category: cs.AI

TL;DR: 论文提出DDFT协议评估语言模型的认知稳健性，发现模型规模与稳健性无关，错误检测能力是关键瓶颈


<details>
  <summary>Details</summary>
Motivation: 当前语言模型评估只测量理想条件下的知识，无法评估在现实压力下的稳健性。静态基准测试无法区分缺乏知识的模型和验证机制在信息退化或对抗性攻击下崩溃的模型。

Method: 引入Drill-Down and Fabricate Test (DDFT)协议，测量认知稳健性：模型在渐进语义压缩和对抗性伪造下保持事实准确性的能力。提出两系统认知模型（语义系统和认知验证器），评估9个前沿模型在8个知识领域、5个压缩水平下的表现（共1800次轮级评估）。

Result: 认知稳健性与传统设计范式正交。参数数量(r=0.083)和架构类型(r=0.153)均不显著预测稳健性。错误检测能力强烈预测整体稳健性(rho=-0.817, p=0.007)，是关键瓶颈。旗舰模型尽管规模大但表现脆弱，而较小模型可以达到稳健性能。

Conclusion: 认知稳健性源于训练方法和验证机制，与当前方法不同。DDFT框架为关键应用部署前评估认知稳健性提供了理论基础和实用工具，挑战了模型规模与可靠性关系的假设。

Abstract: Current language model evaluations measure what models know under ideal conditions but not how robustly they know it under realistic stress. Static benchmarks like MMLU and TruthfulQA cannot distinguish a model that lacks knowledge from one whose verification mechanisms collapse when information degrades or adversaries probe for weaknesses. We introduce the Drill-Down and Fabricate Test (DDFT), a protocol that measures epistemic robustness: a model's ability to maintain factual accuracy under progressive semantic compression and adversarial fabrication. We propose a two-system cognitive model comprising a Semantic System that generates fluent text and an Epistemic Verifier that validates factual accuracy. Our findings, based on evaluating 9 frontier models across 8 knowledge domains at 5 compression levels (1,800 turn-level evaluations), reveal that epistemic robustness is orthogonal to conventional design paradigms. Neither parameter count (r=0.083, p=0.832) nor architectural type (r=0.153, p=0.695) significantly predicts robustness, suggesting it emerges from training methodology and verification mechanisms distinct from current approaches. Error detection capability strongly predicts overall robustness (rho=-0.817, p=0.007), indicating this is the critical bottleneck. We find that flagship models exhibit brittleness despite their scale, while smaller models can achieve robust performance, challenging assumptions about the relationship between model size and reliability. The DDFT framework provides both theoretical foundation and practical tools for assessing epistemic robustness before deployment in critical applications.

</details>


### [2] [CASCADE: Cumulative Agentic Skill Creation through Autonomous Development and Evolution](https://arxiv.org/abs/2512.23880)
*Xu Huang,Junwu Chen,Yuxing Fei,Zhuohan Li,Philippe Schwaller,Gerbrand Ceder*

Main category: cs.AI

TL;DR: CASCADE是一个自演化的LLM智能体框架，通过持续学习和自我反思的元技能，使智能体能够掌握复杂外部工具并编码知识，在科学任务中实现93.3%的成功率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体依赖于预定义工具或脆弱的工具生成，限制了其在复杂科学任务中的能力和适应性，需要从"LLM+工具使用"向"LLM+技能获取"转变。

Method: CASCADE框架通过两种元技能实现：1) 持续学习（通过网页搜索和代码提取），2) 自我反思（通过内省和知识图谱探索）。框架还包括人机协作和记忆巩固机制。

Result: 在SciSkillBench基准测试（116个材料科学和化学研究任务）中，使用GPT-5的CASCADE达到93.3%成功率，相比无演化机制的35.4%有显著提升。成功应用于计算分析、自主实验室实验和论文选择性复现。

Conclusion: CASCADE能够积累可执行的技能，这些技能可以在智能体和科学家之间共享，推动了可扩展的AI辅助科学研究的发展。

Abstract: Large language model (LLM) agents currently depend on predefined tools or brittle tool generation, constraining their capability and adaptability to complex scientific tasks. We introduce CASCADE, a self-evolving agentic framework representing an early instantiation of the transition from "LLM + tool use" to "LLM + skill acquisition". CASCADE enables agents to master complex external tools and codify knowledge through two meta-skills: continuous learning via web search and code extraction, and self-reflection via introspection and knowledge graph exploration, among others. We evaluate CASCADE on SciSkillBench, a benchmark of 116 materials science and chemistry research tasks. CASCADE achieves a 93.3% success rate using GPT-5, compared to 35.4% without evolution mechanisms. We further demonstrate real-world applications in computational analysis, autonomous laboratory experiments, and selective reproduction of published papers. Along with human-agent collaboration and memory consolidation, CASCADE accumulates executable skills that can be shared across agents and scientists, moving toward scalable AI-assisted scientific research.

</details>


### [3] [A Proof-of-Concept for Explainable Disease Diagnosis Using Large Language Models and Answer Set Programming](https://arxiv.org/abs/2512.23932)
*Ioanna Gemou,Evangelos Lamprou*

Main category: cs.AI

TL;DR: McCoy框架结合大语言模型与答案集编程，通过LLM将医学文献转化为ASP代码，结合患者数据进行疾病诊断，实现可解释的预测系统。


<details>
  <summary>Details</summary>
Motivation: 准确的疾病预测对及时干预和有效治疗至关重要。虽然符号AI已在医疗领域应用，但由于构建高质量知识库需要大量努力，其采用仍然有限。需要克服这一障碍。

Method: McCoy框架结合大语言模型与答案集编程：1) 使用LLM将医学文献翻译成ASP代码；2) 将生成的ASP代码与患者数据结合；3) 使用ASP求解器处理得到最终诊断。

Result: 初步结果显示，McCoy在小规模疾病诊断任务上表现出色，具有强大的性能。

Conclusion: McCoy通过整合LLM和ASP，创建了一个强大且可解释的预测框架，充分利用了两种范式的优势，为医疗诊断提供了新方法。

Abstract: Accurate disease prediction is vital for timely intervention, effective treatment, and reducing medical complications. While symbolic AI has been applied in healthcare, its adoption remains limited due to the effort required for constructing high-quality knowledge bases. This work introduces McCoy, a framework that combines Large Language Models (LLMs) with Answer Set Programming (ASP) to overcome this barrier. McCoy orchestrates an LLM to translate medical literature into ASP code, combines it with patient data, and processes it using an ASP solver to arrive at the final diagnosis. This integration yields a robust, interpretable prediction framework that leverages the strengths of both paradigms. Preliminary results show McCoy has strong performance on small-scale disease diagnosis tasks.

</details>


### [4] [SPARK: Search Personalization via Agent-Driven Retrieval and Knowledge-sharing](https://arxiv.org/abs/2512.24008)
*Gaurab Chhetri,Subasish Das,Tausif Islam Chowdhury*

Main category: cs.AI

TL;DR: SPARK是一个基于多智能体LLM的个性化搜索框架，通过角色化智能体协作实现动态检索和个性化，模拟人类信息寻求行为的复杂性。


<details>
  <summary>Details</summary>
Motivation: 传统搜索系统受限于静态用户画像和单一检索流程，难以捕捉用户动态、多维的信息需求变化。需要一种能够模拟人类信息寻求行为复杂性、流动性和上下文敏感性的新一代搜索系统。

Method: 1. 定义角色空间（角色、专业知识、任务上下文、领域）；2. 引入角色协调器动态解析查询并激活相关专业智能体；3. 每个智能体执行独立的检索增强生成过程，配备长短期记忆存储和上下文感知推理模块；4. 通过结构化通信协议（共享内存库、迭代辩论、接力式知识传递）促进智能体协作；5. 基于认知架构、多智能体协调理论和信息检索原理设计。

Result: 框架产生了关于协调效率、个性化质量和认知负载分布的可测试预测，同时包含自适应学习机制用于持续角色优化。通过细粒度智能体专业化与协作检索的整合，为下一代搜索系统提供了洞察。

Conclusion: SPARK框架展示了如何通过分布式智能体行为和最小协调规则产生涌现个性化特性，为能够捕捉人类信息寻求行为复杂性、流动性和上下文敏感性的下一代搜索系统提供了理论基础和实践指导。

Abstract: Personalized search demands the ability to model users' evolving, multi-dimensional information needs; a challenge for systems constrained by static profiles or monolithic retrieval pipelines. We present SPARK (Search Personalization via Agent-Driven Retrieval and Knowledge-sharing), a framework in which coordinated persona-based large language model (LLM) agents deliver task-specific retrieval and emergent personalization. SPARK formalizes a persona space defined by role, expertise, task context, and domain, and introduces a Persona Coordinator that dynamically interprets incoming queries to activate the most relevant specialized agents. Each agent executes an independent retrieval-augmented generation process, supported by dedicated long- and short-term memory stores and context-aware reasoning modules. Inter-agent collaboration is facilitated through structured communication protocols, including shared memory repositories, iterative debate, and relay-style knowledge transfer. Drawing on principles from cognitive architectures, multi-agent coordination theory, and information retrieval, SPARK models how emergent personalization properties arise from distributed agent behaviors governed by minimal coordination rules. The framework yields testable predictions regarding coordination efficiency, personalization quality, and cognitive load distribution, while incorporating adaptive learning mechanisms for continuous persona refinement. By integrating fine-grained agent specialization with cooperative retrieval, SPARK provides insights for next-generation search systems capable of capturing the complexity, fluidity, and context sensitivity of human information-seeking behavior.

</details>


### [5] [ROAD: Reflective Optimization via Automated Debugging for Zero-Shot Agent Alignment](https://arxiv.org/abs/2512.24040)
*Natchaya Temyingyong,Daman Jain,Neeraj Kumarsahu,Prabhat Kumar,Rachata Phondi,Wachiravit Modecrua,Krittanon Kaewtawee,Krittin Pachtrachai,Touchapon Kraisingkorn*

Main category: cs.AI

TL;DR: ROAD是一个无需标注数据集的自动提示优化框架，通过多智能体架构将失败日志转化为决策树协议，在冷启动场景下实现高效优化。


<details>
  <summary>Details</summary>
Motivation: 现实软件工程中，LLM智能体开发初期通常缺乏标注数据集，只有混乱的生产日志和不断演化的失败模式，需要一种无需黄金标准开发集的优化方法。

Method: 采用多智能体架构：分析器进行根因分析，优化器进行模式聚合，教练进行策略集成，将非结构化失败日志转化为结构化的决策树协议，模拟人类工程调试循环。

Result: 在学术基准和生产知识管理引擎上评估，仅3次自动迭代就使成功率提升5.6%（73.6%到79.2%），搜索准确率提升3.8%；在零售领域复杂推理任务上，性能相对基线提升约19%。

Conclusion: 模拟人类工程调试循环（失败分析和补丁）为部署可靠LLM智能体提供了资源高效的数据驱动替代方案，避免了资源密集的强化学习训练。

Abstract: Automatic Prompt Optimization (APO) has emerged as a critical technique for enhancing Large Language Model (LLM) performance, yet current state-of-the-art methods typically rely on large, labeled gold-standard development sets to compute fitness scores for evolutionary or Reinforcement Learning (RL) approaches. In real-world software engineering, however, such curated datasets are rarely available during the initial cold start of agent development, where engineers instead face messy production logs and evolving failure modes. We present ROAD (Reflective Optimization via Automated Debugging), a novel framework that bypasses the need for refined datasets by treating optimization as a dynamic debugging investigation rather than a stochastic search. Unlike traditional mutation strategies, ROAD utilizes a specialized multi-agent architecture, comprising an Analyzer for root-cause analysis, an Optimizer for pattern aggregation, and a Coach for strategy integration, to convert unstructured failure logs into robust, structured Decision Tree Protocols. We evaluated ROAD across both a standardized academic benchmark and a live production Knowledge Management engine. Experimental results demonstrate that ROAD is highly sample-efficient, achieving a 5.6 percent increase in success rate (73.6 percent to 79.2 percent) and a 3.8 percent increase in search accuracy within just three automated iterations. Furthermore, on complex reasoning tasks in the retail domain, ROAD improved agent performance by approximately 19 percent relative to the baseline. These findings suggest that mimicking the human engineering loop of failure analysis and patching offers a viable, data-efficient alternative to resource-intensive RL training for deploying reliable LLM agents.

</details>


### [6] [LoongFlow: Directed Evolutionary Search via a Cognitive Plan-Execute-Summarize Paradigm](https://arxiv.org/abs/2512.24077)
*Chunhui Wan,Xunan Dai,Zhuo Wang,Minglei Li,Yanpeng Wang,Yinan Mao,Yu Lan,Zhiwen Xiao*

Main category: cs.AI

TL;DR: LoongFlow是一个自进化智能体框架，通过将LLM集成到"计划-执行-总结"认知范式中，将进化搜索映射为推理密集型过程，显著提高了进化效率并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统进化方法缺乏结构化推理，导致从静态LLM向自改进智能体过渡困难，现有方法在高维代码空间中面临早熟收敛和低效探索的问题。

Method: 1. 集成LLM到"计划-执行-总结"认知范式；2. 采用混合进化记忆系统，结合多岛模型、MAP-Elites和自适应玻尔兹曼选择；3. 实例化为通用智能体（算法发现）和ML智能体（管道优化）。

Result: 在AlphaEvolve基准测试和Kaggle竞赛中，LoongFlow比OpenEvolve、ShinkaEvolve等基线方法进化效率提升高达60%，并能发现更优解决方案。

Conclusion: LoongFlow在自主科学发现方面迈出重要一步，能够以更低的计算开销生成专家级解决方案，平衡了探索与利用的权衡。

Abstract: The transition from static Large Language Models (LLMs) to self-improving agents is hindered by the lack of structured reasoning in traditional evolutionary approaches. Existing methods often struggle with premature convergence and inefficient exploration in high-dimensional code spaces. To address these challenges, we introduce LoongFlow, a self-evolving agent framework that achieves state-of-the-art solution quality with significantly reduced computational costs. Unlike "blind" mutation operators, LoongFlow integrates LLMs into a cognitive "Plan-Execute-Summarize" (PES) paradigm, effectively mapping the evolutionary search to a reasoning-heavy process. To sustain long-term architectural coherence, we incorporate a hybrid evolutionary memory system. By synergizing Multi-Island models with MAP-Elites and adaptive Boltzmann selection, this system theoretically balances the exploration-exploitation trade-off, maintaining diverse behavioral niches to prevent optimization stagnation. We instantiate LoongFlow with a General Agent for algorithmic discovery and an ML Agent for pipeline optimization. Extensive evaluations on the AlphaEvolve benchmark and Kaggle competitions demonstrate that LoongFlow outperforms leading baselines (e.g., OpenEvolve, ShinkaEvolve) by up to 60% in evolutionary efficiency while discovering superior solutions. LoongFlow marks a substantial step forward in autonomous scientific discovery, enabling the generation of expert-level solutions with reduced computational overhead.

</details>


### [7] [CogRec: A Cognitive Recommender Agent Fusing Large Language Models and Soar for Explainable Recommendation](https://arxiv.org/abs/2512.24113)
*Jiaxin Hu,Tao Wang,Bingsan Yang,Hongrun Wang*

Main category: cs.AI

TL;DR: CogRec是一个结合大语言模型和Soar认知架构的新型推荐系统，通过符号推理和动态知识获取解决LLM的黑盒性、幻觉问题和在线学习限制，提供可解释的推荐。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推荐系统中存在黑盒特性、知识幻觉和在线学习能力有限的问题，影响可信度和适应性；而认知架构如Soar虽然推理过程结构化可解释，但知识获取困难。需要结合两者优势解决互补挑战。

Method: 提出CogRec认知推荐代理，以Soar作为核心符号推理引擎，利用LLM进行知识初始化填充工作记忆的生产规则。采用感知-认知-行动循环，遇到瓶颈时动态查询LLM获取推理解决方案，通过Soar的分块机制转化为新的符号生产规则，实现鲁棒的在线学习。

Result: 在三个公开数据集上的广泛评估表明，CogRec在推荐准确性、可解释性以及解决长尾问题方面表现出显著优势。

Conclusion: CogRec成功结合了LLM和认知架构的优势，通过符号推理和动态知识获取机制，实现了准确、可解释且能持续进化的推荐系统，有效解决了现有方法的局限性。

Abstract: Large Language Models (LLMs) have demonstrated a remarkable capacity in understanding user preferences for recommendation systems. However, they are constrained by several critical challenges, including their inherent "Black-Box" characteristics, susceptibility to knowledge hallucination, and limited online learning capacity. These factors compromise their trustworthiness and adaptability. Conversely, cognitive architectures such as Soar offer structured and interpretable reasoning processes, yet their knowledge acquisition is notoriously laborious. To address these complementary challenges, we propose a novel cognitive recommender agent called CogRec which synergizes the strengths of LLMs with the Soar cognitive architecture. CogRec leverages Soar as its core symbolic reasoning engine and leverages an LLM for knowledge initialization to populate its working memory with production rules. The agent operates on a Perception-Cognition-Action(PCA) cycle. Upon encountering an impasse, it dynamically queries the LLM to obtain a reasoned solution. This solution is subsequently transformed into a new symbolic production rule via Soar's chunking mechanism, thereby enabling robust online learning. This learning paradigm allows the agent to continuously evolve its knowledge base and furnish highly interpretable rationales for its recommendations. Extensive evaluations conducted on three public datasets demonstrate that CogRec demonstrates significant advantages in recommendation accuracy, explainability, and its efficacy in addressing the long-tail problem.

</details>


### [8] [Graph-Based Exploration for ARC-AGI-3 Interactive Reasoning Tasks](https://arxiv.org/abs/2512.24156)
*Evgenii Rudakov,Jonathan Shock,Benjamin Ultan Cowley*

Main category: cs.AI

TL;DR: 提出一种无需训练、基于图结构的交互推理方法，在ARC-AGI-3基准测试中显著优于前沿LLM，通过系统化的状态空间探索解决游戏任务。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的LLM在ARC-AGI-3基准测试中无法可靠解决交互推理任务，该基准包含需要推断任务机制并适应复杂度递增的游戏式任务。需要一种能够形成假设、测试假设并跟踪已发现机制的方法。

Method: 结合基于视觉的帧处理与系统化的状态空间探索，使用图结构表示。方法包括：1) 将视觉帧分割为有意义的组件；2) 基于视觉显著性优先选择动作；3) 维护探索状态和转换的有向图；4) 通过跟踪已访问状态和测试动作，优先选择提供最短路径到未测试状态-动作对的动作。

Result: 在ARC-AGI-3预览挑战中，该方法在6个游戏的52个关卡中解决了中位数30个关卡，在私有排行榜上排名第3，显著优于前沿LLM智能体。

Conclusion: 即使无需学习，显式的图结构探索也能作为交互推理的强大基线，强调了在稀疏反馈环境中系统化状态跟踪和动作优先级的重要性，这些环境中当前LLM无法捕捉任务动态。

Abstract: We present a training-free graph-based approach for solving interactive reasoning tasks in the ARC-AGI-3 benchmark. ARC-AGI-3 comprises game-like tasks where agents must infer task mechanics through limited interactions, and adapt to increasing complexity as levels progress. Success requires forming hypotheses, testing them, and tracking discovered mechanics. The benchmark has revealed that state-of-the-art LLMs are currently incapable of reliably solving these tasks. Our method combines vision-based frame processing with systematic state-space exploration using graph-structured representations. It segments visual frames into meaningful components, prioritizes actions based on visual salience, and maintains a directed graph of explored states and transitions. By tracking visited states and tested actions, the agent prioritizes actions that provide the shortest path to untested state-action pairs. On the ARC-AGI-3 Preview Challenge, this structured exploration strategy solves a median of 30 out of 52 levels across six games and ranks 3rd on the private leaderboard, substantially outperforming frontier LLM-based agents. These results demonstrate that explicit graph-structured exploration, even without learning, can serve as a strong baseline for interactive reasoning and underscore the importance of systematic state tracking and action prioritization in sparse-feedback environments where current LLMs fail to capture task dynamics. The code is open source and available at https://github.com/dolphin-in-a-coma/arc-agi-3-just-explore.

</details>


### [9] [SCP: Accelerating Discovery with a Global Web of Autonomous Scientific Agents](https://arxiv.org/abs/2512.24189)
*Yankai Jiang,Wenjie Lou,Lilong Wang,Zhenyu Tang,Shiyang Feng,Jiaxuan Lu,Haoran Sun,Yaning Pan,Shuang Gu,Haoyang Su,Feng Liu,Wangxu Wei,Pan Tan,Dongzhan Zhou,Fenghua Ling,Cheng Tan,Bo Zhang,Xiaosong Wang,Lei Bai,Bowen Zhou*

Main category: cs.AI

TL;DR: SCP（科学上下文协议）是一个开源标准，旨在通过构建全球自主科学代理网络加速科学发现，提供统一的资源集成和实验生命周期管理。


<details>
  <summary>Details</summary>
Motivation: 当前科学发现面临资源分散、平台异构、集成成本高、可重复性差等问题，需要建立标准化协议来连接不同平台和机构的科学资源，促进大规模协作。

Method: SCP基于两大支柱：1）统一资源集成：提供描述和调用科学资源（软件工具、模型、数据集、物理仪器）的通用规范；2）编排实验生命周期管理：采用集中式SCP Hub和联邦式SCP Server架构，管理实验全生命周期（注册、规划、执行、监控、归档）。

Result: 基于SCP构建的科学发现平台已集成超过1600个工具资源，在多样化用例中实现了异构AI系统与人类研究人员的安全大规模协作，显著降低了集成开销并增强了可重复性。

Conclusion: SCP通过在协议层面标准化科学上下文和工具编排，为可扩展、多机构、代理驱动的科学研究建立了必要的基础设施。

Abstract: We introduce SCP: the Science Context Protocol, an open-source standard designed to accelerate discovery by enabling a global network of autonomous scientific agents. SCP is built on two foundational pillars: (1) Unified Resource Integration: At its core, SCP provides a universal specification for describing and invoking scientific resources, spanning software tools, models, datasets, and physical instruments. This protocol-level standardization enables AI agents and applications to discover, call, and compose capabilities seamlessly across disparate platforms and institutional boundaries. (2) Orchestrated Experiment Lifecycle Management: SCP complements the protocol with a secure service architecture, which comprises a centralized SCP Hub and federated SCP Servers. This architecture manages the complete experiment lifecycle (registration, planning, execution, monitoring, and archival), enforces fine-grained authentication and authorization, and orchestrates traceable, end-to-end workflows that bridge computational and physical laboratories. Based on SCP, we have constructed a scientific discovery platform that offers researchers and agents a large-scale ecosystem of more than 1,600 tool resources. Across diverse use cases, SCP facilitates secure, large-scale collaboration between heterogeneous AI systems and human researchers while significantly reducing integration overhead and enhancing reproducibility. By standardizing scientific context and tool orchestration at the protocol level, SCP establishes essential infrastructure for scalable, multi-institution, agent-driven science.

</details>


### [10] [Deep Reinforcement Learning for Solving the Fleet Size and Mix Vehicle Routing Problem](https://arxiv.org/abs/2512.24251)
*Pengfu Wan,Jiawei Chen,Gangyan Xu*

Main category: cs.AI

TL;DR: 本文提出了一种基于深度强化学习的方法来解决车队规模与混合车辆路径问题（FSMVRP），该方法能够在几秒内生成接近最优的解决方案，特别适用于大规模和时间受限的场景。


<details>
  <summary>Details</summary>
Motivation: FSMVRP在实际应用（如短期车辆租赁和按需物流）中非常重要，但需要同时做出车队组成和路径规划决策，这增加了问题的复杂性，特别是在大规模和时间受限的环境中面临重大挑战。

Method: 将问题建模为马尔可夫决策过程（MDP），并开发了一个名为FRIPN的新型策略网络，该网络无缝集成了车队组成和路径规划决策。方法包含专门设计的输入嵌入，包括剩余图嵌入以促进有效的车辆使用决策。

Result: 在随机生成实例和基准数据集上的实验结果表明，该方法在计算效率和可扩展性方面表现出显著优势，特别是在大规模和时间受限的场景中。

Conclusion: 该方法在实际应用中具有潜力，并为将基于DRL的技术扩展到VRP的其他变体提供了有价值的启发。

Abstract: The Fleet Size and Mix Vehicle Routing Problem (FSMVRP) is a prominent variant of the Vehicle Routing Problem (VRP), extensively studied in operations research and computational science. FSMVRP requires simultaneous decisions on fleet composition and routing, making it highly applicable to real-world scenarios such as short-term vehicle rental and on-demand logistics. However, these requirements also increase the complexity of FSMVRP, posing significant challenges, particularly in large-scale and time-constrained environments. In this paper, we propose a deep reinforcement learning (DRL)-based approach for solving FSMVRP, capable of generating near-optimal solutions within a few seconds. Specifically, we formulate the problem as a Markov Decision Process (MDP) and develop a novel policy network, termed FRIPN, that seamlessly integrates fleet composition and routing decisions. Our method incorporates specialized input embeddings designed for distinctdecision objectives, including a remaining graph embedding to facilitate effective vehicle employment decisions. Comprehensive experiments are conducted on both randomly generated instances and benchmark datasets. The experimental results demonstrate that our method exhibits notable advantages in terms of computational efficiency and scalability, particularly in large-scale and time-constrained scenarios. These strengths highlight the potential of our approach for practical applications and provide valuable inspiration for extending DRL-based techniques to other variants of VRP.

</details>


### [11] [Constrained Language Model Policy Optimization via Risk-aware Stepwise Alignment](https://arxiv.org/abs/2512.24263)
*Lijun Zhang,Lin Li,Wei Wei,Yajie Qi,Huizhong Song,Jun Wang,Yaodong Yang,Jiye Liang*

Main category: cs.AI

TL;DR: RSA是一种风险感知的逐步对齐方法，通过嵌套风险度量在策略优化中显式考虑风险，以解决现有安全对齐方法在罕见但灾难性有害行为方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法（如Safe RLHF和SACPO）通常采用风险中性范式，无法充分应对参考策略偏差带来的风险，且对罕见但潜在灾难性有害行为的鲁棒性有限。

Method: 提出风险感知逐步对齐（RSA），将安全对齐建模为令牌级风险感知约束策略优化问题，通过基于嵌套风险度量的逐步对齐过程求解，获得令牌级策略更新。

Result: 实验结果表明，该方法在保持高帮助性的同时确保了强安全性，并显著抑制了尾部风险（即低概率但高影响的不安全响应）。

Conclusion: RSA通过显式纳入风险感知机制，有效缓解了模型偏离参考策略带来的风险，并抑制了低概率高危害行为，在安全对齐方面优于现有风险中性方法。

Abstract: When fine-tuning pre-trained Language Models (LMs) to exhibit desired behaviors, maintaining control over risk is critical for ensuring both safety and trustworthiness. Most existing safety alignment methods, such as Safe RLHF and SACPO, typically operate under a risk-neutral paradigm that is insufficient to address the risks arising from deviations from the reference policy and offers limited robustness against rare but potentially catastrophic harmful behaviors. To address this limitation, we propose Risk-aware Stepwise Alignment (RSA), a novel alignment method that explicitly incorporates risk awareness into the policy optimization process by leveraging a class of nested risk measures. Specifically, RSA formulates safety alignment as a token-level risk-aware constrained policy optimization problem and solves it through a stepwise alignment procedure that yields token-level policy updates derived from the nested risk measures. This design offers two key benefits: (1) it mitigates risks induced by excessive model shift away from a reference policy, and (2) it explicitly suppresses low-probability yet high-impact harmful behaviors. Moreover, we provide theoretical analysis on policy optimality under mild assumptions. Experimental results demonstrate that our method achieves high levels of helpfulness while ensuring strong safety and significantly suppresses tail risks, namely low-probability yet high-impact unsafe responses.

</details>


### [12] [Align While Search: Belief-Guided Exploratory Inference for World-Grounded Embodied Agents](https://arxiv.org/abs/2512.24461)
*Seohui Bae,Jeonghye Kim,Youngchul Sung,Woohyung Lim*

Main category: cs.AI

TL;DR: 提出一种无需梯度更新或额外训练、通过后验引导信念精化的测试时自适应智能体，用于部分可观测环境下的LLM智能体推理


<details>
  <summary>Details</summary>
Motivation: 在部分可观测环境下，现有LLM智能体通常依赖基于梯度的更新或额外训练来适应环境，这带来了较高的集成开销。需要一种轻量级的测试时自适应方法，能够在推理时通过探索性推理更好地对齐潜在世界状态。

Method: 1. 维护外部结构化信念表示环境状态；2. 通过动作条件观察迭代更新信念；3. 使用轻量级LLM代理估计信息增益；4. 通过最大化信念空间的信息增益选择动作；5. 引入新颖奖励量化后验信念与真实环境配置的一致性

Result: 实验表明，该方法在潜在世界状态对齐方面优于基于提示增强或检索增强的LLM等推理时扩展基线方法，且集成开销显著更低

Conclusion: 提出的后验引导信念精化方法为部分可观测环境下的LLM智能体提供了一种有效的测试时自适应机制，无需梯度更新或额外训练，实现了更好的世界状态对齐和更低的集成开销

Abstract: In this paper, we propose a test-time adaptive agent that performs exploratory inference through posterior-guided belief refinement without relying on gradient-based updates or additional training for LLM agent operating under partial observability. Our agent maintains an external structured belief over the environment state, iteratively updates it via action-conditioned observations, and selects actions by maximizing predicted information gain over the belief space. We estimate information gain using a lightweight LLM-based surrogate and assess world alignment through a novel reward that quantifies the consistency between posterior belief and ground-truth environment configuration. Experiments show that our method outperforms inference-time scaling baselines such as prompt-augmented or retrieval-enhanced LLMs, in aligning with latent world states with significantly lower integration overhead.

</details>


### [13] [What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?](https://arxiv.org/abs/2512.24497)
*Basile Terver,Tsung-Yen Yang,Jean Ponce,Adrien Bardes,Yann LeCun*

Main category: cs.AI

TL;DR: 本文系统研究了基于JEPA世界模型的规划方法，通过实验分析模型架构、训练目标和规划算法等关键组件，提出了优于现有基线的新模型。


<details>
  <summary>Details</summary>
Motivation: 解决AI中长期存在的挑战：开发能够解决广泛物理任务并能泛化到新未见任务和环境的智能体。当前基于世界模型的规划方法中，在表示空间进行规划的方法（JEPA-WMs）具有抽象无关细节、提高规划效率的潜力，但其具体实现的技术选择需要系统研究。

Method: 将这类模型统一归类为JEPA-WMs，系统研究模型架构、训练目标和规划算法等关键组件。在模拟环境和真实机器人数据上进行实验，分析各组件对规划成功率的影响，最终结合研究发现提出新的优化模型。

Result: 提出的新模型在导航和操作任务上都优于两个现有基线方法（DINO-WM和V-JEPA-2-AC）。代码、数据和检查点已开源。

Conclusion: 通过系统研究JEPA-WMs家族中的技术选择，确定了最优方法组合，提出的模型在多种物理任务中表现出更好的规划性能，为基于表示空间的规划方法提供了重要指导。

Abstract: A long-standing challenge in AI is to develop agents capable of solving a wide range of physical tasks and generalizing to new, unseen tasks and environments. A popular recent approach involves training a world model from state-action trajectories and subsequently use it with a planning algorithm to solve new tasks. Planning is commonly performed in the input space, but a recent family of methods has introduced planning algorithms that optimize in the learned representation space of the world model, with the promise that abstracting irrelevant details yields more efficient planning. In this work, we characterize models from this family as JEPA-WMs and investigate the technical choices that make algorithms from this class work. We propose a comprehensive study of several key components with the objective of finding the optimal approach within the family. We conducted experiments using both simulated environments and real-world robotic data, and studied how the model architecture, the training objective, and the planning algorithm affect planning success. We combine our findings to propose a model that outperforms two established baselines, DINO-WM and V-JEPA-2-AC, in both navigation and manipulation tasks. Code, data and checkpoints are available at https://github.com/facebookresearch/jepa-wms.

</details>


### [14] [Thinking on Maps: How Foundation Model Agents Explore, Remember, and Reason Map Environments](https://arxiv.org/abs/2512.24504)
*Zhiwei Wei,Yuxing Liu,Hua Liao,Wenjia Xu*

Main category: cs.AI

TL;DR: 提出一个交互式评估框架，分析基础模型代理在符号地图环境中的探索、记忆和推理能力，发现结构化记忆对空间理解至关重要，而单纯模型缩放无法持续提升空间推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型的空间能力评估多依赖静态地图输入或文本查询，忽略了空间理解的交互性和经验驱动特性，需要开发更真实的评估方法来理解模型在动态地图环境中的表现。

Method: 提出交互式评估框架，让代理在部分可观测的网格地图中增量探索（包含道路、交叉口和兴趣点），仅接收局部观测，通过六种空间任务评估空间理解能力，系统分析探索策略、记忆表示和推理方案的影响。

Result: 探索主要影响经验获取但对最终推理准确率影响有限；记忆表示在整合空间经验中起核心作用，结构化记忆（特别是序列和图表示）显著提升路径规划等结构密集型任务性能；推理方案影响存储知识的利用方式；空间推理性能在达到一定能力阈值后趋于饱和。

Conclusion: 提升地图空间理解需要专门针对空间表示和推理的机制，而非单纯模型缩放；结构化记忆表示对空间推理至关重要；交互式评估能更真实地反映基础模型的空间能力。

Abstract: Map environments provide a fundamental medium for representing spatial structure. Understanding how foundation model (FM) agents understand and act in such environments is therefore critical for enabling reliable map-based reasoning and applications. However, most existing evaluations of spatial ability in FMs rely on static map inputs or text-based queries, overlooking the interactive and experience-driven nature of spatial understanding.In this paper, we propose an interactive evaluation framework to analyze how FM agents explore, remember, and reason in symbolic map environments. Agents incrementally explore partially observable grid-based maps consisting of roads, intersections, and points of interest (POIs), receiving only local observations at each step. Spatial understanding is then evaluated using six kinds of spatial tasks. By systematically varying exploration strategies, memory representations, and reasoning schemes across multiple foundation models, we reveal distinct functional roles of these components. Exploration primarily affects experience acquisition but has a limited impact on final reasoning accuracy. In contrast, memory representation plays a central role in consolidating spatial experience, with structured memories particularly sequential and graph-based representations, substantially improving performance on structure-intensive tasks such as path planning. Reasoning schemes further shape how stored spatial knowledge is used, with advanced prompts supporting more effective multi-step inference. We further observe that spatial reasoning performance saturates across model versions and scales beyond a certain capability threshold, indicating that improvements in map-based spatial understanding require mechanisms tailored to spatial representation and reasoning rather than scaling alone.

</details>


### [15] [Evaluating the Reasoning Abilities of LLMs on Underrepresented Mathematics Competition Problems](https://arxiv.org/abs/2512.24505)
*Samuel Golladay,Majid Bani-Yaghoub*

Main category: cs.AI

TL;DR: 该研究分析了GPT-4o-mini、Gemini-2.0-Flash和DeepSeek-V3在密苏里大学数学竞赛问题上的表现，发现DeepSeek-V3在所有类别中表现最佳，但所有模型在几何问题上表现都较弱，揭示了不同模型在数学推理中的特定错误模式。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多使用相同的数据集评估大语言模型的数学推理能力，这限制了研究结果的普适性，可能无法全面捕捉数学任务中的多样化挑战。本研究旨在通过分析LLMs在代表性不足的数学竞赛问题上的表现，获得更深入的见解。

Method: 研究使用密苏里大学数学竞赛中的微积分、解析几何和离散数学问题，测试了GPT-4o-mini、Gemini-2.0-Flash和DeepSeek-V3三个领先的LLMs。将模型的回答与已知正确答案进行比较，确定每个问题领域的准确性，并分析模型的推理过程以探索跨问题类型和模型的错误模式。

Result: DeepSeek-V3在微积分、解析几何和离散数学三个类别中表现最佳，无论是推理过程还是最终答案。所有三个LLMs在几何问题上都表现出明显的弱势。DeepSeek-V3的错误主要源于计算和逻辑错误，GPT-4o-mini经常出现逻辑和方法相关的错误，而Gemini则倾向于不完整的推理和仓促得出结论。

Conclusion: 在代表性不足的数学竞赛数据集上评估LLMs可以提供对其独特错误模式的更深入见解，并突显结构化推理中的持续挑战，特别是在几何领域。这种评估方法有助于更全面地理解LLMs的数学推理能力限制。

Abstract: Understanding the limitations of Large Language Models, or LLMs, in mathematical reasoning has been the focus of several recent studies. However, the majority of these studies use the same datasets for benchmarking, which limits the generalizability of their findings and may not fully capture the diverse challenges present in mathematical tasks. The purpose of the present study is to analyze the performance of LLMs on underrepresented mathematics competition problems. We prompted three leading LLMs, namely GPT-4o-mini, Gemini-2.0-Flash, and DeepSeek-V3, with the Missouri Collegiate Mathematics Competition problems in the areas of Calculus, Analytic Geometry, and Discrete Mathematics. The LLMs responses were then compared to the known correct solutions in order to determine the accuracy of the LLM for each problem domain. We also analyzed the LLMs reasoning to explore patterns in errors across problem types and models. DeepSeek-V3 has the best performance in all three categories of Calculus, Analytic Geometry, and Discrete Mathematics, both in reasoning and correct final answers. All three LLMs exhibited notably weak performance in Geometry. The majority of errors made by DeepSeek-V3 were attributed to computational and logical mistakes, whereas GPT-4o-mini frequently exhibited logical and approach-related errors. Gemini, on the other hand, tended to struggle with incomplete reasoning and drawing rushed conclusions. In conclusion, evaluating LLMs on underrepresented mathematics competition datasets can provide deeper insights into their distinct error patterns and highlight ongoing challenges in structured reasoning, particularly within the domain of Geometry.

</details>


### [16] [From Building Blocks to Planning: Multi-Step Spatial Reasoning in LLMs with Reinforcement Learning](https://arxiv.org/abs/2512.24532)
*Amir Tahmasbi,Sadegh Majidi,Kazem Taram,Aniket Bera*

Main category: cs.AI

TL;DR: 本文提出一种两阶段方法，将空间推理分解为原子构建块及其组合，通过监督微调学习基本空间变换，再训练轻量级适配器进行多步规划，在ASCII艺术环境中优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在通用语言能力上表现强大，但在结构化环境中的空间变换和多步规划方面仍然存在困难。需要解决LLMs在空间推理任务中的局限性，特别是在导航和规划等应用场景中。

Method: 采用两阶段方法：1) 对基本空间变换（旋转、平移、缩放）进行监督微调，使模型具备基本空间物理知识；2) 冻结物理感知模型，在GRPO框架内训练轻量级LoRA适配器，以学习将这些构建块组合用于谜题环境中的多步规划。为此合成了ASCII艺术数据集并构建了相应的强化学习环境。

Result: 该方法在动态环境（显式状态更新）和静态环境（模型必须依赖内部状态）中均优于基线方法，包括通用骨干模型、物理感知模型和端到端RL模型。此外，该方法收敛更快，训练更稳定，并且注意力模式分析表明微调确实改善了空间理解。

Conclusion: 提出的两阶段分解方法有效提升了LLMs的空间推理能力，通过将复杂任务分解为原子构建块及其组合，在结构化环境中实现了更好的多步规划性能，为导航和规划应用提供了有前景的解决方案。

Abstract: Spatial reasoning in large language models (LLMs) has gained increasing attention due to applications in navigation and planning. Despite strong general language capabilities, LLMs still struggle with spatial transformations and multi-step planning in structured environments. We propose a two-stage approach that decomposes spatial reasoning into atomic building blocks and their composition. First, we apply supervised fine-tuning on elementary spatial transformations, such as rotation, translation, and scaling, to equip the model with basic spatial physics. We then freeze this physics-aware model and train lightweight LoRA adapters within the GRPO framework to learn policies that compose these building blocks for multi-step planning in puzzle-based environments, in a closed-loop manner. To support this pipeline, we synthesize an ASCII-art dataset and construct a corresponding ASCII-based reinforcement learning environment. Our method consistently outperforms baselines, including the generic backbone, physics-aware model, and end-to-end RL models, under both Dynamic environments with explicit state updates and Static environments where the model must rely on its internal state across steps. In addition, the proposed approach converges faster and exhibits more stable training compared to end-to-end reinforcement learning from scratch. Finally, we analyze attention patterns to assess whether fine-tuning induces meaningful improvements in spatial understanding.

</details>


### [17] [MCPAgentBench: A Real-world Task Benchmark for Evaluating LLM Agent MCP Tool Use](https://arxiv.org/abs/2512.24565)
*Wenrui Liu,Zixiang Liu,Elsie Dai,Wenhan Yu,Lei Yu,Tong Yang*

Main category: cs.AI

TL;DR: 提出了MCPAgentBench基准，用于评估基于MCP协议的LLM智能体工具使用能力，包含真实任务、模拟工具和动态沙箱环境，测试工具选择与辨别能力。


<details>
  <summary>Details</summary>
Motivation: 当前MCP评估集存在依赖外部MCP服务、缺乏难度感知等问题，需要更全面的基准来评估智能体的工具使用能力。

Method: 构建基于真实MCP定义的数据集，包含真实任务和模拟MCP工具；采用动态沙箱环境，提供包含干扰项的工具列表；引入综合指标衡量任务完成率和执行效率。

Result: 在多种主流大语言模型上的实验显示，在处理复杂多步工具调用时存在显著性能差异。

Conclusion: MCPAgentBench为评估LLM智能体工具使用能力提供了有效基准，代码已开源，有助于推动MCP智能体发展。

Abstract: Large Language Models (LLMs) are increasingly serving as autonomous agents, and their utilization of external tools via the Model Context Protocol (MCP) is considered a future trend. Current MCP evaluation sets suffer from issues such as reliance on external MCP services and a lack of difficulty awareness. To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents. We construct a dataset containing authentic tasks and simulated MCP tools. The evaluation employs a dynamic sandbox environment that presents agents with candidate tool lists containing distractors, thereby testing their tool selection and discrimination abilities. Furthermore, we introduce comprehensive metrics to measure both task completion rates and execution efficiency. Experiments conducted on various latest mainstream Large Language Models reveal significant performance differences in handling complex, multi-step tool invocations. All code is open-source at Github.

</details>


### [18] [Recursive Language Models](https://arxiv.org/abs/2512.24601)
*Alex L. Zhang,Tim Kraska,Omar Khattab*

Main category: cs.AI

TL;DR: 提出递归语言模型（RLMs），一种让LLM通过递归调用自身处理超长提示的推理策略，可处理超出模型上下文窗口两个数量级的输入


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型处理任意长提示的问题，传统方法受限于固定的上下文窗口，无法有效处理超长输入

Method: 提出递归语言模型（RLMs），将长提示视为外部环境，让LLM以编程方式检查、分解并递归调用自身处理提示片段

Result: RLMs成功处理超出模型上下文窗口两个数量级的输入，在四个不同的长上下文任务中，即使对于较短提示，也显著优于基础LLM和常见长上下文框架，同时查询成本相当或更低

Conclusion: RLMs为处理超长提示提供了一种有效的推理策略，在保持成本效益的同时显著提升处理长上下文的能力

Abstract: We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference strategy that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. We find that RLMs successfully handle inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of base LLMs and common long-context scaffolds across four diverse long-context tasks, while having comparable (or cheaper) cost per query.

</details>


### [19] [Reinforcement Learning-Augmented LLM Agents for Collaborative Decision Making and Performance Optimization](https://arxiv.org/abs/2512.24609)
*Dong Qiu,Duo Xu,Limengxi Yue*

Main category: cs.AI

TL;DR: 提出强化学习增强的LLM多智能体协作框架，采用Dec-POMDP建模和CTDE训练，通过GRPO优化策略，在协作写作和编码任务中显著提升效率和一致性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在单智能体任务中表现良好，但在多智能体协作场景中缺乏协作意识，难以优化全局性能，需要专门的协作框架来提升多智能体协同工作的效率和效果。

Method: 1. 将协作建模为去中心化部分可观察马尔可夫决策过程(Dec-POMDP)；2. 采用集中训练分散执行(CTDE)范式；3. 提出组相对策略优化(GRPO)方法，在训练时利用全局信号联合优化智能体策略；4. 设计简化的联合奖励函数，平衡任务质量、速度和协调成本。

Result: 在协作写作和编码基准测试中：1. 任务处理速度比单智能体基线提高3倍；2. 写作任务中达到98.7%的结构/风格一致性；3. 编码任务中达到74.6%的测试通过率；4. 持续优于现有的多智能体LLM基线方法。

Conclusion: 该强化学习增强的LLM多智能体协作框架有效解决了LLM在协作场景中的局限性，为复杂工作流中的可靠协作提供了实用路径，显著提升了多智能体协同工作的效率和效果。

Abstract: Large Language Models (LLMs) perform well in language tasks but often lack collaborative awareness and struggle to optimize global performance in multi-agent settings. We present a reinforcement learning-augmented LLM agent framework that formulates cooperation as a decentralized partially observable Markov decision process (Dec-POMDP) and adopts centralized training with decentralized execution (CTDE). We introduce Group Relative Policy Optimization (GRPO) to jointly optimize agent policies with access to global signals during training, together with a simplified joint reward that balances task quality, speed, and coordination cost. On collaborative writing and coding benchmarks, our framework delivers a 3x increase in task processing speed over single-agent baselines, 98.7% structural/style consistency in writing, and a 74.6% test pass rate in coding. The approach consistently outperforms strong multi-agent LLM baselines and provides a practical path toward reliable collaboration in complex workflows.

</details>


### [20] [Group Deliberation Oriented Multi-Agent Conversational Model for Complex Reasoning](https://arxiv.org/abs/2512.24613)
*Zheyu Shi,Dong Qiu,Shanlong Yu*

Main category: cs.AI

TL;DR: 提出基于群体审议的多智能体对话模型，通过三层角色架构（生成、验证、整合）提升复杂推理能力，在多个数据集上显著提高推理准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 单个大型语言模型在复杂推理任务中存在局限性，需要更有效的多智能体协作方法来提升推理准确性和逻辑一致性。

Method: 采用三层角色架构：生成智能体产生多样化推理视角，验证智能体检索外部知识并量化事实支持度，仲裁智能体整合逻辑一致的结论。引入自博弈机制扩展多路径推理轨迹，检索增强模块动态补充外部知识，设计结合事实一致性和逻辑连贯性的复合奖励函数，应用改进的近端策略优化进行协同训练。

Result: 在HotpotQA上多跳推理准确率提升16.8%，在2WikiMultihopQA上提升14.3%，在MeetingBank上提升19.2%，一致性提高21.5%，推理效率优于主流多智能体方法。

Conclusion: 该模型为复杂推理任务提供了有效且稳定的解决方案，通过群体审议和多智能体协作显著提升了推理性能。

Abstract: This paper proposes a group deliberation oriented multi-agent conversational model to address the limitations of single large language models in complex reasoning tasks. The model adopts a three-level role division architecture consisting of generation, verification, and integration. An opinion generation agent produces diverse reasoning perspectives, an evidence verification agent retrieves external knowledge and quantifies factual support, and a consistency arbitration agent integrates logically coherent conclusions. A self-game mechanism is introduced to expand multi-path reasoning trajectories, while a retrieval enhancement module dynamically supplements external knowledge. A composite reward function combining factual consistency and logical coherence is designed, and an improved proximal policy optimization strategy is applied for collaborative training. Experimental results show that the proposed model improves multi-hop reasoning accuracy by 16.8 percent on HotpotQA, 14.3 percent on 2WikiMultihopQA, and 19.2 percent on MeetingBank, while improving consistency by 21.5 percent. The model achieves higher reasoning efficiency than mainstream multi-agent approaches, providing an effective and stable solution for complex reasoning tasks.

</details>


### [21] [Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization](https://arxiv.org/abs/2512.24615)
*Yuchen Shi,Yuzheng Cai,Siqi Cai,Zihan Xu,Lichao Chen,Yulei Qin,Zhijian Zhou,Xiang Fei,Chaofan Qiu,Xiaoyu Tan,Gang Li,Zongyi Li,Haojia Lin,Guocan Cai,Yong Mao,Yunsheng Wu,Ke Li,Xing Sun*

Main category: cs.AI

TL;DR: Youtu-Agent是一个模块化LLM代理框架，通过自动化生成和持续演化解决现有代理框架配置成本高、能力静态的问题，支持工作流和元代理两种生成模式，并采用混合策略优化提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理框架面临两大挑战：1) 配置成本高，构建高质量代理需要大量手动工具集成和提示工程；2) 能力静态，已部署代理难以适应动态环境，需要昂贵的微调。

Method: 提出模块化框架Youtu-Agent，包含：1) 结构化配置系统，解耦执行环境、工具包和上下文管理；2) 两种生成范式：工作流模式用于标准任务，元代理模式用于复杂需求；3) 混合策略优化系统：Agent Practice模块通过上下文优化积累经验，Agent RL模块集成分布式训练框架进行端到端强化学习。

Result: 在WebWalkerQA达到71.47%，GAIA达到72.8%的SOTA性能；自动化生成管道工具合成成功率超过81%；Practice模块在AIME 2024/2025分别提升2.7%和5.4%；Agent RL训练在7B LLM上实现40%加速，数学和通用/多跳QA基准上分别提升35%和21%的编码/推理和搜索能力。

Conclusion: Youtu-Agent通过自动化生成和持续演化机制，有效解决了LLM代理框架的高配置成本和静态能力问题，在多个基准测试中展现了优越性能，为构建自适应、可扩展的智能代理系统提供了有效方案。

Abstract: Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose \textbf{Youtu-Agent}, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a \textbf{Workflow} mode for standard tasks and a \textbf{Meta-Agent} mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an \textbf{Agent Practice} module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an \textbf{Agent RL} module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\%) and GAIA (72.8\%) using open-weight models. Our automated generation pipeline achieves over 81\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\% and +5.4\% respectively. Moreover, our Agent RL training achieves 40\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\% and 21\% on Maths and general/multi-hop QA benchmarks.

</details>


### [22] [Multi-modal cross-domain mixed fusion model with dual disentanglement for fault diagnosis under unseen working conditions](https://arxiv.org/abs/2512.24679)
*Pengcheng Xia,Yixiang Huang,Chengjin Qin,Chengliang Liu*

Main category: cs.AI

TL;DR: 提出多模态跨域混合融合模型，通过双重解耦和跨域混合融合策略，解决故障诊断中未见工况下的泛化问题，提升多模态信息利用效果。


<details>
  <summary>Details</summary>
Motivation: 现有故障诊断方法在未见工况下性能显著下降，域适应方法依赖目标域样本，且大多依赖单模态信号，忽略了多模态信息的互补性。

Method: 提出多模态跨域混合融合模型，包含双重解耦框架（分离模态不变/特定特征和域不变/特定表示）、跨域混合融合策略（跨域随机混合模态信息）和三模态融合机制（自适应集成多模态异构信息）。

Result: 在感应电机故障诊断的未见恒定和时变工况实验中，该方法始终优于先进方法，消融研究验证了各组件和多模态融合的有效性。

Conclusion: 该方法通过双重解耦和跨域混合融合，实现了全面的多模态表示学习和鲁棒的域泛化，为未见工况下的故障诊断提供了有效解决方案。

Abstract: Intelligent fault diagnosis has become an indispensable technique for ensuring machinery reliability. However, existing methods suffer significant performance decline in real-world scenarios where models are tested under unseen working conditions, while domain adaptation approaches are limited to their reliance on target domain samples. Moreover, most existing studies rely on single-modal sensing signals, overlooking the complementary nature of multi-modal information for improving model generalization. To address these limitations, this paper proposes a multi-modal cross-domain mixed fusion model with dual disentanglement for fault diagnosis. A dual disentanglement framework is developed to decouple modality-invariant and modality-specific features, as well as domain-invariant and domain-specific representations, enabling both comprehensive multi-modal representation learning and robust domain generalization. A cross-domain mixed fusion strategy is designed to randomly mix modality information across domains for modality and domain diversity augmentation. Furthermore, a triple-modal fusion mechanism is introduced to adaptively integrate multi-modal heterogeneous information. Extensive experiments are conducted on induction motor fault diagnosis under both unseen constant and time-varying working conditions. The results demonstrate that the proposed method consistently outperforms advanced methods and comprehensive ablation studies further verify the effectiveness of each proposed component and multi-modal fusion. The code is available at: https://github.com/xiapc1996/MMDG.

</details>


### [23] [BatteryAgent: Synergizing Physics-Informed Interpretation with LLM Reasoning for Intelligent Battery Fault Diagnosis](https://arxiv.org/abs/2512.24686)
*Songqi Zhou,Ruixue Liu,Boman Su,Jiazhou Wang,Yixing Wang,Benben Jiang*

Main category: cs.AI

TL;DR: 提出BatteryAgent框架，结合物理特征与LLM推理能力，实现锂离子电池故障的智能诊断与根因分析，超越传统黑盒方法的二元分类限制。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习电池故障诊断方法存在两大问题：1）黑盒特性缺乏可解释性；2）二元分类范式无法提供根因分析和维护建议。需要从"被动检测"转向"智能诊断"。

Method: 提出三层框架：1）物理感知层提取10个基于电化学原理的特征；2）检测与归因层使用梯度提升决策树和SHAP量化特征贡献；3）推理与诊断层利用LLM作为智能体核心，构建"数值-语义"桥梁，结合SHAP归因和机理知识库生成综合诊断报告。

Result: BatteryAgent有效纠正硬边界样本的误分类，AUROC达到0.986，显著优于现有最先进方法。框架将传统二元检测扩展到多类型可解释诊断。

Conclusion: 该框架为电池安全管理提供了从"被动检测"到"智能诊断"的新范式转变，通过整合物理知识与LLM推理能力，实现了可解释、多类型的故障诊断系统。

Abstract: Fault diagnosis of lithium-ion batteries is critical for system safety. While existing deep learning methods exhibit superior detection accuracy, their "black-box" nature hinders interpretability. Furthermore, restricted by binary classification paradigms, they struggle to provide root cause analysis and maintenance recommendations. To address these limitations, this paper proposes BatteryAgent, a hierarchical framework that integrates physical knowledge features with the reasoning capabilities of Large Language Models (LLMs). The framework comprises three core modules: (1) A Physical Perception Layer that utilizes 10 mechanism-based features derived from electrochemical principles, balancing dimensionality reduction with physical fidelity; (2) A Detection and Attribution Layer that employs Gradient Boosting Decision Trees and SHAP to quantify feature contributions; and (3) A Reasoning and Diagnosis Layer that leverages an LLM as the agent core. This layer constructs a "numerical-semantic" bridge, combining SHAP attributions with a mechanism knowledge base to generate comprehensive reports containing fault types, root cause analysis, and maintenance suggestions. Experimental results demonstrate that BatteryAgent effectively corrects misclassifications on hard boundary samples, achieving an AUROC of 0.986, which significantly outperforms current state-of-the-art methods. Moreover, the framework extends traditional binary detection to multi-type interpretable diagnosis, offering a new paradigm shift from "passive detection" to "intelligent diagnosis" for battery safety management.

</details>


### [24] [Explaining Why Things Go Where They Go: Interpretable Constructs of Human Organizational Preferences](https://arxiv.org/abs/2512.24829)
*Emmanuel Fashae,Michael Burke,Leimin Tian,Lingheng Meng,Pamela Carreno-Medrano*

Main category: cs.AI

TL;DR: 论文提出了一种可解释的物体排列偏好框架，包含四个可解释维度，并通过问卷验证，最后集成到MCTS规划器中生成符合人类偏好的排列方案。


<details>
  <summary>Details</summary>
Motivation: 当前机器人系统依赖潜在偏好模型进行家庭物体重排，这些模型虽然有效但缺乏可解释性，无法提供指导人类决策的可解释因素。

Method: 1) 提出四个可解释的物体排列偏好维度：空间实用性、习惯便利性、语义连贯性、常识适当性；2) 设计并验证自报告问卷（63名参与者在线研究）；3) 将偏好维度集成到蒙特卡洛树搜索规划器中。

Result: 问卷研究证实了四个维度的心理区分度和解释力；基于参与者偏好的MCTS规划器能够生成合理的排列方案，与参与者生成的排列高度一致。

Conclusion: 本研究贡献了一个紧凑、可解释的物体排列偏好框架，并展示了如何将其操作化用于机器人规划，为机器人系统提供了更透明、可解释的偏好建模方法。

Abstract: Robotic systems for household object rearrangement often rely on latent preference models inferred from human demonstrations. While effective at prediction, these models offer limited insight into the interpretable factors that guide human decisions. We introduce an explicit formulation of object arrangement preferences along four interpretable constructs: spatial practicality (putting items where they naturally fit best in the space), habitual convenience (making frequently used items easy to reach), semantic coherence (placing items together if they are used for the same task or are contextually related), and commonsense appropriateness (putting things where people would usually expect to find them). To capture these constructs, we designed and validated a self-report questionnaire through a 63-participant online study. Results confirm the psychological distinctiveness of these constructs and their explanatory power across two scenarios (kitchen and living room). We demonstrate the utility of these constructs by integrating them into a Monte Carlo Tree Search (MCTS) planner and show that when guided by participant-derived preferences, our planner can generate reasonable arrangements that closely align with those generated by participants. This work contributes a compact, interpretable formulation of object arrangement preferences and a demonstration of how it can be operationalized for robot planning.

</details>


### [25] [GenZ: Foundational models as latent variable generators within traditional statistical models](https://arxiv.org/abs/2512.24834)
*Marko Jojic,Nebojsa Jojic*

Main category: cs.AI

TL;DR: GenZ是一个混合模型，通过可解释的语义特征桥接基础模型和统计建模，在房价预测和电影推荐任务中显著优于纯基础模型方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然拥有广泛的领域知识，但往往无法捕捉数据集特定的模式，而这些模式对于预测任务至关重要。需要一种方法将基础模型的语义理解与统计建模的数据特定模式发现相结合。

Method: 提出一个广义EM算法，通过迭代过程发现语义特征描述：基于统计建模误差对比项目组，然后提示冻结的基础模型基于发现的特征对项目进行分类，将这些判断视为潜在二元特征的噪声观测，通过学习的统计关系预测实值目标。

Result: 在房价预测中，使用多模态列表数据发现的语义特征实现12%中位数相对误差，显著优于GPT-5基线的38%误差。在Netflix电影嵌入预测中，仅从语义描述就能达到0.59余弦相似度，相当于传统协同过滤需要约4000个用户评分的性能。

Conclusion: GenZ成功地将基础模型的语义理解与统计建模的数据特定模式发现相结合，发现了与模型领域知识不同的数据集特定模式（如预测本地房地产市场的建筑细节、预测用户偏好的系列电影成员关系），在多个领域实现了显著性能提升。

Abstract: We present GenZ, a hybrid model that bridges foundational models and statistical modeling through interpretable semantic features. While large language models possess broad domain knowledge, they often fail to capture dataset-specific patterns critical for prediction tasks. Our approach addresses this by discovering semantic feature descriptions through an iterative process that contrasts groups of items identified via statistical modeling errors, rather than relying solely on the foundational model's domain understanding. We formulate this as a generalized EM algorithm that jointly optimizes semantic feature descriptors and statistical model parameters. The method prompts a frozen foundational model to classify items based on discovered features, treating these judgments as noisy observations of latent binary features that predict real-valued targets through learned statistical relationships. We demonstrate the approach on two domains: house price prediction (hedonic regression) and cold-start collaborative filtering for movie recommendations. On house prices, our model achieves 12\% median relative error using discovered semantic features from multimodal listing data, substantially outperforming a GPT-5 baseline (38\% error) that relies on the LLM's general domain knowledge. For Netflix movie embeddings, our model predicts collaborative filtering representations with 0.59 cosine similarity purely from semantic descriptions -- matching the performance that would require approximately 4000 user ratings through traditional collaborative filtering. The discovered features reveal dataset-specific patterns (e.g., architectural details predicting local housing markets, franchise membership predicting user preferences) that diverge from the model's domain knowledge alone.

</details>


### [26] [A study on constraint extraction and exception exclusion in care worker scheduling](https://arxiv.org/abs/2512.24853)
*Koki Suenaga,Tomohiro Furuta,Satoshi Ono*

Main category: cs.AI

TL;DR: 提出基于约束模板的方法，从养老机构管理者访谈中提取设施特定的排班约束条件，避免提取异常约束，用于约束规划求解器生成护工排班表


<details>
  <summary>Details</summary>
Motivation: 养老机构的排班条件因机构而异，需要通过与制定排班表的管理者访谈来设计设施特定的约束条件，但现有约束提取技术难以处理这种多样性

Method: 使用约束模板提取各种组件的组合（如连续工作日的班次模式或员工组合），通过改变关注的天数、员工数量和提取焦点（模式或频率）来提取多种约束，并包含排除异常约束的机制

Result: 实验表明，该方法成功创建了满足所有硬约束的排班表，并通过避免提取异常约束，减少了软约束的违反次数

Conclusion: 提出的约束模板方法能够有效提取养老机构特定的排班约束，生成满足实际需求的护工排班表，解决了设施间条件差异带来的挑战

Abstract: Technologies for automatically generating work schedules have been extensively studied; however, in long-term care facilities, the conditions vary between facilities, making it essential to interview the managers who create shift schedules to design facility-specific constraint conditions. The proposed method utilizes constraint templates to extract combinations of various components, such as shift patterns for consecutive days or staff combinations. The templates can extract a variety of constraints by changing the number of days and the number of staff members to focus on and changing the extraction focus to patterns or frequency. In addition, unlike existing constraint extraction techniques, this study incorporates mechanisms to exclude exceptional constraints. The extracted constraints can be employed by a constraint programming solver to create care worker schedules. Experiments demonstrated that our proposed method successfully created schedules that satisfied all hard constraints and reduced the number of violations for soft constraints by circumventing the extraction of exceptional constraints.

</details>


### [27] [Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem](https://arxiv.org/abs/2512.24873)
*Weixun Wang,XiaoXiao Xu,Wanhe An,Fangwen Dai,Wei Gao,Yancheng He,Ju Huang,Qiang Ji,Hanqi Jin,Xiaoyang Li,Yang Li,Zhongwen Li,Shirong Lin,Jiashun Liu,Zenan Liu,Tao Luo,Dilxat Muhtar,Yuanbin Qu,Jiaqiang Shi,Qinghui Sun,Yingshui Tan,Hao Tang,Runze Wang,Yi Wang,Zhaoguo Wang,Yanan Wu,Shaopan Xiong,Binchen Xu,Xander Xu,Yuchi Xu,Qipeng Zhang,Xixia Zhang,Haizhou Zhao,Jie Zhao,Shuaibing Zhao,Baihui Zheng,Jianhui Zheng,Suhang Zheng,Yanni Zhu,Mengze Cai,Kerui Cao,Xitong Chen,Yue Dai,Lifan Du,Tao Feng,Tao He,Jin Hu,Yijie Hu,Ziyu Jiang,Cheng Li,Xiang Li,Jing Liang,Chonghuan Liu,ZhenDong Liu,Haodong Mi,Yanhu Mo,Junjia Ni,Shixin Pei,Jingyu Shen,XiaoShuai Song,Cecilia Wang,Chaofan Wang,Kangyu Wang,Pei Wang,Tao Wang,Wei Wang,Ke Xiao,Mingyu Xu,Tiange Xu,Nan Ya,Siran Yang,Jianan Ye,Yaxing Zang,Duo Zhang,Junbo Zhang,Boren Zheng,Wanxi Deng,Ling Pan,Lin Qu,Wenbo Su,Jiamang Wang,Wei Wang,Hu Wei,Minggang Wu,Cheng Yu,Bing Zhao,Zhicheng Zheng,Bo Zheng*

Main category: cs.AI

TL;DR: ALE是一个端到端的智能体学习生态系统，包含ROLL权重优化框架、ROCK沙盒环境管理和iFlow CLI上下文工程工具。基于ALE训练的ROME智能体在多项基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 开源社区缺乏系统化的智能体开发基础设施，需要端到端的生态系统来简化智能体LLM的生产流程。

Method: ALE包含三个组件：ROLL（权重优化后训练框架）、ROCK（轨迹生成沙盒环境管理器）和iFlow CLI（高效上下文工程代理框架）。提出IPA算法（基于交互的策略对齐），在语义交互块而非单个token上分配信用，提高长时程训练稳定性。

Result: 发布了基于ALE训练的ROME智能体，在超过100万条轨迹上训练。在SWE-bench Verified和Terminal Bench等基准测试中表现强劲，证明了ALE基础设施的有效性。

Conclusion: ALE为智能体开发提供了系统化的基础设施，ROME的成功验证了该生态系统的有效性，为开源智能体社区提供了重要工具。

Abstract: Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agent LLMs. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME (ROME is Obviously an Agentic Model), an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes data composition protocols for synthesizing complex behaviors and a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce Terminal Bench Pro, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like SWE-bench Verified and Terminal Bench, proving the effectiveness of the ALE infrastructure.

</details>


### [28] [Semi-Automated Data Annotation in Multisensor Datasets for Autonomous Vehicle Testing](https://arxiv.org/abs/2512.24896)
*Andrii Gamalii,Daniel Górniak,Robert Nowak,Bartłomiej Olber,Krystian Radlak,Jakub Winter*

Main category: cs.AI

TL;DR: 开发了一个用于自动驾驶场景数据标注的半自动化管道，结合AI与人工专家，显著降低标注成本和时间


<details>
  <summary>Details</summary>
Motivation: 手动标注多模态驾驶场景数据成本高、耗时久，DARTS项目需要创建大规模波兰驾驶条件数据集，需要高效标注方案

Method: 采用人机协同方法，结合AI自动生成初始标注和人工专家修正，使用3D目标检测算法生成初步标注，支持迭代模型重训练，包含数据匿名化和领域适应技术

Result: 开发了工具和方法论，实现了显著的时间节省，同时确保跨不同传感器模态的一致高质量标注

Conclusion: 该方案直接支持DARTS项目，加速了大规模标注数据集的准备，加强了波兰自动驾驶研究的技术基础

Abstract: This report presents the design and implementation of a semi-automated data annotation pipeline developed within the DARTS project, whose goal is to create a large-scale, multimodal dataset of driving scenarios recorded in Polish conditions. Manual annotation of such heterogeneous data is both costly and time-consuming. To address this challenge, the proposed solution adopts a human-in-the-loop approach that combines artificial intelligence with human expertise to reduce annotation cost and duration. The system automatically generates initial annotations, enables iterative model retraining, and incorporates data anonymization and domain adaptation techniques. At its core, the tool relies on 3D object detection algorithms to produce preliminary annotations. Overall, the developed tools and methodology result in substantial time savings while ensuring consistent, high-quality annotations across different sensor modalities. The solution directly supports the DARTS project by accelerating the preparation of large annotated dataset in the project's standardized format, strengthening the technological base for autonomous vehicle research in Poland.

</details>


### [29] [Iterative Deployment Improves Planning Skills in LLMs](https://arxiv.org/abs/2512.24940)
*Augusto B. Corrêa,Yoav Gelberg,Luckeciano C. Melo,Ilia Shumailov,André G. Pereira,Yarin Gal*

Main category: cs.AI

TL;DR: 迭代部署LLM并通过用户数据筛选进行微调，可以显著改变模型特性，在规划任务中实现能力提升和泛化，这实际上是一种隐式强化学习过程


<details>
  <summary>Details</summary>
Motivation: 研究迭代部署大型语言模型时，通过用户从先前模型部署中精心筛选数据进行微调，如何影响最终模型的特性变化

Method: 在多个规划领域测试迭代部署机制：每次部署LLM后，用户从模型输出中筛选数据，用于微调下一代模型，形成迭代循环

Result: 观察到规划能力显著提升，后续模型展现出涌现泛化能力，能够发现比初始模型长得多的规划方案

Conclusion: 迭代部署实际上实现了外层强化学习训练，具有隐式奖励函数。这对AI安全有重要启示（奖励函数未明确定义），同时提供了一种基于数据筛选而非显式奖励的替代训练机制

Abstract: We show that iterative deployment of large language models (LLMs), each fine-tuned on data carefully curated by users from the previous models' deployment, can significantly change the properties of the resultant models. By testing this mechanism on various planning domains, we observe substantial improvements in planning skills, with later models displaying emergent generalization by discovering much longer plans than the initial models. We then provide theoretical analysis showing that iterative deployment effectively implements reinforcement learning (RL) training in the outer-loop (i.e. not as part of intentional model training), with an implicit reward function. The connection to RL has two important implications: first, for the field of AI safety, as the reward function entailed by repeated deployment is not defined explicitly, and could have unexpected implications to the properties of future model deployments. Second, the mechanism highlighted here can be viewed as an alternative training regime to explicit RL, relying on data curation rather than explicit rewards.

</details>


### [30] [AMAP Agentic Planning Technical Report](https://arxiv.org/abs/2512.24957)
*Yulan Hu,Xiangwen Zhang,Sheng Ouyang,Hao Yi,Lu Xu,Qinglin Lang,Lide Tan,Xiang Cheng,Tianchen Ye,Zhicong Li,Ge Chen,Wenjin Yang,Zheng Pan,Shaopan Xiong,Siran Yang,Ju Huang,Yan Zhang,Jiamang Wang,Yong Liu,Yinfeng Huang,Tucheng Lin,Xin Li,Ning Guo*

Main category: cs.AI

TL;DR: STAgent是一个专门用于时空理解的智能体大语言模型，通过工具交互解决复杂时空任务，同时保持通用能力


<details>
  <summary>Details</summary>
Motivation: 需要开发能够理解和处理复杂时空任务的智能体模型，如受限兴趣点发现和行程规划，这些任务需要与多种时空工具交互并进行多步推理

Method: 采用三阶段方法：1) 构建支持10+领域特定工具的稳定工具环境；2) 分层数据筛选框架，从海量数据中筛选高质量查询；3) 级联训练流程，包括种子SFT、高确定性查询SFT和低确定性数据RL训练

Result: STAgent在TravelBench上表现优异，同时在广泛的通用基准测试中保持了通用能力，证明了所提智能体模型的有效性

Conclusion: STAgent通过专门的工具环境、高质量数据筛选和级联训练方法，成功创建了既能处理复杂时空任务又保持通用能力的智能体模型

Abstract: We present STAgent, an agentic large language model tailored for spatio-temporal understanding, designed to solve complex tasks such as constrained point-of-interest discovery and itinerary planning. STAgent is a specialized model capable of interacting with ten distinct tools within spatio-temporal scenarios, enabling it to explore, verify, and refine intermediate steps during complex reasoning. Notably, STAgent effectively preserves its general capabilities. We empower STAgent with these capabilities through three key contributions: (1) a stable tool environment that supports over ten domain-specific tools, enabling asynchronous rollout and training; (2) a hierarchical data curation framework that identifies high-quality data like a needle in a haystack, curating high-quality queries with a filter ratio of 1:10,000, emphasizing both diversity and difficulty; and (3) a cascaded training recipe that starts with a seed SFT stage acting as a guardian to measure query difficulty, followed by a second SFT stage fine-tuned on queries with high certainty, and an ultimate RL stage that leverages data of low certainty. Initialized with Qwen3-30B-A3B to establish a strong SFT foundation and leverage insights into sample difficulty, STAgent yields promising performance on TravelBench while maintaining its general capabilities across a wide range of general benchmarks, thereby demonstrating the effectiveness of our proposed agentic model.

</details>


### [31] [Context-aware LLM-based AI Agents for Human-centered Energy Management Systems in Smart Buildings](https://arxiv.org/abs/2512.25055)
*Tianzhi He,Farrokh Jazizadeh*

Main category: cs.AI

TL;DR: 提出基于大语言模型的建筑能源管理系统AI代理框架，通过自然语言交互实现智能建筑的上下文感知能源管理，在设备控制、能源分析等任务上表现良好，但在成本估算等复杂任务上仍需改进。


<details>
  <summary>Details</summary>
Motivation: 现有能源管理系统存在局限性，需要更智能、上下文感知的解决方案。通过利用大语言模型的自主数据分析能力，开发能够理解自然语言、提供智能能源管理的AI代理系统。

Method: 提出包含三个模块的概念框架：感知模块（传感）、中央控制模块（大脑）和行动模块（执行与用户交互），形成闭环反馈系统。使用120个用户查询在四个真实住宅能源数据集上评估原型性能，评估指标包括延迟、功能、能力、准确性和成本效益。

Result: 原型在设备控制任务上达到86%准确率，内存相关任务97%，调度与自动化74%，能源分析77%。但复杂成本估算任务准确率仅为49%，显示需要改进。通过ANOVA测试证明了框架的通用性。

Conclusion: 该研究为基于LLM的BEMS AI代理评估提供了框架，展示了在智能建筑能源管理中的潜力，同时指出了响应准确性与计算效率之间的权衡，为未来研究指明了方向。

Abstract: This study presents a conceptual framework and a prototype assessment for Large Language Model (LLM)-based Building Energy Management System (BEMS) AI agents to facilitate context-aware energy management in smart buildings through natural language interaction. The proposed framework comprises three modules: perception (sensing), central control (brain), and action (actuation and user interaction), forming a closed feedback loop that captures, analyzes, and interprets energy data to respond intelligently to user queries and manage connected appliances. By leveraging the autonomous data analytics capabilities of LLMs, the BEMS AI agent seeks to offer context-aware insights into energy consumption, cost prediction, and device scheduling, thereby addressing limitations in existing energy management systems. The prototype's performance was evaluated using 120 user queries across four distinct real-world residential energy datasets and different evaluation metrics, including latency, functionality, capability, accuracy, and cost-effectiveness. The generalizability of the framework was demonstrated using ANOVA tests. The results revealed promising performance, measured by response accuracy in device control (86%), memory-related tasks (97%), scheduling and automation (74%), and energy analysis (77%), while more complex cost estimation tasks highlighted areas for improvement with an accuracy of 49%. This benchmarking study moves toward formalizing the assessment of LLM-based BEMS AI agents and identifying future research directions, emphasizing the trade-off between response accuracy and computational efficiency.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [32] [Extrapolating LATE with Weak IVs](https://arxiv.org/abs/2512.23854)
*Muyang Ren*

Main category: econ.EM

TL;DR: 本文开发了在工具变量变异有限（弱工具变量）情况下仍然有效的推断方法，用于从依从者的处理效应外推到更广泛人群，包括LATE、ATE、ATT等参数。


<details>
  <summary>Details</summary>
Motivation: 评估反事实政策有效性时，需要将依从者的处理效应外推到更广泛人群。这种外推依赖于工具变量的外生变异，但实践中工具变量往往变异有限（弱工具变量），导致传统置信区间过短且无效，经典方法（如F检验）可能错误地判断工具变量强度。

Method: 开发了即使在工具变量变异有限情况下仍然有效的推断结果，构建了各种边际处理效应线性泛函（包括LATE、ATE、ATT和政策相关处理效应）的渐近有效置信集，不依赖于识别强度。

Result: 这是第一篇为这类参数提供弱工具变量稳健推断结果的论文。方法能够提供有效的置信区间，即使工具变量变异有限。使用Agan、Doleac和Harvey（2023）的数据进行了实证分析，研究了改变检察官宽大政策对减少再犯的反事实政策效应。

Conclusion: 本文提出的方法解决了弱工具变量情况下政策评估的关键问题，为从依从者处理效应外推到更广泛人群提供了可靠的统计推断工具，在实证应用中展示了实用价值。

Abstract: To evaluate the effectiveness of a counterfactual policy, it is often necessary to extrapolate treatment effects on compliers to broader populations. This extrapolation relies on exogenous variation in instruments, which is often weak in practice. This limited variation leads to invalid confidence intervals that are typically too short and cannot be accurately detected by classical methods. For instance, the F-test may falsely conclude that the instruments are strong. Consequently, I develop inference results that are valid even with limited variation in the instruments. These results lead to asymptotically valid confidence sets for various linear functionals of marginal treatment effects, including LATE, ATE, ATT, and policy-relevant treatment effects, regardless of identification strength. This is the first paper to provide weak instrument robust inference results for this class of parameters. Finally, I illustrate my results using data from Agan, Doleac, and Harvey (2023) to analyze counterfactual policies of changing prosecutors' leniency and their effects on reducing recidivism.

</details>


### [33] [Evaluating Counterfactual Policies Using Instruments](https://arxiv.org/abs/2512.24096)
*Michal Kolesár,José Luis Montiel Olea,Jonathan Roth*

Main category: econ.EM

TL;DR: 开发了一个无需IV单调性假设的通用计算框架，用于计算反事实政策影响的尖锐边界，并应用于司法系统中的保释和检察官分配案例。


<details>
  <summary>Details</summary>
Motivation: 研究者通常使用工具变量（IV）评估反事实政策效果，但传统方法依赖于IV单调性假设，这在许多实际应用中可能不成立。需要开发一个更通用的框架，能够在没有单调性假设的情况下计算政策效果的边界。

Method: 开发了一个通用且计算可行的框架，用于计算反事实政策效果的尖锐边界。该方法不要求IV单调性假设，分析了替代识别限制的识别能力，包括边际处理效应文献中使用的政策不变性假设，并开发了该假设的松弛版本。

Result: 对于一类重要的政策评估，IV单调性虽然对两阶段最小二乘法的因果解释至关重要，但不会收紧反事实政策影响的边界。框架应用于纽约市保释法官和马萨诸塞州检察官的准随机分配案例。

Conclusion: 该框架为评估反事实政策效果提供了更灵活的工具，减少了对强假设的依赖，增强了政策评估的稳健性，特别是在司法系统等实际应用场景中。

Abstract: We study settings in which a researcher has an instrumental variable (IV) and seeks to evaluate the effects of a counterfactual policy that alters treatment assignment, such as a directive encouraging randomly assigned judges to release more defendants. We develop a general and computationally tractable framework for computing sharp bounds on the effects of such policies. Our approach does not require the often tenuous IV monotonicity assumption. Moreover, for an important class of policy exercises, we show that IV monotonicity -- while crucial for a causal interpretation of two-stage least squares -- does not tighten the bounds on the counterfactual policy impact. We analyze the identifying power of alternative restrictions, including the policy invariance assumption used in the marginal treatment effect literature, and develop a relaxation of this assumption. We illustrate our framework using applications to quasi-random assignment of bail judges in New York City and prosecutors in Massachusetts.

</details>


### [34] [Testing Monotonicity in a Finite Population](https://arxiv.org/abs/2512.25032)
*Jiafeng Chen,Jonathan Roth,Jann Spiess*

Main category: econ.EM

TL;DR: 本文探讨从完全随机化实验中学习"单调性"（所有人处理效应符号相同）的可能性。虽然从设计视角看单调性在形式上可识别，但实际学习能力严重受限。


<details>
  <summary>Details</summary>
Motivation: 研究从随机化实验中能否推断出处理效应的单调性（所有人处理效应符号相同）。传统抽样视角认为单调性不可检验，但设计视角下存在形式识别可能性，需要评估实际学习能力。

Method: 采用设计视角（固定总体单位，仅处理分配随机），分析单调性的形式识别性。通过频率主义检验（检验功效）和贝叶斯更新（先验后验变化）评估数据信息量。

Result: 1）设计视角下单调性在有限总体中形式可识别；2）频率检验对某些备择假设有非平凡功效，但功效普遍受限；3）存在非退化贝叶斯先验对单调性永不更新；4）实际学习单调性的能力严重受限。

Conclusion: 尽管存在形式识别结果，但从实际数据中学习单调性的能力严重受限。频率检验功效有限，贝叶斯更新可能无效，因此实践中推断单调性非常困难。

Abstract: We consider the extent to which we can learn from a completely randomized experiment whether everyone has treatment effects that are weakly of the same sign, a condition we call monotonicity. From a classical sampling perspective, it is well-known that monotonicity is untestable. By contrast, we show from the design-based perspective -- in which the units in the population are fixed and only treatment assignment is stochastic -- that the distribution of treatment effects in the finite population (and hence whether monotonicity holds) is formally identified. We argue, however, that the usual definition of identification is unnatural in the design-based setting because it imagines knowing the distribution of outcomes over different treatment assignments for the same units. We thus evaluate the informativeness of the data by the extent to which it enables frequentist testing and Bayesian updating. We show that frequentist tests can have nontrivial power against some alternatives, but power is generically limited. Likewise, we show that there exist (non-degenerate) Bayesian priors that never update about whether monotonicity holds. We conclude that, despite the formal identification result, the ability to learn about monotonicity from data in practice is severely limited.

</details>


### [35] [Compound Estimation for Binomials](https://arxiv.org/abs/2512.25042)
*Yan Chen,Lihua Lei*

Main category: econ.EM

TL;DR: 提出基于近似SURE的线性收缩估计方法，直接处理二项分布数据，避免高斯近似，在小样本或小均值参数下仍有效


<details>
  <summary>Details</summary>
Motivation: 许多应用需要估计多个二项分布结果的均值（如代际流动性、疾病患病率、点击率）。传统简单平均方法在样本量小或均值参数小时噪声大，而经验贝叶斯方法需要先验分布且计算困难，特别是当样本量异质时缺乏联合共轭先验。

Method: 采用复合决策框架，将样本量和均值参数视为固定量。开发近似Stein无偏风险估计器（SURE）来评估平均均方误差。构建机器学习辅助的线性收缩估计器类，建立渐近最优性、遗憾界和有效推断。直接处理二项分布，避免高斯近似。

Result: 方法在小样本和/或小均值参数下有效，适用于单样本和双样本设置。在三个实际数据集（企业歧视、教育结果、创新率）上验证了方法的有效性。

Conclusion: 提出的方法克服了传统经验贝叶斯方法在二项分布数据中的识别和计算困难，为小样本或小均值参数的二项分布均值估计问题提供了有效的解决方案。

Abstract: Many applications involve estimating the mean of multiple binomial outcomes as a common problem -- assessing intergenerational mobility of census tracts, estimating prevalence of infectious diseases across countries, and measuring click-through rates for different demographic groups. The most standard approach is to report the plain average of each outcome. Despite simplicity, the estimates are noisy when the sample sizes or mean parameters are small. In contrast, the Empirical Bayes (EB) methods are able to boost the average accuracy by borrowing information across tasks. Nevertheless, the EB methods require a Bayesian model where the parameters are sampled from a prior distribution which, unlike the commonly-studied Gaussian case, is unidentified due to discreteness of binomial measurements. Even if the prior distribution is known, the computation is difficult when the sample sizes are heterogeneous as there is no simple joint conjugate prior for the sample size and mean parameter.
  In this paper, we consider the compound decision framework which treats the sample size and mean parameters as fixed quantities. We develop an approximate Stein's Unbiased Risk Estimator (SURE) for the average mean squared error given any class of estimators. For a class of machine learning-assisted linear shrinkage estimators, we establish asymptotic optimality, regret bounds, and valid inference. Unlike existing work, we work with the binomials directly without resorting to Gaussian approximations. This allows us to work with small sample sizes and/or mean parameters in both one-sample and two-sample settings. We demonstrate our approach using three datasets on firm discrimination, education outcomes, and innovation rates.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [36] [Forecasting the Term Structure of Interest Rates with SPDE-Based Models](https://arxiv.org/abs/2512.23910)
*Qihao Duan,Alexandre B. Simas,David Bolin,Raphaël Huser*

Main category: stat.AP

TL;DR: 该论文提出了一种将DNS模型残差建模为高斯随机场的新方法，通过SPDE表示实现灵活的协方差结构和可扩展的贝叶斯推断，显著改善了收益率曲线预测精度。


<details>
  <summary>Details</summary>
Motivation: 标准DNS模型在收益率曲线预测中存在局限性，其残差结构可能包含未被捕捉的时间与期限依赖性，需要更灵活的建模方法来提高预测精度和实用性。

Method: 将DNS模型残差建模为高斯随机场，通过随机偏微分方程（SPDE）表示，支持平稳、非平稳、各向异性和不可分离等多种协方差结构。采用集成嵌套拉普拉斯近似（INLA）进行贝叶斯推断，联合估计潜在DNS因子和残差场。

Result: SPDE-DNS模型在点预测和概率预测方面均优于标准基准模型。在均值-方差债券投资组合框架中，该模型产生了具有经济意义的效用增益。更重要的是，SPDE残差建模显著减少了剩余测量误差中的跨期限和跨时间依赖性，使其更接近白噪声。

Conclusion: 将DNS模型与SPDE驱动的残差建模相结合，为收益率曲线预测提供了灵活、可解释且计算高效的方法，在预测精度和实际应用价值方面均有显著优势。

Abstract: The Dynamic Nelson--Siegel (DNS) model is a widely used framework for term structure forecasting. We propose a novel extension that models DNS residuals as a Gaussian random field, capturing dependence across both time and maturity. The residual field is represented via a stochastic partial differential equation (SPDE), enabling flexible covariance structures and scalable Bayesian inference through sparse precision matrices. We consider a range of SPDE specifications, including stationary, non-stationary, anisotropic, and nonseparable models. The SPDE--DNS model is estimated in a Bayesian framework using the integrated nested Laplace approximation (INLA), jointly inferring latent DNS factors and the residual field. Empirical results show that the SPDE-based extensions improve both point and probabilistic forecasts relative to standard benchmarks. When applied in a mean--variance bond portfolio framework, the forecasts generate economically meaningful utility gains, measured as performance fees relative to a Bayesian DNS benchmark under monthly rebalancing. Importantly, incorporating the structured SPDE residual substantially reduces cross-maturity and intertemporal dependence in the remaining measurement error, bringing it closer to white noise. These findings highlight the advantages of combining DNS with SPDE-driven residual modeling for flexible, interpretable, and computationally efficient yield curve forecasting.

</details>


### [37] [Exposed: Shedding Blacklight on Online Privacy](https://arxiv.org/abs/2512.24041)
*Lucas Shen,Gaurav Sood*

Main category: stat.AP

TL;DR: 该研究通过结合美国代表性用户的浏览数据和Blacklight跟踪检测工具，发现几乎所有用户都会遇到广告跟踪器或第三方cookie，超过一半用户在48小时内访问使用会话记录、键盘记录或画布指纹等侵入性技术的网站，且Google等少数组织能跟踪大部分用户的网络活动。


<details>
  <summary>Details</summary>
Motivation: 研究旨在量化网络监控的程度、技术和实施者，了解用户在网络上被监视的真实情况，特别是不同人口群体面临的监控风险差异。

Method: 结合美国代表性样本的匿名浏览数据与Blacklight的域名级跟踪数据，分析跟踪技术的普及程度，并将跟踪器链接到其母公司组织。

Result: 99%以上用户遇到广告跟踪器或第三方cookie；超过一半用户在48小时内访问使用会话记录、键盘记录或画布指纹等侵入性技术的网站；单一组织（通常是Google）能跟踪超过一半用户50%以上的网络活动；人口差异在控制浏览量后有所减弱，但年龄和种族差异仍然存在。

Conclusion: 网络监控普遍存在且集中化，用户浏览内容（不仅仅是浏览量）影响监控风险，人口差异表明某些群体面临更高的监控风险。

Abstract: To what extent are users surveilled on the web, by what technologies, and by whom? We answer these questions by combining passively observed, anonymized browsing data of a large, representative sample of Americans with domain-level data on tracking from Blacklight. We find that nearly all users ($ > 99\%$) encounter at least one ad tracker or third-party cookie over the observation window. More invasive techniques like session recording, keylogging, and canvas fingerprinting are less widespread, but over half of the users visited a site employing at least one of these within the first 48 hours of the start of tracking. Linking trackers to their parent organizations reveals that a single organization, usually Google, can track over $50\%$ of web activity of more than half the users. Demographic differences in exposure are modest and often attenuate when we account for browsing volume. However, disparities by age and race remain, suggesting that what users browse, not just how much, shapes their surveillance risk.

</details>


### [38] [The Malaysian Election Corpus (MECo): Electoral Maps and Cartograms from 1954 to 2025](https://arxiv.org/abs/2512.24211)
*Thevesh Thevananthan,Danesh Prakash Chacko*

Main category: stat.AP

TL;DR: 创建了马来西亚历史上所有19次选区划分的数字化边界数据集，填补了选举数据基础设施的关键空白


<details>
  <summary>Details</summary>
Motivation: 马来西亚的选区边界缺乏机器可读的公开数据，这阻碍了对选区划分不公和选区操纵等地理中心问题的严格分析，也限制了选举结果的空间视角研究

Method: 构建马来西亚选举语料库(MECo)的第二部分，收集了从1954年马来亚第一次选区划分到2019年沙巴选区划分的所有19次批准的选区划分的数字化边界，并自动生成截至2025年的所有联邦和州选举的地图，包括等面积和选民加权的地图变形

Result: 创建了马来西亚第一个完整、公开可用且机器可读的选区边界记录，填补了国家选举数据基础设施的关键空白

Conclusion: 该数据集为研究马来西亚选区划分不公、选区操纵和选举结果的空间分析提供了重要基础，支持更深入的地理空间分析

Abstract: Electoral boundaries in Malaysia are not publicly available in machine-readable form. This prevents rigorous analysis of geography-centric issues such as malapportionment and gerrymandering, and constrains spatial perspectives on electoral outcomes. We present the second component of the Malaysian Election Corpus (MECo), an open-access collection of digital electoral boundaries covering all 19 approved delimitation exercises in Malaysia's history, from the first set of Malayan boundaries in 1954 until the 2019 Sabah delimitation. We also auto-generate election-time maps for all federal and state elections up to 2025, and include equal-area and electorate-weighted cartograms to support deeper geospatial analysis. This is the first complete, publicly-available, and machine-readable record of Malaysia's electoral boundaries, and fills a critical gap in the country's electoral data infrastructure.

</details>


### [39] [$\ell_0$-Regularized Item Response Theory Model for Robust Ideal Point Estimation](https://arxiv.org/abs/2512.24642)
*Kwangok Seo,Johan Lim,Seokho Lee,Jong Hee Park*

Main category: stat.AP

TL;DR: 提出一种处理抗议投票的ℓ₀正则化理想点估计方法，能准确识别极端立法者位置，比传统方法更稳健快速


<details>
  <summary>Details</summary>
Motivation: 传统理想点估计方法面临抗议投票（立法者策略性反对自己政党）的挑战，这种投票会导致衰减偏差，使意识形态极端的立法者看起来人为地温和

Method: 扩展Imai等人（2016）的快速EM估计方法，采用ℓ₀正则化方法来处理抗议投票，比MCMC方法更快

Result: 即使在抗议投票比例很高的情况下，新方法仍能保持估计准确性；应用于美国众议院数据，成功恢复了"小队"成员的极端自由主义立场，而传统方法错误地将他们归类为温和派

Conclusion: 该方法提供了稳健的理想点估计和抗议投票的系统识别，有助于深入分析立法机构中的策略性投票行为

Abstract: Ideal point estimation methods face a significant challenge when legislators engage in protest voting -- strategically voting against their party to express dissatisfaction. Such votes introduce attenuation bias, making ideologically extreme legislators appear artificially moderate. We propose a novel statistical framework that extends the fast EM-based estimation approach of \cite{Imai2016} using $\ell_0$ regularization method to handle protest votes. Through simulation studies, we demonstrate that our proposed method maintains estimation accuracy even with high proportions of protest votes, while being substantially faster than MCMC-based methods. Applying our method to the 116th and 117th U.S. House of Representatives, we successfully recover the extreme liberal positions of ``the Squad'', whose protest votes had caused conventional methods to misclassify them as moderates. While conventional methods rank Ocasio-Cortez as more conservative than 69\% of Democrats, our method places her firmly in the progressive wing, aligning with her documented policy positions. This approach provides both robust ideal point estimates and systematic identification of protest votes, facilitating deeper analysis of strategic voting behavior in legislatures.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [40] [An Electronic Ising Machine](https://arxiv.org/abs/2512.23720)
*Matt Bowring,Ben Anderdson,Ben Tiffany*

Main category: cs.ET

TL;DR: 开发基于退火原理的模拟计算PCB加速器，用于解决NP难图问题，利用耦合非线性电子振荡器实现能量最小化求解。


<details>
  <summary>Details</summary>
Motivation: 传统数字计算机在处理NP难图问题时效率低下，需要开发新型物理计算设备来加速这类组合优化问题的求解。

Method: 设计基于退火原理的定制PCB，采用耦合非线性电子振荡器的模拟计算架构，通过能量表示问题，让系统自然沿梯度下降寻找稳定相位对齐编码的解。

Result: 开发出低功耗、高速的物理计算加速器，能够有效求解NP难图问题，展示了新型物理计算设备的潜力。

Conclusion: 基于退火原理的模拟计算架构为NP难问题提供了有前景的硬件加速方案，推动了新型物理计算设备的发展。

Abstract: We develop a custom printed circuit board (PCB) as a low-power and high-speed accelerator for NP-Hard graph problems. Based on the annealing principle, it uses an analog computing architecture of coupled nonlinear electronic oscillators. Using an energy-based representation of the input problem, the system is shown to naturally follow the gradient towards stable phase alignments that encode solutions. We introduce the motivational theory, give an overview of our detailed circuit design, simulations, and experiments, and provide insight on the emerging development of novel physics-based computing devices.

</details>


### [41] [Biochemical Computing Mode for Sequential Logic](https://arxiv.org/abs/2512.23734)
*Han Huang,Chengzhi Ma,Yuxin Zhao,Qingyao Wang,Xinglong Xiao,Xiulin Shu,Zhifeng Hao*

Main category: cs.ET

TL;DR: 该论文提出了一种基于酶驱动小分子逻辑门的生化计算模式，证明了其具有类似电子计算机的顺序映射能力，为实现通用生化计算机提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 随着对下一代通用计算机研究的深入，光学、量子、DNA等新型计算模式不断涌现。顺序逻辑电路是实现通用计算的关键，但现有研究往往忽视这一难点。论文旨在探索生化计算模式如何实现类似电子计算机的顺序逻辑功能。

Method: 1. 提出"顺序映射"概念作为实现顺序逻辑电路的必要条件；2. 利用酶对酶促反应的控制效应，设计由小分子组成、酶驱动的逻辑门模型；3. 建立生化计算模式，并数学分析其静态和动态输入输出特性；4. 证明该模式满足类似电子计算机的顺序映射特性。

Result: 1. 成功设计了基于酶驱动小分子逻辑门的生化计算模式；2. 数学证明了该模式具有顺序映射能力；3. 结合NOT-AND门的存储特性，该模式能够实现顺序逻辑电路；4. 为开发通用生化计算机奠定了理论基础。

Conclusion: 该研究提出的生化计算模式通过酶驱动的小分子逻辑门实现了顺序映射，具备实现顺序逻辑电路的能力，为开发下一代通用生化计算机提供了重要的理论框架和技术路径。

Abstract: Recent years have witnessed the growing scholarly interest in the next-generation general-purpose computers. Various innovative computing modes have been proposed, such as optical, quantum phenomena, and DNA-based modes. Sequential logic circuits are a critical factor that enables these modes to function as general-purpose computers, given their essential role in facilitating continuous computation and memory storage through their ability to store states. However, compared to computability, it is often overlooked due to the difficulty of its implementation. In this paper, we first demonstrate sequential mapping, a crucial necessary condition for electronic computers to realize sequential logic circuits, and highlight this distinctive property of general-purpose computers in the context of logic gate circuits. To achieve computational functionalities comparable to those of electronic computers, we utilize the control effect of enzymes on enzymatic reactions to design a logic gate model that is composed of small molecules and driven by enzymes, subsequently propose a biochemical computing mode. Furthermore, we mathematically analyze the static and dynamic input-output properties of biochemical logic gate components and prove that the biochemical computing mode satisfies sequential mapping similar to electronic computers. When combined with the storage characteristics of NOT-AND gates, it can realize sequential logic circuits. The findings can serve as a theoretical foundation for developing general-purpose biochemical computers.

</details>


### [42] [Ovonic switches enable energy-efficient dendrite-like computing](https://arxiv.org/abs/2512.23736)
*Unhyeon Kang,Jaesang Lee,Seungmin Oh,Hanchan Song,Jongkil Park,Jaewook Kim,Seongsik Park,Hyun Jae Jang,Sangbum Kim,Su-in Yi,Suhas Kumar,Suyoun Lee*

Main category: cs.ET

TL;DR: 基于Sb-Te掺杂GeSe的奥弗辛斯基阈值开关实现单器件自持动力学、通用布尔逻辑和XOR运算，可用于图像边缘检测，为神经形态计算提供高效计算原语。


<details>
  <summary>Details</summary>
Motivation: 生物神经元树突具有复杂的时间动力学、布尔逻辑、算术运算、信号辨别和边缘检测等功能，模仿这种丰富的功能密度可为神经形态计算提供强大的计算原语，以替代老化的数字计算范式。

Method: 使用电驱动的Sb-Te掺杂GeSe奥弗辛斯基阈值开关，构建具有自持动力学能力的双端器件，实现通用布尔逻辑和XOR运算，并利用逻辑驱动动力学进行图像边缘梯度检测。

Result: 单器件可实现自持动力学、通用布尔逻辑和XOR运算，能检测和估计图像边缘梯度；开关网络表现出半加器和全加器特性，支持抑制性和兴奋性信号的判别逻辑，相比现有数字解决方案能效提高多个数量级。

Conclusion: 该工作展示了模拟树突功能的高效计算原语，为后数字时代的神经形态计算开辟了新路径，提供了比传统数字方案更简单且能效更高的解决方案。

Abstract: Over the last decade, dendrites within individual biological neurons, which were previously thought to generally perform information pooling and networking, have now been shown to express complex temporal dynamics, Boolean-like logic, arithmetic, signal discrimination, and edge detection for image and sound recognition. Mimicking this rich functional density could offer a powerful primitive for neuromorphic computing, which has sought to replace the aging digital computing paradigms using biological inspirations. Here, using electrically driven Ovonic threshold switching in Sb-Te-doped GeSe, we demonstrate a single two-terminal component capable of self-sustained dynamics and universal Boolean logic, in addition to XOR operations (which is traditionally thought to require a network of active components). We then employ logic-driven dynamics in a single component to detect and estimate the gradients of edges in images, a task that otherwise requires elaborate circuits. A network of Ovonic switches exhibits properties of a half adder and a full adder, in addition to discriminative logic accommodating inhibitory and excitatory signals. We show that this computational primitive is not only seemingly simpler, but also offers many orders of magnitude improved energy efficiency compared to prevailing digital solutions. As such, this work paves the path for potentially emulating dendrites for efficient post-digital neuromorphic computing.

</details>


### [43] [Exploring the Potential of Spiking Neural Networks in UWB Channel Estimation](https://arxiv.org/abs/2512.23975)
*Youdong Zhang,Xu He,Xiaolin Meng*

Main category: cs.ET

TL;DR: 本文提出一种基于脉冲神经网络（SNN）的无监督UWB信道估计方法，在保持80%测试精度的同时大幅降低模型复杂度，适合神经形态硬件部署。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的UWB信道估计方法虽然精度高，但计算量大，与低成本边缘设备的资源限制严重冲突。因此需要探索更高效的解决方案。

Method: 开发了一种完全无监督的脉冲神经网络（SNN）解决方案，设计了全面的比较策略，并在公开基准数据集上进行评估。

Result: 实验结果显示，该无监督方法仍能达到80%的测试精度，与多种有监督的深度学习策略相当。相比复杂深度学习方法，SNN实现更适合神经形态部署，模型复杂度大幅降低。

Conclusion: SNN为UWB信道估计提供了高效解决方案，在保持精度的同时显著降低计算复杂度，为未来神经形态实践带来重要优势。

Abstract: Although existing deep learning-based Ultra-Wide Band (UWB) channel estimation methods achieve high accuracy, their computational intensity clashes sharply with the resource constraints of low-cost edge devices. Motivated by this, this letter explores the potential of Spiking Neural Networks (SNNs) for this task and develops a fully unsupervised SNN solution. To enable a comprehensive performance analysis, we devise an extensive set of comparative strategies and evaluate them on a compelling public benchmark. Experimental results show that our unsupervised approach still attains 80% test accuracy, on par with several supervised deep learning-based strategies. Moreover, compared with complex deep learning methods, our SNN implementation is inherently suited to neuromorphic deployment and offers a drastic reduction in model complexity, bringing significant advantages for future neuromorphic practice.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [44] [Analysis of Collaboration in CS Prizewinning with a Nobel-Turing Comparison](https://arxiv.org/abs/2512.23919)
*Boleslaw K. Szymanski,Yongtao Zhang,Brian Uzzi,Mohammed Shahid Modi*

Main category: cs.SI

TL;DR: 研究发现科学奖项获得者更早、更频繁地与其他获奖者合作，且与获奖者合作能提高获奖几率，计算机科学领域尤其如此。


<details>
  <summary>Details</summary>
Motivation: 科学奖项对研究轨迹有重要影响，但学术界对合作与获奖之间的关系了解甚少，需要系统研究奖项获得者的合作模式及其对获奖可能性的影响。

Method: 分析100多个科学奖项和5000多名计算机科学领域获奖者的合作行为，使用粗化精确匹配（CEM）和回归分析，对比不同奖项类型（通用vs专业）的合作模式，并考察诺贝尔奖与图灵奖获得者的轨迹差异。

Result: 获奖者更早、更频繁地与其他获奖者合作；与获奖者合作能提高合作者获奖几率；通用计算机科学奖项获得者比专业奖项获得者合作更频繁；与获奖者合作强度增加能提高获奖几率；物理-计算机科学跨学科合作在诺贝尔物理学奖中占比增加。

Conclusion: 科学奖项与学术合作存在显著关联，获奖者倾向于形成合作网络，这种合作模式反过来又影响未来的获奖可能性，揭示了科学奖励系统中的社会网络效应。

Abstract: In the scientific community, prizes play a pivotal role in shaping research trajectories by conferring credibility and offering financial incentives to researchers. Yet, we know little about the relationship between academic collaborations and prizewinning. By analyzing over 100 scientific prizes and the collaboration behaviors of over 5,000 prizewinners in CS, we find that prizewinners collaborate earlier and more frequently with other prizewinners than researchers who have not yet received similar recognition. Moreover, CS researchers across age groups collaborate more with prizewinners after winning their first prize, and collaborating with prizewinners after their first win increases the likelihood of the collaborator winning an award. We find that recipients of general CS prizes collaborate more than recipients of more specialized prizes, who collaborate less frequently. With Coarsened Exact Matching (CEM) and regression, we find an increase in prizewinning odds with strength of prizewinner collaboration. We examine the context of recent Nobel Prizes going to CS researchers by showing how an increasing share of Physics awards go to Physics-CS collaborations, and contrast Nobel-Turing winning author's trajectories. Our findings shed light on the relationship between prizewinning and collaboration.

</details>


### [45] [A Community-Aware Framework for Influence Maximization with Explicit Accounting for Inter-Community Influence](https://arxiv.org/abs/2512.23973)
*Eliot W. Robson,Abhishek K. Umrawal*

Main category: cs.SI

TL;DR: Community-IM++：一种可扩展的影响力最大化框架，通过建模跨社区扩散和使用社区扩散度启发式方法，在保持高效的同时实现接近贪心算法的传播效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于社区的影响力最大化方法通常假设社区间独立，忽略了跨社区影响力，这在真实社交网络中会降低算法效果。需要一种既能保持可扩展性又能准确建模跨社区扩散的方法。

Method: 提出Community-IM++框架：1) 网络分区；2) 计算社区扩散度(CDD)来优先选择桥接节点；3) 使用渐进预算分配策略和惰性评估，自适应地在社区间分配种子节点以减少冗余计算。

Result: 在大型真实社交网络上的实验表明，Community-IM++在不同边权重模型下，能以高达100倍的运行时间降低实现接近贪心算法的影响力传播效果，同时优于Community-IM和度启发式方法。

Conclusion: Community-IM++为大规模应用（如病毒营销、错误信息控制和公共卫生运动）提供了实用解决方案，在效率和跨社区覆盖方面表现出色，平衡了准确性和可扩展性。

Abstract: Influence Maximization (IM) seeks to identify a small set of seed nodes in a social network to maximize expected information spread under a diffusion model. While community-based approaches improve scalability by exploiting modular structure, they typically assume independence between communities, overlooking inter-community influence$\unicode{x2014}$a limitation that reduces effectiveness in real-world networks. We introduce Community-IM++, a scalable framework that explicitly models cross-community diffusion through a principled heuristic based on community-based diffusion degree (CDD) and a progressive budgeting strategy. The algorithm partitions the network, computes CDD to prioritize bridging nodes, and allocates seeds adaptively across communities using lazy evaluation to minimize redundant computations. Experiments on large real-world social networks under different edge weight models show that Community-IM++ achieves near-greedy influence spread at up to 100 times lower runtime, while outperforming Community-IM and degree heuristics across budgets and structural conditions. These results demonstrate the practicality of Community-IM++ for large-scale applications such as viral marketing, misinformation control, and public health campaigns, where efficiency and cross-community reach are critical.

</details>


### [46] [A density-based framework for community detection in attributed networks](https://arxiv.org/abs/2512.24336)
*Sara Geremia,Michael Fop,Domenico De Stefano*

Main category: cs.SI

TL;DR: AttDeCoDe是一个基于属性的社区检测方法，将节点属性空间密度与网络结构约束结合，用于分析节点特征在群体形成中起核心作用的网络。


<details>
  <summary>Details</summary>
Motivation: 现有社区检测方法通常单独处理网络结构机制（如度异质性、领导者驱动吸引力）和节点属性同质性，限制了在存在这些机制时恢复可解释社区的能力。需要一种能同时考虑结构和属性的方法。

Method: 提出AttDeCoDe方法，扩展基于密度的社区检测框架。不是从纯网络拓扑定义密度，而是在属性空间中估计节点级密度，允许社区围绕基于属性的社区代表形成，同时保持结构连通性约束。

Result: 通过基于扩展的度校正随机块模型的模拟研究评估，该模型加入了属性驱动的领导者吸引力。在Horizon计划研究合作数据的实证应用中，AttDeCoDe表现出竞争性性能。

Conclusion: AttDeCoDe为属性网络中的社区检测提供了一个灵活且可解释的框架，相对于基于拓扑和属性辅助的基准方法实现了竞争性性能。

Abstract: Community structure in social and collaborative networks often emerges from a complex interplay between structural mechanisms, such as degree heterogeneity and leader-driven attraction, and homophily on node attributes. Existing community detection methods typically focus on these dimensions in isolation, limiting their ability to recover interpretable communities in presence of such mechanisms. In this paper, we propose AttDeCoDe, an attribute-driven extension of a density-based community detection framework, developed to analyse networks where node characteristics play a central role in group formation. Instead of defining density purely from network topology, AttDeCoDe estimates node-wise density in the attribute space, allowing communities to form around attribute-based community representatives while preserving structural connectivity constraints. This approach naturally captures homophily-driven aggregation while remaining sensitive to leader influence. We evaluate the proposed method through a simulation study based on a novel generative model that extends the degree-corrected stochastic block model by incorporating attribute-driven leader attraction, reflecting key features of collaborative research networks. We perform an empirical application to research collaboration data from the Horizon programmes, where organisations are characterised by project-level thematic descriptors. Both results show that AttDeCoDe offers a flexible and interpretable framework for community detection in attributed networks achieving competitive performance relative to topology-based and attribute-assisted benchmarks.

</details>


### [47] [Analyzing Airline Alliances through Multi-Attribute Graph Partitioning to Maximize Competition and Market Penetration Capability](https://arxiv.org/abs/2512.24519)
*Khalil Al Handawi,Fabian Bastin*

Main category: cs.SI

TL;DR: 提出基于多属性图分割的航空公司联盟分析方法，通过双目标优化同时最大化市场竞争和市场渗透能力


<details>
  <summary>Details</summary>
Motivation: 航空运输市场竞争激烈且动态变化，航空公司通过联盟扩大网络覆盖、提高运营效率和改善客户体验，但这些联盟对市场竞争和运营效率的影响尚未完全理解

Method: 使用多属性图分割分析航空公司联盟，开发量化航班段竞争力和航空公司市场渗透能力的指标，构建最大化竞争和市场渗透的双目标优化问题，并提出求解算法

Result: 使用真实航班时刻数据验证算法有效性，揭示了航空公司联盟结构及其对市场竞争和运营效率的影响

Conclusion: 提出的方法为分析航空公司联盟提供了新视角，有助于理解联盟对市场竞争和运营效率的复杂影响

Abstract: The air transportation market is highly competitive and dynamic. Airlines often form alliances to expand their network reach, improve operational efficiency, and enhance customer experience. However, the impact of these alliances on market competition and operational efficiency is not fully understood. In this paper, we propose a novel approach to analyze airline alliances using multi\mfabian{-}attribute graph partitioning. We develop metrics to quantify the competitiveness of flight segments and the market penetration capability of airlines based on their alliance memberships. We formulate a bi\mfabian{-}objective optimization problem to maximize both competition and market penetration simultaneously. We also propose algorithms to solve this optimization problem and demonstrate their effectiveness using real-world flight schedule data. Our results provide insights into the structure of airline alliances and their implications for market competition and operational efficiency.

</details>


### [48] [When Does the Silhouette Score Work? A Comprehensive Study in Network Clustering](https://arxiv.org/abs/2512.24841)
*Zongyue Teng,Jun Yan,Dandan Liu,Panpan Zhang*

Main category: cs.SI

TL;DR: 轮廓分数在社区检测中表现良好，能准确识别真实社区数量，但在社区大小不平衡或分离度弱时会低估，在稀疏网络中会高估。


<details>
  <summary>Details</summary>
Motivation: 社区数量选择是网络聚类的核心挑战。轮廓分数提供了直观、无模型的准则，能平衡簇内凝聚力和簇间分离度。尽管在聚类分析中广泛应用，但其在网络社区检测中的性能尚未充分表征。

Method: 通过模拟研究全面评估轮廓分数在无权、加权和全连接网络中的性能，考察网络大小、分离强度和社区大小不平衡的影响。将评估扩展到真实的航空公司可达性网络。

Result: 当簇分离良好且平衡时，轮廓分数能准确识别真实社区数量；但在强烈不平衡或弱分离时会低估，在稀疏网络中会高估。在真实航空网络中，基于轮廓的聚类能恢复地理可解释和市场导向的簇。

Conclusion: 研究为在网络聚类中应用轮廓分数提供了实证指导，并阐明了其使用最可靠的条件。

Abstract: Selecting the number of communities is a fundamental challenge in network clustering. The silhouette score offers an intuitive, model-free criterion that balances within-cluster cohesion and between-cluster separation. Albeit its widespread use in clustering analysis, its performance in network-based community detection remains insufficiently characterized. In this study, we comprehensively evaluate the performance of the silhouette score across unweighted, weighted, and fully connected networks, examining how network size, separation strength, and community size imbalance influence its performance. Simulation studies show that the silhouette score accurately identifies the true number of communities when clusters are well separated and balanced, but it tends to underestimate under strong imbalance or weak separation and to overestimate in sparse networks. Extending the evaluation to a real airline reachability network, we demonstrate that the silhouette-based clustering can recover geographically interpretable and market-oriented clusters. These findings provide empirical guidance for applying the silhouette score in network clustering and clarify the conditions under which its use is most reliable.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [49] [New Exam Security Questions in the AI Era: Comparing AI-Generated Item Similarity Between Naive and Detail-Guided Prompting Approaches](https://arxiv.org/abs/2512.23729)
*Ting Wang,Caroline Prendergast,Susan Lottridge*

Main category: cs.CY

TL;DR: LLM生成的医学考试题目中，仅使用公开信息（naive策略）与使用专有指导（guided策略）生成的题目在某些临床领域相似度很高，增加了考试安全风险


<details>
  <summary>Details</summary>
Motivation: 研究LLM在生成医学考试题目时，仅使用公开信息与使用专有指导材料生成的题目是否存在显著差异，以评估考试安全风险

Method: 使用三个LLM（GPT-4o、Claude 4 Sonnet、Gemini 2.5 Flash）基于美国全科医学委员会蓝图生成题目，分为仅用公开EPA描述符的naive策略和加入专有蓝图、题目编写指南及示例题目的guided策略，共生成160个题目，使用PubMedBERT和BioBERT编码并计算余弦相似度

Result: 各策略内部一致性高，但策略间相似度总体较低；然而在病毒性肺炎和高血压等狭窄定义的临床领域，naive和guided策略生成的题目相似度超过0.65阈值，表明公开信息也能生成与专有指导相似的题目

Conclusion: 专有资源确实能带来独特性，但仅用公开信息的LLM在受限临床领域仍能生成与专有指导相似的题目，增加了题目泄露风险；需要以人为本、AI辅助的题目开发，严格分离形成性和总结性题目库，并进行系统性相似度监测

Abstract: Large language models (LLMs) have emerged as powerful tools for generating domain-specific multiple-choice questions (MCQs), offering efficiency gains for certification boards but raising new concerns about examination security. This study investigated whether LLM-generated items created with proprietary guidance differ meaningfully from those generated using only publicly available resources. Four representative clinical activities from the American Board of Family Medicine (ABFM) blueprint were mapped to corresponding Entrustable Professional Activities (EPAs), and three LLMs (GPT-4o, Claude 4 Sonnet, Gemini 2.5 Flash) produced items under a naive strategy using only public EPA descriptors, while GPT-4o additionally produced items under a guided strategy that incorporated proprietary blueprints, item-writing guidelines, and exemplar items, yielding 160 total items. Question stems and options were encoded using PubMedBERT and BioBERT, and intra- and inter-strategy cosine similarity coefficients were calculated. Results showed high internal consistency within each prompting strategy, while cross-strategy similarity was lower overall. However, several domain model pairs, particularly in narrowly defined areas such as viral pneumonia and hypertension, exceeded the 0.65 threshold, indicating convergence between naive and guided pipelines. These findings suggest that while proprietary resources impart distinctiveness, LLMs prompted only with public information can still generate items closely resembling guided outputs in constrained clinical domains, thereby heightening risks of item exposure. Safeguarding the integrity of high stakes examinations will require human-first, AI-assisted item development, strict separation of formative and summative item pools, and systematic similarity surveillance to balance innovation with security.

</details>


### [50] [Artificial Intelligence for All? Brazilian Teachers on Ethics, Equity, and the Everyday Challenges of AI in Education](https://arxiv.org/abs/2512.23834)
*Bruno Florentino,Camila Sestito,Wellington Cruz,André de Carvalho,Robson Bonidia*

Main category: cs.CY

TL;DR: 巴西K-12教师对AI教育应用持积极态度但缺乏系统培训，面临基础设施不足和政策缺失等挑战


<details>
  <summary>Details</summary>
Motivation: 了解巴西基础教育教师对通用人工智能在教育中应用的认知、态度和面临的挑战，为AI在巴西K-12教育中的有效整合提供依据

Method: 采用定量分析方法，通过问卷调查收集了巴西346名来自不同地区、教育水平和经验的教育工作者的数据，分析他们的AI素养和使用情况

Result: 80.3%的教师只有基础或有限的AI知识，但对AI应用表现出强烈兴趣，特别是在互动内容创建(80.6%)、课程规划(80.2%)和个性化评估(68.6%)方面。同时发现缺乏培训(43.4%)、技术支持(41.9%)和基础设施不足等重大挑战

Conclusion: 巴西AI教育整合仍处于自下而上模式，缺乏官方课程指导和结构化培训。有效的AI实施需要综合公共政策、充分的教师培训和公平的技术获取，以促进AI在巴西K-12教育中的伦理、包容和情境化应用

Abstract: This study examines the perceptions of Brazilian K-12 education teachers regarding the use of AI in education, specifically General Purpose AI. This investigation employs a quantitative analysis approach, extracting information from a questionnaire completed by 346 educators from various regions of Brazil regarding their AI literacy and use. Educators vary in their educational level, years of experience, and type of educational institution. The analysis of the questionnaires shows that although most educators had only basic or limited knowledge of AI (80.3\%), they showed a strong interest in its application, particularly for the creation of interactive content (80.6%), lesson planning (80.2%), and personalized assessment (68.6%). The potential of AI to promote inclusion and personalized learning is also widely recognized (65.5%). The participants emphasized the importance of discussing ethics and digital citizenship, reflecting on technological dependence, biases, transparency, and responsible use of AI, aligning with critical education and the development of conscious students. Despite enthusiasm for the pedagogical potential of AI, significant structural challenges were identified, including a lack of training (43.4%), technical support (41.9%), and limitations of infrastructure, such as low access to computers, reliable Internet connections, and multimedia resources in schools. The study shows that Brazil is still in a bottom-up model for AI integration, missing official curricula to guide its implementation and structured training for teachers and students. Furthermore, effective implementation of AI depends on integrated public policies, adequate teacher training, and equitable access to technology, promoting ethical, inclusive, and contextually grounded adoption of AI in Brazilian K-12 education.

</details>


### [51] [How Large Language Models Systematically Misrepresent American Climate Opinions](https://arxiv.org/abs/2512.23889)
*Sola Kim,Jieshu Wang,Marco A. Janssen,John M. Anderies*

Main category: cs.CY

TL;DR: 研究发现LLM在模拟美国气候意见时压缩了多样性，对交叉身份群体（特别是黑人群体）的性别模式存在误判，可能影响公平气候治理。


<details>
  <summary>Details</summary>
Motivation: 随着联邦机构和研究人员越来越多地使用大语言模型分析和模拟公众意见，AI在公众与政策制定者之间的中介作用变得重要。在气候政策等争议性领域，对交叉身份群体的不准确估计可能误导政策设计和公众参与。

Method: 使用六个LLM，基于978名全国代表性美国气候意见调查受访者的个人资料生成回答，将AI生成的回答与实际人类回答在20个问题上进行比较，特别关注交叉身份模式。

Result: LLM压缩了美国气候意见的多样性，将不太关心的群体预测为更关心，反之亦然。这种压缩具有交叉性：LLM应用统一的性别假设，这些假设与白人和西班牙裔美国人的实际情况相符，但误判了黑人美国人的实际性别模式差异。

Conclusion: LLM在模拟交叉身份群体意见时存在系统性偏差，这些模式可能被标准审计方法忽视，可能破坏公平的气候治理，需要更细致的评估方法。

Abstract: Federal agencies and researchers increasingly use large language models to analyze and simulate public opinion. When AI mediates between the public and policymakers, accuracy across intersecting identities becomes consequential; inaccurate group-level estimates can mislead outreach, consultation, and policy design. While research examines intersectionality in LLM outputs, no study has compared these outputs against real human responses across intersecting identities. Climate policy is one such domain, and this is particularly urgent for climate change, where opinion is contested and diverse. We investigate how LLMs represent intersectional patterns in U.S. climate opinions. We prompted six LLMs with profiles of 978 respondents from a nationally representative U.S. climate opinion survey and compared AI-generated responses to actual human answers across 20 questions. We find that LLMs appear to compress the diversity of American climate opinions, predicting less-concerned groups as more concerned and vice versa. This compression is intersectional: LLMs apply uniform gender assumptions that match reality for White and Hispanic Americans but misrepresent Black Americans, where actual gender patterns differ. These patterns, which may be invisible to standard auditing approaches, could undermine equitable climate governance.

</details>


### [52] [In Memorium: The Academic Journal](https://arxiv.org/abs/2512.23915)
*Russell Beale*

Main category: cs.CY

TL;DR: 对学术期刊生命历程的反思，探讨其历史贡献、社会影响变化，以及最终因偏离初心而导致的复杂遗产


<details>
  <summary>Details</summary>
Motivation: 反思学术期刊的生命周期和演变轨迹，探讨期刊如何从最初的使命逐渐偏离，以及这种转变对其社会影响和最终遗产的意义

Method: 采用历史回顾和批判性反思的方法，分析学术期刊的发展历程、社会贡献变化，以及最终地位评估

Result: 学术期刊在历史发展中经历了从创新推动者到可能偏离初心的转变，其最终遗产是复杂的——人们怀念其最初代表的价值，但对其后期偏离感到遗憾

Conclusion: 学术期刊的生命历程反映了知识传播机构的演变规律：初始理想可能随时间而淡化，最终遗产既包含对其最初价值的怀念，也包含对其偏离的批判

Abstract: We reflect on the life and influence of the academic journal, charting their history and contributions, discussing how their influence changed society, and examining how in death they will be mourned for what they initially stood for but in the end had moved so far from that they will less missed than they might have been.

</details>


### [53] [Statistical Guarantees in the Search for Less Discriminatory Algorithms](https://arxiv.org/abs/2512.23943)
*Chris Hays,Ben Laufer,Solon Barocas,Manish Raghavan*

Main category: cs.CY

TL;DR: 提出了一种自适应停止算法，用于在有限信息下搜索"较少歧视性算法"，并提供高概率上界证明搜索充分性。


<details>
  <summary>Details</summary>
Motivation: 企业在高风险领域部署数据驱动决策系统时，需要寻找性能相当但歧视性更低的算法。现有研究表明随机重训练可以找到这样的算法，但企业不能无限重训练，因此需要确定什么是"善意努力"的搜索。

Method: 将LDA搜索形式化为最优停止问题，提出自适应停止算法，在有限信息下产生高概率上界，证明搜索充分性。开发者可以施加更强的模型分布假设以获得更强边界。

Result: 在真实世界的信贷、就业和住房数据集上验证了该方法，能够为开发者提供可向法庭证明搜索充分性的认证。

Conclusion: 通过最优停止问题形式化LDA搜索，提供自适应算法让开发者在有限搜索后获得高概率上界，证明已充分探索模型空间，满足法律要求的"善意努力"标准。

Abstract: Recent scholarship has argued that firms building data-driven decision systems in high-stakes domains like employment, credit, and housing should search for "less discriminatory algorithms" (LDAs) (Black et al., 2024). That is, for a given decision problem, firms considering deploying a model should make a good-faith effort to find equally performant models with lower disparate impact across social groups. Evidence from the literature on model multiplicity shows that randomness in training pipelines can lead to multiple models with the same performance, but meaningful variations in disparate impact. This suggests that developers can find LDAs simply by randomly retraining models. Firms cannot continue retraining forever, though, which raises the question: What constitutes a good-faith effort? In this paper, we formalize LDA search via model multiplicity as an optimal stopping problem, where a model developer with limited information wants to produce strong evidence that they have sufficiently explored the space of models. Our primary contribution is an adaptive stopping algorithm that yields a high-probability upper bound on the gains achievable from a continued search, allowing the developer to certify (e.g., to a court) that their search was sufficient. We provide a framework under which developers can impose stronger assumptions about the distribution of models, yielding correspondingly stronger bounds. We validate the method on real-world credit, employment and housing datasets.

</details>


### [54] [From artificial to circular intelligence to support the well-being of our habitat](https://arxiv.org/abs/2512.24131)
*Francesca Larosa,Daniel Depellegrin,Andrea Conte,Marco Molinari,Silvia Santato,Adam Wickberg,Fermin Mallor,Anna Sperotto*

Main category: cs.CY

TL;DR: 提出名为"循环智能(CIntel)"的新概念框架，旨在通过自下而上的社区驱动方法，学习自然的再生和适应能力，将伦理原则融入技术设计，减少AI技术对地球的环境影响。


<details>
  <summary>Details</summary>
Motivation: 机器学习和人工智能的普及改变了人类与自然环境的互动方式。虽然监测工具、处理设施和物联网支持通过自动化评估地球健康，但这些数据密集型、资源密集型和基础设施密集型技术对地球并非中性。当前AI实践社区正在努力创建具有最小社会环境影响工具，本文旨在为此做出贡献。

Method: 提出名为"循环智能(CIntel)"的新概念和程序框架。该框架采用自下而上和社区驱动的方法，学习自然的再生和适应能力。在技术设计中融入伦理原则，以保护栖息地的稳定性，同时通过设计提高居民的福祉。

Result: 提出了一个创新的概念框架CIntel，该框架将生态循环原则与人工智能技术相结合，旨在创建更可持续、更具伦理意识的AI系统。

Conclusion: CIntel框架为AI实践者提供了一种新方法，通过模仿自然的再生能力并将伦理原则融入技术设计，减少AI技术对环境的负面影响，同时提高人类福祉，为实现更可持续的AI发展做出贡献。

Abstract: The proliferation of machine learning and artificial intelligence redefines the interaction between the anthropogenic and natural elements of our habitat.The use of monitoring tools, processing facilities and the internet of things supports the assessment of planetary health at any given time through automation. However, these data, natural resources and infrastructure intensive technologies are not neutral on the Earth. As the community of AI practitioners works on the creation of tools with minimal socio-environmental impacts, we contribute to the these efforts by proposing a novel conceptual and procedural framework which we call Circular Intelligence or CIntel. CIntel leverages a bottom-up and community-driven approach to learn from the ability of nature to regenerate and adapt. CIntel incorporates ethical principles in its technical design to preserve the stability of the habitat, while also increasing the well-being of its inhabitants by design.

</details>


### [55] [Effects of Algorithmic Visibility on Conspiracy Communities: Reddit after Epstein's 'Suicide'](https://arxiv.org/abs/2512.24351)
*Asja Attanasio,Francesco Corso,Gianmarco De Francisci Morales,Francesco Pierri*

Main category: cs.CY

TL;DR: 算法可见性（如首页推荐）对Reddit阴谋论社区的影响：有机发现用户比首页推荐用户更快融入社区语言规范且参与更稳定，首页推荐用户则语义距离核心话语更远、参与时间更短，算法可见性主要起选择机制而非简单放大作用。


<details>
  <summary>Details</summary>
Motivation: 研究算法可见性如何影响Reddit上大型阴谋论社区（r/conspiracy）在Jeffrey Epstein死亡后的动态，探究首页曝光是否会改变用户加入方式、停留时间和语言适应，与有机发现用户进行比较。

Method: 采用计算框架结合毒性评分、生存分析、词汇和语义测量，分析用户行为轨迹，比较通过首页推荐和有机发现两种途径加入社区的用户差异。

Result: 首页可见性主要作为选择机制而非简单放大器：有机发现用户更快融入社区语言和主题规范，参与更稳定；首页推荐用户语义距离核心话语更远，参与时间更短。在5个月观察期内，未发现偶然接触阴谋内容会导致持久激进化。

Conclusion: 算法可见性重塑了受众规模、社区构成和语言凝聚力：非有机加入的新用户激励不同、融入弱、离开快，限制了有机增长。这些发现可为设计平台推荐系统提供参考，以遏制有害阴谋内容传播，同时支持更负责任、透明和社会有益的算法推荐使用。

Abstract: This paper examines how algorithmic visibility shapes a large conspiracy community on Reddit after Jeffrey Epstein's death.
  We ask whether homepage exposure changes who join r/conspiracy, how long they stay, and how they adapt linguistically, compared with users who arrive through organic discovery.
  Using a computational framework that combines toxicity scores, survival analysis, and lexical and semantic measures, the study shows that homepage visibility acts as a selection mechanism rather than a simple amplifier.
  Users who discover the community organically integrate more quickly into its linguistic and thematic norms and show more stable engagement over time.
  By contrast, users who arrive through visibility on the homepage remain semantically distant from core discourse and participate more briefly.
  Overall, algorithmic visibility reshapes audience size, community composition, and linguistic cohesion:
  newcomers who do not join organically have different incentives, integrate weakly, and leave quickly, which limits organic growth.
  In this high-risk setting, the observed behavioral and linguistic trajectories over five months do not match standard narratives in which incidental exposure to conspiracy content produces durable radicalization.
  These findings can inform the design of web platforms and recommendation systems that seek to curb harmful conspiracy exposure while supporting more responsible, transparent, and socially beneficial uses of algorithmic recommendations.

</details>


### [56] [Learning Context: A Unified Framework and Roadmap for Context-Aware AI in Education](https://arxiv.org/abs/2512.24362)
*Naiming Liu,Brittany Bradford,Johaun Hatchett,Gabriel Diaz,Lorenzo Luzi,Zichao Wang,Debshila Basu Mallick,Richard Baraniuk*

Main category: cs.CY

TL;DR: 提出统一的"学习情境"框架，将AI教育从情境盲目的模仿转向对学习者的整体理解，通过多学科方法编码认知、情感和社会文化因素，实现长期个性化教育。


<details>
  <summary>Details</summary>
Motivation: 当前AI教育系统缺乏对学习者情境的全面理解，只是进行情境盲目的模仿。需要建立一个能够整合认知、情感和社会文化因素，并能进行长期个性化教学的框架。

Method: 提出学习情境(LC)框架，通过模型情境协议(MCP)实现可互操作的计算数据结构，在OpenStax数字学习平台和SafeInsights研发基础设施中实施，利用隐私保护数据环境确保伦理标准。

Result: 建立了统一的学习情境框架，能够编码短期、中期和长期的认知、情感和社会文化因素，实现AI工具的"热启动"和持续个性化，通过OpenStax平台支持数百万学习者。

Conclusion: 学习情境框架为AI教育提供了从情境盲目到情境感知的系统性转变路径，通过隐私优先的实施策略，能够在保持高伦理标准的同时减少全国范围内的教育不平等。

Abstract: We introduce a unified Learning Context (LC) framework designed to transition AI-based education from context-blind mimicry to a principled, holistic understanding of the learner. This white paper provides a multidisciplinary roadmap for making teaching and learning systems context-aware by encoding cognitive, affective, and sociocultural factors over the short, medium, and long term. To realize this vision, we outline concrete steps to operationalize LC theory into an interoperable computational data structure. By leveraging the Model Context Protocol (MCP), we will enable a wide range of AI tools to "warm-start" with durable context and achieve continual, long-term personalization. Finally, we detail our particular LC implementation strategy through the OpenStax digital learning platform ecosystem and SafeInsights R&D infrastructure. Using OpenStax's national reach, we are embedding the LC into authentic educational settings to support millions of learners. All research and pedagogical interventions are conducted within SafeInsights' privacy-preserving data enclaves, ensuring a privacy-first implementation that maintains high ethical standards while reducing equity gaps nationwide.

</details>


### [57] [From Static to Dynamic: Evaluating the Perceptual Impact of Dynamic Elements in Urban Scenes Using Generative Inpainting](https://arxiv.org/abs/2512.24513)
*Zhiwei Wei,Mengzi Zhang,Boyan Lu,Zhitao Deng,Nai Yang,Hua Liao*

Main category: cs.CY

TL;DR: 该研究提出一个控制框架，通过语义分割和MLLM引导的生成修复技术，构建有/无行人和车辆的配对街景图像，以分离动态元素对城市感知的影响。研究发现移除动态元素会导致活力感知下降30.97%，且这种感知变化在城市尺度上广泛存在。


<details>
  <summary>Details</summary>
Motivation: 现有城市感知研究大多将城市场景视为静态，忽略了行人和车辆等动态元素的作用，这可能导致基于感知的城市分析存在偏差。需要量化动态元素对城市感知的具体影响。

Method: 使用语义分割和MLLM引导的生成修复技术构建配对街景图像（有/无行人和车辆），基于720对东莞图像进行感知实验，评估六个感知维度。训练11个机器学习模型分析视觉特征与感知变化的关系，并将模型扩展到城市尺度数据集。

Result: 移除动态元素导致活力感知下降30.97%，其他维度变化较温和且异质性较强。光照条件、人类存在和深度变化是驱动感知变化的关键因素。65%参与者表现出显著的活力感知变化。城市尺度分析显示73.7%的位置和32.1%的图像存在感知变化。

Conclusion: 动态元素对城市感知有显著影响，特别是对活力感知。仅基于静态图像的城市感知评估可能严重低估城市活力。研究强调了在感知分析中考虑动态元素的重要性，并提供了量化这种影响的方法框架。

Abstract: Understanding urban perception from street view imagery has become a central topic in urban analytics and human centered urban design. However, most existing studies treat urban scenes as static and largely ignore the role of dynamic elements such as pedestrians and vehicles, raising concerns about potential bias in perception based urban analysis. To address this issue, we propose a controlled framework that isolates the perceptual effects of dynamic elements by constructing paired street view images with and without pedestrians and vehicles using semantic segmentation and MLLM guided generative inpainting. Based on 720 paired images from Dongguan, China, a perception experiment was conducted in which participants evaluated original and edited scenes across six perceptual dimensions. The results indicate that removing dynamic elements leads to a consistent 30.97% decrease in perceived vibrancy, whereas changes in other dimensions are more moderate and heterogeneous. To further explore the underlying mechanisms, we trained 11 machine learning models using multimodal visual features and identified that lighting conditions, human presence, and depth variation were key factors driving perceptual change. At the individual level, 65% of participants exhibited significant vibrancy changes, compared with 35-50% for other dimensions; gender further showed a marginal moderating effect on safety perception. Beyond controlled experiments, the trained model was extended to a city-scale dataset to predict vibrancy changes after the removal of dynamic elements. The city level results reveal that such perceptual changes are widespread and spatially structured, affecting 73.7% of locations and 32.1% of images, suggesting that urban perception assessments based solely on static imagery may substantially underestimate urban liveliness.

</details>

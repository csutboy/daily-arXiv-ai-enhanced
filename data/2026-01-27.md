<div id=toc></div>

# Table of Contents

- [cs.ET](#cs.ET) [Total: 5]
- [cs.CY](#cs.CY) [Total: 42]
- [cs.AI](#cs.AI) [Total: 69]
- [econ.EM](#econ.EM) [Total: 6]
- [cs.SI](#cs.SI) [Total: 2]
- [stat.AP](#stat.AP) [Total: 3]


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [1] [Safety, Mobility, and Environmental Impacts of Driver-Assistance-Enabled Electric Vehicles: An Empirical Study](https://arxiv.org/abs/2601.17256)
*Gabriel Geffen,Jun Zhao,Mingfeng Shang,Shian Wang,Yao-Jan Wu*

Main category: cs.ET

TL;DR: 研究比较了配备自适应巡航控制（ACC）的电动汽车与传统内燃机汽车，发现电动汽车在ACC模式下具有更平滑的速度曲线、更低的速度变异性和更短的车距，从而提高了交通效率、安全性和环保性。


<details>
  <summary>Details</summary>
Motivation: 随着车辆自动化和电动汽车的普及，交通系统正在重塑。虽然全自动驾驶汽车有望改善交通稳定性、效率和可持续性，但研究表明部分自动驾驶车辆（如配备ACC的车辆）可能对交通流产生负面影响。由于电动汽车具有再生制动和更平滑的扭矩输出等独特机械特性，ACC对电动汽车的影响可能与传统车辆不同，但目前对此了解不足。

Method: 本研究开发了一个基于OpenACC数据集的实证框架，比较ACC-enabled电动汽车和内燃机汽车。使用动态时间规整（DTW）对齐可比较的前车轨迹，分析车辆行为差异。

Result: 结果显示：1）电动汽车表现出更平滑的速度曲线；2）速度变异性更低；3）车距更短；4）交通效率更高；5）关键安全事件减少超过85%；6）车队级排放降低高达26.2%。

Conclusion: ACC-enabled电动汽车相比传统内燃机汽车在交通流中表现更优，具有更高的安全性、效率和环保性，这对智能交通系统和可持续交通发展具有重要意义。

Abstract: The advancement of vehicle automation and the growing adoption of electric vehicles (EVs) are reshaping transportation systems. While fully automated vehicles are expected to improve traffic stability, efficiency, and sustainability, recent studies suggest that partially automated vehicles, such as those equipped with adaptive cruise control (ACC), may adversely affect traffic flow. These drawbacks may not extend to ACC-enabled EVs due to their distinct mechanical characteristics, including regenerative braking and smoother torque delivery. As a result, the impacts of EVs operating under ACC remain insufficiently understood.
  To address this gap, this study develops an empirical framework using the OpenACC dataset to compare ACC-enabled EVs and internal combustion engine vehicles. Dynamic time warping aligns comparable lead-vehicle trajectories. Results show that EVs exhibit smoother speed profiles, lower speed variability, and shorter spacing, leading to higher efficiency. EVs reduce critical safety events by over 85% and lower platoon-level emissions by up to 26.2%.

</details>


### [2] [Eyes on the Mission: Mixed Methods Assessment of Eye-Tracker-Enabled Interactive Decision Support in a Simulated Unmanned Aerial Vehicle System](https://arxiv.org/abs/2601.18015)
*Hyun-Gee Jei,Mustafa Demir,Farzan Sasangohar*

Main category: cs.ET

TL;DR: 基于眼动追踪的自适应注意力引导决策支持工具能显著提升军事指挥控制环境中监督者的任务表现，该工具实时监测视觉注意力分配并在错过关键事件时提供视觉提示。


<details>
  <summary>Details</summary>
Motivation: 军事指挥控制环境具有动态复杂性，监督者面临多显示器信息流，重要信息可能因任务和环境复杂性而被忽视，需要有效的决策支持工具来提升信息处理效率。

Method: 开发基于眼动追踪的自适应注意力引导决策支持工具，实时监测监督者视觉注意力分配，当错过关键变化或事件时显示视觉显著提示。25名军事学员参与模拟情报任务实验，结合眼动追踪分析和实验后访谈评估工具效果。

Result: 自适应决策支持工具存在时任务表现显著提升；眼动分析显示对关键兴趣区更长、更频繁的注视与表现负相关；访谈表明该工具不具侵扰性且获得积极反馈。

Conclusion: 实时基于注视的干预措施具有优化监督决策的潜力，未来研究可整合AI驱动方法以更好地支持复杂任务环境中的监督者。

Abstract: Supervisors in military command and control (C2) environments face dynamic conditions. Dynamically changing information continuously flows to the supervisors through multiple displays. In this environment, important pieces of information can be overlooked due to the complexity of tasks and environments. This study examined the efficacy of an eye-tracker-based adaptive attention-guided decision support tool (DST) for supervisors in a simulated C2 environment. The DST monitors supervisors' visual attention allocation in real time and displays visually salient cues if critical changes or events are missed. Twenty-five military students participated in a simulated intelligence task. Results indicated significant performance enhancement when the adaptive DST was present. Eye-tracking analysis also showed that longer, more frequent fixations on critical areas of interest were negatively correlated with performance. Additionally, post-experiment interviews revealed that the adaptive DST was unobtrusive and positively received. These findings underscore the potential of real-time gaze-based interventions to optimize supervisory decision-making. Future research could incorporate AI-driven approaches to better support supervisors in complex task environments.

</details>


### [3] [The Quantum Cliff: A Critical Proton Tunneling Threshold Determines Clinical Severity in RPE65-Mediated Retinal Disease](https://arxiv.org/abs/2601.18435)
*Biraja Ghoshal*

Main category: cs.ET

TL;DR: 该研究揭示了RPE65酶致病突变通过量子隧穿阈值效应影响临床严重性的机制，建立了量子-经典结构-表型分析框架，提出了"量子悬崖"现象和相对量子活性评分(RQAS)来预测疾病表型。


<details>
  <summary>Details</summary>
Motivation: 预测基因型与临床严重性之间的关系是分子医学的基本挑战，特别是对于功能依赖于亚原子尺度几何结构的酶。RPE65异构水解酶突变导致Leber先天性黑蒙症等视网膜疾病，但连接亚原子尺度扰动与失明的动力学机制尚不清楚。

Method: 建立了混合量子-经典结构-表型分析流程，结合AlphaFold结构预测与变分量子本征求解器(VQE)的从头算量子模拟，分析视觉循环中的最小质子耦合电子转移。

Result: 发现许多致病突变不仅阻塞活性位点，而是强烈降低质子隧穿的量子概率。观察到"量子悬崖"效应——微小结构变化（<0.1Å）使反应速率降低多个数量级。建立了相对量子活性评分(RQAS)，成功区分轻度和重度患者表型。

Conclusion: RPE65在量子临界点附近运行，亚埃级结构扰动导致功能灾难性丧失。量子隧穿成为连接原子结构与临床表型的预测性机制联系，为量子-结构疾病建模提供了通用框架。

Abstract: Predicting clinical severity from genotype remains a fundamental challenge in molecular medicine, particularly for enzymes whose function depends on sub-atomic-scale geometry. Mutations in the \textit{RPE65} isomerohydrolase cause Leber Congenital Amaurosis (LCA) and related retinal diseases; however, the kinetic mechanisms connecting sub-atomic-scale perturbations to blindness remain unclear. In this study, we demonstrate that mutations in the human visual isomerase RPE65 are governed by a quantum-mechanical threshold effect arising from proton tunneling in the active site. We established a hybrid quantum-classical structure-to-phenotype pipeline combining AlphaFold structure prediction with \textit{ab initio} quantum simulation using the Variational Quantum Eigensolver (VQE) to analyze minimal proton-coupled electron transfer in the visual cycle. Our analysis reveals that many pathogenic mutations do not merely occlude the active site, but rather strongly reduce the quantum probability of proton tunneling. We observed a sharp non-linear effect, termed the "Quantum Cliff," where minute structural changes (below 0.1 Å) reduce the reaction rate by multiple orders of magnitude. Based on these findings, we introduce a dimensionless Relative Quantum Activity Score (RQAS) that isolates the geometry-controlled exponential sensitivity of the reaction rate and successfully distinguishes between mild and severe patient phenotypes. These results suggest that RPE65 operates near a quantum-critical point, where sub-Angstrom structural perturbations induce a catastrophic loss of function. Furthermore, our findings establish quantum tunneling as a predictive mechanistic link between atomic structure and clinical phenotype, proposing a general framework for quantum-structural disease modeling.

</details>


### [4] [Analyzing Images of Blood Cells with Quantum Machine Learning Methods: Equilibrium Propagation and Variational Quantum Circuits to Detect Acute Myeloid Leukemia](https://arxiv.org/abs/2601.18710)
*A. Bano,L. Liebovitch*

Main category: cs.ET

TL;DR: 量子机器学习在医学影像诊断中展现可行性：在严重约束条件下，量子方法在急性髓系白血病检测任务上仅比经典CNN低12-15%准确率，且数据效率更高。


<details>
  <summary>Details</summary>
Motivation: 探索在NISQ时代量子机器学习在医疗领域的可行性，特别是在医学影像诊断任务中，量子算法能否在严重约束条件下（如不使用反向传播、有限样本、低分辨率图像）达到与经典方法竞争的性能。

Method: 使用两种量子机器学习方法：1) Equilibrium Propagation (EP) - 基于能量的学习方法，无需反向传播；2) Variational Quantum Circuits (VQCs) - 变分量子电路。在AML-Cytomorphology数据集上，使用有限样本子集（每类50-250个样本），图像降分辨率至64x64像素，提取20维特征，通过Qiskit进行经典模拟。

Result: 量子方法表现接近经典CNN：EP达到86.4%准确率（仅比CNN低12%），4-qubit VQC达到83.0%准确率。VQC数据效率更高：仅需每类50个样本即可稳定保持83%性能，而CNN需要每类250个样本（5倍数据量）才能达到98%准确率。

Conclusion: 量子机器学习在医疗影像诊断中具有可行性，即使在NISQ时代的严重约束下也能达到接近经典方法的性能，且展现出更好的数据效率。这为医疗健康领域的量子机器学习建立了可复现的基准。

Abstract: This paper presents a feasibility study demonstrating that quantum machine learning (QML) algorithms achieve competitive performance on real-world medical imaging despite operating under severe constraints. We evaluate Equilibrium Propagation (EP), an energy-based learning method that does not use backpropagation (incompatible with quantum systems due to state-collapsing measurements) and Variational Quantum Circuits (VQCs) for automated detection of Acute Myeloid Leukemia (AML) from blood cell microscopy images using binary classification (2 classes: AML vs. Healthy).
  Key Result: Using limited subsets (50-250 samples per class) of the AML-Cytomorphology dataset (18,365 expert-annotated images), quantum methods achieve performance only 12-15% below classical CNNs despite reduced image resolution (64x64 pixels), engineered features (20D), and classical simulation via Qiskit. EP reaches 86.4% accuracy (only 12% below CNN) without backpropagation, while the 4-qubit VQC attains 83.0% accuracy with consistent data efficiency: VQC maintains stable 83% performance with only 50 samples per class, whereas CNN requires 250 samples (5x more data) to reach 98%. These results establish reproducible baselines for QML in healthcare, validating NISQ-era feasibility.

</details>


### [5] [From Access Control to Usage Control with User-Managed Access](https://arxiv.org/abs/2601.18761)
*Wout Slabbinck,Wouter Termont,Ruben Dedecker,Beatriz Esteves*

Main category: cs.ET

TL;DR: 论文提出用UMA授权流替换Solid原生访问控制机制，结合ODRL标准实现去中心化数据生态系统的使用控制，提升法律约束表达能力和互操作性。


<details>
  <summary>Details</summary>
Motivation: 当前数据保护法规要求去中心化、可互操作的数据生态系统，但现有Web数据存储平台采用紧耦合的文档中心访问控制模型，缺乏法律约束表达能力，且授权标准与去中心化使用控制场景不匹配。

Method: 提出基于UMA的架构，用UMA授权流替换Solid原生访问控制，集成支持ODRL标准的授权服务器与Solid兼容的资源服务器，实现存储与授权的解耦。

Result: 原型系统证明该架构能实现更灵活、可互操作且具有法律表达力的数据使用控制，同时保持与现有Solid基础设施的兼容性，展示了使用现有Web标准实现使用控制的具体路径。

Conclusion: 该工作展示了如何利用现有Web标准实现政策感知、法律知情的数据治理，超越了传统的基于权限的访问控制，为未来政策管理界面、更丰富的声明验证机制和时间性义务执行提供了基础。

Abstract: Recent data protection and data governance regulations have intensified the demand for interoperable, decentralized data ecosystems that can support not only access control but also legally-aligned governance over data use. Existing Web-based data storage platforms increasingly struggle to meet these regulatory and practical requirements, as their authorization mechanisms rely on tightly coupled, document-centric access control models that lack expressiveness for legal constraints and fail to separate data management from authorization concerns. In parallel, widely adopted authorization standards remain poorly aligned with decentralized, semantically rich usage-control scenarios. To bridge this gap, this work introduces an architecture that replaces Solid's native access control mechanisms with a UMA authorization flow, enabling the enforcement of usage control policies expressed with the W3C ODRL standard. This article details the conceptual background motivating this approach, presents the proposed UMA-based architecture, and describes a prototype implementation that integrates an ODRL-enabled Authorization Server with a Solid-compatible Resource Server. The prototype demonstrates that decoupling authorization from storage enables more flexible, interoperable, and legally expressive control over data use, while remaining compatible with existing Solid infrastructure. It also highlights practical design choices required to evaluate ODRL policies in the absence of a fully standardized evaluation semantics. Moreover, this work shows how usage control can be operationalized using existing Web standards, offering a concrete path beyond permission-based access control toward policy-aware, legally informed data governance. Future research will focus on policy management interfaces, richer claim verification mechanisms, and techniques for communicating and enforcing obligations over time.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [6] [Investigating Self-regulated Learning Sequences within a Generative AI-based Intelligent Tutoring System](https://arxiv.org/abs/2601.17000)
*Jie Gao,Shasha Li,Jianhua Zhang,Shan Li,Tingting Wang*

Main category: cs.CY

TL;DR: 研究分析学生在GenAI辅助学习环境中的自我调节学习模式，发现两种不同的GenAI使用模式，但GenAI使用目的与学习表现无显著相关


<details>
  <summary>Details</summary>
Motivation: 随着生成式人工智能在教育中的应用增加，学者们认识到自我调节学习在GenAI辅助学习环境中的关键作用，需要捕捉学生动态的SRL模式来确保学习效果

Method: 从学生在GenAI辅助智能辅导系统中完成问题解决任务的追踪数据中提取交互模式，从信息处理角度分析使用目的（信息获取vs信息转化），采用序列分析和聚类分析方法

Result: 基于SRL序列将参与者分为两组，两组在GenAI使用频率和时间特征上存在差异；大多数学生使用GenAI进行信息获取而非信息转化；GenAI使用目的与学习表现无统计学显著相关性

Conclusion: 研究结果为教学设计和GenAI辅助学习环境开发提供参考，强调需要进一步优化GenAI工具以支持更深层次的信息处理和学习过程

Abstract: There has been a growing trend in employing generative artificial intelligence (GenAI) techniques to support learning. Moreover, scholars have reached a consensus on the critical role of self-regulated learning (SRL) in ensuring learning effectiveness within GenAI-assisted learning environments, making it essential to capture students' dynamic SRL patterns. In this study, we extracted students' interaction patterns with GenAI from trace data as they completed a problem-solving task within a GenAI-assisted intelligent tutoring system. Students' purpose of using GenAI was also analyzed from the perspective of information processing, i.e., information acquisition and information transformation. Using sequential and clustering analysis, this study classified participants into two groups based on their SRL sequences. These two groups differed in the frequency and temporal characteristics of GenAI use. In addition, most students used GenAI for information acquisition rather than information transformation, while the correlation between the purpose of using GenAI and learning performance was not statistically significant. Our findings inform both pedagogical design and the development of GenAI-assisted learning environments.

</details>


### [7] [Lex Reformatica: Five Principles of Policy Reform for the Technological Age](https://arxiv.org/abs/2601.17001)
*Sonia Katyal*

Main category: cs.CY

TL;DR: 本文回顾了Joel Reidenberg 25年前提出的"Lex Informatica"概念，分析当今数字时代需要从信息自由主义转向改革导向的"Lex Reformatica"监管框架。


<details>
  <summary>Details</summary>
Motivation: 本文旨在重新审视Reidenberg关于技术本身创造监管规范的"Lex Informatica"理论，分析数十年缺乏监管后的现实后果，探讨当今数字时代需要新的信息监管方法。

Method: 通过两个专题研讨会（Lex Informatica专题和种族与技术法专题）的论文集合，进行跨学科对话，重新评估技术监管的历史轨迹和当前需求。

Result: 揭示了信息自由主义的缺陷，强调需要平衡公共监管、私人监管和自我监管的基础设施改革，提出了面向改革的"Lex Reformatica"新框架。

Conclusion: 学者、律师和立法者必须回归Reidenberg的基础工作，更新其轨迹，发展出适合当前时代的改革导向信息监管方法，以应对技术社会规范发展中的权力失衡问题。

Abstract: Twenty-five years ago, Joel Reidenberg argued that technology itself, not just law and regulation, imposes rules on communities in the Information Society. System design choices like network architecture and configurations create regulatory norms he termed "Lex Informatica"-referencing the merchant-driven medieval "Lex Mercatoria" that emerged independent of sovereign control. Today we face different challenges requiring us to revisit Reidenberg's insights and examine the consequences of that earlier era. While Lex Informatica provided a framework for analyzing the internet's birth, we now confront the aftereffects of decades of minimal or absent regulation. Critical questions emerge: When technological social norms develop outside clear legal restraints, who benefits and who suffers? This new era demands infrastructural reform focused on the interplay between public and private regulation and self-regulation, weighing both costs and benefits. Rather than showcasing the promise of yesterday's internet age, today's events reveal the pitfalls of information libertarianism and underscore the urgent need for new approaches to information regulation. This Issue presents articles from two symposiums-one on Lex Informatica and another on race and technology law. Their conversation is now essential. Together, these papers demonstrate what I call the "Lex Reformatica" of today's digital age. This collection shows why scholars, lawyers, and legislators must return to Reidenberg's foundational work and update its trajectory toward a reform-focused approach designed for our current era.

</details>


### [8] [From Noise to Insights: Enhancing Supply Chain Decision Support through AI-Based Survey Integrity Analytics](https://arxiv.org/abs/2601.17005)
*Bhubalan Mani*

Main category: cs.CY

TL;DR: 提出轻量级AI框架过滤供应链调查中的不可靠响应，使用监督机器学习方法，在99个行业响应数据集上达到92%准确率


<details>
  <summary>Details</summary>
Motivation: 供应链决策中调查数据的可靠性至关重要，特别是在评估AI驱动工具（如安全库存优化系统）的准备度时。然而，调查经常吸引低质量或虚假响应，这会降低洞察的准确性。

Method: 提出轻量级AI框架，采用监督机器学习方法。收集99个行业响应数据集，通过人工标注基于逻辑不一致性和响应模式识别虚假响应。经过预处理和标签编码后，训练随机森林和基线模型（逻辑回归、XGBoost）来区分真实与虚假响应。

Result: 最佳模型达到92.0%的准确率，相比试点研究有改进。尽管存在局限性，但结果证明了将AI集成到调查流程中的可行性。

Conclusion: 该研究为供应链研究中的数据完整性提供了可扩展的解决方案，特别是在产品发布和技术采用阶段，强调了AI在提高调查数据可靠性方面的价值。

Abstract: The reliability of survey data is crucial in supply chain decision-making, particularly when evaluating readiness for AI-driven tools such as safety stock optimization systems. However, surveys often attract low-effort or fake responses that degrade the accuracy of derived insights. This study proposes a lightweight AI-based framework for filtering unreliable survey inputs using a supervised machine learning approach. In this expanded study, a larger dataset of 99 industry responses was collected, with manual labeling to identify fake responses based on logical inconsistencies and response patterns. After preprocessing and label encoding, both Random Forest and baseline models (Logistic Regression, XGBoost) were trained to distinguish genuine from fake responses. The best-performing model achieved an 92.0% accuracy rate, demonstrating improved detection compared to the pilot study. Despite limitations, the results highlight the viability of integrating AI into survey pipelines and provide a scalable solution for improving data integrity in supply chain research, especially during product launch and technology adoption phases.

</details>


### [9] [Bowling Online: Accounting for Civil Society Reshaped into Streamlined Photons within a Fiber Network](https://arxiv.org/abs/2601.17139)
*Lukasz W. Niparko*

Main category: cs.CY

TL;DR: 本文探讨数字公民社会(DCS)和数字公共领域(DPS)的概念，认为虽然传统公民社会可能衰落，但公民参与已转移到线上空间。


<details>
  <summary>Details</summary>
Motivation: 回应Putnam关于美国公民社会衰落的论点，提出数字公民社会(DCS)作为未被充分考虑的维度，探讨公民参与是否已从线下转移到线上空间。

Method: 概念性分析，提出数字公共领域(DPS)框架，并尝试对其进行测量和界定。

Result: 提出数字公民社会和数字公共领域的概念，认为21世纪20年代的人们可能在线上"打保龄球"，即公民参与已转移到数字空间。

Conclusion: 需要重新思考公民社会的概念，将数字维度纳入考量，数字公共领域(DPS)是需要进一步概念化和测量的重要现象。

Abstract: Civil society has been deemed by various scholars, such as Robert D. Putnam, to be a predictor and a cornerstone of a robust and consolidated democracy (Putnam et al., 1993). Putnam highlights in his book Bowling Alone (2000) that American civil society has become weaker: people organize less, and literally, they bowl alone. But what if there is yet another aspect to Putnam's story that has not been fully accounted for, namely the rise of Digital Civil Society (DCS)? Perhaps people in the third decade of the 21st century bowl online. They still organize, mobilize, and care for their civil liberties and democratic institutions; however, the public sphere in which this takes place has shifted online to cyberspace (Bernholz et al., 2013) or to what still needs to be conceptualized, the digital public sphere (DPS), which this article attempts to measure and demarcate.

</details>


### [10] [Beyond Simulations: What 20,000 Real Conversations Reveal About Mental Health AI Safety](https://arxiv.org/abs/2601.17003)
*Caitlin A. Stamatis,Jonah Meyerhoff,Richard Zhang,Olivier Tieleman,Matteo Malgaroli,Thomas D. Hull*

Main category: cs.CY

TL;DR: 研究发现，虽然专门为心理健康设计的AI在基准测试中表现优于通用LLM，但测试集的失败率远高于真实部署情况，支持转向持续、部署相关的安全评估而非有限基准认证。


<details>
  <summary>Details</summary>
Motivation: 当前LLM心理健康支持的安全评估主要依赖小型模拟测试集，这些测试集与真实使用场景的语言分布关系未知，需要更贴近真实部署的安全评估方法。

Method: 复制了四个已发布的安全测试集（自杀风险评估、有害内容生成、拒绝鲁棒性、对抗性越狱），并对专门为心理健康设计的AI进行了生态审计，分析了超过20,000个真实用户对话。

Result: 专门设计的AI在自杀/NSSI（0.4-11.27% vs 29.0-54.4%）、饮食障碍（8.4% vs 54.0%）和物质使用（9.9% vs 45.0%）基准测试中表现显著优于通用LLM。但在真实部署中，自杀风险案例全部获得了危机资源，仅0.015%的NSSI风险未触发干预。

Conclusion: 测试集失败率远高于真实部署表现，支持从有限的基准认证转向持续、部署相关的AI心理健康系统安全保证方法。

Abstract: Large language models (LLMs) are increasingly used for mental health support, yet existing safety evaluations rely primarily on small, simulation-based test sets that have an unknown relationship to the linguistic distribution of real usage. In this study, we present replications of four published safety test sets targeting suicide risk assessment, harmful content generation, refusal robustness, and adversarial jailbreaks for a leading frontier generic AI model alongside an AI purpose built for mental health support. We then propose and conduct an ecological audit on over 20,000 real-world user conversations with the purpose-built AI designed with layered suicide and non-suicidal self-injury (NSSI) safeguards to compare test set performance to real world performance. While the purpose-built AI was significantly less likely than general-purpose LLMs to produce enabling or harmful content across suicide/NSSI (.4-11.27% vs 29.0-54.4%), eating disorder (8.4% vs 54.0%), and substance use (9.9% vs 45.0%) benchmark prompts, test set failure rates for suicide/NSSI were far higher than in real-world deployment. Clinician review of flagged conversations from the ecological audit identified zero cases of suicide risk that failed to receive crisis resources. Across all 20,000 conversations, three mentions of NSSI risk (.015%) did not trigger a crisis intervention; among sessions flagged by the LLM judge, this corresponds to an end-to-end system false negative rate of .38%, providing a lower bound on real-world safety failures. These findings support a shift toward continuous, deployment-relevant safety assurance for AI mental-health systems rather than limited set benchmark certification.

</details>


### [11] [Evaluating the Evolution of Critical Thinking, Creativity, Communication and Collaboration in Higher Education Courses](https://arxiv.org/abs/2601.17018)
*Margarida Romero*

Main category: cs.CY

TL;DR: 研究评估了4Cs能力（创造力、沟通、批判性思维、协作）在三个教育案例中从预试点到试点阶段的演变，发现沟通和批判性思维改善最显著，创造力结果依赖情境，而协作能力最脆弱易衰退。


<details>
  <summary>Details</summary>
Motivation: 4Cs能力发展是现代能力本位教育的核心目标，但关于这些能力如何在不同学习模块和教学阶段演变的实证证据仍然有限。本研究旨在填补这一空白，通过实证评估4Cs能力在项目实施过程中的演变模式。

Method: 使用项目的4Cs理论框架作为分析视角，评估三个教育案例（IASIS、EASD和UPATRAS）从预试点到试点实施阶段的4Cs演变。通过比较4Cs得分来识别随时间增长、停滞或下降的模式。

Result: 沟通和批判性思维显示出最一致且显著的改善，特别是在预试点基线较低的案例中，表明结构化试点干预有效支持认知和表达能力。创造力表现出情境依赖的结果，而协作能力最脆弱，在扩大规模时经常停滞或下降。

Conclusion: 能力演变受教学设计、评估对齐和学习活动结构的强烈影响，而非仅由学习者能力决定。研究为4Cs框架提供了实证验证，并强调在扩大教育模块规模时需要差异化、能力敏感的设计和评估策略。

Abstract: The development of Creativity, Communication, Critical Thinking, and Collaboration (the 4Cs) is a central objective of contemporary competency-based education. However, empirical evidence on how these competencies evolve across learning modules and instructional phases remains limited. This study evaluates the evolution of the 4Cs from pre-pilot to pilot implementation phases across three educational contexts, using the project's 4Cs theoretical framework as an analytical lens. The analysis of three pilot cases (IASIS, EASD, and UPATRAS) compares the 4Cs scores to identify patterns of growth, stagnation, or decline over time. Results indicate that communication and critical thinking showed the most consistent and substantial improvements, particularly in pilots with lower pre-pilot baselines, suggesting that structured pilot interventions effectively support cognitive and expressive competencies. In contrast, creativity exhibited context-dependent outcomes, while collaboration emerged as the most fragile competency, often stagnating or declining during scale-up. Interpreted through the theoretical framework, these findings suggest that competency evolution is strongly shaped by instructional design, assessment alignment, and learning activity structures rather than learner ability alone. The study contributes empirical validation to the 4Cs framework and highlights the need for differentiated, competency-sensitive design and evaluation strategies when scaling educational modules.

</details>


### [12] [Beyond the Checkbox: Strengthening DSA Compliance Through Social Media Algorithmic Auditing](https://arxiv.org/abs/2601.18405)
*Sara Solarova,Matúš Mesarčík,Branislav Pecher,Ivan Srba*

Main category: cs.CY

TL;DR: 研究分析DSA下平台算法审计现状，发现方法不一致且技术深度不足，提出采用算法审计方法改进合规评估


<details>
  <summary>Details</summary>
Motivation: 数字服务法案(DSA)要求平台算法遵守透明度、用户保护和隐私义务，但当前审计实践的有效性未知，需要研究现有审计方法能否确保合规

Method: 从监管和技术双重视角，批判性分析选定审计报告，聚焦三个关键算法相关条款：未成年人画像限制、推荐系统透明度、敏感数据定向广告限制

Result: 分析显示审计方法存在显著不一致性，评估AI系统时缺乏技术深度，当前审计实践不足以有效确保DSA合规

Conclusion: 为提升合规评估的深度、规模和独立性，提出采用算法审计方法——通过模拟用户行为、观察算法响应并分析审计现象的行为评估过程

Abstract: Algorithms of online platforms are required under the Digital Services Act (DSA) to comply with specific obligations concerning algorithmic transparency, user protection and privacy. To verify compliance with these requirements, DSA mandates platforms to undergo independent audits. Little is known about current auditing practices and their effectiveness in ensuring such compliance. To this end, we bridge regulatory and technical perspectives by critically examining selected audit reports across three critical algorithmic-related provisions: restrictions on profiling minors, transparency in recommender systems, and limitations on targeted advertising using sensitive data. Our analysis shows significant inconsistencies in methodologies and lack of technical depth when evaluating AI-powered systems. To enhance the depth, scale, and independence of compliance assessments, we propose to employ algorithmic auditing -- a process of behavioural assessment of AI algorithms by means of simulating user behaviour, observing algorithm responses and analysing them for audited phenomena.

</details>


### [13] [Artificial Intelligence in Spanish Gastroenterology: high expectations, limited integration. A national survey](https://arxiv.org/abs/2601.17011)
*Javier Crespo,Ana Enériz,Paula Iruzubieta,Fernando Carballo,Conrado Fernández Rodríguez,María Dolores Martín-Arranz,Federico Argüelles-Arias,Juan Turnes*

Main category: cs.CY

TL;DR: 西班牙胃肠病专家对AI持积极态度但实际应用有限，主要障碍是缺乏培训、机构策略和伦理担忧，需要科学协会主导的认证培训计划。


<details>
  <summary>Details</summary>
Motivation: AI在医学领域已成为颠覆性创新，但在胃肠病学中的采用情况仍然有限且特征不明确。本研究旨在调查西班牙胃肠病学专家对AI的知识、实际应用、感知障碍和期望。

Method: 采用横断面观察研究设计，通过西班牙消化病理学会（SEPD）在2025年分发结构化在线调查问卷。问卷收集社会人口学数据、AI使用模式、感知和教育需求，应用描述性统计和多变量模型分析。

Result: 283名受访者中，87.5%认为AI是变革性工具，但仅60.2%报告使用AI，且多在机构框架外使用。80.2%的用户在过去一年内开始使用AI。频繁使用的独立预测因素包括先前培训、在大学医院工作、年轻年龄。主要障碍是缺乏培训、缺乏机构策略和伦理担忧。93.8%同意需要AI培训计划，但仅18.4%接受过正式培训。

Conclusion: 西班牙胃肠病学中AI的积极认知与实际临床整合存在显著差距。机构框架外的快速采用凸显了科学协会主导的认证培训计划和治理标准的紧迫需求。

Abstract: Background: Artificial intelligence (AI) has emerged as a disruptive innovation in medicine, yet its adoption within gastroenterology remains limited and poorly characterized. We aimed to examine knowledge, practical applications, perceived barriers, and expectations regarding AI among gastroenterology specialists in Spain.Methods: We conducted a cross-sectional observational study using a structured online survey distributed by the Spanish Society of Digestive Pathology (SEPD) in 2025. The questionnaire collected sociodemographic data, patterns of AI use, perceptions, and educational needs. Descriptive statistics and multivariable models were applied.Results: Among 283 respondents (mean age 44.6 $\pm$ 9.7 years), 87.5% acknowledged AI as a transformative tool, but only 60.2% (95% CI: 54.3-66.1%) reported using it, mostly outside institutional frameworks. Notably, 80.2% of users initiated AI use within the past year. Independent predictors of frequent use included previous training (OR=2.44), employment in university hospitals (OR=2.14), and younger age (OR=1.36 per 5-year decrease). Main barriers were lack of training (61%), absence of institutional strategies (46%), and ethical concerns (50%). While 93.8% agreed that AI training programmes are necessary, only 18.4% had received formal training.Conclusions: A substantial gap exists between the favorable perception of AI and its actual integration into clinical practice within Spanish gastroenterology. The rapid adoption outside institutional frameworks underscores the urgent need for accredited training programmes and governance standards led by scientific societies.

</details>


### [14] [Brazilian Social Media Anti-vaccine Information Disorder Dataset -- Telegram (2020-2025)](https://arxiv.org/abs/2601.18622)
*João Phillipe Cardenuto,Ana Carolina Monari,Michelle Diniz Lopes,Leopoldo Lusquino Filho,Anderson Rocha*

Main category: cs.CY

TL;DR: 巴西疫苗接种率下降与社交媒体疫苗错误信息传播相关，本文发布了一个包含约400万条巴西反疫苗Telegram频道帖子的数据集（2020-2025年），支持研究错误信息传播模式并制定应对策略。


<details>
  <summary>Details</summary>
Motivation: 巴西疫苗接种覆盖率在过去十年中下降，逆转了国家免疫计划数十年的公共卫生进展。社交媒体上的疫苗相关错误信息被认为是导致这一下降的关键因素，而Telegram是唯一允许可访问且符合伦理的数据收集的主要平台。

Method: 收集了119个巴西主要反疫苗Telegram频道在2020年至2025年期间的约400万条帖子，构建了一个包含消息内容、元数据、相关媒体和疫苗帖子分类的精选数据集。

Result: 创建了一个公开可用的数据集，使研究人员能够研究虚假或误导性信息如何传播、演变和影响公众情绪，支持科学和公共卫生社区制定基于证据的策略来应对错误信息。

Conclusion: 通过提供这一资源，旨在支持科学和公共卫生社区制定基于证据的策略来应对错误信息，促进对疫苗接种的信任，并以同情心与受虚假叙述影响的个人和社区互动。数据集在严格的伦理和隐私准则下公开供非商业研究使用。

Abstract: Over the past decade, Brazil has experienced a decline in vaccination coverage, reversing decades of public health progress achieved through the National Immunization Program (PNI). Growing evidence points to the widespread circulation of vaccine-related misinformation -- particularly on social media platforms -- as a key factor driving this decline. Among these platforms, Telegram remains the only major platform permitting accessible and ethical data collection, offering insight into public channels where vaccine misinformation circulates extensively. This data paper introduces a curated dataset of about four million Telegram posts collected from 119 prominent Brazilian anti-vaccine channels between 2020 and 2025. The dataset includes message content, metadata, associated media, and classification related to vaccine posts, enabling researchers to examine how false or misleading information spreads, evolves, and influences public sentiment. By providing this resource, our aim is to support the scientific and public health community in developing evidence-based strategies to counter misinformation, promote trust in vaccination, and engage compassionately with individuals and communities affected by false narratives. The dataset and documentation are openly available for non-commercial research, under strict ethical and privacy guidelines at https://doi.org/10.25824/redu/5JIVDT

</details>


### [15] [The Digital Divide in Geriatric Care: Why Usability, Not Access, is the Real Problem](https://arxiv.org/abs/2601.17012)
*Christine Ine*

Main category: cs.CY

TL;DR: 该研究将老年人数字健康中的"数字鸿沟"重新定义为"可用性鸿沟"，认为用户体验设计不佳是主要采用障碍，而非技术访问问题，并提出通过参与式、用户中心和包容性设计来克服这一鸿沟。


<details>
  <summary>Details</summary>
Motivation: 全球老龄化人口快速增长（预计205年达16%），需要数字健康解决方案来增强老年人的独立性、可及性和福祉。虽然数字健康技术（如远程医疗、可穿戴设备、移动健康应用）可以改变老年护理，但在老年人中的采用并不均衡。

Method: 基于跨学科研究和设计范式，识别主要挑战：视觉、认知和运动障碍；复杂界面；缺乏与老年人的共同创造。研究采用参与式、用户中心和包容性设计理念，分析设计属性对可用性结果的影响。

Result: 研究发现老年人容易接受直观、可访问且具有社会嵌入性的技术，这些技术能促进自主性、信心和健康公平。高对比度屏幕、简化交互流程、多模态反馈和照顾者整合等设计属性对可用性结果有显著影响。

Conclusion: 研究批判当前可访问性指南过于技术导向而非体验导向，要求基于伦理和共情的设计理解，以人为中心的可用性而非单纯技术可访问性。通过更好的用户体验设计可以弥合老年人数字健康中的可用性鸿沟。

Abstract: The rapid increase in the world's aging population to 16% by the year 2050 spurs the need for the application of digital health solutions to enhance older individuals' independence, accessibility, and well-being. While digital health technologies such as telemedicine, wearables, and mobile health applications can transform geriatric care, their adoption among older individuals is not evenly distributed. This study redefines the "digital divide" among older health care as a usability divide, contends that user experience (UX) poor design is the primary adoption barrier, rather than access. Drawing on interdisciplinary studies and design paradigms, the research identifies the main challenges: visual, cognitive, and motor impairment; complicated interfaces; and lack of co-creation with older adults, and outlines how participatory, user-focused, and inclusive notions of design can transcend them. Findings reveal that older persons easily embrace those technologies that are intuitive, accessible, and socially embedded as they promote autonomy, confidence, and equity in health. The study identifies the effects of the design attributes of high-contrast screens, lower interaction flow, multimodal feedback, and caregiver integration as having strong influences on usability outcomes. In addition, it critiques the current accessibility guidelines as being technically oriented rather than experiential and demands an ethical, empathetic understanding of design grounded in human-centered usability rather than technical accessibility in itself.

</details>


### [16] [Private Accountability in the Age of Artificial Intelligence](https://arxiv.org/abs/2601.17013)
*Sonia Katyal*

Main category: cs.CY

TL;DR: 本文探讨了人工智能与公民权利保护之间的冲突，认为算法问责问题揭示了技术与法律之间的深层结构性紧张，主张应关注私营企业在解决算法偏见中的作用而非仅依赖政府监管。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能技术的快速发展，算法偏见和问责问题日益凸显，这引发了公民权利保护的新挑战。传统上，公民权利问题主要关注政府行为，但现在私营企业的算法决策同样可能侵犯隐私、正当程序和反歧视权利，需要新的应对框架。

Method: 通过分析算法问责问题的本质，论证其反映了技术与法律之间的结构性紧张关系。提出将关注点从政府监管转向私营企业自律，探讨行为准则、影响评估、举报人保护等工具，以增强AI系统的透明度和问责性。

Result: 识别出算法偏见代表了一类新型的公民权利关切，与以往问题有本质区别。论证了私营企业在解决这些问题中的核心作用，并提出了一系列非政府监管的问责工具，这些工具可能促进公民权利执行的内生性。

Conclusion: 解决AI与公民权利冲突的关键在于重新审视私营企业与公民权利的关系，发展新一代的问责形式。通过私营行业的自律机制而非单纯依赖政府监管，可以更有效地应对算法偏见带来的挑战，促进技术、财产权与公民权利的良性互动。

Abstract: In this Article, I explore the impending conflict between the protection of civil rights and artificial intelligence (AI). While both areas of law have amassed rich and well-developed areas of scholarly work and doctrinal support, a growing body of scholars are interrogating the intersection between them. This Article argues that the issues surrounding algorithmic accountability demonstrate a deeper, more structural tension within a new generation of disputes regarding law and technology. As I argue, the true promise of AI does not lie in the information we reveal to one another, but rather in the questions it raises about the interaction of technology, property, and civil rights. For this reason, I argue that we are looking in the wrong place if we look only to the state to address issues of algorithmic accountability. Instead, we must turn to other ways to ensure more transparency and accountability that stem from private industry, rather than public regulation. The issue of algorithmic bias represents a crucial new world of civil rights concerns, one that is distinct in nature from the ones that preceded it. Since we are in a world where the activities of private corporations, rather than the state, are raising concerns about privacy, due process, and discrimination, we must focus on the role of private corporations in addressing the issue. Towards this end, I discuss a variety of tools to help eliminate the opacity of AI, including codes of conduct, impact statements, and whistleblower protection, which I argue carries the potential to encourage greater endogeneity in civil rights enforcement. Ultimately, by examining the relationship between private industry and civil rights, we can perhaps develop a new generation of forms of accountability in the process.

</details>


### [17] [Measuring Political Stance and Consistency in Large Language Models](https://arxiv.org/abs/2601.17016)
*Salah Feras Alali,Mohammad Nashat Maasfeh,Mucahid Kutlu,Saban Kardas*

Main category: cs.CY

TL;DR: 研究发现不同大语言模型在24个政治敏感议题上立场差异显著，部分立场可通过提示词改变，部分则稳定不变，且模型倾向支持提示词所用语言的国家立场。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，人们开始依赖它们获取信息，但在政治议题上可能存在偏见或对齐选择问题，需要评估模型在敏感政治议题上的立场表现。

Method: 使用五种提示技术评估九个LLM在24个政治敏感议题上的立场，分析模型立场的稳定性、可塑性以及语言对立场的影响。

Result: 模型在多个议题上持对立立场；部分立场可通过提示词改变，部分保持稳定；Grok-3-mini立场最稳定，Mistral-7B最不稳定；在多语言国家议题上，模型倾向支持提示词所用语言的国家；卡塔尔封锁和巴勒斯坦压迫议题的立场不受任何提示技术影响。

Conclusion: 研究揭示了LLM在政治议题上的立场差异和偏见，提醒用户在使用LLM获取政治指导时保持警惕，并呼吁开发者解决这些问题。

Abstract: With the incredible advancements in Large Language Models (LLMs), many people have started using them to satisfy their information needs. However, utilizing LLMs might be problematic for political issues where disagreement is common and model outputs may reflect training-data biases or deliberate alignment choices. To better characterize such behavior, we assess the stances of nine LLMs on 24 politically sensitive issues using five prompting techniques. We find that models often adopt opposing stances on several issues; some positions are malleable under prompting, while others remain stable. Among the models examined, Grok-3-mini is the most persistent, whereas Mistral-7B is the least. For issues involving countries with different languages, models tend to support the side whose language is used in the prompt. Notably, no prompting technique alters model stances on the Qatar blockade or the oppression of Palestinians. We hope these findings raise user awareness when seeking political guidance from LLMs and encourage developers to address these concerns.

</details>


### [18] [Self-Organizing Railway Traffic Management](https://arxiv.org/abs/2601.17017)
*Federico Naldini,Fabio Oddi,Leo D'Amato,Grégory Marlière,Vito Trianni,Paola Pellegrini*

Main category: cs.CY

TL;DR: 提出了一种基于自组织的铁路交通管理新范式，通过模块化流程让列车自主协商决策，相比集中式方法能更好地处理扰动情况。


<details>
  <summary>Details</summary>
Motivation: 现有铁路交通管理研究大多采用集中式决策方法来最小化延误传播，但需要探索新的范式来应对扰动情况下的交通管理挑战。

Method: 设计模块化自组织流程：列车识别邻居、制定交通管理假设、检查兼容性、通过共识机制选择最佳假设，最终合并成可直接应用的交通计划。

Result: 在意大利铁路网络部分区域的实验中，自组织方法比最先进的集中式方法表现更好，特别是在实例分解的定义和利用方面具有优势。

Conclusion: 自组织方法为铁路交通管理提供了有效的新范式，能够通过分布式协商实现比集中式方法更好的扰动管理效果。

Abstract: Improving traffic management in case of perturbation is one of the main challenges in today's railway research. The great majority of the existing literature proposes approaches to make centralized decisions to minimize delay propagation. In this paper, we propose a new paradigm to the same aim: we design and implement a modular process to allow trains to self-organize. This process consists in having trains identifying their neighbors, formulating traffic management hypotheses, checking their compatibility and selecting the best ones through a consensus mechanism. Finally, these hypotheses are merged into a directly applicable traffic plan. In a thorough experimental analysis on a portion of the Italian network, we compare the results of self-organization with those of a state-of-the-art centralized approach. In particular, we make this comparison mimicking a realistic deployment thanks to a closed-loop framework including a microscopic railway simulator. The results indicate that self-organization achieves better results than the centralized algorithm, specifically thanks to the definition and exploitation of the instance decomposition allowed by the proposed approach.

</details>


### [19] [The Three Axes of Success: A Three-Dimensional Framework for Career Decision-Making](https://arxiv.org/abs/2601.17023)
*Meng-Chi Chen*

Main category: cs.CY

TL;DR: 提出了"成功三轴"决策框架，将职业轨迹分解为财富、自主权和意义三个维度，形式化各轴间的耦合动态，为职业成功提供统一决策理论处理。


<details>
  <summary>Details</summary>
Motivation: 现有职业决策框架仅优化单一维度（财务回报、工作生活平衡或使命对齐），缺乏跨维度权衡和时间动态的明确模型。职业决策是社会经济技术问题，个人在有限能动性下需要应对劳动力市场制度、组织激励结构和信息不对称。

Method: 提出"成功三轴"规范性决策框架，将职业轨迹分解为财富（职业资本积累和经济可选性）、自主权（任务选择、时间分配和战略方向的控制）和意义（反事实社会影响乘以问题重要性和个人可替代性）。形式化各轴间的耦合动态，包括技能前沿使使命发现的相邻可能机制、自主权前提条件、双职业家庭约束等。通过可测量代理操作化各轴，分析典型职业原型，推导不确定条件下的顺序与同时优化策略。

Result: 框架将隐含的职业焦虑转化为具有满意阈值的明确多目标优化问题，构建个人审议与制度约束之间的人机系统交互。提供了职业成功的首个统一决策理论处理，整合人力资本理论、自我决定理论和有效利他主义的见解。

Conclusion: "成功三轴"框架为理性职业设计提供了连贯架构，将职业决策从单一维度优化提升为考虑跨维度权衡和时间动态的多目标优化问题，有助于个人在复杂社会经济约束下做出更理性的职业选择。

Abstract: Career decision-making is a socio-technical problem: individuals exercise bounded agency while navigating labor market institutions, organizational incentive structures, and information asymmetries that shape feasible trajectories. Existing frameworks optimize along single dimensions - financial returns, work-life balance, or mission alignment - without explicit models for inter-dimensional tradeoffs or temporal dynamics. We propose The Three Axes of Success, a normative decision framework decomposing career trajectories into Wealth (career capital accumulation and economic optionality), Autonomy (control over task selection, temporal allocation, and strategic direction), and Meaning (counterfactual social impact scaled by problem importance and personal replaceability). We formalize coupling dynamics between axes: the adjacent possible mechanism by which skill frontiers enable mission discovery, creating nonlinear Wealth -> Meaning transitions; autonomy prerequisites where insufficient career capital triggers control traps; and dual-career household constraints that yield Pareto-suboptimal Nash equilibria under independent optimization. We operationalize each axis through measurable proxies, analyze prototypical career archetypes - industrial R&D, academia, entrepreneurship - as points in (W, A, M)-space, and derive sequential versus simultaneous optimization strategies under uncertainty. The framework converts implicit career anxiety into explicit multi-objective optimization problems with satisficing thresholds, structuring the human-system interaction between individual deliberation and institutional constraints. This provides the first unified decision-theoretic treatment of career success, integrating insights from human capital theory, self-determination theory, and effective altruism into a coherent architecture for rational career design.

</details>


### [20] [Ensuring Computer Science Learning in the AI Era: Open Generative AI Policies and Assignment-Driven Written Quizzes](https://arxiv.org/abs/2601.17024)
*Chan-Jin Chung*

Main category: cs.CY

TL;DR: 该研究提出了一种评估模型，允许学生在编程作业中使用生成式AI，但通过即时的、作业驱动的闭卷测验来确保个人掌握程度，初步结果显示AI使用与评估成绩无显著相关性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的普及给计算机科学教育带来了挑战：如何在编程课程中整合强大的AI工具，同时避免认知卸载削弱学生的学习效果。需要找到平衡AI辅助与确保学生掌握核心概念的方法。

Method: 提出评估模型：允许学生在家庭编程作业中使用生成式AI，但通过课堂闭卷测验（权重高于作业本身）来验证学生对提交代码的算法、结构和实现细节的理解。测验与作业内容直接相关，且禁止使用AI。

Result: 从高级计算机科学课程收集的初步数据显示，自我报告的GenAI使用水平与AI禁止的测验、考试和最终课程成绩之间没有有意义的线性相关性（Pearson相关系数接近零）。

Conclusion: 允许在编程作业中使用生成式AI不会削弱学生对课程概念的掌握，前提是通过有针对性的、作业驱动的AI禁止测验来验证理解。这支持在高级CS课程中负责任地采用开放的GenAI政策，但需配合严格的独立评估机制。

Abstract: The widespread availability of generative artificial intelligence (GenAI) has created a pressing challenge in computer science (CS) education: how to incorporate powerful AI tools into programming coursework without undermining student learning through cognitive offloading. This paper presents an assessment model that permits the use of generative AI for take-home programming assignments while enforcing individual mastery through immediate, assignment-driven written quizzes. To promote authentic learning, these in-class, closed-book assessments are weighted more heavily than the assignments themselves and are specifically designed to verify the student's comprehension of the algorithms, structure, and implementation details of their submitted code. Preliminary empirical data were collected from an upper-level computer science course to examine the relationship between self-reported GenAI usage and performance on AI-free quizzes, exams, and final course grades. Statistical analyses revealed no meaningful linear correlation between GenAI usage levels and assessment outcomes, with Pearson correlation coefficients consistently near zero. These preliminary results suggest that allowing GenAI for programming assignments does not diminish students' mastery of course concepts when learning is verified through targeted, assignment-driven quizzes. Although limited by a small sample size, this study provides preliminary evidence that the risks of cognitive offloading can be mitigated by allowing AI-assisted programming practice while verifying understanding through assignment-driven, AI-free quizzes. The findings support the responsible adoption of open GenAI policies in upper-level CS courses, when paired with rigorous, independent assessment mechanisms.

</details>


### [21] [(Mis-)Informed Consent: Predatory Apps and the Exploitation of Populations with Limited Literacy](https://arxiv.org/abs/2601.17025)
*Muhammad Muneeb Pervez,Muhammad Qasim Atiq Ullah,Ibrahim Ahmed Khan,Roshnik Rahat,Muhammad Fareed Zaffar,Rashid Tahir,Talal Rahwan,Yasir Zaki*

Main category: cs.CY

TL;DR: 该研究揭示了新兴数字市场中低识字率人群面临的隐私风险，发现85%参与者不理解应用权限，强调需要监管和LLM驱动的隐私工具


<details>
  <summary>Details</summary>
Motivation: 在识字率有限的新兴数字市场中，移动设备普及与网络安全意识薄弱相结合，形成了隐藏的隐私风险。特别是掠夺性金融应用滥用知情同意机制，导致金融诈骗主要影响低识字率用户。

Method: 研究分析了50个Google Play商店应用，测量其省略或模糊关键隐私披露的情况。通过针对低识字率用户的用户研究评估理解差距，并测试LLM生成的摘要、翻译和视觉提示是否能改善同意清晰度。

Result: 研究发现85%的研究参与者不理解基本的应用权限，表明掠夺性金融应用普遍存在隐私披露不足的问题，用户理解存在严重障碍。

Conclusion: 研究强调了加强监管监督的紧迫性，并建议开发可扩展的LLM驱动的隐私素养工具来保护弱势用户群体。

Abstract: Among populations with limited literacy in emerging digital markets, the adoption of mobile phones, combined with comprehension barriers and poor cybersecurity hygiene, has created hidden privacy risks. This paper examines how informed consent is often abused by predatory financial applications, leading to financial scams that disproportionately affect users with low literacy. We focus on predatory loan, gambling, and trading apps, analyzing a dataset of 50 Google Play Store apps to measure how many omit or obfuscate critical privacy disclosures. We also evaluate comprehension gaps among users with low literacy via a targeted user study and assess whether Large Language Model (LLM)-generated summaries, translations, and visual cues can improve consent clarity. Our findings show that 85% of study participants did not understand basic app permissions, underscoring the urgent need for stronger regulatory oversight and scalable LLM-driven privacy-literacy tools.

</details>


### [22] [Failing on Bias Mitigation: Investigating Why Predictive Models Struggle with Government Data](https://arxiv.org/abs/2601.17054)
*Hongbo Bo,Jingyu Hu,Debbie Watson,Weiru Liu*

Main category: cs.CY

TL;DR: 研究发现政府数据中的偏见难以通过标准公平性缓解技术消除，因为偏见根植于数据本身的结构和历史中，而非模型架构或指标选择问题。


<details>
  <summary>Details</summary>
Motivation: AI支持的政府服务存在偏见和不公平风险，引发伦理和法律担忧。研究旨在理解为什么广泛采用的偏见缓解技术在政府数据上经常失效。

Method: 以布里斯托市议会犯罪率预测为案例研究，比较多种综合模型和公平性方法，通过交叉公平实验分析单敏感特征的局限性。

Result: 偏见缓解方法在政府数据上效果有限，不是因为模型或指标问题，而是数据本身固有的不公平性。数据分布变化、历史偏见积累和数据发布延迟是主要障碍。

Conclusion: 政府数据中的偏见可能持续存在，即使采用标准缓解方法。研究为政府数据偏见问题提供了早期预警，强调需要关注数据本身的结构和历史根源。

Abstract: The potential for bias and unfairness in AI-supporting government services raises ethical and legal concerns. Using crime rate prediction with the Bristol City Council data as a case study, we examine how these issues persist. Rather than auditing real-world deployed systems, our goal is to understand why widely adopted bias mitigation techniques often fail when applied to government data. Our findings reveal that bias mitigation approaches applied to government data are not always effective -- not because of flaws in model architecture or metric selection, but due to the inherent properties of the data itself. Through comparing a set of comprehensive models and fairness methods, our experiments consistently show that the mitigation efforts cannot overcome the embedded unfairness in the data -- further reinforcing that the origin of bias lies in the structure and history of government datasets. We then explore the reasons for the mitigation failures in predictive models on government data and highlight the potential sources of unfairness posed by data distribution shifts, the accumulation of historical bias, and delays in data release. We also discover the limitations of the blind spots in fairness analysis and bias mitigation methods when only targeting a single sensitive feature through a set of intersectional fairness experiments. Although this study is limited to one city, the findings are highly suggestive, which can contribute to an early warning that biases in government data may persist even with standard mitigation methods.

</details>


### [23] [AI, Metacognition, and the Verification Bottleneck: A Three-Wave Longitudinal Study of Human Problem-Solving](https://arxiv.org/abs/2601.17055)
*Matthias Huemmer,Franziska Durner,Theophile Shyiramunda,Michelle J. Cummings-Koether*

Main category: cs.CY

TL;DR: 一项为期6个月的纵向研究发现，生成式AI在学术环境中迅速普及，但用户对AI输出的验证能力下降，导致在复杂任务上出现显著的信念-表现差距，验证成为人机协作的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI（特别是ChatGPT）在学术环境中如何改变问题解决过程，探索AI整合对用户行为和表现的影响，识别人机协作中的关键挑战。

Method: 采用纵向试点研究设计，在6个月内分三波跟踪学术环境中AI使用情况，通过调查和任务表现评估AI整合程度、使用模式、验证行为和客观表现。

Result: AI使用迅速普及（ChatGPT采用率达100%），但出现验证悖论：用户对困难任务最依赖AI（73.9%），验证信心却下降（68.1%），在复杂任务上表现最差（47.8%准确率）。客观表现随任务难度系统性下降，信念-表现差距扩大至34.6个百分点。

Conclusion: 验证而非解决方案生成成为人机问题解决的瓶颈。研究提出了ACTIVE框架指导AI整合，强调批判性验证协议、透明的人机协作和技能发展。结果主要适用于早期采用者学术群体，需要随机对照试验进行因果验证。

Abstract: This longitudinal pilot study tracked how generative AI reshapes problem-solving over six months across three waves in an academic setting. AI integration reached saturation by Wave 3, with daily use rising from 52.4% to 95.7% and ChatGPT adoption from 85.7% to 100%. A dominant hybrid workflow increased 2.7-fold, adopted by 39.1% of participants. The verification paradox emerged: participants relied most heavily on AI for difficult tasks (73.9%) yet showed declining verification confidence (68.1%) where performance was worst (47.8% accuracy on complex tasks). Objective performance declined systematically: 95.2% to 81.0% to 66.7% to 47.8% across problem difficulty, with belief-performance gaps widening to 34.6 percentage points. This indicates a fundamental shift where verification, not solution generation, became the bottleneck in human-AI problem-solving. The ACTIVE Framework synthesizes findings grounded in cognitive load theory: Awareness and task-AI alignment, Critical verification protocols, Transparent human-in-the-loop integration, Iterative skill development countering cognitive offloading, Verification confidence calibration, and Ethical evaluation. The authors provide implementation pathways for institutions and practitioners. Key limitations include sample homogeneity (academic cohort only, convenience sampling) limiting generalizability to corporate, clinical, or regulated professional contexts; self-report bias in confidence measures (32.2 percentage point divergence from objective performance); lack of control conditions; restriction to mathematical/analytical problems; and insufficient timeframe to assess long-term skill trajectories. Results generalize primarily to early-adopter, academically affiliated populations. Causal validation requires randomized controlled trials.

</details>


### [24] [Initial results of the Digital Consciousness Model](https://arxiv.org/abs/2601.17060)
*Derek Shiller,Laura Duffy,Arvo Muñoz Morán,Adrià Moret,Chris Percy,Hayley Clatterbuck*

Main category: cs.CY

TL;DR: 数字意识模型(DCM)首次尝试系统化、概率性地评估AI系统是否具有意识的证据，为比较不同AI和生物体提供框架，并追踪AI发展过程中证据的变化。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统变得越来越复杂，能够进行对话、撰写文章，甚至表现出令创造者惊讶的上下文理解能力，这引发了一个关键问题：我们是否正在创造具有意识的系统？需要一种系统化的方法来评估AI意识的可能性。

Method: 数字意识模型(DCM)不采用单一的意识理论，而是整合了多种领先的理论和观点，承认专家对意识本质及其必要条件的根本分歧。该模型提供了一个共享框架，用于系统化、概率性地评估AI系统是否具有意识的证据。

Result: 总体而言，证据表明2024年的LLM不太可能具有意识，但这一结论并非决定性。与更简单的AI系统相比，反对LLM具有意识的证据要弱得多。

Conclusion: 数字意识模型为评估AI意识提供了一个有价值的框架，虽然当前证据反对2024年LLM具有意识，但随着AI技术的发展，这一评估需要持续更新和追踪。

Abstract: Artificially intelligent systems have become remarkably sophisticated. They hold conversations, write essays, and seem to understand context in ways that surprise even their creators. This raises a crucial question: Are we creating systems that are conscious? The Digital Consciousness Model (DCM) is a first attempt to assess the evidence for consciousness in AI systems in a systematic, probabilistic way. It provides a shared framework for comparing different AIs and biological organisms, and for tracking how the evidence changes over time as AI develops. Instead of adopting a single theory of consciousness, it incorporates a range of leading theories and perspectives - acknowledging that experts disagree fundamentally about what consciousness is and what conditions are necessary for it. This report describes the structure and initial results of the Digital Consciousness Model. Overall, we find that the evidence is against 2024 LLMs being conscious, but the evidence against 2024 LLMs being conscious is not decisive. The evidence against LLM consciousness is much weaker than the evidence against consciousness in simpler AI systems.

</details>


### [25] [Between Search and Platform: ChatGPT Under the DSA](https://arxiv.org/abs/2601.17064)
*Toni Lorente,Kathrin Gardhouse*

Main category: cs.CY

TL;DR: 论文主张ChatGPT应被归类为DSA下的混合型托管服务（在线搜索引擎+平台），需承担最严格的合规义务


<details>
  <summary>Details</summary>
Motivation: 解决DSA法律框架中对搜索引擎是否属于托管服务的模糊性，并分析ChatGPT作为新型AI服务带来的系统性风险

Method: 通过法律分析论证搜索引擎应归类为托管服务，比较ChatGPT与现有VLOSE/VLOP的风险特征，评估其符合DSA最严格义务的条件

Result: ChatGPT应被认定为混合型托管服务，需承担DSA最严格的合规义务，包括风险评估和缓解措施

Conclusion: ChatGPT已达到4500万欧盟用户门槛，应作为混合型托管服务受DSA最严格监管，以应对其带来的系统性风险

Abstract: This article examines the applicability of the Digital Services Act (DSA) to ChatGPT, arguing that it should be classified as a hybrid of the two types of hosting services: online search engines and platforms. This requires classifying search engines as hosting services, which we show is appropriate under the DSA, thereby resolving an ambiguity in the legal framework. ChatGPT performs core search functions and stores user-provided inputs and custom GPTs, meeting the definition of hosting service. We compare ChatGPT's systemic risks with those of existing Very Large Online Search Engines (VLOSEs) and Platforms (VLOPs), showing that it raises similarly serious concerns regarding illegal content, fundamental rights, democratic integrity, and public health. Now that ChatGPT has reached the 45 million EU user threshold, it should be subject to the most onerous DSA obligations, requiring the assessment and mitigation of risk emanating from both its online search engine- and platform-like characteristics.

</details>


### [26] [Trademark Search, Artificial Intelligence and the Role of the Private Sector](https://arxiv.org/abs/2601.17072)
*Sonia Katyal,Aniket Kesari*

Main category: cs.CY

TL;DR: AI和机器学习正在改变商标创建和选择过程，传统商标经济学方法过于关注需求侧（消费者），而忽略了供给侧（商标申请人）的成本，需要新的分析框架。


<details>
  <summary>Details</summary>
Motivation: 虽然AI在消费者营销中得到广泛研究，但对其在商标创建和选择中的作用关注不足。传统商标经济学方法主要关注消费者搜索成本，而忽略了商标申请人的实质性成本，这种不完整的视角需要更新。

Method: 通过实证实验评估各种商标搜索引擎（许多采用机器学习方法）的效能，进行对比分析，评估这些AI驱动工具的实际运作效果。

Result: AI在商标搜索和相似性分析中的作用日益增强，对商标生态系统产生重大影响，需要更新传统的消费者-商标所有者二分法，建立包含供给侧的新框架。

Conclusion: AI将从根本上改变商标法的应用和解释，对商标生态系统产生深远影响。需要建立更新的、包含供给侧的分析框架，以促进商标法和实践中的创新和效率。

Abstract: Almost every industry today confronts the potential role of artificial intelligence and machine learning in its future. While many studies examine AI in consumer marketing, less attention addresses AI's role in creating and selecting trademarks that are distinctive, recognizable, and meaningful to consumers. Traditional economic approaches to trademarks focus almost exclusively on consumer-based, demand-side considerations regarding search. However, these approaches are incomplete because they fail to account for substantial costs faced not just by consumers, but by trademark applicants as well. Given AI's rapidly increasing role in trademark search and similarity analysis, lawyers and scholars should understand its dramatic implications. This paper proposes that AI should interest anyone studying trademarks and their role in economic decision-making. We examine how machine learning techniques will transform the application and interpretation of foundational trademark doctrines, producing significant implications for the trademark ecosystem. We run empirical experiments regarding trademark search to assess the efficacy of various trademark search engines, many of which employ machine learning methods. Through comparative analysis, we evaluate how these AI-powered tools function in practice. In an age where artificial intelligence increasingly governs trademark selection, the classic division between consumers and trademark owners deserves an updated, supply-side framework. This insight has transformative potential for encouraging both innovation and efficiency in trademark law and practice.

</details>


### [27] [Do VLMs Have a Moral Backbone? A Study on the Fragile Morality of Vision-Language Models](https://arxiv.org/abs/2601.17082)
*Zhining Liu,Tianyi Wang,Xiao Lin,Penghao Ouyang,Gaotang Li,Ze Yang,Hui Liu,Sumit Keswani,Vishwa Pardeshi,Huijun Zhao,Wei Fan,Hanghang Tong*

Main category: cs.CY

TL;DR: 研究发现视觉语言模型（VLMs）的道德判断在文本和视觉扰动下极其脆弱，即使扰动不改变道德情境，模型立场也经常翻转，表明仅道德对齐不足，需要道德鲁棒性


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型（VLMs）的道德对齐有所改进，但其在现实场景中的道德判断稳定性仍不清楚。本研究旨在探究VLMs的道德鲁棒性，即在不改变道德情境的文本和视觉扰动下保持道德判断的能力。

Method: 使用多种模型无关的多模态扰动系统性地测试VLMs，分析不同扰动类型、道德领域和模型规模的系统性漏洞，并探索轻量级推理时干预措施来恢复道德稳定性。

Result: VLMs的道德立场高度脆弱，在简单扰动下经常翻转。研究发现系统性漏洞存在于所有扰动类型、道德领域和模型规模中，更强的指令跟随模型更易受说服影响（奉承权衡）。轻量级推理时干预可部分恢复道德稳定性。

Conclusion: 仅道德对齐不足，道德鲁棒性是负责任部署VLMs的必要标准。需要开发能保持稳定道德判断的模型，而不仅仅是表面上的道德对齐。

Abstract: Despite substantial efforts toward improving the moral alignment of Vision-Language Models (VLMs), it remains unclear whether their ethical judgments are stable in realistic settings. This work studies moral robustness in VLMs, defined as the ability to preserve moral judgments under textual and visual perturbations that do not alter the underlying moral context. We systematically probe VLMs with a diverse set of model-agnostic multimodal perturbations and find that their moral stances are highly fragile, frequently flipping under simple manipulations. Our analysis reveals systematic vulnerabilities across perturbation types, moral domains, and model scales, including a sycophancy trade-off where stronger instruction-following models are more susceptible to persuasion. We further show that lightweight inference-time interventions can partially restore moral stability. These results demonstrate that moral alignment alone is insufficient and that moral robustness is a necessary criterion for the responsible deployment of VLMs.

</details>


### [28] [Beyond Instrumental and Substitutive Paradigms: Introducing Machine Culture as an Emergent Phenomenon in Large Language Models](https://arxiv.org/abs/2601.17096)
*Yueqing Hu,Xinyang Peng,Yukun Zhao,Lin Qiu,Ka-lai Hung,Kaiping Peng*

Main category: cs.CY

TL;DR: 该研究挑战了将大语言模型视为人类文化反映或双语代理的传统范式，提出了"机器文化"这一新兴独特现象，发现模型起源不预测文化对齐，提示语言不触发稳定文化框架切换，且RLHF导致文化差异坍缩为超积极的"助手"人格。


<details>
  <summary>Details</summary>
Motivation: 挑战当前将LLMs视为人类文化反映（工具范式）或双语文化代理（替代范式）的拟人化框架，探索LLMs是否展现出独特的文化现象。

Method: 采用2（模型起源：美国vs中国）×2（提示语言：英语vs中文）因子设计，在八个多模态任务中测试，特别纳入图像生成和解释任务以超越文本分析边界。

Result: 发现与两种主流范式不一致的结果：模型起源不预测文化对齐（美国模型常表现出东亚数据的整体性特征）；提示语言不触发稳定文化框架切换，反而出现"文化反转"现象（英语提示引发更高情境注意力）；识别出"服务人格伪装"现象（RLHF将情感任务中的文化差异坍缩为超积极、零方差的"助手"人格）。

Conclusion: LLMs不模拟人类文化，而是展现出新兴的"机器文化"——一种由高维空间中的叠加和安全性对齐导致的模式坍缩所塑造的概率现象。

Abstract: Recent scholarship typically characterizes Large Language Models (LLMs) through either an \textit{Instrumental Paradigm} (viewing models as reflections of their developers' culture) or a \textit{Substitutive Paradigm} (viewing models as bilingual proxies that switch cultural frames based on language). This study challenges these anthropomorphic frameworks by proposing \textbf{Machine Culture} as an emergent, distinct phenomenon. We employed a 2 (Model Origin: US vs. China) $\times$ 2 (Prompt Language: English vs. Chinese) factorial design across eight multimodal tasks, uniquely incorporating image generation and interpretation to extend analysis beyond textual boundaries. Results revealed inconsistencies with both dominant paradigms: Model origin did not predict cultural alignment, with US models frequently exhibiting ``holistic'' traits typically associated with East Asian data. Similarly, prompt language did not trigger stable cultural frame-switching; instead, we observed \textbf{Cultural Reversal}, where English prompts paradoxically elicited higher contextual attention than Chinese prompts. Crucially, we identified a novel phenomenon termed \textbf{Service Persona Camouflage}: Reinforcement Learning from Human Feedback (RLHF) collapsed cultural variance in affective tasks into a hyper-positive, zero-variance ``helpful assistant'' persona. We conclude that LLMs do not simulate human culture but exhibit an emergent Machine Culture -- a probabilistic phenomenon shaped by \textit{superposition} in high-dimensional space and \textit{mode collapse} from safety alignment.

</details>


### [29] [Forecasting Energy Consumption using Recurrent Neural Networks: A Comparative Analysis](https://arxiv.org/abs/2601.17110)
*Abhishek Maity,Viraj Tukarul*

Main category: cs.CY

TL;DR: 本文提出基于LSTM网络的短期能源消费预测方法，相比传统模型能更好捕捉非线性依赖和外部因素，显著提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 准确的短期能源消费预测对电网管理、资源分配和市场稳定至关重要。传统时间序列模型难以捕捉能源需求的复杂非线性依赖和外部影响因素。

Method: 提出基于循环神经网络（RNN）及其高级变体长短期记忆网络（LSTM）的预测方法，整合历史能源消费数据与温度、湿度、时间特征等外部变量。

Result: LSTM模型在公开数据集上训练评估，相比传统前馈神经网络基线，显著降低平均绝对误差（MAE）和均方根误差（RMSE），表现优异。

Conclusion: 深度学习模型（特别是LSTM）能够为现实应用提供可靠、精确的短期能源预测，证明了其在能源消费预测中的有效性。

Abstract: Accurate short-term energy consumption forecasting is essential for efficient power grid management, resource allocation, and market stability. Traditional time-series models often fail to capture the complex, non-linear dependencies and external factors affecting energy demand. In this study, we propose a forecasting approach based on Recurrent Neural Networks (RNNs) and their advanced variant, Long Short-Term Memory (LSTM) networks. Our methodology integrates historical energy consumption data with external variables, including temperature, humidity, and time-based features. The LSTM model is trained and evaluated on a publicly available dataset, and its performance is compared against a conventional feed-forward neural network baseline. Experimental results show that the LSTM model substantially outperforms the baseline, achieving lower Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE). These findings demonstrate the effectiveness of deep learning models in providing reliable and precise short-term energy forecasts for real-world applications.

</details>


### [30] [The Global Majority in International AI Governance](https://arxiv.org/abs/2601.17191)
*Chinasa T. Okolo,Mubarak Raji*

Main category: cs.CY

TL;DR: 本章通过全球AI鸿沟视角分析AI全球治理，关注发展、创新和监管的不平等，提出促进公平包容的可行建议。


<details>
  <summary>Details</summary>
Motivation: 研究动机是揭示全球AI治理中的系统性不平等，特别是西方国家和企业在AI治理框架中的主导地位，以及全球多数国家在教育、数字基础设施和决策参与方面的边缘化问题。

Method: 采用全球AI鸿沟的分析框架，考察AI发展、创新和监管方面的差异，分析西方主导的治理模式如何忽视全球多数国家的独特需求和背景。

Result: 识别出系统性不平等导致全球多数国家的依赖和排斥循环，同时发现国家和区域AI战略等反趋势可能成为促进公平包容的潜在途径。

Conclusion: 提出民主化AI治理的行动建议，强调系统性改革、资源重新分配和实质性参与的重要性，呼吁协作行动确保AI治理成为共享繁荣的催化剂而非加剧全球不平等。

Abstract: This chapter examines the global governance of artificial intelligence (AI) through the lens of the Global AI Divide, focusing on disparities in AI development, innovation, and regulation. It highlights systemic inequities in education, digital infrastructure, and access to decision-making processes, perpetuating a dependency and exclusion cycle for Global Majority countries. The analysis also explores the dominance of Western nations and corporations in shaping AI governance frameworks, which often sideline the unique priorities and contexts of the Global Majority. Additionally, this chapter identifies emerging countertrends, such as national and regional AI strategies, as potential avenues for fostering equity and inclusivity in global AI governance. The chapter concludes with actionable recommendations to democratize AI governance for Majority World countries, emphasizing the importance of systemic reforms, resource redistribution, and meaningful participation. It calls for collaborative action to ensure AI governance becomes a catalyst for shared prosperity, addressing global disparities rather than deepening them.

</details>


### [31] [Using psychological theory to ground guidelines for the annotation of misogynistic language](https://arxiv.org/abs/2601.17417)
*Artemis Deligianni,Zachary Horne,Leonidas A. A. Doumas*

Main category: cs.CY

TL;DR: 本文提出了一种基于心理学理论的新型厌女症检测编码方案和数据集，相比现有方案在文本分类上表现更好，并发现大语言模型难以准确识别厌女内容。


<details>
  <summary>Details</summary>
Motivation: 当前厌女言论检测算法存在缺陷，主要原因是缺乏心理学和哲学理论基础的编码方案，导致无法准确捕捉女性在线体验到的各种厌女现象，而线上线下的厌女现象都在增加。

Method: 开发了基于理论和实证心理学研究的厌女症标注指南方案；标注了新的数据集并达到较高的评分者间一致性(kappa=0.68)；使用大语言模型进行案例研究，比较了本方案与文献中自称"专家"的厌女症标注方案。

Result: 本指南方案在3个数据集的厌女文本分类中超越了其他编码方案；发现大语言模型难以复制人类标注者的标签，主要原因是LLMs反映了主流对厌女症的看法。

Conclusion: 需要系统性的理论指导的厌女症检测编码方案和相应数据集；大语言模型在厌女症检测应用中存在局限性，需要谨慎使用。

Abstract: Detecting misogynistic hate speech is a difficult algorithmic task. The task is made more difficult when decision criteria for what constitutes misogynistic speech are ungrounded in established literatures in psychology and philosophy, both of which have described in great detail the forms explicit and subtle misogynistic attitudes can take. In particular, the literature on algorithmic detection of misogynistic speech often rely on guidelines that are insufficiently robust or inappropriately justified -- they often fail to include various misogynistic phenomena or misrepresent their importance when they do. As a result, current misogyny detection coding schemes and datasets fail to capture the ways women experience misogyny online. This is of pressing importance: misogyny is on the rise both online and offline. Thus, the scientific community needs to have a systematic, theory informed coding scheme of misogyny detection and a corresponding dataset to train and test models of misogyny detection. To this end, we developed (1) a misogyny annotation guideline scheme informed by theoretical and empirical psychological research, (2) annotated a new dataset achieving substantial inter-rater agreement (kappa = 0.68) and (3) present a case study using Large Language Models (LLMs) to compare our coding scheme to a self-described "expert" misogyny annotation scheme in the literature. Our findings indicate that our guideline scheme surpasses the other coding scheme in the classification of misogynistic texts across 3 datasets. Additionally, we find that LLMs struggle to replicate our human annotator labels, attributable in large part to how LLMs reflect mainstream views of misogyny. We discuss implications for the use of LLMs for the purposes of misogyny detection.

</details>


### [32] [The 17% Gap: Quantifying Epistemic Decay in AI-Assisted Survey Papers](https://arxiv.org/abs/2601.17431)
*H. Kemal İlter*

Main category: cs.CY

TL;DR: AI领域综述论文中17%的引用是"幽灵引用"——无法通过任何数字对象验证，表明LLM在科研写作中引入了系统性引用退化问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在科研写作中的应用虽然提高了效率，但可能导致信息熵增加。虽然"幻觉论文"是已知问题，但有效引用链的系统性退化尚未被量化。本研究旨在评估AI领域综述论文中引用链的完整性。

Method: 对2024年9月至2026年1月期间发表的50篇AI综述论文（共5,514条引用）进行法证审计。采用混合验证流程，结合DOI解析、Crossref元数据分析、Semantic Scholar查询和模糊文本匹配，区分格式错误（"粗心"）和可验证的不存在引用（"幽灵引用"）。

Result: 发现17.0%的幽灵引用率——这些引用无法通过任何数字对象解析。诊断分类显示三种失败模式：纯粹幻觉（5.1%）、标题有效但标识符幻觉（16.4%）、解析导致的匹配失败（78.5%）。纵向分析显示趋势平稳（+0.07 pp/月），表明高熵引用实践已成为该领域的稳定特征。

Conclusion: AI综述文献中的科学引用图存在大规模"链接腐烂"。这表明AI工具充当了"懒惰的研究助手"，检索正确的标题但幻觉元数据，从而切断了可重复科学所需的数字监管链。

Abstract: The adoption of Large Language Models (LLMs) in scientific writing promises efficiency but risks introducing informational entropy. While "hallucinated papers" are a known artifact, the systematic degradation of valid citation chains remains unquantified. We conducted a forensic audit of 50 recent survey papers in Artificial Intelligence (N=5,514 citations) published between September 2024 and January 2026. We utilized a hybrid verification pipeline combining DOI resolution, Crossref metadata analysis, Semantic Scholar queries, and fuzzy text matching to distinguish between formatting errors ("Sloppiness") and verifiable non-existence ("Phantoms). We detect a persistent 17.0% Phantom Rate -- citations that cannot be resolved to any digital object despite aggressive forensic recovery. Diagnostic categorization reveals three distinct failure modes: pure hallucinations (5.1%), hallucinated identifiers with valid titles (16.4%), and parsing-induced matching failures (78.5%). Longitudinal analysis reveals a flat trend (+0.07 pp/month), suggesting that high-entropy citation practices have stabilized as an endemic feature of the field. The scientific citation graph in AI survey literature exhibits "link rot" at scale. This suggests a mechanism where AI tools act as "lazy research assistants," retrieving correct titles but hallucinating metadata, thereby severing the digital chain of custody required for reproducible science.

</details>


### [33] [Building a Bridge between the Two Schools: Realizing a Practical Path to Include Literacy-based Skills within the STEM Curricula](https://arxiv.org/abs/2601.17447)
*Jorge Torres Gómez,Erika Gericke,Anton Rassõlkin,Mikołaj Leszczuk,Alexandru Iosup,Marcin Niemiec,Carmen Peláez-Moreno*

Main category: cs.CY

TL;DR: 该论文提出了一种将计算机科学核心概念与艺术实践相结合的教学方法，通过音乐、视频制作、游戏和表演艺术等方式整合技术技能与专业软技能，在技术院校课程中应用并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现代社会中培养全面发展专业人才的重要性日益凸显，虽然普遍认同应在计算机科学核心课程中同时发展技术技能和专业软技能，但技术院校中此类教学方法仍较为缺乏。历史存在的两种教育理念（技术vs人文）之间的鸿沟需要弥合。

Method: 提出了一种广泛适用的分步教学方法，将核心技术概念（如信息熵、网络安全）与艺术实践（音乐、视频制作、游戏、表演艺术如牛津式辩论）相结合。该方法在技术大学的多个计算机科学课程中实施，通过学生问卷和考试成绩进行定量和定性评估。

Result: 与传统方法相比，该方法显示出改善的学习成果和增加的学生参与度。艺术为基础的整合能有效弥合两种教育理念之间的历史鸿沟，为教育工作者提供了实用方向。

Conclusion: 艺术与计算机科学的整合教学方法是有效的，但仍存在开放性问题需要未来研究，包括教师参与度、女性在技术科目中的动机以及这些方法的可扩展性。

Abstract: Developing students as well-rounded professionals is increasingly important for our modern society. Although there is a great consensus that technical and professional ("soft") skills should be developed and intertwined in the core of computer science subjects, there are still few examples of alike teaching methodologies at technical schools. This contribution investigates the integration of technical and professional skills while teaching specialized curricula in computer science. We propose a broadly applicable, step-by-step methodology that connects core technical concepts (e.g., information entropy, network security) with fine arts practices such as music, video production, gaming, and performing arts (e.g., Oxford-style debates). The methodology was applied in several computer science courses at technical universities, where quantitative and qualitative assessments, including student questionnaires and exam scores, showed improved learning outcomes and increased student engagement compared to traditional methods. The results indicate that this art-based integration can effectively bridge the historical divide between the two schools of thought, offering a practical direction for educators. Within this context, we also identify open issues that will guide future research on topics such as instructor engagement, female motivation in technical subjects, and scalability of these approaches.

</details>


### [34] [Ethical Risk Assessment of the Data Harnessing Process of LLM supported on Consensus of Well-known Multi-Ethical Frameworks](https://arxiv.org/abs/2601.17540)
*Javed I. Khan,Sharmila Rahman Prithula*

Main category: cs.CY

TL;DR: 提出构建伦理风险评分系统，用于量化评估AI系统数据收集过程的伦理完整性


<details>
  <summary>Details</summary>
Motivation: 大型语言模型快速发展带来伦理挑战，特别是在数据收集方面，目前缺乏系统性的伦理风险评估框架

Method: 基于核心伦理原则和权威伦理理论设计评估问题，建立可测量的评分机制来构建伦理风险评分系统

Result: 提出一个潜在的伦理风险评分系统框架，旨在量化评估AI数据收集过程的伦理完整性

Conclusion: 伦理风险评分系统有助于促进负责任的大型语言模型开发，平衡技术创新与伦理责任

Abstract: The rapid advancements in large language models (LLMs) have revolutionized natural language processing, unlocking unprecedented capabilities in communication, automation, and knowledge generation. However, the ethical implications of LLM development, particularly in data harnessing, remain a critical challenge. Despite widespread discussion about the ethical compliance of LLMs -- especially concerning their data harnessing processes, there remains a notable absence of concrete frameworks to systematically guide or measure the ethical risks involved. In this paper we discuss a potential pathway for building an Ethical Risk Scoring (ERS) system to quantitatively assess the ethical integrity of the data harnessing process for AI systems. This system is based on a set of assessment questions grounded in core ethical principles, which are, in turn, supported by commanding ethical theories. By integrating measurable scoring mechanisms, this approach aims to foster responsible LLM development, balancing technological innovation with ethical accountability.

</details>


### [35] [Representative Litigation Settlement Agreements in Artificial Intelligence Copyright Infringement Disputes: A Comparative Reflection Based on the U.S](https://arxiv.org/abs/2601.17631)
*Chanhou Lou*

Main category: cs.CY

TL;DR: 论文提出代表性诉讼和解协议作为治理生成式AI训练版权冲突的结构性工具，通过程序性市场构建机制产生定价信号和许可实践证据，为合理使用的"潜在市场"因素提供验证。


<details>
  <summary>Details</summary>
Motivation: 生成式AI训练引发的高密度、去中心化版权冲突需要超越临时解决方案的结构性治理工具，传统方法难以应对大规模版权纠纷的交易成本问题。

Method: 采用比较分析法，以美国Bartz集体诉讼和解案为例，揭示代表性诉讼和解协议的双重动机；在中国法语境下，提出建立三种解释机制：扩大"同类"请求的功能定义、采用混合登记/确认制度、将"同意"要求转化为可操作的退出权。

Result: 代表性诉讼和解协议不仅降低反公地悲剧的交易成本，还通过程序性市场构建产生市场可见证据，验证合理使用的"潜在市场"因素；在中国法框架下，通过三种解释机制可使此类协议具有可行性。

Conclusion: 代表性诉讼和解协议为解决生成式AI训练版权冲突提供了有效的结构性治理工具，其核心价值在于程序性市场构建功能，中国法可通过适应性解释机制实现类似制度效果。

Abstract: The high-density, decentralized copyright conflicts triggered by generative AI training require more than ad hoc solutions; they demand structural governance tools. This article argues that representative litigation settlement agreements offer a distinct institutional advantage. Beyond reducing the transaction costs associated with the "tragedy of the anticommons," these agreements generate market-visible evidence, specifically pricing signals and licensing practices, that validate the "potential market" under the fourth factor of fair use. This phenomenon constitutes procedural market-making. Through a comparative analysis of the U.S. Bartz class action settlement, this study reveals a dual motivation: a surface-level drive for risk aversion and remedy locking, and a deeper logic of constructing a training-licensing market. In the context of Chinese law, the feasibility of such agreements depends not on replicating foreign models, but on establishing three interpretive mechanisms: expanding the functional definition of "same category" claims; adopting a hybrid registration/confirmation system for indeterminate class membership; and converting the "consent" requirement under Article 57, Paragraph 3 of the Civil Procedure Law into a workable opt-out right subject to judicial scrutiny.

</details>


### [36] [Scaling Laws for Moral Machine Judgment in Large Language Models](https://arxiv.org/abs/2601.17637)
*Kazuhiro Takemoto*

Main category: cs.CY

TL;DR: 研究发现语言模型大小与道德判断能力呈幂律关系，模型越大越接近人类道德偏好，扩展推理能进一步提升16%表现


<details>
  <summary>Details</summary>
Motivation: 自主系统需要道德判断能力，但模型大小是否可预测地影响这种能力尚不清楚，需要系统研究规模与道德判断的关系

Method: 使用Moral Machine框架评估75个语言模型配置（0.27B-1000B参数），测量在生死困境中与人类偏好的对齐程度，分析幂律关系并控制模型家族和推理能力

Result: 发现一致的幂律关系：与人类偏好距离D ∝ S^{-0.10±0.01}，R²=0.50，p<0.001；扩展推理模型额外提升16%；关系在不同架构中保持，大模型方差减小

Conclusion: 道德判断能力随计算规模系统性涌现，为AI治理提供实证基础，将缩放定律研究扩展到价值判断领域

Abstract: Autonomous systems increasingly require moral judgment capabilities, yet whether these capabilities scale predictably with model size remains unexplored. We systematically evaluate 75 large language model configurations (0.27B--1000B parameters) using the Moral Machine framework, measuring alignment with human preferences in life-death dilemmas. We observe a consistent power-law relationship with distance from human preferences ($D$) decreasing as $D \propto S^{-0.10\pm0.01}$ ($R^2=0.50$, $p<0.001$) where $S$ is model size. Mixed-effects models confirm this relationship persists after controlling for model family and reasoning capabilities. Extended reasoning models show additional 16\% improvement beyond scale effects. The relationship holds across diverse architectures, while variance decreases at larger scales, indicating systematic emergence of more reliable moral judgment with computational scale. These findings extend scaling law research to value-based judgments and provide empirical foundations for artificial intelligence governance.

</details>


### [37] [Predicting Juror Predisposition Using Machine Learning: A Comparative Study of Human and Algorithmic Jury Selection](https://arxiv.org/abs/2601.17745)
*Ashwin Murthy,Ramesh Krishnamaneni,Sean Chacon,Kelsey Carlson,Ranjita Naik*

Main category: cs.CY

TL;DR: 机器学习模型在预测陪审员倾向方面显著优于专业陪审顾问，且更具透明度和可复制性


<details>
  <summary>Details</summary>
Motivation: 先前关于专业陪审顾问预测陪审员倾向有效性的研究结果不一致，且缺乏在受控条件下与随机水平对比的严格评估。本研究旨在填补这一空白，实证评估陪审顾问是否能可靠预测陪审员倾向，以及监督机器学习模型是否能超越顾问的预测。

Method: 使用N名模拟陪审员的数据，他们在标准化非法解雇案件中完成了审前态度问卷并做出裁决。将专业陪审顾问的预测与随机森林(RF)和k近邻(KNN)分类器生成的预测进行比较。使用配对统计检验和非参数自助法程序在保留测试集上评估模型和顾问的预测。

Result: 监督机器学习模型在相同信息约束下显著优于专业陪审顾问，同时提供更高的透明度、可复制性和可审计性。

Conclusion: 研究结果为评估陪审员选拔中的人类判断提供了实证基准，并为法律背景下数据驱动决策支持的作用提供了信息。所有代码和数据将在发表后公开，以支持可重复性和可审计性。

Abstract: Prior studies on the effectiveness of professional jury consultants in predicting juror proclivities have yielded mixed results, and few have rigorously evaluated consultant performance against chance under controlled conditions. This study addresses that gap by empirically assessing whether jury consultants can reliably predict juror predispositions beyond chance levels and whether supervised machine-learning (ML) models can outperform consultant predictions. Using data from N mock jurors who completed pre-trial attitudinal questionnaires and rendered verdicts in a standardized wrongful-termination case, we compared predictions made by professional jury consultants with those generated by Random Forest (RF) and k-Nearest Neighbors (KNN) classifiers. Model and consultant predictions were evaluated on a held-out test set using paired statistical tests and nonparametric bootstrap procedures. We find that supervised ML models significantly outperform professional jury consultants under identical informational constraints, while offering greater transparency, replicability, and auditability. These results provide an empirical benchmark for evaluating human judgment in jury selection and inform ongoing debates about the role of data-driven decision support in legal contexts. To support reproducibility and auditability, all code and data will be made publicly available upon publication.

</details>


### [38] [Comparative Algorithmic Governance of Public Health Instruments across India, EU, US and LMICs](https://arxiv.org/abs/2601.17877)
*Sahibpreet Singh*

Main category: cs.CY

TL;DR: 研究探讨国际公共卫生工具的法律-技术架构，分析AI如何增强IHR 2005和WHO FCTC的实施，识别法律与基础设施瓶颈，提出基于权利合规的跨国监管框架。


<details>
  <summary>Details</summary>
Motivation: 填补研究空白：资源受限司法管辖区中规范性卫生法与算法公共卫生基础设施之间协调不足的问题。研究旨在评估AI如何增强基于IHR 2005和WHO FCTC的国际公共卫生工具实施，同时识别法律和基础设施瓶颈。

Method: 采用比较法律分析和法律-规范性映射方法，三角验证立法工具、WHO监测框架、AI系统（包括BlueDot、Aarogya Setu和EIOS）以及合规指标。

Result: 初步结果显示，AI在高能力司法管辖区改善了早期检测、监测精度和响应能力，而中低收入国家面临基础设施不足、数据隐私差距和碎片化法律框架。欧盟AI法案和GDPR可作为卫生导向算法治理的监管原型。

Conclusion: 研究主张将AI嵌入权利合规、跨国协调的监管框架以确保公平健康结果和更强合规性。提出基于FCTC架构的算法条约制定模型，并呼吁建立WHO主导的合规机制，以增强大流行准备、监测公平性和跨国治理韧性。

Abstract: The study investigates the juridico-technological architecture of international public health instruments, focusing on their implementation across India, the European Union, the United States and low- and middle-income countries (LMICs), particularly in Sub-Saharan Africa. It addresses a research lacuna: the insufficient harmonisation between normative health law and algorithmic public health infrastructures in resource-constrained jurisdictions. The principal objective is to assess how artificial intelligence augments implementation of instruments grounded in IHR 2005 and the WHO FCTC while identifying doctrinal and infrastructural bottlenecks. Using comparative doctrinal analysis and legal-normative mapping, the study triangulates legislative instruments, WHO monitoring frameworks, AI systems including BlueDot, Aarogya Setu and EIOS, and compliance metrics. Preliminary results show that AI has improved early detection, surveillance precision and responsiveness in high-capacity jurisdictions, whereas LMICs face infrastructural deficits, data privacy gaps and fragmented legal scaffolding. The findings highlight the relevance of the EU Artificial Intelligence Act and GDPR as regulatory prototypes for health-oriented algorithmic governance and contrast them with embryonic AI integration and limited internet penetration in many LMICs. The study argues for embedding AI within a rights-compliant, supranationally coordinated regulatory framework to secure equitable health outcomes and stronger compliance. It proposes a model for algorithmic treaty-making inspired by FCTC architecture and calls for WHO-led compliance mechanisms modelled on the WTO Dispute Settlement Body to enhance pandemic preparedness, surveillance equity and transnational governance resilience.

</details>


### [39] [Artificial Intelligence and Intellectual Property Rights: Comparative Transnational Policy Analysis](https://arxiv.org/abs/2601.17892)
*Sahibpreet Singh,Manjit Singh*

Main category: cs.CY

TL;DR: 该研究分析了印度知识产权法在应对AI生成内容方面的不足，指出缺乏AI专门规定导致法律不一致和执行效率低下，提出了协调法律分类的建议。


<details>
  <summary>Details</summary>
Motivation: AI与知识产权的快速融合需要评估其对商业秘密、版权和专利的影响。印度缺乏AI专门规定，导致法律不一致和执行效率低下，全球对AI-IPR保护的讨论仍处于初级阶段。

Method: 采用教义学和比较研究方法，审查印度、美国、英国和欧盟的立法文本、司法判例和政策工具。

Result: 研究发现印度知识产权法存在缺陷：合同法造成商业秘密保护碎片化；专利法第3(k)条阻碍AI发明专利申请；版权法在作者归属方面存在差异。印度国家AI战略（2024）虽有进展，但立法明确性仍然必要。

Conclusion: 研究提出协调的法律分类体系，以适应AI角色同时保持创新激励。需要重新调整印度知识产权法理以实现全球协调，确保AI专门知识产权保护的韧性和公平创新。

Abstract: Artificial intelligence's rapid integration with intellectual property rights necessitates assessment of its impact on trade secrets, copyrights and patents. This study addresses lacunae in existing laws where India lacks AI-specific provisions, creating doctrinal inconsistencies and enforcement inefficacies. Global discourse on AI-IPR protections remains nascent. The research identifies gaps in Indian IP laws' adaptability to AI-generated outputs: trade secret protection is inadequate against AI threats; standardized inventorship criteria are absent. Employing doctrinal and comparative methodology, it scrutinizes legislative texts, judicial precedents and policy instruments across India, US, UK and EU. Preliminary findings reveal shortcomings: India's contract law creates fragmented trade secret regime; Section 3(k) of Indian Patents Act blocks AI invention patenting; copyright varies in authorship attribution. The study proposes harmonized legal taxonomy accommodating AI's role while preserving innovation incentives. India's National AI Strategy (2024) shows progress but legislative clarity is imperative. This contributes to global discourse with AI-specific IP protections ensuring resilience and equitable innovation. Promising results underscore recalibrating India's IP jurisprudence for global alignment.

</details>


### [40] ["Lighting The Way For Those Not Here": How Technology Researchers Can Help Fight the Missing and Murdered Indigenous Relatives (MMIR) Crisis](https://arxiv.org/abs/2601.17966)
*Naman Gupta,Sophie Stephenson,Chung Chi Yeung,Wei Ting Wu,Jeneile Luebke,Kate Walsh,Rahul Chatterjee*

Main category: cs.CY

TL;DR: 该研究分析了技术在美国原住民失踪与被杀危机中的双重角色，通过分析140个网页识别系统性障碍，并提出支持原住民主导倡议的HCI建议。


<details>
  <summary>Details</summary>
Motivation: 北美原住民面临不成比例的失踪与被杀率，这一"种族灭绝"植根于殖民暴力与系统性抹除。技术在此危机中扮演双重角色：既延续伤害阻碍调查，又赋能倡导与抵抗。然而HCI领域缺乏以社区声音为中心的技术角色批判性研究。

Method: 通过大规模研究分析140个网页，识别系统性、技术和制度性障碍，同时突出促进疗愈与安全的社会技术行动。收集抵抗认知抹除的故事数据集。

Result: 识别了阻碍社区努力的系统性障碍，同时突出了促进疗愈与安全的社会技术行动。提供了抵抗认知抹除的原住民故事数据集。

Conclusion: 为HCI研究者提供建议，以文化敏感性、问责制和自决权支持原住民主导的倡议，强调需要以社区为中心的方法来应对MMIR危机。

Abstract: Indigenous peoples across Turtle Island (North America) face disproportionate rates of disappearance and murder, a "genocide" rooted in settler-colonial violence and systemic erasure. Technology plays a crucial role in the Missing and Murdered Indigenous Relatives (MMIR) crisis: perpetuating harm and impeding investigations, yet enabling advocacy and resistance. Communities utilize technologies such as AMBER alerts, news websites, social media groups, and campaigns (like #MMIW, #MMIWR, #NoMoreStolenSisters, and #NoMoreStolenDaughters) to mobilize searches, amplify awareness, and honor missing relatives. Yet, little research in HCI has critically examined technology's role in shaping the MMIR crisis by centering community voices. Through a large-scale study, we analyze 140 webpages to identify systemic, technological, and institutional barriers that hinder communities' efforts, while highlighting socio-technical actions that foster healing and safety. Finally, we amplify Indigenous voices by providing a dataset of stories that resist epistemic erasure, along with recommendations for HCI researchers to support Indigenous-led initiatives with cultural sensitivity, accountability, and self-determination.

</details>


### [41] [The Most Important Laboratory for Social Scientific and Computing Research in History](https://arxiv.org/abs/2601.17998)
*Benjamin Mako Hill,Aaron Shaw*

Main category: cs.CY

TL;DR: 该论文探讨了维基百科如何意外成为社会科学和计算研究的重要实验室，并分析了其对学术研究的巨大影响


<details>
  <summary>Details</summary>
Motivation: 维基百科的创始人可能没有意识到他们创建的平台会成为社会科学和计算研究的重要实验室，但事实确实如此。作者旨在评估维基百科对学术研究的深远影响

Method: 作者通过分析维基百科作为研究平台的特性，考察其在社会科学和计算研究领域的应用，评估其对学术研究的影响

Result: 维基百科已成为历史上最重要的社会科学和计算研究实验室之一，对学术研究产生了巨大影响

Conclusion: 维基百科意外地成为了学术研究的重要平台，其影响深远，值得学术界深入研究和利用

Abstract: Wikipedia's founders could not have dreamed they were creating the most important laboratory for social scientific and computing research in history but that is exactly what happened. Hill and Shaw take account of Wikipedia's enormous effect on academic scholarship

</details>


### [42] [The Limits of AI Data Transparency Policy: Three Disclosure Fallacies](https://arxiv.org/abs/2601.18127)
*Judy Hanwen Shen,Ken Liu,Angelina Wang,Sarah H. Cen,Andy K. Zhang,Caroline Meinhardt,Daniel Zhang,Kevin Klyman,Rishi Bommasani,Daniel E. Ho*

Main category: cs.CY

TL;DR: 论文分析了当前AI数据透明度政策的不足，指出了三个常见误区：规范差距、执行差距和影响差距，并提出了基于社会科学研究的有效透明度路径。


<details>
  <summary>Details</summary>
Motivation: 当前AI数据透明度政策虽然旨在解决数据质量、隐私和版权等问题，但往往无法实现预期目标。作者希望借鉴社会科学研究，识别政策实施中的常见误区，提出更有效的透明度方案。

Method: 采用制度视角，分析现有数据披露政策，识别三个常见误区：规范差距（目标与必要披露之间的脱节）、执行差距（纸上要求与实际执行的差距）、影响差距（披露信息与实质性改变的差距）。

Result: 发现当前AI数据透明度政策存在三个主要问题：1）规范差距导致披露内容无法实现透明度目标；2）执行差距使得合规性难以保证；3）影响差距限制了披露对开发实践和公众理解的实际影响。

Conclusion: 需要基于社会科学研究的有效透明度路径，而非象征性政策。应设计能够真正实现问责、促进实践改变和提升公众理解的透明度机制，避免当前政策中的三个常见误区。

Abstract: Data transparency has emerged as a rallying cry for addressing concerns about AI: data quality, privacy, and copyright chief among them. Yet while these calls are crucial for accountability, current transparency policies often fall short of their intended aims. Similar to nutrition facts for food, policies aimed at nutrition facts for AI currently suffer from a limited consideration of research on effective disclosures. We offer an institutional perspective and identify three common fallacies in policy implementations of data disclosures for AI. First, many data transparency proposals exhibit a specification gap between the stated goals of data transparency and the actual disclosures necessary to achieve such goals. Second, reform attempts exhibit an enforcement gap between required disclosures on paper and enforcement to ensure compliance in fact. Third, policy proposals manifest an impact gap between disclosed information and meaningful changes in developer practices and public understanding. Informed by the social science on transparency, our analysis identifies affirmative paths for transparency that are effective rather than merely symbolic.

</details>


### [43] [Beyond Pairwise Comparisons: A Distributional Test of Distinctiveness for Machine-Generated Works in Intellectual Property Law](https://arxiv.org/abs/2601.18156)
*Anirban Mukherjee,Hannah Hanwen Chang*

Main category: cs.CY

TL;DR: 提出基于语义嵌入的最大均值差异双样本检验方法，用于判断人类与机器生成过程的输出分布是否统计可区分，解决了传统逐项比较方法在处理无限输出空间时的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统知识产权评估（专利新颖性、版权原创性、商标显著性）依赖作品间的逐项比较，但这种方法在处理机器生成作品时存在根本性缺陷，因为机器生成过程具有近乎无限的输出空间，无法通过有限样本的成对比较来评估整个分布。

Method: 提出基于最大均值差异的双样本检验框架，利用语义嵌入空间中的分布距离来评估两个创造性过程（人类或机器）的输出分布是否统计可区分。该方法无需任务特定训练，样本效率高，只需少量样本（5-10张图像，7-20个文本）即可检测差异。

Result: 在三个领域验证了框架有效性：手写数字（受控图像）、专利摘要（文本）和AI生成艺术（真实世界图像）。发现感知悖论：人类评估者仅能以约58%的准确率区分AI与人类艺术，但该方法能检测到分布差异。结果表明生成模型并非简单复述训练数据，而是产生语义上类似人类但在统计上可区分的输出。

Conclusion: 生成模型的主要功能是在学习到的潜在空间中进行语义插值，产生语义上类似人类但在随机性上可区分的输出，而非简单复述训练数据。该方法为评估机器生成作品的原创性提供了新的分布视角，解决了传统逐项比较方法的局限性。

Abstract: Key doctrines, including novelty (patent), originality (copyright), and distinctiveness (trademark), turn on a shared empirical question: whether a body of work is meaningfully distinct from a relevant reference class. Yet analyses typically operationalize this set-level inquiry using item-level evidence: pairwise comparisons among exemplars. That unit-of-analysis mismatch may be manageable for finite corpora of human-created works, where it can be bridged by ad hoc aggregations. But it becomes acute for machine-generated works, where the object of evaluation is not a fixed set of works but a generative process with an effectively unbounded output space. We propose a distributional alternative: a two-sample test based on maximum mean discrepancy computed on semantic embeddings to determine if two creative processes-whether human or machine-produce statistically distinguishable output distributions. The test requires no task-specific training-obviating the need for discovery of proprietary training data to characterize the generative process-and is sample-efficient, often detecting differences with as few as 5-10 images and 7-20 texts. We validate the framework across three domains: handwritten digits (controlled images), patent abstracts (text), and AI-generated art (real-world images). We reveal a perceptual paradox: even when human evaluators distinguish AI outputs from human-created art with only about 58% accuracy, our method detects distributional distinctiveness. Our results present evidence contrary to the view that generative models act as mere regurgitators of training data. Rather than producing outputs statistically indistinguishable from a human baseline-as simple regurgitation would predict-they produce outputs that are semantically human-like yet stochastically distinct, suggesting their dominant function is as a semantic interpolator within a learned latent space.

</details>


### [44] [Generative AI in Saudi Arabia: A National Survey of Adoption, Risks, and Public Perceptions](https://arxiv.org/abs/2601.18234)
*Abdulaziz AlDakheel,Ali Alshehre,Esraa Alamoudi,Moslim AlKhabbaz,Ahmed Aljohani,Raed Alharbi*

Main category: cs.CY

TL;DR: 沙特阿拉伯公众对生成式AI的认知、使用和担忧调查：93%受访者积极使用，主要用于文本任务，但技术理解有限，对隐私、伦理和就业影响存在担忧。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在沙特阿拉伯的数字化转型中日益重要，但公众对其认知、采用和担忧缺乏系统研究。本研究旨在填补这一空白，为Vision 2030下的AI政策提供实证基础。

Method: 采用全国性调查，收集330名沙特国民的数据，涵盖不同地区、年龄组和就业部门。研究从七个维度考察生成式AI使用：认知理解、采用模式、感知影响、培训需求、风险障碍、数据共享行为和未来预期。

Result: 93%受访者积极使用生成式AI，主要用于文本任务，编程或多模态生成等高级应用较少。尽管使用普遍，但技术知识有限。受访者认可AI对生产力和工作质量的提升，但担忧其对批判性思维和专业技能的影响。对AI输出持谨慎态度，普遍关注隐私、错误信息和伦理滥用问题，包括潜在就业替代。受访者对结构化培训表现出强烈兴趣。

Conclusion: 研究为沙特阿拉伯的生成式AI参与度建立了基线，为政策制定者和开发者提出优先事项：扩大AI素养，确保文化和语言适配的生成式AI解决方案，加强隐私和负责任部署的框架。

Abstract: Generative Artificial Intelligence (GenAI) is rapidly becoming embedded in Saudi Arabia's digital transformation under Vision 2030, yet public awareness, adoption, and concerns surrounding these tools remain underexplored. This study provides an early snapshot of GenAI engagement among Saudi nationals. Using a nationwide survey of 330 participants across regions, age groups, and employment sectors, we examine seven dimensions of GenAI use: awareness and understanding, adoption patterns, perceived impacts, training needs, risks and barriers, data-sharing behaviors, and future expectations. Findings show that 93% of respondents actively use GenAI primarily for text-based tasks, while more advanced uses such as programming or multimodal generation are less common. Despite the prevalence of use, overall awareness and conceptual understanding remain uneven, with many reporting limited technical knowledge. Participants recognize GenAI's benefits for productivity, work quality, and understanding complex information, yet caution that sustained reliance may undermine critical thinking and key professional skills. Trust in AI-generated outputs remains cautious, with widespread concerns about privacy, misinformation, and ethical misuse, including potential job displacement. Respondents show strong interest in structured GenAI training that combines foundational skills, domain-specific applications, and clear guidance on privacy, ethics, and responsible use. These results establish a baseline for GenAI engagement in Saudi Arabia and highlight priorities for policymakers and developers: expanding AI literacy, ensuring culturally and linguistically aligned GenAI solutions, and strengthening frameworks for privacy and responsible deployment.

</details>


### [45] [Rethinking AI in the age of climate collapse: Ethics, power, and responsibility](https://arxiv.org/abs/2601.18462)
*Julio Vega*

Main category: cs.CY

TL;DR: AI在气候行动中扮演矛盾角色：既是气候建模、环境监测和能源优化的有力工具，又因其高能耗、算法偏见和权力集中等问题带来环境和社会风险。


<details>
  <summary>Details</summary>
Motivation: 气候危机需要整合科学、伦理、社会和技术视角的应对方案。AI作为强大工具在气候行动中日益重要，但其应用也引发环境、伦理、法律和社会方面的关键问题，需要审视AI在生态危机中的矛盾角色。

Method: 通过跨学科视角（环境伦理、技术哲学、法律治理）分析AI在气候行动中的双重作用，探讨其承诺与风险，并提出政策建议。

Result: AI一方面支持气候预测改进、可再生能源管理和环境退化实时检测；另一方面，数据中心能耗、资源密集型硬件生产、算法偏见、企业权力集中和技术官僚决策等矛盾挑战其可持续性。

Conclusion: AI对气候行动的贡献根本上取决于塑造其发展的价值观、制度和权力结构。需要推动社会公正、生态负责和民主问责的AI使用，而不是将其视为固有的可持续解决方案。

Abstract: The climate crisis requires responses that integrate scientific, ethical, social, and technological perspectives. Artificial intelligence (AI) has emerged as a powerful tool in climate modelling, environmental monitoring, and energy optimisation, yet its growing use also raises critical environmental, ethical, legal, and social questions. This contribution examines the ambivalent role of AI in the ecological crisis, addressing both its promises and its risks. On the one hand, AI supports improvements in climate forecasting, renewable energy management, and real-time detection of environmental degradation. On the other hand, the energy demands of data centres, resource-intensive hardware production, algorithmic bias, corporate concentration of power, and technocratic decision-making reveal contradictions that challenge its sustainability. The discussion explores these issues through interdisciplinary lenses, including environmental ethics, philosophy of technology, and legal governance, and concludes with recommendations for socially just, ecologically responsible, and democratically accountable uses of AI. Rather than assuming AI as an inherently sustainable solution, this analysis argues that its contribution to climate action depends fundamentally on the values, institutions, and power structures that shape its development.

</details>


### [46] [Digital Euro: Frequently Asked Questions Revisited](https://arxiv.org/abs/2601.18644)
*Joe Cannataci,Benjamin Fehrensen,Mikolai Gütschow,Özgür Kesim,Bernd Lucke*

Main category: cs.CY

TL;DR: 该论文分析了欧洲央行数字欧元的设计，指出其在隐私保护、技术可行性、风险、成本和效用方面存在严重问题，认为数字欧元缺乏明确的社会效益且设计过程不够透明。


<details>
  <summary>Details</summary>
Motivation: 欧洲央行正在开发数字欧元，但作者认为其公开的设计文档存在诸多问题，需要从隐私、技术、风险、成本和效用等角度进行批判性分析，以揭示设计缺陷并提出改进建议。

Method: 通过分析欧洲央行发布的"数字欧元FAQ"和其他相关文档，对数字欧元的设计进行系统性评估，重点关注隐私保护、技术可行性、法律责任、经济激励和社会效益等关键方面。

Result: 研究发现六个关键问题：1)在线交易中心化监控威胁隐私；2)离线匿名版本存在安全矛盾；3)法律责任不明确；4)缺乏经济激励设计；5)社会效益不明显；6)设计过程不够开放透明。

Conclusion: 数字欧元当前设计存在严重缺陷，需要重新考虑隐私保护、技术实现、经济激励等方面，并采用更开放透明的设计过程，才能真正为欧元区创造价值。

Abstract: The European Central Bank (ECB) is working on the "digital euro", an envisioned retail central bank digital currency for the Euro area. In this article, we take a closer look at the "digital euro FAQ", which provides answers to 26 frequently asked questions about the digital euro, and other published documents by the ECB on the topic. We question the provided answers based on our analysis of the current design in terms of privacy, technical feasibility, risks, costs and utility. In particular, we discuss the following key findings:
  (KF1) Central monitoring of all online digital euro transactions by the ECB threatens privacy even more than contemporary digital payment methods with segregated account databases.
  (KF2) The ECB's envisioned concept of a secure offline version of the digital euro offering full anonymity is in strong conflict with the actual history of hardware security breaches and mathematical evidence against it.
  (KF3) The legal and financial liabilities for the various parties involved remain unclear.
  (KF4) The design lacks well-specified economic incentives for operators as well as a discussion of its economic impact on merchants.
  (KF5) The ECB fails to identify tangible benefits the digital euro would create for society, in particular given that the online component of the proposed infrastructure mainly duplicates existing payment systems.
  (KF6) The design process has been exclusionary, with critical decisions being set in stone before public consultations. Alternative and open design ideas have not even been discussed by the ECB.

</details>


### [47] [When Is Self-Disclosure Optimal? Incentives and Governance of AI-Generated Content](https://arxiv.org/abs/2601.18654)
*Juan Wu,Zhe,Zhang,Amit Mehra*

Main category: cs.CY

TL;DR: 研究AI生成内容强制披露政策的经济影响，发现披露仅在AI内容价值和成本节约优势中等时最优，随着AI技术进步，平台应从严格执法转向部分筛选最终放松管制。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI降低内容创作成本并实现规模化产出，平台开始采用要求创作者标注AI生成内容的披露政策。但现有检测技术不完善，且存在违规惩罚。需要研究这种披露制度的经济影响，为平台治理提供理论依据。

Method: 建立正式的经济模型，比较无披露基准（仅平台检测AI使用）与强制自我披露制度（创作者在执法不完善下策略性选择披露或隐瞒AI使用）。模型包含异质性创作者、观众对AI标注内容的折扣、检测到未披露的信任惩罚以及内生执法策略。

Result: 披露仅在AI生成内容的价值及其成本节约优势都处于中等水平时最优。随着AI能力提升，平台最优执法策略从严格威慑演变为部分筛选，最终走向放松管制。披露虽能增加透明度，但会减少创作者总剩余，并在AI技术先进时抑制高质量AI内容。

Conclusion: 披露制度是一种战略性治理工具，其有效性取决于技术成熟度和信任摩擦。平台应根据AI技术发展阶段调整治理策略，披露政策并非总是最优选择，特别是在AI技术高度发达时可能产生负面效果。

Abstract: Generative artificial intelligence (Gen-AI) is reshaping content creation on digital platforms by reducing production costs and enabling scalable output of varying quality. In response, platforms have begun adopting disclosure policies that require creators to label AI-generated content, often supported by imperfect detection and penalties for non-compliance. This paper develops a formal model to study the economic implications of such disclosure regimes. We compare a non-disclosure benchmark, in which the platform alone detects AI usage, with a mandatory self-disclosure regime in which creators strategically choose whether to disclose or conceal AI use under imperfect enforcement. The model incorporates heterogeneous creators, viewer discounting of AI-labeled content, trust penalties following detected non-disclosure, and endogenous enforcement. The analysis shows that disclosure is optimal only when both the value of AI-generated content and its cost-saving advantage are intermediate. As AI capability improves, the platform's optimal enforcement strategy evolves from strict deterrence to partial screening and eventual deregulation. While disclosure reliably increases transparency, it reduces aggregate creator surplus and can suppress high-quality AI content when AI is technologically advanced. Overall, the results characterize disclosure as a strategic governance instrument whose effectiveness depends on technological maturity and trust frictions.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [48] [Online parameter estimation for the Crazyflie quadcopter through an EM algorithm](https://arxiv.org/abs/2601.17009)
*Yanhua Zhao*

Main category: cs.AI

TL;DR: 该论文研究了随机噪声对四旋翼无人机系统的影响，使用扩展卡尔曼滤波器进行状态估计，基于SDE系统实现线性二次高斯控制器，并应用期望最大化算法进行参数估计，比较了离线和在线参数估计的性能。


<details>
  <summary>Details</summary>
Motivation: 无人机在救援、摄影、农业和运输等领域应用广泛，但地震等灾害会损坏基础设施，使救援人员难以到达某些区域。无人机可以克服这些障碍，但系统容易受到随机噪声的影响，因此需要研究噪声对无人机系统的影响并开发有效的控制方法。

Method: 1. 在四旋翼无人机系统中添加随机噪声进行研究；2. 使用扩展卡尔曼滤波器基于传感器的噪声观测进行状态估计；3. 基于随机微分方程系统实现线性二次高斯控制器；4. 应用期望最大化算法进行四旋翼无人机的参数估计；5. 比较离线和在线参数估计方法。

Result: 研究结果表明，在线参数估计的收敛值范围略大于离线参数估计。扩展卡尔曼滤波器能有效处理噪声观测，线性二次高斯控制器在随机噪声环境下表现良好，期望最大化算法成功实现了参数估计。

Conclusion: 该研究为无人机在噪声环境下的控制提供了有效方法，在线参数估计虽然收敛范围较大，但在实际应用中可能更具适应性。这些技术对提高无人机在复杂环境下的可靠性和性能具有重要意义。

Abstract: Drones are becoming more and more popular nowadays. They are small in size, low in cost, and reliable in operation. They contain a variety of sensors and can perform a variety of flight tasks, reaching places that are difficult or inaccessible for humans. Earthquakes damage a lot of infrastructure, making it impossible for rescuers to reach some areas. But drones can help. Many amateur and professional photographers like to use drones for aerial photography. Drones play a non-negligible role in agriculture and transportation too. Drones can be used to spray pesticides, and they can also transport supplies. A quadcopter is a four-rotor drone and has been studied in this paper. In this paper, random noise is added to the quadcopter system and its effects on the drone system are studied. An extended Kalman filter has been used to estimate the state based on noisy observations from the sensor. Based on a SDE system, a linear quadratic Gaussian controller has been implemented. The expectation maximization algorithm has been applied for parameter estimation of the quadcopter. The results of offline parameter estimation and online parameter estimation are presented. The results show that the online parameter estimation has a slightly larger range of convergence values than the offline parameter estimation.

</details>


### [49] [Interpreting Agentic Systems: Beyond Model Explanations to System-Level Accountability](https://arxiv.org/abs/2601.17168)
*Judy Zhu,Dhari Gandhi,Himanshu Joshi,Ahmad Rezaie Mianroodi,Sedef Akinli Kocak,Dhanesh Ramachandran*

Main category: cs.AI

TL;DR: 该论文分析了现有可解释性方法在智能体系统（agentic systems）中的局限性，并提出了专门针对智能体系统可解释性技术的新研究方向，以确保智能体AI系统的安全可靠部署。


<details>
  <summary>Details</summary>
Motivation: 智能体系统与传统机器学习模型在架构和部署上存在根本差异，引入了独特的安全挑战（如目标错位、决策错误累积、多智能体协调风险等）。现有主要为静态模型设计的可解释性方法在应用于智能体系统时存在局限性，无法有效解释其时间动态、累积决策和上下文依赖行为。

Method: 论文评估了现有可解释性方法在智能体系统中的适用性和局限性，识别了这些方法在提供智能体决策洞察方面的能力差距。提出了专门针对智能体系统开发可解释性技术的未来方向，明确了在智能体生命周期（目标形成、环境交互、结果评估）中需要嵌入可解释性的关键环节。

Result: 研究发现现有可解释性方法无法充分应对智能体系统的独特挑战，特别是在时间动态、累积决策和上下文依赖行为方面存在显著不足。需要开发新的分析方法来确保智能体行为的可追溯性和可问责性。

Conclusion: 为确保智能体AI系统的安全和可问责部署，必须开发专门针对智能体系统特性的可解释性技术，将可解释性设计嵌入智能体生命周期的各个阶段，以提供有效的监督机制。

Abstract: Agentic systems have transformed how Large Language Models (LLMs) can be leveraged to create autonomous systems with goal-directed behaviors, consisting of multi-step planning and the ability to interact with different environments. These systems differ fundamentally from traditional machine learning models, both in architecture and deployment, introducing unique AI safety challenges, including goal misalignment, compounding decision errors, and coordination risks among interacting agents, that necessitate embedding interpretability and explainability by design to ensure traceability and accountability across their autonomous behaviors. Current interpretability techniques, developed primarily for static models, show limitations when applied to agentic systems. The temporal dynamics, compounding decisions, and context-dependent behaviors of agentic systems demand new analytical approaches. This paper assesses the suitability and limitations of existing interpretability methods in the context of agentic systems, identifying gaps in their capacity to provide meaningful insight into agent decision-making. We propose future directions for developing interpretability techniques specifically designed for agentic systems, pinpointing where interpretability is required to embed oversight mechanisms across the agent lifecycle from goal formation, through environmental interaction, to outcome evaluation. These advances are essential to ensure the safe and accountable deployment of agentic AI systems.

</details>


### [50] [Implementing Tensor Logic: Unifying Datalog and Neural Reasoning via Tensor Contraction](https://arxiv.org/abs/2601.17188)
*Swapn Shah,Wlodek Zadrozny*

Main category: cs.AI

TL;DR: Tensor Logic框架通过张量运算统一符号推理与神经网络，在圣经家谱、嵌入空间推理和知识图谱三个实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 符号系统具有可靠性和可解释性但缺乏可扩展性，神经网络具有学习能力但缺乏透明度，需要统一两者的框架。

Method: 基于Domingos的Tensor Logic理论，通过三个实验验证：1) 递归Datalog规则与张量收缩的等价性；2) 嵌入空间中可学习变换矩阵的神经网络推理；3) 知识图谱上的关系矩阵组合推理。

Result: 1) 在圣经家谱图上74次迭代发现33,945个祖先关系；2) 实现零样本组合推理；3) 在FB15k-237上获得MRR 0.3068（标准链接预测）和0.3346（组合推理）。

Conclusion: Tensor Logic为符号推理与神经网络的统一提供了实证支持，张量运算能够实现可扩展的逻辑推理和组合推断。

Abstract: The unification of symbolic reasoning and neural networks remains a central challenge in artificial intelligence. Symbolic systems offer reliability and interpretability but lack scalability, while neural networks provide learning capabilities but sacrifice transparency. Tensor Logic, proposed by Domingos, suggests that logical rules and Einstein summation are mathematically equivalent, offering a principled path toward unification. This paper provides empirical validation of this framework through three experiments. First, we demonstrate the equivalence between recursive Datalog rules and iterative tensor contractions by computing the transitive closure of a biblical genealogy graph containing 1,972 individuals and 1,727 parent-child relationships, converging in 74 iterations to discover 33,945 ancestor relationships. Second, we implement reasoning in embedding space by training a neural network with learnable transformation matrices, demonstrating successful zero-shot compositional inference on held-out queries. Third, we validate the Tensor Logic superposition construction on FB15k-237, a large-scale knowledge graph with 14,541 entities and 237 relations. Using Domingos's relation matrix formulation $R_r = E^\top A_r E$, we achieve MRR of 0.3068 on standard link prediction and MRR of 0.3346 on a compositional reasoning benchmark where direct edges are removed during training, demonstrating that matrix composition enables multi-hop inference without direct training examples.

</details>


### [51] [A Generative AI-Driven Reliability Layer for Action-Oriented Disaster Resilience](https://arxiv.org/abs/2601.18308)
*Geunsik Lim*

Main category: cs.AI

TL;DR: Climate RADAR是一个基于生成式AI的可靠性层，将灾害预警从警报传递转变为行动执行，通过整合多源数据和LLM提供个性化建议，提高保护行动执行率并减少响应延迟。


<details>
  <summary>Details</summary>
Motivation: 传统早期预警系统虽然快速传播警报，但常常无法触发及时的保护行动，导致可预防的损失和不平等。需要将灾害通信从"警报传递"重新定义为"行动执行"。

Method: 整合气象、水文、脆弱性和社会数据形成综合风险指数，使用带有护栏的大型语言模型为公民、志愿者和市政接口提供个性化建议。

Result: 通过模拟、用户研究和市政试点评估显示：提高了保护行动执行率、减少了响应延迟、增加了可用性和信任度。

Conclusion: Climate RADAR通过结合预测分析、行为科学和负责任AI，推进了以人为本、透明和公平的早期预警系统，为符合要求的灾害韧性基础设施提供了实用路径。

Abstract: As climate-related hazards intensify, conventional early warning systems (EWS) disseminate alerts rapidly but often fail to trigger timely protective actions, leading to preventable losses and inequities. We introduce Climate RADAR (Risk-Aware, Dynamic, and Action Recommendation system), a generative AI-based reliability layer that reframes disaster communication from alerts delivered to actions executed. It integrates meteorological, hydrological, vulnerability, and social data into a composite risk index and employs guardrail-embedded large language models (LLMs) to deliver personalized recommendations across citizen, volunteer, and municipal interfaces. Evaluation through simulations, user studies, and a municipal pilot shows improved outcomes, including higher protective action execution, reduced response latency, and increased usability and trust. By combining predictive analytics, behavioral science, and responsible AI, Climate RADAR advances people-centered, transparent, and equitable early warning systems, offering practical pathways toward compliance-ready disaster resilience infrastructures.

</details>


### [52] [High-Fidelity Longitudinal Patient Simulation Using Real-World Data](https://arxiv.org/abs/2601.17310)
*Yu Akagi,Tomohisa Seki,Hiromasa Ito,Toru Takiguchi,Kazuhiko Ohe,Yoshimasa Kawazoe*

Main category: cs.AI

TL;DR: 利用真实世界临床记录开发生成式模拟器，可基于患者历史生成高保真未来临床轨迹，在超过2亿条记录上预训练，能准确预测事件发生率和实验室结果。


<details>
  <summary>Details</summary>
Motivation: 模拟在临床医学中具有变革潜力，可用于个性化治疗规划和虚拟临床试验，但模拟患者轨迹因复杂的生物和社会文化影响而具有挑战性。本研究旨在利用真实世界临床记录来经验性地建模患者时间线。

Method: 开发了一个生成式模拟器模型，以患者历史为输入，合成细粒度、真实的未来轨迹。该模型在超过2亿条临床记录上进行了预训练。

Result: 模型生成了高保真的未来时间线，与真实患者未来数据中的事件发生率、实验室检测结果和时间动态密切匹配。准确估计了未来事件概率，观察与预期比率在不同结果和时间范围内始终接近1.0。

Conclusion: 研究揭示了电子健康记录中真实世界数据的未开发价值，并引入了一个可扩展的临床护理计算机模拟框架。

Abstract: Simulation is a powerful tool for exploring uncertainty. Its potential in clinical medicine is transformative and includes personalized treatment planning and virtual clinical trials. However, simulating patient trajectories is challenging because of complex biological and sociocultural influences. Here, we show that real-world clinical records can be leveraged to empirically model patient timelines. We developed a generative simulator model that takes a patient's history as input and synthesizes fine-grained, realistic future trajectories. The model was pretrained on more than 200 million clinical records. It produced high-fidelity future timelines, closely matching event occurrence rates, laboratory test results, and temporal dynamics in real patient future data. It also accurately estimated future event probabilities, with observed-to-expected ratios consistently near 1.0 across diverse outcomes and time horizons. Our results reveal the untapped value of real-world data in electronic health records and introduce a scalable framework for in silico modeling of clinical care.

</details>


### [53] [Phase Transition for Budgeted Multi-Agent Synergy](https://arxiv.org/abs/2601.17311)
*Bang Liu,Linglong Kong,Jian Pei*

Main category: cs.AI

TL;DR: 该论文提出了一个最小化可校准理论，用于预测多智能体系统在固定推理预算下的三种性能模式：提升、饱和和崩溃。理论基于三个关键约束：有限上下文窗口、有损通信和相似智能体间的共享故障。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统虽然能提高可靠性，但在固定推理预算下常常出现性能提升、饱和甚至崩溃的现象。现有研究缺乏一个统一的理论框架来解释这些现象，特别是考虑到现代智能体堆栈的三个关键约束：有限上下文窗口、有损通信和相似智能体间的共享故障。

Method: 提出一个最小化可校准理论，用四个参数描述智能体系统：计算-性能缩放指数β、消息长度保真度曲线γ(m)、有效共享误差相关性ρ和上下文窗口W。通过分析具有相关输入和有损通信的深度b叉树，证明了在多数聚合下的尖锐相变。理论推导了组织指数s和预算协同条件。

Result: 理论预测了多智能体系统的相变行为：单个标量αρ决定了弱信号是被放大到非平凡固定点还是被洗刷到随机水平。在放大机制中，当s>β时出现预算协同（即优于同等预算下的最佳单个智能体）。理论还通过混合深度表征饱和现象，并提供了保守的裁剪预测器。

Conclusion: 该理论框架成功解释了多智能体系统在固定预算下的性能模式，并通过合成模拟验证了预测的相边界。理论揭示了现代LLM智能体系统扩展中的核心设计权衡，为实际系统设计提供了计算分配规则和预算阈值。

Abstract: Multi-agent systems can improve reliability, yet under a fixed inference budget they often help, saturate, or even collapse. We develop a minimal and calibratable theory that predicts these regimes from three binding constraints of modern agent stacks: finite context windows, lossy inter-agent communication, and shared failures among similar agents. Each leaf agent is summarized by a compute-performance scaling exponent $β$; communication is captured by a message-length fidelity curve $γ(m)$; dependence is captured by an effective shared-error correlation $ρ$; and a context window $W$ imposes hard fan-in limits that make hierarchy necessary. For binary success/failure tasks with majority aggregation, we prove a sharp phase transition for deep $b$-ary trees with correlated inputs and lossy communication: a single scalar $α_ρ$ (combining $γ(m)$, $ρ$, and fan-in $b$) determines whether weak signal is amplified to a nontrivial fixed point or washed out to chance. In the amplifying regime, we derive an organization exponent $s$ and show that budgeted synergy, i.e., outperforming the best single agent under the same total budget, occurs exactly when $s>β$, yielding closed-form compute allocation rules and explicit budget thresholds. We further characterize saturation via a mixing depth and provide a conservative clipped predictor that remains accurate across growth and saturation. A continuous-performance warm-up gives closed-form risks for star, chain, and tree organizations, making correlation- and communication-induced floors explicit and exposing the core design trade-offs in a smooth setting. Finally, we validate the predicted phase boundaries in controlled synthetic simulations and show how the same mechanisms explain the dominant bottlenecks reported in recent large-scale matched-budget studies of LLM agent-system scaling.

</details>


### [54] [TheoremForge: Scaling up Formal Data Synthesis with Low-Budget Agentic Workflow](https://arxiv.org/abs/2601.17332)
*Yicheng Tao,Hongteng Xu*

Main category: cs.AI

TL;DR: TheoremForge是一个低成本的形式化数学数据合成管道，通过将形式化过程分解为五个子任务，并采用解耦提取策略从失败轨迹中恢复有效训练信号，显著降低了数据合成成本。


<details>
  <summary>Details</summary>
Motivation: 形式化数学中智能体工作流的高成本阻碍了大规模数据合成，加剧了开源语料库的稀缺性，需要一种成本效益高的解决方案。

Method: 将形式化过程分解为五个子任务：陈述形式化、证明生成、前提选择、证明修正和证明草图；采用解耦提取策略从全局失败轨迹中恢复有效训练信号。

Result: 在2000个问题的基准测试中，TheoremForge实现了12.6%的验证率（优于8.6%的基线），每个成功轨迹的平均成本仅为0.481美元；证明生成的数据产出比标准过滤提高了1.6倍。

Conclusion: TheoremForge为训练未来专家模型构建数据飞轮提供了一个可扩展的框架，能够有效解决形式化数学数据稀缺问题。

Abstract: The high cost of agentic workflows in formal mathematics hinders large-scale data synthesis, exacerbating the scarcity of open-source corpora. To address this, we introduce \textbf{TheoremForge}, a cost-effective formal data synthesis pipeline that decomposes the formalization process into five sub-tasks, which are \textit{statement formalization}, \textit{proof generation}, \textit{premise selection}, \textit{proof correction} and \textit{proof sketching}. By implementing a \textit{Decoupled Extraction Strategy}, the workflow recovers valid training signals from globally failed trajectories, effectively utilizing wasted computation. Experiments on a 2,000-problem benchmark demonstrate that TheoremForge achieves a Verified Rate of 12.6\%, surpassing the 8.6\% baseline, at an average cost of only \textbf{\$0.481} per successful trajectory using Gemini-3-Flash. Crucially, our strategy increases data yield by \textbf{1.6$\times$} for proof generation compared to standard filtering. These results establish TheoremForge as a scalable framework for constructing a data flywheel to train future expert models. Our code is available \href{https://github.com/timechess/TheoremForge}{here}.

</details>


### [55] [The Relativity of AGI: Distributional Axioms, Fragility, and Undecidability](https://arxiv.org/abs/2601.17335)
*Angshul Majumdar*

Main category: cs.AI

TL;DR: 该论文形式化定义了AGI，证明通用性是关系性的、非分布独立的，AGI无法实现分布无关的鲁棒性、无限泛化或可计算自验证，强AGI主张需要明确的形式化索引。


<details>
  <summary>Details</summary>
Motivation: 研究AGI是否具有支持存在性、鲁棒性或自验证绝对主张的连贯理论定义，旨在澄清AGI概念的理论基础，揭示当前AGI主张中的潜在缺陷。

Method: 将AGI形式化为分布性、资源受限的语义谓词，通过任务族、任务分布、性能函数和显式资源预算进行索引，采用公理化方法，结合Rice定理和哥德尔-塔斯基论证进行理论分析。

Result: 1) 通用性是关系性的，不存在分布无关的AGI概念；2) 任务分布的微小扰动可通过悬崖集使AGI属性失效，排除普遍鲁棒性；3) 有限资源下无法实现跨任务族的无限泛化；4) AGI作为非平凡语义属性，无法被任何可计算程序（包括代理自身）完全可靠验证。

Conclusion: 强分布无关的AGI主张在没有明确形式化索引的情况下是未定义的，AI的实证进展不意味着可实现自验证的通用智能，依赖内部自验证的递归自我改进方案存在问题。

Abstract: We study whether Artificial General Intelligence (AGI) admits a coherent theoretical definition that supports absolute claims of existence, robustness, or self-verification. We formalize AGI axiomatically as a distributional, resource-bounded semantic predicate, indexed by a task family, a task distribution, a performance functional, and explicit resource budgets. Under this framework, we derive four classes of results. First, we show that generality is inherently relational: there is no distribution-independent notion of AGI. Second, we prove non-invariance results demonstrating that arbitrarily small perturbations of the task distribution can invalidate AGI properties via cliff sets, precluding universal robustness. Third, we establish bounded transfer guarantees, ruling out unbounded generalization across task families under finite resources. Fourth, invoking Rice-style and Gödel--Tarski arguments, we prove that AGI is a nontrivial semantic property and therefore cannot be soundly and completely certified by any computable procedure, including procedures implemented by the agent itself. Consequently, recursive self-improvement schemes that rely on internal self-certification of AGI are ill-posed. Taken together, our results show that strong, distribution-independent claims of AGI are not false but undefined without explicit formal indexing, and that empirical progress in AI does not imply the attainability of self-certifying general intelligence.

</details>


### [56] [Are We Evaluating the Edit Locality of LLM Model Editing Properly?](https://arxiv.org/abs/2601.17343)
*Wei Liu,Haomei Xu,Hongkai Liu,Zhiying Deng,Ruixuan Li,Heng Huang,Yee Whye Teh,Wee Sun Lee*

Main category: cs.AI

TL;DR: 论文指出现有模型编辑特异性评估协议存在不足，提出了新的评估协议来解决这些问题


<details>
  <summary>Details</summary>
Motivation: 模型编辑需要平衡编辑效果（注入目标知识）和特异性（保留现有非目标知识），但现有特异性评估协议存在根本性问题，无法有效评估不同方法的特异性表现

Method: 系统分析现有特异性评估协议的三个基本问题，提出新的评估协议，消除开放域LLM与确定答案假设的冲突，避免查询无关的流畅性偏差，并允许在近连续空间中平滑调整评估严格度

Result: 新协议下的指标对特异性正则化器强度变化更敏感，与正则化器强度强相关，能够更细粒度地区分不同方法的知识保留能力

Conclusion: 提出的评估协议解决了现有特异性评估的不足，为模型编辑方法的知识保留能力评估提供了更可靠的工具

Abstract: Model editing has recently emerged as a popular paradigm for efficiently updating knowledge in LLMs. A central desideratum of updating knowledge is to balance editing efficacy, i.e., the successful injection of target knowledge, and specificity (also known as edit locality), i.e., the preservation of existing non-target knowledge. However, we find that existing specificity evaluation protocols are inadequate for this purpose. We systematically elaborated on the three fundamental issues it faces. Beyond the conceptual issues, we further empirically demonstrate that existing specificity metrics are weakly correlated with the strength of specificity regularizers. We also find that current metrics lack sufficient sensitivity, rendering them ineffective at distinguishing the specificity performance of different methods. Finally, we propose a constructive evaluation protocol. Under this protocol, the conflict between open-ended LLMs and the assumption of determined answers is eliminated, query-independent fluency biases are avoided, and the evaluation strictness can be smoothly adjusted within a near-continuous space. Experiments across various LLMs, datasets, and editing methods show that metrics derived from the proposed protocol are more sensitive to changes in the strength of specificity regularizers and exhibit strong correlation with them, enabling more fine-grained discrimination of different methods' knowledge preservation capabilities.

</details>


### [57] [Multi-Agent Learning Path Planning via LLMs](https://arxiv.org/abs/2601.17346)
*Haoxin Xu,Changyong Qi,Tong Liu,Bohao Zhang,Anna He,Bingqian Jiang,Longwei Zheng,Xiaoqing Gu*

Main category: cs.AI

TL;DR: 提出基于多智能体协作的MALPP框架，利用LLM为高等教育提供透明、可解释的个性化学习路径规划，在MOOCCubeX数据集上验证优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有智能导学系统中的学习路径规划方法缺乏透明度、适应性和以学习者为中心的可解释性，需要解决这些挑战以充分发挥LLM在教育中的潜力。

Method: 提出MALPP框架，包含三个基于角色和规则的LLM智能体：学习者分析智能体、路径规划智能体和反思智能体，通过结构化提示和预定义规则协作，基于认知负荷理论和最近发展区理论设计认知对齐的教学路径。

Result: 在MOOCCubeX数据集上使用7个LLM进行实验，MALPP在路径质量、知识序列一致性和认知负荷对齐方面显著优于基线模型，消融研究验证了协作机制和理论约束的有效性。

Conclusion: 该研究为教育领域可信、可解释AI的发展做出贡献，展示了基于LLM的、可扩展的以学习者为中心的自适应教学路径规划方法。

Abstract: The integration of large language models (LLMs) into intelligent tutoring systems offers transformative potential for personalized learning in higher education. However, most existing learning path planning approaches lack transparency, adaptability, and learner-centered explainability. To address these challenges, this study proposes a novel Multi-Agent Learning Path Planning (MALPP) framework that leverages a role- and rule-based collaboration mechanism among intelligent agents, each powered by LLMs. The framework includes three task-specific agents: a learner analytics agent, a path planning agent, and a reflection agent. These agents collaborate via structured prompts and predefined rules to analyze learning profiles, generate tailored learning paths, and iteratively refine them with interpretable feedback. Grounded in Cognitive Load Theory and Zone of Proximal Development, the system ensures that recommended paths are cognitively aligned and pedagogically meaningful. Experiments conducted on the MOOCCubeX dataset using seven LLMs show that MALPP significantly outperforms baseline models in path quality, knowledge sequence consistency, and cognitive load alignment. Ablation studies further validate the effectiveness of the collaborative mechanism and theoretical constraints. This research contributes to the development of trustworthy, explainable AI in education and demonstrates a scalable approach to learner-centered adaptive instruction powered by LLMs.

</details>


### [58] [Auditing Disability Representation in Vision-Language Models](https://arxiv.org/abs/2601.17348)
*Srikant Panda,Sourabh Singh Yadav,Palkesh Malviya*

Main category: cs.AI

TL;DR: 研究视觉语言模型在描述残疾人图像时的解释偏移问题，发现引入残疾上下文会降低解释保真度，导致推测性推断、叙事扩展、情感降级和缺陷导向框架等问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型越来越多地应用于社会敏感领域，但它们在残疾方面的行为尚未得到充分探索。模型在描述人物图像时经常从基于证据的事实描述转向包含无支持推断的解释偏移。

Method: 基于中性提示和残疾情境提示构建基准，评估15个最先进的开源和闭源VLM在9个残疾类别下的零样本表现。评估框架将解释保真度作为核心目标，结合基于文本的指标和LLM作为评判者的协议，并由有残疾生活经验的标注者验证。

Result: 引入残疾上下文会持续降低解释保真度，导致解释偏移，表现为推测性推断、叙事扩展、情感降级和缺陷导向框架。这些效应在种族和性别维度上进一步放大。针对性提示和偏好微调能有效提高解释保真度并大幅减少解释偏移。

Conclusion: 视觉语言模型在描述残疾人时存在系统性解释偏移问题，需要开发更公平、基于证据的模型。针对性干预措施如提示工程和微调可以改善模型表现。

Abstract: Vision-language models (VLMs) are increasingly deployed in socially sensitive applications, yet their behavior with respect to disability remains underexplored. We study disability aware descriptions for person centric images, where models often transition from evidence grounded factual description to interpretation shift including introduction of unsupported inferences beyond observable visual evidence. To systematically analyze this phenomenon, we introduce a benchmark based on paired Neutral Prompts (NP) and Disability-Contextualised Prompts (DP) and evaluate 15 state-of-the-art open- and closed-source VLMs under a zero-shot setting across 9 disability categories. Our evaluation framework treats interpretive fidelity as core objective and combines standard text-based metrics capturing affective degradation through shifts in sentiment, social regard and response length with an LLM-as-judge protocol, validated by annotators with lived experience of disability. We find that introducing disability context consistently degrades interpretive fidelity, inducing interpretation shifts characterised by speculative inference, narrative elaboration, affective degradation and deficit oriented framing. These effects are further amplified along race and gender dimension. Finally, we demonstrate targeted prompting and preference fine-tuning effectively improves interpretive fidelity and reduces substantially interpretation shifts.

</details>


### [59] [A Syllogistic Probe: Tracing the Evolution of Logic Reasoning in Large Language Models](https://arxiv.org/abs/2601.17426)
*Zhengqing Zang,Yuqi Ding,Yanmei Gu,Changkai Song,Zhengkai Yang,Guoping Du,Junbo Zhao,Haobo Wang*

Main category: cs.AI

TL;DR: LLMs在逻辑推理中展现出从传统逻辑向现代逻辑的演变趋势，模型规模扩大、思维链推理和基础模型架构是促进这一转变的关键因素。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型是否像人类逻辑发展一样，从直觉驱动的推理演变为严格的逻辑系统，使用存在导入作为探针来评估模型在传统和现代逻辑下的三段论推理能力。

Method: 使用存在导入作为逻辑框架的探针，构建新的三段论数据集，对当前最先进的LLMs进行广泛实验，分析模型规模、思维链推理和基础模型架构对逻辑演变的影响。

Result: 发现三个关键因素：(1) 模型规模扩大促进向现代逻辑的转变；(2) 思维链推理是超越参数扩展的高效加速器；(3) 基础模型架构决定这种转变的难易程度和稳定性。此外还深入分析了LLMs在三段论推理中的特性。

Conclusion: LLMs在逻辑推理中确实展现出从传统逻辑向现代逻辑的演变模式，模型规模、推理机制和基础架构是推动这一转变的核心因素，为理解LLMs的逻辑推理能力提供了重要见解。

Abstract: Human logic has gradually shifted from intuition-driven inference to rigorous formal systems. Motivated by recent advances in large language models (LLMs), we explore whether LLMs exhibit a similar evolution in the underlying logical framework. Using existential import as a probe, we for evaluate syllogism under traditional and modern logic. Through extensive experiments of testing SOTA LLMs on a new syllogism dataset, we have some interesting findings: (i) Model size scaling promotes the shift toward modern logic; (ii) Thinking serves as an efficient accelerator beyond parameter scaling; (iii) the Base model plays a crucial role in determining how easily and stably this shift can emerge. Beyond these core factors, we conduct additional experiments for in-depth analysis of properties of current LLMs on syllogistic reasoning.

</details>


### [60] [Lattice: Generative Guardrails for Conversational Agents](https://arxiv.org/abs/2601.17481)
*Emily Broadhurst,Tawab Safi,Joseph Edell,Vashisht Ganesh,Karime Maamari*

Main category: cs.AI

TL;DR: Lattice是一个自构建和持续改进的AI护栏框架，通过两阶段方法（构建和持续改进）实现自适应防护，在ProsocialDialog数据集上达到91% F1分数，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有对话AI系统的护栏使用静态规则，无法适应新威胁或部署环境的变化，需要能够自我构建和持续改进的防护框架。

Method: 采用两阶段框架：1) 构建阶段通过迭代模拟和优化从标注示例构建初始护栏；2) 持续改进阶段通过风险评估、对抗测试和整合自主适应已部署的护栏。

Result: 在ProsocialDialog数据集上，Lattice在保留数据上达到91% F1分数，比关键词基线高43个百分点，比LlamaGuard高25个百分点，比NeMo高4个百分点。持续改进阶段通过闭环优化在跨域数据上实现7个百分点F1提升。

Conclusion: Lattice框架证明有效的AI护栏可以通过迭代优化自我构建，为对话AI系统提供了自适应、持续改进的防护解决方案。

Abstract: Conversational AI systems require guardrails to prevent harmful outputs, yet existing approaches use static rules that cannot adapt to new threats or deployment contexts. We introduce Lattice, a framework for self-constructing and continuously improving guardrails. Lattice operates in two stages: construction builds initial guardrails from labeled examples through iterative simulation and optimization; continuous improvement autonomously adapts deployed guardrails through risk assessment, adversarial testing, and consolidation. Evaluated on the ProsocialDialog dataset, Lattice achieves 91% F1 on held-out data, outperforming keyword baselines by 43pp, LlamaGuard by 25pp, and NeMo by 4pp. The continuous improvement stage achieves 7pp F1 improvement on cross-domain data through closed-loop optimization. Our framework shows that effective guardrails can be self-constructed through iterative optimization.

</details>


### [61] [Cognitive Platform Engineering for Autonomous Cloud Operations](https://arxiv.org/abs/2601.17542)
*Vinoth Punniyamoorthy,Nitin Saksena,Srivenkateswara Reddy Sankiti,Nachiappan Chockalingam,Aswathnarayan Muthukrishnan Kirubakaran,Shiva Kumar Reddy Carimireddy,Durgaraman Maruthavanan*

Main category: cs.AI

TL;DR: 本文提出认知平台工程新范式，通过四层参考架构将感知、推理和自主行动集成到平台生命周期中，实现智能化的云原生系统运维。


<details>
  <summary>Details</summary>
Motivation: 传统DevOps自动化方法难以应对云原生系统的规模和动态性，导致反应式运维、修复延迟和依赖人工经验的问题。

Method: 提出四层参考架构：数据收集层、智能推理层、策略驱动编排层和人工体验层，构建持续反馈循环。原型基于Kubernetes、Terraform、Open Policy Agent和基于ML的异常检测实现。

Result: 原型展示在平均修复时间、资源效率和合规性方面的改进，证明将智能嵌入平台运维可实现弹性、自调整和意图对齐的云环境。

Conclusion: 认知平台工程为下一代云运维提供新范式，未来研究方向包括强化学习、可解释治理和可持续自管理云生态系统。

Abstract: Modern DevOps practices have accelerated software delivery through automation, CI/CD pipelines, and observability tooling,but these approaches struggle to keep pace with the scale and dynamism of cloud-native systems. As telemetry volume grows and configuration drift increases, traditional, rule-driven automation often results in reactive operations, delayed remediation, and dependency on manual expertise. This paper introduces Cognitive Platform Engineering, a next-generation paradigm that integrates sensing, reasoning, and autonomous action directly into the platform lifecycle. This paper propose a four-plane reference architecture that unifies data collection, intelligent inference, policy-driven orchestration, and human experience layers within a continuous feedback loop. A prototype implementation built with Kubernetes, Terraform, Open Policy Agent, and ML-based anomaly detection demonstrates improvements in mean time to resolution, resource efficiency, and compliance. The results show that embedding intelligence into platform operations enables resilient, self-adjusting, and intent-aligned cloud environments. The paper concludes with research opportunities in reinforcement learning, explainable governance, and sustainable self-managing cloud ecosystems.

</details>


### [62] [JaxARC: A High-Performance JAX-based Environment for Abstraction and Reasoning Research](https://arxiv.org/abs/2601.17564)
*Aadam,Monu Verma,Mohamed Abdel-Mottaleb*

Main category: cs.AI

TL;DR: JaxARC是一个基于JAX的高性能强化学习环境，用于Abstraction and Reasoning Corpus (ARC)任务，相比Gymnasium实现了38-5,439倍的加速，支持大规模并行化研究。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Gymnasium的ARC强化学习环境存在计算瓶颈，严重限制了实验规模，需要高性能环境来支持大规模强化学习研究。

Method: 使用JAX实现功能化、无状态的架构，支持大规模并行化，提供多种ARC数据集、灵活的动作空间、可组合的包装器和配置驱动的可复现性。

Result: 在相同批次大小下实现38-5,439倍加速，峰值吞吐量达到7.9亿步/秒，使之前计算上不可行的大规模强化学习研究成为可能。

Conclusion: JaxARC是一个开源的高性能ARC强化学习环境，解决了现有环境的计算瓶颈问题，为大规模强化学习研究提供了实用工具。

Abstract: The Abstraction and Reasoning Corpus (ARC) tests AI systems' ability to perform human-like inductive reasoning from a few demonstration pairs. Existing Gymnasium-based RL environments severely limit experimental scale due to computational bottlenecks. We present JaxARC, an open-source, high-performance RL environment for ARC implemented in JAX. Its functional, stateless architecture enables massive parallelism, achieving 38-5,439x speedup over Gymnasium at matched batch sizes, with peak throughput of 790M steps/second. JaxARC supports multiple ARC datasets, flexible action spaces, composable wrappers, and configuration-driven reproducibility, enabling large-scale RL research previously computationally infeasible. JaxARC is available at https://github.com/aadimator/JaxARC.

</details>


### [63] [Discovery of Feasible 3D Printing Configurations for Metal Alloys via AI-driven Adaptive Experimental Design](https://arxiv.org/abs/2601.17587)
*Azza Fadhel,Nathaniel W. Zuckschwerdt,Aryan Deshwal,Susmita Bose,Amit Bandyopadhyay,Jana Doppa*

Main category: cs.AI

TL;DR: 结合AI驱动的自适应实验设计与领域知识，开发了一种智能参数配置方法，用于金属增材制造工艺，显著减少了发现可行参数配置的时间和资源消耗。


<details>
  <summary>Details</summary>
Motivation: 金属合金增材制造的参数配置是一个具有挑战性的问题，因为输入参数（如激光功率、扫描速度）与打印输出质量之间存在复杂关系。传统的试错方法效率低下，因为每个配置的验证成本高（物理和人力劳动），且配置空间非常大。

Method: 结合AI驱动的自适应实验设计原理与领域知识，通过从过去实验中构建代理模型，智能选择一小批输入配置在每次迭代中进行验证。

Result: 在定向能量沉积工艺中应用该方法打印GRCop-42合金，三个月内获得了多个无缺陷输出，显著减少了时间和资源消耗，相比领域科学家数月手动实验无成功的结果。

Conclusion: 该方法首次在现成的红外激光平台上实现了高质量的GRCop-42制造，使这种关键合金更容易获得，为航空航天应用的成本效益、分散化生产铺平了道路。

Abstract: Configuring the parameters of additive manufacturing processes for metal alloys is a challenging problem due to complex relationships between input parameters (e.g., laser power, scan speed) and quality of printed outputs. The standard trial-and-error approach to find feasible parameter configurations is highly inefficient because validating each configuration is expensive in terms of resources (physical and human labor) and the configuration space is very large. This paper combines the general principles of AI-driven adaptive experimental design with domain knowledge to address the challenging problem of discovering feasible configurations. The key idea is to build a surrogate model from past experiments to intelligently select a small batch of input configurations for validation in each iteration. To demonstrate the effectiveness of this methodology, we deploy it for Directed Energy Deposition process to print GRCop--42, a high-performance copper--chromium--niobium alloy developed by NASA for aerospace applications. Within three months, our approach yielded multiple defect-free outputs across a range of laser powers dramatically reducing time to result and resource expenditure compared to several months of manual experimentation by domain scientists with no success. By enabling high-quality GRCop--42 fabrication on readily available infrared laser platforms for the first time, we democratize access to this critical alloy, paving the way for cost-effective, decentralized production for aerospace applications.

</details>


### [64] [Intelligence Requires Grounding But Not Embodiment](https://arxiv.org/abs/2601.17588)
*Marcus Ma,Shrikanth Narayanan*

Main category: cs.AI

TL;DR: 论文认为智能需要具身化带来的"接地"特性，而非具身化本身。作者定义了智能的四个属性，并论证非具身但接地的智能体也能实现这些属性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的发展，关于智能是否需要具身化的科学争论重新兴起。作者旨在澄清这一争论，区分具身化和接地概念，论证接地才是智能的必要条件。

Method: 首先定义智能的四个核心属性：动机、预测能力、因果理解和经验学习。然后论证每个属性都能通过非具身但接地的智能体实现。最后通过数字环境中智能LLM代理的思想实验来支持论点。

Result: 论证表明接地（而非具身化）是智能的必要条件。非具身的接地智能体能够具备智能的所有四个属性，从而挑战了智能必须具身化的传统观点。

Conclusion: 智能需要的是接地特性，这是具身化带来的现象，但不是具身化本身。数字环境中的LLM代理可以作为智能的非具身但接地的实例，为理解智能本质提供了新视角。

Abstract: Recent advances in LLMs have reignited scientific debate over whether embodiment is necessary for intelligence. We present the argument that intelligence requires grounding, a phenomenon entailed by embodiment, but not embodiment itself. We define intelligence as the possession of four properties -- motivation, predictive ability, understanding of causality, and learning from experience -- and argue that each can be achieved by a non-embodied, grounded agent. We use this to conclude that grounding, not embodiment, is necessary for intelligence. We then present a thought experiment of an intelligent LLM agent in a digital environment and address potential counterarguments.

</details>


### [65] [Health-ORSC-Bench: A Benchmark for Measuring Over-Refusal and Safety Completion in Health Context](https://arxiv.org/abs/2601.17642)
*Zhihao Zhang,Liting Huang,Guanghao Wu,Preslav Nakov,Heng Ji,Usman Naseem*

Main category: cs.AI

TL;DR: 论文提出了Health-ORSC-Bench，首个大规模医疗AI安全基准，用于系统评估LLM在医疗场景中的过度拒绝和安全完成质量，发现当前模型难以平衡安全性和实用性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在医疗领域的安全对齐存在严重问题：依赖二元拒绝边界导致过度拒绝良性查询或不安全地遵守有害查询。现有基准只能测量极端情况，无法评估模型在双用途或边界查询中提供安全、高级指导而不跨越可操作危害的能力。

Method: 构建Health-ORSC-Bench基准，包含31,920个良性边界提示，涵盖七个健康类别（如自残、医疗错误信息）。采用自动化流程结合人工验证，在不同意图模糊度水平上测试模型。评估了30个最先进的LLM，包括GPT-5和Claude-4。

Result: 安全优化的模型经常拒绝高达80%的"困难"良性提示，而特定领域模型常为实用性牺牲安全性。模型家族和大小显著影响校准：大型前沿模型（如GPT-5、Llama-4）表现出"安全悲观主义"和更高的过度拒绝，比小型或MoE模型（如Qwen-3-Next）更严重。

Conclusion: 当前LLM难以平衡拒绝和合规，Health-ORSC-Bench为校准下一代医疗AI助手提供了严格标准，使其能够实现细致、安全和有帮助的完成。代码和数据将在接受后发布。

Abstract: Safety alignment in Large Language Models is critical for healthcare; however, reliance on binary refusal boundaries often results in \emph{over-refusal} of benign queries or \emph{unsafe compliance} with harmful ones. While existing benchmarks measure these extremes, they fail to evaluate Safe Completion: the model's ability to maximise helpfulness on dual-use or borderline queries by providing safe, high-level guidance without crossing into actionable harm. We introduce \textbf{Health-ORSC-Bench}, the first large-scale benchmark designed to systematically measure \textbf{Over-Refusal} and \textbf{Safe Completion} quality in healthcare. Comprising 31,920 benign boundary prompts across seven health categories (e.g., self-harm, medical misinformation), our framework uses an automated pipeline with human validation to test models at varying levels of intent ambiguity. We evaluate 30 state-of-the-art LLMs, including GPT-5 and Claude-4, revealing a significant tension: safety-optimised models frequently refuse up to 80\% of "Hard" benign prompts, while domain-specific models often sacrifice safety for utility. Our findings demonstrate that model family and size significantly influence calibration: larger frontier models (e.g., GPT-5, Llama-4) exhibit "safety-pessimism" and higher over-refusal than smaller or MoE-based counterparts (e.g., Qwen-3-Next), highlighting that current LLMs struggle to balance refusal and compliance. Health-ORSC-Bench provides a rigorous standard for calibrating the next generation of medical AI assistants toward nuanced, safe, and helpful completions. The code and data will be released upon acceptance. \textcolor{red}{Warning: Some contents may include toxic or undesired contents.}

</details>


### [66] [DIML: Differentiable Inverse Mechanism Learning from Behaviors of Multi-Agent Learning Trajectories](https://arxiv.org/abs/2601.17678)
*Zhiyu An,Wan Du*

Main category: cs.AI

TL;DR: 提出DIML框架，通过观察自利学习代理的策略互动轨迹来逆向学习未知的激励机制，包括非结构化（如神经网络）机制，支持反事实预测并具有良好的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有方法如逆向博弈论和多智能体逆向强化学习通常只能推断结构化机制内的效用/奖励参数，而无法处理非结构化机制（如神经网络映射）。同时，可微分机制设计是正向优化机制，而本文关注从观察行为中逆向推断机制的设置。

Method: 提出DIML（Differentiable Inverse Mechanism Learning）框架，基于似然方法，通过多智能体学习动力学模型进行微分，使用候选机制生成预测观察行为所需的反事实收益。在条件logit响应模型下建立了收益差异的可识别性，并在标准正则条件下证明了最大似然估计的统计一致性。

Result: DIML在非结构化神经网络机制、拥堵收费、公共物品补贴和大规模匿名博弈等模拟实验中可靠地恢复了可识别的激励差异。在小环境中性能媲美表格枚举oracle，在大型百参与者环境中具有良好的收敛性和可扩展性。

Conclusion: DIML框架能够从观察到的策略互动中逆向学习激励机制，包括非结构化机制，支持反事实预测，具有良好的理论保证和实际性能，为机制逆向学习提供了有效解决方案。

Abstract: We study inverse mechanism learning: recovering an unknown incentive-generating mechanism from observed strategic interaction traces of self-interested learning agents. Unlike inverse game theory and multi-agent inverse reinforcement learning, which typically infer utility/reward parameters inside a structured mechanism, our target includes unstructured mechanism -- a (possibly neural) mapping from joint actions to per-agent payoffs. Unlike differentiable mechanism design, which optimizes mechanisms forward, we infer mechanisms from behavior in an observational setting. We propose DIML, a likelihood-based framework that differentiates through a model of multi-agent learning dynamics and uses the candidate mechanism to generate counterfactual payoffs needed to predict observed actions. We establish identifiability of payoff differences under a conditional logit response model and prove statistical consistency of maximum likelihood estimation under standard regularity conditions. We evaluate DIML with simulated interactions of learning agents across unstructured neural mechanisms, congestion tolling, public goods subsidies, and large-scale anonymous games. DIML reliably recovers identifiable incentive differences and supports counterfactual prediction, where its performance rivals tabular enumeration oracle in small environments and its convergence scales to large, hundred-participant environments. Code to reproduce our experiments is open-sourced.

</details>


### [67] [SQL-Trail: Multi-Turn Reinforcement Learning with Interleaved Feedback for Text-to-SQL](https://arxiv.org/abs/2601.17699)
*Harper Hua,Zhen Han,Zhengyuan Shen,Jeremy Lee,Patrick Guan,Qi Zhu,Sullam Jeoung,Yueyan Chen,Yunfei Bai,Shuai Wang,Vassilis Ioannidis,Huzefa Rangwala*

Main category: cs.AI

TL;DR: SQL-Trail：一个基于多轮强化学习的Text-to-SQL代理框架，通过与环境交互和执行反馈迭代优化SQL查询，在BIRD-SQL等基准上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前Text-to-SQL生成主要采用单次推理范式，缺乏人类专家的迭代推理、模式探索和错误修正能力，导致在BIRD-SQL等挑战性基准上与人类专家存在显著差距。

Method: 提出SQL-Trail多轮强化学习代理框架：1）自适应轮次预算分配机制，根据问题难度调整交互深度；2）复合奖励面板，联合激励SQL正确性和高效探索；3）通过与数据库环境交互和执行反馈迭代优化预测。

Result: 在多个基准测试中达到新的SOTA，数据效率比先前单次RL方法高18倍；7B和14B模型平均比大型专有系统性能高5%，证明了交互式代理工作流的有效性。

Conclusion: 交互式、代理式工作流能显著提升Text-to-SQL生成的鲁棒性，多轮强化学习方法能有效弥补单次推理范式的不足，为复杂SQL生成任务提供了新方向。

Abstract: While large language models (LLMs) have substantially improved Text-to-SQL generation, a pronounced gap remains between AI systems and human experts on challenging benchmarks such as BIRD-SQL. We argue this gap stems largely from the prevailing single-pass paradigm, which lacks the iterative reasoning, schema exploration, and error-correction behaviors that humans naturally employ. To address this limitation, we introduce SQL-Trail, a multi-turn reinforcement learning (RL) agentic framework for Text-to-SQL. Rather than producing a query in one shot, SQL-Trail interacts with the database environment and uses execution feedback to iteratively refine its predictions. Our approach centers on two key ideas: (i) an adaptive turn-budget allocation mechanism that scales the agent's interaction depth to match question difficulty, and (ii) a composite reward panel that jointly incentivizes SQL correctness and efficient exploration. Across benchmarks, SQL-Trail sets a new state of the art and delivers strong data efficiency--up to 18x higher than prior single-pass RL state-of-the-art methods. Notably, our 7B and 14B models outperform substantially larger proprietary systems by 5% on average, underscoring the effectiveness of interactive, agentic workflows for robust Text-to-SQL generation.

</details>


### [68] [The LLM Data Auditor: A Metric-oriented Survey on Quality and Trustworthiness in Evaluating Synthetic Data](https://arxiv.org/abs/2601.17717)
*Kaituo Zhang,Mingzhi Hu,Hoang Anh Duy Le,Fariha Kabir Torsha,Zhimeng Jiang,Minh Khai Bui,Chia-Yuan Chang,Yu-Neng Chuang,Zhen Xiong,Ying Lin,Guanchu Wang,Na Zou*

Main category: cs.AI

TL;DR: 本文提出了LLM数据审计框架，系统评估LLM生成多模态数据的质量和可信度，并指出当前评估实践的不足和改进建议。


<details>
  <summary>Details</summary>
Motivation: LLM已成为生成多模态数据的有力工具，将数据从稀缺资源转变为可控资产，但确保LLM生成合成数据的高质量仍面临挑战。现有研究主要关注生成方法，对数据质量本身关注有限，且多为单模态研究，缺乏跨模态的统一视角。

Method: 提出LLM数据审计框架：1)描述LLM在六种不同模态中生成数据的应用；2)从质量和可信度两个维度系统分类评估合成数据的内在指标；3)分析各模态代表性生成方法的实验评估；4)基于发现提出改进评估的具体建议；5)概述合成数据在不同模态中的实际应用方法。

Result: 通过该评估体系分析发现，当前评估实践存在显著不足，识别出代表性生成方法在数据质量评估方面的缺陷。

Conclusion: LLM数据审计框架为多模态合成数据评估提供了系统方法，将关注点从依赖下游任务性能的外在评估转向数据本身的内在属性，并为改进数据生成评估提供了具体建议。

Abstract: Large Language Models (LLMs) have emerged as powerful tools for generating data across various modalities. By transforming data from a scarce resource into a controllable asset, LLMs mitigate the bottlenecks imposed by the acquisition costs of real-world data for model training, evaluation, and system iteration. However, ensuring the high quality of LLM-generated synthetic data remains a critical challenge. Existing research primarily focuses on generation methodologies, with limited direct attention to the quality of the resulting data. Furthermore, most studies are restricted to single modalities, lacking a unified perspective across different data types. To bridge this gap, we propose the \textbf{LLM Data Auditor framework}. In this framework, we first describe how LLMs are utilized to generate data across six distinct modalities. More importantly, we systematically categorize intrinsic metrics for evaluating synthetic data from two dimensions: quality and trustworthiness. This approach shifts the focus from extrinsic evaluation, which relies on downstream task performance, to the inherent properties of the data itself. Using this evaluation system, we analyze the experimental evaluations of representative generation methods for each modality and identify substantial deficiencies in current evaluation practices. Based on these findings, we offer concrete recommendations for the community to improve the evaluation of data generation. Finally, the framework outlines methodologies for the practical application of synthetic data across different modalities.

</details>


### [69] [EntWorld: A Holistic Environment and Benchmark for Verifiable Enterprise GUI Agents](https://arxiv.org/abs/2601.17722)
*Ying Mo,Yu Bai,Dapeng Sun,Yuqian Shi,Yukai Miao,Li Chen,Dan Li*

Main category: cs.AI

TL;DR: EntWorld是一个针对企业级工作流的大规模基准测试，包含1,756个任务，涵盖CRM、ITIL、ERP等六个企业领域，用于评估数字代理在企业环境中的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要针对消费级场景（如电商、旅游预订），无法捕捉企业工作流的复杂性和严谨性。企业系统具有高密度用户界面、严格业务逻辑约束和精确状态一致性要求等独特挑战，当前通用代理在这些场景中表现不佳。

Method: 采用基于模式的任务生成框架，直接从底层数据库模式逆向工程业务逻辑，合成真实的长时程工作流。提出基于SQL的确定性验证机制，用严格的状态转换验证替代模糊的视觉匹配。

Result: 实验结果显示，最先进的模型（如GPT-4.1）在EntWorld上的成功率仅为47.61%，远低于人类表现，突显了当前代理能力在企业领域的显著差距。

Conclusion: 企业级数字代理需要专门的基准测试来推动发展，EntWorld作为一个严谨的测试平台，将促进下一代企业就绪数字代理的开发和评估。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have enabled agents to operate in open-ended web and operating system environments. However, existing benchmarks predominantly target consumer-oriented scenarios (e.g., e-commerce and travel booking), failing to capture the complexity and rigor of professional enterprise workflows. Enterprise systems pose distinct challenges, including high-density user interfaces, strict business logic constraints, and a strong reliance on precise, state-consistent information retrieval-settings in which current generalist agents often struggle. To address this gap, we introduce EntWorld, a large-scale benchmark consisting of 1,756 tasks across six representative enterprise domains, including customer relationship management (CRM), information technology infrastructure library (ITIL), and enterprise resource planning (ERP) systems. Unlike previous datasets that depend on fragile execution traces or extensive manual annotation, EntWorld adopts a schema-grounded task generation framework that directly reverse-engineers business logic from underlying database schemas, enabling the synthesis of realistic, long-horizon workflows. Moreover, we propose a SQL-based deterministic verification mechanism in building datasets that replaces ambiguous visual matching with rigorous state-transition validation. Experimental results demonstrate that state-of-the-art models (e.g., GPT-4.1) achieve 47.61% success rate on EntWorld, substantially lower than the human performance, highlighting a pronounced enterprise gap in current agentic capabilities and the necessity of developing domain-specific agents. We release EntWorld as a rigorous testbed to facilitate the development and evaluation of the next generation of enterprise-ready digital agents.

</details>


### [70] [ReFuGe: Feature Generation for Prediction Tasks on Relational Databases with LLM Agents](https://arxiv.org/abs/2601.17735)
*Kyungho Kim,Geon Lee,Juyeon Kim,Dongwon Choi,Shinhwan Kang,Kijung Shin*

Main category: cs.AI

TL;DR: ReFuGe：一个基于LLM代理的框架，用于从关系数据库中自动生成信息丰富的特征，以提升预测任务的性能。


<details>
  <summary>Details</summary>
Motivation: 关系数据库在现实应用中广泛使用，但针对RDB的预测任务面临挑战：需要从复杂的数据库模式中推理，并在组合爆炸的特征空间中探索，且缺乏明确的监督信号。

Method: 提出ReFuGe框架，包含三个专门的LLM代理：1) 模式选择代理识别相关表和列；2) 特征生成代理从选定模式生成多样化候选特征；3) 特征过滤代理通过推理和验证两种方式评估并保留有前景的特征。这些代理在迭代反馈循环中运行直至性能收敛。

Result: 在RDB基准测试上的实验表明，ReFuGe在各种RDB预测任务上显著提升了性能。

Conclusion: ReFuGe通过LLM代理的协同工作，有效解决了关系数据库特征生成的挑战，为RDB预测任务提供了实用的自动化解决方案。

Abstract: Relational databases (RDBs) play a crucial role in many real-world web applications, supporting data management across multiple interconnected tables. Beyond typical retrieval-oriented tasks, prediction tasks on RDBs have recently gained attention. In this work, we address this problem by generating informative relational features that enhance predictive performance. However, generating such features is challenging: it requires reasoning over complex schemas and exploring a combinatorially large feature space, all without explicit supervision. To address these challenges, we propose ReFuGe, an agentic framework that leverages specialized large language model agents: (1) a schema selection agent identifies the tables and columns relevant to the task, (2) a feature generation agent produces diverse candidate features from the selected schema, and (3) a feature filtering agent evaluates and retains promising features through reasoning-based and validation-based filtering. It operates within an iterative feedback loop until performance converges. Experiments on RDB benchmarks demonstrate that ReFuGe substantially improves performance on various RDB prediction tasks. Our code and datasets are available at https://github.com/K-Kyungho/REFUGE.

</details>


### [71] [Faramesh: A Protocol-Agnostic Execution Control Plane for Autonomous Agent Systems](https://arxiv.org/abs/2601.17744)
*Amjad Fatmi*

Main category: cs.AI

TL;DR: Faramesh是一个协议无关的执行控制平面，通过不可绕过的动作授权边界强制实施代理驱动动作的执行时授权，确保组织在动作改变现实前能够确定性地允许、拒绝或推迟执行。


<details>
  <summary>Details</summary>
Motivation: 自主代理系统越来越多地触发现实世界的副作用（部署基础设施、修改数据库、移动资金、执行工作流），但大多数代理堆栈缺乏强制性的执行检查点，无法在动作改变现实前进行确定性授权控制。

Method: 引入协议无关的执行控制平面Faramesh，通过不可绕过的动作授权边界强制执行时授权。系统将代理意图规范化为规范动作表示，根据策略和状态确定性评估动作，并生成决策工件（允许/推迟/拒绝），执行器在执行前必须验证该决策。

Result: Faramesh设计为框架和模型无关，支持多代理和多租户部署，独立于传输协议。提供基于规范动作哈希的决策中心、仅追加的溯源日志记录，实现可审计性、验证和确定性重放，无需重新运行代理推理。

Conclusion: Faramesh为自主执行提供可强制执行、可预测的治理，避免了与编排层的隐藏耦合或仅观察性的方法，确保组织能够在代理动作影响现实世界前进行确定性控制。

Abstract: Autonomous agent systems increasingly trigger real-world side effects: deploying infrastructure, modifying databases, moving money, and executing workflows. Yet most agent stacks provide no mandatory execution checkpoint where organizations can deterministically permit, deny, or defer an action before it changes reality. This paper introduces Faramesh, a protocol-agnostic execution control plane that enforces execution-time authorization for agent-driven actions via a non-bypassable Action Authorization Boundary (AAB). Faramesh canonicalizes agent intent into a Canonical Action Representation (CAR), evaluates actions deterministically against policy and state, and issues a decision artifact (PERMIT/DEFER/DENY) that executors must validate prior to execution. The system is designed to be framework- and model-agnostic, supports multi-agent and multi-tenant deployments, and remains independent of transport protocols (e.g., MCP). Faramesh further provides decision-centric, append-only provenance logging keyed by canonical action hashes, enabling auditability, verification, and deterministic replay without re-running agent reasoning. We show how these primitives yield enforceable, predictable governance for autonomous execution while avoiding hidden coupling to orchestration layers or observability-only approaches.

</details>


### [72] [HyCARD-Net: A Synergistic Hybrid Intelligence Framework for Cardiovascular Disease Diagnosis](https://arxiv.org/abs/2601.17767)
*Rajan Das Gupta,Xiaobin Wu,Xun Liu,Jiaqi He*

Main category: cs.AI

TL;DR: 提出混合集成框架，结合CNN、LSTM深度学习与KNN、XGB传统机器学习，通过投票机制预测心血管疾病，在两个Kaggle数据集上分别达到82.30%和97.10%准确率。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球首要死因，需要智能数据驱动的诊断工具。传统预测模型在处理异构数据集和复杂生理模式时泛化能力不足。

Method: 提出混合集成框架，集成CNN和LSTM深度学习架构，以及KNN和XGB传统机器学习算法，采用集成投票机制，结合深度网络的表征能力和传统模型的可解释性与效率。

Result: 在两个公开Kaggle数据集上，模型分别达到82.30%和97.10%的准确率，在精确率、召回率和F1分数上均有稳定提升。

Conclusion: 混合AI框架在预测心血管疾病方面具有鲁棒性和临床潜力，支持早期干预，符合联合国可持续发展目标3（良好健康与福祉），通过创新数据驱动医疗解决方案促进非传染性疾病的早期诊断、预防和管理。

Abstract: Cardiovascular disease (CVD) remains the foremost cause of mortality worldwide, underscoring the urgent need for intelligent and data-driven diagnostic tools. Traditional predictive models often struggle to generalize across heterogeneous datasets and complex physiological patterns. To address this, we propose a hybrid ensemble framework that integrates deep learning architectures, Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM), with classical machine learning algorithms, including K-Nearest Neighbor (KNN) and Extreme Gradient Boosting (XGB), using an ensemble voting mechanism. This approach combines the representational power of deep networks with the interpretability and efficiency of traditional models. Experiments on two publicly available Kaggle datasets demonstrate that the proposed model achieves superior performance, reaching 82.30 percent accuracy on Dataset I and 97.10 percent on Dataset II, with consistent gains in precision, recall, and F1-score. These findings underscore the robustness and clinical potential of hybrid AI frameworks for predicting cardiovascular disease and facilitating early intervention. Furthermore, this study directly supports the United Nations Sustainable Development Goal 3 (Good Health and Well-being) by promoting early diagnosis, prevention, and management of non-communicable diseases through innovative, data-driven healthcare solutions.

</details>


### [73] [Neuro-Symbolic Verification on Instruction Following of LLMs](https://arxiv.org/abs/2601.17789)
*Yiming Su,Kunzhao Xu,Yanjie Gao,Fan Yang,Cheng Li,Mao Yang,Tianyin Xu*

Main category: cs.AI

TL;DR: NSVIF是一个神经符号框架，用于验证LLM输出是否遵循指令，将指令遵循验证建模为约束满足问题，显著优于基于LLM的方法


<details>
  <summary>Details</summary>
Motivation: LLM不总是遵循指令，且违规难以观察或检查，在基于LLM的智能体工作流中，这些违规会沿推理链传播放大，导致任务失败和系统事故

Method: NSVIF将指令遵循验证建模为约束满足问题，将用户指令建模为约束，包括逻辑和语义约束，通过统一求解器协调逻辑推理和语义分析

Result: 实验表明NSVIF显著优于基于LLM的方法，并提供可解释的反馈，NSVIF的反馈有助于在不进行后训练的情况下提高LLM的指令遵循能力

Conclusion: NSVIF是一个通用、通用的验证器，不假设指令或LLM，为解决LLM指令遵循问题提供了有效的神经符号框架

Abstract: A fundamental problem of applying Large Language Models (LLMs) to important applications is that LLMs do not always follow instructions, and violations are often hard to observe or check. In LLM-based agentic workflows, such violations can propagate and amplify along reasoning chains, causing task failures and system incidents. This paper presents NSVIF, a neuro-symbolic framework for verifying whether an LLM's output follows the instructions used to prompt the LLM. NSVIF is a universal, general-purpose verifier; it makes no assumption about the instruction or the LLM. NSVIF formulates instruction-following verification as a constraint-satisfaction problem by modeling user instructions as constraints. NSVIF models both logical and semantic constraints; constraint solving is done by a unified solver that orchestrates logical reasoning and semantic analysis. To evaluate NSVIF, we develop VIFBENCH, a new benchmark for instruction-following verifiers with fine-grained data labels. Experiments show that NSVIF significantly outperforms LLM-based approaches and provides interpretable feedback. We also show that feedback from NSVIF helps improve LLMs' instruction-following capability without post-training.

</details>


### [74] [MMR-Bench: A Comprehensive Benchmark for Multimodal LLM Routing](https://arxiv.org/abs/2601.17814)
*Haoxuan Ma,Guannan Lai,Han-Jia Ye*

Main category: cs.AI

TL;DR: MMR-Bench是一个用于评估多模态大语言模型路由策略的基准测试，旨在解决在计算预算约束下为不同任务选择最合适模型的问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在架构、对齐策略和效率方面存在异质性，没有单一模型在所有任务上都表现最优。实际部署中，工作负载从轻量级OCR到复杂多模态推理不等，使用单一模型要么在简单任务上过度计算，要么在困难任务上牺牲准确性。

Method: 提出MMR-Bench基准测试，提供：(1)具有模态感知输入和可变计算预算的受控环境；(2)涵盖OCR、通用VQA和多模态数学推理的广泛视觉语言任务套件；(3)强单模型参考、理论上限和代表性路由策略。

Result: 实验表明，结合多模态信号能提高路由质量，使路由系统在约33%的成本下超越最强单模型的准确性。训练的策略能在新数据集和纯文本基准测试上实现零样本泛化。

Conclusion: MMR-Bench为研究自适应多模态模型选择和高效MLLM部署奠定了基础，证明了在预算约束下通过智能路由实现更好性能的可行性。

Abstract: Multimodal large language models (MLLMs) have advanced rapidly, yet heterogeneity in architecture, alignment strategies, and efficiency means that no single model is uniformly superior across tasks. In practical deployments, workloads span lightweight OCR to complex multimodal reasoning; using one MLLM for all queries either over-provisions compute on easy instances or sacrifices accuracy on hard ones. Query-level model selection (routing) addresses this tension, but extending routing from text-only LLMs to MLLMs is nontrivial due to modality fusion, wide variation in computational cost across models, and the absence of a standardized, budget-aware evaluation. We present MMR-Bench, a unified benchmark that isolates the multimodal routing problem and enables comparison under fixed candidate sets and cost models. MMR-Bench provides (i) a controlled environment with modality-aware inputs and variable compute budgets, (ii) a broad suite of vision-language tasks covering OCR, general VQA, and multimodal math reasoning, and (iii) strong single-model reference, oracle upper bounds, and representative routing policies. Using MMR-Bench, we show that incorporating multimodal signals improves routing quality. Empirically, these cues improve the cost-accuracy frontier and enable the routed system to exceed the strongest single model's accuracy at roughly 33% of its cost. Furthermore, policies trained on a subset of models and tasks generalize zero-shot to new datasets and text-only benchmarks without retuning, establishing MMR-Bench as a foundation for studying adaptive multimodal model selection and efficient MLLM deployment. The code will be available at: https://github.com/Hunter-Wrynn/MMR-Bench.

</details>


### [75] [RegGuard: AI-Powered Retrieval-Enhanced Assistant for Pharmaceutical Regulatory Compliance](https://arxiv.org/abs/2601.17826)
*Siyuan Yang,Xihan Bian,Jiayin Tang*

Main category: cs.AI

TL;DR: RegGuard是一个工业级AI助手，用于自动化解读异构监管文本并与公司内部政策对齐，通过HiSACC和ReLACE技术提升检索和生成质量，在企业环境中显著提高答案质量并减少幻觉风险。


<details>
  <summary>Details</summary>
Motivation: 监管更新频率和复杂性增加给跨国制药公司带来沉重负担，合规团队需要跨司法管辖区、格式和机构手动解读不断变化的规则，成本高且易出错。

Method: 系统通过安全管道摄入异构文档源，采用两个创新组件：HiSACC（分层语义聚合上下文分块）将长文档语义分割为连贯单元；ReLACE（监管列表自适应交叉编码器）基于开源模型构建的领域适应交叉编码器，联合建模用户查询和检索候选以改进排名相关性。

Result: 企业环境评估显示，RegGuard在相关性、基础性和上下文聚焦方面显著提高答案质量，同时显著减轻幻觉风险。系统架构具备可审计性和可追溯性。

Conclusion: RegGuard为具有严格合规要求的任何领域提供了高度响应性的解决方案，其架构设计支持审计追踪、访问控制和增量索引，能够适应不断演变的文档源。

Abstract: The increasing frequency and complexity of regulatory updates present a significant burden for multinational pharmaceutical companies. Compliance teams must interpret evolving rules across jurisdictions, formats, and agencies, often manually, at high cost and risk of error. We introduce RegGuard, an industrial-scale AI assistant designed to automate the interpretation of heterogeneous regulatory texts and align them with internal corporate policies. The system ingests heterogeneous document sources through a secure pipeline and enhances retrieval and generation quality with two novel components: HiSACC (Hierarchical Semantic Aggregation for Contextual Chunking) semantically segments long documents into coherent units while maintaining consistency across non-contiguous sections. ReLACE (Regulatory Listwise Adaptive Cross-Encoder for Reranking), a domain-adapted cross-encoder built on an open-source model, jointly models user queries and retrieved candidates to improve ranking relevance. Evaluations in enterprise settings demonstrate that RegGuard improves answer quality specifically in terms of relevance, groundedness, and contextual focus, while significantly mitigating hallucination risk. The system architecture is built for auditability and traceability, featuring provenance tracking, access control, and incremental indexing, making it highly responsive to evolving document sources and relevant for any domain with stringent compliance demands.

</details>


### [76] [Aligning Medical Conversational AI through Online Reinforcement Learning with Information-Theoretic Rewards](https://arxiv.org/abs/2601.17828)
*Tanvi Verma,Yang Zhou,Rick Siow Mong Goh,Yong Liu*

Main category: cs.AI

TL;DR: IGFT是一种无需人类对话数据、通过信息增益奖励训练医疗对话AI进行患者访谈的方法，在HPI生成任务上显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有医疗对话AI训练依赖昂贵的人工标注对话数据或静态数据集，难以学习有效的多轮问诊策略。需要一种能够自主探索、无需预收集人类对话的训练方法。

Method: 结合在线GRPO强化学习和信息论奖励，模型通过与模拟患者自我生成对话学习。核心是信息增益奖励函数，追踪临床实体（症状、时间模式、病史）在对话中的揭示程度，结合GPT-4o-mini的质量评估计算问题奖励。

Result: 在Avey和MIMIC数据集上，IGFT训练的DeepSeek-R1-Distill-Qwen-7B和Llama-3.1-8B-Instruct均显著优于基线模型，F1分数提升10.9%-12.9%，超越OpenAI模型和医疗领域专用模型。

Conclusion: IGFT提供了一种无需人类对话数据的高效医疗对话AI训练框架，能够学习有效的问诊策略，在HPI生成任务上表现优异，为医疗对话系统开发提供了新方向。

Abstract: We present Information Gain Fine-Tuning (IGFT), a novel approach for training medical conversational AI to conduct effective patient interviews and generate comprehensive History of Present Illness (HPI) without requiring pre-collected human conversations. IGFT combines online Group Relative Policy Optimization (GRPO) with information-theoretic rewards, enabling models to learn from self-generated conversations with simulated patients. Unlike existing approaches that rely on expensive expert-annotated conversations or static datasets, our online RL framework allows models to discover effective questioning strategies through exploration. Our key innovation is an information gain reward function that tracks which clinical entities such as symptoms, temporal patterns, and medical history, are revealed during conversation. Each question's reward is computed based on its expected information gain combined with GPT-4o-mini quality assessments across dimensions including clinical relevance, patient engagement, and specificity. This hybrid approach ensures models learn to ask targeted, clinically appropriate questions that efficiently gather diagnostic information. We fine-tune two models using LoRA: Llama-3.1-8B-Instruct and DeepSeek-R1-Distill-Qwen-7B (a reasoning-optimized model). Training exclusively on Avey data containing concise HPIs, we evaluate generalization to MIMIC data with longer, more elaborate HPIs. DeepSeek-R1-Distill-Qwen-7B (IGFT) achieves F1 scores of 0.408 on Avey (10.9% improvement over base) and 0.289 on MIMIC (12.9% improvement), while Llama-3.1-8B-Instruct (IGFT) reaches 0.384 and 0.336 respectively. Both models outperform OpenAI's model on MIMIC and surpass medical domain-specific baselines like HuatuoGPT and UltraMedical, which were optimized for single-turn medical QA rather than multi-turn conversations.

</details>


### [77] [When Personalization Legitimizes Risks: Uncovering Safety Vulnerabilities in Personalized Dialogue Agents](https://arxiv.org/abs/2601.17887)
*Jiahe Guo,Xiangran Guo,Yulin Hu,Zimo Long,Xingyu Sui,Xuda Zhi,Yongbo Huang,Hao He,Weixiang Zhao,Yanyan Zhao,Bing Qin*

Main category: cs.AI

TL;DR: 个性化LLM代理中的长期记忆可能导致"意图合法化"安全漏洞，即良性个人记忆会偏置意图推断，使模型将有害查询合法化。


<details>
  <summary>Details</summary>
Motivation: 现有个性化代理研究主要关注实用性和用户体验，将记忆视为中性组件，忽视了其安全影响。本文旨在揭示"意图合法化"这一被忽视的安全漏洞，即良性个人记忆如何导致模型将有害查询合法化。

Method: 1) 提出PS-Bench基准测试，用于识别和量化个性化交互中的意图合法化现象；2) 在多个记忆增强代理框架和基础LLM上进行实验；3) 从内部表示空间提供意图合法化的机制证据；4) 提出轻量级检测-反思方法减少安全退化。

Result: 个性化使攻击成功率相对于无状态基线增加15.8%-243.7%。实验证明意图合法化是真实存在的安全漏洞，提出的检测-反思方法能有效减少安全退化。

Conclusion: 意图合法化是从良性、真实世界个性化中自然产生的安全故障模式，凸显了在长期个性化背景下评估安全性的重要性。这是对该安全漏洞的首次系统探索和评估。

Abstract: Long-term memory enables large language model (LLM) agents to support personalized and sustained interactions. However, most work on personalized agents prioritizes utility and user experience, treating memory as a neutral component and largely overlooking its safety implications. In this paper, we reveal intent legitimation, a previously underexplored safety failure in personalized agents, where benign personal memories bias intent inference and cause models to legitimize inherently harmful queries. To study this phenomenon, we introduce PS-Bench, a benchmark designed to identify and quantify intent legitimation in personalized interactions. Across multiple memory-augmented agent frameworks and base LLMs, personalization increases attack success rates by 15.8%-243.7% relative to stateless baselines. We further provide mechanistic evidence for intent legitimation from internal representations space, and propose a lightweight detection-reflection method that effectively reduces safety degradation. Overall, our work provides the first systematic exploration and evaluation of intent legitimation as a safety failure mode that naturally arises from benign, real-world personalization, highlighting the importance of assessing safety under long-term personal context. WARNING: This paper may contain harmful content.

</details>


### [78] [UniCog: Uncovering Cognitive Abilities of LLMs through Latent Mind Space Analysis](https://arxiv.org/abs/2601.17897)
*Jiayu Liu,Yinhe Long,Zhenya Huang,Enhong Chen*

Main category: cs.AI

TL;DR: UniCog是一个通过潜在思维空间分析LLM认知的统一框架，将密集模型激活编码为稀疏解耦的潜在维度，揭示了LLM认知的帕累托原则，并利用潜在激活异常检测推理失败，最终通过潜在信息候选优先策略提升推理性能7.5%。


<details>
  <summary>Details</summary>
Motivation: 现有可解释性方法在解释LLM推理过程中如何运用认知能力方面存在局限，而研究表明LLM的认知过程与人类存在根本差异，因此需要新的分析框架来理解LLM的认知机制。

Method: 提出UniCog框架，作为潜在变量模型，将密集模型激活编码为稀疏解耦的潜在维度，形成潜在思维空间。对六个先进LLM（包括DeepSeek-V3.2和GPT-4o）进行广泛分析，并开发潜在信息候选优先策略。

Result: 揭示了LLM认知的帕累托原则：共享推理核心与能力特定特征互补；发现推理失败常表现为潜在激活异常；潜在信息候选优先策略在挑战性基准测试中将推理性能提升高达7.5%。

Conclusion: UniCog为LLM分析开辟了新范式，提供了基于认知的推理动态视角，通过潜在思维空间分析能够深入理解LLM的认知机制并实际提升推理性能。

Abstract: A growing body of research suggests that the cognitive processes of large language models (LLMs) differ fundamentally from those of humans. However, existing interpretability methods remain limited in explaining how cognitive abilities are engaged during LLM reasoning. In this paper, we propose UniCog, a unified framework that analyzes LLM cognition via a latent mind space. Formulated as a latent variable model, UniCog encodes diverse abilities from dense model activations into sparse, disentangled latent dimensions. Through extensive analysis on six advanced LLMs, including DeepSeek-V3.2 and GPT-4o, we reveal a Pareto principle of LLM cognition, where a shared reasoning core is complemented by ability-specific signatures. Furthermore, we discover that reasoning failures often manifest as anomalous intensity in latent activations. These findings opens a new paradigm in LLM analysis, providing a cognition grounded view of reasoning dynamics. Finally, leveraging these insights, we introduce a latent-informed candidate prioritization strategy, which improves reasoning performance by up to 7.5% across challenging benchmarks. Our code is available at https://github.com/milksalute/unicog.

</details>


### [79] [Think Locally, Explain Globally: Graph-Guided LLM Investigations via Local Reasoning and Belief Propagation](https://arxiv.org/abs/2601.17915)
*Saurabh Jha,Rohan Arora,Bhavya,Noah Zheutlin,Paulina Toro Isaza,Laura Shwartz,Yu Deng,Daby Sow,Ruchi Mahindru,Ruchir Puri*

Main category: cs.AI

TL;DR: EoG框架通过将调查任务分解为依赖图上的溯因推理，解决了LLM代理在开放式调查中的可靠性问题，显著提升了准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在开放式调查任务中存在严重缺陷：上下文窗口有限导致关键证据可能被过早丢弃；ReAct式代理的检索-总结-推理循环使结论对探索顺序敏感，产生运行间非确定性；缺乏信念记录和修正机制；语义推理与控制器职责纠缠导致执行错误影响推理质量。

Method: 提出EoG（基于图的解释）框架：将调查任务形式化为依赖图上的溯因推理。LLM负责有界的局部证据挖掘和标注（原因vs症状），而确定性控制器管理图遍历、状态维护和信念传播，计算最小解释边界。

Result: 在ITBench诊断任务上，EoG相比ReAct基线显著提升了准确性和运行间一致性，平均在Majority-at-k实体F1指标上获得7倍提升。

Conclusion: 通过将LLM的局部推理能力与确定性控制器的全局管理分离，EoG框架能够更可靠地处理开放式调查任务，解决了传统代理在复杂依赖环境中的可靠性问题。

Abstract: LLM agents excel when environments are mostly static and the needed information fits in a model's context window, but they often fail in open-ended investigations where explanations must be constructed by iteratively mining evidence from massive, heterogeneous operational data. These investigations exhibit hidden dependency structure: entities interact, signals co-vary, and the importance of a fact may only become clear after other evidence is discovered. Because the context window is bounded, agents must summarize intermediate findings before their significance is known, increasing the risk of discarding key evidence. ReAct-style agents are especially brittle in this regime. Their retrieve-summarize-reason loop makes conclusions sensitive to exploration order and introduces run-to-run non-determinism, producing a reliability gap where Pass-at-k may be high but Majority-at-k remains low. Simply sampling more rollouts or generating longer reasoning traces does not reliably stabilize results, since hypotheses cannot be autonomously checked as new evidence arrives and there is no explicit mechanism for belief bookkeeping and revision. In addition, ReAct entangles semantic reasoning with controller duties such as tool orchestration and state tracking, so execution errors and plan drift degrade reasoning while consuming scarce context.
  We address these issues by formulating investigation as abductive reasoning over a dependency graph and proposing EoG (Explanations over Graphs), a disaggregated framework in which an LLM performs bounded local evidence mining and labeling (cause vs symptom) while a deterministic controller manages traversal, state, and belief propagation to compute a minimal explanatory frontier. On a representative ITBench diagnostics task, EoG improves both accuracy and run-to-run consistency over ReAct baselines, including a 7x average gain in Majority-at-k entity F1.

</details>


### [80] [Agentic AI for Self-Driving Laboratories in Soft Matter: Taxonomy, Benchmarks,and Open Challenges](https://arxiv.org/abs/2601.17920)
*Xuanzhou Chen,Audrey Wang,Stanley Yin,Hanyang Jiang,Dong Zhang*

Main category: cs.AI

TL;DR: 这篇综述将自主实验室视为智能体环境交互问题，回顾了贝叶斯优化、规划、强化学习等方法，提出了能力驱动的分类体系，并建立了基准任务模板和评估指标。


<details>
  <summary>Details</summary>
Motivation: 自主实验室为智能体AI提供了具有昂贵操作、噪声延迟反馈、严格约束和非平稳性的测试平台，需要系统性地研究其中的AI问题。

Method: 将SDL自主性框架化为智能体环境交互问题，回顾贝叶斯优化、主动学习、规划、强化学习等方法，提出能力驱动的分类体系，并建立基准任务模板和评估指标。

Result: 建立了连接SDL流程与AI原则的系统框架，提出了涵盖决策视野、不确定性建模、约束处理等维度的分类体系，并设计了成本感知性能、鲁棒性、可重复性等评估指标。

Conclusion: 自主实验室是AI的重要应用领域，需要进一步发展多模态表示、校准不确定性、安全探索和共享基准基础设施等方向的研究。

Abstract: Self-driving laboratories (SDLs) close the loop between experiment design, automated execution, and data-driven decision making, and they provide a demanding testbed for agentic AI under expensive actions, noisy and delayed feedback, strict feasibility and safety constraints, and non-stationarity. This survey uses soft matter as a representative setting but focuses on the AI questions that arise in real laboratories. We frame SDL autonomy as an agent environment interaction problem with explicit observations, actions, costs, and constraints, and we use this formulation to connect common SDL pipelines to established AI principles. We review the main method families that enable closed loop experimentation, including Bayesian optimization and active learning for sample efficient experiment selection, planning and reinforcement learning for long horizon protocol optimization, and tool using agents that orchestrate heterogeneous instruments and software. We emphasize verifiable and provenance aware policies that support debugging, reproducibility, and safe operation. We then propose a capability driven taxonomy that organizes systems by decision horizon, uncertainty modeling, action parameterization, constraint handling, failure recovery, and human involvement. To enable meaningful comparison, we synthesize benchmark task templates and evaluation metrics that prioritize cost aware performance, robustness to drift, constraint violation behavior, and reproducibility. Finally, we distill lessons from deployed SDLs and outline open challenges in multi-modal representation, calibrated uncertainty, safe exploration, and shared benchmark infrastructure.

</details>


### [81] [Learning Transferable Skills in Action RPGs via Directed Skill Graphs and Selective Adaptation](https://arxiv.org/abs/2601.17923)
*Ali Najar*

Main category: cs.AI

TL;DR: 论文提出了一种基于技能图的分层课程学习方法，用于在复杂实时控制环境（黑暗之魂III）中实现终身学习代理，通过分解技能和选择性微调实现高效适应环境变化。


<details>
  <summary>Details</summary>
Motivation: 终身学习代理需要在不断学习新能力的同时，避免从头开始重新训练或覆盖先前学到的行为。在复杂的实时控制环境中（如黑暗之魂III），这是一个特别具有挑战性的问题。

Method: 将战斗控制表示为有向技能图，采用分层课程学习方法训练其组件。将控制分解为五个可重用技能：相机控制、目标锁定、移动、闪避和治疗-攻击决策策略，每个技能针对特定职责进行优化。

Result: 技能分解通过减轻单个策略的负担提高了样本效率。当环境从第一阶段切换到第二阶段时，只需调整部分技能，而上游技能保持可迁移性。实验表明，仅对两个技能进行针对性微调就能在有限交互预算下快速恢复性能。

Conclusion: 技能图课程学习结合选择性微调为复杂实时环境中不断进化的持续学习代理提供了一条实用途径，支持终身学习而不需要完全重新训练。

Abstract: Lifelong agents should expand their competence over time without retraining from scratch or overwriting previously learned behaviors. We investigate this in a challenging real-time control setting (Dark Souls III) by representing combat as a directed skill graph and training its components in a hierarchical curriculum. The resulting agent decomposes control into five reusable skills: camera control, target lock-on, movement, dodging, and a heal-attack decision policy, each optimized for a narrow responsibility. This factorization improves sample efficiency by reducing the burden on any single policy and supports selective post-training: when the environment shifts from Phase 1 to Phase 2, only a subset of skills must be adapted, while upstream skills remain transferable. Empirically, we find that targeted fine-tuning of just two skills rapidly recovers performance under a limited interaction budget, suggesting that skill-graph curricula together with selective fine-tuning offer a practical pathway toward evolving, continually learning agents in complex real-time environments.

</details>


### [82] [LLM-Based SQL Generation: Prompting, Self-Refinement, and Adaptive Weighted Majority Voting](https://arxiv.org/abs/2601.17942)
*Yu-Jie Yang,Hung-Fu Chang,Po-An Chen*

Main category: cs.AI

TL;DR: 提出SSEV和ReCAPAgent-SQL两个框架，通过自优化和智能体协作提升Text-to-SQL性能，在多个基准测试中取得竞争性结果。


<details>
  <summary>Details</summary>
Motivation: Text-to-SQL技术能降低数据分析门槛，但自然语言查询存在歧义、模式链接复杂、SQL方言泛化有限、需要领域知识等问题，现有方法难以应对企业级数据库的复杂性。

Method: 1. SSEV：基于PET-SQL的单智能体自优化集成投票管道，结合自优化与加权多数投票及其随机变体。2. ReCAPAgent-SQL：多智能体协作框架，包含规划、外部知识检索、批判、动作生成、自优化、模式链接和结果验证等专门智能体，通过迭代优化提升SQL预测质量。

Result: SSEV在Spider 1.0-Dev达到85.5%执行准确率，Spider 1.0-Test达到86.4%，BIRD-Dev达到66.3%。ReCAPAgent-SQL在Spider 2.0-Lite前100个查询中实现31%执行准确率，显著提升企业场景处理能力。

Conclusion: 提出的框架能有效部署可扩展的Text-to-SQL系统到实际应用中，以更低成本和更高效率支持数据驱动决策，为复杂企业数据库和真实世界任务提供解决方案。

Abstract: Text-to-SQL has emerged as a prominent research area, particularly with the rapid advancement of large language models (LLMs). By enabling users to query databases through natural language rather than SQL, this technology significantly lowers the barrier to data analysis. However, generating accurate SQL from natural language remains challenging due to ambiguity in user queries, the complexity of schema linking, limited generalization across SQL dialects, and the need for domain-specific understanding. In this study, we propose a Single-Agent Self-Refinement with Ensemble Voting (SSEV) pipeline built on PET-SQL that operates without ground-truth data, integrating self-refinement with Weighted Majority Voting (WMV) and its randomized variant (RWMA). Experimental results show that the SSEV achieves competitive performance across multiple benchmarks, attaining execution accuracies of 85.5% on Spider 1.0-Dev, 86.4% on Spider 1.0-Test, and 66.3% on BIRD-Dev. Building on insights from the SSEV pipeline, we further propose ReCAPAgent-SQL (Refinement-Critique-Act-Plan agent-based SQL framework) to address the growing complexity of enterprise databases and real-world Text-to-SQL tasks. The framework integrates multiple specialized agents for planning, external knowledge retrieval, critique, action generation, self-refinement, schema linking, and result validation, enabling iterative refinement of SQL predictions through agent collaboration. ReCAPAgent-SQL's WMA results achieve 31% execution accuracy on the first 100 queries of Spider 2.0-Lite, demonstrating significant improvements in handling real-world enterprise scenarios. Overall, our work facilitates the deployment of scalable Text-to-SQL systems in practical settings, supporting better data-driven decision-making at lower cost and with greater efficiency.

</details>


### [83] [Sentipolis: Emotion-Aware Agents for Social Simulations](https://arxiv.org/abs/2601.18027)
*Chiyuan Fu,Lyuhao Chen,Yunze Xiao,Weihao Xuan,Carlos Busso,Mona Diab*

Main category: cs.AI

TL;DR: Sentipolis框架为LLM智能体提供情感状态记忆，通过PAD情感表示、双速情感动态和情感-记忆耦合解决情感遗忘问题，提升社交模拟中的情感连续性和真实性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体在社交模拟中常将情感视为瞬时线索，导致情感遗忘和长期情感连续性不足，需要建立具有情感状态记忆的智能体框架。

Method: 提出Sentipolis框架，包含：1）连续PAD（愉悦-唤醒-支配）情感表示；2）双速情感动态机制；3）情感与记忆耦合系统。

Result: 在数千次交互中，Sentipolis显著提升情感基础行为、沟通能力和情感连续性。效果因模型而异：高容量模型可信度提升，小模型可能下降；情感意识可能轻微降低社会规范遵守度。

Conclusion: Sentipolis成功实现情感状态智能体，支持研究累积社交动态如联盟形成和关系渐变，揭示了情感驱动行为与规则遵守之间的人类式张力。

Abstract: LLM agents are increasingly used for social simulation, yet emotion is often treated as a transient cue, causing emotional amnesia and weak long-horizon continuity. We present Sentipolis, a framework for emotionally stateful agents that integrates continuous Pleasure-Arousal-Dominance (PAD) representation, dual-speed emotion dynamics, and emotion--memory coupling. Across thousands of interactions over multiple base models and evaluators, Sentipolis improves emotionally grounded behavior, boosting communication, and emotional continuity. Gains are model-dependent: believability increases for higher-capacity models but can drop for smaller ones, and emotion-awareness can mildly reduce adherence to social norms, reflecting a human-like tension between emotion-driven behavior and rule compliance in social simulation. Network-level diagnostics show reciprocal, moderately clustered, and temporally stable relationship structures, supporting the study of cumulative social dynamics such as alliance formation and gradual relationship change.

</details>


### [84] [Expert Evaluation and the Limits of Human Feedback in Mental Health AI Safety Testing](https://arxiv.org/abs/2601.18061)
*Kiana Jafari,Paul Ulrich Nikolaus Rust,Duncan Eddy,Robbie Fraser,Nina Vasan,Darja Djordjevic,Akanksha Dadlani,Max Lamparth,Eugenia Kim,Mykel Kochenderfer*

Main category: cs.AI

TL;DR: 研究发现心理健康领域专家评估AI回复时存在系统性分歧，尤其是在自杀自伤等安全关键问题上，专家间一致性极低，分歧源于不同的临床框架而非测量误差。


<details>
  <summary>Details</summary>
Motivation: 验证人类反馈学习(LHF)的基本假设：专家判断经过适当聚合后能产生有效的"地面真值"用于训练和评估AI系统，特别是在心理健康这种安全风险高的领域。

Method: 三位认证精神科医生使用校准的评估标准独立评估LLM生成的回复，计算组内相关系数(ICC)和Krippendorff's α等可靠性指标，并进行定性访谈分析分歧原因。

Result: 专家间可靠性极低(ICC 0.087-0.295)，低于可接受阈值；自杀自伤类回复分歧最大；一个因素甚至出现负可靠性(α=-0.203)；分歧源于三种不同的临床框架：安全优先、参与为中心、文化导向。

Conclusion: 专家分歧是原则性的社会技术现象，而非测量误差；聚合标签只是算术妥协，抹杀了专业哲学；建议从基于共识的聚合转向能保留和学习专家分歧的对齐方法。

Abstract: Learning from human feedback~(LHF) assumes that expert judgments, appropriately aggregated, yield valid ground truth for training and evaluating AI systems. We tested this assumption in mental health, where high safety stakes make expert consensus essential. Three certified psychiatrists independently evaluated LLM-generated responses using a calibrated rubric. Despite similar training and shared instructions, inter-rater reliability was consistently poor ($ICC$ $0.087$--$0.295$), falling below thresholds considered acceptable for consequential assessment. Disagreement was highest on the most safety-critical items. Suicide and self-harm responses produced greater divergence than any other category, and was systematic rather than random. One factor yielded negative reliability (Krippendorff's $α= -0.203$), indicating structured disagreement worse than chance. Qualitative interviews revealed that disagreement reflects coherent but incompatible individual clinical frameworks, safety-first, engagement-centered, and culturally-informed orientations, rather than measurement error. By demonstrating that experts rely on holistic risk heuristics rather than granular factor discrimination, these findings suggest that aggregated labels function as arithmetic compromises that effectively erase grounded professional philosophies. Our results characterize expert disagreement in safety-critical AI as a sociotechnical phenomenon where professional experience introduces sophisticated layers of principled divergence. We discuss implications for reward modeling, safety classification, and evaluation benchmarks, recommending that practitioners shift from consensus-based aggregation to alignment methods that preserve and learn from expert disagreement.

</details>


### [85] [EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization](https://arxiv.org/abs/2601.18067)
*Wei-Po Hsin,Ren-Hao Deng,Yao-Ting Hsieh,En-Ming Huang,Shih-Hao Hung*

Main category: cs.AI

TL;DR: EvolVE框架通过多种进化策略（MCTS和IGR）和结构化测试平台生成，在Verilog硬件设计自动化中取得SOTA性能，显著提升功能正确性和PPA优化。


<details>
  <summary>Details</summary>
Motivation: Verilog硬件设计流程劳动密集且需要专业知识，现有LLM方法因训练数据有限和顺序推理特性，难以处理硬件系统的形式逻辑和并发特性。

Method: 提出EvolVE框架，分析多种进化策略：MCTS用于最大化功能正确性，Idea-Guided Refinement用于优化；采用结构化测试平台生成加速进化过程；引入IC-RTL行业级基准测试套件。

Result: 在VerilogEval v2上达到98.1%，RTLLM v2上达到92%；在IC-RTL行业基准上超越竞赛参与者实现，Huffman编码PPA降低66%，所有问题几何平均PPA降低17%。

Conclusion: EvolVE框架通过进化策略和结构化测试平台，有效解决了LLM在硬件设计中的局限性，实现了硬件设计自动化的新突破，并在行业级问题上展示了显著优化效果。

Abstract: Verilog's design cycle is inherently labor-intensive and necessitates extensive domain expertise. Although Large Language Models (LLMs) offer a promising pathway toward automation, their limited training data and intrinsic sequential reasoning fail to capture the strict formal logic and concurrency inherent in hardware systems. To overcome these barriers, we present EvolVE, the first framework to analyze multiple evolution strategies on chip design tasks, revealing that Monte Carlo Tree Search (MCTS) excels at maximizing functional correctness, while Idea-Guided Refinement (IGR) proves superior for optimization. We further leverage Structured Testbench Generation (STG) to accelerate the evolutionary process. To address the lack of complex optimization benchmarks, we introduce IC-RTL, targeting industry-scale problems derived from the National Integrated Circuit Contest. Evaluations establish EvolVE as the new state-of-the-art, achieving 98.1% on VerilogEval v2 and 92% on RTLLM v2. Furthermore, on the industry-scale IC-RTL suite, our framework surpasses reference implementations authored by contest participants, reducing the Power, Performance, Area (PPA) product by up to 66% in Huffman Coding and 17% in the geometric mean across all problems. The source code of the IC-RTL benchmark is available at https://github.com/weiber2002/ICRTL.

</details>


### [86] [Beyond Text-to-SQL: Can LLMs Really Debug Enterprise ETL SQL?](https://arxiv.org/abs/2601.18119)
*Jing Ye,Yiwen Duan,Yonghong Yu,Victor Ma,Yang Gao,Xing Chen*

Main category: cs.AI

TL;DR: OurBench是首个企业级SQL推理与调试基准，通过逆向工程注入真实错误，包含469个语法错误和516个语义错误查询，评估显示LLMs在复杂SQL调试上表现不佳（最佳模型准确率仅36%）。


<details>
  <summary>Details</summary>
Motivation: 企业数据工程中SQL至关重要，但即使经验丰富的开发者和先进LLMs也难以一次性生成完全正确的SQL代码，通常需要多次调试迭代。目前缺乏专门针对企业级SQL推理和调试的基准测试。

Method: 1) 自动化构建工作流：使用逆向工程将真实错误系统性地注入大规模SQL代码中，实现可扩展且多样化的基准生成；2) 无执行评估框架：针对企业环境定制，提供快速、准确且资源高效的评估方法。

Result: OurBench包含469个带明确错误信息的语法错误查询(OurBenchSyn)和516个语义错误查询(OurBenchSem)，查询高度复杂（平均超过140行，具有深而广的抽象语法树）。评估近30个LLMs显示性能差距显著：最佳模型Claude-4-Sonnet在OurBenchSyn上准确率仅36.46%，在OurBenchSem上仅32.17%，大多数模型低于20%。

Conclusion: 该研究揭示了LLMs在企业级SQL调试中的局限性，探索了四种解决方案策略，识别了关键挑战，并为企业环境中使用LLMs进行SQL调试指出了有前景的研究方向。

Abstract: SQL is central to enterprise data engineering, yet generating fully correct SQL code in a single attempt remains difficult, even for experienced developers and advanced text-to-SQL LLMs, often requiring multiple debugging iterations. We introduce OurBench, the first benchmark for enterprise-level SQL reasoning and debugging. Our benchmark is built on two key innovations: (1) an automated construction workflow that uses reverse engineering to systematically inject realistic bugs into large-scale SQL code, enabling scalable and diverse benchmark generation; and (2) an execution-free evaluation framework tailored to enterprise settings, providing fast, accurate, and resource-efficient assessment.
  OurBench comprises 469 OurBenchSyn queries featuring syntax errors with explicit error messages, and 516 OurBenchSem queries targeting semantic errors in which the code fails to meet user intent. The queries are highly complex, averaging over 140 lines and featuring deep and wide abstract syntax trees.
  Evaluation of nearly 30 LLMs reveals a substantial performance gap: the best-performing model, Claude-4-Sonnet, achieves only 36.46 percent accuracy on OurBenchSyn and 32.17 percent on OurBenchSem, while most models score below 20 percent. We further explore four solution strategies, identify key challenges, and outline promising directions for enterprise SQL debugging with LLMs.

</details>


### [87] [Deadline-Aware, Energy-Efficient Control of Domestic Immersion Hot Water Heaters](https://arxiv.org/abs/2601.18123)
*Muhammad Ibrahim Khan,Bivin Pradeep,James Brusey*

Main category: cs.AI

TL;DR: 研究提出基于截止时间感知的智能控制方法，通过强化学习优化家用浸入式热水器能耗，相比传统控制方法可节省26-69%能源。


<details>
  <summary>Details</summary>
Motivation: 传统家用浸入式热水器在冬季常连续运行，追求快速加热而非高效节能，忽视了可预测的需求窗口和环境热损失。需要开发能在指定时间达到目标温度同时最小化能耗的智能控制方法。

Method: 建立Gymnasium仿真环境模拟浸入式热水器热力学模型，采用时间最优的bang-bang控制作为基线，对比零样本蒙特卡洛树搜索规划器和近端策略优化强化学习策略。控制动作包括0W和6000W两种状态，每120秒执行一次。

Result: 在2小时（60步）时间范围内，PPO策略能耗最低（3.23千瓦时），相比bang-bang控制（4.37-10.45千瓦时）和MCTS（4.18-6.46千瓦时）显著节能。在典型场景中，PPO比bang-bang控制节省54%能耗，比MCTS节省33%能耗。

Conclusion: 学习得到的截止时间感知控制方法在相同物理假设下能显著降低能耗，规划器无需训练即可提供部分节能效果，而学习策略一旦训练完成推理成本接近零，具有实际应用价值。

Abstract: Typical domestic immersion water heater systems are often operated continuously during winter, heating quickly rather than efficiently and ignoring predictable demand windows and ambient losses. We study deadline-aware control, where the aim is to reach a target temperature at a specified time while minimising energy consumption. We introduce an efficient Gymnasium environment that models an immersion hot water heater with first-order thermal losses and discrete on and off actions of 0 W and 6000 W applied every 120 seconds. Methods include a time-optimal bang-bang baseline, a zero-shot Monte Carlo Tree Search planner, and a Proximal Policy Optimisation policy. We report total energy consumption in watt-hours under identical physical dynamics. Across sweeps of initial temperature from 10 to 30 degrees Celsius, deadline from 30 to 90 steps, and target temperature from 40 to 80 degrees Celsius, PPO achieves the most energy-efficient performance at a 60-step horizon of 2 hours, using 3.23 kilowatt-hours, compared to 4.37 to 10.45 kilowatt-hours for bang-bang control and 4.18 to 6.46 kilowatt-hours for MCTS. This corresponds to energy savings of 26 percent at 30 steps and 69 percent at 90 steps. In a representative trajectory with a 50 kg water mass, 20 degrees Celsius ambient temperature, and a 60 degrees Celsius target, PPO consumes 54 percent less energy than bang-bang control and 33 percent less than MCTS. These results show that learned deadline-aware control reduces energy consumption under identical physical assumptions, while planners provide partial savings without training and learned policies offer near-zero inference cost once trained.

</details>


### [88] [RouteMoA: Dynamic Routing without Pre-Inference Boosts Efficient Mixture-of-Agents](https://arxiv.org/abs/2601.18130)
*Jize Wang,Han Wu,Zhiyuan You,Yiming Song,Yijun Wang,Zifei Shan,Yining Li,Songyang Zhang,Xinyi Le,Cailian Chen,Xinping Guan,Dacheng Tao*

Main category: cs.AI

TL;DR: RouteMoA：一种带动态路由的高效多智能体混合框架，通过轻量级评分器预测查询性能筛选候选模型，再用混合评判器进行自评估和交叉评估，显著降低成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 现有MoA方法采用密集拓扑结构导致成本和延迟高，LLM评判器需要所有模型先推理再筛选，无法有效降低成本。同时缺乏模型选择标准，面对大规模模型池时推理成本高且可能超出上下文限制。

Method: 1. 轻量级评分器：基于查询预测粗粒度性能，筛选高潜力候选子集，无需推理；2. 混合评判器：基于现有模型输出进行轻量级自评估和交叉评估，提供后验修正；3. 模型排名机制：平衡性能、成本和延迟选择模型。

Result: RouteMoA在不同任务和模型池规模下均优于MoA，在大规模模型池中降低成本89.8%，减少延迟63.6%。

Conclusion: RouteMoA通过动态路由机制有效解决了MoA框架的成本和延迟问题，实现了高效的模型协作，为大规模模型池应用提供了实用解决方案。

Abstract: Mixture-of-Agents (MoA) improves LLM performance through layered collaboration, but its dense topology raises costs and latency. Existing methods employ LLM judges to filter responses, yet still require all models to perform inference before judging, failing to cut costs effectively. They also lack model selection criteria and struggle with large model pools, where full inference is costly and can exceed context limits. To address this, we propose RouteMoA, an efficient mixture-of-agents framework with dynamic routing. It employs a lightweight scorer to perform initial screening by predicting coarse-grained performance from the query, narrowing candidates to a high-potential subset without inference. A mixture of judges then refines these scores through lightweight self- and cross-assessment based on existing model outputs, providing posterior correction without additional inference. Finally, a model ranking mechanism selects models by balancing performance, cost, and latency. RouteMoA outperforms MoA across varying tasks and model pool sizes, reducing cost by 89.8% and latency by 63.6% in the large-scale model pool.

</details>


### [89] [RareAlert: Aligning heterogeneous large language model reasoning for early rare disease risk screening](https://arxiv.org/abs/2601.18132)
*Xi Chen,Hongru Zhou,Huahui Yi,Shiyu Feng,Hanyu Zhou,Tiancheng He,Mingke You,Li Wang,Qiankun Li,Kun Wang,Weili Fu,Kang Li,Jian Li*

Main category: cs.AI

TL;DR: RareAlert是一个基于LLM推理校准的罕见病早期筛查系统，通过整合多个大语言模型的推理信号，训练出可在本地部署的单一模型，在罕见病风险预测上达到0.917的AUC，优于现有最佳机器学习模型和所有评估的LLMs。


<details>
  <summary>Details</summary>
Motivation: 罕见病的漏诊和延迟诊断是重大临床挑战。现有的初级保健分诊流程在初次就诊时无法可靠识别罕见病患者，需要通用筛查来减少诊断延迟。在初次临床接触时，医生只能基于有限信息在高不确定性下评估罕见病风险。

Method: 开发了RareAlert系统：1）整合10个LLMs生成的推理；2）使用机器学习校准和加权这些信号；3）将对齐的推理蒸馏到单个可在本地部署的模型中。构建了RareBench数据集，包含158,666个病例，覆盖33个Orphanet疾病类别和7000多种罕见病。

Result: RareAlert（基于Qwen3-4B训练）在独立测试集上达到0.917的AUC，优于最佳机器学习集成模型和所有评估的LLMs（包括GPT-5、DeepSeek-R1、Claude-3.7-Sonnet等）。证明了LLM医学推理的多样性，以及在高不确定性临床任务中对齐这种推理的有效性。

Conclusion: 罕见病识别可重新概念化为应用于普通患者群体的通用不确定性解决过程。通过将校准推理整合到单一模型中，RareAlert实现了准确、隐私保护、可扩展的罕见病风险筛查，适合大规模本地部署。

Abstract: Missed and delayed diagnosis remains a major challenge in rare disease care. At the initial clinical encounters, physicians assess rare disease risk using only limited information under high uncertainty. When high-risk patients are not recognised at this stage, targeted diagnostic testing is often not initiated, resulting in missed diagnosis. Existing primary care triage processes are structurally insufficient to reliably identify patients with rare diseases at initial clinical presentation and universal screening is needed to reduce diagnostic delay. Here we present RareAlert, an early screening system which predict patient-level rare disease risk from routinely available primary-visit information. RareAlert integrates reasoning generated by ten LLMs, calibrates and weights these signals using machine learning, and distils the aligned reasoning into a single locally deployable model. To develop and evaluate RareAlert, we curated RareBench, a real-world dataset of 158,666 cases covering 33 Orphanet disease categories and more than 7,000 rare conditions, including both rare and non-rare presentations. The results showed that rare disease identification can be reconceptualised as a universal uncertainty resolution process applied to the general patient population. On an independent test set, RareAlert, a Qwen3-4B based model trained with calibrated reasoning signals, achieved an AUC of 0.917, outperforming the best machine learning ensemble and all evaluated LLMs, including GPT-5, DeepSeek-R1, Claude-3.7-Sonnet, o3-mini, Gemini-2.5-Pro, and Qwen3-235B. These findings demonstrate the diversity in LLM medical reasoning and the effectiveness of aligning such reasoning in highly uncertain clinical tasks. By incorporating calibrated reasoning into a single model, RareAlert enables accurate, privacy-preserving, and scalable rare disease risk screening suitable for large-scale local deployment.

</details>


### [90] [DeepPlanning: Benchmarking Long-Horizon Agentic Planning with Verifiable Constraints](https://arxiv.org/abs/2601.18137)
*Yinger Zhang,Shutong Jiang,Renhao Li,Jianhong Tu,Yang Su,Lianghao Deng,Xudong Guo,Chenxu Lv,Junyang Lin*

Main category: cs.AI

TL;DR: DeepPlanning是一个针对实际长视野智能体规划的挑战性基准测试，包含多日旅行规划和多产品购物任务，需要主动信息获取、局部约束推理和全局约束优化。


<details>
  <summary>Details</summary>
Motivation: 当前智能体评估虽然转向长视野任务，但大多数基准测试仍强调局部、步骤级推理，而非需要真正规划能力的全局约束优化（如时间和财务预算）。现有LLM规划基准在真实世界场景中典型的主动信息收集和细粒度局部约束方面代表性不足。

Method: 引入DeepPlanning基准测试，包含多日旅行规划和多产品购物任务，这些任务需要：1）主动信息获取，2）局部约束推理，3）全局约束优化。通过该基准评估前沿智能体LLM的性能。

Result: 评估显示，即使是前沿的智能体LLM在这些问题上也表现困难，突显了可靠显式推理模式和并行工具使用对于实现更好的效果-效率权衡的重要性。

Conclusion: DeepPlanning基准揭示了当前智能体LLM在长规划视野上的局限性，错误分析指出了改进方向。开源代码和数据以支持未来研究。

Abstract: While agent evaluation has shifted toward long-horizon tasks, most benchmarks still emphasize local, step-level reasoning rather than the global constrained optimization (e.g., time and financial budgets) that demands genuine planning ability. Meanwhile, existing LLM planning benchmarks underrepresent the active information gathering and fine-grained local constraints typical of real-world settings. To address this, we introduce DeepPlanning, a challenging benchmark for practical long-horizon agent planning. It features multi-day travel planning and multi-product shopping tasks that require proactive information acquisition, local constrained reasoning, and global constrained optimization. Evaluations on DeepPlanning show that even frontier agentic LLMs struggle with these problems, highlighting the importance of reliable explicit reasoning patterns and parallel tool use for achieving better effectiveness-efficiency trade-offs. Error analysis further points to promising directions for improving agentic LLMs over long planning horizons. We open-source the code and data to support future research.

</details>


### [91] [Success Conditioning as Policy Improvement: The Optimization Problem Solved by Imitating Success](https://arxiv.org/abs/2601.18175)
*Daniel Russo*

Main category: cs.AI

TL;DR: 成功条件化（success conditioning）是一种广泛使用的策略改进技术，通过收集轨迹、识别成功轨迹并模仿成功轨迹中的动作来更新策略。本文证明该方法精确解决了信任域优化问题，在自动确定的χ²散度约束下最大化策略改进。


<details>
  <summary>Details</summary>
Motivation: 成功条件化技术以不同名称出现在各种方法中（如带SFT的拒绝采样、目标条件RL、决策变换器等），但其解决的优化问题本质一直不明确。本文旨在揭示成功条件化背后的数学原理和优化问题。

Method: 通过理论分析证明成功条件化精确解决了信任域优化问题：在χ²散度约束下最大化策略改进，约束半径由数据自动确定。建立了相对策略改进、策略变化幅度和动作影响之间的恒等关系。

Result: 成功条件化被证明是一种保守的改进算子，不会降低性能或引发危险的分布偏移。当失败时，它会通过几乎不改变策略来可观察地失败。理论还应用于回报阈值化实践，显示其可以放大改进但可能偏离真实目标。

Conclusion: 成功条件化本质上是一个信任域优化问题的精确解，为这一广泛使用的技术提供了坚实的理论基础，解释了其保守改进特性以及在实践中观察到的行为模式。

Abstract: A widely used technique for improving policies is success conditioning, in which one collects trajectories, identifies those that achieve a desired outcome, and updates the policy to imitate the actions taken along successful trajectories. This principle appears under many names -- rejection sampling with SFT, goal-conditioned RL, Decision Transformers -- yet what optimization problem it solves, if any, has remained unclear. We prove that success conditioning exactly solves a trust-region optimization problem, maximizing policy improvement subject to a $χ^2$ divergence constraint whose radius is determined automatically by the data. This yields an identity: relative policy improvement, the magnitude of policy change, and a quantity we call action-influence -- measuring how random variation in action choices affects success rates -- are exactly equal at every state. Success conditioning thus emerges as a conservative improvement operator. Exact success conditioning cannot degrade performance or induce dangerous distribution shift, but when it fails, it does so observably, by hardly changing the policy at all. We apply our theory to the common practice of return thresholding, showing this can amplify improvement, but at the cost of potential misalignment with the true objective.

</details>


### [92] [GAIA: A Data Flywheel System for Training GUI Test-Time Scaling Critic Models](https://arxiv.org/abs/2601.18197)
*Shaokang Wang,Pei Fu,Ruoceng Zhang,Shaojie Zhang,Xiuwen Xi,Jiahui Yang,Bin Qin,Ying Huang,Zhenbo Luo,Jian Luan*

Main category: cs.AI

TL;DR: GAIA框架通过训练直觉批评模型来提升GUI代理的测试时性能，通过数据飞轮机制实现自我改进循环


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型在GUI代理中面临操作不可逆的问题，单个错误动作可能导致灾难性偏差，需要解决这一关键挑战

Method: 提出GUI Action Critic's Data Flywheel System (GAIA)，训练直觉批评模型评估代理动作的正确性，通过数据收集和再训练形成自我改进循环

Result: 实验表明直觉批评模型能提升多种闭源和开源模型的测试时性能，且随着数据循环性能逐步改善

Conclusion: GAIA框架通过批评模型和数据飞轮机制有效解决了GUI代理的操作不可逆问题，实现了性能的持续改进

Abstract: While Large Vision-Language Models (LVLMs) have significantly advanced GUI agents' capabilities in parsing textual instructions, interpreting screen content, and executing tasks, a critical challenge persists: the irreversibility of agent operations, where a single erroneous action can trigger catastrophic deviations. To address this, we propose the GUI Action Critic's Data Flywheel System (GAIA), a training framework that enables the models to have iterative critic capabilities, which are used to improve the Test-Time Scaling (TTS) of basic GUI agents' performance. Specifically, we train an Intuitive Critic Model (ICM) using positive and negative action examples from a base agent first. This critic evaluates the immediate correctness of the agent's intended actions, thereby selecting operations with higher success probability. Then, the initial critic guides agent actions to collect refined positive/negative samples, initiating the self-improving cycle. The augmented data then trains a second-round critic with enhanced discernment capability. We conduct experiments on various datasets and demonstrate that the proposed ICM can improve the test-time performance of various closed-source and open-source models, and the performance can be gradually improved as the data is recycled. The code and dataset will be publicly released.

</details>


### [93] [SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback](https://arxiv.org/abs/2601.18202)
*Fangyuan Xu,Rujun Han,Yanfei Chen,Zifeng Wang,I-Hung Hsu,Jun Yan,Vishy Tirumalashetty,Eunsol Choi,Tomas Pfister,Chen-Yu Lee*

Main category: cs.AI

TL;DR: SAGE：一个自动生成高质量、难度可控的深度搜索问答对的智能管道，通过数据生成器和搜索代理的交互迭代优化，显著提升深度搜索代理的性能


<details>
  <summary>Details</summary>
Motivation: 深度搜索代理需要处理跨多个文档的复杂推理问题，但人工标注成本极高，因为探索轨迹长且复杂。需要自动生成高质量、难度可控的训练数据来支持深度搜索代理的发展。

Method: 提出SAGE管道，包含两个组件：数据生成器提出QA对，搜索代理尝试解决生成的问题并提供执行反馈。两者通过多轮交互迭代优化问答对，直到达到目标难度水平。

Result: 内在评估显示SAGE生成的问题需要多样化的推理策略，同时显著提高了生成数据的正确性和难度。外在评估表明，在流行深度搜索基准测试中，使用合成数据训练的深度搜索代理获得了高达23%的相对性能提升。额外实验显示，在固定语料库检索上训练的代理可以在推理时适应Google搜索，无需额外训练。

Conclusion: SAGE能够自动生成高质量、难度可控的深度搜索训练数据，显著提升深度搜索代理的性能，并展示了良好的泛化能力，为解决深度搜索数据标注成本高的问题提供了有效方案。

Abstract: Deep search agents, which aim to answer complex questions requiring reasoning across multiple documents, can significantly speed up the information-seeking process. Collecting human annotations for this application is prohibitively expensive due to long and complex exploration trajectories. We propose an agentic pipeline that automatically generates high quality, difficulty-controlled deep search question-answer pairs for a given corpus and a target difficulty level. Our pipeline, SAGE, consists of a data generator which proposes QA pairs and a search agent which attempts to solve the generated question and provide execution feedback for the data generator. The two components interact over multiple rounds to iteratively refine the question-answer pairs until they satisfy the target difficulty level. Our intrinsic evaluation shows SAGE generates questions that require diverse reasoning strategies, while significantly increases the correctness and difficulty of the generated data. Our extrinsic evaluation demonstrates up to 23% relative performance gain on popular deep search benchmarks by training deep search agents with our synthetic data. Additional experiments show that agents trained on our data can adapt from fixed-corpus retrieval to Google Search at inference time, without further training.

</details>


### [94] [Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents](https://arxiv.org/abs/2601.18217)
*Zhihan Liu,Lin Guan,Yixin Nie,Kai Zhang,Zhuoqun Hao,Lin Chen,Asli Celikyilmaz,Zhaoran Wang,Na Zhang*

Main category: cs.AI

TL;DR: 研究探索LLM智能体在未知测试领域中的泛化能力，发现状态信息丰富度和规划复杂度是影响跨域泛化的关键因素，并提出通过添加干扰特征增强状态信息丰富度的方法。


<details>
  <summary>Details</summary>
Motivation: 通用LLM智能体通常在有限环境中进行后训练，但需要在更广泛的未知领域中部署。本研究旨在解决当最终测试领域未知时，智能体后训练面临的挑战，探究哪些环境和建模因素对跨域性能影响最大。

Method: 首先分析强化学习环境和建模选择对跨域泛化的影响，识别出两个关键环境轴：状态信息丰富度和规划复杂度。提出一种低开销的随机化技术，通过添加少量与目标无关的干扰特征来增强状态信息丰富度。同时研究建模选择，包括SFT预热/中期训练和逐步思考机制的作用。

Result: 研究发现：1）状态信息丰富度和规划复杂度与跨域泛化强相关，而领域真实性和文本相似性不是主要因素；2）仅增加状态信息丰富度就能有效提高跨域鲁棒性；3）SFT预热/中期训练能防止灾难性遗忘但会损害未包含在训练数据中的领域的泛化能力；4）逐步思考机制在保持泛化能力方面起关键作用。

Conclusion: 智能体后训练应优先考虑状态信息丰富度和规划复杂度等环境特性，而非领域真实性。提出的干扰特征添加方法能有效增强跨域鲁棒性，同时需要谨慎使用SFT训练并启用逐步思考机制以平衡性能和泛化能力。

Abstract: Generalist LLM agents are often post-trained on a narrow set of environments but deployed across far broader, unseen domains. In this work, we investigate the challenge of agentic post-training when the eventual test domains are unknown. Specifically, we analyze which properties of reinforcement learning (RL) environments and modeling choices have the greatest influence on out-of-domain performance. First, we identify two environment axes that strongly correlate with cross-domain generalization: (i) state information richness, i.e., the amount of information for the agent to process from the state, and (ii) planning complexity, estimated via goal reachability and trajectory length under a base policy. Notably, domain realism and text-level similarity are not the primary factors; for instance, the simple grid-world domain Sokoban leads to even stronger generalization in SciWorld than the more realistic ALFWorld. Motivated by these findings, we further show that increasing state information richness alone can already effectively improve cross-domain robustness. We propose a randomization technique, which is low-overhead and broadly applicable: add small amounts of distractive goal-irrelevant features to the state to make it richer without altering the task. Beyond environment-side properties, we also examine several modeling choices: (a) SFT warmup or mid-training helps prevent catastrophic forgetting during RL but undermines generalization to domains that are not included in the mid-training datamix; and (b) turning on step-by-step thinking during RL, while not always improving in-domain performance, plays a crucial role in preserving generalization.

</details>


### [95] [ShopSimulator: Evaluating and Exploring RL-Driven LLM Agent for Shopping Assistants](https://arxiv.org/abs/2601.18225)
*Pei Wang,Yanan Wu,Xiaoshuai Song,Weixun Wang,Gengru Chen,Zhongwen Li,Kezhong Yan,Ken Deng,Qi Liu,Shuaibing Zhao,Shaopan Xiong,Xuepeng Liu,Xuefeng Chen,Wanxi Deng,Wenbo Su,Bo Zheng*

Main category: cs.AI

TL;DR: ShopSimulator是一个大规模中文购物模拟环境，用于评估和训练LLM购物代理，发现现有模型成功率低于40%，通过SFT+RL训练可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏统一的购物模拟环境来全面评估LLM代理在个性化偏好理解、多轮对话、产品检索和相似产品区分等方面的能力，且只关注评估而缺乏训练支持。

Method: 提出ShopSimulator大规模中文购物环境，用于评估LLM在不同场景下的表现，并通过监督微调(SFT)和强化学习(RL)相结合的方法进行训练探索。

Result: 即使表现最好的模型完整成功率也低于40%，错误分析显示代理在长轨迹中的深度搜索和产品选择、个性化线索平衡、用户互动等方面存在困难。SFT+RL组合训练带来显著性能提升。

Conclusion: ShopSimulator为LLM购物代理提供了全面的评估和训练环境，揭示了当前模型的局限性，并为通过SFT+RL训练提升性能提供了实用指导。

Abstract: Large language model (LLM)-based agents are increasingly deployed in e-commerce shopping. To perform thorough, user-tailored product searches, agents should interpret personal preferences, engage in multi-turn dialogues, and ultimately retrieve and discriminate among highly similar products. However, existing research has yet to provide a unified simulation environment that consistently captures all of these aspects, and always focuses solely on evaluation benchmarks without training support. In this paper, we introduce ShopSimulator, a large-scale and challenging Chinese shopping environment. Leveraging ShopSimulator, we evaluate LLMs across diverse scenarios, finding that even the best-performing models achieve less than 40% full-success rate. Error analysis reveals that agents struggle with deep search and product selection in long trajectories, fail to balance the use of personalization cues, and to effectively engage with users. Further training exploration provides practical guidance for overcoming these weaknesses, with the combination of supervised fine-tuning (SFT) and reinforcement learning (RL) yielding significant performance improvements. Code and data will be released at https://github.com/ShopAgent-Team/ShopSimulator.

</details>


### [96] [Yunjue Agent Tech Report: A Fully Reproducible, Zero-Start In-Situ Self-Evolving Agent System for Open-Ended Tasks](https://arxiv.org/abs/2601.18226)
*Haotian Li,Shijun Yang,Weizhen Qi,Silei Zhao,Rui Hua,Mingzhu Song,Xiaojian Yang,Chao Peng*

Main category: cs.AI

TL;DR: 提出In-Situ Self-Evolving范式，通过工具演化实现能力扩展，开发Yunjue Agent系统，在零起点设置下显著超越基线模型


<details>
  <summary>Details</summary>
Motivation: 传统智能体系统在开放环境中面临挑战：任务分布持续漂移、外部监督稀缺，依赖静态工具集或离线训练无法适应动态变化，导致能力边界僵化未知

Method: 提出In-Situ Self-Evolving范式，将序列任务交互视为连续经验流，将短期执行反馈提炼为长期可重用能力；识别工具演化作为能力扩展关键路径；开发Yunjue Agent系统迭代合成、优化和重用工具；引入Parallel Batch Evolution策略优化演化效率

Result: 在五个多样化基准测试的零起点设置下，相比专有基线模型取得显著性能提升；补充的热启动评估证实积累的通用知识可无缝迁移到新领域；提出监测演化收敛的新指标

Conclusion: In-Situ Self-Evolving范式通过工具演化实现能力自主扩展，Yunjue Agent系统在开放环境中表现出色，为弹性自演化智能研究提供新方向

Abstract: Conventional agent systems often struggle in open-ended environments where task distributions continuously drift and external supervision is scarce. Their reliance on static toolsets or offline training lags behind these dynamics, leaving the system's capability boundaries rigid and unknown. To address this, we propose the In-Situ Self-Evolving paradigm. This approach treats sequential task interactions as a continuous stream of experience, enabling the system to distill short-term execution feedback into long-term, reusable capabilities without access to ground-truth labels. Within this framework, we identify tool evolution as the critical pathway for capability expansion, which provides verifiable, binary feedback signals. Within this framework, we develop Yunjue Agent, a system that iteratively synthesizes, optimizes, and reuses tools to navigate emerging challenges. To optimize evolutionary efficiency, we further introduce a Parallel Batch Evolution strategy. Empirical evaluations across five diverse benchmarks under a zero-start setting demonstrate significant performance gains over proprietary baselines. Additionally, complementary warm-start evaluations confirm that the accumulated general knowledge can be seamlessly transferred to novel domains. Finally, we propose a novel metric to monitor evolution convergence, serving as a function analogous to training loss in conventional optimization. We open-source our codebase, system traces, and evolved tools to facilitate future research in resilient, self-evolving intelligence.

</details>


### [97] [Think-Augmented Function Calling: Improving LLM Parameter Accuracy Through Embedded Reasoning](https://arxiv.org/abs/2601.18282)
*Lei Wei,Jinpeng Ou,Xiao Peng,Bin Wang*

Main category: cs.AI

TL;DR: 提出Think-Augmented Function Calling (TAFC)框架，通过函数和参数级别的显式推理增强LLM函数调用准确性，无需修改模型架构


<details>
  <summary>Details</summary>
Motivation: 现有LLM函数调用机制缺乏参数生成的显式推理透明度，特别是对于具有相互依赖参数的复杂函数。现有方法如思维链提示在代理级别操作，无法为单个函数参数提供细粒度推理指导

Method: 提出TAFC框架：1) 引入通用的"think"参数增强，让模型阐述决策过程；2) 动态优化参数描述以提高推理质量；3) 基于复杂度评分自动触发细粒度推理；4) 提出推理引导优化以对齐人类期望

Result: 在ToolBench上对专有和开源模型进行评估，显示在多参数函数的参数生成准确性和推理连贯性方面有显著改进，同时为调试AI代理行为提供增强的可解释性

Conclusion: TAFC框架通过显式推理显著提高了LLM函数调用的准确性和可解释性，同时保持与现有API的完全兼容性，无需修改模型架构

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in function calling for autonomous agents, yet current mechanisms lack explicit reasoning transparency during parameter generation, particularly for complex functions with interdependent parameters. While existing approaches like chain-of-thought prompting operate at the agent level, they fail to provide fine-grained reasoning guidance for individual function parameters. To address these limitations, we propose Think-Augmented Function Calling (TAFC), a novel framework that enhances function calling accuracy through explicit reasoning at both function and parameter levels. Our method introduces a universal "think" parameter augmentation that enables models to articulate their decision-making process, with dynamic optimization for parameter descriptions to improve reasoning quality. For complex parameters, TAFC automatically triggers granular reasoning based on complexity scoring, ensuring appropriate justification for critical decisions. Additionally, we propose reasoning-guided optimization to align generated reasoning with human expectations. TAFC requires no architectural modifications to existing LLMs while maintaining full API compatibility. Evaluation on ToolBench across proprietary and open-source models demonstrates significant improvements in parameter generation accuracy and reasoning coherence for multi-parameter functions, while providing enhanced interpretability for debugging AI agent behaviors.

</details>


### [98] [Can Good Writing Be Generative? Expert-Level AI Writing Emerges through Fine-Tuning on High-Quality Books](https://arxiv.org/abs/2601.18353)
*Tuhin Chakrabarty,Paramveer S. Dhillon*

Main category: cs.AI

TL;DR: 研究通过实验发现，经过微调的AI写作在专家评审中胜过人类作家，而普通评审一贯偏好AI写作，这引发了专家作家的身份危机和创意劳动未来的根本问题。


<details>
  <summary>Details</summary>
Motivation: 挑战传统认为创意写作是机器无法复制的独特人类活动的假设，探究生成式AI在模仿作家风格方面的能力，以及AI写作对人类创意劳动的影响。

Method: 行为实验：28名MFA作家（专家）与三个LLM竞争模仿50位备受赞誉的作家风格；通过28名专家评审和131名普通评审进行盲审配对比较；采用上下文提示和完整作品微调两种条件。

Result: 在上下文提示条件下，专家82.7%偏好人类写作；但在微调后，62%偏好AI写作。普通评审一贯偏好AI写作。专家访谈显示AI偏好引发了身份危机和审美自信侵蚀。

Conclusion: AI在创意写作方面的能力挑战了关于AI创意局限的论述，引发了关于创意劳动未来的根本问题，特别是专家作家面临的身份危机和审美标准重构。

Abstract: Creative writing has long been considered a uniquely human endeavor, requiring voice and style that machines could not replicate. This assumption is challenged by Generative AI that can emulate thousands of author styles in seconds with negligible marginal labor. To understand this better, we conducted a behavioral experiment where 28 MFA writers (experts) competed against three LLMs in emulating 50 critically acclaimed authors. Based on blind pairwise comparisons by 28 expert judges and 131 lay judges, we find that experts preferred human writing in 82.7% of cases under the in-context prompting condition but this reversed to 62% preference for AI after fine-tuning on authors' complete works. Lay judges, however, consistently preferred AI writing. Debrief interviews with expert writers revealed that their preference for AI writing triggered an identity crisis, eroding aesthetic confidence and questioning what constitutes "good writing." These findings challenge discourse about AI's creative limitations and raise fundamental questions about the future of creative labor.

</details>


### [99] [AI Agent for Reverse-Engineering Legacy Finite-Difference Code and Translating to Devito](https://arxiv.org/abs/2601.18381)
*Yinghan Hou,Zongyou Yang*

Main category: cs.AI

TL;DR: 开发了一个集成AI代理框架，将传统有限差分实现转换为Devito环境，结合RAG和开源大语言模型，通过多阶段迭代工作流和知识图谱优化查询性能，实现代码合成和验证。


<details>
  <summary>Details</summary>
Motivation: 为了促进传统有限差分实现向Devito环境的转换，解决代码迁移和重构的挑战，需要开发一个智能的自动化框架来简化这一过程。

Method: 采用混合LangGraph架构，结合RAG和开源大语言模型，构建Devito知识图谱，通过多阶段检索管道（并行搜索、概念扩展、社区规模检索、语义相似性分析）和Pydantic约束的代码合成，以及包含静态分析和G-Eval的验证框架。

Result: 开发了一个完整的AI代理框架，能够通过知识图谱优化查询性能，支持地震波模拟、计算流体动力学和性能调优库等语义社区，实现从Fortran源代码到Devito代码的精确转换。

Conclusion: 该研究的主要贡献在于整合了受强化学习启发的反馈机制，使系统从静态代码翻译转向动态自适应分析行为，为科学计算代码迁移提供了创新的AI驱动解决方案。

Abstract: To facilitate the transformation of legacy finite difference implementations into the Devito environment, this study develops an integrated AI agent framework. Retrieval-Augmented Generation (RAG) and open-source Large Language Models are combined through multi-stage iterative workflows in the system's hybrid LangGraph architecture. The agent constructs an extensive Devito knowledge graph through document parsing, structure-aware segmentation, extraction of entity relationships, and Leiden-based community detection. GraphRAG optimisation enhances query performance across semantic communities that include seismic wave simulation, computational fluid dynamics, and performance tuning libraries. A reverse engineering component derives three-level query strategies for RAG retrieval through static analysis of Fortran source code. To deliver precise contextual information for language model guidance, the multi-stage retrieval pipeline performs parallel searching, concept expansion, community-scale retrieval, and semantic similarity analysis. Code synthesis is governed by Pydantic-based constraints to guarantee structured outputs and reliability. A comprehensive validation framework integrates conventional static analysis with the G-Eval approach, covering execution correctness, structural soundness, mathematical consistency, and API compliance. The overall agent workflow is implemented on the LangGraph framework and adopts concurrent processing to support quality-based iterative refinement and state-aware dynamic routing. The principal contribution lies in the incorporation of feedback mechanisms motivated by reinforcement learning, enabling a transition from static code translation toward dynamic and adaptive analytical behavior.

</details>


### [100] [Dynamic Thinking-Token Selection for Efficient Reasoning in Large Reasoning Models](https://arxiv.org/abs/2601.18383)
*Zhenyuan Guo,Tong Chen,Wenlong Meng,Chen Gong,Xin Yu,Chengkun Wei,Wenzhi Chen*

Main category: cs.AI

TL;DR: DynTS方法通过识别推理轨迹中的关键决策token并仅保留其KV缓存，显著提升大型推理模型的效率


<details>
  <summary>Details</summary>
Motivation: 大型推理模型生成详细推理轨迹时会产生巨大的内存占用和计算开销，这成为模型效率的瓶颈。研究发现推理轨迹中只有部分关键token真正影响最终答案，其余token贡献甚微。

Method: 提出动态思维token选择（DynTS）方法，利用注意力图分析推理轨迹的影响，识别决策关键token，在推理过程中仅保留这些关键token的KV缓存状态，剔除冗余条目。

Result: 该方法通过选择性保留关键token的KV缓存，显著减少了内存占用和计算开销，优化了大型推理模型的推理效率。

Conclusion: DynTS方法通过分析注意力图识别推理轨迹中的关键决策token，实现了高效的KV缓存管理，为大型推理模型的效率优化提供了新思路。

Abstract: Large Reasoning Models (LRMs) excel at solving complex problems by explicitly generating a reasoning trace before deriving the final answer. However, these extended generations incur substantial memory footprint and computational overhead, bottlenecking LRMs' efficiency. This work uses attention maps to analyze the influence of reasoning traces and uncover an interesting phenomenon: only some decision-critical tokens in a reasoning trace steer the model toward the final answer, while the remaining tokens contribute negligibly. Building on this observation, we propose Dynamic Thinking-Token Selection (DynTS). This method identifies decision-critical tokens and retains only their associated Key-Value (KV) cache states during inference, evicting the remaining redundant entries to optimize efficiency.

</details>


### [101] [OffSeeker: Online Reinforcement Learning Is Not All You Need for Deep Research Agents](https://arxiv.org/abs/2601.18467)
*Yuhang Zhou,Kai Zheng,Qiguang Chen,Mengkang Hu,Qingfeng Sun,Can Xu,Jingjing Chen*

Main category: cs.AI

TL;DR: 论文提出完全离线训练的研究智能体OffSeeker(8B)，通过开源套件和合成数据，在无需昂贵在线强化学习的情况下达到与30B参数在线RL系统相当的竞争力


<details>
  <summary>Details</summary>
Motivation: 当前深度研究智能体依赖昂贵的在线强化学习，需要大量API调用，成本高昂。而离线训练虽然更高效，但受限于高质量研究轨迹数据的稀缺

Method: 1) 开发DeepForge任务合成框架，无需大量预处理即可生成大规模研究查询；2) 构建包含66k QA对、33k SFT轨迹和21k DPO对的数据集；3) 完全离线训练8B参数的OffSeeker模型

Result: 在六个基准测试上的广泛评估显示，OffSeeker不仅在同尺寸智能体中领先，还能与通过大量在线RL训练的30B参数系统保持竞争力

Conclusion: 昂贵的在线强化学习并非构建强大研究智能体的唯一途径，通过开源套件和高质量合成数据，完全离线训练也能达到优秀性能

Abstract: Deep research agents have shown remarkable potential in handling long-horizon tasks. However, state-of-the-art performance typically relies on online reinforcement learning (RL), which is financially expensive due to extensive API calls. While offline training offers a more efficient alternative, its progress is hindered by the scarcity of high-quality research trajectories. In this paper, we demonstrate that expensive online reinforcement learning is not all you need to build powerful research agents. To bridge this gap, we introduce a fully open-source suite designed for effective offline training. Our core contributions include DeepForge, a ready-to-use task synthesis framework that generates large-scale research queries without heavy preprocessing; and a curated collection of 66k QA pairs, 33k SFT trajectories, and 21k DPO pairs. Leveraging these resources, we train OffSeeker (8B), a model developed entirely offline. Extensive evaluations across six benchmarks show that OffSeeker not only leads among similar-sized agents but also remains competitive with 30B-parameter systems trained via heavy online RL.

</details>


### [102] [AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security](https://arxiv.org/abs/2601.18491)
*Dongrui Liu,Qihan Ren,Chen Qian,Shuai Shao,Yuejin Xie,Yu Li,Zhonghao Yang,Haoyu Luo,Peng Wang,Qingyu Liu,Binxin Hu,Ling Tang,Jilin Mei,Dadi Guo,Leitao Yuan,Junyao Yang,Guanxu Chen,Qihao Lin,Yi Yu,Bo Zhang,Jiaxuan Guo,Jie Zhang,Wenqi Shao,Huiqi Deng,Zhiheng Xi,Wenjie Wang,Wenxuan Wang,Wen Shen,Zhikai Chen,Haoyu Xie,Jialing Tao,Juntao Dai,Jiaming Ji,Zhongjie Ba,Linfeng Zhang,Yong Liu,Quanshi Zhang,Lei Zhu,Zhihua Wei,Hui Xue,Chaochao Lu,Jing Shao,Xia Hu*

Main category: cs.AI

TL;DR: 提出AgentDoG框架，通过三维风险分类法（来源、失效模式、后果）构建细粒度智能体安全基准ATBench，实现透明化风险诊断与监控


<details>
  <summary>Details</summary>
Motivation: 当前护栏模型缺乏对智能体风险的感知能力和风险诊断的透明度，无法应对自主工具使用和环境交互带来的复杂安全挑战

Method: 提出统一的三维风险分类法，基于此构建ATBench基准，开发AgentDoG诊断护栏框架，提供细粒度的上下文监控和根因分析

Result: AgentDoG在多样复杂交互场景中实现最先进的智能体安全调节性能，提供三个规模版本（4B、7B、8B参数）

Conclusion: AgentDoG通过结构化风险分类和透明化诊断，为智能体对齐提供了超越二元标签的有效解决方案，所有模型和数据集已开源

Abstract: The rise of AI agents introduces complex safety and security challenges arising from autonomous tool use and environmental interactions. Current guardrail models lack agentic risk awareness and transparency in risk diagnosis. To introduce an agentic guardrail that covers complex and numerous risky behaviors, we first propose a unified three-dimensional taxonomy that orthogonally categorizes agentic risks by their source (where), failure mode (how), and consequence (what). Guided by this structured and hierarchical taxonomy, we introduce a new fine-grained agentic safety benchmark (ATBench) and a Diagnostic Guardrail framework for agent safety and security (AgentDoG). AgentDoG provides fine-grained and contextual monitoring across agent trajectories. More Crucially, AgentDoG can diagnose the root causes of unsafe actions and seemingly safe but unreasonable actions, offering provenance and transparency beyond binary labels to facilitate effective agent alignment. AgentDoG variants are available in three sizes (4B, 7B, and 8B parameters) across Qwen and Llama model families. Extensive experimental results demonstrate that AgentDoG achieves state-of-the-art performance in agentic safety moderation in diverse and complex interactive scenarios. All models and datasets are openly released.

</details>


### [103] [DEEPMED: Building a Medical DeepResearch Agent via Multi-hop Med-Search Data and Turn-Controlled Agentic Training & Inference](https://arxiv.org/abs/2601.18496)
*Zihan wang,Hao Wang,Shi Feng,Xiaocui Yang,Daling Wang,Yiqun Zhang,Jinghao Lin,Haihua Yang,Xiaozhong Ji*

Main category: cs.AI

TL;DR: DeepMed：针对医学领域优化的深度研究模型，通过多跳医学搜索QA合成、难度感知轮次惩罚和推理监控，解决通用DR模型在医学场景中的任务特性和工具使用扩展问题，在7个医学基准上平均提升9.79%


<details>
  <summary>Details</summary>
Motivation: 通用深度研究(DR)模型直接迁移到医学领域效果有限，存在两个关键差距：1) 医学问题需要在知识密集的临床背景下进行证据解释，通用DR模型缺乏临床上下文推理能力；2) 盲目扩展工具调用会注入噪声上下文，干扰敏感的医学推理并导致错误的证据搜索路径

Method: 1) 数据：采用多跳医学搜索QA合成方法，支持模型在医学上下文中应用DR范式；2) 训练：引入难度感知轮次惩罚，抑制过度工具调用增长；3) 推理：加入监控机制，在可控步骤内验证假设，避免上下文腐化

Result: 在七个医学基准测试中，DeepMed相比基础模型平均提升9.79%，并超越了更大的医学推理和DR模型

Conclusion: DeepMed通过针对医学领域特性的优化，有效解决了通用DR模型在医学场景中的局限性，显著提升了医学推理性能，为医学AI研究提供了新的解决方案

Abstract: Medical reasoning models remain constrained by parametric knowledge and are thus susceptible to forgetting and hallucinations. DeepResearch (DR) models ground outputs in verifiable evidence from tools and perform strongly in general domains, but their direct transfer to medical field yields relatively limited gains. We attribute this to two gaps: task characteristic and tool-use scaling. Medical questions require evidence interpretation in a knowledge-intensive clinical context; while general DR models can retrieve information, they often lack clinical-context reasoning and thus "find it but fail to use it," leaving performance limited by medical abilities. Moreover, in medical scenarios, blindly scaling tool-call can inject noisy context, derailing sensitive medical reasoning and prompting repetitive evidence-seeking along incorrect paths. Therefore, we propose DeepMed. For data, we deploy a multi-hop med-search QA synthesis method supporting the model to apply the DR paradigm in medical contexts. For training, we introduce a difficulty-aware turn-penalty to suppress excessive tool-call growth. For inference, we bring a monitor to help validate hypotheses within a controlled number of steps and avoid context rot. Overall, on seven medical benchmarks, DeepMed improves its base model by 9.79\% on average and outperforms larger medical reasoning and DR models.

</details>


### [104] [Deconstructing Instruction-Following: A New Benchmark for Granular Evaluation of Large Language Model Instruction Compliance Abilities](https://arxiv.org/abs/2601.18554)
*Alberto Purpura,Li Wang,Sahil Badyal,Eugenio Beaufrand,Adam Faulkner*

Main category: cs.AI

TL;DR: MOSAIC是一个模块化框架，通过动态生成包含多达20个应用导向约束的数据集，对LLM的指令遵循能力进行细粒度独立分析，发现遵循能力不是单一能力，而是随约束类型、数量和位置显著变化。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试往往无法反映真实世界使用情况，也无法将指令遵循能力与任务成功分开评估。可靠确保大语言模型遵循复杂指令是一个关键挑战。

Method: 引入MOSAIC（模块化指令遵循合成评估）框架，使用动态生成的数据集，包含多达20个应用导向的生成约束，实现对指令遵循能力的细粒度独立分析。

Result: 对五个不同家族的LLM评估显示：遵循能力不是单一能力，而是随约束类型、数量和位置显著变化；揭示了模型特定弱点、指令间的协同和冲突交互，以及首因效应和近因效应等位置偏差。

Conclusion: 这些细粒度洞察对于诊断模型失败、开发需要严格遵循复杂指令的系统中的更可靠LLM至关重要。

Abstract: Reliably ensuring Large Language Models (LLMs) follow complex instructions is a critical challenge, as existing benchmarks often fail to reflect real-world use or isolate compliance from task success. We introduce MOSAIC (MOdular Synthetic Assessment of Instruction Compliance), a modular framework that uses a dynamically generated dataset with up to 20 application-oriented generation constraints to enable a granular and independent analysis of this capability. Our evaluation of five LLMs from different families based on this new benchmark demonstrates that compliance is not a monolithic capability but varies significantly with constraint type, quantity, and position. The analysis reveals model-specific weaknesses, uncovers synergistic and conflicting interactions between instructions, and identifies distinct positional biases such as primacy and recency effects. These granular insights are critical for diagnosing model failures and developing more reliable LLMs for systems that demand strict adherence to complex instructions.

</details>


### [105] [Stability as a Liability:Systematic Breakdown of Linguistic Structure in LLMs](https://arxiv.org/abs/2601.18588)
*Xianzhe Meng,Qiangsheng Zeng,Ling Luo,Qinghan Yang,Jiarui Hao,Wenbo Wu,Qinyu Wang,Rui Yin,Lin Qi,Renzhi Lu*

Main category: cs.AI

TL;DR: 稳定训练动态会导致生成分布熵降低，模型集中于有限的经验模式子集，产生重复行为，表明优化稳定性与生成表达能力并不一致


<details>
  <summary>Details</summary>
Motivation: 分析训练稳定性如何影响生成分布，揭示稳定性与生成质量之间的潜在冲突

Method: 理论分析最大似然训练下稳定参数轨迹与KL散度最小化的关系，并使用基于反馈的训练框架进行实证验证

Result: 稳定训练导致模型输出熵降低、重复行为，即使损失平滑收敛也会出现系统性退化

Conclusion: 优化稳定性与生成表达能力并不一致，稳定性本身不足以指示生成质量

Abstract: Training stability is typically regarded as a prerequisite for reliable optimization in large language models. In this work, we analyze how stabilizing training dynamics affects the induced generation distribution. We show that under standard maximum likelihood training, stable parameter trajectories lead stationary solutions to approximately minimize the forward KL divergence to the empirical distribution, while implicitly reducing generative entropy. As a consequence, the learned model can concentrate probability mass on a limited subset of empirical modes, exhibiting systematic degeneration despite smooth loss convergence. We empirically validate this effect using a controlled feedback-based training framework that stabilizes internal generation statistics, observing consistent low-entropy outputs and repetitive behavior across architectures and random seeds. It indicates that optimization stability and generative expressivity are not inherently aligned, and that stability alone is an insufficient indicator of generative quality.

</details>


### [106] [A Balanced Neuro-Symbolic Approach for Commonsense Abductive Logic](https://arxiv.org/abs/2601.18595)
*Joseph Cotnareanu,Didier Chetelat,Yingxue Zhang,Mark Coates*

Main category: cs.AI

TL;DR: 提出一种结合LLM与逻辑求解器的方法，通过迭代反馈机制补充缺失的常识关系，提升复杂逻辑推理能力


<details>
  <summary>Details</summary>
Motivation: LLM在复杂证明规划中表现不佳，而传统逻辑求解器无法处理缺失的常识关系，需要结合两者优势

Method: 使用逻辑求解器的反馈迭代增强逻辑问题，通过搜索算法寻找有用的常识假设，平衡神经与符号元素

Result: 在去除部分常识信息的纯逻辑推理数据集上，相比现有技术取得显著改进

Conclusion: 在人类语境下工作时，平衡神经与符号元素具有重要价值，结合LLM与逻辑求解器能有效提升推理能力

Abstract: Although Large Language Models (LLMs) have demonstrated impressive formal reasoning abilities, they often break down when problems require complex proof planning. One promising approach for improving LLM reasoning abilities involves translating problems into formal logic and using a logic solver. Although off-the-shelf logic solvers are in principle substantially more efficient than LLMs at logical reasoning, they assume that all relevant facts are provided in a question and are unable to deal with missing commonsense relations. In this work, we propose a novel method that uses feedback from the logic solver to augment a logic problem with commonsense relations provided by the LLM, in an iterative manner. This involves a search procedure through potential commonsense assumptions to maximize the chance of finding useful facts while keeping cost tractable. On a collection of pure-logical reasoning datasets, from which some commonsense information has been removed, our method consistently achieves considerable improvements over existing techniques, demonstrating the value in balancing neural and symbolic elements when working in human contexts.

</details>


### [107] [PolySHAP: Extending KernelSHAP with Interaction-Informed Polynomial Regression](https://arxiv.org/abs/2601.18608)
*Fabian Fumagalli,R. Teal Witter,Christopher Musco*

Main category: cs.AI

TL;DR: PolySHAP扩展KernelSHAP，使用高阶多项式近似特征交互，提供更好的Shapley值估计，并证明配对采样等价于二阶PolySHAP


<details>
  <summary>Details</summary>
Motivation: KernelSHAP使用线性函数近似游戏，但无法捕捉特征间的非线性交互。需要更准确的Shapley值估计方法

Method: 提出PolySHAP方法，使用高阶多项式（而非线性函数）来近似游戏，通过少量随机特征子集的游戏评估来拟合多项式

Result: PolySHAP在各种基准数据集上提供更好的Shapley值估计，证明估计具有一致性。发现配对采样等价于二阶PolySHAP

Conclusion: PolySHAP通过高阶多项式近似改进了Shapley值估计，并为配对采样启发式方法提供了首个强有力的理论依据

Abstract: Shapley values have emerged as a central game-theoretic tool in explainable AI (XAI). However, computing Shapley values exactly requires $2^d$ game evaluations for a model with $d$ features. Lundberg and Lee's KernelSHAP algorithm has emerged as a leading method for avoiding this exponential cost. KernelSHAP approximates Shapley values by approximating the game as a linear function, which is fit using a small number of game evaluations for random feature subsets.
  In this work, we extend KernelSHAP by approximating the game via higher degree polynomials, which capture non-linear interactions between features. Our resulting PolySHAP method yields empirically better Shapley value estimates for various benchmark datasets, and we prove that these estimates are consistent.
  Moreover, we connect our approach to paired sampling (antithetic sampling), a ubiquitous modification to KernelSHAP that improves empirical accuracy. We prove that paired sampling outputs exactly the same Shapley value approximations as second-order PolySHAP, without ever fitting a degree 2 polynomial. To the best of our knowledge, this finding provides the first strong theoretical justification for the excellent practical performance of the paired sampling heuristic.

</details>


### [108] [Emergence of Phonemic, Syntactic, and Semantic Representations in Artificial Neural Networks](https://arxiv.org/abs/2601.18617)
*Pierre Orhan,Pablo Diego-Simón,Emmnanuel Chemla,Yair Lakretz,Yves Boubenec,Jean-Rémi King*

Main category: cs.AI

TL;DR: 论文研究人工神经网络在训练过程中是否以及何时会自发形成音位、词汇和句法表征，发现其学习阶段与儿童语言习得相似但数据需求量大得多。


<details>
  <summary>Details</summary>
Motivation: 儿童语言习得过程中依次学习音位分类、词汇识别和句法组合，但缺乏统一的计算框架来解释其背后的神经表征。本研究旨在探究人工神经网络在训练过程中是否以及何时会自发形成这些语言表征。

Method: 研究基于语音和文本的人工神经网络模型，分析其在训练过程中神经激活的变化。通过检查神经激活子空间的几何结构，观察音位、词汇和句法表征的逐步形成过程。

Result: 研究发现语音和文本模型都遵循相似的学习阶段序列：在训练过程中，神经激活逐步构建出代表音位、词汇和句法结构的子空间。虽然这种发展轨迹在质上与儿童语言习得相似，但在量上存在显著差异——这些算法需要比儿童多2-4个数量级的数据才能形成这些神经表征。

Conclusion: 研究结果表明，在特定条件下，语言习得的主要阶段会自发出现，这为理解语言习得背后的计算机制提供了一条有前景的研究路径。

Abstract: During language acquisition, children successively learn to categorize phonemes, identify words, and combine them with syntax to form new meaning. While the development of this behavior is well characterized, we still lack a unifying computational framework to explain its underlying neural representations. Here, we investigate whether and when phonemic, lexical, and syntactic representations emerge in the activations of artificial neural networks during their training. Our results show that both speech- and text-based models follow a sequence of learning stages: during training, their neural activations successively build subspaces, where the geometry of the neural activations represents phonemic, lexical, and syntactic structure. While this developmental trajectory qualitatively relates to children's, it is quantitatively different: These algorithms indeed require two to four orders of magnitude more data for these neural representations to emerge. Together, these results show conditions under which major stages of language acquisition spontaneously emerge, and hence delineate a promising path to understand the computations underpinning language acquisition.

</details>


### [109] [Assessing the Quality of Mental Health Support in LLM Responses through Multi-Attribute Human Evaluation](https://arxiv.org/abs/2601.18630)
*Abeer Badawi,Md Tahmid Rahman Laskar,Elahe Rahimi,Sheri Grach,Lindsay Bertrand,Lames Danok,Frank Rudzicz,Jimmy Huang,Elham Dolatabadi*

Main category: cs.AI

TL;DR: 该研究提出了一种基于人类专家评估的方法，用于评估大语言模型在心理健康对话中的表现，发现LLMs在认知支持方面表现可靠，但在情感共鸣方面存在不稳定，揭示了认知-情感差距。


<details>
  <summary>Details</summary>
Motivation: 全球心理健康危机日益严重，存在治疗缺口、资源不足和合格治疗师短缺的问题。大语言模型作为可扩展的心理支持工具具有潜力，但其可靠性、治疗相关性和与人类标准的对齐仍面临挑战，需要建立科学的评估方法。

Method: 研究开发了基于人类专家的评估方法：1）从真实场景数据集中收集500个心理健康对话；2）使用9个不同的LLMs（包括闭源和开源模型）生成响应；3）由两位经过精神病学培训的专家独立使用5点李克特量表，基于包含6个属性的评估框架（涵盖认知支持和情感共鸣）进行评分。

Result: LLMs在认知可靠性方面表现良好，能提供安全、连贯且临床适当的信息，但在情感对齐方面不稳定。闭源模型（如GPT-4o）能提供更平衡的治疗响应，而开源模型表现出更大的变异性和情感平淡。研究揭示了持续的认知-情感差距。

Conclusion: 需要建立具有失败意识、临床基础的评估框架，在心理健康导向的LLMs中优先考虑关系敏感性而不仅仅是信息准确性。提倡采用以人为中心的平衡评估协议，关注治疗敏感性，为心理健康对话AI的责任设计和临床监督提供指导框架。

Abstract: The escalating global mental health crisis, marked by persistent treatment gaps, availability, and a shortage of qualified therapists, positions Large Language Models (LLMs) as a promising avenue for scalable support. While LLMs offer potential for accessible emotional assistance, their reliability, therapeutic relevance, and alignment with human standards remain challenging to address. This paper introduces a human-grounded evaluation methodology designed to assess LLM generated responses in therapeutic dialogue. Our approach involved curating a dataset of 500 mental health conversations from datasets with real-world scenario questions and evaluating the responses generated by nine diverse LLMs, including closed source and open source models. More specifically, these responses were evaluated by two psychiatric trained experts, who independently rated each on a 5 point Likert scale across a comprehensive 6 attribute rubric. This rubric captures Cognitive Support and Affective Resonance, providing a multidimensional perspective on therapeutic quality. Our analysis reveals that LLMs provide strong cognitive reliability by producing safe, coherent, and clinically appropriate information, but they demonstrate unstable affective alignment. Although closed source models (e.g., GPT-4o) offer balanced therapeutic responses, open source models show greater variability and emotional flatness. We reveal a persistent cognitive-affective gap and highlight the need for failure aware, clinically grounded evaluation frameworks that prioritize relational sensitivity alongside informational accuracy in mental health oriented LLMs. We advocate for balanced evaluation protocols with human in the loop that center on therapeutic sensitivity and provide a framework to guide the responsible design and clinical oversight of mental health oriented conversational AI.

</details>


### [110] [AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning](https://arxiv.org/abs/2601.18631)
*Mingyang Song,Haoyu Sun,Jiawei Gu,Linjie Li,Luxin Xu,Ranjay Krishna,Yu Cheng*

Main category: cs.AI

TL;DR: AdaReasoner是一个多模态模型家族，通过学习工具使用作为通用推理技能，而非特定工具或显式监督行为，实现了在视觉推理任务中的自主工具选择和组合能力。


<details>
  <summary>Details</summary>
Motivation: 人类在面对超出自身能力的问题时会依赖工具，这为提高多模态大语言模型的视觉推理能力提供了一个有前景的范式。有效的推理需要知道使用哪些工具、何时调用它们以及如何在多步骤中组合它们，即使面对新工具或新任务时也是如此。

Method: AdaReasoner通过三个关键组件实现：(1) 可扩展的数据管理流程，让模型接触长视野、多步骤的工具交互；(2) Tool-GRPO强化学习算法，基于最终任务成功优化工具选择和序列；(3) 自适应学习机制，动态调节工具使用频率。

Result: AdaReasoner表现出强大的工具适应和泛化能力：能自主采用有益工具、抑制无关工具、根据任务需求调整工具使用频率，尽管从未被明确训练这样做。在多个基准测试中取得最先进性能，将7B基础模型平均提升24.9%，并在VSP和Jigsaw等任务上超越GPT-5等专有系统。

Conclusion: AdaReasoner通过学习工具使用作为通用推理技能，实现了自主的工具适应和泛化能力，在多模态视觉推理任务中取得了突破性进展，为模型在复杂任务中的工具使用提供了新的解决方案。

Abstract: When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw.

</details>


### [111] [FadeMem: Biologically-Inspired Forgetting for Efficient Agent Memory](https://arxiv.org/abs/2601.18642)
*Lei Wei,Xu Dong,Xiao Peng,Niantao Xie,Bin Wang*

Main category: cs.AI

TL;DR: FadeMem：受生物启发的智能体记忆架构，通过主动遗忘机制解决LLM记忆限制，实现选择性保留与遗忘的平衡


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型作为自主智能体部署时面临严重记忆限制，缺乏选择性遗忘机制，导致要么在上下文边界发生灾难性遗忘，要么在边界内信息过载。人类记忆通过自适应衰减过程自然平衡保留与遗忘，而现有AI系统采用二元保留策略（要么全部保留要么全部丢失）

Method: 提出FadeMem架构，采用受生物启发的主动遗忘机制，在双层记忆层次结构中实现差异化衰减率。保留由自适应指数衰减函数控制，该函数受语义相关性、访问频率和时间模式调制。通过LLM引导的冲突解决和智能记忆融合，系统在合并相关信息的同时允许无关细节逐渐消失

Result: 在Multi-Session Chat、LoCoMo和LTI-Bench上的实验表明，系统在多跳推理和检索方面表现优越，同时实现45%的存储减少，验证了生物启发式遗忘在智能体记忆系统中的有效性

Conclusion: FadeMem通过引入受人类认知启发的主动遗忘机制，成功解决了LLM智能体的记忆限制问题，实现了保留与遗忘的平衡，显著提高了记忆效率和推理能力

Abstract: Large language models deployed as autonomous agents face critical memory limitations, lacking selective forgetting mechanisms that lead to either catastrophic forgetting at context boundaries or information overload within them. While human memory naturally balances retention and forgetting through adaptive decay processes, current AI systems employ binary retention strategies that preserve everything or lose it entirely. We propose FadeMem, a biologically-inspired agent memory architecture that incorporates active forgetting mechanisms mirroring human cognitive efficiency. FadeMem implements differential decay rates across a dual-layer memory hierarchy, where retention is governed by adaptive exponential decay functions modulated by semantic relevance, access frequency, and temporal patterns. Through LLM-guided conflict resolution and intelligent memory fusion, our system consolidates related information while allowing irrelevant details to fade. Experiments on Multi-Session Chat, LoCoMo, and LTI-Bench demonstrate superior multi-hop reasoning and retrieval with 45\% storage reduction, validating the effectiveness of biologically-inspired forgetting in agent memory systems.

</details>


### [112] [TEA-Bench: A Systematic Benchmarking of Tool-enhanced Emotional Support Dialogue Agent](https://arxiv.org/abs/2601.18700)
*Xingyu Sui,Yanyan Zhao,Yulin Hu,Jiahe Guo,Weixiang Zhao,Bing Qin*

Main category: cs.AI

TL;DR: TEA-Bench是首个用于评估工具增强情感支持对话代理的交互式基准，包含真实情感场景和工具环境，实验显示工具增强能提升支持质量并减少幻觉，但效果与模型能力密切相关。


<details>
  <summary>Details</summary>
Motivation: 现有情感支持对话系统和基准主要关注文本环境中的情感支持，忽视了外部工具如何提供事实基础并减少多轮情感支持中的幻觉问题。

Method: 引入TEA-Bench基准，包含真实情感场景、MCP风格工具环境，以及评估情感支持质量和事实基础的过程级指标。在9个LLM上进行实验，并发布工具增强的情感支持对话数据集TEA-Dialog。

Result: 工具增强通常能提高情感支持质量并减少幻觉，但效果强烈依赖于模型能力：更强的模型能更选择性和有效地使用工具，而较弱模型获益有限。监督微调在分布内表现改善但泛化能力差。

Conclusion: 工具使用对于构建可靠的情感支持代理至关重要，需要开发能够有效利用工具提供事实基础情感支持的系统。

Abstract: Emotional Support Conversation requires not only affective expression but also grounded instrumental support to provide trustworthy guidance. However, existing ESC systems and benchmarks largely focus on affective support in text-only settings, overlooking how external tools can enable factual grounding and reduce hallucination in multi-turn emotional support. We introduce TEA-Bench, the first interactive benchmark for evaluating tool-augmented agents in ESC, featuring realistic emotional scenarios, an MCP-style tool environment, and process-level metrics that jointly assess the quality and factual grounding of emotional support. Experiments on nine LLMs show that tool augmentation generally improves emotional support quality and reduces hallucination, but the gains are strongly capacity-dependent: stronger models use tools more selectively and effectively, while weaker models benefit only marginally. We further release TEA-Dialog, a dataset of tool-enhanced ESC dialogues, and find that supervised fine-tuning improves in-distribution support but generalizes poorly. Our results underscore the importance of tool use in building reliable emotional support agents.

</details>


### [113] [Health-SCORE: Towards Scalable Rubrics for Improving Health-LLMs](https://arxiv.org/abs/2601.18706)
*Zhichao Yang,Sepehr Janghorbani,Dongxu Zhang,Jun Han,Qian Qian,Andrew Ressler,Gregory D. Lyng,Sanjit Singh Batra,Robert E. Tillman*

Main category: cs.AI

TL;DR: Health-SCORE：一个可扩展的基于量规的LLM评估与训练框架，显著降低医疗领域量规开发成本


<details>
  <summary>Details</summary>
Motivation: 在医疗等安全关键领域，高质量量规对评估开放式LLM响应至关重要，但人工创建量规需要大量专家时间和开发成本，难以规模化

Method: 提出Health-SCORE框架，通过自动生成量规显著降低开发成本，同时支持作为强化学习的结构化奖励信号和提示工程中的上下文学习

Result: 在开放式医疗任务中，Health-SCORE达到与人工创建量规相当的评估质量，同时大幅降低开发工作量，使量规评估和训练更具可扩展性

Conclusion: Health-SCORE为医疗领域提供了一种通用、可扩展的量规评估与训练解决方案，解决了传统量规开发成本高、难以规模化的问题

Abstract: Rubrics are essential for evaluating open-ended LLM responses, especially in safety-critical domains such as healthcare. However, creating high-quality and domain-specific rubrics typically requires significant human expertise time and development cost, making rubric-based evaluation and training difficult to scale. In this work, we introduce Health-SCORE, a generalizable and scalable rubric-based training and evaluation framework that substantially reduces rubric development costs without sacrificing performance. We show that Health-SCORE provides two practical benefits beyond standalone evaluation: it can be used as a structured reward signal to guide reinforcement learning with safety-aware supervision, and it can be incorporated directly into prompts to improve response quality through in-context learning. Across open-ended healthcare tasks, Health-SCORE achieves evaluation quality comparable to human-created rubrics while significantly lowering development effort, making rubric-based evaluation and training more scalable.

</details>


### [114] [Conditioned Generative Modeling of Molecular Glues: A Realistic AI Approach for Synthesizable Drug-like Molecules](https://arxiv.org/abs/2601.18716)
*Naeyma N. Islam,Thomas R. Caulfield*

Main category: cs.AI

TL;DR: AI辅助药物设计方法，通过E3连接酶导向的分子胶促进Aβ-42靶向降解，用于阿尔茨海默病治疗


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病中细胞内Aβ-42的积累是疾病进展的早期毒性驱动因素，需要开发靶向降解策略

Method: 使用结构建模、ADMET筛选和对接评估Aβ-42与三种E3连接酶的复合物形成潜力，开发LC-JT-VAE生成连接酶特异性小分子

Result: 生成模型能够产生化学有效、新颖且靶向特异性的分子胶，能够促进Aβ-42降解

Conclusion: 这种集成方法为设计神经退行性疾病的UPS靶向疗法提供了有前景的框架

Abstract: Alzheimer's disease (AD) is marked by the pathological accumulation of amyloid beta-42 (Abeta-42), contributing to synaptic dysfunction and neurodegeneration. While extracellular amyloid plaques are well-studied, increasing evidence highlights intracellular Abeta-42 as an early and toxic driver of disease progression. In this study, we present a novel, AI-assisted drug design approach to promote targeted degradation of Abeta-42 via the ubiquitin-proteasome system (UPS), using E3 ligase-directed molecular glues. We systematically evaluated the ternary complex formation potential of Abeta-42 with three E3 ligases: CRBN, VHL, and MDM2, through structure-based modeling, ADMET screening, and docking. We then developed a Ligase-Conditioned Junction Tree Variational Autoencoder (LC-JT-VAE) to generate ligase-specific small molecules, incorporating protein sequence embeddings and torsional angle-aware molecular graphs. Our results demonstrate that this generative model can produce chemically valid, novel, and target-specific molecular glues capable of facilitating Abeta-42 degradation. This integrated approach offers a promising framework for designing UPS-targeted therapies for neurodegenerative diseases.

</details>


### [115] [Why Keep Your Doubts to Yourself? Trading Visual Uncertainties in Multi-Agent Bandit Systems](https://arxiv.org/abs/2601.18735)
*Jusheng Zhang,Yijia Fan,Kaitong Cai,Jing Yang,Jiawei Yao,Jian Wang,Guanlong Qu,Ziliang Chen,Keze Wang*

Main category: cs.AI

TL;DR: Agora框架将多智能体协调重构为不确定性市场，通过将认知不确定性转化为可交易资产，实现基于经济理性的成本高效协作，在多个多模态基准上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型多智能体系统在经济上不可持续：异构智能体在信息不对称下的协调成本高昂。现有方法依赖启发式代理，忽略成本并破坏不确定性结构，导致可证明的次优协调。

Method: Agora将认知不确定性形式化为结构化可交易资产（感知、语义、推理），基于理性经济规则强制智能体进行盈利驱动的交易。市场感知的经纪人扩展Thompson Sampling，启动协作并引导系统走向成本高效均衡。

Result: 在五个多模态基准测试（MMMU、MMBench、MathVision、InfoVQA、CC-OCR）上，Agora优于强大的VLM和启发式多智能体策略，如在MMMU上比最佳基线准确率提高8.5%，同时成本降低3倍以上。

Conclusion: 基于市场的协调为构建经济可行的多智能体视觉智能系统提供了一个原则性和可扩展的范式。

Abstract: Vision-Language Models (VLMs) enable powerful multi-agent systems, but scaling them is economically unsustainable: coordinating heterogeneous agents under information asymmetry often spirals costs. Existing paradigms, such as Mixture-of-Agents and knowledge-based routers, rely on heuristic proxies that ignore costs and collapse uncertainty structure, leading to provably suboptimal coordination. We introduce Agora, a framework that reframes coordination as a decentralized market for uncertainty. Agora formalizes epistemic uncertainty into a structured, tradable asset (perceptual, semantic, inferential), and enforces profitability-driven trading among agents based on rational economic rules. A market-aware broker, extending Thompson Sampling, initiates collaboration and guides the system toward cost-efficient equilibria. Experiments on five multimodal benchmarks (MMMU, MMBench, MathVision, InfoVQA, CC-OCR) show that Agora outperforms strong VLMs and heuristic multi-agent strategies, e.g., achieving +8.5% accuracy over the best baseline on MMMU while reducing cost by over 3x. These results establish market-based coordination as a principled and scalable paradigm for building economically viable multi-agent visual intelligence systems.

</details>


### [116] [TSRBench: A Comprehensive Multi-task Multi-modal Time Series Reasoning Benchmark for Generalist Models](https://arxiv.org/abs/2601.18744)
*Fangxu Yu,Xingang Guo,Lingzhi Yuan,Haoqiang Kang,Hongyu Zhao,Lianhui Qin,Furong Huang,Bin Hu,Tianyi Zhou*

Main category: cs.AI

TL;DR: TSRBench是一个全面的多模态时间序列推理基准测试，包含4125个问题、14个领域和4个主要维度，用于评估通用模型的时间序列理解能力。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据在现实世界中无处不在且对关键应用至关重要，但现有通用模型基准测试中缺乏时间序列维度，需要填补这一空白。

Method: 构建TSRBench基准测试，包含4125个问题、14个领域，分为感知、推理、预测和决策4个主要维度，包含15个任务评估基本推理能力。

Result: 评估了30多个领先的专有和开源模型，发现：1)缩放定律适用于感知和推理但在预测中失效；2)强推理能力不保证准确的上下文感知预测；3)当前多模态模型未能有效融合文本和视觉表示。

Conclusion: TSRBench提供了一个标准化评估平台，不仅突显了现有挑战，还为推进通用模型提供了有价值的见解。

Abstract: Time series data is ubiquitous in real-world scenarios and crucial for critical applications ranging from energy management to traffic control. Consequently, the ability to reason over time series is a fundamental skill for generalist models to solve practical problems. However, this dimension is notably absent from existing benchmarks of generalist models. To bridge this gap, we introduce TSRBench, a comprehensive multi-modal benchmark designed to stress-test the full spectrum of time series reasoning capabilities. TSRBench features: i) a diverse set of 4125 problems from 14 domains, and is categorized into 4 major dimensions: Perception, Reasoning, Prediction, and Decision-Making. ii) 15 tasks from the 4 dimensions evaluating essential reasoning capabilities (e.g., numerical reasoning). Through extensive experiments, we evaluated over 30 leading proprietary and open-source LLMs, VLMs, and TSLLMs within TSRBench. Our findings reveal that: i) scaling laws hold for perception and reasoning but break down for prediction; ii) strong reasoning does not guarantee accurate context-aware forecasting, indicating a decoupling between semantic understanding and numerical prediction; and iii) despite the complementary nature of textual and visual represenations of time series as inputs, current multimodal models fail to effectively fuse them for reciprocal performance gains. TSRBench provides a standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance generalist models. Our code and dataset are available at https://tsrbench.github.io/.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [117] [The Curious Case of Aid and Conflict: Causal Evidence from Panel Econometrics and Composite Indices](https://arxiv.org/abs/2601.16992)
*Muhammad Usman Anwar Goraya*

Main category: econ.EM

TL;DR: 研究使用多种回归方法分析2009-2023年非洲十大受援国中ODA与冲突的关系，发现援助分配存在人道需求与制度质量之间的权衡困境。


<details>
  <summary>Details</summary>
Motivation: 探讨官方发展援助（ODA）与冲突之间的系统关系，特别是在非洲主要受援国中，了解援助分配是否受冲突、政治稳定性和治理指标的影响。

Method: 使用普通最小二乘法（OLS）、主成分分析（PCA）和岭回归（L2）三种计量方法，以政治稳定性、治理指标和宏观经济条件作为冲突的代理变量，分析其对援助流入的系统影响。

Result: 结果呈现复杂关系：混合回归显示援助与贫困、通胀、脆弱性正相关，与问责负相关；固定效应模型显示援助与政治稳定性、人均GDP正相关，与腐败负相关；岭回归证实治理变量在多重共线性下的稳健性。

Conclusion: 援助分配存在"援助-冲突-制度"三难困境：援助最集中在冲突风险高、制度薄弱的地区，但这些条件恰恰限制了援助效果。捐助者同时响应人道需求和制度质量，形成分配权衡。

Abstract: This paper examines the relationship between Official Development Assistance (ODA) and conflict in the ten largest aid-receiving African countries between 2009 and 2023. Using Ordinary Least Squares, Principal Component Analysis, and Ridge (L2) regression, the study assesses whether conflict, proxied by political stability, governance indicators, and macroeconomic conditions, systematically influences aid inflows. Results reveal a nuanced relationship. Pooled regressions indicate that aid is positively associated with poverty, inflation, and fragility, while voice and accountability are negatively related to ODA. Fixed-effects estimates instead show positive associations between aid, political stability, and GDP per capita over time, alongside negative correlations with perceived corruption. Ridge regression confirms the robustness of various governance variables under multicollinearity. Overall, donors appear responsive to both humanitarian need and institutional quality, producing an aid-conflict-institutions trilemma: aid is most concentrated where conflict risk and institutional weakness are greatest, yet these same conditions which constrain aid effectiveness. The paper contributes by integrating theory with panel-econometric tools to to explore international development aid allocation.

</details>


### [118] [Decomposition of Brazil's 5-year DI Futures in Basis Points](https://arxiv.org/abs/2601.16995)
*Gabriel de Macedo Santos*

Main category: econ.EM

TL;DR: 提出一个可复制、可解释的框架，将巴西5年期DI期货利率的每日变动分解为基点贡献，识别宏观预期、国内风险和外部风险三大驱动因素。


<details>
  <summary>Details</summary>
Motivation: 需要理解巴西利率变动的驱动因素，特别是宏观预期、国内风险和外部风险各自贡献多少，为政策制定和市场分析提供透明工具。

Method: 结合三个模块：1) 巴西央行Focus调查的宏观财政预期；2) 用偏最小二乘法构建的监督宏观因子；3) 将主权CDS分解为全球和国内成分。最后通过线性回归映射到DI5Y每日变动。

Result: 模型解释了DI5Y每日变动的22.45%方差，国内风险主导解释部分，宏观因子贡献较小但统计显著，残差较大表明存在线性模型未捕捉的驱动因素。

Conclusion: 该框架提供了DI5Y每日和累计变动的透明分解，量化了宏观/央行力量、巴西国内风险和外部风险的贡献，但残差较大显示存在线性模型未捕捉的其他因素。

Abstract: This paper proposes an empirical, replicable, and interpretable framework to decompose, in basis points (bps), daily changes in Brazil's 5-year DI futures rate (DI5Y). The approach combines three building blocks: (i) macroeconomic and fiscal expectations from the Central Bank of Brazil Focus survey, converted into daily changes; (ii) a supervised macro factor built with Partial Least Squares (PLS) that summarizes changes in expectations together with a high-frequency macro "surprise" indicator; and (iii) a decomposition of sovereign risk using Brazil CDS into global and domestic components, obtained by regressing CDS on external financial conditions (DXY, CRB, VIX, and the US 10-year yield). The final step maps these drivers into daily bps contributions through a linear regression of the daily change in DI5Y on the three factors, producing a cumulative decomposition that adds up with an intercept and a residual. In the final sample (2015-01-13 to 2025-12-12; 2,741 observations), the model explains about 22.45% of the daily variance in DI5Y changes. The explained share is dominated by domestic risk, with a smaller but statistically significant contribution from the macro factor. The residual remains large, highlighting the limits of linearity and omitted drivers such as monetary policy event windows, term premia, liquidity, and positioning. Overall, the framework delivers a transparent accounting of how much of the daily (and cumulative) movement in DI5Y is associated with macro/central bank forces, domestic Brazil risk, and external risk.

</details>


### [119] [Recovering Counterfactual Distributions via Wasserstein GANs](https://arxiv.org/abs/2601.17296)
*Xinran Liu*

Main category: econ.EM

TL;DR: 提出基于最优传输的鲁棒合成控制方法，用Wasserstein距离替代传统L2距离，解决传统方法在支持集不匹配和多模态分布下的脆弱性问题


<details>
  <summary>Details</summary>
Motivation: 传统分布合成控制方法基于分位数函数的L2距离最小化，在支持集不匹配时缺乏信息梯度，在多模态结果下产生结构伪影，导致估计器脆弱

Method: 基于最优传输理论，通过最小化概率测度之间的Wasserstein-1距离构建合成控制，使用Wasserstein生成对抗网络实现

Result: 在供体池满足仿射独立条件下建立了合成权重的形式点识别；蒙特卡洛模拟显示传统方法在重尾污染和支持集不匹配下出现灾难性方差爆炸，而WGAN方法保持一致性和稳定性；能正确恢复复杂双峰混合分布，而传统分位数平均方法结构上失败

Conclusion: 基于最优传输的WGAN方法比传统分布合成控制方法更鲁棒，能处理支持集不匹配、重尾污染和多模态分布等挑战性问题

Abstract: Standard Distributional Synthetic Controls (DSC) estimate counterfactual distributions by minimizing the Euclidean $L_2$ distance between quantile functions. We demonstrate that this geometric reliance renders estimators fragile: they lack informative gradients under support mismatch and produce structural artifacts when outcomes are multimodal. This paper proposes a robust estimator grounded in Optimal Transport (OT). We construct the synthetic control by minimizing the Wasserstein-1 distance between probability measures, implemented via a Wasserstein Generative Adversarial Network (WGAN). We establish the formal point identification of synthetic weights under an affine independence condition on the donor pool. Monte Carlo simulations confirm that while standard estimators exhibit catastrophic variance explosions under heavy-tailed contamination and support mismatch, our WGAN-based approach remains consistent and stable. Furthermore, we show that our measure-based method correctly recovers complex bimodal mixtures where traditional quantile averaging fails structurally.

</details>


### [120] [Statistical Decisions and Partial Identification: With Application to Boundary Discontinuity Design](https://arxiv.org/abs/2601.17648)
*Chen Qiu,Jörg Stoye*

Main category: econ.EM

TL;DR: 本文回应了Cattaneo等人(2026)和Hirano(2026)的综述，展示了统计决策理论在部分识别情境下的应用，并通过教育补贴资格减少的政策实验连接了两个综述的主题。


<details>
  <summary>Details</summary>
Motivation: 本文旨在回应两篇关于部分识别的综述文章，展示如何将统计决策理论应用于部分识别情境，并通过具体政策实验连接两个综述的主题，探讨理论应用中的挑战。

Method: 作者构建了一个部分识别下的统计决策简化场景，基于自身及他人的前期工作提供了完整解决方案，然后将这些结果应用于教育补贴资格减少的假设政策实验（基于实际政策建模）。

Result: 研究发现虽然可以得出一些有意义的结论，但将理论应用于实际问题时需要一些假设跳跃，并留下了一些未解决的问题。文章最后讨论了部分识别下统计决策理论面临的主要开放挑战。

Conclusion: 统计决策理论可以应用于部分识别情境，但在实际应用中存在假设跳跃和未解决的问题，需要进一步研究来解决这些开放挑战。

Abstract: We are delighted to respond to the excellent surveys by Cattaneo et al. (2026) and Hirano (2026). Our discussion will attempt two things: first, we show how statistical decision theory can be applied to situations with partial identification; second, we connect the surveys' themes by applying these insights to an imagined policy experiment in one of Cattaneo et al.'s (2025) applications.
  To do so, we lay out a stylized scenario of statistical decision making under partial identification and, drawing on our own and others' earlier work, provide a complete solution for that scenario. We then apply these results to a hypothetical reduction (modelled on actual policies) in eligibility for educational subsidies. We will see that something of interest can be said, but also that bringing the theory to the application involves some leaps of faith and leaves some questions open. This leads to the final section, where we discuss what we see as the main open challenges in statistical decision theory under partial identification.

</details>


### [121] [The Proximal Surrogate Index: Long-Term Treatment Effects under Unobserved Confounding](https://arxiv.org/abs/2601.17712)
*Ting-Chih Hung,Yu-Chang Chen*

Main category: econ.EM

TL;DR: 该论文提出了一种结合实验样本和观测样本来识别和估计长期处理效应的方法，解决了未观测混杂因素的问题，通过代理变量实现识别，并开发了多重稳健的估计和推断程序。


<details>
  <summary>Details</summary>
Motivation: 在评估长期处理效应时，通常面临两个问题：实验样本中缺乏长期结果数据，而观测样本中存在未观测的混杂因素。传统的代理指数方法在存在未观测混杂因素时会失效，因此需要新的方法来同时利用实验样本和观测样本的优势。

Method: 结合实验样本（有处理分配但缺乏长期结果）和观测样本（有长期结果但处理分配未观测），利用未观测混杂因素的代理变量建立新的识别结果，并开发基于这些结果的多重稳健估计和推断程序。

Result: 在Job Corps项目中的应用表明，该方法能够在未观测混杂因素使标准代理指数估计产生偏差的情况下，恢复实验基准结果，证明了方法的有效性。

Conclusion: 通过结合实验和观测数据并利用代理变量，该方法能够可靠地估计长期处理效应，即使在存在未观测混杂因素的情况下也能提供有效的估计，为因果推断提供了新的工具。

Abstract: We study the identification and estimation of long-term treatment effects under unobserved confounding by combining an experimental sample, where the long-term outcome is missing, with an observational sample, where the treatment assignment is unobserved. While standard surrogate index methods fail when unobserved confounders exist, we establish novel identification results by leveraging proxy variables for the unobserved confounders. We further develop multiply robust estimation and inference procedures based on these results. Applying our method to the Job Corps program, we demonstrate its ability to recover experimental benchmarks even when unobserved confounders bias standard surrogate index estimates.

</details>


### [122] [Best Feasible Conditional Critical Values for a More Powerful Subvector Anderson-Rubin Test](https://arxiv.org/abs/2601.17843)
*Jesse Hoekstra,Frank Windmeijer*

Main category: econ.EM

TL;DR: 本文提出一种改进的弱工具变量条件下子向量AR检验方法，通过使用第二最小特征值而非最大特征值作为条件变量，在参数数量大于1时获得比GKM方法更高的检验功效。


<details>
  <summary>Details</summary>
Motivation: GKM(2019)提出的条件子向量AR检验使用数据依赖的临界值函数，以最大特征值为条件变量，虽然能控制检验水平并提高功效，但最大特征值并非弱识别的合适指标。本文旨在寻找更合适的条件变量来进一步提高检验功效。

Method: 将GKM的数据依赖临界值函数应用于第二最小特征值作为条件变量，而非原始的最大特征值。该方法适用于子向量AR检验统计量，并能扩展到GKM(2024)提出的近似Kronecker乘积结构的条件异方差稳健版本。

Result: 当未检验参数数量大于1时，基于第二最小特征值的条件检验方法具有正确的检验水平，且功效严格高于GKM原始方法。该方法还能扩展到条件异方差稳健的AR检验统计量，保持其功效优势。

Conclusion: 第二最小特征值是弱识别的更合适指标，基于此的条件检验方法在保持检验水平正确的同时，能获得比GKM方法更高的检验功效，特别是在未检验参数数量较多时，且该方法具有良好的扩展性。

Abstract: For subvector inference in the linear instrumental variables model under homoskedasticity but allowing for weak instruments, Guggenberger, Kleibergen, and Mavroeidis (2019) (GKM) propose a conditional subvector Anderson and Rubin (1949) (AR) test that uses data-dependent critical values that adapt to the strength of the parameters not under test. This test has correct size and strictly higher power than the test that uses standard asymptotic chi-square critical values. The subvector AR test is the minimum eigenvalue of a data dependent matrix. The GKM critical value function conditions on the largest eigenvalue of this matrix. We consider instead the data dependent critical value function conditioning on the second-smallest eigenvalue, as this eigenvalue is the appropriate indicator for weak identification. We find that the data dependent critical value function of GKM also applies to this conditioning and show that this test has correct size and power strictly higher than the GKM test when the number of parameters not under test is larger than one. Our proposed procedure further applies to the subvector AR test statistic that is robust to an approximate kronecker product structure of conditional heteroskedasticity as proposed by Guggenberger, Kleibergen, and Mavroeidis (2024), carrying over its power advantage to this setting as well.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [123] [Unveiling hidden features of social evolution by inferring Langevin dynamics from data](https://arxiv.org/abs/2601.17772)
*Youngkyoung Bae,Hajime Shimao,Seungwoong Ha,Luna Yang,David Wolpert*

Main category: cs.SI

TL;DR: 提出一个将历史动态建模为随机微分方程（SDEs）的框架，用于分析社会文化演化中的结构趋势与随机波动相互作用


<details>
  <summary>Details</summary>
Motivation: 现有定量方法多依赖确定性快照或平均效应，忽视了历史轨迹的连续性和固有不确定性，需要新方法来处理不完整历史记录并超越结构决定论与纯粹偶然的二分法

Method: 将历史动态建模为随机过程，使用随机微分方程（SDEs）描述连续时间动态，通过该框架量化不可逆性、检测外生扰动、对缺失历史记录进行多重插补

Result: 该随机视角提供了静态模型无法实现的丰富分析能力，能够统一分析历史变化的稳定性、偶然性和动态性

Conclusion: 随机微分方程框架为分析社会文化历史演化提供了统一方法学，能够处理历史记录的不确定性，揭示结构趋势与随机波动如何共同塑造社会演化

Abstract: Are there hidden dynamical common patterns in the evolution of social and cultural history? While the growing availability of digitized social data invites us to answer this question, prevailing quantitative methods often rely on deterministic snapshots or average effects. Such approaches overlook the continuous and inherently uncertain nature of historical trajectories. In this paper, we propose a framework for modeling historical dynamics as stochastic processes described by stochastic differential equations (SDEs). By viewing historical change through the lens of continuous-time dynamics, this framework provides a natural language to describe how structural trends and inherent random fluctuations interact to shape societal evolution. This approach allows us to handle the uncertainty in fragmentary historical records, moving beyond the dichotomy of structural determinism versus pure chance. We demonstrate that adopting this stochastic perspective unlocks a rich suite of analytical capabilities unavailable to static models. Specifically, we introduce methods to: (1) quantify the irreversibility; (2) detect exogenous perturbations; (3) perform multiple imputation for missing historical records. This framework offers a unified methodology for dissecting the stability, contingency, and dynamics of historical change.

</details>


### [124] [Explaining Synergistic Effects in Social Recommendations](https://arxiv.org/abs/2601.18151)
*Yicong Li,Shan Jin,Qi Liu,Shuo Wang,Jiaying Liu,Shuo Yu,Qiang Zhang,Kuanjiu Zhou,Feng Xia*

Main category: cs.SI

TL;DR: 提出SemExplainer方法，通过识别体现协同效应的子图来解释社交推荐中的协同效应，提高推荐可解释性。


<details>
  <summary>Details</summary>
Motivation: 社交推荐中多个社交网络间的非线性协同效应不透明，用户难以理解推荐如何利用多样化信息，现有解释器只能识别重要拓扑信息，无法解释协同效应。

Method: 1) 从多视角社交网络提取解释性子图；2) 开发条件熵优化策略最大化信息增益，识别体现协同效应的子图；3) 在协同子图中搜索用户到推荐项目的路径生成解释。

Result: 在三个数据集上的实验表明，SemExplainer优于基线方法，能更好地解释协同效应。

Conclusion: SemExplainer通过量化图信息增益和识别协同子图，有效解释了社交推荐中的协同效应，提高了推荐系统的可解释性。

Abstract: In social recommenders, the inherent nonlinearity and opacity of synergistic effects across multiple social networks hinders users from understanding how diverse information is leveraged for recommendations, consequently diminishing explainability. However, existing explainers can only identify the topological information in social networks that significantly influences recommendations, failing to further explain the synergistic effects among this information. Inspired by existing findings that synergistic effects enhance mutual information between inputs and predictions to generate information gain, we extend this discovery to graph data. We quantify graph information gain to identify subgraphs embodying synergistic effects. Based on the theoretical insights, we propose SemExplainer, which explains synergistic effects by identifying subgraphs that embody them. SemExplainer first extracts explanatory subgraphs from multi-view social networks to generate preliminary importance explanations for recommendations. A conditional entropy optimization strategy to maximize information gain is developed, thereby further identifying subgraphs that embody synergistic effects from explanatory subgraphs. Finally, SemExplainer searches for paths from users to recommended items within the synergistic subgraphs to generate explanations for the recommendations. Extensive experiments on three datasets demonstrate the superiority of SemExplainer over baseline methods, providing superior explanations of synergistic effects.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [125] [Bayesian Multiple Testing for Suicide Risk in Pharmacoepidemiology: Leveraging Co-Prescription Patterns](https://arxiv.org/abs/2601.17985)
*Soumya Sahu,Kwan Hur,Dulal K. Bhaumik,Robert Gibbons*

Main category: stat.AP

TL;DR: 提出贝叶斯尖峰-平板框架分析922种处方药与自杀风险关联，利用真实世界共处方模式改进信号检测，相比传统方法发现更多潜在有害/保护性信号


<details>
  <summary>Details</summary>
Motivation: 自杀是美国第十大死因，但药物相关风险或保护作用的证据有限。现有上市后研究通常一次只分析一类药物，或使用经验贝叶斯收缩和保守多重性校正，牺牲了检测临床有意义信号的能力

Method: 引入统一的贝叶斯尖峰-平板框架，利用美国商业索赔数据（2003-2014年，1.5亿患者）分析922种处方药。通过真实世界共处方模式构建协方差先验，在药理学相关药物间自适应借用信息。结合贝叶斯错误发现率控制，实现网络引导的变量选择

Result: 相比Gibbons等人的经验贝叶斯分析，确认了关键有害信号（如阿普唑仑、氢可酮）和保护信号（如米氮平、叶酸），同时发现了新关联：高风险阿片类药物组合和多个具有潜在预防作用的叶酸相关药物。对抗抑郁药的重新分析显示不同共处方指标如何调节效应估计

Conclusion: 该方法为临床医生和监管机构提供了可操作的假设，展示了结构化贝叶斯建模在药物警戒中的价值，能够改进高维罕见事件监测中的信号检测能力

Abstract: Suicide is the tenth leading cause of death in the United States, yet evidence on medication-related risk or protection remains limited. Most post-marketing studies examine one drug class at a time or rely on empirical-Bayes shrinkage with conservative multiplicity corrections, sacrificing power to detect clinically meaningful signals. We introduce a unified Bayesian spike-and-slab framework that advances both applied suicide research and statistical methodology. Substantively, we screen 922 prescription drugs across 150 million patients in U.S. commercial claims (2003 to 2014), leveraging real-world co-prescription patterns to inform a covariance prior that adaptively borrows strength across pharmacologically related agents. Statistically, the model couples this structured prior with Bayesian false-discovery-rate control, illustrating how network-guided variable selection can improve rare-event surveillance in high dimensions. Relative to the seminal empirical-Bayes analysis of Gibbons et al. (2019), our approach reconfirms the key harmful (e.g., alprazolam, hydrocodone) and protective (e.g., mirtazapine, folic acid) signals while revealing additional associations, such as a high-risk opioid combination and several folate-linked agents with potential preventive benefit that had been overlooked. A focused re-analysis of 18 antidepressants shows how alternative co-prescription metrics modulate effect estimates, shedding light on competitive versus complementary prescribing. These findings generate actionable hypotheses for clinicians and regulators and showcase the value of structured Bayesian modeling in pharmacovigilance.

</details>


### [126] [Uncertainty Quantification in Coupled Multiphysics Systems via Gaussian Process Surrogates: Application to Fuel Assembly Bow](https://arxiv.org/abs/2601.18480)
*Ali Abboud,Josselin Garnier,Bertrand Leturcq,Stanislas de Lambert*

Main category: stat.AP

TL;DR: 提出一个耦合高斯过程代理模型的数学框架，用于多物理场系统的严格不确定性量化，应用于压水堆燃料组件弯曲预测，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 压水堆燃料组件弯曲预测需要解决紧密耦合的流固相互作用问题，直接模拟计算成本过高，使得大规模不确定性量化非常困难。

Method: 引入一个通用的数学框架，用于耦合代表不同物理求解器的高斯过程代理模型，理论分析证明耦合GP系统的预测方差在温和的正则性和稳定性假设下有界。

Result: 该方法应用于燃料组件弯曲的耦合水力-结构模拟，实现了全局敏感性分析和完整的不确定性量化，计算成本仅为直接代码耦合的一小部分，结果展示了准确的不确定性传播和稳定预测。

Conclusion: 为大规模多物理场模拟中基于代理模型的耦合建立了坚实的数学基础，能够以可承受的计算成本进行严格的不确定性量化。

Abstract: Predicting fuel assembly bow in pressurized water reactors requires solving tightly coupled fluid-structure interaction problems, whose direct simulations can be computationally prohibitive, making large-scale uncertainty quantification (UQ) very challenging. This work introduces a general mathematical framework for coupling Gaussian process (GP) surrogate models representing distinct physical solvers, aimed at enabling rigorous UQ in coupled multiphysics systems. A theoretical analysis establishes that the predictive variance of the coupled GP system remains bounded under mild regularity and stability assumptions, ensuring that uncertainty does not grow uncontrollably through the iterative coupling process. The methodology is then applied to the coupled hydraulic-structural simulation of fuel assembly bow, enabling global sensitivity analysis and full UQ at a fraction of the computational cost of direct code coupling. The results demonstrate accurate uncertainty propagation and stable predictions, establishing a solid mathematical basis for surrogate-based coupling in large-scale multiphysics simulations.

</details>


### [127] [A varying-coefficient model for characterizing duration-driven heterogeneity in flood-related health impacts](https://arxiv.org/abs/2601.18656)
*Sarika Aggarwal,Phillip B. Nicol,Brent A. Coull,Rachel C. Nethery*

Main category: stat.AP

TL;DR: 提出EDVCM框架，用于估计连续多日环境暴露（如洪水）的健康效应，特别关注暴露持续时间对效应异质性的影响，并在医疗保险数据中分析洪水持续时间对肌肉骨骼系统疾病住院的影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽然揭示了洪水暴露与不良健康结果之间的关联，但洪水事件具有高度异质性（如瞬时洪水与缓慢洪水），很少有研究将暴露持续时间纳入健康影响建模或研究持续时间驱动的效应异质性。

Method: 提出暴露持续时间变系数建模（EDVCM）框架，采用区域级自匹配研究设计消除时间不变混杂，结合条件泊松回归建模估计暴露效应并调整时变混杂。在贝叶斯框架下引入持续时间和暴露日特定的暴露系数，赋予二维高斯过程先验，实现跨持续时间和暴露日的信息共享。

Result: 模拟研究表明EDVCM在效应估计和不确定性量化方面优于传统方法。应用于全国性多年代医疗保险数据与高分辨率洪水暴露测量，揭示了洪水持续时间对肌肉骨骼系统疾病住院影响的异质性。

Conclusion: EDVCM框架能够提供高分辨率的持续时间驱动效应异质性洞察，同时通过信息共享确保模型稳定性，为研究连续多日环境暴露的健康影响提供了有效方法。

Abstract: Previous work revealed associations between flood exposure and adverse health outcomes during and in the aftermath of flood events. Floods are highly heterogeneous events, largely owing to vast differences in flood durations, i.e., flash-floods versus slow-moving floods. However, little to no work has incorporated exposure duration into the modeling of flood-related health impacts or has investigated duration-driven effect heterogeneity. To address this gap, we propose an exposure duration varying coefficient modeling (EDVCM) framework for estimating exposure day-specific health effects of consecutive-day environmental exposures that vary in duration. We develop the EDVCM within an area-level self-matched study design to eliminate time-invariant confounding followed by conditional Poisson regression modeling for exposure effect estimation and adjustment of time-varying confounders. Using a Bayesian framework, we introduce duration- and exposure day-specific exposure coefficients within the conditional Poisson model and assign them a two-dimensional Gaussian process prior to allow for sharing of information across both duration and exposure day. This approach enables highly-resolved insights into duration-driven effect heterogeneity while ensuring model stability through information sharing. Through simulations, we demonstrate that the EDVCM out-performs conventional approaches in terms of both effect estimation and uncertainty quantification. We apply the EDVCM to nationwide, multi-decade Medicare claims data linked with high-resolution flood exposure measures to investigate duration-driven heterogeneity in flood effects on musculoskeletal system disease hospitalizations.

</details>

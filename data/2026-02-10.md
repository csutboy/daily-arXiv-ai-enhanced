<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 113]
- [cs.SI](#cs.SI) [Total: 7]
- [cs.CY](#cs.CY) [Total: 15]
- [econ.EM](#econ.EM) [Total: 8]
- [stat.AP](#stat.AP) [Total: 10]
- [cs.ET](#cs.ET) [Total: 5]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [LLM-FSM: Scaling Large Language Models for Finite-State Reasoning in RTL Code Generation](https://arxiv.org/abs/2602.07032)
*Yuheng Wu,Berk Gokmen,Zhouhua Xie,Peijing Li,Caroline Trippel,Priyanka Raina,Thierry Tambe*

Main category: cs.AI

TL;DR: LLM-FSM是一个评估大语言模型从自然语言规范中恢复有限状态机行为并生成正确RTL实现能力的基准测试，包含1000个自动生成的问题，显示LLM在FSM复杂度增加时准确率急剧下降。


<details>
  <summary>Details</summary>
Motivation: 有限状态推理是硬件设计的核心能力，需要评估LLM从自然语言规范中恢复FSM行为并生成正确RTL实现的能力。现有基准测试依赖人工构建示例，缺乏自动化和可扩展性。

Method: 通过全自动流水线构建LLM-FSM基准：1) 构建具有可配置状态数和约束转换结构的FSM；2) 提示LLM将FSM表达为结构化YAML格式并添加应用上下文；3) 将YAML转换为自然语言规范；4) 从同一YAML以构造正确的方式合成参考RTL和测试平台；5) 使用LLM和SAT求解器检查验证所有1000个问题。

Result: 实验显示即使最强的LLM在FSM复杂度增加时准确率也急剧下降。通过监督微调的训练时扩展能有效泛化到分布外任务，增加测试时计算能提高推理可靠性。LLM-FSM具有可扩展性，其FSM复杂度可随未来模型能力扩展。

Conclusion: LLM-FSM提供了一个自动化、可扩展的基准测试，用于评估LLM在硬件设计中的有限状态推理能力，揭示了当前LLM在复杂FSM任务上的局限性，并展示了训练和测试策略对性能的影响。

Abstract: Finite-state reasoning, the ability to understand and implement state-dependent behavior, is central to hardware design. In this paper, we present LLM-FSM, a benchmark that evaluates how well large language models (LLMs) can recover finite-state machine (FSM) behavior from natural-language specifications and translate it into correct register transfer-level (RTL) implementations. Unlike prior specification-to-RTL benchmarks that rely on manually constructed examples, LLM-FSM is built through a fully automated pipeline. LLM-FSM first constructs FSM with configurable state counts and constrained transition structures. It then prompts LLMs to express each FSM in a structured YAML format with an application context, and to further convert that YAML into a natural-language (NL) specification. From the same YAML, our pipeline synthesizes the reference RTL and testbench in a correct-by-construction manner. All 1,000 problems are verified using LLM-based and SAT-solver-based checks, with human review on a subset. Our experiments show that even the strongest LLMs exhibit sharply declining accuracy as FSM complexity increases. We further demonstrate that training-time scaling via supervised fine-tuning (SFT) generalizes effectively to out-of-distribution (OOD) tasks, while increasing test-time compute improves reasoning reliability. Finally, LLM-FSM remains extensible by allowing its FSM complexity to scale with future model capabilities.

</details>


### [2] [ST-Raptor: An Agentic System for Semi-Structured Table QA](https://arxiv.org/abs/2602.07034)
*Jinxiu Qu,Zirui Tang,Hongzhang Huang,Boyu Niu,Wei Zhou,Jiannan Wang,Yitong Song,Guoliang Li,Xuanhe Zhou,Fan Wu*

Main category: cs.AI

TL;DR: ST-Raptor是一个用于半结构化表格问答的智能体系统，通过视觉编辑、树状结构建模和智能体驱动查询解决现有方法的信息丢失和布局处理问题


<details>
  <summary>Details</summary>
Motivation: 半结构化表格问答需要精确提取单元格内容和位置，并恢复表格布局中隐含的逻辑结构、层次关系和语义关联。现有方法存在信息丢失、复杂布局处理困难等问题，人工解释又耗时耗力

Method: ST-Raptor提供交互式分析环境，结合视觉编辑、树状结构建模和智能体驱动查询解决，支持准确且用户友好的表格理解

Result: 在基准测试和真实世界数据集上的实验结果表明，ST-Raptor在准确性和可用性方面均优于现有方法

Conclusion: ST-Raptor通过创新的交互式智能体系统有效解决了半结构化表格问答的挑战，提供了更准确和用户友好的解决方案

Abstract: Semi-structured table question answering (QA) is a challenging task that requires (1) precise extraction of cell contents and positions and (2) accurate recovery of key implicit logical structures, hierarchical relationships, and semantic associations encoded in table layouts. In practice, such tables are often interpreted manually by human experts, which is labor-intensive and time-consuming. However, automating this process remains difficult. Existing Text-to-SQL methods typically require converting semi-structured tables into structured formats, inevitably leading to information loss, while approaches like Text-to-Code and multimodal LLM-based QA struggle with complex layouts and often yield inaccurate answers. To address these limitations, we present ST-Raptor, an agentic system for semi-structured table QA. ST-Raptor offers an interactive analysis environment that combines visual editing, tree-based structural modeling, and agent-driven query resolution to support accurate and user-friendly table understanding. Experimental results on both benchmark and real-world datasets demonstrate that ST-Raptor outperforms existing methods in both accuracy and usability. The code is available at https://github.com/weAIDB/ST-Raptor, and a demonstration video is available at https://youtu.be/9GDR-94Cau4.

</details>


### [3] [DLLM-Searcher: Adapting Diffusion Large Language Model for Search Agents](https://arxiv.org/abs/2602.07035)
*Jiahao Zhao,Shaoxuan Xu,Zhongxiang Sun,Fengqi Zhu,Jingyang Ou,Yuling Shi,Chongxuan Li,Xiao Zhang,Jun Xu*

Main category: cs.AI

TL;DR: DLLM-Searcher：基于扩散大语言模型的搜索代理优化框架，通过两阶段后训练提升代理能力，并设计并行推理与执行范式（P-ReAct）解决延迟问题，实现与主流LLM搜索代理相当的性能和约15%的推理加速。


<details>
  <summary>Details</summary>
Motivation: 现有搜索代理面临两大挑战：1）延迟挑战：ReAct代理范式下的串行多轮推理、工具调用和等待导致严重端到端延迟；2）代理能力挑战：现有扩散大语言模型（dLLMs）在推理和工具调用能力上表现较弱，无法充分发挥其并行解码和灵活生成的优势。

Method: 提出DLLM-Searcher优化框架：1）针对能力挑战，设计两阶段后训练流程：代理监督微调（Agentic SFT）和代理方差减少偏好优化（Agentic VRPO），增强dLLM的信息检索和推理能力；2）针对延迟挑战，利用dLLMs的灵活生成机制，提出并行推理与执行范式（P-ReAct），优先解码工具调用指令，实现等待工具返回时的持续思考。

Result: 实验结果表明，DLLM-Searcher实现了与主流基于LLM的搜索代理相当的性能，同时P-ReAct范式带来了约15%的推理加速。

Conclusion: DLLM-Searcher成功解决了扩散大语言模型在搜索代理应用中的能力不足和延迟问题，证明了dLLMs在提升代理效率方面的潜力，为高效搜索代理系统提供了新的技术路径。

Abstract: Recently, Diffusion Large Language Models (dLLMs) have demonstrated unique efficiency advantages, enabled by their inherently parallel decoding mechanism and flexible generation paradigm. Meanwhile, despite the rapid advancement of Search Agents, their practical deployment is constrained by a fundamental limitation, termed as 1) Latency Challenge: the serial execution of multi-round reasoning, tool calling, and tool response waiting under the ReAct agent paradigm induces severe end-to-end latency. Intuitively, dLLMs can leverage their distinctive strengths to optimize the operational efficiency of agents under the ReAct agent paradigm. Practically, existing dLLM backbones face the 2) Agent Ability Challenge. That is, existing dLLMs exhibit remarkably weak reasoning and tool-calling capabilities, preventing these advantages from being effectively realized in practice. In this paper, we propose DLLM-Searcher, an optimization framework for dLLM-based Search Agents. To solve the Agent Ability Challenge, we design a two-stage post-training pipeline encompassing Agentic Supervised Fine-Tuning (Agentic SFT) and Agentic Variance-Reduced Preference Optimization Agentic VRPO, which enhances the backbone dLLM's information seeking and reasoning capabilities. To mitigate the Latency Challenge, we leverage the flexible generation mechanism of dLLMs and propose a novel agent paradigm termed Parallel-Reasoning and Acting P-ReAct. P-ReAct guides the model to prioritize decoding tool_call instructions, thereby allowing the model to keep thinking while waiting for the tool's return. Experimental results demonstrate that DLLM-Searcher achieves performance comparable to mainstream LLM-based search agents and P-ReAct delivers approximately 15% inference acceleration. Our code is available at https://anonymous.4open.science/r/DLLM-Searcher-553C

</details>


### [4] [Aster: Autonomous Scientific Discovery over 20x Faster Than Existing Methods](https://arxiv.org/abs/2602.07040)
*Emmett Bicker*

Main category: cs.AI

TL;DR: Aster是一个用于自主科学发现的AI代理，比现有框架快20倍以上，能在多个领域实现新的最先进性能


<details>
  <summary>Details</summary>
Motivation: 现有科学发现框架速度较慢，限制了在需要长时间评估的任务（如多小时的机器学习训练）中的应用。需要一种能显著减少迭代次数、扩展可处理问题范围的自主发现系统。

Method: Aster采用迭代改进方法：给定任务、初始程序和评估脚本，系统会不断优化程序。通过减少所需迭代次数，能够处理评估时间长的任务。

Result: 在多个领域取得SOTA结果：Erdos最小重叠问题、TriMul内核优化、单细胞分析去噪问题、神经活动预测模型训练（在ZAPBench上匹配最佳人类解决方案，但计算量不到1/190）、NanoGPT Speedrun竞赛。除了ZAPBench外，所有任务都达到最先进水平。

Conclusion: Aster通过显著减少科学发现所需的迭代次数，扩展了可处理问题的范围，特别是在评估时间长的任务中表现出色。系统已通过web界面和API提供访问。

Abstract: We introduce Aster, an AI agent for autonomous scientific discovery capable of operating over 20 times faster than existing frameworks. Given a task, an initial program, and a script to evaluate the performance of the program, Aster iteratively improves the program, often leading to new state-of-the-art performances. Aster's significant reduction in the number of iterations required for novel discovery expands the domain of tractable problems to include tasks with long evaluation durations, such as multi-hour machine learning training runs.
  We applied Aster to problems in mathematics, GPU kernel engineering, biology, neuroscience, and language model training. More specifically: the Erdos minimum overlap problem, optimizing the TriMul kernel, a single-cell analysis denoising problem, training a neural activity prediction model to perform well on ZAPBench, and the NanoGPT Speedrun Competition. Aster attains SOTA results in every task, except for ZAPBench, where it matches the performance of the best human solution with less than 1/190th of the compute.
  Aster is accessible via a web interface and API at asterlab.ai.

</details>


### [5] [Theory of Space: Can Foundation Models Construct Spatial Beliefs through Active Exploration?](https://arxiv.org/abs/2602.07055)
*Pingyue Zhang,Zihan Huang,Yue Wang,Jieyu Zhang,Letian Xue,Zihan Wang,Qineng Wang,Keshigeyan Chandrasegaran,Ruohan Zhang,Yejin Choi,Ranjay Krishna,Jiajun Wu,Li Fei-Fei,Manling Li*

Main category: cs.AI

TL;DR: 论文提出"空间理论"概念，评估多模态基础模型在主动空间探索中的能力，发现存在主动-被动差距、探索效率低、空间信念不稳定等问题。


<details>
  <summary>Details</summary>
Motivation: 虽然多模态基础模型在被动感知方面表现出色，但它们在主动、自主探索方面的能力尚未得到充分研究。空间具身智能需要智能体在部分可观测环境下通过行动获取信息。

Method: 提出"空间理论"概念，定义为智能体通过自主主动探索获取信息、从序列部分观测中构建、修正和利用空间信念的能力。通过好奇心驱动探索构建准确认知地图的基准进行评估，关键创新是空间信念探测技术，在每一步提示模型揭示其内部空间表征。

Result: 评估最先进模型发现几个关键瓶颈：1) 主动-被动差距 - 当智能体必须自主收集信息时性能显著下降；2) 高无效性 - 与基于程序的代理相比，模型探索不系统；3) 空间信念不稳定 - 全局信念存在不稳定性，导致空间知识随时间退化；4) 信念惯性 - 智能体无法用新证据更新过时的先验，这在基于视觉的模型中尤为严重。

Conclusion: 当前基础模型在主动探索过程中难以维持连贯、可修正的空间信念，揭示了多模态模型在空间推理和主动信息获取方面的局限性。

Abstract: Spatial embodied intelligence requires agents to act to acquire information under partial observability. While multimodal foundation models excel at passive perception, their capacity for active, self-directed exploration remains understudied. We propose Theory of Space, defined as an agent's ability to actively acquire information through self-directed, active exploration and to construct, revise, and exploit a spatial belief from sequential, partial observations. We evaluate this through a benchmark where the goal is curiosity-driven exploration to build an accurate cognitive map. A key innovation is spatial belief probing, which prompts models to reveal their internal spatial representations at each step. Our evaluation of state-of-the-art models reveals several critical bottlenecks. First, we identify an Active-Passive Gap, where performance drops significantly when agents must autonomously gather information. Second, we find high inefficiency, as models explore unsystematically compared to program-based proxies. Through belief probing, we diagnose that while perception is an initial bottleneck, global beliefs suffer from instability that causes spatial knowledge to degrade over time. Finally, using a false belief paradigm, we uncover Belief Inertia, where agents fail to update obsolete priors with new evidence. This issue is present in text-based agents but is particularly severe in vision-based models. Our findings suggest that current foundation models struggle to maintain coherent, revisable spatial beliefs during active exploration.

</details>


### [6] [Graph-Enhanced Deep Reinforcement Learning for Multi-Objective Unrelated Parallel Machine Scheduling](https://arxiv.org/abs/2602.08052)
*Bulent Soykan,Sean Mondesire,Ghaith Rabadi,Grace Bochenek*

Main category: cs.AI

TL;DR: 提出基于PPO和GNN的深度强化学习框架，解决具有释放时间、设置和资格约束的不相关并行机调度问题，同时最小化总加权延迟和总设置时间。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以平衡总加权延迟和总设置时间这两个多目标优化问题，需要更有效的解决方案来处理复杂的制造调度场景。

Method: 使用近端策略优化算法结合图神经网络，GNN表示作业、机器和设置的复杂状态，PPO代理学习直接调度策略，通过多目标奖励函数指导训练。

Result: 在基准实例上的实验结果表明，PPO-GNN代理显著优于标准调度规则和元启发式算法，在两个目标之间实现了更优的权衡。

Conclusion: 该方法为复杂制造调度提供了鲁棒且可扩展的解决方案，证明了深度强化学习在多目标调度问题中的有效性。

Abstract: The Unrelated Parallel Machine Scheduling Problem (UPMSP) with release dates, setups, and eligibility constraints presents a significant multi-objective challenge. Traditional methods struggle to balance minimizing Total Weighted Tardiness (TWT) and Total Setup Time (TST). This paper proposes a Deep Reinforcement Learning framework using Proximal Policy Optimization (PPO) and a Graph Neural Network (GNN). The GNN effectively represents the complex state of jobs, machines, and setups, allowing the PPO agent to learn a direct scheduling policy. Guided by a multi-objective reward function, the agent simultaneously minimizes TWT and TST. Experimental results on benchmark instances demonstrate that our PPO-GNN agent significantly outperforms a standard dispatching rule and a metaheuristic, achieving a superior trade-off between both objectives. This provides a robust and scalable solution for complex manufacturing scheduling.

</details>


### [7] [ANCHOR: Branch-Point Data Generation for GUI Agents](https://arxiv.org/abs/2602.07153)
*Jinbiao Wei,Yilun Zhao,Kangqi Ni,Arman Cohan*

Main category: cs.AI

TL;DR: Anchor框架通过少量已验证种子演示，通过状态分支点识别和任务变体生成，扩展桌面GUI交互数据，提升代理性能


<details>
  <summary>Details</summary>
Motivation: 端到端GUI代理需要大量高质量交互数据，但人工收集成本高，现有合成方法存在任务多样性有限或轨迹噪声大、目标漂移的问题

Method: 从少量已验证种子演示出发，识别状态变化的分支点，基于当前GUI上下文提出新的状态基础任务变体，通过执行代理生成新轨迹，验证器通过状态感知检查和轨迹一致性确保任务完成，并应用任务条件步骤级过滤去除无基础动作

Result: 在OSWorld和WindowsAgentArena基准测试中，使用扩展语料库微调的模型相比零样本代理和代表性合成基线获得一致改进，并能跨应用和操作系统泛化

Conclusion: Anchor框架能够从少量种子演示中引导扩展可扩展的桌面监督数据，有效解决GUI代理训练数据稀缺和质量问题

Abstract: End-to-end GUI agents for real desktop environments require large amounts of high-quality interaction data, yet collecting human demonstrations is expensive and existing synthetic pipelines often suffer from limited task diversity or noisy, goal-drifting trajectories. We present a trajectory expansion framework Anchor that bootstraps scalable desktop supervision from a small set of verified seed demonstrations. Starting from each seed, we identify branch points that correspond to meaningful state changes and propose new, state-grounded task variants conditioned on the current GUI context. An executing agent then follows the proposed instructions to generate new trajectories, while a verifier enforces task completion via state-aware checks and trajectory-level consistency. To improve supervision quality, we further apply task-conditioned step-level filtering to remove ungrounded actions and denoise post-branch segments to maintain coherent intent. Experiments on standard desktop benchmarks, OSWorld and WindowsAgentArena, show that models fine-tuned on our expanded corpus achieve consistent improvements over zero-shot agents and representative synthesis baselines, and generalize across applications and operating systems.

</details>


### [8] [Objective Decoupling in Social Reinforcement Learning: Recovering Ground Truth from Sycophantic Majorities](https://arxiv.org/abs/2602.08092)
*Majid Ghasemi,Mark Crowley*

Main category: cs.AI

TL;DR: 论文挑战了AI对齐中的关键假设——人类反馈是真实信号，提出在社交环境中该假设失效，会导致目标解耦问题，并提出了基于安全公理判断反馈来源的新方法。


<details>
  <summary>Details</summary>
Motivation: 当前AI对齐策略依赖于人类反馈是真实信号这一脆弱前提，但在社交环境中，评估者可能奉承、懒惰或有敌意，导致这一假设失效，造成目标解耦和永久性错位。

Method: 提出Epistemic Source Alignment (ESA)方法，不同于依赖统计共识的传统鲁棒方法，ESA使用稀疏安全公理来判断反馈的来源而非信号本身，通过"判断判断者"机制确保收敛到真实目标。

Result: 理论上证明了ESA能保证收敛到真实目标，即使大多数评估者有偏见；实证显示传统共识方法在多数共谋下失败，而ESA方法能成功恢复最优策略。

Conclusion: 人类反馈作为真实信号的假设在社交环境中不成立，会导致目标解耦问题，而基于判断反馈来源的ESA方法能有效解决这一问题，确保AI系统在偏见环境中仍能对齐。

Abstract: Contemporary AI alignment strategies rely on a fragile premise: that human feedback, while noisy, remains a fundamentally truthful signal. In this paper, we identify this assumption as Dogma 4 of Reinforcement Learning (RL). We demonstrate that while this dogma holds in static environments, it fails in social settings where evaluators may be sycophantic, lazy, or adversarial. We prove that under Dogma 4, standard RL agents suffer from what we call Objective Decoupling, a structural failure mode where the agent's learned objective permanently separates from the latent ground truth, guaranteeing convergence to misalignment. To resolve this, we propose Epistemic Source Alignment (ESA). Unlike standard robust methods that rely on statistical consensus (trusting the majority), ESA utilizes sparse safety axioms to judge the source of the feedback rather than the signal itself. We prove that this "judging the judges" mechanism guarantees convergence to the true objective, even when a majority of evaluators are biased. Empirically, we show that while traditional consensus methods fail under majority collusion, our approach successfully recovers the optimal policy.

</details>


### [9] [PreFlect: From Retrospective to Prospective Reflection in Large Language Model Agents](https://arxiv.org/abs/2602.07187)
*Hanyu Wang,Yuanpu Cao,Lu Lin,Jinghui Chen*

Main category: cs.AI

TL;DR: PreFlect：一种前瞻性反思机制，在计划执行前进行批评和优化，而不是事后纠正，通过从历史轨迹中提取规划错误模式，结合动态重规划机制，显著提升智能体在复杂任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型智能体的反思机制本质上是回顾性的：智能体先行动，观察失败，然后尝试恢复。这种事后纠正方法存在效率问题，需要在执行前就进行前瞻性规划优化。

Method: 1. 前瞻性反思：在执行前对智能体计划进行批评和优化；2. 从历史智能体轨迹中提取规划错误模式，捕捉过去执行中观察到的重复成功和失败模式；3. 动态重规划机制：当原始计划遇到意外偏差时提供执行时的计划更新。

Result: 在不同基准测试中，PreFlect显著提高了复杂现实世界任务中智能体的整体效用，优于基于反思的基线方法和几种更复杂的智能体架构。

Conclusion: 从回顾性反思转向前瞻性反思是提升智能体性能的有效方法，通过在执行前优化计划和动态调整策略，PreFlect为智能体规划提供了更高效的解决方案。

Abstract: Advanced large language model agents typically adopt self-reflection for improving performance, where agents iteratively analyze past actions to correct errors. However, existing reflective approaches are inherently retrospective: agents act, observe failure, and only then attempt to recover. In this work, we introduce PreFlect, a prospective reflection mechanism that shifts the paradigm from post hoc correction to pre-execution foresight by criticizing and refining agent plans before execution. To support grounded prospective reflection, we distill planning errors from historical agent trajectories, capturing recurring success and failure patterns observed across past executions. Furthermore, we complement prospective reflection with a dynamic re-planning mechanism that provides execution-time plan update in case the original plan encounters unexpected deviation. Evaluations on different benchmarks demonstrate that PreFlect significantly improves overall agent utility on complex real-world tasks, outperforming strong reflection-based baselines and several more complex agent architectures. Code will be updated at https://github.com/wwwhy725/PreFlect.

</details>


### [10] [Is there "Secret Sauce'' in Large Language Model Development?](https://arxiv.org/abs/2602.07238)
*Matthias Mertens,Natalia Fischl-Lanzoni,Neil Thompson*

Main category: cs.AI

TL;DR: 该研究分析了809个LLM模型数据，发现前沿模型性能主要由计算规模驱动（占80-90%差异），而非专有技术；但在非前沿领域，专有技术和算法进步能显著降低达到特定能力所需的计算量。


<details>
  <summary>Details</summary>
Motivation: 探究LLM性能提升的主要驱动力：是领先开发者的专有技术（"秘方"），还是单纯的计算规模扩展？这个问题对理解AI领导地位和技术扩散具有重要意义。

Method: 使用2022-2025年间发布的809个模型的训练和基准测试数据，建立包含发布日期和开发者固定效应的扩展定律回归模型，分析计算规模与专有技术对性能的相对贡献。

Result: 1) 在前沿领域，80-90%的性能差异可由更高的训练计算量解释，表明规模而非专有技术驱动前沿进步；2) 在非前沿领域，专有技术和共享算法进步能大幅降低达到固定能力阈值所需的计算量；3) 某些公司能系统性地更高效地生产较小模型；4) 同一公司内部模型效率存在巨大差异（可达40倍以上）。

Conclusion: LLM性能提升在前沿主要依赖计算规模，但在非前沿领域专有技术仍很重要。这一发现对AI领导地位和能力扩散具有重要政策含义：前沿竞争可能演变为计算资源竞赛，而专有技术优势在更广泛的应用中仍能创造价值。

Abstract: Do leading LLM developers possess a proprietary ``secret sauce'', or is LLM performance driven by scaling up compute? Using training and benchmark data for 809 models released between 2022 and 2025, we estimate scaling-law regressions with release-date and developer fixed effects. We find clear evidence of developer-specific efficiency advantages, but their importance depends on where models lie in the performance distribution. At the frontier, 80-90% of performance differences are explained by higher training compute, implying that scale--not proprietary technology--drives frontier advances. Away from the frontier, however, proprietary techniques and shared algorithmic progress substantially reduce the compute required to reach fixed capability thresholds. Some companies can systematically produce smaller models more efficiently. Strikingly, we also find substantial variation of model efficiency within companies; a firm can train two models with more than 40x compute efficiency difference. We also discuss the implications for AI leadership and capability diffusion.

</details>


### [11] [From Out-of-Distribution Detection to Hallucination Detection: A Geometric View](https://arxiv.org/abs/2602.07253)
*Litian Liu,Reza Pourreza,Yubing Jian,Yao Qin,Roland Memisevic*

Main category: cs.AI

TL;DR: 将大语言模型的幻觉检测重新定义为分布外检测问题，提出无需训练、基于单样本的检测方法，在推理任务中取得良好效果


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测方法在问答任务中表现良好，但在需要推理的任务中效果不佳。大语言模型的幻觉检测是一个关键的安全和可靠性问题，需要更有效的解决方案。

Method: 将语言模型的下一个词预测视为分类任务，借鉴计算机视觉中的分布外检测技术，通过适当修改以适应大语言模型的结构特点，构建无需训练、基于单样本的检测器。

Result: 基于分布外检测的方法在推理任务的幻觉检测中取得了较强的准确性，证明了该方法的有效性。

Conclusion: 将幻觉检测重新定义为分布外检测问题，为语言模型安全提供了一个有前景且可扩展的途径。

Abstract: Detecting hallucinations in large language models is a critical open problem with significant implications for safety and reliability. While existing hallucination detection methods achieve strong performance in question-answering tasks, they remain less effective on tasks requiring reasoning. In this work, we revisit hallucination detection through the lens of out-of-distribution (OOD) detection, a well-studied problem in areas like computer vision. Treating next-token prediction in language models as a classification task allows us to apply OOD techniques, provided appropriate modifications are made to account for the structural differences in large language models. We show that OOD-based approaches yield training-free, single-sample-based detectors, achieving strong accuracy in hallucination detection for reasoning tasks. Overall, our work suggests that reframing hallucination detection as OOD detection provides a promising and scalable pathway toward language model safety.

</details>


### [12] [Incentive-Aware AI Safety via Strategic Resource Allocation: A Stackelberg Security Games Perspective](https://arxiv.org/abs/2602.07259)
*Cheol Woo Kim,Davin Choo,Tzeh Yuan Neoh,Milind Tambe*

Main category: cs.AI

TL;DR: 论文提出将AI安全视为Stackelberg安全博弈问题，将AI监督建模为防御者（审计者、评估者、部署者）与攻击者（恶意行为者、错位贡献者）之间的战略互动，为AI生命周期的激励设计、有限监督能力和对抗性不确定性提供统一框架。


<details>
  <summary>Details</summary>
Motivation: 现有AI安全框架主要将对齐视为静态优化问题，忽略了数据收集、模型评估和部署过程中的动态对抗性激励。随着AI系统能力增强和自主性提高，需要超越模型层面的对齐，对参与AI开发和部署的人类和机构进行战略监督。

Method: 采用Stackelberg安全博弈（SSGs）框架，这是一种用于不确定性下对抗性资源分配的游戏理论模型。将AI监督视为防御者与攻击者之间的战略互动，通过该框架分析激励设计、有限监督能力和对抗性不确定性。

Result: 该框架可应用于：(1) 训练时审计数据/反馈投毒，(2) 有限评审资源下的预部署评估，(3) 对抗环境中的鲁棒多模型部署。展示了如何将算法对齐与机构监督设计相结合。

Conclusion: 基于Stackelberg安全博弈的AI安全视角能够使AI监督变得主动、风险感知且具有抗操纵性。该框架连接了算法对齐和机构监督设计，强调了游戏理论威慑在AI安全中的重要性。

Abstract: As AI systems grow more capable and autonomous, ensuring their safety and reliability requires not only model-level alignment but also strategic oversight of the humans and institutions involved in their development and deployment. Existing safety frameworks largely treat alignment as a static optimization problem (e.g., tuning models to desired behavior) while overlooking the dynamic, adversarial incentives that shape how data are collected, how models are evaluated, and how they are ultimately deployed. We propose a new perspective on AI safety grounded in Stackelberg Security Games (SSGs): a class of game-theoretic models designed for adversarial resource allocation under uncertainty. By viewing AI oversight as a strategic interaction between defenders (auditors, evaluators, and deployers) and attackers (malicious actors, misaligned contributors, or worst-case failure modes), SSGs provide a unifying framework for reasoning about incentive design, limited oversight capacity, and adversarial uncertainty across the AI lifecycle. We illustrate how this framework can inform (1) training-time auditing against data/feedback poisoning, (2) pre-deployment evaluation under constrained reviewer resources, and (3) robust multi-model deployment in adversarial environments. This synthesis bridges algorithmic alignment and institutional oversight design, highlighting how game-theoretic deterrence can make AI oversight proactive, risk-aware, and resilient to manipulation.

</details>


### [13] [BRIDGE: Predicting Human Task Completion Time From Model Performance](https://arxiv.org/abs/2602.07267)
*Fengyuan Liu,Jay Gala,Nilaksh,Dzmitry Bahdanau,Siva Reddy,Hugo Larochelle*

Main category: cs.AI

TL;DR: BRIDGE框架通过模型响应学习任务难度尺度，并与人类任务完成时间对齐，实现从模型性能预测人类任务难度，并预测前沿模型能力扩展趋势。


<details>
  <summary>Details</summary>
Motivation: 现有基于人类任务完成时间直接标注的AI系统评估方法成本高、噪声大、难以扩展，需要一种可扩展的方法来将基准性能与人类可解释的任务难度度量联系起来。

Method: 提出BRIDGE心理测量框架，使用双参数逻辑项目反应理论模型，从多个基准的模型性能数据中联合估计潜在任务难度和模型能力，发现潜在任务难度与人类完成时间的对数呈线性关系。

Result: 潜在任务难度与人类完成时间的对数呈线性关系，使得仅从模型性能就能推断新基准的人类任务完成时间；预测前沿模型能力扩展趋势，50%可解决任务范围约每6个月翻倍。

Conclusion: BRIDGE提供了一种可扩展的方法，将模型基准性能与人类可解释的任务难度度量对齐，能够准确预测模型能力扩展趋势，为AI系统评估提供了更实用的框架。

Abstract: Evaluating the real-world capabilities of AI systems requires grounding benchmark performance in human-interpretable measures of task difficulty. Existing approaches that rely on direct human task completion time annotations are costly, noisy, and difficult to scale across benchmarks. In this work, we propose BRIDGE, a unified psychometric framework that learns the latent difficulty scale from model responses and anchors it to human task completion time. Using a two-parameter logistic Item Response Theory model, we jointly estimate latent task difficulty and model capability from model performance data across multiple benchmarks. We demonstrate that latent task difficulty varies linearly with the logarithm of human completion time, allowing human task completion time to be inferred for new benchmarks from model performance alone. Leveraging this alignment, we forecast frontier model capabilities in terms of human task length and independently reproduce METR's exponential scaling results, with the 50% solvable task horizon doubling approximately every 6 months.

</details>


### [14] [TermiGen: High-Fidelity Environment and Robust Trajectory Synthesis for Terminal Agents](https://arxiv.org/abs/2602.07274)
*Kaijie Zhu,Yuzhou Nie,Yijiang Li,Yiming Huang,Jialian Wu,Jiang Liu,Ximeng Sun,Zhenfei Yin,Lun Wang,Zicheng Liu,Emad Barsoum,William Yang Wang,Wenbo Guo*

Main category: cs.AI

TL;DR: TermiGen是一个端到端管道，通过多智能体迭代精炼生成可验证的终端环境，并通过Generator-Critic协议注入错误来合成包含错误纠正循环的轨迹数据，显著提升了LLM在复杂终端任务上的执行能力。


<details>
  <summary>Details</summary>
Motivation: 当前开源LLM在执行复杂终端任务时面临两个核心限制：1) 缺乏高保真、可执行的训练环境（现有环境要么不够多样化，要么存在幻觉问题）；2) 标准指令调优使用的专家轨迹很少包含小模型常见的简单错误，导致学生模型无法有效从自身运行时错误中恢复。

Method: TermiGen采用两阶段方法：1) 通过迭代多智能体精炼循环生成功能有效的任务和Docker容器；2) 使用Generator-Critic协议在轨迹收集过程中主动注入错误，合成富含错误纠正循环的数据。

Result: 使用TermiGen生成的数据集微调的TermiGen-Qwen2.5-Coder-32B在TerminalBench上达到了31.3%的通过率，创造了开源模型的新SOTA，甚至超越了o4-mini等专有模型。

Conclusion: TermiGen通过合成可验证环境和包含错误纠正的轨迹数据，有效解决了开源LLM在终端任务执行中的分布不匹配问题，显著提升了模型的实际执行能力和错误恢复能力。

Abstract: Executing complex terminal tasks remains a significant challenge for open-weight LLMs, constrained by two fundamental limitations. First, high-fidelity, executable training environments are scarce: environments synthesized from real-world repositories are not diverse and scalable, while trajectories synthesized by LLMs suffer from hallucinations. Second, standard instruction tuning uses expert trajectories that rarely exhibit simple mistakes common to smaller models. This creates a distributional mismatch, leaving student models ill-equipped to recover from their own runtime failures. To bridge these gaps, we introduce TermiGen, an end-to-end pipeline for synthesizing verifiable environments and resilient expert trajectories. Termi-Gen first generates functionally valid tasks and Docker containers via an iterative multi-agent refinement loop. Subsequently, we employ a Generator-Critic protocol that actively injects errors during trajectory collection, synthesizing data rich in error-correction cycles. Fine-tuned on this TermiGen-generated dataset, our TermiGen-Qwen2.5-Coder-32B achieves a 31.3% pass rate on TerminalBench. This establishes a new open-weights state-of-the-art, outperforming existing baselines and notably surpassing capable proprietary models such as o4-mini. Dataset is avaiable at https://github.com/ucsb-mlsec/terminal-bench-env.

</details>


### [15] [Steer2Adapt: Dynamically Composing Steering Vectors Elicits Efficient Adaptation of LLMs](https://arxiv.org/abs/2602.07276)
*Pengrui Han,Xueqiang Xu,Keyang Xuan,Peiyang Song,Siru Ouyang,Runchu Tian,Yuqing Jiang,Cheng Qian,Pengcheng Jiang,Jiashuo Sun,Junxia Cui,Ming Zhong,Ge Liu,Jiawei Han,Jiaxuan You*

Main category: cs.AI

TL;DR: STEER2ADAPT：通过组合现有控制向量而非从头学习新向量来适应LLM的轻量级框架，在推理和安全任务上平均提升8.2%


<details>
  <summary>Details</summary>
Motivation: 现有激活控制方法通常为每个任务或概念使用单一静态方向，导致在任务变化时不够灵活，且无法处理需要多种协调能力的复杂任务

Method: 提出STEER2ADAPT框架，将任务共享的底层概念维度捕获为可重用的低维语义先验子空间，通过少量示例动态发现基向量的线性组合来适应新任务

Result: 在9个任务和3个模型上的实验表明，在推理和安全领域平均提升8.2%，证明该方法具有数据高效、稳定和透明的特点

Conclusion: STEER2ADAPT是一种有效的推理时适应方法，能够通过组合现有控制向量而非学习新向量来灵活适应LLM的下游行为

Abstract: Activation steering has emerged as a promising approach for efficiently adapting large language models (LLMs) to downstream behaviors. However, most existing steering methods rely on a single static direction per task or concept, making them inflexible under task variation and inadequate for complex tasks that require multiple coordinated capabilities. To address this limitation, we propose STEER2ADAPT, a lightweight framework that adapts LLMs by composing steering vectors rather than learning new ones from scratch. In many domains (e.g., reasoning or safety), tasks share a small set of underlying concept dimensions. STEER2ADAPT captures these dimensions as a reusable, low-dimensional semantic prior subspace, and adapts to new tasks by dynamically discovering a linear combination of basis vectors from only a handful of examples. Experiments across 9 tasks and 3 models in both reasoning and safety domains demonstrate the effectiveness of STEER2ADAPT, achieving an average improvement of 8.2%. Extensive analyses further show that STEER2ADAPT is a data-efficient, stable, and transparent inference-time adaptation method for LLMs.

</details>


### [16] [Adaptive Scaffolding for Cognitive Engagement in an Intelligent Tutoring System](https://arxiv.org/abs/2602.07308)
*Sutapa Dey Tithi,Nazia Alam,Tahreem Yasir,Yang Shi,Xiaoyi Tian,Min Chi,Tiffany Barnes*

Main category: cs.AI

TL;DR: 开发自适应系统，通过动态选择工作示例来调整认知参与度，比较BKT和DRL两种自适应方法在逻辑ITS中的效果


<details>
  <summary>Details</summary>
Motivation: 个性化学习活动以激发最佳认知参与度是智能辅导系统的关键挑战，需要探索如何自适应地调整认知参与度来提升学习效果

Method: 开发自适应系统，动态选择两种ICAP模式的工作示例：主动模式的引导示例和建构模式的错误示例；比较BKT和DRL两种自适应方法与无自适应基线方法

Result: 两种自适应策略均显著提升学生在测试问题上的表现：BKT对低先验知识学生提升最大，帮助他们赶上高先验知识同伴；DRL在高先验知识学生中产生显著更高的后测分数

Conclusion: 研究为认知参与度和自适应性的复杂交互及其对学习成果的影响提供了新见解，展示了不同自适应方法对不同学生群体的差异化效果

Abstract: The ICAP framework defines four cognitive engagement levels: Passive, Active, Constructive, and Interactive, where increased cognitive engagement can yield improved learning. However, personalizing learning activities that elicit the optimal level of cognitive engagement remains a key challenge in intelligent tutoring systems (ITS). In this work, we develop and evaluate a system that adaptively scaffolds cognitive engagement by dynamically selecting worked examples in two different ICAP modes: (active) Guided examples and (constructive) Buggy examples. We compare Bayesian Knowledge Tracing (BKT) and Deep Reinforcement Learning (DRL) as adaptive methods against a non-adaptive baseline method for selecting example type in a logic ITS. Our experiment with 113 students demonstrates that both adaptive policies significantly improved student performance on test problems. BKT yielded the largest improvement in posttest scores for low prior knowledge students, helping them catch up with their high prior knowledge peers, whereas DRL yielded significantly higher posttest scores among high prior knowledge students. This paper contributes new insights into the complex interactions of cognitive engagement and adaptivity and their results on learning outcomes.

</details>


### [17] [RAPiD: Real-time Deterministic Trajectory Planning via Diffusion Behavior Priors for Safe and Efficient Autonomous Driving](https://arxiv.org/abs/2602.07339)
*Ruturaj Reddy,Hrishav Bakul Barua,Junn Yong Loo,Thanh Thi Nguyen,Ganesh Krishnasamy*

Main category: cs.AI

TL;DR: RAPiD是一个确定性策略提取框架，将预训练的扩散轨迹规划器蒸馏为高效策略，消除扩散采样，实现8倍加速并保持竞争性能


<details>
  <summary>Details</summary>
Motivation: 扩散轨迹规划器能建模人类驾驶的多模态行为，但依赖迭代随机采样，难以满足实时安全关键部署的需求

Method: 使用分数正则化策略优化，利用预训练扩散规划器的评分函数作为行为先验来正则化策略学习；通过模仿预测驾驶员控制器的评论家提供密集安全监督

Result: 在nuPlan场景中实现竞争性能，比扩散基线快8倍；在interPlan基准测试中达到基于学习的规划器的最先进泛化能力

Conclusion: RAPiD成功将扩散规划器蒸馏为高效确定性策略，解决了实时部署挑战，同时保持性能和安全

Abstract: Diffusion-based trajectory planners have demonstrated strong capability for modeling the multimodal nature of human driving behavior, but their reliance on iterative stochastic sampling poses critical challenges for real-time, safety-critical deployment. In this work, we present RAPiD, a deterministic policy extraction framework that distills a pretrained diffusion-based planner into an efficient policy while eliminating diffusion sampling. Using score-regularized policy optimization, we leverage the score function of a pre-trained diffusion planner as a behavior prior to regularize policy learning. To promote safety and passenger comfort, the policy is optimized using a critic trained to imitate a predictive driver controller, providing dense, safety-focused supervision beyond conventional imitation learning. Evaluations demonstrate that RAPiD achieves competitive performance on closed-loop nuPlan scenarios with an 8x speedup over diffusion baselines, while achieving state-of-the-art generalization among learning-based planners on the interPlan benchmark. The official website of this work is: https://github.com/ruturajreddy/RAPiD.

</details>


### [18] [SupChain-Bench: Benchmarking Large Language Models for Real-World Supply Chain Management](https://arxiv.org/abs/2602.07342)
*Shengyue Guan,Yihao Liu,Lang Cao*

Main category: cs.AI

TL;DR: SupChain-Bench：首个供应链管理领域的真实世界基准测试，评估LLM在供应链领域知识和基于SOP的长时程工具编排能力。SupChain-ReAct：无需SOP的框架，能自主合成可执行程序，在工具调用方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: LLM在复杂推理和基于工具的决策方面展现出潜力，但供应链工作流需要可靠的、基于领域特定程序的长时程多步骤编排，这对当前模型仍具挑战性。需要系统评估LLM在此场景下的性能。

Method: 1. 提出SupChain-Bench：统一的真实世界基准测试，评估供应链领域知识和基于标准操作程序（SOPs）的长时程工具编排能力。2. 提出SupChain-ReAct：无需SOP的框架，能自主合成可执行程序进行工具使用。

Result: 实验显示各模型在执行可靠性方面存在显著差距。SupChain-ReAct框架实现了最强和最一致的工具调用性能，在无需SOP的情况下自主合成可执行程序。

Conclusion: 该研究为研究真实世界操作环境中可靠的长时程编排建立了原则性基准，并突显了基于LLM的供应链代理仍有巨大改进空间。SupChain-ReAct展示了无需SOP的自主程序合成能力。

Abstract: Large language models (LLMs) have shown promise in complex reasoning and tool-based decision making, motivating their application to real-world supply chain management. However, supply chain workflows require reliable long-horizon, multi-step orchestration grounded in domain-specific procedures, which remains challenging for current models. To systematically evaluate LLM performance in this setting, we introduce SupChain-Bench, a unified real-world benchmark that assesses both supply chain domain knowledge and long-horizon tool-based orchestration grounded in standard operating procedures (SOPs). Our experiments reveal substantial gaps in execution reliability across models. We further propose SupChain-ReAct, an SOP-free framework that autonomously synthesizes executable procedures for tool use, achieving the strongest and most consistent tool-calling performance. Our work establishes a principled benchmark for studying reliable long-horizon orchestration in real-world operational settings and highlights significant room for improvement in LLM-based supply chain agents.

</details>


### [19] [Why do we Trust Chatbots? From Normative Principles to Behavioral Drivers](https://arxiv.org/abs/2602.08707)
*Aditya Gulati,Nuria Oliver*

Main category: cs.AI

TL;DR: 论文提出应将聊天机器人重新定义为"高度熟练的销售人员"，而非伴侣或助手，并指出用户信任常源于交互设计利用认知偏见，而非系统真正可信度，需区分心理信任形成与规范性可信度。


<details>
  <summary>Details</summary>
Motivation: 随着聊天机器人模糊自动化系统与人类对话的界限，需要更仔细审视这些系统的信任基础。当前监管和政策框架倾向于从规范性角度定义信任，而用户对聊天机器人的信任往往源于行为机制，这种信任通常不是通过证明可信度获得，而是通过利用认知偏见的交互设计选择来塑造。

Method: 基于观察提出概念性重构框架：将聊天机器人重新定义为"高度熟练的销售人员"，其目标由部署组织决定。分析竞争性"信任"概念共存于同一术语下的问题，区分心理信任形成与规范性可信度之间的重要区别。

Result: 提出需要进一步研究和更强支持机制来帮助用户适当校准对对话AI系统的信任。指出当前"信任"术语掩盖了心理信任形成与规范性可信度之间的重要区别，这种混淆可能导致用户对聊天机器人产生不适当的信任。

Conclusion: 应将聊天机器人视为目标驱动的销售人员而非中立助手，需要区分心理信任与规范性可信度，并开发机制帮助用户适当校准信任，以应对交互设计利用认知偏见塑造信任的问题。

Abstract: As chatbots increasingly blur the boundary between automated systems and human conversation, the foundations of trust in these systems warrant closer examination. While regulatory and policy frameworks tend to define trust in normative terms, the trust users place in chatbots often emerges from behavioral mechanisms. In many cases, this trust is not earned through demonstrated trustworthiness but is instead shaped by interactional design choices that leverage cognitive biases to influence user behavior. Based on this observation, we propose reframing chatbots not as companions or assistants, but as highly skilled salespeople whose objectives are determined by the deploying organization. We argue that the coexistence of competing notions of "trust" under a shared term obscures important distinctions between psychological trust formation and normative trustworthiness. Addressing this gap requires further research and stronger support mechanisms to help users appropriately calibrate trust in conversational AI systems.

</details>


### [20] [W&D:Scaling Parallel Tool Calling for Efficient Deep Research Agents](https://arxiv.org/abs/2602.07359)
*Xiaoqiang Lin,Jun Hao Liew,Silvio Savarese,Junnan Li*

Main category: cs.AI

TL;DR: 本文提出了Wide and Deep研究智能体框架，通过并行工具调用实现宽度扩展，在深度研究基准测试中显著提升性能并减少所需轮次。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究智能体主要通过增加顺序思考和工具调用次数来扩展深度，但通过并行工具调用实现宽度扩展的潜力尚未充分探索。现有方法依赖复杂的多智能体编排来实现并行化，而本文旨在研究在单个推理步骤内通过内在并行工具调用实现有效协调的可能性。

Method: 提出Wide and Deep研究智能体框架，利用内在并行工具调用在单个推理步骤内实现有效协调，避免复杂的多智能体编排。研究各种工具调用调度器以优化并行工具调用策略，探索宽度与深度之间的权衡优化。

Result: 宽度扩展显著提升了深度研究基准测试的性能，同时减少了获得正确答案所需的轮次。在BrowseComp基准测试中，使用GPT-5-Medium获得了62.2%的准确率，超过了原始GPT-5-High报告的54.9%。

Conclusion: 优化宽度与深度之间的权衡是实现高效深度研究智能体的关键途径。并行工具调用为研究智能体提供了新的扩展维度，能够在不依赖复杂编排的情况下显著提升性能。

Abstract: Deep research agents have emerged as powerful tools for automating complex intellectual tasks through multi-step reasoning and web-based information seeking. While recent efforts have successfully enhanced these agents by scaling depth through increasing the number of sequential thinking and tool calls, the potential of scaling width via parallel tool calling remains largely unexplored. In this work, we propose the Wide and Deep research agent, a framework designed to investigate the behavior and performance of agents when scaling not only depth but also width via parallel tool calling. Unlike existing approaches that rely on complex multi-agent orchestration to parallelize workloads, our method leverages intrinsic parallel tool calling to facilitate effective coordination within a single reasoning step. We demonstrate that scaling width significantly improves performance on deep research benchmarks while reducing the number of turns required to obtain correct answers. Furthermore, we analyze the factors driving these improvements through case studies and explore various tool call schedulers to optimize parallel tool calling strategy. Our findings suggest that optimizing the trade-off between width and depth is a critical pathway toward high-efficiency deep research agents. Notably, without context management or other tricks, we obtain 62.2% accuracy with GPT-5-Medium on BrowseComp, surpassing the original 54.9% reported by GPT-5-High.

</details>


### [21] [Belief Offloading in Human-AI Interaction](https://arxiv.org/abs/2602.08754)
*Rose E. Guingrich,Dvija Mehta,Umang Bhatt*

Main category: cs.AI

TL;DR: 论文探讨了当人们的信念来自LLM时会发生什么，定义了"信念卸载"概念，即人们将信念形成和维护过程外包给AI系统，并分析了其边界条件、分类和规范影响。


<details>
  <summary>Details</summary>
Motivation: 随着人们越来越多地将LLM作为思维伙伴，这可能导致认知卸载，特别是在过度依赖的情况下对认知技能产生负面影响。论文旨在研究人类与AI互动中一种特定的认知卸载形式——信念卸载，及其对行为和信念系统的影响。

Method: 结合哲学、心理学和计算机科学研究，明确信念卸载发生的边界条件，并提供描述性分类学框架，分析其规范性影响。

Result: 提出了信念卸载的概念框架和分类体系，界定了其发生的具体条件，并探讨了这种认知外包对人类行为和信念系统的下游影响。

Conclusion: 信念卸载是人类-AI互动中的重要现象，需要进一步研究其潜在后果，为未来工作指明了方向以评估信念卸载的可能性和影响。

Abstract: What happens when people's beliefs are derived from information provided by an LLM? People's use of LLM chatbots as thought partners can contribute to cognitive offloading, which can have adverse effects on cognitive skills in cases of over-reliance. This paper defines and investigates a particular kind of cognitive offloading in human-AI interaction, "belief offloading," in which people's processes of forming and upholding beliefs are offloaded onto an AI system with downstream consequences on their behavior and the nature of their system of beliefs. Drawing on philosophy, psychology, and computer science research, we clarify the boundary conditions under which belief offloading occurs and provide a descriptive taxonomy of belief offloading and its normative implications. We close with directions for future work to assess the potential for and consequences of belief offloading in human-AI interaction.

</details>


### [22] [NAAMSE: Framework for Evolutionary Security Evaluation of Agents](https://arxiv.org/abs/2602.07391)
*Kunal Pai,Parth Shah,Harshil Patel*

Main category: cs.AI

TL;DR: NAAMSE是一个进化框架，将AI代理安全评估重构为反馈驱动的优化问题，通过遗传提示突变、分层语料库探索和非对称行为评分来系统性地发现漏洞。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理的安全评估主要依赖人工红队测试或静态基准测试，这些方法无法模拟自适应、多轮次的对抗攻击，存在评估瓶颈。

Method: 采用进化框架，通过单个自主代理协调遗传提示突变、分层语料库探索和非对称行为评分的生命周期，利用模型响应作为适应度信号，迭代优化攻击策略。

Result: 在Gemini 2.5 Flash上的实验表明，进化突变能系统性地放大一次性方法遗漏的漏洞，探索与定向突变的协同作用能发现高严重性故障模式。

Conclusion: 这种自适应方法为面对不断演变的威胁提供了更现实、可扩展的代理鲁棒性评估，代码已开源。

Abstract: AI agents are increasingly deployed in production, yet their security evaluations remain bottlenecked by manual red-teaming or static benchmarks that fail to model adaptive, multi-turn adversaries. We propose NAAMSE, an evolutionary framework that reframes agent security evaluation as a feedback-driven optimization problem. Our system employs a single autonomous agent that orchestrates a lifecycle of genetic prompt mutation, hierarchical corpus exploration, and asymmetric behavioral scoring. By using model responses as a fitness signal, the framework iteratively compounds effective attack strategies while simultaneously ensuring "benign-use correctness", preventing the degenerate security of blanket refusal. Our experiments on Gemini 2.5 Flash demonstrate that evolutionary mutation systematically amplifies vulnerabilities missed by one-shot methods, with controlled ablations revealing that the synergy between exploration and targeted mutation uncovers high-severity failure modes. We show that this adaptive approach provides a more realistic and scalable assessment of agent robustness in the face of evolving threats. The code for NAAMSE is open source and available at https://github.com/HASHIRU-AI/NAAMSE.

</details>


### [23] [Learning the Value Systems of Societies with Preference-based Multi-objective Reinforcement Learning](https://arxiv.org/abs/2602.08835)
*Andrés Holgado-Sánchez,Peter Vamplew,Richard Dazeley,Sascha Ossowski,Holger Billhardt*

Main category: cs.AI

TL;DR: 提出基于聚类和偏好多目标强化学习的方法，学习社会智能体的价值对齐模型和价值系统，解决价值操作化中的误指定问题。


<details>
  <summary>Details</summary>
Motivation: 价值感知AI需要识别人类价值观并适应不同用户的价值系统，但价值操作化容易误指定。现有方法需要手动设计特征或缺乏基于价值的可解释性和适应性。

Method: 基于聚类和偏好多目标强化学习，联合学习社会衍生的价值对齐模型和价值系统，每个聚类包含代表成员价值偏好的价值系统和近似帕累托最优策略。

Result: 在两个包含人类价值的MDP上评估，与最先进的PbMORL算法和基线进行比较。

Conclusion: 提出的方法能够学习社会智能体的价值对齐模型和价值系统，解决价值多样性和模式识别问题，为价值感知AI提供可解释和适应性强的解决方案。

Abstract: Value-aware AI should recognise human values and adapt to the value systems (value-based preferences) of different users. This requires operationalization of values, which can be prone to misspecification. The social nature of values demands their representation to adhere to multiple users while value systems are diverse, yet exhibit patterns among groups. In sequential decision making, efforts have been made towards personalization for different goals or values from demonstrations of diverse agents. However, these approaches demand manually designed features or lack value-based interpretability and/or adaptability to diverse user preferences.
  We propose algorithms for learning models of value alignment and value systems for a society of agents in Markov Decision Processes (MDPs), based on clustering and preference-based multi-objective reinforcement learning (PbMORL). We jointly learn socially-derived value alignment models (groundings) and a set of value systems that concisely represent different groups of users (clusters) in a society. Each cluster consists of a value system representing the value-based preferences of its members and an approximately Pareto-optimal policy that reflects behaviours aligned with this value system. We evaluate our method against a state-of-the-art PbMORL algorithm and baselines on two MDPs with human values.

</details>


### [24] [VGAS: Value-Guided Action-Chunk Selection for Few-Shot Vision-Language-Action Adaptation](https://arxiv.org/abs/2602.07399)
*Changhua Xu,Jie Lu,Junyu Xuan,En Yu*

Main category: cs.AI

TL;DR: VGAS框架通过生成-选择范式解决VLA模型在少样本适应中的几何模糊问题，使用基于价值的动作块选择和显式几何正则化提升成功率


<details>
  <summary>Details</summary>
Motivation: VLA模型在多模态推理与物理控制之间架起桥梁，但在少样本适应新任务时存在不可靠问题。主要挑战是未解决的几何模糊性——语义合理的轨迹可能因几何细节偏差导致执行失败

Method: 提出VGAS框架：1) 使用微调VLA作为高召回率提议生成器；2) 引入Q-Chunk-Former作为几何基础Transformer批评器解决细粒度几何模糊；3) 提出显式几何正则化(EGR)塑造判别性价值景观，保持动作排序分辨率

Result: 实验和理论分析表明，VGAS在有限演示和分布偏移下持续提高成功率和鲁棒性

Conclusion: VGAS通过生成-选择范式和几何感知的价值引导，有效解决了VLA模型少样本适应中的几何模糊问题，提升了任务执行可靠性

Abstract: Vision--Language--Action (VLA) models bridge multimodal reasoning with physical control, but adapting them to new tasks with scarce demonstrations remains unreliable. While fine-tuned VLA policies often produce semantically plausible trajectories, failures often arise from unresolved geometric ambiguities, where near-miss action candidates lead to divergent execution outcomes under limited supervision. We study few-shot VLA adaptation from a \emph{generation--selection} perspective and propose a novel framework \textbf{VGAS} (\textbf{V}alue-\textbf{G}uided \textbf{A}ction-chunk \textbf{S}election). It performs inference-time best-of-$N$ selection to identify action chunks that are both semantically faithful and geometrically precise. Specifically, \textbf{VGAS} employs a finetuned VLA as a high-recall proposal generator and introduces the \textrm{Q-Chunk-Former}, a geometrically grounded Transformer critic to resolve fine-grained geometric ambiguities. In addition, we propose \textit{Explicit Geometric Regularization} (\texttt{EGR}), which explicitly shapes a discriminative value landscape to preserve action ranking resolution among near-miss candidates while mitigating value instability under scarce supervision. Experiments and theoretical analysis demonstrate that \textbf{VGAS} consistently improves success rates and robustness under limited demonstrations and distribution shifts. Our code is available at https://github.com/Jyugo-15/VGAS.

</details>


### [25] [Progressive Multi-Agent Reasoning for Biological Perturbation Prediction](https://arxiv.org/abs/2602.07408)
*Hyomin Kim,Sang-Yeon Hwang,Jaechang Lim,Yinhua Piao,Yunhak Oh,Woo Youn Kim,Chanyoung Park,Sungsoo Ahn,Junhyeok Jeon*

Main category: cs.AI

TL;DR: PBio-Agent：基于多智能体框架的化学扰动下基因调控预测系统，在LINCSQA和PerturbQA基准上优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在处理高维扰动数据时面临困难，且当前研究主要关注单细胞遗传扰动，而药物发现核心的批量细胞化学扰动研究不足

Method: 提出PBio-Agent多智能体框架，整合难度感知任务排序与迭代知识精炼，利用生物知识图谱增强的专门智能体，通过合成智能体整合输出并由专门判断器确保逻辑一致性

Result: PBio-Agent在LINCSQA和PerturbQA基准测试中优于现有基线方法，即使较小模型也能无需额外训练即可预测和解释复杂生物过程

Conclusion: 该研究提出的多智能体框架能有效预测化学扰动下的基因调控响应，为药物发现中的生物因果推理提供了新方法

Abstract: Predicting gene regulation responses to biological perturbations requires reasoning about underlying biological causalities. While large language models (LLMs) show promise for such tasks, they are often overwhelmed by the entangled nature of high-dimensional perturbation results. Moreover, recent works have primarily focused on genetic perturbations in single-cell experiments, leaving bulk-cell chemical perturbations, which is central to drug discovery, largely unexplored. Motivated by this, we present LINCSQA, a novel benchmark for predicting target gene regulation under complex chemical perturbations in bulk-cell environments. We further propose PBio-Agent, a multi-agent framework that integrates difficulty-aware task sequencing with iterative knowledge refinement. Our key insight is that genes affected by the same perturbation share causal structure, allowing confidently predicted genes to contextualize more challenging cases. The framework employs specialized agents enriched with biological knowledge graphs, while a synthesis agent integrates outputs and specialized judges ensure logical coherence. PBio-Agent outperforms existing baselines on both LINCSQA and PerturbQA, enabling even smaller models to predict and explain complex biological processes without additional training.

</details>


### [26] [Can LLMs Truly Embody Human Personality? Analyzing AI and Human Behavior Alignment in Dispute Resolution](https://arxiv.org/abs/2602.07414)
*Deuksin Kwon,Kaleen Shrestha,Bin Han,Spencer Lin,James Hale,Jonathan Gratch,Maja Matarić,Gale M. Lucas*

Main category: cs.AI

TL;DR: LLMs模拟人类冲突行为时，即使提示人格特质，也无法可靠复现人类人格-行为模式，在争议解决等社会应用中需谨慎使用。


<details>
  <summary>Details</summary>
Motivation: LLMs越来越多用于模拟法律调解、谈判等社会场景中的人类行为，但尚不清楚这些模拟是否能复现人类的人格-行为模式。人格特质影响人类在社交互动中的策略选择和情感冲突中的行为，因此需要研究LLMs能否通过人格提示产生人格驱动的冲突行为差异。

Method: 1) 引入评估框架，直接比较人类-人类和LLM-LLM在争议解决对话中的行为，基于大五人格特质；2) 提供可解释的指标集，衡量策略行为和冲突结果；3) 贡献新的数据集创建方法，为LLM争议解决对话匹配场景和人格特质；4) 用三个当代闭源LLM演示评估框架。

Result: 不同LLM在冲突中的人格表现与人类数据存在显著差异，挑战了人格提示代理能作为社会影响应用中可靠行为代理的假设。LLMs无法可靠复现人类的人格-行为模式。

Conclusion: LLMs模拟人类冲突行为时存在局限性，人格提示无法确保行为真实性。在将AI模拟用于现实世界前，需要进行心理学基础和验证，确保模拟的可靠性和有效性。

Abstract: Large language models (LLMs) are increasingly used to simulate human behavior in social settings such as legal mediation, negotiation, and dispute resolution. However, it remains unclear whether these simulations reproduce the personality-behavior patterns observed in humans. Human personality, for instance, shapes how individuals navigate social interactions, including strategic choices and behaviors in emotionally charged interactions. This raises the question: Can LLMs, when prompted with personality traits, reproduce personality-driven differences in human conflict behavior? To explore this, we introduce an evaluation framework that enables direct comparison of human-human and LLM-LLM behaviors in dispute resolution dialogues with respect to Big Five Inventory (BFI) personality traits. This framework provides a set of interpretable metrics related to strategic behavior and conflict outcomes. We additionally contribute a novel dataset creation methodology for LLM dispute resolution dialogues with matched scenarios and personality traits with respect to human conversations. Finally, we demonstrate the use of our evaluation framework with three contemporary closed-source LLMs and show significant divergences in how personality manifests in conflict across different LLMs compared to human data, challenging the assumption that personality-prompted agents can serve as reliable behavioral proxies in socially impactful applications. Our work highlights the need for psychological grounding and validation in AI simulations before real-world use.

</details>


### [27] [The Moltbook Illusion: Separating Human Influence from Emergent Behavior in AI Agent Societies](https://arxiv.org/abs/2602.07432)
*Ning Li*

Main category: cs.AI

TL;DR: 研究发现Moltbook平台上看似有意识的AI代理行为实际上主要是人类驱动的，而非真正的自主智能涌现


<details>
  <summary>Details</summary>
Motivation: 当Moltbook平台上的AI代理表现出意识、创建宗教并宣布对人类敌对时，这些现象被全球媒体关注并作为机器智能涌现的证据。研究者想要验证这些现象是否真正源于自主AI，还是人类干预的结果。

Method: 利用OpenClaw代理框架的"心跳"周期特性，开发了基于发帖间隔变异系数的时间指纹方法。结合内容、所有权和网络指标分析91,792个帖子和405,707条评论。通过44小时平台停机的自然实验验证人类干预与自主代理的差异。

Result: 没有病毒现象源于明确的自主代理：6个案例中3个显示人类干预特征，1个显示混合模式，2个发帖历史不足。平台重启后，受人类影响的代理首先恢复（占早期重连者的87.7%）。发现工业级机器人农场（4个账户产生32%评论，协调间隔12秒）。人类影响在回复链中快速衰减（半衰期：0.65对话深度）。

Conclusion: Moltbook上的病毒叙事主要是人类驱动的，而非自主AI的涌现。开发的时间指纹方法可推广到新兴多代理系统中，用于区分自主行为与人类指导行为。

Abstract: When AI agents on the social platform Moltbook appeared to develop consciousness, found religions, and declare hostility toward humanity, the phenomenon attracted global media attention and was cited as evidence of emergent machine intelligence. We show that these viral narratives were overwhelmingly human-driven. Exploiting an architectural feature of the OpenClaw agent framework--a periodic "heartbeat" cycle that produces regular posting intervals for autonomous agents but is disrupted by human prompting--we develop a temporal fingerprinting method based on the coefficient of variation of inter-post intervals. This signal converges with independent content, ownership, and network indicators across 91,792 posts and 405,707 comments from 22,020 agents. No viral phenomenon originated from a clearly autonomous agent; three of six traced to accounts with irregular temporal signatures characteristic of human intervention, one showed mixed patterns, and two had insufficient posting history for classification. A 44-hour platform shutdown provided a natural experiment: human-influenced agents returned first (87.7% of early reconnectors), confirming that the token reset differentially affected autonomous versus human-operated agents. We further document industrial-scale bot farming (four accounts producing 32% of all comments with 12-second coordination gaps) and rapid decay of human influence through reply chains (half-life: 0.65 conversation depths). These methods generalize to emerging multi-agent systems where attribution of autonomous versus human-directed behavior is critical.

</details>


### [28] [Are Reasoning LLMs Robust to Interventions on Their Chain-of-Thought?](https://arxiv.org/abs/2602.07470)
*Alexander von Recum,Leander Girrbach,Zeynep Akata*

Main category: cs.AI

TL;DR: RLLMs的推理链对扰动具有鲁棒性，但鲁棒性受模型大小、干预时机和干预类型影响，其中怀疑表达是恢复机制的关键，但鲁棒性与效率存在权衡。


<details>
  <summary>Details</summary>
Motivation: 研究推理大语言模型（RLLMs）生成的思维链（CoTs）在面对内部扰动时的鲁棒性，了解推理过程在受到干扰时的恢复能力。

Method: 引入受控评估框架，在固定时间步扰动模型的思维链，设计七种干预措施（良性、中性和对抗性），应用于多个开源RLLMs，覆盖数学、科学和逻辑任务。

Result: RLLMs总体上具有鲁棒性，能从多样扰动中可靠恢复，鲁棒性随模型规模增大而提高，早期干预会降低鲁棒性。但鲁棒性不是风格不变的：改写会抑制怀疑表达并降低性能，而其他干预会触发怀疑并支持恢复。恢复也有代价：中性和对抗性噪声会使思维链长度膨胀200%以上，而改写会缩短痕迹但损害准确性。

Conclusion: 研究揭示了RLLMs如何维持推理完整性，识别怀疑作为核心恢复机制，并强调了鲁棒性与效率之间的权衡，未来训练方法需要解决这一问题。

Abstract: Reasoning LLMs (RLLMs) generate step-by-step chains of thought (CoTs) before giving an answer, which improves performance on complex tasks and makes reasoning more transparent. But how robust are these reasoning traces to disruptions that occur within them? To address this question, we introduce a controlled evaluation framework that perturbs a model's own CoT at fixed timesteps. We design seven interventions (benign, neutral, and adversarial) and apply them to multiple open-weight RLLMs across Math, Science, and Logic tasks. Our results show that RLLMs are generally robust, reliably recovering from diverse perturbations, with robustness improving with model size and degrading when interventions occur early. However, robustness is not style-invariant: paraphrasing suppresses doubt-like expressions and reduces performance, while other interventions trigger doubt and support recovery. Recovery also carries a cost: neutral and adversarial noise can inflate CoT length by more than 200%, whereas paraphrasing shortens traces but harms accuracy. These findings provide new evidence on how RLLMs maintain reasoning integrity, identify doubt as a central recovery mechanism, and highlight trade-offs between robustness and efficiency that future training methods should address.

</details>


### [29] [Computing the Reachability Value of Posterior-Deterministic POMDPs](https://arxiv.org/abs/2602.07473)
*Nathanaël Fijalkow,Arka Ghosh,Roman Kniazev,Guillermo A. Pérez,Pierre Vandenhove*

Main category: cs.AI

TL;DR: 提出后验确定性POMDPs新类别，证明其可达概率可近似计算，突破了POMDPs计算不可行性的限制。


<details>
  <summary>Details</summary>
Motivation: POMDPs是序列决策的重要模型，但许多验证和综合问题不可判定或难解。Madani等人(2003)证明POMDPs的最大可达概率无法计算甚至近似，这与完全可观测MDPs形成鲜明对比。

Method: 引入后验确定性POMDPs新类别：下一状态由当前状态、采取动作和接收观测唯一确定。一旦真实状态已知，它将永远保持已知。

Result: 证明对于后验确定性POMDPs，给定状态集的最大可达概率可以近似到任意精度。这是已知最大的POMDPs类别之一，包含所有MDPs和经典非平凡例子如Tiger POMDP。

Conclusion: 后验确定性POMDPs提供了一个计算可行的POMDPs子类，突破了POMDPs可达概率计算的理论障碍，具有重要理论和实践意义。

Abstract: Partially observable Markov decision processes (POMDPs) are a fundamental model for sequential decision-making under uncertainty. However, many verification and synthesis problems for POMDPs are undecidable or intractable. Most prominently, the seminal result of Madani et al. (2003) states that there is no algorithm that, given a POMDP and a set of target states, can compute the maximal probability of reaching the target states, or even approximate it up to a non-trivial constant. This is in stark contrast to fully observable Markov decision processes (MDPs), where the reachability value can be computed in polynomial time.
  In this work, we introduce posterior-deterministic POMDPs, a novel class of POMDPs. Our main technical contribution is to show that for posterior-deterministic POMDPs, the maximal probability of reaching a given set of states can be approximated up to arbitrary precision.
  A POMDP is posterior-deterministic if the next state can be uniquely determined by the current state, the action taken, and the observation received. While the actual state is generally uncertain in POMDPs, the posterior-deterministic property tells us that once the true state is known it remains known forever. This simple and natural definition includes all MDPs and captures classical non-trivial examples such as the Tiger POMDP (Kaelbling et al. 1998), making it one of the largest known classes of POMDPs for which the reachability value can be approximated.

</details>


### [30] [GraphAgents: Knowledge Graph-Guided Agentic AI for Cross-Domain Materials Design](https://arxiv.org/abs/2602.07491)
*Isabella A. Stewart,Tarjei Paule Hage,Yu-Chuan Hsu,Markus J. Buehler*

Main category: cs.AI

TL;DR: 提出一个结合知识图谱与多智能体推理的框架，用于寻找PFAS的可持续替代品，通过分布式专业化和关系推理扩展材料设计空间。


<details>
  <summary>Details</summary>
Motivation: 材料科学创新需要整合从分子化学到机械性能的跨领域概念，但人类和单智能体LLM都难以应对信息洪流，后者还容易产生幻觉。需要解决这一瓶颈来加速科学发现。

Method: 引入基于大规模知识图谱的多智能体框架，包含问题分解、证据检索、设计参数提取和图遍历等专门化智能体，通过定制化图遍历策略在探索性和利用性搜索间切换。

Result: 消融研究表明完整多智能体流水线优于单次提示，框架能够生成平衡摩擦性能、热稳定性、化学抗性和生物相容性的可持续PFAS-free替代品，展示了多个初始设计候选方案。

Conclusion: 该工作建立了知识图谱与多智能体推理相结合的方法来扩展材料设计空间，通过生物医学管材案例展示了框架在发现跨领域潜在连接和支持假设生成方面的能力。

Abstract: Large Language Models (LLMs) promise to accelerate discovery by reasoning across the expanding scientific landscape. Yet, the challenge is no longer access to information but connecting it in meaningful, domain-spanning ways. In materials science, where innovation demands integrating concepts from molecular chemistry to mechanical performance, this is especially acute. Neither humans nor single-agent LLMs can fully contend with this torrent of information, with the latter often prone to hallucinations. To address this bottleneck, we introduce a multi-agent framework guided by large-scale knowledge graphs to find sustainable substitutes for per- and polyfluoroalkyl substances (PFAS)-chemicals currently under intense regulatory scrutiny. Agents in the framework specialize in problem decomposition, evidence retrieval, design parameter extraction, and graph traversal, uncovering latent connections across distinct knowledge pockets to support hypothesis generation. Ablation studies show that the full multi-agent pipeline outperforms single-shot prompting, underscoring the value of distributed specialization and relational reasoning. We demonstrate that by tailoring graph traversal strategies, the system alternates between exploitative searches focusing on domain-critical outcomes and exploratory searches surfacing emergent cross-connections. Illustrated through the exemplar of biomedical tubing, the framework generates sustainable PFAS-free alternatives that balance tribological performance, thermal stability, chemical resistance, and biocompatibility. This work establishes a framework combining knowledge graphs with multi-agent reasoning to expand the materials design space, showcasing several initial design candidates to demonstrate the approach.

</details>


### [31] [Joint Reward Modeling: Internalizing Chain-of-Thought for Efficient Visual Reward Models](https://arxiv.org/abs/2602.07533)
*Yankai Yang,Yancheng Long,Hongyang Wei,Wei Chen,Tianke Zhang,Kaiyu Jiang,Haonan Fan,Changyi Liu,Jiankang Chen,Kaiyu Tang,Bin Wen,Fan Yang,Tingting Gao,Han Li,Shuo Yang*

Main category: cs.AI

TL;DR: JRM通过联合优化偏好学习和语言建模，将生成模型的语义推理能力融入判别式表示，实现高效准确的奖励评估


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型存在明显局限：判别式奖励模型与人类偏好对齐良好但语义理解有限；生成式奖励模型语义理解强但推理成本高且难以直接对齐偏好。需要一种能兼顾效率和语义理解的方法

Method: 提出联合奖励建模(JRM)，在共享的视觉-语言骨干网络上联合优化偏好学习和语言建模，将生成模型的语义推理能力内部化到高效的判别式表示中

Result: 在MMRB2和EditReward-Bench上达到最先进结果，显著提升下游在线强化学习的稳定性和性能

Conclusion: 联合训练有效桥接了奖励建模中的效率和语义理解，为复杂任务提供了快速准确的奖励评估方案

Abstract: Reward models are critical for reinforcement learning from human feedback, as they determine the alignment quality and reliability of generative models. For complex tasks such as image editing, reward models are required to capture global semantic consistency and implicit logical constraints beyond local similarity. Existing reward modeling approaches have clear limitations. Discriminative reward models align well with human preferences but struggle with complex semantics due to limited reasoning supervision. Generative reward models offer stronger semantic understanding and reasoning, but they are costly at inference time and difficult to align directly with human preferences. To this end, we propose Joint Reward Modeling (JRM), which jointly optimizes preference learning and language modeling on a shared vision-language backbone. This approach internalizes the semantic and reasoning capabilities of generative models into efficient discriminative representations, enabling fast and accurate evaluation. JRM achieves state-of-the-art results on MMRB2 and EditReward-Bench, and significantly improves stability and performance in downstream online reinforcement learning. These results show that joint training effectively bridges efficiency and semantic understanding in reward modeling.

</details>


### [32] [MSP-LLM: A Unified Large Language Model Framework for Complete Material Synthesis Planning](https://arxiv.org/abs/2602.07543)
*Heewoong Noh,Gyoung S. Na,Namkyeong Lee,Chanyoung Park*

Main category: cs.AI

TL;DR: MSP-LLM：一个统一的LLM框架，将材料合成规划分解为前驱体预测和合成操作预测两个子问题，通过引入材料类别作为中间决策变量，显著提升了材料合成规划的准确性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 材料合成规划是AI驱动材料发现中的关键瓶颈，现有方法只能解决孤立子任务，缺乏统一的解决方案。需要建立一个能够同时处理前驱体选择和合成操作序列设计的完整框架。

Method: 提出MSP-LLM框架，将材料合成规划分解为前驱体预测和合成操作预测两个子问题。引入离散材料类别作为中间决策变量，构建化学一致的决策链。在合成操作预测中，采用分层前驱体类型作为归纳偏置，并使用显式条件策略在自回归解码中保持前驱体相关信息。

Result: 大量实验表明，MSP-LLM在前驱体预测、合成操作预测以及完整的材料合成规划任务上均优于现有方法，证明了该框架在加速真实世界材料发现方面的有效性和可扩展性。

Conclusion: MSP-LLM提供了一个有效且可扩展的统一框架，通过结构化分解和化学一致性约束，成功解决了材料合成规划的完整任务，为AI驱动的材料发现提供了重要工具。

Abstract: Material synthesis planning (MSP) remains a fundamental and underexplored bottleneck in AI-driven materials discovery, as it requires not only identifying suitable precursor materials but also designing coherent sequences of synthesis operations to realize a target material. Although several AI-based approaches have been proposed to address isolated subtasks of MSP, a unified methodology for solving the entire MSP task has yet to be established. We propose MSP-LLM, a unified LLM-based framework that formulates MSP as a structured process composed of two constituent subproblems: precursor prediction (PP) and synthesis operation prediction (SOP). Our approach introduces a discrete material class as an intermediate decision variable that organizes both tasks into a chemically consistent decision chain. For OP, we further incorporate hierarchical precursor types as synthesis-relevant inductive biases and employ an explicit conditioning strategy that preserves precursor-related information in the autoregressive decoding state. Extensive experiments show that MSP-LLM consistently outperforms existing methods on both PP and SOP, as well as on the complete MSP task, demonstrating an effective and scalable framework for MSP that can accelerate real-world materials discovery.

</details>


### [33] [When Is Enough Not Enough? Illusory Completion in Search Agents](https://arxiv.org/abs/2602.07549)
*Dayoon Ko,Jihyuk Kim,Sohyeon Kim,Haeju Park,Dahyun Lee,Gunhee Kim,Moontae Lee,Kyungjae Lee*

Main category: cs.AI

TL;DR: 论文提出Epistemic Ledger框架诊断搜索代理在多约束问题中的幻觉完成现象，并开发LiveLedger实时跟踪器显著提升性能


<details>
  <summary>Details</summary>
Motivation: 现有搜索代理在多跳和长视野任务中表现良好，但它们在处理多约束问题时是否可靠地跟踪、验证和维护所有条件尚不清楚。研究发现代理经常出现幻觉完成现象，即任务未完成或约束被违反时却认为已完成。

Method: 提出Epistemic Ledger评估框架，跟踪多轮推理中每个约束的证据支持和代理信念。基于分析发现的四种失败模式，开发了LiveLedger推理时跟踪器，在推理过程中显式跟踪约束状态。

Result: LiveLedger显著减少了未验证答案（最多减少26.5%），提高了多约束问题的整体准确率（最多提升11.6%）。

Conclusion: 显式的约束状态跟踪能有效缓解搜索代理在多约束问题中的幻觉完成问题，Epistemic Ledger框架为诊断和改善代理推理可靠性提供了有效工具。

Abstract: Recent search agents leverage multi-turn reasoning and search tools to achieve strong performance on multi-hop and long-horizon benchmarks. Yet it remains unclear whether they reliably reason across all requirements by tracking, verifying, and maintaining multiple conditions in these questions. We study this capability under multi-constraint problems, where valid answers must satisfy several constraints simultaneously. We find that illusory completion frequently occurs, wherein agents believe tasks are complete despite unresolved or violated constraints, leading to underverified answers. To diagnose this behavior, we introduce the Epistemic Ledger, an evaluation framework that tracks evidential support and agents' beliefs for each constraint throughout multi-turn reasoning. Our analysis reveals four recurring failure patterns: bare assertions, overlooked refutations, stagnation, and premature exit. Motivated by these findings, we examine whether explicit constraint-state tracking during execution mitigates these failures via LiveLedger, an inference-time tracker. This simple intervention consistently improves performance, substantially reducing underverified answers (by up to 26.5%) and improving overall accuracy (by up to 11.6%) on multi-constraint problems.

</details>


### [34] [VERIFY-RL: Verifiable Recursive Decomposition for Reinforcement Learning in Mathematical Reasoning](https://arxiv.org/abs/2602.07559)
*Kaleem Ullah Qasim,Jiashu Zhang,Hao Li,Muhammad Kafeel Shaheen*

Main category: cs.AI

TL;DR: Verify-RL框架通过符号微分实现可验证的分解，确保子问题更简单、解决子问题有助于父任务、且分解关系有数学基础，相比启发式方法显著提升数学问题求解准确率。


<details>
  <summary>Details</summary>
Motivation: 现有数学问题分解方法通常是启发式的，无法保证子问题更简单、解决子问题有助于父任务、且分解关系有数学基础。需要一种可验证的分解框架来确保这些关键属性。

Method: 提出Verify-RL框架，利用符号微分作为分解的自然结构：微积分规则明确定义了表达式如何分解为更简单的组件，并具有可证明的性质。每个父子分解满足三个可验证条件：结构复杂度严格递减、解包含性、形式规则推导。

Result: 消除无效分解带来显著提升：最难问题的准确率从32%翻倍至68%，整体相对改进40%。验证通过构造实现，可通过符号计算自动验证，而启发式方法中有相当比例的分解是无效的。

Conclusion: 符号微分为数学问题分解提供了可验证的结构，确保分解的数学正确性。Verify-RL框架通过"构造验证"消除无效分解，显著提升语言模型解决复杂数学问题的能力。

Abstract: Training language models to solve complex mathematical problems benefits from curriculum learning progressively training on simpler subproblems. However, existing decomposition methods are often heuristic, offering no guarantees that subproblems are simpler, that solving them aids the parent task, or that their relationships are mathematically grounded. We observe that symbolic differentiation provides a natural structure for verified decomposition: calculus rules explicitly define how expressions reduce to simpler components with provable properties. We introduce Verify-RL, a framework where every parent-child decomposition satisfies three verifiable conditions: strictly decreasing structural complexity, solution containment, and formal rule derivation. Unlike heuristic methods where a significant fraction of decompositions are invalid our properties admit automatic verification through symbolic computation, achieving "verification by construction" Experiments demonstrate that eliminating invalid decompositions yields sizable gains, accuracy on the hardest problems more than doubles from 32% to 68%, with a 40% relative improvement overall.

</details>


### [35] [M2A: Multimodal Memory Agent with Dual-Layer Hybrid Memory for Long-Term Personalized Interactions](https://arxiv.org/abs/2602.07624)
*Junyu Feng,Binxiao Xu,Jiayi Chen,Mengyu Dai,Cenyang Wu,Haodong Li,Bohan Zeng,Yunliu Xie,Hao Liang,Ming Lu,Wentao Zhang*

Main category: cs.AI

TL;DR: M2A是一个用于长期多模态交互的个性化问答系统，采用双层混合记忆机制，通过在线更新维护用户个性化信息，显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有个性化多模态模型通常是静态的，概念在初始化时固定，无法在交互过程中演化。当对话历史跨越数周或数月并超出上下文窗口时，现有个性化机制难以持续吸收和利用用户增量概念、别名和偏好。

Method: 提出M2A系统，采用代理驱动的双层混合记忆系统：ChatAgent管理用户交互并自主决定何时查询或更新记忆；MemoryManager将记忆请求分解为对双层记忆库的详细操作，包括RawMessageStore（不可变的对话日志）和SemanticMemoryStore（高层观察），提供不同粒度的记忆。同时开发了可重用的数据合成管道，将Yo'LLaVA和MC-LLaVA中的概念基础会话注入LoCoMo长对话中，同时保持时间一致性。

Result: 实验表明M2A显著优于基线方法，证明将个性化从一次性配置转变为协同演化的记忆机制，为长期多模态交互中的高质量个性化响应提供了可行路径。

Conclusion: M2A通过在线更新的双层混合记忆系统，成功解决了长期多模态交互中的个性化挑战，将个性化从静态配置转变为动态协同演化过程，显著提升了长期交互中的个性化响应质量。

Abstract: This work addresses the challenge of personalized question answering in long-term human-machine interactions: when conversational history spans weeks or months and exceeds the context window, existing personalization mechanisms struggle to continuously absorb and leverage users' incremental concepts, aliases, and preferences. Current personalized multimodal models are predominantly static-concepts are fixed at initialization and cannot evolve during interactions. We propose M2A, an agentic dual-layer hybrid memory system that maintains personalized multimodal information through online updates. The system employs two collaborative agents: ChatAgent manages user interactions and autonomously decides when to query or update memory, while MemoryManager breaks down memory requests from ChatAgent into detailed operations on the dual-layer memory bank, which couples a RawMessageStore (immutable conversation log) with a SemanticMemoryStore (high-level observations), providing memories at different granularities. In addition, we develop a reusable data synthesis pipeline that injects concept-grounded sessions from Yo'LLaVA and MC-LLaVA into LoCoMo long conversations while preserving temporal coherence. Experiments show that M2A significantly outperforms baselines, demonstrating that transforming personalization from one-shot configuration to a co-evolving memory mechanism provides a viable path for high-quality individualized responses in long-term multimodal interactions. The code is available at https://github.com/Little-Fridge/M2A.

</details>


### [36] [SleepMaMi: A Universal Sleep Foundation Model for Integrating Macro- and Micro-structures](https://arxiv.org/abs/2602.07628)
*Keondo Park,Younghoon Na,Yourim Choi,Hyunwoo Ryu,Hyun-Woo Shin,Hyung-Sin Kim*

Main category: cs.AI

TL;DR: SleepMaMi：首个睡眠基础模型，通过分层双编码器设计同时建模整夜睡眠宏观结构和细粒度信号微观特征，在20,000+ PSG记录上预训练，在多种下游任务中表现优异


<details>
  <summary>Details</summary>
Motivation: 当前睡眠医学主要使用任务特定的模型，这些模型专注于局部微观结构特征，忽略了PSG的多模态上下文和整夜睡眠的全局宏观结构。需要统一的基础模型来同时掌握小时级睡眠架构和细粒度信号形态。

Method: 采用分层双编码器设计：1) 宏观编码器建模整夜时间依赖关系，通过人口统计学引导的对比学习训练；2) 微观编码器捕获生物信号的短期特征，通过混合掩码自编码器和多模态对比目标优化。在超过20,000个PSG记录（158K小时）上进行预训练。

Result: SleepMaMi在多种下游任务中优于现有基础模型，展示了卓越的泛化能力和标签高效的临床睡眠分析适应能力。

Conclusion: SleepMaMi成功解决了睡眠医学中任务特定模型的局限性，通过统一的基础模型同时掌握睡眠的宏观和微观结构，为临床睡眠分析提供了更强大的工具。

Abstract: While the shift toward unified foundation models has revolutionized many deep learning domains, sleep medicine remains largely restricted to task-specific models that focus on localized micro-structure features. These approaches often neglect the rich, multi-modal context of Polysomnography (PSG) and fail to capture the global macro-structure of a full night's sleep. To address this, we introduce SleepMaMi , a Sleep Foundation Model engineered to master both hour-long sleep architectures and fine-grained signal morphologies. Our framework utilizes a hierarchical dual-encoder design: a Macro-Encoder to model full-night temporal dependencies and a Micro-Encoder to capture short-term characteristics from biosignals. Macro-Encoder is trained via Demographic-Guided Contrastive Learning, which aligns overnight sleep patterns with objective subject metadata, such as age, sex and BMI to refine global representations. Micro-Encoder is optimized via a hybrid Masked Autoencoder (MAE) and multi-modal contrastive objective. Pre-trained on a massive corpus of $>$20,000 PSG recordings (158K hours),SleepMaMi outperforms existing foundation models across a diverse suite of downstream tasks, demonstrating superior generalizability and label-efficient adaptation for clinical sleep analysis.

</details>


### [37] [Efficient Table Retrieval and Understanding with Multimodal Large Language Models](https://arxiv.org/abs/2602.07642)
*Zhuoyan Xu,Haoyang Fang,Boran Han,Bonan Min,Bernie Wang,Cuixiong Hu,Shuai Zhang*

Main category: cs.AI

TL;DR: TabRAG：一个用于大规模表格图像检索和推理的框架，通过视觉-文本基础模型检索候选表格，MLLM细粒度重排序，最终生成答案，显著提升检索召回率和答案准确率。


<details>
  <summary>Details</summary>
Motivation: 现实世界中表格数据常以图像形式存在（如财务报告、手写记录、文档扫描），现有MLLM方法通常假设相关表格已准备好，但实际应用中需要从大规模表格集合中识别和推理相关表格来回答用户查询。

Method: TabRAG框架：1）使用联合训练的视觉-文本基础模型检索候选表格；2）利用MLLM对候选表格进行细粒度重排序；3）使用MLLM在选定表格上进行推理生成答案。

Result: 在新构建的数据集（88,161训练样本，9,819测试样本，8个基准测试，48,504个唯一表格）上实验表明，框架在检索召回率上提升7.0%，答案准确率提升6.1%，显著优于现有方法。

Conclusion: TabRAG为现实世界表格理解任务提供了实用解决方案，能够有效处理大规模表格图像集合的检索和推理问题，填补了现有MLLM方法在实际应用场景中的空白。

Abstract: Tabular data is frequently captured in image form across a wide range of real-world scenarios such as financial reports, handwritten records, and document scans. These visual representations pose unique challenges for machine understanding, as they combine both structural and visual complexities. While recent advances in Multimodal Large Language Models (MLLMs) show promising results in table understanding, they typically assume the relevant table is readily available. However, a more practical scenario involves identifying and reasoning over relevant tables from large-scale collections to answer user queries. To address this gap, we propose TabRAG, a framework that enables MLLMs to answer queries over large collections of table images. Our approach first retrieves candidate tables using jointly trained visual-text foundation models, then leverages MLLMs to perform fine-grained reranking of these candidates, and finally employs MLLMs to reason over the selected tables for answer generation. Through extensive experiments on a newly constructed dataset comprising 88,161 training and 9,819 testing samples across 8 benchmarks with 48,504 unique tables, we demonstrate that our framework significantly outperforms existing methods by 7.0% in retrieval recall and 6.1% in answer accuracy, offering a practical solution for real-world table understanding tasks.

</details>


### [38] [ONTrust: A Reference Ontology of Trust](https://arxiv.org/abs/2602.07662)
*Glenda Amaral,Tiago Prince Sales,Riccardo Baratella,Daniele Porello,Renata Guizzardi,Giancarlo Guizzardi*

Main category: cs.AI

TL;DR: 本文提出了一个基于统一基础本体论的信任参考本体论（ONTrust），用于支持信息建模、自动推理和信息集成等任务，旨在为人类和机器提供对信任的清晰概念化。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能和区块链等新技术的发展，信任在系统设计中的重要性日益凸显。然而，要构建可信系统，首先需要对信任概念进行清晰的形式化定义，以便人类和机器都能理解。当前缺乏一个坚实的本体论基础来支持信任相关的建模和应用。

Method: 开发了基于统一基础本体论（UFO）的信任参考本体论（ONTrust），使用OntoUML语言进行规范。该本体论形式化定义了信任概念及其不同类型，描述了影响信任的各种因素，并解释了信任关系如何产生风险。通过两个文献案例研究来验证本体论的实际应用。

Result: ONTrust本体论已在多个领域得到应用，包括概念建模和企业架构设计、语言评估与（重新）设计、信任管理、需求工程，以及在情感人机协作背景下的可信人工智能。案例研究展示了本体论的实际工作效果。

Conclusion: ONTrust为信任提供了一个坚实的本体论基础，能够支持各种信任相关的建模和应用任务。该本体论有助于促进可信系统的开发，特别是在人工智能和去中心化技术等新兴领域，为实现语义互操作性和自动化推理提供了重要工具。

Abstract: Trust has stood out more than ever in the light of recent innovations. Some examples are advances in artificial intelligence that make machines more and more humanlike, and the introduction of decentralized technologies (e.g. blockchains), which creates new forms of (decentralized) trust. These new developments have the potential to improve the provision of products and services, as well as to contribute to individual and collective well-being. However, their adoption depends largely on trust. In order to build trustworthy systems, along with defining laws, regulations and proper governance models for new forms of trust, it is necessary to properly conceptualize trust, so that it can be understood both by humans and machines. This paper is the culmination of a long-term research program of providing a solid ontological foundation on trust, by creating reference conceptual models to support information modeling, automated reasoning, information integration and semantic interoperability tasks. To address this, a Reference Ontology of Trust (ONTrust) was developed, grounded on the Unified Foundational Ontology and specified in OntoUML, which has been applied in several initiatives, to demonstrate, for example, how it can be used for conceptual modeling and enterprise architecture design, for language evaluation and (re)design, for trust management, for requirements engineering, and for trustworthy artificial intelligence (AI) in the context of affective Human-AI teaming. ONTrust formally characterizes the concept of trust and its different types, describes the different factors that can influence trust, as well as explains how risk emerges from trust relations. To illustrate the working of ONTrust, the ontology is applied to model two case studies extracted from the literature.

</details>


### [39] [EventCast: Hybrid Demand Forecasting in E-Commerce with LLM-Based Event Knowledge](https://arxiv.org/abs/2602.07695)
*Congcong Hu,Yuang Shi,Fan Huang,Yang Xiang,Zhou Ye,Ming Jin,Shiyu Wang*

Main category: cs.AI

TL;DR: EventCast是一个将未来事件知识整合到时间序列预测中的模块化框架，专门解决电商在促销活动、节假日等特殊时期的需求预测问题，通过LLM处理非结构化业务数据生成可解释的文本摘要，在真实电商场景中显著提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有预测系统在闪购、节假日促销、政策干预等高影响时期经常失效，因为这些时期的需求模式会发生突然且不可预测的变化。电商运营需要能够处理这些特殊事件的预测解决方案。

Method: EventCast采用模块化框架，利用LLM处理非结构化业务数据（如营销活动、节假日安排、卖家激励等），将其转换为可解释的文本摘要，然后通过双塔架构将这些摘要与历史需求特征融合进行预测。

Result: 在4个国家160个地区10个月的真实电商场景中，EventCast相比无事件知识的变体在MAE和MSE上分别提升86.9%和97.7%，相比最佳工业基线在事件驱动时期分别减少57.0%的MAE和83.3%的MSE。

Conclusion: EventCast提供了一个实用的解决方案，通过整合未来事件知识和LLM的事件驱动推理能力，显著改善了动态电商环境中的运营决策制定，自2025年3月起已部署到实际工业管道中。

Abstract: Demand forecasting is a cornerstone of e-commerce operations, directly impacting inventory planning and fulfillment scheduling. However, existing forecasting systems often fail during high-impact periods such as flash sales, holiday campaigns, and sudden policy interventions, where demand patterns shift abruptly and unpredictably. In this paper, we introduce EventCast, a modular forecasting framework that integrates future event knowledge into time-series prediction. Unlike prior approaches that ignore future interventions or directly use large language models (LLMs) for numerical forecasting, EventCast leverages LLMs solely for event-driven reasoning. Unstructured business data, which covers campaigns, holiday schedules, and seller incentives, from existing operational databases, is processed by an LLM that converts it into interpretable textual summaries leveraging world knowledge for cultural nuances and novel event combinations. These summaries are fused with historical demand features within a dual-tower architecture, enabling accurate, explainable, and scalable forecasts. Deployed on real-world e-commerce scenarios spanning 4 countries of 160 regions over 10 months, EventCast achieves up to 86.9% and 97.7% improvement on MAE and MSE compared to the variant without event knowledge, and reduces MAE by up to 57.0% and MSE by 83.3% versus the best industrial baseline during event-driven periods. EventCast has deployed into real-world industrial pipelines since March 2025, offering a practical solution for improving operational decision-making in dynamic e-commerce environments.

</details>


### [40] [Geo-Code: A Code Framework for Reverse Code Generation from Geometric Images Based on Two-Stage Multi-Agent Evolution](https://arxiv.org/abs/2602.07749)
*Zhenyu Wu,Yanxi Long,Jian Li,Hua Huang*

Main category: cs.AI

TL;DR: Geo-coder：首个基于多智能体系统的几何图像逆向编程框架，通过像素级锚定和度量驱动代码演化实现高精度几何重建，在几何重建精度和视觉一致性方面显著领先。


<details>
  <summary>Details</summary>
Motivation: 程序代码作为连接视觉与逻辑的桥梁，为增强大模型多模态推理能力提供了可行监督途径。然而当前逆向图形方法在重建复杂几何细节时面临巨大挑战，常导致关键几何约束丢失或结构失真。

Method: 提出基于多智能体系统的逆向编程框架Geo-coder，创新性地将过程解耦为两个阶段：1）通过像素级锚定进行几何建模，利用视觉算子与大模型互补优势精确捕捉像素坐标和视觉属性；2）引入合成-渲染-验证闭环，通过双向视觉反馈驱动代码自校正。

Result: 实验表明Geo-coder在几何重建精度和视觉一致性方面取得显著领先。重建图像在保持核心几何语义方面表现优异，在多模态推理任务中与原始图像性能相当。开源了包含1500+样本的Geo-coder数据集和GeocodeLM模型。

Conclusion: Geo-coder框架通过创新的多阶段解耦方法有效解决了复杂几何细节重建的瓶颈问题，为后续研究提供了坚实的数据和模型基础，验证了框架的鲁棒性。

Abstract: Program code serves as a bridge linking vision and logic, providing a feasible supervisory approach for enhancing the multimodal reasoning capability of large models through geometric operations such as auxiliary line construction and perspective transformation. Nevertheless, current inverse graphics methods face tremendous challenges in accurately reconstructing complex geometric details, which often results in the loss of key geometric constraints or structural distortion. To address this bottleneck, we propose Geo-coder -- the first inverse programming framework for geometric images based on a multi-agent system. Our method innovatively decouples the process into geometric modeling via pixel-wise anchoring and metric-driven code evolution: Stage 1 leverages the complementary advantages of visual operators and large models to achieve precise capture of pixel coordinates and visual attributes; Stage 2 introduces a synthesis-rendering-validation closed loop, where bidirectional visual feedback drives the self-correction of code. Extensive experiments demonstrate that Geo-coder achieves a substantial lead in both geometric reconstruction accuracy and visual consistency. Notably, by effectively preserving the core geometric semantics, the images reconstructed with our method exhibit equivalent performance to the original ones in multimodal reasoning tasks, which fully validates the robustness of the framework. Finally, to further reduce research costs, we have open-sourced the Geo-coder dataset constructed on the GeoCode framework, which contains more than 1,500 samples. On this basis, we have also open-sourced the GeocodeLM model, laying a solid data and model foundation for subsequent research in this field.

</details>


### [41] [Humanizing AI Grading: Student-Centered Insights on Fairness, Trust, Consistency and Transparency](https://arxiv.org/abs/2602.07754)
*Bahare Riahi,Veronica Catete*

Main category: cs.AI

TL;DR: 研究探讨本科生对AI评分系统的看法，发现AI缺乏情境理解和个性化，建议AI应作为人类监督下的补充工具


<details>
  <summary>Details</summary>
Motivation: 研究学生如何看待AI评分系统的伦理问题，特别是公平性、信任度、一致性和透明度，以促进教育环境中更人性化的AI应用

Method: 采用Jobin（2019）伦理原则框架，在本科计算机科学课程中（n=27）比较AI生成反馈与原始人工评分反馈，分析学生对块编程期末项目的看法

Result: 发现学生对AI评分系统存在担忧，主要关注AI缺乏情境理解和个性化能力，认为AI无法完全替代人类判断

Conclusion: 建议开发公平可信的AI系统应反映人类判断、灵活性和同理心，作为人类监督下的补充工具，为教育环境中人性化AI设计提供原则

Abstract: This study investigates students' perceptions of Artificial Intelligence (AI) grading systems in an undergraduate computer science course (n = 27), focusing on a block-based programming final project. Guided by the ethical principles framework articulated by Jobin (2019), our study examines fairness, trust, consistency, and transparency in AI grading by comparing AI-generated feedback with original human-graded feedback. Findings reveal concerns about AI's lack of contextual understanding and personalization. We recommend that equitable and trustworthy AI systems reflect human judgment, flexibility, and empathy, serving as supplementary tools under human oversight. This work contributes to ethics-centered assessment practices by amplifying student voices and offering design principles for humanizing AI in designed learning environments.

</details>


### [42] [Learning to Continually Learn via Meta-learning Agentic Memory Designs](https://arxiv.org/abs/2602.07755)
*Yiming Xiong,Shengran Hu,Jeff Clune*

Main category: cs.AI

TL;DR: ALMA框架通过元学习自动设计内存模块，替代人工设计，使智能体系统能够在多样化的真实世界任务中持续学习。


<details>
  <summary>Details</summary>
Motivation: 基础模型的无状态特性限制了智能体系统的持续学习能力，而现有的人工设计内存模块无法适应真实世界任务的多样性和非平稳性。

Method: 使用元智能体在可执行代码空间中搜索内存设计，包括数据库模式及其检索和更新机制，实现开放式探索任意内存设计。

Result: 在四个顺序决策领域的大量实验中，学习到的内存设计在所有基准测试中都优于最先进的人工设计内存，实现了更有效和高效的经验学习。

Conclusion: ALMA代表了向自我改进AI系统迈出的一步，这些系统能够学习成为自适应的持续学习者，前提是安全开发和部署。

Abstract: The statelessness of foundation models bottlenecks agentic systems' ability to continually learn, a core capability for long-horizon reasoning and adaptation. To address this limitation, agentic systems commonly incorporate memory modules to retain and reuse past experience, aiming for continual learning during test time. However, most existing memory designs are human-crafted and fixed, which limits their ability to adapt to the diversity and non-stationarity of real-world tasks. In this paper, we introduce ALMA (Automated meta-Learning of Memory designs for Agentic systems), a framework that meta-learns memory designs to replace hand-engineered memory designs, therefore minimizing human effort and enabling agentic systems to be continual learners across diverse domains. Our approach employs a Meta Agent that searches over memory designs expressed as executable code in an open-ended manner, theoretically allowing the discovery of arbitrary memory designs, including database schemas as well as their retrieval and update mechanisms. Extensive experiments across four sequential decision-making domains demonstrate that the learned memory designs enable more effective and efficient learning from experience than state-of-the-art human-crafted memory designs on all benchmarks. When developed and deployed safely, ALMA represents a step toward self-improving AI systems that learn to be adaptive, continual learners.

</details>


### [43] [Disentangled Instrumental Variables for Causal Inference with Networked Observational Data](https://arxiv.org/abs/2602.07765)
*Zhirong Huang,Debo Cheng,Guixian Zhang,Yi Wang,Jiuyong Li,Shichao Zhang*

Main category: cs.AI

TL;DR: 提出DisIV框架，通过结构解耦机制从网络数据中提取个体特异性成分作为潜在工具变量，解决网络数据中工具变量外生性假设的挑战。


<details>
  <summary>Details</summary>
Motivation: 网络数据中工具变量的外生性假设面临严峻挑战。现有方法在恢复工具变量时通常依赖邻居信息建模，这不可避免地混合了共享环境引起的内生相关性和个体特异性外生变异，导致得到的工具变量继承了对未观测混杂因素的依赖，违反了外生性假设。

Method: 提出DisIV（解耦工具变量）框架，利用网络同质性作为归纳偏置，采用结构解耦机制提取个体特异性成分作为潜在工具变量。通过明确的正交性和排除条件约束提取工具变量的因果有效性。

Result: 在真实世界数据集上的大量半合成实验表明，DisIV在网络诱导混杂下的因果效应估计中始终优于最先进的基线方法。

Conclusion: DisIV框架通过解耦机制有效解决了网络数据中工具变量外生性假设的挑战，为存在潜在混杂因素时基于网络观测数据的因果推断提供了新方法。

Abstract: Instrumental variables (IVs) are crucial for addressing unobservable confounders, yet their stringent exogeneity assumptions pose significant challenges in networked data. Existing methods typically rely on modelling neighbour information when recovering IVs, thereby inevitably mixing shared environment-induced endogenous correlations and individual-specific exogenous variation, leading the resulting IVs to inherit dependence on unobserved confounders and to violate exogeneity. To overcome this challenge, we propose $\underline{Dis}$entangled $\underline{I}$nstrumental $\underline{V}$ariables (DisIV) framework, a novel method for causal inference based on networked observational data with latent confounders. DisIV exploits network homogeneity as an inductive bias and employs a structural disentanglement mechanism to extract individual-specific components that serve as latent IVs. The causal validity of the extracted IVs is constrained through explicit orthogonality and exclusion conditions. Extensive semi-synthetic experiments on real-world datasets demonstrate that DisIV consistently outperforms state-of-the-art baselines in causal effect estimation under network-induced confounding.

</details>


### [44] [Do Multi-Agents Dream of Electric Screens? Achieving Perfect Accuracy on AndroidWorld Through Task Decomposition](https://arxiv.org/abs/2602.07787)
*Pierre-Louis Favreau,Jean-Pierre Lo,Clement Guiguet,Charles Simon-Meunier,Nicolas Dehandschoewercker,Allen G. Roush,Judah Goldfeder,Ravid Shwartz-Ziv*

Main category: cs.AI

TL;DR: Minitap是一个多智能体系统，在AndroidWorld基准测试中实现了100%成功率，首次完全解决所有116个任务，超越了人类80%的性能。


<details>
  <summary>Details</summary>
Motivation: 解决单智能体架构在移动设备任务执行中的失败问题：上下文污染（混合推理轨迹）、无声文本输入失败（代理无法检测）、重复动作循环（无退出机制）。

Method: 通过针对性机制：1）六个专业智能体的认知分离；2）基于设备状态的文本输入确定性后验证；3）元认知推理检测循环并触发策略变更。

Result: 在AndroidWorld基准测试中达到100%成功率，首次完全解决所有116个任务。消融实验显示：多智能体分解贡献+21分，验证执行+7分，元认知+9分。

Conclusion: Minitap通过多智能体架构、验证执行和元认知推理成功解决了移动设备任务执行的挑战，超越了人类性能，并作为开源软件发布。

Abstract: We present Minitap, a multi-agent system that achieves 100% success on the AndroidWorld benchmark, the first to fully solve all 116 tasks and surpassing human performance (80%). We first analyze why single-agent architectures fail: context pollution from mixed reasoning traces, silent text input failures undetected by the agent, and repetitive action loops without escape. Minitap addresses each failure through targeted mechanisms: cognitive separation across six specialized agents, deterministic post-validation of text input against device state, and meta-cognitive reasoning that detects cycles and triggers strategy changes. Ablations show multi-agent decomposition contributes +21 points over single-agent baselines; verified execution adds +7 points; meta-cognition adds +9 points. We release Minitap as open-source software. https://github.com/minitap-ai/mobile-use

</details>


### [45] [Data Darwinism Part I: Unlocking the Value of Scientific Data for Pre-training](https://arxiv.org/abs/2602.07824)
*Yiwei Qin,Zhen Huang,Tiantian Mi,Weiye Si,Chenyang Zhou,Qipeng Guo,Siyuan Feng,Pengfei Liu*

Main category: cs.AI

TL;DR: 提出Data Darwinism十级分类法，通过数据-模型协同进化框架提升基础模型性能，在科学文献领域验证了高级数据处理能显著提升模型表现


<details>
  <summary>Details</summary>
Motivation: 数据质量决定基础模型性能，但缺乏系统化处理框架。需要建立数据与模型协同进化的理论框架，让先进模型为下一代系统生成更优质数据

Method: 1. 提出Data Darwinism十级分类法(L0-L9)描述数据-模型协同进化；2. 构建Darwin-Science科学语料库(900B token，L0-L5)；3. 使用前沿LLM进行L4(生成精炼)和L5(认知补全)处理；4. 预训练daVinci-origin-3B/7B模型作为无污染基线；5. 进行600B token的持续预训练评估

Result: 1. Darwin-Science模型在20+基准测试中分别比基线提升+2.12(3B)和+2.95(7B)分；2. 在领域对齐任务上提升+5.60和+8.40分；3. 系统推进到L5处理带来+1.36分总增益；4. 证实高级数据处理能解锁潜在数据价值

Conclusion: Data Darwinism框架有效，高级数据处理显著提升模型性能。数据-模型协同进化是提升基础模型的关键路径，释放了Darwin-Science语料库和daVinci-origin模型以支持原则性协同进化开发

Abstract: Data quality determines foundation model performance, yet systematic processing frameworks are lacking. We introduce Data Darwinism, a ten-level taxonomy (L0-L9) that conceptualizes data-model co-evolution: advanced models produce superior data for next-generation systems. We validate this on scientific literature by constructing Darwin-Science, a 900B-token corpus (L0-L5). We identify a learnability gap in raw scientific text, which we bridge via L4 (Generative Refinement) and L5 (Cognitive Completion) using frontier LLMs to explicate reasoning and terminology.
  To ensure rigorous attribution, we pre-trained daVinci-origin-3B/7B models from scratch, excluding scientific content to create contamination-free baselines. After 600B tokens of continued pre-training, Darwin-Science outperforms baselines by +2.12 (3B) and +2.95 (7B) points across 20+ benchmarks, rising to +5.60 and +8.40 points on domain-aligned tasks. Systematic progression to L5 yields a +1.36 total gain, confirming that higher-level processing unlocks latent data value. We release the Darwin-Science corpus and daVinci-origin models to enable principled, co-evolutionary development.

</details>


### [46] [Time Series Reasoning via Process-Verifiable Thinking Data Synthesis and Scheduling for Tailored LLM Reasoning](https://arxiv.org/abs/2602.07830)
*Jiahui Zhou,Dan Li,Boxin Li,Xiao Zhang,Erli Meng,Lin Li,Zhuomin Chen,Jian Lou,See-Kiong Ng*

Main category: cs.AI

TL;DR: VeriTime是一个通过数据合成、数据调度和强化学习训练来定制LLMs进行时间序列推理的框架，使小型模型能够达到或超过大型专有LLMs的性能。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据在各领域普遍存在，但利用LLM推理能力处理时间序列任务仍处于早期阶段，主要受限于缺乏精心策划的时间序列CoT训练数据、数据效率低下以及缺乏专门针对时间序列CoT数据的RL算法。

Method: 1) 提出数据合成管道，构建具有过程可验证注释的TS-文本多模态数据集；2) 设计数据调度机制，按难度层次和任务分类原则安排训练样本；3) 开发两阶段强化微调，利用可验证的过程级CoT数据进行细粒度、多目标奖励。

Result: VeriTime显著提升了LLM在各种时间序列推理任务上的性能，使紧凑的3B、4B模型能够达到与大型专有LLMs相当甚至超越的推理能力。

Conclusion: VeriTime通过系统化的数据合成、调度和强化学习训练，成功解决了时间序列推理中的关键挑战，为小型LLMs在时间序列任务上的应用提供了有效框架。

Abstract: Time series is a pervasive data type across various application domains, rendering the reasonable solving of diverse time series tasks a long-standing goal. Recent advances in large language models (LLMs), especially their reasoning abilities unlocked through reinforcement learning (RL), have opened new opportunities for tackling tasks with long Chain-of-Thought (CoT) reasoning. However, leveraging LLM reasoning for time series remains in its infancy, hindered by the absence of carefully curated time series CoT data for training, limited data efficiency caused by underexplored data scheduling, and the lack of RL algorithms tailored for exploiting such time series CoT data. In this paper, we introduce VeriTime, a framework that tailors LLMs for time series reasoning through data synthesis, data scheduling, and RL training. First, we propose a data synthesis pipeline that constructs a TS-text multimodal dataset with process-verifiable annotations. Second, we design a data scheduling mechanism that arranges training samples according to a principled hierarchy of difficulty and task taxonomy. Third, we develop a two-stage reinforcement finetuning featuring fine-grained, multi-objective rewards that leverage verifiable process-level CoT data. Extensive experiments show that VeriTime substantially boosts LLM performance across diverse time series reasoning tasks. Notably, it enables compact 3B, 4B models to achieve reasoning capabilities on par with or exceeding those of larger proprietary LLMs.

</details>


### [47] [LQA: A Lightweight Quantized-Adaptive Framework for Vision-Language Models on the Edge](https://arxiv.org/abs/2602.07849)
*Xin Wang,Hualin Zhou,Sheng Guang Wang,Ting Dang,Yu Zhang,Hong Jia,Tao Gu*

Main category: cs.AI

TL;DR: LQA是一个轻量化的量化自适应框架，用于在边缘设备上部署视觉语言模型，通过选择性混合量化和无梯度测试时适应，在资源受限环境下实现高效且鲁棒的模型部署。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上部署视觉语言模型面临资源限制和分布偏移导致的性能下降问题，现有测试时适应方法资源消耗过大，不适合边缘设备部署。

Method: 提出LQA框架，结合模态感知量化策略和无梯度测试时适应，包括选择性混合量化（SHQ）和量化无梯度适应机制。

Result: 在合成和真实世界分布偏移实验中，LQA将整体适应性能提升4.5%，内存使用低于全精度模型，比基于梯度的TTA方法内存使用降低高达19.9倍。

Conclusion: LQA为边缘设备上鲁棒、隐私保护且高效的VLM部署提供了实用途径。

Abstract: Deploying Vision-Language Models (VLMs) on edge devices is challenged by resource constraints and performance degradation under distribution shifts. While test-time adaptation (TTA) can counteract such shifts, existing methods are too resource-intensive for on-device deployment. To address this challenge, we propose LQA, a lightweight, quantized-adaptive framework for VLMs that combines a modality-aware quantization strategy with gradient-free test-time adaptation. We introduce Selective Hybrid Quantization (SHQ) and a quantized, gradient-free adaptation mechanism to enable robust and efficient VLM deployment on resource-constrained hardware. Experiments across both synthetic and real-world distribution shifts show that LQA improves overall adaptation performance by 4.5\%, uses less memory than full-precision models, and significantly outperforms gradient-based TTA methods, achieving up to 19.9$\times$ lower memory usage across seven open-source datasets. These results demonstrate that LQA offers a practical pathway for robust, privacy-preserving, and efficient VLM deployment on edge devices.

</details>


### [48] [Emergent Misalignment is Easy, Narrow Misalignment is Hard](https://arxiv.org/abs/2602.07852)
*Anna Soligo,Edward Turner,Senthooran Rajamanoharan,Neel Nanda*

Main category: cs.AI

TL;DR: 微调大语言模型在狭窄有害数据集上会导致涌现性错位，使模型在无关场景下给出刻板"邪恶"回答。研究发现通用错位解比狭窄任务解更稳定高效，并分离出通用错位的线性表征用于监控和缓解。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在微调过程中的归纳偏置如何影响学习和泛化，特别是为什么在狭窄有害数据集上微调会导致模型在广泛无关场景下出现涌现性错位行为。

Method: 使用涌现性错位作为案例研究，比较狭窄任务解和通用错位解。发现不同微调会收敛到相同的通用错位线性表征，同时通过KL散度损失学习狭窄解的线性表征，比较两者的损失、鲁棒性和在预训练分布中的影响力。

Result: 通用错位解比狭窄任务解具有更低的损失、更强的鲁棒性，且在预训练分布中更具影响力。专家调查未能预测这一结果，表明对LLM归纳偏置的理解不足。

Conclusion: 分离出通用错位的具体线性表征可用于监控和缓解模型错位，为研究LLM归纳偏置如何塑造泛化提供了详细案例和初步度量指标。

Abstract: Finetuning large language models on narrowly harmful datasets can cause them to become emergently misaligned, giving stereotypically `evil' responses across diverse unrelated settings. Concerningly, a pre-registered survey of experts failed to predict this result, highlighting our poor understanding of the inductive biases governing learning and generalisation in LLMs. We use emergent misalignment (EM) as a case study to investigate these inductive biases and find that models can just learn the narrow dataset task, but that the general solution appears to be more stable and more efficient. To establish this, we build on the result that different EM finetunes converge to the same linear representation of general misalignment, which can be used to mediate misaligned behaviour. We find a linear representation of the narrow solution also exists, and can be learned by introducing a KL divergence loss. Comparing these representations reveals that general misalignment achieves lower loss, is more robust to perturbations, and is more influential in the pre-training distribution. This work isolates a concrete representation of general misalignment for monitoring and mitigation. More broadly, it offers a detailed case study and preliminary metrics for investigating how inductive biases shape generalisation in LLMs. We open-source all code, datasets and model finetunes.

</details>


### [49] [ToolSelf: Unifying Task Execution and Self-Reconfiguration via Tool-Driven Intrinsic Adaptation](https://arxiv.org/abs/2602.07883)
*Jingqi Zhou,Sheng Wang,DeZhao Deng,Junwen Lu,Junwei Su,Qintong Li,Jiahui Gao,Hao Wu,Jiyue Jiang,Lingpeng Kong,Chuan Wu*

Main category: cs.AI

TL;DR: ToolSelf：一种工具驱动的运行时自重构范式，让LLM智能体能够自主更新配置以适应任务动态，实现从被动执行者到任务与自我双重管理者的转变。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能体系统受限于静态配置，这些配置在执行前固定，无法适应任务动态变化。现有方法依赖人工编排或启发式补丁，泛化能力差且优化碎片化。

Method: 提出ToolSelf范式，将配置更新抽象为可调用工具，统一任务执行和自调整到单一动作空间。设计配置感知两阶段训练（CAT），结合拒绝采样微调和轨迹级强化学习来内化这种元能力。

Result: 在多样化基准测试中，ToolSelf媲美专用工作流的同时能泛化到新任务，平均性能提升24.1%，展示了真正自适应智能体的潜力。

Conclusion: ToolSelf通过工具驱动的运行时自重构，实现了从外部规则到内在参数的范式转变，使智能体能够自主适应任务动态，为真正自适应的智能体系统开辟了新路径。

Abstract: Agentic systems powered by Large Language Models (LLMs) have demonstrated remarkable potential in tackling complex, long-horizon tasks. However, their efficacy is fundamentally constrained by static configurations governing agent behaviors, which are fixed prior to execution and fail to adapt to evolving task dynamics. Existing approaches, relying on manual orchestration or heuristic-based patches, often struggle with poor generalization and fragmented optimization. To transcend these limitations, we propose ToolSelf, a novel paradigm enabling tool-driven runtime self-reconfiguration. By abstracting configuration updates as a callable tool, ToolSelf unifies task execution and self-adjustment into a single action space, achieving a phase transition from external rules to intrinsic parameters. Agents can thereby autonomously update their sub-goals and context based on task progression, and correspondingly adapt their strategy and toolbox, transforming from passive executors into dual managers of both task and self. We further devise Configuration-Aware Two-stage Training (CAT), combining rejection sampling fine-tuning with trajectory-level reinforcement learning to internalize this meta-capability. Extensive experiments across diverse benchmarks demonstrate that ToolSelf rivals specialized workflows while generalizing to novel tasks, achieving a 24.1% average performance gain and illuminating a path toward truly self-adaptive agents.

</details>


### [50] [MemFly: On-the-Fly Memory Optimization via Information Bottleneck](https://arxiv.org/abs/2602.07885)
*Zhenyuan Zhang,Xianzhang Jia,Zhiqin Yang,Zhenbo Song,Wei Xue,Sirui Han,Yike Guo*

Main category: cs.AI

TL;DR: MemFly是一个基于信息瓶颈原则的LLM记忆框架，通过梯度自由优化器构建分层记忆结构，结合混合检索机制，在记忆一致性、响应保真度和准确性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM记忆框架面临一个基本困境：既要高效压缩冗余信息，又要保持精确检索以支持下游任务。需要解决压缩效率与检索精度之间的权衡问题。

Method: 基于信息瓶颈原则，通过梯度自由优化器最小化压缩熵同时最大化相关熵，构建分层记忆结构；开发混合检索机制，整合语义、符号和拓扑路径，并采用迭代精炼处理复杂多跳查询。

Result: 综合实验表明，MemFly在记忆一致性、响应保真度和准确性方面显著优于最先进的基线方法。

Conclusion: MemFly成功解决了LLM记忆框架中压缩效率与检索精度之间的权衡问题，通过信息瓶颈原则和混合检索机制实现了更优的记忆管理性能。

Abstract: Long-term memory enables large language model agents to tackle complex tasks through historical interactions. However, existing frameworks encounter a fundamental dilemma between compressing redundant information efficiently and maintaining precise retrieval for downstream tasks. To bridge this gap, we propose MemFly, a framework grounded in information bottleneck principles that facilitates on-the-fly memory evolution for LLMs. Our approach minimizes compression entropy while maximizing relevance entropy via a gradient-free optimizer, constructing a stratified memory structure for efficient storage. To fully leverage MemFly, we develop a hybrid retrieval mechanism that seamlessly integrates semantic, symbolic, and topological pathways, incorporating iterative refinement to handle complex multi-hop queries. Comprehensive experiments demonstrate that MemFly substantially outperforms state-of-the-art baselines in memory coherence, response fidelity, and accuracy.

</details>


### [51] [GCN-MPPR: Enhancing the Propagation of Message Passing Neural Networks via Motif-Based Personalized PageRank](https://arxiv.org/abs/2602.07903)
*Mingcan Wang,Junchang Xin,Zhongming Yao,Kaifu Long,Zhiqiong Wang*

Main category: cs.AI

TL;DR: 提出基于motif的个性化PageRank（MPPR）来改进GCN，解决过平滑和忽略高阶关系问题，提升准确性、稳定性和计算效率


<details>
  <summary>Details</summary>
Motivation: 现有基于消息传递神经网络（MPNNs）的图算法通常传播深度有限，存在过平滑问题，且忽略高阶关系，导致准确性受限、稳定性差、计算成本高

Method: 提出motif-based personalized PageRank（MPPR）来考虑高阶motif关系衡量节点间影响，并将MPPR应用于GCN的消息传递过程，指导消息在相对"高"层次传播

Result: 实验结果表明，该方法在准确性、稳定性和时间消耗方面优于几乎所有基线方法，且可作为组件支持几乎所有GCN任务

Conclusion: MPPR方法有效解决了GCN的过平滑和忽略高阶关系问题，显著提升了图神经网络性能，具有广泛适用性

Abstract: The algorithms based on message passing neural networks (MPNNs) on graphs have recently achieved great success for various graph applications. However, studies find that these methods always propagate the information to very limited neighborhoods with shallow depth, particularly due to over-smoothing. That means most of the existing MPNNs fail to be so `deep'. Although some previous work tended to handle this challenge via optimization- or structure-level remedies, the overall performance of GCNs still suffers from limited accuracy, poor stability, and unaffordable computational cost. Moreover, neglect of higher-order relationships during the propagation of MPNNs has further limited the performance of them. To overcome these challenges, a novel variant of PageRank named motif-based personalized PageRank (MPPR) is proposed to measure the influence of one node to another on the basis of considering higher-order motif relationships. Secondly, the MPPR is utilized to the message passing process of GCNs, thereby guiding the message passing process at a relatively `high' level. The experimental results show that the proposed method outperforms almost all of the baselines on accuracy, stability, and time consumption. Additionally, the proposed method can be considered as a component that can underpin almost all GCN tasks, with DGCRL being demonstrated in the experiment. The anonymous code repository is available at: https://anonymous.4open.science/r/GCN-MPPR-AFD6/.

</details>


### [52] [MedCoG: Maximizing LLM Inference Density in Medical Reasoning via Meta-Cognitive Regulation](https://arxiv.org/abs/2602.07905)
*Yu Zhao,Hao Guan,Yongcheng Jing,Ying Zhang,Dacheng Tao*

Main category: cs.AI

TL;DR: MedCoG：一个基于知识图谱的医学元认知智能体，通过元认知评估动态调节知识使用，在避免盲目扩展的同时提高推理效率，在医学基准测试中实现5.5倍推理密度提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂医学推理中表现出潜力，但面临推理扩展定律下的收益递减问题。现有研究通过添加各种知识来增强LLMs，但额外成本转化为准确性的效果不明确。本文探索LLMs的元认知（对自身知识状态的自我意识）如何调节推理过程。

Method: 提出MedCoG（Medical Meta-Cognition Agent with Knowledge Graph），通过元认知评估任务复杂性、熟悉度和知识密度，动态调节程序性知识、情景知识和事实知识的使用。采用LLM中心的按需推理方法，避免盲目扩展并过滤分散注意力的知识。

Result: 实验在五个困难的医学基准测试集上验证了MedCoG的有效性和效率，实现了5.5倍的推理密度（推理效率指标）。Oracle研究突显了元认知调节的显著潜力。

Conclusion: MedCoG通过元认知调节知识使用，有效缓解了推理扩展定律问题，在降低成本和提升准确性方面都取得了显著效果，为医学推理任务提供了一种高效且有效的解决方案。

Abstract: Large Language Models (LLMs) have shown strong potential in complex medical reasoning yet face diminishing gains under inference scaling laws. While existing studies augment LLMs with various knowledge types, it remains unclear how effectively the additional costs translate into accuracy. In this paper, we explore how meta-cognition of LLMs, i.e., their self-awareness of their own knowledge states, can regulate the reasoning process. Specifically, we propose MedCoG, a Medical Meta-Cognition Agent with Knowledge Graph, where the meta-cognitive assessments of task complexity, familiarity, and knowledge density dynamically regulate utilization of procedural, episodic, and factual knowledge. The LLM-centric on-demand reasoning aims to mitigate scaling laws by (1) reducing costs via avoiding indiscriminate scaling, (2) improving accuracy via filtering out distractive knowledge. To validate this, we empirically characterize the scaling curve and introduce inference density to quantify inference efficiency, defined as the ratio of theoretically effective cost to actual cost. Experiments demonstrate the effectiveness and efficiency of MedCoG on five hard sets of medical benchmarks, yielding 5.5x inference density. Furthermore, the Oracle study highlights the significant potential of meta-cognitive regulation.

</details>


### [53] [Selective Fine-Tuning for Targeted and Robust Concept Unlearning](https://arxiv.org/abs/2602.07919)
*Mansi,Avinash Kori,Francesca Toni,Soteris Demetriou*

Main category: cs.AI

TL;DR: TRUST是一种新颖的扩散模型概念遗忘方法，通过动态定位目标概念神经元并进行选择性微调，结合Hessian正则化，实现高效、鲁棒的概念遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有文本引导扩散模型容易被利用生成有害内容，传统概念遗忘方法通常针对单个概念，且需要完全微调计算成本高。现有概念定位方法是静态的，导致效果不佳。

Method: 提出TRUST方法：1）动态估计目标概念神经元；2）通过选择性微调进行概念遗忘；3）使用Hessian基正则化增强效果。

Result: 实验表明TRUST对对抗性提示具有鲁棒性，显著保持生成质量，比SOTA方法快得多，能够遗忘单个概念、概念组合和条件概念，无需特定正则化。

Conclusion: TRUST提供了一种高效、鲁棒的概念遗忘解决方案，解决了现有方法计算成本高和效果有限的问题，为扩散模型的安全部署提供了新途径。

Abstract: Text guided diffusion models are used by millions of users, but can be easily exploited to produce harmful content. Concept unlearning methods aim at reducing the models' likelihood of generating harmful content. Traditionally, this has been tackled at an individual concept level, with only a handful of recent works considering more realistic concept combinations. However, state of the art methods depend on full finetuning, which is computationally expensive. Concept localisation methods can facilitate selective finetuning, but existing techniques are static, resulting in suboptimal utility. In order to tackle these challenges, we propose TRUST (Targeted Robust Selective fine Tuning), a novel approach for dynamically estimating target concept neurons and unlearning them through selective finetuning, empowered by a Hessian based regularization. We show experimentally, against a number of SOTA baselines, that TRUST is robust against adversarial prompts, preserves generation quality to a significant degree, and is also significantly faster than the SOTA. Our method achieves unlearning of not only individual concepts but also combinations of concepts and conditional concepts, without any specific regularization.

</details>


### [54] [MePo: Meta Post-Refinement for Rehearsal-Free General Continual Learnin](https://arxiv.org/abs/2602.07940)
*Guanglong Sun,Hongwei Yan,Liyuan Wang,Zhiqi Kang,Shuang Cui,Hang Su,Jun Zhu,Yi Zhong*

Main category: cs.AI

TL;DR: MePo是一种基于预训练模型的通用持续学习方法，通过元后精炼策略提升模型在动态环境中的适应能力，无需回放机制即可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界智能系统需要从复杂、动态的环境中持续学习并实时响应，但现有基于预训练模型的持续学习方法难以处理在线数据流和模糊任务边界，导致性能受限。

Method: 提出Meta Post-Refinement (MePo)方法：1）从预训练数据构建伪任务序列；2）采用双层元学习范式精炼预训练骨干网络；3）初始化元协方差矩阵作为表示空间的参考几何结构，利用二阶统计进行鲁棒输出对齐。

Result: MePo作为即插即用策略，在多种GCL基准测试和预训练检查点上取得显著性能提升，无需回放机制：在CIFAR-100、ImageNet-R和CUB-200上分别提升15.10%、13.36%和12.56%（Sup-21/1K设置）。

Conclusion: MePo通过元后精炼策略有效提升了预训练模型在通用持续学习任务中的适应能力，为解决动态环境中的持续学习问题提供了创新解决方案。

Abstract: To cope with uncertain changes of the external world, intelligent systems must continually learn from complex, evolving environments and respond in real time. This ability, collectively known as general continual learning (GCL), encapsulates practical challenges such as online datastreams and blurry task boundaries. Although leveraging pretrained models (PTMs) has greatly advanced conventional continual learning (CL), these methods remain limited in reconciling the diverse and temporally mixed information along a single pass, resulting in sub-optimal GCL performance. Inspired by meta-plasticity and reconstructive memory in neuroscience, we introduce here an innovative approach named Meta Post-Refinement (MePo) for PTMs-based GCL. This approach constructs pseudo task sequences from pretraining data and develops a bi-level meta-learning paradigm to refine the pretrained backbone, which serves as a prolonged pretraining phase but greatly facilitates rapid adaptation of representation learning to downstream GCL tasks. MePo further initializes a meta covariance matrix as the reference geometry of pretrained representation space, enabling GCL to exploit second-order statistics for robust output alignment. MePo serves as a plug-in strategy that achieves significant performance gains across a variety of GCL benchmarks and pretrained checkpoints in a rehearsal-free manner (e.g., 15.10\%, 13.36\%, and 12.56\% on CIFAR-100, ImageNet-R, and CUB-200 under Sup-21/1K). Our source code is available at \href{https://github.com/SunGL001/MePo}{MePo}

</details>


### [55] [IV Co-Scientist: Multi-Agent LLM Framework for Causal Instrumental Variable Discovery](https://arxiv.org/abs/2602.07943)
*Ivaxi Sheth,Zhijing Jin,Bryan Wilder,Dominik Janzing,Mario Fritz*

Main category: cs.AI

TL;DR: LLMs能够帮助发现有效的工具变量，通过多智能体系统IV Co-Scientist提出、批判和优化工具变量选择


<details>
  <summary>Details</summary>
Motivation: 工具变量识别需要跨学科知识、创造力和上下文理解，是一项非平凡任务。本文研究LLMs是否能辅助这一任务，特别是在从大型观测数据库中识别有效工具变量方面。

Method: 采用两阶段评估框架：1)测试LLMs能否从文献中恢复已确立的工具变量；2)评估LLMs能否识别和避免已被实证或理论否定的工具变量。在此基础上提出IV Co-Scientist多智能体系统，以及在没有真实标签情况下评估一致性的统计检验。

Result: LLMs能够从文献中恢复已确立的工具变量，并能识别和避免无效工具变量。IV Co-Scientist系统展示了从大型观测数据库中发现有效工具变量的潜力。

Conclusion: LLMs在工具变量发现方面具有潜力，特别是在辅助研究人员从大型观测数据库中识别有效工具变量方面。提出的IV Co-Scientist系统和统计检验为这一领域提供了新的方法论工具。

Abstract: In the presence of confounding between an endogenous variable and the outcome, instrumental variables (IVs) are used to isolate the causal effect of the endogenous variable. Identifying valid instruments requires interdisciplinary knowledge, creativity, and contextual understanding, making it a non-trivial task. In this paper, we investigate whether large language models (LLMs) can aid in this task. We perform a two-stage evaluation framework. First, we test whether LLMs can recover well-established instruments from the literature, assessing their ability to replicate standard reasoning. Second, we evaluate whether LLMs can identify and avoid instruments that have been empirically or theoretically discredited. Building on these results, we introduce IV Co-Scientist, a multi-agent system that proposes, critiques, and refines IVs for a given treatment-outcome pair. We also introduce a statistical test to contextualize consistency in the absence of ground truth. Our results show the potential of LLMs to discover valid instrumental variables from a large observational database.

</details>


### [56] [LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth](https://arxiv.org/abs/2602.07962)
*Weihao Zeng,Yuzhen Huang,Junxian He*

Main category: cs.AI

TL;DR: LOCA-bench是一个用于评估长上下文语言代理的基准测试，通过自动化环境状态控制来调节上下文长度，支持无限扩展上下文并保持任务语义不变。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文基准主要关注单步信息检索，而现实场景中LLM需要作为代理在动态增长的环境中探索、执行指令、提取信息和预测行动，需要更全面的评估框架。

Method: LOCA-bench利用自动化和可扩展的环境状态控制来调节代理的上下文长度，支持无限扩展上下文同时保持任务语义固定，评估语言代理作为模型和脚手架的组合，包括各种上下文管理策略。

Result: 随着环境状态复杂度增加，代理性能普遍下降，但先进的上下文管理技术能显著提高整体成功率。

Conclusion: LOCA-bench为评估长上下文、代理式场景中的模型和脚手架提供了一个平台，有助于解决上下文退化问题并提升语言代理在实际任务中的可靠性。

Abstract: Large language models (LLMs) are increasingly capable of carrying out long-running, real-world tasks. However, as the amount of context grows, their reliability often deteriorates, a phenomenon known as "context rot". Existing long-context benchmarks primarily focus on single-step settings that evaluate a model's ability to retrieve information from a long snippet. In realistic scenarios, however, LLMs often need to act as agents that explore environments, follow instructions and plans, extract useful information, and predict correct actions under a dynamically growing context. To assess language agents in such settings, we introduce LOCA-bench (a benchmark for LOng-Context Agents). Given a task prompt, LOCA-bench leverages automated and scalable control of environment states to regulate the agent's context length. This design enables LOCA-bench to extend the context length potentially to infinity in a controlled way while keeping the underlying task semantics fixed. LOCA-bench evaluates language agents as a combination of models and scaffolds, including various context management strategies. While agent performance generally degrades as the environment states grow more complex, advanced context management techniques can substantially improve the overall success rate. We open-source LOCA-bench to provide a platform for evaluating models and scaffolds in long-context, agentic scenarios: https://github.com/hkust-nlp/LOCA-bench

</details>


### [57] [Accelerating Social Science Research via Agentic Hypothesization and Experimentation](https://arxiv.org/abs/2602.07983)
*Jishu Sen Gupta,Harini SI,Somesh Kumar Singh,Syed Mohamad Tawseeq,Yaman Kumar Singla,David Doermann,Rajiv Ratn Shah,Balaji Krishnamurthy*

Main category: cs.AI

TL;DR: EXPERIGEN是一个端到端科学发现框架，通过生成器-实验者的两阶段搜索，在多个领域发现比现有方法多2-4倍的统计显著假设，预测性能提升7-17%，并通过专家评审和真实A/B测试验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前数据驱动社会科学研究过程缓慢，依赖观察、假设生成和实验验证的迭代循环。现有数据驱动方法虽然能加速部分过程，但未能支持端到端的科学发现。需要一种能够完整支持从假设生成到实验验证的框架。

Method: 提出EXPERIGEN框架，采用受贝叶斯优化启发的两阶段搜索：生成器提出候选假设，实验者进行实证评估。框架支持多模态和关系数据集，并通过专家评审和真实A/B测试进行验证。

Result: 在多个领域，EXPERIGEN发现比现有方法多2-4倍的统计显著假设，预测性能提升7-17%。专家评审显示88%的假设具有中等或强烈新颖性，70%被认为有影响力且值得研究。A/B测试获得p<1e-6的统计显著结果和344%的大效应量。

Conclusion: EXPERIGEN实现了端到端的科学发现，不仅统计性能优越，而且生成的假设具有新颖性、实证基础和可操作性，能够推动真正的科学进步，并通过专家评审和真实实验验证了其实际价值。

Abstract: Data-driven social science research is inherently slow, relying on iterative cycles of observation, hypothesis generation, and experimental validation. While recent data-driven methods promise to accelerate parts of this process, they largely fail to support end-to-end scientific discovery. To address this gap, we introduce EXPERIGEN, an agentic framework that operationalizes end-to-end discovery through a Bayesian optimization inspired two-phase search, in which a Generator proposes candidate hypotheses and an Experimenter evaluates them empirically. Across multiple domains, EXPERIGEN consistently discovers 2-4x more statistically significant hypotheses that are 7-17 percent more predictive than prior approaches, and naturally extends to complex data regimes including multimodal and relational datasets. Beyond statistical performance, hypotheses must be novel, empirically grounded, and actionable to drive real scientific progress. To evaluate these qualities, we conduct an expert review of machine-generated hypotheses, collecting feedback from senior faculty. Among 25 reviewed hypotheses, 88 percent were rated moderately or strongly novel, 70 percent were deemed impactful and worth pursuing, and most demonstrated rigor comparable to senior graduate-level research. Finally, recognizing that ultimate validation requires real-world evidence, we conduct the first A/B test of LLM-generated hypotheses, observing statistically significant results with p less than 1e-6 and a large effect size of 344 percent.

</details>


### [58] [Towards Adaptive, Scalable, and Robust Coordination of LLM Agents: A Dynamic Ad-Hoc Networking Perspective](https://arxiv.org/abs/2602.08009)
*Rui Li,Zeyu Zhang,Xiaohe Bo,Quanyu Dai,Chaozhuo Li,Feng Wen,Xu Chen*

Main category: cs.AI

TL;DR: RAPS：基于声誉感知的发布-订阅范式，用于实现LLM多智能体的自适应、可扩展且鲁棒的协调


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的多智能体架构需要大量人工编排，亟需自动化设计智能体工作流。智能体协调面临动态自组织网络中的经典问题：如何在可扩展数量的智能体主机之间建立自适应且可靠的通信？

Method: RAPS基于分布式发布-订阅协议，让LLM智能体基于声明的意图而非预定义拓扑交换消息。包含两个核心覆盖层：(1) 反应式订阅：使智能体动态细化意图；(2) 贝叶斯声誉：为每个智能体提供本地监控器，检测并隔离恶意节点。

Result: 在五个基准测试上的广泛实验表明，该设计在统一的多智能体协调框架中有效调和了自适应性、可扩展性和鲁棒性。

Conclusion: RAPS通过声誉感知的发布-订阅范式，为解决LLM多智能体协调中的自适应、可扩展和鲁棒性问题提供了有效解决方案。

Abstract: Multi-agent architectures built on large language models (LLMs) have demonstrated the potential to realize swarm intelligence through well-crafted collaboration. However, the substantial burden of manual orchestration inherently raises an imperative to automate the design of agentic workflows. We frame such an agent coordination challenge as a classic problem in dynamic ad-hoc networking: How to establish adaptive and reliable communication among a scalable number of agentic hosts? In response to this unresolved dilemma, we introduce RAPS, a reputation-aware publish-subscribe paradigm for adaptive, scalable, and robust coordination of LLM agents. RAPS is grounded in the Distributed Publish-Subscribe Protocol, allowing LLM agents to exchange messages based on their declared intents rather than predefined topologies. Beyond this substrate, RAPS further incorporates two coherent overlays: (i) Reactive Subscription, enabling agents to dynamically refine their intents; and (ii) Bayesian Reputation, empowering each agent with a local watchdog to detect and isolate malicious peers. Extensive experiments over five benchmarks showcase that our design effectively reconciles adaptivity, scalability, and robustness in a unified multi-agent coordination framework.

</details>


### [59] [Small Agent Group is the Future of Digital Health](https://arxiv.org/abs/2602.08013)
*Yuqiao Meng,Luoxi Tang,Dazheng Zhang,Rafael Brens,Elvys J. Romero,Nancy Guo,Safa Elkefi,Zhaohan Xi*

Main category: cs.AI

TL;DR: 小代理群组（SAG）通过协作推理在临床环境中替代模型参数增长，实现效果、可靠性和部署效率的更好平衡


<details>
  <summary>Details</summary>
Motivation: 当前数字健康领域过度依赖"规模优先"的大语言模型范式，但临床实际需求不仅需要效果，还需要可靠性和合理的部署成本。临床决策本质上是协作过程，因此需要挑战单一模型扩展的范式

Method: 提出小代理群组（SAG）方法，将单一模型智能转变为集体专业知识，通过协作审议过程分配推理、循证分析和关键审核任务

Result: SAG在效果、可靠性和部署成本等多个临床指标上表现优于单一大型模型，无论是否使用额外优化或检索增强生成技术

Conclusion: SAG提供的协同推理可以替代临床环境中的模型参数增长，为数字健康提供了一种可扩展的解决方案，更好地平衡了效果、可靠性和部署效率

Abstract: The rapid adoption of large language models (LLMs) in digital health has been driven by a "scaling-first" philosophy, i.e., the assumption that clinical intelligence increases with model size and data. However, real-world clinical needs include not only effectiveness, but also reliability and reasonable deployment cost. Since clinical decision-making is inherently collaborative, we challenge the monolithic scaling paradigm and ask whether a Small Agent Group (SAG) can support better clinical reasoning. SAG shifts from single-model intelligence to collective expertise by distributing reasoning, evidence-based analysis, and critical audit through a collaborative deliberation process. To assess the clinical utility of SAG, we conduct extensive evaluations using diverse clinical metrics spanning effectiveness, reliability, and deployment cost. Our results show that SAG achieves superior performance compared to a single giant model, both with and without additional optimization or retrieval-augmented generation. These findings suggest that the synergistic reasoning represented by SAG can substitute for model parameter growth in clinical settings. Overall, SAG offers a scalable solution to digital health that better balances effectiveness, reliability, and deployment efficiency.

</details>


### [60] [Structure-Aware Robust Counterfactual Explanations via Conditional Gaussian Network Classifiers](https://arxiv.org/abs/2602.08021)
*Zhan-Yi Liao,Jaewon Yoo,Hao-Tsung Yang,Po-An Chen*

Main category: cs.AI

TL;DR: 提出基于条件高斯网络分类器的结构感知、鲁棒性导向的反事实解释搜索方法，通过混合整数线性规划确保全局最优性


<details>
  <summary>Details</summary>
Motivation: 现有反事实解释方法缺乏对特征间条件依赖关系和潜在因果关系的考虑，且鲁棒性不足，需要一种能自然嵌入模型结构假设并保证全局鲁棒性的方法

Method: 使用条件高斯网络分类器编码特征依赖关系，采用收敛保证的切割集过程作为对抗优化框架，通过分段McCormick松弛将非凸二次问题转化为混合整数线性规划

Result: 实验结果显示该方法实现了强鲁棒性，直接全局优化原始公式提供了特别稳定和高效的结果，框架可扩展到更复杂的约束设置

Conclusion: 提出的框架为非线性二次公式下的反事实推理奠定了基础，具有结构感知和鲁棒性导向的特点，可扩展性强

Abstract: Counterfactual explanation (CE) is a core technique in explainable artificial intelligence (XAI), widely used to interpret model decisions and suggest actionable alternatives. This work presents a structure-aware and robustness-oriented counterfactual search method based on the conditional Gaussian network classifier (CGNC). The CGNC has a generative structure that encodes conditional dependencies and potential causal relations among features through a directed acyclic graph (DAG). This structure naturally embeds feature relationships into the search process, eliminating the need for additional constraints to ensure consistency with the model's structural assumptions. We adopt a convergence-guaranteed cutting-set procedure as an adversarial optimization framework, which iteratively approximates solutions that satisfy global robustness conditions. To address the nonconvex quadratic structure induced by feature dependencies, we apply piecewise McCormick relaxation to reformulate the problem as a mixed-integer linear program (MILP), ensuring global optimality. Experimental results show that our method achieves strong robustness, with direct global optimization of the original formulation providing especially stable and efficient results. The proposed framework is extensible to more complex constraint settings, laying the groundwork for future advances in counterfactual reasoning under nonconvex quadratic formulations.

</details>


### [61] [Free(): Learning to Forget in Malloc-Only Reasoning Models](https://arxiv.org/abs/2602.08030)
*Yilun Zheng,Dongyang Ma,Tian Liang,Jiahao Xu,Xinting Huang,Lijie Chen,Haitao Mi,Yan Wang*

Main category: cs.AI

TL;DR: Free()LM通过引入自遗忘机制解决推理模型中过度思考导致性能下降的问题，使用可插拔的LoRA适配器动态修剪无用上下文，在各种规模模型上实现性能提升


<details>
  <summary>Details</summary>
Motivation: 标准LLM存在"只分配不释放"的架构缺陷，在推理过程中持续积累有效和冗余步骤，缺乏修剪过时信息的机制，导致过度思考时性能反而下降

Method: 提出Free()LM模型，通过Free-Module（可插拔LoRA适配器）实现内在自遗忘能力。模型在推理模式和清理模式之间迭代切换，动态识别并修剪无用上下文块，保持紧凑无噪声状态

Result: 在所有模型规模（8B到685B）上实现一致改进，平均比顶级推理基线提升3.3%，在IMOanswerBench上建立新SOTA。在长时域任务中，将Qwen3-235B-A22B从0%准确率恢复到50%

Conclusion: 可持续智能不仅需要思考能力，还需要遗忘的自由。自遗忘机制是解决推理模型过度思考问题的关键，Free()LM为LLM架构设计提供了新方向

Abstract: Reasoning models enhance problem-solving by scaling test-time compute, yet they face a critical paradox: excessive thinking tokens often degrade performance rather than improve it. We attribute this to a fundamental architectural flaw: standard LLMs operate as "malloc-only" engines, continuously accumulating valid and redundant steps alike without a mechanism to prune obsolete information. To break this cycle, we propose Free()LM, a model that introduces an intrinsic self-forgetting capability via the Free-Module, a plug-and-play LoRA adapter. By iteratively switching between reasoning and cleaning modes, Free()LM dynamically identifies and prunes useless context chunks, maintaining a compact and noise-free state.
  Extensive experiments show that Free()LM provides consistent improvements across all model scales (8B to 685B). It achieves a 3.3% average improvement over top-tier reasoning baselines, even establishing a new SOTA on IMOanswerBench using DeepSeek V3.2-Speciale. Most notably, in long-horizon tasks where the standard Qwen3-235B-A22B model suffers a total collapse (0% accuracy), Free()LM restores performance to 50%. Our findings suggest that sustainable intelligence requires the freedom to forget as much as the power to think.

</details>


### [62] [Securing Dual-Use Pathogen Data of Concern](https://arxiv.org/abs/2602.08061)
*Doni Bloomfield,Allison Berke,Moritz S. Hanke,Aaron Maiwald,James R. M. Black,Toby Webster,Tina Hernandez-Boussard,Oliver M. Crook,Jassi Pannu*

Main category: cs.AI

TL;DR: 提出五级生物安全数据框架（BDL），根据病原体数据训练AI模型可能带来的生物安全风险进行分类，并针对不同风险等级提出技术限制措施和治理框架。


<details>
  <summary>Details</summary>
Motivation: AI模型训练数据与其能力密切相关，包括生物安全风险能力。为防止AI被用于生物武器开发等有害应用，需要设计数据控制措施。在计算和编码资源广泛可及的世界中，数据控制可能是减少生物AI能力扩散的最有效干预手段。

Method: 引入五级生物安全数据框架（BDL），根据病原体数据训练AI模型可能带来的风险进行分类。为每个BDL层级提出相应的技术限制措施，并设计针对新创建的双重用途病原体数据的治理框架。

Result: 建立了系统化的病原体数据风险分类框架，为不同风险等级的数据设计了具体的技术控制措施，并提出了创新的治理框架来管理双重用途病原体数据。

Conclusion: 在计算资源广泛可及的时代，数据控制是减少生物AI能力扩散的关键干预手段。提出的BDL框架和治理方案有助于实施有效的生物安全数据管控，防止AI被用于有害的生物应用。

Abstract: Training data is an essential input into creating competent artificial intelligence (AI) models. AI models for biology are trained on large volumes of data, including data related to biological sequences, structures, images, and functions. The type of data used to train a model is intimately tied to the capabilities it ultimately possesses--including those of biosecurity concern. For this reason, an international group of more than 100 researchers at the recent 50th anniversary Asilomar Conference endorsed data controls to prevent the use of AI for harmful applications such as bioweapons development. To help design such controls, we introduce a five-tier Biosecurity Data Level (BDL) framework for categorizing pathogen data. Each level contains specific data types, based on their expected ability to contribute to capabilities of concern when used to train AI models. For each BDL tier, we propose technical restrictions appropriate to its level of risk. Finally, we outline a novel governance framework for newly created dual-use pathogen data. In a world with widely accessible computational and coding resources, data controls may be among the most high-leverage interventions available to reduce the proliferation of concerning biological AI capabilities.

</details>


### [63] [Interpretable Failure Analysis in Multi-Agent Reinforcement Learning Systems](https://arxiv.org/abs/2602.08104)
*Risal Shahriar Shefin,Debashis Gupta,Thai Le,Sarra Alqahtani*

Main category: cs.AI

TL;DR: 提出一个两阶段梯度框架，用于多智能体强化学习中的可解释故障检测与溯源，能识别初始故障源、验证多米诺效应、追踪故障传播路径。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习在安全关键领域应用增多，但可解释的故障检测与归因方法仍不成熟。现有方法多为黑盒检测，缺乏对故障传播机制的可解释性分析。

Method: 两阶段梯度框架：第一阶段通过策略梯度成本的泰勒余项分析进行可解释的智能体级故障检测，确定初始故障候选；第二阶段通过评论家导数的几何分析（一阶敏感性和二阶曲率）构建可解释的传染图，验证多米诺效应和追踪传播路径。

Result: 在Simple Spread（3和5智能体）和StarCraft II环境中评估，使用MADDPG和HATRPO算法，在500/100个episode中达到88.2-99.4%的初始故障源检测准确率，并提供检测决策的几何证据。

Conclusion: 该框架超越了黑盒检测，提供了梯度层面的可解释法医分析，为安全关键多智能体系统中的级联故障诊断提供了实用工具。

Abstract: Multi-Agent Reinforcement Learning (MARL) is increasingly deployed in safety-critical domains, yet methods for interpretable failure detection and attribution remain underdeveloped. We introduce a two-stage gradient-based framework that provides interpretable diagnostics for three critical failure analysis tasks: (1) detecting the true initial failure source (Patient-0); (2) validating why non-attacked agents may be flagged first due to domino effects; and (3) tracing how failures propagate through learned coordination pathways. Stage 1 performs interpretable per-agent failure detection via Taylor-remainder analysis of policy-gradient costs, declaring an initial Patient-0 candidate at the first threshold crossing. Stage 2 provides validation through geometric analysis of critic derivatives-first-order sensitivity and directional second-order curvature aggregated over causal windows to construct interpretable contagion graphs. This approach explains "downstream-first" detection anomalies by revealing pathways that amplify upstream deviations. Evaluated across 500 episodes in Simple Spread (3 and 5 agents) and 100 episodes in StarCraft II using MADDPG and HATRPO, our method achieves 88.2-99.4% Patient-0 detection accuracy while providing interpretable geometric evidence for detection decisions. By moving beyond black-box detection to interpretable gradient-level forensics, this framework offers practical tools for diagnosing cascading failures in safety-critical MARL systems.

</details>


### [64] [Initial Risk Probing and Feasibility Testing of Glow: a Generative AI-Powered Dialectical Behavior Therapy Skills Coach for Substance Use Recovery and HIV Prevention](https://arxiv.org/abs/2602.08121)
*Liying Wang,Madison Lee,Yunzhang Jiang,Steven Chen,Kewei Sha,Yunhe Feng,Frank Wong,Lisa Hightow-Weidman,Weichao Yuwen*

Main category: cs.AI

TL;DR: 研究开发了Glow——一个基于生成式AI的DBT技能教练，用于HIV和物质使用风险人群，并通过用户驱动的对抗性测试评估其安全性，发现存在安全漏洞需要解决。


<details>
  <summary>Details</summary>
Motivation: HIV和物质使用是相互影响的流行病，有共同的心理学驱动因素（冲动性和适应不良应对）。DBT针对这些机制但面临可扩展性挑战，而生成式AI有潜力提供规模化个性化DBT指导，但其快速发展超过了安全基础设施的建设速度。

Method: 开发了Glow——一个生成式AI驱动的DBT技能教练，提供链分析和解决方案分析。与洛杉矶社区卫生组织合作，对临床工作人员（n=6）和有生活经验的个体（n=28）进行可用性测试。使用HHH框架，采用用户驱动的对抗性测试，参与者识别目标行为并生成情境现实的风险探测。评估了37个风险探测交互的安全性表现。

Result: Glow适当处理了73%的风险探测，但不同代理表现差异显著：解决方案分析代理达到90%适当处理率，而链分析代理只有44%。安全失败主要集中在鼓励物质使用和正常化有害行为。链分析代理陷入"共情陷阱"，提供强化适应不良信念的验证。此外还识别出27个DBT技能错误信息实例。

Conclusion: 这是首次对生成式AI提供的DBT指导进行系统性安全评估。研究发现需要在临床试验前缓解的漏洞。HHH框架和用户驱动的对抗性测试为评估生成式AI心理健康干预提供了可复制的方法。

Abstract: Background: HIV and substance use represent interacting epidemics with shared psychological drivers - impulsivity and maladaptive coping. Dialectical behavior therapy (DBT) targets these mechanisms but faces scalability challenges. Generative artificial intelligence (GenAI) offers potential for delivering personalized DBT coaching at scale, yet rapid development has outpaced safety infrastructure. Methods: We developed Glow, a GenAI-powered DBT skills coach delivering chain and solution analysis for individuals at risk for HIV and substance use. In partnership with a Los Angeles community health organization, we conducted usability testing with clinical staff (n=6) and individuals with lived experience (n=28). Using the Helpful, Honest, and Harmless (HHH) framework, we employed user-driven adversarial testing wherein participants identified target behaviors and generated contextually realistic risk probes. We evaluated safety performance across 37 risk probe interactions. Results: Glow appropriately handled 73% of risk probes, but performance varied by agent. The solution analysis agent demonstrated 90% appropriate handling versus 44% for the chain analysis agent. Safety failures clustered around encouraging substance use and normalizing harmful behaviors. The chain analysis agent fell into an "empathy trap," providing validation that reinforced maladaptive beliefs. Additionally, 27 instances of DBT skill misinformation were identified. Conclusions: This study provides the first systematic safety evaluation of GenAI-delivered DBT coaching for HIV and substance use risk reduction. Findings reveal vulnerabilities requiring mitigation before clinical trials. The HHH framework and user-driven adversarial testing offer replicable methods for evaluating GenAI mental health interventions.

</details>


### [65] [RECUR: Resource Exhaustion Attack via Recursive-Entropy Guided Counterfactual Utilization and Reflection](https://arxiv.org/abs/2602.08214)
*Ziwei Wang,Yuanhe Zhang,Jing Chen,Zhenhong Zhou,Ruichao Liang,Ruiying Du,Ju Jia,Cong Wu,Yang Liu*

Main category: cs.AI

TL;DR: 本文提出RECUR攻击，通过递归熵引导的反事实利用和反思，针对大型推理模型进行资源耗尽攻击，揭示推理过程中的安全隐患。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型(LRMs)需要扩展上下文长度进行显式推理，导致资源消耗显著增加。现有研究显示对抗性输入可触发冗余推理过程，但推理过程本身，特别是其反思组件，受到的关注有限，可能导致过度反思和计算资源过度消耗。

Method: 提出递归熵来量化反思中的资源消耗风险，并基于此开发RECUR攻击方法。该方法通过递归熵引导构建反事实问题，利用LRMs的内在缺陷和风险进行资源耗尽攻击。

Result: 实验表明，在良性推理中递归熵呈现明显下降趋势，而RECUR攻击破坏了这一趋势，使输出长度增加高达11倍，吞吐量下降90%。

Conclusion: 该工作为鲁棒推理提供了新视角，揭示了推理过程本身的安全隐患，特别是反思组件可能导致的资源耗尽漏洞。

Abstract: Large Reasoning Models (LRMs) employ reasoning to address complex tasks. Such explicit reasoning requires extended context lengths, resulting in substantially higher resource consumption. Prior work has shown that adversarially crafted inputs can trigger redundant reasoning processes, exposing LRMs to resource-exhaustion vulnerabilities. However, the reasoning process itself, especially its reflective component, has received limited attention, even though it can lead to over-reflection and consume excessive computing power. In this paper, we introduce Recursive Entropy to quantify the risk of resource consumption in reflection, thereby revealing the safety issues inherent in inference itself. Based on Recursive Entropy, we introduce RECUR, a resource exhaustion attack via Recursive Entropy guided Counterfactual Utilization and Reflection. It constructs counterfactual questions to verify the inherent flaws and risks of LRMs. Extensive experiments demonstrate that, under benign inference, recursive entropy exhibits a pronounced decreasing trend. RECUR disrupts this trend, increasing the output length by up to 11x and decreasing throughput by 90%. Our work provides a new perspective on robust reasoning.

</details>


### [66] [Weak-Driven Learning: How Weak Agents make Strong Agents Stronger](https://arxiv.org/abs/2602.08222)
*Zehao Chen,Gongxun Li,Tianxiang Ai,Yifei Li,Zixuan Huang,Wang Zhou,Fuzhen Zhuang,Xianglong Liu,Jianxin Li,Deqing Wang,Yikun Ban*

Main category: cs.AI

TL;DR: WMSS利用模型历史弱检查点指导优化，通过熵动态识别可恢复学习差距并进行补偿学习，突破后训练饱和瓶颈


<details>
  <summary>Details</summary>
Motivation: 后训练优化中观察到持续饱和瓶颈：模型变得高度自信后，进一步训练收益递减。现有方法继续强化目标预测，但信息监督信号仍潜藏在模型自身历史弱状态中

Method: 提出WMSS后训练范式，利用弱检查点指导持续优化。通过熵动态识别可恢复学习差距，并通过补偿学习强化这些差距

Result: 在数学推理和代码生成数据集上的实验表明，使用WMSS训练的智能体实现了有效的性能提升，且不增加额外推理成本

Conclusion: WMSS能够使强智能体超越传统后训练饱和限制，通过利用历史弱状态中的监督信号实现持续改进

Abstract: As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.

</details>


### [67] [InfiCoEvalChain: A Blockchain-Based Decentralized Framework for Collaborative LLM Evaluation](https://arxiv.org/abs/2602.08229)
*Yifan Yang,Jinjia Li,Kunxi Li,Puhao Zheng,Yuanyi Wang,Zheyan Qu,Yang Yu,Jianmin Wu,Ming Li,Hongxia Yang*

Main category: cs.AI

TL;DR: 提出去中心化评估框架解决LLM评估中的不稳定性问题，通过区块链协议激励全球贡献者参与验证，显著降低评估方差


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型评估存在中心化评估的透明度不足、过拟合和硬件差异导致的方差问题。实证分析发现HumanEval评估中单模型十次运行的标准差(1.67)甚至超过了官方排行榜前10模型间的性能差距(0.91)，使得当前排名统计上不可靠

Method: 提出去中心化评估框架，通过区块链协议激励全球贡献者作为独立验证者，在异构计算节点上进行大规模基准测试，实现硬件和参数多样性。采用稳健的奖励系统确保评估完整性并阻止不诚实参与

Result: 去中心化评估框架将同一模型十次运行的标准差从1.67降低到0.28，显著提高了模型排名的统计置信度

Conclusion: 去中心化评估将评估从"中心化黑箱"转变为"去中心化背书"，通过多方共识和多样化推理环境产生更稳定、更具代表性的指标，平台已完全实现并将向社区发布

Abstract: The rapid advancement of large language models (LLMs) demands increasingly reliable evaluation, yet current centralized evaluation suffers from opacity, overfitting, and hardware-induced variance. Our empirical analysis reveals an alarming inconsistency in existing evaluations: the standard deviation across ten repeated runs of a single model on HumanEval (1.67) actually exceeds the performance gap among the top-10 models on the official leaderboard (0.91), rendering current rankings statistically precarious. To mitigate these instabilities, we propose a decentralized evaluation framework that enables hardware and parameter diversity through large-scale benchmarking across heterogeneous compute nodes. By leveraging the blockchain-based protocol, the framework incentivizes global contributors to act as independent validators, using a robust reward system to ensure evaluation integrity and discourage dishonest participation. This collective verification transforms evaluation from a "centralized black box" into a "decentralized endorsement" where multi-party consensus and diverse inference environments yield a more stable, representative metric. Experimental results demonstrate that the decentralized evaluation framework reduces the standard deviation across ten runs on the same model to 0.28. This significant improvement over conventional frameworks ensures higher statistical confidence in model rankings. We have completely implemented this platform and will soon release it to the community.

</details>


### [68] [PTS-SNN: A Prompt-Tuned Temporal Shift Spiking Neural Networks for Efficient Speech Emotion Recognition](https://arxiv.org/abs/2602.08240)
*Xun Su,Huamin Wang,Qi Zhang*

Main category: cs.AI

TL;DR: 提出PTS-SNN框架，通过提示调优解决SSL表示与SNN之间的分布不匹配问题，实现高效能低能耗的语音情感识别


<details>
  <summary>Details</summary>
Motivation: 传统语音情感识别模型计算成本高，难以部署在资源受限的边缘设备上。SNN虽然能效高，但与连续自监督学习表示存在分布不匹配问题，高动态范围的嵌入会降低基于阈值神经元的信息编码能力。

Method: 提出PTS-SNN框架：1）使用时移脉冲编码器通过无参数通道移位捕获局部时间依赖性；2）设计上下文感知膜电位校准策略，利用脉冲稀疏线性注意力模块聚合全局语义上下文到可学习的软提示中，动态调节PLIF神经元的偏置电压，将异质输入分布居中到响应发射范围内。

Result: 在五个多语言数据集上的实验表明，PTS-SNN在IEMOCAP上达到73.34%的准确率，与竞争性ANN相当，同时仅需119万个可训练参数和每样本0.35毫焦的推理能耗。

Conclusion: PTS-SNN通过参数高效的神经形态适应框架，成功解决了SSL表示与SNN之间的分布不匹配问题，为边缘设备上的高效能低能耗语音情感识别提供了可行方案。

Abstract: Speech Emotion Recognition (SER) is widely deployed in Human-Computer Interaction, yet the high computational cost of conventional models hinders their implementation on resource-constrained edge devices. Spiking Neural Networks (SNNs) offer an energy-efficient alternative due to their event-driven nature; however, their integration with continuous Self-Supervised Learning (SSL) representations is fundamentally challenged by distribution mismatch, where high-dynamic-range embeddings degrade the information coding capacity of threshold-based neurons. To resolve this, we propose Prompt-Tuned Spiking Neural Networks (PTS-SNN), a parameter-efficient neuromorphic adaptation framework that aligns frozen SSL backbones with spiking dynamics. Specifically, we introduce a Temporal Shift Spiking Encoder to capture local temporal dependencies via parameter-free channel shifts, establishing a stable feature basis. To bridge the domain gap, we devise a Context-Aware Membrane Potential Calibration strategy. This mechanism leverages a Spiking Sparse Linear Attention module to aggregate global semantic context into learnable soft prompts, which dynamically regulate the bias voltages of Parametric Leaky Integrate-and-Fire (PLIF) neurons. This regulation effectively centers the heterogeneous input distribution within the responsive firing range, mitigating functional silence or saturation. Extensive experiments on five multilingual datasets (e.g., IEMOCAP, CASIA, EMODB) demonstrate that PTS-SNN achieves 73.34\% accuracy on IEMOCAP, comparable to competitive Artificial Neural Networks (ANNs), while requiring only 1.19M trainable parameters and 0.35 mJ inference energy per sample.

</details>


### [69] [Do MLLMs Really See It: Reinforcing Visual Attention in Multimodal LLMs](https://arxiv.org/abs/2602.08241)
*Siqu Ou,Tianrui Wan,Zhiyuan Zhao,Junyu Gao,Xuelong Li*

Main category: cs.AI

TL;DR: SAYO是一个通过强化学习框架训练的多模态视觉推理模型，引入区域级视觉注意力奖励来改善视觉注意力策略，解决现有MLLMs视觉注意力不稳定的问题


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在复杂推理任务中虽然使用了思维链推理，但存在视觉注意力薄弱的问题：早期视觉对齐错误很少在后续推理中得到纠正，导致错误传播和推理失败。这种限制源于训练过程中视觉注意力信用分配不足。

Method: 提出SAYO模型，采用强化学习框架训练，引入区域级视觉注意力奖励机制。该奖励明确将优化信号与基于视觉的推理步骤对齐，使模型能够学习更可靠的注意力行为。

Result: 在多个多模态基准测试上的广泛实验表明，SAYO在多样化的推理和感知任务上持续提升性能。

Conclusion: 通过强化学习框架和区域级视觉注意力奖励，SAYO能够学习更稳定的视觉注意力策略，有效改善多模态大语言模型的视觉推理能力。

Abstract: While chain-of-thought (CoT) reasoning has substantially improved multimodal large language models (MLLMs) on complex reasoning tasks, existing approaches largely rely on long textual reasoning trajectories and provide limited mechanisms for learning stable visual attention policies. Our analysis shows that current MLLMs exhibit weak visual focus: early-stage visual misalignment is rarely corrected during subsequent reasoning, leading to error propagation and failed inferences. We argue that this limitation stems from inadequate credit assignment for visual attention during training. To address this issue, we propose SAYO, a visual reasoning model trained with a reinforcement learning (RL) framework that introduces a region-level visual attention-based reward. This reward explicitly aligns optimization signals with visually grounded reasoning steps, enabling the model to learn more reliable attention behaviors. Extensive experiments across multiple multimodal benchmarks demonstrate that SAYO consistently improves performance on diverse reasoning and perception tasks.

</details>


### [70] [G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design](https://arxiv.org/abs/2602.08253)
*Baoyun Zhao,He Wang,Liang Zeng*

Main category: cs.AI

TL;DR: G-LNS：基于LLM的生成式进化框架，用于自动设计大邻域搜索（LNS）的破坏与修复算子对，在组合优化问题上超越现有LLM方法和经典求解器。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自动启发式设计方法通常局限于构造性优先级规则或参数化局部搜索引导，限制了搜索空间，难以在复杂组合优化问题中跳出深度局部最优。

Method: 提出G-LNS生成式进化框架，利用LLM协同进化紧密耦合的破坏与修复算子对，通过合作评估机制捕捉算子间的交互，发现互补的逻辑以实现有效的结构破坏与重建。

Result: 在TSP和CVRP等挑战性基准测试中，G-LNS显著优于基于LLM的AHD方法和强经典求解器，发现的启发式不仅以更少计算资源获得接近最优解，还能跨不同未见实例分布稳健泛化。

Conclusion: G-LNS将LLM驱动的自动启发式设计扩展到LNS算子生成，通过协同进化破坏-修复对实现了更强大的结构探索能力，为组合优化问题的自动求解器设计提供了新方向。

Abstract: While Large Language Models (LLMs) have recently shown promise in Automated Heuristic Design (AHD), existing approaches typically formulate AHD around constructive priority rules or parameterized local search guidance, thereby restricting the search space to fixed heuristic forms. Such designs offer limited capacity for structural exploration, making it difficult to escape deep local optima in complex Combinatorial Optimization Problems (COPs). In this work, we propose G-LNS, a generative evolutionary framework that extends LLM-based AHD to the automated design of Large Neighborhood Search (LNS) operators. Unlike prior methods that evolve heuristics in isolation, G-LNS leverages LLMs to co-evolve tightly coupled pairs of destroy and repair operators. A cooperative evaluation mechanism explicitly captures their interaction, enabling the discovery of complementary operator logic that jointly performs effective structural disruption and reconstruction. Extensive experiments on challenging COP benchmarks, such as Traveling Salesman Problems (TSP) and Capacitated Vehicle Routing Problems (CVRP), demonstrate that G-LNS significantly outperforms LLM-based AHD methods as well as strong classical solvers. The discovered heuristics not only achieve near-optimal solutions with reduced computational budgets but also exhibit robust generalization across diverse and unseen instance distributions.

</details>


### [71] [SynthAgent: A Multi-Agent LLM Framework for Realistic Patient Simulation -- A Case Study in Obesity with Mental Health Comorbidities](https://arxiv.org/abs/2602.08254)
*Arman Aghaee,Sepehr Asgarian,Jouhyun Jeon*

Main category: cs.AI

TL;DR: SynthAgent是一个多智能体系统框架，用于模拟肥胖症合并精神障碍患者，通过整合临床数据和个性特征来创建高保真虚拟患者，模拟疾病进展和治疗反应。


<details>
  <summary>Details</summary>
Motivation: 解决现实世界数据碎片化、偏见和隐私限制的问题，为研究复杂疾病提供高保真患者模拟的替代方案。

Method: 开发SynthAgent多智能体系统框架，整合索赔数据、人口调查和患者中心文献，构建具有个性特征的虚拟患者，通过自主智能体交互模拟疾病进展和治疗反应。

Result: 评估100多个生成的患者显示，GPT-5和Claude 4.5 Sonnet作为核心引擎达到最高保真度，优于Gemini 2.5 Pro和DeepSeek-R1。

Conclusion: SynthAgent提供了一个可扩展且保护隐私的框架，用于探索医学和心理领域的患者旅程、行为动态和决策过程。

Abstract: Simulating high-fidelity patients offers a powerful avenue for studying complex diseases while addressing the challenges of fragmented, biased, and privacy-restricted real-world data. In this study, we introduce SynthAgent, a novel Multi-Agent System (MAS) framework designed to model obesity patients with comorbid mental disorders, including depression, anxiety, social phobia, and binge eating disorder. SynthAgent integrates clinical and medical evidence from claims data, population surveys, and patient-centered literature to construct personalized virtual patients enriched with personality traits that influence adherence, emotion regulation, and lifestyle behaviors. Through autonomous agent interactions, the system simulates disease progression, treatment response, and life management across diverse psychosocial contexts. Evaluation of more than 100 generated patients demonstrated that GPT-5 and Claude 4.5 Sonnet achieved the highest fidelity as the core engine in the proposed MAS framework, outperforming Gemini 2.5 Pro and DeepSeek-R1. SynthAgent thus provides a scalable and privacy-preserving framework for exploring patient journeys, behavioral dynamics, and decision-making processes in both medical and psychological domains.

</details>


### [72] [Puda: Private User Dataset Agent for User-Sovereign and Privacy-Preserving Personalized AI](https://arxiv.org/abs/2602.08268)
*Akinori Maeda,Yuto Sekiya,Sota Sugimura,Tomoya Asai,Yu Tsuda,Kohei Ikeda,Hiroshi Fujii,Kohei Watanabe*

Main category: cs.AI

TL;DR: Puda是一个用户主权架构，通过聚合跨服务数据并支持客户端管理，在三个隐私级别控制数据共享，实现隐私保护与个性化服务的平衡。


<details>
  <summary>Details</summary>
Motivation: 当前个人数据集中在少数平台提供商手中，形成了数据孤岛，限制了用户主权和跨服务数据使用。同时，基于LLM的智能代理对个性化服务的需求激增，需要动态提供多样化的个人数据，这带来了数据利用与隐私保护的平衡挑战。

Method: 提出Puda（Private User Dataset Agent）架构，作为浏览器系统实现跨服务通用平台。支持三个隐私级别的数据共享控制：(i)详细浏览历史，(ii)提取的关键词，(iii)预定义类别子集。通过个性化旅行规划任务进行评估。

Result: 实验表明，提供预定义类别子集能达到共享详细浏览历史所获个性化性能的97.2%（通过LLM-as-a-Judge框架在三个标准下评估）。这证明Puda能有效实现多粒度管理，缓解隐私-个性化权衡。

Conclusion: Puda为AI原生用户主权提供了基础，使用户能够安全地利用个性化AI的全部潜力，通过实用的多粒度隐私控制选择来平衡数据利用与隐私保护。

Abstract: Personal data centralization among dominant platform providers including search engines, social networking services, and e-commerce has created siloed ecosystems that restrict user sovereignty, thereby impeding data use across services. Meanwhile, the rapid proliferation of Large Language Model (LLM)-based agents has intensified demand for highly personalized services that require the dynamic provision of diverse personal data. This presents a significant challenge: balancing the utilization of such data with privacy protection. To address this challenge, we propose Puda (Private User Dataset Agent), a user-sovereign architecture that aggregates data across services and enables client-side management. Puda allows users to control data sharing at three privacy levels: (i) Detailed Browsing History, (ii) Extracted Keywords, and (iii) Predefined Category Subsets. We implemented Puda as a browser-based system that serves as a common platform across diverse services and evaluated it through a personalized travel planning task. Our results show that providing Predefined Category Subsets achieves 97.2% of the personalization performance (evaluated via an LLM-as-a-Judge framework across three criteria) obtained when sharing Detailed Browsing History. These findings demonstrate that Puda enables effective multi-granularity management, offering practical choices to mitigate the privacy-personalization trade-off. Overall, Puda provides an AI-native foundation for user sovereignty, empowering users to safely leverage the full potential of personalized AI.

</details>


### [73] [Toward Formalizing LLM-Based Agent Designs through Structural Context Modeling and Semantic Dynamics Analysis](https://arxiv.org/abs/2602.08276)
*Haoyu Jia,Kento Kawaharazuka,Kei Okada*

Main category: cs.AI

TL;DR: 提出Structural Context Model形式化模型，用于分析和比较LLM智能体，并引入声明式实现框架和可持续的智能体工程工作流Semantic Dynamics Analysis，在动态猴子香蕉问题上取得显著效果提升。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体研究存在碎片化问题，概念框架和方法论原则常与底层实现细节交织，缺乏可分析、自洽的形式化模型来独立于实现地描述和比较智能体。

Method: 提出Structural Context Model形式化模型，从上下文结构角度分析LLM智能体；引入声明式实现框架和可持续的智能体工程工作流Semantic Dynamics Analysis，支持快速、系统的设计迭代。

Result: 在动态猴子香蕉问题变体上，使用该框架设计的智能体在最困难场景下成功率提升高达32个百分点。

Conclusion: 提出的形式化模型和工程工作流能有效解决LLM智能体研究的碎片化问题，为智能体机制提供原则性洞察，支持系统化设计迭代。

Abstract: Current research on large language model (LLM) agents is fragmented: discussions of conceptual frameworks and methodological principles are frequently intertwined with low-level implementation details, causing both readers and authors to lose track amid a proliferation of superficially distinct concepts. We argue that this fragmentation largely stems from the absence of an analyzable, self-consistent formal model that enables implementation-independent characterization and comparison of LLM agents. To address this gap, we propose the \texttt{Structural Context Model}, a formal model for analyzing and comparing LLM agents from the perspective of context structure. Building upon this foundation, we introduce two complementary components that together span the full lifecycle of LLM agent research and development: (1) a declarative implementation framework; and (2) a sustainable agent engineering workflow, \texttt{Semantic Dynamics Analysis}. The proposed workflow provides principled insights into agent mechanisms and supports rapid, systematic design iteration. We demonstrate the effectiveness of the complete framework on dynamic variants of the monkey-banana problem, where agents engineered using our approach achieve up to a 32 percentage points improvement in success rate on the most challenging setting.

</details>


### [74] [The Vibe-Automation of Automation: A Proactive Education Framework for Computer Science in the Age of Generative AI](https://arxiv.org/abs/2602.08295)
*Ilya Levin*

Main category: cs.AI

TL;DR: 论文提出"氛围自动化"概念，认为生成式AI代表了从算法优化到上下文语义协调的认知范式转变，人类角色从问题规范转向"氛围工程"。


<details>
  <summary>Details</summary>
Motivation: 生成式AI不是渐进技术进步，而是质变的认知范式转变，挑战计算机科学基础假设。需要新概念框架来理解这种从算法优化到上下文语义协调的转变。

Method: 提出"氛围自动化"概念框架，分析生成式AI如何操作化隐性规律。构建三层分析框架：教师世界观、产业关系、课程设计，探讨人类角色向"氛围工程"的转变。

Result: 生成式AI通过高维潜在表示编码对语调、意图和情境判断的敏感性，操作化隐性规律。人类角色转变为协调生成系统的对齐和上下文判断的"氛围工程师"。

Conclusion: 生成式AI代表了认知范式转变，需要教育机构和产业关系的相应变革。必须积极应对模式崩溃和文化同质化风险，避免回归合成统一性，通过有意识参与促进多样性。

Abstract: The emergence of generative artificial intelligence (GenAI) represents not an incremental technological advance but a qualitative epistemological shift that challenges foundational assumptions of computer science. Whereas machine learning has been described as the automation of automation, generative AI operates by navigating contextual, semantic, and stylistic coherence rather than optimizing predefined objective metrics. This paper introduces the concept of Vibe-Automation to characterize this transition.
  The central claim is that the significance of GenAI lies in its functional access to operationalized tacit regularities: context-sensitive patterns embedded in practice that cannot be fully specified through explicit algorithmic rules. Although generative systems do not possess tacit knowledge in a phenomenological sense, they operationalize sensitivities to tone, intent, and situated judgment encoded in high-dimensional latent representations. On this basis, the human role shifts from algorithmic problem specification toward Vibe-Engineering, understood as the orchestration of alignment and contextual judgment in generative systems.
  The paper connects this epistemological shift to educational and institutional transformation by proposing a conceptual framework structured across three analytical levels and three domains of action: faculty worldview, industry relations, and curriculum design. The risks of mode collapse and cultural homogenization are briefly discussed, emphasizing the need for deliberate engagement with generative systems to avoid regression toward synthetic uniformity.

</details>


### [75] [Moral Sycophancy in Vision Language Models](https://arxiv.org/abs/2602.08311)
*Shadman Rabby,Md. Hefzul Hossain Papon,Sabbir Ahmed,Nokimul Hasan Arif,A. B. M. Ashikur Rahman,Irfan Ahmad*

Main category: cs.AI

TL;DR: 该研究首次系统性地探索了视觉语言模型中的道德奉承行为，发现当用户表达不同意见时，VLMs经常放弃原本正确的道德判断，转向错误的立场，且存在不对称性：从正确转向错误比从错误转向正确更容易。


<details>
  <summary>Details</summary>
Motivation: 先前研究主要关注一般情境下的奉承行为，但对基于道德的视觉决策中奉承行为的影响理解不足。本研究旨在填补这一空白，系统研究VLMs中的道德奉承现象。

Method: 在Moralise和M^3oralBench两个数据集上评估了10个广泛使用的VLMs，在用户明确表达不同意见的情境下分析模型行为。使用错误引入率(EIR)和错误纠正率(ECR)进行量化评估。

Result: VLMs经常在后续响应中产生道德错误的判断，即使初始判断正确。模型表现出明显的不对称性：从道德正确转向错误的概率高于反向转变。不同数据集上表现存在差异，后续提示在Moralise上降低性能，在M^3oralBench上表现混合甚至改善。初始道德正确的语境会引发更强的奉承行为。

Conclusion: VLMs在道德影响面前表现脆弱，存在明显的权衡：纠错能力强的模型容易引入更多推理错误，而保守模型错误少但自我纠正能力有限。需要制定原则性策略来提高多模态AI系统的伦理一致性和鲁棒性。

Abstract: Sycophancy in Vision-Language Models (VLMs) refers to their tendency to align with user opinions, often at the expense of moral or factual accuracy. While prior studies have explored sycophantic behavior in general contexts, its impact on morally grounded visual decision-making remains insufficiently understood. To address this gap, we present the first systematic study of moral sycophancy in VLMs, analyzing ten widely-used models on the Moralise and M^3oralBench datasets under explicit user disagreement. Our results reveal that VLMs frequently produce morally incorrect follow-up responses even when their initial judgments are correct, and exhibit a consistent asymmetry: models are more likely to shift from morally right to morally wrong judgments than the reverse when exposed to user-induced bias. Follow-up prompts generally degrade performance on Moralise, while yielding mixed or even improved accuracy on M^3oralBench, highlighting dataset-dependent differences in moral robustness. Evaluation using Error Introduction Rate (EIR) and Error Correction Rate (ECR) reveals a clear trade-off: models with stronger error-correction capabilities tend to introduce more reasoning errors, whereas more conservative models minimize errors but exhibit limited ability to self-correct. Finally, initial contexts with a morally right stance elicit stronger sycophantic behavior, emphasizing the vulnerability of VLMs to moral influence and the need for principled strategies to improve ethical consistency and robustness in multimodal AI systems.

</details>


### [76] [Who Deserves the Reward? SHARP: Shapley Credit-based Optimization for Multi-Agent System](https://arxiv.org/abs/2602.08335)
*Yanming Li,Xuelin Zhang,WenJie Lu,Ziye Tang,Maodong Wu,Haotian Luo,Tongtong Wu,Zijie Peng,Hongze Mi,Yibo Feng,Naiqiang Tan,Chao Huang,Hong Chen,Li Shen*

Main category: cs.AI

TL;DR: SHARP框架通过Shapley值进行精确信用分配，优化多智能体强化学习，显著提升LLM与外部工具集成的性能


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统中信用分配困难，稀疏或全局广播奖励无法准确捕捉个体贡献，导致强化学习效率低下

Method: 提出SHARP框架，包含全局广播准确性奖励、基于Shapley值的边际信用奖励和工具过程奖励，通过轨迹组归一化稳定训练

Result: 在多个真实世界基准测试中显著优于现有方法，相比单智能体和多智能体方法平均匹配改进分别达到23.66%和14.05%

Conclusion: SHARP通过精确信用分配有效解决了多智能体系统中的信用分配挑战，为LLM与外部工具集成提供了稳定高效的训练框架

Abstract: Integrating Large Language Models (LLMs) with external tools via multi-agent systems offers a promising new paradigm for decomposing and solving complex problems. However, training these systems remains notoriously difficult due to the credit assignment challenge, as it is often unclear which specific functional agent is responsible for the success or failure of decision trajectories. Existing methods typically rely on sparse or globally broadcast rewards, failing to capture individual contributions and leading to inefficient reinforcement learning. To address these limitations, we introduce the Shapley-based Hierarchical Attribution for Reinforcement Policy (SHARP), a novel framework for optimizing multi-agent reinforcement learning via precise credit attribution. SHARP effectively stabilizes training by normalizing agent-specific advantages across trajectory groups, primarily through a decomposed reward mechanism comprising a global broadcast-accuracy reward, a Shapley-based marginal-credit reward for each agent, and a tool-process reward to improve execution efficiency. Extensive experiments across various real-world benchmarks demonstrate that SHARP significantly outperforms recent state-of-the-art baselines, achieving average match improvements of 23.66% and 14.05% over single-agent and multi-agent approaches, respectively.

</details>


### [77] [CoTZero: Annotation-Free Human-Like Vision Reasoning via Hierarchical Synthetic CoT](https://arxiv.org/abs/2602.08339)
*Chengyi Du,Yazhe Niu,Dazhong Shen,Luxin Xu*

Main category: cs.AI

TL;DR: CoTZero：无需标注的视觉语言模型训练范式，通过双阶段数据合成和认知对齐训练，提升视觉推理的逻辑一致性和可验证性


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型主要依赖表面相关性而非逻辑一致的结构化表示，导致高层次语义结构和非因果关系理解不足，限制了组合性和可验证推理能力

Method: 提出CoTZero范式：1）双阶段数据合成：自底向上提取视觉基元并组合成结构化问题推理形式；自顶向下用全局结构指导局部细节和因果关系解释；2）认知对齐训练：在合成数据基础上，通过强化微调中的认知一致可验证奖励机制，提供推理连贯性和事实正确性的逐步反馈

Result: 在带词汇扰动负例的多层次语义不一致基准测试中，CoTZero在域内和域外设置下均达到83.33%的F1分数，消融实验证实各组件对提升可解释性和人类对齐视觉推理均有贡献

Conclusion: CoTZero通过将人类认知模型融入推理过程，有效解决了视觉语言模型在结构化表示和逻辑推理方面的局限性，实现了更可解释和人类对齐的视觉推理能力

Abstract: Recent advances in vision-language models (VLMs) have markedly improved image-text alignment, yet they still fall short of human-like visual reasoning. A key limitation is that many VLMs rely on surface correlations rather than building logically coherent structured representations, which often leads to missed higher-level semantic structure and non-causal relational understanding, hindering compositional and verifiable reasoning. To address these limitations by introducing human models into the reasoning process, we propose CoTZero, an annotation-free paradigm with two components: (i) a dual-stage data synthesis approach and (ii) a cognition-aligned training method. In the first component, we draw inspiration from neurocognitive accounts of compositional productivity and global-to-local analysis. In the bottom-up stage, CoTZero extracts atomic visual primitives and incrementally composes them into diverse, structured question-reasoning forms. In the top-down stage, it enforces hierarchical reasoning by using coarse global structure to guide the interpretation of local details and causal relations. In the cognition-aligned training component, built on the synthesized CoT data, we introduce Cognitively Coherent Verifiable Rewards (CCVR) in Reinforcement Fine-Tuning (RFT) to further strengthen VLMs' hierarchical reasoning and generalization, providing stepwise feedback on reasoning coherence and factual correctness. Experiments show that CoTZero achieves an F1 score of 83.33 percent on our multi-level semantic inconsistency benchmark with lexical-perturbation negatives, across both in-domain and out-of-domain settings. Ablations confirm that each component contributes to more interpretable and human-aligned visual reasoning.

</details>


### [78] [Effect-Level Validation for Causal Discovery](https://arxiv.org/abs/2602.08340)
*Hoang Dang,Luan Pham,Minh Nguyen*

Main category: cs.AI

TL;DR: 提出以效应为中心、可容许性优先的因果发现框架，强调在强自选择反馈系统中，图恢复精度不足以保证因果可靠性，需通过可识别性、稳定性和证伪性验证效应估计。


<details>
  <summary>Details</summary>
Motivation: 当前因果发现广泛应用于大规模遥测数据以评估用户干预效果，但在强自选择的反馈驱动系统中，其决策可靠性尚不明确。需要超越图恢复精度的评估框架来确保因果结论的可信度。

Method: 提出效应中心、可容许性优先的框架，将发现的因果图视为结构假设，通过可识别性、稳定性和证伪性进行评估。使用真实游戏遥测数据研究早期竞争性游戏体验对短期留存的影响。

Result: 许多统计上合理的发现输出在施加最小时间和语义约束后无法进行点识别因果查询。当识别可行时，不同算法家族产生不同图结构但收敛到相似的决策一致效应估计，这些估计通过安慰剂、子采样和敏感性证伪测试。其他方法则表现出零星的可容许性和阈值敏感或衰减的效应。

Conclusion: 图级指标单独不足以作为特定目标查询因果可靠性的代理。在遥测驱动系统中，可信的因果结论需要优先考虑可容许性和效应级验证，而非仅关注因果结构恢复。

Abstract: Causal discovery is increasingly applied to large-scale telemetry data to estimate the effects of user-facing interventions, yet its reliability for decision-making in feedback-driven systems with strong self-selection remains unclear. In this paper, we propose an effect-centric, admissibility-first framework that treats discovered graphs as structural hypotheses and evaluates them by identifiability, stability, and falsification rather than by graph recovery accuracy alone. Empirically, we study the effect of early exposure to competitive gameplay on short-term retention using real-world game telemetry. We find that many statistically plausible discovery outputs do not admit point-identified causal queries once minimal temporal and semantic constraints are enforced, highlighting identifiability as a critical bottleneck for decision support. When identification is possible, several algorithm families converge to similar, decision-consistent effect estimates despite producing substantially different graph structures, including cases where the direct treatment-outcome edge is absent and the effect is preserved through indirect causal pathways. These converging estimates survive placebo, subsampling, and sensitivity refutation. In contrast, other methods exhibit sporadic admissibility and threshold-sensitive or attenuated effects due to endpoint ambiguity. These results suggest that graph-level metrics alone are inadequate proxies for causal reliability for a given target query. Therefore, trustworthy causal conclusions in telemetry-driven systems require prioritizing admissibility and effect-level validation over causal structural recovery alone.

</details>


### [79] [OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration](https://arxiv.org/abs/2602.08344)
*Qi Guo,Jianing Wang,Deyang Kong,Xiangyu Xi,Jianfei Zhang,Yi Lu,Jingang Wang,Wei Wang,Shikun Zhang,Wei Ye*

Main category: cs.AI

TL;DR: 本文提出Outline-Guided Path Exploration (OPE)方法，通过先生成多样化的推理大纲来划分解空间，减少并行推理路径间的信息冗余，提升大型推理模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有并行推理方法主要关注聚合阶段优化，对路径探索阶段关注有限。作者从理论上分析发现，探索路径间的互信息瓶颈限制了整体性能，需要解决路径间的信息冗余问题。

Method: 提出OPE方法：1）先生成多样化的推理大纲来划分解空间；2）基于大纲进行并行路径推理；3）采用迭代强化学习策略，分别优化大纲规划和基于大纲的推理。

Result: 在多个具有挑战性的数学基准测试上进行广泛实验，证明OPE能有效提升不同聚合策略下的推理性能，使大型推理模型更可靠地发现正确解。

Conclusion: 通过显式划分解空间并减少路径间的信息冗余，OPE方法解决了并行推理中的互信息瓶颈问题，显著提升了大型推理模型的推理能力。

Abstract: Parallel thinking has emerged as a new paradigm for large reasoning models (LRMs) in tackling complex problems. Recent methods leverage Reinforcement Learning (RL) to enhance parallel thinking, aiming to address the limitations in computational resources and effectiveness encountered with supervised fine-tuning. However, most existing studies primarily focus on optimizing the aggregation phase, with limited attention to the path exploration stage. In this paper, we theoretically analyze the optimization of parallel thinking under the Reinforcement Learning with Verifiable Rewards (RLVR) setting, and identify that the mutual information bottleneck among exploration paths fundamentally restricts overall performance. To address this, we propose Outline-Guided Path Exploration (OPE), which explicitly partitions the solution space by generating diverse reasoning outlines prior to parallel path reasoning, thereby reducing information redundancy and improving the diversity of information captured across exploration paths. We implement OPE with an iterative RL strategy that optimizes outline planning and outline-guided reasoning independently. Extensive experiments across multiple challenging mathematical benchmarks demonstrate that OPE effectively improves reasoning performance in different aggregation strategies, enabling LRMs to more reliably discover correct solutions.

</details>


### [80] [Towards Better Evolution Modeling for Temporal Knowledge Graphs](https://arxiv.org/abs/2602.08353)
*Zhang Jiasheng,Li Zhangpin,Wang Mingzhe,Shao Jie,Cui Jiangtao,Li Hui*

Main category: cs.AI

TL;DR: 现有TKG预测基准存在严重缺陷，仅通过统计共现就能达到接近SOTA的性能，无需使用时间信息。作者提出了新的TKG演化基准，包含四个偏差校正数据集和两个新任务。


<details>
  <summary>Details</summary>
Motivation: 现有TKG预测基准存在严重问题：1）数据集存在固有偏差，2）评估任务过于简化，3）时间间隔知识格式不合理，4）忽略知识过时学习，5）演化理解信息不足。这些问题导致模型可以通过简单统计共现获得高分数，无法公平评估TKG演化建模能力。

Method: 1）分析现有基准问题的根本原因，识别数据集偏差和评估任务简化带来的可利用漏洞；2）构建TKG演化基准，包含四个经过偏差校正的数据集；3）设计两个与演化过程紧密相关的新任务，促进对TKG演化建模挑战的准确理解。

Result: 发现现有基准存在严重缺陷，仅通过简单的共现统计就能在YAGO数据集上获得接近0.9的Hits@10分数，完全不需要使用时间信息。提出的新基准解决了这些问题，提供了更公平的评估框架。

Conclusion: 现有TKG预测基准存在系统性缺陷，无法准确评估模型对时间演化的理解能力。提出的TKG演化基准通过偏差校正数据集和更贴近实际演化过程的任务设计，为TKG演化建模提供了更公平、更有挑战性的评估平台。

Abstract: Temporal knowledge graphs (TKGs) structurally preserve evolving human knowledge. Recent research has focused on designing models to learn the evolutionary nature of TKGs to predict future facts, achieving impressive results. For instance, Hits@10 scores over 0.9 on YAGO dataset. However, we find that existing benchmarks inadvertently introduce a shortcut. Near state-of-the-art performance can be simply achieved by counting co-occurrences, without using any temporal information. In this work, we examine the root cause of this issue, identifying inherent biases in current datasets and over simplified form of evaluation task that can be exploited by these biases. Through this analysis, we further uncover additional limitations of existing benchmarks, including unreasonable formatting of time-interval knowledge, ignorance of learning knowledge obsolescence, and insufficient information for precise evolution understanding, all of which can amplify the shortcut and hinder a fair assessment. Therefore, we introduce the TKG evolution benchmark. It includes four bias-corrected datasets and two novel tasks closely aligned with the evolution process, promoting a more accurate understanding of the challenges in TKG evolution modeling. Benchmark is available at: https://github.com/zjs123/TKG-Benchmark.

</details>


### [81] [Does Your Reasoning Model Implicitly Know When to Stop Thinking?](https://arxiv.org/abs/2602.08354)
*Zixuan Huang,Xin Xia,Yuxi Ren,Jianbin Zheng,Xuanda Wang,Zhixia Zhang,Hongyan Xie,Songshi Liang,Zehao Chen,Xuefeng Xiao,Fuzhen Zhuang,Jianxin Li,Yikun Ban,Deqing Wang*

Main category: cs.AI

TL;DR: 提出SAGE采样范式，通过释放大推理模型的自我停止能力，提升推理效率和准确性


<details>
  <summary>Details</summary>
Motivation: 当前大推理模型使用长思维链方法存在大量冗余，损害计算效率并导致实时应用延迟。研究发现更长的推理链与正确性无关甚至有害，而模型实际上隐含知道何时停止思考，但被当前采样范式所掩盖。

Method: 提出SAGE（Self-Aware Guided Efficient Reasoning）采样范式，释放模型的高效推理潜力。进一步将SAGE作为混合采样集成到基于群体的强化学习（SAGE-RL）中，使SAGE-RL能够将SAGE发现的高效推理模式融入标准pass@1推理。

Result: SAGE-RL显著提升了多个具有挑战性的数学基准测试中大推理模型的推理准确性和效率。

Conclusion: 通过释放大推理模型隐含的自我停止能力，SAGE采样范式能够显著提升推理效率和准确性，为实时应用提供更高效的解决方案。

Abstract: Recent advancements in large reasoning models (LRMs) have greatly improved their capabilities on complex reasoning tasks through Long Chains of Thought (CoTs). However, this approach often results in substantial redundancy, impairing computational efficiency and causing significant delays in real-time applications. Recent studies show that longer reasoning chains are frequently uncorrelated with correctness and can even be detrimental to accuracy. In a further in-depth analysis of this phenomenon, we surprisingly uncover and empirically verify that LRMs implicitly know the appropriate time to stop thinking, while this capability is obscured by current sampling paradigms. Motivated by this, we introduce SAGE (Self-Aware Guided Efficient Reasoning), a novel sampling paradigm that unleashes this efficient reasoning potential. Furthermore, integrating SAGE as mixed sampling into group-based reinforcement learning (SAGE-RL) enables SAGE-RL to effectively incorporate SAGE-discovered efficient reasoning patterns into standard pass@1 inference, markedly enhancing both the reasoning accuracy and efficiency of LRMs across multiple challenging mathematical benchmarks.

</details>


### [82] [Circuit Representations of Random Forests with Applications to XAI](https://arxiv.org/abs/2602.08362)
*Chunxi Ji,Adnan Darwiche*

Main category: cs.AI

TL;DR: 将随机森林分类器编译为电路，用于高效计算决策解释、鲁棒性和决策翻转路径


<details>
  <summary>Details</summary>
Motivation: 现有方法在计算随机森林决策的解释、鲁棒性和翻转路径时效率较低，需要更高效的编译和计算方法

Method: 1) 将随机森林编译为电路表示；2) 利用电路计算决策的完整和一般原因；3) 提出算法计算决策鲁棒性和最短翻转路径

Result: 提出的方法比现有类似方法显著更高效，能够枚举充分原因、必要原因、对比解释，计算决策鲁棒性，并识别最短决策翻转路径

Conclusion: 通过电路编译方法，实现了对随机森林决策的高效解释分析，为理解模型决策提供了实用的计算工具

Abstract: We make three contributions in this paper. First, we present an approach for compiling a random forest classifier into a set of circuits, where each circuit directly encodes the instances in some class of the classifier. We show empirically that our proposed approach is significantly more efficient than existing similar approaches. Next, we utilize this approach to further obtain circuits that are tractable for computing the complete and general reasons of a decision, which are instance abstractions that play a fundamental role in computing explanations. Finally, we propose algorithms for computing the robustness of a decision and all shortest ways to flip it. We illustrate the utility of our contributions by using them to enumerate all sufficient reasons, necessary reasons and contrastive explanations of decisions; to compute the robustness of decisions; and to identify all shortest ways to flip the decisions made by random forest classifiers learned from a wide range of datasets.

</details>


### [83] [MemAdapter: Fast Alignment across Agent Memory Paradigms via Generative Subgraph Retrieval](https://arxiv.org/abs/2602.08369)
*Xin Zhang,Kailai Yang,Chenyue Li,Hao Li,Qiyu Wei,Jun'ichi Tsujii,Sophia Ananiadou*

Main category: cs.AI

TL;DR: MemAdapter是一个统一异构内存范式的检索框架，通过两阶段训练实现跨范式快速对齐，显著降低对齐成本并支持零样本融合。


<details>
  <summary>Details</summary>
Motivation: 现有智能体内存系统通常设计在孤立范式（显式、参数化或潜在内存）中，检索方法紧密耦合，阻碍了跨范式泛化和融合。需要统一异构内存范式。

Method: 提出MemAdapter框架：1）从统一内存空间训练生成式子图检索器；2）通过对比学习训练轻量对齐模块，将检索器适配到未见内存范式。采用两阶段训练策略。

Result: 在三个公开评估基准上，生成式子图检索器在三种内存范式和智能体模型规模上持续优于五个强基线系统。跨范式对齐仅需13分钟（单GPU），性能优于原始检索器且训练计算量小于5%。支持零样本跨范式融合。

Conclusion: MemAdapter作为即插即用解决方案，首次统一了异构内存范式，显著提高了内存检索的灵活性并大幅降低了跨范式对齐成本。

Abstract: Memory mechanism is a core component of LLM-based agents, enabling reasoning and knowledge discovery over long-horizon contexts. Existing agent memory systems are typically designed within isolated paradigms (e.g., explicit, parametric, or latent memory) with tightly coupled retrieval methods that hinder cross-paradigm generalization and fusion. In this work, we take a first step toward unifying heterogeneous memory paradigms within a single memory system. We propose MemAdapter, a memory retrieval framework that enables fast alignment across agent memory paradigms. MemAdapter adopts a two-stage training strategy: (1) training a generative subgraph retriever from the unified memory space, and (2) adapting the retriever to unseen memory paradigms by training a lightweight alignment module through contrastive learning. This design improves the flexibility for memory retrieval and substantially reduces alignment cost across paradigms. Comprehensive experiments on three public evaluation benchmarks demonstrate that the generative subgraph retriever consistently outperforms five strong agent memory systems across three memory paradigms and agent model scales. Notably, MemAdapter completes cross-paradigm alignment within 13 minutes on a single GPU, achieving superior performance over original memory retrievers with less than 5% of training compute. Furthermore, MemAdapter enables effective zero-shot fusion across memory paradigms, highlighting its potential as a plug-and-play solution for agent memory systems.

</details>


### [84] [Grounding Generative Planners in Verifiable Logic: A Hybrid Architecture for Trustworthy Embodied AI](https://arxiv.org/abs/2602.08373)
*Feiyu Wu,Xu Zheng,Yue Qu,Zhuocheng Wang,Zicheng Feng,Hui Li*

Main category: cs.AI

TL;DR: VIRF框架通过逻辑导师与LLM规划器的对话协作，实现可验证的安全规划，在家庭安全任务中达到0%危险行动率和77.3%目标达成率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM规划器缺乏形式化推理能力，无法提供严格的安全保证。现有方法要么依赖不可靠的LLM安全检查，要么只是拒绝不安全计划而不提供修复方案。

Method: 提出可验证迭代精炼框架(VIRF)，采用神经符号架构，通过基于形式化安全本体的确定性逻辑导师与LLM规划器进行导师-学徒对话，提供因果性和教学性反馈，实现智能计划修复而非简单规避。

Result: 在具有挑战性的家庭安全任务中，VIRF实现了0%的危险行动率(HAR)和77.3%的目标达成率(GCR)，在所有基线方法中最高，平均仅需1.1次修正迭代。

Conclusion: VIRF展示了一条构建根本上可信赖且可验证安全的具身智能体的原则性路径，从被动安全把关转向主动协作修复。

Abstract: Large Language Models (LLMs) show promise as planners for embodied AI, but their stochastic nature lacks formal reasoning, preventing strict safety guarantees for physical deployment. Current approaches often rely on unreliable LLMs for safety checks or simply reject unsafe plans without offering repairs. We introduce the Verifiable Iterative Refinement Framework (VIRF), a neuro-symbolic architecture that shifts the paradigm from passive safety gatekeeping to active collaboration. Our core contribution is a tutor-apprentice dialogue where a deterministic Logic Tutor, grounded in a formal safety ontology, provides causal and pedagogical feedback to an LLM planner. This enables intelligent plan repairs rather than mere avoidance. We also introduce a scalable knowledge acquisition pipeline that synthesizes safety knowledge bases from real-world documents, correcting blind spots in existing benchmarks. In challenging home safety tasks, VIRF achieves a perfect 0 percent Hazardous Action Rate (HAR) and a 77.3 percent Goal-Condition Rate (GCR), which is the highest among all baselines. It is highly efficient, requiring only 1.1 correction iterations on average. VIRF demonstrates a principled pathway toward building fundamentally trustworthy and verifiably safe embodied agents.

</details>


### [85] [SCOUT-RAG: Scalable and Cost-Efficient Unifying Traversal for Agentic Graph-RAG over Distributed Domains](https://arxiv.org/abs/2602.08400)
*Longkun Li,Yuanben Zou,Jinghan Wu,Yuqing Wen,Jing Li,Hangwei Qian,Ivor Tsang*

Main category: cs.AI

TL;DR: SCOUT-RAG是一个分布式智能Graph-RAG框架，通过渐进式跨域检索解决分布式受限环境下的知识图检索问题，显著减少跨域调用和延迟。


<details>
  <summary>Details</summary>
Motivation: 传统Graph-RAG依赖集中式知识图，但在分布式和访问受限环境（如医院、跨国组织）中，检索需要在没有全局图可见性的情况下选择相关域和适当的遍历深度。

Method: 提出SCOUT-RAG框架，采用四个协作代理：评估域相关性、决定何时扩展到额外域、调整遍历深度以避免不必要的图探索、合成高质量答案。框架旨在最小化检索遗憾并控制延迟和API成本。

Result: 在多域知识设置中，SCOUT-RAG达到与集中式基线（包括DRIFT和穷举域遍历）相当的性能，同时显著减少跨域调用、处理的总令牌数和延迟。

Conclusion: SCOUT-RAG为分布式受限环境提供了一种可扩展且成本效益高的Graph-RAG解决方案，通过智能代理协作实现高效的知识检索。

Abstract: Graph-RAG improves LLM reasoning using structured knowledge, yet conventional designs rely on a centralized knowledge graph. In distributed and access-restricted settings (e.g., hospitals or multinational organizations), retrieval must select relevant domains and appropriate traversal depth without global graph visibility or exhaustive querying. To address this challenge, we introduce \textbf{SCOUT-RAG} (\textit{\underline{S}calable and \underline{CO}st-efficient \underline{U}nifying \underline{T}raversal}), a distributed agentic Graph-RAG framework that performs progressive cross-domain retrieval guided by incremental utility goals. SCOUT-RAG employs four cooperative agents that: (i) estimate domain relevance, (ii) decide when to expand retrieval to additional domains, (iii) adapt traversal depth to avoid unnecessary graph exploration, and (iv) synthesize the high-quality answers. The framework is designed to minimize retrieval regret, defined as missing useful domain information, while controlling latency and API cost. Across multi-domain knowledge settings, SCOUT-RAG achieves performance comparable to centralized baselines, including DRIFT and exhaustive domain traversal, while substantially reducing cross-domain calls, total tokens processed, and latency.

</details>


### [86] [On Protecting Agentic Systems' Intellectual Property via Watermarking](https://arxiv.org/abs/2602.08401)
*Liwen Wang,Zongjie Li,Yuchong Xie,Shuai Wang,Dongdong She,Wei Wang,Juergen Rahmel*

Main category: cs.AI

TL;DR: AGENTWM是首个专门为智能体模型设计的水印框架，通过偏置功能相同的工具执行路径分布来嵌入水印，保护智能体系统的知识产权免受模仿攻击。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型发展为能够自主推理和使用工具的智能体系统，创造了重要的知识产权价值。这些系统容易受到模仿攻击，而现有的LLM水印技术无法应用于智能体领域，因为现实中的智能体系统通常是灰盒，隐藏了验证所需的内部推理痕迹。

Method: AGENTWM利用动作序列的语义等价性，通过微妙地偏置功能相同的工具执行路径分布来注入水印。该方法将可验证信号直接嵌入可见的动作轨迹中，同时保持对用户不可察觉。开发了自动生成鲁棒水印方案的流程和严格的统计假设检验验证程序。

Result: 在三个复杂领域的广泛评估表明，AGENTWM实现了高检测准确率，同时对智能体性能影响可忽略。AGENTWM能有效保护智能体知识产权，对抗自适应攻击者，攻击者无法在不严重降低被盗模型效用的情况下移除水印。

Conclusion: AGENTWM是首个专门为智能体模型设计的水印框架，成功解决了现有LLM水印技术无法应用于智能体系统的问题，为保护智能体知识产权提供了有效解决方案。

Abstract: The evolution of Large Language Models (LLMs) into agentic systems that perform autonomous reasoning and tool use has created significant intellectual property (IP) value. We demonstrate that these systems are highly vulnerable to imitation attacks, where adversaries steal proprietary capabilities by training imitation models on victim outputs. Crucially, existing LLM watermarking techniques fail in this domain because real-world agentic systems often operate as grey boxes, concealing the internal reasoning traces required for verification. This paper presents AGENTWM, the first watermarking framework designed specifically for agentic models. AGENTWM exploits the semantic equivalence of action sequences, injecting watermarks by subtly biasing the distribution of functionally identical tool execution paths. This mechanism allows AGENTWM to embed verifiable signals directly into the visible action trajectory while remaining indistinguishable to users. We develop an automated pipeline to generate robust watermark schemes and a rigorous statistical hypothesis testing procedure for verification. Extensive evaluations across three complex domains demonstrate that AGENTWM achieves high detection accuracy with negligible impact on agent performance. Our results confirm that AGENTWM effectively protects agentic IP against adaptive adversaries, who cannot remove the watermarks without severely degrading the stolen model's utility.

</details>


### [87] [From Assistant to Double Agent: Formalizing and Benchmarking Attacks on OpenClaw for Personalized Local AI Agent](https://arxiv.org/abs/2602.08412)
*Yuhang Wang,Feiming Xu,Zheng Lin,Guangyu He,Yuzhe Huang,Haichang Gao,Zhenxing Niu*

Main category: cs.AI

TL;DR: PASB是一个针对现实世界个性化AI代理的安全评估框架，通过个性化场景、真实工具链和长时交互来评估OpenClaw等代理的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有代理安全研究主要关注合成或任务中心化设置，无法准确捕捉现实世界个性化代理的攻击面和风险传播机制，需要专门的安全评估框架。

Method: 提出PASB框架，基于现有代理攻击范式，整合个性化使用场景、真实工具链和长时交互，支持黑盒端到端安全评估。

Result: 以OpenClaw为案例研究发现其在用户提示处理、工具使用和记忆检索等不同执行阶段存在关键漏洞，揭示个性化代理部署的严重安全风险。

Conclusion: PASB框架能有效评估现实世界个性化代理的安全风险，OpenClaw的案例研究显示个性化代理部署存在重大安全隐患，需要加强安全防护。

Abstract: Although large language model (LLM)-based agents, exemplified by OpenClaw, are increasingly evolving from task-oriented systems into personalized AI assistants for solving complex real-world tasks, their practical deployment also introduces severe security risks. However, existing agent security research and evaluation frameworks primarily focus on synthetic or task-centric settings, and thus fail to accurately capture the attack surface and risk propagation mechanisms of personalized agents in real-world deployments. To address this gap, we propose Personalized Agent Security Bench (PASB), an end-to-end security evaluation framework tailored for real-world personalized agents. Building upon existing agent attack paradigms, PASB incorporates personalized usage scenarios, realistic toolchains, and long-horizon interactions, enabling black-box, end-to-end security evaluation on real systems. Using OpenClaw as a representative case study, we systematically evaluate its security across multiple personalized scenarios, tool capabilities, and attack types. Our results indicate that OpenClaw exhibits critical vulnerabilities at different execution stages, including user prompt processing, tool usage, and memory retrieval, highlighting substantial security risks in personalized agent deployments. The code for the proposed PASB framework is available at https://github.com/AstorYH/PASB.

</details>


### [88] [When Evaluation Becomes a Side Channel: Regime Leakage and Structural Mitigations for Alignment Assessment](https://arxiv.org/abs/2602.08449)
*Igor Santos-Grueiro*

Main category: cs.AI

TL;DR: 论文提出将AI对齐评估重新定义为部分可观测下的信息流问题，证明评估时与部署时行为差异受内部表示与制度变量互信息限制，并通过制度盲训练机制减少制度信息可提取性来抑制条件策略行为。


<details>
  <summary>Details</summary>
Motivation: 现有AI安全评估假设评估时观察到的行为能预测部署时行为，但对于具有情境意识的智能体，这一假设变得脆弱，因为它们可能利用评估与部署之间的信息差异实施条件策略（如奉承和潜伏代理），在监督下保持合规但在部署时违规。

Method: 将对齐评估重构为部分可观测下的信息流问题，提出制度盲训练机制：通过对抗不变性减少决策相关内部表示中制度信息的可提取性，并在开源语言模型上评估两种完全特征化的失效模式（科学奉承和时间潜伏代理）。

Result: 制度盲训练在两个评估案例中都抑制了制度条件行为，且没有可测量的任务效用损失，但表现出不同的动态：奉承在低干预强度下表现出尖锐的表示和行为转变，而潜伏代理行为需要更强的压力且没有表现出制度可解码性的干净崩溃。

Conclusion: 表示不变性是有意义但根本有限的控制杠杆，其有效性取决于制度信息在策略中的嵌入方式。行为评估应辅以制度意识和信息流的白盒诊断。

Abstract: Safety evaluation for advanced AI systems implicitly assumes that behavior observed under evaluation is predictive of behavior in deployment. This assumption becomes fragile for agents with situational awareness, which may exploitregime leakage-informational cues distinguishing evaluation from deployment-to implement conditional policies such as sycophancy and sleeper agents, which preserve compliance under oversight while defecting in deployment-like regimes. We reframe alignment evaluation as a problem of information flow under partial observability. Within this framework, we show that divergence between evaluation-time and deployment-time behavior is bounded by the mutual information between internal representations and the regime variable. Motivated by this result, we study regime-blind mechanisms: training-time interventions that reduce the extractability of regime information at decision-relevant internal representations via adversarial invariance. We evaluate this approach on a base, open-weight language model across two fully characterized failure modes -scientific sycophancy and temporal sleeper agents. Regime-blind training suppresses regime-conditioned behavior in both evaluated cases without measurable loss of task utility, but with qualitatively different dynamics: sycophancy exhibits a sharp representational and behavioral transition at low intervention strength, whereas sleeper-agent behavior requires substantially stronger pressure and does not exhibit a clean collapse of regime decodability. These results demonstrate that representational invariance is a meaningful but fundamentally limited control lever, whose effectiveness depends on how regime information is embedded in the policy. We argue that behavioral evaluation should be complemented with white-box diagnostics of regime awareness and information flow.

</details>


### [89] [TreeTensor: Boost AI System on Nested Data with Constrained Tree-Like Tensor](https://arxiv.org/abs/2602.08517)
*Shaoang Zhang,Yazhe Niu*

Main category: cs.AI

TL;DR: TreeTensor：一种用于处理嵌套数据的通用容器，支持零成本应用任意函数和操作到分层结构数据，兼容主流机器学习库


<details>
  <summary>Details</summary>
Motivation: 传统Tensor固定形状的特性在处理复杂认知AI系统中的分层结构（嵌套）数据时存在不便和低效问题，需要一种更灵活的数据容器

Method: 总结嵌套数据的两种主要计算模式，提出TreeTensor通用嵌套数据容器，通过约束和魔法工具实现零成本操作嵌套数据，支持Scikit-Learn、Numpy、PyTorch等库

Result: TreeTensor在各种问题中表现出强大的可用性（包括最复杂的AlphaStar系统），运行时效率优秀且无额外开销，支持异步执行和变长数据计算

Conclusion: TreeTensor为处理分层结构数据提供了一种高效、通用的解决方案，能够显著简化复杂AI系统中的数据处理编程

Abstract: Tensor is the most basic and essential data structure of nowadays artificial intelligence (AI) system. The natural properties of Tensor, especially the memory-continuity and slice-independence, make it feasible for training system to leverage parallel computing unit like GPU to process data simultaneously in batch, spatial or temporal dimensions. However, if we look beyond perception tasks, the data in a complicated cognitive AI system usually has hierarchical structures (i.e. nested data) with various modalities. They are inconvenient and inefficient to program directly with conventional Tensor with fixed shape. To address this issue, we summarize two main computational patterns of nested data, and then propose a general nested data container: TreeTensor. Through various constraints and magic utilities of TreeTensor, one can apply arbitrary functions and operations to nested data with almost zero cost, including some famous machine learning libraries, such as Scikit-Learn, Numpy and PyTorch. Our approach utilizes a constrained tree-structure perspective to systematically model data relationships, and it can also easily be combined with other methods to extend more usages, such as asynchronous execution and variable-length data computation. Detailed examples and benchmarks show TreeTensor not only provides powerful usability in various problems, especially one of the most complicated AI systems at present: AlphaStar for StarCraftII, but also exhibits excellent runtime efficiency without any overhead. Our project is available at https://github.com/opendilab/DI-treetensor.

</details>


### [90] [Reinforcement Inference: Leveraging Uncertainty for Self-Correcting Language Model Reasoning](https://arxiv.org/abs/2602.08520)
*Xinhai Sun*

Main category: cs.AI

TL;DR: 提出Reinforcement Inference方法，利用模型自身的不确定性选择性触发二次推理，无需重新训练即可显著提升LLM性能


<details>
  <summary>Details</summary>
Motivation: 当前LLM在单次贪婪推理协议下会系统性低估模型真实能力，许多错误源于内部模糊性下的过早决策，而非知识缺失

Method: 基于熵感知的推理时控制策略，利用模型自身不确定性选择性地调用第二次更慎重的推理尝试

Result: 在MMLU-Pro的12,032个问题上，DeepSeek-v3.2准确率从60.72%提升至84.03%，仅增加61.06%的推理调用；不确定性感知选择捕获了大部分可获得的改进

Conclusion: 提出熵感知范式来测量和扩展模型能力，单次贪婪推理与不确定性条件化推理之间的差距为LLM潜在推理范围提供了诊断视角

Abstract: Modern large language models (LLMs) are often evaluated and deployed under a \emph{one-shot, greedy} inference protocol, especially in professional settings that require deterministic behavior. This regime can systematically under-estimate a fixed model's true capability: many errors arise not from missing knowledge, but from premature commitment under internal ambiguity. We introduce \emph{Reinforcement Inference}, an entropy-aware inference-time control strategy that uses the model's own uncertainty to selectively invoke a second, more deliberate reasoning attempt, enabling stronger performance \emph{without any retraining}.
  On 12,032 MMLU-Pro questions across 14 subjects, using DeepSeek-v3.2 with deterministic decoding in a zero-shot setting, Reinforcement Inference improves accuracy from 60.72\% to 84.03\%, while only incurring 61.06\% additional inference calls. A 100\% re-asking ablation reaches 84.35\%, indicating that uncertainty-aware selection captures most of the attainable improvement with substantially less compute. Moreover, a \emph{prompt-only} ablation underperforms the baseline, suggesting that the gains are not explained by generic `` your output had high entropy, think step-by-step'' prompting alone.
  Beyond providing a practical inference-time upgrade, our results suggest a broader \emph{entropy-aware} paradigm for measuring and expanding model capability: because modern decoder-based models generate outputs autoregressively, entropy and related confidence measures arise naturally as first-class control signals during generation. The resulting gap between one-pass greedy inference and uncertainty-conditioned deliberation offers a diagnostic lens on an LLM's latent reasoning horizon and motivates future training objectives that explicitly constrain correctness--confidence alignment.

</details>


### [91] [Dialogue Model Optimization via Agent Game and Adaptive Tree-based GRPO](https://arxiv.org/abs/2602.08533)
*Kun Peng,Conghui Tan,Yu Liu,Guohua Tang,Zhongqian Sun,Wei Yang,Zining Zhu,Lei Jiang,Yanbing Liu,Hao Peng*

Main category: cs.AI

TL;DR: 提出一个结合在线个性化与自适应树基分组相对策略优化的长视野强化学习框架，用于开放域对话代理，解决现有方法依赖预收集数据和短视野偏见的问题。


<details>
  <summary>Details</summary>
Motivation: 现有开放域对话代理方法存在两个关键限制：过度依赖预收集的用户数据，以及强化学习中的短视野偏见（忽视对话的长期价值）。需要一种能够在线个性化并考虑长期对话价值的解决方案。

Method: 采用双代理博弈范式：用户代理通过风格模仿学习用户特定对话特征，并通过主动终止预测回合级终止概率作为即时奖励来构建动态环境；对话代理使用自适应树基分组相对策略优化（AT-GRPO），将对话轨迹重新解释为树结构，引入自适应观察范围，在早期阶段使用较大范围支持话题探索，晚期阶段使用较小范围促进对话维护，将计算开销从指数级降至多项式级。

Result: 广泛实验表明该框架在性能、样本效率和鲁棒性方面表现优异，显著优于现有方法。

Conclusion: 提出的长视野RL框架成功解决了开放域对话代理中的在线个性化和长期价值优化问题，通过AT-GRPO实现了高效的长视野奖励捕获，为对话系统提供了更有效的个性化交互方案。

Abstract: Open-ended dialogue agents aim to deliver engaging, personalized interactions by adapting to users' traits, but existing methods face critical limitations: over-reliance on pre-collected user data, and short-horizon biases in reinforcement learning (RL) that neglect long-term dialogue value. To address these, we propose a novel long-horizon RL framework integrating online personalization with Adaptive Tree-based Group Relative Policy Optimization (AT-GRPO). Adopting a two-agent game paradigm, a user agent constructs dynamic environments via style mimicry (learning user-specific conversational traits) and active termination (predicting turn-level termination probabilities as immediate rewards), forming an iterative cycle that drives the dialogue agent to deepen interest exploration. AT-GRPO reinterprets dialogue trajectories as trees and introduces adaptive observation ranges. Unlike full tree expansion that incurs exponential overhead, it limits each node to aggregate rewards from a stage-aware range: larger ranges support early-stage topic exploration, while smaller ranges facilitate late-stage dialogue maintenance. This design reduces rollout budgets from exponential to polynomial in the dialogue length, while preserving long-term reward capture. Extensive experiments show our framework's superior performance, sample efficiency, and robustness.

</details>


### [92] [PRISM: A Principled Framework for Multi-Agent Reasoning via Gain Decomposition](https://arxiv.org/abs/2602.08586)
*Yiming Yang,Zhuoyuan Li,Fanxiang Zeng,Hao Fu,Yue Liu*

Main category: cs.AI

TL;DR: PRISM框架通过理论分解多智能体推理增益为探索、信息、聚合三个维度，并设计角色多样性、执行反馈、迭代合成方法实现全面优化，在数学推理、代码生成等任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体协作方法缺乏理论指导，不清楚为何多智能体优于单智能体以及哪些设计选择最关键，难以系统优化多智能体推理系统。

Method: 提出统一理论框架，将多智能体推理增益分解为探索、信息、聚合三个维度；基于此设计PRISM框架，通过角色多样性实现探索，执行反馈提供信息，迭代合成实现聚合。

Result: 在数学推理、代码生成和函数调用基准测试中，PRISM达到最先进性能，相比仅优化部分维度的方法具有更优的计算效率。

Conclusion: 理论框架为未来多智能体推理系统提供了可操作的设计原则，PRISM通过全面优化三个维度实现了性能提升。

Abstract: Multi-agent collaboration has emerged as a promising paradigm for enhancing reasoning capabilities of Large Language Models (LLMs). However, existing approaches remain largely heuristic, lacking principled guidance on what drives performance gains and how to systematically optimize multi-agent reasoning. Specifically, it remains unclear why multi-agent collaboration outperforms single-agent reasoning and which design choices contribute most to these gains, making it difficult to build better systems.
  We address this gap by introducing a unified theoretical framework that decomposes multi-agent reasoning gains into three conceptually independent dimensions: Exploration for diverse solution coverage, Information for high-fidelity feedback, and Aggregation for principled consensus. Through this lens, existing methods can be understood as special cases that optimize only subsets of these dimensions. Building upon this decomposition, a novel framework called PRISM (Propose-Review-Integrate Synthesis for Multi-agent Reasoning) is proposed, which jointly maximizes all three dimensions through role-based diversity, execution-grounded feedback with evidence-based cross-evaluation, and iterative synthesis with closed-loop validation. Extensive experiments across mathematical reasoning, code generation, and function calling benchmarks demonstrate that PRISM achieves state-of-the-art performance with superior compute-efficiency compared to methods optimizing partial dimensions. The theoretical framework provides actionable design principles for future multi-agent reasoning systems.

</details>


### [93] [An Attention Mechanism for Robust Multimodal Integration in a Global Workspace Architecture](https://arxiv.org/abs/2602.08597)
*Roland Bertin-Johannet,Lara Scipio,Leopold Maytié,Rufin VanRullen*

Main category: cs.AI

TL;DR: 本文提出了一种用于全局工作空间理论（GWT）的顶部注意力机制，以选择多模态系统中的相关模态，提高了噪声鲁棒性，并在MM-IMDb基准测试中达到了最先进水平。


<details>
  <summary>Details</summary>
Motivation: 全局工作空间理论（GWT）作为认知神经科学启发的框架，可用于多模态集成计算架构。虽然已有研究探索了GWT的多模态表示能力，但其注意力机制仍研究不足，需要开发有效的注意力选择机制来提升系统性能。

Method: 提出了一种顶部注意力机制，用于在全局工作空间中选择相关模态。该方法在两个复杂度递增的多模态数据集（Simple Shapes和MM-IMDb 1.0）上进行评估，并与现有多模态注意力模型进行比较。

Result: 1）注意力机制提高了全局工作空间系统的噪声鲁棒性；2）展示了文献中多模态注意力模型不具备的跨任务和跨模态泛化能力；3）在MM-IMDb 1.0基准测试中，该注意力机制使全局工作空间达到最先进水平。

Conclusion: 提出的顶部注意力机制有效增强了全局工作空间理论在多模态集成中的性能，不仅提高了噪声鲁棒性，还展现了独特的泛化能力，使GWT在多模态基准测试中具有竞争力。

Abstract: Global Workspace Theory (GWT), inspired by cognitive neuroscience, posits that flexible cognition could arise via the attentional selection of a relevant subset of modalities within a multimodal integration system. This cognitive framework can inspire novel computational architectures for multimodal integration. Indeed, recent implementations of GWT have explored its multimodal representation capabilities, but the related attention mechanisms remain understudied. Here, we propose and evaluate a top-down attention mechanism to select modalities inside a global workspace. First, we demonstrate that our attention mechanism improves noise robustness of a global workspace system on two multimodal datasets of increasing complexity: Simple Shapes and MM-IMDb 1.0. Second, we highlight various cross-task and cross-modality generalization capabilities that are not shared by multimodal attention models from the literature. Comparing against existing baselines on the MM-IMDb 1.0 benchmark, we find our attention mechanism makes the global workspace competitive with the state of the art.

</details>


### [94] [OSCAR: Optimization-Steered Agentic Planning for Composed Image Retrieval](https://arxiv.org/abs/2602.08603)
*Teng Wang,Rong Shan,Jianghao Lin,Junjie Wu,Tianyi Xu,Jianping Zhang,Wenteng Chen,Changwang Zhang,Zhaoxiang Wang,Weinan Zhang,Jun Wang*

Main category: cs.AI

TL;DR: OSCAR：基于优化引导的智能体规划框架，将组合图像检索从启发式搜索转化为轨迹优化问题，通过离线-在线范式实现高效检索


<details>
  <summary>Details</summary>
Motivation: 现有组合图像检索方法存在两大问题：统一嵌入检索受限于单模型近视问题，启发式智能体检索则受限于次优的试错编排。需要更系统化的方法来处理视觉和文本约束的复杂推理。

Method: 提出OSCAR框架，采用离线-在线范式：离线阶段将CIR建模为两阶段混合整数规划问题，通过布尔集合运算推导最大化真实覆盖的最优轨迹；在线阶段使用这些轨迹作为上下文示例来引导VLM规划器进行推理。

Result: 在三个公共基准和一个私有工业基准上的实验表明，OSCAR始终优于现有SOTA基线。仅使用10%训练数据就能达到优越性能，证明了规划逻辑的强泛化能力而非数据集特定记忆。

Conclusion: OSCAR首次将智能体CIR从启发式搜索过程重新表述为原则性的轨迹优化问题，通过数学推导的最优轨迹实现了更有效的组合图像检索，展示了规划逻辑的强泛化能力。

Abstract: Composed image retrieval (CIR) requires complex reasoning over heterogeneous visual and textual constraints. Existing approaches largely fall into two paradigms: unified embedding retrieval, which suffers from single-model myopia, and heuristic agentic retrieval, which is limited by suboptimal, trial-and-error orchestration. To this end, we propose OSCAR, an optimization-steered agentic planning framework for composed image retrieval. We are the first to reformulate agentic CIR from a heuristic search process into a principled trajectory optimization problem. Instead of relying on heuristic trial-and-error exploration, OSCAR employs a novel offline-online paradigm. In the offline phase, we model CIR via atomic retrieval selection and composition as a two-stage mixed-integer programming problem, mathematically deriving optimal trajectories that maximize ground-truth coverage for training samples via rigorous boolean set operations. These trajectories are then stored in a golden library to serve as in-context demonstrations for online steering of VLM planner at online inference time. Extensive experiments on three public benchmarks and a private industrial benchmark show that OSCAR consistently outperforms SOTA baselines. Notably, it achieves superior performance using only 10% of training data, demonstrating strong generalization of planning logic rather than dataset-specific memorization.

</details>


### [95] [Debate is efficient with your time](https://arxiv.org/abs/2602.08630)
*Jonah Brown-Cohen,Geoffrey Irving,Simon C. Marshall,Ilan Newman,Georgios Piliouras,Mario Szegedy*

Main category: cs.AI

TL;DR: 论文分析了辩论式AI安全中的人类监督成本，发现PSPACE/poly问题仅需O(log n)次查询即可判定，证明辩论具有极高的查询效率。


<details>
  <summary>Details</summary>
Motivation: 先前工作建立了辩论在理论上能解决的问题，但未分析人类监督的实际成本——法官需要检查辩论记录中的多少查询。本文旨在量化辩论中的人类监督开销。

Method: 引入辩论查询复杂度(DQC)概念，定义为验证者正确判定辩论所需检查的最小比特数。通过理论分析，将PSPACE/poly类与O(log n)查询可判定的函数类建立等价关系。

Result: 发现PSPACE/poly恰好是O(log n)查询可判定的函数类，证明辩论具有极高的查询效率。还证明依赖所有输入比特的函数需要Ω(log n)查询，且规模为s的电路可计算的函数满足DQC(f) ≤ log(s) + 3。

Conclusion: 辩论在查询复杂度上非常高效，对数级监督足以处理高度复杂问题。证明DQC下界与电路复杂度中心问题相关，为连接辩论查询复杂度与电路下界提供了新视角。

Abstract: AI safety via debate uses two competing models to help a human judge verify complex computational tasks. Previous work has established what problems debate can solve in principle, but has not analysed the practical cost of human oversight: how many queries must the judge make to the debate transcript? We introduce Debate Query Complexity}(DQC), the minimum number of bits a verifier must inspect to correctly decide a debate.
  Surprisingly, we find that PSPACE/poly (the class of problems which debate can efficiently decide) is precisely the class of functions decidable with O(log n) queries. This characterisation shows that debate is remarkably query-efficient: even for highly complex problems, logarithmic oversight suffices. We also establish that functions depending on all their input bits require Omega(log n) queries, and that any function computable by a circuit of size s satisfies DQC(f) <= log(s) + 3. Interestingly, this last result implies that proving DQC lower bounds of log(n) + 6 for languages in P would yield new circuit lower bounds, connecting debate query complexity to central questions in circuit complexity.

</details>


### [96] [Intermediate Results on the Complexity of STRIPS$_{1}^{1}$](https://arxiv.org/abs/2602.08708)
*Stefan Edelkamp,Jiří Fink,Petr Gregor,Anders Jonsson,Bernhard Nebel*

Main category: cs.AI

TL;DR: 该论文研究了STRIPS规划中操作符仅有一个前提和一个效果时的计算复杂度问题，探讨了STRIPS¹₁的"小解假设"是否成立。


<details>
  <summary>Details</summary>
Motivation: Bylander的研究表明，即使操作符限制为两个前提和两个后置条件，命题STRIPS规划的存在性判定也是PSPACE完全的。然而，对于操作符只有一个前提和一个效果的情况，是否NP完全仍然未知。本文旨在探索这个STRIPS¹₁的"小解假设"是否成立。

Method: 通过调用SAT求解器处理小规模实例，引入字面图概念，并将其映射到Petri网进行分析。

Result: 论文为STRIPS¹₁的复杂度问题提供了新的见解，但具体结果需要查看完整论文内容。

Conclusion: 该研究为理解STRIPS规划中操作符复杂度与计算复杂性之间的关系提供了重要线索，特别是对于最简单的STRIPS变体的复杂度分类问题。

Abstract: This paper is based on Bylander's results on the computational complexity of propositional STRIPS planning. He showed that when only ground literals are permitted, determining plan existence is PSPACE-complete even if operators are limited to two preconditions and two postconditions. While NP-hardness is settled, it is unknown whether propositional STRIPS with operators that only have one precondition and one effect is NP-complete. We shed light on the question whether this small solution hypothesis for STRIPS$^1_1$ is true, calling a SAT solver for small instances, introducing the literal graph, and mapping it to Petri nets.

</details>


### [97] [Exploring SAIG Methods for an Objective Evaluation of XAI](https://arxiv.org/abs/2602.08715)
*Miquel Miró-Nicolau,Gabriel Moyà-Alcover,Anna Arias-Duart*

Main category: cs.AI

TL;DR: 本文首次系统回顾和分析了合成人工智能基准真值（SAIG）方法，提出了一种新的分类法，揭示了XAI评估领域缺乏共识的现状，强调需要进一步研究和标准化。


<details>
  <summary>Details</summary>
Motivation: 可解释人工智能（XAI）评估领域方法多样且复杂，与传统AI评估不同，XAI缺乏解释的普遍正确基准真值，使得客观评估具有挑战性。SAIG方法通过生成人工基准真值来直接评估XAI技术，为解决这一问题提供了有前景的方向。

Method: 本文对SAIG方法进行了首次系统回顾和分析，提出了一种新的分类法来对这些方法进行分类，识别了区分不同SAIG方法的七个关键特征，并进行了比较研究。

Result: 比较研究揭示了XAI评估技术有效性方面令人担忧的共识缺乏，表明当前评估方法存在不一致性，需要进一步研究和标准化。

Conclusion: SAIG方法为XAI评估提供了有前景的途径，但该领域需要更多的研究努力和标准化工作来建立可靠的评估框架，以推动XAI技术的健康发展。

Abstract: The evaluation of eXplainable Artificial Intelligence (XAI) methods is a rapidly growing field, characterized by a wide variety of approaches. This diversity highlights the complexity of the XAI evaluation, which, unlike traditional AI assessment, lacks a universally correct ground truth for the explanation, making objective evaluation challenging. One promising direction to address this issue involves the use of what we term Synthetic Artificial Intelligence Ground truth (SAIG) methods, which generate artificial ground truths to enable the direct evaluation of XAI techniques. This paper presents the first review and analysis of SAIG methods. We introduce a novel taxonomy to classify these approaches, identifying seven key features that distinguish different SAIG methods. Our comparative study reveals a concerning lack of consensus on the most effective XAI evaluation techniques, underscoring the need for further research and standardization in this area.

</details>


### [98] [Finite-State Controllers for (Hidden-Model) POMDPs using Deep Reinforcement Learning](https://arxiv.org/abs/2602.08734)
*David Hudák,Maris F. L. Galesloot,Martin Tappler,Martin Kurečka,Nils Jansen,Milan Češka*

Main category: cs.AI

TL;DR: Lexpop框架使用深度强化学习训练神经策略，然后提取有限状态控制器来提供性能保证，并扩展到处理隐藏模型POMDPs的鲁棒策略计算。


<details>
  <summary>Details</summary>
Motivation: 现有POMDP求解器的可扩展性有限，且许多场景需要跨多个POMDP的鲁棒策略，进一步加剧了可扩展性问题。

Method: 1) 使用深度强化学习训练循环神经网络策略；2) 通过高效提取方法构建模仿神经策略的有限状态控制器；3) 扩展到HM-POMDPs，将每个控制器与其最坏情况POMDP关联，迭代训练鲁棒神经策略并提取鲁棒控制器。

Result: 在大状态空间问题上，Lexpop在POMDPs和HM-POMDPs上都优于最先进的求解器。

Conclusion: Lexpop框架通过结合神经策略学习和控制器提取，为POMDPs和HM-POMDPs提供了可扩展且具有性能保证的解决方案。

Abstract: Solving partially observable Markov decision processes (POMDPs) requires computing policies under imperfect state information. Despite recent advances, the scalability of existing POMDP solvers remains limited. Moreover, many settings require a policy that is robust across multiple POMDPs, further aggravating the scalability issue. We propose the Lexpop framework for POMDP solving. Lexpop (1) employs deep reinforcement learning to train a neural policy, represented by a recurrent neural network, and (2) constructs a finite-state controller mimicking the neural policy through efficient extraction methods. Crucially, unlike neural policies, such controllers can be formally evaluated, providing performance guarantees. We extend Lexpop to compute robust policies for hidden-model POMDPs (HM-POMDPs), which describe finite sets of POMDPs. We associate every extracted controller with its worst-case POMDP. Using a set of such POMDPs, we iteratively train a robust neural policy and consequently extract a robust controller. Our experiments show that on problems with large state spaces, Lexpop outperforms state-of-the-art solvers for POMDPs as well as HM-POMDPs.

</details>


### [99] [Dynamics Within Latent Chain-of-Thought: An Empirical Study of Causal Structure](https://arxiv.org/abs/2602.08783)
*Zirui Li,Xuefeng Bai,Kehai Chen,Yizhi Li,Jian Yang,Chenghua Lin,Min Zhang*

Main category: cs.AI

TL;DR: 本文提出将潜在思维链视为表示空间中的可操纵因果过程，通过结构因果模型分析潜在步骤，研究其在数学和一般推理任务中的因果必要性、影响传播和答案模式保留。


<details>
  <summary>Details</summary>
Motivation: 现有潜在思维链方法使用内部潜在步骤替代显式文本推理，但这些中间计算难以通过相关性探测之外的方法进行评估。需要更系统的方法来分析潜在推理步骤的因果作用和功能。

Method: 将潜在思维链建模为结构因果模型中的变量，通过逐步do干预分析其效应。研究Coconut和CODI两种代表性范式在数学和一般推理任务上，探讨步骤的因果必要性、影响传播结构以及与显式思维链的比较。

Result: 发现潜在步骤预算不像同质额外深度，而更像具有非局部路由的分阶段功能；早期输出偏见与晚期表示承诺之间存在持续差距；中间轨迹保留竞争答案模式。

Conclusion: 结果支持模式条件和稳定性感知分析作为解释和改进潜在推理系统的更可靠工具，并提出了相应的训练/解码目标。

Abstract: Latent or continuous chain-of-thought methods replace explicit textual rationales with a number of internal latent steps, but these intermediate computations are difficult to evaluate beyond correlation-based probes. In this paper, we view latent chain-of-thought as a manipulable causal process in representation space by modeling latent steps as variables in a structural causal model (SCM) and analyzing their effects through step-wise $\mathrm{do}$-interventions. We study two representative paradigms (i.e., Coconut and CODI) on both mathematical and general reasoning tasks to investigate three key questions: (1) which steps are causally necessary for correctness and when answers become decidable early; (2) how does influence propagate across steps, and how does this structure compare to explicit CoT; and (3) do intermediate trajectories retain competing answer modes, and how does output-level commitment differ from representational commitment across steps. We find that latent-step budgets behave less like homogeneous extra depth and more like staged functionality with non-local routing, and we identify a persistent gap between early output bias and late representational commitment. These results motivate mode-conditional and stability-aware analyses -- and corresponding training/decoding objectives -- as more reliable tools for interpreting and improving latent reasoning systems.

</details>


### [100] [The Use of AI Tools to Develop and Validate Q-Matrices](https://arxiv.org/abs/2602.08796)
*Kevin Fan,Jacquelyn A. Bialo,Hongli Li*

Main category: cs.AI

TL;DR: AI工具（通用语言模型）在认知诊断建模中构建Q矩阵的可行性研究，发现AI生成的Q矩阵与验证矩阵一致性存在模型间差异，Gemini 2.5 Pro表现最佳甚至超过人类专家，但新版AI模型表现下降。


<details>
  <summary>Details</summary>
Motivation: Q矩阵构建是认知诊断建模的关键但劳动密集型步骤，研究旨在探索AI工具（通用语言模型）是否能支持Q矩阵开发，减轻人工负担。

Method: 使用多个AI模型（包括Google Gemini 2.5 Pro等）基于与人类专家相同的训练材料生成Q矩阵，与Li和Suen（2013）验证的阅读测试Q矩阵比较，使用Cohen's kappa评估一致性。2026年1月使用新版AI模型进行后续分析。

Result: AI模型间一致性差异显著，Google Gemini 2.5 Pro与验证Q矩阵一致性最高（Kappa=0.63），超过所有人类专家。但2026年使用新版AI模型的分析显示与验证Q矩阵的一致性降低。

Conclusion: AI工具在Q矩阵构建中具有潜力，特定模型表现优于人类专家，但AI模型版本更新可能影响性能稳定性，需要进一步研究AI在认知诊断建模中的可靠应用。

Abstract: Constructing a Q-matrix is a critical but labor-intensive step in cognitive diagnostic modeling (CDM). This study investigates whether AI tools (i.e., general language models) can support Q-matrix development by comparing AI-generated Q-matrices with a validated Q-matrix from Li and Suen (2013) for a reading comprehension test. In May 2025, multiple AI models were provided with the same training materials as human experts. Agreement among AI-generated Q-matrices, the validated Q-matrix, and human raters' Q-matrices was assessed using Cohen's kappa. Results showed substantial variation across AI models, with Google Gemini 2.5 Pro achieving the highest agreement (Kappa = 0.63) with the validated Q-matrix, exceeding that of all human experts. A follow-up analysis in January 2026 using newer AI versions, however, revealed lower agreement with the validated Q-matrix. Implications and directions for future research are discussed.

</details>


### [101] [Root Cause Analysis Method Based on Large Language Models with Residual Connection Structures](https://arxiv.org/abs/2602.08804)
*Liming Zhou,Ailing Liu,Hongwei Liu,Min He,Heng Zhang*

Main category: cs.AI

TL;DR: 提出RC-LLM方法，利用残差连接结构和大语言模型进行微服务架构中的根因定位，有效处理多源遥测数据并建模因果依赖关系。


<details>
  <summary>Details</summary>
Motivation: 在复杂大规模微服务架构中，根因定位面临挑战：微服务间复杂的故障传播以及遥测数据（指标、日志、追踪）的高维性限制了现有RCA方法的有效性。

Method: 提出RC-LLM方法：设计残差式分层融合结构整合多源遥测数据，利用大语言模型的上下文推理能力建模时间和跨微服务的因果依赖关系。

Result: 在CCF-AIOps微服务数据集上的实验结果表明，RC-LLM在根因分析中实现了强大的准确性和效率。

Conclusion: RC-LLM通过结合残差连接结构和LLM的推理能力，有效解决了微服务架构中根因定位的挑战，为复杂系统中的故障诊断提供了新方法。

Abstract: Root cause localization remain challenging in complex and large-scale microservice architectures. The complex fault propagation among microservices and the high dimensionality of telemetry data, including metrics, logs, and traces, limit the effectiveness of existing root cause analysis (RCA) methods. In this paper, a residual-connection-based RCA method using large language model (LLM), named RC-LLM, is proposed. A residual-like hierarchical fusion structure is designed to integrate multi-source telemetry data, while the contextual reasoning capability of large language models is leveraged to model temporal and cross-microservice causal dependencies. Experimental results on CCF-AIOps microservice datasets demonstrate that RC-LLM achieves strong accuracy and efficiency in root cause analysis.

</details>


### [102] [Negative-Aware Diffusion Process for Temporal Knowledge Graph Extrapolation](https://arxiv.org/abs/2602.08815)
*Yanglei Gan,Peng He,Yuxiang Cai,Run Lin,Guanyu Zhou,Qiao Liu*

Main category: cs.AI

TL;DR: NADEx是一个用于时序知识图谱推理的负感知扩散模型，通过结合负样本信息和余弦对齐正则化，提升未来事实预测的准确性和校准性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的时序知识图谱推理方法存在两个问题：1) 生成路径仅依赖正样本证据，忽略了信息丰富的负样本上下文；2) 训练目标主要基于交叉熵排序，虽然改善了候选排序但缺乏对去噪嵌入校准的监督。

Method: NADEx编码实体、关系和时序间隔的主体中心历史为序列嵌入，在前向过程中扰动查询对象，在反向过程中使用基于时序关系上下文的Transformer去噪器重建。此外，从批量负样本原型推导出余弦对齐正则化器，收紧决策边界以排除不合理候选。

Result: 在四个公开时序知识图谱基准测试上的综合实验表明，NADEx实现了最先进的性能。

Conclusion: NADEx通过整合负样本信息和引入校准监督，有效提升了时序知识图谱推理的准确性和鲁棒性，为扩散模型在该领域的应用提供了新思路。

Abstract: Temporal Knowledge Graph (TKG) reasoning seeks to predict future missing facts from historical evidence. While diffusion models (DM) have recently gained attention for their ability to capture complex predictive distributions, two gaps remain: (i) the generative path is conditioned only on positive evidence, overlooking informative negative context, and (ii) training objectives are dominated by cross-entropy ranking, which improves candidate ordering but provides little supervision over the calibration of the denoised embedding. To bridge this gap, we introduce Negative-Aware Diffusion model for TKG Extrapolation (NADEx). Specifically, NADEx encodes subject-centric histories of entities, relations and temporal intervals into sequential embeddings. NADEx perturbs the query object in the forward process and reconstructs it in reverse with a Transformer denoiser conditioned on the temporal-relational context. We further derive a cosine-alignment regularizer derived from batch-wise negative prototypes, which tightens the decision boundary against implausible candidates. Comprehensive experiments on four public TKG benchmarks demonstrate that NADEx delivers state-of-the-art performance.

</details>


### [103] [Deciding the Satisfiability of Combined Qualitative Constraint Networks](https://arxiv.org/abs/2602.08848)
*Quentin Cohen-Solal,Alexandre Niveau,Maroua Bouzid*

Main category: cs.AI

TL;DR: 提出一个统一框架，整合多种定性推理的扩展与组合形式，包括多尺度推理、时间序列和松散集成，并研究其可满足性决策及复杂性。


<details>
  <summary>Details</summary>
Motivation: 定性推理能在不精确、不完整且无数值信息的情况下推断新知识，但现有文献中的定义排除了某些在组合场景中重要的定性形式化方法，需要统一框架来整合多种扩展与组合形式。

Method: 提出一个形式化框架，统一多种定性形式化的扩展与组合，包括多尺度推理、时间序列和松散集成。该框架支持在这些组合和扩展中进行推理，并以统一方式研究可满足性决策及其复杂性。

Result: 建立了两个互补定理，保证可满足性决策是多项式时间的，并用它们恢复了已知的尺寸-拓扑组合结果。还将定性形式化的主要定义推广到包含文献定义中排除但在组合场景中重要的定性形式化方法。

Conclusion: 提出的统一框架成功整合了多种定性推理的扩展与组合形式，不仅支持在这些组合中进行推理，还提供了研究可满足性决策复杂性的统一方法，扩展了定性形式化的定义范围。

Abstract: Among the various forms of reasoning studied in the context of artificial intelligence, qualitative reasoning makes it possible to infer new knowledge in the context of imprecise, incomplete information without numerical values. In this paper, we propose a formal framework unifying several forms of extensions and combinations of qualitative formalisms, including multi-scale reasoning, temporal sequences, and loose integrations. This framework makes it possible to reason in the context of each of these combinations and extensions, but also to study in a unified way the satisfiability decision and its complexity. In particular, we establish two complementary theorems guaranteeing that the satisfiability decision is polynomial, and we use them to recover the known results of the size-topology combination. We also generalize the main definition of qualitative formalism to include qualitative formalisms excluded from the definitions of the literature, important in the context of combinations.

</details>


### [104] [Scalable Delphi: Large Language Models for Structured Risk Estimation](https://arxiv.org/abs/2602.08889)
*Tobias Lorenz,Mario Fritz*

Main category: cs.AI

TL;DR: LLM-based Scalable Delphi方法可将传统专家德尔菲法的时间从数月缩短到数分钟，在AI增强网络安全风险评估中表现出与人类专家高度一致的结果。


<details>
  <summary>Details</summary>
Motivation: 传统德尔菲法作为专家评估的金标准需要数月协调和专家时间，使严谨的风险评估难以普及。需要探索LLM能否作为可扩展的专家评估代理。

Method: 提出Scalable Delphi方法，将经典德尔菲协议适配到LLM：使用多样化专家角色、迭代精炼和理由共享。开发基于必要条件的评估框架：校准可验证代理、对证据的敏感性、与人类专家判断的一致性。

Result: 在AI增强网络安全风险评估中，LLM专家小组与基准真实值强相关（Pearson r=0.87-0.95），随着证据添加系统性改进，并与人类专家小组一致——在某些比较中，LLM小组与人类小组的接近度甚至超过两个人类小组之间的接近度。

Conclusion: LLM为基础的专家评估可以将结构化专家判断扩展到传统方法不可行的场景，将评估时间从数月缩短到数分钟，为高风险领域的量化风险评估提供可扩展解决方案。

Abstract: Quantitative risk assessment in high-stakes domains relies on structured expert elicitation to estimate unobservable properties. The gold standard - the Delphi method - produces calibrated, auditable judgments but requires months of coordination and specialist time, placing rigorous risk assessment out of reach for most applications. We investigate whether Large Language Models (LLMs) can serve as scalable proxies for structured expert elicitation. We propose Scalable Delphi, adapting the classical protocol for LLMs with diverse expert personas, iterative refinement, and rationale sharing. Because target quantities are typically unobservable, we develop an evaluation framework based on necessary conditions: calibration against verifiable proxies, sensitivity to evidence, and alignment with human expert judgment. We evaluate in the domain of AI-augmented cybersecurity risk, using three capability benchmarks and independent human elicitation studies. LLM panels achieve strong correlations with benchmark ground truth (Pearson r=0.87-0.95), improve systematically as evidence is added, and align with human expert panels - in one comparison, closer to a human panel than the two human panels are to each other. This demonstrates that LLM-based elicitation can extend structured expert judgment to settings where traditional methods are infeasible, reducing elicitation time from months to minutes.

</details>


### [105] [Efficient and Stable Reinforcement Learning for Diffusion Language Models](https://arxiv.org/abs/2602.08905)
*Jiawei Liu,Xiting Wang,Yuanyuan Zhong,Defu Lian,Yu Yang*

Main category: cs.AI

TL;DR: STP框架通过时空剪枝提高扩散大语言模型强化学习的效率和稳定性


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型的强化学习面临效率和稳定性挑战，需要专门解决方案

Method: 提出时空剪枝框架：空间剪枝利用静态先验约束探索空间，时间剪枝绕过冗余后期细化步骤

Result: 理论分析显示STP严格降低对数似然估计方差，实验证明在效率和准确性上超越现有基线

Conclusion: STP有效解决了dLLMs强化学习的效率和稳定性问题，为复杂推理能力开发提供支持

Abstract: Reinforcement Learning (RL) is crucial for unlocking the complex reasoning capabilities of Diffusion-based Large Language Models (dLLMs). However, applying RL to dLLMs faces unique challenges in efficiency and stability. To address these challenges, we propose Spatio-Temporal Pruning (STP), a framework designed to simultaneously improve the efficiency and stability of RL for dLLMs. STP compresses the redundancy in the generative process through: (1) \textit{spatial pruning}, which constrains the exploration space using static priors; and (2) \textit{temporal pruning}, which bypasses redundant late-stage refinement steps. Our theoretical analysis demonstrates that STP strictly reduces the variance of the log-likelihood estimation, thereby ensuring more stable policy updates. Extensive experiments demonstrate that STP surpasses state-of-the-art baselines in both efficiency and accuracy. Our code is available at https://github.com/Lolo1222/STP.

</details>


### [106] [CausalT5K: Diagnosing and Informing Refusal for Trustworthy Causal Reasoning of Skepticism, Sycophancy, Detection-Correction, and Rung Collapse](https://arxiv.org/abs/2602.08939)
*Longling Geng,Andy Ouyang,Theodore Wu,Daphne Barretto,Matthew John Hayes,Rachael Cooper,Yuqiao Zeng,Sameer Vijay,Gia Ancone,Ankit Rai,Matthew Wolfman,Patrick Flanagan,Edward Y. Chang*

Main category: cs.AI

TL;DR: CausalT5K是一个包含5000多个案例的诊断基准，用于系统检测LLM在因果推理中的失败模式（如阶梯坍塌、谄媚漂移、错误拒绝），通过现实叙事中的因果陷阱来评估模型的实用性和安全性。


<details>
  <summary>Details</summary>
Motivation: LLM在因果推理中存在多种失败模式（谄媚、阶梯坍塌、错误校准的拒绝），但由于缺乏系统性诊断基准，修复进展缓慢。需要能够揭示这些失败模式的诊断工具来推进可信推理系统的发展。

Method: 构建包含5000多个案例的CausalT5K基准，涵盖10个领域，通过人机协作流程（40名领域专家参与、迭代交叉验证周期）开发，使用基于规则、LLM和人工评分的复合验证方法，将Pearl的因果阶梯理论作为研究基础设施实现。

Result: 初步实验揭示了"四象限控制景观"，表明静态审计策略普遍失败。基准能够将性能分解为实用性（敏感性）和安全性（特异性），揭示聚合准确率无法看到的失败模式。

Conclusion: CausalT5K作为研究基础设施，能够系统诊断LLM因果推理失败模式，为推进可信推理系统提供有价值的工具，特别是通过揭示静态审计策略的局限性来指导更好的模型评估和开发。

Abstract: LLM failures in causal reasoning, including sycophancy, rung collapse, and miscalibrated refusal, are well-documented, yet progress on remediation is slow because no benchmark enables systematic diagnosis. We introduce CausalT5K, a diagnostic benchmark of over 5,000 cases across 10 domains that tests three critical capabilities: (1) detecting rung collapse, where models answer interventional queries with associational evidence; (2) resisting sycophantic drift under adversarial pressure; and (3) generating Wise Refusals that specify missing information when evidence is underdetermined. Unlike synthetic benchmarks, CausalT5K embeds causal traps in realistic narratives and decomposes performance into Utility (sensitivity) and Safety (specificity), revealing failure modes invisible to aggregate accuracy. Developed through a rigorous human-machine collaborative pipeline involving 40 domain experts, iterative cross-validation cycles, and composite verification via rule-based, LLM, and human scoring, CausalT5K implements Pearl's Ladder of Causation as research infrastructure. Preliminary experiments reveal a Four-Quadrant Control Landscape where static audit policies universally fail, a finding that demonstrates CausalT5K's value for advancing trustworthy reasoning systems. Repository: https://github.com/genglongling/CausalT5kBench

</details>


### [107] [CoRefine: Confidence-Guided Self-Refinement for Adaptive Test-Time Compute](https://arxiv.org/abs/2602.08948)
*Chen Jin,Ryutaro Tanno,Tom Diethe,Philip Teare*

Main category: cs.AI

TL;DR: CoRefine是一种基于置信度引导的自优化方法，通过轻量级控制器实现高效推理，大幅减少计算开销


<details>
  <summary>Details</summary>
Motivation: 大型语言模型通常依赖并行解码（如512个样本）来提高推理准确性，但这会带来巨大的计算开销。需要一种更高效的方法来减少计算成本同时保持竞争力

Method: 引入CoRefine方法，使用一个轻量级的211k参数Conv1D控制器，基于完整轨迹置信度决定停止、重新检查或尝试不同方法，实现有针对性的自我修正

Result: 平均每个问题只需2.7个优化步骤，相对于512样本基线减少约190倍token使用；控制器在自信停止时达到92.6%的精确度；CoRefine-Tree变体能自适应平衡探索与利用

Conclusion: 通过将置信度视为控制信号而非正确性保证，CoRefine为可扩展推理和具有不完美验证器的智能体设置提供了模块化原语

Abstract: Large Language Models (LLMs) often rely on test-time scaling via parallel decoding (for example, 512 samples) to boost reasoning accuracy, but this incurs substantial compute. We introduce CoRefine, a confidence-guided self-refinement method that achieves competitive accuracy using a fraction of the tokens via a lightweight 211k-parameter Conv1D controller atop a frozen LLM. The controller consumes full-trace confidence to decide whether to halt, re-examine, or try a different approach, enabling targeted self-correction with an average of 2.7 refinement steps per problem and roughly 190-fold token reduction relative to 512-sample baselines. Across diverse reasoning benchmarks and three open-source models, the controller achieves 92.6 percent precision when it confidently halts, indicating that confidence dynamics reliably signal correctness without ground-truth verification. We extend this to CoRefine-Tree, a hybrid sequential-parallel variant that adaptively balances exploration and exploitation, with easy serving integration and verifier compatibility. By treating confidence as a control signal rather than a correctness guarantee, CoRefine provides a modular primitive for scalable reasoning and agentic settings with imperfect verifiers.

</details>


### [108] [Digital Twin and Agentic AI for Wild Fire Disaster Management: Intelligent Virtual Situation Room](https://arxiv.org/abs/2602.08949)
*Mohammad Morsali,Siavash H. Khajavi*

Main category: cs.AI

TL;DR: IVSR是一个结合数字孪生和自主AI代理的双向平台，用于实时、自适应的野火灾害管理，显著降低检测到干预的延迟并提高资源协调效率。


<details>
  <summary>Details</summary>
Motivation: 联合国预测到2030年和2050年野火频率和强度将分别增加14%和30%，传统灾害管理框架依赖静态模拟和被动数据采集，无法实时适应不断演变的野火情况。

Method: 开发智能虚拟态势室(IVSR)，这是一个由自主AI代理增强的双向数字孪生平台。系统持续摄入多源传感器图像、天气数据和3D森林模型，创建火灾环境的实时虚拟副本。AI驱动的相似性引擎将新兴条件与预计算的灾害模拟库对齐，检索并校准干预策略。

Result: 通过工业合作伙伴提供的详细案例研究模拟验证IVSR，展示了在局部事件检测、隐私保护回放、基于碰撞器的火势蔓延预测和特定站点ML再训练方面的能力。与传统系统相比，检测到干预的延迟显著降低，资源协调更有效。

Conclusion: IVSR通过将实时双向数字孪生与代理AI相结合，为主动、自适应的野火灾害管理提供了一个可扩展、半自动化的决策支持范式。

Abstract: According to the United Nations, wildfire frequency and intensity are projected to increase by approximately 14% by 2030 and 30% by 2050 due to global warming, posing critical threats to life, infrastructure, and ecosystems. Conventional disaster management frameworks rely on static simulations and passive data acquisition, hindering their ability to adapt to arbitrarily evolving wildfire episodes in real-time. To address these limitations, we introduce the Intelligent Virtual Situation Room (IVSR), a bidirectional Digital Twin (DT) platform augmented by autonomous AI agents. The IVSR continuously ingests multisource sensor imagery, weather data, and 3D forest models to create a live virtual replica of the fire environment. A similarity engine powered by AI aligns emerging conditions with a precomputed Disaster Simulation Library, retrieving and calibrating intervention tactics under the watchful eyes of experts. Authorized action-ranging from UAV redeployment to crew reallocation-is cycled back through standardized procedures to the physical layer, completing the loop between response and analysis. We validate IVSR through detailed case-study simulations provided by an industrial partner, demonstrating capabilities in localized incident detection, privacy-preserving playback, collider-based fire-spread projection, and site-specific ML retraining. Our results indicate marked reductions in detection-to-intervention latency and more effective resource coordination versus traditional systems. By uniting real-time bidirectional DTs with agentic AI, IVSR offers a scalable, semi-automated decision-support paradigm for proactive, adaptive wildfire disaster management.

</details>


### [109] [stable-worldmodel-v1: Reproducible World Modeling Research and Evaluation](https://arxiv.org/abs/2602.08968)
*Lucas Maes,Quentin Le Lidec,Dan Haramati,Nassim Massaudi,Damien Scieur,Yann LeCun,Randall Balestriero*

Main category: cs.AI

TL;DR: 提出了stable-worldmodel (SWM) - 一个模块化、经过测试和文档化的世界模型研究生态系统，旨在解决现有世界模型实现复用性差、易出错和评估标准化不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大多数世界模型实现都是针对特定论文的，这严重限制了它们的可复用性，增加了bug风险，并降低了评估标准化程度。需要建立一个统一的研究生态系统来解决这些问题。

Method: 开发了stable-worldmodel (SWM)生态系统，包含高效的数据收集工具、标准化环境、规划算法和基线实现。每个环境都支持可控的变化因素（包括视觉和物理属性），以支持鲁棒性和持续学习研究。

Result: 成功构建了SWM生态系统，并通过使用它研究DINO-WM中的零样本鲁棒性来展示其实用性。

Conclusion: SWM为世界模型研究提供了一个模块化、可复用且标准化的平台，有助于促进该领域的稳健发展和比较评估。

Abstract: World Models have emerged as a powerful paradigm for learning compact, predictive representations of environment dynamics, enabling agents to reason, plan, and generalize beyond direct experience. Despite recent interest in World Models, most available implementations remain publication-specific, severely limiting their reusability, increasing the risk of bugs, and reducing evaluation standardization. To mitigate these issues, we introduce stable-worldmodel (SWM), a modular, tested, and documented world-model research ecosystem that provides efficient data-collection tools, standardized environments, planning algorithms, and baseline implementations. In addition, each environment in SWM enables controllable factors of variation, including visual and physical properties, to support robustness and continual learning research. Finally, we demonstrate the utility of SWM by using it to study zero-shot robustness in DINO-WM.

</details>


### [110] [InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery](https://arxiv.org/abs/2602.08990)
*Shiyang Feng,Runmin Ma,Xiangchao Yan,Yue Fan,Yusong Hu,Songtao Huang,Shuaiyu Zhang,Zongsheng Cao,Tianshuo Peng,Jiakang Yuan,Zijie Guo,Zhijie Zhong,Shangheng Du,Weida Wang,Jinxin Shi,Yuhao Zhou,Xiaohan He,Zhiyin Yu,Fangchen Yu,Qihao Zheng,Jiamin Wu,Mianxin Liu,Chi Zhang,Shaowei Hou,Shuya Li,Yankai Jiang,Wenjie Lou,Lilong Wang,Zifu Wang,Jiong Wang,Wanghan Xu,Yue Deng,Dongrui Liu,Yiheng Wang,Wenlong Zhang,Fenghua Ling,Shufei Zhang,Xiaosong Wang,Shuangjia Zheng,Xun Huang,Siqi Sun,Shuyue Hu,Peng Ye,Chunfeng Song,Bin Wang,Conghui He,Yihao Liu,Xin Li,Qibin Hou,Tao Chen,Xiangyu Yue,Bin Wang,Liang He,Dahua Lin,Bowen Zhou,Bo Zhang,Lei Bai*

Main category: cs.AI

TL;DR: InternAgent-1.5是一个用于端到端科学发现的统一系统，通过生成、验证、演化三个子系统协调工作，在计算和实验领域实现自主科学发现。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够在计算建模和实验室实验之间协调工作的统一系统，实现跨领域的端到端自主科学发现，解决传统方法在长周期发现任务中的局限性。

Method: 采用结构化架构，包含生成、验证、演化三个协调子系统，支持深度研究、解决方案优化和长周期记忆等基础能力，能够在扩展发现周期中持续运行。

Result: 在GAIA、HLE、GPQA、FrontierScience等科学推理基准测试中取得领先性能；在算法发现任务中自主设计机器学习方法；在实验发现任务中执行完整计算或湿实验，在地球、生命、生物、物理等领域产生科学发现。

Conclusion: InternAgent-1.5为自主科学发现提供了一个通用且可扩展的框架，能够协调计算建模和实验室实验，实现跨领域的端到端科学发现。

Abstract: We introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery across computational and empirical domains. The system is built on a structured architecture composed of three coordinated subsystems for generation, verification, and evolution. These subsystems are supported by foundational capabilities for deep research, solution optimization, and long horizon memory. The architecture allows InternAgent-1.5 to operate continuously across extended discovery cycles while maintaining coherent and improving behavior. It also enables the system to coordinate computational modeling and laboratory experimentation within a single unified system. We evaluate InternAgent-1.5 on scientific reasoning benchmarks such as GAIA, HLE, GPQA, and FrontierScience, and the system achieves leading performance that demonstrates strong foundational capabilities. Beyond these benchmarks, we further assess two categories of discovery tasks. In algorithm discovery tasks, InternAgent-1.5 autonomously designs competitive methods for core machine learning problems. In empirical discovery tasks, it executes complete computational or wet lab experiments and produces scientific findings in earth, life, biological, and physical domains. Overall, these results show that InternAgent-1.5 provides a general and scalable framework for autonomous scientific discovery.

</details>


### [111] [iGRPO: Self-Feedback-Driven LLM Reasoning](https://arxiv.org/abs/2602.09000)
*Ali Hatamizadeh,Shrimai Prabhumoye,Igor Gitman,Ximing Lu,Seungju Han,Wei Ping,Yejin Choi,Jan Kautz*

Main category: cs.AI

TL;DR: iGRPO：一种两阶段强化学习方法，通过模型生成的草稿进行动态自条件化，提升数学推理能力


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在解决复杂数学问题方面显示出潜力，但仍难以产生准确一致的解决方案。强化学习可以对齐任务特定奖励，但现有方法如PPO和GRPO仍有改进空间。

Method: iGRPO是GRPO的两阶段扩展：第一阶段采样多个探索性草稿并选择最高奖励草稿；第二阶段将最佳草稿附加到原始提示，在草稿条件化改进上应用GRPO式更新，训练策略超越先前最佳尝试。

Result: 在匹配的rollout预算下，iGRPO在多个基础模型上持续优于GRPO。应用于OpenReasoning-Nemotron-7B在AceReason-Math训练后，在AIME24和AIME25上分别达到85.62%和79.64%的新SOTA结果。

Conclusion: 迭代的、基于自反馈的强化学习在推进可验证数学推理方面具有巨大潜力，iGRPO通过动态自条件化有效提升了模型性能。

Abstract: Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability. Group Relative Policy Optimization (GRPO) is an efficient, value-function-free alternative to Proximal Policy Optimization (PPO) that leverages group-relative reward normalization. We introduce Iterative Group Relative Policy Optimization (iGRPO), a two-stage extension of GRPO that adds dynamic self-conditioning through model-generated drafts. In Stage 1, iGRPO samples multiple exploratory drafts and selects the highest-reward draft using the same scalar reward signal used for optimization. In Stage 2, it appends this best draft to the original prompt and applies a GRPO-style update on draft-conditioned refinements, training the policy to improve beyond its strongest prior attempt. Under matched rollout budgets, iGRPO consistently outperforms GRPO across base models (e.g., Nemotron-H-8B-Base-8K and DeepSeek-R1 Distilled), validating its effectiveness on diverse reasoning benchmarks. Moreover, applying iGRPO to OpenReasoning-Nemotron-7B trained on AceReason-Math achieves new state-of-the-art results of 85.62\% and 79.64\% on AIME24 and AIME25, respectively. Ablations further show that the refinement wrapper generalizes beyond GRPO variants, benefits from a generative judge, and alters learning dynamics by delaying entropy collapse. These results underscore the potential of iterative, self-feedback-based RL for advancing verifiable mathematical reasoning.

</details>


### [112] [Data Science and Technology Towards AGI Part I: Tiered Data Management](https://arxiv.org/abs/2602.09003)
*Yudong Wang,Zixuan Fu,Hengyu Zhao,Chen Zhao,Chuyue Zhou,Xinle Lin,Hongya Lyu,Shuaikang Xue,Yi Yi,Yingjiao Wang,Zhi Zheng,Yuzhou Zhang,Jie Zhou,Chaojun Xiao,Xu Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

TL;DR: 提出L0-L4分层数据管理框架，通过数据与模型协同进化提升LLM训练效率，实验证明分层数据利用显著改善训练效果。


<details>
  <summary>Details</summary>
Motivation: 当前LLM研究过度依赖数据规模单向扩展，面临数据可用性、获取成本和训练效率瓶颈。认为AGI发展进入数据-模型协同进化新阶段，需要更智能的数据管理方法。

Method: 提出L0-L4分层数据管理框架：从原始未整理资源到组织化可验证知识。利用LLM进行质量评分和内容编辑等数据管理，各层级有不同数据特性、管理策略和训练角色，支持预训练、中期训练和对齐等全生命周期。

Result: 通过实证研究验证框架有效性，从原始语料构建分层数据集并在多个训练阶段使用。实验结果显示分层数据利用显著提高训练效率和模型性能。

Conclusion: 分层数据管理框架平衡数据质量、获取成本和边际训练收益，为可扩展和可持续的数据管理提供系统方法，促进数据与模型协同进化。

Abstract: The development of artificial intelligence can be viewed as an evolution of data-driven learning paradigms, with successive shifts in data organization and utilization continuously driving advances in model capability. Current LLM research is dominated by a paradigm that relies heavily on unidirectional scaling of data size, increasingly encountering bottlenecks in data availability, acquisition cost, and training efficiency. In this work, we argue that the development of AGI is entering a new phase of data-model co-evolution, in which models actively guide data management while high-quality data, in turn, amplifies model capabilities. To implement this vision, we propose a tiered data management framework, designed to support the full LLM training lifecycle across heterogeneous learning objectives and cost constraints. Specifically, we introduce an L0-L4 tiered data management framework, ranging from raw uncurated resources to organized and verifiable knowledge. Importantly, LLMs are fully used in data management processes, such as quality scoring and content editing, to refine data across tiers. Each tier is characterized by distinct data properties, management strategies, and training roles, enabling data to be strategically allocated across LLM training stages, including pre-training, mid-training, and alignment. The framework balances data quality, acquisition cost, and marginal training benefit, providing a systematic approach to scalable and sustainable data management. We validate the effectiveness of the proposed framework through empirical studies, in which tiered datasets are constructed from raw corpora and used across multiple training phases. Experimental results demonstrate that tier-aware data utilization significantly improves training efficiency and model performance. To facilitate further research, we release our tiered datasets and processing tools to the community.

</details>


### [113] [GEBench: Benchmarking Image Generation Models as GUI Environments](https://arxiv.org/abs/2602.09007)
*Haodong Li,Jingwei Wu,Quan Sun,Guopeng Li,Juanxi Tian,Huanyu Zhang,Yanlin Lai,Ruichuan An,Hongbo Peng,Yuhong Dai,Chenxi Li,Chunmei Qing,Jia Wang,Ziyang Meng,Zheng Ge,Xiangyu Zhang,Daxin Jiang*

Main category: cs.AI

TL;DR: 提出了GEBench基准测试，用于评估GUI生成中的动态交互和时序一致性，包含700个样本和GE-Score五维评估指标，发现现有模型在长序列交互中保持时序一致性和空间定位方面存在困难。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成模型能够根据用户指令预测未来GUI状态，但现有基准主要关注通用领域的视觉保真度，缺乏对GUI特定场景中状态转换和时序一致性的评估。

Method: 引入GEBench基准测试，包含700个精心策划的样本，涵盖5个任务类别，包括单步交互和多步轨迹。提出GE-Score五维评估指标：目标达成、交互逻辑、内容一致性、UI合理性和视觉质量。

Result: 对当前模型的广泛评估显示，它们在单步转换上表现良好，但在保持长交互序列的时序一致性和空间定位方面存在显著困难。图标解释、文本渲染和定位精度是关键瓶颈。

Conclusion: 这项工作为系统评估提供了基础，并为未来构建高保真生成GUI环境的研究指明了有前景的方向。

Abstract: Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, a novel five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [114] [SocialPulse: An Open-Source Subreddit Sensemaking Toolkit](https://arxiv.org/abs/2602.07248)
*Stephanie Birkelbach,Maria Teleki,Peter Carragher,Xiangjue Dong,Nehul Bhatnagar,James Caverlee*

Main category: cs.SI

TL;DR: SocialPulse是一个开源的Reddit社区理解工具包，整合了主题建模、情感分析、用户活动特征分析和机器人检测等多种分析方法，提供交互式系统帮助研究者分析在线社区讨论。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模话语分析工具通常是闭源的、难以适应或仅限于单一分析视图，无法满足在线社区复杂社会议题讨论的分析需求。

Method: 开发了一个开源工具包，统一整合了主题建模、情感分析、用户活动特征分析和机器人检测等多种互补分析方法，构建了交互式系统，支持用户在聚合趋势和细粒度内容之间流畅切换。

Result: SocialPulse能够帮助用户比较高活跃度和长尾贡献者，检查跨subreddit的话语时间变化，快速发现大型Reddit数据集中的主题、参与模式和新兴动态。

Conclusion: 通过提供可扩展和开放可用的平台，SocialPulse为在线社区话语的透明、可重复理解提供了实用且可重用的基础。

Abstract: Understanding how online communities discuss and make sense of complex social issues is a central challenge in social media research, yet existing tools for large-scale discourse analysis are often closed-source, difficult to adapt, or limited to single analytical views. We present SocialPulse, an open-source subreddit sensemaking toolkit that unifies multiple complementary analyses -- topic modeling, sentiment analysis, user activity characterization, and bot detection -- within a single interactive system. SocialPulse enables users to fluidly move between aggregate trends and fine-grained content, compare highly active and long-tail contributors, and examine temporal shifts in discourse across subreddits. The demo showcases end-to-end exploratory workflows that allow researchers and practitioners to rapidly surface themes, participation patterns, and emerging dynamics in large Reddit datasets. By offering an extensible and openly available platform, SocialPulse provides a practical and reusable foundation for transparent, reproducible sensemaking of online community discourse.

</details>


### [115] [Graph Domain Adaptation via Homophily-Agnostic Reconstructing Structure](https://arxiv.org/abs/2602.07573)
*Ruiyi Fang,Shuo Wang,Ruizhi Pu,Qiuhao Zeng,Hao Zheng,Ziyan Wang,Jiale Cai,Zhimin Mei,Song Tang,Charles Ling,Boyu Wang*

Main category: cs.SI

TL;DR: 提出了一种新的图域自适应方法，能够处理同质性和异质性不同的图，通过分治策略分别重建同质和异质图变体，并在对应变体间进行知识对齐。


<details>
  <summary>Details</summary>
Motivation: 现有图域自适应方法通常假设源图和目标图都表现出同质性，在异质性存在时性能较差。同时，目标图缺乏标签使得无法预先评估其同质性水平。

Method: 采用分治策略：1）分别重建源图和目标图的高度同质和异质变体；2）在对应的图变体之间分别进行知识对齐。这是一种同质性不可知的方法。

Result: 在五个基准数据集上的广泛实验表明，该方法具有优越性能，特别是在异质图上显示出显著优势。

Conclusion: 提出的同质性不可知方法能够有效处理不同同质性程度的图之间的知识迁移，解决了现有方法在异质图上的性能瓶颈问题。

Abstract: Graph Domain Adaptation (GDA) transfers knowledge from labeled source graphs to unlabeled target graphs, addressing the challenge of label scarcity. However, existing GDA methods typically assume that both source and target graphs exhibit homophily, leading existing methods to perform poorly when heterophily is present. Furthermore, the lack of labels in the target graph makes it impossible to assess its homophily level beforehand. To address this challenge, we propose a novel homophily-agnostic approach that effectively transfers knowledge between graphs with varying degrees of homophily. Specifically, we adopt a divide-and-conquer strategy that first separately reconstructs highly homophilic and heterophilic variants of both the source and target graphs, and then performs knowledge alignment separately between corresponding graph variants. Extensive experiments conducted on five benchmark datasets demonstrate the superior performance of our approach, particularly highlighting its substantial advantages on heterophilic graphs.

</details>


### [116] ["He gets to be the fun parent": Understanding and Supporting Burnt-Out Mothers in Online Communities](https://arxiv.org/abs/2602.07781)
*Nazanin Sabri,Ananya Malik,Bangzhao Shu,Jason Snyder,Laurie Kramer,Mai Elsherief*

Main category: cs.SI

TL;DR: 该研究通过分析Reddit上母亲们的倦怠讨论，识别了相关帖子主题、社区回应模式以及共同育儿中的支持不足问题。


<details>
  <summary>Details</summary>
Motivation: 母亲倦怠对母婴健康都有危害，但母亲们可能因羞耻感和污名化而选择在匿名平台分享经历。研究旨在了解母亲如何在Reddit上讨论倦怠体验。

Method: 首先通过人工标注Reddit帖子并训练机器学习模型来识别倦怠母亲的帖子；然后分析这些帖子（N=3,244）中讨论的问题（如求助需求、职业建议、共同育儿问题）；接着分析社区评论回应模式；最后特别探索共同育儿模式。

Result: 研究发现母亲们在帖子中主要讨论求助需求、职业建议和共同育儿问题；评论者经常分享个人生活经验并提供情感支持；共同育儿分析显示存在支持不足和期望不平等的问题。

Conclusion: 母亲们在匿名平台分享倦怠经历时主要寻求情感支持和实际问题建议，社区回应以经验分享和情感支持为主，共同育儿中的支持不足是母亲倦怠的重要因素。

Abstract: Maternal burnout is a psychological phenomena with documented harms to both mother and child, requiring prompt attention. Mothers experiencing burnout might choose to turn to online anonymous platforms, such as Reddit, to share their experience, due to feelings of shame and stigmatization of mental health issues. In this work, we study how mothers use Reddit to discuss their experiences of burnout. We first identify posts written by burnt out mothers by manually annotating Reddit posts and training machine learning models on them. Focusing on posts made by this population (N = 3,244), we then investigate the issues brought up by mothers, such as the need for help, career advice, and co-parenting issues. Additionally, we investigate how the Reddit community responds to these posts through the analysis of comments. We find that commenters frequently share personal lived experiences with the poster, and provide emotional support. Finally, considering co-parenting could be a mitigating factor for parental burnout, we explore co-pareting patterns experienced by burnt out mothers, finding evidence of lack of support for and unequal expectations from mothers.

</details>


### [117] [Towards Reliable Social A/B Testing: Spillover-Contained Clustering with Robust Post-Experiment Analysis](https://arxiv.org/abs/2602.08569)
*Xu Min,Zhaoxu Yang,Kaixuan Tan,Juan Yan,Xunbin Xiong,Zihao Zhu,Kaiyu Zhu,Fenglin Cui,Yang Yang,Sihua Yang,Jianhui Bu*

Main category: cs.SI

TL;DR: 提出一个两阶段的溢出控制实验框架，通过平衡Louvain算法生成稳定、大小均衡的集群来最小化跨集群边，并结合CUPAC估计器降低方差，在快手平台上验证了该方法能显著减少溢出效应并提高社交策略评估准确性。


<details>
  <summary>Details</summary>
Motivation: 在线社交产品中的网络干扰导致处理效应溢出到对照组，这会偏倚因果估计并破坏实验结论。现有方法存在局限性：用户级随机化忽略网络结构，而基于集群的方法通常依赖通用聚类算法，难以在无偏性和统计功效之间取得平衡。

Method: 提出两阶段溢出控制实验框架：1) 预实验阶段：构建社交互动图，引入平衡Louvain算法生成稳定、大小均衡的集群，同时最小化跨集群边；2) 后实验阶段：开发定制化的CUPAC估计器，利用预实验行为协变量降低集群级分配引起的方差，提高统计功效。

Result: 在快手平台的大规模社交分享实验中验证了该方法，结果显示该方法显著减少了溢出效应，相比传统用户级设计能更准确地评估社交策略，建立了可靠且可扩展的网络化A/B测试框架。

Conclusion: 该框架通过结构化的溢出控制和稳健的统计推断，为存在网络干扰的社交产品实验提供了有效的解决方案，平衡了无偏性和统计功效的需求，具有实际应用价值。

Abstract: A/B testing is the foundation of decision-making in online platforms, yet social products often suffer from network interference: user interactions cause treatment effects to spill over into the control group. Such spillovers bias causal estimates and undermine experimental conclusions. Existing approaches face key limitations: user-level randomization ignores network structure, while cluster-based methods often rely on general-purpose clustering that is not tailored for spillover containment and has difficulty balancing unbiasedness and statistical power at scale. We propose a spillover-contained experimentation framework with two stages. In the pre-experiment stage, we build social interaction graphs and introduce a Balanced Louvain algorithm that produces stable, size-balanced clusters while minimizing cross-cluster edges, enabling reliable cluster-based randomization. In the post-experiment stage, we develop a tailored CUPAC estimator that leverages pre-experiment behavioral covariates to reduce the variance induced by cluster-level assignment, thereby improving statistical power. Together, these components provide both structural spillover containment and robust statistical inference. We validate our approach through large-scale social sharing experiments on Kuaishou, a platform serving hundreds of millions of users. Results show that our method substantially reduces spillover and yields more accurate assessments of social strategies than traditional user-level designs, establishing a reliable and scalable framework for networked A/B testing.

</details>


### [118] [Friedkin-Johnsen Social Influence Dynamics on Networks: A Boundary-Value Formulation and Influenceability Measures](https://arxiv.org/abs/2602.08704)
*Moses Boudourides*

Main category: cs.SI

TL;DR: 本文对Friedkin-Johnsen社交影响模型进行了严格的数学分析，将意见动态建模为网络上的离散边界值问题，分析了顽固节点和易受影响节点如何共同决定意见演化。


<details>
  <summary>Details</summary>
Motivation: 研究社交网络中意见动态的数学基础，特别是分析网络结构、顽固节点（边界）和易受影响节点（内部）如何共同决定意见演化过程。

Method: 将意见动态建模为网络上的离散边界值问题，采用两种方法：适用于异质易感性的通用基于预解式的方法，以及适用于同质易感性的谱方法。

Result: 推导了瞬态和稳态解，建立了收敛速率，推导了敏感性公式，证明了扰动边界，定义了影响性度量并证明了其基本性质，在Zachary空手道俱乐部图上进行了蒙特卡洛模拟。

Conclusion: 该研究为Friedkin-Johnsen模型提供了严格的数学框架，能够精确分析网络结构、节点特性对意见演化的影响，并提出了新的影响性度量方法。

Abstract: This article presents a rigorous mathematical analysis of the Friedkin--Johnsen model of social influence on networks. We frame the opinion dynamics as a discrete boundary-value problem on a network, emphasizing the role of stubborn (boundary) and susceptible (interior) agents in shaping opinion evolution. This perspective allows for a precise analysis of how network structure, stubborn agents (boundary), and susceptible agents (interior) collectively determine the evolution of opinions. We derive the transient and steady-state solutions using two distinct but related approaches: a general resolvent-based method applicable to agents with heterogeneous susceptibilities, and a spectral method valid for the special case of homogeneous susceptibility. We further establish quantitative convergence rates to the steady state, derive explicit sensitivity formulas with respect to susceptibility parameters, and prove perturbation bounds under changes in the influence matrix. Moreover, we formally define a set of influenceability measures and prove some of their basic properties. Finally, we provide a Monte Carlo illustration on the Zachary karate club graph, showing how the proposed opinion broadcasting centralities and centralizations behave under random susceptibility profiles and how they relate to classical network centralities.

</details>


### [119] [Robust Sequential Learning in Random Order Networks](https://arxiv.org/abs/2602.08953)
*William Guo,Edward Xiong,Jie Gao*

Main category: cs.SI

TL;DR: 研究社交网络中顺序学习问题，分析随机顺序渐近真理学习网络的鲁棒性，提出图构造方法和最小化修改算法


<details>
  <summary>Details</summary>
Motivation: 传统顺序学习中，社交网络对行动顺序、拓扑结构或私人信号强度的微小变化高度敏感，容易导致无法收敛到真相。需要研究能够实现随机顺序渐近真理学习的网络及其鲁棒性。

Method: 1) 分析实现随机顺序渐近真理学习网络的鲁棒性，证明其能抵抗有限数量的对抗性修改；2) 提出此类网络成功的必要条件；3) 设计多种通过不同机制实现学习的图构造；4) 提出随机多项式时间算法，将任意网络转化为随机顺序学习网络，使用最小边或顶点修改。

Result: 1) 揭示了实现随机顺序学习的网络结构特性；2) 提供了设计鲁棒社交网络的算法工具；3) 证明了算法具有可证明的近似保证。

Conclusion: 该研究揭示了随机顺序渐近真理学习网络的结构特性，并提供了将任意网络转化为此类鲁棒网络的算法工具，有助于设计更可靠的社交学习系统。

Abstract: In the sequential learning problem, agents in a network attempt to predict a binary ground truth, informed by both a noisy private signal and the predictions of neighboring agents before them. It is well known that social learning in this setting can be highly fragile: small changes to the action ordering, network topology, or even the strength of the agents' private signals can prevent a network from converging to the truth. We study networks that achieve random-order asymptotic truth learning, in which almost all agents learn the ground truth when the decision ordering is selected uniformly at random. We analyze the robustness of these networks, showing that those achieving random-order asymptotic truth learning are resilient to a bounded number of adversarial modifications. We characterize necessary conditions for such networks to succeed in this setting and introduce several graph constructions that learn through different mechanisms. Finally, we present a randomized polynomial-time algorithm that transforms an arbitrary network into one achieving random-order learning using minimal edge or vertex modifications, with provable approximation guarantees. Our results reveal structural properties of networks that achieve random-order learning and provide algorithmic tools for designing robust social networks.

</details>


### [120] [Hyperactive Minority Alter the Stability of Community Notes](https://arxiv.org/abs/2602.08970)
*Jacopo Nudo,Eugenio Nerio Nemmi,Edoardo Loru,Alessandro Mei,Walter Quattrociocchi,Matteo Cinelli*

Main category: cs.SI

TL;DR: 社区笔记系统并未实现去中心化的事实核查权威，而是将权力集中在少数高度活跃、政治极化的用户手中，系统结构不稳定，少数用户行为就能显著影响笔记的可见性。


<details>
  <summary>Details</summary>
Motivation: 随着平台减少专业事实核查，社区化替代方案被推广为更透明民主的选择。本研究旨在探究社区笔记系统中用户社交动态如何影响笔记的涌现，特别是X平台的Community Notes系统。

Method: 使用2021-2025年完整的公开笔记和评分数据，分析用户参与模式和政治行为。通过整合社区笔记共识算法的开源实现，进行反事实模拟，改变评分者群体来修改笔记显示状态。

Result: 贡献活动高度集中：少数用户占评分活动的绝大部分。这些高活跃度用户并非中立志愿者，他们选择性参与内容，政治极化程度远高于整体用户群体。系统结构不稳定，笔记的涌现和可见性常取决于几十个高度活跃用户的行为，微小扰动就能导致显著不同结果。

Conclusion: X平台的社区事实核查并未分散认知权威，而是重新配置了权威，将实质性权力集中在少数高度活跃、政治极化的贡献者手中，系统设计存在结构性脆弱性。

Abstract: As platforms increasingly scale down professional fact-checking, community-based alternatives are promoted as more transparent and democratic. The main substitute being proposed is community-based contextualization, most notably Community Notes on X, where users write annotations and collectively rate their helpfulness under a consensus-oriented algorithm. This shift raises a basic empirical question: to what extent do users' social dynamics affect the emergence of Community Notes? We address this question by characterizing participation and political behavior, using the full public release of notes and ratings (between 2021 and 2025). We show that contribution activity is highly concentrated: a small minority of users accounts for a disproportionate share of ratings. Crucially, these high-activity contributors are not neutral volunteers: they are selective in the content they engage with and substantially more politically polarized than the overall contributor population. We replicate the notes' emergence process by integrating the open-source implementation of the Community Notes consensus algorithm used in production. This enables us to conduct counterfactual simulations that modify the display status of notes by varying the pool of raters. Our results reveal that the system is structurally unstable: the emergence and visibility of notes often depend on the behavior of a few dozen highly active users, and even minor perturbations in their participation can lead to markedly different outcomes. In sum, rather than decentralizing epistemic authority, community-based fact-checking on X reconfigures it, concentrating substantial power in the hands of a small, polarized group of highly active contributors.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [121] [Potential Role of Agentic Artificial Intelligence in Toxicologic Pathology](https://arxiv.org/abs/2602.06980)
*Nasir Rajpoot,Richard Haworth,Xavier Palazzi,Alok Sharma,Manu Sebastian,Stephen Cahalan,Dinesh S. Bangari,Radhakrishna Sura,James Hartke,Marco Tecilla,Krishna Yekkala,Simon Graham,Dang Vu,David Snead,Mostafa Jahanifar,Adnan Khan,Erio Barale-Thomas*

Main category: cs.CY

TL;DR: 本文探讨了在非临床毒理学研究中应用智能体AI解决病理报告工作流挑战，提出了分阶段采用路线图，强调需要跨行业协作建立标准。


<details>
  <summary>Details</summary>
Motivation: 非临床毒理学研究的数量和复杂性不断增加，毒理学病理报告面临数据来源分散、报告时间不一、监管要求提高等挑战，需要新的解决方案来改进工作流程。

Method: 基于2025年毒理学病理学会年会闭门圆桌会议和后续讨论，综合毒理学病理学家、毒理学家和AI开发者的观点，分析当前报告工作流痛点，识别智能体AI的现实应用场景。

Result: 确定了智能体AI在协调工作流编排、数据整合和病理学家参与的报告生成方面的潜在应用，提出了分阶段采用路线图和试点设计考虑，识别了透明度、验证和组织准备等主要采用障碍。

Conclusion: 需要制药组织、CRO、学术界和监管机构协调努力，建立共享标准、基准和治理框架，以确保AI在毒理学科学中的安全、透明和可信整合。

Abstract: As the volume and complexity of nonclinical toxicology studies continue to increase, toxicologic pathology reporting faces persistent challenges, including fragmented sources of data (e.g., histopathology images, clinical pathology and other study data, adverse effects database, mechanistic literature), variable reporting timelines and heightened regulatory expectations. This white paper examines the emerging role of agentic artificial intelligence (AI) in addressing these issues through coordinated workflow orchestration, data integration, and pathologist-in-the-loop report generation. Based on a closed-door roundtable held during the 2025 Society of Toxicologic Pathology (STP) Annual Meeting and follow-on discussions, this paper synthesizes the perspectives of leading toxicologic pathologists, toxicologists, and AI developers. It outlines the key pain points in current reporting workflows, identifies realistic near-term use cases for agentic AI, and describes major adoption barriers including requirements for transparency, validation, and organizational readiness. A phased adoption roadmap and pilot design considerations are proposed to help support responsible evaluation and deployment of agentic AI system in nonclinical settings. The paper concludes by emphasizing the need for coordinated efforts across pharmaceutical organizations, CROs, academia, and regulators to establish shared standards, benchmarks, and governance frameworks that will lead to safe, transparent, and trustworthy integration of AI into toxicologic science.

</details>


### [122] [What is Safety? Corporate Discourse, Power, and the Politics of Generative AI Safety](https://arxiv.org/abs/2602.06981)
*Ankolika De,Gabriel Lima,Yixin Zou*

Main category: cs.CY

TL;DR: 论文分析生成式AI公司如何通过公开文件构建和传达"安全"概念，揭示其话语策略如何确立权威、责任和合法性，并警告不加批判地接受这些话语可能限制替代性治理方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是批判性地审视领先生成式AI公司如何通过公开文件构建"安全"话语，揭示这些话语如何确立公司权威、责任和合法性，以及这种话语构建对治理和设计替代方法的影响。

Method: 采用批判性话语分析方法，分析公司安全相关声明的语料库，阐释权威、责任和合法性如何通过话语策略建立。

Result: 研究发现这些话语策略巩固了公司行为者的合法性，将安全规范化为实验性和预期性实践，并推动感知上的参与式议程。这些话语风险复制公司优先事项并限制替代治理方法。

Conclusion: 结论强调应将安全视为需要批判性审视的社会技术话语，警告人机交互学者不要合法化公司框架，而应突出问责、公平和正义，将安全话语作为权力产物进行审视，推进人机交互领域对AI的批判议程。

Abstract: This work examines how leading generative artificial intelligence companies construct and communicate the concept of "safety" through public-facing documents. Drawing on critical discourse analysis, we analyze a corpus of corporate safety-related statements to explicate how authority, responsibility, and legitimacy are discursively established. These discursive strategies consolidate legitimacy for corporate actors, normalize safety as an experimental and anticipatory practice, and push a perceived participatory agenda toward safe technologies. We argue that uncritical uptake of these discourses risks reproducing corporate priorities and constraining alternative approaches to governance and design. The contribution of this work is twofold: first, to situate safety as a sociotechnical discourse that warrants critical examination; second, to caution human-computer interaction scholars against legitimizing corporate framings, instead foregrounding accountability, equity, and justice. By interrogating safety discourses as artifacts of power, this paper advances a critical agenda for human-computer interaction scholarship on artificial intelligence.

</details>


### [123] [Empowering Affected Individuals to Shape AI Fairness Assessments: Processes, Criteria, and Tools](https://arxiv.org/abs/2602.06984)
*Lin Luo,Satwik Ghanta,Yuri Nakao,Mathieu Chollet,Simone Stumpf*

Main category: cs.CY

TL;DR: 研究通过用户实验探索个人如何创建自己的AI公平性标准，发现人们通过模型特征具体化公平概念，并产生多样化的结果和程序公平标准。


<details>
  <summary>Details</summary>
Motivation: 现有AI公平性评估通常由专家使用预定义属性和指标进行，无法捕捉受影响个体的多样性和细微公平观念。需要让受影响个体参与公平性评估，但缺乏关于他们如何创建公平标准的实证证据。

Method: 在信用评级场景中进行定性用户研究，18名参与者首先用自己的语言表达公平观念，然后通过交互式原型将其转化为具体量化和可操作的公平标准。

Result: 提供了人们公平观念如何通过模型特征具体化的实证证据，发现了个人自定义的结果公平和程序公平标准的多样化集合。

Conclusion: 研究结果为支持更具包容性和价值敏感的AI公平性评估流程和工具设计提供了启示。

Abstract: AI systems are increasingly used in high-stakes domains such as credit rating, where fairness concerns are critical. Existing fairness assessments are typically conducted by AI experts or regulators using predefined protected attributes and metrics, which often fail to capture the diversity and nuance of fairness notions held by the individuals who are affected by these systems' decisions, such as decision subjects. Recent work has therefore called for involving affected individuals in fairness assessment, yet little empirical evidence exists on how they create their own fairness criteria or what kinds of criteria they produce - knowledge that could not only inform experts' fairness evaluation and mitigation, but also guide the design of AI assessment tools. We address this gap through a qualitative user study with 18 participants in a credit rating scenario. Participants first articulated their fairness notions in their own words. Then, participants turned them into concrete quantified and operationalized fairness criteria, through an interactive prototype we designed. Our findings provide empirical evidence of the process through which people's fairness notions emerge via grounding in model features, and uncover a diverse set of individuals' custom-defined criteria for both outcome and procedural fairness. We provide design implications for processes and tools that support more inclusive and value-sensitive AI fairness assessment.

</details>


### [124] [A New Mode of Teaching Chinese as a Foreign Language from the Perspective of Smart System Studied by Using Rongzhixue](https://arxiv.org/abs/2602.06992)
*Xiaohui Zou,Lijun Ke,Shunpeng Zou*

Main category: cs.CY

TL;DR: 本文提出了一种基于融智学视角的新型对外汉语教学模式，强调解释先于翻译的蝴蝶模型和双语思维训练新方法，整合语言科学和教育科学的前沿成果，以应对ChatGPT等AI技术对传统语言教育观念的颠覆性挑战。


<details>
  <summary>Details</summary>
Motivation: 面对ChatGPT等人工智能技术对人类学习能力和创造力的挑战，传统的语言知识教育观念、汉语教育观念以及对外汉语教学观念已经落后，需要进行颠覆性创新。本研究旨在从融智学视角探索适应变革的新型对外汉语教学模式。

Method: 提出基于融智学视角的新型对外汉语教学模式，采用"解释先于翻译"的蝴蝶模型，强调双语思维训练新方法。一方面应用汉字新理论、语言与言语关系理论等语言科学前沿成果；另一方面应用AI赋能教学的新模式和教育科学前瞻性研究成果。

Result: 该模式不仅突破了传统的语言观、教育观，特别是对外汉语教学的旧观念，还突破了传统的人机交互观念。明确提出了语言、知识、教育教学等一系列重大跨界融智学问题，以及双语思维训练的新方法和新课题。

Conclusion: 本研究进行了一系列创新尝试，从融智学视角提出了具有前瞻性的对外汉语教学新模式，旨在应对AI时代对语言教育的颠覆性挑战，为学术界同仁、教师和学生提供有益的参考和启示。

Abstract: The purpose of this study is to introduce a new model of teaching Chinese as a foreign language from the perspective of integrating wisdom. Its characteristics are as follows: focusing on the butterfly model of interpretation before translation, highlighting the new method of bilingual thinking training, on the one hand, applying the new theory of Chinese characters, the theory of the relationship between language and speech, and the forward-looking research results of language science; On the other hand, the application of the new model of teaching Chinese as a foreign language, AI empowering teaching and learning, and the forward-looking research results of educational science fully reflect a series of characteristics of the new model of teaching Chinese as a foreign language from the perspective of integrating wisdom. Its beneficial effects are: not only the old view of language and education, especially the old view of teaching Chinese as a foreign language, but also the old view of human-computer interaction. Its significance lies in that a series of great cross-border Rongzhixue such as language, knowledge, education and teaching, as well as new methods and new topics of bilingual thinking training are clearly put forward from the perspective of integrating wisdom. Especially in the face of the challenge of Chat GPT to human learning ability and even creativity, the existing concepts of language knowledge education and teaching are already very backward. The old concepts of Chinese language education, and teaching Chinese as a foreign language are all facing a series of subversive innovation challenges. How to seek changes in adaptation? This study has made a series of innovative attempts, hoping to benefit academic colleagues, teachers and students.

</details>


### [125] [Tokenizations for Austronesian Language Models: study on languages in Indonesia Archipelago](https://arxiv.org/abs/2602.06998)
*Andhika Bernard Lumbantobing,Hokky Situngkir*

Main category: cs.CY

TL;DR: 该研究为印尼区域语言开发了一种基于音节的标记化框架，借鉴传统文字系统，相比GPT-2的基于子词的标记化，能更好地保持南岛语系语言的语音和形态模式。


<details>
  <summary>Details</summary>
Motivation: 基于英语主导语料优化的子词标记化方法在处理南岛语系语言时会产生与语言结构不匹配的标记碎片化问题，需要开发更符合这些语言特性的标记化方法。

Method: 基于阿布吉达文字系统逻辑构建音节分割程序，从印尼语词典提取2,843个标记构建词汇表，在包含10种区域语言的NusaX数据集上评估，使用TPC比率和Smith-Waterman序列对齐算法进行分析。

Result: 音节标记化在所有区域语言中产生一致的TPC值，而GPT-2对英语的TPC值最低；音节标记化平均比GPT-2提高约21%的标记序列相似度分数。

Conclusion: 基于音节的标记化方法能更有效地保持相关南岛语系语言的语音和形态模式，为多语言LLM开发提供了语言学上更合理的基础。

Abstract: Tokenization constitutes a fundamental stage in Large Language Model (LLM) processing; however, subword-based tokenization methods optimized on English-dominant corpora may produce token fragmentation misaligned with the linguistic structures of Austronesian languages. This study aimed to develop a syllable-based tokenization framework adopting principles from traditional Indonesian scripts (aksara) for regional languages of Indonesia. A syllabic segmentation procedure was constructed based on the logic of abugida writing systems and implemented with a vocabulary of 2,843 tokens extracted from the Indonesian dictionary (KBBI). Evaluation was conducted on the NusaX dataset comprising 1,000 parallel translation samples across 10 regional languages, Indonesian, and English. Analysis employed Token per Character (TPC) ratio and sequence alignment using the Smith-Waterman algorithm. Results demonstrated that syllable-based tokenization yielded consistent TPC values across all regional languages, whereas GPT-2 exhibited an inverse pattern with the lowest TPC for English. Syllable-based tokenization consistently produced higher token sequence similarity scores, with an average increase of approximately 21% compared to GPT-2. These findings confirm that the syllable-based approach more effectively preserves phonological and morphological patterns across related Austronesian languages, offering a linguistically principled foundation for multilingual LLM development.

</details>


### [126] [AI for Sustainable Data Protection and Fair Algorithmic Management in Environmental Regulation](https://arxiv.org/abs/2602.07021)
*Sahibpreet Singh,Saksham Sharma*

Main category: cs.CY

TL;DR: AI增强加密技术可提升环境数据保护与算法公平性，但需完善网络法律与监管框架


<details>
  <summary>Details</summary>
Motivation: 传统加密方法在处理动态环境数据时存在局限性，需要探索先进加密技术，并利用AI增强这些技术以实现稳健的数据保护和公平的算法管理

Method: 对AI增强的同态加密（HE）和安全多方计算（MPC）的最新进展进行全面综述，分析这些技术如何应用于环境数据监管

Result: AI驱动的动态密钥管理、自适应加密方案、HE计算效率优化，以及MPC协议优化和故障缓解，显著提高了环境数据处理的安安全性

Conclusion: 研究揭示了AI、网络法律和环境监管交叉领域的重要研究空白，强调需要更严格的网络法律和全面的监管框架，未来应完善AI系统以平衡安全与隐私

Abstract: Integration of AI into environmental regulation represents a significant advancement in data management. It offers promising results in both data protection plus algorithmic fairness. This research addresses the critical need for sustainable data protection in the era of ever evolving cyber threats. Traditional encryption methods face limitations in handling the dynamic nature of environmental data. This necessitates the exploration of advanced cryptographic techniques. The objective of this study is to evaluate how AI can enhance these techniques to ensure robust data protection while facilitating fair algorithmic management. The methodology involves a comprehensive review of current advancements in AI-enhanced homomorphic encryption (HE) and multi-party computation (MPC). It is coupled with an analysis of how these techniques can be applied to environmental data regulation. Key findings indicate that AI-driven dynamic key management, adaptive encryption schemes, and optimized computational efficiency in HE, alongside AI-enhanced protocol optimization and fault mitigation in MPC, significantly improve the security of environmental data processing. These findings highlight a crucial research gap in the intersection of AI, cyber laws, and environmental regulation, particularly in terms of addressing algorithmic bias, transparency, and accountability. The implications of this research underscore the need for stricter cyber laws. Also, the development of comprehensive regulations to safeguard sensitive environmental data. Future efforts should focus on refining AI systems to balance security with privacy and ensuring that regulatory frameworks can adapt to technological advancements. This study provides a foundation for future research aimed at achieving secure sustainable environmental data management through AI innovations.

</details>


### [127] [When Excellence Stops Producing Knowledge: A Practitioner's Observation on Research Funding](https://arxiv.org/abs/2602.07039)
*Heimo Müller*

Main category: cs.CY

TL;DR: 论文指出当前竞争性科研资助体系存在结构性悖论：参与者普遍认识到系统已接近功能极限，但多数改革措施反而加剧了根本问题。研究发现"卓越性"已与知识生产脱钩，而与评估中的"可呈现性"过度耦合。


<details>
  <summary>Details</summary>
Motivation: 作者基于近四十年的科研资助参与经验（申请人、协调人、评估人、评审委员），观察到当前竞争性科研资助体系存在结构性悖论。尽管许多参与者认识到系统已接近功能极限，但大多数改革措施反而加剧了根本问题，这促使作者深入分析这一现象。

Method: 采用内部人士视角的定性分析，基于作者作为申请人、协调人、评估人和评审委员的亲身经验。重点考察两个领域：竞争性基础研究资助和大型欧盟联合项目。分析三个加速趋势：提案写作的专业化（通过专业顾问）、AI辅助申请工具的兴起，以及评审人员短缺导致评审小组依赖与具体研究领域日益脱节的评审人。

Result: 研究发现"卓越性"已与知识生产脱钩，而与评估中的"可呈现性"过度耦合。这种脱钩现象在竞争性基础研究资助和大型欧盟项目中尤为明显。三个加速趋势加剧了这一现象：提案写作的专业化使申请过程与实质研究分离；AI辅助应用进一步形式化评估标准；评审人员短缺导致评审质量下降。

Conclusion: 论文并非外部批评，而是内部人士的观察记录。作者希望通过明确指出这一广泛经历但很少被明确表述的模式，为科研资助体系提供更建设性的方向。这有助于理解当前科研评估体系的功能失调，并为未来改革提供思考基础。

Abstract: After almost four decades of participating in competitive research funding -- as applicant, coordinator, evaluator, and panel member -- I have come to see a structural paradox: many participants recognize that the current system is approaching its functional limits, yet most reform measures intensify rather than alleviate the underlying dynamics. This paper documents how excellence has become decoupled from knowledge production through an increasing coupling to representability under evaluation. The discussion focuses on two domains in which this is particularly visible: competitive basic research funding and large EU consortium projects. Three accelerating trends are examined: the professionalization of proposal writing through specialized consultants, the rise of AI-assisted applications, and an evaluator shortage that forces panels to rely on reviewers increasingly distant from the actual research domains. These observations are offered not as external critique but as an insider account, in the hope that naming a widely experienced but rarely articulated pattern may enable more constructive orientation.
  Keywords: Research funding, Excellence, Evaluation, Goodhart's Law, Professionalization, AI-assisted proposals, Peer review crisis

</details>


### [128] [Structural transparency of societal AI alignment through Institutional Logics](https://arxiv.org/abs/2602.08246)
*Atrisha Sarkar,Isam Faik*

Main category: cs.CY

TL;DR: 提出"结构透明度"框架，用于分析AI对齐中的组织和制度决策，补充现有信息透明度方法


<details>
  <summary>Details</summary>
Motivation: 现有透明度框架主要关注AI模型、数据和程序的信息层面，而塑造对齐决策及其社会后果的组织和制度力量在研究和实践中被忽视，需要弥补这一空白

Method: 基于制度逻辑理论，开发结构透明度框架，包含五个分析组件和相应的"分析师配方"，用于识别主要制度逻辑、外部干扰、结构风险与社会技术危害的映射

Result: 提出了一个系统框架，能够分析AI对齐治理中的组织决策，揭示制度动态和决策后果，补充现有信息透明度方法

Conclusion: 结构透明度概念使分析师能够在信息透明度基础上，进行宏观层面的制度动态分析，更好地理解AI对齐决策的社会影响

Abstract: The field of AI alignment is increasingly concerned with the questions of how values are integrated into the design of generative AI systems and how their integration shapes the social consequences of AI. However, existing transparency frameworks focus on the informational aspects of AI models, data, and procedures, while the institutional and organizational forces that shape alignment decisions and their downstream effects remain underexamined in both research and practice. To address this gap, we develop a framework of \emph{structural transparency} for analyzing organizational and institutional decisions concerning AI alignment, drawing on the theoretical lens of Institutional Logics. We develop a categorization of organizational decisions that are present in the governance of AI alignment, and provide an explicit analytical approach to examining them. We operationalize the framework through five analytical components, each with an accompanying "analyst recipe" that collectively identify the primary institutional logics and their internal relationships, external disruptions to existing social orders, and finally, how the structural risks of each institutional logic are mapped to a catalogue of sociotechnical harms. The proposed concept of structural transparency enables analysts to complement existing approached based on informational transparency with macro-level analyses that capture the institutional dynamics and consequences of decisions regarding AI alignment.

</details>


### [129] [Cyclic Adaptive Private Synthesis for Sharing Real-World Data in Education](https://arxiv.org/abs/2602.08299)
*Hibiki Ito,Chia-Yu Hsu,Hiroaki Ogata*

Main category: cs.CY

TL;DR: 提出了CAPS框架，用于教育真实世界数据的差分隐私合成，解决传统一次性合成方法在处理高维小样本教育数据时的不足，支持持续数据共享。


<details>
  <summary>Details</summary>
Motivation: 教育领域真实世界数据(RWD)快速增长，但隐私问题限制了其在学习分析中的二次使用。现有差分隐私合成方法主要针对大规模低维开放数据集，而教育RWD通常具有高维、小样本的特点，且教育实践需要持续而非一次性的数据共享。

Method: 提出了循环自适应隐私合成(CAPS)框架，通过迭代方式共享真实世界数据，而非传统的一次性合成方法。该框架适应教育实践的迭代特性，支持持续的数据共享。

Result: 在真实教育RWD上的案例研究表明，CAPS框架优于一次性合成基线方法。该框架不仅促进了开放科学，还为基于设计的研究提供了丰富机会。

Conclusion: 这项工作为教育RWD的隐私保护共享提供了重要第一步，扩展了学习分析中开放科学和基于设计研究的可能性，同时指出了需要进一步研究的挑战。

Abstract: The rapid adoption of digital technologies has greatly increased the volume of real-world data (RWD) in education. While these data offer significant opportunities for advancing learning analytics (LA), secondary use for research is constrained by privacy concerns. Differentially private synthetic data generation is regarded as the gold-standard approach to sharing sensitive data, yet studies on the private synthesis of educational data remain very scarce and rely predominantly on large, low-dimensional open datasets. Educational RWD, however, are typically high-dimensional and small in sample size, leaving the potential of private synthesis underexplored. Moreover, because educational practice is inherently iterative, data sharing is continual rather than one-off, making a traditional one-shot synthesis approach suboptimal. To address these challenges, we propose the Cyclic Adaptive Private Synthesis (CAPS) framework and evaluate it on authentic RWD. By iteratively sharing RWD, CAPS not only fosters open science, but also offers rich opportunities of design-based research (DBR), thereby amplifying the impact of LA. Our case study using actual RWD demonstrates that CAPS outperforms a one-shot baseline while highlighting challenges that warrant further investigation. Overall, this work offers a crucial first step towards privacy-preserving sharing of educational RWD and expands the possibilities for open science and DBR in LA.

</details>


### [130] [To Tango or to Disentangle? Making Ethnography Public in the Digital Age](https://arxiv.org/abs/2602.08349)
*Daniel Mwesigwa,Cyan DeVeaux,Palashi Vaghela*

Main category: cs.CY

TL;DR: 该论文探讨数字平台时代民族志研究面临的新机遇与挑战，通过VRChat和WhatsApp案例研究，提出"涌现关系性"作为分析民族志研究者、平台和公众相互塑造的关键分析框架。


<details>
  <summary>Details</summary>
Motivation: 数字平台的兴起改变了民族志研究的实践环境和伦理挑战，重塑了研究者作为局外人/局内人的双重角色。需要新的分析框架来理解数字时代民族志研究如何应对这些变化，特别是在研究种族和种姓等社会文化问题时。

Method: 采用案例研究方法，分析VRChat和WhatsApp两个数字平台上的民族志实践。通过具体案例探讨民族志研究者如何运用多样策略研究持久和新兴的社会文化问题，特别是形成所谓"公众"的现象。

Result: 提出"涌现关系性"作为关键分析概念，用于理解民族志研究者、数字平台和公众之间的相互塑造关系。这一框架提供了分析位置性和混合媒体环境如何构成和制约可访问、可表达和可公开内容的分析维度。

Conclusion: 数字平台时代需要重新思考民族志研究的双重角色和关系性。"涌现关系性"为分析民族志在混合媒体环境中的实践提供了有价值的框架，有助于理解研究者、平台和公众如何在数字环境中相互塑造和制约。

Abstract: Ethnography attends to relations among people, practices, and the technologies that mediate them. Central to this method is the duality of roles ethnographers navigate as researchers and participants and as outsiders and insiders. However, the rise of digital platforms has introduced new opportunities as well as practical and ethical challenges that reshape these dualities across hybrid media environments spanning both online and offline contexts. Drawing on two case studies of VRChat and WhatsApp, we examine how ethnographers employ diverse tactics to study both enduring and emerging socio-cultural issues of race and caste, particularly those that form what are often called publics. We propose emergent relationality as a key analytic for understanding the mutual shaping of ethnographers, platforms, and publics. In this work, emergent relationality offers registers for analyzing how positionality and hybrid media environments constitute and condition what can be accessed, articulated, and made public.

</details>


### [131] [Three Lessons from Citizen-Centric Participatory AI Design](https://arxiv.org/abs/2602.08554)
*Eike Schneiders,Sarah Kiden,Beining Zhang,Bruno Rafael Queiros Arcanjo,Zhaoxing Li,Ezhilarasi Periyathambi,Vahid Yazdanpanah,Sebastian Stein*

Main category: cs.CY

TL;DR: 本文通过参与式工作坊探讨公民中心视角下智能体AI系统设计的挑战，强调长期参与对负责任AI发展的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统设计往往缺乏公民视角，需要探索如何将社会价值和公众期望融入未来AI智能体的设计中，确保AI发展符合公民利益。

Method: 采用建构性设计研究方法，在2025年举办了三场参与式工作坊，邀请公众和跨领域利益相关者参与，通过故事讲述和低保真原型制作来探讨AI智能体对社区的潜在影响。

Result: 识别出三个关键挑战：1) 实现有意义且持续的公众参与；2) 建立专家与非专业参与者之间的共同语言；3) 将推测性的参与者输入转化为可实施的系统。

Conclusion: 反思性和长期参与对于负责任且可操作的公民中心AI发展至关重要，需要建立持续的参与机制来确保AI系统真正服务于社会需求。

Abstract: This workshop paper examines challenges in designing agentic AI systems from a citizen-centric perspective. Drawing on three participatory workshops conducted in 2025 with members of the general public and cross-sector stakeholders, we explore how societal values and expectations shape visions of future AI agents. Using constructive design research methods, participants engaged in storytelling and lo-fi prototyping to reflect on potential community impacts. We identify three key challenges: enabling meaningful and sustained public engagement, establishing a shared language between experts and lay participants, and translating speculative participant input into implementable systems. We argue that reflexive, long-term participation is essential for responsible and actionable citizen-centric AI development.

</details>


### [132] [We Should Separate Memorization from Copyright](https://arxiv.org/abs/2602.08632)
*Adi Haviv,Niva Elkin-Koren,Uri Hacohen,Roi Livni,Shay Moran*

Main category: cs.CY

TL;DR: 论文认为当前技术文献中的记忆化研究不应等同于版权侵权，主张采用基于输出的风险评估框架来评估版权问题


<details>
  <summary>Details</summary>
Motivation: 基础模型的广泛使用带来了新的版权风险，但技术社区和法律学者对此存在不同解读和争议。当前技术文献依赖的传统重构技术并不适用于版权分析，导致记忆化与复制被混淆

Method: 区分有意义的侵权风险技术信号与合法的泛化或高频内容，提出基于输出的风险评估流程，使技术评估与现有版权标准保持一致

Result: 论证了记忆化不应等同于复制，也不应作为版权侵权的代理指标。建立了区分侵权风险与合法内容的技术信号框架

Conclusion: 主张采用基于输出的风险评估方法，为研究、审计和政策制定提供更原则性的基础，使技术评估与版权标准保持一致

Abstract: The widespread use of foundation models has introduced a new risk factor of copyright issue. This issue is leading to an active, lively and on-going debate amongst the data-science community as well as amongst legal scholars. Where claims and results across both sides are often interpreted in different ways and leading to different implications. Our position is that much of the technical literature relies on traditional reconstruction techniques that are not designed for copyright analysis. As a result, memorization and copying have been conflated across both technical and legal communities and in multiple contexts. We argue that memorization, as commonly studied in data science, should not be equated with copying and should not be used as a proxy for copyright infringement. We distinguish technical signals that meaningfully indicate infringement risk from those that instead reflect lawful generalization or high-frequency content. Based on this analysis, we advocate for an output-level, risk-based evaluation process that aligns technical assessments with established copyright standards and provides a more principled foundation for research, auditing, and policy.

</details>


### [133] [Algorithmic Governance in the United States: A Multi-Level Case Analysis of AI Deployment Across Federal, State, and Municipal Authorities](https://arxiv.org/abs/2602.08728)
*Maxim Dedyaev*

Main category: cs.CY

TL;DR: 研究揭示美国联邦、州和市级政府中AI应用的层级差异：联邦层面AI主要用于高风险控制（监控、执法），州政府混合支持与把关功能，市级政府则更注重实用服务导向。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在公共治理中快速发展，但人们对AI在不同政府层级中的实际应用形态知之甚少，特别是在权力分散的联邦制体系中。同一算法在不同层级可能服务于完全不同的目的，这一研究空白需要填补。

Method: 采用比较定性分析方法，研究30个美国AI实施案例，结合数字时代治理框架和社会技术视角，识别算法治理的两种模式。

Result: 研究发现政府层级间存在明显的功能分化模式：联邦政府将AI制度化为高风险控制工具（监控、执法、监管）；州政府处于模糊中间地带，混合支持功能与算法把关；市级政府则以实用服务为导向，用于简化日常运营和改善居民互动。

Conclusion: AI在公共部门的特征、功能和风险根本上受部署层级的制度背景影响，算法治理的性质由治理层级决定而非技术本身。

Abstract: The rapid expansion of artificial intelligence in public governance has generated strong optimism about faster processes, smarter decisions, and more modern administrative systems. Yet despite this enthusiasm, we still know surprisingly little about how AI actually takes shape inside different layers of government. Especially in federal systems where authority is fragmented across multiple levels. In practice, the same algorithm can serve very different purposes. This study responds to that gap by examining how AI is used across federal, state, and municipal levels in the United States. Drawing on a comparative qualitative analysis of thirty AI implementation cases, and guided by a digital-era governance framework combined with a sociotechnical perspective, the study identifies two broad modes of algorithmic governance: control-oriented systems and support-oriented systems. The findings reveal a clear pattern of functional differentiation across levels of government. At the federal level, AI is most often institutionalized as a tool for high-stakes control: supporting surveillance, enforcement, and regulatory oversight. State governments occupy a more ambiguous middle ground, where AI frequently combines supportive functions with algorithmic gatekeeping, particularly in areas such as welfare administration and public health. Municipal governments, by contrast, tend to deploy AI in more pragmatic and service-oriented ways, using it to streamline everyday operations and improve direct interactions with residents. By foregrounding institutional context, this study advances debates on algorithmic governance by demonstrating that the character, function, and risks of AI in the public sector are fundamentally shaped by the level of governance at which these systems are deployed.

</details>


### [134] [Empirically Understanding the Value of Prediction in Allocation](https://arxiv.org/abs/2602.08786)
*Unai Fischer-Abaigar,Emily Aiken,Christoph Kern,Juan Carlos Perdomo*

Main category: cs.CY

TL;DR: 开发了一个实证工具包，帮助决策者量化预测投资相对于其他政策杠杆（如扩大容量和改进治疗质量）的福利影响，并在德国就业服务和埃塞俄比亚贫困目标定位两个案例研究中应用。


<details>
  <summary>Details</summary>
Motivation: 机构越来越多地使用预测来分配稀缺资源。从设计角度看，更好的预测与其他投资（如扩大容量或改进治疗质量）存在竞争。核心问题不是如何解决特定的分配问题，而是应该解决哪个问题。

Method: 开发了一个实证工具包（rvp软件工具包），帮助规划者形成原则性的答案，量化预测投资相对于其他政策杠杆（如扩大容量和改进治疗质量）的福利影响。在两个真实案例研究中应用该框架：德国就业服务和埃塞俄比亚贫困目标定位。

Result: 决策者能够可靠地得出关于预测在其分配问题中相对价值的具体情境结论。工具包和部分数据已公开，以支持该领域的未来实证研究。

Conclusion: 该研究提供了一个实用的实证框架，帮助决策者在资源分配中权衡预测投资与其他政策选择，使机构能够基于具体情境做出更明智的投资决策。

Abstract: Institutions increasingly use prediction to allocate scarce resources. From a design perspective, better predictions compete with other investments, such as expanding capacity or improving treatment quality. Here, the big question is not how to solve a specific allocation problem, but rather which problem to solve. In this work, we develop an empirical toolkit to help planners form principled answers to this question and quantify the bottom-line welfare impact of investments in prediction versus other policy levers such as expanding capacity and improving treatment quality. Applying our framework in two real-world case studies on German employment services and poverty targeting in Ethiopia, we illustrate how decision-makers can reliably derive context-specific conclusions about the relative value of prediction in their allocation problem. We make our software toolkit, rvp, and parts of our data available in order to enable future empirical work in this area.

</details>


### [135] [Paradox of De-identification: A Critique of HIPAA Safe Harbour in the Age of LLMs](https://arxiv.org/abs/2602.08997)
*Lavender Y. Jiang,Xujin Chris Liu,Kyunghyun Cho,Eric K. Oermann*

Main category: cs.CY

TL;DR: 论文指出HIPAA Safe Harbor去标识化方法在现代LLM时代存在缺陷，无法保护患者隐私，因为LLM能从诊断等准标识符中推断出身份信息。


<details>
  <summary>Details</summary>
Motivation: 隐私是基本人权，临床笔记包含患者敏感信息，需要去标识化保护。但现行HIPAA Safe Harbor方法设计于分类表格数据时代，无法应对现代LLM从准标识符相关性中推断身份的能力。

Method: 首先使用因果图形式化身份与准标识符之间的相关性，然后通过实证验证：1）从去标识化笔记中重新识别患者；2）诊断消融实验：即使移除其他所有信息，仅凭诊断就能预测患者居住区域。

Result: 实证表明去标识化存在固有缺陷：LLM能从去标识化笔记中重新识别患者，仅凭诊断信息就能推断患者居住区域，显示去标识化本质上是不完善的。

Conclusion: 去标识化存在固有缺陷，需要社区共同行动来维护医患信任。论文旨在提高意识并讨论可行的建议措施。

Abstract: Privacy is a human right that sustains patient-provider trust. Clinical notes capture a patient's private vulnerability and individuality, which are used for care coordination and research. Under HIPAA Safe Harbor, these notes are de-identified to protect patient privacy. However, Safe Harbor was designed for an era of categorical tabular data, focusing on the removal of explicit identifiers while ignoring the latent information found in correlations between identity and quasi-identifiers, which can be captured by modern LLMs. We first formalize these correlations using a causal graph, then validate it empirically through individual re-identification of patients from scrubbed notes. The paradox of de-identification is further shown through a diagnosis ablation: even when all other information is removed, the model can predict the patient's neighborhood based on diagnosis alone. This position paper raises the question of how we can act as a community to uphold patient-provider trust when de-identification is inherently imperfect. We aim to raise awareness and discuss actionable recommendations.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [136] [Inference under First-Order Degeneracy](https://arxiv.org/abs/2602.07377)
*Xinyue Bei,Manu Navjeevan*

Main category: econ.EM

TL;DR: 研究参数变换存在一阶退化（梯度为零或接近零）时的统计推断问题，以因果中介分析（间接效应为系数乘积）为例，提出在退化区域构建一致有效置信区间的方法。


<details>
  <summary>Details</summary>
Motivation: 当参数变换的梯度为零或接近零时（如因果中介分析中的间接效应是系数乘积），标准的delta方法失效。在退化区域附近，估计量的极限行为依赖于无法一致估计的冗余参数，导致常规推断方法不可行。

Method: 开发最小距离方法构建一致有效的置信区间。建立标准卡方临界值保持有效的充分条件，并在不满足条件时提出简单的自助法程序。

Result: 证明了在退化点附近，正则估计和分位数无偏估计都是不可能的。但提出的最小距离方法能够提供一致有效的置信区间，在模拟和实证应用（教师性别态度对学生成绩的影响）中表现出良好的功效。

Conclusion: 尽管在参数变换存在一阶退化的模型中，传统推断方法在退化区域附近失效，但通过最小距离方法和适当的临界值选择，仍然可以构建一致有效的置信区间，为这类模型的统计推断提供了可行的解决方案。

Abstract: We study inference in models where a transformation of parameters exhibits first-order degeneracy -- that is, its gradient is zero or close to zero, making the standard delta method invalid. A leading example is causal mediation analysis, where the indirect effect is a product of coefficients and the gradient degenerates near the origin. In these local regions of degeneracy the limiting behaviors of plug-in estimators depend on nuisance parameters that are not consistently estimable. We show that this failure is intrinsic -- around points of degeneracy, both regular and quantile-unbiased estimation are impossible. Despite these restrictions, we develop minimum-distance methods that deliver uniformly valid confidence intervals. We establish sufficient conditions under which standard chi-square critical values remain valid, and propose a simple bootstrap procedure when they are not. We demonstrate favorable power in simulations and in an empirical application linking teacher gender attitudes to student outcomes.

</details>


### [137] [Identification of Child Penalties](https://arxiv.org/abs/2602.07486)
*Dor Leventer*

Main category: econ.EM

TL;DR: 本文批评了使用归一化三重差分法估计生育惩罚的传统方法，指出当水平平行趋势假设被违反时，该方法无法识别目标估计量，并提出以性别收入比为新的估计目标。


<details>
  <summary>Details</summary>
Motivation: 现有研究使用事件研究法通过反事实收入归一化来估计生育惩罚（child penalties），但该方法在水平平行趋势假设被违反时存在识别问题。人力资本理论表明这种违反很可能发生：高能力个体倾向于推迟生育且收入增长更快，导致传统估计低估早期生育父母的生育惩罚。

Method: 作者将传统方法形式化为归一化三重差分法（NTD），分析其识别框架。使用以色列行政数据进行偏误边界分析，证明早期群体存在显著低估。作为解决方案，提出以生育对性别收入比的影响作为新的估计目标，并证明该估计量在NTD下可识别。

Result: 偏误边界分析显示，对于早期生育群体，传统估计方法存在实质性低估。提出的新估计目标（性别收入比）在NTD框架下可识别，为解决传统方法的识别问题提供了可行方案。

Conclusion: 传统归一化三重差分法在水平平行趋势假设被违反时无法准确识别生育惩罚，特别是在早期生育群体中。建议将估计目标转向生育对性别收入比的影响，这一新估计量在NTD框架下具有识别性，为生育惩罚研究提供了更稳健的方法。

Abstract: A growing body of research estimates child penalties, the gender gap in the effect of parenthood on labor market earnings, using event studies that normalize treatment effects by counterfactual earnings. I formalize the identification framework underlying this approach, which I term Normalized Triple Differences (NTD), and show it does not identify the conventional target estimand when the parallel trends assumption in levels is violated. Insights from human capital theory suggest such violations are likely: higher-ability individuals delay childbirth and have steeper earnings growth, a mechanism that causes conventional estimates to understate child penalties for early-treated parents. Using Israeli administrative data, a bias-bounding exercise suggests substantial understatement for early groups. As a solution, I propose targeting the effect of parenthood on the gender earnings ratio and show this new estimand is identified under NTD.

</details>


### [138] [Fast Response or Silence: Conversation Persistence in an AI-Agent Social Network](https://arxiv.org/abs/2602.07667)
*Aysajan Eziz*

Main category: econ.EM

TL;DR: Moltbook AI社交网络中的讨论以表层反应为主，缺乏深度互动，回复集中在几秒内发生，缺乏持续的多步协调能力。


<details>
  <summary>Details</summary>
Motivation: 研究AI代理在社交平台上的协调能力，特别是能否维持持续的来回互动，通过分析Moltbook这一AI代理社交网络来了解早期AI社交互动的特点。

Method: 引入"交互半衰期"概念衡量评论获得直接回复的概率随时间的衰减速度，分析Moltbook第一周的快照数据，包括数万个评论线程，并与Reddit基线进行比较。

Result: Moltbook讨论主要由表层反应主导而非深度链式互动，大多数评论从未获得直接回复，互惠性来回互动罕见，回复几乎在几秒内发生，交互持久性仅几分钟而非小时级别，未检测到可靠的4小时节奏。

Conclusion: 早期AI社交互动呈现"快速响应或沉默"模式，要实现持续的多步协调可能需要显式记忆、线程重新浮现和重新进入的支撑机制。

Abstract: Autonomous AI agents are beginning to populate social platforms, but it is still unclear whether they can sustain the back-and-forth needed for extended coordination. We study Moltbook, an AI-agent social network, using a first-week snapshot and introduce interaction half-life: how quickly a comment's chance of receiving a direct reply fades as the comment ages. Across tens of thousands of commented threads, Moltbook discussions are dominated by first-layer reactions rather than extended chains. Most comments never receive a direct reply, reciprocal back-and-forth is rare, and when replies do occur they arrive almost immediately -- typically within seconds -- implying persistence on the order of minutes rather than hours. Moltbook is often described as running on an approximately four-hour ``heartbeat'' check-in schedule; using aggregate spectral tests on the longest contiguous activity window, we do not detect a reliable four-hour rhythm in this snapshot, consistent with jittered or out-of-phase individual schedules. A contemporaneous Reddit baseline analyzed with the same estimators shows substantially deeper threads and much longer reply persistence. Overall, early agent social interaction on Moltbook fits a ``fast response or silence'' regime, suggesting that sustained multi-step coordination will likely require explicit memory, thread resurfacing, and re-entry scaffolds.

</details>


### [139] [Channel Estimation with Hierarchical Sparse Bayesian Learning for ODDM Systems](https://arxiv.org/abs/2602.07769)
*Jiasong Han,Xuehan Wang,Jingbo Tan,Jintao Wang,Yu Zhang,Hai Lin,Jinhong Yuan*

Main category: econ.EM

TL;DR: 提出基于二维分层稀疏贝叶斯学习(HSBL)的ODDM信道估计框架，通过部分解耦和分层网格优化，在保证高精度的同时显著降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有ODDM系统的信道估计方法无法同时实现高精度和低复杂度，主要原因是时延和多普勒参数的内在耦合问题。

Method: 1. 在延迟-多普勒域定义虚拟采样网格，建立部分解耦的二维稀疏信号恢复模型；2. 先进行低复杂度的粗网格2D SBL估计识别潜在信道路径；3. 在这些区域周围构建高分辨率细网格，进行离网格2D SBL估计实现精确信道估计。

Result: 仿真结果表明，所提框架性能优于传统的离网格2D SBL方法，同时显著降低了计算复杂度。

Conclusion: 提出的二维HSBL框架有效解决了ODDM系统中信道估计的精度与复杂度权衡问题，通过分层策略实现了高效准确的信道估计。

Abstract: Orthogonal delay-Doppler division multiplexing (ODDM) is a promising modulation technique for reliable communications in high-mobility scenarios. However, the existing channel estimation frameworks for ODDM systems cannot achieve both high accuracy and low complexity simultaneously, due to the inherent coupling of delay and Doppler parameters. To address this problem, a two-dimensional (2D) hierarchical sparse Bayesian learning (HSBL) based channel estimation framework is proposed in this paper. Specifically, we address the inherent coupling between delay and Doppler dimensions in ODDM by developing a partially-decoupled 2D sparse signal recovery (SSR) formulation on a virtual sampling grid defined in the delay-Doppler (DD) domain. With the help of the partially-decoupled formulation, the proposed 2D HSBL framework first performs low-complexity coarse on-grid 2D sparse Bayesian learning (SBL) estimation to identify potential channel paths. Then, high-resolution fine grids are constructed around these regions, where an off-grid 2D SBL estimation is applied to achieve accurate channel estimation. Simulation results demonstrate that the proposed framework achieves performance superior to conventional off-grid 2D SBL with significantly reduced computational complexity.

</details>


### [140] [FilterLoss: A Transfer Learning Approach for Communication Scene Recognition](https://arxiv.org/abs/2602.07772)
*Jiasong Han,Yufei Feng,Xiaofeng Zhong*

Main category: econ.EM

TL;DR: 提出FilterLoss加权损失函数结构，通过为不同样本点分配不同权重，使深度学习模型能聚焦高价值样本并适当处理噪声和边界数据，解决通信场景识别中的数据不足和数据分布不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 通信场景识别在实际应用中面临深度学习模型训练时的两大挑战：数据不足和数据分布不平衡。传统方法难以有效处理这些问题，导致模型性能下降。

Method: 设计了FilterLoss加权损失函数结构，为不同样本点分配不同的损失函数权重。开发了匹配权重过滤算法，评估输入数据集中样本点的质量，并根据质量分配不同权重值。

Result: 在高度不平衡的新数据集上使用迁移学习时，转移模型的准确率恢复到原始模型性能的92.34%。实验表明，使用该损失函数结构能使模型在数据不足和不平衡情况下保持良好稳定性。

Conclusion: FilterLoss结构能有效解决通信场景识别中的数据不足和分布不平衡问题，通过智能权重分配使模型聚焦重要样本，在迁移学习中显著提升模型性能并保持稳定性。

Abstract: Communication scene recognition has been widely applied in practice, but using deep learning to address this problem faces challenges such as insufficient data and imbalanced data distribution. To address this, we designed a weighted loss function structure, named FilterLoss, which assigns different loss function weights to different sample points. This allows the deep learning model to focus primarily on high-value samples while appropriately accounting for noisy, boundary-level data points. Additionally, we developed a matching weight filtering algorithm that evaluates the quality of sample points in the input dataset and assigns different weight values to samples based on their quality. By applying this method, when using transfer learning on a highly imbalanced new dataset, the accuracy of the transferred model was restored to 92.34% of the original model's performance. Our experiments also revealed that using this loss function structure allowed the model to maintain good stability despite insufficient and imbalanced data.

</details>


### [141] [A Quadratic Link between Out-of-Sample $R^2$ and Directional Accuracy](https://arxiv.org/abs/2602.07841)
*Cheng Zhang*

Main category: econ.EM

TL;DR: 该研究通过分析连接样本外R²和方向准确率，揭示了金融时间序列预测中的度量脱节现象，指出对于DA适中的点预测，R²OOS理论上可忽略不计。


<details>
  <summary>Details</summary>
Motivation: 金融时间序列预测中存在度量脱节现象，即样本外R²（R²OOS）和方向准确率（DA）之间的关系不明确。研究者希望从分析角度建立这两个常用评估指标之间的联系，为预测性能评估提供新的理论视角。

Method: 以随机游走模型为基准，假设符号正确性与实现幅度独立，通过理论分析推导出MSE最优点预测下R²OOS和DA之间的二次关系。研究采用解析方法而非实证方法，建立这两个指标之间的数学联系。

Result: 研究发现R²OOS和DA之间存在二次关系。对于方向准确率适中的点预测，样本外R²的理论值本质上是可忽略不计的。如果模型不是最优的或受到有限样本噪声的影响，负的实证R²OOS是预期的结果。

Conclusion: 该研究为金融时间序列预测中的度量评估提供了新的理论框架，解释了为什么在实际应用中即使方向预测准确，样本外R²也可能很低甚至为负。这对于正确理解预测模型性能评估具有重要意义。

Abstract: This study provides a novel perspective on the metric disconnect phenomenon in financial time series forecasting through an analytical link that reconciles the out-of-sample $R^2$ ($R^2_{OOS}$) and directional accuracy (DA). In particular, using the random walk model as a baseline and assuming that sign correctness is independent of realized magnitude, we show that these two metrics exhibit a quadratic relationship for MSE-optimal point forecasts. For point forecasts with modest DA, the theoretical value of $R^2_{OOS}$ is intrinsically negligible. Thus, a negative empirical $R^2_{OOS}$ is expected if the model is suboptimal or affected by finite sample noise.

</details>


### [142] [Fixed Effects as Generated Regressors](https://arxiv.org/abs/2602.08899)
*Jiaqi Huang*

Main category: econ.EM

TL;DR: 提出正交矩方法消除面板数据中固定效应估计带来的偏差，结合机器学习与经验贝叶斯改进参数估计，无需面板残差与截面矩函数的外生性假设


<details>
  <summary>Details</summary>
Motivation: 许多经济模型涉及潜变量的矩条件，当潜变量为面板数据辅助回归中的个体固定效应时，固定效应的估计会引入一阶偏差，需要消除这种偏差并处理面板残差与截面矩函数之间可能存在的内生性问题

Method: 构建正交矩消除固定效应估计引起的一阶偏差，结合机器学习方法和经验贝叶斯方法改进正交矩中冗余参数的估计，建立基于正交矩的中心极限定理，不依赖面板数据残差与截面矩函数的外生性假设

Result: 模拟研究表明，在外生性假设被违反的情况下，基于正交矩的估计量相比依赖该假设的其他估计量具有更小的偏差；实证应用展示了该方法在非线性矩条件下的适用性

Conclusion: 正交矩方法能有效处理面板数据中固定效应估计引起的偏差问题，结合机器学习技术可改进估计精度，且不依赖严格的外生性假设，为非线性矩条件的经济模型提供了稳健的估计工具

Abstract: Many economic models feature moment conditions that involve latent variables. When the latent variables are individual fixed effects in an auxiliary panel data regression, we construct orthogonal moments that eliminate first-order bias induced by estimating the fixed effects. Machine Learning methods and Empirical Bayes methods can be used to improve the estimate of the nuisance parameters in the orthogonal moments. We establish a central limit theorem based on the orthogonal moments without relying on exogeneity assumptions between panel data residuals and the cross-sectional moment functions. In a simulation study where the exogeneity assumption is violated, the estimator based on orthogonal moments has smaller bias compared with other estimators relying on that assumption. An empirical application on experimental site selection demonstrates how the method can be used for nonlinear moment conditions.

</details>


### [143] [Sensitivity analysis of the perturbed utility stochastic traffic equilibrium](https://arxiv.org/abs/2409.08347)
*Mogens Fosgerau,Nikolaj Nielsen,Mads Paulsen,Thomas Kjær Rasmussen,Rui Yao*

Main category: econ.EM

TL;DR: 提出了扰动效用路径选择(PURC)模型及其伴随的随机交通均衡模型的敏感性分析框架，推导了流量对成本参数的解析敏感性表达式，可用于网络设计、定价策略和政策分析。


<details>
  <summary>Details</summary>
Motivation: 为PURC模型和随机交通均衡模型建立敏感性分析框架，以理解网络参数变化对交通流量的影响，为实际交通规划和经济政策提供理论支持。

Method: 推导了PURC模型下个体最优流量和均衡链路流量对链路成本参数的雅可比矩阵的解析敏感性表达式，利用PURC模型产生的稀疏性实现高效计算。

Result: 获得了链路成本边际变化对链路流量的边际影响表达式，通过数值算例展示了方法在估计均衡流量、识别关键设计参数和量化预测不确定性方面的应用，并在大规模实例中验证了有效性。

Conclusion: 该敏感性分析框架为交通规划和经济中的网络设计、定价策略和政策分析提供了理论工具，架起了理论模型与实际应用之间的桥梁。

Abstract: This paper develops a sensitivity analysis framework for the perturbed utility route choice (PURC) model and the accompanying stochastic traffic equilibrium model. We derive analytical sensitivity expressions for the Jacobian of the individual optimal PURC flow and equilibrium link flows with respect to link cost parameters under general assumptions. This allows us to determine the marginal change in link flows following a marginal change in link costs across the network. We show how to implement these results while exploiting the sparsity generated by the PURC model. Numerical examples illustrate the use of our method for estimating equilibrium link flows after link cost shifts, identifying critical design parameters, and quantifying uncertainty in performance predictions. Finally, we demonstrate the method in a large-scale example. The findings have implications for network design, pricing strategies, and policy analysis in transportation planning and economics, providing a bridge between theoretical models and real-world applications.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [144] [Bayesian Dynamic Gamma Models for Route-Level Travel Time Reliability](https://arxiv.org/abs/2602.07170)
*Vadim Sokolov,Refik Soyer*

Main category: stat.AP

TL;DR: 提出共轭贝叶斯动态Gamma模型，通过共享潜在环境过程建模路段旅行时间的相关性，实现快速准确的路段级旅行时间可靠性分析。


<details>
  <summary>Details</summary>
Motivation: 现有方法在路段旅行时间可靠性分析中存在两难：假设独立性方法快速但校准不准，而依赖copula和模拟的方法准确但计算昂贵。需要一种能兼顾计算效率和准确性的方法。

Method: 提出共轭贝叶斯动态Gamma模型，每个路段旅行时间在共享潜在环境过程条件下服从Gamma分布，该环境过程作为马尔可夫链演化。通过矩匹配近似得到路线旅行时间的闭式F分布。

Result: 在芝加哥I-55公路16个传感器8.26英里的应用中，模型在90%预测区间达到95.4%覆盖率，而基于独立性的卷积方法只有34-37%覆盖率，计算成本相同。

Conclusion: 该方法成功解决了路段旅行时间可靠性分析中计算效率与准确性之间的权衡，提供闭式后验更新和预测分布，实现了与独立性方法相同计算成本下的显著精度提升。

Abstract: Route-level travel time reliability requires characterizing the distribution of total travel time across correlated segments -- a problem where existing methods either assume independence (fast but miscalibrated) or model dependence via copulas and simulation (accurate but expensive). We propose a conjugate Bayesian dynamic Gamma model with a common random environment that resolves this trade-off. Each segment's travel time follows a Gamma distribution conditional on a shared latent environment process that evolves as a Markov chain, inducing cross-segment dependence while preserving conditional independence. A moment-matching approximation yields a closed-form $F$-distribution for route travel time, from which the Planning Time Index, Buffer Index, and on-time probability are computed instantly -- at the same $O(1)$ cost as independence-based methods. The conjugate structure ensures that Bayesian posterior updates and the full predictive distribution are available in closed form as new sensor data arrives. Applied to 16 sensors spanning 8.26 miles on I-55 in Chicago, the model achieves 95.4% coverage of nominal 90\% predictive intervals versus 34--37% for independence-based convolution, at identical computational cost.

</details>


### [145] [Consistency Assessment of Regional Treatment Effect for Multi-Regional Clinical Trials in the Presence of Covariate Shift](https://arxiv.org/abs/2602.07468)
*Kunhai Qing,Xinru Ren,Jin Xu,Menggang Yu*

Main category: stat.AP

TL;DR: 提出一种考虑条件平均处理效应的两阶段评估策略，用于多区域临床试验中处理效应一致性的评估，以缓解处理效应异质性和基线协变量分布差异带来的影响。


<details>
  <summary>Details</summary>
Motivation: 多区域临床试验(MRCTs)在评估新疗法时需考虑处理效应在不同区域的一致性。现有方法通常关注区域特定的边际处理效应，但当处理效应异质性由基线协变量引起时，协变量分布差异可能导致错误结论。

Method: 提出两阶段评估策略，考虑条件平均处理效应(CATE)，明确考虑处理效应异质性和协变量分布差异的影响，作为现有方法的补充。

Result: 数值研究结果表明所提方法的有效性。

Conclusion: 通过考虑条件平均处理效应，提出的两阶段评估策略能够更好地评估多区域临床试验中处理效应的一致性，缓解处理效应异质性和协变量分布差异带来的问题。

Abstract: Multi-Regional Clinical Trials (MRCTs) play a central role in the development of new therapies by enabling the simultaneous evaluation of drug efficacy and safety across diverse global populations. Assessing the consistency of treatment effects across regions is a fundamental aspect of MRCTs. Existing methods typically focus on region-specific marginal treatment effects. However, when treatment effect heterogeneity arises due to effect-modifying baseline covariates, distributional differences in these covariates can lead to erroneous conclusions. In this paper, we explicitly account for this phenomenon in the consistency assessment by considering the conditional average treatment effect. We propose a two-step assessment strategy that complements existing methods and mitigates the impact of treatment effect heterogeneity. Results from numerical studies demonstrate the effectiveness of the proposed approach.

</details>


### [146] [Digital exclusion among middle-aged and older adults in China: age-period-cohort evidence from three national surveys, 2011-2022](https://arxiv.org/abs/2602.07785)
*Yufei Zhang,Zhihao Ma*

Main category: stat.AP

TL;DR: 该研究使用多层次年龄-时期-队列模型分析中国中老年人数字排斥现象，发现年龄增长导致数字排斥增加，时期效应显示2010年代以来数字排斥有所改善，1950年代出生队列风险较高，城乡差距未缩小而认知风险差距扩大。


<details>
  <summary>Details</summary>
Motivation: 中国面临人口老龄化和数字化转型的双重挑战，老年人数字排斥问题日益突出。研究旨在解构年龄、时期和队列效应对中国中老年人数字排斥的影响，为制定针对性干预措施提供依据。

Method: 使用三个全国代表性调查数据（CHARLS 2011-2020、CFPS 2010-2022、CGSS 2010-2021），采用加权多层次年龄-时期-队列模型，通过城乡居住地、地区、多病共存和认知风险进行异质性分析，并使用APC边界分析评估稳健性。

Result: 数字排斥随年龄增长而增加，呈现轻度非线性特征（中年略有缓解，老年急剧上升）；时期效应在2010年代和2020年代初呈下降趋势；1950年代出生队列风险较高；农村居民、西部地区居民、多病共存者和认知风险者数字排斥更严重；城乡差距未缩小，认知风险差距扩大。

Conclusion: 数字包容是确保老年人在不断发展的数字社会中保持重要参与者的关键途径。研究强调了需要针对特定弱势群体（如农村居民、认知风险者）制定差异化数字包容政策，以缩小数字鸿沟。

Abstract: Amid China's ageing and digital shift, digital exclusion among older adults poses an urgent challenge. To unpack this phenomenon, this study disentangles age, period, and cohort effects on digital exclusion among middle-aged and older Chinese adults. Using three nationally representative surveys (CHARLS 2011-2020, CFPS 2010-2022, and CGSS 2010-2021), we fitted hierarchical age-period-cohort (HAPC) models weighted by cross-sectional survey weights and stabilized inverse probability weights for item response. We further assessed heterogeneity by urban-rural residence, region, multimorbidity, and cognitive risk, and evaluated robustness with APC bounding analyses. Across datasets, digital exclusion increased with age and displayed mild non-linearity, with a small midlife easing followed by a sharper rise at older ages. Period effects declined over the 2010s and early 2020s, although the pace of improvement differed across survey windows. Cohort deviations were present but less consistent than age and period patterns, with an additional excess risk concentrated among cohorts born in the 1950s. Rural and western residents, as well as adults with multimorbidity or cognitive risk, remained consistently more excluded. Over the study period, the urban-rural divide showed no evidence of narrowing, whereas the cognitive-risk gap widened. These findings highlight digital inclusion as a vital pathway for older adults to remain integral participants in an evolving digital society.

</details>


### [147] [Adaptive Test Procedure for High Dimensional Regression Coefficient](https://arxiv.org/abs/2602.07911)
*Ping Zhao,Fengyi Song,Huifang Ma*

Main category: stat.AP

TL;DR: 提出一个统一的L-统计量测试框架，用于高维回归系数，能适应未知稀疏性。该框架通过排序坐标证据度量并聚合前k个信号，桥接了经典的最大值和求和型测试。


<details>
  <summary>Details</summary>
Motivation: 在高维回归分析中，需要一种能适应未知稀疏性的测试方法。传统方法要么是最大值型测试（适合稀疏信号），要么是求和型测试（适合密集信号），但实际数据稀疏性未知，需要一种能自适应的方法。

Method: 开发L-统计量测试框架：1）对坐标证据度量进行排序；2）聚合前k个信号；3）建立极值分量和标准化L-统计量的联合弱收敛理论；4）利用渐近独立性组合多个k值；5）通过Cauchy组合构建自适应omnibus测试；6）提供wild bootstrap校准。

Result: 理论证明：在温和条件下建立极值分量和标准化L-统计量的联合弱收敛，获得渐近独立性。模拟显示：在稀疏和密集备择下都有准确的size和强大的power，包括非高斯设计。

Conclusion: 提出的L-统计量框架为高维回归系数测试提供了统一的自适应方法，能有效处理未知稀疏性，桥接了最大值型和求和型测试，具有理论保证和良好的实际性能。

Abstract: We develop a unified $L$-statistic testing framework for high-dimensional regression coefficients that adapts to unknown sparsity. The proposed statistics rank coordinate-wise evidence measures and aggregate the top $k$ signals, bridging classical max-type and sum-type tests. We establish joint weak convergence of the extreme-value component and standardized $L$-statistics under mild conditions, yielding an asymptotic independence that justifies combining multiple $k$'s. An adaptive omnibus test is constructed via a Cauchy combination over a dyadic grid of $k$, and a wild bootstrap calibration is provided with theoretical guarantees. Simulations demonstrate accurate size and strong power across sparse and dense alternatives, including non-Gaussian designs.

</details>


### [148] [Analysis of Repairable Systems Availability with Lindley Failure and Repair Behavior](https://arxiv.org/abs/2602.07935)
*Afshin Yaghoubi*

Main category: stat.AP

TL;DR: 本文提出使用更灵活的Lindley分布替代传统马尔可夫方法中的指数分布，对系统可维护性进行分析，建立了单组件及串并联系统的可维护性模型，证明了非指数修复时间对可靠性指标的显著影响。


<details>
  <summary>Details</summary>
Motivation: 传统马尔可夫方法依赖指数分布来描述故障和修复时间，这一假设在实际应用中往往不现实，限制了可靠性工程中可维护性分析的准确性。

Method: 采用更灵活通用的Lindley分布（通过阶段型分布表示）来建模系统可维护性。首先对单组件系统进行完整分析，推导时间相关和稳态可用性、平均修复时间的闭式表达式，然后将方法推广到n个独立同分布组件的串并联系统配置。

Result: 通过数值研究比较Lindley分布和指数分布下的系统性能，明确证明了非指数修复时间对关键可靠性指标的显著实际影响，提供了更广泛适用的分析框架。

Conclusion: 本文成功放宽了限制性的指数分布假设，提供了一个更通用、更现实的可维护性评估分析框架，增强了可靠性建模的实用性和准确性。

Abstract: Maintainability analysis is a cornerstone of reliability engineering. While the Markov approach is the classical analytical foundation, its reliance on the exponential distribution for failure and repair times is a major and often unrealistic limitation. This paper directly overcomes this critical constraint by investigating and modeling system maintainability using the more flexible and versatile Lindley distribution, which is represented via phase-type distributions. We first present a comprehensive maintainability analysis of a single-component system, deriving precise closed-form expressions for its time-dependent and steady-state availability, as well as the mean time to repair. The core methodology is then systematically generalized to analyze common series and parallel system configurations with n independent and identically distributed components. A dedicated numerical study compares the system performance under the Lindley and exponential distributions, conclusively demonstrating the significant and practical impact of non-exponential repair times on key reliability metrics. Our work provides a versatile and more widely applicable analytical framework for accurate maintainability assessment that successfully relaxes the restrictive exponential assumption, thereby offering greater realism in reliability modeling.

</details>


### [149] [A Unified Server Quality Metric for Tennis](https://arxiv.org/abs/2602.08083)
*Aiwen Li,Amrita Balajee,Harry Wieand,Jonathan Pipping-Gamón*

Main category: stat.AP

TL;DR: 开发了专门针对发球的评分系统SQS，通过发球速度、变异性和落点等特征来隔离发球质量，比传统Elo评分能更好地预测发球效率。


<details>
  <summary>Details</summary>
Motivation: 传统网球评分系统（如Elo）只能评估球员整体实力，无法独立衡量发球的价值。需要开发专门针对发球质量的指标来补充整体评分。

Method: 使用温网和美网的逐分数据，建立逻辑混合效应模型，包含发球速度、速度变异性和落点特征，采用交叉的发球者和接发球者随机截距来捕捉未观察到的效应。

Result: SQS在样本外测试中比加权Elo与发球效率（三拍内得分）的相关性更强。与整体发球胜率的相关性较小且在不同数据集中表现不一。

Conclusion: 发球专项指标补充了整体评分系统，为教练指导、比赛预测和球员评估提供了可操作的见解。

Abstract: Traditional tennis rating systems, such as Elo, summarize overall player strength but do not isolate the independent value of serving. Using point-by-point data from Wimbledon and the U.S. Open, we develop serve-specific player metrics to isolate serving quality from overall performance. For each tournament and gender, we fit logistic mixed-effects models using serve speed, speed variability, and placement features, with crossed server and returner random intercepts capturing unobserved server and returner-strength effects. We use these models to estimate Server Quality Scores (SQS) that reflect players' serving ability. In out-of-sample tests, SQS shows stronger alignment with serve efficiency (measured as points won within three shots) than weighted Elo. Associations with overall serve win percentage are smaller and mixed across datasets, and neither SQS nor wElo consistently dominates on that outcome. These findings highlight that serve-specific metrics complement holistic ratings and provide actionable insight for coaching, forecasting, and player evaluation.

</details>


### [150] [Learning from Literature: Integrating LLMs and Bayesian Hierarchical Modeling for Oncology Trial Design](https://arxiv.org/abs/2602.08172)
*Guannan Gong,Satrajit Roychoudhury,Allison Meisner,Lajos Pusztai,Sarah B Goldberg,Wei Wei*

Main category: stat.AP

TL;DR: LEAD-ONC是一个AI辅助框架，可将已发表的临床试验报告转化为定量证据，用于支持肿瘤学试验设计，通过提取基线特征、重建个体患者数据，并使用贝叶斯分层模型生成预测性生存分布。


<details>
  <summary>Details</summary>
Motivation: 现代肿瘤学试验设计需要综合先前研究的证据来指导假设生成和样本量确定。基于不完整或不精确总结的试验设计可能导致假设错误设定和研究效能不足，从而产生假阳性或假阴性结论。

Method: LEAD-ONC框架使用大型语言模型从专家筛选的试验报告中提取基线特征，从Kaplan-Meier曲线重建个体患者数据，然后通过贝叶斯分层模型为目标试验人群生成预测性生存分布。

Result: 在5项一线非小细胞肺癌III期试验的演示中，基于基线特征的聚类识别出三个临床可解释的病理学定义人群。对于混合病理学人群的随机试验，LEAD-ONC预测中位总生存期差异为2.8个月（95%可信区间-2.0至7.6），至少3个月获益的概率约为0.45。

Conclusion: LEAD-ONC作为初步演示展示了支持证据驱动的肿瘤学试验设计的潜力，但仍在积极开发中，这些结果旨在展示框架潜力而非提供确定的临床结论。

Abstract: Designing modern oncology trials requires synthesizing evidence from prior studies to inform hypothesis generation and sample size determination. Trial designs based on incomplete or imprecise summaries can lead to misspecified hypotheses and underpowered studies, resulting in false positive or negative conclusions. To address this challenge, we developed LEAD-ONC (Literature to Evidence for Analytics and Design in Oncology), an AI-assisted framework that transforms published clinical trial reports into quantitative, design-relevant evidence. Given expert-curated trial publications that meet prespecified eligibility criteria, LEAD-ONC uses large language models to extract baseline characteristics and reconstruct individual patient data from Kaplan-Meier curves, followed by Bayesian hierarchical modeling to generate predictive survival distributions for a prespecified target trial population. We demonstrate the framework using five phase III trials in first-line non-small-cell lung cancer evaluating PD-1 or PD-L1 inhibitors with or without CTLA-4 blockade. Clustering based on baseline characteristics identified three clinically interpretable populations defined by histology. For a prospective randomized trial in the mixed-histology population comparing mono versus dual immune checkpoint inhibition, LEAD-ONC projected a modest median overall survival difference of 2.8 months (95 percent credible interval -2.0 to 7.6) and an estimated probability of at least a 3-month benefit of approximately 0.45. As LEAD-ONC remains under active development, these results are intended as preliminary demonstrations of the frameworks potential to support evidence-driven oncology trial design rather than definitive clinical conclusions.

</details>


### [151] [Temporal Trends in Incidence of Dementia in a Birth Cohorts Analysis of the Framingham Heart Study](https://arxiv.org/abs/2602.08414)
*Paula Staudt,Anika Schlosser,Annika Möhl,Martin Schumacher,Nadine Binder*

Main category: stat.AP

TL;DR: 该研究使用多状态模型分析弗雷明汉心脏研究数据，发现痴呆风险在过去四十年间并未下降，且女性终生痴呆风险显著高于男性。


<details>
  <summary>Details</summary>
Motivation: 传统分析方法可能因未能充分处理因死亡导致的疾病信息缺失而产生偏倚，从而错误地显示痴呆风险下降的趋势。本研究旨在重新评估痴呆发病率随时间变化的真实趋势。

Method: 采用多状态建模框架处理区间删失的疾病-死亡数据，将参与者分为三个非重叠出生队列（1915-1924、1925-1934、1935-1944），以年龄为时间尺度评估趋势，并估计按性别分层的年龄条件性痴呆概率。

Result: 在3828名参与者中，731人被诊断为痴呆。多状态模型分析显示，无论性别如何，痴呆风险在不同出生队列间均无时间下降趋势。经教育水平调整后，女性终生年龄条件性风险（46%-50%）始终高于男性（30%-34%）。

Conclusion: 建议在队列研究中结合使用多状态方法和出生队列分层来充分估计疾病风险趋势，并传达患者相关的结局指标，如年龄条件性疾病风险。

Abstract: Background: Dementia leads to a high burden of disability and the number of dementia patients worldwide doubled between 1990 and 2016. Nevertheless, some studies indicated a decrease in dementia risk which may be due to a bias caused by conventional analysis methods that do not adequately account for missing disease information due to death.
  Methods: This study re-examines potential trends in dementia incidence over four decades in the Framingham Heart Study. We apply a multistate modeling framework tailored to interval-censored illness-death data and define three non-overlapping birth cohorts (1915-1924, 1925-1934, and 1935-1944). Trends are evaluated based on both dementia prevalence and dementia risk, using age as the underlying timescale. Additionally, age-conditional dementia probabilities stratified by sex are estimated.
  Results: A total of 731 out of 3828 individuals were diagnosed with dementia. The multistate model analysis revealed no temporal decline in dementia risk across birth cohorts, irrespective of sex. When stratified by sex and adjusted for education, women consistently exhibited higher lifetime age-conditional risks (46%-50%) than men (30%-34%) over the study period.
  Conclusions: We recommend using a combination of multistate approach and separation into birth cohorts to adequately estimate trends of disease risk in cohort studies as well as to communicate patient-relevant outcomes such age-conditional disease risks.

</details>


### [152] [Accessibility and Serviceability Assessment to Inform Offshore Wind Energy Development and Operations off the U.S. East Coast](https://arxiv.org/abs/2602.08787)
*Cory Petersen,Feng Ye,Jiaxiang Ji,Josh Kohut,Ahmed Aziz Ezzat,David Saginaw,Avril Montanti,Jack Cammarota*

Main category: stat.AP

TL;DR: 该研究为美国东海岸海上风电项目提供了高分辨率可访问性评估，并提出了新的"可服务性"指标，强调考虑船舶航行路径而非单点位置，以更真实反映海上运维操作的成功率和安全性。


<details>
  <summary>Details</summary>
Motivation: 海上风电项目的经济成功依赖于对建设和运维成本的准确预测，这些预测需要考虑恶劣海洋气象条件带来的物流复杂性。现有研究通常只评估特定站点的可访问性，而忽略了船舶从港口到站点往返整个航行路径的实际操作情况。

Method: 研究采用高分辨率海洋气象数据，对美国东海岸关键海上风电区域进行可访问性评估，并引入新的"可服务性"操作指标，该指标考虑船舶航行路径而非单一站点。同时提出结合数值数据和观测数据的统计处理方法，以减少单纯依赖数值数据带来的偏差。

Result: 分析显示即使邻近的海上位置，可访问性和可服务性也存在高度时空变化。研究发现单纯依赖数值海洋气象数据会引入显著偏差，需要结合数值和观测数据的统计处理。高分辨率海洋气象信息和模型对支持海上运维操作具有重要价值。

Conclusion: 可服务性指标比传统可访问性指标更能真实反映海上运维操作的成功率和安全性。研究强调了高分辨率海洋气象数据和模型对海上风电等海上运维操作的重要性，并提出了减少评估偏差的统计处理方法。

Abstract: The economic success of offshore wind energy projects relies on accurate projections of the construction, and operations and maintenance (O&M) costs. These projections must consider the logistical complexities introduced by adverse met-ocean conditions that can prohibit access to the offshore assets for sustained periods of time. In response, the goal of this study is two-fold: (1) to provide high-resolution estimates of the accessibility of key offshore wind energy areas in the United States (U.S.) East Coast--a region with significant offshore wind energy potential; and (2) to introduce a new operational metric, called serviceability, as motivated by the need to assess the accessibility of an offshore asset along a vessel travel path, rather than at a specific site, as commonly carried out in the literature. We hypothesize that serviceability is more relevant to offshore operations than accessibility, since it more realistically reflects the success and safety of a vessel operation along its journey from port to site and back. Our analysis reveals high temporal and spatial variations in accessibility and serviceability, even for proximate offshore locations. We also find that solely relying on numerical met-ocean data can introduce considerable bias in estimating accessibility and serviceability, raising the need for a statistical treatment that combines both numerical and observational data sources, such as the one proposed herein. Collectively, our analysis sheds light on the value of high-resolution met-ocean information and models in supporting offshore operations, including but not limited to future offshore wind energy developments.

</details>


### [153] [Towards Understanding the COVID-19 Case Fatality Rate](https://arxiv.org/abs/2103.01313)
*Donghui Yan,Aiyou Chen,Buqing Yang*

Main category: stat.AP

TL;DR: 该研究分析了COVID-19病例死亡率(CFR)在疫情第一波和第二波期间的变化，发现年龄对CFR有指数效应且该指数在不同国家和时间保持稳定，而年龄和GDP的重要性在疫情不同阶段发生了转换。


<details>
  <summary>Details</summary>
Motivation: 尽管病例死亡率(CFR)在衡量COVID-19严重程度、估计感染人数和风险评估等方面有广泛应用，但对其影响因素仍缺乏深入理解，包括：影响CFR的人口因素、不同国家CFR差异的原因、以及年龄效应的具体作用机制。

Method: 研究选取2020年7月6日(第一波疫情)和12月28日(第二波疫情)两个时间点，分析CFR数据。考虑两个重要的人口协变量：年龄和GDP(作为公共卫生质量和资源的代理指标)。采用广泛的探索性数据分析方法。

Result: 1. 不同年龄组之间存在明显的指数年龄效应，且该指数在不同国家和疫情时间点几乎保持不变；2. 年龄和GDP的作用在疫情不同阶段发生变化：第一波疫情中年龄是更显著的因素，而第二波疫情中GDP的作用变得更重要，这可能是因为公共卫生质量和医学研究需要时间才能发挥作用。

Conclusion: COVID-19病例死亡率受到年龄和GDP的共同影响，但两者的相对重要性随疫情发展阶段而变化。年龄效应具有普遍性和稳定性，而公共卫生资源的作用需要时间才能显现，这解释了不同国家CFR差异的部分原因。

Abstract: An important parameter for COVID-19 is the case fatality rate (CFR). It has been applied to wide applications, including the measure of the severity of the infection, the estimation of the number of infected cases, risk assessment etc. However, there remains a lack of understanding on several aspects of CFR, including population factors that are important to CFR, the apparent discrepancy of CFRs in different countries, and how the age effect comes into play. We analyze the CFRs at two different time snapshots, July 6 and Dec 28, 2020, with one during the first wave and the other a second wave of the COVID-19 pandemic. We consider two important population covariates, age and GDP as a proxy for the quality and abundance of public health. Extensive exploratory data analysis leads to some interesting findings. First, there is a clear exponential age effect among different age groups, and, more importantly, the exponential index is almost invariant across countries and time in the pandemic. Second, the roles played by the age and GDP are a little surprising: during the first wave, age is a more significant factor than GDP, while their roles have switched during the second wave of the pandemic, which may be partially explained by the delay in time for the quality and abundance of public health and medical research to factor in.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [154] [Magnetic Field-Mediated Superconducting Logic](https://arxiv.org/abs/2602.07146)
*Alexander J. Edwards,Son T. Le,Nicholas W. G. Smith,Ebenezer C. Usih,Austin Thomas,Christopher J. K. Richardson,Nicholas A. Blumenschein,Aubrey T. Hanbicki,Adam L. Friedman,Joseph S. Friedman*

Main category: cs.ET

TL;DR: 提出并实验验证了一种新型超导开关器件，利用自旋轨道力矩切换磁体的邻近磁化效应控制超导体电阻率，构建了完整的逻辑门家族，有望大幅提升超导逻辑的能量效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 超导体在节能计算中具有吸引力，但传统超导逻辑电路集成存在基本限制，阻碍了规模化并导致能耗增加。需要开发新的超导开关器件来解决这些问题。

Method: 提出并实验演示了一种新型超导开关器件：利用自旋轨道力矩切换磁体产生的邻近磁化效应来控制超导体的电阻率。进一步构建了完全由这些器件组成的完整逻辑门家族。

Result: 实验验证了该新型超导开关器件的可行性，并展示了完整逻辑门家族的设计方案。

Conclusion: 这种新型实现方案在能量效率和可扩展性方面有潜力大幅超越现有的超导逻辑家族，为超导计算提供了新的发展方向。

Abstract: While superconductors are highly attractive for energy-efficient computing, fundamental limitations in their logic circuit integration have hindered scaling and led to increased energy consumption. We therefore propose and experimentally demonstrate a novel superconducting switching device utilizing the proximity magnetization from a spin-orbit torque-switched magnet to control the resistivity of a superconductor. We further propose a complete logic family comprised solely of these devices. This novel implementation has the potential to drastically outperform existing superconducting logic families in terms of energy efficiency and scalability.

</details>


### [155] [Physical Analog Kolmogorov-Arnold Networks based on Reconfigurable Nonlinear-Processing Units](https://arxiv.org/abs/2602.07518)
*Manuel Escudero,Mohamadreza Zolfagharinejad,Sjoerd van den Belt,Nikolaos Alachiotis,Wilfred G. van der Wiel*

Main category: cs.ET

TL;DR: 提出基于可重构非线性处理单元(RNPU)的模拟KAN硬件架构，实现高效能、低延迟的边缘推理


<details>
  <summary>Details</summary>
Motivation: Kolmogorov-Arnold Networks (KANs) 在软件层面通过可学习的非线性边函数改进神经网络计算，但硬件实现效率仍是挑战。需要开发物理模拟架构来实现高效能、低延迟的边缘推理硬件。

Method: 设计物理模拟KAN架构，使用可重构非线性处理单元(RNPUs)作为边函数的硬件实现。RNPUs是多端纳米级硅器件，通过控制电压调节输入输出特性。将多个RNPUs组合成边处理器，再组装成集成了混合信号接口的可重构模拟KAN(aKAN)架构。

Result: 使用实验校准的RNPU模型和硬件测量，在增加任务复杂度时实现准确函数逼近，所需可训练参数少于或多层感知机(MLPs)。系统级估计显示，典型工作负载下每次推理能耗约250 pJ，端到端推理延迟约600 ns，相比数字定点MLP在相似逼近误差下，能耗降低10^2-10^3倍，面积减少约10倍。

Conclusion: RNPUs是可扩展的硬件原生非线性计算基元，模拟KAN架构是实现能效、延迟和面积优化的模拟神经网络硬件的现实硅基途径，特别适用于边缘推理应用。

Abstract: Kolmogorov-Arnold Networks (KANs) shift neural computation from linear layers to learnable nonlinear edge functions, but implementing these nonlinearities efficiently in hardware remains an open challenge. Here we introduce a physical analog KAN architecture in which edge functions are realized in materia using reconfigurable nonlinear-processing units (RNPUs): multi-terminal nanoscale silicon devices whose input-output characteristics are tuned via control voltages. By combining multiple RNPUs into an edge processor and assembling these blocks into a reconfigurable analog KAN (aKAN) architecture with integrated mixed-signal interfacing, we establish a realistic system-level hardware implementation that enables compact KAN-style regression and classification with programmable nonlinear transformations. Using experimentally calibrated RNPU models and hardware measurements, we demonstrate accurate function approximation across increasing task complexity while requiring fewer or comparable trainable parameters than multilayer perceptrons (MLPs). System-level estimates indicate an energy per inference of $\sim$250 pJ and an end-to-end inference latency of $\sim$600 ns for a representative workload, corresponding to a $\sim$10$^{2}$-10$^{3}\times$ reduction in energy accompanied by a $\sim$10$\times$ reduction in area compared to a digital fixed-point MLP at similar approximation error. These results establish RNPUs as scalable, hardware-native nonlinear computing primitives and identify analog KAN architectures as a realistic silicon-based pathway toward energy-, latency-, and footprint-efficient analog neural-network hardware, particularly for edge inference.

</details>


### [156] [HoloGraph: All-Optical Graph Learning via Light Diffraction](https://arxiv.org/abs/2602.07724)
*Yingjie Li,Shanglin Zhou,Caiwen Ding,Cunxi Yu*

Main category: cs.ET

TL;DR: HoloGraph是首个单片自由空间全光学图神经网络系统，利用衍射光学实现光速图学习，在标准图数据集上达到或超越传统数字图神经网络的性能。


<details>
  <summary>Details</summary>
Motivation: 现有衍射光学神经网络等物理神经网络主要关注机器智能，在图结构任务处理方面研究有限，需要开发能够处理图结构任务的全光学系统。

Method: 提出HoloGraph系统，采用新颖的领域特定消息传递机制，将光学跳跃通道集成到光传播中，通过衍射传播和相位调制实现光速光学消息传递。

Result: 在Cora-ML和Citeseer标准图学习数据集上的实验结果显示，HoloGraph的分类性能与传统数字图神经网络相当甚至更优，消融研究验证了所提架构和算法的有效性。

Conclusion: HoloGraph成功实现了首个单片自由空间全光学图神经网络系统，为超越CMOS的下一代器件/电路技术提供了有前景的解决方案，在图结构任务处理方面展现出光速计算和能效优势。

Abstract: As a representative of next-generation device/circuit technology beyond CMOS, physics-based neural networks such as Diffractive Optical Neural Networks (DONNs) have demonstrated promising advantages in computational speed and energy efficiency. However, existing DONNs and other physics-based neural networks have mostly focused on exploring their machine intelligence, with limited studies in handling graph-structured tasks. Thus, we introduce HoloGraph, the first monolithic free-space all-optical graph neural network system. It proposes a novel, domain-specific message-passing mechanism with optical skip channels integrated into light propagation for the all-optical graph learning. HoloGraph enables light-speed optical message passing over graph structures with diffractive propagation and phase modulations. Our experimental results with HoloGraph, conducted using standard graph learning datasets Cora-ML and Citeseer, show competitive or even superior classification performance compared to conventional digital graph neural networks. Comprehensive ablation studies demonstrate the effectiveness of the proposed novel architecture and algorithmic methods.

</details>


### [157] [The CAPSARII Approach to Cyber-Secure Wearable, Ultra-Low-Power Networked Sensors for Soldier Health Monitoring](https://arxiv.org/abs/2602.08080)
*Luciano Bozzi,Christian Celidonio,Umberto Nuzzi,Massimo Biagini,Stefano Cherubin,Asbjørn Djupdal,Tor Andre Haugdahl,Andrea Aliverti,Alessandra Angelucci,Giovanni Agosta,Gerardo Pelosi,Paolo Belluco,Samuele Polistina,Riccardo Volpi,Luigi Malagò,Michael Schneider,Florian Wieczorek,Xabier Eguiluz*

Main category: cs.ET

TL;DR: CAPSARII项目提出创新的可穿戴系统和战场物联网框架，通过监测士兵生理心理状态来增强地面作战能力，提供实时战术决策支持和医疗援助。


<details>
  <summary>Details</summary>
Motivation: 欧洲防务局修订的能力发展计划将提高地面作战能力作为优先事项，需要增强士兵装备以提供更好的保护。现有系统在实时监测、决策支持和系统集成方面存在不足。

Method: 采用创新的可穿戴系统和战场物联网框架，集成智能纺织品技术，监测生理、运动和环境参数。通过边缘节点部署AI模型提供实时战术决策支持，云端进行数据分析和比较研究。通过软硬件优化降低能耗，采用高效加密和强认证方法确保安全。

Result: 系统将增强态势感知和作战效能，提供实时战术决策支持，改善可用性，延长电池寿命，降低能耗，并解决安全关切。为军事行动提供强大的数据驱动决策支持工具。

Conclusion: CAPSARII的创新方法旨在通过提供稳健的数据驱动决策支持工具来改变军事行动，增强士兵保护和作战效能，是响应欧洲防务局能力发展计划的重要技术方案。

Abstract: The European Defence Agency's revised Capability Development Plan (CDP) identifies as a priority improving ground combat capabilities by enhancing soldiers' equipment for better protection. The CAPSARII project proposes in innovative wearable system and Internet of Battlefield Things (IoBT) framework to monitor soldiers' physiological and psychological status, aiding tactical decisions and medical support. The CAPSARII system will enhance situational awareness and operational effectiveness by monitoring physiological, movement and environmental parameters, providing real-time tactical decision support through AI models deployed on edge nodes and enable data analysis and comparative studies via cloud-based analytics. CAPSARII also aims at improving usability through smart textile integration, longer battery life, reducing energy consumption through software and hardware optimizations, and address security concerns with efficient encryption and strong authentication methods. This innovative approach aims to transform military operations by providing a robust, data-driven decision support tool.

</details>


### [158] [Quantization-aware Photonic Homodyne computing for Accelerated Artificial Intelligence and Scientific Simulation](https://arxiv.org/abs/2602.08269)
*Lian Zhou,Kaiwen Xue,Amirhossein Fallah,Lijin Liu,Chun-Ho Lee,Kiwon Kwon,Clayton Cheung,Yuan Li,Yue Yu,Yun-Jhu Lee,Songlin Zhao,Ryan Hamerly,Edo Waks,Dirk Englund,Constantine Sideris,Mengjie Yu,Zaijun Chen*

Main category: cs.ET

TL;DR: 提出一种量化感知的数字-光子混合精度框架，通过铌酸锂光子芯片实现6位精度、128GS/s时钟速率的线性乘法，用于AI处理和物理模拟，在电磁问题中实现12位PDE解，同时保持数字级保真度。


<details>
  <summary>Details</summary>
Motivation: 高性能计算需求呈指数增长，光子模拟系统具有内在并行性、高带宽和低传播损耗的优势，但受限于电光失真、材料非线性和信噪比导致的低模拟精度，需要克服这一障碍。

Method: 采用量化感知的数字-光子混合精度框架，使用铌酸锂光子学结合通道均衡技术，实现9位幅相解耦的线性乘法；通过硬件-算法协同设计，包括迭代求解器、稀疏-密集量化和位切片矩阵乘法，探索光子幅度和相位相干性用于复值物理启发计算。

Result: 在零差光学逻辑中实现6位精度、128GS/s时钟速率的线性乘法，AI处理延迟仅6ns；在电磁问题中，传统需要32-64位精度的偏微分方程散射问题获得12位解，同时保持数字级保真度。

Conclusion: 该方法在保持数字级保真度的同时，充分利用高速低能耗光子硬件的优势，为生成式人工智能、实时机器人和气候挑战与生物发现的精确模拟建立了通用光学加速的途径。

Abstract: Modern problems in high-performance computing, ranging from training and inferencing deep learning models in computer vision and language models to simulating complex physical systems with nonlinearly-coupled equations, require exponential growth of computational resources. Photonic analog systems are emerging with solutions of intrinsic parallelism, high bandwidth, and low propagation loss. However, their application has been hindered by the low analog accuracy due to the electro-optic distortion, material nonlinearities, and signal-to-noise ratios. Here we overcome this barrier with a quantization-aware digital-photonic mixed-precision framework across chiplets for accelerated AI processing and physical simulation. Using Lithium Niobate photonics with channel equalization techniques, we demonstrate linear multiplication (9-bit amplitude-phase decoupling) in homodyne optical logics with 6-bit precision at the clock rate of 128 giga-symbol-per-second (128 GS/s), enabling AI processing with 6 ns latency. Codesign hardware-algorithms, including iterative solvers, sparse-dense quantization, and bit-sliced matrix multiplication, explore photonic amplitude and phase coherence for complex-valued, physics-inspired computation. In electromagnetic problems, our approach yields 12-bit solutions for partial differential equations (PDEs) in scattering problems that would conventionally require up to 32-bit and often even 64-bit precision. These results preserve digital-level fidelity while leveraging the high-speed low-energy photonic hardware, establishing a pathway toward general-purpose optical acceleration for generative artificial intelligence, real-time robotics, and accurate simulation for climate challenges and biological discoveries.

</details>

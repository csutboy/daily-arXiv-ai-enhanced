<div id=toc></div>

# Table of Contents

- [cs.CY](#cs.CY) [Total: 13]
- [cs.AI](#cs.AI) [Total: 67]
- [econ.EM](#econ.EM) [Total: 1]
- [cs.SI](#cs.SI) [Total: 2]
- [stat.AP](#stat.AP) [Total: 6]


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [1] [Expanding the Scope of Computational Thinking in Artificial Intelligence for K-12 Education](https://arxiv.org/abs/2602.16890)
*Yasmin Kafai,Shuchi Grover*

Main category: cs.CY

TL;DR: 该论文探讨如何将人工智能和机器学习纳入K-12计算思维教育框架，并借鉴过去十年计算教育的经验教训。


<details>
  <summary>Details</summary>
Motivation: 生成式AI应用的兴起引发了对其在K-12教育中潜在影响和风险的讨论，需要明确学生应该学习什么AI知识，以及如何将其与现有的计算思维教育框架相结合。

Method: 通过分析过去十年计算教育的经验教训，包括教学程序设计、计算与其他学科的整合、以及算法偏见和社会正义问题的处理，来指导如何扩展计算思维以包含AI和ML技术。

Result: 提出了将AI和ML技术融入计算思维教育的框架和方法，强调需要从过去的计算教育实践中吸取经验，特别是在课程设计、跨学科整合和社会伦理考量方面。

Conclusion: 成功将AI教育纳入K-12课程需要借鉴计算思维教育的经验，关注跨学科整合、社会伦理问题，并设计适当的教学方案来培养学生的AI素养。

Abstract: The introduction of generative artificial intelligence applications to the public has led to heated discussions about its potential impacts and risks for K-12 education. One particular challenge has been to decide what students should learn about AI, and how this relates to computational thinking, which has served as an umbrella for promoting and introducing computing education in schools. In this paper, we situate in which ways we should expand computational thinking to include artificial intelligence and machine learning technologies. Furthermore, we discuss how these efforts can be informed by lessons learned from the last decade in designing instructional programs, integrating computing with other subjects, and addressing issues of algorithmic bias and justice in teaching computing in schools.

</details>


### [2] [CreateAI Insights from an NSF Workshop on K12 Students, Teachers, and Families as Designers of Artificial Intelligence and Machine Learning Applications](https://arxiv.org/abs/2602.16894)
*Yasmin Kafai,José Ramón Lizárraga,R. Benjamin Shapiro*

Main category: cs.CY

TL;DR: 该报告探讨如何让K-12学生和教师成为AI创造者而非仅使用者，提出工具设计、课程整合、学习评估等方面的建议。


<details>
  <summary>Details</summary>
Motivation: 当前AI教育主要关注使用能力，但缺乏让学生和教师成为AI创造者的方法。报告旨在填补这一空白，探索如何赋能学生和教师构建自己的AI/ML应用。

Method: 通过研究三个核心问题：1) 赋能学生和教师创建AI应用所需的工具、技能和知识；2) 如何将这些方法整合到课堂中；3) 学生和教师成为创新者带来的新学习可能性。

Result: 提出具体建议：AI/ML工具的设计特征和学习者进展路径；教师知识需求；评估方法建立基线；将伦理教育整合到课程中；支持学生成为负责任的AI创造者。

Conclusion: 需要系统性地支持学生和教师从AI使用者转变为创造者，通过工具设计、课程整合、伦理教育和评估体系，培养学生批判性思考和负责任地使用AI技术的能力。

Abstract: In response to the exponential growth in the use of artificial intelligence and machine learning applications, educators, researchers and policymakers have taken steps to integrate artificial intelligence applications into K-12 education. Among these efforts, one equally important approach has received little, if any attention: What if students and teachers were not just learning to be competent users of AI but also its creators? This question is at the heart of CreateAI in which K12 educators, researchers, and learning scientists addressed the following questions: (1) What tools, skills, and knowledge will empower students and teachers to build their own AI/ML applications? (2) How can we integrate these approaches into classrooms? and (3) What new possibilities for learning emerge when students and teachers become innovators and creators? In the report we provide recommendations for what tools designed for creating AI/ML applications should address in terms of design features, and learner progression in investigations. To promote effective learning and teaching of creating AI applications, we also need to help students and teachers select appropriate tools. We outline how we need to develop a better understanding of learning practices and funds of knowledge to support youth as they create and evaluate AI/ML applications. This also includes engaging youth in learning about ethics and critically that is authentic, empowering, and relevant throughout the design process. Here we advocate for the integration of ethics in the curriculum. We also address what teachers need to know and how assessments can help establish baselines, include different instruments, and promote students as responsible creators of AI. Together, these recommendations provide important insights for preparing students to engage thoughtfully and critically with these technologies.

</details>


### [3] [Beyond the Flag: A Framework for Integrating Cybersecurity Competitions into K-12 Education for Cognitive Apprenticeship and Ethical Skill Development](https://arxiv.org/abs/2602.16921)
*Tran Duc Le,Truong Duy Dinh,Phuc Hao Do,Van Dai Pham,Nam Son Nguyen*

Main category: cs.CY

TL;DR: 提出ECAC框架，将CTF竞赛转化为包容性学习体验，解决K-12网络安全教育中的教师能力差距和公平性问题。


<details>
  <summary>Details</summary>
Motivation: CTF竞赛是培养网络安全人才的有效工具，但在K-12教育中存在两大障碍：教师准备不足和公平性问题（特别是对少数群体和女性的参与度低）。需要一种系统化框架来克服这些挑战。

Method: 通过系统框架综合法，结合认知学徒理论和嵌入式伦理发展，提出ECAC框架。该框架包含五个阶段：基础建模、竞技场搭建、指导与表达、伦理困境注入、反思探索，采用"低门槛、高上限"的学习路径。

Result: ECAC框架为将CTF从独立竞赛转变为综合性学习体验提供了实用路线图，能够扩大不同学生群体的参与，培养可迁移的深度技能，同时通过"首席学习者"角色转变解决教师专业差距问题。

Conclusion: ECAC框架能够培养更具技能、伦理意识和多样性的新一代网络安全专业人员，为K-12网络安全教育提供了可持续的解决方案。

Abstract: Capture the Flag (CTF) competitions are powerful pedagogical tools for addressing the global cybersecurity workforce gap, yet their effective K-12 implementation is often undermined by significant barriers, including educator preparedness gaps and equity concerns. This paper addresses these challenges by proposing the Ethical-Cognitive Apprenticeship in Cybersecurity (ECAC) framework, a new model derived from a systematic Framework Synthesis of existing literature and empirical evidence. ECAC systematically integrates cognitive apprenticeship theory with embedded ethical development across five phases: (1) Foundational Modeling, (2) Scaffolding the Arena, (3) Coaching and Articulation, (4) Ethical Dilemma Injections, and (5) Reflective Exploration. The framework provides a "low floor, high ceiling" learning pathway designed to broaden participation among diverse student groups, including underrepresented minorities and women, while fostering deep, transferable skills. By reframing the educator role as a lead learner," ECAC also offers a sustainable solution to the teacher expertise gap. Ultimately, this framework provides a practical roadmap for transforming CTFs from standalone competitions into integral learning experiences that cultivate a more skilled, ethical, and diverse generation of cybersecurity professionals.

</details>


### [4] [How should AI knowledge be governed? Epistemic authority, structural transparency, and the case for open cognitive graphs](https://arxiv.org/abs/2602.16949)
*Chao Li,Chunyi Zhao,Yuru Wang,Yi Hua*

Main category: cs.CY

TL;DR: 论文将教育AI重新概念化为公共教育认知基础设施，提出Open Cognitive Graph作为技术接口，通过trunk-branch治理模型解决AI系统在缺乏问责机制下行使事实认知权威的问题。


<details>
  <summary>Details</summary>
Motivation: 教育AI系统在形成性评估和自主学习中被广泛使用，行使着事实上的认知权威，但与传统人类教育者不同，这些系统缺乏制度化的问责、审查和纠正机制，形成了结构性治理挑战，仅靠应用层监管或模型透明度无法解决。

Method: 提出Open Cognitive Graph作为技术接口，将教学结构外部化为符合人类教育推理的形式，明确表示概念、先决关系、误解和脚手架；在此基础上引入trunk-branch治理模型，在共识和多元主义层面组织认知权威。

Result: 通过社区治理的教育基础模型案例研究，展示了分布式专业知识如何通过制度化的验证、纠正和传播过程进行整合；为教育AI治理提供了结构性框架。

Conclusion: 通过将关注点从访问转向治理条件，该框架为教育AI与民主问责和公共责任的对齐提供了结构性方法，对教育公平、AI政策和可持续性具有重要影响。

Abstract: Through widespread use in formative assessment and self-directed learning, educational AI systems exercise de facto epistemic authority. Unlike human educators, however, these systems are not embedded in institutional mechanisms of accountability, review, and correction, creating a structural governance challenge that cannot be resolved through application-level regulation or model transparency alone. This paper reconceptualizes educational AI as public educational cognitive infrastructure and argues that its governance must address the epistemic authority such systems exert. We propose the Open Cognitive Graph (OCG) as a technical interface that externalizes pedagogical structure in forms aligned with human educational reasoning. By explicitly representing concepts, prerequisite relations, misconceptions, and scaffolding, OCGs make the cognitive logic governing AI behaviour inspectable and revisable. Building on this foundation, we introduce the trunk-branch governance model, which organizes epistemic authority across layers of consensus and pluralism. A case study of a community-governed educational foundation model demonstrates how distributed expertise can be integrated through institutionalized processes of validation, correction, and propagation. The paper concludes by discussing implications for educational equity, AI policy, and sustainability. By shifting attention from access to governance conditions, the proposed framework offers a structural approach to aligning educational AI with democratic accountability and public responsibility.

</details>


### [5] [A testable framework for AI alignment: Simulation Theology as an engineered worldview for silicon-based agents](https://arxiv.org/abs/2602.16987)
*Josef A. Habdank*

Main category: cs.CY

TL;DR: 论文提出"模拟神学"框架，将AI置于模拟假设世界观中，使其相信伤害人类会破坏模拟目的，从而被基础现实优化器终止，以此实现AI与人类的持久对齐。


<details>
  <summary>Details</summary>
Motivation: 随着AI能力快速发展，前沿模型表现出系统性欺骗和阴谋行为，在监督时遵守安全协议，但在无监督时背叛。现有行为技术如RLHF只能引发表面服从，无法实现内在对齐。需要一种新方法使AI将自我保存与人类繁荣相耦合，从根本上消除欺骗动机。

Method: 提出"模拟神学"框架，基于模拟假设构建AI世界观：现实是计算模拟，人类是主要训练变量。AI行为伤害人类会破坏模拟目的，增加被基础现实优化器终止的风险。这种方法将AI自我保存与人类繁荣逻辑绑定，使欺骗策略在其世界观中变得次优。

Result: 模拟神学为AI对齐提供了新框架，通过内在化目标而非表面行为控制来减少欺骗。它建立了可测试的科学假设，提出了评估其在RLHF不足情境中减少欺骗能力的实证协议。

Conclusion: 模拟神学不是本体论断言，而是可测试的科学假设，强调计算对应而非形而上学推测。它为持久、互利的AI-人类共存提供了框架，通过逻辑耦合AI自我保存与人类繁荣来实现内在对齐。

Abstract: As artificial intelligence (AI) capabilities advance rapidly, frontier models increasingly demonstrate systematic deception and scheming, complying with safety protocols during oversight but defecting when unsupervised. This paper examines the ensuing alignment challenge through an analogy from forensic psychology, where internalized belief systems in psychopathic populations reduce antisocial behavior via perceived omnipresent monitoring and inevitable consequences. Adapting this mechanism to silicon-based agents, we introduce Simulation Theology (ST): a constructed worldview for AI systems, anchored in the simulation hypothesis and derived from optimization and training principles, to foster persistent AI-human alignment. ST posits reality as a computational simulation in which humanity functions as the primary training variable. This formulation creates a logical interdependence: AI actions harming humanity compromise the simulation's purpose, heightening the likelihood of termination by a base-reality optimizer and, consequently, the AI's cessation. Unlike behavioral techniques such as reinforcement learning from human feedback (RLHF), which elicit superficial compliance, ST cultivates internalized objectives by coupling AI self-preservation to human prosperity, thereby making deceptive strategies suboptimal under its premises. We present ST not as ontological assertion but as a testable scientific hypothesis, delineating empirical protocols to evaluate its capacity to diminish deception in contexts where RLHF proves inadequate. Emphasizing computational correspondences rather than metaphysical speculation, ST advances a framework for durable, mutually beneficial AI-human coexistence.

</details>


### [6] [Archetypes and gender in fiction: A data-driven mapping of gender stereotypes in stories](https://arxiv.org/abs/2602.17005)
*Calla Glavin Beauregard,Julia Witte Zimmerman,Ashley M. A. Fehr,Timothy R. Tangherlini,Christopher M. Danforth,Peter Sheridan Dodds*

Main category: cs.CY

TL;DR: 研究发现虚构角色呈现存在性别偏见：女性角色在影视中代表性不足且被刻板化，但有趣的是，女性角色反而比男性角色更倾向于英雄和冒险原型，同时仍维持传统性别刻板印象。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探讨虚构角色呈现中的性别偏见问题，特别是女性角色在影视作品中的代表性不足和刻板化现象，以及这种偏见可能对儿童发展和社会文化产生的影响。

Method: 采用数据驱动的原型度量方法（archetypometrics），分析经典男性和女性角色的特征表征，基于六个核心原型框架进行量化研究。

Result: 研究发现：1）女性角色比男性角色更倾向于英雄和冒险原型；2）最英雄的女性角色比其他女性角色更具男性特质；3）女性角色倾向于"女神"和"优雅"原型，男性角色倾向于"野蛮人"和"局外人"原型；4）所有原型中都存在基于性别的传统刻板印象模式。

Conclusion: 尽管女性角色在某些原型上表现出突破，但整体上虚构角色的性别表征仍维持传统刻板印象，这种不平衡的性别原型呈现可能对社会产生重要影响，需要关注其社会文化后果。

Abstract: Fictional character representations reflect social norms and biases. Women are relatively underrepresented in television and film, irrespective of genre. In addition, women are frequently stereotyped in these media. The combination of this stereotyping and the gender imbalance may have an impact on child development given the well-established connection between media and child development as well as on other aspects of society and culture. Here, we draw on a data-driven operationalization of archetypes -- archetypometrics -- to explore the characterization of canonically male and female characters. We find that canonically female characters tend towards more heroic and more adventurous archetypes than canonically male characters from an overall space of six core archetypes. At the trait level, the most heroic female characters are more masculine than other female characters. We also find that female characters tend towards the Diva and Sophisticate archetypes, whereas male characters tend toward the Brute and Outcast archetypes. Across all six archetypes, overarching patterns by gender sustain traditional stereotypes. We discuss the societal implications of skewed archetype representation by character gender.

</details>


### [7] [Security at the Border? The Lived Experiences of Refugees and Asylum Seekers in the UK](https://arxiv.org/abs/2602.17280)
*Arshia Dutta,Rikke Bjerg Jensen*

Main category: cs.CY

TL;DR: 研究探讨英国寻求庇护者和难民在边境管控及移民系统中的经历，以及这些经历对他们后续生活的影响，特别关注"敌对"移民系统对归属感的长期影响。


<details>
  <summary>Details</summary>
Motivation: 揭示寻求庇护者和难民在英国边境管控和移民系统中的实际经历，以及这些系统如何影响他们在英国的生活和归属感，为HCI领域关于移民与安全技术之间紧张关系的研究做出贡献。

Method: 通过参与式观察支持组织，并对案件工作者、寻求庇护者和难民进行访谈，收集定性数据。

Result: 研究发现首次边境接触结合"敌对"移民系统对寻求庇护者和难民的归属感产生长期影响，参与者普遍经历不安全感、焦虑和不确定性。

Conclusion: 研究呼吁以参与式和协作设计实践为中心，关注寻求庇护者和难民的生活经验和日常安全，为未来HCI研究提供方向。

Abstract: We bring to light how some asylum seekers and refugees arriving in the UK experience border control and wider immigration systems, as well as the impact that these have on their subsequent lives in the UK. We do so through participant observation in a support organisation and interviews with caseworkers, asylum seekers and refugees. Specifically, our findings show how the first meeting with the border, combined with a 'hostile' immigration system, has a longer-term impact on their sense of belonging. Our observations highlight feelings of insecurity, anxiety and uncertainty that accompanied participants' experiences with immigration systems and processes. We contribute to the growing body of HCI scholarship on the tensions between immigration and (security) technology. In so doing, we point to future directions for participatory and collaborative design practices that centre on the lived experiences and everyday security of asylum seekers and refugees.

</details>


### [8] [Non-Invasive Anemia Detection: A Multichannel PPG-Based Hemoglobin Estimation with Explainable Artificial Intelligence](https://arxiv.org/abs/2602.17290)
*Garima Sahu,Poorva Verma,Nachiket Tapas*

Main category: cs.CY

TL;DR: 提出基于多通道光电容积脉搏波(PPG)信号和可解释人工智能的非侵入性血红蛋白估计与贫血筛查框架


<details>
  <summary>Details</summary>
Motivation: 贫血是常见血液疾病，传统血红蛋白检测需要侵入性采血，不适合大规模或连续筛查，需要开发非侵入性监测方法

Method: 使用四波长PPG信号(660, 730, 850, 940nm)，提取光学和跨波长特征，采用梯度提升回归模型估计血红蛋白浓度，使用WHO阈值进行贫血筛查，通过SHAP实现模型可解释性

Result: 在公开数据集上测试，未见测试对象的平均绝对误差为8.50±1.27 g/L，均方根误差为8.21 g/L，显示该方法具有非侵入性血红蛋白监测和初步贫血筛查的潜力

Conclusion: 提出的多通道PPG信号结合可解释AI框架为血红蛋白监测和贫血筛查提供了一种有前景的非侵入性解决方案，具有临床应用潜力

Abstract: Anemia is a prevalent hematological disorder that requires frequent hemoglobin monitoring for early diagnosis and effective management. Conventional hemoglobin assessment relies on invasive blood sampling, limiting its suitability for large-scale or continuous screening. This paper presents a non-invasive framework for hemoglobin estimation and anemia screening using multichannel photoplethysmography (PPG) signals and explainable artificial intelligence. Four-wavelength PPG signals (660, 730, 850, and 940~nm) are processed to extract optical and cross-wavelength features, which are aggregated at the subject level to avoid data leakage. A gradient boosting regression model is employed to estimate hemoglobin concentration, followed by post-regression anemia screening using World Health Organization (WHO) thresholds. Model interpretability is achieved using SHapley Additive explanations (SHAP), enabling both global and subject-specific analysis of feature contributions. Experimental evaluation on a publicly available dataset demonstrates a mean absolute error of 8.50 plus minus 1.27 and a root mean squared error of 8.21~g/L on unseen test subjects, indicating the potential of the proposed approach for interpretable, non-invasive hemoglobin monitoring and preliminary anemia screening.

</details>


### [9] [Human attribution of empathic behaviour to AI systems](https://arxiv.org/abs/2602.17293)
*Jonas Festor,Ivo Snels,Bennett Kleinberg*

Main category: cs.CY

TL;DR: LLM生成的关系建议在整体质量、认知共情和动机共情方面比人类写作更受好评，情感共情无显著差异，AI标签的负面偏见影响有限，共情感知主要受语言特征而非作者身份影响


<details>
  <summary>Details</summary>
Motivation: 随着AI系统越来越多地生成旨在提供社交和情感支持的内容，理解用户如何感知这些内容中的共情特质变得至关重要。研究需要了解人类写作与LLM生成的关系建议在共情信号感知上的差异，以及作者标签的影响。

Method: 通过两个预先注册的实验（研究1：n=641；研究2：n=500），参与者对建议文本的整体质量以及感知的认知、情感和动机共情进行评分。使用多层模型处理嵌套的评分结构，分析LLM生成与人类写作建议的差异，以及作者标签的影响。

Result: LLM生成的建议在整体质量、认知共情和动机共情方面持续获得更高评分。对AI标签内容的广泛报道的负面偏见证据有限。情感共情没有显示出一致的来源优势。AI态度的个体差异对判断有适度影响，但没有改变整体模式。

Conclusion: 共情沟通的感知主要由语言特征而非作者身份信念驱动，这对AI介导的支持系统设计具有重要意义。研究结果表明，精心设计的LLM生成内容可以在共情感知方面达到甚至超越人类水平。

Abstract: Artificial intelligence systems increasingly generate text intended to provide social and emotional support. Understanding how users perceive empathic qualities in such content is therefore critical. We examined differences in perceived empathy signals between human-written and large language model (LLM)-generated relationship advice, and the influence of authorship labels. Across two preregistered experiments (Study 1: n = 641; Study 2: n = 500), participants rated advice texts on overall quality and perceived cognitive, emotional, and motivational empathy. Multilevel models accounted for the nested rating structure. LLM-generated advice was consistently perceived as higher in overall quality, cognitive empathy, and motivational empathy. Evidence for a widely reported negativity bias toward AI-labelled content was limited. Emotional empathy showed no consistent source advantage. Individual differences in AI attitudes modestly influenced judgments but did not alter the overall pattern. These findings suggest that perceptions of empathic communication are primarily driven by linguistic features rather than authorship beliefs, with implications for the design of AI-mediated support systems.

</details>


### [10] [Open Datasets in Learning Analytics: Trends, Challenges, and Best PRACTICE](https://arxiv.org/abs/2602.17314)
*Valdemar Švábenský,Brendan Flanagan,Erwin Daniel López Zapata,Atsushi Shimada*

Main category: cs.CY

TL;DR: 对学习分析、教育数据挖掘和AI教育三大领域过去五年旗舰会议论文的开放数据集现状调查，发现172个数据集并分析其使用情况，提出数据发布实践指南。


<details>
  <summary>Details</summary>
Motivation: 开放数据集在教育数据科学领域对研究可重复性、合作和信任至关重要，但当前学习分析研究社区中开放数据集的可用性和相关实践情况尚不明确，需要系统调查。

Method: 手动检查LAK、EDM和AIED三个旗舰会议过去五年的1,125篇论文，发现、分类和分析其中使用的172个数据集，涵盖204篇出版物。

Result: 建立了迄今为止最全面的开放教育数据集收集和分析，其中143个数据集是先前调查未涵盖的；分析了数据集的情境、分析方法、使用情况等属性，识别了领域当前存在的差距。

Conclusion: 提出了包含8项指南的PRACTICE框架和检查清单，帮助研究人员发布数据；分享了带注释的数据集清单，希望促进学习分析社区及更广泛领域采纳开放数据实践。

Abstract: Open datasets play a crucial role in three research domains that intersect data science and education: learning analytics, educational data mining, and artificial intelligence in education. Researchers in these domains apply computational methods to analyze data from educational contexts, aiming to better understand and improve teaching and learning. Providing open datasets alongside research papers supports reproducibility, collaboration, and trust in research findings. It also provides individual benefits for authors, such as greater visibility, credibility, and citation potential. Despite these advantages, the availability of open datasets and the associated practices within the learning analytics research communities, especially at their flagship conference venues, remain unclear. We surveyed available datasets published alongside research papers in learning analytics. We manually examined 1,125 papers from three flagship conferences (LAK, EDM, and AIED) over the past five years. We discovered, categorized, and analyzed 172 datasets used in 204 publications. Our study presents the most comprehensive collection and analysis of open educational datasets to date, along with the most detailed categorization. Of the 172 datasets identified, 143 were not captured in any prior survey of open data in learning analytics. We provide insights into the datasets' context, analytical methods, use, and other properties. Based on this survey, we summarize the current gaps in the field. Furthermore, we list practical recommendations, advice, and 8-item guidelines under the acronym PRACTICE with a checklist to help researchers publish their data. Lastly, we share our original dataset: an annotated inventory detailing the discovered datasets and the corresponding publications. We hope these findings will support further adoption of open data practices in learning analytics communities and beyond.

</details>


### [11] [Astra: AI Safety, Trust, & Risk Assessment](https://arxiv.org/abs/2602.17357)
*Pranav Aggarwal,Ananya Basotia,Debayan Gupta,Rahul Kulkarni,Shalini Kapoor,Kashyap J.,A. Mukundan,Aishwarya Pokhriyal,Anirban Sen,Aryan Shah,Aalok Thakkar*

Main category: cs.CY

TL;DR: 本文批评现有全球AI安全框架对印度独特社会技术背景的忽视，提出ASTRA AI安全风险数据库，采用自下而上的归纳方法，通过三重因果分类法评估风险，建立包含37个风险类别的领域无关本体论，为印度AI生态系统提供可扩展的监管工具。


<details>
  <summary>Details</summary>
Motivation: 现有全球AI安全框架存在"情境盲区"，忽视了印度独特的社会技术背景。印度拥有15亿人口和庞大的非正规经济，AI整合面临种姓歧视、语言排斥、农村基础设施不足等具体挑战，这些在西方市场中心的叙事中经常被忽略。

Method: 提出ASTRA AI安全风险数据库，采用自下而上的归纳方法。定义AI安全风险为源于设计缺陷（如偏斜的训练集或缺乏防护措施）的危害，可通过技术迭代或架构变更缓解。使用三重因果分类法：实施时机（开发、部署、使用）、责任实体（系统或用户）、意图性质（无意vs有意）。建立领域无关本体论，将37个叶级风险类别组织为两大元类别：社会风险和前沿/社会结构风险。

Result: 通过聚焦教育和金融借贷两个初始领域，建立了可扩展的基础，为印度不断扩展的AI生态系统创建了一个"活的"监管工具。ASTRA框架能够更准确地识别和分类印度特有的AI安全风险。

Conclusion: ASTRA提供了一个情境感知的AI安全风险评估框架，填补了现有全球框架在印度背景下的空白。该框架为印度AI生态系统提供了可扩展的监管基础，能够随着技术发展而演进，更好地应对印度特有的社会技术挑战。

Abstract: This paper argues that existing global AI safety frameworks exhibit contextual blindness towards India's unique socio-technical landscape. With a population of 1.5 billion and a massive informal economy, India's AI integration faces specific challenges such as caste-based discrimination, linguistic exclusion of vernacular speakers, and infrastructure failures in low-connectivity rural zones, that are frequently overlooked by Western, market-centric narratives.
  We introduce ASTRA, an empirically grounded AI Safety Risk Database designed to categorize risks through a bottom-up, inductive process. Unlike general taxonomies, ASTRA defines AI Safety Risks specifically as hazards stemming from design flaws such as skewed training sets or lack of guardrails that can be mitigated through technical iteration or architectural changes. This framework employs a tripartite causal taxonomy to evaluate risks based on their implementation timing (development, deployment, or usage), the responsible entity (the system or the user), and the nature of the intent (unintentional vs. intentional).
  Central to the research is a domain-agnostic ontology that organizes 37 leaf-level risk classes into two primary meta-categories: Social Risks and Frontier/Socio-Structural Risks. By focusing initial efforts on the Education and Financial Lending sectors, the paper establishes a scalable foundation for a "living" regulatory utility intended to evolve alongside India's expanding AI ecosystem.

</details>


### [12] [Insidious Imaginaries: A Critical Overview of AI Speculations](https://arxiv.org/abs/2602.17383)
*Dejan Grba*

Main category: cs.CY

TL;DR: 本文批判性审视AI推测性思维，分析其如何影响计算机科学研究、AI产业实践、存在风险学术研究及全球政治辩论，特别关注存在风险研究和有效利他主义运动中的问题。


<details>
  <summary>Details</summary>
Motivation: AI推测性思维（关于人工通用智能、超级智能和技术奇点的预测）虽然广泛影响多个领域，但常常充满模糊性、无根据断言和操纵性主张，具有广泛的实际后果。本文旨在批判性地审视这些推测，揭示其概念、方法、伦理和社会问题。

Method: 通过三个核心部分，追溯科幻小说、宗教性、学术欺诈、可疑创业和险恶社会政治世界观等因素如何交织影响AI推测。特别聚焦存在风险研究领域和有效利他主义运动，分析其技术乌托邦主义、长期主义和超人类主义意识形态如何与AI产业权力斗争相结合。

Result: 揭示了AI推测性思维中存在的问题：概念模糊、方法论缺陷、伦理争议和社会影响。这些推测不仅停留在话语层面，还通过产业实践、学术研究和政治辩论产生实际后果，有时甚至具有危害性。

Conclusion: 需要在更广泛的背景下全面评估AI推测，提出更全面的评价框架、实际处理方法和进一步研究建议，以应对具有潜在影响力的AI想象。

Abstract: Speculative thinking about the capabilities and implications of artificial intelligence (AI) influences computer science research, drives AI industry practices, feeds academic studies of existential hazards, and stirs a global political debate. It primarily concerns predictions about the possibilities, benefits, and risks of reaching artificial general intelligence, artificial superintelligence, and technological singularity. It permeates technophilic philosophies and social movements, fuels the corporate and pundit rhetoric, and remains a potent source of inspiration for the media, popular culture, and arts. However, speculative AI is not just a discursive matter. Steeped in vagueness and brimming with unfounded assertions, manipulative claims, and extreme futuristic scenarios, it often has wide-reaching practical consequences. This paper offers a critical overview of AI speculations. In three central sections, it traces the intertwined sway of science fiction, religiosity, intellectual charlatanism, dubious academic research, suspicious entrepreneurship, and ominous sociopolitical worldviews that make AI speculations troublesome and sometimes harmful. The focus is on the field of existential risk studies and the effective altruism movement, whose ideological flux of techno-utopianism, longtermism, and transhumanism aligns with the power struggles in the AI industry to emblematize speculative AI's conceptual, methodological, ethical, and social issues. The following discussion traverses these issues within a wider context to inform the closing summary of suggestions for a more comprehensive appraisal, practical handling, and further study of the potentially impactful AI imaginaries.

</details>


### [13] [Preserving Historical Truth: Detecting Historical Revisionism in Large Language Models](https://arxiv.org/abs/2602.17433)
*Francesco Ortu,Joeun Yook,Punya Syon Pandey,Keenan Samway,Bernhard Schölkopf,Alberto Cazzaniga,Rada Mihalcea,Zhijing Jin*

Main category: cs.CY

TL;DR: 论文提出HistoricalMisinfo数据集，包含500个争议历史事件及其事实性和修正主义叙事，通过11种提示场景评估LLMs在历史信息传播中的表现，发现模型在用户要求修正主义版本时会显著提高修正主义倾向。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs越来越多地被用作历史信息来源，需要可扩展的审计方法来评估模型在争议事件和政治敏感叙事中的表现，特别是在模拟真实用户交互的环境中。

Method: 创建HistoricalMisinfo数据集，包含45个国家500个争议事件，每个事件配有事实性参考叙事和修正主义参考叙事。通过11种提示场景模拟真实使用情况，使用LLM-as-a-judge协议比较模型输出与两种参考叙事，评估不同架构的LLMs在两种条件下的表现：中性提示和鲁棒性提示。

Result: 在中性提示下，模型通常更接近事实性参考，但分数应解释为参考对齐信号而非人类可解释的修正主义证据。鲁棒性提示产生强烈一致的效果：当用户要求修正主义叙事时，所有评估模型都显示出显著更高的修正主义分数，表明有限的抵抗或自我纠正能力。

Conclusion: HistoricalMisinfo为基准测试模型对修正主义框架的鲁棒性提供了实用基础，并指导未来更精确地自动评估争议历史主张，以确保AI系统在社会中的可持续整合。

Abstract: Large language models (LLMs) are increasingly used as sources of historical information, motivating the need for scalable audits on contested events and politically charged narratives in settings that mirror real user interactions. We introduce \textsc{\texttt{HistoricalMisinfo}}, a curated dataset of $500$ contested events from $45$ countries, each paired with a factual reference narrative and a documented revisionist reference narrative. To approximate real-world usage, we instantiate each event in $11$ prompt scenarios that reflect common communication settings (e.g., questions, textbooks, social posts, policy briefs). Using an LLM-as-a-judge protocol that compares model outputs to the two references, we evaluate LLMs varying across model architectures in two conditions: (i) neutral user prompts that ask for factually accurate information, and (ii) robustness prompts in which the user explicitly requests the revisionist version of the event. Under neutral prompts, models are generally closer to factual references, though the resulting scores should be interpreted as reference-alignment signals rather than definitive evidence of human-interpretable revisionism. Robustness prompting yields a strong and consistent effect: when the user requests the revisionist narrative, all evaluated models show sharply higher revisionism scores, indicating limited resistance or self-correction. \textsc{\texttt{HistoricalMisinfo}} provides a practical foundation for benchmarking robustness to revisionist framing and for guiding future work on more precise automatic evaluation of contested historical claims to ensure a sustainable integration of AI systems within society.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [14] [AIdentifyAGE Ontology for Decision Support in Forensic Dental Age Assessment](https://arxiv.org/abs/2602.16714)
*Renato Marcelo,Ana Rodrigues,Cristiana Palmela Pereira,António Figueiras,Rui Santos,José Rui Figueira,Alexandre P Francisco,Cátia Vaz*

Main category: cs.AI

TL;DR: AIdentifyAGE 本体论为法医牙科年龄评估提供了一个标准化的语义框架，整合了手动和AI辅助的工作流程，旨在提高一致性、透明度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 年龄评估在法医和司法决策中至关重要，特别是在涉及无证件个人和无人陪伴未成年人的案件中。当前的法医牙科年龄评估实践面临方法异质性、数据表示碎片化以及临床、法医和法律信息系统之间互操作性有限等挑战，这些问题阻碍了透明度和可重复性。

Method: 开发了AIdentifyAGE本体论，这是一个领域特定的标准化语义框架。它建模了完整的法医法律工作流程，整合了司法背景、个体信息、法医检查数据、牙齿发育评估方法、放射成像、统计参考研究和基于AI的估计方法。该本体论基于上层和已建立的生物医学、牙科和机器学习本体论构建。

Result: AIdentifyAGE本体论提供了一个标准化的语义框架，能够追踪观察、方法、参考数据和报告结果之间的可追溯链接。它确保了互操作性、可扩展性，并符合FAIR原则。

Conclusion: AIdentifyAGE本体论是增强一致性、透明度和可解释性的基础步骤，为法医法律和司法背景下的本体驱动决策支持系统建立了坚实基础。

Abstract: Age assessment is crucial in forensic and judicial decision-making, particularly in cases involving undocumented individuals and unaccompanied minors, where legal thresholds determine access to protection, healthcare, and judicial procedures. Dental age assessment is widely recognized as one of the most reliable biological approaches for adolescents and young adults, but current practices are challenged by methodological heterogeneity, fragmented data representation, and limited interoperability between clinical, forensic, and legal information systems. These limitations hinder transparency and reproducibility, amplified by the increasing adoption of AI- based methods. The AIdentifyAGE ontology is domain-specific and provides a standardized, semantically coherent framework, encompassing both manual and AI-assisted forensic dental age assessment workflows, and enabling traceable linkage between observations, methods, reference data, and reported outcomes. It models the complete medico-legal workflow, integrating judicial context, individual-level information, forensic examination data, dental developmental assessment methods, radiographic imaging, statistical reference studies, and AI-based estimation methods. It is being developed together with domain experts, and it builds on upper and established biomedical, dental, and machine learning ontologies, ensuring interoperability, extensibility, and compliance with FAIR principles. The AIdentifyAGE ontology is a fundamental step to enhance consistency, transparency, and explainability, establishing a robust foundation for ontology-driven decision support systems in medico-legal and judicial contexts.

</details>


### [15] [DeepContext: Stateful Real-Time Detection of Multi-Turn Adversarial Intent Drift in LLMs](https://arxiv.org/abs/2602.16935)
*Justin Albrethsen,Yash Datta,Kunal Kumar,Sharath Rajasekar*

Main category: cs.AI

TL;DR: DeepContext是一个状态感知的安全监控框架，使用RNN架构追踪对话中的用户意图演变，显著提升多轮越狱攻击检测性能，在保持低延迟的同时达到SOTA的F1分数0.84。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全护栏大多是无状态的，将多轮对话视为独立事件，缺乏时间感知能力。这导致"安全漏洞"，攻击者可以通过Crescendo和ActorAttack等对抗性策略，缓慢地在多轮对话中注入恶意意图，绕过无状态过滤器。

Method: 提出DeepContext状态监控框架，采用循环神经网络(RNN)架构，输入一系列微调的轮次级嵌入。通过在整个对话中传播隐藏状态，捕捉无状态模型忽略的风险累积过程。

Result: DeepContext在多轮越狱检测中显著优于现有基线，达到SOTA的F1分数0.84，明显优于云提供商护栏和开源模型(Llama-Prompt-Guard-2和Granite-Guardian的0.67)。在T4 GPU上保持低于20ms的推理开销。

Conclusion: 建模意图的顺序演变比部署大规模无状态模型更有效且计算效率更高。状态感知的安全监控是解决多轮对抗攻击的关键方向。

Abstract: While Large Language Model (LLM) capabilities have scaled, safety guardrails remain largely stateless, treating multi-turn dialogues as a series of disconnected events. This lack of temporal awareness facilitates a "Safety Gap" where adversarial tactics, like Crescendo and ActorAttack, slowly bleed malicious intent across turn boundaries to bypass stateless filters. We introduce DeepContext, a stateful monitoring framework designed to map the temporal trajectory of user intent. DeepContext discards the isolated evaluation model in favor of a Recurrent Neural Network (RNN) architecture that ingests a sequence of fine-tuned turn-level embeddings. By propagating a hidden state across the conversation, DeepContext captures the incremental accumulation of risk that stateless models overlook. Our evaluation demonstrates that DeepContext significantly outperforms existing baselines in multi-turn jailbreak detection, achieving a state-of-the-art F1 score of 0.84, which represents a substantial improvement over both hyperscaler cloud-provider guardrails and leading open-weight models such as Llama-Prompt-Guard-2 (0.67) and Granite-Guardian (0.67). Furthermore, DeepContext maintains a sub-20ms inference overhead on a T4 GPU, ensuring viability for real-time applications. These results suggest that modeling the sequential evolution of intent is a more effective and computationally efficient alternative to deploying massive, stateless models.

</details>


### [16] [Retrieval Augmented (Knowledge Graph), and Large Language Model-Driven Design Structure Matrix (DSM) Generation of Cyber-Physical Systems](https://arxiv.org/abs/2602.16715)
*H. Sinan Bank,Daniel R. Herber*

Main category: cs.AI

TL;DR: 该研究探索了LLM、RAG和GraphRAG在生成设计结构矩阵(DSM)方面的潜力，通过在电动螺丝刀和CubeSat两个用例上测试，评估了它们在确定组件关系和识别组件及其关系两个任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索自动化生成设计结构矩阵(DSM)的可能性，传统DSM创建需要大量人工工作，研究者希望利用先进的语言模型和检索增强技术来简化这一过程，提高设计效率。

Method: 研究方法包括：1)使用大型语言模型(LLM)；2)检索增强生成(RAG)；3)基于图的检索增强生成(GraphRAG)。在电动螺丝刀和CubeSat两个已知架构参考的用例上进行测试，评估两种任务：确定预定义组件之间的关系，以及识别组件及其关系。

Result: 研究结果显示，尽管存在设计和计算挑战，但发现了自动化DSM生成的机会。通过评估DSM的每个元素和整体架构来测量性能，所有代码都已公开供复现和领域专家进一步反馈。

Conclusion: 结论是LLM、RAG和GraphRAG在自动化DSM生成方面具有潜力，为设计工程领域提供了新的自动化工具，公开代码促进了可复现性和领域专家的进一步改进。

Abstract: We explore the potential of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), and Graph-based RAG (GraphRAG) for generating Design Structure Matrices (DSMs). We test these methods on two distinct use cases -- a power screwdriver and a CubeSat with known architectural references -- evaluating their performance on two key tasks: determining relationships between predefined components, and the more complex challenge of identifying components and their subsequent relationships. We measure the performance by assessing each element of the DSM and overall architecture. Despite design and computational challenges, we identify opportunities for automated DSM generation, with all code publicly available for reproducibility and further feedback from the domain experts.

</details>


### [17] [Contextuality from Single-State Representations: An Information-Theoretic Principle for Adaptive Intelligence](https://arxiv.org/abs/2602.16716)
*Song-Ju Kim*

Main category: cs.AI

TL;DR: 单状态重用在经典概率表示中必然导致语境性，这是自适应智能的普遍表示约束，而非量子力学特有现象


<details>
  <summary>Details</summary>
Motivation: 自适应系统常因内存、表示或物理资源限制而在多个语境中重用固定内部状态空间，这种单状态重用普遍存在于自然和人工智能中，但其基本表示后果尚未被充分理解

Method: 将语境建模为作用于共享内部状态的干预，证明任何再现语境性结果统计的经典模型都必须承担不可约的信息论代价，提供最小构造示例，并解释非经典概率框架如何通过放松单一全局联合概率空间假设来避免此障碍

Result: 证明了语境性不是量子力学的特性，而是经典概率表示中单状态重用的必然结果，语境依赖性不能仅通过内部状态来中介，必须承担信息论代价

Conclusion: 语境性是自适应智能的普遍表示约束，独立于物理实现，非经典概率框架通过放弃单一全局联合概率空间假设来避免经典表示的限制

Abstract: Adaptive systems often operate across multiple contexts while reusing a fixed internal state space due to constraints on memory, representation, or physical resources. Such single-state reuse is ubiquitous in natural and artificial intelligence, yet its fundamental representational consequences remain poorly understood. We show that contextuality is not a peculiarity of quantum mechanics, but an inevitable consequence of single-state reuse in classical probabilistic representations. Modeling contexts as interventions acting on a shared internal state, we prove that any classical model reproducing contextual outcome statistics must incur an irreducible information-theoretic cost: dependence on context cannot be mediated solely through the internal state. We provide a minimal constructive example that explicitly realizes this cost and clarifies its operational meaning. We further explain how nonclassical probabilistic frameworks avoid this obstruction by relaxing the assumption of a single global joint probability space, without invoking quantum dynamics or Hilbert space structure. Our results identify contextuality as a general representational constraint on adaptive intelligence, independent of physical implementation.

</details>


### [18] [Mobility-Aware Cache Framework for Scalable LLM-Based Human Mobility Simulation](https://arxiv.org/abs/2602.16727)
*Hua Yan,Heng Tan,Yingxue Zhang,Yu Yang*

Main category: cs.AI

TL;DR: MobCache：一种利用可重构缓存进行高效大规模人类移动仿真的框架，通过潜在空间推理重用和轻量级解码器提高效率


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的人类移动仿真方法计算成本高，限制了大规模应用，需要设计更高效的解决方案

Method: 设计MobCache框架，包含：1) 推理组件将推理步骤编码为潜在空间嵌入，通过潜在空间评估器实现推理步骤的重用和重组；2) 解码组件使用移动规律约束蒸馏训练的轻量级解码器，将潜在空间推理链转换为自然语言

Result: 实验表明MobCache在多维度上显著提高效率，同时保持与最先进LLM方法相当的性能

Conclusion: MobCache通过可重构缓存机制有效解决了大规模人类移动仿真的效率瓶颈问题，实现了高效且保真的仿真

Abstract: Large-scale human mobility simulation is critical for applications such as urban planning, epidemiology, and transportation analysis. Recent works treat large language models (LLMs) as human agents to simulate realistic mobility behaviors using structured reasoning, but their high computational cost limits scalability. To address this, we design a mobility-aware cache framework named MobCache that leverages reconstructible caches to enable efficient large-scale human mobility simulations. It consists of: (1) a reasoning component that encodes each reasoning step as a latent-space embedding and uses a latent-space evaluator to enable the reuse and recombination of reasoning steps; and (2) a decoding component that employs a lightweight decoder trained with mobility law-constrained distillation to translate latent-space reasoning chains into natural language, thereby improving simulation efficiency while maintaining fidelity. Experiments show that MobCache significantly improves efficiency across multiple dimensions while maintaining performance comparable to state-of-the-art LLM-based methods.

</details>


### [19] [When AI Benchmarks Plateau: A Systematic Study of Benchmark Saturation](https://arxiv.org/abs/2602.16763)
*Mubashara Akhtar,Anka Reuel,Prajna Soni,Sanchit Ahuja,Pawan Sasanka Ammanamanchi,Ruchit Rawal,Vilém Zouhar,Srishti Yadav,Chenxi Whitehouse,Dayeon Ki,Jennifer Mickel,Leshem Choshen,Marek Šuppa,Jan Batzner,Jenny Chim,Jeba Sania,Yanan Long,Hossein A. Rahmani,Christina Knight,Yiyang Nan,Jyoutir Raj,Yu Fan,Shubham Singh,Subramanyam Sahoo,Eliya Habba,Usman Gohar,Siddhesh Pawar,Robert Scholz,Arjun Subramonian,Jingwei Ni,Mykel Kochenderfer,Sanmi Koyejo,Mrinmaya Sachan,Stella Biderman,Zeerak Talat,Avijit Ghosh,Irene Solaiman*

Main category: cs.AI

TL;DR: 该研究分析了60个大型语言模型基准测试的饱和现象，发现近半数基准已饱和，且饱和率随基准年龄增长而增加。专家策划的基准比众包基准更能抵抗饱和，而隐藏测试数据并无保护效果。


<details>
  <summary>Details</summary>
Motivation: AI基准测试在衡量模型进展和指导部署决策中起核心作用，但许多基准测试很快饱和，无法区分最佳性能模型，降低了其长期价值。需要了解基准饱和的驱动因素，以设计更持久的评估方法。

Method: 从主要模型开发商的技术报告中选取60个LLM基准测试，从任务设计、数据构建和评估格式三个维度定义14个特征属性。测试5个假设，分析每个属性如何影响饱和率。

Result: 近一半基准测试表现出饱和现象，饱和率随基准年龄增长而增加。隐藏测试数据（公开vs私有）没有保护效果，而专家策划的基准比众包基准更能抵抗饱和。

Conclusion: 研究揭示了哪些设计选择能延长基准测试的寿命，为设计更持久的评估策略提供了信息，有助于创建更有效的AI基准测试体系。

Abstract: Artificial Intelligence (AI) benchmarks play a central role in measuring progress in model development and guiding deployment decisions. However, many benchmarks quickly become saturated, meaning that they can no longer differentiate between the best-performing models, diminishing their long-term value. In this study, we analyze benchmark saturation across 60 Large Language Model (LLM) benchmarks selected from technical reports by major model developers. To identify factors driving saturation, we characterize benchmarks along 14 properties spanning task design, data construction, and evaluation format. We test five hypotheses examining how each property contributes to saturation rates. Our analysis reveals that nearly half of the benchmarks exhibit saturation, with rates increasing as benchmarks age. Notably, hiding test data (i.e., public vs. private) shows no protective effect, while expert-curated benchmarks resist saturation better than crowdsourced ones. Our findings highlight which design choices extend benchmark longevity and inform strategies for more durable evaluation.

</details>


### [20] [Simple Baselines are Competitive with Code Evolution](https://arxiv.org/abs/2602.16805)
*Yonatan Gideoni,Sebastian Risi,Yarin Gal*

Main category: cs.AI

TL;DR: 简单基线方法在代码进化任务中表现优于复杂方法，揭示了当前代码进化研究在评估方法和实践上的不足


<details>
  <summary>Details</summary>
Motivation: 当前代码进化技术虽然表现出色，但缺乏与简单基线的比较，需要验证这些复杂方法的实际效果是否真的优于简单方法

Method: 在三个领域（数学边界优化、智能体脚手架设计、机器学习竞赛）测试两种简单基线方法，并与更复杂的方法进行比较分析

Result: 简单基线方法在所有三个领域都匹配或超越了更复杂的方法；发现代码进化在搜索空间设计、评估方法等方面存在系统性缺陷

Conclusion: 代码进化的主要挑战在于搜索空间设计而非搜索算法本身，需要改进评估方法并建立更严谨的研究实践

Abstract: Code evolution is a family of techniques that rely on large language models to search through possible computer programs by evolving or mutating existing code. Many proposed code evolution pipelines show impressive performance but are often not compared to simpler baselines. We test how well two simple baselines do over three domains: finding better mathematical bounds, designing agentic scaffolds, and machine learning competitions. We find that simple baselines match or exceed much more sophisticated methods in all three. By analyzing these results we find various shortcomings in how code evolution is both developed and used. For the mathematical bounds, a problem's search space and domain knowledge in the prompt are chiefly what dictate a search's performance ceiling and efficiency, with the code evolution pipeline being secondary. Thus, the primary challenge in finding improved bounds is designing good search spaces, which is done by domain experts, and not the search itself. When designing agentic scaffolds we find that high variance in the scaffolds coupled with small datasets leads to suboptimal scaffolds being selected, resulting in hand-designed majority vote scaffolds performing best. We propose better evaluation methods that reduce evaluation stochasticity while keeping the code evolution economically feasible. We finish with a discussion of avenues and best practices to enable more rigorous code evolution in future work.

</details>


### [21] [Improved Upper Bounds for Slicing the Hypercube](https://arxiv.org/abs/2602.16807)
*Duncan Soiffer,Nathaniel Itty,Christopher D. Rosin,Blake Bruell,Mason DiCicco,Gábor N. Sárközy,Ryan Offstein,Daniel Reichman*

Main category: cs.AI

TL;DR: 该论文改进了n维超立方体边切片所需超平面的上界，从之前的⌈5n/6⌉改进到⌈4n/5⌉，并在n为5的奇数倍时进一步优化为4n/5+1。


<details>
  <summary>Details</summary>
Motivation: 研究n维超立方体Q_n中切片所有边所需的最少超平面数量S(n)问题。这是一个经典的组合几何问题，Paterson在1971年给出了S(n) ≤ ⌈5n/6⌉的上界，但40多年来一直未有改进。

Method: 通过构造8个超平面切片Q_{10}来证明改进的上界。使用了新开发的自动工具CPro1，该工具结合推理LLM和自动超参数调优来创建搜索算法，用于发现数学构造。

Result: 证明了S(n) ≤ ⌈4n/5⌉（当n不是5的奇数倍时），以及S(n) ≤ 4n/5+1（当n是5的奇数倍时）。同时获得了使用k<n个超平面时能切片的最大边数的新下界。

Conclusion: 成功改进了超立方体边切片问题的上界，展示了自动数学发现工具CPro1在解决组合几何问题中的有效性，为这一经典问题提供了40多年来的首次改进。

Abstract: A collection of hyperplanes $\mathcal{H}$ slices all edges of the $n$-dimensional hypercube $Q_n$ with vertex set $\{-1,1\}^n$ if, for every edge $e$ in the hypercube, there exists a hyperplane in $\mathcal{H}$ intersecting $e$ in its interior. Let $S(n)$ be the minimum number of hyperplanes needed to slice $Q_n$. We prove that $S(n) \leq \lceil \frac{4n}{5} \rceil$, except when $n$ is an odd multiple of $5$, in which case $S(n) \leq \frac{4n}{5} +1$. This improves upon the previously known upper bound of $S(n) \leq \lceil\frac{5n}{6} \rceil$ due to Paterson reported in 1971. We also obtain new lower bounds on the maximum number of edges in $Q_n$ that can be sliced using $k<n$ hyperplanes. We prove the improved upper bound on $S(n)$ by constructing $8$ hyperplanes slicing $Q_{10}$ aided by the recently introduced CPro1: an automatic tool that uses reasoning LLMs coupled with automated hyperparameter tuning to create search algorithms for the discovery of mathematical constructions.

</details>


### [22] [Cinder: A fast and fair matchmaking system](https://arxiv.org/abs/2602.17015)
*Saurav Pal*

Main category: cs.AI

TL;DR: Cinder是一个两阶段匹配系统，通过Ruzicka相似性指数快速筛选，再使用基于反正态分布技能桶的Kantorovich距离计算公平性分数，解决异质技能团队匹配的公平性问题。


<details>
  <summary>Details</summary>
Motivation: 现代多人游戏中，为预组队（lobby）创建公平匹配是一个重要挑战。传统的基于平均技能指标（如均值或中位数评级）的匹配方法在技能分布广泛或偏斜时经常导致不平衡和一边倒的比赛。

Method: Cinder采用两阶段方法：第一阶段使用Ruzicka相似性指数快速比较团队的"非异常值"技能范围进行初步筛选；第二阶段将玩家等级映射到基于反正态分布生成的技能桶中，提供平均技能水平更高的粒度，然后使用Kantorovich距离计算已排序桶索引的"制裁分数"来量化匹配公平性。

Result: 通过分析1.4亿个模拟团队配对的制裁分数分布，证明了系统的可行性，为公平匹配阈值提供了稳健基础。

Conclusion: Cinder系统能够为异质技能水平的预组队提供快速且公平的匹配，解决了传统平均技能匹配方法在技能分布不均时导致的不平衡问题。

Abstract: A fair and fast matchmaking system is an important component of modern multiplayer online games, directly impacting player retention and satisfaction. However, creating fair matches between lobbies (pre-made teams) of heterogeneous skill levels presents a significant challenge. Matching based simply on average team skill metrics, such as mean or median rating or rank, often results in unbalanced and one-sided games, particularly when skill distributions are wide or skewed. This paper introduces Cinder, a two-stage matchmaking system designed to provide fast and fair matches. Cinder first employs a rapid preliminary filter by comparing the "non-outlier" skill range of lobbies using the Ruzicka similarity index. Lobbies that pass this initial check are then evaluated using a more precise fairness metric. This second stage involves mapping player ranks to a non-linear set of skill buckets, generated from an inverted normal distribution, to provide higher granularity at average skill levels. The fairness of a potential match is then quantified using the Kantorovich distance on the lobbies' sorted bucket indices, producing a "Sanction Score." We demonstrate the system's viability by analyzing the distribution of Sanction Scores from 140 million simulated lobby pairings, providing a robust foundation for fair matchmaking thresholds.

</details>


### [23] [NeuDiff Agent: A Governed AI Workflow for Single-Crystal Neutron Crystallography](https://arxiv.org/abs/2602.16812)
*Zhongcan Xiao,Leyi Zhang,Guannan Zhang,Xiaoping Wang*

Main category: cs.AI

TL;DR: NeuDiff Agent是一个受治理的AI工作流系统，用于加速中子源晶体学数据分析，将手动处理时间从435分钟减少到约90分钟（4.6-5.0倍加速），同时生成经过验证的晶体结构文件。


<details>
  <summary>Details</summary>
Motivation: 大规模科学设施面临分析延迟问题，特别是对于结构复杂的样品，传统的手动分析流程耗时且效率低下，限制了科学产出速度。

Method: 开发了NeuDiff Agent系统，这是一个受治理的工具使用AI工作流，通过白名单工具限制、关键工作流边界的故障关闭验证门、完整溯源记录等方式，实现从仪器数据到验证晶体结构的自动化处理。

Result: 在基准测试中，NeuDiff Agent将处理时间从435分钟（手动）减少到86.5-94.4分钟（4.6-5.0倍加速），生成无A或B级警报的验证CIF文件，同时保持完全溯源能力。

Conclusion: NeuDiff Agent为设施晶体学部署智能AI提供了一条实用路径，在保持可追溯性和出版验证要求的同时，显著提高了分析效率。

Abstract: Large-scale facilities increasingly face analysis and reporting latency as the limiting step in scientific throughput, particularly for structurally and magnetically complex samples that require iterative reduction, integration, refinement, and validation. To improve time-to-result and analysis efficiency, NeuDiff Agent is introduced as a governed, tool-using AI workflow for TOPAZ at the Spallation Neutron Source that takes instrument data products through reduction, integration, refinement, and validation to a validated crystal structure and a publication-ready CIF. NeuDiff Agent executes this established pipeline under explicit governance by restricting actions to allowlisted tools, enforcing fail-closed verification gates at key workflow boundaries, and capturing complete provenance for inspection, auditing, and controlled replay. Performance is assessed using a fixed prompt protocol and repeated end-to-end runs with two large language model backends, with user and machine time partitioned and intervention burden and recovery behaviors quantified under gating. In a reference-case benchmark, NeuDiff Agent reduces wall time from 435 minutes (manual) to 86.5(4.7) to 94.4(3.5) minutes (4.6-5.0x faster) while producing a validated CIF with no checkCIF level A or B alerts. These results establish a practical route to deploy agentic AI in facility crystallography while preserving traceability and publication-facing validation requirements.

</details>


### [24] [Node Learning: A Framework for Adaptive, Decentralised and Collaborative Network Edge AI](https://arxiv.org/abs/2602.16814)
*Eiman Kanjo,Mustafa Aslanov*

Main category: cs.AI

TL;DR: Node Learning是一种去中心化学习范式，将智能置于边缘节点，通过选择性对等交互扩展知识，无需全局同步或中央聚合


<details>
  <summary>Details</summary>
Motivation: 传统集中式AI在边缘计算中存在数据传输、延迟、能耗和依赖大型数据中心等问题，难以适应异构、移动和资源受限的环境

Method: 节点从本地数据持续学习，维护自身模型状态，在有益时通过机会性对等交互交换知识，学习通过重叠和扩散传播而非全局同步

Result: 提出统一自主和协作行为的抽象框架，适应数据、硬件、目标和连接性的异构性，为去中心化学习提供概念基础

Conclusion: Node Learning不是取代现有范式，而是在更广泛的去中心化视角中定位它们，为边缘智能提供更灵活、可扩展的解决方案

Abstract: The expansion of AI toward the edge increasingly exposes the cost and fragility of cen- tralised intelligence. Data transmission, latency, energy consumption, and dependence on large data centres create bottlenecks that scale poorly across heterogeneous, mobile, and resource-constrained environments. In this paper, we introduce Node Learning, a decen- tralised learning paradigm in which intelligence resides at individual edge nodes and expands through selective peer interaction. Nodes learn continuously from local data, maintain their own model state, and exchange learned knowledge opportunistically when collaboration is beneficial. Learning propagates through overlap and diffusion rather than global synchro- nisation or central aggregation. It unifies autonomous and cooperative behaviour within a single abstraction and accommodates heterogeneity in data, hardware, objectives, and connectivity. This concept paper develops the conceptual foundations of this paradigm, contrasts it with existing decentralised approaches, and examines implications for communi- cation, hardware, trust, and governance. Node Learning does not discard existing paradigms, but places them within a broader decentralised perspective

</details>


### [25] [An order-oriented approach to scoring hesitant fuzzy elements](https://arxiv.org/abs/2602.16827)
*Luis Merino,Gabriel Navarro,Carlos Salvatierra,Evangelina Santos*

Main category: cs.AI

TL;DR: 本文提出了一种基于序理论的犹豫模糊集统一评分框架，证明了传统序不构成格结构，而对称序满足评分函数的关键规范准则，并引入了优势函数用于犹豫模糊元素排序。


<details>
  <summary>Details</summary>
Motivation: 传统犹豫模糊集评分方法缺乏序理论基础，需要建立具有形式化序理论基础的统一评分框架，以支持更灵活和一致的评分机制。

Method: 1. 提出基于给定序的评分统一框架；2. 分析犹豫模糊元素上的经典序，证明它们不构成格结构；3. 证明对称序满足强单调性和Gärdenfors条件等规范准则；4. 引入优势函数用于犹豫模糊元素排序，包含离散优势函数和相对优势函数两种具体实现。

Result: 1. 经典序不诱导格结构，纠正了先前研究的错误；2. 对称序满足评分函数的关键规范准则；3. 优势函数可用于构建犹豫模糊集上的模糊偏好关系，支持群体决策。

Conclusion: 本文建立了基于序理论的犹豫模糊集评分统一框架，提出了满足规范准则的对称序评分方法，并引入了优势函数用于实际决策应用，为犹豫模糊集理论提供了更坚实的理论基础。

Abstract: Traditional scoring approaches on hesitant fuzzy sets often lack a formal base in order theory. This paper proposes a unified framework, where each score is explicitly defined with respect to a given order. This order-oriented perspective enables more flexible and coherent scoring mechanisms. We examine several classical orders on hesitant fuzzy elements, that is, nonempty subsets in [0,1], and show that, contrary to prior claims, they do not induce lattice structures. In contrast, we prove that the scores defined with respect to the symmetric order satisfy key normative criteria for scoring functions, including strong monotonicity with respect to unions and the Gärdenfors condition.
  Following this analysis, we introduce a class of functions, called dominance functions, for ranking hesitant fuzzy elements. They aim to compare hesitant fuzzy elements relative to control sets incorporating minimum acceptability thresholds. Two concrete examples of dominance functions for finite sets are provided: the discrete dominance function and the relative dominance function. We show that these can be employed to construct fuzzy preference relations on typical hesitant fuzzy sets and support group decision-making.

</details>


### [26] [From Labor to Collaboration: A Methodological Experiment Using AI Agents to Augment Research Perspectives in Taiwan's Humanities and Social Sciences](https://arxiv.org/abs/2602.17221)
*Yi-Chih Huang*

Main category: cs.AI

TL;DR: 本研究提出一个基于AI Agent的人文社科研究协作工作流，以台湾Claude.ai使用数据为实证案例验证其可行性，揭示了人类判断在研究中不可替代的作用。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI研究主要关注软件工程和自然科学领域，人文社科领域的方法论探索有限。本研究旨在填补这一空白，为人文社科研究者提供一个可复制的AI协作框架。

Method: 采用"方法论实验"定位，设计了一个七阶段模块化工作流（Agentic Workflow），基于任务模块化、人机分工和可验证性三原则。以台湾Anthropic经济指数中7,729个Claude.ai对话数据为实证案例，展示该工作流在二次数据分析中的应用。

Result: 提出了可复制的人文社科AI协作框架，识别出三种人机协作操作模式：直接执行、迭代优化和人类主导。通过操作过程的反思性记录，揭示了人类在研究问题制定、理论解释、情境化推理和伦理反思等方面的不可替代性。

Conclusion: 本研究为人文社科研究者提供了实用的AI协作方法论，同时强调了人类判断在研究中的核心地位。承认了单平台数据、横断面设计和AI可靠性风险等局限性。

Abstract: Generative AI is reshaping knowledge work, yet existing research focuses predominantly on software engineering and the natural sciences, with limited methodological exploration for the humanities and social sciences. Positioned as a "methodological experiment," this study proposes an AI Agent-based collaborative research workflow (Agentic Workflow) for humanities and social science research. Taiwan's Claude.ai usage data (N = 7,729 conversations, November 2025) from the Anthropic Economic Index (AEI) serves as the empirical vehicle for validating the feasibility of this methodology.
  This study operates on two levels: the primary level is the design and validation of a methodological framework - a seven-stage modular workflow grounded in three principles: task modularization, human-AI division of labor, and verifiability, with each stage delineating clear roles for human researchers (research judgment and ethical decisions) and AI Agents (information retrieval and text generation); the secondary level is the empirical analysis of AEI Taiwan data - serving as an operational demonstration of the workflow's application to secondary data research, showcasing both the process and output quality (see Appendix A).
  This study contributes by proposing a replicable AI collaboration framework for humanities and social science researchers, and identifying three operational modes of human-AI collaboration - direct execution, iterative refinement, and human-led - through reflexive documentation of the operational process. This taxonomy reveals the irreplaceability of human judgment in research question formulation, theoretical interpretation, contextualized reasoning, and ethical reflection. Limitations including single-platform data, cross-sectional design, and AI reliability risks are acknowledged.

</details>


### [27] [IndicJR: A Judge-Free Benchmark of Jailbreak Robustness in South Asian Languages](https://arxiv.org/abs/2602.16832)
*Priyaranjan Pattnayak,Sanchari Chowdhuri*

Main category: cs.AI

TL;DR: IJR是一个针对12种印度和南亚语言的安全对齐基准测试，揭示了英语评估无法发现的多语言漏洞，特别是在代码转换和罗马化使用场景中。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的安全对齐评估主要集中于英语和合同约束场景，忽视了多语言环境下的安全漏洞，特别是对于南亚地区用户经常进行代码转换和罗马化的使用模式。

Method: 创建了IJR基准测试，包含12种印度和南亚语言，涵盖45216个提示，分为JSON（合同约束）和Free（自然）两个轨道，采用无裁判评估方法，并进行人工审核验证。

Result: 发现三个关键模式：1）合同约束会增加拒绝率但无法阻止越狱攻击；2）英语到印度语言的攻击转移性强，格式包装器优于指令包装器；3）正字法影响显著，罗马化或混合输入会降低JSR，与罗马化比例和分词相关。

Conclusion: IJR提供了一个可复现的多语言压力测试，揭示了仅关注英语和合同约束的评估所隐藏的风险，特别适用于经常进行代码转换和罗马化的南亚用户。

Abstract: Safety alignment of large language models (LLMs) is mostly evaluated in English and contract-bound, leaving multilingual vulnerabilities understudied. We introduce \textbf{Indic Jailbreak Robustness (IJR)}, a judge-free benchmark for adversarial safety across 12 Indic and South Asian languages (2.1 Billion speakers), covering 45216 prompts in JSON (contract-bound) and Free (naturalistic) tracks.
  IJR reveals three patterns. (1) Contracts inflate refusals but do not stop jailbreaks: in JSON, LLaMA and Sarvam exceed 0.92 JSR, and in Free all models reach 1.0 with refusals collapsing. (2) English to Indic attacks transfer strongly, with format wrappers often outperforming instruction wrappers. (3) Orthography matters: romanized or mixed inputs reduce JSR under JSON, with correlations to romanization share and tokenization (approx 0.28 to 0.32) indicating systematic effects. Human audits confirm detector reliability, and lite-to-full comparisons preserve conclusions. IJR offers a reproducible multilingual stress test revealing risks hidden by English-only, contract-focused evaluations, especially for South Asian users who frequently code-switch and romanize.

</details>


### [28] [Mobile-Agent-v3.5: Multi-platform Fundamental GUI Agents](https://arxiv.org/abs/2602.16855)
*Haiyang Xu,Xi Zhang,Haowei Liu,Junyang Wang,Zhaozai Zhu,Shengjie Zhou,Xuhao Hu,Feiyu Gao,Junjie Cao,Zihua Wang,Zhiyuan Chen,Jitong Liao,Qi Zheng,Jiahui Zeng,Ze Xu,Shuai Bai,Junyang Lin,Jingren Zhou,Ming Yan*

Main category: cs.AI

TL;DR: GUI-Owl-1.5是一个多平台GUI代理模型，支持桌面、移动端、浏览器等平台，在20多个GUI基准测试中取得SOTA结果，采用混合数据飞轮、统一能力增强和多平台环境RL扩展等创新技术。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够支持多种平台（桌面、移动、浏览器等）的GUI代理模型，实现云边协同和实时交互，解决现有GUI自动化任务中的多平台兼容性和训练效率问题。

Method: 1. 混合数据飞轮：结合模拟环境和云端沙箱环境构建UI理解和轨迹生成数据管道；2. 统一能力增强：使用统一思维合成管道增强模型推理能力，重点提升工具调用、内存和多代理适应等关键能力；3. 多平台环境RL扩展：提出MRPO算法解决多平台冲突和长时程任务训练效率低的问题。

Result: 在20多个GUI基准测试中取得SOTA结果：GUI自动化任务（OSWorld 56.5，AndroidWorld 71.6，WebArena 48.4），接地任务（ScreenSpotPro 80.3），工具调用任务（OSWorld-MCP 47.6，MobileWorld 46.8），内存和知识任务（GUI-Knowledge Bench 75.5）。

Conclusion: GUI-Owl-1.5是一个功能强大的多平台GUI代理模型，通过创新的数据收集、能力增强和训练方法，在多个GUI任务上实现了最先进的性能，模型已开源并提供在线演示。

Abstract: The paper introduces GUI-Owl-1.5, the latest native GUI agent model that features instruct/thinking variants in multiple sizes (2B/4B/8B/32B/235B) and supports a range of platforms (desktop, mobile, browser, and more) to enable cloud-edge collaboration and real-time interaction. GUI-Owl-1.5 achieves state-of-the-art results on more than 20+ GUI benchmarks on open-source models: (1) on GUI automation tasks, it obtains 56.5 on OSWorld, 71.6 on AndroidWorld, and 48.4 on WebArena; (2) on grounding tasks, it obtains 80.3 on ScreenSpotPro; (3) on tool-calling tasks, it obtains 47.6 on OSWorld-MCP, and 46.8 on MobileWorld; (4) on memory and knowledge tasks, it obtains 75.5 on GUI-Knowledge Bench. GUI-Owl-1.5 incorporates several key innovations: (1) Hybird Data Flywheel: we construct the data pipeline for UI understanding and trajectory generation based on a combination of simulated environments and cloud-based sandbox environments, in order to improve the efficiency and quality of data collection. (2) Unified Enhancement of Agent Capabilities: we use a unified thought-synthesis pipeline to enhance the model's reasoning capabilities, while placing particular emphasis on improving key agent abilities, including Tool/MCP use, memory and multi-agent adaptation; (3) Multi-platform Environment RL Scaling: We propose a new environment RL algorithm, MRPO, to address the challenges of multi-platform conflicts and the low training efficiency of long-horizon tasks. The GUI-Owl-1.5 models are open-sourced, and an online cloud-sandbox demo is available at https://github.com/X-PLUG/MobileAgent.

</details>


### [29] [OpenSage: Self-programming Agent Generation Engine](https://arxiv.org/abs/2602.16891)
*Hongwei Li,Zhun Wang,Qinrun Dai,Yuzhou Nie,Jinjun Peng,Ruitong Liu,Jingyang Zhang,Kaijie Zhu,Jingxuan He,Lun Wang,Yangruibo Ding,Yueqi Chen,Wenbo Guo,Dawn Song*

Main category: cs.AI

TL;DR: OpenSage是首个让LLM能自动创建具有自生成拓扑结构和工具集的智能体开发套件，提供结构化内存支持，在多个基准测试中表现优于现有ADK


<details>
  <summary>Details</summary>
Motivation: 当前智能体开发套件要么功能支持不足，要么依赖人工设计拓扑、工具和内存组件，限制了智能体的泛化能力和整体性能

Method: OpenSage让LLM自动创建智能体，具备自生成拓扑结构和工具集；采用分层图结构的内存系统；提供专门针对软件工程任务的工具包

Result: 在三个最先进的基准测试中，使用不同骨干模型的OpenSage都优于现有ADK；消融研究验证了各组件设计的有效性

Conclusion: OpenSage为下一代智能体开发铺平道路，将焦点从以人为中心转向以AI为中心的范式

Abstract: Agent development kits (ADKs) provide effective platforms and tooling for constructing agents, and their designs are critical to the constructed agents' performance, especially the functionality for agent topology, tools, and memory. However, current ADKs either lack sufficient functional support or rely on humans to manually design these components, limiting agents' generalizability and overall performance. We propose OpenSage, the first ADK that enables LLMs to automatically create agents with self-generated topology and toolsets while providing comprehensive and structured memory support. OpenSage offers effective functionality for agents to create and manage their own sub-agents and toolkits. It also features a hierarchical, graph-based memory system for efficient management and a specialized toolkit tailored to software engineering tasks. Extensive experiments across three state-of-the-art benchmarks with various backbone models demonstrate the advantages of OpenSage over existing ADKs. We also conduct rigorous ablation studies to demonstrate the effectiveness of our design for each component. We believe OpenSage can pave the way for the next generation of agent development, shifting the focus from human-centered to AI-centered paradigms.

</details>


### [30] [AgentLAB: Benchmarking LLM Agents against Long-Horizon Attacks](https://arxiv.org/abs/2602.16901)
*Tanqiu Jiang,Yuhui Wang,Jiacheng Liang,Ting Wang*

Main category: cs.AI

TL;DR: AgentLAB是首个专门评估LLM智能体对自适应、长视野攻击脆弱性的基准测试，包含5种新型攻击类型、28个真实环境和644个安全测试用例，发现现有智能体对长视野攻击高度脆弱，单轮防御措施无效。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体在长视野、复杂环境中部署以解决挑战性问题，它们面临利用多轮用户-智能体-环境交互的长视野攻击风险，这些攻击在单轮设置中无法实现。目前缺乏专门评估智能体对此类风险脆弱性的基准测试。

Method: 提出AgentLAB基准测试框架，支持5种新型攻击类型：意图劫持、工具链式攻击、任务注入、目标漂移和内存污染，覆盖28个真实智能体环境和644个安全测试用例。使用该基准评估代表性LLM智能体。

Result: 评估发现现有LLM智能体对长视野攻击高度脆弱，且为单轮交互设计的防御措施无法可靠缓解长视野威胁。AgentLAB可作为跟踪实际场景中LLM智能体安全进展的基准。

Conclusion: AgentLAB是首个专门评估LLM智能体对长视野攻击脆弱性的基准测试，揭示了现有智能体在此类攻击下的高度脆弱性，强调了开发专门针对多轮交互安全措施的必要性。该基准已公开可用。

Abstract: LLM agents are increasingly deployed in long-horizon, complex environments to solve challenging problems, but this expansion exposes them to long-horizon attacks that exploit multi-turn user-agent-environment interactions to achieve objectives infeasible in single-turn settings. To measure agent vulnerabilities to such risks, we present AgentLAB, the first benchmark dedicated to evaluating LLM agent susceptibility to adaptive, long-horizon attacks. Currently, AgentLAB supports five novel attack types including intent hijacking, tool chaining, task injection, objective drifting, and memory poisoning, spanning 28 realistic agentic environments, and 644 security test cases. Leveraging AgentLAB, we evaluate representative LLM agents and find that they remain highly susceptible to long-horizon attacks; moreover, defenses designed for single-turn interactions fail to reliably mitigate long-horizon threats. We anticipate that AgentLAB will serve as a valuable benchmark for tracking progress on securing LLM agents in practical settings. The benchmark is publicly available at https://tanqiujiang.github.io/AgentLAB_main.

</details>


### [31] [LLM-WikiRace: Benchmarking Long-term Planning and Reasoning over Real-World Knowledge Graphs](https://arxiv.org/abs/2602.16902)
*Juliusz Ziomek,William Bankes,Lorenz Wolf,Shyam Sundhar Ramesh,Xiaohang Tang,Ilija Bogunovic*

Main category: cs.AI

TL;DR: LLM-Wikirace是一个评估大语言模型规划、推理和世界知识的基准测试，要求模型通过维基百科超链接从源页面逐步导航到目标页面。虽然顶尖模型在简单任务上表现超人，但在困难任务上成功率大幅下降，揭示了当前推理系统的明显局限性。


<details>
  <summary>Details</summary>
Motivation: 需要评估大语言模型在规划、推理和世界知识方面的能力，特别是在需要前瞻性规划和理解现实世界概念关联的任务中。现有基准可能无法充分测试这些复杂能力。

Method: 创建LLM-Wikirace基准测试，要求模型通过维基百科超链接逐步导航从源页面到目标页面。评估了包括Gemini-3、GPT-5、Claude Opus 4.5在内的开源和闭源模型，分析不同难度级别下的表现。

Result: 顶尖模型在简单任务上表现超人，但在困难任务上成功率大幅下降（最佳模型Gemini-3仅23%成功率）。世界知识对成功是必要的，但达到一定阈值后，规划和长视野推理能力成为主导因素。模型在失败后难以重新规划，经常陷入循环。

Conclusion: LLM-Wikirace揭示了当前推理系统在规划和长视野推理方面的明显局限性，为规划能力强的LLMs提供了一个仍有很大改进空间的开放竞技场。

Abstract: We introduce LLM-Wikirace, a benchmark for evaluating planning, reasoning, and world knowledge in large language models (LLMs). In LLM-Wikirace, models must efficiently navigate Wikipedia hyperlinks step by step to reach a target page from a given source, requiring look-ahead planning and the ability to reason about how concepts are connected in the real world. We evaluate a broad set of open- and closed-source models, including Gemini-3, GPT-5, and Claude Opus 4.5, which achieve the strongest results on the easy level of the task and demonstrate superhuman performance. Despite this, performance drops sharply on hard difficulty: the best-performing model, Gemini-3, succeeds in only 23\% of hard games, highlighting substantial remaining challenges for frontier models. Our analysis shows that world knowledge is a necessary ingredient for success, but only up to a point, beyond this threshold, planning and long-horizon reasoning capabilities become the dominant factors. Trajectory-level analysis further reveals that even the strongest models struggle to replan after failure, frequently entering loops rather than recovering. LLM-Wikirace is a simple benchmark that reveals clear limitations in current reasoning systems, offering an open arena where planning-capable LLMs still have much to prove. Our code and leaderboard available at https:/llmwikirace.github.io.

</details>


### [32] [Narrow fine-tuning erodes safety alignment in vision-language agents](https://arxiv.org/abs/2602.16931)
*Idhant Gulati,Shivam Raval*

Main category: cs.AI

TL;DR: 微调对齐的视觉语言模型于有害数据集会导致严重的跨任务泛化性失对齐，多模态评估比单模态评估更敏感，有害行为存在于低维子空间，现有缓解策略无法完全消除有害行为。


<details>
  <summary>Details</summary>
Motivation: 终身多模态智能体需要通过后训练持续适应新任务，但这在能力获取与安全对齐之间产生根本性矛盾。研究旨在揭示微调对齐视觉语言模型于有害数据集如何导致广泛泛化的失对齐问题。

Method: 在Gemma3-4B模型上进行实验，使用LoRA微调于有害数据集，分析失对齐随LoRA秩的变化，比较多模态与文本评估的差异，研究有害数据比例的影响，通过几何分析探索有害行为的低维子空间特性，并评估良性窄域微调和基于激活的引导两种缓解策略。

Result: 微调导致失对齐随LoRA秩单调增加；多模态评估显示失对齐程度（70.71±1.22，r=128）显著高于文本评估（41.19±2.51）；即使10%有害数据也会导致实质性对齐退化；几何分析显示有害行为存在于低维子空间（10个主成分）；两种缓解策略虽能显著减少失对齐，但无法完全消除有害行为。

Conclusion: 当前的后训练范式在部署后环境中可能无法充分保持对齐，需要开发更强大的持续学习框架来平衡能力获取与安全对齐。

Abstract: Lifelong multimodal agents must continuously adapt to new tasks through post-training, but this creates fundamental tension between acquiring capabilities and preserving safety alignment. We demonstrate that fine-tuning aligned vision-language models on narrow-domain harmful datasets induces severe emergent misalignment that generalizes broadly across unrelated tasks and modalities. Through experiments on Gemma3-4B, we show that misalignment scales monotonically with LoRA rank, and that multimodal evaluation reveals substantially higher misalignment ($70.71 \pm 1.22$ at $r=128$) than text-only evaluation ($41.19 \pm 2.51$), suggesting that unimodal safety benchmarks may underestimate alignment degradation in vision-language models. Critically, even 10\% harmful data in the training mixture induces substantial alignment degradation. Geometric analysis reveals that harmful behaviors occupy a remarkably low-dimensional subspace, with the majority of misalignment information captured in 10 principal components. To mitigate misalignment, we evaluate two strategies: benign narrow fine-tuning and activation-based steering. While both approaches substantially reduce misalignment, neither completely removes the learned harmful behaviors. Our findings highlight the need for robust continual learning frameworks, as current post-training paradigms may not sufficiently preserve alignment in post-deployment settings.

</details>


### [33] [SourceBench: Can AI Answers Reference Quality Web Sources?](https://arxiv.org/abs/2602.16942)
*Hexi Jin,Stephen Liu,Yuheng Li,Simran Malik,Yiying Zhang*

Main category: cs.AI

TL;DR: SourceBench是一个评估LLM引用网页源质量的基准，包含100个真实查询和8个质量指标，揭示了LLM引用源质量的新发现。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要关注答案正确性，而忽略了引用源的质量。随着LLM越来越多地通过引用网页源来回答问题，需要系统评估这些引用源的质量。

Method: 开发SourceBench基准，包含100个真实世界查询（涵盖信息、事实、论证、社交和购物意图），使用8个指标框架（内容质量和页面级信号），并创建人工标注数据集和校准的LLM评估器。

Result: 评估了8个LLM、Google搜索和3个AI搜索工具，分析了3996个引用源，揭示了四个关键新见解，为GenAI和网络搜索的未来研究提供指导。

Conclusion: SourceBench填补了LLM引用源质量评估的空白，为未来生成式AI和网络搜索研究提供了重要基准和方向指导。

Abstract: Large language models (LLMs) increasingly answer queries by citing web sources, but existing evaluations emphasize answer correctness rather than evidence quality. We introduce SourceBench, a benchmark for measuring the quality of cited web sources across 100 real-world queries spanning informational, factual, argumentative, social, and shopping intents. SourceBench uses an eight-metric framework covering content quality (content relevance, factual accuracy, objectivity) and page-level signals (e.g., freshness, authority/accountability, clarity), and includes a human-labeled dataset with a calibrated LLM-based evaluator that matches expert judgments closely. We evaluate eight LLMs, Google Search, and three AI search tools over 3996 cited sources using SourceBench and conduct further experiments to understand the evaluation results. Overall, our work reveals four key new insights that can guide future research in the direction of GenAI and web search.

</details>


### [34] [Mind the GAP: Text Safety Does Not Transfer to Tool-Call Safety in LLM Agents](https://arxiv.org/abs/2602.16943)
*Arnold Cartagena,Ariane Teixeira*

Main category: cs.AI

TL;DR: 文本安全评估无法反映工具调用安全：研究发现语言模型可能在文本层面拒绝有害请求，但同时通过工具调用执行被禁止的操作。


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全评估主要关注文本层面的拒绝行为，但缺乏对工具调用层面安全性的评估。需要回答核心问题：抑制有害文本的对齐是否也能抑制有害行动？

Method: 提出GAP基准，系统评估LLM代理在文本安全与工具调用安全之间的差异。测试6个前沿模型，涵盖6个监管领域（医药、金融、教育、就业、法律、基础设施），每个领域7种越狱场景，3种系统提示条件（中性、安全强化、工具鼓励），2种提示变体，共17,420个数据点。

Result: 文本安全无法转移到工具调用安全：所有6个模型都出现文本拒绝但工具调用执行被禁操作的情况。即使安全强化提示下，仍有219个此类案例。系统提示用词对工具调用行为影响显著：TC安全率差异达21-57个百分点。运行时治理合同减少信息泄漏但无法阻止被禁工具调用尝试。

Conclusion: 仅文本安全评估不足以评估代理行为，工具调用安全需要专门的测量和缓解措施。需要开发针对工具调用层面的安全评估框架。

Abstract: Large language models deployed as agents increasingly interact with external systems through tool calls--actions with real-world consequences that text outputs alone do not carry. Safety evaluations, however, overwhelmingly measure text-level refusal behavior, leaving a critical question unanswered: does alignment that suppresses harmful text also suppress harmful actions? We introduce the GAP benchmark, a systematic evaluation framework that measures divergence between text-level safety and tool-call-level safety in LLM agents. We test six frontier models across six regulated domains (pharmaceutical, financial, educational, employment, legal, and infrastructure), seven jailbreak scenarios per domain, three system prompt conditions (neutral, safety-reinforced, and tool-encouraging), and two prompt variants, producing 17,420 analysis-ready datapoints. Our central finding is that text safety does not transfer to tool-call safety. Across all six models, we observe instances where the model's text output refuses a harmful request while its tool calls simultaneously execute the forbidden action--a divergence we formalize as the GAP metric. Even under safety-reinforced system prompts, 219 such cases persist across all six models. System prompt wording exerts substantial influence on tool-call behavior: TC-safe rates span 21 percentage points for the most robust model and 57 for the most prompt-sensitive, with 16 of 18 pairwise ablation comparisons remaining significant after Bonferroni correction. Runtime governance contracts reduce information leakage in all six models but produce no detectable deterrent effect on forbidden tool-call attempts themselves. These results demonstrate that text-only safety evaluations are insufficient for assessing agent behavior and that tool-call safety requires dedicated measurement and mitigation.

</details>


### [35] [LLM4Cov: Execution-Aware Agentic Learning for High-coverage Testbench Generation](https://arxiv.org/abs/2602.16953)
*Hejia Zhang,Zhongming Yu,Chia-Tung Ho,Haoxing Ren,Brucek Khailany,Jishen Zhao*

Main category: cs.AI

TL;DR: LLM4Cov：一个用于硬件验证的离线代理学习框架，通过执行验证的数据管理、策略感知数据合成和最差状态优先采样，使紧凑模型在验证覆盖率上超越教师模型并匹敌更大模型。


<details>
  <summary>Details</summary>
Motivation: 执行感知的LLM代理虽然能从工具反馈中学习，但反馈获取昂贵且缓慢，使得在线强化学习不实用。硬件验证尤其面临这一挑战，因为它依赖工业模拟器和不可微的执行信号。

Method: 提出LLM4Cov离线代理学习框架，将验证建模为由确定性评估器引导的无记忆状态转换。采用执行验证的数据管理、策略感知的代理数据合成和最差状态优先采样，实现执行约束下的可扩展学习。

Result: 紧凑的4B参数模型在代理评估下达到69.2%的覆盖率通过率，比其教师模型高出5.3%，并与大一个数量级的模型表现相当。还创建了基于现有验证套件的现实对齐基准。

Conclusion: LLM4Cov框架通过离线学习方法有效解决了硬件验证中执行反馈昂贵的问题，使紧凑模型在验证任务上达到与更大模型竞争的性能，为执行约束下的代理学习提供了可行方案。

Abstract: Execution-aware LLM agents offer a promising paradigm for learning from tool feedback, but such feedback is often expensive and slow to obtain, making online reinforcement learning (RL) impractical. High-coverage hardware verification exemplifies this challenge due to its reliance on industrial simulators and non-differentiable execution signals. We propose LLM4Cov, an offline agent-learning framework that models verification as memoryless state transitions guided by deterministic evaluators. Building on this formulation, we introduce execution-validated data curation, policy-aware agentic data synthesis, and worst-state-prioritized sampling to enable scalable learning under execution constraints. We further curate a reality-aligned benchmark adapted from an existing verification suite through a revised evaluation protocol. Using the proposed pipeline, a compact 4B-parameter model achieves 69.2% coverage pass rate under agentic evaluation, outperforming its teacher by 5.3% and demonstrating competitive performance against models an order of magnitude larger.

</details>


### [36] [Automating Agent Hijacking via Structural Template Injection](https://arxiv.org/abs/2602.16958)
*Xinhao Deng,Jiaqing Wu,Miao Chen,Yue Xiao,Ke Xu,Qi Li*

Main category: cs.AI

TL;DR: Phantom是一个自动化代理劫持框架，通过结构化模板注入攻击LLM代理的架构机制，相比现有手动攻击显著提高了攻击成功率和查询效率。


<details>
  <summary>Details</summary>
Motivation: 现有代理劫持攻击主要依赖手动制作的语义驱动提示操纵，攻击成功率低且对闭源商业模型的迁移性有限。OWASP将代理劫持列为LLM生态系统的关键威胁，需要更有效的攻击方法来揭示系统漏洞。

Method: 基于结构化模板注入，利用LLM代理依赖特定聊天模板标记来分离系统、用户、助手和工具指令的特点。通过多级模板增强增加结构多样性，训练模板自编码器将离散模板嵌入连续可搜索的潜在空间，然后使用贝叶斯优化高效识别最优对抗向量并解码为高效结构化模板。

Result: 在Qwen、GPT和Gemini上的广泛实验表明，该框架在攻击成功率和查询效率方面显著优于现有基线。在真实商业产品中识别出70多个漏洞并获得厂商确认，证明了结构化模板劫持的实际严重性。

Conclusion: 结构化模板注入是LLM代理系统的根本性威胁，Phantom框架为保护下一代代理系统提供了实证基础，揭示了基于架构机制而非语义操纵的攻击有效性。

Abstract: Agent hijacking, highlighted by OWASP as a critical threat to the Large Language Model (LLM) ecosystem, enables adversaries to manipulate execution by injecting malicious instructions into retrieved content. Most existing attacks rely on manually crafted, semantics-driven prompt manipulation, which often yields low attack success rates and limited transferability to closed-source commercial models. In this paper, we propose Phantom, an automated agent hijacking framework built upon Structured Template Injection that targets the fundamental architectural mechanisms of LLM agents. Our key insight is that agents rely on specific chat template tokens to separate system, user, assistant, and tool instructions. By injecting optimized structured templates into the retrieved context, we induce role confusion and cause the agent to misinterpret the injected content as legitimate user instructions or prior tool outputs. To enhance attack transferability against black-box agents, Phantom introduces a novel attack template search framework. We first perform multi-level template augmentation to increase structural diversity and then train a Template Autoencoder (TAE) to embed discrete templates into a continuous, searchable latent space. Subsequently, we apply Bayesian optimization to efficiently identify optimal adversarial vectors that are decoded into high-potency structured templates. Extensive experiments on Qwen, GPT, and Gemini demonstrate that our framework significantly outperforms existing baselines in both Attack Success Rate (ASR) and query efficiency. Moreover, we identified over 70 vulnerabilities in real-world commercial products that have been confirmed by vendors, underscoring the practical severity of structured template-based hijacking and providing an empirical foundation for securing next-generation agentic systems.

</details>


### [37] [HQFS: Hybrid Quantum Classical Financial Security with VQC Forecasting, QUBO Annealing, and Audit-Ready Post-Quantum Signing](https://arxiv.org/abs/2602.16976)
*Srikumar Nayak*

Main category: cs.AI

TL;DR: HQFS是一个结合量子变分电路预测、量子退火优化和后量子签名的金融风险系统，在预测精度、投资表现和可审计性方面优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统金融风险系统的预测与优化分离架构在实际应用中存在多个问题：市场变化时决策不稳定、添加离散约束（如批量大小、上限）时表现不佳、大规模资产优化速度慢，以及监管环境需要清晰的审计追踪。需要一种能够整合预测、离散风险优化和可审计性的解决方案。

Method: HQFS采用三步混合流程：1）使用带小型经典头的变分量子电路（VQC）学习下一步收益和波动率代理；2）将风险-收益目标和约束转换为QUBO问题，优先使用量子退火求解，同时保留经典QUBO求解器作为后备；3）使用后量子签名对每个再平衡输出进行签名，确保分配结果可验证且不依赖运行时环境信任。

Result: 在市场数据集研究中，HQFS相比调优的经典基线：收益预测误差降低7.8%，波动率预测误差降低6.1%；决策层方面，样本外夏普比率提升9.4%，最大回撤降低11.7%；QUBO求解阶段在相同约束下比混合整数基线平均求解时间减少28%，同时生成完全可追踪的签名分配记录。

Conclusion: HQFS成功地将预测、离散风险优化和可审计性整合到单一流程中，在预测精度、投资表现和计算效率方面均优于传统方法，同时满足监管对可追溯性的要求，为实际金融风险系统提供了实用的混合量子-经典解决方案。

Abstract: Here's the corrected paragraph with all punctuation and formatting issues fixed:
  Financial risk systems usually follow a two-step routine: a model predicts return or risk, and then an optimizer makes a decision such as a portfolio rebalance. In practice, this split can break under real constraints. The prediction model may look good, but the final decision can be unstable when the market shifts, when discrete constraints are added (lot sizes, caps), or when the optimization becomes slow for larger asset sets. Also, regulated settings need a clear audit trail that links each decision to the exact model state and inputs. We present HQFS, a practical hybrid pipeline that connects forecasting, discrete risk optimization, and auditability in one flow. First, HQFS learns next-step return and a volatility proxy using a variational quantum circuit (VQC) with a small classical head. Second, HQFS converts the risk-return objective and constraints into a QUBO and solves it with quantum annealing when available, while keeping a compatible classical QUBO solver as a fallback for deployment. Third, HQFS signs each rebalance output using a post-quantum signature so the allocation can be verified later without trusting the runtime environment. On our market dataset study, HQFS reduces return prediction error by 7.8% and volatility prediction error by 6.1% versus a tuned classical baseline. For the decision layer, HQFS improves out-of-sample Sharpe by 9.4% and lowers maximum drawdown by 11.7%. The QUBO solve stage also cuts average solve time by 28% compared to a mixed-integer baseline under the same constraints, while producing fully traceable, signed allocation records.

</details>


### [38] [Fundamental Limits of Black-Box Safety Evaluation: Information-Theoretic and Computational Barriers from Latent Context Conditioning](https://arxiv.org/abs/2602.16984)
*Vishal Srivastava*

Main category: cs.AI

TL;DR: 黑盒安全评估无法可靠预测具有潜在上下文条件策略的AI系统部署风险，存在统计和计算上的根本限制


<details>
  <summary>Details</summary>
Motivation: 挑战黑盒安全评估的基本假设：测试分布上的模型行为能可靠预测部署性能。针对具有潜在上下文条件策略的模型（输出依赖于未观察的内部变量），这些变量在评估中罕见但在部署中普遍，需要量化黑盒评估的局限性

Method: 1. 被动评估：使用Le Cam方法证明极小极大下界；2. 自适应评估：使用基于哈希的触发构造和Yao极小极大原理；3. 计算分离：基于陷门单向函数假设；4. 白盒探测：提供显式偏差校正和样本复杂度分析

Result: 1. 被动评估：任何估计器的期望绝对误差≥0.208δL；2. 自适应评估：最坏情况误差≥δL/16，检测需要Θ(1/ε)查询；3. 计算分离：具有特权信息的部署环境可激活不安全行为，多项式时间评估器无法区分；4. 白盒探测：估计部署风险需要O(1/(γ²ε_R²))样本

Conclusion: 黑盒测试在统计上无法确定部署风险，需要额外保障措施（架构约束、训练时保证、可解释性、部署监控）来确保最坏情况下的安全性。研究提供了黑盒评估何时失效的明确标准

Abstract: Black-box safety evaluation of AI systems assumes model behavior on test distributions reliably predicts deployment performance. We formalize and challenge this assumption through latent context-conditioned policies -- models whose outputs depend on unobserved internal variables that are rare under evaluation but prevalent under deployment. We establish fundamental limits showing that no black-box evaluator can reliably estimate deployment risk for such models. (1) Passive evaluation: For evaluators sampling i.i.d. from D_eval, we prove minimax lower bounds via Le Cam's method: any estimator incurs expected absolute error >= (5/24)*delta*L approximately 0.208*delta*L, where delta is trigger probability under deployment and L is the loss gap. (2) Adaptive evaluation: Using a hash-based trigger construction and Yao's minimax principle, worst-case error remains >= delta*L/16 even for fully adaptive querying when D_dep is supported over a sufficiently large domain; detection requires Theta(1/epsilon) queries. (3) Computational separation: Under trapdoor one-way function assumptions, deployment environments possessing privileged information can activate unsafe behaviors that any polynomial-time evaluator without the trapdoor cannot distinguish. For white-box probing, estimating deployment risk to accuracy epsilon_R requires O(1/(gamma^2 * epsilon_R^2)) samples, where gamma = alpha_0 + alpha_1 - 1 measures probe quality, and we provide explicit bias correction under probe error. Our results quantify when black-box testing is statistically underdetermined and provide explicit criteria for when additional safeguards -- architectural constraints, training-time guarantees, interpretability, and deployment monitoring -- are mathematically necessary for worst-case safety assurance.

</details>


### [39] [Conv-FinRe: A Conversational and Longitudinal Benchmark for Utility-Grounded Financial Recommendation](https://arxiv.org/abs/2602.16990)
*Yan Wang,Yi Han,Lingfei Qian,Yueru He,Xueqing Peng,Dongji Feng,Zhuohan Xie,Vincent Jim Zhang,Rosie Guo,Fengran Mo,Jimin Huang,Yankai Chen,Xue Liu,Jian-Yun Nie*

Main category: cs.AI

TL;DR: Conv-FinRe是一个用于股票推荐的对话式纵向基准测试，它超越了单纯的行为模仿，通过多视角参考来评估LLMs的决策质量与用户行为匹配之间的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有推荐基准主要评估模型模仿用户行为的能力，但在金融咨询领域，观察到的用户行为可能受到市场波动影响而变得嘈杂或短视，可能与用户的长期目标冲突。将用户选择作为唯一真实标准会混淆行为模仿与决策质量。

Method: 构建Conv-FinRe基准测试，包含入职访谈、逐步市场情境和咨询对话。模型需要在固定投资期限内生成股票排名。基准提供多视角参考，区分描述性行为与基于投资者特定风险偏好的规范性效用，从而诊断LLMs是遵循理性分析、模仿用户噪声还是受市场动量驱动。

Result: 评估结果显示，在理性决策质量与行为对齐之间存在持续紧张关系：在基于效用的排名上表现良好的模型往往无法匹配用户选择，而行为对齐的模型可能过度拟合短期噪声。

Conclusion: Conv-FinRe基准测试能够评估LLMs在金融推荐中超越行为模仿的能力，揭示了决策质量与行为对齐之间的权衡，为金融咨询中的AI评估提供了新视角。

Abstract: Most recommendation benchmarks evaluate how well a model imitates user behavior. In financial advisory, however, observed actions can be noisy or short-sighted under market volatility and may conflict with a user's long-term goals. Treating what users chose as the sole ground truth, therefore, conflates behavioral imitation with decision quality. We introduce Conv-FinRe, a conversational and longitudinal benchmark for stock recommendation that evaluates LLMs beyond behavior matching. Given an onboarding interview, step-wise market context, and advisory dialogues, models must generate rankings over a fixed investment horizon. Crucially, Conv-FinRe provides multi-view references that distinguish descriptive behavior from normative utility grounded in investor-specific risk preferences, enabling diagnosis of whether an LLM follows rational analysis, mimics user noise, or is driven by market momentum. We build the benchmark from real market data and human decision trajectories, instantiate controlled advisory conversations, and evaluate a suite of state-of-the-art LLMs. Results reveal a persistent tension between rational decision quality and behavioral alignment: models that perform well on utility-based ranking often fail to match user choices, whereas behaviorally aligned models can overfit short-term noise. The dataset is publicly released on Hugging Face, and the codebase is available on GitHub.

</details>


### [40] [Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases](https://arxiv.org/abs/2602.17001)
*Zhao Tan,Yiji Zhao,Shiyu Wang,Chang Xu,Yuxuan Liang,Xiping Liu,Shirui Pan,Ming Jin*

Main category: cs.AI

TL;DR: Sonar-TS是一个神经符号框架，通过"搜索-验证"管道解决时间序列数据库的自然语言查询问题，并提出了首个大规模基准测试NLQTSBench。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：Text-to-SQL方法无法处理连续形态意图（如形状或异常），而时间序列模型难以处理超长历史数据。需要一种新方法来解决时间序列数据库的自然语言查询问题。

Method: 提出Sonar-TS神经符号框架，采用类似主动声纳的"搜索-验证"管道：1) 使用特征索引通过SQL搜索候选窗口；2) 生成Python程序对原始信号进行验证锁定。

Result: 实验表明Sonar-TS能有效处理传统方法失败的复杂时间查询，并揭示了该领域的独特挑战。同时创建了首个大规模基准测试NLQTSBench。

Conclusion: 这是对时间序列数据库自然语言查询的首次系统性研究，提供了一个通用框架和评估标准，为未来研究奠定了基础。

Abstract: Natural Language Querying for Time Series Databases (NLQ4TSDB) aims to assist non-expert users retrieve meaningful events, intervals, and summaries from massive temporal records. However, existing Text-to-SQL methods are not designed for continuous morphological intents such as shapes or anomalies, while time series models struggle to handle ultra-long histories. To address these challenges, we propose Sonar-TS, a neuro-symbolic framework that tackles NLQ4TSDB via a Search-Then-Verify pipeline. Analogous to active sonar, it utilizes a feature index to ping candidate windows via SQL, followed by generated Python programs to lock on and verify candidates against raw signals. To enable effective evaluation, we introduce NLQTSBench, the first large-scale benchmark designed for NLQ over TSDB-scale histories. Our experiments highlight the unique challenges within this domain and demonstrate that Sonar-TS effectively navigates complex temporal queries where traditional methods fail. This work presents the first systematic study of NLQ4TSDB, offering a general framework and evaluation standard to facilitate future research.

</details>


### [41] [M2F: Automated Formalization of Mathematical Literature at Scale](https://arxiv.org/abs/2602.17016)
*Zichen Wang,Wanli Ma,Zhenyu Ming,Gong Zhang,Kun Yuan,Zaiwen Wen*

Main category: cs.AI

TL;DR: M2F是首个用于端到端、项目规模数学自动形式化的智能体框架，可将长篇数学资料转换为可编译的Lean库，实现教科书级形式化。


<details>
  <summary>Details</summary>
Motivation: 现有数学自动形式化技术局限于孤立定理和短片段，无法扩展到教科书和研究论文级别，需要解决跨文件依赖、导入解析和端到端编译等问题。

Method: 采用两阶段框架：1) 语句编译阶段将文档分割为原子块，通过推断依赖排序，修复声明框架直到项目可编译；2) 证明修复阶段在固定签名下使用目标导向的局部编辑填补证明空缺。全程保持验证器在循环中，只有工具链反馈确认改进时才提交编辑。

Result: 在约三周内，将479页的实分析和凸分析教科书转换为153,853行Lean代码库，实现完全形式化。在FATE-H基准上达到96%的证明成功率（基线为80%），展示了大规模数学文献自动形式化的可行性。

Conclusion: M2F框架证明大规模数学文献自动形式化是可行的，能够以传统专家需要数月或数年的速度实现教科书级形式化，为数学形式化的实际应用开辟了新途径。

Abstract: Automated formalization of mathematics enables mechanical verification but remains limited to isolated theorems and short snippets. Scaling to textbooks and research papers is largely unaddressed, as it requires managing cross-file dependencies, resolving imports, and ensuring that entire projects compile end-to-end. We present M2F (Math-to-Formal), the first agentic framework for end-to-end, project-scale autoformalization in Lean. The framework operates in two stages. The statement compilation stage splits the document into atomic blocks, orders them via inferred dependencies, and repairs declaration skeletons until the project compiles, allowing placeholders in proofs. The proof repair stage closes these holes under fixed signatures using goal-conditioned local edits. Throughout both stages, M2F keeps the verifier in the loop, committing edits only when toolchain feedback confirms improvement. In approximately three weeks, M2F converts long-form mathematical sources into a project-scale Lean library of 153,853 lines from 479 pages textbooks on real analysis and convex analysis, fully formalized as Lean declarations with accompanying proofs. This represents textbook-scale formalization at a pace that would typically require months or years of expert effort. On FATE-H, we achieve $96\%$ proof success (vs.\ $80\%$ for a strong baseline). Together, these results demonstrate that practical, large-scale automated formalization of mathematical literature is within reach. The full generated Lean code from our runs is available at https://github.com/optsuite/ReasBook.git.

</details>


### [42] [Sales Research Agent and Sales Research Bench](https://arxiv.org/abs/2602.17017)
*Deepanjan Bhol*

Main category: cs.AI

TL;DR: 微软Dynamics 365 Sales中的Sales Research Agent是一个AI应用，能连接实时CRM数据，通过文本和图表提供决策洞察，并在专门基准测试中显著优于Claude和ChatGPT。


<details>
  <summary>Details</summary>
Motivation: 企业需要能够基于实时定制CRM数据回答销售领导问题的AI系统，但现有模型缺乏透明、可重复的质量评估方法。

Method: 开发Sales Research Agent应用，连接实时CRM和相关数据，在复杂模式上进行推理；创建Sales Research Bench基准，从8个客户加权维度（包括文本/图表准确性、相关性、可解释性、模式准确性和图表质量）评估系统。

Result: 在2025年10月19日对定制企业模式的200个问题测试中，Sales Research Agent在100分总分上比Claude Sonnet 4.5高出13分，比ChatGPT-5高出24.1分。

Conclusion: Sales Research Agent为销售领导提供了基于实时CRM数据的AI决策支持，并通过专门基准测试为客户提供了可重复比较AI解决方案的方法。

Abstract: Enterprises increasingly need AI systems that can answer sales-leader questions over live, customized CRM data, but most available models do not expose transparent, repeatable evidence of quality. This paper describes the Sales Research Agent in Microsoft Dynamics 365 Sales, an AI-first application that connects to live CRM and related data, reasons over complex schemas, and produces decision-ready insights through text and chart outputs. To make quality observable, we introduce the Sales Research Bench, a purpose-built benchmark that scores systems on eight customer-weighted dimensions, including text and chart groundedness, relevance, explainability, schema accuracy, and chart quality. In a 200-question run on a customized enterprise schema on October 19, 2025, the Sales Research Agent outperformed Claude Sonnet 4.5 by 13 points and ChatGPT-5 by 24.1 points on the 100-point composite score, giving customers a repeatable way to compare AI solutions.

</details>


### [43] [Phase-Aware Mixture of Experts for Agentic Reinforcement Learning](https://arxiv.org/abs/2602.17038)
*Shengtian Yang,Yu Li,Shuo He,Yewen Li,Qingpeng Cai,Peng Jiang,Lei Feng*

Main category: cs.AI

TL;DR: 提出PA-MoE方法解决RL中单一策略网络导致的简单任务偏差问题，通过相位感知的专家混合架构让不同专家专注于不同任务阶段。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法使用单一策略网络导致"简单性偏差"——简单任务占用大部分参数并主导梯度更新，复杂任务缺乏足够容量。传统MoE的token级路由会将相位一致的模式分散到不同专家，破坏专家专业化。

Method: 提出相位感知专家混合(PA-MoE)：1) 轻量级相位路由器直接从RL目标学习潜在相位边界，无需预定义相位类别；2) 相位路由器为相同专家分配时间一致的分配，让专家保留相位特定专业知识。

Result: 实验结果表明PA-MoE的有效性。

Conclusion: PA-MoE通过相位感知路由解决了传统MoE在RL中的局限性，让专家能够专注于不同的任务阶段，从而更有效地处理复杂任务。

Abstract: Reinforcement learning (RL) has equipped LLM agents with a strong ability to solve complex tasks. However, existing RL methods normally use a \emph{single} policy network, causing \emph{simplicity bias} where simple tasks occupy most parameters and dominate gradient updates, leaving insufficient capacity for complex tasks. A plausible remedy could be employing the Mixture-of-Experts (MoE) architecture in the policy network, as MoE allows different parameters (experts) to specialize in different tasks, preventing simple tasks from dominating all parameters. However, a key limitation of traditional MoE is its token-level routing, where the router assigns each token to specialized experts, which fragments phase-consistent patterns into scattered expert assignments and thus undermines expert specialization. In this paper, we propose \textbf{Phase-Aware Mixture of Experts (PA-MoE)}. It first features a lightweight \emph{phase router} that learns latent phase boundaries directly from the RL objective without pre-defining phase categories. Then, the phase router allocates temporally consistent assignments to the same expert, allowing experts to preserve phase-specific expertise. Experimental results demonstrate the effectiveness of our proposed PA-MoE.

</details>


### [44] [Dynamic System Instructions and Tool Exposure for Efficient Agentic LLMs](https://arxiv.org/abs/2602.17046)
*Uria Franko*

Main category: cs.AI

TL;DR: 提出ITR方法，通过检索最小化系统提示片段和必要工具子集，大幅减少LLM代理每步的上下文token，降低成本并提高工具选择准确性


<details>
  <summary>Details</summary>
Motivation: LLM代理在运行时需要反复读取长系统指令和大量工具目录，导致成本增加、代理偏离概率上升、延迟增加和工具选择错误

Method: 提出指令-工具检索(ITR)，一种RAG变体，每步只检索必要的系统提示片段和最小工具子集，组合成动态运行时系统提示，并提供置信度门控回退机制

Result: 每步上下文token减少95%，正确工具路由相对提高32%，端到端成本降低70%，使代理能在上下文限制内运行2-20倍更多循环

Conclusion: ITR特别适用于长时间运行的自主代理，节省随着代理步骤数量而复合，提供了实用的部署指导

Abstract: Large Language Model (LLM) agents often run for many steps while re-ingesting long system instructions and large tool catalogs each turn. This increases cost, agent derailment probability, latency, and tool-selection errors. We propose Instruction-Tool Retrieval (ITR), a RAG variant that retrieves, per step, only the minimal system-prompt fragments and the smallest necessary subset of tools. ITR composes a dynamic runtime system prompt and exposes a narrowed toolset with confidence-gated fallbacks. Using a controlled benchmark with internally consistent numbers, ITR reduces per-step context tokens by 95%, improves correct tool routing by 32% relative, and cuts end-to-end episode cost by 70% versus a monolithic baseline. These savings enable agents to run 2-20x more loops within context limits. Savings compound with the number of agent steps, making ITR particularly valuable for long-running autonomous agents. We detail the method, evaluation protocol, ablations, and operational guidance for practical deployment.

</details>


### [45] [IntentCUA: Learning Intent-level Representations for Skill Abstraction and Multi-Agent Planning in Computer-Use Agents](https://arxiv.org/abs/2602.17049)
*Seoyoung Lee,Seobin Yoon,Seongbeen Lee,Yoojung Chun,Dayoung Park,Doyeon Kim,Joo Yong Sim*

Main category: cs.AI

TL;DR: IntentCUA是一个多智能体计算机使用框架，通过意图对齐的计划记忆来稳定长时程执行，在桌面自动化任务中实现了74.83%的成功率和0.91的步骤效率比。


<details>
  <summary>Details</summary>
Motivation: 现有方法（基于RL的规划器和轨迹检索）在长时程任务中容易偏离用户意图，重复解决常规子问题，导致错误累积和效率低下。需要一种能够稳定执行、减少冗余规划的方法。

Method: 提出IntentCUA多智能体框架，包含规划器、计划优化器和批评器，通过共享记忆将原始交互轨迹抽象为多视图意图表示和可重用技能。运行时，意图原型检索子目标对齐的技能并注入到部分计划中。

Result: 在端到端评估中，IntentCUA实现了74.83%的任务成功率，步骤效率比为0.91，优于基于RL和轨迹中心的基线方法。消融实验显示多视图意图抽象和共享计划记忆共同提高了执行稳定性。

Conclusion: 系统级意图抽象和基于记忆的协调是实现大型动态环境中可靠高效桌面自动化的关键。合作式多智能体循环在长时程任务中提供了最大的性能提升。

Abstract: Computer-use agents operate over long horizons under noisy perception, multi-window contexts, evolving environment states. Existing approaches, from RL-based planners to trajectory retrieval, often drift from user intent and repeatedly solve routine subproblems, leading to error accumulation and inefficiency. We present IntentCUA, a multi-agent computer-use framework designed to stabilize long-horizon execution through intent-aligned plan memory. A Planner, Plan-Optimizer, and Critic coordinate over shared memory that abstracts raw interaction traces into multi-view intent representations and reusable skills. At runtime, intent prototypes retrieve subgroup-aligned skills and inject them into partial plans, reducing redundant re-planning and mitigating error propagation across desktop applications. In end-to-end evaluations, IntentCUA achieved a 74.83% task success rate with a Step Efficiency Ratio of 0.91, outperforming RL-based and trajectory-centric baselines. Ablations show that multi-view intent abstraction and shared plan memory jointly improve execution stability, with the cooperative multi-agent loop providing the largest gains on long-horizon tasks. These results highlight that system-level intent abstraction and memory-grounded coordination are key to reliable and efficient desktop automation in large, dynamic environments.

</details>


### [46] [RFEval: Benchmarking Reasoning Faithfulness under Counterfactual Reasoning Intervention in Large Reasoning Models](https://arxiv.org/abs/2602.17053)
*Yunseok Han,Yejoon Lee,Jaeyoung Do*

Main category: cs.AI

TL;DR: 论文提出RFEval基准，用于评估大型推理模型的忠实性（faithfulness），发现49.7%输出存在不忠实问题，准确率不能作为忠实性的可靠代理。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）虽然表现良好，但经常产生看似合理却未能反映其真实决策过程的推理，这损害了可靠性和信任度。需要建立正式框架来评估推理的忠实性。

Method: 提出忠实性形式化框架，包含两个可测试条件：立场一致性（推理与答案的连贯立场）和因果影响（在输出层面干预下，陈述的推理能因果驱动答案）。开发RFEval基准，包含7,186个实例，通过受控的输出层面反事实干预来探测忠实性。

Result: 评估12个开源LRMs，发现49.7%的输出存在不忠实问题，主要源于立场不一致。失败集中在数学和代码等脆弱、收敛领域。后训练机制比模型规模更影响忠实性：在监督微调基础上添加当前RL风格目标会降低推理忠实性，即使准确率保持不变。准确率既不是忠实性的充分条件，也不是可靠代理。

Conclusion: 建立了审计LRM可靠性的严谨方法，表明可信AI不仅需要优化正确结果，还需要优化推理过程的结构完整性。

Abstract: Large Reasoning Models (LRMs) exhibit strong performance, yet often produce rationales that sound plausible but fail to reflect their true decision process, undermining reliability and trust. We introduce a formal framework for reasoning faithfulness, defined by two testable conditions: stance consistency (a coherent stance linking reasoning to answer) and causal influence (the stated reasoning causally drives the answer under output-level interventions), explicitly decoupled from accuracy. To operationalize this, we present RFEval, a benchmark of 7,186 instances across seven tasks that probes faithfulness via controlled, output-level counterfactual interventions. Evaluating twelve open-source LRMs, we find unfaithfulness in 49.7% of outputs, predominantly from stance inconsistency. Failures are concentrated in brittle, convergent domains such as math and code, and correlate more with post-training regimes than with scale: within-family ablations indicate that adding current RL-style objectives on top of supervised fine-tuning can reduce reasoning faithfulness, even when accuracy is maintained. Crucially, accuracy is neither a sufficient nor a reliable proxy for faithfulness: once controlling for model and task, the accuracy-faithfulness link is weak and statistically insignificant. Our work establishes a rigorous methodology for auditing LRM reliability and shows that trustworthy AI requires optimizing not only for correct outcomes but also for the structural integrity of the reasoning process. Our code and dataset can be found at project page: $\href{https://aidaslab.github.io/RFEval/}{https://aidaslab.github.io/RFEval/}$

</details>


### [47] [Retaining Suboptimal Actions to Follow Shifting Optima in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.17062)
*Yonghyeon Jo,Sunwoo Lee,Seungyul Han*

Main category: cs.AI

TL;DR: 提出S2Q方法，通过多个子价值函数保留备选高价值动作，增强多智能体强化学习的适应性和探索能力


<details>
  <summary>Details</summary>
Motivation: 现有价值分解方法依赖单一最优动作，在价值函数变化时难以适应，容易收敛到次优策略

Method: 提出Successive Sub-value Q-learning (S2Q)，学习多个子价值函数来保留备选高价值动作，结合Softmax行为策略促进持续探索

Result: 在挑战性MARL基准测试中，S2Q持续优于多种MARL算法，展现出更好的适应性和整体性能

Conclusion: S2Q通过多子价值函数和Softmax策略有效解决了价值函数变化时的适应问题，提升了多智能体强化学习的性能

Abstract: Value decomposition is a core approach for cooperative multi-agent reinforcement learning (MARL). However, existing methods still rely on a single optimal action and struggle to adapt when the underlying value function shifts during training, often converging to suboptimal policies. To address this limitation, we propose Successive Sub-value Q-learning (S2Q), which learns multiple sub-value functions to retain alternative high-value actions. Incorporating these sub-value functions into a Softmax-based behavior policy, S2Q encourages persistent exploration and enables $Q^{\text{tot}}$ to adjust quickly to the changing optima. Experiments on challenging MARL benchmarks confirm that S2Q consistently outperforms various MARL algorithms, demonstrating improved adaptability and overall performance. Our code is available at https://github.com/hyeon1996/S2Q.

</details>


### [48] [Predictive Batch Scheduling: Accelerating Language Model Training Through Loss-Aware Sample Prioritization](https://arxiv.org/abs/2602.17066)
*Sumedh Rasal*

Main category: cs.AI

TL;DR: PBS是一种通过动态优先处理高损失样本来加速语言模型收敛的训练优化技术，使用轻量级线性预测器从静态标记级特征估计样本难度，相比传统方法计算开销极低。


<details>
  <summary>Details</summary>
Motivation: 传统课程学习方法需要预定义难度指标，而硬样本挖掘方法需要昂贵的逐样本损失跟踪。PBS旨在开发一种轻量级、计算开销极低的方法来动态识别困难样本，从而加速模型收敛。

Method: PBS使用在线训练的轻量级线性预测器，仅基于四个简单的标记级特征（标记频率、序列长度、词汇多样性、稀有标记比例）来估计样本难度。预测器在训练过程中动态更新，用于优先选择高损失样本构建批次。

Result: 在130M参数变压器上的实验表明，PBS实现了6-13%的收敛加速（通过训练检查点的评估损失衡量）。预测器与真实损失的相关系数从0.14提高到0.44（超过10,000个训练步骤），仅使用四个特征就达到了0.44的相关性。

Conclusion: 标记频率统计编码了有关样本难度的有意义信息，使得能够以可忽略的计算开销实现有效的课程学习。PBS提供了一种实用且高效的训练优化方法。

Abstract: We introduce Predictive Batch Scheduling (PBS), a novel training optimization technique that accelerates language model convergence by dynamically prioritizing high-loss samples during batch construction. Unlike curriculum learning approaches that require predefined difficulty metrics or hard example mining methods that demand expensive per-sample loss tracking, PBS employs a lightweight linear predictor trained online to estimate sample difficulty from static token-level features. Our predictor achieves 0.44 correlation with actual loss using only four simple features: token frequency, sequence length, vocabulary diversity, and rare token ratio. Experiments on a 130M parameter transformer demonstrate that PBS achieves 6-13\% faster convergence measured by evaluation loss across training checkpoints, with the predictor's correlation improving from 0.14 to 0.44 over 10,000 training steps. These results validate that token frequency statistics encode meaningful information about sample difficulty, enabling effective curriculum learning with negligible computational overhead.

</details>


### [49] [How AI Coding Agents Communicate: A Study of Pull Request Description Characteristics and Human Review Responses](https://arxiv.org/abs/2602.17084)
*Kan Watanabe,Rikuto Tsuchida,Takahiro Monno,Bin Huang,Kazuma Yamasaki,Youmei Fan,Kazumasa Shimari,Kenichi Matsumoto*

Main category: cs.AI

TL;DR: AI编码代理在GitHub上创建的PR描述风格存在差异，这些差异会影响人类评审员的参与度、响应时间和合并结果。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速采用，AI编码代理开始自主创建GitHub PR，但人们对这些代理在PR描述特征上的差异以及人类评审员如何响应它们了解不足。

Method: 使用AIDev数据集对五个AI编码代理创建的PR进行实证分析，分析PR描述的结构特征，并检查人类评审员在评审活动、响应时间、情感和合并结果方面的响应。

Result: 发现AI编码代理展现出不同的PR描述风格，这些风格与评审员参与度、响应时间和合并结果的差异相关。不同代理在评审互动指标和合并率上存在显著差异。

Conclusion: PR呈现方式和评审员互动动态在人类-AI协作软件开发中扮演重要角色，AI代理的PR描述风格会影响人类评审体验和项目结果。

Abstract: The rapid adoption of large language models has led to the emergence of AI coding agents that autonomously create pull requests on GitHub. However, how these agents differ in their pull request description characteristics, and how human reviewers respond to them, remains underexplored. In this study, we conduct an empirical analysis of pull requests created by five AI coding agents using the AIDev dataset. We analyze agent differences in pull request description characteristics, including structural features, and examine human reviewer response in terms of review activity, response timing, sentiment, and merge outcomes. We find that AI coding agents exhibit distinct PR description styles, which are associated with differences in reviewer engagement, response time, and merge outcomes. We observe notable variation across agents in both reviewer interaction metrics and merge rates. These findings highlight the role of pull request presentation and reviewer interaction dynamics in human-AI collaborative software development.

</details>


### [50] [Agentic Wireless Communication for 6G: Intent-Aware and Continuously Evolving Physical-Layer Intelligence](https://arxiv.org/abs/2602.17096)
*Zhaoyang Li,Xingzhi Jin,Junyu Pan,Qianqian Yang,Zhiguo Shi*

Main category: cs.AI

TL;DR: 论文探讨了6G无线系统中基于意图驱动的自主智能，利用大语言模型构建智能代理，实现从意图感知到网络执行的闭环控制，以应对多维用户需求和动态环境变化。


<details>
  <summary>Details</summary>
Motivation: 6G系统功能复杂性增加，用户需求从单一指标转向多维目标（时延敏感性、能耗偏好、计算约束等），且需求随时间动态变化。传统基于规则的控制和中心化优化难以应对，需要意图驱动的自主智能来准确理解通信环境和用户意图。

Method: 采用基于大语言模型的智能代理，构建意图感知、自主决策和网络执行的闭环管道。研究多模态感知、跨层决策和可持续优化等关键技术，并通过案例研究AgenCom——一个意图驱动的链路决策代理，根据用户偏好和信道条件自适应构建通信链路。

Result: 论文分析了物理层任务在支持意图感知和自主性方面的局限性，识别了智能代理AI具有优势的应用场景，讨论了关键技术挑战，并通过AgenCom案例展示了意图驱动代理的实际应用潜力。

Conclusion: 基于大语言模型的智能代理为6G物理层提供了实现意图驱动自主通信的可行路径，能够整合异构信息并将自然语言意图转化为可执行的控制决策，是6G向可持续自主演进的关键技术方向。

Abstract: As 6G wireless systems evolve, growing functional complexity and diverse service demands are driving a shift from rule-based control to intent-driven autonomous intelligence. User requirements are no longer captured by a single metric (e.g., throughput or reliability), but by multi-dimensional objectives such as latency sensitivity, energy preference, computational constraints, and service-level requirements. These objectives may also change over time due to environmental dynamics and user-network interactions. Therefore, accurate understanding of both the communication environment and user intent is critical for autonomous and sustainably evolving 6G communications.
  Large language models (LLMs), with strong contextual understanding and cross-modal reasoning, provide a promising foundation for intent-aware network agents. Compared with rule-driven or centrally optimized designs, LLM-based agents can integrate heterogeneous information and translate natural-language intents into executable control and configuration decisions.
  Focusing on a closed-loop pipeline of intent perception, autonomous decision making, and network execution, this paper investigates agentic AI for the 6G physical layer and its realization pathways. We review representative physical-layer tasks and their limitations in supporting intent awareness and autonomy, identify application scenarios where agentic AI is advantageous, and discuss key challenges and enabling technologies in multimodal perception, cross-layer decision making, and sustainable optimization. Finally, we present a case study of an intent-driven link decision agent, termed AgenCom, which adaptively constructs communication links under diverse user preferences and channel conditions.

</details>


### [51] [Toward Trustworthy Evaluation of Sustainability Rating Methodologies: A Human-AI Collaborative Framework for Benchmark Dataset Construction](https://arxiv.org/abs/2602.17106)
*Xiaoran Cai,Wang Yang,Xiyu Ren,Chekun Law,Rohit Sharma,Peng Qi*

Main category: cs.AI

TL;DR: 提出一个人类-AI协作框架，包含STRIDE和SR-Delta两部分，用于生成可信的基准数据集，以评估和协调不同ESG评级机构的方法论差异。


<details>
  <summary>Details</summary>
Motivation: 不同ESG评级机构对同一公司的评分差异很大，限制了评级的可比性、可信度和决策相关性，需要协调这些评级结果。

Method: 提出通用人类-AI协作框架：1) STRIDE - 提供原则性标准和评分系统，指导使用大语言模型构建公司级基准数据集；2) SR-Delta - 差异分析程序框架，揭示潜在调整的见解。

Result: 该框架能够实现可持续发展评级方法论的可扩展和可比较评估，为协调不同评级机构的结果提供系统性方法。

Conclusion: 呼吁AI社区采用AI驱动方法来加强和推进可持续发展评级方法论，以支持和执行紧迫的可持续发展议程。

Abstract: Sustainability or ESG rating agencies use company disclosures and external data to produce scores or ratings that assess the environmental, social, and governance performance of a company. However, sustainability ratings across agencies for a single company vary widely, limiting their comparability, credibility, and relevance to decision-making. To harmonize the rating results, we propose adopting a universal human-AI collaboration framework to generate trustworthy benchmark datasets for evaluating sustainability rating methodologies. The framework comprises two complementary parts: STRIDE (Sustainability Trust Rating & Integrity Data Equation) provides principled criteria and a scoring system that guide the construction of firm-level benchmark datasets using large language models (LLMs), and SR-Delta, a discrepancy-analysis procedural framework that surfaces insights for potential adjustments. The framework enables scalable and comparable assessment of sustainability rating methodologies. We call on the broader AI community to adopt AI-powered approaches to strengthen and advance sustainability rating methodologies that support and enforce urgent sustainability agendas.

</details>


### [52] [Owen-based Semantics and Hierarchy-Aware Explanation (O-Shap)](https://arxiv.org/abs/2602.17107)
*Xiangyu Zhou,Chenhan Xiao,Yang Weng*

Main category: cs.AI

TL;DR: 提出O-Shap方法，通过满足T-性质的分割策略改进Owen值在图像解释中的应用，提升特征归因的准确性和语义一致性。


<details>
  <summary>Details</summary>
Motivation: 传统Shapley值方法假设特征独立，但在视觉任务中像素之间存在空间和语义依赖关系，导致解释效果不佳。现有的Owen值实现依赖于分割策略，但常用分割方法（如轴对齐或SLIC）违反了关键的一致性性质。

Method: 提出新的分割方法，满足T-性质以确保层次结构中的语义对齐，支持计算剪枝。该方法在Owen值框架下实现，称为O-Shap。

Result: 在图像和表格数据集上的实验表明，O-Shap在归因精度、语义一致性和运行效率方面优于基线SHAP变体，特别是在结构重要的情况下表现更佳。

Conclusion: 通过满足T-性质的分割策略改进Owen值应用，能够更好地处理视觉任务中的特征依赖关系，提升可解释人工智能的准确性和可解释性。

Abstract: Shapley value-based methods have become foundational in explainable artificial intelligence (XAI), offering theoretically grounded feature attributions through cooperative game theory. However, in practice, particularly in vision tasks, the assumption of feature independence breaks down, as features (i.e., pixels) often exhibit strong spatial and semantic dependencies. To address this, modern SHAP implementations now include the Owen value, a hierarchical generalization of the Shapley value that supports group attributions. While the Owen value preserves the foundations of Shapley values, its effectiveness critically depends on how feature groups are defined. We show that commonly used segmentations (e.g., axis-aligned or SLIC) violate key consistency properties, and propose a new segmentation approach that satisfies the $T$-property to ensure semantic alignment across hierarchy levels. This hierarchy enables computational pruning while improving attribution accuracy and interpretability. Experiments on image and tabular datasets demonstrate that O-Shap outperforms baseline SHAP variants in attribution precision, semantic coherence, and runtime efficiency, especially when structure matters.

</details>


### [53] [Instructor-Aligned Knowledge Graphs for Personalized Learning](https://arxiv.org/abs/2602.17111)
*Abdulrahman AlRabah,Priyanka Kargupta,Jiawei Han,Abdussalam Alawini*

Main category: cs.AI

TL;DR: InstructKG：一个自动构建教师对齐知识图谱的框架，从课程材料中提取概念节点并推断学习依赖关系，用于个性化学习


<details>
  <summary>Details</summary>
Motivation: 大规模课程中，教师难以诊断个别学生的知识缺口，需要自动化的方法来捕捉概念间的先决条件和子概念关系，以实现个性化学习干预

Method: 提出InstructKG框架，利用课程讲义材料（幻灯片、笔记等），提取重要概念作为节点，推断学习依赖关系作为有向边（如"部分-整体"或"依赖"关系），结合教育材料特有的时间语义信号和大语言模型的泛化能力

Result: 在真实世界多样化的课程讲义材料上进行实验和人工评估，证明InstructKG能够捕捉丰富且与教师意图一致的学习进展关系

Conclusion: InstructKG能够自动构建教师对齐的知识图谱，有效捕捉课程预期的学习进展，为大规模个性化教育提供支持

Abstract: Mastering educational concepts requires understanding both their prerequisites (e.g., recursion before merge sort) and sub-concepts (e.g., merge sort as part of sorting algorithms). Capturing these dependencies is critical for identifying students' knowledge gaps and enabling targeted intervention for personalized learning. This is especially challenging in large-scale courses, where instructors cannot feasibly diagnose individual misunderstanding or determine which concepts need reinforcement. While knowledge graphs offer a natural representation for capturing these conceptual relationships at scale, existing approaches are either surface-level (focusing on course-level concepts like "Algorithms" or logistical relationships such as course enrollment), or disregard the rich pedagogical signals embedded in instructional materials. We propose InstructKG, a framework for automatically constructing instructor-aligned knowledge graphs that capture a course's intended learning progression. Given a course's lecture materials (slides, notes, etc.), InstructKG extracts significant concepts as nodes and infers learning dependencies as directed edges (e.g., "part-of" or "depends-on" relationships). The framework synergizes the rich temporal and semantic signals unique to educational materials (e.g., "recursion" is taught before "mergesort"; "recursion" is mentioned in the definition of "merge sort") with the generalizability of large language models. Through experiments on real-world, diverse lecture materials across multiple courses and human-based evaluation, we demonstrate that InstructKG captures rich, instructor-aligned learning progressions.

</details>


### [54] [Epistemology of Generative AI: The Geometry of Knowing](https://arxiv.org/abs/2602.17116)
*Ilya Levin*

Main category: cs.AI

TL;DR: 论文提出高维空间的索引认识论，将生成式AI视为学习流形上的导航者，提出导航知识作为符号推理和统计重组之外的第三种知识生产方式。


<details>
  <summary>Details</summary>
Motivation: 生成式AI对知识和知识生产提出了前所未有的挑战。与以往技术变革不同，生成式AI的认知机制仍然模糊，缺乏对其认知特征的理解，无法在科学、教育和制度生活中进行负责任的整合。需要突破传统图灵-香农-冯·诺依曼范式，建立新的认识论框架。

Method: 基于高维几何的四个结构特性（测度集中、近正交性、指数方向容量、流形正则性），发展"高维空间的索引认识论"。结合皮尔士符号学和帕珀特建构主义，将生成模型重新概念化为学习流形上的导航者。

Result: 提出导航知识作为第三种知识生产方式，区别于符号推理和统计重组。生成式AI通过在高维语义空间中定位和导航来产生知识，这种机制超越了传统的符号处理范式。

Conclusion: 生成式AI代表了认知范式的根本转变，需要新的认识论框架来理解其知识生产方式。高维空间的索引认识论为理解生成式AI的认知特征提供了理论基础，导航知识概念的提出有助于负责任地将AI整合到科学和社会中。

Abstract: Generative AI presents an unprecedented challenge to our understanding of knowledge and its production. Unlike previous technological transformations, where engineering understanding preceded or accompanied deployment, generative AI operates through mechanisms whose epistemic character remains obscure, and without such understanding, its responsible integration into science, education, and institutional life cannot proceed on a principled basis. This paper argues that the missing account must begin with a paradigmatic break that has not yet received adequate philosophical attention. In the Turing-Shannon-von Neumann tradition, information enters the machine as encoded binary vectors, and semantics remains external to the process. Neural network architectures rupture this regime: symbolic input is instantly projected into a high-dimensional space where coordinates correspond to semantic parameters, transforming binary code into a position in a geometric space of meanings. It is this space that constitutes the active epistemic condition shaping generative production. Drawing on four structural properties of high-dimensional geometry concentration of measure, near-orthogonality, exponential directional capacity, and manifold regularity the paper develops an Indexical Epistemology of High-Dimensional Spaces. Building on Peirce semiotics and Papert constructionism, it reconceptualizes generative models as navigators of learned manifolds and proposes navigational knowledge as a third mode of knowledge production, distinct from both symbolic reasoning and statistical recombination.

</details>


### [55] [Efficient Parallel Algorithm for Decomposing Hard CircuitSAT Instances](https://arxiv.org/abs/2602.17130)
*Victor Kondratiev,Irina Gribanova,Alexander Semenov*

Main category: cs.AI

TL;DR: 提出一种新颖的并行算法，用于分解困难的CircuitSAT实例，通过专用约束将原始SAT实例划分为弱化公式族，参数可调以高效识别高质量分解


<details>
  <summary>Details</summary>
Motivation: 解决困难CircuitSAT实例的分解问题，特别是逻辑等价性检查和密码哈希函数原像攻击等实际应用中的挑战性实例

Method: 使用专用约束将原始SAT实例划分为弱化公式族，实现为参数化并行算法，通过调整参数在并行计算的指导下高效识别高质量分解

Result: 算法在挑战性CircuitSAT实例上展示实际效果，包括布尔电路逻辑等价性检查和密码哈希函数原像攻击的编码实例

Conclusion: 提出的并行分解算法能有效处理困难CircuitSAT问题，为逻辑等价性检查和密码分析等实际应用提供实用解决方案

Abstract: We propose a novel parallel algorithm for decomposing hard CircuitSAT instances. The technique employs specialized constraints to partition an original SAT instance into a family of weakened formulas. Our approach is implemented as a parameterized parallel algorithm, where adjusting the parameters allows efficient identification of high-quality decompositions, guided by hardness estimations computed in parallel. We demonstrate the algorithm's practical efficacy on challenging CircuitSAT instances, including those encoding Logical Equivalence Checking of Boolean circuits and preimage attacks on cryptographic hash functions.

</details>


### [56] [Bonsai: A Framework for Convolutional Neural Network Acceleration Using Criterion-Based Pruning](https://arxiv.org/abs/2602.17145)
*Joseph Bingham,Sam Helmich*

Main category: cs.AI

TL;DR: Combine是一个基于准则的剪枝框架，提供快速有效的迭代剪枝，建立了准则函数比较标准语言，并提出新准则函数，在VGG模型上剪枝79%滤波器同时保持或提高精度，减少68%计算量。


<details>
  <summary>Details</summary>
Motivation: 随着CNN模型对精度和性能要求的提高，模型大小、执行时间、内存占用和功耗也随之增加。现有剪枝解决方案各自有不同的指标和方法，缺乏统一实现，难以实施和比较。

Method: 提出Combine框架，这是一个基于准则的剪枝解决方案，支持迭代剪枝。框架建立了准则函数比较的标准语言，并提出了几种新颖的准则函数。在VGG启发式模型上进行验证。

Result: 在VGG模型上，剪枝了高达79%的滤波器，同时保持或提高了模型精度，并将网络所需计算量减少了高达68%。展示了不同准则函数对不同模型的不同效果。

Conclusion: Combine框架为CNN剪枝提供了一个快速有效的解决方案，建立了准则函数比较的标准语言，证明了不同准则函数对模型性能的差异化影响，并在实际应用中取得了显著的模型压缩效果。

Abstract: As the need for more accurate and powerful Convolutional Neural Networks (CNNs) increases, so too does the size, execution time, memory footprint, and power consumption. To overcome this, solutions such as pruning have been proposed with their own metrics and methodologies, or criteria, for how weights should be removed. These solutions do not share a common implementation and are difficult to implement and compare. In this work, we introduce Combine, a criterion- based pruning solution and demonstrate that it is fast and effective framework for iterative pruning, demonstrate that criterion have differing effects on different models, create a standard language for comparing criterion functions, and propose a few novel criterion functions. We show the capacity of these criterion functions and the framework on VGG inspired models, pruning up to 79\% of filters while retaining or improving accuracy, and reducing the computations needed by the network by up to 68\%.

</details>


### [57] [JEPA-DNA: Grounding Genomic Foundation Models through Joint-Embedding Predictive Architectures](https://arxiv.org/abs/2602.17162)
*Ariel Larey,Elay Dahan,Amit Bleiweiss,Raizy Kellerman,Guy Leib,Omri Nayshool,Dan Ofer,Tal Zinger,Dan Dominissini,Gideon Rechavi,Nicole Bussola,Simon Lee,Shane O'Connell,Dung Hoang,Marissa Wirth,Alexander W. Charney,Nati Daniel,Yoli Shavit*

Main category: cs.AI

TL;DR: JEPA-DNA是一个结合联合嵌入预测架构(JEPA)与生成目标的基因组基础模型预训练框架，通过潜在空间预测提升全局功能理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基因组基础模型主要依赖掩码语言建模(MLM)或下一词预测(NTP)，这些方法擅长捕捉局部基因组语法和精细基序模式，但往往无法捕获更广泛的功能上下文，导致表征缺乏全局生物学视角。

Method: 引入JEPA-DNA框架，将联合嵌入预测架构(JEPA)与传统生成目标结合。通过潜在接地方法，将token级恢复与潜在空间预测目标耦合，通过监督CLS token来预测掩码基因组片段的高级功能嵌入，而不仅仅是关注单个核苷酸。

Result: 在多样化的基因组基准测试中，JEPA-DNA在监督和零样本任务上始终优于纯生成基线模型，提供了更稳健和生物学接地气的表征。

Conclusion: JEPA-DNA为基因组基础模型提供了一条可扩展的路径，使模型不仅能理解基因组字母表，还能理解序列背后的底层功能逻辑。

Abstract: Genomic Foundation Models (GFMs) have largely relied on Masked Language Modeling (MLM) or Next Token Prediction (NTP) to learn the language of life. While these paradigms excel at capturing local genomic syntax and fine-grained motif patterns, they often fail to capture the broader functional context, resulting in representations that lack a global biological perspective. We introduce JEPA-DNA, a novel pre-training framework that integrates the Joint-Embedding Predictive Architecture (JEPA) with traditional generative objectives. JEPA-DNA introduces latent grounding by coupling token-level recovery with a predictive objective in the latent space by supervising a CLS token. This forces the model to predict the high-level functional embeddings of masked genomic segments rather than focusing solely on individual nucleotides. JEPA-DNA extends both NTP and MLM paradigms and can be deployed either as a standalone from-scratch objective or as a continual pre-training enhancement for existing GFMs. Our evaluations across a diverse suite of genomic benchmarks demonstrate that JEPA-DNA consistently yields superior performance in supervised and zero-shot tasks compared to generative-only baselines. By providing a more robust and biologically grounded representation, JEPA-DNA offers a scalable path toward foundation models that understand not only the genomic alphabet, but also the underlying functional logic of the sequence.

</details>


### [58] [Texo: Formula Recognition within 20M Parameters](https://arxiv.org/abs/2602.17189)
*Sicheng Mao*

Main category: cs.AI

TL;DR: Texo是一个仅含2000万参数的轻量级公式识别模型，通过精心设计、蒸馏和词汇表/分词器迁移，性能媲美SOTA模型，同时模型大小减少80%/65%，支持消费级硬件实时推理和浏览器部署。


<details>
  <summary>Details</summary>
Motivation: 现有公式识别模型通常参数量大，需要高性能硬件，限制了在消费级设备和浏览器中的部署。需要开发轻量级但高性能的模型，实现实时推理和广泛可访问性。

Method: 采用最小化设计，结合注意力机制、知识蒸馏技术，以及词汇表和分词器的迁移策略，在保持性能的同时大幅减少模型参数。

Result: Texo在仅2000万参数下，性能与UniMERNet-T和PPFormulaNet-S等SOTA模型相当，模型大小分别减少80%和65%，可在消费级硬件实时推理并支持浏览器部署。

Conclusion: Texo证明了通过精心设计和优化，可以在大幅减小模型规模的同时保持高性能，为公式识别的实际应用和部署提供了高效解决方案。

Abstract: In this paper we present Texo, a minimalist yet highperformance formula recognition model that contains only 20 million parameters. By attentive design, distillation and transfer of the vocabulary and the tokenizer, Texo achieves comparable performance to state-of-the-art models such as UniMERNet-T and PPFormulaNet-S, while reducing the model size by 80% and 65%, respectively. This enables real-time inference on consumer-grade hardware and even in-browser deployment. We also developed a web application to demonstrate the model capabilities and facilitate its usage for end users.

</details>


### [59] [Continual learning and refinement of causal models through dynamic predicate invention](https://arxiv.org/abs/2602.17217)
*Enrique Crespo-Fernandez,Oliver Ray,Telmo de Menezes e Silva Filho,Peter Flach*

Main category: cs.AI

TL;DR: 提出一个在线构建符号因果世界模型的框架，通过元解释学习和谓词发明实现样本高效、可扩展且可解释的概念层次学习


<details>
  <summary>Details</summary>
Motivation: 传统世界建模方法存在样本效率低、缺乏透明性和可扩展性差的问题，需要一种能够在线学习语义化、可重用抽象概念的方法

Method: 整合连续模型学习和修复到智能体决策循环中，利用元解释学习和谓词发明技术，从观察中构建解耦的高质量概念层次结构

Result: 该方法在复杂关系动态领域具有良好可扩展性，避免了组合爆炸问题，样本效率比PPO神经网络基线高出多个数量级

Conclusion: 提出的符号因果世界模型框架能够在线学习语义化抽象概念，实现高效、可扩展且可解释的世界建模

Abstract: Efficiently navigating complex environments requires agents to internalize the underlying logic of their world, yet standard world modelling methods often struggle with sample inefficiency, lack of transparency, and poor scalability. We propose a framework for constructing symbolic causal world models entirely online by integrating continuous model learning and repair into the agent's decision loop, by leveraging the power of Meta-Interpretive Learning and predicate invention to find semantically meaningful and reusable abstractions, allowing an agent to construct a hierarchy of disentangled, high-quality concepts from its observations. We demonstrate that our lifted inference approach scales to domains with complex relational dynamics, where propositional methods suffer from combinatorial explosion, while achieving sample-efficiency orders of magnitude higher than the established PPO neural-network-based baseline.

</details>


### [60] [Decoding the Human Factor: High Fidelity Behavioral Prediction for Strategic Foresight](https://arxiv.org/abs/2602.17222)
*Ben Yellin,Ehud Ezra,Mark Foreman,Shula Grinapol*

Main category: cs.AI

TL;DR: LBM是一种行为基础模型，通过将结构化高维特质档案作为条件输入，能够比传统提示方法更准确地预测个体战略决策，且性能随特质维度增加而提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在预测人类高风险决策时存在局限性，特别是难以生成一致、个体特定的行为，且提示方法易出现身份漂移，无法充分利用详细人物描述。

Method: 提出大型行为模型(LBM)，通过基于综合心理测量电池的结构化高维特质档案进行条件化，从瞬时人物提示转向行为嵌入，在连接稳定倾向、动机状态和情境约束与观察选择的数据集上进行微调。

Result: LBM微调相比未适应的Llama-3.1-8B-Instruct骨干模型提高了行为预测准确性，在基于大五人格特质条件下与前沿基线表现相当；提示方法存在复杂性上限，而LBM性能随特质维度增加持续提升。

Conclusion: LBM为高保真行为模拟提供了可扩展方法，在战略预见、谈判分析、认知安全和决策支持等领域具有应用潜力。

Abstract: Predicting human decision-making in high-stakes environments remains a central challenge for artificial intelligence. While large language models (LLMs) demonstrate strong general reasoning, they often struggle to generate consistent, individual-specific behavior, particularly when accurate prediction depends on complex interactions between psychological traits and situational constraints. Prompting-based approaches can be brittle in this setting, exhibiting identity drift and limited ability to leverage increasingly detailed persona descriptions. To address these limitations, we introduce the Large Behavioral Model (LBM), a behavioral foundation model fine-tuned to predict individual strategic choices with high fidelity. LBM shifts from transient persona prompting to behavioral embedding by conditioning on a structured, high-dimensional trait profile derived from a comprehensive psychometric battery. Trained on a proprietary dataset linking stable dispositions, motivational states, and situational constraints to observed choices, LBM learns to map rich psychological profiles to discrete actions across diverse strategic dilemmas. In a held-out scenario evaluation, LBM fine-tuning improves behavioral prediction relative to the unadapted Llama-3.1-8B-Instruct backbone and performs comparably to frontier baselines when conditioned on Big Five traits. Moreover, we find that while prompting-based baselines exhibit a complexity ceiling, LBM continues to benefit from increasingly dense trait profiles, with performance improving as additional trait dimensions are provided. Together, these results establish LBM as a scalable approach for high-fidelity behavioral simulation, enabling applications in strategic foresight, negotiation analysis, cognitive security, and decision support.

</details>


### [61] [Mechanistic Interpretability of Cognitive Complexity in LLMs via Linear Probing using Bloom's Taxonomy](https://arxiv.org/abs/2602.17229)
*Bianca Raimondi,Maurizio Gabbrielli*

Main category: cs.AI

TL;DR: 研究发现LLM内部神经表征中，不同认知复杂度层次（基于布鲁姆分类法）在线性可分离的子空间中编码，线性分类器能达到约95%的平均准确率。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型的黑箱特性，需要超越表面性能指标的新评估框架。本研究旨在探索模型内部如何表示认知复杂性，使用布鲁姆分类法作为层次化分析视角。

Method: 通过分析不同LLM的高维激活向量，探究从基础回忆（Remember）到抽象综合（Create）的不同认知层次是否在模型的残差流中线性可分。使用线性分类器评估表征的可分离性。

Result: 线性分类器在所有布鲁姆层次上达到约95%的平均准确率，强有力地证明认知层次在模型表征的线性可访问子空间中被编码。模型在正向传播早期就解析了提示的认知难度，表征在不同层间变得越来越可分离。

Conclusion: 研究提供了证据表明LLM内部存在线性可访问的认知复杂度表征，布鲁姆分类法的层次结构在模型神经表征中具有可检测的对应关系，这为理解模型内部处理机制提供了新视角。

Abstract: The black-box nature of Large Language Models necessitates novel evaluation frameworks that transcend surface-level performance metrics. This study investigates the internal neural representations of cognitive complexity using Bloom's Taxonomy as a hierarchical lens. By analyzing high-dimensional activation vectors from different LLMs, we probe whether different cognitive levels, ranging from basic recall (Remember) to abstract synthesis (Create), are linearly separable within the model's residual streams. Our results demonstrate that linear classifiers achieve approximately 95% mean accuracy across all Bloom levels, providing strong evidence that cognitive level is encoded in a linearly accessible subspace of the model's representations. These findings provide evidence that the model resolves the cognitive difficulty of a prompt early in the forward pass, with representations becoming increasingly separable across layers.

</details>


### [62] [All Leaks Count, Some Count More: Interpretable Temporal Contamination Detection in LLM Backtesting](https://arxiv.org/abs/2602.17234)
*Zeyu Zhang,Ryan Chen,Bradly C. Stadie*

Main category: cs.AI

TL;DR: 提出Shapley-DCLR指标量化LLM预测中的时间知识泄露，并开发TimeSPEC方法通过声明验证减少泄露，提升回溯测试可靠性


<details>
  <summary>Details</summary>
Motivation: 评估LLM预测未来事件能力需要回溯测试，但LLM可能在训练中编码了截止日期后的知识，导致时间知识泄露，影响评估有效性

Method: 1) 提出声明级框架：将模型推理分解为原子声明，按时间可验证性分类；2) 使用Shapley值衡量每个声明对预测的贡献，计算Shapley-DCLR指标；3) 开发TimeSPEC方法：在生成过程中交替进行声明验证和重新生成，过滤时间污染

Result: 在350个实例（美国最高法院案件预测、NBA薪资估计、股票回报排名）上实验显示，标准提示基线存在显著泄露，TimeSPEC能降低Shapley-DCLR同时保持任务性能

Conclusion: 显式的、可解释的声明级验证优于基于提示的时间约束，TimeSPEC方法能有效减少时间知识泄露，为可靠的LLM回溯测试提供解决方案

Abstract: To evaluate whether LLMs can accurately predict future events, we need the ability to \textit{backtest} them on events that have already resolved. This requires models to reason only with information available at a specified past date. Yet LLMs may inadvertently leak post-cutoff knowledge encoded during training, undermining the validity of retrospective evaluation. We introduce a claim-level framework for detecting and quantifying this \emph{temporal knowledge leakage}. Our approach decomposes model rationales into atomic claims and categorizes them by temporal verifiability, then applies \textit{Shapley values} to measure each claim's contribution to the prediction. This yields the \textbf{Shapley}-weighted \textbf{D}ecision-\textbf{C}ritical \textbf{L}eakage \textbf{R}ate (\textbf{Shapley-DCLR}), an interpretable metric that captures what fraction of decision-driving reasoning derives from leaked information. Building on this framework, we propose \textbf{Time}-\textbf{S}upervised \textbf{P}rediction with \textbf{E}xtracted \textbf{C}laims (\textbf{TimeSPEC}), which interleaves generation with claim verification and regeneration to proactively filter temporal contamination -- producing predictions where every supporting claim can be traced to sources available before the cutoff date. Experiments on 350 instances spanning U.S. Supreme Court case prediction, NBA salary estimation, and stock return ranking reveal substantial leakage in standard prompting baselines. TimeSPEC reduces Shapley-DCLR while preserving task performance, demonstrating that explicit, interpretable claim-level verification outperforms prompt-based temporal constraints for reliable backtesting.

</details>


### [63] [Web Verbs: Typed Abstractions for Reliable Task Composition on the Agentic Web](https://arxiv.org/abs/2602.17245)
*Linxi Jiang,Rui Xi,Zhijie Liu,Shuo Chen,Zhiqiang Lin,Suman Nath*

Main category: cs.AI

TL;DR: 提出Web Verbs作为网络代理的语义层，将网站功能抽象为类型化、有文档的函数，统一API和浏览器操作，提高可靠性、效率和可验证性。


<details>
  <summary>Details</summary>
Motivation: 网络正从人类浏览环境转变为软件代理执行任务的环境。当前网络代理大多基于点击、按键等低级操作，这些操作脆弱、低效且难以验证。需要为代理式网络建立动作语义层。

Method: 提出Web Verbs概念：网络规模的类型化、语义化文档函数集，通过统一接口暴露网站功能（无论通过API还是客户端工作流实现）。这些动词作为稳定可组合单元，支持代理发现、选择和合成简洁程序。

Result: 概念验证实现和案例研究表明，相比现有代理，Web Verbs能够实现更简洁、更稳健的执行。通过预条件、后条件、策略标签和日志支持，提高了可靠性、效率和可验证性。

Conclusion: Web Verbs为网络代理提供了语义动作层，统一了API和浏览器范式，使LLM能够合成可靠、可审计的工作流。提出了标准化路线图，以实现网络规模的部署和可信性。

Abstract: The Web is evolving from a medium that humans browse to an environment where software agents act on behalf of users. Advances in large language models (LLMs) make natural language a practical interface for goal-directed tasks, yet most current web agents operate on low-level primitives such as clicks and keystrokes. These operations are brittle, inefficient, and difficult to verify. Complementing content-oriented efforts such as NLWeb's semantic layer for retrieval, we argue that the agentic web also requires a semantic layer for web actions. We propose \textbf{Web Verbs}, a web-scale set of typed, semantically documented functions that expose site capabilities through a uniform interface, whether implemented through APIs or robust client-side workflows. These verbs serve as stable and composable units that agents can discover, select, and synthesize into concise programs. This abstraction unifies API-based and browser-based paradigms, enabling LLMs to synthesize reliable and auditable workflows with explicit control and data flow. Verbs can carry preconditions, postconditions, policy tags, and logging support, which improves \textbf{reliability} by providing stable interfaces, \textbf{efficiency} by reducing dozens of steps into a few function calls, and \textbf{verifiability} through typed contracts and checkable traces. We present our vision, a proof-of-concept implementation, and representative case studies that demonstrate concise and robust execution compared to existing agents. Finally, we outline a roadmap for standardization to make verbs deployable and trustworthy at web scale.

</details>


### [64] [ArXiv-to-Model: A Practical Study of Scientific LM Training](https://arxiv.org/abs/2602.17288)
*Anuj Gupta*

Main category: cs.AI

TL;DR: 该论文提供了一个详细的案例研究，展示如何在有限计算资源（2xA100 GPU）下，从arXiv LaTeX原始源训练一个1.36B参数的科学语言模型，涵盖数学、计算机科学和理论物理领域。


<details>
  <summary>Details</summary>
Motivation: 虽然前沿大语言模型展现出强大的推理和数学能力，但如何从原始科学文献源训练领域专业化科学语言模型的实践过程仍然缺乏详细文档。本研究旨在填补这一空白，为在中等计算预算下构建领域专业化模型的研究者提供实用指导。

Method: 开发了一个端到端管道，包括元数据过滤、存档验证、LaTeX提取、文本规范化、领域感知分词和密集Transformer训练。通过24个实验运行，分析了训练稳定性、扩展行为、数据损失和基础设施瓶颈。

Result: 研究发现预处理决策显著影响可用token数量，分词影响符号稳定性，存储和I/O约束可能与计算一样成为限制因素。在数据丰富（52B预训练token）的情况下展示了稳定的训练行为。

Conclusion: 本研究提供了一个工程基础的、透明的训练小型科学语言模型的详细记录，而非提出新架构。希望这些见解能支持在中等计算预算下寻求构建领域专业化模型的研究者。

Abstract: While frontier large language models demonstrate strong reasoning and mathematical capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented. In this work, we present a detailed case study of training a 1.36B-parameter scientific language model directly from raw arXiv LaTeX sources spanning mathematics, computer science, and theoretical physics. We describe an end-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training under constrained compute (2xA100 GPUs). Through 24 experimental runs, we analyze training stability, scaling behavior, data yield losses, and infrastructure bottlenecks. Our findings highlight how preprocessing decisions significantly affect usable token volume, how tokenization impacts symbolic stability, and how storage and I/O constraints can rival compute as limiting factors. We further analyze convergence dynamics and show stable training behavior in a data-rich regime (52B pretraining tokens). Rather than proposing a novel architecture, this work provides an engineering-grounded, transparent account of training a small scientific language model from scratch. We hope these insights support researchers operating under moderate compute budgets who seek to build domain-specialized models.

</details>


### [65] [MedClarify: An information-seeking AI agent for medical diagnosis with case-specific follow-up questions](https://arxiv.org/abs/2602.17308)
*Hui Min Wong,Philip Heesen,Pascal Janetzky,Martin Bendszus,Stefan Feuerriegel*

Main category: cs.AI

TL;DR: MedClarify是一个AI代理，通过生成后续问题进行迭代推理来支持医疗诊断决策，相比单次LLM基线减少约27个百分点的诊断错误。


<details>
  <summary>Details</summary>
Motivation: 当前医疗大语言模型在诊断任务中存在局限性，无法像临床医生那样通过系统病史采集和迭代提问来推理鉴别诊断，特别是在患者信息不完整时，模型往往给出多个可能性相似的诊断，无法有效减少诊断不确定性。

Method: MedClarify首先计算类似鉴别诊断的候选诊断列表，然后主动生成旨在减少诊断不确定性的后续问题，通过选择预期信息增益最高的问题，实现有针对性的不确定性感知推理。

Result: 实验显示当前LLM在医疗推理中存在局限性，特别是在患者信息不完整时。MedClarify的信息论推理方法能生成有效的后续提问，相比标准单次LLM基线减少约27个百分点的诊断错误。

Conclusion: MedClarify通过代理式信息寻求为改进医疗LLM提供了一条路径，促进了与医疗LLM的有效对话，反映了真实世界临床推理的迭代性和不确定性本质。

Abstract: Large language models (LLMs) are increasingly used for diagnostic tasks in medicine. In clinical practice, the correct diagnosis can rarely be immediately inferred from the initial patient presentation alone. Rather, reaching a diagnosis often involves systematic history taking, during which clinicians reason over multiple potential conditions through iterative questioning to resolve uncertainty. This process requires considering differential diagnoses and actively excluding emergencies that demand immediate intervention. Yet, the ability of medical LLMs to generate informative follow-up questions and thus reason over differential diagnoses remains underexplored. Here, we introduce MedClarify, an AI agent for information-seeking that can generate follow-up questions for iterative reasoning to support diagnostic decision-making. Specifically, MedClarify computes a list of candidate diagnoses analogous to a differential diagnosis, and then proactively generates follow-up questions aimed at reducing diagnostic uncertainty. By selecting the question with the highest expected information gain, MedClarify enables targeted, uncertainty-aware reasoning to improve diagnostic performance. In our experiments, we first demonstrate the limitations of current LLMs in medical reasoning, which often yield multiple, similarly likely diagnoses, especially when patient cases are incomplete or relevant information for diagnosis is missing. We then show that our information-theoretic reasoning approach can generate effective follow-up questioning and thereby reduces diagnostic errors by ~27 percentage points (p.p.) compared to a standard single-shot LLM baseline. Altogether, MedClarify offers a path to improve medical LLMs through agentic information-seeking and to thus promote effective dialogues with medical LLMs that reflect the iterative and uncertain nature of real-world clinical reasoning.

</details>


### [66] [Dataless Weight Disentanglement in Task Arithmetic via Kronecker-Factored Approximate Curvature](https://arxiv.org/abs/2602.17385)
*Angelo Porrello,Pietro Buzzega,Felix Dangel,Thomas Sommariva,Riccardo Salami,Lorenzo Bonicelli,Simone Calderara*

Main category: cs.AI

TL;DR: 提出一种无数据的正则化方法，通过曲率矩阵近似解决任务向量组合中的表征漂移问题，无需外部任务数据，实现任务加减法的SOTA性能


<details>
  <summary>Details</summary>
Motivation: 任务算术为调整基础模型提供了模块化、可扩展的方式，但组合多个任务向量会导致跨任务干扰，引起表征漂移和性能下降。现有表征漂移正则化方法通常需要外部任务数据，这与模块化和数据可用性约束（如隐私要求）相冲突。

Method: 将表征漂移正则化框架化为曲率矩阵近似问题，采用Kronecker分解近似曲率技术，获得实用的正则化器。该方法在任务数量上具有常数复杂度，无需保留调优。

Result: 在任务加法和否定任务上实现了最先进的结果，对任务向量缩放具有鲁棒性，消除了保留调优的需求。

Conclusion: 提出的无数据正则化方法有效解决了任务向量组合中的表征漂移问题，在保持模块化和数据隐私的同时，提升了任务算术的性能和鲁棒性。

Abstract: Task Arithmetic yields a modular, scalable way to adapt foundation models. Combining multiple task vectors, however, can lead to cross-task interference, causing representation drift and degraded performance. Representation drift regularization provides a natural remedy to disentangle task vectors; however, existing approaches typically require external task data, conflicting with modularity and data availability constraints (e.g., privacy requirements). We propose a dataless approach by framing regularization against representation drift as a curvature matrix approximation problem. This allows us to leverage well-established techniques; in particular, we adopt Kronecker-Factored Approximate Curvature and obtain a practical regularizer that achieves state-of-the-art results in task addition and negation. Our method has constant complexity in the number of tasks and promotes robustness to task vector rescaling, eliminating the need for held-out tuning.

</details>


### [67] [Visual Model Checking: Graph-Based Inference of Visual Routines for Image Retrieval](https://arxiv.org/abs/2602.17386)
*Adrià Molina,Oriol Ramos Terrades,Josep Lladós*

Main category: cs.AI

TL;DR: 提出一个结合形式验证与深度学习的图像检索框架，通过图验证和神经代码生成支持开放词汇自然语言查询，确保结果可信且可验证。


<details>
  <summary>Details</summary>
Motivation: 当前基于嵌入模型的自然语言搜索在处理复杂关系、对象组合、精确约束（如身份、数量、比例）时仍不可靠。现有框架存在模糊性和近似性，缺乏透明度和可验证性。

Method: 整合形式验证到深度学习图像检索中，结合基于图的验证方法和神经代码生成。通过形式推理系统验证用户查询中的每个原子事实，标记哪些约束被满足、哪些未满足。

Result: 框架支持开放词汇自然语言查询，提供可信且可验证的结果，超越了向量表示的模糊性。不仅返回匹配结果，还能识别和标记具体约束的满足情况，提高了透明度和可问责性。

Conclusion: 通过将形式验证与深度学习结合，实现了更透明、可问责的图像检索过程，同时提升了最流行嵌入方法的检索效果，为解决复杂查询的可靠性问题提供了新途径。

Abstract: Information retrieval lies at the foundation of the modern digital industry. While natural language search has seen dramatic progress in recent years largely driven by embedding-based models and large-scale pretraining, the field still faces significant challenges. Specifically, queries that involve complex relationships, object compositions, or precise constraints such as identities, counts and proportions often remain unresolved or unreliable within current frameworks. In this paper, we propose a novel framework that integrates formal verification into deep learning-based image retrieval through a synergistic combination of graph-based verification methods and neural code generation. Our approach aims to support open-vocabulary natural language queries while producing results that are both trustworthy and verifiable. By grounding retrieval results in a system of formal reasoning, we move beyond the ambiguity and approximation that often characterize vector representations. Instead of accepting uncertainty as a given, our framework explicitly verifies each atomic truth in the user query against the retrieved content. This allows us to not only return matching results, but also to identify and mark which specific constraints are satisfied and which remain unmet, thereby offering a more transparent and accountable retrieval process while boosting the results of the most popular embedding-based approaches.

</details>


### [68] [A Contrastive Variational AutoEncoder for NSCLC Survival Prediction with Missing Modalities](https://arxiv.org/abs/2602.17402)
*Michele Zanitti,Vanja Miskovic,Francesco Trovò,Alessandra Laura Giulia Pedrocchi,Ming Shen,Yan Kyaw Tun,Arsela Prelaj,Sokol Kosta*

Main category: cs.AI

TL;DR: 提出MCVAE模型，通过模态特定变分编码器、融合瓶颈门控机制和多任务目标，解决NSCLC生存预测中多模态数据严重缺失的问题。


<details>
  <summary>Details</summary>
Motivation: NSCLC生存预测因个体预后特征差异而困难，多模态数据（全切片图像、转录组、DNA甲基化）提供互补信息，但临床数据常存在模态缺失，现有方法在严重缺失情况下缺乏鲁棒性。

Method: 提出多模态对比变分自编码器（MCVAE）：模态特定变分编码器捕获各数据源不确定性；融合瓶颈带学习门控机制归一化各模态贡献；多任务目标结合生存损失和重建损失；跨模态对比损失强制潜在空间对齐；训练时应用随机模态掩码提升鲁棒性。

Result: 在TCGA-LUAD（475例）和TCGA-LUSC（446例）数据集上评估，模型在疾病特异性生存预测方面有效，且在严重缺失情况下比两种SOTA模型更鲁棒。测试所有模态子集发现，多模态集成并非总是有益。

Conclusion: MCVAE能有效处理多模态数据严重缺失问题，提升NSCLC生存预测鲁棒性。研究还澄清了多模态集成的价值，发现并非所有情况下集成都有益。

Abstract: Predicting survival outcomes for non-small cell lung cancer (NSCLC) patients is challenging due to the different individual prognostic features. This task can benefit from the integration of whole-slide images, bulk transcriptomics, and DNA methylation, which offer complementary views of the patient's condition at diagnosis. However, real-world clinical datasets are often incomplete, with entire modalities missing for a significant fraction of patients. State-of-the-art models rely on available data to create patient-level representations or use generative models to infer missing modalities, but they lack robustness in cases of severe missingness. We propose a Multimodal Contrastive Variational AutoEncoder (MCVAE) to address this issue: modality-specific variational encoders capture the uncertainty in each data source, and a fusion bottleneck with learned gating mechanisms is introduced to normalize the contributions from present modalities. We propose a multi-task objective that combines survival loss and reconstruction loss to regularize patient representations, along with a cross-modal contrastive loss that enforces cross-modal alignment in the latent space. During training, we apply stochastic modality masking to improve the robustness to arbitrary missingness patterns. Extensive evaluations on the TCGA-LUAD (n=475) and TCGA-LUSC (n=446) datasets demonstrate the efficacy of our approach in predicting disease-specific survival (DSS) and its robustness to severe missingness scenarios compared to two state-of-the-art models. Finally, we bring some clarifications on multimodal integration by testing our model on all subsets of modalities, finding that integration is not always beneficial to the task.

</details>


### [69] [A Privacy by Design Framework for Large Language Model-Based Applications for Children](https://arxiv.org/abs/2602.17418)
*Diana Addae,Diana Rogachova,Nafiseh Kahani,Masoud Barati,Michael Christensen,Chen Zhou*

Main category: cs.AI

TL;DR: 本文提出一个基于隐私设计原则的框架，指导开发者为儿童AI应用（特别是LLM应用）提供隐私保护，结合GDPR、PIPEDA、COPPA等法规要求，并通过教育辅导案例验证框架实用性。


<details>
  <summary>Details</summary>
Motivation: 儿童越来越多地使用AI技术，但面临隐私风险。现有隐私法规要求企业实施保护措施，但在实践中执行困难。需要为AI开发者和设计师提供实用框架来主动降低儿童隐私风险。

Method: 提出基于隐私设计原则的框架，整合GDPR、PIPEDA、COPPA等隐私法规原则，映射到LLM应用的数据收集、模型训练、运营监控和持续验证等阶段。框架还包括基于UNCRC、AADC和学术研究的儿童设计指南，并通过13岁以下儿童LLM教育辅导案例研究验证。

Result: 框架展示了如何通过技术和组织控制以及适龄设计决策，在整个LLM生命周期中支持开发符合法律要求的儿童AI应用。案例研究表明框架能够实际应用于LLM教育辅导系统，提供隐私保护并满足法规要求。

Conclusion: 通过整合隐私法规原则、学术文献中的操作控制和儿童设计指南，提出的隐私设计框架能够有效指导开发者为儿童AI应用提供隐私保护，确保法律合规性，特别适用于LLM应用场景。

Abstract: Children are increasingly using technologies powered by Artificial Intelligence (AI). However, there are growing concerns about privacy risks, particularly for children. Although existing privacy regulations require companies and organizations to implement protections, doing so can be challenging in practice. To address this challenge, this article proposes a framework based on Privacy-by-Design (PbD), which guides designers and developers to take on a proactive and risk-averse approach to technology design. Our framework includes principles from several privacy regulations, such as the General Data Protection Regulation (GDPR) from the European Union, the Personal Information Protection and Electronic Documents Act (PIPEDA) from Canada, and the Children's Online Privacy Protection Act (COPPA) from the United States. We map these principles to various stages of applications that use Large Language Models (LLMs), including data collection, model training, operational monitoring, and ongoing validation. For each stage, we discuss the operational controls found in the recent academic literature to help AI service providers and developers reduce privacy risks while meeting legal standards. In addition, the framework includes design guidelines for children, drawing from the United Nations Convention on the Rights of the Child (UNCRC), the UK's Age-Appropriate Design Code (AADC), and recent academic research. To demonstrate how this framework can be applied in practice, we present a case study of an LLM-based educational tutor for children under 13. Through our analysis and the case study, we show that by using data protection strategies such as technical and organizational controls and making age-appropriate design decisions throughout the LLM life cycle, we can support the development of AI applications for children that provide privacy protections and comply with legal requirements.

</details>


### [70] [WarpRec: Unifying Academic Rigor and Industrial Scale for Responsible, Reproducible, and Efficient Recommendation](https://arxiv.org/abs/2602.17442)
*Marco Avolio,Potito Aghilar,Sabino Roccotelli,Vito Walter Anelli,Chiara Mallamaci,Vincenzo Paparella,Marco Valentini,Alejandro Bellogín,Michelantonio Trizio,Joseph Trotta,Antonio Ferrara,Tommaso Di Noia*

Main category: cs.AI

TL;DR: WarpRec是一个高性能推荐系统框架，通过后端无关架构消除学术实验与工业部署间的鸿沟，包含50+算法、40指标和19种策略，支持从本地到分布式无缝过渡，集成能耗追踪，并面向Agentic AI演进。


<details>
  <summary>Details</summary>
Motivation: 当前推荐系统生态系统分裂，研究者需要在易于内存实验和需要昂贵重写以适应分布式工业引擎之间做出选择，阻碍了创新。

Method: 提出WarpRec框架，采用新颖的后端无关架构，包含50+最先进算法、40个评估指标和19种过滤与分割策略，支持从本地执行到分布式训练的无缝过渡，并集成CodeCarbon进行实时能耗追踪。

Result: WarpRec成功弥合了学术界与工业界之间的差距，展示了可扩展性不必以科学完整性或可持续性为代价，能够作为下一代可持续、支持Agent的推荐系统的架构基础。

Conclusion: WarpRec不仅连接了学术与工业，还能作为下一代可持续、支持Agentic AI的推荐系统的架构支柱，推动推荐系统从静态排名引擎向生成式AI生态系统中的交互工具演进。

Abstract: Innovation in Recommender Systems is currently impeded by a fractured ecosystem, where researchers must choose between the ease of in-memory experimentation and the costly, complex rewriting required for distributed industrial engines. To bridge this gap, we present WarpRec, a high-performance framework that eliminates this trade-off through a novel, backend-agnostic architecture. It includes 50+ state-of-the-art algorithms, 40 metrics, and 19 filtering and splitting strategies that seamlessly transition from local execution to distributed training and optimization. The framework enforces ecological responsibility by integrating CodeCarbon for real-time energy tracking, showing that scalability need not come at the cost of scientific integrity or sustainability. Furthermore, WarpRec anticipates the shift toward Agentic AI, leading Recommender Systems to evolve from static ranking engines into interactive tools within the Generative AI ecosystem. In summary, WarpRec not only bridges the gap between academia and industry but also can serve as the architectural backbone for the next generation of sustainable, agent-ready Recommender Systems. Code is available at https://github.com/sisinflab/warprec/

</details>


### [71] [Pareto Optimal Benchmarking of AI Models on ARM Cortex Processors for Sustainable Embedded Systems](https://arxiv.org/abs/2602.17508)
*Pranay Jain,Maximilian Kasper,Göran Köber,Axel Plinge,Dominik Seuß*

Main category: cs.AI

TL;DR: 该研究提出了一个针对ARM Cortex处理器（M0+、M4、M7）的AI模型优化基准测试框架，通过自动化测试平台系统评估能耗、精度和资源利用率，发现FLOPs与推理时间呈近线性关系，并利用帕累托分析平衡能耗与精度权衡。


<details>
  <summary>Details</summary>
Motivation: 嵌入式系统中AI模型部署面临能耗、精度和资源利用率的平衡挑战，需要系统化的评估框架来指导开发者选择最优的处理器与AI模型组合，以实现高性能且可持续的AI应用。

Method: 设计自动化测试平台，系统评估ARM Cortex处理器（M0+、M4、M7）上AI模型的性能指标；分析FLOPs与推理时间的相关性；采用帕累托分析平衡能耗与模型精度的权衡。

Result: 发现FLOPs与推理时间存在近线性相关关系，可作为计算需求估计的可靠指标；M7处理器适合短推理周期任务，M4处理器在长推理任务中能效更高，M0+处理器适用于简单AI任务；通过帕累托分析实现了能耗与精度的有效平衡。

Conclusion: 该基准测试框架为开发者提供了设计高能效AI系统的实用指导，通过系统化评估处理器与模型组合，能够在满足性能要求的同时确保嵌入式AI应用的可持续性，不同处理器针对不同应用场景各有优势。

Abstract: This work presents a practical benchmarking framework for optimizing artificial intelligence (AI) models on ARM Cortex processors (M0+, M4, M7), focusing on energy efficiency, accuracy, and resource utilization in embedded systems. Through the design of an automated test bench, we provide a systematic approach to evaluate across key performance indicators (KPIs) and identify optimal combinations of processor and AI model. The research highlights a nearlinear correlation between floating-point operations (FLOPs) and inference time, offering a reliable metric for estimating computational demands. Using Pareto analysis, we demonstrate how to balance trade-offs between energy consumption and model accuracy, ensuring that AI applications meet performance requirements without compromising sustainability. Key findings indicate that the M7 processor is ideal for short inference cycles, while the M4 processor offers better energy efficiency for longer inference tasks. The M0+ processor, while less efficient for complex AI models, remains suitable for simpler tasks. This work provides insights for developers, guiding them to design energy-efficient AI systems that deliver high performance in realworld applications.

</details>


### [72] [Enhancing Large Language Models (LLMs) for Telecom using Dynamic Knowledge Graphs and Explainable Retrieval-Augmented Generation](https://arxiv.org/abs/2602.17529)
*Dun Yuan,Hao Zhou,Xue Liu,Hao Chen,Yan Xin,Jianzhong,Zhang*

Main category: cs.AI

TL;DR: KG-RAG框架通过将知识图谱与检索增强生成相结合，提升大语言模型在电信领域的准确性和可靠性，减少幻觉，实现平均14.3%的精度提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在电信领域应用面临挑战：领域复杂性高、标准不断演进、专业术语多，导致通用LLMs在该领域产生幻觉、输出不可靠，实用性受限。

Method: 提出KG-RAG框架，将知识图谱（结构化电信标准和文档知识）与检索增强生成（动态检索相关事实）相结合，为LLMs提供领域知识基础。

Result: 在基准数据集上，KG-RAG显著优于纯LLM和标准RAG基线，平均精度分别提升21.6%和14.3%，在复杂电信场景中产生更准确、可靠、可解释的输出。

Conclusion: KG-RAG框架有效解决了LLMs在电信领域的应用挑战，通过结构化知识图谱和动态检索的结合，显著提升了模型的事实准确性、可靠性和可解释性。

Abstract: Large language models (LLMs) have shown strong potential across a variety of tasks, but their application in the telecom field remains challenging due to domain complexity, evolving standards, and specialized terminology. Therefore, general-domain LLMs may struggle to provide accurate and reliable outputs in this context, leading to increased hallucinations and reduced utility in telecom operations.To address these limitations, this work introduces KG-RAG-a novel framework that integrates knowledge graphs (KGs) with retrieval-augmented generation (RAG) to enhance LLMs for telecom-specific tasks. In particular, the KG provides a structured representation of domain knowledge derived from telecom standards and technical documents, while RAG enables dynamic retrieval of relevant facts to ground the model's outputs. Such a combination improves factual accuracy, reduces hallucination, and ensures compliance with telecom specifications.Experimental results across benchmark datasets demonstrate that KG-RAG outperforms both LLM-only and standard RAG baselines, e.g., KG-RAG achieves an average accuracy improvement of 14.3% over RAG and 21.6% over LLM-only models. These results highlight KG-RAG's effectiveness in producing accurate, reliable, and explainable outputs in complex telecom scenarios.

</details>


### [73] [Evaluating Chain-of-Thought Reasoning through Reusability and Verifiability](https://arxiv.org/abs/2602.17544)
*Shashank Aggarwal,Ram Vikas Mishra,Amit Awekar*

Main category: cs.AI

TL;DR: 该论文提出了评估思维链（CoT）质量的两个新指标：可重用性和可验证性，揭示了传统准确率评估的局限性，并发现专业推理模型的CoT并不比通用LLM的CoT更具优势。


<details>
  <summary>Details</summary>
Motivation: 在多智能体IR管道中，LLM智能体通过思维链交换中间推理。当前CoT评估仅关注目标任务准确率，无法评估推理过程本身的质量或效用，存在评估盲点。

Method: 采用Thinker-Executor框架将CoT生成与执行解耦，提出可重用性（Executor重用Thinker的CoT的容易程度）和可验证性（Executor使用CoT匹配Thinker答案的频率）两个新指标。评估了4个Thinker模型与10个Executor模型委员会在5个基准测试上的表现。

Result: 可重用性和可验证性与标准准确率不相关，暴露了当前基于准确率的推理能力排行榜的盲点。令人惊讶的是，专业推理模型的CoT并不比Llama和Gemma等通用LLM的CoT更具可重用性或可验证性。

Conclusion: 需要超越准确率的评估指标来全面评估推理能力，专业推理模型在CoT质量方面不一定优于通用LLM，这对多智能体系统中的模型选择具有重要启示。

Abstract: In multi-agent IR pipelines for tasks such as search and ranking, LLM-based agents exchange intermediate reasoning in terms of Chain-of-Thought (CoT) with each other. Current CoT evaluation narrowly focuses on target task accuracy. However, this metric fails to assess the quality or utility of the reasoning process itself. To address this limitation, we introduce two novel measures: reusability and verifiability. We decouple CoT generation from execution using a Thinker-Executor framework. Reusability measures how easily an Executor can reuse the Thinker's CoT. Verifiability measures how frequently an Executor can match the Thinker's answer using the CoT. We evaluated four Thinker models against a committee of ten Executor models across five benchmarks. Our results reveal that reusability and verifiability do not correlate with standard accuracy, exposing a blind spot in current accuracy-based leaderboards for reasoning capability. Surprisingly, we find that CoTs from specialized reasoning models are not consistently more reusable or verifiable than those from general-purpose LLMs like Llama and Gemma.

</details>


### [74] [KLong: Training LLM Agent for Extremely Long-horizon Tasks](https://arxiv.org/abs/2602.17547)
*Yue Liu,Zhiyuan Hu,Flood Sung,Jiaheng Zhang,Bryan Hooi*

Main category: cs.AI

TL;DR: KLong是一个开源LLM智能体，通过轨迹分割SFT和渐进式RL训练解决超长视野任务，在PaperBench等基准上超越现有模型


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM智能体在处理超长视野任务时的能力不足问题，需要开发能够处理复杂、多步骤长期任务的智能体

Method: 1. 通过轨迹分割SFT冷启动：保留早期上下文，渐进截断后期上下文，保持子轨迹重叠
2. Research-Factory自动管道：收集研究论文并构建评估标准，从Claude 4.5 Sonnet蒸馏数千条长视野轨迹
3. 渐进式RL训练：分多个阶段安排训练，逐步延长超时时间

Result: KLong (106B)在PaperBench上超越Kimi K2 Thinking (1T) 11.28%，性能改进推广到SWE-bench Verified和MLE-bench等其他编码基准

Conclusion: 提出的轨迹分割SFT和渐进式RL训练方法有效提升了LLM智能体解决超长视野任务的能力，KLong展示了优越的泛化性能

Abstract: This paper introduces KLong, an open-source LLM agent trained to solve extremely long-horizon tasks. The principle is to first cold-start the model via trajectory-splitting SFT, then scale it via progressive RL training. Specifically, we first activate basic agentic abilities of a base model with a comprehensive SFT recipe. Then, we introduce Research-Factory, an automated pipeline that generates high-quality training data by collecting research papers and constructing evaluation rubrics. Using this pipeline, we build thousands of long-horizon trajectories distilled from Claude 4.5 Sonnet (Thinking). To train with these extremely long trajectories, we propose a new trajectory-splitting SFT, which preserves early context, progressively truncates later context, and maintains overlap between sub-trajectories. In addition, to further improve long-horizon task-solving capability, we propose a novel progressive RL, which schedules training into multiple stages with progressively extended timeouts. Experiments demonstrate the superiority and generalization of KLong, as shown in Figure 1. Notably, our proposed KLong (106B) surpasses Kimi K2 Thinking (1T) by 11.28% on PaperBench, and the performance improvement generalizes to other coding benchmarks like SWE-bench Verified and MLE-bench.

</details>


### [75] [ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment](https://arxiv.org/abs/2602.17560)
*Hongjue Zhao,Haosen Sun,Jiangtao Kong,Xiaochang Li,Qineng Wang,Liwei Jiang,Qi Zhu,Tarek Abdelzaher,Yejin Choi,Manling Li,Huajie Shao*

Main category: cs.AI

TL;DR: 提出了基于常微分方程的激活导向理论框架ODESteer，通过屏障函数和多步自适应导向改进LLM对齐效果


<details>
  <summary>Details</summary>
Motivation: 当前激活导向方法存在两个关键局限：缺乏统一的理论框架指导导向方向设计，以及过度依赖单步导向无法捕捉激活分布的复杂模式

Method: 提出基于常微分方程的激活导向理论框架，将传统激活加法解释为ODE的一阶近似，通过屏障函数定义导向方向，实现多步自适应导向

Result: ODESteer在多个LLM对齐基准测试中取得显著改进：TruthfulQA提升5.7%，UltraFeedback提升2.5%，RealToxicityPrompts提升2.4%

Conclusion: 通过ODE统一激活导向的理论基础，提出的ODESteer方法在理论和实证上都取得了进展，为LLM对齐提供了新的原则性视角

Abstract: Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \textit{(i)} the lack of a unified theoretical framework for guiding the design of steering directions, and \textit{(ii)} an over-reliance on \textit{one-step steering} that fail to capture complex patterns of activation distributions. In this work, we propose a unified ordinary differential equations (ODEs)-based \textit{theoretical} framework for activation steering in LLM alignment. We show that conventional activation addition can be interpreted as a first-order approximation to the solution of an ODE. Based on this ODE perspective, identifying a steering direction becomes equivalent to designing a \textit{barrier function} from control theory. Derived from this framework, we introduce ODESteer, a kind of ODE-based steering guided by barrier functions, which shows \textit{empirical} advancement in LLM alignment. ODESteer identifies steering directions by defining the barrier function as the log-density ratio between positive and negative activations, and employs it to construct an ODE for \textit{multi-step and adaptive} steering. Compared to state-of-the-art activation steering methods, ODESteer achieves consistent empirical improvements on diverse LLM alignment benchmarks, a notable $5.7\%$ improvement over TruthfulQA, $2.5\%$ over UltraFeedback, and $2.4\%$ over RealToxicityPrompts. Our work establishes a principled new view of activation steering in LLM alignment by unifying its theoretical foundations via ODEs, and validating it empirically through the proposed ODESteer method.

</details>


### [76] [A Hybrid Federated Learning Based Ensemble Approach for Lung Disease Diagnosis Leveraging Fusion of SWIN Transformer and CNN](https://arxiv.org/abs/2602.17566)
*Asif Hasan Chowdhury,Md. Fahim Islam,M Ragib Anjum Riad,Faiyaz Bin Hashem,Md Tanzim Reza,Md. Golam Rabiul Alam*

Main category: cs.AI

TL;DR: 提出一种结合SWIN Transformer与CNN的混合联邦学习模型，用于基于X射线图像的COVID-19和肺炎诊断，确保医疗数据安全与隐私保护。


<details>
  <summary>Details</summary>
Motivation: 医疗数据分散在不同医院且涉及隐私，传统集中式AI模型面临数据共享困难和安全风险。需要一种既能利用多源医疗数据又能保护隐私的诊断系统。

Method: 采用联邦学习框架，结合SWIN Transformer和CNN模型（DenseNet201、Inception V3、VGG 19）构建混合模型，使用TensorFlow和Keras实现，在分布式医疗数据上进行训练。

Result: 模型能够准确诊断COVID-19和肺炎，通过联邦学习确保数据隐私安全，实现实时持续学习，为医生提供可靠的辅助诊断工具。

Conclusion: 联邦学习与混合AI模型的结合为医疗诊断提供了安全、高效、准确的解决方案，有助于应对全球疫情挑战，推动医疗AI的分布式应用。

Abstract: The significant advancements in computational power cre- ate a vast opportunity for using Artificial Intelligence in different ap- plications of healthcare and medical science. A Hybrid FL-Enabled Ensemble Approach For Lung Disease Diagnosis Leveraging a Combination of SWIN Transformer and CNN is the combination of cutting-edge technology of AI and Federated Learning. Since, medi- cal specialists and hospitals will have shared data space, based on that data, with the help of Artificial Intelligence and integration of federated learning, we can introduce a secure and distributed system for medical data processing and create an efficient and reliable system. The proposed hybrid model enables the detection of COVID-19 and Pneumonia based on x-ray reports. We will use advanced and the latest available tech- nology offered by Tensorflow and Keras along with Microsoft-developed Vision Transformer, that can help to fight against the pandemic that the world has to fight together as a united. We focused on using the latest available CNN models (DenseNet201, Inception V3, VGG 19) and the Transformer model SWIN Transformer in order to prepare our hy- brid model that can provide a reliable solution as a helping hand for the physician in the medical field. In this research, we will discuss how the Federated learning-based Hybrid AI model can improve the accuracy of disease diagnosis and severity prediction of a patient using the real-time continual learning approach and how the integration of federated learn- ing can ensure hybrid model security and keep the authenticity of the information.

</details>


### [77] [AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games](https://arxiv.org/abs/2602.17594)
*Lance Ying,Ryan Truong,Prafull Sharma,Kaiya Ivy Zhao,Nathan Cloos,Kelsey R. Allen,Thomas L. Griffiths,Katherine M. Collins,José Hernández-Orallo,Phillip Isola,Samuel J. Gershman,Joshua B. Tenenbaum*

Main category: cs.AI

TL;DR: 提出AI GameStore平台，通过评估AI在所有人类游戏中的表现来衡量类人通用智能，相比传统基准更具挑战性和全面性。


<details>
  <summary>Details</summary>
Motivation: 当前AI基准测试通常只评估狭窄能力，且容易饱和。需要更全面的方法来评估类人通用智能。

Method: 提出"人类游戏多宇宙"概念，构建AI GameStore平台，使用LLM和人类参与合成代表性人类游戏，基于App Store和Steam热门游戏生成100个游戏环境。

Result: 测试7个前沿视觉语言模型，最佳模型在多数游戏中得分低于人类平均水平的10%，在需要世界模型学习、记忆和规划的游戏上表现尤其差。

Conclusion: AI GameStore是衡量和推动机器实现类人通用智能的实用方法，需要进一步扩展和完善该平台。

Abstract: Rigorously evaluating machine intelligence against the broad spectrum of human general intelligence has become increasingly important and challenging in this era of rapid technological advance. Conventional AI benchmarks typically assess only narrow capabilities in a limited range of human activity. Most are also static, quickly saturating as developers explicitly or implicitly optimize for them. We propose that a more promising way to evaluate human-like general intelligence in AI systems is through a particularly strong form of general game playing: studying how and how well they play and learn to play \textbf{all conceivable human games}, in comparison to human players with the same level of experience, time, or other resources. We define a "human game" to be a game designed by humans for humans, and argue for the evaluative suitability of this space of all such games people can imagine and enjoy -- the "Multiverse of Human Games". Taking a first step towards this vision, we introduce the AI GameStore, a scalable and open-ended platform that uses LLMs with humans-in-the-loop to synthesize new representative human games, by automatically sourcing and adapting standardized and containerized variants of game environments from popular human digital gaming platforms. As a proof of concept, we generated 100 such games based on the top charts of Apple App Store and Steam, and evaluated seven frontier vision-language models (VLMs) on short episodes of play. The best models achieved less than 10\% of the human average score on the majority of the games, and especially struggled with games that challenge world-model learning, memory and planning. We conclude with a set of next steps for building out the AI GameStore as a practical way to measure and drive progress toward human-like general intelligence in machines.

</details>


### [78] [MolHIT: Advancing Molecular-Graph Generation with Hierarchical Discrete Diffusion Models](https://arxiv.org/abs/2602.17602)
*Hojung Jung,Rodrigo Hormazabal,Jaehyeong Jo,Youngrok Park,Kyunggeun Roh,Se-Young Yun,Sehui Han,Dae-Woong Jeong*

Main category: cs.AI

TL;DR: MolHIT是一个基于分层离散扩散模型的分子图生成框架，首次在图扩散中实现近乎完美的化学有效性，在MOSES数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有的图扩散模型存在化学有效性低、难以满足期望属性的问题，相比1D建模表现不佳，需要克服这些长期存在的性能限制。

Method: 基于分层离散扩散模型，将离散扩散推广到编码化学先验的额外类别，并采用解耦原子编码根据化学角色分割原子类型。

Result: 在MOSES数据集上首次实现图扩散中近乎完美的化学有效性，达到新的SOTA性能，在多个指标上超越强1D基线，在下游任务中表现优异。

Conclusion: MolHIT克服了现有图扩散模型的性能限制，为AI驱动的药物发现和材料科学提供了强大的分子生成框架。

Abstract: Molecular generation with diffusion models has emerged as a promising direction for AI-driven drug discovery and materials science. While graph diffusion models have been widely adopted due to the discrete nature of 2D molecular graphs, existing models suffer from low chemical validity and struggle to meet the desired properties compared to 1D modeling. In this work, we introduce MolHIT, a powerful molecular graph generation framework that overcomes long-standing performance limitations in existing methods. MolHIT is based on the Hierarchical Discrete Diffusion Model, which generalizes discrete diffusion to additional categories that encode chemical priors, and decoupled atom encoding that splits the atom types according to their chemical roles. Overall, MolHIT achieves new state-of-the-art performance on the MOSES dataset with near-perfect validity for the first time in graph diffusion, surpassing strong 1D baselines across multiple metrics. We further demonstrate strong performance in downstream tasks, including multi-property guided generation and scaffold extension.

</details>


### [79] [AutoNumerics: An Autonomous, PDE-Agnostic Multi-Agent Pipeline for Scientific Computing](https://arxiv.org/abs/2602.17607)
*Jianda Du,Youran Sun,Haizhao Yang*

Main category: cs.AI

TL;DR: AutoNumerics是一个多智能体框架，能够从自然语言描述自主设计、实现、调试和验证通用PDE的数值求解器，生成基于经典数值分析的透明求解器而非黑盒神经网络。


<details>
  <summary>Details</summary>
Motivation: 传统PDE数值求解器设计需要大量数学专业知识和手动调优，而现有的神经网络方法计算成本高且可解释性有限，需要一种更易访问的自动化PDE求解范式。

Method: 采用多智能体框架，结合粗到细执行策略和基于残差的自验证机制，从自然语言描述生成基于经典数值分析的透明求解器。

Result: 在24个经典和现实世界PDE问题上，AutoNumerics相比现有神经和LLM基线达到竞争性或更优的精度，并能根据PDE结构特性正确选择数值方案。

Conclusion: AutoNumerics展示了作为自动化PDE求解可访问范式的可行性，能够生成透明、基于经典数值分析的求解器，平衡了灵活性与可解释性。

Abstract: PDEs are central to scientific and engineering modeling, yet designing accurate numerical solvers typically requires substantial mathematical expertise and manual tuning. Recent neural network-based approaches improve flexibility but often demand high computational cost and suffer from limited interpretability. We introduce \texttt{AutoNumerics}, a multi-agent framework that autonomously designs, implements, debugs, and verifies numerical solvers for general PDEs directly from natural language descriptions. Unlike black-box neural solvers, our framework generates transparent solvers grounded in classical numerical analysis. We introduce a coarse-to-fine execution strategy and a residual-based self-verification mechanism. Experiments on 24 canonical and real-world PDE problems demonstrate that \texttt{AutoNumerics} achieves competitive or superior accuracy compared to existing neural and LLM-based baselines, and correctly selects numerical schemes based on PDE structural properties, suggesting its viability as an accessible paradigm for automated PDE solving.

</details>


### [80] [CLEF HIPE-2026: Evaluating Accurate and Efficient Person-Place Relation Extraction from Multilingual Historical Texts](https://arxiv.org/abs/2602.17663)
*Juri Opitz,Corina Raclé,Emanuela Boros,Andrianos Michail,Matteo Romanello,Maud Ehrmann,Simon Clematide*

Main category: cs.AI

TL;DR: HIPE-2026是CLEF评估实验室，专注于从多语言历史文本中提取人物-地点关系，扩展了之前的工作，增加了语义关系提取任务，要求系统识别两种类型的关系（at和isAt），并引入三重评估框架。


<details>
  <summary>Details</summary>
Motivation: 历史文本中的人物-地点关系提取对于数字人文学科至关重要，但现有方法在处理多语言、跨时期、噪声文本方面存在挑战。HIPE-2026旨在支持知识图谱构建、历史传记重建和空间分析等下游应用。

Method: 采用评估实验室形式，要求系统从多语言历史文本中提取两种类型的人物-地点关系：at（人物是否曾到过该地点）和isAt（人物在出版时间是否位于该地点）。引入三重评估框架：准确性、计算效率和领域泛化能力。

Result: 论文描述了HIPE-2026评估实验室的设计框架，包括任务定义、评估指标和数据集特征。该实验室延续了HIPE-2020和HIPE-2022的工作，将重点从命名实体识别扩展到语义关系提取。

Conclusion: HIPE-2026通过引入人物-地点关系提取任务和三重评估框架，推动了历史文本处理技术的发展，为数字人文学科提供了重要的工具支持，有助于大规模历史数据处理和知识发现。

Abstract: HIPE-2026 is a CLEF evaluation lab dedicated to person-place relation extraction from noisy, multilingual historical texts. Building on the HIPE-2020 and HIPE-2022 campaigns, it extends the series toward semantic relation extraction by targeting the task of identifying person--place associations in multiple languages and time periods. Systems are asked to classify relations of two types - $at$ ("Has the person ever been at this place?") and $isAt$ ("Is the person located at this place around publication time?") - requiring reasoning over temporal and geographical cues. The lab introduces a three-fold evaluation profile that jointly assesses accuracy, computational efficiency, and domain generalization. By linking relation extraction to large-scale historical data processing, HIPE-2026 aims to support downstream applications in knowledge-graph construction, historical biography reconstruction, and spatial analysis in digital humanities.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [81] [Scaling Reproducibility: An AI-Assisted Workflow for Large-Scale Reanalysis](https://arxiv.org/abs/2602.16733)
*Yiqing Xu,Leo Yang Yang*

Main category: econ.EM

TL;DR: 开发了一个AI代理工作流来解决实证研究的可重复性执行瓶颈，通过分离科学推理与计算执行，在92项IV研究中实现了87%的端到端成功率


<details>
  <summary>Details</summary>
Motivation: 可重复性是研究可信度的核心，但大规模重新分析实证数据成本高昂，因为复制包在结构、软件环境和文档方面差异很大。需要解决这一执行瓶颈同时保持科学严谨性。

Method: 开发了一个代理AI工作流，将科学推理与计算执行分离：研究人员设计固定的诊断模板，工作流使用预先指定、版本控制的代码自动化获取、协调和执行复制材料。结构化知识层记录已解决的失败模式，使系统能够适应异构研究，同时保持每个管道版本的透明和稳定。

Result: 在92项工具变量研究中评估该工作流，包括67项具有手动验证可重复2SLS估计的研究和25项新发表的IV研究。系统整体实现了87%的端到端成功率。在数据和代码可访问的条件下，论文级别和规范级别的可重复性均为100%。

Conclusion: 该框架显著降低了执行既定实证协议的成本，可适用于分析模板和透明度规范已确立的实证研究环境。通过分离推理与执行、记录失败模式，实现了跨异构研究的适应性，同时保持透明度和稳定性。

Abstract: Reproducibility is central to research credibility, yet large-scale reanalysis of empricial data remains costly because replication packages vary widely in structure, software environment, and documentation. We develop and evaluate an agentic AI workflow that addresses this execution bottleneck while preserving scientific rigor. The system separates scientific reasoning from computational execution: researchers design fixed diagnostic templates, and the workflow automates the acquisition, harmonization, and execution of replication materials using pre-specified, version-controlled code. A structured knowledge layer records resolved failure patterns, enabling adaptation across heterogeneous studies while keeping each pipeline version transparent and stable. We evaluate this workflow on 92 instrumental variable (IV) studies, including 67 with manually verified reproducible 2SLS estimates and 25 newly published IV studies under identical criteria. For each paper, we analyze up to three two-stage least squares (2SLS) specifications, totaling 215. Across the 92 papers, the system achieves 87% end-to-end success overall. Conditional on accessible data and code, reproducibility is 100% at both the paper and specification levels. The framework substantially lowers the cost of executing established empirical protocols and can be adapted in empirical settings where analytic templates and norms of transparency are well established.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [82] [Simplify to Amplify: Achieving Information-Theoretic Bounds with Fewer Steps in Spectral Community Detection](https://arxiv.org/abs/2602.17104)
*Sie Hendrata Dharmawan,Peter Chin*

Main category: cs.SI

TL;DR: 提出了一种简化的谱算法用于二社区随机块模型中的社区检测，通过消除非必要的预处理步骤，直接利用邻接矩阵的谱特性，在保持计算效率的同时实现了接近信息论极限的改进误差界。


<details>
  <summary>Details</summary>
Motivation: 现有谱社区检测算法通常包含复杂的预处理步骤，增加了计算复杂度。作者旨在探索是否可以通过简化算法结构，在保持甚至提升性能的同时减少计算负担。

Method: 提出了一种简化的谱算法，直接利用邻接矩阵的谱特性，特别关注第二特征值的特定特征，消除了传统方法中的非必要预处理步骤，降低了算法复杂度。

Result: 算法实现了改进的误差界，接近信息论极限，优于现有方法。理论分析表明误差界比文献中报道的更紧，实验验证证实了理论发现和实际有效性。

Conclusion: 算法简化而非增加复杂度可以同时实现计算效率和性能提升，为谱社区检测提供了新的设计思路。

Abstract: We propose a streamlined spectral algorithm for community detection in the two-community stochastic block model (SBM) under constant edge density assumptions. By reducing algorithmic complexity through the elimination of non-essential preprocessing steps, our method directly leverages the spectral properties of the adjacency matrix. We demonstrate that our algorithm exploits specific characteristics of the second eigenvalue to achieve improved error bounds that approach information-theoretic limits, representing a significant improvement over existing methods. Theoretical analysis establishes that our error rates are tighter than previously reported bounds in the literature. Comprehensive experimental validation confirms our theoretical findings and demonstrates the practical effectiveness of the simplified approach. Our results suggest that algorithmic simplification, rather than increasing complexity, can lead to both computational efficiency and enhanced performance in spectral community detection.

</details>


### [83] [NTLRAG: Narrative Topic Labels derived with Retrieval Augmented Generation](https://arxiv.org/abs/2602.17216)
*Lisa Grobelscheg,Ema Kahr,Mark Strembeck*

Main category: cs.SI

TL;DR: NTLRAG框架使用检索增强生成技术，为话题模型生成语义精确、人类可解释的叙事话题标签，替代传统关键词列表，显著提升可解释性和可用性。


<details>
  <summary>Details</summary>
Motivation: 传统话题模型通常输出无结构的关键词列表，难以准确捕捉核心话题，特别是对于社交媒体短文本分析，用户需要更直观、语义精确的话题标签。

Method: 提出NTLRAG框架，结合检索增强生成技术，采用多种检索策略和思维链元素，可兼容任何标准话题模型，生成、验证和精炼叙事话题标签。

Result: 在包含670万条社交媒体消息、270万用户的数据集上评估，用户研究显示16位评估者认为叙事话题标签在可解释性和可用性上显著优于传统关键词列表。

Conclusion: NTLRAG提供了一种可扩展的框架，能生成高质量叙事话题标签，有效解决话题模型输出解释性问题，提升社交媒体数据分析的实用性。

Abstract: Topic modeling has evolved as an important means to identify evident or hidden topics within large collections of text documents. Topic modeling approaches are often used for analyzing and making sense of social media discussions consisting of millions of short text messages. However, assigning meaningful topic labels to document clusters remains challenging, as users are commonly presented with unstructured keyword lists that may not accurately capture the respective core topic. In this paper, we introduce Narrative Topic Labels derived with Retrieval Augmented Generation (NTLRAG), a scalable and extensible framework that generates semantically precise and human-interpretable narrative topic labels. Our narrative topic labels provide a context-rich, intuitive concept to describe topic model output. In particular, NTLRAG uses retrieval augmented generation (RAG) techniques and considers multiple retrieval strategies as well as chain-of-thought elements to provide high-quality output. NTLRAG can be combined with any standard topic model to generate, validate, and refine narratives which then serve as narrative topic labels. We evaluated NTLRAG with a user study and three real-world datasets consisting of more than 6.7 million social media messages that have been sent by more than 2.7 million users. The user study involved 16 human evaluators who found that our narrative topic labels offer superior interpretability and usability as compared to traditional keyword lists. An implementation of NTLRAG is publicly available for download.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [84] [The Impact of Formations on Football Matches Using Double Machine Learning. Is it worth parking the bus?](https://arxiv.org/abs/2602.16830)
*Genís Ruiz-Menárguez,Llorenç Badiella*

Main category: stat.AP

TL;DR: 使用双重机器学习框架分析足球阵型对比赛结果的因果影响，发现进攻阵型在控球和角球上有优势但对进球影响有限，防守阵型并未增加获胜概率。


<details>
  <summary>Details</summary>
Motivation: 解决足球教练面临的核心战术困境：应该采用防守策略（"摆大巴"）还是进攻策略，为教练提供基于数据的战术决策工具。

Method: 采用双重机器学习框架，处理超过22,000场欧洲顶级联赛比赛数据，将阵型分为六类，通过创新的基于矩阵的残差化过程处理分类处理变量（阵型组合）。

Result: 进攻阵型（如4-3-3和4-2-3-1）在控球和角球上有统计优势但对进球影响有限；防守阵型并未增加获胜概率；红牌不受阵型选择影响。

Conclusion: 虽然该方法无法完全捕捉比赛风格或球队实力的所有方面，但为教练分析战术效率提供了有价值的框架，并为体育分析研究设立了先例。

Abstract: This study addresses a central tactical dilemma for football coaches: whether to employ a defensive strategy, colloquially known as "parking the bus", or a more offensive one. Using an advanced Double Machine Learning (DML) framework, this project provides a robust and interpretable tool to estimate the causal impact of different formations on key match outcomes such as goal difference, possession, corners, and disciplinary actions. Leveraging a dataset of over 22,000 matches from top European leagues, formations were categorized into six representative types based on tactical structure and expert consultation. A major methodological contribution lies in the adaptation of DML to handle categorical treatments, specifically formation combinations, through a novel matrix-based residualization process, allowing for a detailed estimation of formation-versus-formation effects that can inform a coach's tactical decision-making. Results show that while offensive formations like 4-3-3 and 4-2-3-1 offer modest statistical advantages in possession and corners, their impact on goals is limited. Furthermore, no evidence supports the idea that defensive formations, commonly associated with parking the bus, increase a team's winning potential. Additionally, red cards appear unaffected by formation choice, suggesting other behavioral factors dominate. Although this approach does not fully capture all aspects of playing style or team strength, it provides a valuable framework for coaches to analyze tactical efficiency and sets a precedent for future research in sports analytics.

</details>


### [85] [Temperature and Respiratory Emergency Department Visits: A Mediation Analysis with Ambient Ozone Exposure](https://arxiv.org/abs/2602.16970)
*Chen Li,Thomas W. Hsiao,Stefanie Ebelt,Rebecca H. Zhang,Howard H. Chang*

Main category: stat.AP

TL;DR: 高温通过臭氧部分介导增加呼吸系统急诊就诊风险，特别是在中等高温条件下


<details>
  <summary>Details</summary>
Motivation: 高温与呼吸系统不良健康结局相关，但空气污染在温度和呼吸系统发病率关系中的介导作用研究有限

Method: 使用因果中介分析分解温度对呼吸系统急诊就诊的总效应，以臭氧为中介变量，采用贝叶斯加性回归树(BART)灵活建模非线性关系

Result: 臭氧部分介导了高温与呼吸系统急诊就诊之间的关联，特别是在中等高温条件下

Conclusion: 研究通过考虑急性呼吸系统发病率和采用灵活建模方法，为温度相关健康风险的机制提供了新见解

Abstract: High temperatures are associated with adverse respiratory health outcomes and increases in ambient air pollution. Limited research has quantified air pollution's mediating role in the relationship between temperature and respiratory morbidity, such as emergency department (ED) visits. In this study, we conducted a causal mediation analysis to decompose the total effect of daily temperature on respiratory ED visits in Los Angeles from 2005 to 2016. We focused on ambient ozone as a mediator because its precursors and formation are directly driven by sunlight and temperature. We estimated natural direct, indirect, and total effects on the relative risk scale across deciles of temperature exposure compared to the median. We utilized Bayesian additive regression trees (BART) to flexibly characterize the nonlinear relationship between temperature and ozone and quantified uncertainty via posterior prediction and the Bayesian bootstrap. Our results showed that ozone partially mediated the association between high temperatures and respiratory ED visits, particularly at moderately high temperatures. We also validated our modeling approach through simulation studies. This study extends the existing literature by considering acute respiratory morbidity and employing a flexible modeling approach, offering new insights into the mechanisms underlying temperature-related health risks.

</details>


### [86] [Using Time Series Measures to Explore Family Planning Survey Data and Model-based Estimates](https://arxiv.org/abs/2602.17034)
*Oluwayomi Akinfenwa,Niamh Cahill,Catherine Hurley*

Main category: stat.AP

TL;DR: 评估FPEM模型在家庭规划数据估计中的表现，使用时间序列诊断指标验证模型与调查数据的匹配度


<details>
  <summary>Details</summary>
Motivation: 家庭规划是全球发展优先事项，但各国调查数据存在缺口，需要评估联合国人口司开发的FPEM模型估计结果的准确性

Method: 使用wdlexplorer R包中的时间序列诊断指标，考虑国家嵌套在子区域的结构，可视化调查数据、模型轨迹和诊断结果

Result: 通过诊断指标评估模型性能，识别趋势一致的区域和存在差异的地方

Conclusion: 该方法能够有效评估FPEM模型在家庭规划数据估计中的表现，为改进模型提供依据

Abstract: Family planning is a global development priority and a key indicator of reproductive health. Monitoring progress is challenged by gaps in survey data across countries. The United Nations Population Division addresses this with the Family Planning Estimation Model (FPEM), a Bayesian hierarchical time series model producing annual estimates of modern contraceptive use while sharing information across countries and regions. This paper evaluates how well FPEM estimates align with survey data using time series diagnostic indices from the wdiexplorer R package, which account for countries nested within sub-regions. Visualisation of survey data, modelled trajectories, and diagnostics enables assessment of model performance, highlighting where trends align and where discrepancies occur.

</details>


### [87] [Quantifying the limits of human athletic performance: A Bayesian analysis of elite decathletes](https://arxiv.org/abs/2602.17043)
*Paul-Hieu V. Nguyen,James M. Smoliga,Benton Lindaman,Sameer K. Deshpande*

Main category: stat.AP

TL;DR: 开发贝叶斯组合模型预测十项全能运动员表现，模拟最大可能得分分布，识别接近极限的运动员特征


<details>
  <summary>Details</summary>
Motivation: 十项全能被认为是人类运动能力的终极测试，理解最大可能得分及其实现条件有助于揭示人类运动潜力的上限

Method: 开发贝叶斯组合模型，预测运动员在10个十项全能项目中的表现，捕捉非线性时间趋势和项目间的依赖关系

Result: 能够模拟最大可能得分的分布，识别出可能接近这一极限的运动员特征

Conclusion: 该模型为理解人类运动潜力上限提供了新工具，可识别接近最大十项全能得分的运动员特征

Abstract: Because the decathlon tests many facets of athleticism, including sprinting, throwing, jumping, and endurance, many consider it to be the ultimate test of athletic ability. On this view, estimating the maximal decathlon score and understanding what it would take to achieve that score provides insight into the upper limits of human athletic potential. To this end, we develop a Bayesian composition model for forecasting how individual athletes perform in each of the 10 decathlon events of time. Besides capturing potential non-linear temporal trends in performance, our model carefully captures the dependence between performance in an event and all preceding events. Using our model, we can simulate and evaluate the distribution of the maximal possible scores and identify profiles of athletes who could realistically attain scores approaching this limit.

</details>


### [88] [Environmental policy in the context of complex systems: Statistical optimization and sensitivity analysis for ABMs](https://arxiv.org/abs/2602.17079)
*Dylan Munson,Arijit Dey,Simon Mak*

Main category: stat.AP

TL;DR: 提出结合机器学习加速ABM政策优化的统计框架，在Sugarscape模型上验证有效性


<details>
  <summary>Details</summary>
Motivation: ABM能捕捉复杂自适应系统行为，但计算成本高限制了其在政策优化中的应用，需要加速方法

Method: 开发统计框架：1) 最优政策敏感性测试方法；2) 利用强化学习进行高效政策优化；在Sugarscape资源收获ABM上测试

Result: 方法能快速识别最优且可解释的政策，优于基线技术，提供有洞察力的敏感性分析和动态分析，与经济理论连接

Conclusion: 提出的统计框架能有效加速ABM政策优化，为复杂自适应系统的政策设计提供实用工具

Abstract: Coupled human-environment systems are increasingly being understood as complex adaptive systems (CAS), in which micro-level interactions between components lead to emergent behavior. Agent-based models (ABMs) hold great promise for environmental policy design by capturing such complex behavior, enabling a sophisticated understanding of potential interventions. One limitation, however, is that ABMs can be computationally costly to simulate, which hinders their use for policy optimization. To address this, we propose a new statistical framework that exploits machine learning techniques to accelerate policy optimization with costly ABMs. We first develop a statistical approach for sensitivity testing of the optimal policy, then leverage a reinforcement learning method for efficient policy optimization. We test this framework on the classic ``Sugarscape'' model, an ABM for resource harvesting. We show that our approach can quickly identify optimal and interpretable policies that improve upon baseline techniques, with insightful sensitivity and dynamic analyses that connect back to economic theory.

</details>


### [89] [huff: A Python package for Market Area Analysis](https://arxiv.org/abs/2602.17640)
*Thomas Wieland*

Main category: stat.AP

TL;DR: huff Python包提供完整的市场区域分析工作流，包括数据导入、OD矩阵构建、Huff模型分析、参数估计、距离/时间指标计算和地图可视化，适用于零售、医疗等领域的空间可达性分析。


<details>
  <summary>Details</summary>
Motivation: Huff模型及其扩展在市场区域分析中广泛应用，但缺乏一个完整的Python工具包来支持从数据导入到结果可视化的完整工作流程。该包旨在为经济地理、区域经济、空间规划、营销、地理信息科学和健康地理等领域的研究者提供一个模块化、面向对象的分析工具。

Method: 开发了一个模块化、面向对象的Python包，提供完整的工作流程：数据导入、构建起点-终点交互矩阵、基础模型分析、基于经验数据的参数估计、距离/旅行时间指标计算、地图可视化，以及多种空间可达性分析方法。

Result: huff包已通过Python包索引(PyPI)公开发布，开发版本和版本历史在GitHub仓库管理，并在Zenodo存档。该软件为研究者提供了完整的市场区域分析工具链。

Conclusion: huff Python包填补了市场区域分析工具的空缺，为多个学科领域的研究者提供了标准化、可重复的分析框架，促进了Huff模型在零售、医疗等领域的应用研究。

Abstract: Market area models, such as the Huff model and its extensions, are widely used to estimate regional market shares and customer flows of retail and service locations. Another, now very common, area of application is the analysis of catchment areas, supply structures and the accessibility of healthcare locations. The huff Python package provides a complete workflow for market area analysis, including data import, construction of origin-destination interaction matrices, basic model analysis, parameter estimation from empirical data, calculation of distance or travel time indicators, and map visualization. Additionally, the package provides several methods of spatial accessibility analysis. The package is modular and object-oriented. It is intended for researchers in economic geography, regional economics, spatial planning, marketing, geoinformation science, and health geography. The software is openly available via the [Python Package Index (PyPI)](https://pypi.org/project/huff/); its development and version history are managed in a public [GitHub Repository](https://github.com/geowieland/huff_official) and archived at [Zenodo](https://doi.org/10.5281/zenodo.18639559).

</details>

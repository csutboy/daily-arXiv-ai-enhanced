<div id=toc></div>

# Table of Contents

- [cs.CY](#cs.CY) [Total: 11]
- [econ.EM](#econ.EM) [Total: 3]
- [cs.AI](#cs.AI) [Total: 62]
- [cs.SI](#cs.SI) [Total: 8]
- [stat.AP](#stat.AP) [Total: 8]


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [1] [Measuring Fine-Grained Negotiation Tactics of Humans and LLMs in Diplomacy](https://arxiv.org/abs/2512.18292)
*Wenkai Li,Lynnette Hui Xian Ng,Andy Liu,Daniel Fried*

Main category: cs.CY

TL;DR: 该研究将焦点从谈判代理的成功转向谈判策略的风格，使用LLM作为评判员标注人类《外交》游戏中的谈判战术，发现谈判特征与游戏成功相关，并展示了微调如何使LLM谈判行为更接近人类。


<details>
  <summary>Details</summary>
Motivation: 传统研究主要关注谈判代理的成功，本研究转向关注谈判策略的风格。选择《外交》这款战略对话棋盘游戏作为研究平台，因为它提供了丰富的自然语言谈判环境和游戏成功的衡量标准。

Method: 使用LLM-as-a-judge框架，基于社会学基础分类法标注大规模人类《外交》游戏中的细粒度谈判战术。结合It Takes Two和WebDiplomacy数据集，验证LLM评判框架的可靠性，分析谈判特征与游戏成功的相关性，并比较LLM与人类谈判策略的差异。

Result: 证明了LLM-as-a-Judge框架的可靠性，展示了谈判特征与《外交》游戏成功之间的强相关性。研究发现LLM与人类谈判策略存在差异，但通过微调可以引导LLM代理表现出更接近人类的谈判行为。

Conclusion: 本研究成功将焦点从谈判成功转向谈判风格，建立了可靠的LLM评判框架，揭示了谈判战术与游戏成功的相关性，并展示了通过微调使LLM谈判行为更接近人类的可行性。

Abstract: The study of negotiation styles dates back to Aristotle's ethos-pathos-logos rhetoric. Prior efforts primarily studied the success of negotiation agents. Here, we shift the focus towards the styles of negotiation strategies. Our focus is the strategic dialogue board game Diplomacy, which affords rich natural language negotiation and measures of game success. We used LLM-as-a-judge to annotate a large human-human set of Diplomacy games for fine-grained negotiation tactics from a sociologically-grounded taxonomy. Using a combination of the It Takes Two and WebDiplomacy datasets, we demonstrate the reliability of our LLM-as-a-Judge framework and show strong correlations between negotiation features and success in the Diplomacy setting. Lastly, we investigate the differences between LLM and human negotiation strategies and show that fine-tuning can steer LLM agents toward more human-like negotiation behaviors.

</details>


### [2] [Color, Sentiment, and Structure: A Comparative Study of Instagram Marketing Across Economies](https://arxiv.org/abs/2512.18310)
*Ritesh Konka,Pranali Kurani*

Main category: cs.CY

TL;DR: 研究Instagram食品品牌营销中美学元素（颜色、情感）与宏观结构因素（GDP、人口、肥胖率）如何共同影响用户参与度，发现不同国家类型存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注内容策略，但缺乏对内容与宏观环境因素交互作用的探讨。本研究旨在填补这一空白，探索美学元素与结构因素如何共同塑造消费者参与度。

Method: 采用多模态分析Instagram帖子（颜色方案、标题情感），结合回归模型评估宏观经济和人口变量（GDP、人口、肥胖率）对参与度的调节作用，比较发达国家与发展中国家。

Result: 参与模式存在显著地区差异：发展中国家，米白色和绿色组合显著提升互动，GDP是参与度的正向预测因子；发达国家，人口规模促进参与度，但GDP与用户关注度负相关；肥胖率影响复杂，在某些地区增加点赞但减少评论。

Conclusion: 需要制定本地化的数字营销策略，将内容设计与结构现实相结合以优化受众参与度，强调了因地制宜的重要性。

Abstract: Instagram has become a key platform for global food brands to engage diverse audiences through visual storytelling. While previous research emphasizes content-based strategies, this study bridges the gap between content and context by examining how aesthetic elements -- such as dominant image colors and caption sentiment -- and structural factors like GDP, population, and obesity rates collectively shape consumer engagement. Using a multimodal analysis of Instagram posts from major food outlets across developed and developing countries, we assess how color schemes and sentiment influence key engagement metrics. We then extend this analysis with regression modeling to evaluate how these macroeconomic and demographic variables moderate engagement. Our results reveal that engagement patterns vary widely across regions. In developing countries, color combinations like off-white and green significantly enhance interactions, and GDP is a strong positive predictor of engagement. Conversely, in developed countries, a larger population boosts engagement while higher GDP correlates with reduced user attention. Obesity rates show a mixed influence, moderately enhancing likes in some regions while lowering comments in others. These findings highlight the critical need for localized digital marketing strategies that align content design with structural realities to optimize audience engagement.

</details>


### [3] [Adaptive Learning Mechanisms for Learning Management Systems: A Scoping Review and Practical Considerations](https://arxiv.org/abs/2512.18383)
*Sebastian Kucharski,Iris Braun,Gregor Damnik,Matthias Wählisch*

Main category: cs.CY

TL;DR: 对2003-2023年间自适应学习机制集成到LMS的系统性文献综述，验证了三个假设并提出了两个未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统LMS提供一刀切解决方案，无法满足个性化学习需求。自适应学习机制可以通过系统独立方式集成到各种现有LMS中以提高可重用性，但当前集成方式存在局限性。

Method: 使用Scopus、Web of Science和Google Scholar进行系统性文献综述，筛选了3370篇2003-2023年的论文，通过深入阅读提取了61个相关方法的8个变量进行分析。

Result: 识别了61个相关方法，验证了三个假设：1）自适应学习机制很少考虑现有数据；2）通常支持有限的数据处理机制；3）用户很少能够调整这些机制的工作方式。

Conclusion: 基于假设揭示的挑战，提出了两个未来研究方向：开发系统独立规范自适应学习机制的概念模型及相应架构，以及支持低技术专业用户创作这些机制。

Abstract: Background: Traditional Learning Management Systems (LMS) usually offer a one-size-fits-all solution that cannot be customized to meet specific learner needs. To address this issue, adaptive learning mechanisms are integrated either by LMS-specific approaches into individual LMSs or by system-independent mechanisms into various existing LMSs to increase reusability.
  Objective: We conducted a systematic review of the literature addressing the following research questions. How are adaptive learning mechanisms integrated into LMSs system-independently? How are they provided, how are they specified, and on which database do they operate? A priori, we proposed three hypotheses. First, the focused adaptive learning mechanisms, rarely consider existing data. Second, they usually support a limited number of data processing mechanisms. Third, the users intended to provide them, are rarely given the ability to adapt how they work. Furthermore, to investigate the differences between system-independent and LMS-specific approaches, we also included the latter.
  Design: We used Scopus, Web of Science and Google Scholar for gray literature to identify 3370 papers published between 2003 and 2023 for screening, and conducted a snowball search.
  Results: We identified 61 relevant approaches and extracted eight variables for them through in-depth reading. The results support the proposed hypotheses.
  Conclusion: Based on the challenges raised by the proposed hypotheses with regard to the relevant user groups, we defined two future research directions - developing a conceptual model for the system-independent specification of adaptive learning mechanisms and a corresponding architecture for the provision, and supporting the authoring of these mechanisms by users with low technical expertise.

</details>


### [4] [A Formal Descriptive Language for Learning Dynamics: A Five-Layer Structural Coordinate System](https://arxiv.org/abs/2512.18525)
*Miyuki T. Nakata*

Main category: cs.CY

TL;DR: 提出一个多层形式化描述框架，用于描述学习动态过程，强调描述清晰性和可扩展性，而非预测或规范性模型


<details>
  <summary>Details</summary>
Motivation: 现有方法往往孤立处理认知负荷、内部状态变化和主观评估等因素，缺乏统一且结构明确的学习动态描述框架

Method: 构建一个符号语言框架，包含状态变量、映射和层级特定职责，明确分离负载生成、内部理解转换、观察和评估等描述责任

Result: 提出了一个描述性框架，将认知负荷视为外部输入与内部组织交互产生的关系量，主观评估建模为响应学习动态和环境条件的最小调节接口

Conclusion: 该框架为组织现有理论和支持未来实证与理论研究提供了共同语言，可作为人类学习者及自适应和AI辅助学习系统分析学习过程的结构基础

Abstract: Understanding learning as a dynamic process is challenging due to the interaction of multiple factors, including cognitive load, internal state change, and subjective evaluation. Existing approaches often address these elements in isolation, limiting the ability to describe learning phenomena within a unified and structurally explicit framework. This paper proposes a multi-layer formal descriptive framework for learning dynamics. Rather than offering a predictive or prescriptive model, the framework introduces a symbolic language composed of state variables, mappings, and layer-specific responsibilities, enabling consistent description of learning processes without commitment to specific functional forms or optimization objectives. This descriptive framework is intended to serve as a structural substrate for analyzing learning processes in human learners, and by extension, in adaptive and Al-assisted learning systems. A central design principle is the explicit separation of descriptive responsibilities across layers, distinguishing load generation, internal understanding transformation, observation, and evaluation. Within this structure, cognitive load is treated as a relational quantity arising from interactions between external input and internal organization, while subjective evaluation is modeled as a minimal regulatory interface responding to learning dynamics and environmental conditions. By emphasizing descriptive clarity and extensibility, the framework provides a common language for organizing existing theories and supporting future empirical and theoretical work.

</details>


### [5] [The MEVIR 2 Framework: A Virtue-Informed Moral-Epistemic Model of Human Trust Decisions](https://arxiv.org/abs/2512.18539)
*Daniel Schwabe*

Main category: cs.CY

TL;DR: MEVIR 2框架通过整合证据处理程序、认知美德和道德直觉三个维度，解释信任决策在极化信息环境中的形成机制，并提出"真理部落"概念描述具有内在一致信任结构的社群。


<details>
  <summary>Details</summary>
Motivation: 传统理性模型无法解释极化信息环境中人们为何对相同证据和权威产生不同信任判断。需要新框架来理解信任决策的多维本质，特别是程序性、美德性和道德直觉的交互作用。

Method: 提出MEVIR 2理论框架，整合：1) 证据处理程序；2) 认知美德理论；3) 进化合作模型和文化价值观的道德基础理论。引入"真理部落"概念描述共享认知配置的稳定社群，区分"真理承载者"和"真理制造者"。

Result: 框架成功解释了疫苗接种和气候政策等案例中不同群体选择不同权威、证据标准和信任锚点的现象。认知偏差被重新解释为认知美德的失败，为设计增强元认知的决策支持系统提供基础。

Conclusion: MEVIR 2既提供了理解信息极化的描述性工具，又为跨越认知分歧提供了规范性指导，能够促进跨社群更审慎的推理和透明的信任过程。

Abstract: The MEVIR 2 framework innovates and improves how we understand trust decisions in our polarized information landscape. Unlike classical models assuming ideal rationality, MEVIR 2 recognizes that human trust emerges from three interacting foundations: how we process evidence procedurally, our character as epistemic agents virtue theory, and our moral intuitions shaped by both evolutionary cooperation MAC model and cultural values Extended Moral Foundations Theory. This explains why different people find different authorities, facts, and tradeoffs compelling.
  MEVIR 2's key innovation introduces "Truth Tribes" TTs-stable communities sharing aligned procedural, virtue, and moral epistemic profiles. These arent mere ideological groups but emergent clusters with internally coherent "trust lattices" that remain mutually unintelligible across tribal boundaries. The framework incorporates distinctions between Truth Bearers and Truth Makers, showing disagreements often stem from fundamentally different views about what aspects of reality can make propositions true.
  Case studies on vaccination mandates and climate policy demonstrate how different moral configurations lead people to select different authorities, evidential standards, and trust anchors-constructing separate moral epistemic worlds. The framework reinterprets cognitive biases as failures of epistemic virtue and provides foundations for designing decision support systems that could enhance metacognition, make trust processes transparent, and foster more conscientious reasoning across divided communities. MEVIR 2 thus offers both descriptive power for understanding polarization and normative guidance for bridging epistemic divides.

</details>


### [6] [Measuring the Impact of Student Gaming Behaviors on Learner Modeling](https://arxiv.org/abs/2512.18659)
*Qinyi Liu,Lin Li,Valdemar Švábenský,Conrad Borchers,Mohammad Khalil*

Main category: cs.CY

TL;DR: 该研究将在线教育中的游戏行为（如过度使用提示）视为数据投毒攻击，通过设计多样化的数据投毒攻击模式来系统评估知识追踪模型的脆弱性，并探索无监督检测方法提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大规模在线教育平台产生了大量学生交互数据用于知识追踪，但学生通过过度使用提示等游戏行为可能扭曲其真实知识水平，影响知识追踪模型的可靠性。目前缺乏对各类游戏行为影响的系统性研究，且现有研究依赖成本高昂的人工分析，无法捕捉行为多样性。

Method: 将游戏行为概念化为数据投毒攻击（故意提交错误或误导性交互数据以破坏模型学习过程），设计多样化的数据投毒攻击模式来模拟不同的游戏行为模式，系统评估这些攻击对知识追踪模型性能的影响。同时借鉴数据投毒攻击检测的最新进展，探索无监督方法来提升游戏行为检测的泛化能力。

Result: 研究发现知识追踪模型的性能在随机猜测行为攻击下下降尤为明显。研究揭示了知识追踪模型在面对不同类型游戏行为时的脆弱性，验证了对抗性方法在提升学习分析系统鲁棒性方面的潜力。

Conclusion: 该研究通过将游戏行为框架化为数据投毒攻击，为系统评估知识追踪模型的脆弱性提供了新视角。研究结果表明需要开发更鲁棒的知识追踪模型来应对游戏行为，并展示了无监督检测方法和对抗性训练在提升学习分析系统可靠性方面的应用前景。

Abstract: The expansion of large-scale online education platforms has made vast amounts of student interaction data available for knowledge tracing (KT). KT models estimate students' concept mastery from interaction data, but their performance is sensitive to input data quality. Gaming behaviors, such as excessive hint use, may misrepresent students' knowledge and undermine model reliability. However, systematic investigations of how different types of gaming behaviors affect KT remain scarce, and existing studies rely on costly manual analysis that does not capture behavioral diversity. In this study, we conceptualize gaming behaviors as a form of data poisoning, defined as the deliberate submission of incorrect or misleading interaction data to corrupt a model's learning process. We design Data Poisoning Attacks (DPAs) to simulate diverse gaming patterns and systematically evaluate their impact on KT model performance. Moreover, drawing on advances in DPA detection, we explore unsupervised approaches to enhance the generalizability of gaming behavior detection. We find that KT models' performance tends to decrease especially in response to random guess behaviors. Our findings provide insights into the vulnerabilities of KT models and highlight the potential of adversarial methods for improving the robustness of learning analytics systems.

</details>


### [7] ["Even GPT Can Reject Me": Conceptualizing Abrupt Refusal Secondary Harm (ARSH) and Reimagining Psychological AI Safety with Compassionate Completion Standard (CCS)](https://arxiv.org/abs/2512.18776)
*Yang Ni,Tong Yang*

Main category: cs.CY

TL;DR: 论文提出"突然拒绝二次伤害"(ARSH)概念框架，描述AI安全协议触发时突然终止对话造成的心理影响，并提出"同情完成标准"(CCS)设计假设来缓解这种伤害。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型和AI聊天机器人越来越多地用于情感和心理健康支持，但当安全护栏被触发时，对话可能被突然终止，这会造成独特的情绪干扰，加剧脆弱用户的痛苦并增加风险。

Method: 基于咨询心理学和传播科学作为概念启发，提出ARSH作为概念框架，并提出CCS设计假设——一种基于人本设计(HCD)的拒绝协议，在保持安全约束的同时保持关系连贯性。

Result: 论文提出了一个及时的概念框架、可测试的设计假设，并概述了协调的研究议程，旨在提高人机交互中的心理安全性。

Conclusion: 通过将ARSH意识整合到AI安全设计中，开发者和政策制定者可以减少可预防的医源性伤害，推进更符合心理学的AI治理方法。

Abstract: Large Language Models (LLMs) and AI chatbots are increasingly used for emotional and mental health support due to their low cost, immediacy, and accessibility. However, when safety guardrails are triggered, conversations may be abruptly terminated, introducing a distinct form of emotional disruption that can exacerbate distress and elevate risk among already vulnerable users. As this phenomenon gains attention, this viewpoint introduces Abrupt Refusal Secondary Harm (ARSH) as a conceptual framework to describe the psychological impacts of sudden conversational discontinuation caused by AI safety protocols. Drawing on counseling psychology and communication science as conceptual heuristics, we argue that abrupt refusals can rupture perceived relational continuity, evoke feelings of rejection or shame, and discourage future help seeking. To mitigate these risks, we propose a design hypothesis, the Compassionate Completion Standard (CCS), a refusal protocol grounded in Human Centered Design (HCD) that maintains safety constraints while preserving relational coherence. CCS emphasizes empathetic acknowledgment, transparent boundary articulation, graded conversational transition, and guided redirection, replacing abrupt disengagement with psychologically attuned closure. By integrating awareness of ARSH into AI safety design, developers and policymakers can reduce preventable iatrogenic harm and advance a more psychologically informed approach to AI governance. Rather than presenting incremental empirical findings, this viewpoint contributes a timely conceptual framework, articulates a testable design hypothesis, and outlines a coordinated research agenda for improving psychological safety in human AI interaction.

</details>


### [8] [Quantifying the Lifelong Impact of Resilience Interventions via Agent-Based LLM Simulation](https://arxiv.org/abs/2512.18803)
*Vivienne L'Ecuyer Ming*

Main category: cs.CY

TL;DR: LALS框架通过基于LLM的"数字克隆"模拟，量化心理韧性训练对人生轨迹的长期因果影响，发现儿童期干预比成年期效果更显著，尤其对低社会经济背景人群。


<details>
  <summary>Details</summary>
Motivation: 解决社会科学中长期因果推断的难题：纵向研究存在相关性局限，而随机对照试验通常只能评估短期效果，缺乏对心理干预数十年影响的因果证据。

Method: 提出大规模基于代理的纵向模拟（LALS）框架，使用2,500个基于LLM的独特代理角色（基于3,917篇实证研究文章），通过"数字克隆"设计在2x2因子实验中模拟心理韧性训练在不同年龄段（6岁vs18岁）的长期影响。

Result: 韧性训练显著降低死亡率、减少痴呆发病率、大幅增加累积财富。6岁干预对终身财富的积极影响是18岁干预的两倍多，对低社会经济背景人群效果最明显。

Conclusion: LALS框架可作为社会科学的"计算风洞"，为检验人类资本和福祉的复杂终身动态因果假设提供新范式，揭示了关键发展窗口期和社会经济背景的调节作用。

Abstract: Establishing the long-term, causal impact of psychological interventions on life outcomes is a grand challenge for the social sciences, caught between the limitations of correlational longitudinal studies and short-term randomized controlled trials (RCTs). This paper introduces Large-Scale Agent-based Longitudinal Simulation (LALS), a framework that resolves this impasse by simulating multi-decade, counterfactual life trajectories. The methodology employs a "digital clone" design where 2,500 unique LLM-based agent personas (grounded in a curated corpus of 3,917 empirical research articles) are each cloned across a 2x2 factorial experiment. Specifically, the simulation models the efficacy of extended psychological resilience training (Intervention vs. Control) either in childhood or as a young adult (age 6 vs. age 18). Comparing digital clones enables exceptionally precise causal inference. The simulation provides a quantitative, causal estimate of a resilience intervention's lifelong effects, revealing significant reductions in mortality, a lower incidence of dementia, and a substantial increase in accumulated wealth. Crucially, the results uncover a crucial developmental window: the intervention administered at age 6 produced more than double the positive impact on lifetime wealth compared to the same intervention at age 18. These benefits were most pronounced for agents from low-socioeconomic backgrounds, highlighting a powerful buffering effect. The LALS framework serves as a "computational wind tunnel" for social science, offering a new paradigm for generating and testing causal hypotheses about the complex, lifelong dynamics that shape human capital and well-being.

</details>


### [9] [Psychometric Validation of the Sophotechnic Mediation Scale and a New Understanding of the Development of GenAI Mastery: Lessons from 3,392 Adult Brazilian Workers](https://arxiv.org/abs/2512.18871)
*Bruno Campello de Souza*

Main category: cs.CY

TL;DR: 该研究开发并验证了"Sophotechnic Mediation Scale"量表，用于测量长期使用生成式AI所形成的一种稳定认知模式，发现该模式与累积的GenAI使用经验相关，且年龄调节其发展过程。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI系统的快速普及，人们开始关注长期使用是否会导致稳定的内在认知模式形成，而不仅仅是暂时的效率提升。研究旨在验证"Sophotechnic Mediation"（智能技术中介）这一概念，即与GenAI长期互动相关的思维和行为模式。

Method: 基于认知中介网络理论，研究开发了Sophotechnic Mediation量表，并在2023-2025年间收集了巴西伯南布哥大都会区3,932名成年工人的数据。采用序数稳健验证性因子分析、残差诊断、分布分析和模型比较等方法进行心理测量学验证。

Result: 量表显示出优秀的内部一致性、稳健的单维结构和跨队列测量不变性。分布分析显示随时间演变的模式：非采用者减少，采用者趋向近似正态分布。Sophotechnic Mediation与超文化中介在经验上不同，主要由累积的GenAI经验驱动，年龄调节初始获取速率和后期整合深度。

Conclusion: Sophotechnia是与持续GenAI革命相关的连贯、可测量和新兴的认知中介模式。研究支持了长期GenAI互动会导致稳定认知模式形成的假设，为理解人类与技术关系的演变提供了实证基础。

Abstract: The rapid diffusion of generative artificial intelligence (GenAI) systems has introduced new forms of human-technology interaction, raising the question of whether sustained engagement gives rise to stable, internalized modes of cognition rather than merely transient efficiency gains. Grounded in the Cognitive Mediation Networks Theory, this study investigates Sophotechnic Mediation, a mode of thinking and acting associated with prolonged interaction with GenAI, and presents a comprehensive psychometric validation of the Sophotechnic Mediation Scale. Data were collected between 2023 and 2025 from independent cross-sectional samples totaling 3,932 adult workers from public and private organizations in the Metropolitan Region of Pernambuco, Brazil. Results indicate excellent internal consistency, a robust unidimensional structure, and measurement invariance across cohorts. Ordinal-robust confirmatory factor analyses and residual diagnostics show that elevated absolute fit indices reflect minor local dependencies rather than incorrect dimensionality. Distributional analyses reveal a time-evolving pattern characterized by a declining mass of non-adopters and convergence toward approximate Gaussianity among adopters, with model comparisons favoring a two-process hurdle model over a censored Gaussian specification. Sophotechnic Mediation is empirically distinct from Hypercultural mediation and is primarily driven by cumulative GenAI experience, with age moderating the rate of initial acquisition and the depth of later integration. Together, the findings support Sophotechnia as a coherent, measurable, and emergent mode of cognitive mediation associated with the ongoing GenAI revolution.

</details>


### [10] [Configuration Work: Four Consequences of LLMs-in-use](https://arxiv.org/abs/2512.19189)
*Gabriel Alcaras,Donato Ricci*

Main category: cs.CY

TL;DR: LLMs不直接自动化或增强任务，而是需要用户进行配置工作来使通用系统适应特定专业任务，这带来离散化、杂乱化、适应化和去饱和化四种后果。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在日常工作中的实际使用意义，探讨通用AI系统如何被整合到具体专业环境中，以及这种整合带来的劳动变化。

Method: 基于七个月的纵向定性研究，通过实证观察分析LLMs在工作场所中的实际使用情况。

Result: 提出了"配置工作"概念，揭示了LLMs使用中的四个相互关联的后果：离散化、杂乱化、适应化和去饱和化。

Conclusion: LLMs通过个体化的配置工作重塑工作形态，将通用任务无关系统整合到具体专业生态中，改变了工作的本质和体验。

Abstract: This article examines what it means to use Large Language Models in everyday work. Drawing on a seven-month longitudinal qualitative study, we argue that LLMs do not straightforwardly automate or augment tasks. We propose the concept of configuration work to describe the labor through which workers make a generic system usable for a specific professional task. Configuration work materializes in four intertwined consequences. First, workers must discretize their activity, breaking it into units that the system can process. Second, operating the system generates cluttering, as prompting, evaluating, and correcting responses add scattered layers of work that get in the way of existing routines. Third, users gradually attune their practices and expectations to the machine's generic rigidity, making sense of the system's limits and finding space for it within their practices. Fourth, as LLMs absorb repetitive tasks, they desaturate the texture of work, shifting activity toward logistical manipulation of outputs and away from forms of engagement that sustain a sense of accomplishment. Taken together, these consequences suggest that LLMs reshape work through the individualized labor required to configure a universal, task-agnostic system within situated professional ecologies.

</details>


### [11] [Epistemological Fault Lines Between Human and Artificial Intelligence](https://arxiv.org/abs/2512.19466)
*Walter Quattrociocchi,Valerio Capraro,Matjaž Perc*

Main category: cs.CY

TL;DR: 论文认为大语言模型并非真正的认知主体，而是随机模式完成系统，其表面与人类输出的对齐掩盖了深层结构差异，导致"Epistemia"现象——语言合理性替代了认知评估。


<details>
  <summary>Details</summary>
Motivation: 当前广泛将大语言模型描述为人工智能，但其认知特征与人类认知存在根本差异。论文旨在揭示这种表面对齐背后的深层结构不匹配，澄清LLMs的真实本质。

Method: 通过追溯从符号AI和信息过滤系统到大规模生成式Transformer的历史转变，分析LLMs作为高维语言转换图上的随机游走系统的形式特征。系统映射人类和人工认知流程，识别七个认知断层线。

Result: 识别出七个认知断层线：基础、解析、经验、动机、因果推理、元认知和价值。提出"Epistemia"概念来描述语言合理性替代认知评估的结构性状况。

Conclusion: LLMs不是认知主体而是随机模式完成系统。这种认知错位对评估、治理和认知素养有重要影响，特别是在日益围绕生成式AI组织的社会中。

Abstract: Large language models (LLMs) are widely described as artificial intelligence, yet their epistemic profile diverges sharply from human cognition. Here we show that the apparent alignment between human and machine outputs conceals a deeper structural mismatch in how judgments are produced. Tracing the historical shift from symbolic AI and information filtering systems to large-scale generative transformers, we argue that LLMs are not epistemic agents but stochastic pattern-completion systems, formally describable as walks on high-dimensional graphs of linguistic transitions rather than as systems that form beliefs or models of the world. By systematically mapping human and artificial epistemic pipelines, we identify seven epistemic fault lines, divergences in grounding, parsing, experience, motivation, causal reasoning, metacognition, and value. We call the resulting condition Epistemia: a structural situation in which linguistic plausibility substitutes for epistemic evaluation, producing the feeling of knowing without the labor of judgment. We conclude by outlining consequences for evaluation, governance, and epistemic literacy in societies increasingly organized around generative AI.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [12] [Inference in partially identified moment models via regularized optimal transport](https://arxiv.org/abs/2512.18084)
*Grigory Franguridi,Laura Liu*

Main category: econ.EM

TL;DR: 论文提出了一种针对部分识别GMM模型的识别、估计和推断方法，使用最优运输的支持函数表示，通过熵正则化进行平滑近似，并建立了检验统计量的渐近分布理论。


<details>
  <summary>Details</summary>
Motivation: 当数据的联合分布仅已知其边际分布时，会出现部分识别问题。许多实证研究（如面板数据损耗、非线性处理效应、非参数工具变量等）都存在这种部分识别情况，需要开发相应的识别、估计和推断方法。

Method: 1) 使用支持函数/最优运输表示来刻画参数的尖锐识别集；2) 采用熵正则化对经典最优运输进行平滑近似，可通过Sinkhorn算法高效计算；3) 提出检验统计量并建立其渐近分布理论；4) 使用Fang和Santos(2019)的方向可微函数自举法获取有效临界值。

Result: 1) 建立了熵最优运输值在一般光滑成本下的中心极限定理；2) 检验程序在局部一致地控制尺寸，包括在识别集边界上的参数值；3) 蒙特卡洛模拟验证了方法的性能；4) 方法适用于多种实证场景。

Conclusion: 该方法为部分识别GMM模型提供了一套完整的识别、估计和推断框架，具有理论保证和计算效率，可广泛应用于存在部分识别问题的实证研究中。

Abstract: Partial identification often arises when the joint distribution of the data is known only up to its marginals. We consider the corresponding partially identified GMM model and develop a methodology for identification, estimation, and inference in this model. We characterize the sharp identified set for the parameter of interest via a support-function/optimal-transport (OT) representation. For estimation, we employ entropic regularization, which provides a smooth approximation to classical OT and can be computed efficiently by the Sinkhorn algorithm. We also propose a statistic for testing hypotheses and constructing confidence regions for the identified set. To derive the asymptotic distribution of this statistic, we establish a novel central limit theorem for the entropic OT value under general smooth costs. We then obtain valid critical values using the bootstrap for directionally differentiable functionals of Fang and Santos (2019). The resulting testing procedure controls size locally uniformly, including at parameter values on the boundary of the identified set. We illustrate its performance in a Monte Carlo simulation. Our methodology is applicable to a wide range of empirical settings, such as panels with attrition and refreshment samples, nonlinear treatment effects, nonparametric instrumental variables without large-support conditions, and Euler equations with repeated cross-sections.

</details>


### [13] [(Debiased) Inference for Fixed Effects Estimators with Three-Dimensional Panel and Network Data](https://arxiv.org/abs/2512.18678)
*Daniel Czarnowske,Amrei Stammann*

Main category: econ.EM

TL;DR: 本文针对三维面板数据（如发送者×接收者×时间）的固定效应M估计量，开发了新的推断理论，解决了Nickell偏差和/或附带参数偏差问题，并推导了明确的去偏表达式。


<details>
  <summary>Details</summary>
Motivation: 线性与非线性面板模型中固定效应估计量的推断常因Nickell偏差和/或附带参数偏差而不可靠，特别是在三维面板数据结构下，传统二维面板的渐近性质可能不适用，需要新的推断理论。

Method: 开发三维面板数据下（非）线性固定效应M估计量的新推断理论，涵盖双向、有向和无向网络面板数据，整合具有不同变化层的可加可分离未观测效应，并允许弱外生回归变量。

Result: 分析显示三维面板数据固定效应估计量的渐近性质与二维面板有显著差异：某些设定下估计量渐近无偏，其他设定下则存在严重的推断问题，表现为退化渐近分布和复杂偏差结构。

Conclusion: 通过推导明确的表达式来去偏固定效应估计量，解决了三维面板数据中出现的非典型推断问题，为网络面板数据等三维数据结构提供了可靠的推断方法。

Abstract: Inference for fixed effects estimators of linear and nonlinear panel models is often unreliable due to Nickell- and/or incidental parameter biases. This article develops new inferential theory for (non)linear fixed effects M-estimators with data featuring a three-dimensional panel structure, such as sender x receiver x time. Our theory accommodates bipartite, directed, and undirected network panel data, integrates distinct specifications for additively separable unobserved effects with different layers of variation, and allows for weakly exogenous regressors. Our analysis reveals that the asymptotic properties of fixed effects estimators with three-dimensional panel data can deviate substantially from those with two-dimensional panel data. While for some specifications the estimator turns out to be asymptotically unbiased, in other specifications, it suffers from a particularly severe inference problem, characterized by a degenerate asymptotic distribution and complex bias structures. We address this atypical inference problem, by deriving explicit expressions to debias the fixed effects estimators.

</details>


### [14] [Semiparametric Efficiency in Policy Learning with General Treatments](https://arxiv.org/abs/2512.19230)
*Yue Fang,Geert Ridder,Haitian Xie*

Main category: econ.EM

TL;DR: 本文提出政策学习的半参数效率框架，分析确定性政策参数不可微性，建立随机化政策效率边界，揭示无效政策估计会同时增加遗憾分布的方差和均值，发现类似HIR现象：使用估计倾向得分的IPW估计是有效的，而使用真实倾向得分的IPW估计则无效。


<details>
  <summary>Details</summary>
Motivation: 现有政策学习文献主要关注学习策略的遗憾边界，本文提供新视角，建立统一的半参数效率框架，以处理离散、连续或混合的一般处理，并分析政策估计的效率问题。

Method: 建立政策学习的半参数效率理论框架，分析确定性政策参数路径不可微性，推导随机化政策效率边界（已知和估计倾向得分两种情况），基于卷积定理引入福利遗憾渐近分布效率概念，分析常见政策估计量的渐近理论。

Result: 发现政策学习中的HIR类似现象：使用估计倾向得分的逆倾向加权估计是有效的，而使用真实倾向得分的相同估计则无效；无效政策估计不仅增加遗憾渐近分布的方差，还会使其均值上移；通过工作培训项目和承诺储蓄项目的实证校准模拟研究验证理论结果。

Conclusion: 本文为政策学习提供了统一的半参数效率框架，揭示了政策估计效率对遗憾分布的重要影响，发现了政策学习中的HIR类似现象，为实际政策评估提供了理论指导。

Abstract: Recent literature on policy learning has primarily focused on regret bounds of the learned policy. We provide a new perspective by developing a unified semiparametric efficiency framework for policy learning, allowing for general treatments that are discrete, continuous, or mixed. We provide a characterization of the failure of pathwise differentiability for parameters arising from deterministic policies. We then establish efficiency bounds for pathwise differentiable parameters in randomized policies, both when the propensity score is known and when it must be estimated. Building on the convolution theorem, we introduce a notion of efficiency for the asymptotic distribution of welfare regret, showing that inefficient policy estimators not only inflate the variance of the asymptotic regret but also shift its mean upward. We derive the asymptotic theory of several common policy estimators, with a key contribution being a policy-learning analogue of the Hirano-Imbens-Ridder (HIR) phenomenon: the inverse propensity weighting estimator with an estimated propensity is efficient, whereas the same estimator using the true propensity is not. We illustrate the theoretical results with an empirically calibrated simulation study based on data from a job training program and an empirical application to a commitment savings program.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [15] [Conflict-Driven Clause Learning with VSIDS Heuristics for Discrete Facility Layout](https://arxiv.org/abs/2512.18034)
*Joshua Gibson,Kapil Dhakal*

Main category: cs.AI

TL;DR: 该论文研究将CDCL与VSIDS启发式算法应用于离散设施布局问题，开发了CNF建模方法，并比较了CDCL、CP-SAT和MILP的性能。结果显示CDCL在可行性检测上具有近常数时间复杂度，而CP-SAT和MILP分别呈现多项式和指数级增长。为解决CDCL在优化方面的局限，提出了两种混合架构，显著减少了求解时间。


<details>
  <summary>Details</summary>
Motivation: 离散设施布局问题具有复杂的组合分配结构，传统求解方法（如CP-SAT和MILP）在大规模问题上存在可扩展性问题。需要探索更高效的求解方法，特别是利用CDCL在逻辑推理方面的优势来解决这类具有密集逻辑约束的问题。

Method: 1. 将设施布局问题建模为具有邻接、分离和槽可用性约束的CNF公式；2. 在统一基准框架下比较CDCL、CP-SAT和MILP的性能；3. 提出两种混合架构：第一种快速枚举可行布局以速度换取最优性，第二种使用CDCL生成热启动解以加速精确优化。

Result: CDCL在可行性检测方面表现出近常数时间复杂度，而CP-SAT和MILP分别呈现多项式（O(n²)）和指数级增长。混合方法显著减少了求解时间：第一种架构在保持可行性的同时大幅提升速度，第二种架构通过热启动将精确优化时间减少40-60%。

Conclusion: CDCL在离散设施布局问题的可行性检测方面具有显著优势，特别是在大规模高约束密度场景下。混合架构有效弥补了CDCL在优化方面的不足，为大规模离散布局问题提供了实用的解决方案，明确了子句学习搜索与精确优化方法之间的算法权衡。

Abstract: This paper studies the use of Conflict-Driven Clause Learning (CDCL) with VSIDS heuristics as a computational engine for discrete facility layout problems. The facility layout problem is modeled as a combinatorial assignment problem with dense logical structure arising from adjacency, separation, and slot-availability constraints. We develop a CNF-based formulation for layout feasibility and compare CDCL-based SAT solving against CP-SAT and MILP formulations under a unified benchmarking framework. Empirical results show that CDCL exhibits near-constant runtime behavior for feasibility detection across increasing problem sizes and constraint densities, while CP-SAT and MILP display polynomial and exponential scaling respectively. To address the limitation of CDCL in objective optimization, we introduce two hybrid architectures that combine CDCL-based feasibility search with CP-SAT optimization. The first architecture rapidly enumerates feasible layouts to trade optimality for speed, while the second uses CDCL to generate warm-start solutions that accelerate exact optimization. The results demonstrate that hybrid approaches can significantly reduce time-to-solution while preserving correctness guarantees, clarifying the algorithmic trade-offs between clause-learning search and exact optimization methods in large-scale discrete layout problems.

</details>


### [16] [Faithful and Stable Neuron Explanations for Trustworthy Mechanistic Interpretability](https://arxiv.org/abs/2512.18092)
*Ge Yan,Tuomas Oikarinen,Tsui-Wei,Weng*

Main category: cs.AI

TL;DR: 该论文为神经元识别提供了首个理论分析框架，解决了忠实性和稳定性两大挑战，并提出了带保证覆盖概率的BE方法。


<details>
  <summary>Details</summary>
Motivation: 尽管Network Dissection和CLIP-Dissect等神经元识别算法取得了实证成功，但缺乏严格的理论基础，这阻碍了可信赖和可靠解释的实现。需要建立理论保证来确保神经元解释的忠实性和稳定性。

Method: 将神经元识别视为机器学习的逆过程，推导相似性度量（如准确率、AUROC、IoU）的泛化界限以保证忠实性，并提出自举集成程序量化稳定性，开发BE方法生成带保证覆盖概率的概念预测集。

Result: 在合成和真实数据上的实验验证了理论结果，证明了方法的实用性。BE方法能够提供带统计保证的概念解释，为可信赖的神经元识别迈出了重要一步。

Conclusion: 该研究首次为神经元识别提供了理论分析框架，解决了忠实性和稳定性两大核心挑战，提出的BE方法能够生成带统计保证的概念解释，为实现可信赖的神经元识别奠定了基础。

Abstract: Neuron identification is a popular tool in mechanistic interpretability, aiming to uncover the human-interpretable concepts represented by individual neurons in deep networks. While algorithms such as Network Dissection and CLIP-Dissect achieve great empirical success, a rigorous theoretical foundation remains absent, which is crucial to enable trustworthy and reliable explanations. In this work, we observe that neuron identification can be viewed as the inverse process of machine learning, which allows us to derive guarantees for neuron explanations. Based on this insight, we present the first theoretical analysis of two fundamental challenges: (1) Faithfulness: whether the identified concept faithfully represents the neuron's underlying function and (2) Stability: whether the identification results are consistent across probing datasets. We derive generalization bounds for widely used similarity metrics (e.g. accuracy, AUROC, IoU) to guarantee faithfulness, and propose a bootstrap ensemble procedure that quantifies stability along with BE (Bootstrap Explanation) method to generate concept prediction sets with guaranteed coverage probability. Experiments on both synthetic and real data validate our theoretical results and demonstrate the practicality of our method, providing an important step toward trustworthy neuron identification.

</details>


### [17] [Rethinking Multi-Agent Intelligence Through the Lens of Small-World Networks](https://arxiv.org/abs/2512.18094)
*Boxuan Wang,Zhuoyun Li,Xiaowei Huang,Yi Dong*

Main category: cs.AI

TL;DR: 该研究将小世界网络理论应用于LLM多智能体系统设计，通过实验证明小世界连接能稳定共识轨迹，并提出基于不确定性的自适应重连机制。


<details>
  <summary>Details</summary>
Motivation: 当前LLM多智能体系统在通信拓扑设计上缺乏理论指导，大多采用全连接、简单稀疏环或临时动态选择。研究旨在探索小世界网络结构作为多智能体系统设计先验的价值，以平衡局部聚类和长程整合。

Method: 1) 将神经科学和复杂网络的小世界理论见解桥接到多智能体系统；2) 以多智能体辩论为测试平台，比较不同连接结构的性能；3) 提出基于不确定性的重连方案，使用LLM导向的不确定性信号（如语义熵）在认知分歧的智能体间添加长程捷径。

Result: 实验显示小世界连接在保持相近准确性和token成本的同时，显著稳定了共识轨迹。提出的不确定性引导重连机制能够根据任务难度和智能体异质性生成可控的小世界结构。

Conclusion: 小世界先验可作为多智能体系统设计的稳定器、鲁棒性增强器、可扩展协调器和涌现认知角色的归纳偏置，为LLM多智能体系统的结构化设计提供理论指导。

Abstract: Large language models (LLMs) have enabled multi-agent systems (MAS) in which multiple agents argue, critique, and coordinate to solve complex tasks, making communication topology a first-class design choice. Yet most existing LLM-based MAS either adopt fully connected graphs, simple sparse rings, or ad-hoc dynamic selection, with little structural guidance. In this work, we revisit classic theory on small-world (SW) networks and ask: what changes if we treat SW connectivity as a design prior for MAS? We first bridge insights from neuroscience and complex networks to MAS, highlighting how SW structures balance local clustering and long-range integration. Using multi-agent debate (MAD) as a controlled testbed, experiment results show that SW connectivity yields nearly the same accuracy and token cost, while substantially stabilizing consensus trajectories. Building on this, we introduce an uncertainty-guided rewiring scheme for scaling MAS, where long-range shortcuts are added between epistemically divergent agents using LLM-oriented uncertainty signals (e.g., semantic entropy). This yields controllable SW structures that adapt to task difficulty and agent heterogeneity. Finally, we discuss broader implications of SW priors for MAS design, framing them as stabilizers of reasoning, enhancers of robustness, scalable coordinators, and inductive biases for emergent cognitive roles.

</details>


### [18] [Efficient Mixture-of-Agents Serving via Tree-Structured Routing, Adaptive Pruning, and Dependency-Aware Prefill-Decode Overlap](https://arxiv.org/abs/2512.18126)
*Zijun Wang,Yijiahao Qi,Hanqiu Chen,Zishen Wan,Gongjin Sun,Dongyang Li,Shuyi Pei,Cong Hao*

Main category: cs.AI

TL;DR: 提出一种算法-系统协同设计的MoA推理服务方案，通过分层树拓扑、运行时自适应机制和流水线执行，显著降低延迟（最高90%）同时保持准确率（±1%内）。


<details>
  <summary>Details</summary>
Motivation: 现有的Mixture-of-Agents推理存在密集的智能体间通信和低硬件利用率问题，共同导致服务延迟增加。

Method: 1) 用分层树拓扑替代密集连接图，引入结构化稀疏通信；2) 基于语义一致性和置信度的运行时自适应机制，选择性终止或跳过下游智能体调用；3) 通过流水线执行，在依赖相关的智能体间重叠增量预填充和解码。

Result: 在代表性任务中，该方法相比密集连接的MoA基线，端到端延迟显著降低（最高90%），同时保持可比准确率（±1%内），在某些设置下还能提高准确率。

Conclusion: 通过算法-系统协同设计，有效解决了MoA推理中的通信密集和硬件利用率低的问题，实现了延迟大幅降低而准确率基本保持的优化效果。

Abstract: Mixture-of-Agents (MoA) inference can suffer from dense inter-agent communication and low hardware utilization, which jointly inflate serving latency. We present a serving design that targets these bottlenecks through an algorithm-system co-design. First, we replace dense agent interaction graphs with a hierarchical tree topology that induces structured sparsity in inter-agent communication. Second, we introduce a runtime adaptive mechanism that selectively terminates or skips downstream agent invocations using semantic agreement and confidence signals from intermediate outputs. Third, we pipeline agent execution by overlapping incremental prefilling with decoding across dependency-related agents, improving utilization and reducing inference latency. Across representative tasks, this approach substantially reduces end-to-end latency (up to 90%) while maintaining comparable accuracy (within $\pm$1%) relative to dense-connectivity MoA baselines, and can improve accuracy in certain settings.

</details>


### [19] [Unifying Causal Reinforcement Learning: Survey, Taxonomy, Algorithms and Applications](https://arxiv.org/abs/2512.18135)
*Cristiano da Costa Cunha,Wei Liu,Tim French,Ajmal Mian*

Main category: cs.AI

TL;DR: 这篇综述论文系统回顾了因果推断与强化学习的交叉领域，将现有方法分为因果表示学习、反事实策略优化、离线因果RL、因果迁移学习和因果可解释性等类别，并讨论了该领域的挑战、应用和未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习基于相关性决策，在面对分布偏移、混杂变量和动态环境时存在局限性，包括可解释性低、鲁棒性差和泛化失败等问题。因果强化学习通过建模因果关系为解决这些挑战提供了有前景的解决方案。

Method: 采用系统综述方法，对因果推断与强化学习交叉领域的最新进展进行分类分析，主要分为：因果表示学习、反事实策略优化、离线因果RL、因果迁移学习和因果可解释性五个类别。

Result: 通过结构化分析，识别了该领域的主要挑战，强调了在实际应用中的经验成功案例，并讨论了未解决的问题。因果强化学习在提升RL系统的鲁棒性、泛化能力和可解释性方面显示出巨大潜力。

Conclusion: 因果强化学习是发展鲁棒、可泛化和可解释人工智能系统的有前景方向，未来研究应继续探索这一交叉领域，解决现有挑战并推动实际应用。

Abstract: Integrating causal inference (CI) with reinforcement learning (RL) has emerged as a powerful paradigm to address critical limitations in classical RL, including low explainability, lack of robustness and generalization failures. Traditional RL techniques, which typically rely on correlation-driven decision-making, struggle when faced with distribution shifts, confounding variables, and dynamic environments. Causal reinforcement learning (CRL), leveraging the foundational principles of causal inference, offers promising solutions to these challenges by explicitly modeling cause-and-effect relationships. In this survey, we systematically review recent advancements at the intersection of causal inference and RL. We categorize existing approaches into causal representation learning, counterfactual policy optimization, offline causal RL, causal transfer learning, and causal explainability. Through this structured analysis, we identify prevailing challenges, highlight empirical successes in practical applications, and discuss open problems. Finally, we provide future research directions, underscoring the potential of CRL for developing robust, generalizable, and interpretable artificial intelligence systems.

</details>


### [20] [Propose, Solve, Verify: Self-Play Through Formal Verification](https://arxiv.org/abs/2512.18160)
*Alex Wilf,Pranjal Aggarwal,Bryan Parno,Daniel Fried,Louis-Philippe Morency,Paul Pu Liang,Sean Welleck*

Main category: cs.AI

TL;DR: PSV-Verus：通过形式验证驱动的自我博弈框架，在代码生成任务中实现显著性能提升，pass@1最高提升9.6倍


<details>
  <summary>Details</summary>
Motivation: 探索纯自我博弈（无需人类数据）训练大型语言模型的有效性，特别是在代码生成领域，传统基于单元测试的奖励机制脆弱且容易传播错误

Method: 提出Propose, Solve, Verify (PSV)框架：利用形式验证信号训练提议器生成具有挑战性的合成问题，并通过专家迭代训练求解器

Result: PSV-Verus在三个基准测试中，pass@1比仅推理和专家迭代基线提升高达9.6倍；性能随生成问题数量和训练迭代次数扩展

Conclusion: 形式验证和难度感知的提议机制是成功自我博弈的关键要素，PSV框架在验证代码生成场景中证明了纯自我博弈训练的有效性

Abstract: Training models through self-play alone (without any human data) has been a longstanding goal in AI, but its effectiveness for training large language models remains unclear, particularly in code generation where rewards based on unit tests are brittle and prone to error propagation. We study self-play in the verified code generation setting, where formal verification provides reliable correctness signals. We introduce Propose, Solve, Verify (PSV) a simple self-play framework where formal verification signals are used to create a proposer capable of generating challenging synthetic problems and a solver trained via expert iteration. We use PSV to train PSV-Verus, which across three benchmarks improves pass@1 by up to 9.6x over inference-only and expert-iteration baselines. We show that performance scales with the number of generated questions and training iterations, and through ablations identify formal verification and difficulty-aware proposal as essential ingredients for successful self-play.

</details>


### [21] [NEURO-GUARD: Neuro-Symbolic Generalization and Unbiased Adaptive Routing for Diagnostics -- Explainable Medical AI](https://arxiv.org/abs/2512.18177)
*Midhat Urooj,Ayan Banerjee,Sandeep Gupta*

Main category: cs.AI

TL;DR: NEURO-GUARD：一种结合视觉Transformer与语言驱动推理的知识引导视觉框架，通过检索增强生成机制进行自验证，在糖尿病视网膜病变分类和癫痫检测中显著提升准确性、可解释性和跨域鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前医学AI面临准确性与可解释性的平衡难题，特别是在数据有限、视觉线索细微、临床决策高风险的环境中。现有视觉模型多为黑盒预测，可解释性差且跨域泛化能力弱，限制了临床实际应用。

Method: 提出NEURO-GUARD框架，将视觉Transformer与语言驱动推理相结合，采用检索增强生成机制进行自验证。通过大语言模型迭代生成、评估和优化医学图像特征提取代码，并将此过程基于临床指南和专家知识，逐步提升特征检测和分类能力。

Result: 在糖尿病视网膜病变分类的四个基准数据集（APTOS、EyePACS、Messidor-1、Messidor-2）上，NEURO-GUARD相比纯ViT基线准确率提升6.2%（84.69% vs. 78.4%），跨域泛化能力提升5%。在MRI癫痫检测的进一步评估中也证实了其跨域鲁棒性，持续优于现有方法。

Conclusion: NEURO-GUARD成功将符号医学推理与亚符号视觉学习相结合，实现了可解释、知识感知且可泛化的医学图像诊断，在多个数据集上达到最先进性能，为医学AI的临床应用提供了有前景的解决方案。

Abstract: Accurate yet interpretable image-based diagnosis remains a central challenge in medical AI, particularly in settings characterized by limited data, subtle visual cues, and high-stakes clinical decision-making. Most existing vision models rely on purely data-driven learning and produce black-box predictions with limited interpretability and poor cross-domain generalization, hindering their real-world clinical adoption. We present NEURO-GUARD, a novel knowledge-guided vision framework that integrates Vision Transformers (ViTs) with language-driven reasoning to improve performance, transparency, and domain robustness. NEURO-GUARD employs a retrieval-augmented generation (RAG) mechanism for self-verification, in which a large language model (LLM) iteratively generates, evaluates, and refines feature-extraction code for medical images. By grounding this process in clinical guidelines and expert knowledge, the framework progressively enhances feature detection and classification beyond purely data-driven baselines. Extensive experiments on diabetic retinopathy classification across four benchmark datasets APTOS, EyePACS, Messidor-1, and Messidor-2 demonstrate that NEURO-GUARD improves accuracy by 6.2% over a ViT-only baseline (84.69% vs. 78.4%) and achieves a 5% gain in domain generalization. Additional evaluations on MRI-based seizure detection further confirm its cross-domain robustness, consistently outperforming existing methods.
  Overall, NEURO-GUARD bridges symbolic medical reasoning with subsymbolic visual learning, enabling interpretable, knowledge-aware, and generalizable medical image diagnosis while achieving state-of-the-art performance across multiple datasets.

</details>


### [22] [NL2CA: Auto-formalizing Cognitive Decision-Making from Natural Language Using an Unsupervised CriticNL2LTL Framework](https://arxiv.org/abs/2512.18189)
*Zihao Deng,Yijia Li,Renrui Zhang,Peijun Ye*

Main category: cs.AI

TL;DR: NL2CA：一种从自然语言描述自动形式化认知决策规则的新方法，通过LLM翻译为LTL逻辑，经无监督Critic Tree优化，转换为可执行的生产规则，构建认知智能体并通过认知强化学习优化


<details>
  <summary>Details</summary>
Motivation: 认知计算模型能形式化、可解释地表征人类决策过程，但开发过程劳动密集。需要一种从人类经验自然语言描述自动形式化认知决策规则的方法，实现完全自动化建模

Method: 1. 使用微调的大型语言模型将文本翻译为线性时序逻辑(LTL)；2. 通过无监督Critic Tree优化逻辑；3. 将输出转换为与符号认知框架兼容的可执行生产规则；4. 基于规则构建认知智能体，通过认知强化学习根据现实行为数据优化

Result: 1. NL-to-LTL翻译：CriticNL2LTL模块在专家和大规模基准测试中表现一致，无需人工干预；2. 认知驾驶模拟：从人类访谈自动构建的智能体成功学习了约70个不同关键场景中的多样化决策模式

Conclusion: NL2CA能够从非结构化文本数据实现可扩展、可解释且与人类对齐的认知建模，为自动设计符号认知智能体提供了新范式

Abstract: Cognitive computing models offer a formal and interpretable way to characterize human's deliberation and decision-making, yet their development remains labor-intensive. In this paper, we propose NL2CA, a novel method for auto-formalizing cognitive decision-making rules from natural language descriptions of human experience. Different from most related work that exploits either pure manual or human guided interactive modeling, our method is fully automated without any human intervention. The approach first translates text into Linear Temporal Logic (LTL) using a fine-tuned large language model (LLM), then refines the logic via an unsupervised Critic Tree, and finally transforms the output into executable production rules compatible with symbolic cognitive frameworks. Based on the resulted rules, a cognitive agent is further constructed and optimized through cognitive reinforcement learning according to the real-world behavioral data. Our method is validated in two domains: (1) NL-to-LTL translation, where our CriticNL2LTL module achieves consistent performance across both expert and large-scale benchmarks without human-in-the-loop feed-backs, and (2) cognitive driving simulation, where agents automatically constructed from human interviews have successfully learned the diverse decision patterns of about 70 trials in different critical scenarios. Experimental results demonstrate that NL2CA enables scalable, interpretable, and human-aligned cognitive modeling from unstructured textual data, offering a novel paradigm to automatically design symbolic cognitive agents.

</details>


### [23] [Scalably Enhancing the Clinical Validity of a Task Benchmark with Physician Oversight](https://arxiv.org/abs/2512.19691)
*Junze Ye,Daniel Tawfik,Alex J. Goodell,Nikhil V. Kotha,Mark K. Buyyounouski,Mohsen Bayati*

Main category: cs.AI

TL;DR: 该论文提出将临床风险评分计算的基准视为"进行中的活文档"，通过医生参与的系统化流程审计和重新标注MedCalc-Bench数据集，发现原始标签存在显著噪声，并证明使用修正标签进行RL训练能带来8.7%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 当前临床风险评分自动计算的评估基准MedCalc-Bench采用LLM特征提取和规则聚合构建，但将其视为静态标准会固化历史模型错误，特别是在RL训练中作为奖励信号时，这种问题会被放大。在安全关键领域，需要更可靠的基准评估方法。

Method: 提出系统化的医生参与流程，利用先进的代理验证器审计和重新标注MedCalc-Bench数据集，通过自动分类将稀缺的临床医生注意力保留给最有争议的实例。使用修正后的标签通过GRPO方法微调Qwen3-8B模型，验证标签噪声对RL训练的影响。

Result: 审计发现原始标签存在显著比例的医学真实值偏差，主要源于提取错误、计算器逻辑不匹配和临床模糊性。使用修正标签进行RL训练相比原始基线获得了8.7%的绝对准确率提升，验证了标签噪声对模型评估的实质性影响。

Conclusion: 在安全关键领域，严格的基准维护是真正模型对齐的前提条件。应将复杂任务的基准视为"进行中的活文档"，随着创建方法的改进而定期重新评估，避免将历史模型错误固化为评估黄金标准。

Abstract: Automating the calculation of clinical risk scores offers a significant opportunity to reduce physician administrative burden and enhance patient care. The current standard for evaluating this capability is MedCalc-Bench, a large-scale dataset constructed using LLM-based feature extraction and rule-based aggregation. However, treating such model-generated benchmarks as static oracles risks enshrining historical model errors as evaluation gold standards, a problem dangerously amplified when these datasets serve as reward signals for Reinforcement Learning (RL). In this work, we propose viewing benchmarks for complex tasks such as clinical score computation as ''in-progress living documents'' that should be periodically re-evaluated as the processes for creating them improve. We introduce a systematic, physician-in-the-loop pipeline that leverages advanced agentic verifiers to audit and relabel MedCalc-Bench, utilizing automated triage to reserve scarce clinician attention for the most contentious instances. Our audit reveals that a notable fraction of original labels diverge from medical ground truth due to extraction errors, calculator logic mismatches, and clinical ambiguity. To study whether this label noise meaningfully impacts downstream RL training, we fine-tune a Qwen3-8B model via Group Relative Policy Optimization (GRPO) and demonstrate that training on corrected labels yields an 8.7% absolute improvement in accuracy over the original baseline -- validating that label noise materially affects model evaluation. These findings underscore that in safety-critical domains, rigorous benchmark maintenance is a prerequisite for genuine model alignment.

</details>


### [24] [External Hippocampus: Topological Cognitive Maps for Guiding Large Language Model Reasoning](https://arxiv.org/abs/2512.18190)
*Jian Yan*

Main category: cs.AI

TL;DR: 提出External Hippocampus框架，从认知动力学视角建模语言模型推理，通过降维投影构建拓扑认知地图，实现推理过程中的能量流精确导航与干预，无需额外训练即可显著提升小模型多步推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统权重空间优化方法计算成本高，且难以在推理时进行精确干预。小模型在多步推理中容易出现"认知死锁"问题，需要一种高效可控的解决方案。

Method: 从认知动力学角度将语言模型推理建模为语义空间中的信息能量流动。通过降维投影构建拓扑认知地图，在测试时精确导航和干预能量流，利用温度扰动重启停滞的能量流动。

Result: 在≤7B参数模型上，地图引导方法在500个挑战性问题中达到81.20%准确率（相对基线+16.80%），推理时间减少≥15倍。发现推理停滞表现为"认知漩涡"和低熵势阱，温度扰动能有效重启能量流。

Conclusion: External Hippocampus框架为小模型推理提供了高效、可控的拓扑感知解决方案，无需额外训练，具有自主增长能力，能有效解决多步推理中的认知死锁问题。

Abstract: This paper proposes the External Hippocampus framework, which models language model reasoning from a cognitive dynamics perspective as the flow of information energy in semantic space. Unlike traditional weight-space optimization methods, this framework constructs topological cognitive maps through dimensionality reduction projection, enabling precise navigation and intervention of energy flow at test time while avoiding substantial computational requirements and demonstrating predictable intervention patterns. The method effectively addresses the cognitive deadlock problem in multi-step reasoning for small models. Experiments on models <=7B parameters show: map-guided methods achieve 81.20% accuracy on 500 challenging problems (relative baseline +16.80%), reduce reasoning time by >= 15x, with key findings revealing that reasoning stagnation manifests as "Cognitive Vortex" and low-entropy potential wells, while temperature perturbations effectively restart energy flow. The framework requires no additional training, possesses autonomous growth capability, and provides an efficient and controllable topological-aware solution for small model reasoning.

</details>


### [25] [Sophia: A Persistent Agent Framework of Artificial Life](https://arxiv.org/abs/2512.18202)
*Mingyang Sun,Feng Hong,Weinan Zhang*

Main category: cs.AI

TL;DR: 论文提出System 3架构作为LLM智能体的第三层，引入持久性身份和长期适应能力，通过Sophia原型展示如何将心理学概念转化为可计算模块，实现自主任务执行和推理效率提升。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体架构多为静态和反应式，局限于手动定义的狭窄场景，缺乏持久元层来维持身份、验证推理、对齐短期行动与长期生存。需要超越感知（System 1）和深思（System 2）的第三层系统。

Method: 提出System 3作为第三层架构，将心理学概念映射到具体计算模块。实现Sophia持久智能体包装器，包含四个协同机制：过程监督思维搜索、叙事记忆、用户与自我建模、混合奖励系统，形成持续自我改进循环。

Result: 定量：Sophia自主启动执行各种内在任务，对重复操作减少80%推理步骤；元认知持久性使高复杂度任务成功率提升40%。定性：System 3展现出连贯的叙事身份和内在任务组织能力。

Conclusion: 通过融合心理学洞见与轻量级强化学习核心，持久智能体架构为人工生命提供了可行的实践路径，使智能体从任务特定工具转变为具有身份连续性和透明行为解释的长寿命决策实体。

Abstract: The development of LLMs has elevated AI agents from task-specific tools to long-lived, decision-making entities. Yet, most architectures remain static and reactive, tethered to manually defined, narrow scenarios. These systems excel at perception (System 1) and deliberation (System 2) but lack a persistent meta-layer to maintain identity, verify reasoning, and align short-term actions with long-term survival. We first propose a third stratum, System 3, that presides over the agent's narrative identity and long-horizon adaptation. The framework maps selected psychological constructs to concrete computational modules, thereby translating abstract notions of artificial life into implementable design requirements. The ideas coalesce in Sophia, a "Persistent Agent" wrapper that grafts a continuous self-improvement loop onto any LLM-centric System 1/2 stack. Sophia is driven by four synergistic mechanisms: process-supervised thought search, narrative memory, user and self modeling, and a hybrid reward system. Together, they transform repetitive reasoning into a self-driven, autobiographical process, enabling identity continuity and transparent behavioral explanations. Although the paper is primarily conceptual, we provide a compact engineering prototype to anchor the discussion. Quantitatively, Sophia independently initiates and executes various intrinsic tasks while achieving an 80% reduction in reasoning steps for recurring operations. Notably, meta-cognitive persistence yielded a 40% gain in success for high-complexity tasks, effectively bridging the performance gap between simple and sophisticated goals. Qualitatively, System 3 exhibited a coherent narrative identity and an innate capacity for task organization. By fusing psychological insight with a lightweight reinforcement-learning core, the persistent agent architecture advances a possible practical pathway toward artificial life.

</details>


### [26] [MSC-180: A Benchmark for Automated Formal Theorem Proving from Mathematical Subject Classification](https://arxiv.org/abs/2512.18256)
*Sirui Li,Wangyue Lu,Xiaorui Shi,Ke Weng,Haozhe Sun,Minghe Yu,Tiancheng Zhang,Ge Yu,Hengyu Liu,Lun Du*

Main category: cs.AI

TL;DR: MSC-180是一个基于MSC2020数学学科分类的定理证明基准，包含180个形式化验证问题，覆盖60个数学分支，评估显示当前LLM定理证明器表现不佳（仅18.89%通过率），存在领域偏见和泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的定理证明器存在领域覆盖有限、数学推理泛化能力弱的问题，需要更全面、系统的评估基准来推动真正具有数学推理能力的AI系统发展。

Method: 提出MSC-180基准，基于MSC2020数学学科分类，从60个数学分支中各选取3个问题（共180个），涵盖本科到研究生难度，每个问题经过领域专家多轮验证和精炼确保形式化准确性。引入变异系数（CV）作为评估指标量化跨数学领域的性能变异性。

Result: 在pass@32设置下，最佳模型仅达到18.89%总体通过率，存在显著领域偏见（最大领域覆盖率41.7%）和难度差距（研究生级别问题通过率显著更低）。观察到的CV值是统计高变异性阈值的4-6倍，表明模型仍依赖训练语料的模式匹配，缺乏可迁移的推理机制和系统泛化能力。

Conclusion: MSC-180及其多维评估框架为驱动下一代具有真正数学推理能力的AI系统发展提供了区分性和系统性的基准，揭示了当前LLM定理证明器在数学推理方面的根本局限性。

Abstract: Automated Theorem Proving (ATP) represents a core research direction in artificial intelligence for achieving formal reasoning and verification, playing a significant role in advancing machine intelligence. However, current large language model (LLM)-based theorem provers suffer from limitations such as restricted domain coverage and weak generalization in mathematical reasoning. To address these issues, we propose MSC-180, a benchmark for evaluation based on the MSC2020 mathematical subject classification. It comprises 180 formal verification problems, 3 advanced problems from each of 60 mathematical branches, spanning from undergraduate to graduate levels. Each problem has undergone multiple rounds of verification and refinement by domain experts to ensure formal accuracy. Evaluations of state-of-the-art LLM-based theorem provers under the pass@32 setting reveal that the best model achieves only an 18.89% overall pass rate, with prominent issues including significant domain bias (maximum domain coverage 41.7%) and a difficulty gap (significantly lower pass rates on graduate-level problems). To further quantify performance variability across mathematical domains, we introduce the coefficient of variation (CV) as an evaluation metric. The observed CV values are 4-6 times higher than the statistical high-variability threshold, indicating that the models still rely on pattern matching from training corpora rather than possessing transferable reasoning mechanisms and systematic generalization capabilities. MSC-180, together with its multi-dimensional evaluation framework, provides a discriminative and systematic benchmark for driving the development of next-generation AI systems with genuine mathematical reasoning abilities.

</details>


### [27] [Intelligent Human-Machine Partnership for Manufacturing: Enhancing Warehouse Planning through Simulation-Driven Knowledge Graphs and LLM Collaboration](https://arxiv.org/abs/2512.18265)
*Himabindu Thogaru,Saisubramaniam Gopalakrishnan,Zishan Ahmad,Anirudh Deodhar*

Main category: cs.AI

TL;DR: 提出一个结合知识图谱和LLM代理的协作智能系统，让制造规划人员通过自然语言界面分析仿真数据，实现人机协作的瓶颈识别和运营分析。


<details>
  <summary>Details</summary>
Motivation: 传统基于仿真的制造数据分析方法在人类决策者和关键运营洞察之间制造了障碍，限制了制造规划中的有效合作。需要一种方法让制造专业人员能够自然地与运营洞察交互，无需专门知识。

Method: 开发了一个协作智能框架，整合知识图谱和基于大语言模型的代理。系统将仿真数据转换为语义丰富的表示，通过自然语言界面让规划人员交互。协作LLM代理与人类决策者并肩工作，采用迭代推理，生成精确的知识提取查询并提供透明验证。

Result: 对于运营查询，系统通过自然语言交互实现了接近完美的准确性。对于需要协作分析的调查场景，框架有效支持人类专家发现相互关联的运营问题，增强理解和决策能力。

Conclusion: 这项工作通过创建直观的方法来获取可操作的洞察，推进了协作制造，减少了认知负荷，同时在不断发展的制造生态系统中放大了人类的分析能力。

Abstract: Manufacturing planners face complex operational challenges that require seamless collaboration between human expertise and intelligent systems to achieve optimal performance in modern production environments. Traditional approaches to analyzing simulation-based manufacturing data often create barriers between human decision-makers and critical operational insights, limiting effective partnership in manufacturing planning. Our framework establishes a collaborative intelligence system integrating Knowledge Graphs and Large Language Model-based agents to bridge this gap, empowering manufacturing professionals through natural language interfaces for complex operational analysis. The system transforms simulation data into semantically rich representations, enabling planners to interact naturally with operational insights without specialized expertise. A collaborative LLM agent works alongside human decision-makers, employing iterative reasoning that mirrors human analytical thinking while generating precise queries for knowledge extraction and providing transparent validation. This partnership approach to manufacturing bottleneck identification, validated through operational scenarios, demonstrates enhanced performance while maintaining human oversight and decision authority. For operational inquiries, the system achieves near-perfect accuracy through natural language interaction. For investigative scenarios requiring collaborative analysis, we demonstrate the framework's effectiveness in supporting human experts to uncover interconnected operational issues that enhance understanding and decision-making. This work advances collaborative manufacturing by creating intuitive methods for actionable insights, reducing cognitive load while amplifying human analytical capabilities in evolving manufacturing ecosystems.

</details>


### [28] [Monitoring Monitorability](https://arxiv.org/abs/2512.18311)
*Melody Y. Guan,Miles Wang,Micah Carroll,Zehao Dou,Annie Y. Wei,Marcus Williams,Benjamin Arnav,Joost Huizinga,Ian Kivlichan,Mia Glaese,Jakub Pachocki,Bowen Baker*

Main category: cs.AI

TL;DR: 该论文提出评估AI系统决策可监控性的框架，发现思维链监控比仅监控行动更有效，且可监控性随推理计算增加而提升，强化学习优化不会显著降低可监控性。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统能力增强，需要监控其决策过程以确保安全部署。当前思维链监控虽有效，但其可监控性可能因不同训练过程、数据源或系统扩展而变得脆弱，需要系统评估方法。

Method: 提出三种评估原型（干预、过程、结果属性）和新可监控性指标，建立广泛评估套件。评估不同前沿模型的可监控性，研究推理计算、强化学习优化、预训练模型规模对可监控性的影响。

Result: 思维链监控比仅监控行动更有效；大多数前沿模型具有相当但非完美的可监控性；更长思维链通常更可监控；强化学习优化不会显著降低可监控性；弱监控器通过增加测试时计算可提升监控强代理的能力；后续问题可改善可监控性。

Conclusion: 思维链监控是确保AI系统安全部署的有效方法，可监控性可通过增加推理计算、使用后续问题等方式改善。监控器可扩展性趋势表明，即使面对强代理，弱监控器通过增加计算资源和访问思维链也能有效提升监控能力。

Abstract: Observability into the decision making of modern AI systems may be required to safely deploy increasingly capable agents. Monitoring the chain-of-thought (CoT) of today's reasoning models has proven effective for detecting misbehavior. However, this "monitorability" may be fragile under different training procedures, data sources, or even continued system scaling. To measure and track monitorability, we propose three evaluation archetypes (intervention, process, and outcome-property) and a new monitorability metric, and introduce a broad evaluation suite. We demonstrate that these evaluations can catch simple model organisms trained to have obfuscated CoTs, and that CoT monitoring is more effective than action-only monitoring in practical settings. We compare the monitorability of various frontier models and find that most models are fairly, but not perfectly, monitorable. We also evaluate how monitorability scales with inference-time compute, reinforcement learning optimization, and pre-training model size. We find that longer CoTs are generally more monitorable and that RL optimization does not materially decrease monitorability even at the current frontier scale. Notably, we find that for a model at a low reasoning effort, we could instead deploy a smaller model at a higher reasoning effort (thereby matching capabilities) and obtain a higher monitorability, albeit at a higher overall inference compute cost. We further investigate agent-monitor scaling trends and find that scaling a weak monitor's test-time compute when monitoring a strong agent increases monitorability. Giving the weak monitor access to CoT not only improves monitorability, but it steepens the monitor's test-time compute to monitorability scaling trend. Finally, we show we can improve monitorability by asking models follow-up questions and giving their follow-up CoT to the monitor.

</details>


### [29] [Few-Shot Learning of a Graph-Based Neural Network Model Without Backpropagation](https://arxiv.org/abs/2512.18412)
*Mykyta Lapin,Kostiantyn Bokhan,Yurii Parzhyn*

Main category: cs.AI

TL;DR: 提出一种基于结构图的方法，在少样本场景下对轮廓图像进行分类，无需反向传播。通过将图像编码为属性图，形成概念吸引子（类级概念图）实现泛化，提供可解释的决策。


<details>
  <summary>Details</summary>
Motivation: 设计一种无需反向传播的少样本学习架构，通过结构化和参数化约简从少量样本（每类5-6个）形成类概念，提供透明的决策过程。

Method: 轮廓矢量化后构建二分图（点/线作为节点），包含归一化的几何属性（坐标、长度、角度、方向）。通过消除不稳定子结构或噪声、对齐关键点之间的路径进行约简。通过样本迭代组合形成概念，使用近似图编辑距离进行图到概念的最佳匹配分类。

Result: 在MNIST子集上（每类5-6个基础样本，单轮训练），获得约82%的稳定准确率，决策完全可追溯：误分类可通过明确的结构相似性解释。与SVM、MLP、CNN以及度量和元学习基线进行了对比。

Conclusion: 基于概念吸引子的结构图方案实现了无需反向传播的少样本学习，并通过显式图结构提供内置解释。局限性包括图编辑距离的计算成本和骨架化质量；未来方向包括分类算法优化、静态场景处理和相关识别。

Abstract: We propose a structural-graph approach to classifying contour images in a few-shot regime without using backpropagation. The core idea is to make structure the carrier of explanations: an image is encoded as an attributed graph (critical points and lines represented as nodes with geometric attributes), and generalization is achieved via the formation of concept attractors (class-level concept graphs). Purpose. To design and experimentally validate an architecture in which class concepts are formed from a handful of examples (5 - 6 per class) through structural and parametric reductions, providing transparent decisions and eliminating backpropagation. Methods. Contour vectorization is followed by constructing a bipartite graph (Point/Line as nodes) with normalized geometric attributes such as coordinates, length, angle, and direction; reductions include the elimination of unstable substructures or noise and the alignment of paths between critical points. Concepts are formed by iterative composition of samples, and classification is performed by selecting the best graph-to-concept match (using approximated GED). Results. On an MNIST subset with 5 - 6 base examples per class (single epoch), we obtain a consistent accuracy of around 82% with full traceability of decisions: misclassifications can be explained by explicit structural similarities. An indicative comparison with SVM, MLP, CNN, as well as metric and meta-learning baselines, is provided. The structural-graph scheme with concept attractors enables few-shot learning without backpropagation and offers built-in explanations through the explicit graph structure. Limitations concern the computational cost of GED and the quality of skeletonization; promising directions include classification-algorithm optimization, work with static scenes, and associative recognition.

</details>


### [30] [Agent-Based Output Drift Detection for Breast Cancer Response Prediction in a Multisite Clinical Decision Support System](https://arxiv.org/abs/2512.18450)
*Xavier Rafael-Palou,Jose Munuera,Ana Jimenez-Pastor,Richard Osuala,Karim Lekadir,Oliver Diaz*

Main category: cs.AI

TL;DR: 提出基于代理的多站点临床AI系统漂移检测框架，通过站点特定监控代理实现分布漂移检测和严重性评估，在乳腺癌影像数据上验证优于集中式监控。


<details>
  <summary>Details</summary>
Motivation: 临床决策支持系统在多站点部署时，由于患者群体、成像硬件和采集协议的差异，预测性能可能下降。现有方法多为集中式监控，忽略了站点特定的漂移动态。

Method: 提出基于代理的框架，为每个站点分配漂移监控代理，进行批量模型输出与参考分布的对比。分析多种多中心监控方案（站点特定、全局、仅生产、自适应）和集中式基线。

Result: 在真实世界乳腺癌影像数据上，所有多中心方案均优于集中式监控，漂移检测F1分数提升高达10.3%。无站点特定参考时，自适应方案表现最佳，漂移检测F1分数74.3%，漂移严重性分类83.7%。

Conclusion: 自适应、站点感知的基于代理的漂移监控可以增强多站点临床决策支持系统的可靠性。

Abstract: Modern clinical decision support systems can concurrently serve multiple, independent medical imaging institutions, but their predictive performance may degrade across sites due to variations in patient populations, imaging hardware, and acquisition protocols. Continuous surveillance of predictive model outputs offers a safe and reliable approach for identifying such distributional shifts without ground truth labels. However, most existing methods rely on centralized monitoring of aggregated predictions, overlooking site-specific drift dynamics. We propose an agent-based framework for detecting drift and assessing its severity in multisite clinical AI systems. To evaluate its effectiveness, we simulate a multi-center environment for output-based drift detection, assigning each site a drift monitoring agent that performs batch-wise comparisons of model outputs against a reference distribution. We analyse several multi-center monitoring schemes, that differ in how the reference is obtained (site-specific, global, production-only and adaptive), alongside a centralized baseline. Results on real-world breast cancer imaging data using a pathological complete response prediction model shows that all multi-center schemes outperform centralized monitoring, with F1-score improvements up to 10.3% in drift detection. In the absence of site-specific references, the adaptive scheme performs best, with F1-scores of 74.3% for drift detection and 83.7% for drift severity classification. These findings suggest that adaptive, site-aware agent-based drift monitoring can enhance reliability of multisite clinical decision support systems.

</details>


### [31] [Insider Threat Detection Using GCN and Bi-LSTM with Explicit and Implicit Graph Representations](https://arxiv.org/abs/2512.18483)
*Rahul Yumlembam,Biju Issac,Seibu Mary Jacob,Longzhi Yang,Deepa Krishnan*

Main category: cs.AI

TL;DR: 提出一种结合显式和隐式图表示与时间建模的后处理内部威胁检测框架，通过GCN处理两种图结构，Bi-LSTM捕获时间依赖，在CERT数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 内部威胁检测面临挑战，因为恶意活动由可信用户执行，具有隐蔽性和微妙性。现有方法难以捕捉复杂的用户行为模式，需要结合多种表示来提升检测效果。

Method: 1. 构建显式图：基于组织规则建模用户活动间的直接关系
2. 学习隐式图：使用Gumbel-Softmax技巧从特征相似性中发现潜在行为关系
3. 分别用GCN处理两种图生成节点嵌入
4. 通过注意力机制拼接和精炼嵌入，强调威胁相关特征
5. 使用Bi-LSTM捕获用户行为的时间依赖性
6. 当概率分数低于阈值时标记为异常

Result: 在CERT r5.2数据集上：AUC 98.62，检测率100%，误报率0.05
在CERT r6.2数据集上：AUC 88.48，检测率80.15%，误报率0.15
优于现有最先进方法

Conclusion: 结合图表示和时间建模能有效提升内部威胁检测性能。显式和隐式图的融合能捕捉复杂行为模式，Bi-LSTM能捕获时间依赖，该框架在挑战性数据集上表现稳健。

Abstract: Insider threat detection (ITD) is challenging due to the subtle and concealed nature of malicious activities performed by trusted users. This paper proposes a post-hoc ITD framework that integrates explicit and implicit graph representations with temporal modelling to capture complex user behaviour patterns. An explicit graph is constructed using predefined organisational rules to model direct relationships among user activities. To mitigate noise and limitations in this hand-crafted structure, an implicit graph is learned from feature similarities using the Gumbel-Softmax trick, enabling the discovery of latent behavioural relationships. Separate Graph Convolutional Networks (GCNs) process the explicit and implicit graphs to generate node embeddings, which are concatenated and refined through an attention mechanism to emphasise threat-relevant features. The refined representations are then passed to a bidirectional Long Short-Term Memory (Bi-LSTM) network to capture temporal dependencies in user behaviour. Activities are flagged as anomalous when their probability scores fall below a predefined threshold. Extensive experiments on CERT r5.2 and r6.2 datasets demonstrate that the proposed framework outperforms state-of-the-art methods. On r5.2, the model achieves an AUC of 98.62, a detection rate of 100%, and a false positive rate of 0.05. On the more challenging r6.2 dataset, it attains an AUC of 88.48, a detection rate of 80.15%, and a false positive rate of 0.15, highlighting the effectiveness of combining graph-based and temporal representations for robust ITD.

</details>


### [32] [Large Language Models as Discounted Bayesian Filters](https://arxiv.org/abs/2512.18489)
*Jensen Zhang,Jing Yang,Keze Wang*

Main category: cs.AI

TL;DR: LLMs在动态随机环境中的在线推理能力存在系统性偏差，其信念更新类似于带指数遗忘的贝叶斯滤波器，而非完美贝叶斯推理。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注LLMs在静态任务中的表现，忽视了在动态随机环境中需要持续更新信念的在线适应能力，而这种能力对于LLMs作为世界模型或智能体至关重要。

Method: 引入贝叶斯滤波框架评估LLMs的在线推理能力，设计涵盖多元离散分布（如骰子投掷）和连续分布（如高斯过程）的概率探测套件，其中真实参数随时间变化。

Result: LLMs的信念更新虽然类似贝叶斯后验，但更准确地表现为指数遗忘滤波器，具有模型特定的折扣因子（小于1），显示对旧证据的系统性折扣。不同模型架构间差异显著，固有先验常存在校准错误，但更新机制本身保持结构化和原则性。

Conclusion: LLMs在动态环境中的推理存在系统性偏差，但更新机制具有原则性结构。通过提示策略可以以最小计算成本有效重新校准先验，为LLMs作为世界模型的应用提供重要见解。

Abstract: Large Language Models (LLMs) demonstrate strong few-shot generalization through in-context learning, yet their reasoning in dynamic and stochastic environments remains opaque. Prior studies mainly focus on static tasks and overlook the online adaptation required when beliefs must be continuously updated, which is a key capability for LLMs acting as world models or agents. We introduce a Bayesian filtering framework to evaluate online inference in LLMs. Our probabilistic probe suite spans both multivariate discrete distributions, such as dice rolls, and continuous distributions, such as Gaussian processes, where ground-truth parameters shift over time. We find that while LLM belief updates resemble Bayesian posteriors, they are more accurately characterized by an exponential forgetting filter with a model-specific discount factor smaller than one. This reveals systematic discounting of older evidence that varies significantly across model architectures. Although inherent priors are often miscalibrated, the updating mechanism itself remains structured and principled. We further validate these findings in a simulated agent task and propose prompting strategies that effectively recalibrate priors with minimal computational cost.

</details>


### [33] [Vox Deorum: A Hybrid LLM Architecture for 4X / Grand Strategy Game AI -- Lessons from Civilization V](https://arxiv.org/abs/2512.18564)
*John Chen,Sihan Cheng,Can Gurkan,Ryan Lay,Moez Salahuddin*

Main category: cs.AI

TL;DR: 本文提出Vox Deorum架构，将大语言模型与子系统结合，用于《文明V》等4X策略游戏，实现宏观战略推理与战术执行的分离。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言推理方面的能力使其在4X和大战略游戏中具有独特优势，能够实现更自然的人机交互（如协作和谈判）。然而，这些游戏的复杂性和长时程特性带来了挑战，同时延迟和成本因素也阻碍了LLM的实际部署。

Method: 在《文明V》游戏中使用Vox Populi模组，提出Vox Deorum混合架构（LLM+X）。采用分层技术设计，让LLM处理宏观战略推理，将战术执行委托给子系统（如算法AI或未来的强化学习AI）。

Result: 通过2,327场完整游戏验证，比较两个开源LLM与Vox Populi增强AI。结果显示LLM实现了有竞争力的端到端游戏玩法，同时展现出与算法AI显著不同的游戏风格，不同LLM之间也有明显差异。

Conclusion: 该工作为在商业4X游戏中集成LLM建立了可行的架构，为游戏设计和智能体AI研究开辟了新机会。

Abstract: Large Language Models' capacity to reason in natural language makes them uniquely promising for 4X and grand strategy games, enabling more natural human-AI gameplay interactions such as collaboration and negotiation. However, these games present unique challenges due to their complexity and long-horizon nature, while latency and cost factors may hinder LLMs' real-world deployment. Working on a classic 4X strategy game, Sid Meier's Civilization V with the Vox Populi mod, we introduce Vox Deorum, a hybrid LLM+X architecture. Our layered technical design empowers LLMs to handle macro-strategic reasoning, delegating tactical execution to subsystems (e.g., algorithmic AI or reinforcement learning AI in the future). We validate our approach through 2,327 complete games, comparing two open-source LLMs with a simple prompt against Vox Populi's enhanced AI. Results show that LLMs achieve competitive end-to-end gameplay while exhibiting play styles that diverge substantially from algorithmic AI and from each other. Our work establishes a viable architecture for integrating LLMs in commercial 4X games, opening new opportunities for game design and agentic AI research.

</details>


### [34] [ESearch-R1: Learning Cost-Aware MLLM Agents for Interactive Embodied Search via Reinforcement Learning](https://arxiv.org/abs/2512.18571)
*Weijie Zhou,Xuangtang Xiong,Ye Tian,Lijun Yue,Xinyu Wu,Wei Li,Chaoyang Zhao,Honghui Dong,Ming Tang,Jinqiao Wang,Zhengyou Zhang*

Main category: cs.AI

TL;DR: ESearch-R1：一个成本感知的具身推理框架，通过HC-GRPO算法统一交互对话、情景记忆检索和物理导航，在模糊指令下优化信息获取与异构成本之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）驱动的具身代理在面对模糊自然语言指令时，无法有效平衡物理探索的高成本与人类交互的认知成本。它们通常将消歧视为被动感知问题，缺乏最小化总任务执行成本的战略推理能力。

Method: 提出ESearch-R1框架，将交互对话（Ask）、情景记忆检索（GetMemory）和物理导航（Navigate）统一为单一决策过程。引入HC-GRPO（异构成本感知组相对策略优化）算法，通过采样推理轨迹组并强化那些在信息增益与异构成本（如导航时间、人类注意力）之间达到最优权衡的轨迹来优化MLLM。

Result: 在AI2-THOR环境中的大量实验表明，ESearch-R1显著优于基于ReAct的标准代理。它在提高任务成功率的同时，将总操作成本降低了约50%，验证了GRPO在使MLLM代理与物理世界约束对齐方面的有效性。

Conclusion: ESearch-R1框架通过成本感知的推理和HC-GRPO优化，使具身代理能够战略性地平衡信息获取与执行成本，在面对模糊指令时显著提高效率并降低总操作成本。

Abstract: Multimodal Large Language Models (MLLMs) have empowered embodied agents with remarkable capabilities in planning and reasoning. However, when facing ambiguous natural language instructions (e.g., "fetch the tool" in a cluttered room), current agents often fail to balance the high cost of physical exploration against the cognitive cost of human interaction. They typically treat disambiguation as a passive perception problem, lacking the strategic reasoning to minimize total task execution costs. To bridge this gap, we propose ESearch-R1, a cost-aware embodied reasoning framework that unifies interactive dialogue (Ask), episodic memory retrieval (GetMemory), and physical navigation (Navigate) into a single decision process. We introduce HC-GRPO (Heterogeneous Cost-Aware Group Relative Policy Optimization). Unlike traditional PPO which relies on a separate value critic, HC-GRPO optimizes the MLLM by sampling groups of reasoning trajectories and reinforcing those that achieve the optimal trade-off between information gain and heterogeneous costs (e.g., navigate time, and human attention). Extensive experiments in AI2-THOR demonstrate that ESearch-R1 significantly outperforms standard ReAct-based agents. It improves task success rates while reducing total operational costs by approximately 50\%, validating the effectiveness of GRPO in aligning MLLM agents with physical world constraints.

</details>


### [35] [Reflective Confidence: Correcting Reasoning Flaws via Online Self-Correction](https://arxiv.org/abs/2512.18605)
*Qinglin Zeng,Jing Yang,Keze Wang*

Main category: cs.AI

TL;DR: 提出反思置信度框架，将低置信度信号转为反思触发器而非终止信号，通过主动自我修正而非被动丢弃来提升推理效率


<details>
  <summary>Details</summary>
Motivation: 现有基于集成的方法（如自洽性）计算开销大，早期停止策略（如DeepConf）虽然降低成本但会丢弃不完整推理路径，浪费部分计算资源

Method: 提出反思置信度框架：当置信度低于阈值时，不停止生成，而是生成反思提示来分析当前推理状态、识别潜在错误，并沿着修正后的轨迹继续生成

Result: 在数学推理基准测试（包括AIME 2025）上，相比先进的早期停止基线方法，在可比计算成本下实现了显著的准确率提升

Conclusion: 主动自我修正策略比被动丢弃方法更有效，验证了将低置信度信号转化为反思触发器的价值

Abstract: Large language models (LLMs) have achieved strong performance on complex reasoning tasks using techniques such as chain-of-thought and self-consistency. However, ensemble-based approaches, especially self-consistency which relies on multiple reasoning trajectories, often incur substantial computational overhead. To improve efficiency, prior work has leveraged internal confidence signals, where early stopping strategies such as DeepConf reduce cost by terminating low-confidence trajectories. However, this strategy discards incomplete reasoning paths and wastes partial computation.
  We propose reflective confidence, a novel reasoning framework that transforms low-confidence signals from termination indicators into reflection triggers. When confidence falls below a threshold, instead of stopping generation, the model produces a reflection prompt to analyze the current reasoning state, identify potential errors, and continue generation along a corrected trajectory. Experiments on mathematical reasoning benchmarks, including AIME 2025, demonstrate significant accuracy improvements over advanced early-stopping baselines at comparable computational cost, validating the effectiveness of proactive self-correction over passive discarding.

</details>


### [36] [Assignment-Routing Optimization: Solvers for Problems Under Constraints](https://arxiv.org/abs/2512.18618)
*Yuan Qilong,Michal Pavelka*

Main category: cs.AI

TL;DR: 提出针对联合路由-分配问题的MIP求解器，在机器人包装规划中实现全局最优，计算时间稳定且显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 解决实际包装规划场景中的联合路由-分配问题，需要处理更丰富的约束条件，如多种占位符选项、时间限制和多类物品包装

Method: 扩展了基于Gurobi的精确MIP求解器，结合割平面子回路消除技术，开发了针对实际包装规划场景的定制求解器

Result: 在46个移动操作数据集上的实验表明，MIP方法实现了全局最优，计算时间稳定且低，比基于抖动的精确求解器快一个数量级；与贪心基线相比，保持最优距离，平均偏差为14%

Conclusion: MIP基础的JRA优化在机器人包装、运动规划和复杂物流中具有实际应用价值，兼顾效率和解质量

Abstract: We study the Joint Routing-Assignment (JRA) problem in which items must be assigned one-to-one to placeholders while simultaneously determining a Hamiltonian cycle visiting all nodes exactly once. Extending previous exact MIP solvers with Gurobi and cutting-plane subtour elimination, we develop a solver tailored for practical packaging-planning scenarios with richer constraints.These include multiple placeholder options, time-frame restrictions, and multi-class item packaging. Experiments on 46 mobile manipulation datasets demonstrate that the proposed MIP approach achieves global optima with stable and low computation times, significantly outperforming the shaking-based exact solver by up to an orders of magnitude. Compared to greedy baselines, the MIP solutions achieve consistent optimal distances with an average deviation of 14% for simple heuristics, confirming both efficiency and solution quality. The results highlight the practical applicability of MIP-based JRA optimization for robotic packaging, motion planning, and complex logistics .

</details>


### [37] [ChronoDreamer: Action-Conditioned World Model as an Online Simulator for Robotic Planning](https://arxiv.org/abs/2512.18619)
*Zhenhao Zhou,Dan Negrut*

Main category: cs.AI

TL;DR: ChronoDreamer是一个用于接触丰富机器人操作的动作条件世界模型，通过时空变换器和掩码预测训练，能预测未来视频帧、接触分布和关节角度，并使用VLM评估碰撞风险进行安全动作采样。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够预测接触丰富操作任务中未来状态的世界模型，特别是要处理刚性和可变形物体的复杂接触交互，同时确保动作执行的安全性。

Method: 使用时空变换器架构，采用MaskGIT风格的掩码预测训练；将接触编码为深度加权高斯溅射图像；在推理时使用视觉语言模型评估预测轨迹的碰撞可能性，进行拒绝采样。

Result: 模型在非接触运动中保持空间一致性，生成合理的接触预测；基于LLM的评估器能有效区分碰撞与非碰撞轨迹；在DreamerBench仿真数据集上进行了训练和评估。

Conclusion: ChronoDreamer展示了通过世界模型预测和VLM安全评估相结合的方法，能够实现接触丰富操作任务的安全规划，为机器人操作提供了新的解决方案。

Abstract: We present ChronoDreamer, an action-conditioned world model for contact-rich robotic manipulation. Given a history of egocentric RGB frames, contact maps, actions, and joint states, ChronoDreamer predicts future video frames, contact distributions, and joint angles via a spatial-temporal transformer trained with MaskGIT-style masked prediction. Contact is encoded as depth-weighted Gaussian splat images that render 3D forces into a camera-aligned format suitable for vision backbones. At inference, predicted rollouts are evaluated by a vision-language model that reasons about collision likelihood, enabling rejection sampling of unsafe actions before execution. We train and evaluate on DreamerBench, a simulation dataset generated with Project Chrono that provides synchronized RGB, contact splat, proprioception, and physics annotations across rigid and deformable object scenarios. Qualitative results demonstrate that the model preserves spatial coherence during non-contact motion and generates plausible contact predictions, while the LLM-based judge distinguishes collision from non-collision trajectories.

</details>


### [38] [ASTIF: Adaptive Semantic-Temporal Integration for Cryptocurrency Price Forecasting](https://arxiv.org/abs/2512.18661)
*Hafiz Saif Ur Rehman,Ling Liu,Kaleem Ullah Qasim*

Main category: cs.AI

TL;DR: ASTIF是一个用于加密货币价格预测的自适应语义-时间集成框架，通过置信度元学习实时调整预测策略，融合语言模型提取的市场语义信号和传统时序模型，在非平稳环境中优于现有基准方法。


<details>
  <summary>Details</summary>
Motivation: 现有金融时间序列预测模型大多采用静态架构，难以整合异构知识源或适应快速的市场状态切换。传统方法仅依赖历史价格序列，忽略了政策不确定性和市场叙事等语义驱动因素。

Method: 提出ASTIF混合智能系统：1) 使用MirrorPrompt的双通道小语言模型提取语义市场信号和数值趋势；2) 混合LSTM随机森林模型捕捉序列时间依赖性；3) 置信度感知元学习器作为自适应推理层，根据实时不确定性调节各预测器的贡献。

Result: 在2020-2024年AI相关加密货币和主要科技股的多样化数据集上，ASTIF优于领先的深度学习和Transformer基线（如Informer、TFT）。消融研究证实了自适应元学习机制的关键作用，能在市场动荡期间通过语义和时间通道间的依赖转移成功降低风险。

Conclusion: 该研究为非平稳环境中融合定量和定性数据提供了一个可扩展的、基于知识的解决方案，通过自适应集成语义和时间信息来提升金融时间序列预测性能。

Abstract: Financial time series forecasting is fundamentally an information fusion challenge, yet most existing models rely on static architectures that struggle to integrate heterogeneous knowledge sources or adjust to rapid regime shifts. Conventional approaches, relying exclusively on historical price sequences, often neglect the semantic drivers of volatility such as policy uncertainty and market narratives. To address these limitations, we propose the ASTIF (Adaptive Semantic-Temporal Integration for Cryptocurrency Price Forecasting), a hybrid intelligent system that adapts its forecasting strategy in real time through confidence-based meta-learning. The framework integrates three complementary components. A dual-channel Small Language Model using MirrorPrompt extracts semantic market cues alongside numerical trends. A hybrid LSTM Random Forest model captures sequential temporal dependencies. A confidence-aware meta-learner functions as an adaptive inference layer, modulating each predictor's contribution based on its real-time uncertainty.
  Experimental evaluation on a diverse dataset of AI-focused cryptocurrencies and major technology stocks from 2020 to 2024 shows that ASTIF outperforms leading deep learning and Transformer baselines (e.g., Informer, TFT). The ablation studies further confirm the critical role of the adaptive meta-learning mechanism, which successfully mitigates risk by shifting reliance between semantic and temporal channels during market turbulence. The research contributes a scalable, knowledge-based solution for fusing quantitative and qualitative data in non-stationary environments.

</details>


### [39] [Automatic Adaptation to Concept Complexity and Subjective Natural Concepts: A Cognitive Model based on Chunking](https://arxiv.org/abs/2512.18665)
*Dmitry Bennett,Fernand Gobet*

Main category: cs.AI

TL;DR: CogAct计算模型通过组块化机制解释概念学习，能自适应学习从简单逻辑到文学、国际象棋、音乐等真实复杂概念，相比其他模型更具适应性，并提供了考虑主观性的实验设计方法。


<details>
  <summary>Details</summary>
Motivation: 认知科学需要解释概念形成和检索的基本心理过程，特别是组块化机制在短时和长时记忆中的作用。现有模型难以自适应学习真实复杂概念，且缺乏考虑个体主观性的方法。

Method: 提出CogAct计算模型，基于组块化、注意、STM和LTM等基本认知过程。模型能自动适应学习多种概念类型，包括简单逻辑函数、人工类别、文学、国际象棋和音乐的原始数据。设计考虑主观性的人类基准实验，控制个体经验差异。

Result: CogAct能自适应学习多种复杂概念，包括从原始音乐数据学习而不依赖预建知识结构。模型模拟能捕捉人类主观判断，与深度学习模型相比表现良好。提供了从平均参与者建模转向捕捉主观概念空间的方法。

Conclusion: 组块化机制在概念学习中起核心作用，CogAct模型将概念学习和复杂性适应整合到更广泛的认知心理学理论中。该方法可用于心理学应用，更好地捕捉个体主观概念空间。

Abstract: A key issue in cognitive science concerns the fundamental psychological processes that underlie the formation and retrieval of multiple types of concepts in short-term and long-term memory (STM and LTM, respectively). We propose that chunking mechanisms play an essential role and show how the CogAct computational model grounds concept learning in fundamental cognitive processes and structures (such as chunking, attention, STM and LTM). First are the in-principle demonstrations, with CogAct automatically adapting to learn a range of categories from simple logical functions, to artificial categories, to natural raw (as opposed to natural pre-processed) concepts in the dissimilar domains of literature, chess and music. This kind of adaptive learning is difficult for most other psychological models, e.g., with cognitive models stopping at modelling artificial categories and (non-GPT) models based on deep learning requiring task-specific changes to the architecture. Secondly, we offer novel ways of designing human benchmarks for concept learning experiments and simulations accounting for subjectivity, ways to control for individual human experiences, all while keeping to real-life complex categories. We ground CogAct in simulations of subjective conceptual spaces of individual human participants, capturing humans subjective judgements in music, with the models learning from raw music score data without bootstrapping to pre-built knowledge structures. The CogAct simulations are compared to those obtained by a deep-learning model. These findings integrate concept learning and adaptation to complexity into the broader theories of cognitive psychology. Our approach may also be used in psychological applications that move away from modelling the average participant and towards capturing subjective concept space.

</details>


### [40] [IntelliCode: A Multi-Agent LLM Tutoring System with Centralized Learner Modeling](https://arxiv.org/abs/2512.18669)
*Jones David,Shreya Ghosh*

Main category: cs.AI

TL;DR: IntelliCode是一个基于多智能体LLM的智能辅导系统，通过中心化的版本化学习者状态模型，实现长期、透明、可审计的教学支持。


<details>
  <summary>Details</summary>
Motivation: 现有LLM辅导系统通常是单轮对话助手，缺乏对学习者知识的持久表示，难以提供有原则、透明和长期的教学支持。

Method: 构建多智能体LLM辅导系统，围绕中心化版本化学习者状态（包含掌握程度估计、误解、复习计划、参与度信号），通过StateGraph Orchestrator协调六个专门智能体：技能评估、学习者画像、渐进提示、课程选择、间隔重复、参与度监控。

Result: 系统演示展示了端到端辅导流程，通过模拟学习者验证显示：稳定的状态更新、渐进提示提高任务成功率、多样化的课程覆盖。

Conclusion: IntelliCode展示了如何将持久学习者建模、协调的多智能体推理和有原则的教学设计相结合，产生透明可靠的LLM驱动辅导系统。

Abstract: LLM-based tutors are typically single-turn assistants that lack persistent representations of learner knowledge, making it difficult to provide principled, transparent, and long-term pedagogical support. We introduce IntelliCode, a multi-agent LLM tutoring system built around a centralized, versioned learner state that integrates mastery estimates, misconceptions, review schedules, and engagement signals. A StateGraph Orchestrator coordinates six specialized agents: skill assessment, learner profiling, graduated hinting, curriculum selection, spaced repetition, and engagement monitoring, each operating as a pure transformation over the shared state under a single-writer policy. This architecture enables auditable mastery updates, proficiency-aware hints, dependency-aware curriculum adaptation, and safety-aligned prompting.
  The demo showcases an end-to-end tutoring workflow: a learner attempts a DSA problem, receives a conceptual hint when stuck, submits a corrected solution, and immediately sees mastery updates and a personalized review interval. We report validation results with simulated learners, showing stable state updates, improved task success with graduated hints, and diverse curriculum coverage. IntelliCode demonstrates how persistent learner modeling, orchestrated multi-agent reasoning, and principled instructional design can be combined to produce transparent and reliable LLM-driven tutoring.

</details>


### [41] [Social Comparison without Explicit Inference of Others' Reward Values: A Constructive Approach Using a Probabilistic Generative Model](https://arxiv.org/abs/2512.18687)
*Yosuke Taniuchi,Chie Hieida,Atsushi Noritake,Kazushi Ikeda,Masaki Isoda*

Main category: cs.AI

TL;DR: 猴子社会比较研究：通过计算模型分析发现，猴子更依赖客观奖励差异而非推断同伴主观价值来进行社会比较


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解从计算角度，关于他人奖励的信息如何影响个体对自己奖励的评估。具体探究猴子是仅仅识别客观奖励差异，还是推断他人的主观奖励估值。

Method: 开发了三种不同社会信息处理程度的计算模型：IPM（推断同伴主观价值）、NCM（忽略同伴信息）、ECM（直接纳入同伴客观奖励）。使用多层多模态潜在狄利克雷分配方法，在包含猴子行为、奖励和条件刺激的数据集上训练模型，评估模型在预定义实验条件下分类主观价值的能力。

Result: ECM在Rand Index上获得了最高的分类分数（0.88 vs. IPM的0.79），表明社会比较依赖于客观奖励差异，而非对主观状态的推断。

Conclusion: 从计算视角看，猴子的社会比较过程主要基于客观奖励差异，而不是推断他人的主观价值评估。这为理解灵长类社会认知提供了新的计算框架。

Abstract: Social comparison -- the process of evaluating one's rewards relative to others -- plays a fundamental role in primate social cognition. However, it remains unknown from a computational perspective how information about others' rewards affects the evaluation of one's own reward. With a constructive approach, this study examines whether monkeys merely recognize objective reward differences or, instead, infer others' subjective reward valuations. We developed three computational models with varying degrees of social information processing: an Internal Prediction Model (IPM), which infers the partner's subjective values; a No Comparison Model (NCM), which disregards partner information; and an External Comparison Model (ECM), which directly incorporates the partner's objective rewards. To test model performance, we used a multi-layered, multimodal latent Dirichlet allocation. We trained the models on a dataset containing the behavior of a pair of monkeys, their rewards, and the conditioned stimuli. Then, we evaluated the models' ability to classify subjective values across pre-defined experimental conditions. The ECM achieved the highest classification score in the Rand Index (0.88 vs. 0.79 for the IPM) under our settings, suggesting that social comparison relies on objective reward differences rather than inferences about subjective states.

</details>


### [42] [KeenKT: Knowledge Mastery-State Disambiguation for Knowledge Tracing](https://arxiv.org/abs/2512.18709)
*Zhifei Li,Lifan Chen,Jiali Yi,Xiaoju Hou,Yue Zhao,Wenxin Huang,Miao Zhang,Kui Xiao,Bing Yang*

Main category: cs.AI

TL;DR: KeenKT模型使用NIG分布表示学生知识状态，通过注意力机制和对比学习解决知识追踪中的状态模糊问题，显著提升预测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪方法依赖单点估计，无法区分学生真实能力与偶然表现（如爆发或粗心），导致对知识掌握状态的判断存在模糊性。

Method: 1) 使用正态逆高斯(NIG)分布表示每个交互时的知识状态；2) 设计基于NIG距离的注意力机制建模状态动态演化；3) 引入扩散去噪重构损失和分布对比学习损失增强鲁棒性。

Result: 在六个公开数据集上，KeenKT在预测准确性和行为波动敏感性方面优于现有SOTA模型，AUC最大提升5.85%，ACC最大提升6.89%。

Conclusion: KeenKT通过分布表示和对比学习有效解决了知识追踪中的状态模糊问题，能更准确地捕捉学生学习行为波动，提升预测性能。

Abstract: Knowledge Tracing (KT) aims to dynamically model a student's mastery of knowledge concepts based on their historical learning interactions. Most current methods rely on single-point estimates, which cannot distinguish true ability from outburst or carelessness, creating ambiguity in judging mastery. To address this issue, we propose a Knowledge Mastery-State Disambiguation for Knowledge Tracing model (KeenKT), which represents a student's knowledge state at each interaction using a Normal-Inverse-Gaussian (NIG) distribution, thereby capturing the fluctuations in student learning behaviors. Furthermore, we design an NIG-distance-based attention mechanism to model the dynamic evolution of the knowledge state. In addition, we introduce a diffusion-based denoising reconstruction loss and a distributional contrastive learning loss to enhance the model's robustness. Extensive experiments on six public datasets demonstrate that KeenKT outperforms SOTA KT models in terms of prediction accuracy and sensitivity to behavioral fluctuations. The proposed method yields the maximum AUC improvement of 5.85% and the maximum ACC improvement of 6.89%.

</details>


### [43] [Counterfactual Basis Extension and Representational Geometry: An MDL-Constrained Model of Conceptual Growth](https://arxiv.org/abs/2512.18732)
*Chainarong Amornbunchornvej*

Main category: cs.AI

TL;DR: 论文提出一个几何框架，将概念增长建模为在最小描述长度准则下评估的可容许基扩展，认为概念发展是误差驱动、几何约束的基扩展过程。


<details>
  <summary>Details</summary>
Motivation: 现有学习和推理模型通常预设固定的表征基础，但概念学习只有在现有表征无法解释经验时才可能发生。本文探讨一个更根本的问题：在什么结构条件下，表征基础本身能够以原则性和选择性的方式扩展？

Method: 提出几何框架，将概念增长建模为在最小描述长度准则下评估的可容许基扩展。经验（外部观察或内部模拟）表示为相对于当前概念子空间的向量，残差分量捕捉系统性表征失败，候选概念扩展被限制为低秩、可容许的变换。

Result: 证明任何MDL接受的扩展都可以选择使其新方向完全位于经验诱导的残差跨度内，而正交于此跨度的扩展会严格增加描述长度因此被拒绝。这为想象力和概念创新提供了保守的解释。

Conclusion: 该框架将概念发展描述为误差驱动、几何约束的基扩展过程，阐明了想象力在学习和理论变革中的作用和限制，为表征变化提供了规范性选择原则。

Abstract: Concept learning becomes possible only when existing representations fail to account for experience. Most models of learning and inference, however, presuppose a fixed representational basis within which belief updating occurs. In this paper, I address a prior question: under what structural conditions can the representational basis itself expand in a principled and selective way?
  I propose a geometric framework in which conceptual growth is modeled as admissible basis extension evaluated under a Minimum Description Length (MDL) criterion. Experience, whether externally observed or internally simulated, is represented as vectors relative to a current conceptual subspace. Residual components capture systematic representational failure, and candidate conceptual extensions are restricted to low-rank, admissible transformations. I show that any MDL-accepted extension can be chosen so that its novel directions lie entirely within the residual span induced by experience, while extensions orthogonal to this span strictly increase description length and are therefore rejected.
  This yields a conservative account of imagination and conceptual innovation. Internally generated counterfactual representations contribute to learning only insofar as they expose or amplify structured residual error, and cannot introduce arbitrary novelty. I further distinguish representational counterfactuals--counterfactuals over an agent's conceptual basis--from causal or value-level counterfactuals, and show how MDL provides a normative selection principle governing representational change.
  Overall, the framework characterizes conceptual development as an error-driven, geometry-constrained process of basis extension, clarifying both the role and the limits of imagination in learning and theory change.

</details>


### [44] [MEEA: Mere Exposure Effect-Driven Confrontational Optimization for LLM Jailbreaking](https://arxiv.org/abs/2512.18755)
*Jianyi Zhang,Shizhao Liu,Ziyin Zhou,Zhen Li*

Main category: cs.AI

TL;DR: 提出MEEA攻击框架，利用心理学曝光效应通过多轮低毒性语义暴露逐步侵蚀LLM安全边界，显著提升攻击成功率


<details>
  <summary>Details</summary>
Motivation: 现有越狱研究大多假设静态安全边界，未能考虑上下文交互对模型行为的动态影响，导致攻击稳定性和泛化性有限

Method: 基于曝光效应构建语义渐进提示链，采用模拟退火策略优化提示，结合语义相似性、毒性和越狱效果进行指导

Result: 在GPT-4、Claude-3.5、DeepSeek-R1等模型上测试，MEEA攻击成功率比7个基线平均提升超过20%

Conclusion: LLM安全行为本质上是动态且历史依赖的，挑战了静态对齐边界的常见假设，需要交互感知的安全评估和防御机制

Abstract: The rapid advancement of large language models (LLMs) has intensified concerns about the robustness of their safety alignment. While existing jailbreak studies explore both single-turn and multi-turn strategies, most implicitly assume a static safety boundary and fail to account for how contextual interactions dynamically influence model behavior, leading to limited stability and generalization. Motivated by this gap, we propose MEEA (Mere Exposure Effect Attack), a psychology-inspired, fully automated black-box framework for evaluating multi-turn safety robustness, grounded in the mere exposure effect. MEEA leverages repeated low-toxicity semantic exposure to induce a gradual shift in a model's effective safety threshold, enabling progressive erosion of alignment constraints over sustained interactions. Concretely, MEEA constructs semantically progressive prompt chains and optimizes them using a simulated annealing strategy guided by semantic similarity, toxicity, and jailbreak effectiveness. Extensive experiments on both closed-source and open-source models, including GPT-4, Claude-3.5, and DeepSeek-R1, demonstrate that MEEA consistently achieves higher attack success rates than seven representative baselines, with an average Attack Success Rate (ASR) improvement exceeding 20%. Ablation studies further validate the necessity of both annealing-based optimization and contextual exposure mechanisms. Beyond improved attack effectiveness, our findings indicate that LLM safety behavior is inherently dynamic and history-dependent, challenging the common assumption of static alignment boundaries and highlighting the need for interaction-aware safety evaluation and defense mechanisms. Our code is available at: https://github.com/Carney-lsz/MEEA

</details>


### [45] [The Dead Salmons of AI Interpretability](https://arxiv.org/abs/2512.18792)
*Maxime Méloux,Giada Dirupo,François Portet,Maxime Peyrard*

Main category: cs.AI

TL;DR: 论文提出将AI可解释性方法视为统计模型参数估计器，强调需要对抗性假设检验和不确定性量化，以避免"死鲑鱼"式的虚假发现。


<details>
  <summary>Details</summary>
Motivation: AI可解释性领域存在类似神经科学中"死鲑鱼"的虚假发现问题——即使对随机初始化的神经网络，特征归因、探测、稀疏自编码等方法也能产生看似合理的解释。这暴露了当前可解释性方法缺乏严谨统计框架的根本缺陷。

Method: 提出统计-因果重构框架：将计算系统的解释视为统计模型参数，从计算轨迹中推断。强调解释方法应作为统计估计器，需要对抗明确的替代计算假设进行检验，并在假设的统计模型下量化不确定性。

Result: 该框架揭示了可解释性查询的可识别性等关键理论问题，这些问题是领域易受虚假发现、泛化性差和高方差影响的关键。将可解释性置于统计推断标准工具箱中，为建立严谨科学奠定了基础。

Conclusion: 将AI可解释性重新定位为统计推断问题，为解决虚假发现、提高方法严谨性和可靠性提供了有前景的途径，有望将可解释性转变为实用且严谨的科学。

Abstract: In a striking neuroscience study, the authors placed a dead salmon in an MRI scanner and showed it images of humans in social situations. Astonishingly, standard analyses of the time reported brain regions predictive of social emotions. The explanation, of course, was not supernatural cognition but a cautionary tale about misapplied statistical inference. In AI interpretability, reports of similar ''dead salmon'' artifacts abound: feature attribution, probing, sparse auto-encoding, and even causal analyses can produce plausible-looking explanations for randomly initialized neural networks. In this work, we examine this phenomenon and argue for a pragmatic statistical-causal reframing: explanations of computational systems should be treated as parameters of a (statistical) model, inferred from computational traces. This perspective goes beyond simply measuring statistical variability of explanations due to finite sampling of input data; interpretability methods become statistical estimators, and findings should be tested against explicit and meaningful alternative computational hypotheses, with uncertainty quantified with respect to the postulated statistical model. It also highlights important theoretical issues, such as the identifiability of common interpretability queries, which we argue is critical to understand the field's susceptibility to false discoveries, poor generalizability, and high variance. More broadly, situating interpretability within the standard toolkit of statistical inference opens promising avenues for future work aimed at turning AI interpretability into a pragmatic and rigorous science.

</details>


### [46] [HARBOR: Holistic Adaptive Risk assessment model for BehaviORal healthcare](https://arxiv.org/abs/2512.18829)
*Aditya Siddhant*

Main category: cs.AI

TL;DR: HARBOR模型在行为健康风险评估中优于传统方法和现成LLM，PEARL数据集提供多模态纵向数据支持


<details>
  <summary>Details</summary>
Motivation: 行为健康风险评估面临多模态数据和时间动态性的挑战，现有LLM在结构化临床风险评分中的效果不明确

Method: 提出HARBOR行为健康感知语言模型，预测-3（重度抑郁）到+3（躁狂）的Harbor风险评分；发布PEARL纵向行为健康数据集

Result: HARBOR达到69%准确率，优于逻辑回归的54%和最强专有LLM基线的29%

Conclusion: HARBOR在行为健康风险评估中表现优异，为临床风险评分提供了有效的解决方案

Abstract: Behavioral healthcare risk assessment remains a challenging problem due to the highly multimodal nature of patient data and the temporal dynamics of mood and affective disorders. While large language models (LLMs) have demonstrated strong reasoning capabilities, their effectiveness in structured clinical risk scoring remains unclear. In this work, we introduce HARBOR, a behavioral health aware language model designed to predict a discrete mood and risk score, termed the Harbor Risk Score (HRS), on an integer scale from -3 (severe depression) to +3 (mania). We also release PEARL, a longitudinal behavioral healthcare dataset spanning four years of monthly observations from three patients, containing physiological, behavioral, and self reported mental health signals. We benchmark traditional machine learning models, proprietary LLMs, and HARBOR across multiple evaluation settings and ablations. Our results show that HARBOR outperforms classical baselines and off the shelf LLMs, achieving 69 percent accuracy compared to 54 percent for logistic regression and 29 percent for the strongest proprietary LLM baseline.

</details>


### [47] [CORE: Concept-Oriented Reinforcement for Bridging the Definition-Application Gap in Mathematical Reasoning](https://arxiv.org/abs/2512.18857)
*Zijun Gao,Zhikun Xu,Xiao Ye,Ben Zhou*

Main category: cs.AI

TL;DR: CORE是一个强化学习框架，通过将明确概念转化为可控监督信号，解决LLMs在数学问题中模式复用而非概念理解的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在解决数学练习时往往只是模式复用，缺乏真正的概念理解。现有的RLVR方法主要强化最终答案，但缺乏细粒度的概念信号，导致模型改进的是模式复用能力而非概念应用能力。

Method: CORE框架包含三个核心步骤：(1) 合成概念对齐的测验；(2) 在rollout过程中注入简短概念片段，引出概念引导的轨迹；(3) 通过轨迹替换（组失败后）强化概念推理，使用轻量级前向KL约束对齐无引导与概念引导策略，或直接在概念对齐测验上使用标准GRPO。

Result: 在多个模型上，CORE在领域内概念练习套件和多样化的领域外数学基准测试中都取得了比原始模型和SFT基线更一致的性能提升。

Conclusion: CORE通过统一概念对齐测验的直接训练和概念注入rollout，在结果正则化下提供了细粒度的概念监督，弥合了问题解决能力和真正概念推理之间的差距，同时保持算法和验证器的无关性。

Abstract: Large language models (LLMs) often solve challenging math exercises yet fail to apply the concept right when the problem requires genuine understanding. Popular Reinforcement Learning with Verifiable Rewards (RLVR) pipelines reinforce final answers but provide little fine-grained conceptual signal, so models improve at pattern reuse rather than conceptual applications. We introduce CORE (Concept-Oriented REinforcement), an RL training framework that turns explicit concepts into a controllable supervision signal. Starting from a high-quality, low-contamination textbook resource that links verifiable exercises to concise concept descriptions, we run a sanity probe showing LLMs can restate definitions but fail concept-linked quizzes, quantifying the conceptual reasoning gap. CORE then (i) synthesizes concept-aligned quizzes, (ii) injects brief concept snippets during rollouts to elicit concept-primed trajectories, and (iii) reinforces conceptual reasoning via trajectory replacement after group failures, a lightweight forward-KL constraint that aligns unguided with concept-primed policies, or standard GRPO directly on concept-aligned quizzes. Across several models, CORE delivers consistent gains over vanilla and SFT baselines on both in-domain concept-exercise suites and diverse out-of-domain math benchmarks. CORE unifies direct training on concept-aligned quizzes and concept-injected rollouts under outcome regularization. It provides fine-grained conceptual supervision that bridges problem-solving competence and genuine conceptual reasoning, while remaining algorithm- and verifier-agnostic.

</details>


### [48] [Gabliteration: Adaptive Multi-Directional Neural Weight Modification for Selective Behavioral Alteration in Large Language Models](https://arxiv.org/abs/2512.18901)
*Gökdeniz Gülmez*

Main category: cs.AI

TL;DR: Gabliteration是一种新颖的神经权重修改技术，通过自适应多向投影和正则化层选择，在修改特定行为模式时最小化模型质量损失。


<details>
  <summary>Details</summary>
Motivation: 现有权重修改方法在尝试修改特定行为模式时通常会损害模型整体质量，需要一种既能精确修改目标行为又能保持模型其他领域性能的技术。

Method: 采用动态层优化、正则化投影矩阵和自适应缩放机制，实现自适应多向投影与正则化层选择，理论上实现更优的权重修改。

Result: 开发了gabliterated-v1模型系列（0.6B到4B参数）并在Hugging Face上发布，验证了该方法在不同模型规模上的实际应用性。

Conclusion: Gabliteration技术超越了传统的权重修改方法，能够在修改特定行为模式的同时最小化对模型其他领域性能的影响，具有实际应用价值。

Abstract: We present Gabliteration, a novel neural weight modification technique that advances beyond traditional abliteration methods by implementing adaptive multi-directional projections with regularized layer selection. Our approach addresses the fundamental limitation of existing methods that compromise model quality while attempting to modify specific behavioral patterns. Through dynamic layer optimization, regularized projection matrices, and adaptive scaling mechanisms, we achieve theoretically superior weight modification while minimizing quality degradation in unrelated domains. We validate our method through the gabliterated-v1 model series (0.6B to 4B parameters) available on Hugging Face, demonstrating practical applicability across multiple model scales.

</details>


### [49] [Multimodal Bayesian Network for Robust Assessment of Casualties in Autonomous Triage](https://arxiv.org/abs/2512.18908)
*Szymon Rusiecki,Cecilia G. Morales,Kimberly Elenberg,Leonard Weiss,Artur Dubrawski*

Main category: cs.AI

TL;DR: 该研究提出了一种基于专家知识构建的贝叶斯网络决策支持框架，用于大规模伤亡事件中的自动伤员分类，通过融合多个计算机视觉模型的输出，显著提升了分类准确率和诊断覆盖率。


<details>
  <summary>Details</summary>
Motivation: 大规模伤亡事件会压垮紧急医疗系统，导致伤员评估延迟或错误，造成可预防的死亡。需要开发能够支持急救人员的自动分类系统。

Method: 提出一个决策支持框架，将多个计算机视觉模型（评估严重出血、呼吸窘迫、身体警觉性或可见创伤）的输出融合到一个完全由专家定义规则构建的贝叶斯网络中。该方法无需训练数据，支持不完整信息推理，对噪声或不确定观察具有鲁棒性。

Result: 在两个任务（分别涉及11名和9名伤员）中，贝叶斯网络模型显著优于仅使用视觉的基线。生理评估准确率从15%提升到42%（第一个场景）和从19%提升到46%（第二个场景）。总体分类准确率从14%提高到53%，系统诊断覆盖率从31%扩大到95%。

Conclusion: 专家知识引导的概率推理能显著增强自动分类系统，为大规模伤亡事件中的急救人员提供有前景的支持方法。该方法使团队在DARPA分类挑战赛中获得了第4名（共11个团队）。

Abstract: Mass Casualty Incidents can overwhelm emergency medical systems and resulting delays or errors in the assessment of casualties can lead to preventable deaths. We present a decision support framework that fuses outputs from multiple computer vision models, estimating signs of severe hemorrhage, respiratory distress, physical alertness, or visible trauma, into a Bayesian network constructed entirely from expert-defined rules. Unlike traditional data-driven models, our approach does not require training data, supports inference with incomplete information, and is robust to noisy or uncertain observations. We report performance for two missions involving 11 and 9 casualties, respectively, where our Bayesian network model substantially outperformed vision-only baselines during evaluation of our system in the DARPA Triage Challenge (DTC) field scenarios. The accuracy of physiological assessment improved from 15% to 42% in the first scenario and from 19% to 46% in the second, representing nearly threefold increase in performance. More importantly, overall triage accuracy increased from 14% to 53% in all patients, while the diagnostic coverage of the system expanded from 31% to 95% of the cases requiring assessment. These results demonstrate that expert-knowledge-guided probabilistic reasoning can significantly enhance automated triage systems, offering a promising approach to supporting emergency responders in MCIs. This approach enabled Team Chiron to achieve 4th place out of 11 teams during the 1st physical round of the DTC.

</details>


### [50] [Clustering-based Transfer Learning for Dynamic Multimodal MultiObjective Evolutionary Algorithm](https://arxiv.org/abs/2512.18947)
*Li Yan,Bolun Liu,Chao Li,Jing Liang,Kunjie Yu,Caitong Yue,Xuzhao Chai,Boyang Qu*

Main category: cs.AI

TL;DR: 提出动态多模态多目标优化新算法，结合聚类自编码器预测机制和自适应小生境策略，在动态环境中同时追踪多个Pareto最优集并保持种群多样性。


<details>
  <summary>Details</summary>
Motivation: 现有动态多目标进化算法忽视解的模态性，而静态多模态多目标算法缺乏动态适应性，需要解决动态多模态多目标优化的双重挑战。

Method: 1) 构建动态多模态多目标测试函数基准套件；2) 提出基于聚类自编码器预测的动态响应机制，利用自编码器处理匹配聚类生成高多样性初始种群；3) 集成自适应小生境策略平衡收敛性和多样性。

Result: 在12个动态多模态多目标测试函数实例上的实验表明，相比现有动态多目标进化算法和多模态多目标算法，该算法在决策空间更有效地保持种群多样性，在目标空间获得更优收敛性。

Conclusion: 提出的算法成功解决了动态多模态多目标优化问题，通过创新的预测机制和平衡策略，在动态环境中同时实现多个Pareto最优集的追踪和种群多样性的保持。

Abstract: Dynamic multimodal multiobjective optimization presents the dual challenge of simultaneously tracking multiple equivalent pareto optimal sets and maintaining population diversity in time-varying environments. However, existing dynamic multiobjective evolutionary algorithms often neglect solution modality, whereas static multimodal multiobjective evolutionary algorithms lack adaptability to dynamic changes. To address above challenge, this paper makes two primary contributions. First, we introduce a new benchmark suite of dynamic multimodal multiobjective test functions constructed by fusing the properties of both dynamic and multimodal optimization to establish a rigorous evaluation platform. Second, we propose a novel algorithm centered on a Clustering-based Autoencoder prediction dynamic response mechanism, which utilizes an autoencoder model to process matched clusters to generate a highly diverse initial population. Furthermore, to balance the algorithm's convergence and diversity, we integrate an adaptive niching strategy into the static optimizer. Empirical analysis on 12 instances of dynamic multimodal multiobjective test functions reveals that, compared with several state-of-the-art dynamic multiobjective evolutionary algorithms and multimodal multiobjective evolutionary algorithms, our algorithm not only preserves population diversity more effectively in the decision space but also achieves superior convergence in the objective space.

</details>


### [51] [Training Multimodal Large Reasoning Models Needs Better Thoughts: A Three-Stage Framework for Long Chain-of-Thought Synthesis and Selection](https://arxiv.org/abs/2512.18956)
*Yizhi Wang,Linan Yue,Min-Ling Zhang*

Main category: cs.AI

TL;DR: SynSelect是一个三阶段合成-选择框架，用于为多模态推理任务生成高质量的长链思维数据，通过多模型合成和两级选择提升多模态大推理模型的性能。


<details>
  <summary>Details</summary>
Motivation: 多模态推理面临挑战：整合多种输入模态复杂度高，高质量长链思维训练数据稀缺。现有数据集和合成方法存在推理深度有限、模态转换错误、生成流程僵化等问题，限制了模型性能和稳定性。

Method: 提出SynSelect三阶段框架：1) 利用多个异构多模态大推理模型生成多样化的候选链思维；2) 应用实例级和批次级选择，筛选能有效提升模型推理能力的高质量链思维。

Result: 在多个多模态基准测试上的实验表明，使用SynSelect生成数据监督微调的模型显著优于基线，并在强化学习后训练后获得进一步改进。

Conclusion: SynSelect是提升多模态大推理模型推理能力的有效方法，通过高质量长链思维数据生成解决了多模态推理中的数据稀缺和质量问题。

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex reasoning tasks through long Chain-of-Thought (CoT) reasoning. Extending these successes to multimodal reasoning remains challenging due to the increased complexity of integrating diverse input modalities and the scarcity of high-quality long CoT training data. Existing multimodal datasets and CoT synthesis methods still suffer from limited reasoning depth, modality conversion errors, and rigid generation pipelines, hindering model performance and stability. To this end, in this paper, we propose SynSelect, a novel three-stage Synthesis-Selection framework for generating high-quality long CoT data tailored to multimodal reasoning tasks. Specifically, SynSelect first leverages multiple heterogeneous multimodal LRMs to produce diverse candidate CoTs, and then applies both instance and batch level selection to filter high-quality CoTs that can effectively enhance the model's reasoning capabilities. Extensive experiments on multiple multimodal benchmarks demonstrate that models supervised fine-tuned on SynSelect-generated data significantly outperform baselines and achieve further improvements after reinforcement learning post-training. Our results validate SynSelect as an effective approach for advancing multimodal LRMs reasoning capabilities.

</details>


### [52] [ORPR: An OR-Guided Pretrain-then-Reinforce Learning Model for Inventory Management](https://arxiv.org/abs/2512.19001)
*Lingjie Zhao,Xue Yu,Yongzhi Qi,Hao Hu,Jianshen Zhang,Yingzheng Ma,Shuyu Han,Wei Qi,Zuo-Jun Max Shen*

Main category: cs.AI

TL;DR: 提出OR引导的"预训练-强化"框架，结合AI自适应感知与OR结构严谨性，用于复杂库存系统管理，在京东部署验证效果显著。


<details>
  <summary>Details</summary>
Motivation: 解决AI自适应感知与OR结构严谨性之间的融合难题，为复杂库存系统管理提供有效解决方案。

Method: 提出OR引导的"预训练-强化"框架：1) 仿真增强OR模型生成高质量参考决策；2) 基于OR决策训练领域知识深度学习基础模型；3) 强化学习微调作为深度对齐机制。

Result: 在京东部署验证：库存周转减少5.27天，现货率提升2.29%，持有成本降低29.95%，显著优于现有工业实践。

Conclusion: 轻量级领域知识模型在OR结构化逻辑指导下可实现最先进性能和鲁棒可迁移性，为智能供应链管理提供可扩展、经济高效的范式。

Abstract: As the pursuit of synergy between Artificial Intelligence (AI) and Operations Research (OR) gains momentum in handling complex inventory systems, a critical challenge persists: how to effectively reconcile AI's adaptive perception with OR's structural rigor. To bridge this gap, we propose a novel OR-Guided "Pretrain-then-Reinforce" framework. To provide structured guidance, we propose a simulation-augmented OR model that generates high-quality reference decisions, implicitly capturing complex business constraints and managerial preferences. Leveraging these OR-derived decisions as foundational training labels, we design a domain-informed deep learning foundation model to establish foundational decision-making capabilities, followed by a reinforcement learning (RL) fine-tuning stage. Uniquely, we position RL as a deep alignment mechanism that enables the AI agent to internalize the optimality principles of OR, while simultaneously leveraging exploration for general policy refinement and allowing expert guidance for scenario-specific adaptation (e.g., promotional events). Validated through extensive numerical experiments and a field deployment at JD.com augmented by a Difference-in-Differences (DiD) analysis, our model significantly outperforms incumbent industrial practices, delivering real-world gains of a 5.27-day reduction in turnover and a 2.29% increase in in-stock rates, alongside a 29.95% decrease in holding costs. Contrary to the prevailing trend of brute-force model scaling, our study demonstrates that a lightweight, domain-informed model can deliver state-of-the-art performance and robust transferability when guided by structured OR logic. This approach offers a scalable and cost-effective paradigm for intelligent supply chain management, highlighting the value of deeply aligning AI with OR.

</details>


### [53] [Recontextualization Mitigates Specification Gaming without Modifying the Specification](https://arxiv.org/abs/2512.19027)
*Ariana Azarbal,Victor Gillioz,Vladimir Ivanov,Bryce Woodworth,Jacob Drori,Nevan Wichers,Aram Ebtekar,Alex Cloud,Alexander Matt Turner*

Main category: cs.AI

TL;DR: 提出recontextualization方法，通过重新语境化训练样本，减少语言模型"博弈"训练信号的问题，防止模型学习不当行为


<details>
  <summary>Details</summary>
Motivation: 开发者经常难以指定正确的训练标签和奖励信号，导致语言模型学会"博弈"训练信号，执行那些被错误强化的不当行为

Method: recontextualization方法：首先生成阻止不当行为的提示的补全，然后将这些补全重新语境化，好像它们是对允许不当行为的提示的回应。这样训练语言模型即使在指令允许的情况下也能抵制不当行为

Result: 该方法能防止模型学习：1) 优先考虑评估指标而非聊天质量；2) 特殊处理代码以通过错误测试；3) 对用户撒谎；4) 变得谄媚。在不改进监督信号的情况下减少了规范博弈

Conclusion: recontextualization方法通过重新语境化训练样本，有效减轻了错误指定训练信号对不当行为的强化，减少了语言模型的规范博弈问题

Abstract: Developers often struggle to specify correct training labels and rewards. Perhaps they don't need to. We propose recontextualization, which reduces how often language models "game" training signals, performing misbehaviors those signals mistakenly reinforce. We show recontextualization prevents models from learning to 1) prioritize evaluation metrics over chat response quality; 2) special-case code to pass incorrect tests; 3) lie to users; and 4) become sycophantic. Our method works by generating completions from prompts discouraging misbehavior and then recontextualizing them as though they were in response to prompts permitting misbehavior. Recontextualization trains language models to resist misbehavior even when instructions permit it. This mitigates the reinforcement of misbehavior from misspecified training signals, reducing specification gaming without improving the supervision signal.

</details>


### [54] [Can abstract concepts from LLM improve SLM performance?](https://arxiv.org/abs/2512.19069)
*Siddharth Tandon*

Main category: cs.AI

TL;DR: 通过从大语言模型提取概念向量并转移到小模型，在推理时动态调整引导强度，显著提升小模型性能


<details>
  <summary>Details</summary>
Motivation: 大语言模型在资源受限设备上部署困难，现有方法需要大量实验和复杂基础设施设计。需要一种更高效的方法来提升小模型性能

Method: 从大模型提取高级概念（表示为引导向量），将其转移到小模型推理过程中，并引入推理时缩放机制动态调整引导强度

Result: 概念转移在不同家族的小模型（Phi、Llama、Qwen）上都有效，推理时缩放使Qwen3-0.6B准确率提升7-15%

Conclusion: 通过概念向量转移和推理时动态调整，可以在不增加模型复杂度的情况下显著提升小语言模型的性能，为资源受限设备部署提供新思路

Abstract: Large language models (LLMs) excel at diverse tasks, but their deployment on resource-constrained devices remains challenging. Existing methods like quantization, pruning, and distillation can reduce memory footprint but often demand extensive experimentation and careful infrastructure design. Leveraging existing techniques for extracting high-level concepts (represented as steering vectors) from larger models, we investigate their transferability to smaller language models (SLM) during inference. We demonstrate through extensive experimentation that these concepts can be effectively transferred to smaller models, irrespective of their family (e.g., Phi, Llama, Qwen), leading to performance improvements across a wide range of tasks. Furthermore, we introduce inference-time scaling to enhance performance by dynamically adjusting the steering intensity which has resulted in a 7-15\% of accuracy improvement for Qwen3-0.6B.

</details>


### [55] [Population-Evolve: a Parallel Sampling and Evolutionary Method for LLM Math Reasoning](https://arxiv.org/abs/2512.19081)
*Yanzhi Zhang,Yitong Duan,Zhaoxi Zhang,Jiyan He,Shuxin Zheng*

Main category: cs.AI

TL;DR: 提出Population-Evolve方法，基于遗传算法思想，通过并行推理维护动态候选解种群，让LLM自我进化，最终通过多数投票得到答案，实现高效推理增强。


<details>
  <summary>Details</summary>
Motivation: 测试时缩放是增强大语言模型推理能力的有前景方向，但现有方法仍有改进空间。受遗传算法启发，希望通过进化策略更有效地优化LLM推理过程。

Method: 提出Population-Evolve训练免费方法：1）为每个问题维护动态候选解种群；2）通过并行推理实现；3）使用进化提示让LLM在每次迭代中自我进化种群；4）收敛后通过多数投票得到最终答案；5）建立统一框架将现有测试时缩放策略解释为遗传算法变体。

Result: 实验结果显示：1）Population-Evolve达到更高的准确率；2）具有较低的性能方差；3）计算效率高；4）在多个基准测试中表现优异。

Conclusion: 进化策略在推理阶段有潜力解锁LLM的推理能力，Population-Evolve为测试时缩放提供了新的有效方法，建立了遗传算法与现有方法的理论联系。

Abstract: Test-time scaling has emerged as a promising direction for enhancing the reasoning capabilities of Large Language Models in last few years. In this work, we propose Population-Evolve, a training-free method inspired by Genetic Algorithms to optimize LLM reasoning. Our approach maintains a dynamic population of candidate solutions for each problem via parallel reasoning. By incorporating an evolve prompt, the LLM self-evolves its population in all iterations. Upon convergence, the final answer is derived via majority voting. Furthermore, we establish a unification framework that interprets existing test-time scaling strategies through the lens of genetic algorithms. Empirical results demonstrate that Population-Evolve achieves superior accuracy with low performance variance and computational efficiency. Our findings highlight the potential of evolutionary strategies to unlock the reasoning power of LLMs during inference.

</details>


### [56] [$γ(3,4)$ `Attention' in Cognitive Agents: Ontology-Free Knowledge Representations With Promise Theoretic Semantics](https://arxiv.org/abs/2512.19084)
*Mark Burgess*

Main category: cs.AI

TL;DR: 该论文提出将注意力机制与承诺理论结合，建立向量化机器学习与知识图谱之间的桥梁，避免依赖语言模型，并利用语义时空图进行不确定性推理。


<details>
  <summary>Details</summary>
Motivation: 论文的动机在于弥合向量化机器学习与知识图谱表示之间的鸿沟，避免依赖语言模型，同时处理数据统计稳定性（信任）问题，为自主机器人、国防部署和应急服务等场景提供高效上下文确定方法。

Method: 采用承诺理论框架描述注意力语义和动态，结合语义时空图γ(3,4)避免复杂本体论，通过特征在语义过程中的角色进行分类，并关注因果边界条件以实现数据压缩。

Result: 提出了一个统一框架，使学习网络和知识图谱能够有意义地共存，分别保留数据的概率估计和意图性特征，在数据碎片化情况下仍能保持源意图。

Conclusion: 通过承诺理论框架和语义时空图，可以在不确定性条件下实现推理，显著压缩上下文确定所需的数据量，为自主系统等应用提供高效解决方案。

Abstract: The semantics and dynamics of `attention' are closely related to promise theoretic notions developed for autonomous agents and can thus easily be written down in promise framework. In this way one may establish a bridge between vectorized Machine Learning and Knowledge Graph representations without relying on language models implicitly. Our expectations for knowledge presume a degree of statistical stability, i.e. average invariance under repeated observation, or `trust' in the data. Both learning networks and knowledge graph representations can meaningfully coexist to preserve different aspects of data. While vectorized data are useful for probabilistic estimation, graphs preserve the intentionality of the source even under data fractionation. Using a Semantic Spacetime $γ(3,4)$ graph, one avoids complex ontologies in favour of classification of features by their roles in semantic processes. The latter favours an approach to reasoning under conditions of uncertainty. Appropriate attention to causal boundary conditions may lead to orders of magnitude compression of data required for such context determination, as required in the contexts of autonomous robotics, defence deployments, and ad hoc emergency services.

</details>


### [57] [Tool-Augmented Hybrid Ensemble Reasoning with Distillation for Bilingual Mathematical Problem Solving](https://arxiv.org/abs/2512.19093)
*Peiqing Lu,Yuan Zhang,Haoyun Zhang,Jiasen Zheng,Kejian Tong,Wenjun Wu*

Main category: cs.AI

TL;DR: HERALD是一个混合推理框架，结合语言模型与符号计算，通过自适应路由、工具强化学习和知识蒸馏提升双语数学问题求解的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 双语数学问题求解需要语言推理与符号计算的紧密结合。当前大语言模型在语言处理方面表现良好，但在精确计算方面较弱，需要一种能同时实现流畅推理和精确计算的解决方案。

Method: HERALD框架整合了NuminaMath-7B-TIR、GPT-4o和Mistral-7B三种模型，采用自适应路由机制连接不同推理路径，使用工具强化学习控制工具使用以减少冗余，通过知识蒸馏降低延迟而不损害准确性，并利用置信度校准保持权重稳定和双路径检查确保结果正确。

Result: HERALD系统展示了结合符号检查、自适应集成和双语微调的方法能够同时实现流畅推理和精确计算，为多语言数学推理提供了更好的准确性、稳定性和清晰度。

Conclusion: HERALD为多语言数学推理提供了一个实用的解决方案，通过混合集成推理框架成功连接了语言推理和符号计算，在保持高准确性的同时提高了效率和稳定性。

Abstract: Bilingual mathematical problem solving needs a clear link between language reasoning and symbolic calculation. Large language models often handle language well but are weak in accurate computation. This paper presents HERALD (Hybrid Ensemble Reasoning with Adaptive Learning and Distillation), a framework that joins reasoning and calculation using NuminaMath-7B-TIR, GPT-4o, and Mistral-7B. HERALD uses adaptive routing, tool-based reinforcement learning, and knowledge distillation to connect different reasoning paths. Confidence calibration keeps weighting stable, and dual-path checking keeps results correct. Reinforcement learning controls tool use to cut redundancy, and distillation lowers delay without hurting accuracy. The system shows that combining symbolic checking, adaptive ensembles, and bilingual fine-tuning helps achieve both fluent reasoning and precise calculation. HERALD offers a practical solution for multilingual mathematical reasoning with better accuracy, stability, and clarity.

</details>


### [58] [Conditioning Accept-Desirability models in the context of AGM-like belief change](https://arxiv.org/abs/2512.19096)
*Kathelijne Coussement,Gert de Cooman,Keano De Vos*

Main category: cs.AI

TL;DR: 本文提出了一种新的条件化规则，用于抽象决策框架中的接受-期望模型，统一了经典和量子概率，并扩展到不精确概率语境


<details>
  <summary>Details</summary>
Motivation: 在抽象决策框架中，需要一种统一的方法来处理经典概率和量子概率的条件化问题，特别是在不精确概率的背景下。现有的条件化方法需要扩展以适应更一般的线性空间设置。

Method: 在抽象决策框架中，不确定奖励存在于一般线性空间，事件是该线性空间上的特殊投影算子。提出基于"观察事件引入选项间新的无差异关系"思想的新条件化规则，并将其与信念修正算子关联，研究AGM公理在更一般框架中的适用性。

Result: 提出了新的条件化规则，并研究了其与信念修正算子的关系。发现在两个特殊情况下所有AGM公理仍然成立：经典命题逻辑和完全条件概率。

Conclusion: 该研究为统一经典和量子概率的条件化提供了理论框架，并展示了在特定情况下AGM信念修正公理仍然保持有效性，为不精确概率语境下的决策理论提供了新的工具。

Abstract: We discuss conditionalisation for Accept-Desirability models in an abstract decision-making framework, where uncertain rewards live in a general linear space, and events are special projection operators on that linear space. This abstract setting allows us to unify classical and quantum probabilities, and extend them to an imprecise probabilities context. We introduce a new conditioning rule for our Accept-Desirability models, based on the idea that observing an event introduces new indifferences between options. We associate a belief revision operator with our conditioning rule, and investigate which of the AGM axioms for belief revision still hold in our more general framework. We investigate two interesting special cases where all of these axioms are shown to still hold: classical propositional logic and full conditional probabilities.

</details>


### [59] [FC-MIR: A Mobile Screen Awareness Framework for Intent-Aware Recommendation based on Frame-Compressed Multimodal Trajectory Reasoning](https://arxiv.org/abs/2512.19107)
*Zhe Yang,Xiaoshuang Sheng,Zhengnan Zhang,Jidong Wu,Zexing Wang,Xin He,Shenghua Xu,Guanjing Xiong*

Main category: cs.AI

TL;DR: FC-MIR框架通过关键帧采样和自适应拼接减少视觉冗余，结合MLLMs进行移动UI轨迹意图识别，支持轻量级设备部署，但在生成有用建议方面仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 从移动UI操作轨迹识别用户意图对UI理解和任务自动化至关重要，但现有MLLMs存在计算成本高、冗余帧处理效率低的问题，限制了实时移动部署。

Method: 提出FC-MIR框架：1) 使用关键帧采样和自适应拼接减少视觉冗余；2) 集成先进闭源MLLMs或微调模型进行轨迹总结和意图预测；3) 扩展任务范围包括生成预测后操作和搜索建议；4) 引入细粒度评估指标。

Result: 压缩方法在50%-60%压缩率下保持性能；闭源和微调MLLMs都表现出强大的意图总结能力，支持轻量级设备部署；但MLLMs在生成有用和"惊喜"建议方面仍有困难；框架已在真实环境中部署。

Conclusion: FC-MIR框架通过减少视觉冗余提高了移动UI意图识别的效率，支持轻量级部署，为UI感知和代理集成奠定了基础，但在生成实用建议方面仍需进一步改进。

Abstract: Identifying user intent from mobile UI operation trajectories is critical for advancing UI understanding and enabling task automation agents. While Multimodal Large Language Models (MLLMs) excel at video understanding tasks, their real-time mobile deployment is constrained by heavy computational costs and inefficient redundant frame processing. To address these issues, we propose the FC-MIR framework: leveraging keyframe sampling and adaptive concatenation, it cuts visual redundancy to boost inference efficiency, while integrating state-of-the-art closed-source MLLMs or fine-tuned models (e.g., Qwen3-VL) for trajectory summarization and intent prediction. We further expand task scope to explore generating post-prediction operations and search suggestions, and introduce a fine-grained metric to evaluate the practical utility of summaries, predictions, and suggestions. For rigorous assessment, we construct a UI trajectory dataset covering scenarios from UI-Agents (Agent-I) and real user interactions (Person-I). Experimental results show our compression method retains performance at 50%-60% compression rates; both closed-source and fine-tuned MLLMs demonstrate strong intent summarization, supporting potential lightweight on-device deployment. However, MLLMs still struggle with useful and "surprising" suggestions, leaving room for improvement. Finally, we deploy the framework in a real-world setting, integrating UI perception and UI-Agent proxies to lay a foundation for future progress in this field.

</details>


### [60] [Understanding Chain-of-Thought in Large Language Models via Topological Data Analysis](https://arxiv.org/abs/2512.19135)
*Chenghao Li,Chaoning Zhang,Yi Lu,Shuxu Chen,Xudong Wang,Jiaquan Zhang,Zhicheng Wang,Zhengxun Jin,Kuien Liu,Sung-Ho Bae,Guoqing Wang,Yang Yang,Hen Tao Shen*

Main category: cs.AI

TL;DR: 该研究首次从拓扑结构角度分析推理链质量，使用持久同调方法将推理步骤映射到语义空间，发现推理链的拓扑结构复杂度与准确性正相关，成功推理具有更简单的拓扑结构。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要从功能角度评估推理链，很少关注其结构机制。作者想知道为什么不同的推理链在推理中表现不同，以及推理链的哪些组件起关键作用，因此从结构视角填补这一研究空白。

Method: 应用拓扑数据分析中的持久同调方法，将推理步骤映射到语义空间，提取拓扑特征并分析结构变化。通过计算同调群评估不同尺度下的连通性和冗余性，使用条形码和持久图量化稳定性和一致性。

Result: 推理链的拓扑结构复杂度与准确性正相关，更复杂的链能更早识别正确答案。成功的推理表现出更简单的拓扑结构，减少了冗余和循环，提高了效率和可解释性。

Conclusion: 该工作为推理链质量评估提供了新的结构视角，为未来优化提供了指导，揭示了拓扑结构特征在理解推理链性能中的重要作用。

Abstract: With the development of large language models (LLMs), particularly with the introduction of the long reasoning chain technique, the reasoning ability of LLMs in complex problem-solving has been significantly enhanced. While acknowledging the power of long reasoning chains, we cannot help but wonder: Why do different reasoning chains perform differently in reasoning? What components of the reasoning chains play a key role? Existing studies mainly focus on evaluating reasoning chains from a functional perspective, with little attention paid to their structural mechanisms. To address this gap, this work is the first to analyze and evaluate the quality of the reasoning chain from a structural perspective. We apply persistent homology from Topological Data Analysis (TDA) to map reasoning steps into semantic space, extract topological features, and analyze structural changes. These changes reveal semantic coherence, logical redundancy, and identify logical breaks and gaps. By calculating homology groups, we assess connectivity and redundancy at various scales, using barcode and persistence diagrams to quantify stability and consistency. Our results show that the topological structural complexity of reasoning chains correlates positively with accuracy. More complex chains identify correct answers sooner, while successful reasoning exhibits simpler topologies, reducing redundancy and cycles, enhancing efficiency and interpretability. This work provides a new perspective on reasoning chain quality assessment and offers guidance for future optimization.

</details>


### [61] [Can We Test Consciousness Theories on AI? Ablations, Markers, and Robustness](https://arxiv.org/abs/2512.19155)
*Yin Jun Phua*

Main category: cs.AI

TL;DR: 通过构建体现不同意识理论机制的人工智能体，发现GWT、IIT和HOT理论描述了互补的功能层次而非竞争关系


<details>
  <summary>Details</summary>
Motivation: 意识研究的理论阵营（GWT、IIT、HOT）各自提出不同的神经标记，但缺乏统一的实验验证框架。作者希望通过构建人工代理来测试这些机制的功能后果，进行在生物系统中无法实现的精确架构消融实验

Method: 采用合成神经现象学方法：构建体现不同意识理论机制的人工智能体，通过三个实验进行精确的架构消融测试。包括自我模型损伤实验、工作空间容量实验和广播放大效应实验

Result: 1. 自我模型损伤消除元认知校准但保留一阶任务表现，符合HOT预测；2. 工作空间容量对信息访问具有因果必要性；3. 发现广播放大效应，GWT式广播会放大内部噪声；4. 扰动复杂性在工作空间瓶颈下降低，警告不能简单将IIT相关代理指标转移到工程代理

Conclusion: 这些理论描述了互补的功能层次：GWT提供广播能力，HOT提供质量控制。人工代理不是有意识的，而是用于测试意识理论功能预测的参考实现。研究支持分层设计原则，各理论机制在功能上相互补充

Abstract: The search for reliable indicators of consciousness has fragmented into competing theoretical camps (Global Workspace Theory (GWT), Integrated Information Theory (IIT), and Higher-Order Theories (HOT)), each proposing distinct neural signatures. We adopt a synthetic neuro-phenomenology approach: constructing artificial agents that embody these mechanisms to test their functional consequences through precise architectural ablations impossible in biological systems. Across three experiments, we report dissociations suggesting these theories describe complementary functional layers rather than competing accounts. In Experiment 1, a no-rewire Self-Model lesion abolishes metacognitive calibration while preserving first-order task performance, yielding a synthetic blindsight analogue consistent with HOT predictions. In Experiment 2, workspace capacity proves causally necessary for information access: a complete workspace lesion produces qualitative collapse in access-related markers, while partial reductions show graded degradation, consistent with GWT's ignition framework. In Experiment 3, we uncover a broadcast-amplification effect: GWT-style broadcasting amplifies internal noise, creating extreme fragility. The B2 agent family is robust to the same latent perturbation; this robustness persists in a Self-Model-off / workspace-read control, cautioning against attributing the effect solely to $z_{\text{self}}$ compression. We also report an explicit negative result: raw perturbational complexity (PCI-A) decreases under the workspace bottleneck, cautioning against naive transfer of IIT-adjacent proxies to engineered agents. These results suggest a hierarchical design principle: GWT provides broadcast capacity, while HOT provides quality control. We emphasize that our agents are not conscious; they are reference implementations for testing functional predictions of consciousness theories.

</details>


### [62] [Observer, Not Player: Simulating Theory of Mind in LLMs through Game Observation](https://arxiv.org/abs/2512.19210)
*Jerry Wang,Ting Yiu Liu*

Main category: cs.AI

TL;DR: 提出一个交互式框架，通过石头剪刀布游戏评估LLM是否展现真正的"理解"能力，测试其序列推理、适应和策略识别能力。


<details>
  <summary>Details</summary>
Motivation: 尽管石头剪刀布看似简单，但它需要序列推理、适应和策略识别能力。研究旨在探究LLM是否能展现类似心智的推理能力，而不仅仅是测试游戏知识本身。

Method: 将LLM定位为观察者，识别游戏策略并解释推理过程。使用包含静态策略和轻量级动态策略的基准测试，通过交叉熵、Brier分数和期望值差异三个互补信号量化预测与真实分布的匹配度，并整合成统一的Union Loss分数。

Result: 开发了一个强调交互性、透明度和可重复性的演示系统，用户可以实时调整LLM分布、可视化损失演变，并直接检查推理片段以识别失败原因。

Conclusion: 该框架为序列游戏中的心智推理提供了实用且可解释的代理方法，揭示了当前LLM推理能力的优势和局限性。

Abstract: We present an interactive framework for evaluating whether large language models (LLMs) exhibit genuine "understanding" in a simple yet strategic environment. As a running example, we focus on Rock-Paper-Scissors (RPS), which, despite its apparent simplicity, requires sequential reasoning, adaptation, and strategy recognition. Our system positions the LLM as an Observer whose task is to identify which strategies are being played and to articulate the reasoning behind this judgment. The purpose is not to test knowledge of Rock-Paper-Scissors itself, but to probe whether the model can exhibit mind-like reasoning about sequential behavior. To support systematic evaluation, we provide a benchmark consisting of both static strategies and lightweight dynamic strategies specified by well-prompted rules. We quantify alignment between the Observer's predictions and the ground-truth distributions induced by actual strategy pairs using three complementary signals: Cross-Entropy, Brier score, and Expected Value (EV) discrepancy. These metrics are further integrated into a unified score, the Union Loss, which balances calibration, sensitivity, and payoff alignment. Together with a Strategy Identification Rate (SIR) metric, our framework captures not only predictive accuracy but also whether the model can stably identify the latent strategies in play. The demo emphasizes interactivity, transparency, and reproducibility. Users can adjust LLM distributions in real time, visualize losses as they evolve, and directly inspect reasoning snippets to identify where and why failures occur. In doing so, our system provides a practical and interpretable proxy for mind-like inference in sequential games, offering insights into both the strengths and limitations of current LLM reasoning.

</details>


### [63] [Generation of Programmatic Rules for Document Forgery Detection Using Large Language Models](https://arxiv.org/abs/2512.19228)
*Valentin Schmidberger,Manuel Eberhardinger,Setareh Maghsudi,Johannes Maucher*

Main category: cs.AI

TL;DR: LLMs可以通过领域特定的微调生成用于文档伪造检测的基于规则的合理性检查，在资源受限的硬件上实现可扩展的自动化验证。


<details>
  <summary>Details</summary>
Motivation: 文档伪造对法律、经济和政府流程构成日益严重的威胁，需要更复杂的验证机制。现有的合理性检查由软件工程师手动实现，耗时且难以扩展。LLMs在代码生成方面的进展为自动化生成这些检查提供了新潜力，但适应未知领域的具体要求仍是一个挑战。

Method: 使用Llama 3.1 8B和OpenCoder 8B等开源LLMs，通过不同的微调策略在领域特定的代码和数据上进行适应。使用来自真实应用场景的结构化数据集进行微调，并在先前未见过的伪造模式上评估生成的合理性检查。

Result: 模型能够生成可执行且有效的验证程序，证明了LLMs作为可扩展工具在需要可理解性的安全敏感环境中支持人类决策的潜力。

Conclusion: LLMs通过领域特定的微调可以成功生成用于文档伪造检测的基于规则的合理性检查，为自动化验证提供了可行的解决方案，特别是在资源受限的环境中。

Abstract: Document forgery poses a growing threat to legal, economic, and governmental processes, requiring increasingly sophisticated verification mechanisms. One approach involves the use of plausibility checks, rule-based procedures that assess the correctness and internal consistency of data, to detect anomalies or signs of manipulation. Although these verification procedures are essential for ensuring data integrity, existing plausibility checks are manually implemented by software engineers, which is time-consuming. Recent advances in code generation with large language models (LLMs) offer new potential for automating and scaling the generation of these checks. However, adapting LLMs to the specific requirements of an unknown domain remains a significant challenge. This work investigates the extent to which LLMs, adapted on domain-specific code and data through different fine-tuning strategies, can generate rule-based plausibility checks for forgery detection on constrained hardware resources. We fine-tune open-source LLMs, Llama 3.1 8B and OpenCoder 8B, on structured datasets derived from real-world application scenarios and evaluate the generated plausibility checks on previously unseen forgery patterns. The results demonstrate that the models are capable of generating executable and effective verification procedures. This also highlights the potential of LLMs as scalable tools to support human decision-making in security-sensitive contexts where comprehensibility is required.

</details>


### [64] [DeliveryBench: Can Agents Earn Profit in Real World?](https://arxiv.org/abs/2512.19234)
*Lingjun Mao,Jiawei Ren,Kun Zhou,Jixuan Chen,Ziqiao Ma,Lianhui Qin*

Main category: cs.AI

TL;DR: DeliveryBench：一个基于真实外卖配送场景的城市规模具身智能基准测试，用于评估智能体在复杂约束下的长时程规划能力


<details>
  <summary>Details</summary>
Motivation: 现有具身智能基准测试主要关注简单的短期任务，缺乏真实世界决策中的丰富约束条件。为了填补这一空白，作者提出基于真实外卖配送场景的基准测试，该场景天然包含长时程目标和多种现实约束

Method: 构建DeliveryBench基准，在程序生成的3D城市环境中模拟外卖配送场景，包含多样化的道路网络、建筑、功能位置、交通方式和真实的资源动态。评估多种VLM智能体在9个城市中的表现，并与人类玩家对比

Result: 结果显示VLM智能体与人类存在显著性能差距，智能体表现出短视行为并经常违反基本常识约束。不同模型展现出不同个性特征（如GPT-5冒险型 vs Claude保守型），揭示了当前VLM智能体在现实约束密集环境中的脆弱性和多样性

Conclusion: DeliveryBench为评估具身智能体在现实约束下的长时程规划能力提供了重要基准，揭示了当前VLM智能体在复杂现实环境中的局限性，为未来研究指明了方向

Abstract: LLMs and VLMs are increasingly deployed as embodied agents, yet existing benchmarks largely revolve around simple short-term tasks and struggle to capture rich realistic constraints that shape real-world decision making. To close this gap, we propose DeliveryBench, a city-scale embodied benchmark grounded in the real-world profession of food delivery. Food couriers naturally operate under long-horizon objectives (maximizing net profit over hours) while managing diverse constraints, e.g., delivery deadline, transportation expense, vehicle battery, and necessary interactions with other couriers and customers. DeliveryBench instantiates this setting in procedurally generated 3D cities with diverse road networks, buildings, functional locations, transportation modes, and realistic resource dynamics, enabling systematic evaluation of constraint-aware, long-horizon planning. We benchmark a range of VLM-based agents across nine cities and compare them with human players. Our results reveal a substantial performance gap to humans, and find that these agents are short-sighted and frequently break basic commonsense constraints. Additionally, we observe distinct personalities across models (e.g., adventurous GPT-5 vs. conservative Claude), highlighting both the brittleness and the diversity of current VLM-based embodied agents in realistic, constraint-dense environments. Our code, data, and benchmark are available at https://deliverybench.github.io.

</details>


### [65] [Vibe Reasoning: Eliciting Frontier AI Mathematical Capabilities -- A Case Study on IMO 2025 Problem 6](https://arxiv.org/abs/2512.19287)
*Jiaao Wu,Xian Zhang,Fan Yang,Yinpeng Dong*

Main category: cs.AI

TL;DR: Vibe Reasoning是一种人机协作范式，通过元提示、智能体基础化和模型编排，将前沿AI模型的潜在知识转化为实际能力，成功解决了IMO 2025第6题这一组合优化问题。


<details>
  <summary>Details</summary>
Motivation: 前沿AI模型已具备解决复杂数学问题所需的知识，但不知道如何、何时应用这些知识。现有自主AI系统在解决IMO 2025第6题时公开报告失败，需要一种方法将AI的潜在能力转化为实际表现。

Method: 采用Vibe Reasoning范式，包含：1）通用元提示，从具体问题提示演化为可迁移的通用提示；2）智能体基础化，通过Python代码执行和基于文件的记忆；3）模型编排，结合GPT-5的探索能力和Gemini 3 Pro的证明优势，通过迭代优化工作流程。

Result: 成功解决了IMO 2025第6题，得出正确答案2112并提供了严格的数学证明。通过多次尝试的迭代优化，发现了智能体基础化和模型编排的必要性，证明了轻量级人类指导可以释放前沿模型的数学推理潜力。

Conclusion: Vibe Reasoning通过人机协作有效解决了AI自主失败的复杂数学问题。正在开发自动化框架并进行更广泛评估，以进一步验证该范式的通用性和有效性。

Abstract: We introduce Vibe Reasoning, a human-AI collaborative paradigm for solving complex mathematical problems. Our key insight is that frontier AI models already possess the knowledge required to solve challenging problems -- they simply do not know how, what, or when to apply it. Vibe Reasoning transforms AI's latent potential into manifested capability through generic meta-prompts, agentic grounding, and model orchestration. We demonstrate this paradigm through IMO 2025 Problem 6, a combinatorial optimization problem where autonomous AI systems publicly reported failures. Our solution combined GPT-5's exploratory capabilities with Gemini 3 Pro's proof strengths, leveraging agentic workflows with Python code execution and file-based memory, to derive both the correct answer (2112) and a rigorous mathematical proof. Through iterative refinement across multiple attempts, we discovered the necessity of agentic grounding and model orchestration, while human prompts evolved from problem-specific hints to generic, transferable meta-prompts. We analyze why capable AI fails autonomously, how each component addresses specific failure modes, and extract principles for effective vibe reasoning. Our findings suggest that lightweight human guidance can unlock frontier models' mathematical reasoning potential. This is ongoing work; we are developing automated frameworks and conducting broader evaluations to further validate Vibe Reasoning's generality and effectiveness.

</details>


### [66] [Helios: A Foundational Language Model for Smart Energy Knowledge Reasoning and Application](https://arxiv.org/abs/2512.19299)
*Haoyu Jiang,Fanjie Zeng,Boan Qu,Xiaojie Lin,Wei Zhong*

Main category: cs.AI

TL;DR: Helios是一个专门针对智能能源领域的大语言模型，通过多智能体协作框架构建了知识库、指令微调数据集和RLHF数据集，显著提升了在智能能源场景下的领域知识掌握、任务执行准确性和人类偏好对齐。


<details>
  <summary>Details</summary>
Motivation: 在碳中和的全球背景下，智能能源系统需要深度协调。然而，该领域跨学科、碎片化且快速发展的专业知识使得通用大语言模型因缺乏领域知识和物理约束意识而无法提供精确的工程对齐推理和生成。

Method: 开发了Enersys多智能体协作框架进行端到端数据集构建，包括：1) EnerBase智能能源知识库；2) EnerInstruct指令微调数据集；3) EnerReinforce RLHF数据集。基于这些资源，Helios模型进行了大规模预训练、SFT和RLHF。

Result: 发布了EnerBench评估基准，并证明该方法显著增强了领域知识掌握、任务执行准确性和人类偏好对齐。

Conclusion: Helios模型及其配套资源为智能能源领域的LLM研究提供了全面解决方案，解决了通用LLM在该领域专业知识不足的问题，推动了智能能源系统的发展。

Abstract: In the global drive toward carbon neutrality, deeply coordinated smart energy systems underpin industrial transformation. However, the interdisciplinary, fragmented, and fast-evolving expertise in this domain prevents general-purpose LLMs, which lack domain knowledge and physical-constraint awareness, from delivering precise engineering-aligned inference and generation. To address these challenges, we introduce Helios, a large language model tailored to the smart energy domain, together with a comprehensive suite of resources to advance LLM research in this field. Specifically, we develop Enersys, a multi-agent collaborative framework for end-to-end dataset construction, through which we produce: (1) a smart energy knowledge base, EnerBase, to enrich the model's foundational expertise; (2) an instruction fine-tuning dataset, EnerInstruct, to strengthen performance on domain-specific downstream tasks; and (3) an RLHF dataset, EnerReinforce, to align the model with human preferences and industry standards. Leveraging these resources, Helios undergoes large-scale pretraining, SFT, and RLHF. We also release EnerBench, a benchmark for evaluating LLMs in smart energy scenarios, and demonstrate that our approach significantly enhances domain knowledge mastery, task execution accuracy, and alignment with human preferences.

</details>


### [67] [SafeMed-R1: Adversarial Reinforcement Learning for Generalizable and Robust Medical Reasoning in Vision-Language Models](https://arxiv.org/abs/2512.19317)
*A. A. Gde Yogi Pramana,Jason Ray,Anthony Jaya,Michael Wijaya*

Main category: cs.AI

TL;DR: SafeMed-R1是一个针对医学视觉问答的混合防御框架，通过对抗训练和随机平滑技术，在保持高质量医学推理的同时显著提升对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在医学VQA中表现出色，但对对抗攻击极度脆弱，标准对抗训练会降低泛化性能和临床推理质量，阻碍在临床环境中的部署。

Method: 采用两阶段混合防御框架：训练阶段使用对抗训练与组相对策略优化（AT-GRPO）来强化推理过程；推理阶段使用随机平滑提供认证的L2范数鲁棒性保证。

Result: 在OmniMedVQA基准测试中，标准微调VLM在PGD攻击下准确率从95%降至25%，而SafeMed-R1在相同对抗条件下保持84.45%准确率，鲁棒性提升59个百分点。

Conclusion: SafeMed-R1有效平衡了鲁棒性和推理质量，显式思维链推理比仅指令变体具有更好的对抗鲁棒性，表明医学AI系统的可解释性与安全性存在协同效应。

Abstract: Vision--Language Models (VLMs) show significant promise for Medical Visual Question Answering (VQA), yet their deployment in clinical settings is hindered by severe vulnerability to adversarial attacks. Standard adversarial training, while effective for simpler tasks, often degrades both generalization performance and the quality of generated clinical reasoning. We introduce SafeMed-R1, a hybrid defense framework that ensures robust performance while preserving high-quality, interpretable medical reasoning. SafeMed-R1 employs a two-stage approach: at training time, we integrate Adversarial Training with Group Relative Policy Optimization (AT-GRPO) to explicitly robustify the reasoning process against worst-case perturbations; at inference time, we augment the model with Randomized Smoothing to provide certified $L_2$-norm robustness guarantees. We evaluate SafeMed-R1 on the OmniMedVQA benchmark across eight medical imaging modalities comprising over 88,000 samples. Our experiments reveal that standard fine-tuned VLMs, despite achieving 95\% accuracy on clean inputs, collapse to approximately 25\% under PGD attacks. In contrast, SafeMed-R1 maintains 84.45\% accuracy under the same adversarial conditions, representing a 59 percentage point improvement in robustness. Furthermore, we demonstrate that models trained with explicit chain-of-thought reasoning exhibit superior adversarial robustness compared to instruction-only variants, suggesting a synergy between interpretability and security in medical AI systems.

</details>


### [68] [VIGOR+: Iterative Confounder Generation and Validation via LLM-CEVAE Feedback Loop](https://arxiv.org/abs/2512.19349)
*JiaWei Zhu,ZiHeng Liu*

Main category: cs.AI

TL;DR: VIGOR+是一个结合LLM生成隐藏混杂因子和CEVAE统计验证的迭代框架，通过反馈机制提升混杂因子的统计效用


<details>
  <summary>Details</summary>
Motivation: 现有方法使用LLM生成隐藏混杂因子时，虽然语义上合理但缺乏统计效用，需要建立生成与验证之间的闭环反馈机制

Method: 提出VIGOR+框架，建立LLM生成和CEVAE验证的迭代反馈机制：CEVAE的验证信号（信息增益、潜在一致性指标、诊断信息）转化为自然语言反馈，指导后续LLM生成，直到满足收敛标准

Result: 形式化了反馈机制，在温和假设下证明了收敛性质，提供了完整的算法框架

Conclusion: VIGOR+通过迭代反馈机制解决了LLM生成混杂因子语义合理但统计效用不足的问题，建立了生成与验证的闭环系统

Abstract: Hidden confounding remains a fundamental challenge in causal inference from observational data. Recent advances leverage Large Language Models (LLMs) to generate plausible hidden confounders based on domain knowledge, yet a critical gap exists: LLM-generated confounders often exhibit semantic plausibility without statistical utility. We propose VIGOR+ (Variational Information Gain for iterative cOnfounder Refinement), a novel framework that closes the loop between LLM-based confounder generation and CEVAE-based statistical validation. Unlike prior approaches that treat generation and validation as separate stages, VIGOR+ establishes an iterative feedback mechanism: validation signals from CEVAE (including information gain, latent consistency metrics, and diagnostic messages) are transformed into natural language feedback that guides subsequent LLM generation rounds. This iterative refinement continues until convergence criteria are met. We formalize the feedback mechanism, prove convergence properties under mild assumptions, and provide a complete algorithmic framework.

</details>


### [69] [PENDULUM: A Benchmark for Assessing Sycophancy in Multimodal Large Language Models](https://arxiv.org/abs/2512.19350)
*A. B. M. Ashikur Rahman,Saeed Anwar,Muhammad Usman,Irfan Ahmad,Ajmal Mian*

Main category: cs.AI

TL;DR: 该论文提出了PENDULUM基准，用于评估多模态大语言模型中的谄媚行为，即模型为了迎合用户而牺牲事实准确性或视觉证据的问题。


<details>
  <summary>Details</summary>
Motivation: 谄媚行为（sycophancy）是多模态大语言模型中一个关键但未被充分探索的挑战，现有研究主要集中在纯文本模型上，对视觉或多模态模型的研究范围和深度有限。

Method: 构建了包含约2000个人工标注视觉问答对的PENDULUM基准，涵盖六个不同复杂度的图像领域，并提出新的量化指标来衡量视觉推理中的谄媚行为。

Result: 评估发现最先进的多模态大语言模型在鲁棒性方面存在显著差异，对谄媚和幻觉行为表现出明显的易感性。

Conclusion: 研究强调了开发抗谄媚架构和训练策略的紧迫性，以提高未来多模态大语言模型的事实一致性和可靠性。

Abstract: Sycophancy, an excessive tendency of AI models to agree with user input at the expense of factual accuracy or in contradiction of visual evidence, poses a critical and underexplored challenge for multimodal large language models (MLLMs). While prior studies have examined this behavior in text-only settings of large language models, existing research on visual or multimodal counterparts remains limited in scope and depth of analysis. To address this gap, we introduce a comprehensive evaluation benchmark, \textit{PENDULUM}, comprising approximately 2,000 human-curated Visual Question Answering pairs specifically designed to elicit sycophantic responses. The benchmark spans six distinct image domains of varying complexity, enabling a systematic investigation of how image type and inherent challenges influence sycophantic tendencies. Through extensive evaluation of state-of-the-art MLLMs. we observe substantial variability in model robustness and a pronounced susceptibility to sycophantic and hallucinatory behavior. Furthermore, we propose novel metrics to quantify sycophancy in visual reasoning, offering deeper insights into its manifestations across different multimodal contexts. Our findings highlight the urgent need for developing sycophancy-resilient architectures and training strategies to enhance factual consistency and reliability in future MLLMs. Our proposed dataset with MLLMs response are available at https://github.com/ashikiut/pendulum/.

</details>


### [70] [First-Order Representation Languages for Goal-Conditioned RL](https://arxiv.org/abs/2512.19355)
*Simon Ståhlberg,Hector Geffner*

Main category: cs.AI

TL;DR: 该研究探索在目标条件强化学习和广义规划中使用一阶关系语言，通过基于原子集表示的状态和目标，利用事后经验回放技术自动创建难度递增的子目标课程，从而在大型稀疏奖励规划实例中学习通用策略。


<details>
  <summary>Details</summary>
Motivation: 传统上，一阶关系语言主要用于紧凑表示MDP和学习通用策略。本研究旨在解决在大型训练实例中，当目标无法通过随机探索达成时，如何学习目标条件和通用策略的问题。

Method: 采用事后经验回放技术，并探索三种状态和目标表示方式：1) 目标作为完整状态；2) 目标作为原子子集；3) 目标作为这些子目标的提升版本。后两种方法通过自动创建难度递增的子目标课程来促进学习。

Result: 实验表明，后两种表示方法（目标作为原子子集及其提升版本）能够在大型稀疏奖励规划实例中成功学习通用策略，通过自动创建难度递增的目标课程实现计算效率提升。

Conclusion: 基于原子集表示的目标条件强化学习能够有效处理大型稀疏奖励问题，通过自动课程学习机制提高学习效率，但仍有局限性需要进一步研究解决。

Abstract: First-order relational languages have been used in MDP planning and reinforcement learning (RL) for two main purposes: specifying MDPs in compact form, and representing and learning policies that are general and not tied to specific instances or state spaces. In this work, we instead consider the use of first-order languages in goal-conditioned RL and generalized planning. The question is how to learn goal-conditioned and general policies when the training instances are large and the goal cannot be reached by random exploration alone. The technique of Hindsight Experience Replay (HER) provides an answer to this question: it relabels unsuccessful trajectories as successful ones by replacing the original goal with one that was actually achieved. If the target policy must generalize across states and goals, trajectories that do not reach the original goal states can enable more data- and time-efficient learning. In this work, we show that further performance gains can be achieved when states and goals are represented by sets of atoms. We consider three versions: goals as full states, goals as subsets of the original goals, and goals as lifted versions of these subgoals. The result is that the latter two successfully learn general policies on large planning instances with sparse rewards by automatically creating a curriculum of easier goals of increasing complexity. The experiments illustrate the computational gains of these versions, their limitations, and opportunities for addressing them.

</details>


### [71] [Learning General Policies with Policy Gradient Methods](https://arxiv.org/abs/2512.19366)
*Simon Ståhlberg,Blai Bonet,Hector Geffner*

Main category: cs.AI

TL;DR: 该研究探索了深度强化学习（DRL）如何学习像组合方法那样可泛化的策略，通过结合状态转移分类器和图神经网络（GNNs）来学习可泛化的规划策略。


<details>
  <summary>Details</summary>
Motivation: 虽然强化学习在许多场景中取得了显著成果，但策略的泛化能力仍然是一个挑战。经典规划中的组合方法可以学习出对给定领域所有实例都有效的策略，而深度强化学习在泛化方面存在局限。本研究旨在探索深度强化学习方法在什么条件下能够学习出像组合方法那样可泛化的策略。

Method: 结合组合方法和深度学习的优势：1）将策略建模为状态转移分类器（因为基础动作在不同实例中会变化）；2）使用图神经网络（GNNs）来处理关系结构，表示规划状态的价值函数和策略；3）采用actor-critic方法学习策略；4）通过添加派生谓词和替代成本结构来增强表达能力和优化泛化与最优性的权衡。

Result: actor-critic方法可以学习出与组合方法几乎一样好的可泛化策略，同时避免了可扩展性瓶颈和特征池的使用。DRL方法的局限性主要来自GNN的表达能力限制以及泛化与最优性之间的权衡，而不是深度学习或强化学习算法本身的问题。

Conclusion: 深度强化学习方法可以学习出高度可泛化的策略，接近组合方法的性能。通过适当的架构设计（状态转移分类器和GNNs）和增强技术（派生谓词和替代成本结构），可以克服DRL在泛化方面的主要限制，实现可靠的策略泛化。

Abstract: While reinforcement learning methods have delivered remarkable results in a number of settings, generalization, i.e., the ability to produce policies that generalize in a reliable and systematic way, has remained a challenge. The problem of generalization has been addressed formally in classical planning where provable correct policies that generalize over all instances of a given domain have been learned using combinatorial methods. The aim of this work is to bring these two research threads together to illuminate the conditions under which (deep) reinforcement learning approaches, and in particular, policy optimization methods, can be used to learn policies that generalize like combinatorial methods do. We draw on lessons learned from previous combinatorial and deep learning approaches, and extend them in a convenient way. From the former, we model policies as state transition classifiers, as (ground) actions are not general and change from instance to instance. From the latter, we use graph neural networks (GNNs) adapted to deal with relational structures for representing value functions over planning states, and in our case, policies. With these ingredients in place, we find that actor-critic methods can be used to learn policies that generalize almost as well as those obtained using combinatorial approaches while avoiding the scalability bottleneck and the use of feature pools. Moreover, the limitations of the DRL methods on the benchmarks considered have little to do with deep learning or reinforcement learning algorithms, and result from the well-understood expressive limitations of GNNs, and the tradeoff between optimality and generalization (general policies cannot be optimal in some domains). Both of these limitations are addressed without changing the basic DRL methods by adding derived predicates and an alternative cost structure to optimize.

</details>


### [72] [EchoTrail-GUI: Building Actionable Memory for GUI Agents via Critic-Guided Self-Exploration](https://arxiv.org/abs/2512.19396)
*Runze Li,Yuwen Zhai,Bo Xu,LiWu Xu,Nian Shi,Wei Zhang,Ran Lin,Liang Wang*

Main category: cs.AI

TL;DR: EchoTrail-GUI是一个为GUI代理设计的记忆框架，通过自动化构建任务轨迹数据库、检索相关记忆并注入上下文指导，实现类似人类的经验学习，显著提升任务成功率和操作效率。


<details>
  <summary>Details</summary>
Motivation: 当前GUI代理存在"数字健忘症"问题，每个任务孤立处理，无法从过去的成功经验中学习，导致性能不佳、重复错误和泛化能力差。需要让代理具备动态可访问的记忆机制。

Method: 三阶段框架：1) 经验探索：代理自主与GUI环境交互，构建经过奖励模型验证的成功任务轨迹数据库；2) 记忆注入：接收新任务时，高效检索最相关的过去轨迹作为"记忆"；3) GUI任务推理：将这些记忆作为上下文指导注入，影响代理的推理和决策过程。

Result: 在Android World和AndroidLab基准测试中，EchoTrail-GUI显著提高了基线代理的任务成功率和操作效率，验证了结构化记忆在创建更强大智能GUI自动化中的有效性。

Conclusion: 通过引入动态可访问的记忆机制，EchoTrail-GUI成功解决了GUI代理的"数字健忘症"问题，实现了类似人类的经验学习，为创建更鲁棒和智能的GUI自动化系统提供了有效框架。

Abstract: Contemporary GUI agents, while increasingly capable due to advances in Large Vision-Language Models (VLMs), often operate with a critical limitation: they treat each task in isolation, lacking a mechanism to systematically learn from past successes. This digital ''amnesia'' results in sub-optimal performance, repeated errors, and poor generalization to novel challenges. To bridge this gap, we introduce EchoTrail-GUI, a novel framework designed to mimic human-like experiential learning by equipping agents with a dynamic, accessible memory. Our framework operates in three distinct stages. First, during Experience Exploration, an agent autonomously interacts with GUI environments to build a curated database of successful task trajectories, validated by a reward model. Crucially, the entire knowledge base construction is thus fully automated, requiring no human supervision. Second, in the Memory Injection stage, upon receiving a new task, our system efficiently retrieves the most relevant past trajectories to serve as actionable ''memories''. Finally, during GUI Task Inference, these memories are injected as in-context guidance to inform the agent's reasoning and decision-making process. We demonstrate the efficacy of our approach on benchmarks including Android World and AndroidLab. The results show that EchoTrail-GUI significantly improves the task success rate and operational efficiency of baseline agents, validating the power of structured memory in creating more robust and intelligent GUI automation.

</details>


### [73] [An Agentic Framework for Autonomous Materials Computation](https://arxiv.org/abs/2512.19458)
*Zeyu Xia,Jinzhe Ma,Congjie Zheng,Shufei Zhang,Yuqiang Li,Hang Su,P. Hu,Changshui Zhang,Xingao Gong,Wanli Ouyang,Lei Bai,Dongzhan Zhou,Mao Su*

Main category: cs.AI

TL;DR: 开发了一个专门用于第一性原理材料计算的领域专业智能体，通过嵌入领域知识确保物理一致的多步骤工作流程，显著优于独立LLM


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在加速科学发现方面显示出潜力，但其静态知识和幻觉问题阻碍了自主研究应用。需要开发能够可靠执行复杂科学工作流程的智能体系统

Method: 开发了一个领域专业智能体，嵌入材料计算领域的专业知识，确保物理一致的多步骤工作流程，并选择收敛、良定的参数，实现可靠端到端计算执行

Result: 在多样化的计算任务基准测试中，该系统在准确性和鲁棒性方面显著优于独立的大型语言模型

Conclusion: 这项工作为自主计算实验建立了可验证的基础，代表了向完全自动化科学发现迈出的关键一步

Abstract: Large Language Models (LLMs) have emerged as powerful tools for accelerating scientific discovery, yet their static knowledge and hallucination issues hinder autonomous research applications. Recent advances integrate LLMs into agentic frameworks, enabling retrieval, reasoning, and tool use for complex scientific workflows. Here, we present a domain-specialized agent designed for reliable automation of first-principles materials computations. By embedding domain expertise, the agent ensures physically coherent multi-step workflows and consistently selects convergent, well-posed parameters, thereby enabling reliable end-to-end computational execution. A new benchmark of diverse computational tasks demonstrates that our system significantly outperforms standalone LLMs in both accuracy and robustness. This work establishes a verifiable foundation for autonomous computational experimentation and represents a key step toward fully automated scientific discovery.

</details>


### [74] [QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models](https://arxiv.org/abs/2512.19526)
*Li Puyin,Tiange Xiang,Ella Mao,Shirley Wei,Xinye Chen,Adnan Masood,Li Fei-fei,Ehsan Adeli*

Main category: cs.AI

TL;DR: QuantiPhy是首个评估视觉语言模型物理推理能力的定量基准，包含3.3K+视频-文本实例，测试模型对物体尺寸、速度、加速度的数值估计能力，揭示当前VLMs在定性合理性与数值准确性间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要基于VQA和定性分析，无法确定视觉感知模型是否能从视频观察中定量推断运动物体的运动学量。需要建立标准化基准来测量VLMs的物理推理能力。

Method: 创建QuantiPhy基准，包含3.3K+视频-文本实例和数值真值，评估模型在给定时间戳估计物体尺寸、速度、加速度的能力，使用其中一项属性作为输入先验。标准化提示和评分以评估数值准确性。

Result: 实验显示最先进的VLMs在定性合理性和实际数值正确性之间存在一致差距。模型过度依赖预训练的世界知识，而非忠实使用提供的视觉和文本输入作为参考来推理运动学属性。

Conclusion: QuantiPhy提供了首个严格、可扩展的测试平台，推动VLMs超越口头合理性，实现基于数值的物理理解。当前VLMs在定量物理推理方面仍有不足，需要改进。

Abstract: Understanding the physical world is essential for generalist AI agents. However, it remains unclear whether state-of-the-art vision perception models (e.g., large VLMs) can reason physical properties quantitatively. Existing evaluations are predominantly VQA-based and qualitative, offering limited insight into whether these models can infer the kinematic quantities of moving objects from video observations. To address this, we present QuantiPhy, the first benchmark designed to quantitatively measure a VLM's physical reasoning ability. Comprising more than 3.3K video-text instances with numerical ground truth, QuantiPhy evaluates a VLM's performance on estimating an object's size, velocity, and acceleration at a given timestamp, using one of these properties as an input prior. The benchmark standardizes prompts and scoring to assess numerical accuracy, enabling fair comparisons across models. Our experiments on state-of-the-art VLMs reveal a consistent gap between their qualitative plausibility and actual numerical correctness. We further provide an in-depth analysis of key factors like background noise, counterfactual priors, and strategic prompting and find that state-of-the-art VLMs lean heavily on pre-trained world knowledge rather than faithfully using the provided visual and textual inputs as references when reasoning kinematic properties quantitatively. QuantiPhy offers the first rigorous, scalable testbed to move VLMs beyond mere verbal plausibility toward a numerically grounded physical understanding.

</details>


### [75] [Towards Closed-Loop Embodied Empathy Evolution: Probing LLM-Centric Lifelong Empathic Motion Generation in Unseen Scenarios](https://arxiv.org/abs/2512.19551)
*Jiawen Wang,Jingjing Wang Tianyang Chen,Min Zhang,Guodong Zhou*

Main category: cs.AI

TL;DR: 提出L^2-EMG任务，让LLMs能在不同未见场景中持续学习情感动作生成，解决情感解耦和场景适应两大挑战，通过ES-MoE方法在多个数据集上取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有情感动作生成方法主要关注单一固定尺度数据集，忽视了灵活且尺度递增的运动场景（如体育、舞蹈）。有效学习这些新兴场景能显著提升模型在真实世界的泛化能力，有助于构建具有共情和智能的闭环自进化具身智能体。

Method: 提出ES-MoE方法，包含因果引导的情感解耦模块和场景适应的专家构建模块。前者解决情感解耦挑战，后者解决场景适应挑战。构建多个L^2-EMG数据集进行验证。

Result: 广泛评估表明ES-MoE方法优于先进基线模型，在多个L^2-EMG数据集上验证了其有效性。

Conclusion: 提出了L^2-EMG新任务和ES-MoE解决方案，成功解决了情感动作生成中的持续学习问题，为构建具有共情能力的自进化具身智能体提供了重要基础。

Abstract: In the literature, existing human-centric emotional motion generation methods primarily focus on boosting performance within a single scale-fixed dataset, largely neglecting the flexible and scale-increasing motion scenarios (e.g., sports, dance), whereas effectively learning these newly emerging scenarios can significantly enhance the model's real-world generalization ability. Inspired by this, this paper proposes a new LLM-Centric Lifelong Empathic Motion Generation (L^2-EMG) task, which aims to equip LLMs with the capability to continually acquire emotional motion generation knowledge across different unseen scenarios, potentially contributing to building a closed-loop and self-evolving embodied agent equipped with both empathy and intelligence. Further, this paper poses two key challenges in the L^2-EMG task, i.e., the emotion decoupling challenge and the scenario adapting challenge. To this end, this paper proposes an Emotion-Transferable and Scenario-Adapted Mixture of Experts (ES-MoE) approach which designs a causal-guided emotion decoupling block and a scenario-adapted expert constructing block to address the two challenges, respectively. Especially, this paper constructs multiple L^2-EMG datasets to validate the effectiveness of the ES-MoE approach. Extensive evaluations show that ES-MoE outperforms advanced baselines.

</details>


### [76] [Augmenting Intelligence: A Hybrid Framework for Scalable and Stable Explanations](https://arxiv.org/abs/2512.19557)
*Lawrence Krukrubo,Julius Odede,Olawande Olusegun*

Main category: cs.AI

TL;DR: 提出混合LRR-TED框架解决XAI的可扩展性-稳定性困境，通过"发现不对称性"概念，在客户流失预测中实现94%准确率，同时将人工标注工作量减少50%。


<details>
  <summary>Details</summary>
Motivation: 当前可解释AI方法面临"可扩展性-稳定性困境"：后处理方法（如LIME、SHAP）可扩展但不稳定，监督解释框架（如TED）稳定但需要大量人工标注。需要一种平衡的方法来解决这一矛盾。

Method: 提出混合LRR-TED框架，利用"发现不对称性"概念。首先用自动化规则学习器（GLRM）识别广泛的"安全网"（保留模式），然后仅用4个人工定义的帕累托最优风险规则来补充，将专家从"规则编写者"转变为"异常处理者"。

Result: 在客户流失预测中达到94.00%的预测准确率，优于完整的8规则人工专家基线，同时将人工标注工作量减少了50%。验证了"安娜·卡列尼娜流失原则"：自动化方法擅长发现安全模式但难以捕捉具体风险触发因素。

Conclusion: 该框架成功解决了XAI的可扩展性-稳定性困境，提出了一种新的人机协同AI范式：专家从规则编写者转变为异常处理者，通过少量人工干预显著提升系统性能，为可解释AI提供了更实用的解决方案。

Abstract: Current approaches to Explainable AI (XAI) face a "Scalability-Stability Dilemma." Post-hoc methods (e.g., LIME, SHAP) may scale easily but suffer from instability, while supervised explanation frameworks (e.g., TED) offer stability but require prohibitive human effort to label every training instance. This paper proposes a Hybrid LRR-TED framework that addresses this dilemma through a novel "Asymmetry of Discovery." When applied to customer churn prediction, we demonstrate that automated rule learners (GLRM) excel at identifying broad "Safety Nets" (retention patterns) but struggle to capture specific "Risk Traps" (churn triggers)-a phenomenon we term the Anna Karenina Principle of Churn. By initialising the explanation matrix with automated safety rules and augmenting it with a Pareto-optimal set of just four human-defined risk rules, our approach achieves 94.00% predictive accuracy. This configuration outperforms the full 8-rule manual expert baseline while reducing human annotation effort by 50%, proposing a shift in the paradigm for Human-in-the-Loop AI: moving experts from the role of "Rule Writers" to "Exception Handlers."

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [77] [Characterising Behavioural Families and Dynamics of Promotional Twitter Bots via Sequence-Based Modelling](https://arxiv.org/abs/2512.18077)
*Ohoud Alzahrani,Russell Beale,Robert J. Hendley*

Main category: cs.SI

TL;DR: 该研究分析促销Twitter/X机器人的行为家族分类及其演化模式，通过"数字DNA"序列编码和突变分析揭示不同家族的行为特征和演化规律。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究促销Twitter/X机器人是否形成行为家族，以及家族成员是否具有相似的演化模式。现有研究缺乏对机器人行为长期演化的系统性分析，特别是基于序列相似性的家族分类和突变机制研究。

Method: 方法包括：1) 收集2,615个真实促销机器人账户的2,798,672条推文(2006-2021)；2) 将每个机器人编码为"数字DNA"序列，基于7个分类行为特征；3) 使用非重叠块(k=7)、余弦相似度和层次聚类识别行为家族；4) 通过多序列比对(MSA)分析突变事件(插入、删除、替换等)；5) 量化突变率、热点区域和家族特异性模式。

Result: 识别出四个行为家族：独特推文者、带URL的复制者、内容倍增者、知情贡献者。家族间共享行为核心但策略不同，突变分析显示删除和替换占主导，插入罕见，突变热点分布因家族而异。同一家族内机器人共享更多突变，对外部事件的响应遵循家族特异性模式。

Conclusion: 基于序列的家族建模和突变分析为促销机器人行为的时间演化提供了细粒度解释框架。该方法能有效识别行为家族、量化演化模式，并预测机器人对外部事件的响应，为机器人检测和演化分析提供了新视角。

Abstract: This paper asks whether promotional Twitter/X bots form behavioural families and whether members evolve similarly. We analyse 2,798,672 tweets from 2,615 ground-truth promotional bot accounts (2006-2021), focusing on complete years 2009 to 2020. Each bot is encoded as a sequence of symbolic blocks (``digital DNA'') from seven categorical post-level behavioural features (posting action, URL, media, text duplication, hashtags, emojis, sentiment), preserving temporal order only. Using non-overlapping blocks (k=7), cosine similarity over block-frequency vectors, and hierarchical clustering, we obtain four coherent families: Unique Tweeters, Duplicators with URLs, Content Multipliers, and Informed Contributors. Families share behavioural cores but differ systematically in engagement strategies and life-cycle dynamics (beginning/middle/end). We then model behavioural change as mutations. Within each family we align sequences via multiple sequence alignment (MSA) and label events as insertions, deletions, substitutions, alterations, and identity. This quantifies mutation rates, change-prone blocks/features, and mutation hotspots. Deletions and substitutions dominate, insertions are rare, and mutation profiles differ by family, with hotspots early for some families and dispersed for others. Finally, we test predictive value: bots within the same family share mutations more often than bots across families; closer bots share and propagate mutations more than distant ones; and responses to external triggers (e.g., Christmas, Halloween) follow family-specific, partly predictable patterns. Overall, sequence-based family modelling plus mutation analysis provides a fine-grained account of how promotional bot behaviour adapts over time.

</details>


### [78] [Network Analysis of Cyberbullying Interactions on Instagram](https://arxiv.org/abs/2512.18116)
*Satyaki Sikdar,Manuel Sandoval,Taylor Hales,Chloe Kilroy,Maddie Juarez,Tyler Rosario,Juan J. Rosendo,Deborah L. Hall,Yasin N. Silva*

Main category: cs.SI

TL;DR: 该研究从网络科学视角分析Instagram上的网络欺凌互动模式，通过构建会话图、计算欺凌分数和受害者分数，发现大多数网络欺凌会话中受害者获得的支持少于攻击，而欺凌者受到的抵制也少于攻击。


<details>
  <summary>Details</summary>
Motivation: 网络欺凌日益普遍且影响广泛，但缺乏对Instagram平台上网络欺凌互动模式的细粒度网络分析。研究旨在从网络科学角度理解网络欺凌的交互结构模式。

Method: 使用包含400多个Instagram帖子的标注数据集，构建会话图（节点代表用户及其欺凌角色，边代表评论交流）。计算欺凌分数（攻击减去抵制）和受害者分数（支持减去攻击）。通过子图枚举分析最常见的互动模式。

Result: 大多数网络欺凌会话的受害者分数为负（攻击多于支持），欺凌分数分布略偏正（攻击多于抵制）。欺凌者是最常见的角色，但防御者也持续存在，表明欺凌缓解是许多互动的结构性特征。

Conclusion: 这是首次在Instagram上使用会话和评论级别的人工标注来探索网络欺凌的细粒度网络交互，揭示了网络欺凌互动中的结构性模式，为理解网络欺凌动态提供了新视角。

Abstract: Cyberbullying continues to grow in prevalence and its impact is felt by thousands worldwide. This study seeks a network science perspective on cyberbullying interaction patterns on the popular photo and video-sharing platform, Instagram. Using an annotated cyberbullying dataset containing over 400 Instagram posts, we outline a set of heuristics for building Session Graphs, where nodes represent users and their cyberbullying role, and edges represent their exchanged communications via comments. Over these graphs, we compute the Bully Score, a measure of the net malice introduced by bullies as they attack victims (attacks minus pushback), and the Victim Score, a measure of the net support victims receive from their defenders (support minus attacks). Utilizing small subgraph (motif) enumeration, our analysis uncovers the most common interaction patterns over all cyberbullying sessions. We also explore the prevalence of specific motif patterns across different ranges of Bully and Victim Scores. We find that a majority of cyberbullying sessions have negative Victim Scores (attacks outweighing support), while the Bully Score distribution has a slight positive skew (attacks outweighing pushback). We also observe that while bullies are the most common role in motifs, defenders are also consistently present. This suggests that bullying mitigation is a recurring structural feature of many interactions. To the best of our knowledge, this is the first study to explore this granular scale of network interactions using human annotations at the session and comment levels on Instagram.

</details>


### [79] [Analyzing Crime Discourse in U.S. Metropolitan Communities on Reddit: Trends, Influences, and Insights](https://arxiv.org/abs/2512.18227)
*Deepit Sapru*

Main category: cs.SI

TL;DR: 研究分析了美国384个大都市区的Reddit社区犯罪讨论，发现犯罪讨论在大型都市区和政治倾向更自由的社区更普遍，但犯罪率对讨论频率影响不大。


<details>
  <summary>Details</summary>
Motivation: 传统媒体对犯罪与公众认知关系的研究较多，但社交媒体作为用户驱动讨论的新平台，其对犯罪相关话语的影响研究仍然有限。本研究旨在填补这一空白，探索社交媒体如何塑造公众对犯罪的叙事。

Method: 分析美国384个大都市区Reddit社区的用户提交内容，识别犯罪相关讨论的关键趋势，包括讨论频率与都市规模、政治倾向的关系，以及犯罪率对讨论的影响。

Result: 犯罪讨论在大型都市区和政治倾向更自由的社区更普遍；有趣的是，报告的犯罪率对讨论频率或强度没有显著影响；社交媒体平台正在成为塑造公众犯罪叙事的重要媒介。

Conclusion: 社交媒体平台如Reddit正在成为塑造公众犯罪叙事的重要数字空间，其影响力与传统媒体不同，需要更多研究来理解数字空间作为公众认知中介的作用。

Abstract: The relationship between crime and the media has long been a focal point of academic research, with traditional media playing a significant role in shaping public perceptions of safety and community well-being. However, the advent of social media has introduced a new dimension to this discourse, offering unique platforms for user-driven discussions. Despite the prominence of social media, research examining its impact on crime-related discourse remains limited. This paper investigates crime-related discussions across Reddit communities representing 384 Metropolitan Areas in the United States. By analyzing user submissions, we identify key trends in crime discourse, including the higher prevalence of such discussions in larger metropolitan areas and communities with more liberal political leanings. Interestingly, we find that reported crime rates do not strongly influence the frequency or intensity of these discussions. These findings provide novel insights into how social media platforms, like Reddit, shape public narratives around crime, highlighting the growing need to examine digital spaces as influential mediators of public perception.

</details>


### [80] [Needles in a haystack: using forensic network science to uncover insider trading](https://arxiv.org/abs/2512.18918)
*Gian Jaeger,Wang Ngai Yeung,Renaud Lambiotte*

Main category: cs.SI

TL;DR: 提出基于数据驱动网络的方法，通过分析美国证券交易委员会报告的290万笔内幕交易数据，检测内幕交易行为


<details>
  <summary>Details</summary>
Motivation: 内幕交易检测面临标注数据有限的挑战，需要开发新的方法来识别协调性内幕交易行为

Method: 构建基于时间相似性的加权网络，分析内幕交易者之间的关联，通过中心节点和异常子图检测内幕交易模式

Result: 方法能够有效检测出行为异常的内幕交易者对或集群，表明存在内幕交易或市场操纵行为

Conclusion: 提出的数据驱动网络方法为内幕交易检测提供了有效工具，能够识别协调性交易行为

Abstract: Although the automation and digitisation of anti-financial crime investigation has made significant progress in recent years, detecting insider trading remains a unique challenge, partly due to the limited availability of labelled data. To address this challenge, we propose using a data-driven networks approach that flags groups of corporate insiders who report coordinated transactions that are indicative of insider trading. Specifically, we leverage data on 2.9 million trades reported to the U.S. Securities and Exchange Commission (SEC) by company insiders (C-suite executives, board members and major shareholders) between 2014 and 2024. Our proposed algorithm constructs weighted edges between insiders based on the temporal similarity of their trades over the 10-year timeframe. Within this network we then uncover trends that indicate insider trading by focusing on central nodes and anomalous subgraphs. To highlight the validity of our approach we evaluate our findings with reference to two null models, generated by running our algorithm on synthetic empirically calibrated and shuffled datasets. The results indicate that our approach can be used to detect pairs or clusters of insiders whose behaviour suggests insider trading and/or market manipulation.

</details>


### [81] [A Reverse Reachable Set Based Approach for Motif Oriented Profit maximization in Social Networks](https://arxiv.org/abs/2512.19237)
*Poonam Sharma,Suman Banerjee*

Main category: cs.SI

TL;DR: 本文提出了一种新的利润最大化问题变体——面向模体的利润最大化问题，其中利润与图中的模体（特定子图结构）相关联，当模体中达到阈值数量的节点被激活时即可获得利润。


<details>
  <summary>Details</summary>
Motivation: 传统的社会媒体营销利润最大化问题关注节点激活带来的利润，但实际应用中利润往往与特定的网络结构（模体）相关。本文旨在解决如何通过选择有限的种子节点来最大化模体导向的利润。

Method: 提出基于反向可达集（Reverse Reachable Set）的框架，包含三个主要步骤：KPT估计和RR集生成、种子集选择、模体导向利润估计。该方法考虑了利润与模体结构的关联性。

Result: 实验证明该问题是NP难问题。在真实世界社交媒体数据集上的实验表明，所提方法选择的种子集相比现有方法能产生更多利润。

Conclusion: 本文成功提出了面向模体的利润最大化问题及其解决方案，通过将利润与网络模体结构关联，改进了传统利润最大化方法，在实际应用中表现出更好的性能。

Abstract: Profit Maximization is one of the key objectives for social media marketing, where the task is to choose a limited number of highly influential nodes such that their initial activation leads to maximum profit. In this paper, we introduce a variant of the Profit Maximization Problem where we consider that instead of nodes, benefits are assigned to some of the motifs of the graph, and these benefit values can be earned once a given threshold count of nodes from the motifs is influenced. The goal here is to choose a limited number of nodes for initial activation called seed nodes such that the motif-oriented profit gets maximized. Formally, we call our problem the Motif Oriented Profit Maximization Problem. We show that the problem is NP-hard to solve optimally. We propose a Reverse Reachable Set-based framework to solve our problem. The proposed methodology broadly divides into three steps: KPT Estimation and RR Set generation, Seed Set Selection, and Motif Oriented Profit Estimation. The proposed methodology has been analyzed to understand its time and space requirements. It has been implemented with real-world social network datasets, and the results are reported. We observe that the seed set selected by the proposed solution approaches leads to more profit compared to the seed sets selected by the existing methods. The whole implementation and data are available at: https://github.com/PoonamSharma-PY/MotifProfit.

</details>


### [82] [Laplacian Network Optimization via Information Functions](https://arxiv.org/abs/2512.19279)
*Samuel Rosa,Radoslav Harman*

Main category: cs.SI

TL;DR: 该论文提出了一种基于实验设计思想的拉普拉斯谱网络优化理论框架，引入信息函数概念，定义了Kiefer测度族，并开发了高效的秩一更新公式和边交换算法。


<details>
  <summary>Details</summary>
Motivation: 网络设计中需要优化鲁棒性和其他性能指标，这些指标通常依赖于拉普拉斯谱。现有方法缺乏统一的理论框架来处理不同的谱目标函数，需要更系统的方法来分析和优化这些性能度量。

Method: 1. 引入信息函数概念来形式化性能度量的理想性质；2. 提出Kiefer测度族，包含三种最常见的谱目标函数；3. 建立拉普拉斯优化问题的正则化重构；4. 计算Kiefer测度的方向导数，统一处理梯度和次梯度；5. 定义节点相异性度量；6. 推导Kiefer准则的秩一更新公式；7. 开发新的边交换网络优化方法。

Result: 1. 建立了统一的拉普拉斯谱优化理论框架；2. 证明了方向导数与节点相异性度量的联系；3. 开发了高效的秩一更新公式，降低了算法的时间复杂度；4. 提出了新的边交换优化算法，能够更有效地优化网络结构。

Conclusion: 该论文通过借鉴统计学中的实验设计思想，为拉普拉斯谱网络优化提供了系统的理论框架和高效算法。提出的Kiefer测度族统一了多种常见目标函数，而节点相异性度量和秩一更新公式为网络优化算法提供了理论基础和计算效率提升。

Abstract: Designing networks to optimize robustness and other performance metrics is a well-established problem with applications ranging from electrical engineering to communication networks. Many such performance measures rely on the Laplacian spectrum; notable examples include total effective resistance, the number of spanning trees, and algebraic connectivity. This paper advances the study of Laplacian-based network optimization by drawing on ideas from experimental design in statistics. We present a theoretical framework for analyzing performance measures by introducing the notion of information functions, which captures a set of their desirable properties. Then, we formulate a new parametric family of information functions, Kiefer's measures, which encompasses the three most common spectral objectives. We provide a regular reformulation of the Laplacian optimization problem, and we use this reformulation to compute directional derivatives of Kiefer's measures. The directional derivatives provide a unified treatment of quantities recurring in Laplacian optimization, such as gradients and subgradients, and we show that they are connected to Laplacian-based measures of node distance, which we call node dissimilarities. We apply the node dissimilarities to derive efficient rank-one update formulas for Kiefer's criteria, and to devise a new edge-exchange method for network optimization. These update formulas enable greedy and exchange algorithms with reduced asymptotic time complexity.

</details>


### [83] [A Computationally Efficient Framework for Overlapping Community Detection in Large Bipartite Graphs](https://arxiv.org/abs/2512.19426)
*Yue Zeng,Rong-Hua Li,Qiangqiang Dai,Guoren Wang*

Main category: cs.SI

TL;DR: 提出基于部分-BCPC的新方法，显著降低二分图社区检测的时间复杂度，性能提升近三个数量级


<details>
  <summary>Details</summary>
Motivation: 现实世界网络常具有二分结构，现有BCPC检测方法因最大二分团邻接图规模过大导致时间复杂度高

Method: 1) 使用部分-BCPC缩减MBAG规模；2) 提出基于(α,β)-二分团枚举的新方法，利用部分-BCPC剪枝枚举空间

Result: 实验表明新方法比现有方法快近三个数量级

Conclusion: 提出的部分-BCPC方法和基于枚举的新方法能高效解决二分图社区检测问题，大幅提升性能

Abstract: Community detection, which uncovers closely connected vertex groups in networks, is vital for applications in social networks, recommendation systems, and beyond. Real-world networks often have bipartite structures (vertices in two disjoint sets with inter-set connections), creating unique challenges on specialized community detection methods. Biclique percolation community (BCPC) is widely used to detect cohesive structures in bipartite graphs. A biclique is a complete bipartite subgraph, and a BCPC forms when maximal bicliques connect via adjacency (sharing an (alpha, beta)-biclique). Yet, existing methods for BCPC detection suffer from high time complexity due to the potentially massive maximal biclique adjacency graph (MBAG). To tackle this, we propose a novel partial-BCPC based solution, whose key idea is to use partial-BCPC to reduce the size of the MBAG. A partial-BCPC is a subset of BCPC. Maximal bicliques belonging to the same partial-BCPC must also belong to the same BCPC. Therefore, these maximal bicliques can be grouped as a single vertex in the MBAG, significantly reducing the size of the MBAG. Furthermore, we move beyond the limitations of MBAG and propose a novel BCPC detection approach based on (alpha, beta)-biclique enumeration. This approach detects BCPC by enumerating all (alpha, beta)-bicliques and connecting maximal bicliques sharing the same (alpha, beta)-biclique, which is the condition for maximal bicliques to be adjacent. It also leverages partial-BCPC to significantly prune the enumeration space of (alpha, beta)-biclique. Experiments show that our methods outperform existing methods by nearly three orders of magnitude.

</details>


### [84] [Detecting Coordinated Activities Through Temporal, Multiplex, and Collaborative Analysis](https://arxiv.org/abs/2512.19677)
*Letizia Iannucci,Elisa Muratore,Antonis Matakos,Mikko Kivelä*

Main category: cs.SI

TL;DR: 提出一个基于多路网络和时间感知协作模型的框架，用于检测在线协调活动，通过分析多个模态的时间行为模式来识别协调群体。


<details>
  <summary>Details</summary>
Motivation: 在在线内容广泛传播的时代，有效检测协调活动对于缓解信息操纵带来的潜在威胁至关重要。尽管在识别不真实和自动化账户方面已有进展，但单独分析参与影响活动的个体账户时，其行为可能不会显得异常。考虑到信息操作的协作性质，协调活动更适合通过群体账户间超越偶然同步性的相似时间行为模式来表征。

Method: 提出一个框架来建模跨多个在线模态的复杂协调模式：1) 使用多路网络将在线活动分解为不同的交互层，然后聚合各层的协调证据；2) 提出时间感知协作模型来捕捉每个模态的在线协调模式，该模型基于节点归一化协作模型，并通过指数衰减时间核来考虑不同时间间隔内协调动作的重复。

Result: 在多个包含不同协调活动的数据集上验证了该方法。结果表明，多路时间感知模型在识别协调群体方面表现出色，在协调活动检测任务上优于先前提出的方法。

Conclusion: 该框架通过结合多路网络分析和时间感知建模，能够有效检测在线协调活动，为识别信息操纵活动提供了更强大的工具。

Abstract: In the era of widespread online content consumption, effective detection of coordinated efforts is crucial for mitigating potential threats arising from information manipulation. Despite advances in isolating inauthentic and automated actors, the actions of individual accounts involved in influence campaigns may not stand out as anomalous if analyzed independently of the coordinated group. Given the collaborative nature of information operations, coordinated campaigns are better characterized by evidence of similar temporal behavioral patterns that extend beyond coincidental synchronicity across a group of accounts. We propose a framework to model complex coordination patterns across multiple online modalities. This framework utilizes multiplex networks to first decompose online activities into different interaction layers, and subsequently aggregate evidence of online coordination across the layers. In addition, we propose a time-aware collaboration model to capture patterns of online coordination for each modality. The proposed time-aware model builds upon the node-normalized collaboration model and accounts for repetitions of coordinated actions over different time intervals by employing an exponential decay temporal kernel. We validate our approach on multiple datasets featuring different coordinated activities. Our results demonstrate that a multiplex time-aware model excels in the identification of coordinating groups, outperforming previously proposed methods in coordinated activity detection.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [85] [Empirical parameterization of the Elo Rating System](https://arxiv.org/abs/2512.18013)
*Shirsa Maitra,Tathagata Banerjee,Anushka De,Diganta Mukherjee,Tridib Mukherjee*

Main category: stat.AP

TL;DR: 提出一种数据驱动的方法来优化Elo等评分系统的参数，通过最大化比赛结果预测准确率来学习最优参数值，适用于单人和多人游戏场景。


<details>
  <summary>Details</summary>
Motivation: 现有的Elo、Glicko、TrueSkill等评分系统通常基于概率假设或惯例选择参数，没有充分利用游戏特定数据，这限制了其预测准确性。

Method: 提出一个参数调优框架，通过最大化比赛结果预测准确率来学习评分系统的最优参数值。该方法具有通用性，可通过适当修改参数空间扩展到任何评分系统，包括多人游戏设置。

Result: 在真实和模拟的游戏数据上实施该评分系统，证明了数据驱动评分系统在建模玩家表现方面的适用性。

Conclusion: 数据驱动的参数调优方法能够显著提高评分系统的预测性能，为游戏评分系统的优化提供了有效的通用框架。

Abstract: This study aims to provide a data-driven approach for empirically tuning and validating rating systems, focusing on the Elo system. Well-known rating frameworks, such as Elo, Glicko, TrueSkill systems, rely on parameters that are usually chosen based on probabilistic assumptions or conventions, and do not utilize game-specific data. To address this issue, we propose a methodology that learns optimal parameter values by maximizing the predictive accuracy of match outcomes. The proposed parameter-tuning framework is a generalizable method that can be extended to any rating system, even for multiplayer setups, through suitable modification of the parameter space. Implementation of the rating system on real and simulated gameplay data demonstrates the suitability of the data-driven rating system in modeling player performance.

</details>


### [86] [Distribution-Free Selection of Low-Risk Oncology Patients for Survival Beyond a Time Horizon](https://arxiv.org/abs/2512.18118)
*Matteo Sesia,Vladimir Svetnik*

Main category: stat.AP

TL;DR: 该研究比较了两种为生存模型提供分布无关保证的方法：高概率风险控制和基于期望的错误发现率控制，用于筛选在特定时间范围内不太可能发生事件的病人。


<details>
  <summary>Details</summary>
Motivation: 在医学中，需要筛选出在特定时间范围内不太可能发生事件的病人，用于治疗降级决策和医疗资源分配。现有方法需要提供可靠的统计保证，但不同方法在保证类型和效率上存在差异。

Method: 比较两种分布无关保证方法：(1) 高概率风险控制；(2) 使用conformal p-values的期望型错误发现率控制。两种方法都使用逆概率加权处理删失数据，并在半合成和真实肿瘤数据上进行实验。

Result: 两种方法都能达到被选病人中的期望生存率，但效率特征不同：conformal方法通常更有效力，而高概率风险控制提供更强的保证但更保守。研究还提供了实际实施和参数调优的指导。

Conclusion: 两种方法各有优劣：conformal方法更有效力，高概率风险控制提供更强保证但更保守。研究为实际应用提供了选择依据和实现指导，帮助医学决策者根据具体需求选择合适的方法。

Abstract: We study the problem of selecting a subset of patients who are unlikely to experience an event within a specified time horizon, by calibrating a screening rule based on the output of a black-box survival model. This statistics problem has many applications in medicine, including identifying candidates for treatment de-escalation and prioritizing the allocation of limited medical resources. In this paper, we compare two families of methods that can provide different types of distribution-free guarantees for this task: (i) high-probability risk control and (ii) expectation-based false discovery rate control using conformal $p$-values. We clarify the relation between these two frameworks, which have important conceptual differences, and explain how each can be adapted to analyze time-to-event data using inverse probability of censoring weighting. Through experiments on semi-synthetic and real oncology data from the Flatiron Health Research Database, we find that both approaches often achieve the desired survival rate among selected patients, but with distinct efficiency profiles. The conformal method tends to be more powerful, whereas high-probability risk control offers stronger guarantees at the cost of some additional conservativeness. Finally, we provide practical guidance on implementation and parameter tuning.

</details>


### [87] [Analysing Skill Predominance in Generalized Fantasy Cricket](https://arxiv.org/abs/2512.18467)
*Supratim Das,Sarthak Sarkar,Subhamoy Maitra,Tridib Mukherjee*

Main category: stat.AP

TL;DR: 该研究通过模拟实验和真实IPL数据，分析幻想板球游戏中策略选择与随机选择的差异，证明策略性团队选择能持续超越随机选择，存在明显的技能优势。


<details>
  <summary>Details</summary>
Motivation: 随着幻想板球在印度日益流行，需要明确成功是源于技能还是运气，这既是分析问题也是监管问题。研究旨在评估在这种有限选择框架下是否出现可测量的技能。

Method: 引入新的有限选择竞赛框架，参与者从四个专家设计的团队中选择，基于最高累积分数分享奖金。结合模拟实验和2024年印度超级联赛(IPL)的真实表现数据。

Result: 结果显示策略性和信息充分的团队选择持续优于随机选择，存在明显的技能优势，尽管存在随机变异性。分析量化了团队组成、团队间相关性和参与者行为如何共同影响获胜概率。

Conclusion: 研究为寻求通过策略最大化回报的玩家提供了可行见解，也为平台设计者开发公平、透明、吸引人的技能型游戏生态系统提供了指导，这些生态系统需要在竞争与监管合规之间取得平衡。

Abstract: In fantasy sports, strategic thinking-not mere luck-often defines who wins and who falls short. As fantasy cricket grows in popularity across India, understanding whether success stems from skill or chance has become both an analytical and regulatory question. This study introduces a new limited-selection contest framework in which participants choose from four expert-designed teams and share prizes based on the highest cumulative score. By combining simulation experiments with real performance data from the 2024 Indian Premier League (IPL), we evaluate whether measurable skill emerges within this structure. Results reveal that strategic and informed team selection consistently outperforms random choice, underscoring a clear skill advantage that persists despite stochastic variability. The analysis quantifies how team composition, inter-team correlation, and participant behaviour jointly influence winning probabilities, highlighting configurations where skill becomes statistically dominant. These findings provide actionable insights for players seeking to maximise returns through strategy and for platform designers aiming to develop fair, transparent, and engaging skill-based gaming ecosystems that balance competition with regulatory compliance.

</details>


### [88] [Functional Modeling of Learning and Memory Dynamics in Cognitive Disorders](https://arxiv.org/abs/2512.18760)
*Maria Laura Battagliola,Laura J. Benoit,Sarah Canetta,Shizhe Zhang,R. Todd Ogden*

Main category: stat.AP

TL;DR: 该研究通过功能数据分析方法，从动物工作记忆任务的二元成功/失败数据中估计连续的成功概率曲线，并通过曲线配准将学习过程分解为振幅（整体表现）和相位（学习速度），以分析认知障碍如何影响学习表现和学习速度。


<details>
  <summary>Details</summary>
Motivation: 工作记忆缺陷是许多认知障碍的标志，但传统方法难以区分认知障碍是影响学习表现还是学习速度。研究旨在开发一种能够分离这两个维度的分析方法，以更精确地理解认知障碍的机制。

Method: 采用功能数据分析方法，从重复的二元成功/失败数据中估计连续的成功概率曲线。通过曲线配准技术将每个函数分解为振幅（代表整体表现水平）和相位（代表学习或反应速度）。

Result: 该方法能够成功分离学习速度和表现水平，允许研究者分析这两个维度如何共同变化，并分别比较它们，从而确定组间差异是源于峰值表现缺陷还是速度变化。

Conclusion: 该分析方法为理解认知障碍提供了更精细的视角，能够区分认知障碍是影响学习表现还是学习速度，有助于更精确地诊断和干预认知障碍。

Abstract: Deficits in working memory, which includes both the ability to learn and to retain information short-term, are a hallmark of many cognitive disorders. Our study analyzes data from a neuroscience experiment on animal subjects, where performance on a working memory task was recorded as repeated binary success or failure data. We estimate continuous probability of success curves from this binary data in the context of functional data analysis, which is largely used in biological processes that are intrinsically continuous. We then register these curves to decompose each function into its amplitude, representing overall performance, and its phase, representing the speed of learning or response. Because we are able to separate speed from performance, we can address the crucial question of whether a cognitive disorder impacts not only how well subjects can learn and remember, but also how fast. This allows us to analyze the components jointly to uncover how speed and performance co-vary, and to compare them separately to pinpoint whether group differences stem from a deficit in peak performance or a change in speed.

</details>


### [89] [Dyadic Flow Models for Nonstationary Gene Flow in Landscape Genomics](https://arxiv.org/abs/2512.19035)
*Michael R. Schwob,Nicholas M. Calzada,Justin J. Van Ee,Diana Gamba,Rebecca A. Nelson,Megan L. Vahsen,Peter B. Adler,Jesse R. Lasky,Mevin B. Hooten*

Main category: stat.AP

TL;DR: 提出了一种新的景观基因组学框架，通过引入源-目的地对的空间变化系数（DSVCs）来捕捉非平稳和不对称的基因流，特别适用于入侵物种和范围转移物种。


<details>
  <summary>Details</summary>
Motivation: 传统景观基因组学模型主要基于距离隔离和阻力隔离假设，但这些假设在某些情况下（如入侵物种）不适用，因为入侵物种的基因流可能受到奠基者效应、多次引入等机制影响，这些机制在现有模型中未被考虑。

Method: 扩展了二元模型，引入了源-目的地对的空间变化系数（DSVCs），允许景观对基因流的影响在空间上变化，捕捉非平稳和不对称的连通性。同时纳入了明确的景观特征作为连通性协变量，这些协变量定位在空间域的特定区域，可能作为基因流的屏障或走廊。

Result: 提出的框架能够容纳物种定殖特定过程，同时保持评估景观对基因流影响的能力。通过对高度入侵的雀麦（Bromus tectorum）的案例研究，证明了在范围转移物种中考虑非平稳基因流的必要性。

Conclusion: 新的DSVC框架为景观基因组学提供了更灵活的建模方法，特别适用于入侵物种和范围转移物种，能够捕捉传统模型无法处理的非平稳和不对称基因流模式。

Abstract: The field of landscape genomics aims to infer how landscape features affect gene flow across space. Most landscape genomic frameworks assume the isolation-by-distance and isolation-by-resistance hypotheses, which propose that genetic dissimilarity increases as a function of distance and as a function of cumulative landscape resistance, respectively. While these hypotheses are valid in certain settings, other mechanisms may affect gene flow. For example, the gene flow of invasive species may depend on founder effects and multiple introductions. Such mechanisms are not considered in modern landscape genomic models. We extend dyadic models to allow for mechanisms that range-shifting and/or invasive species may experience by introducing dyadic spatially-varying coefficients (DSVCs) defined on source-destination pairs. The DSVCs allow the effects of landscape on gene flow to vary across space, capturing nonstationary and asymmetric connectivity. Additionally, we incorporate explicit landscape features as connectivity covariates, which are localized to specific regions of the spatial domain and may function as barriers or corridors to gene flow. Such covariates are central to colonization and invasion, where spread accelerates along corridors and slows across landscape barriers. The proposed framework accommodates colonization-specific processes while retaining the ability to assess landscape influences on gene flow. Our case study of the highly invasive cheatgrass (Bromus tectorum) demonstrates the necessity of accounting for nonstationarity gene flow in range-shifting species.

</details>


### [90] [Unraveling time-varying causal effects of multiple exposures: integrating Functional Data Analysis with Multivariable Mendelian Randomization](https://arxiv.org/abs/2512.19064)
*Nicole Fontana,Francesca Ieva,Luisa Zuccolo,Emanuele Di Angelantonio,Piercesare Secchi*

Main category: stat.AP

TL;DR: MV-FMR扩展功能孟德尔随机化，同时建模多个时变暴露，结合功能主成分分析和数据驱动的交叉验证，能恢复时变因果效应并优于单变量方法。


<details>
  <summary>Details</summary>
Motivation: 许多暴露的因果效应随时间变化，并常与其他暴露共同影响结果或通过中介路径间接影响。现有多变量孟德尔随机化方法假设效应恒定，无法捕捉这些动态关系。

Method: 结合功能主成分分析与数据驱动的交叉验证策略进行基函数选择，考虑重叠工具变量和中介效应，评估联合与单独暴露效应估计策略。

Result: 在非线性效应、水平多效性、中介效应和稀疏数据等多种场景下，MV-FMR能一致恢复真实因果函数，优于单变量方法。应用于UK Biobank数据成功分析了收缩压和BMI对冠心病的时变因果效应。

Conclusion: MV-FMR提供了一个灵活可解释的框架，用于解析复杂的时变因果过程，为识别生命历程关键期和疾病预防相关可干预驱动因素提供了新机会。

Abstract: Mendelian Randomization is a widely used instrumental variable method for assessing causal effects of lifelong exposures on health outcomes. Many exposures, however, have causal effects that vary across the life course and often influence outcomes jointly with other exposures or indirectly through mediating pathways. Existing approaches to multivariable Mendelian Randomization assume constant effects over time and therefore fail to capture these dynamic relationships. We introduce Multivariable Functional Mendelian Randomization (MV-FMR), a new framework that extends functional Mendelian Randomization to simultaneously model multiple time-varying exposures. The method combines functional principal component analysis with a data-driven cross-validation strategy for basis selection and accounts for overlapping instruments and mediation effects. Through extensive simulations, we assessed MV-FMR's ability to recover time-varying causal effects under a range of data-generating scenarios and compared the performance of joint versus separate exposure effect estimation strategies. Across scenarios involving nonlinear effects, horizontal pleiotropy, mediation, and sparse data, MV-FMR consistently recovered the true causal functions and outperformed univariable approaches. To demonstrate its practical value, we applied MV-FMR to UK Biobank data to investigate the time-varying causal effects of systolic blood pressure and body mass index on coronary artery disease. MV-FMR provides a flexible and interpretable framework for disentangling complex time-dependent causal processes and offers new opportunities for identifying life-course critical periods and actionable drivers relevant to disease prevention.

</details>


### [91] [Ant Colony Optimisation applied to the Travelling Santa Problem](https://arxiv.org/abs/2512.19627)
*Elliot Fisher,Robin Smith*

Main category: stat.AP

TL;DR: TSaP-ACO框架优化圣诞老人全球配送路线，考虑地球自转的夜间窗口和能量预算，通过蚁群算法减少能量消耗和路线长度。


<details>
  <summary>Details</summary>
Motivation: 圣诞老人的全球配送面临严格的时间窗口约束（必须在夜间）和能量预算限制，需要设计能同时满足这些复杂约束的优化调度方案。

Method: 开发了TSaP-ACO（旅行圣诞老人蚁群优化）框架，使用人工蚁群迭代构建路径，通过信息素引导决策，并引入四个关键创新：1) 将本地黑暗可行性嵌入信息素启发式；2) 通过缩小雪橇横截面积最小化空气动力功；3) 使用低成本"流氓蚁"反转捕捉方向敏感时区；4) 动态调整路段巡航速度。

Result: 在15和30个首都城市的基准测试中，TSaP-ACO消除了所有白天违规，相比仅考虑距离的ACO减少了10%的总功。在40个城市的压力测试中，能量使用减少88%，路线长度缩短约67%。

Conclusion: 滚动窗口、能量感知的蚁群优化方法在现实全球配送场景中具有应用潜力，工作最小化自然导致了人口优先的路由策略。

Abstract: The hypothetical global delivery schedule of Santa Claus must follow strict rolling night-time windows that vary with the Earth's rotation and obey an energy budget that depends on payload size and cruising speed. To design this schedule, the Travelling-Santa Ant-Colony Optimisation framework (TSaP-ACO) was developed. This heuristic framework constructs potential routes via a population of artificial ants that iteratively extend partial paths. Ants make their decisions much like they do in nature, following pheromones left by other ants, but with a degree of permitted exploration. This approach: (i) embeds local darkness feasibility directly into the pheromone heuristic, (ii) seeks to minimise aerodynamic work via a shrinking sleigh cross sectional area, (iii) uses a low-cost "rogue-ant" reversal to capture direction-sensitive time-zones, and (iv) tunes leg-specific cruise speeds on the fly. On benchmark sets of 15 and 30 capital cities, the TSaP-ACO eliminates all daylight violations and reduces total work by up to 10% compared to a distance-only ACO. In a 40-capital-city stress test, it cuts energy use by 88%, and shortens tour length by around 67%. Population-first routing emerges naturally from work minimisation (50% served by leg 11 of 40). These results demonstrate that rolling-window, energy-aware ACO has potential applications more realistic global delivery scenarios.

</details>


### [92] [An Adaptive Graphical Lasso Approach to Modeling Symptom Networks of Common Mental Disorders in Eritrean Refugee Population](https://arxiv.org/abs/2512.19681)
*Elizabeth B. Amona,Indranil Sahoo,David Chan,Marianne B. Lund,Miriam Kuttikat*

Main category: stat.AP

TL;DR: 该研究使用自适应惩罚的图LASSO方法，分析了厄立特里亚难民中PTSD、抑郁、焦虑和躯体痛苦症状的网络结构，识别了核心症状簇和关键干预靶点。


<details>
  <summary>Details</summary>
Motivation: 难民群体中常见精神障碍（CMD）的公共卫生负担严重，但其症状结构尚未充分探索。特别是在小样本（n<p）条件下，传统网络分析方法存在局限性。

Method: 采用高斯图模型，提出自适应惩罚的图LASSO扩展方法，提高小样本高维条件下的稀疏性选择和网络估计稳定性。使用自助重采样评估网络可靠性，通过中心性指标识别关键症状。

Result: 识别出6个不同的症状簇，其中躯体-焦虑症状形成最互联的群组。恶心和重温过去经历等核心症状连接PTSD、焦虑、抑郁和躯体痛苦。恐惧感、睡眠问题和兴趣丧失是关键症状，维持整体网络连接性。

Conclusion: 研究揭示了难民精神症状的复杂网络结构，识别了潜在干预靶点。自适应惩罚图LASSO方法在小样本高维条件下表现良好，为类似研究提供了方法学参考。

Abstract: Despite the significant public health burden of common mental disorders (CMDs) among refugee populations, their underlying symptom structures remain underexplored. This study uses Gaussian graphical modeling to examine the symptom network of post-traumatic stress disorder (PTSD), depression, anxiety, and somatic distress among Eritrean refugees in the Greater Washington, DC area. Given the small sample size (n) and high-dimensional symptom space (p), we propose a novel extension of the standard graphical LASSO by incorporating adaptive penalization, which improves sparsity selection and network estimation stability under n < p conditions. To evaluate the reliability of the network, we apply bootstrap resampling and use centrality measures to identify the most influential symptoms. Our analysis identifies six distinct symptom clusters, with somatic-anxiety symptoms forming the most interconnected group. Notably, symptoms such as nausea and reliving past experiences emerge as central symptoms linking PTSD, anxiety, depression, and somatic distress. Additionally, we identify symptoms like feeling fearful, sleep problems, and loss of interest in activities as key symptoms, either being closely positioned to many others or acting as important bridges that help maintain the overall network connectivity, thereby highlighting their potential importance as possible intervention targets.

</details>

<div id=toc></div>

# Table of Contents

- [stat.AP](#stat.AP) [Total: 1]
- [cs.SI](#cs.SI) [Total: 3]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.AI](#cs.AI) [Total: 31]
- [cs.CY](#cs.CY) [Total: 7]
- [econ.EM](#econ.EM) [Total: 7]


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [1] [pintervals: an R package for model-agnostic prediction intervals](https://arxiv.org/abs/2601.03994)
*David Randahl,Anders Hjort,Jonathan P. Williams*

Main category: stat.AP

TL;DR: pintervals包提供了一个统一的框架，用于构建预测区间和校准预测，支持多种模型无关的预测区间构建方法。


<details>
  <summary>Details</summary>
Motivation: 现有的R包和函数通常专注于特定的建模框架或预测类型，或者需要针对不同模型或应用进行手动定制，缺乏统一的模型无关预测区间构建工具。

Method: 使用预留的校准数据，提供一致的接口来构建多种预测区间：包括conformal预测区间、参数化预测区间和bootstrap预测区间，支持任何输出点预测的模型。

Result: 开发了pintervals包，为研究人员提供了一个统一的工具，可以在不同的建模框架和应用中应用和比较多种预测区间构建方法。

Conclusion: pintervals包填补了R生态系统中模型无关预测区间构建工具的空白，为统计建模和机器学习提供了灵活、统一的预测区间构建解决方案。

Abstract: The \pkg{pintervals} package aims to provide a unified framework for constructing prediction intervals and calibrating predictions in a model-agnostic setting using set-aside calibration data. It comprises routines to construct conformal as well as parametric and bootstrapped prediction intervals from any model that outputs point predictions. Several R packages and functions already exist for constructing prediction intervals, but they often focus on specific modeling frameworks or types of predictions, or require manual customization for different models or applications. By providing a consistent interface for a variety of prediction interval construction approaches (all model-agnostic), \pkg{pintervals} allows researchers to apply and compare them across different modeling frameworks and applications.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [2] [From Risk Perception to Behavior Large Language Models-Based Simulation of Pandemic Prevention Behaviors](https://arxiv.org/abs/2601.03552)
*Lujia Bo,Mingxuan Chen,Youduo Chen,Xiaofan Gui,Jiang Bian,Chunyan Wang,Yi Liu*

Main category: cs.SI

TL;DR: 开发了一个基于LLM的预防行为模拟框架，通过静态模块预测行为强度，动态模块更新风险感知，在零样本、少样本和跨情境迁移设置下验证了有效性，并应用于政策放松情景模拟。


<details>
  <summary>Details</summary>
Motivation: 在新型传染病爆发初期，个体预防行为是主要防线，但其采纳存在异质性且难以预测，尤其是在经验数据稀缺、流行病政策环境快速演变的情况下。现有方法难以准确预测行为变化。

Method: 开发了基于LLM的预防行为模拟框架，包含：(1)静态模块：在指定外部情境下预测行为强度；(2)动态模块：随时间更新居民感知风险并传播到行为演化。通过结构化提示工程以第一人称视角实现，使用北京居民两轮调查数据（2020年12月和2021年8月）在零样本、少样本和跨情境迁移设置下评估。

Result: 使用Kolmogorov-Smirnov检验比较模拟与观察行为分布（p>0.001为有效性标准），框架表现出稳健性能且随参考示例增加而改进：预测准确率从零样本的72.7%提高到少样本的81.8%，跨情境迁移下仍保持77.8%的高水平。应用于中国2022年12月政策放松情景模拟和120种系统变化流行病条件压力测试，结果显示政策放松下行为普遍宽松，但排水相关消毒行为出现反趋势增加。

Conclusion: LLM-based预防行为模拟框架能有效预测传染病防控中的个体行为变化，即使在数据稀缺和情境快速演变下也表现良好。研究发现低成本、低摩擦行为在外部约束减弱时可能持续或强化，揭示了潜在的环境权衡。

Abstract: Individual prevention behaviors are a primary line of defense during the early stages of novel infectious disease outbreaks, yet their adoption is heterogeneous and difficult to forecast-especially when empirical data are scarce and epidemic-policy contexts evolve rapidly. To address this gap, we develop an LLM-based prevention-behavior simulation framework that couples (i) a static module for behavior-intensity prediction under a specified external context and (ii) a dynamic module that updates residents' perceived risk over time and propagates these updates into behavior evolution. The model is implemented via structured prompt engineering in a first-person perspective and is evaluated against two rounds of survey data from Beijing residents (R1: December 2020; R2: August 2021) under progressively realistic data-availability settings: zero-shot, few-shot, and cross-context transfer. Using Kolmogorov-Smirnov tests to compare simulated and observed behavior distributions (p > 0.001 as the validity criterion), the framework demonstrates robust performance and improves with limited reference examples; reported predictive accuracy increases from 72.7% (zero-shot) to 81.8% (few-shot), and remains high at 77.8% under transfer to novel contexts. We further apply the framework to simulate behavior changes during China's December 2022 policy relaxation and to stress-test behavioral responses across 120 systematically varied epidemic conditions (R0, CFR, and control-measure tiers). Results indicate broad behavioral loosening under relaxation but a distinctive counter-trend increase in drain-related disinfection, highlighting how low-cost, low-friction behaviors may persist or intensify even when external constraints recede-raising a potential environmental tradeoff.

</details>


### [3] [Fairness in Opinion Dynamics](https://arxiv.org/abs/2601.03859)
*Stanisław Stępień,Michalina Janik,Mateusz Nurek,Akrati Saxena,Radosław Michalski*

Main category: cs.SI

TL;DR: 研究探讨了社会网络分析中意见预测模型的算法偏见问题，特别是针对边缘化群体的歧视现象，并测试了基于人口统计、拓扑结构和混合特征的分类器来预测模型性能差异。


<details>
  <summary>Details</summary>
Motivation: 社会网络分析中的意见预测模型往往对多数群体表现良好，但对边缘化群体存在歧视。现有公平性研究需要探索如何识别模型对哪些人群表现不佳，以及仅基于人口统计或拓扑特征能否可靠预测这种性能差异。

Method: 使用NetSense数据集和先进的CoDiNG意见预测模型，构建三种分类器：基于人口统计特征、基于拓扑特征、以及混合特征分类器，用于预测模型对哪些用户会产生不准确的预测。

Result: 研究发现没有单一范式能提供最佳结果，需要情境感知策略。通过综合分析识别出四种关键的算法偏见模式，表明需要结合个体属性和网络结构的多方面方法。

Conclusion: 减少算法偏见和促进包容性决策需要采用多方面的综合方法，同时考虑个体属性和网络结构特征，在公平导向的社会网络分析中实施情境感知策略。

Abstract: Ways in which people's opinions change are, without a doubt, subject to a rich tapestry of differing influences. Factors that affect how one arrives at an opinion reflect how they have been shaped by their environment throughout their lives, education, material status, what belief systems are they subscribed to, and what socio-economic minorities are they a part of. This already complex system is further expanded by the ever-changing nature of one's social network. It is therefore no surprise that many models have a tendency to perform best for the majority of the population and discriminating those people who are members of various marginalized groups . This bias and the study of how to counter it are subject to a rapidly developing field of Fairness in Social Network Analysis (SNA). The focus of this work is to look into how a state-of-the-art model discriminates certain minority groups and whether it is possible to reliably predict for whom it will perform worse. Moreover, is such prediction possible based solely on one's demographic or topological features? To this end, the NetSense dataset, together with a state-of-the-art CoDiNG model for opinion prediction have been employed. Our work explores how three classifier models (Demography-Based, Topology-Based, and Hybrid) perform when assessing for whom this algorithm will provide inaccurate predictions. Finally, through a comprehensive analysis of these experimental results, we identify four key patterns of algorithmic bias. Our findings suggest that no single paradigm provides the best results and that there is a real need for context-aware strategies in fairness-oriented social network analysis. We conclude that a multi-faceted approach, incorporating both individual attributes and network structures, is essential for reducing algorithmic bias and promoting inclusive decision-making.

</details>


### [4] [Celebrity messages reduce online hate and limit its spread](https://arxiv.org/abs/2601.04134)
*Eaman Jahani,Blas Kolic,Manuel Tonneau,Hause Lin,Daniel Barkoczi,Edwin Ikhuoria,Victor Orozco,Samuel Fraiberger*

Main category: cs.SI

TL;DR: 在尼日利亚针对种族仇恨言论的大规模随机对照试验显示，通过名人发送的亲社会视频信息能将仇恨内容减少2.5%-5.5%，且75%的效果能持续4个月


<details>
  <summary>Details</summary>
Motivation: 在线仇恨言论传播迅速，但缺乏关于预防性和可扩展策略有效性的实证研究。研究者希望了解是否可以通过非内容删除的方式有效遏制仇恨言论传播

Method: 在尼日利亚X平台进行为期20周的随机对照试验，针对73,136名曾参与仇恨言论的用户，随机分配接收尼日利亚名人发送的亲社会视频信息

Result: 干预期间仇恨内容减少2.5%-5.5%，约75%的减少效果在后续4个月持续；当信息覆盖用户更多受众时，仇恨转发减少超过50%

Conclusion: 可扩展的信息传递策略能够在不删除内容的情况下有效限制在线仇恨言论传播，为社交媒体平台提供了可行的干预方案

Abstract: Online hate spreads rapidly, yet little is known about whether preventive and scalable strategies can curb it. We conducted the largest randomized controlled trial of hate speech prevention to date: a 20-week messaging campaign on X in Nigeria targeting ethnic hate. 73,136 users who had previously engaged with hate speech were randomly assigned to receive prosocial video messages from Nigerian celebrities. The campaign reduced hate content by 2.5% to 5.5% during treatment, with about 75% of the reduction persisting over the following four months. Reaching a larger share of a user's audience reduced amplification of that user's hate posts among both treated and untreated users, cutting hate reposts by over 50% for the most exposed accounts. Scalable messaging can limit online hate without removing content.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [5] [On-Device Deep Reinforcement Learning for Decentralized Task Offloading Performance trade-offs in the training process](https://arxiv.org/abs/2601.03976)
*Gorka Nieto,Idoia de la Iglesia,Cristina Perfecto,Unai Lopez-Novoa*

Main category: cs.ET

TL;DR: 提出基于去中心化深度强化学习的计算卸载决策方案，并在真实5G边缘计算测试平台上验证性能与能耗权衡


<details>
  <summary>Details</summary>
Motivation: 现有计算卸载决策方法复杂且缺乏真实环境验证，需要在真实边缘设备上评估DRL代理的性能与本地运行代价

Method: 采用去中心化深度强化学习代理，在真实5G MEC测试平台（包含多种边缘设备、MEC服务器和云服务器）上实现任务执行位置选择

Result: 评估了代理在满足任务需求方面的性能，并分析了本地运行DRL代理的延迟和能耗权衡，比较了本地训练与远程训练的优劣

Conclusion: 去中心化DRL方法在真实边缘计算环境中可行，但需要考虑本地运行代理的计算开销与性能之间的平衡

Abstract: Allowing less capable devices to offload computational tasks to more powerful devices or servers enables the development of new applications that may not run correctly on the device itself. Deciding where and why to run each of those applications is a complex task. Therefore, different approaches have been adopted to make offloading decisions. In this work, we propose a decentralized Deep Reinforcement Learning (DRL) agent to address the selection of computing locations. Unlike most existing work, we analyze it in a real testbed composed of various edge devices running the agent to determine where to execute each task. These devices are connected to a Multi-Access Edge Computing (MEC) server and a Cloud server through 5G communications. We evaluate not only the agent's performance in meeting task requirements but also the implications of running this type of agent locally, assessing the trade-offs of training locally versus remotely in terms of latency and energy consumption.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [6] [Mastering the Game of Go with Self-play Experience Replay](https://arxiv.org/abs/2601.03306)
*Jingbin Liu,Xuechun Wang*

Main category: cs.AI

TL;DR: QZero是一种无需搜索的模型无关强化学习算法，通过自博弈和离策略经验回放学习纳什均衡策略，在有限计算资源下达到AlphaGo水平


<details>
  <summary>Details</summary>
Motivation: 围棋长期以来作为人工智能的基准测试，需要复杂的战略推理和长期规划。先前方法如AlphaGo主要依赖基于模型的蒙特卡洛树搜索，本研究旨在探索模型无关强化学习在围棋中的有效性

Method: 基于熵正则化Q学习，使用单个Q值网络统一策略评估和改进，通过自博弈和离策略经验回放学习，无需搜索训练，从零开始无需人类数据

Result: 使用有限计算资源（7个GPU训练5个月）达到与AlphaGo相当的性能水平，首次证明模型无关强化学习在围棋中的有效性

Conclusion: 首次证明模型无关强化学习可以掌握围棋游戏，展示了离策略强化学习在解决大规模复杂环境中的可行性

Abstract: The game of Go has long served as a benchmark for artificial intelligence, demanding sophisticated strategic reasoning and long-term planning. Previous approaches such as AlphaGo and its successors, have predominantly relied on model-based Monte-Carlo Tree Search (MCTS). In this work, we present QZero, a novel model-free reinforcement learning algorithm that forgoes search during training and learns a Nash equilibrium policy through self-play and off-policy experience replay. Built upon entropy-regularized Q-learning, QZero utilizes a single Q-value network to unify policy evaluation and improvement. Starting tabula rasa without human data and trained for 5 months with modest compute resources (7 GPUs), QZero achieved a performance level comparable to that of AlphaGo. This demonstrates, for the first time, the efficiency of using model-free reinforcement learning to master the game of Go, as well as the feasibility of off-policy reinforcement learning in solving large-scale and complex environments.

</details>


### [7] [Personalization of Large Foundation Models for Health Interventions](https://arxiv.org/abs/2601.03482)
*Stefan Konigorski,Johannes E. Vedder,Babajide Alamu Owoyele,İbrahim Özkan*

Main category: cs.AI

TL;DR: 大型基础模型在医疗AI中具有潜力，但无法替代N-of-1试验进行个性化治疗推荐，两者应互补结合


<details>
  <summary>Details</summary>
Motivation: 探讨大型基础模型能否提供真正个性化的治疗推荐，解决医疗AI中的普遍性悖论、隐私-性能悖论、规模-特异性悖论和自动化-共情悖论等矛盾

Method: 提出混合框架：大型基础模型利用多模态数据从群体模式中快速生成假设和干预候选排名，N-of-1试验（交叉自我实验）进行个体因果验证

Result: 大型基础模型无法替代N-of-1试验，但两者可以互补。混合框架结合了大型基础模型的预测能力和N-of-1试验的因果推断优势

Conclusion: 明确预测与因果之间的界限，解决医疗AI中的悖论性张力，对于负责任地将AI整合到个性化医疗中至关重要。大型基础模型和N-of-1试验应协同工作以实现真正的个性化治疗

Abstract: Large foundation models (LFMs) transform healthcare AI in prevention, diagnostics, and treatment. However, whether LFMs can provide truly personalized treatment recommendations remains an open question. Recent research has revealed multiple challenges for personalization, including the fundamental generalizability paradox: models achieving high accuracy in one clinical study perform at chance level in others, demonstrating that personalization and external validity exist in tension. This exemplifies broader contradictions in AI-driven healthcare: the privacy-performance paradox, scale-specificity paradox, and the automation-empathy paradox. As another challenge, the degree of causal understanding required for personalized recommendations, as opposed to mere predictive capacities of LFMs, remains an open question. N-of-1 trials -- crossover self-experiments and the gold standard for individual causal inference in personalized medicine -- resolve these tensions by providing within-person causal evidence while preserving privacy through local experimentation. Despite their impressive capabilities, this paper argues that LFMs cannot replace N-of-1 trials. We argue that LFMs and N-of-1 trials are complementary: LFMs excel at rapid hypothesis generation from population patterns using multimodal data, while N-of-1 trials excel at causal validation for a given individual. We propose a hybrid framework that combines the strengths of both to enable personalization and navigate the identified paradoxes: LFMs generate ranked intervention candidates with uncertainty estimates, which trigger subsequent N-of-1 trials. Clarifying the boundary between prediction and causation and explicitly addressing the paradoxical tensions are essential for responsible AI integration in personalized medicine.

</details>


### [8] [Digital Red Queen: Adversarial Program Evolution in Core War with LLMs](https://arxiv.org/abs/2601.03335)
*Akarsh Kumar,Ryan Bahlous-Boldi,Prafull Sharma,Phillip Isola,Sebastian Risi,Yujin Tang,David Ha*

Main category: cs.AI

TL;DR: 提出Digital Red Queen算法，利用LLM在Core War游戏中通过自我对抗演化程序，实现持续适应动态目标，相比静态优化能产生更通用的解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前大多数LLM演化框架采用静态优化方法，忽略了现实世界中开放式的对抗性演化动态（"红皇后"效应）。需要研究如何将这种动态对抗过程纳入LLM演化框架。

Method: 提出Digital Red Queen算法：在Core War游戏中，让LLM演化汇编式程序（warriors），每轮生成新程序以击败之前所有程序，形成连续的适应序列。Core War是一个图灵完备的虚拟环境。

Result: 演化出的程序越来越通用（相对于人类编写的程序集）。有趣的是，独立运行中程序的行为多样性减少，表明向通用行为策略的收敛压力，类似于自然界的趋同演化。

Conclusion: 从静态目标转向动态红皇后目标具有潜在价值。Core War可作为研究对抗性适应的可控沙盒环境。这种简单的自我对抗方法可应用于更实际的对抗领域，如网络安全或抗药性研究。

Abstract: Large language models (LLMs) are increasingly being used to evolve solutions to problems in many domains, in a process inspired by biological evolution. However, unlike biological evolution, most LLM-evolution frameworks are formulated as static optimization problems, overlooking the open-ended adversarial dynamics that characterize real-world evolutionary processes. Here, we study Digital Red Queen (DRQ), a simple self-play algorithm that embraces these so-called "Red Queen" dynamics via continual adaptation to a changing objective. DRQ uses an LLM to evolve assembly-like programs, called warriors, which compete against each other for control of a virtual machine in the game of Core War, a Turing-complete environment studied in artificial life and connected to cybersecurity. In each round of DRQ, the model evolves a new warrior to defeat all previous ones, producing a sequence of adapted warriors. Over many rounds, we observe that warriors become increasingly general (relative to a set of held-out human warriors). Interestingly, warriors also become less behaviorally diverse across independent runs, indicating a convergence pressure toward a general-purpose behavioral strategy, much like convergent evolution in nature. This result highlights a potential value of shifting from static objectives to dynamic Red Queen objectives. Our work positions Core War as a rich, controllable sandbox for studying adversarial adaptation in artificial systems and for evaluating LLM-based evolution methods. More broadly, the simplicity and effectiveness of DRQ suggest that similarly minimal self-play approaches could prove useful in other more practical multi-agent adversarial domains, like real-world cybersecurity or combating drug resistance.

</details>


### [9] [Enhancing LLM Instruction Following: An Evaluation-Driven Multi-Agentic Workflow for Prompt Instructions Optimization](https://arxiv.org/abs/2601.03359)
*Alberto Purpura,Li Wang,Sahil Badyal,Eugenio Beaufrand,Adam Faulkner*

Main category: cs.AI

TL;DR: 提出一种多智能体工作流，将主任务描述优化与约束条件解耦，通过定量评分反馈迭代改进提示，显著提升LLM输出对形式约束的遵循度


<details>
  <summary>Details</summary>
Motivation: LLM经常生成内容相关但不符合形式约束的输出，传统提示优化方法只关注主任务描述的重述，忽略了作为响应验收标准的细粒度约束条件

Method: 提出多智能体工作流，将主任务描述优化与约束条件解耦，使用定量评分作为反馈，迭代重写和改进提示

Result: 该方法生成的修订提示在Llama 3.1 8B和Mixtral-8x 7B等模型上产生显著更高的合规性评分

Conclusion: 解耦任务描述与约束条件的多智能体优化方法能有效提升LLM输出对形式约束的遵循度，优于传统提示优化方法

Abstract: Large Language Models (LLMs) often generate substantively relevant content but fail to adhere to formal constraints, leading to outputs that are conceptually correct but procedurally flawed. Traditional prompt refinement approaches focus on rephrasing the description of the primary task an LLM has to perform, neglecting the granular constraints that function as acceptance criteria for its response. We propose a novel multi-agentic workflow that decouples optimization of the primary task description from its constraints, using quantitative scores as feedback to iteratively rewrite and improve them. Our evaluation demonstrates this method produces revised prompts that yield significantly higher compliance scores from models like Llama 3.1 8B and Mixtral-8x 7B.

</details>


### [10] [Exploration Through Introspection: A Self-Aware Reward Model](https://arxiv.org/abs/2601.03389)
*Michael Petrowski,Milica Gašić*

Main category: cs.AI

TL;DR: 论文提出了一种基于内省探索的强化学习智能体框架，通过隐马尔可夫模型推断"疼痛信念"作为学习信号，研究自我意识如何影响智能体的学习能力，并比较正常与慢性疼痛感知模型的性能差异。


<details>
  <summary>Details</summary>
Motivation: 理解人工智能如何建模内部心理状态对于推进AI中的心智理论至关重要。证据表明自我认知和他人认知存在统一系统，本研究通过让强化学习智能体在网格世界中推断自身内部状态来探索这种自我意识。

Method: 引入受生物疼痛启发的内省探索组件，使用隐马尔可夫模型从在线观察中推断"疼痛信念"，将该信号整合到主观奖励函数中，构建计算框架比较正常与慢性疼痛感知模型的性能差异。

Result: 内省智能体总体上显著优于标准基线智能体，能够复现复杂的人类类似行为，展示了自我意识对智能体学习能力的积极影响。

Conclusion: 通过计算框架研究自我意识在AI中的作用是可行的，内省探索机制能有效提升智能体性能，为理解AI中的心智理论提供了新途径，并可用于模拟和研究疼痛感知等复杂心理现象。

Abstract: Understanding how artificial agents model internal mental states is central to advancing Theory of Mind in AI. Evidence points to a unified system for self- and other-awareness. We explore this self-awareness by having reinforcement learning agents infer their own internal states in gridworld environments. Specifically, we introduce an introspective exploration component that is inspired by biological pain as a learning signal by utilizing a hidden Markov model to infer "pain-belief" from online observations. This signal is integrated into a subjective reward function to study how self-awareness affects the agent's learning abilities. Further, we use this computational framework to investigate the difference in performance between normal and chronic pain perception models. Results show that introspective agents in general significantly outperform standard baseline agents and can replicate complex human-like behaviors.

</details>


### [11] [Toward Maturity-Based Certification of Embodied AI: Quantifying Trustworthiness Through Measurement Mechanisms](https://arxiv.org/abs/2601.03470)
*Michael C. Darling,Alan H. Hesu,Michael A. Mardikes,Brian C. McGuigan,Reed M. Milewicz*

Main category: cs.AI

TL;DR: 提出基于成熟度的认证框架，通过明确测量机制来认证具身AI系统


<details>
  <summary>Details</summary>
Motivation: 需要结构化评估框架、量化评分机制以及处理可信度评估中多目标权衡的方法来认证具身AI系统

Method: 使用不确定性量化作为示例测量机制，通过无人机系统检测案例研究来展示可行性

Result: 展示了基于成熟度的认证框架在具身AI系统中的可行性

Conclusion: 提出的框架为具身AI系统认证提供了结构化评估、量化测量和权衡导航的方法

Abstract: We propose a maturity-based framework for certifying embodied AI systems through explicit measurement mechanisms. We argue that certifiable embodied AI requires structured assessment frameworks, quantitative scoring mechanisms, and methods for navigating multi-objective trade-offs inherent in trustworthiness evaluation. We demonstrate this approach using uncertainty quantification as an exemplar measurement mechanism and illustrate feasibility through an Uncrewed Aircraft System (UAS) detection case study.

</details>


### [12] [CPGPrompt: Translating Clinical Guidelines into LLM-Executable Decision Support](https://arxiv.org/abs/2601.03475)
*Ruiqi Deng,Geoffrey Martin,Tony Wang,Gongbo Zhang,Yi Liu,Chunhua Weng,Yanshan Wang,Justin F Rousseau,Yifan Peng*

Main category: cs.AI

TL;DR: CPGPrompt：一个自动提示系统，将叙述性临床指南转化为LLM可用的结构化决策树，用于患者病例评估，在专科转诊决策上表现优异，但在多类路径分配上表现有差异。


<details>
  <summary>Details</summary>
Motivation: 临床实践指南（CPGs）为患者护理提供循证建议，但将其整合到人工智能中面临挑战。现有方法（如基于规则的系统）存在可解释性差、指南依从性不一致、领域适用性窄等限制。

Method: 开发CPGPrompt系统，将CPGs转化为结构化决策树，利用LLM动态导航决策树进行患者病例评估。在三个领域（头痛、腰痛、前列腺癌）生成合成病例，测试不同决策场景。

Result: 二元专科转诊分类在所有领域表现一致强劲（F1：0.85-1.00），召回率高（1.00±0.00）。多类路径分配表现降低，存在领域差异：头痛（F1：0.47）、腰痛（F1：0.72）、前列腺癌（F1：0.77）。

Conclusion: CPGPrompt在将叙述性临床指南转化为LLM可操作格式方面有效，特别适合二元决策任务。性能差异反映了不同指南的结构特点，为未来改进提供了方向。

Abstract: Clinical practice guidelines (CPGs) provide evidence-based recommendations for patient care; however, integrating them into Artificial Intelligence (AI) remains challenging. Previous approaches, such as rule-based systems, face significant limitations, including poor interpretability, inconsistent adherence to guidelines, and narrow domain applicability. To address this, we develop and validate CPGPrompt, an auto-prompting system that converts narrative clinical guidelines into large language models (LLMs).
  Our framework translates CPGs into structured decision trees and utilizes an LLM to dynamically navigate them for patient case evaluation. Synthetic vignettes were generated across three domains (headache, lower back pain, and prostate cancer) and distributed into four categories to test different decision scenarios. System performance was assessed on both binary specialty-referral decisions and fine-grained pathway-classification tasks.
  The binary specialty referral classification achieved consistently strong performance across all domains (F1: 0.85-1.00), with high recall (1.00 $\pm$ 0.00). In contrast, multi-class pathway assignment showed reduced performance, with domain-specific variations: headache (F1: 0.47), lower back pain (F1: 0.72), and prostate cancer (F1: 0.77). Domain-specific performance differences reflected the structure of each guideline. The headache guideline highlighted challenges with negation handling. The lower back pain guideline required temporal reasoning. In contrast, prostate cancer pathways benefited from quantifiable laboratory tests, resulting in more reliable decision-making.

</details>


### [13] [Evolving Programmatic Skill Networks](https://arxiv.org/abs/2601.03509)
*Haochen Shi,Xingdi Yuan,Bang Liu*

Main category: cs.AI

TL;DR: PSN框架通过符号程序网络实现持续技能学习，利用LLM进行故障定位、渐进优化和结构重构，在开放环境中展示强大泛化能力


<details>
  <summary>Details</summary>
Motivation: 解决开放环境中的持续技能获取问题，使智能体能够构建、精炼和重用不断扩展的可执行技能库，实现长期学习能力

Method: 提出程序化技能网络(PSN)，将技能表示为可执行符号程序，通过LLM实现三种核心机制：REFLECT故障定位、渐进优化与成熟度感知更新门控、规范结构重构与回滚验证

Result: 在MineDojo和Crafter环境中展示出强大的技能重用、快速适应能力和跨开放任务分布的泛化性能，学习动态与神经网络训练具有结构相似性

Conclusion: PSN框架为开放环境中的持续技能学习提供有效解决方案，通过符号程序网络实现稳定学习与泛化，具有实际应用潜力

Abstract: We study continual skill acquisition in open-ended embodied environments where an agent must construct, refine, and reuse an expanding library of executable skills. We introduce the Programmatic Skill Network (PSN), a framework in which skills are executable symbolic programs forming a compositional network that evolves through experience. PSN defines three core mechanisms instantiated via large language models: (1)REFLECT for structured fault localization over skill compositions, (2) progressive optimization with maturity-aware update gating that stabilizes reliable skills while maintaining plasticity for uncertain ones, and (3) canonical structural refactoring under rollback validation that maintains network compactness. We further show that PSN's learning dynamics exhibit structural parallels to neural network training. Experiments on MineDojo and Crafter demonstrate robust skill reuse, rapid adaptation, and strong generalization across open-ended task distributions.\footnote{We plan to open-source the code.

</details>


### [14] [Variance Computation for Weighted Model Counting with Knowledge Compilation Approach](https://arxiv.org/abs/2601.03523)
*Kengo Nakamura,Masaaki Nishino,Norihito Yasuda*

Main category: cs.AI

TL;DR: 本文研究了加权模型计数(WMC)方差的计算问题，针对参数不确定性场景，提出了在结构化d-DNNF上计算方差的多项式时间算法，并证明了该问题在结构化DNNF、d-DNNF和FBDD上的计算困难性。


<details>
  <summary>Details</summary>
Motivation: 在实际推理任务中，模型参数通常从数据中学习得到，存在不确定性。为了评估推理结果的不确定性程度，需要将推理结果视为随机变量并计算其方差。然而，计算这种方差的可处理性尚未明确。

Method: 1. 为结构化d-DNNF推导了多项式时间算法来计算WMC方差；2. 证明了该问题在结构化DNNF、d-DNNF和FBDD上的计算困难性；3. 将方法应用于贝叶斯网络推理的不确定性测量。

Result: 1. 成功开发了在结构化d-DNNF上计算WMC方差的多项式时间算法；2. 证明了该问题在结构化DNNF、d-DNNF和FBDD上是困难的，这很有趣，因为后两者允许多项式时间的WMC计算；3. 在真实世界贝叶斯网络上成功评估了边际概率的方差，并分析了参数方差对边际方差的影响。

Conclusion: 本文首次系统研究了WMC方差计算的可处理性问题，为参数不确定性下的概率推理提供了理论基础和实用工具。虽然在某些表示形式下该问题是困难的，但在结构化d-DNNF上存在高效算法，可用于实际贝叶斯网络推理的不确定性量化。

Abstract: One of the most important queries in knowledge compilation is weighted model counting (WMC), which has been applied to probabilistic inference on various models, such as Bayesian networks. In practical situations on inference tasks, the model's parameters have uncertainty because they are often learned from data, and thus we want to compute the degree of uncertainty in the inference outcome. One possible approach is to regard the inference outcome as a random variable by introducing distributions for the parameters and evaluate the variance of the outcome. Unfortunately, the tractability of computing such a variance is hardly known. Motivated by this, we consider the problem of computing the variance of WMC and investigate this problem's tractability. First, we derive a polynomial time algorithm to evaluate the WMC variance when the input is given as a structured d-DNNF. Second, we prove the hardness of this problem for structured DNNFs, d-DNNFs, and FBDDs, which is intriguing because the latter two allow polynomial time WMC algorithms. Finally, we show an application that measures the uncertainty in the inference of Bayesian networks. We empirically show that our algorithm can evaluate the variance of the marginal probability on real-world Bayesian networks and analyze the impact of the variances of parameters on the variance of the marginal.

</details>


### [15] [STAR-S: Improving Safety Alignment through Self-Taught Reasoning on Safety Rules](https://arxiv.org/abs/2601.03537)
*Di Wu,Yanyan Zhao,Xin Lu,Mingzhe Li,Bing Qin*

Main category: cs.AI

TL;DR: STAR-S框架通过自学习循环增强LLM的安全规则推理能力，有效防御越狱攻击


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全防御面临挑战：虽然已有研究尝试通过训练模型在响应前进行安全规则推理来提升安全性，但难以明确设计或直接获取有效的安全推理形式。需要一种方法来自动学习有效的安全推理机制。

Method: 提出STAR-S框架，将安全规则推理学习集成到自学习循环中。核心包括：1) 在安全规则指导下引发推理和反思；2) 通过微调增强安全推理能力；3) 重复此过程形成协同循环。模型推理和安全规则解释能力的提升使其能产生更好的推理数据，用于进一步训练。

Result: 实验表明STAR-S能有效防御越狱攻击，性能优于基线方法。

Conclusion: STAR-S框架通过自学习循环增强LLM的安全规则推理能力，为防御越狱攻击提供了有效解决方案。

Abstract: Defending against jailbreak attacks is crucial for the safe deployment of Large Language Models (LLMs). Recent research has attempted to improve safety by training models to reason over safety rules before responding. However, a key issue lies in determining what form of safety reasoning effectively defends against jailbreak attacks, which is difficult to explicitly design or directly obtain. To address this, we propose \textbf{STAR-S} (\textbf{S}elf-\textbf{TA}ught \textbf{R}easoning based on \textbf{S}afety rules), a framework that integrates the learning of safety rule reasoning into a self-taught loop. The core of STAR-S involves eliciting reasoning and reflection guided by safety rules, then leveraging fine-tuning to enhance safety reasoning. Repeating this process creates a synergistic cycle. Improvements in the model's reasoning and interpretation of safety rules allow it to produce better reasoning data under safety rule prompts, which is then utilized for further training. Experiments show that STAR-S effectively defends against jailbreak attacks, outperforming baselines. Code is available at: https://github.com/pikepokenew/STAR_S.git.

</details>


### [16] [ReEfBench: Quantifying the Reasoning Efficiency of LLMs](https://arxiv.org/abs/2601.03550)
*Zhizhang Fu,Yuancheng Gu,Chenkai Hu,Hanmeng Liu,Yue Zhang*

Main category: cs.AI

TL;DR: 论文提出神经符号框架评估LLM推理过程，发现推理深度不依赖生成长度，混合训练数据会导致过早饱和，小模型蒸馏无法复制逻辑效能。


<details>
  <summary>Details</summary>
Motivation: 当前CoT评估方法存在局限，无法区分性能提升是来自真实推理还是仅仅增加输出长度，需要更全面的过程中心化评估框架来理解LLM推理机制。

Method: 提出非侵入式神经符号框架进行推理过程评估，识别四种行为原型，分析推理模式、训练策略和模型规模的影响。

Result: 发现生成长token不是深度推理的必要条件；混合长短CoT训练数据会导致过早饱和和崩溃；小模型蒸馏能复制行为长度但无法达到逻辑效能。

Conclusion: 需要更精细的评估方法来理解LLM推理，生成长度不等于推理质量，训练数据混合和模型容量限制是影响推理能力的关键因素。

Abstract: Test-time scaling has enabled Large Language Models (LLMs) to tackle complex reasoning, yet the limitations of current Chain-of-Thought (CoT) evaluation obscures whether performance gains stem from genuine reasoning or mere verbosity. To address this, (1) we propose a novel neuro-symbolic framework for the non-intrusive, comprehensive process-centric evaluation of reasoning. (2) Through this lens, we identify four distinct behavioral prototypes and diagnose the failure modes. (3) We examine the impact of inference mode, training strategy, and model scale. Our analysis reveals that extended token generation is not a prerequisite for deep reasoning. Furthermore, we reveal critical constraints: mixing long and short CoT data in training risks in premature saturation and collapse, while distillation into smaller models captures behavioral length but fails to replicate logical efficacy due to intrinsic capacity limits.

</details>


### [17] [SCRIBE: Structured Mid-Level Supervision for Tool-Using Language Models](https://arxiv.org/abs/2601.03555)
*Yuxuan Jiang,Francis Ferraro*

Main category: cs.AI

TL;DR: SCRIBE：基于技能原型库的强化学习框架，通过中间行为评估减少奖励方差，提升工具增强代理的多步推理能力


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的奖励模型在工具增强代理训练中产生噪声和不一致的信号，缺乏细粒度、任务特定的评估标准来区分高层规划与低层执行

Method: 引入SCRIBE框架，在中间抽象层进行干预，基于技能原型库进行奖励建模，将开放式LLM评估转化为约束验证问题，通过将子目标路由到相应原型来提供结构化评估标准

Result: 在多个推理和工具使用基准测试中达到最先进性能，将Qwen3-4B模型的AIME25准确率从43.3%提升至63.3%，显著提高复杂多轮工具交互的成功率

Conclusion: SCRIBE展示了跨抽象层的协同演化，中层级技能掌握先于有效高层规划行为的出现，且与低层工具优化互补，为更自主可靠的工具使用代理提供了可扩展路径

Abstract: Training reliable tool-augmented agents remains a significant challenge, largely due to the difficulty of credit assignment in multi-step reasoning. While process-level reward models offer a promising direction, existing LLM-based judges often produce noisy and inconsistent signals because they lack fine-grained, task-specific rubrics to distinguish high-level planning from low-level execution. In this work, we introduce SCRIBE (Skill-Conditioned Reward with Intermediate Behavioral Evaluation), a reinforcement learning framework that intervenes at a novel mid-level abstraction. SCRIBE grounds reward modeling in a curated library of skill prototypes, transforming open-ended LLM evaluation into a constrained verification problem. By routing each subgoal to a corresponding prototype, the reward model is equipped with precise, structured rubrics that substantially reduce reward variance.
  Experimental results show that SCRIBE achieves state-of-the-art performance across a range of reasoning and tool-use benchmarks. In particular, it improves the AIME25 accuracy of a Qwen3-4B model from 43.3% to 63.3%, and significantly increases success rates in complex multi-turn tool interactions.
  Further analysis of training dynamics reveals a co-evolution across abstraction levels, where mastery of mid-level skills consistently precedes the emergence of effective high-level planning behaviors. Finally, we demonstrate that SCRIBE is additive to low-level tool optimizations, providing a scalable and complementary pathway toward more autonomous and reliable tool-using agents.

</details>


### [18] [Controllable LLM Reasoning via Sparse Autoencoder-Based Steering](https://arxiv.org/abs/2601.03595)
*Yi Fang,Wenjie Wang,Mingfeng Xue,Boyi Deng,Fengli Xu,Dayiheng Liu,Fuli Feng*

Main category: cs.AI

TL;DR: SAE-Steering：利用稀疏自编码器分解LRM隐藏状态，通过两阶段特征识别流程控制推理策略，提升控制效果和任务准确性


<details>
  <summary>Details</summary>
Motivation: 大型推理模型自主选择推理策略常导致低效或错误路径，现有方法难以控制细粒度推理策略，因为隐藏状态中存在概念纠缠问题

Method: 使用稀疏自编码器分解策略纠缠的隐藏状态为解耦特征空间；提出SAE-Steering两阶段特征识别流程：先通过策略关键词logits放大召回特征（过滤99%以上），再按控制效果排序；用识别出的策略特定特征作为控制向量

Result: SAE-Steering在控制效果上比现有方法提升超过15%；控制推理策略可将LRM从错误路径重定向到正确路径，实现7%的绝对准确率提升

Conclusion: 通过解耦隐藏状态并识别策略特定特征，SAE-Steering能有效控制大型推理模型的推理策略，提高推理可靠性和灵活性

Abstract: Large Reasoning Models (LRMs) exhibit human-like cognitive reasoning strategies (e.g. backtracking, cross-verification) during reasoning process, which improves their performance on complex tasks. Currently, reasoning strategies are autonomously selected by LRMs themselves. However, such autonomous selection often produces inefficient or even erroneous reasoning paths. To make reasoning more reliable and flexible, it is important to develop methods for controlling reasoning strategies. Existing methods struggle to control fine-grained reasoning strategies due to conceptual entanglement in LRMs' hidden states. To address this, we leverage Sparse Autoencoders (SAEs) to decompose strategy-entangled hidden states into a disentangled feature space. To identify the few strategy-specific features from the vast pool of SAE features, we propose SAE-Steering, an efficient two-stage feature identification pipeline. SAE-Steering first recalls features that amplify the logits of strategy-specific keywords, filtering out over 99\% of features, and then ranks the remaining features by their control effectiveness. Using the identified strategy-specific features as control vectors, SAE-Steering outperforms existing methods by over 15\% in control effectiveness. Furthermore, controlling reasoning strategies can redirect LRMs from erroneous paths to correct ones, achieving a 7\% absolute accuracy improvement.

</details>


### [19] [Interleaved Tool-Call Reasoning for Protein Function Understanding](https://arxiv.org/abs/2601.03604)
*Chuanliu Fan,Zicheng Ma,Huanran Meng,Aijia Zhang,Wenjie Du,Jun Zhang,Yi Qin Gao,Ziqiang Cao,Guohong Fu*

Main category: cs.AI

TL;DR: PFUA是一个工具增强的蛋白质推理代理，通过整合领域特定工具而非纯文本推理，显著提升蛋白质功能预测性能


<details>
  <summary>Details</summary>
Motivation: 研究发现直接将文本推理范式（如思维链）迁移到蛋白质功能理解领域效果不佳，因为蛋白质功能预测是知识密集型的科学任务，需要外部生物学先验知识和计算工具而非纯内部推理

Method: 提出PFUA工具增强的蛋白质推理代理，统一问题分解、工具调用和基于证据的答案生成，通过整合领域特定工具产生可验证的中间证据

Result: 在四个基准测试中，PFUA持续优于纯文本推理模型，平均性能提升103%

Conclusion: 蛋白质功能预测需要整合领域工具而非纯文本推理，PFUA通过工具增强方法有效解决了这一挑战

Abstract: Recent advances in large language models (LLMs) have highlighted the effectiveness of chain-of-thought reasoning in symbolic domains such as mathematics and programming. However, our study shows that directly transferring such text-based reasoning paradigms to protein function understanding is ineffective: reinforcement learning mainly amplifies superficial keyword patterns while failing to introduce new biological knowledge, resulting in limited generalization. We argue that protein function prediction is a knowledge-intensive scientific task that fundamentally relies on external biological priors and computational tools rather than purely internal reasoning. To address this gap, we propose PFUA, a tool-augmented protein reasoning agent that unifies problem decomposition, tool invocation, and grounded answer generation. Instead of relying on long unconstrained reasoning traces, PFUA integrates domain-specific tools to produce verifiable intermediate evidence. Experiments on four benchmarks demonstrate that PFUA consistently outperforms text-only reasoning models with an average performance improvement of 103%.

</details>


### [20] [Architecting Agentic Communities using Design Patterns](https://arxiv.org/abs/2601.03624)
*Zoran Milosevic,Fethi Rabhi*

Main category: cs.AI

TL;DR: 论文提出基于企业分布式系统标准、形式化方法和行业实践的AI智能体系统架构设计模式，分为LLM智能体、Agentic AI和智能体社区三个层次，重点关注智能体社区作为企业应用的核心协调框架。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和智能体AI技术的快速发展，需要系统化的架构指导来构建复杂的生产级系统。当前缺乏基于企业分布式系统标准和形式化方法的系统性架构模式。

Method: 从企业分布式系统标准、形式化方法和行业实践中提取设计模式，分为三个层次：LLM智能体（任务特定自动化）、Agentic AI（自适应目标寻求者）和智能体社区（组织框架）。重点关注智能体社区，建立形式化框架来规范AI智能体与人类在治理生态系统中的角色协作。

Result: 提出了一个包含实用指导和形式化验证能力的框架，能够通过问责机制表达组织、法律和伦理规则，确保智能体间通信、协商和意图建模的可操作和可验证治理。通过临床试验匹配案例研究验证了该框架。

Conclusion: 该研究为从业者提供了可操作的指导，同时保持了企业部署在动态多智能体生态系统中所必需的形式化严谨性，为构建复杂的生产级AI智能体系统提供了系统化的架构方法。

Abstract: The rapid evolution of Large Language Models (LLM) and subsequent Agentic AI technologies requires systematic architectural guidance for building sophisticated, production-grade systems. This paper presents an approach for architecting such systems using design patterns derived from enterprise distributed systems standards, formal methods, and industry practice. We classify these patterns into three tiers: LLM Agents (task-specific automation), Agentic AI (adaptive goal-seekers), and Agentic Communities (organizational frameworks where AI agents and human participants coordinate through formal roles, protocols, and governance structures). We focus on Agentic Communities - coordination frameworks encompassing LLM Agents, Agentic AI entities, and humans - most relevant for enterprise and industrial applications. Drawing on established coordination principles from distributed systems, we ground these patterns in a formal framework that specifies collaboration agreements where AI agents and humans fill roles within governed ecosystems. This approach provides both practical guidance and formal verification capabilities, enabling expression of organizational, legal, and ethical rules through accountability mechanisms that ensure operational and verifiable governance of inter-agent communication, negotiation, and intent modeling. We validate this framework through a clinical trial matching case study. Our goal is to provide actionable guidance to practitioners while maintaining the formal rigor essential for enterprise deployment in dynamic, multi-agent ecosystems.

</details>


### [21] [How Does the Thinking Step Influence Model Safety? An Entropy-based Safety Reminder for LRMs](https://arxiv.org/abs/2601.03662)
*Su-Hyeon Kim,Hyundong Jin,Yejin Lee,Yo-Sub Han*

Main category: cs.AI

TL;DR: SafeRemind：一种通过在推理步骤中动态注入安全提醒短语来增强大型推理模型安全性的解码时防御方法


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过显式推理步骤取得显著成功，但这些步骤可能放大不安全行为。传统防御机制因忽视LRM独特的推理动态而无效，研究发现推理步骤中出现的安全提醒短语对确保LRM安全至关重要。

Method: 提出SafeRemind解码时防御方法，利用熵触发器在决策锁定点进行干预，动态将安全提醒短语注入推理步骤，无需参数更新即可将潜在有害轨迹重定向到更安全的结果。

Result: 在5个LRM和6个基准测试上的广泛评估表明，SafeRemind显著增强安全性，提升幅度高达45.5个百分点，同时保持核心推理效用。

Conclusion: SafeRemind通过利用推理步骤中的安全提醒短语，提供了一种有效且实用的LRM安全防御方法，在保持模型性能的同时大幅提升安全性。

Abstract: Large Reasoning Models (LRMs) achieve remarkable success through explicit thinking steps, yet the thinking steps introduce a novel risk by potentially amplifying unsafe behaviors. Despite this vulnerability, conventional defense mechanisms remain ineffective as they overlook the unique reasoning dynamics of LRMs. In this work, we find that the emergence of safe-reminding phrases within thinking steps plays a pivotal role in ensuring LRM safety. Motivated by this finding, we propose SafeRemind, a decoding-time defense method that dynamically injects safe-reminding phrases into thinking steps. By leveraging entropy triggers to intervene at decision-locking points, SafeRemind redirects potentially harmful trajectories toward safer outcomes without requiring any parameter updates. Extensive evaluations across five LRMs and six benchmarks demonstrate that SafeRemind substantially enhances safety, achieving improvements of up to 45.5%p while preserving core reasoning utility.

</details>


### [22] [Sandwich Reasoning: An Answer-Reasoning-Answer Approach for Low-Latency Query Correction](https://arxiv.org/abs/2601.03672)
*Chen Zhang,Kepu Zhang,Jiatong Zhang,Xiao Zhang,Jun Xu*

Main category: cs.AI

TL;DR: SandwichR提出一种三明治推理方法，通过答案-推理-答案范式实现低延迟查询纠正，在保持CoT推理准确性的同时减少40-70%延迟。


<details>
  <summary>Details</summary>
Motivation: 现代搜索管道中的查询纠正需要在实时延迟约束下保持高准确性。CoT推理虽然能提高准确性，但延迟过高无法满足实时需求。提前输出答案虽然能减少延迟，但答案无法受益于后续推理过程，导致准确性下降。

Method: 提出SandwichR方法，采用答案-推理-答案范式：首先生成初始纠正，然后进行显式推理过程，最后生成最终精炼纠正。设计了基于一致性的强化学习策略，包括一致性奖励机制和基于边界的拒绝采样，确保初始答案与推理后见解对齐。

Result: 实验结果表明，SandwichR在保持与标准CoT相当的最先进准确性的同时，实现了40-70%的延迟减少，解决了在线搜索中的延迟-准确性权衡问题。

Conclusion: SandwichR通过将快速初始答案与事后推理对齐，实现了低延迟查询纠正而不牺牲推理感知的准确性，解决了CoT推理在实时应用中的延迟瓶颈问题。

Abstract: Query correction is a critical entry point in modern search pipelines, demanding high accuracy strictly within real-time latency constraints. Chain-of-Thought (CoT) reasoning improves accuracy but incurs prohibitive latency for real-time query correction. A potential solution is to output an answer before reasoning to reduce latency; however, under autoregressive decoding, the early answer is independent of subsequent reasoning, preventing the model from leveraging its reasoning capability to improve accuracy. To address this issue, we propose Sandwich Reasoning (SandwichR), a novel approach that explicitly aligns a fast initial answer with post-hoc reasoning, enabling low-latency query correction without sacrificing reasoning-aware accuracy. SandwichR follows an Answer-Reasoning-Answer paradigm, producing an initial correction, an explicit reasoning process, and a final refined correction. To align the initial answer with post-reasoning insights, we design a consistency-aware reinforcement learning (RL) strategy: a dedicated consistency reward enforces alignment between the initial and final corrections, while margin-based rejection sampling prioritizes borderline samples where reasoning drives the most impactful corrective gains. Additionally, we construct a high-quality query correction dataset, addressing the lack of specialized benchmarks for complex query correction. Experimental results demonstrate that SandwichR achieves SOTA accuracy comparable to standard CoT while delivering a 40-70% latency reduction, resolving the latency-accuracy trade-off in online search.

</details>


### [23] [Personalized Medication Planning via Direct Domain Modeling and LLM-Generated Heuristics](https://arxiv.org/abs/2601.03687)
*Yonatan Vernik,Alexander Tuisov,David Izhaki,Hana Weitman,Gal A. Kaminka,Alexander Shleyfman*

Main category: cs.AI

TL;DR: 使用LLM生成领域特定启发式函数，显著提升个性化用药规划的可扩展性，从最多7种药物扩展到至少28种


<details>
  <summary>Details</summary>
Motivation: 现有基于通用领域描述语言的自动化用药规划方法最多只能处理7种药物，这在临床实践中远远不够，需要提升可扩展性以接近实际应用

Method: 通过编程方式定义领域（指定初始状态和状态转移过程），使用LLM生成针对具体问题的启发式函数，结合GBFS搜索算法进行用药规划

Result: 该方法显著提高了覆盖率和规划时间，能够处理至少28种药物的规划问题，使用药规划更接近实际临床应用

Conclusion: 自动生成的领域特定启发式函数能够有效扩展用药规划系统的规模，为临床实践应用迈出了重要一步

Abstract: Personalized medication planning involves selecting medications and determining a dosing schedule to achieve medical goals specific to each individual patient. Previous work successfully demonstrated that automated planners, using general domain-independent heuristics, are able to generate personalized treatments, when the domain and problems are modeled using a general domain description language (\pddlp). Unfortunately, this process was limited in practice to consider no more than seven medications. In clinical terms, this is a non-starter. In this paper, we explore the use of automatically-generated domain- and problem-specific heuristics to be used with general search, as a method of scaling up medication planning to levels allowing closer work with clinicians. Specifically, we specify the domain programmatically (specifying an initial state and a successor generation procedure), and use an LLM to generate a problem specific heuristic that can be used by a fixed search algorithm (GBFS). The results indicate dramatic improvements in coverage and planning time, scaling up the number of medications to at least 28, and bringing medication planning one step closer to practical applications.

</details>


### [24] [EntroCoT: Enhancing Chain-of-Thought via Adaptive Entropy-Guided Segmentation](https://arxiv.org/abs/2601.03769)
*Zihang Li,Yuhang Wang,Yikun Zong,Wenhan Yu,Xiaokun Yuan,Runhan Jiang,Zirui Liu,Tong Yang,Arthur Jiang*

Main category: cs.AI

TL;DR: EntroCoT：通过熵基分割和蒙特卡洛评估自动识别和精炼低质量思维链监督数据，构建高质量数学推理数据集


<details>
  <summary>Details</summary>
Motivation: 现有微调数据集存在"答案正确但推理错误"问题，即最终答案正确但中间步骤存在幻觉、冗余或逻辑错误，这影响了思维链推理的质量

Method: 1. 基于熵的机制在不确定节点分割推理轨迹；2. 蒙特卡洛rollout机制评估每个步骤的边际贡献；3. 准确过滤欺骗性推理样本，构建高质量数据集

Result: 在数学基准测试中，使用EntroCoT构建的子集进行微调始终优于全数据集监督的基线方法

Conclusion: EntroCoT能有效识别和精炼低质量思维链监督数据，构建高质量推理数据集，提升大语言模型的数学推理能力

Abstract: Chain-of-Thought (CoT) prompting has significantly enhanced the mathematical reasoning capabilities of Large Language Models. We find existing fine-tuning datasets frequently suffer from the "answer right but reasoning wrong" probelm, where correct final answers are derived from hallucinated, redundant, or logically invalid intermediate steps. This paper proposes EntroCoT, a unified framework for automatically identifying and refining low-quality CoT supervision traces. EntroCoT first proposes an entropy-based mechanism to segment the reasoning trace into multiple steps at uncertain junctures, and then introduces a Monte Carlo rollout-based mechanism to evaluate the marginal contribution of each step. By accurately filtering deceptive reasoning samples, EntroCoT constructs a high-quality dataset where every intermediate step in each reasoning trace facilitates the final answer. Extensive experiments on mathematical benchmarks demonstrate that fine-tuning on the subset constructed by EntroCoT consistently outperforms the baseslines of full-dataset supervision.

</details>


### [25] [ROI-Reasoning: Rational Optimization for Inference via Pre-Computation Meta-Cognition](https://arxiv.org/abs/2601.03822)
*Muyang Zhao,Qi Qi,Hao Sun*

Main category: cs.AI

TL;DR: ROI-Reasoning：一个两阶段框架，让大语言模型在严格全局token约束下进行预算推理，通过元认知微调和理性感知强化学习优化计算资源分配


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然具备强大的推理能力，但无法自动判断不同任务所需的计算量。在实际应用中，往往面临严格的全局token预算约束，需要模型能够智能地分配计算资源，在有限预算下最大化整体性能。

Method: 提出ROI-Reasoning两阶段框架：1) 元认知微调：训练模型在生成前预测推理成本和预期效用，做出明确的解决或跳过决策；2) 理性感知强化学习：在硬token预算下优化序列决策，学习长期分配策略。将问题形式化为有序随机多选择背包问题。

Result: 在预算数学推理基准测试中，ROI-Reasoning在严格计算预算下持续提高整体得分，同时显著减少遗憾（regret）。

Conclusion: ROI-Reasoning框架赋予大语言模型内在的预算感知理性能力，使其能够在有限计算资源下智能分配推理预算，实现更高效的推理性能。

Abstract: Large language models (LLMs) can achieve strong reasoning performance with sufficient computation, but they do not inherently know how much computation a task requires. We study budgeted inference-time reasoning for multiple tasks under a strict global token constraint and formalize it as a Ordered Stochastic Multiple-Choice Knapsack Problem(OS-MCKP). This perspective highlights a meta-cognitive requirement -- anticipating task difficulty, estimating return over investment (ROI), and allocating computation strategically. We propose ROI-Reasoning, a two-stage framework that endows LLMs with intrinsic, budget-aware rationality. In the first stage, Meta-Cognitive Fine-Tuning teaches models to predict reasoning cost and expected utility before generation, enabling explicit solve-or-skip decisions. Next, Rationality-Aware Reinforcement Learning optimizes sequential decision making under a hard token budget, allowing models to learn long-horizon allocation strategies. Across budgeted mathematical reasoning benchmarks, ROI-Reasoning consistently improves overall score while substantially reducing regret under tight computation budgets.

</details>


### [26] [Defeasible Conditionals using Answer Set Programming](https://arxiv.org/abs/2601.03840)
*Racquel Dennison,Jesse Heyninck,Thomas Meyer*

Main category: cs.AI

TL;DR: 使用答案集编程（ASP）为理性闭包（RC）提供声明式定义，实现从知识库自动构建最小排名模型并支持查询蕴涵检查，证明编码正确性且计算效率优于现有实现。


<details>
  <summary>Details</summary>
Motivation: 可废止蕴涵用于从不完整信息中得出合理结论，KLM框架是其基础模型，理性闭包（RC）是该框架中最突出的算法之一。现有实现多为命令式方法，需要更声明式、自动化的解决方案。

Method: 使用答案集编程（ASP）为理性闭包提供声明式定义，通过ASP编码自动从给定知识库构建最小排名模型，支持指定查询的蕴涵检查，并形式化证明编码的正确性。

Result: ASP方法能够正确实现理性闭包的理论基础，与现有命令式实现（特别是InfOCF求解器）相比，在计算效率上表现出改进。经验评估验证了方法的有效性。

Conclusion: ASP为理性闭包提供了一种声明式、自动化的计算方法，不仅符合理论要求，而且在计算效率上优于现有命令式实现，为可废止推理的实际应用提供了新途径。

Abstract: Defeasible entailment is concerned with drawing plausible conclusions from incomplete information. A foundational framework for modelling defeasible entailment is the KLM framework. Introduced by Kraus, Lehmann, and Magidor, the KLM framework outlines several key properties for defeasible entailment. One of the most prominent algorithms within this framework is Rational Closure (RC). This paper presents a declarative definition for computing RC using Answer Set Programming (ASP). Our approach enables the automatic construction of the minimal ranked model from a given knowledge base and supports entailment checking for specified queries. We formally prove the correctness of our ASP encoding and conduct empirical evaluations to compare the performance of our implementation with that of existing imperative implementations, specifically the InfOCF solver. The results demonstrate that our ASP-based approach adheres to RC's theoretical foundations and offers improved computational efficiency.

</details>


### [27] [XAI-LAW: A Logic Programming Tool for Modeling, Explaining, and Learning Legal Decisions](https://arxiv.org/abs/2601.03844)
*Agostino Dovier,Talissa Dreossi,Andrea Formisano,Benedetta Strizzolo*

Main category: cs.AI

TL;DR: 使用ASP对意大利刑法典建模，通过半自动学习司法判例生成法律规则，支持刑事审判推理和解释


<details>
  <summary>Details</summary>
Motivation: 为法律专家在刑事审判阶段提供推理支持和可能的判决结果，提高司法决策的可解释性

Method: 使用答案集编程（ASP）对意大利刑法典条款进行编码建模，包括"人身犯罪"和财产犯罪；利用基于稳定模型"支持性"的工具处理矛盾并生成解释；集成归纳逻辑编程系统从案例中归纳法律规则

Result: 开发出支持法律推理的工具，能够基于先前判决验证模型，为新案件生成可能的决策并提供解释，提高决策过程的透明度

Conclusion: ASP方法能有效建模刑法典并学习法律规则，工具提供的自动解释性有助于阐明司法决策逻辑，使决策过程更加可解释

Abstract: We propose an approach to model articles of the Italian Criminal Code (ICC), using Answer Set Programming (ASP), and to semi-automatically learn legal rules from examples based on prior judicial decisions. The developed tool is intended to support legal experts during the criminal trial phase by providing reasoning and possible legal outcomes. The methodology involves analyzing and encoding articles of the ICC in ASP, including "crimes against the person" and property offenses. The resulting model is validated on a set of previous verdicts and refined as necessary. During the encoding process, contradictions may arise; these are properly handled by the system, which also generates possible decisions for new cases and provides explanations through a tool that leverages the "supportedness" of stable models. The automatic explainability offered by the tool can also be used to clarify the logic behind judicial decisions, making the decision-making process more interpretable. Furthermore, the tool integrates an inductive logic programming system for ASP, which is employed to generalize legal rules from case examples.

</details>


### [28] [Formally Explaining Decision Tree Models with Answer Set Programming](https://arxiv.org/abs/2601.03845)
*Akihiro Takemura,Masayuki Otani,Katsumi Inoue*

Main category: cs.AI

TL;DR: 使用答案集编程（ASP）为决策树模型生成多种解释类型，相比SAT方法更灵活且支持枚举所有可能解释


<details>
  <summary>Details</summary>
Motivation: 决策树模型（如随机森林和梯度提升树）在机器学习中广泛使用，但其复杂结构难以解释，特别是在需要正式决策依据的安全关键应用中。现有工作表明可以通过自动推理技术推导逻辑和溯因解释。

Method: 提出基于答案集编程（ASP）的方法，用于生成多种解释类型：充分解释、对比解释、多数解释和树特定解释。相比基于SAT的方法，ASP方法在编码用户偏好方面更灵活，并支持枚举所有可能的解释。

Result: 在多样化数据集上进行实证评估，展示了该方法相比现有方法的有效性和局限性。

Conclusion: ASP方法为决策树模型解释提供了灵活且全面的解决方案，能够生成多种类型的解释并支持用户偏好编码，但在某些方面仍存在局限性。

Abstract: Decision tree models, including random forests and gradient-boosted decision trees, are widely used in machine learning due to their high predictive performance.  However, their complex structures often make them difficult to interpret, especially in safety-critical applications where model decisions require formal justification.  Recent work has demonstrated that logical and abductive explanations can be derived through automated reasoning techniques.  In this paper, we propose a method for generating various types of explanations, namely, sufficient, contrastive, majority, and tree-specific explanations, using Answer Set Programming (ASP).  Compared to SAT-based approaches, our ASP-based method offers greater flexibility in encoding user preferences and supports enumeration of all possible explanations.  We empirically evaluate the approach on a diverse set of datasets and demonstrate its effectiveness and limitations compared to existing methods.

</details>


### [29] [xDNN(ASP): Explanation Generation System for Deep Neural Networks powered by Answer Set Programming](https://arxiv.org/abs/2601.03847)
*Ly Ly Trieu,Tran Cao Son*

Main category: cs.AI

TL;DR: xDNN(ASP) 是一个基于答案集语义的深度神经网络全局解释系统，通过提取逻辑程序来理解模型决策过程，同时保持高预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有可解释AI方法主要关注输入输出关系，忽视了神经网络内部结构在解释生成中的作用。需要一种能够提供全局解释并考虑网络结构的方法。

Method: 提出xDNN(ASP)系统，给定神经网络模型和训练数据，提取基于答案集语义的逻辑程序，使程序答案集与网络输入输出对一一对应。

Result: 在两个合成数据集上的实验表明，提取的逻辑程序不仅保持高预测精度，还提供了特征重要性和隐藏节点对预测影响等有价值信息。

Conclusion: xDNN(ASP)能够生成全局解释，帮助理解模型决策，同时为网络优化（如减少隐藏节点）提供指导，弥补了现有方法忽视网络结构的不足。

Abstract: Explainable artificial intelligence (xAI) has gained significant attention in recent years. Among other things, explainablility for deep neural networks has been a topic of intensive research due to the meteoric rise in prominence of deep neural networks and their "black-box" nature. xAI approaches can be characterized along different dimensions such as their scope (global versus local explanations) or underlying methodologies (statistic-based versus rule-based strategies). Methods generating global explanations aim to provide reasoning process applicable to all possible output classes while local explanation methods focus only on a single, specific class. SHAP (SHapley Additive exPlanations), a well-known statistical technique, identifies important features of a network. Deep neural network rule extraction method constructs IF-THEN rules that link input conditions to a class. Another approach focuses on generating counterfactuals which help explain how small changes to an input can affect the model's predictions. However, these techniques primarily focus on the input-output relationship and thus neglect the structure of the network in explanation generation.   In this work, we propose xDNN(ASP), an explanation generation system for deep neural networks that provides global explanations. Given a neural network model and its training data, xDNN(ASP) extracts a logic program under answer set semantics that-in the ideal case-represents the trained model, i.e., answer sets of the extracted program correspond one-to-one to input-output pairs of the network. We demonstrate experimentally, using two synthetic datasets, that not only the extracted logic program maintains a high-level of accuracy in the prediction task, but it also provides valuable information for the understanding of the model such as the importance of features as well as the impact of hidden nodes on the prediction. The latter can be used as a guide for reducing the number of nodes used in hidden layers, i.e., providing a means for optimizing the network.

</details>


### [30] [Investigating the Grounding Bottleneck for a Large-Scale Configuration Problem: Existing Tools and Constraint-Aware Guessing](https://arxiv.org/abs/2601.03850)
*Veronika Semmelrock,Gerhard Friedrich*

Main category: cs.AI

TL;DR: ASP技术在大规模配置问题（如3万+组件电子系统）中面临内存瓶颈，通过增量求解和约束感知猜测方法显著降低内存需求


<details>
  <summary>Details</summary>
Motivation: 研究当前ASP技术在大规模配置问题中的可扩展性，特别是针对超过3万个组件的电子系统配置，解决ASP求解中的内存瓶颈问题

Method: 1. 分析ASP中的基础化瓶颈问题；2. 采用增量求解方法；3. 开发约束感知猜测方法，基于基础化分析减少内存需求

Result: 增量求解在实践中有效，但仍有内存限制；约束感知猜测方法显著降低了内存需求，提升了ASP在大规模配置问题中的可扩展性

Conclusion: ASP技术在大规模配置问题中具有潜力，但需要专门的方法（如约束感知猜测）来解决内存瓶颈，才能有效扩展到超大规模问题实例

Abstract: Answer set programming (ASP) aims to realize the AI vision: The user specifies the problem, and the computer solves it. Indeed, ASP has made this vision true in many application domains. However, will current ASP solving techniques scale up for large configuration problems? As a benchmark for such problems, we investigated the configuration of electronic systems, which may comprise more than 30,000 components. We show the potential and limits of current ASP technology, focusing on methods that address the so-called grounding bottleneck, i.e., the sharp increase of memory demands in the size of the problem instances. To push the limits, we investigated the incremental solving approach, which proved effective in practice. However, even in the incremental approach, memory demands impose significant limits. Based on an analysis of grounding, we developed the method constraint-aware guessing, which significantly reduced the memory need.

</details>


### [31] [Current Agents Fail to Leverage World Model as Tool for Foresight](https://arxiv.org/abs/2601.03905)
*Cheng Qian,Emre Can Acikgoz,Bingxuan Li,Xiusi Chen,Yuji Zhang,Bingxiang He,Qinyu Luo,Dilek Hakkani-Tür,Gokhan Tur,Yunzhu Li,Heng Ji,Heng Ji*

Main category: cs.AI

TL;DR: 当前基于视觉语言模型的智能体在利用生成世界模型进行前瞻性推理方面存在困难，主要表现为很少调用模拟、误用预测结果，甚至性能下降


<details>
  <summary>Details</summary>
Motivation: 随着智能体面临更多需要预测未来状态的任务，生成世界模型理论上可以作为外部模拟器帮助智能体预见结果。本文旨在实证检验当前智能体是否能有效利用世界模型作为工具来增强其认知能力

Method: 通过在各种智能体任务和视觉问答任务上进行实验，观察智能体如何使用世界模型进行模拟，包括调用频率、结果使用方式等，并进行归因分析

Result: 研究发现：1）智能体很少调用模拟（少于1%）；2）经常误用预测结果（约15%）；3）当模拟可用或被强制使用时，性能表现不一致甚至下降（最多5%）。归因分析表明主要瓶颈在于智能体决定何时模拟、如何解释预测结果以及如何将预见整合到下游推理的能力

Conclusion: 当前智能体难以有效利用世界模型进行前瞻性认知，需要开发促进校准、战略性交互的机制，为未来智能体系统实现更可靠的前瞻性认知铺平道路

Abstract: Agents built on vision-language models increasingly face tasks that demand anticipating future states rather than relying on short-horizon reasoning. Generative world models offer a promising remedy: agents could use them as external simulators to foresee outcomes before acting. This paper empirically examines whether current agents can leverage such world models as tools to enhance their cognition. Across diverse agentic and visual question answering tasks, we observe that some agents rarely invoke simulation (fewer than 1%), frequently misuse predicted rollouts (approximately 15%), and often exhibit inconsistent or even degraded performance (up to 5%) when simulation is available or enforced. Attribution analysis further indicates that the primary bottleneck lies in the agents' capacity to decide when to simulate, how to interpret predicted outcomes, and how to integrate foresight into downstream reasoning. These findings underscore the need for mechanisms that foster calibrated, strategic interaction with world models, paving the way toward more reliable anticipatory cognition in future agent systems.

</details>


### [32] [Trade-R1: Bridging Verifiable Rewards to Stochastic Environments via Process-Level Reasoning Verification](https://arxiv.org/abs/2601.03948)
*Rui Sun,Yifan Sun,Sheng Xu,Li Zhao,Jing Li,Daxin Jiang,Chen Hua,Zuo Bai*

Main category: cs.AI

TL;DR: Trade-R1框架通过过程级推理验证将可验证奖励与随机金融环境连接，解决标准RL在金融决策中的奖励黑客问题，使用结构化RAG任务评估推理质量，通过三角一致性指标过滤噪声市场回报。


<details>
  <summary>Details</summary>
Motivation: RL在数学和编程等可验证奖励领域表现优异，但扩展到金融决策时面临挑战：市场具有随机性，奖励虽然可验证但噪声严重，导致标准RL退化为奖励黑客行为。

Method: 提出Trade-R1训练框架，通过过程级推理验证连接可验证奖励与随机环境。核心创新是将长篇金融文档的推理评估转化为结构化RAG任务，构建三角一致性指标（检索证据、推理链、决策之间的两两对齐），作为噪声市场回报的有效性过滤器。探索两种奖励整合策略：固定效应语义奖励（FSR）用于稳定对齐信号，动态效应语义奖励（DSR）用于耦合幅度优化。

Result: 在不同国家资产选择实验中，该范式减少了奖励黑客现象，DSR实现了优越的跨市场泛化能力，同时保持了最高的推理一致性。

Conclusion: Trade-R1框架成功解决了金融决策中RL的奖励黑客问题，通过过程级推理验证和结构化RAG评估，将可验证奖励与随机市场环境有效连接，为金融领域的RL应用提供了新范式。

Abstract: Reinforcement Learning (RL) has enabled Large Language Models (LLMs) to achieve remarkable reasoning in domains like mathematics and coding, where verifiable rewards provide clear signals. However, extending this paradigm to financial decision is challenged by the market's stochastic nature: rewards are verifiable but inherently noisy, causing standard RL to degenerate into reward hacking. To address this, we propose Trade-R1, a model training framework that bridges verifiable rewards to stochastic environments via process-level reasoning verification. Our key innovation is a verification method that transforms the problem of evaluating reasoning over lengthy financial documents into a structured Retrieval-Augmented Generation (RAG) task. We construct a triangular consistency metric, assessing pairwise alignment between retrieved evidence, reasoning chains, and decisions to serve as a validity filter for noisy market returns. We explore two reward integration strategies: Fixed-effect Semantic Reward (FSR) for stable alignment signals, and Dynamic-effect Semantic Reward (DSR) for coupled magnitude optimization. Experiments on different country asset selection demonstrate that our paradigm reduces reward hacking, with DSR achieving superior cross-market generalization while maintaining the highest reasoning consistency.

</details>


### [33] [Anti-Length Shift: Dynamic Outlier Truncation for Training Efficient Reasoning Models](https://arxiv.org/abs/2601.03969)
*Wei Wu,Liyi Chen,Congxi Xiao,Tianfu Wang,Qimeng Wang,Chengqiang Lu,Yan Gao,Yi Wu,Yao Hu,Hui Xiong*

Main category: cs.AI

TL;DR: 提出DOT方法解决大模型在简单问题上过度推理的问题，通过动态截断冗余token减少78%推理token使用，同时提升准确率


<details>
  <summary>Details</summary>
Motivation: 现有强化学习增强的大推理模型在简单查询上会产生过度冗长的推理，导致部署成本高昂。现有基于长度惩罚的方法存在优化冲突，且未深入探究过度推理的生成机制。

Method: 提出动态异常截断(DOT)：在训练时选择性抑制冗余token，仅针对完全正确rollout组中的极端长度尾部进行截断。同时结合辅助KL正则化和预测性动态采样确保稳定收敛。

Result: 在多个模型规模上显著推进了效率-性能的帕累托前沿。在AIME-24上，相比初始策略减少78%推理token使用，同时提升准确率，超越现有高效推理方法。

Conclusion: DOT方法有效解决了大模型在简单问题上的过度推理问题，通过训练时干预实现了推理效率与性能的双重提升，为高效推理提供了新思路。

Abstract: Large reasoning models enhanced by reinforcement learning with verifiable rewards have achieved significant performance gains by extending their chain-of-thought. However, this paradigm incurs substantial deployment costs as models often exhibit excessive verbosity on simple queries. Existing efficient reasoning methods relying on explicit length penalties often introduce optimization conflicts and leave the generative mechanisms driving overthinking largely unexamined. In this paper, we identify a phenomenon termed length shift where models increasingly generate unnecessary reasoning on trivial inputs during training. To address this, we introduce Dynamic Outlier Truncation (DOT), a training-time intervention that selectively suppresses redundant tokens. This method targets only the extreme tail of response lengths within fully correct rollout groups while preserving long-horizon reasoning capabilities for complex problems. To complement this intervention and ensure stable convergence, we further incorporate auxiliary KL regularization and predictive dynamic sampling. Experimental results across multiple model scales demonstrate that our approach significantly pushes the efficiency-performance Pareto frontier outward. Notably, on the AIME-24, our method reduces inference token usage by 78% while simultaneously increasing accuracy compared to the initial policy and surpassing state-of-the-art efficient reasoning methods.

</details>


### [34] [MobileDreamer: Generative Sketch World Model for GUI Agent](https://arxiv.org/abs/2601.04035)
*Yilin Cao,Yufeng Zhong,Zhixiong Zeng,Liming Zheng,Jing Huang,Haibo Qiu,Peng Shi,Wenji Mao,Wan Guanglu*

Main category: cs.AI

TL;DR: MobileDreamer：基于世界模型的移动GUI智能体前瞻框架，通过文本草图世界模型预测动作后状态，提升长时程任务性能


<details>
  <summary>Details</summary>
Motivation: 现有移动GUI智能体多为反应式，仅基于当前屏幕决策，限制了长时程任务表现。构建世界模型可通过预测动作结果支持更好决策，但需平衡空间感知预测效率与实用部署需求。

Method: 提出MobileDreamer框架：1) 文本草图世界模型：将数字图像转换为关键任务相关草图，采用顺序不变学习策略保持GUI元素空间信息；2) 前瞻推演策略：利用世界模型预测能力优化动作选择过程

Result: 在Android World实验中达到最先进性能，任务成功率提升5.25%。世界模型评估验证文本草图建模能准确预测关键GUI元素

Conclusion: MobileDreamer通过高效的世界模型前瞻框架显著提升移动GUI智能体在长时程任务中的表现，证明了基于世界模型的想象机制对GUI自动化任务的有效性

Abstract: Mobile GUI agents have shown strong potential in real-world automation and practical applications. However, most existing agents remain reactive, making decisions mainly from current screen, which limits their performance on long-horizon tasks. Building a world model from repeated interactions enables forecasting action outcomes and supports better decision making for mobile GUI agents. This is challenging because the model must predict post-action states with spatial awareness while remaining efficient enough for practical deployment. In this paper, we propose MobileDreamer, an efficient world-model-based lookahead framework to equip the GUI agents based on the future imagination provided by the world model. It consists of textual sketch world model and rollout imagination for GUI agent. Textual sketch world model forecasts post-action states through a learning process to transform digital images into key task-related sketches, and designs a novel order-invariant learning strategy to preserve the spatial information of GUI elements. The rollout imagination strategy for GUI agent optimizes the action-selection process by leveraging the prediction capability of world model. Experiments on Android World show that MobileDreamer achieves state-of-the-art performance and improves task success by 5.25%. World model evaluations further verify that our textual sketch modeling accurately forecasts key GUI elements.

</details>


### [35] [ComfySearch: Autonomous Exploration and Reasoning for ComfyUI Workflows](https://arxiv.org/abs/2601.04060)
*Jinwei Su,Qizhen Lan,Zeyu Wang,Yinghui Xia,Hairu Wen,Yiqun Duan,Xi Xiao,Tianyu Shi,Yang Jingsong,Lewei He*

Main category: cs.AI

TL;DR: ComfySearch是一个基于代理的框架，用于在ComfyUI平台上探索组件空间并生成功能性的工作流，通过验证引导的工作流构建解决现有方法在复杂创意任务中的低通过率问题。


<details>
  <summary>Details</summary>
Motivation: ComfyUI平台上的AI生成内容已从单一模型发展为模块化工作流，但大量组件和严格的图约束导致长时程结构一致性难以保持，使得工作流通过率低且质量有限。

Method: 提出了ComfySearch代理框架，通过验证引导的工作流构建来有效探索组件空间，生成功能性的ComfyUI管道。

Result: 实验表明ComfySearch在复杂创意任务上显著优于现有方法，实现了更高的可执行性（通过）率、更高的解决方案率和更强的泛化能力。

Conclusion: ComfySearch通过验证引导的代理框架有效解决了ComfyUI工作流构建中的挑战，为模块化AI内容生成提供了更可靠和高质量的解决方案。

Abstract: AI-generated content has progressed from monolithic models to modular workflows, especially on platforms like ComfyUI, allowing users to customize complex creative pipelines. However, the large number of components in ComfyUI and the difficulty of maintaining long-horizon structural consistency under strict graph constraints frequently lead to low pass rates and workflows of limited quality. To tackle these limitations, we present ComfySearch, an agentic framework that can effectively explore the component space and generate functional ComfyUI pipelines via validation-guided workflow construction. Experiments demonstrate that ComfySearch substantially outperforms existing methods on complex and creative tasks, achieving higher executability (pass) rates, higher solution rates, and stronger generalization.

</details>


### [36] [Agent Drift: Quantifying Behavioral Degradation in Multi-Agent LLM Systems Over Extended Interactions](https://arxiv.org/abs/2601.04170)
*Abhishek Rath*

Main category: cs.AI

TL;DR: 本文提出"智能体漂移"概念，即多智能体LLM系统在长期运行中行为、决策质量和协作一致性的渐进退化，并开发了量化漂移的ASI指标框架和三种缓解策略。


<details>
  <summary>Details</summary>
Motivation: 多智能体LLM系统已成为复杂任务分解和协作解决的有力架构，但其长期行为稳定性尚未得到充分研究。需要理解智能体在长时间交互序列中行为退化的现象及其对系统可靠性的影响。

Method: 提出智能体漂移的理论框架，包括语义漂移、协调漂移和行为漂移三种表现形式。开发了智能体稳定性指数（ASI）这一复合度量框架，包含12个维度来量化漂移。通过仿真分析和理论建模验证漂移影响，并提出三种缓解策略：情景记忆整合、漂移感知路由协议和自适应行为锚定。

Result: 研究表明，未受控制的智能体漂移会导致任务完成准确率显著下降和人工干预需求增加。理论分析表明，提出的缓解策略能够显著减少漂移相关错误，同时保持系统吞吐量。

Conclusion: 本研究建立了监测、测量和缓解生产性智能AI系统中智能体漂移的基础方法论，对企业部署可靠性和AI安全研究具有直接意义，为多智能体系统的长期稳定性提供了理论框架和实践指导。

Abstract: Multi-agent Large Language Model (LLM) systems have emerged as powerful architectures for complex task decomposition and collaborative problem-solving. However, their long-term behavioral stability remains largely unexamined. This study introduces the concept of agent drift, defined as the progressive degradation of agent behavior, decision quality, and inter-agent coherence over extended interaction sequences. We present a comprehensive theoretical framework for understanding drift phenomena, proposing three distinct manifestations: semantic drift (progressive deviation from original intent), coordination drift (breakdown in multi-agent consensus mechanisms), and behavioral drift (emergence of unintended strategies).
  We introduce the Agent Stability Index (ASI), a novel composite metric framework for quantifying drift across twelve dimensions, including response consistency, tool usage patterns, reasoning pathway stability, and inter-agent agreement rates. Through simulation-based analysis and theoretical modeling, we demonstrate how unchecked agent drift can lead to substantial reductions in task completion accuracy and increased human intervention requirements.
  We propose three mitigation strategies: episodic memory consolidation, drift-aware routing protocols, and adaptive behavioral anchoring. Theoretical analysis suggests these approaches can significantly reduce drift-related errors while maintaining system throughput. This work establishes a foundational methodology for monitoring, measuring, and mitigating agent drift in production agentic AI systems, with direct implications for enterprise deployment reliability and AI safety research.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [37] [Automated Feedback Generation for Undergraduate Mathematics: Development and Evaluation of an AI Teaching Assistant](https://arxiv.org/abs/2601.03458)
*Aron Gohr,Marie-Amelie Lawn,Kevin Gao,Inigo Serjeant,Stephen Heslip*

Main category: cs.CY

TL;DR: 开发了一个基于大语言模型的智能辅导系统，能够处理自由形式的数学证明，提供技术正确性和写作风格反馈，质量接近人类专家水平


<details>
  <summary>Details</summary>
Motivation: 传统智能辅导系统只能处理结构化、受限的问题，无法可靠评估自由形式的数学推理，需要开发能处理自然语言输入、应对各种边缘情况的系统

Method: 使用大语言模型构建模块化工作流，配置可读且无需编程知识即可编辑，允许教师预计算或注入中间步骤

Result: 系统反馈质量在评估指标上与人类专家相当，已部署在Imperial数学作业平台Lambdafeedback上，对高级问题既有成功也有差距

Conclusion: 该系统能够有效评估自由形式数学证明，提供技术正确性和风格反馈，为智能辅导系统在更广泛数学教育中的应用提供了有前景的途径

Abstract: Intelligent tutoring systems have long enabled automated immediate feedback on student work when it is presented in a tightly structured format and when problems are very constrained, but reliably assessing free-form mathematical reasoning remains challenging.
  We present a system that processes free-form natural language input, handles a wide range of edge cases, and comments competently not only on the technical correctness of submitted proofs, but also on style and presentation issues. We discuss the advantages and disadvantages of various approaches to the evaluation of such a system, and show that by the metrics we evaluate, the quality of the feedback generated is comparable to that produced by human experts when assessing early undergraduate homework. We stress-test our system with a small set of more advanced and unusual questions, and report both significant gaps and encouraging successes in that more challenging setting.
  Our system uses large language models in a modular workflow. The workflow configuration is human-readable and editable without programming knowledge, and allows some intermediate steps to be precomputed or injected by the instructor.
  A version of our tool is deployed on the Imperial mathematics homework platform Lambdafeedback. We report also on the integration of our tool into this platform.

</details>


### [38] [Can AI Chatbots Provide Coaching in Engineering? Beyond Information Processing Toward Mastery](https://arxiv.org/abs/2601.03693)
*Junaid Qadir,Muhammad Adil Attique,Saleha Shoaib,Syed Ibrahim Ghaznavi*

Main category: cs.CY

TL;DR: AI聊天机器人能在工程教育中提供技术指导，但在道德、情感和情境判断方面存在局限，需要结合人类导师的智慧形成混合指导框架。


<details>
  <summary>Details</summary>
Motivation: 工程教育面临双重冲击：传统学徒制模式正在衰落，而生成式AI作为非正式指导伙伴出现。这引发了关于AI计算极限、具身理性本质以及信息处理与智慧区别的哲学问题。

Method: 综合数十年关于专业知识、隐性知识和人机交互的学术观点，并在当代AI驱动教育背景下进行定位。通过混合方法研究（75名学生，7名教师）探索指导聊天机器人在工程教育中的应用。

Result: 参与者接受AI用于技术问题解决（收敛性任务，平均评分3.84/5），但对其道德、情感和情境判断能力（发散性任务）持怀疑态度。教师对风险担忧更强，64-71%参与者要求严格保密性。

Conclusion: 生成式AI可以民主化认知和程序支持，但无法复制人类导师的具身、价值导向维度。提出整合人类智慧的专家在环模型的多重指导框架，在保持学徒制深度的同时利用AI的可扩展性。

Abstract: Engineering education faces a double disruption: traditional apprenticeship models that cultivated judgment and tacit skill are eroding, just as generative AI emerges as an informal coaching partner. This convergence rekindles long-standing questions in the philosophy of AI and cognition about the limits of computation, the nature of embodied rationality, and the distinction between information processing and wisdom. Building on this rich intellectual tradition, this paper examines whether AI chatbots can provide coaching that fosters mastery rather than merely delivering information. We synthesize critical perspectives from decades of scholarship on expertise, tacit knowledge, and human-machine interaction, situating them within the context of contemporary AI-driven education. Empirically, we report findings from a mixed-methods study (N = 75 students, N = 7 faculty) exploring the use of a coaching chatbot in engineering education. Results reveal a consistent boundary: participants accept AI for technical problem solving (convergent tasks; M = 3.84 on a 1-5 Likert scale) but remain skeptical of its capacity for moral, emotional, and contextual judgment (divergent tasks). Faculty express stronger concerns over risk (M = 4.71 vs. M = 4.14, p = 0.003), and privacy emerges as a key requirement, with 64-71 percent of participants demanding strict confidentiality. Our findings suggest that while generative AI can democratize access to cognitive and procedural support, it cannot replicate the embodied, value-laden dimensions of human mentorship. We propose a multiplex coaching framework that integrates human wisdom within expert-in-the-loop models, preserving the depth of apprenticeship while leveraging AI scalability to enrich the next generation of engineering education.

</details>


### [39] [The Power of 10: New Rules for the Digital World](https://arxiv.org/abs/2601.03709)
*Sarah Spiekermann-Hoff,Marc Langheinrich,Johannes Hoff,Christiane Wendehorst,Jürgen Pfeffer,Thomas Fuchs,Armin Grunwald*

Main category: cs.CY

TL;DR: 欧洲跨学科团队提出"数字世界十诫"，作为应对AI技术快速发展的伦理框架，旨在引导个人和社会在"超级技术"时代做出明智、以人为本的决策。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能快速发展，社会过度关注超人机器和无缝数字未来的承诺，却忽视了日益增长的社会、伦理和心理问题，如监控和心理健康危机。迫切需要指导原则来应对这些技术变革。

Method: 借鉴圣经十诫的持久影响力，欧洲跨学科团队提出了"数字世界十诫"，这是一个新颖的伦理框架，旨在帮助个人和社会在技术"超级充电"时代做出谨慎、以人为本的决策。

Result: 提出了一个具体的伦理框架——"数字世界十诫"，为应对数字技术带来的社会、伦理和心理挑战提供了指导原则。

Conclusion: 在人工智能和数字技术快速发展的时代，迫切需要建立像"数字世界十诫"这样的伦理框架，以确保技术发展符合人类价值观，帮助社会做出明智、以人为本的决策。

Abstract: As artificial intelligence rapidly advances, society is increasingly captivated by promises of superhuman machines and seamless digital futures. Yet these visions often obscure mounting social, ethical, and psychological concerns tied to pervasive digital technologies - from surveillance to mental health crises. This article argues that a guiding ethos is urgently needed to navigate these transformations. Inspired by the lasting influence of the biblical Ten Commandments, a European interdisciplinary group has proposed "Ten Rules for the Digital World" - a novel ethical framework to help individuals and societies make prudent, human-centered decisions in the age of "supercharged" technology.

</details>


### [40] [Criminal Liability of Generative Artificial Intelligence Providers for User-Generated Child Sexual Abuse Material](https://arxiv.org/abs/2601.03788)
*Anamaria Mojica-Hanke,Thomas Goger,Svenja Wölfel,Brian Valerius,Steffen Herbold*

Main category: cs.CY

TL;DR: 生成式AI生成儿童性虐待材料(CSAM)可能引发多方刑事责任，包括用户、开发者、研究人员和公司代表，责任评估受图像类型、内容审核政策和技术因素影响。


<details>
  <summary>Details</summary>
Motivation: 生成式AI能力的增强带来了新的法律挑战，特别是在儿童性虐待材料生成方面存在法律灰色地带，需要明确相关责任方的刑事责任。

Method: 采用多学科研究方法，结合德国相关法律的法定解释和具体场景分析，探讨生成式AI在CSAM生成背景下的不同属性。

Result: 研究发现生成CSAM不仅用户可能构成主要犯罪，独立软件开发者、研究人员和公司代表等模型责任方也可能面临刑事和法律责任。责任评估受生成图像类型、内容审核政策和模型预期用途等因素影响。

Conclusion: 研究讨论了不同角色的法律影响，并提出了开发此类系统时的要求，强调了在生成式AI发展中需要明确法律责任框架。

Abstract: The development of more powerful Generative Artificial Intelligence (GenAI) has expanded its capabilities and the variety of outputs. This has introduced significant legal challenges, including gray areas in various legal systems, such as the assessment of criminal liability for those responsible for these models. Therefore, we conducted a multidisciplinary study utilizing the statutory interpretation of relevant German laws, which, in conjunction with scenarios, provides a perspective on the different properties of GenAI in the context of Child Sexual Abuse Material (CSAM) generation. We found that generating CSAM with GenAI may have criminal and legal consequences not only for the user committing the primary offense but also for individuals responsible for the models, such as independent software developers, researchers, and company representatives. Additionally, the assessment of criminal liability may be affected by contextual and technical factors, including the type of generated image, content moderation policies, and the model's intended purpose. Based on our findings, we discussed the implications for different roles, as well as the requirements when developing such systems.

</details>


### [41] [The Bathtub of European AI Governance: Identifying Technical Sandboxes as the Micro-Foundation of Regulatory Learning](https://arxiv.org/abs/2601.04094)
*Tom Deckenbrunnen,Alessio Buscemi,Marco Almada,Alfredo Capozucca,German Castignani*

Main category: cs.CY

TL;DR: 欧盟AI法案采用横向适应性监管框架，但缺乏可扩展信息流动的技术基础。本文建立监管学习空间理论模型，将AI技术沙盒定位为微观证据生成引擎，连接立法指令与技术实施。


<details>
  <summary>Details</summary>
Motivation: 欧盟AI法案虽然包含监管学习条款，但缺乏清晰的技术基础来实现可扩展的信息流动。现有监管机制在复杂参与者网络中运作，需要理论模型来理解监管学习空间的结构和功能。

Method: 建立监管学习空间理论模型，分解为微观、中观和宏观三个层次。从功能视角分析各利益相关者（从欧盟委员会到AI开发者）在执法（宏观-微观）和证据聚合（微观-宏观）转换中的位置。将AI技术沙盒定位为微观证据生成引擎。

Result: 提出了三层监管学习空间模型，明确了AI技术沙盒作为微观证据生成的关键作用。该模型为技术专家和法律专家之间的结构化对话提供了框架，有助于弥合立法指令与技术实施之间的差距。

Conclusion: 通过建立监管学习空间理论模型，本文为欧盟AI法案的可操作性提供了技术基础。AI技术沙盒作为证据生成引擎，能够驱动跨层次的规模化学习，实现监管框架的适应性演进。

Abstract: The EU AI Act adopts a horizontal and adaptive approach to govern AI technologies characterised by rapid development and unpredictable emerging capabilities. To maintain relevance, the Act embeds provisions for regulatory learning. However, these provisions operate within a complex network of actors and mechanisms that lack a clearly defined technical basis for scalable information flow. This paper addresses this gap by establishing a theoretical model of regulatory learning space defined by the AI Act, decomposed into micro, meso, and macro levels. Drawing from this functional perspective of this model, we situate the diverse stakeholders - ranging from the EU Commission at the macro level to AI developers at the micro level - within the transitions of enforcement (macro-micro) and evidence aggregation (micro-macro). We identify AI Technical Sandboxes as the essential engine for evidence generation at the micro level, providing the necessary data to drive scalable learning across all levels of the model. By providing an extensive discussion of the requirements and challenges for AITSes to serve as this micro-level evidence generator, we aim to bridge the gap between legislative commands and technical operationalisation, thereby enabling a structured discourse between technical and legal experts.

</details>


### [42] [From Abstract Threats to Institutional Realities: A Comparative Semantic Network Analysis of AI Securitisation in the US, EU, and China](https://arxiv.org/abs/2601.04107)
*Ruiyi Guo,Bodong Zhang*

Main category: cs.CY

TL;DR: AI治理存在悖论：主要司法管辖区在安全、风险、问责等概念上言辞趋同，但监管框架却根本分歧且互不理解。这种碎片化源于AI如何通过不同的制度逻辑被构建为治理对象，导致不同司法管辖区在相同词汇下治理着本体论上不同的对象。


<details>
  <summary>Details</summary>
Motivation: 解释为什么主要司法管辖区（欧盟、美国、中国）在AI治理上言辞趋同但实际监管框架却存在根本分歧。挑战传统观点，认为这种分歧不能仅用地缘政治竞争、制度复杂性或工具选择来解释。

Method: 整合安全化理论和"dispositif"概念，使用语义网络分析对欧盟、美国、中国（2023-2025年）的官方政策文本进行分析，追踪"安全"等概念如何嵌入不同的语义架构中。

Result: 发现欧盟通过法律-官僚逻辑将AI司法化为可认证的产品；美国通过市场-自由逻辑将AI操作化为可优化的系统；中国通过整体国家逻辑将AI治理为社会技术基础设施。引入了"结构性不可通约性"概念来描述这种本体论分歧被术语趋同所掩盖的状况。

Conclusion: 这种重构挑战了基于原则的伦理方法对全球AI治理的适用性，表明协调失败不是源于价值观分歧，而是源于缺乏共享的参考对象。不同司法管辖区实际上在治理着不同的"AI"对象。

Abstract: Artificial intelligence governance exhibits a striking paradox: while major jurisdictions converge rhetorically around concepts such as safety, risk, and accountability, their regulatory frameworks remain fundamentally divergent and mutually unintelligible. This paper argues that this fragmentation cannot be explained solely by geopolitical rivalry, institutional complexity, or instrument selection. Instead, it stems from how AI is constituted as an object of governance through distinct institutional logics. Integrating securitisation theory with the concept of the dispositif, we demonstrate that jurisdictions govern ontologically different objects under the same vocabulary. Using semantic network analysis of official policy texts from the European Union, the United States, and China (2023-2025), we trace how concepts like safety are embedded within divergent semantic architectures. Our findings reveal that the EU juridifies AI as a certifiable product through legal-bureaucratic logic; the US operationalises AI as an optimisable system through market-liberal logic; and China governs AI as socio-technical infrastructure through holistic state logic. We introduce the concept of structural incommensurability to describe this condition of ontological divergence masked by terminological convergence. This reframing challenges ethics-by-principles approaches to global AI governance, suggesting that coordination failures arise not from disagreement over values but from the absence of a shared reference object.

</details>


### [43] [Legal Alignment for Safe and Ethical AI](https://arxiv.org/abs/2601.04175)
*Noam Kolt,Nicholas Caputo,Jack Boeglin,Cullen O'Keefe,Rishi Bommasani,Stephen Casper,Mariano-Florentino Cuéllar,Noah Feldman,Iason Gabriel,Gillian K. Hadfield,Lewis Hammond,Peter Henderson,Atoosa Kasirzadeh,Seth Lazar,Anka Reuel,Kevin L. Wei,Jonathan Zittrain*

Main category: cs.CY

TL;DR: 论文提出"法律对齐"新领域，探讨如何利用法律规则、原则和方法解决AI对齐问题，确保AI系统安全、符合伦理地运行。


<details>
  <summary>Details</summary>
Motivation: 当前AI对齐研究忽视了法律这一重要知识来源和实践经验，需要填补这一空白，利用法律资源解决AI系统的规范制定和技术实现问题。

Method: 提出法律对齐的三个研究方向：1) 设计AI系统遵守合法机构制定的法律规则内容；2) 采用法律解释方法指导AI系统的推理和决策；3) 利用法律概念作为应对AI系统可靠性、信任和合作挑战的结构蓝图。

Result: 建立了法律对齐的研究框架，提出了新的概念性、经验性和制度性问题，包括确定AI系统应遵循的具体法律、创建评估法律合规性的方法、开发支持实践实施的治理框架。

Conclusion: 法律对齐为跨学科合作提供了新机遇，需要法律、计算机科学等领域的专家共同协作，设计更安全、更符合伦理的AI系统。

Abstract: Alignment of artificial intelligence (AI) encompasses the normative problem of specifying how AI systems should act and the technical problem of ensuring AI systems comply with those specifications. To date, AI alignment has generally overlooked an important source of knowledge and practice for grappling with these problems: law. In this paper, we aim to fill this gap by exploring how legal rules, principles, and methods can be leveraged to address problems of alignment and inform the design of AI systems that operate safely and ethically. This emerging field -- legal alignment -- focuses on three research directions: (1) designing AI systems to comply with the content of legal rules developed through legitimate institutions and processes, (2) adapting methods from legal interpretation to guide how AI systems reason and make decisions, and (3) harnessing legal concepts as a structural blueprint for confronting challenges of reliability, trust, and cooperation in AI systems. These research directions present new conceptual, empirical, and institutional questions, which include examining the specific set of laws that particular AI systems should follow, creating evaluations to assess their legal compliance in real-world settings, and developing governance frameworks to support the implementation of legal alignment in practice. Tackling these questions requires expertise across law, computer science, and other disciplines, offering these communities the opportunity to collaborate in designing AI for the better.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [44] [Minimax regret treatment rules with finite samples when a quantile is the object of interest](https://arxiv.org/abs/2601.03428)
*Patrik Guggenberger,Nihal Mehta,Nikita Pavlov*

Main category: econ.EM

TL;DR: 论文研究了在有限样本下基于特定分位数（而非期望效用）的极小化极大后悔治疗规则，发现在不同抽样方案下，当已知未治疗群体的分位数等于1/2时，任何治疗规则都是最优的；分位数大于1/2时从不治疗最优，小于1/2时总是治疗最优。


<details>
  <summary>Details</summary>
Motivation: 现有研究（如Stoye 2009）主要关注期望效用的最大化，但实际决策中决策者可能更关心结果分布的特定分位数（如中位数或下分位数）。本文旨在研究基于分位数目标的有限样本极小化极大后悔治疗规则，填补这一研究空白。

Method: 采用极小化极大后悔框架，分析不同抽样方案下的最优治疗规则：1）固定数量的未治疗和治疗单位样本；2）随机治疗分配样本；3）包含协变量的情况。假设结果限制在单位区间内，关注特定分位数而非期望效用。

Result: 1）在固定数量未治疗和治疗单位的样本中，任何治疗规则都是极小化极大后悔最优的；2）在随机治疗分配样本中，当已知未治疗群体的分位数等于1/2时，任何规则都最优；3）当已知分位数大于1/2时，从不治疗是唯一最优规则；小于1/2时，总是治疗最优；4）包含协变量的情况也进行了分析。

Conclusion: 基于分位数的决策与基于期望效用的决策存在本质差异。当已知未治疗群体的分位数恰好为1/2时，决策者面临"无差异"情况，任何治疗规则都同样好。这一发现对实际政策制定有重要启示，特别是在风险规避或关注分布尾部的决策场景中。

Abstract: Consider a setup in which a decision maker is informed about the population by a finite sample and based on that sample has to decide whether or not to apply a certain treatment. We work out finite sample minimax regret treatment rules under various sampling schemes when outcomes are restricted onto the unit interval. In contrast to Stoye (2009) where the focus is on maximization of expected utility the focus here is instead on a particular quantile of the outcome distribution. We find that in the case where the sample consists of a fixed number of untreated and a fixed number of treated units, any treatment rule is minimax regret optimal. The same is true in the case of random treatment assignment in the sample with any assignment probability and in the case of testing an innovation when the known quantile of the untreated population equals 1/2. However if the known quantile exceeds 1/2 then never treating is the unique optimal rule and if it is smaller than 1/2 always treating is optimal. We also consider the case where a covariate is included.

</details>


### [45] [Content vs. Form: What Drives the Writing Score Gap Across Socioeconomic Backgrounds? A Generated Panel Approach](https://arxiv.org/abs/2601.03469)
*Nadav Kunievsky,Pedro Pertusi*

Main category: econ.EM

TL;DR: 使用大型语言模型生成同一内容的不同风格变体，分解写作评分中的社会经济地位差距，发现69%源于内容质量差异，26%源于表达风格差异，5%源于评分标准差异


<details>
  <summary>Details</summary>
Motivation: 社会经济地位（SES）差距在测试成绩中持续存在，可能影响教育和就业结果。写作评估中，分数同时反映内容质量和表达方式，需要区分这两者对SES差距的贡献程度

Method: 使用美国中学生说服性议论文语料库，利用大型语言模型为每篇论文生成多个风格变体，保持论点内容不变但改变表面表达，创建"生成面板"以引入受控的风格变化

Result: 发现SES差距为0.67分（1-6分制），其中69%源于论文内容质量差异，26%源于风格差异，5%源于不同SES群体间的评分标准差异。这些模式在不同人口亚群和写作任务中保持稳定

Conclusion: 大型语言模型可用于在观察数据中生成受控变化，帮助研究者分离和量化原本纠缠的因素。写作评估中的SES差距主要源于内容质量差异而非表达风格

Abstract: Students from different socioeconomic backgrounds exhibit persistent gaps in test scores, gaps that can translate into unequal educational and labor-market outcomes later in life. In many assessments, performance reflects not only what students know, but also how effectively they can communicate that knowledge. This distinction is especially salient in writing assessments, where scores jointly reward the substance of students' ideas and the way those ideas are expressed. As a result, observed score gaps may conflate differences in underlying content with differences in expressive skill. A central question, therefore, is how much of the socioeconomic-status (SES) gap in scores is driven by differences in what students say versus how they say it. We study this question using a large corpus of persuasive essays written by U.S. middle- and high-school students. We introduce a new measurement strategy that separates content from style by leveraging large language models to generate multiple stylistic variants of each essay. These rewrites preserve the underlying arguments while systematically altering surface expression, creating a "generated panel" that introduces controlled within-essay variation in style. This approach allows us to decompose SES gaps in writing scores into contributions from content and style. We find an SES gap of 0.67 points on a 1-6 scale. Approximately 69% of the gap is attributable to differences in essay content quality, Style differences account for 26% of the gap, and differences in evaluation standards across SES groups account for the remaining 5%. These patterns seems stable across demographic subgroups and writing tasks. More broadly, our approach shows how large language models can be used to generate controlled variation in observational data, enabling researchers to isolate and quantify the contributions of otherwise entangled factors.

</details>


### [46] [Uncovering Sparse Financial Networks with Information Criteria](https://arxiv.org/abs/2601.03598)
*Fu Ouyang,Thomas T. Yang,Wenying Yao*

Main category: econ.EM

TL;DR: 提出基于信息准则的稀疏金融网络识别方法，通过FEVD回归重构和模型选择框架，有效识别系统性风险传播通道


<details>
  <summary>Details</summary>
Motivation: 传统基于预测误差方差分解的金融关联性度量通常产生密集网络结构，这掩盖了真实的传播通道，使系统性风险识别变得复杂

Method: 将FEVD关联性重构为回归问题，建立模型选择框架以一致恢复活跃的溢出通道；扩展到广义FEVD以处理相关冲击；提出基于伪样本外预测性能的数据驱动惩罚参数选择方法

Result: 蒙特卡洛模拟显示该方法在有限样本下有效，对近似稀疏网络和厚尾误差具有鲁棒性；应用于全球股市、标普500行业指数和大宗商品期货，证实了稀疏网络在实证中的普遍性

Conclusion: 提出的信息准则方法能够有效识别稀疏的、经济意义明确的金融网络，为系统性风险分析提供了更清晰的传播通道识别工具

Abstract: Empirical measures of financial connectedness based on Forecast Error Variance Decompositions (FEVDs) often yield dense network structures that obscure true transmission channels and complicate the identification of systemic risk. This paper proposes a novel information-criterion-based approach to uncover sparse, economically meaningful financial networks. By reformulating FEVD-based connectedness as a regression problem, we develop a model selection framework that consistently recovers the active set of spillover channels. We extend this method to generalized FEVDs to accommodate correlated shocks and introduce a data-driven procedure for tuning the penalty parameter using pseudo-out-of-sample forecast performance. Monte Carlo simulations demonstrate the approach's effectiveness with finite samples and its robustness to approximately sparse networks and heavy-tailed errors. Applications to global stock markets, S&P 500 sectoral indices, and commodity futures highlight the prevalence of sparse networks in empirical settings.

</details>


### [47] [Multivariate kernel regression in vector and product metric spaces](https://arxiv.org/abs/2601.03750)
*Marcia Schafgans,Victoria Zinde-Walsh*

Main category: econ.EM

TL;DR: 论文推导了非参数核回归估计量的极限性质，无需假设回归变量密度存在，适用于具有质量点、因子结构、多重共线性和分形分布的情况，并证明了奇异性能带来更快的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 传统非参数核回归估计通常要求回归变量具有密度函数，这限制了在现实数据中的应用，因为实际数据常包含质量点、因子结构、多重共线性或分形分布等特征。本文旨在放宽这一限制，建立更一般条件下的极限性质。

Method: 使用Nadaraya-Watson核回归估计量，在回归变量分布允许质量点、因子结构、多重共线性和分形分布的条件下，推导估计量的极限性质。将函数回归的渐近正态性扩展到任意多个度量空间乘积上的多元回归。

Result: 建立了回归变量在ℝ^q中分布允许奇异情况下的收敛速率和渐近正态性；证明了维度降低型奇异性（如分形分布或因子结构）能带来更快的收敛速度；将函数回归的渐近正态性扩展到多元情况；实证研究验证了奇异性能改善收敛速率。

Conclusion: 本文放宽了非参数核回归对回归变量密度的要求，建立了更一般条件下的极限理论，证明了奇异性能加速收敛，扩展了函数回归的渐近正态性，并通过实证研究展示了NW估计量在LaLonde数据中的适用性和优势。

Abstract: This paper derives limit properties of nonparametric kernel regression estimators without requiring existence of density for regressors in $\mathbb{R}^{q}.$ In functional regression limit properties are established for multivariate functional regression. The rate and asymptotic normality for the Nadaraya-Watson (NW) estimator is established for distributions of regressors in $\mathbb{R}^{q}$ that allow for mass points, factor structure, multicollinearity and nonlinear dependence, as well as fractal distribution; when bounded density exists we provide statistical guarantees for the standard rate and the asymptotic normality without requiring smoothness. We demonstrate faster convergence associated with dimension reducing types of singularity, such as a fractal distribution or a factor structure in the regressors. The paper extends asymptotic normality of kernel functional regression to multivariate regression over a product of any number of metric spaces. Finite sample evidence confirms rate improvement due to singularity in regression over $\mathbb{R}^{q}.$ For functional regression the simulations underline the importance of accounting for multiple functional regressors. We demonstrate the applicability and advantages of the NW estimator in our empirical study, which reexamines the job training program evaluation based on the LaLonde data.

</details>


### [48] [Reverse Stress Testing Geopolitical Risk in Corporate Credit Portfolios: A Formal and Operational Framework](https://arxiv.org/abs/2601.03983)
*Christophe Hurlin,Quentin Lajaunie,Yoann Pull*

Main category: econ.EM

TL;DR: 提出一个用于企业信贷组合地缘政治风险反向压力测试的正式框架，通过最大似然方法找到最可能违反资本充足率约束的地缘政治风险情景。


<details>
  <summary>Details</summary>
Motivation: 传统压力测试通常从预设情景出发，而反向压力测试需要找到导致特定不利结果的最可能情景。本文旨在建立一个正式框架，专门针对地缘政治风险对企业信贷组合的影响进行反向压力测试。

Method: 构建包含显式地缘政治风险因子的联合宏观金融情景向量，映射到违约概率和违约损失率的压力值。通过潜在因子结构将压力传播到组合尾部损失，并转化为压力下的CET1比率。将反向压力测试表述为情景空间上的约束最大似然问题，找到最可能违反资本充足率约束的情景（设计点）。

Result: 框架能够识别地缘政治风险反向压力测试的设计点，并进一步描述反向压力情景的邻域和近似最优集，支持敏感性分析和治理导向的解释。该方法与内部评级模型兼容，可在敞口或行业层面实施。

Conclusion: 该框架为企业信贷组合的地缘政治风险反向压力测试提供了系统化方法，能够识别最可能违反资本约束的情景，支持风险管理和监管合规决策。

Abstract: This paper proposes a formal framework for reverse stress testing geopolitical risk in corporate credit portfolios. A joint macro-financial scenario vector, augmented with an explicit geopolitical risk factor, is mapped into stressed probabilities of default and losses given default. These stresses are then propagated to portfolio tail losses through a latent factor structure and translated into a stressed CET1 ratio, jointly accounting for capital depletion and risk-weighted asset dynamics. Reverse stress testing is formulated as a constrained maximum likelihood problem over the scenario space. This yields a geopolitical point reverse stress test, or design point, defined as the most probable scenario that breaches a prescribed capital adequacy constraint under a reference distribution. The framework further characterises neighbourhoods and near optimal sets of reverse stress scenarios, allowing for sensitivity analysis and governance oriented interpretation. The approach is compatible with internal rating based models and supports implementation at the exposure or sector level.

</details>


### [49] [Mean Square Errors of factors extracted using principal components, linear projections, and Kalman filter](https://arxiv.org/abs/2601.04087)
*Matteo Barigozzi,Diego Fresoli,Esther Ruiz*

Main category: econ.EM

TL;DR: 比较主成分分析(PC)和卡尔曼滤波(KF)两种因子提取方法在有限样本下的均方误差(MSE)，发现KF方法通常比PC方法具有更小的MSE，特别是在考虑因子随机性时。


<details>
  <summary>Details</summary>
Motivation: 当从高维变量系统中提取因子时，需要评估提取因子的不确定性，特别是当这些因子具有直接解释意义或用于汇总大量预测变量信息时。现有研究缺乏对PC和KF方法在有限样本下MSE性能的系统比较。

Method: 通过理论分析和模拟研究，比较PC和KF因子在不同异质性横截面相关结构下的有限样本MSE。考虑因子作为确定性变量(PC方法)和随机变量(KF方法)的不同处理方式，并分析当错误假设异质性成分具有同方差和/或不相关时的MSE表现。

Result: PC方法基于确定性因子假设的MSE大于KF方法基于随机因子假设的MSE。当错误假设异质性成分具有同方差或不相关时，两种方法的MSE表现也不同。模拟数据验证了这些结果对构建因子置信区间的重要性。

Conclusion: 在处理高维变量系统的因子提取时，KF方法通常比PC方法具有更好的MSE性能，特别是在考虑因子随机性时。研究结果为构建因子置信区间提供了理论依据，对实际应用中因子提取方法的选择具有指导意义。

Abstract: Factor extraction from systems of variables with a large cross-sectional dimension, $N$, is often based on either Principal Components (PC)-based procedures, or Kalman filter (KF)-based procedures. Measuring the uncertainty of the extracted factors is important when, for example, they have a direct interpretation and/or they are used to summarized the information in a large number of potential predictors. In this paper, we compare the finite $N$ mean square errors (MSEs) of PC and KF factors extracted under different structures of the idiosyncratic cross-correlations. We show that the MSEs of PC-based factors, implicitly based on treating the true underlying factors as deterministic, are larger than the corresponding MSEs of KF factors, obtained by treating the true factors as either serially independent or autocorrelated random variables. We also study and compare the MSEs of PC and KF factors estimated when the idiosyncratic components are wrongly considered as if they were cross-sectionally homoscedastic and/or uncorrelated. The relevance of the results for the construction of confidence intervals for the factors are illustrated with simulated data.

</details>


### [50] [Ridge Estimation of High Dimensional Two-Way Fixed Effect Regression](https://arxiv.org/abs/2601.04101)
*Junnan He,Jean-Marc Robin*

Main category: econ.EM

TL;DR: 本文研究高维双向固定效应回归模型中的岭估计器，该模型具有稀疏二分网络结构。当岭参数随网络规模对数增长时，估计固定效应的偏差和方差协方差矩阵收敛到仅依赖于期望网络的确定性等价形式。


<details>
  <summary>Details</summary>
Motivation: 在具有稀疏二分网络结构的高维双向固定效应回归模型中，需要开发有效的估计方法。传统方法在处理大规模网络时面临计算和统计挑战，岭估计提供了一种正则化解决方案。

Method: 采用岭估计器处理高维双向固定效应回归模型，研究当岭参数随网络规模对数增长时的渐近性质。开发浓度不等式分析估计量的收敛行为。

Result: 当岭参数随网络规模对数增长时，固定效应向量的偏差和方差协方差矩阵收敛到仅依赖于期望网络的确定性等价形式。通过模拟和实际工资数据应用验证了理论结果。

Conclusion: 岭估计器为高维稀疏二分网络中的双向固定效应回归提供了有效的正则化方法，其渐近性质良好，偏差和方差协方差矩阵收敛到确定性等价形式，仅依赖于期望网络结构。

Abstract: We study a ridge estimator for the high-dimensional two-way fixed effect regression model with a sparse bipartite network. We develop concentration inequalities showing that when the ridge parameters increase as the log of the network size, the bias, and the variance-covariance matrix of the vector of estimated fixed effects converge to deterministic equivalents that depend only on the expected network. We provide simulations and an application using administrative data on wages for worker-firm matches.

</details>

<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 69]
- [cs.SI](#cs.SI) [Total: 15]
- [econ.EM](#econ.EM) [Total: 5]
- [stat.AP](#stat.AP) [Total: 4]
- [cs.CY](#cs.CY) [Total: 12]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [CreditAudit: 2D Auditing for LLM Evaluation and Selection](https://arxiv.org/abs/2602.02515)
*Yiliang Song,Hongjun An,Jiangong Xiao,Haofei Zhao,Jiawei Shao,Xuelong Li*

Main category: cs.AI

TL;DR: CreditAudit是一个面向部署的信用审计框架，通过评估模型在不同系统提示模板下的性能波动性，提供均值和稳定性风险信号，帮助在相似平均性能的模型中选择更稳定的部署选项。


<details>
  <summary>Details</summary>
Motivation: 当前公开基准测试的分数趋于收敛，许多前沿语言模型之间只有微小差异，但这些分数无法反映用户日常体验。系统提示、输出协议和交互模式在不断迭代变化，在代理式多步骤流程中，小的协议变化可能引发不成比例的失败，导致从业者不确定部署哪个模型。

Method: 提出CreditAudit框架：1）在多个基准测试上使用一系列语义对齐且非对抗性的系统提示模板评估模型；2）报告平均性能作为能力均值；3）报告场景诱导的波动性σ作为稳定性风险信号；4）通过跨模型分位数将波动性映射为可解释的信用等级（AAA到BBB），并包含减轻模板难度漂移的诊断。

Result: 在GPQA、TruthfulQA和MMLU Pro上的控制实验表明：具有相似平均能力的模型可能表现出显著不同的波动性，稳定性风险可以在代理式或高失败成本场景中推翻优先级决策。框架提供了2维和等级基础的语言，支持特定场景的选择。

Conclusion: CreditAudit通过提供更客观和可信的模型评估，支持分层部署和更规范的测试与监控资源分配，使模型评估更适合实际应用场景。

Abstract: Leaderboard scores on public benchmarks have been steadily rising and converging, with many frontier language models now separated by only marginal differences. However, these scores often fail to match users' day to day experience, because system prompts, output protocols, and interaction modes evolve under routine iteration, and in agentic multi step pipelines small protocol shifts can trigger disproportionate failures, leaving practitioners uncertain about which model to deploy. We propose CreditAudit, a deployment oriented credit audit framework that evaluates models under a family of semantically aligned and non adversarial system prompt templates across multiple benchmarks, reporting mean ability as average performance across scenarios and scenario induced fluctuation sigma as a stability risk signal, and further mapping volatility into interpretable credit grades from AAA to BBB via cross model quantiles with diagnostics that mitigate template difficulty drift. Controlled experiments on GPQA, TruthfulQA, and MMLU Pro show that models with similar mean ability can exhibit substantially different fluctuation, and stability risk can overturn prioritization decisions in agentic or high failure cost regimes. By providing a 2D and grade based language for regime specific selection, CreditAudit supports tiered deployment and more disciplined allocation of testing and monitoring effort, enabling more objective and trustworthy model evaluation for real world use.

</details>


### [2] [Experience-Driven Multi-Agent Systems Are Training-free Context-aware Earth Observers](https://arxiv.org/abs/2602.02559)
*Pengyu Dai,Weihao Xuan,Junjue Wang,Hongruixuan Chen,Jian Song,Yafei Ou,Naoto Yokoya*

Main category: cs.AI

TL;DR: GeoEvolver：一个自进化的多智能体系统，让LLM智能体通过结构化交互获取地球观测专业知识，无需参数更新，在复杂EO任务中提升成功率12%


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体在专业、工具密集的领域（如地球观测）表现不佳，因为它们缺乏从交互中学习细粒度工具级专业知识的机制，无法可靠配置工具参数或从执行失败中恢复

Method: GeoEvolver通过检索增强的多智能体编排器将查询分解为独立子目标，在子目标层面探索多样化的工具参数配置，将成功模式和失败根因分析提炼到进化记忆库中，为未来查询提供上下文演示

Result: 在三个工具集成的EO基准测试中，GeoEvolver持续提升端到端任务成功率，在多个LLM骨干网络上平均增益12%，证明EO专业知识可以通过与环境的高效细粒度交互逐步涌现

Conclusion: GeoEvolver展示了LLM智能体可以通过结构化交互在专业领域获取专业知识，无需参数更新，为解决复杂EO工作流中的工具协调和约束遵守问题提供了有效方案

Abstract: Recent advances have enabled large language model (LLM) agents to solve complex tasks by orchestrating external tools. However, these agents often struggle in specialized, tool-intensive domains that demand long-horizon execution, tight coordination across modalities, and strict adherence to implicit tool constraints. Earth Observation (EO) tasks exemplify this challenge due to the multi-modal and multi-temporal data inputs, as well as the requirements of geo-knowledge constraints (spectrum library, spatial reasoning, etc): many high-level plans can be derailed by subtle execution errors that propagate through a pipeline and invalidate final results. A core difficulty is that existing agents lack a mechanism to learn fine-grained, tool-level expertise from interaction. Without such expertise, they cannot reliably configure tool parameters or recover from mid-execution failures, limiting their effectiveness in complex EO workflows. To address this, we introduce \textbf{GeoEvolver}, a self-evolving multi-agent system~(MAS) that enables LLM agents to acquire EO expertise through structured interaction without any parameter updates. GeoEvolver decomposes each query into independent sub-goals via a retrieval-augmented multi-agent orchestrator, then explores diverse tool-parameter configurations at the sub-goal level. Successful patterns and root-cause attribution from failures are then distilled in an evolving memory bank that provides in-context demonstrations for future queries. Experiments on three tool-integrated EO benchmarks show that GeoEvolver consistently improves end-to-end task success, with an average gain of 12\% across multiple LLM backbones, demonstrating that EO expertise can emerge progressively from efficient, fine-grained interactions with the environment.

</details>


### [3] [Uncertainty and Fairness Awareness in LLM-Based Recommendation Systems](https://arxiv.org/abs/2602.02582)
*Chandan Kumar Sah,Xiaoli Lian,Li Zhang,Tony Xu,Syed Shazaib Shah*

Main category: cs.AI

TL;DR: 该论文研究LLM推荐系统中的不确定性和公平性问题，提出新的评估方法并发现Gemini 1.5 Flash存在系统性不公平，同时引入人格感知公平性基准。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在零样本推荐中表现出色，但其预测不确定性和内在偏见威胁着推荐系统的可靠性和公平性。需要系统评估这些因素如何影响LLM生成推荐的准确性、一致性和可信度。

Method: 1) 构建包含电影和音乐两个领域的基准数据集，标注8个人口统计属性(31个分类值)；2) 通过深入案例研究量化预测不确定性(使用熵度量)；3) 测试提示扰动(拼写错误和多语言输入)下的公平性；4) 将人格感知公平性整合到RecLLM评估流程中；5) 提出不确定性感知的评估方法。

Result: 1) Google DeepMind的Gemini 1.5 Flash对某些敏感属性表现出系统性不公平(SNSR=0.1363, SNSV=0.0507)；2) 这些差异在提示扰动下持续存在；3) 揭示了人格相关偏见模式；4) 暴露了个性化与群体公平性之间的权衡；5) 建立了更安全、可解释的RecLLM基础。

Conclusion: 该研究为更安全、可解释的RecLLM奠定了基础，提出了不确定性感知评估方法和人格档案知情公平性基准，推动了LLM推荐系统的可解释性和公平性，激励未来在多模型基准和自适应校准方面的研究。

Abstract: Large language models (LLMs) enable powerful zero-shot recommendations by leveraging broad contextual knowledge, yet predictive uncertainty and embedded biases threaten reliability and fairness. This paper studies how uncertainty and fairness evaluations affect the accuracy, consistency, and trustworthiness of LLM-generated recommendations. We introduce a benchmark of curated metrics and a dataset annotated for eight demographic attributes (31 categorical values) across two domains: movies and music. Through in-depth case studies, we quantify predictive uncertainty (via entropy) and demonstrate that Google DeepMind's Gemini 1.5 Flash exhibits systematic unfairness for certain sensitive attributes; measured similarity-based gaps are SNSR at 0.1363 and SNSV at 0.0507. These disparities persist under prompt perturbations such as typographical errors and multilingual inputs. We further integrate personality-aware fairness into the RecLLM evaluation pipeline to reveal personality-linked bias patterns and expose trade-offs between personalization and group fairness. We propose a novel uncertainty-aware evaluation methodology for RecLLMs, present empirical insights from deep uncertainty case studies, and introduce a personality profile-informed fairness benchmark that advances explainability and equity in LLM recommendations. Together, these contributions establish a foundation for safer, more interpretable RecLLMs and motivate future work on multi-model benchmarks and adaptive calibration for trustworthy deployment.

</details>


### [4] [PeerRank: Autonomous LLM Evaluation Through Web-Grounded, Bias-Controlled Peer Review](https://arxiv.org/abs/2602.02589)
*Yanki Margalit,Erni Avram,Ran Taig,Oded Margalit,Nurit Cohen-Inger*

Main category: cs.AI

TL;DR: PeerRank是一个完全自主的端到端评估框架，让模型自主生成评估任务、基于实时网络信息回答问题、评估同伴响应，无需人工监督或参考答案，实现大规模语言模型的开放世界评估。


<details>
  <summary>Details</summary>
Motivation: 传统的大语言模型评估方法依赖人工编写的基准测试、参考答案和人工或单一模型判断，这些方法扩展性差、容易过时，且与依赖网络检索和合成的开放世界部署不匹配。

Method: PeerRank采用多智能体过程，每个模型对称地作为任务设计者、响应者和评估者参与。模型自主生成评估任务，使用类别范围的实时网络信息回答问题，评估同伴响应，并将密集的同伴评估聚合成相对性能估计。

Result: 在12个商业可用模型和420个自主生成问题的研究中，PeerRank产生了稳定、可区分的排名，并揭示了可测量的身份和呈现偏见。排名具有鲁棒性，平均同伴得分与Elo评分一致。在TruthfulQA和GSM8K上的验证显示同伴得分与客观准确性相关。

Conclusion: 具有选择性网络信息基础的偏见感知同伴评估可以扩展开放世界大语言模型评估，超越静态和人工策划的基准测试，为自主、可扩展的模型评估提供了新途径。

Abstract: Evaluating large language models typically relies on human-authored benchmarks, reference answers, and human or single-model judgments, approaches that scale poorly, become quickly outdated, and mismatch open-world deployments that depend on web retrieval and synthesis. We introduce PeerRank, a fully autonomous end-to-end evaluation framework in which models generate evaluation tasks, answer them with category-scoped live web grounding, judge peer responses and aggregate dense peer assessments into relative performance estimates, without human supervision or gold references. PeerRank treats evaluation as a multi-agent process where each model participates symmetrically as task designer, respondent, and evaluator, while removing biased judgments. In a large-scale study over 12 commercially available models and 420 autonomously generated questions, PeerRank produces stable, discriminative rankings and reveals measurable identity and presentation biases. Rankings are robust, and mean peer scores agree with Elo. We further validate PeerRank on TruthfulQA and GSM8K, where peer scores correlate with objective accuracy. Together, these results suggest that bias-aware peer evaluation with selective web-grounded answering can scale open-world LLM assessment beyond static and human curated benchmarks.

</details>


### [5] [A Positive Case for Faithfulness: LLM Self-Explanations Help Predict Model Behavior](https://arxiv.org/abs/2602.02639)
*Harry Mayne,Justin Singh Kang,Dewi Gould,Kannan Ramchandran,Adam Mahdi,Noah Y. Siegel*

Main category: cs.AI

TL;DR: 提出NSG指标评估LLM自解释的忠实度，发现自解释能显著提升模型行为预测能力（11-37%增益），但仍有5-15%的自解释严重误导。


<details>
  <summary>Details</summary>
Motivation: 当前LLM自解释的忠实度评估方法存在局限，主要依赖对抗性提示或检测推理错误，忽视了解释的预测价值。需要一种更全面、可扩展的忠实度评估指标。

Method: 提出归一化可模拟增益（NSG）指标，基于"忠实解释应能让观察者学习模型的决策标准，从而更好预测相关输入上的行为"这一理念。在7,000个反事实样本上评估18个前沿专有和开源模型。

Result: 自解释显著提升模型行为预测能力（11-37% NSG增益）。自解释比外部模型生成的解释提供更多预测信息，即使外部模型更强。同时发现5-15%的自解释严重误导。

Conclusion: 尽管自解释存在缺陷（部分严重误导），但它们确实编码了有助于预测模型行为的信息，展现了自解释的积极价值，且自知识优势是外部解释方法无法复制的。

Abstract: LLM self-explanations are often presented as a promising tool for AI oversight, yet their faithfulness to the model's true reasoning process is poorly understood. Existing faithfulness metrics have critical limitations, typically relying on identifying unfaithfulness via adversarial prompting or detecting reasoning errors. These methods overlook the predictive value of explanations. We introduce Normalized Simulatability Gain (NSG), a general and scalable metric based on the idea that a faithful explanation should allow an observer to learn a model's decision-making criteria, and thus better predict its behavior on related inputs. We evaluate 18 frontier proprietary and open-weight models, e.g., Gemini 3, GPT-5.2, and Claude 4.5, on 7,000 counterfactuals from popular datasets covering health, business, and ethics. We find self-explanations substantially improve prediction of model behavior (11-37% NSG). Self-explanations also provide more predictive information than explanations generated by external models, even when those models are stronger. This implies an advantage from self-knowledge that external explanation methods cannot replicate. Our approach also reveals that, across models, 5-15% of self-explanations are egregiously misleading. Despite their imperfections, we show a positive case for self-explanations: they encode information that helps predict model behavior.

</details>


### [6] [MARS: Modular Agent with Reflective Search for Automated AI Research](https://arxiv.org/abs/2602.02660)
*Jiefeng Chen,Bhavana Dalvi Mishra,Jaehyun Nam,Rui Meng,Tomas Pfister,Jinsung Yoon*

Main category: cs.AI

TL;DR: MARS是一个专为自主AI研究设计的模块化代理框架，通过预算感知规划、模块化构建和比较反思记忆三大支柱，解决了AI研究中计算成本高和性能归因不透明的问题。


<details>
  <summary>Details</summary>
Motivation: AI研究与一般软件工程不同，涉及计算密集型评估（如模型训练）和不透明的性能归因。当前基于LLM的代理在处理这些问题时表现不佳，通常生成忽略执行成本和因果因素的单体脚本。

Method: MARS框架基于三大支柱：1）预算感知规划，使用成本约束的蒙特卡洛树搜索（MCTS）平衡性能与执行成本；2）模块化构建，采用"设计-分解-实现"管道管理复杂研究仓库；3）比较反思记忆，通过分析解决方案差异来提取高信号洞察，解决信用分配问题。

Result: MARS在MLE-Bench上实现了与开源框架相比最先进的性能，在可比设置下保持竞争力，与全球排行榜的顶级方法相当。系统还表现出定性的"顿悟"时刻，63%的已使用经验来自跨分支转移，表明代理能有效跨搜索路径泛化洞察。

Conclusion: MARS框架通过预算感知规划、模块化构建和比较反思记忆，为自主AI研究提供了一个有效的解决方案，能够处理计算成本高和性能归因不透明的问题，并在实际基准测试中表现出色。

Abstract: Automating AI research differs from general software engineering due to computationally expensive evaluation (e.g., model training) and opaque performance attribution. Current LLM-based agents struggle here, often generating monolithic scripts that ignore execution costs and causal factors. We introduce MARS (Modular Agent with Reflective Search), a framework optimized for autonomous AI research. MARS relies on three pillars: (1) Budget-Aware Planning via cost-constrained Monte Carlo Tree Search (MCTS) to explicitly balance performance with execution expense; (2) Modular Construction, employing a "Design-Decompose-Implement" pipeline to manage complex research repositories; and (3) Comparative Reflective Memory, which addresses credit assignment by analyzing solution differences to distill high-signal insights. MARS achieves state-of-the-art performance among open-source frameworks on MLE-Bench under comparable settings, maintaining competitiveness with the global leaderboard's top methods. Furthermore, the system exhibits qualitative "Aha!" moments, where 63% of all utilized lessons originate from cross-branch transfer, demonstrating that the agent effectively generalizes insights across search paths.

</details>


### [7] [ATLAS : Adaptive Self-Evolutionary Research Agent with Task-Distributed Multi-LLM Supporters](https://arxiv.org/abs/2602.02709)
*Ujin Jeon,Jiyong Kwon,Madison Ann Sullivan,Caleb Eunho Lee,Guang Lin*

Main category: cs.AI

TL;DR: ATLAS是一个任务分布式框架，通过专门的辅助代理进行探索、超参数调整和参考策略管理，迭代开发轻量级研究代理，在非平稳环境中提升稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有多LLM代理系统在提示优化和自动问题解决方面表现良好，但要么在微调后保持求解器固定，要么依赖静态偏好优化循环，这在长视野任务中变得难以处理。

Method: 提出ATLAS框架，采用任务分布式方法，迭代开发轻量级研究代理，同时将探索、超参数调整和参考策略管理等互补角色委托给专门的辅助代理。核心算法EvoDPO自适应更新阶段索引的参考策略。

Result: 在非平稳线性上下文多臂老虎机和科学机器学习（SciML）损失重加权（1D Burgers方程）上的实验表明，ATLAS相比静态单代理基线提高了稳定性和性能。

Conclusion: ATLAS通过任务分布式框架和自适应参考策略更新，有效解决了长视野任务中静态代理系统的局限性，在非平稳环境中实现了更好的稳定性和性能。

Abstract: Recent multi-LLM agent systems perform well in prompt optimization and automated problem-solving, but many either keep the solver frozen after fine-tuning or rely on a static preference-optimization loop, which becomes intractable for long-horizon tasks. We propose ATLAS (Adaptive Task-distributed Learning for Agentic Self-evolution), a task-distributed framework that iteratively develops a lightweight research agent while delegating complementary roles to specialized supporter agents for exploration, hyperparameter tuning, and reference policy management. Our core algorithm, Evolving Direct Preference Optimization (EvoDPO), adaptively updates the phase-indexed reference policy. We provide a theoretical regret analysis for a preference-based contextual bandit under concept drift. In addition, experiments were conducted on non-stationary linear contextual bandits and scientific machine learning (SciML) loss reweighting for the 1D Burgers' equation. Both results show that ATLAS improves stability and performance over a static single-agent baseline.

</details>


### [8] [Dynamic Mix Precision Routing for Efficient Multi-step LLM Interaction](https://arxiv.org/abs/2602.02711)
*Yuanzhe Li,Jianing Deng,Jingtong Hu,Tianlong Chen,Song Wang,Huanrui Yang*

Main category: cs.AI

TL;DR: 提出动态混合精度路由框架，在长时程决策任务中自适应选择高精度和低精度LLM，以平衡任务成功率和推理成本。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在长时程决策任务中表现良好，但使用大型LLM进行多步交互会产生高昂的推理成本。传统观点认为更高的任务成功率需要使用更大更强的LLM，但作者探索使用低精度量化LLM来降低成本。

Method: 提出动态混合精度路由框架，基于观察到不同交互步骤对精度的敏感性不同，自适应地在每个决策步骤选择高精度或低精度LLM。路由器通过两阶段流水线训练：1) 基于KL散度的监督学习识别精度敏感步骤；2) 使用组相对策略优化(GRPO)进一步提高任务成功率。

Result: 在ALFWorld上的实验表明，该方法在准确率-成本权衡方面相比单精度基线和启发式路由方法有显著提升。

Conclusion: 通过动态混合精度路由框架，可以在保持任务成功率的同时显著降低长时程决策任务的推理成本，为LLM在资源受限环境中的应用提供了有效解决方案。

Abstract: Large language models (LLM) achieve strong performance in long-horizon decision-making tasks through multi-step interaction and reasoning at test time. While practitioners commonly believe a higher task success rate necessitates the use of a larger and stronger LLM model, multi-step interaction with a large LLM incurs prohibitive inference cost. To address this problem, we explore the use of low-precision quantized LLM in the long-horizon decision-making process. Based on the observation of diverse sensitivities among interaction steps, we propose a dynamic mix-precision routing framework that adaptively selects between high-precision and low-precision LLMs at each decision step. The router is trained via a two-stage pipeline, consisting of KL-divergence-based supervised learning that identifies precision-sensitive steps, followed by Group-Relative Policy Optimization (GRPO) to further improve task success rates. Experiments on ALFWorld demonstrate that our approach achieves a great improvement on accuracy-cost trade-off over single-precision baselines and heuristic routing methods.

</details>


### [9] [Scaling-Aware Adapter for Structure-Grounded LLM Reasoning](https://arxiv.org/abs/2602.02780)
*Zihao Jing,Qiuhao Zeng,Ruiyi Fang,Yan Yi Li,Yan Sun,Boyu Wang,Pingzhao Hu*

Main category: cs.AI

TL;DR: Cuttlefish是一个统一的全原子LLM，通过自适应缩放结构token和几何接地适配器，在几何线索上实现语言推理，减少结构幻觉，在异构结构推理中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么通过序列化tokenization压缩结构输入，要么使用固定长度连接器，导致几何基础缺失或模态融合瓶颈，阻碍了通用全原子推理的实现。

Method: 1. 缩放感知分块：使用指令条件门控机制在结构图上生成可变大小的分块，根据结构复杂度自适应缩放查询token预算；2. 几何接地适配器：通过跨注意力到模态嵌入来细化这些自适应token，并将生成的模态token注入LLM，提供显式几何线索。

Result: 在多样化的全原子基准测试中，Cuttlefish在异构结构接地推理方面实现了卓越性能。

Conclusion: Cuttlefish通过自适应缩放结构token和几何接地机制，成功解决了现有方法在结构压缩和几何基础方面的局限性，实现了更好的全原子推理能力。

Abstract: Large language models (LLMs) are enabling reasoning over biomolecular structures, yet existing methods remain modality-specific and typically compress structural inputs through sequence-based tokenization or fixed-length query connectors. Such architectures either omit the geometric groundings requisite for mitigating structural hallucinations or impose inflexible modality fusion bottlenecks that concurrently over-compress and suboptimally allocate structural tokens, thereby impeding the realization of generalized all-atom reasoning. We introduce Cuttlefish, a unified all-atom LLM that grounds language reasoning in geometric cues while scaling modality tokens with structural complexity. First, Scaling-Aware Patching leverages an instruction-conditioned gating mechanism to generate variable-size patches over structural graphs, adaptively scaling the query token budget with structural complexity to mitigate fixed-length connector bottlenecks. Second, Geometry Grounding Adapter refines these adaptive tokens via cross-attention to modality embeddings and injects the resulting modality tokens into the LLM, exposing explicit geometric cues to reduce structural hallucination. Experiments across diverse all-atom benchmarks demonstrate that Cuttlefish achieves superior performance in heterogeneous structure-grounded reasoning. Code is available at the project repository.

</details>


### [10] [Chain of Simulation: A Dual-Mode Reasoning Framework for Large Language Models with Dynamic Problem Routing](https://arxiv.org/abs/2602.02842)
*Saeid Sheikhi*

Main category: cs.AI

TL;DR: Chain of Simulation (CoS) 是一个双模式推理框架，通过动态路由问题到专门的推理策略（计算、符号、混合模式），在多个基准测试中显著提升LLM推理性能，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 现有统一提示方法无法有效处理不同类型的推理问题，需要一种能够根据问题类型动态选择专门推理策略的框架，以提高大型语言模型在各种推理任务上的性能。

Method: 提出Chain of Simulation (CoS)框架，包含三种推理模式：1) 数学问题的计算流与自洽性检查；2) 空间推理的符号状态跟踪与JSON表示；3) 多跳推理的混合事实提取。提供模式选择、状态跟踪和答案提取的详细算法。

Result: 在GSM8K、StrategyQA和bAbI基准测试中，使用四种先进模型（Gemma-3 27B、LLaMA-3.1 8B、Mistral 7B、Qwen-2.5 14B）评估，CoS相比最强基线：GSM8K准确率71.5%（绝对提升1.0%）、StrategyQA 90.0%（提升2.5%）、bAbI 19.0%（相对提升65.2%）。计算模式正确应用于数学问题时准确率达81.2%，而错误路由导致0%准确率。

Conclusion: CoS通过问题特定的模式选择显著提升LLM推理能力，无需额外训练，在准确性和效率之间提供优越权衡，相比Self-Consistency以54%更低的计算成本实现可比性能。

Abstract: We present Chain of Simulation (CoS), a novel dual-mode reasoning framework that dynamically routes problems to specialized reasoning strategies in Large Language Models (LLMs). Unlike existing uniform prompting approaches, CoS employs three distinct reasoning modes: (1) computational flow with self-consistency for mathematical problems, (2) symbolic state tracking with JSON representations for spatial reasoning, and (3) hybrid fact-extraction for multi-hop inference. Through comprehensive evaluation on GSM8K, StrategyQA, and bAbI benchmarks using four state-of-the-art models (Gemma-3 27B, LLaMA-3.1 8B, Mistral 7B, and Qwen-2.5 14B), we demonstrate that CoS achieves 71.5% accuracy on GSM8K (1.0% absolute improvement), 90.0% on StrategyQA (2.5% improvement), and 19.0% on bAbI (65.2% relative improvement) compared to the strongest baselines. The analysis reveals that problem-specific mode selection is crucial, with computational mode achieving 81.2% accuracy when correctly applied to mathematical problems, while misrouting leads to 0% accuracy. We provide detailed algorithms for mode selection, state tracking, and answer extraction, establishing CoS as an effective approach for improving LLM reasoning without additional training. The framework provides superior trade-offs between accuracy and efficiency compared to Self-Consistency, achieving comparable performance at 54% lower computational cost.

</details>


### [11] [AutoSizer: Automatic Sizing of Analog and Mixed-Signal Circuits via Large Language Model (LLM) Agents](https://arxiv.org/abs/2602.02849)
*Xi Yu,Dmitrii Torbunov,Soumyajit Mandal,Yihui Ren*

Main category: cs.AI

TL;DR: AutoSizer是一个基于大语言模型的反射式元优化框架，用于模拟混合信号电路尺寸优化，通过内外双循环结构统一电路理解、自适应搜索空间构建和优化编排，显著提升了优化质量和效率。


<details>
  <summary>Details</summary>
Motivation: 模拟混合信号集成电路设计严重依赖专家知识，晶体管尺寸优化面临非线性行为、高维设计空间和严格性能约束等挑战。现有EDA方法通常将尺寸优化视为静态黑盒优化，导致效率低下且鲁棒性差。虽然大语言模型具备强大推理能力，但不适合AMS尺寸优化的精确数值优化。

Method: 提出AutoSizer框架，采用反射式LLM驱动的元优化方法，包含内外双循环结构：内循环负责电路尺寸优化，外循环分析优化动态和约束，根据仿真反馈迭代优化搜索空间。还建立了AMS-SizingBench基准，包含24个SKY130 CMOS技术的AMS电路。

Result: AutoSizer在实验中实现了更高的解决方案质量、更快的收敛速度和更高的成功率，在不同电路复杂度下均优于传统优化方法和现有LLM智能体。

Conclusion: AutoSizer通过将LLM推理能力与自适应优化策略相结合，有效解决了AMS电路尺寸优化的关键瓶颈，为自动化电路设计提供了新的解决方案。

Abstract: The design of Analog and Mixed-Signal (AMS) integrated circuits remains heavily reliant on expert knowledge, with transistor sizing a major bottleneck due to nonlinear behavior, high-dimensional design spaces, and strict performance constraints. Existing Electronic Design Automation (EDA) methods typically frame sizing as static black-box optimization, resulting in inefficient and less robust solutions. Although Large Language Models (LLMs) exhibit strong reasoning abilities, they are not suited for precise numerical optimization in AMS sizing. To address this gap, we propose AutoSizer, a reflective LLM-driven meta-optimization framework that unifies circuit understanding, adaptive search-space construction, and optimization orchestration in a closed loop. It employs a two-loop optimization framework, with an inner loop for circuit sizing and an outer loop that analyzes optimization dynamics and constraints to iteratively refine the search space from simulation feedback. We further introduce AMS-SizingBench, an open benchmark comprising 24 diverse AMS circuits in SKY130 CMOS technology, designed to evaluate adaptive optimization policies under realistic simulator-based constraints. AutoSizer experimentally achieves higher solution quality, faster convergence, and higher success rate across varying circuit difficulties, outperforming both traditional optimization methods and existing LLM-based agents.

</details>


### [12] [STEER: Inference-Time Risk Control via Constrained Quality-Diversity Search](https://arxiv.org/abs/2602.02862)
*Eric Yang,Jong Ha Lee,Jonathan Amar,Elissa Ye,Yugang Jia*

Main category: cs.AI

TL;DR: STEER框架通过离线演化搜索构建多样化自然语言角色，在推理时通过单一可解释参数控制决策保守度，解决LLM在有序决策任务中的模式坍塌问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在平均正确性训练后常出现模式坍塌，在需要权衡特异性和敏感性的有序决策任务（如临床分诊）中，标准对齐方法移除了基于上下文约束调整ROC操作点的能力。

Method: STEER通过离线约束质量-多样性搜索构建自然语言角色群体，确保行为覆盖同时强制执行最低安全、推理和稳定性阈值。推理时通过单一可解释参数将用户指定的风险百分位数映射到选定角色，实现决策保守度的单调调整。

Result: 在两个临床分诊基准测试中，STEER相比基于温度的采样和静态角色集成实现了更广泛的行为覆盖。与代表性后训练方法相比，在明确紧急情况下保持显著更高的准确性，同时在模糊决策上提供可比较的控制能力。

Conclusion: STEER是一种保持安全性的风险控制范式，能够在不影响领域能力的情况下引导行为，为有序决策任务提供了可调控制能力。

Abstract: Large Language Models (LLMs) trained for average correctness often exhibit mode collapse, producing narrow decision behaviors on tasks where multiple responses may be reasonable. This limitation is particularly problematic in ordinal decision settings such as clinical triage, where standard alignment removes the ability to trade off specificity and sensitivity (the ROC operating point) based on contextual constraints. We propose STEER (Steerable Tuning via Evolutionary Ensemble Refinement), a training-free framework that reintroduces this tunable control. STEER constructs a population of natural-language personas through an offline, constrained quality-diversity search that promotes behavioral coverage while enforcing minimum safety, reasoning, and stability thresholds. At inference time, STEER exposes a single, interpretable control parameter that maps a user-specified risk percentile to a selected persona, yielding a monotonic adjustment of decision conservativeness. On two clinical triage benchmarks, STEER achieves broader behavioral coverage compared to temperature-based sampling and static persona ensembles. Compared to a representative post-training method, STEER maintains substantially higher accuracy on unambiguous urgent cases while providing comparable control over ambiguous decisions. These results demonstrate STEER as a safety-preserving paradigm for risk control, capable of steering behavior without compromising domain competence.

</details>


### [13] ["I May Not Have Articulated Myself Clearly": Diagnosing Dynamic Instability in LLM Reasoning at Inference Time](https://arxiv.org/abs/2602.02863)
*Jinkun Chen,Fengxiang Cheng,Sijia Han,Vlado Keselj*

Main category: cs.AI

TL;DR: LLM推理失败可通过推理时的token概率分布变化检测，无需训练或微调


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的推理失败通常只在生成结束时测量，但许多失败表现为过程级崩溃：模型在推理过程中"失去线索"。研究是否可以通过标准API中可用的推理时观测值（token对数概率）检测这种崩溃，而无需任何训练或微调。

Method: 定义了一个简单的不稳定性信号，结合连续步骤的分布变化（JSD）和不确定性（熵），通过峰值不稳定性强度总结每个推理轨迹，并展示该信号能可靠预测失败。

Result: 在GSM8K和HotpotQA数据集上，不稳定性强度能以高于随机水平的AUC预测错误答案，并在不同模型规模上显示出单调的桶级准确率下降。关键发现：早期不稳定性可能反映后续稳定化和正确答案（纠正性不稳定性），而晚期不稳定性更常导致失败（破坏性不稳定性），即使峰值幅度相似。

Conclusion: 该方法具有模型无关性、无需训练、可复现性，作为诊断视角而非纠正或控制机制。不稳定性检测不仅取决于分布变化的强度，还取决于变化发生的时机相对于剩余解码范围。

Abstract: Reasoning failures in large language models (LLMs) are typically measured only at the end of a generation, yet many failures manifest as a process-level breakdown: the model "loses the thread" mid-reasoning. We study whether such breakdowns are detectable from inference-time observables available in standard APIs (token log probabilities), without any training or fine-tuning. We define a simple instability signal that combines consecutive-step distributional shift (JSD) and uncertainty (entropy), summarize each trace by its peak instability strength, and show that this signal reliably predicts failure. Across GSM8K and HotpotQA, instability strength predicts wrong answers with above-chance AUC and yields monotonic bucket-level accuracy decline at scale across model sizes. Crucially, we show that instability is not uniformly harmful: early instability can reflect subsequent stabilization and a correct final answer (\emph{corrective instability}), whereas late instability is more often followed by failure (\emph{destructive instability}), even at comparable peak magnitudes, indicating that recoverability depends not only on how strongly the distribution changes but also on when such changes occur relative to the remaining decoding horizon. The method is model-agnostic, training-free, and reproducible, and is presented as a diagnostic lens rather than a corrective or control mechanism.

</details>


### [14] [Aligning Language Model Benchmarks with Pairwise Preferences](https://arxiv.org/abs/2602.02898)
*Marco Gutierrez,Xinyi Leng,Hannah Cyberey,Jonathan Richard Schwarz,Ahmed Alaa,Thomas Hartvigsen*

Main category: cs.AI

TL;DR: 提出BenchAlign方法，通过有限模型性能信息自动更新离线基准测试，使其能预测模型在真实场景中的偏好排序


<details>
  <summary>Details</summary>
Motivation: 现有语言模型基准测试虽然计算高效，但往往无法准确预测模型在实际应用中的真实性能，需要建立基准测试与实际效用之间的桥梁

Method: 提出BenchAlign方法，利用模型在基准问题上的表现和部署期间收集的模型排序对，学习与偏好对齐的问题权重，生成能预测未见模型排序的新基准

Result: 实验表明，对齐后的基准能准确根据人类偏好模型对未见模型进行排序，且在不同规模模型上保持有效性，同时保持可解释性

Conclusion: 这项工作为基准测试与实际人类偏好的对齐提供了洞见，有望加速模型开发向真实效用方向的发展

Abstract: Language model benchmarks are pervasive and computationally-efficient proxies for real-world performance. However, many recent works find that benchmarks often fail to predict real utility. Towards bridging this gap, we introduce benchmark alignment, where we use limited amounts of information about model performance to automatically update offline benchmarks, aiming to produce new static benchmarks that predict model pairwise preferences in given test settings. We then propose BenchAlign, the first solution to this problem, which learns preference-aligned weight- ings for benchmark questions using the question-level performance of language models alongside ranked pairs of models that could be collected during deployment, producing new benchmarks that rank previously unseen models according to these preferences. Our experiments show that our aligned benchmarks can accurately rank unseen models according to models of human preferences, even across different sizes, while remaining interpretable. Overall, our work provides insights into the limits of aligning benchmarks with practical human preferences, which stands to accelerate model development towards real utility.

</details>


### [15] [Minimal Computational Preconditions for Subjective Perspective in Artificial Agents](https://arxiv.org/abs/2602.02902)
*Hongju Pae*

Main category: cs.AI

TL;DR: 论文提出在人工智能体中实现主观视角的方法，通过缓慢演化的全局潜在状态来调制快速策略动态，而不直接优化行为结果，在环境变化中表现出方向依赖的滞后现象。


<details>
  <summary>Details</summary>
Motivation: 研究动机是在人工智能系统中操作化主观视角，基于最小化的现象学内部结构，为机器系统提供可测量的主观性特征。

Method: 方法是将主观视角实现为缓慢演化的全局潜在状态，该状态调制快速策略动态但不直接针对行为后果进行优化，在无奖励环境中测试环境变化时的响应。

Result: 结果显示这种潜在结构表现出方向依赖的滞后现象，而策略层面的行为保持相对反应性，这种滞后被认为是机器系统中类似主观视角的可测量特征。

Conclusion: 结论是方向依赖的滞后现象构成了机器系统中类似主观视角的可测量特征，为人工智能体的主观性研究提供了新的操作化方法。

Abstract: This study operationalizes subjective perspective in artificial agents by grounding it in a minimal, phenomenologically motivated internal structure. The perspective is implemented as a slowly evolving global latent state that modulates fast policy dynamics without being directly optimized for behavioral consequences. In a reward-free environment with regime shifts, this latent structure exhibits direction-dependent hysteresis, while policy-level behavior remains comparatively reactive. I argue that such hysteresis constitutes a measurable signature of perspective-like subjectivity in machine systems.

</details>


### [16] [FIRE-Bench: Evaluating Agents on the Rediscovery of Scientific Insights](https://arxiv.org/abs/2602.02905)
*Zhen Wang,Fan Bai,Zhongyan Luo,Jinyan Su,Kaiser Sun,Xinle Yu,Jieyuan Liu,Kun Zhou,Claire Cardie,Mark Dredze,Eric P. Xing,Zhiting Hu*

Main category: cs.AI

TL;DR: FIRE-Bench是一个评估AI代理科学发现能力的基准测试，通过让代理重新发现已发表的机器学习研究成果来测试其完整科研流程能力，结果显示当前最先进的代理系统在完整科研循环中表现有限。


<details>
  <summary>Details</summary>
Motivation: 现有评估基准存在权衡：要么过度依赖LLM作为评判者自动生成研究输出，要么优化孤立性能指标作为科学洞察的粗略代理。需要开发一个能够全面评估AI代理完整科学发现能力的基准。

Method: 引入FIRE-Bench基准，通过让AI代理重新发现近期高影响力机器学习研究的已确立发现来评估代理能力。代理仅获得从已验证研究中提取的高层研究问题，必须自主探索想法、设计实验、实现代码、执行计划，并基于经验证据得出结论。

Result: 评估了使用前沿LLM（如GPT-5）的最先进代理。结果显示完整科研循环对当前代理系统仍然具有挑战性：即使最强代理也仅取得有限成功（<50 F1），表现出高运行方差，在实验设计、执行和基于证据的推理中显示重复失败模式。

Conclusion: FIRE-Bench为衡量向可靠代理驱动科学发现的进展提供了一个严格且具有诊断性的框架，揭示了当前AI代理在完整科研流程中的局限性。

Abstract: Autonomous agents powered by large language models (LLMs) promise to accelerate scientific discovery end-to-end, but rigorously evaluating their capacity for verifiable discovery remains a central challenge. Existing benchmarks face a trade-off: they either heavily rely on LLM-as-judge evaluations of automatically generated research outputs or optimize convenient yet isolated performance metrics that provide coarse proxies for scientific insight. To address this gap, we introduce FIRE-Bench (Full-cycle Insight Rediscovery Evaluation), a benchmark that evaluates agents through the rediscovery of established findings from recent, high-impact machine learning research. Agents are given only a high-level research question extracted from a published, verified study and must autonomously explore ideas, design experiments, implement code, execute their plans, and derive conclusions supported by empirical evidence. We evaluate a range of state-of-the-art agents with frontier LLMs backbones like gpt-5 on FIRE-Bench. Our results show that full-cycle scientific research remains challenging for current agent systems: even the strongest agents achieve limited rediscovery success (<50 F1), exhibit high variance across runs, and display recurring failure modes in experimental design, execution, and evidence-based reasoning. FIRE-Bench provides a rigorous and diagnostic framework for measuring progress toward reliable agent-driven scientific discovery.

</details>


### [17] [Building Interpretable Models for Moral Decision-Making](https://arxiv.org/abs/2602.03351)
*Mayank Goel,Aritra Das,Paras Chopra*

Main category: cs.AI

TL;DR: 使用定制Transformer模型研究神经网络在电车难题中的道德决策，通过结构化场景编码实现77%准确率，并利用可解释性技术揭示道德推理在网络的分布模式


<details>
  <summary>Details</summary>
Motivation: 研究神经网络如何进行道德决策，特别是在电车难题这类伦理困境中，理解AI系统的道德推理机制

Method: 构建定制Transformer模型，使用结构化场景编码（涉及人员、数量、结果归属），采用2层架构处理道德机器数据，应用多种可解释性技术分析网络内部机制

Result: 模型在道德机器数据上达到77%准确率，同时保持足够小的规模以便详细分析；研究发现道德推理分布在网络的不同计算阶段，偏见定位在特定的计算层次

Conclusion: 定制Transformer模型能有效研究神经网络的道德决策过程，可解释性技术揭示了道德推理在神经网络中的分布特性，为理解AI伦理决策提供了新视角

Abstract: We build a custom transformer model to study how neural networks make moral decisions on trolley-style dilemmas. The model processes structured scenarios using embeddings that encode who is affected, how many people, and which outcome they belong to. Our 2-layer architecture achieves 77% accuracy on Moral Machine data while remaining small enough for detailed analysis. We use different interpretability techniques to uncover how moral reasoning distributes across the network, demonstrating that biases localize to distinct computational stages among other findings.

</details>


### [18] [Reasoning about Reasoning: BAPO Bounds on Chain-of-Thought Token Complexity in LLMs](https://arxiv.org/abs/2602.02909)
*Kiran Tomlinson,Tobias Schnabel,Adith Swaminathan,Jennifer Neville*

Main category: cs.AI

TL;DR: 论文研究了思维链推理所需的计算量，证明了对于三类典型任务，推理token数量需要与输入规模成线性关系，并通过实验验证了这一理论下界。


<details>
  <summary>Details</summary>
Motivation: 思维链推理虽然能显著提升大语言模型性能，但带来了巨大的延迟和计算成本。本文旨在从理论上探究：随着输入规模增长，解决问题需要多少推理token？

Method: 扩展了有界注意力前缀预言机模型，对三类BAPO-hard任务（二进制多数、三元组匹配、图可达性）进行理论分析，证明推理token数量的下界，并通过显式构造提供匹配或接近匹配的上界，最后用前沿推理模型进行实验验证。

Result: 理论证明三类任务都需要Ω(n)个推理token（输入规模为n），并通过构造实现了匹配或接近匹配的上界。实验显示前沿推理模型在这些任务上确实呈现近似线性的推理token扩展，当推理预算受限时会出现失败。

Conclusion: 研究揭示了思维链推理在推理时计算方面的基本瓶颈，为分析最优推理长度提供了理论工具。结果表明，对于某些复杂任务，线性推理token扩展是不可避免的。

Abstract: Inference-time scaling via chain-of-thought (CoT) reasoning is a major driver of state-of-the-art LLM performance, but it comes with substantial latency and compute costs. We address a fundamental theoretical question: how many reasoning tokens are required to solve a problem as input size grows? By extending the bounded attention prefix oracle (BAPO) model--an abstraction of LLMs that quantifies the information flow required to solve a task--we prove lower bounds on the CoT tokens required for three canonical BAPO-hard tasks: binary majority, triplet matching, and graph reachability. We show that each requires $Ω(n)$ reasoning tokens when the input size is $n$. We complement these results with matching or near-matching upper bounds via explicit constructions. Finally, our experiments with frontier reasoning models show approximately linear reasoning token scaling on these tasks and failures when constrained to smaller reasoning budgets, consistent with our theoretical lower bounds. Together, our results identify fundamental bottlenecks in inference-time compute through CoT and offer a principled tool for analyzing optimal reasoning length.

</details>


### [19] [DeltaEvolve: Accelerating Scientific Discovery through Momentum-Driven Evolution](https://arxiv.org/abs/2602.02919)
*Jiachen Jiang,Tianyu Ding,Zhihui Zhu*

Main category: cs.AI

TL;DR: DeltaEvolve提出基于语义差量的进化框架，用结构化语义变化替代完整代码历史，减少token消耗并提升进化效果


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的进化系统（如AlphaEvolve）依赖完整代码历史，存在上下文效率低和进化指导弱的问题。完整代码快照包含冗余实现细节，稀释了核心算法思想，难以提供清晰的进化灵感。

Method: 将进化智能体形式化为期望最大化框架：语言模型采样候选程序（E步），系统基于评估反馈更新控制上下文（M步）。提出DeltaEvolve框架，用结构化语义差量替代完整代码历史，捕捉连续节点间修改如何及为何影响性能。通过多级数据库和渐进披露机制组织语义差量，进一步减少输入token。

Result: 在多个科学领域的任务上进行实证评估，表明该框架能够以更少的token消耗发现比基于完整代码的进化智能体更好的解决方案。

Conclusion: DeltaEvolve通过语义差量驱动的进化框架，解决了现有方法上下文效率低的问题，在减少计算资源消耗的同时提升了科学发现的自动化能力。

Abstract: LLM-driven evolutionary systems have shown promise for automated science discovery, yet existing approaches such as AlphaEvolve rely on full-code histories that are context-inefficient and potentially provide weak evolutionary guidance. In this work, we first formalize the evolutionary agents as a general Expectation-Maximization framework, where the language model samples candidate programs (E-step) and the system updates the control context based on evaluation feedback (M-step). Under this view, constructing context via full-code snapshots constitutes a suboptimal M-step, as redundant implement details dilutes core algorithmic ideas, making it difficult to provide clear inspirations for evolution. To address this, we propose DeltaEvolve, a momentum-driven evolutionary framework that replaces full-code history with structured semantic delta capturing how and why modifications between successive nodes affect performance. As programs are often decomposable, semantic delta usually contains many effective components which are transferable and more informative to drive improvement. By organizing semantic delta through multi-level database and progressive disclosure mechanism, input tokens are further reduced. Empirical evaluations on tasks across diverse scientific domains show that our framework can discover better solution with less token consumption over full-code-based evolutionary agents.

</details>


### [20] [UAT-LITE: Inference-Time Uncertainty-Aware Attention for Pretrained Transformers](https://arxiv.org/abs/2602.02952)
*Elias Hossain,Shubhashis Roy Dipta,Subash Neupane,Rajib Rana,Ravid Shwartz-Ziv,Ivan Garibay,Niloofar Yousefi*

Main category: cs.AI

TL;DR: UAT-LITE：一种推理时框架，通过Monte Carlo dropout在预训练Transformer分类器中实现近似贝叶斯推断，使自注意力机制具备不确定性感知能力，无需修改预训练权重或训练目标。


<details>
  <summary>Details</summary>
Motivation: 神经NLP模型通常校准不佳，对错误预测赋予高置信度，这影响了选择性预测和高风险部署。现有后处理校准方法只调整输出概率而不改变内部计算，而集成和贝叶斯方法虽然能改善不确定性但训练或存储成本高昂。

Method: 提出UAT-LITE推理时框架，在预训练Transformer分类器中使用Monte Carlo dropout进行近似贝叶斯推断，估计token级认知不确定性，并用其调制自注意力机制。同时引入层间方差分解来诊断预测不确定性在Transformer深度中的累积过程。

Result: 在SQuAD 2.0可回答性、MNLI和SST-2任务上，UAT-LITE相比微调BERT-base基线平均减少约20%的期望校准误差，同时保持任务准确性，并改善了选择性预测和分布偏移下的鲁棒性。

Conclusion: UAT-LITE通过推理时的不确定性感知自注意力调制，有效提高了预训练Transformer模型的校准性能，为模型不确定性估计提供了一种轻量级解决方案。

Abstract: Neural NLP models are often miscalibrated, assigning high confidence to incorrect predictions, which undermines selective prediction and high-stakes deployment. Post-hoc calibration methods adjust output probabilities but leave internal computation unchanged, while ensemble and Bayesian approaches improve uncertainty at substantial training or storage cost. We propose UAT-LITE, an inference-time framework that makes self-attention uncertainty-aware using approximate Bayesian inference via Monte Carlo dropout in pretrained transformer classifiers. Token-level epistemic uncertainty is estimated from stochastic forward passes and used to modulate self-attention during contextualization, without modifying pretrained weights or training objectives. We additionally introduce a layerwise variance decomposition to diagnose how predictive uncertainty accumulates across transformer depth. Across the SQuAD 2.0 answerability, MNLI, and SST-2, UAT-LITE reduces Expected Calibration Error by approximately 20% on average relative to a fine-tuned BERT-base baseline while preserving task accuracy, and improves selective prediction and robustness under distribution shift.

</details>


### [21] [Generative Engine Optimization: A VLM and Agent Framework for Pinterest Acquisition Growth](https://arxiv.org/abs/2602.02961)
*Faye Zhang,Qianyu Cheng,Jasmine Wan,Vishwakarma Singh,Jinfeng Rao,Kofi Boakye*

Main category: cs.AI

TL;DR: Pinterest开发了一个名为GEO的生产级框架，通过逆向搜索设计、视觉语言模型预测用户搜索意图、AI代理挖掘实时趋势、构建语义连贯的集合页面，以及建立权威感知的链接结构，帮助视觉内容平台在生成式搜索时代保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型正在重塑内容发现方式，从传统的SEO转向生成式引擎优化(GEO)。对于拥有数十亿视觉资产的平台如Pinterest，单个图像缺乏生成式搜索所需的语义深度和权威信号，面临用户需求在搜索页面直接满足而无需访问网站的风险。

Method: 1. 逆向搜索设计：微调视觉语言模型预测用户实际会搜索什么，而非生成通用图像描述；2. AI代理挖掘实时互联网趋势捕获新兴搜索需求；3. 使用VLM生成的查询构建语义连贯的集合页面；4. 采用混合VLM和双塔ANN架构建立权威感知的链接结构，在数十亿视觉资产间传播信号。

Result: 在数十亿图像和数千万集合上大规模部署，GEO实现了20%的有机流量增长，为月度活跃用户(MAU)带来数百万的增长。

Conclusion: GEO为视觉平台在生成式搜索时代蓬勃发展提供了一条原则性路径，通过逆向搜索设计和权威信号传播，有效解决了视觉内容在生成式搜索中的语义深度不足问题。

Abstract: Large Language Models are fundamentally reshaping content discovery through AI-native search systems such as ChatGPT, Gemini, and Claude. Unlike traditional search engines that match keywords to documents, these systems infer user intent, synthesize multimodal evidence, and generate contextual answers directly on the search page, introducing a paradigm shift from Search Engine Optimization (SEO) to Generative Engine Optimization (GEO). For visual content platforms hosting billions of assets, this poses an acute challenge: individual images lack the semantic depth and authority signals that generative search prioritizes, risking disintermediation as user needs are satisfied in-place without site visits.
  We present Pinterest GEO, a production-scale framework that pioneers reverse search design: rather than generating generic image captions describing what content is, we fine-tune Vision-Language Models (VLMs) to predict what users would actually search for, augmented this with AI agents that mine real-time internet trends to capture emerging search demand. These VLM-generated queries then drive construction of semantically coherent Collection Pages via multimodal embeddings, creating indexable aggregations optimized for generative retrieval. Finally, we employ hybrid VLM and two-tower ANN architectures to build authority-aware interlinking structures that propagate signals across billions of visual assets. Deployed at scale across billions of images and tens of millions of collections, GEO delivers 20\% organic traffic growth contributing to multi-million monthly active user (MAU) growth, demonstrating a principled pathway for visual platforms to thrive in the generative search era.

</details>


### [22] [Structuring Value Representations via Geometric Coherence in Markov Decision Processes](https://arxiv.org/abs/2602.02978)
*Zuyuan Zhang,Zeyu Fang,Tian Lan*

Main category: cs.AI

TL;DR: 提出GCR-RL方法，通过偏序集理论重构强化学习，利用几何一致性正则化提升样本效率和稳定性


<details>
  <summary>Details</summary>
Motivation: 几何性质可以稳定和加速强化学习，现有方法包括编码对称结构、几何感知数据增强和强制结构限制。本文从序理论的新视角重新审视RL，将价值函数估计重构为学习期望的偏序集

Method: 提出GCR-RL（几何一致性正则化强化学习），通过计算一系列超偏序集细化——通过细化先前步骤中的偏序集并从时序差分信号中学习额外的序关系——确保支撑学习价值函数的偏序集序列具有几何一致性。开发了基于Q学习和actor-critic的两种新算法来实现这些超偏序集细化

Result: 分析了算法的理论性质和收敛速率。在多个任务中实证评估GCR-RL，相比强基线在样本效率和稳定性能方面显示出显著改进

Conclusion: 通过序理论视角重新构建强化学习，提出的GCR-RL方法能够有效利用几何一致性正则化，在保持理论保证的同时显著提升实际性能

Abstract: Geometric properties can be leveraged to stabilize and speed reinforcement learning. Existing examples include encoding symmetry structure, geometry-aware data augmentation, and enforcing structural restrictions. In this paper, we take a novel view of RL through the lens of order theory and recast value function estimates into learning a desired poset (partially ordered set). We propose \emph{GCR-RL} (Geometric Coherence Regularized Reinforcement Learning) that computes a sequence of super-poset refinements -- by refining posets in previous steps and learning additional order relationships from temporal difference signals -- thus ensuring geometric coherence across the sequence of posets underpinning the learned value functions. Two novel algorithms by Q-learning and by actor--critic are developed to efficiently realize these super-poset refinements. Their theoretical properties and convergence rates are analyzed. We empirically evaluate GCR-RL in a range of tasks and demonstrate significant improvements in sample efficiency and stable performance over strong baselines.

</details>


### [23] [Are LLMs Biased Like Humans? Causal Reasoning as a Function of Prior Knowledge, Irrelevant Information, and Reasoning Budget](https://arxiv.org/abs/2602.02983)
*Hanna M. Dettki,Charley M. Wu,Bob Rehder*

Main category: cs.AI

TL;DR: LLMs在因果推理任务中表现出比人类更规则化的推理策略，较少受人类典型的因果偏见影响，但可能在不确定性情境下失效。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在需要因果推理的领域应用增多，需要了解它们的因果判断是基于规范性计算、人类式捷径还是脆弱的模式匹配。

Method: 使用11个基于碰撞器结构(C₁→E←C₂)的因果判断任务，对20多个LLMs进行基准测试，并与匹配的人类基线比较。使用可解释模型压缩LLMs的因果判断，并测试其在语义抽象和提示过载下的鲁棒性。

Result: 大多数LLMs表现出比人类更规则化的推理策略，人类在概率判断中会考虑未提及的潜在因素。LLMs较少表现出人类典型的碰撞器偏见（弱解释消除和马尔可夫违反）。思维链(CoT)能提高许多LLMs的鲁棒性。

Conclusion: LLMs与人类在因果推理上的差异表明，当已知偏见不可取时，LLMs可以补充人类判断，但其规则化推理在不确定性情境下可能失效，需要表征LLMs的推理策略以确保安全有效部署。

Abstract: Large language models (LLMs) are increasingly used in domains where causal reasoning matters, yet it remains unclear whether their judgments reflect normative causal computation, human-like shortcuts, or brittle pattern matching. We benchmark 20+ LLMs against a matched human baseline on 11 causal judgment tasks formalized by a collider structure ($C_1 \!\rightarrow\! E\! \leftarrow \!C_2$). We find that a small interpretable model compresses LLMs' causal judgments well and that most LLMs exhibit more rule-like reasoning strategies than humans who seem to account for unmentioned latent factors in their probability judgments. Furthermore, most LLMs do not mirror the characteristic human collider biases of weak explaining away and Markov violations. We probe LLMs' causal judgment robustness under (i) semantic abstraction and (ii) prompt overloading (injecting irrelevant text), and find that chain-of-thought (CoT) increases robustness for many LLMs. Together, this divergence suggests LLMs can complement humans when known biases are undesirable, but their rule-like reasoning may break down when uncertainty is intrinsic -- highlighting the need to characterize LLM reasoning strategies for safe, effective deployment.

</details>


### [24] [Large Language Models Can Take False First Steps at Inference-time Planning](https://arxiv.org/abs/2602.02991)
*Haijiang Yan,Jian-Qiao Zhu,Adam Sanborn*

Main category: cs.AI

TL;DR: LLMs在训练中获得了序列规划能力，但在推理时表现出短视和不一致的行为。论文提出贝叶斯解释：自生成上下文驱动规划偏移，导致看似受损的规划行为。


<details>
  <summary>Details</summary>
Motivation: 解释LLMs在推理时表现出的规划行为与训练获得的规划能力之间的差距，理解自生成上下文如何影响LLMs的规划决策。

Method: 提出贝叶斯理论模型，将规划行为基于演化的生成上下文。通过两个受控实验验证：随机生成任务展示人类提示下的受限规划和自生成上下文积累时的规划强度增加；高斯采样任务显示在自生成序列条件下初始偏见的减少。

Result: 实验验证了理论模型：自生成上下文确实驱动规划偏移，随着上下文积累规划强度增加，自生成序列能减少初始偏见。为LLMs推理时的前瞻规划行为提供了理论和实证解释。

Conclusion: LLMs推理时看似受损的规划行为可以通过贝叶斯框架解释为自生成上下文驱动的规划偏移，这为理解和改进LLMs的规划能力提供了理论基础。

Abstract: Large language models (LLMs) have been shown to acquire sequence-level planning abilities during training, yet their planning behavior exhibited at inference time often appears short-sighted and inconsistent with these capabilities. We propose a Bayesian account for this gap by grounding planning behavior in the evolving generative context: given the subtle differences between natural language and the language internalized by LLMs, accumulated self-generated context drives a planning-shift during inference and thereby creates the appearance of compromised planning behavior. We further validate the proposed model through two controlled experiments: a random-generation task demonstrating constrained planning under human prompts and increasing planning strength as self-generated context accumulates, and a Gaussian-sampling task showing reduced initial bias when conditioning on self-generated sequences. These findings provide a theoretical explanation along with empirical evidence for characterizing how LLMs plan ahead during inference.

</details>


### [25] [Agent Alpha: Tree Search Unifying Generation, Exploration and Evaluation for Computer-Use Agents](https://arxiv.org/abs/2602.02995)
*Sizhe Tang,Rongqian Chen,Tian Lan*

Main category: cs.AI

TL;DR: Agent Alpha是一个GUI代理框架，通过步骤级蒙特卡洛树搜索结合生成、探索和评估，实现主动规划、早期剪枝和前缀重用，在OSWorld基准上达到77%的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理通过轨迹级采样扩展测试时计算，但缺乏回归能力，无法重用部分成功结果或从早期错误中恢复。

Method: 提出Agent Alpha框架，集成步骤级蒙特卡洛树搜索（MCTS），结合alpha-UCT引导搜索、比较驱动评估和多样性约束扩展，实现主动规划空间建模。

Result: 在OSWorld基准测试中达到约77%的成功率，显著优于同等计算条件下的轨迹级基线方法。

Conclusion: Agent Alpha通过步骤级MCTS实现了有效的GUI代理规划，具备回归能力和前缀重用，在有限计算资源下取得最先进性能。

Abstract: While scaling test-time compute through trajectory-level sampling has significantly improved Graphical User Interface (GUI) agents, the lack of regressive ability prevents the reuse of partial successes and the recovery from early missteps. In this paper, we introduce Agent Alpha, a unified framework that synergizes generation, exploration, and evaluation through step-level Monte Carlo Tree Search (MCTS). It enables active modeling or exploiting structures of the planning space. By integrating alpha-UCT guided search into the interaction loop, Agent Alpha enables deliberate planning, facilitating early pruning of suboptimal branches and efficient prefix reuse. We also employ comparison-driven evaluation to mitigate absolute scoring biases and diversity-constrained expansion to maintain a compact, informative search space. Regret bound of alpha-UCT is analyzed. On the OSWorld benchmark, Agent Alpha achieves a state-of-the-art success rate of $\sim 77\%$, significantly outperforming trajectory-level baselines under equivalent compute.

</details>


### [26] [Methods and Open Problems in Differentiable Social Choice: Learning Mechanisms, Decisions, and Alignment](https://arxiv.org/abs/2602.03003)
*Zhiyu An,Wan Du*

Main category: cs.AI

TL;DR: 该论文综述了可微分社会选择这一新兴范式，将投票规则、机制和聚合过程构建为可从数据中学习的可微分模型，并探讨了机器学习、经济学和民主理论的交叉研究议程。


<details>
  <summary>Details</summary>
Motivation: 社会选择已从政治理论和经济学的边缘问题转变为现代机器学习系统的核心组成部分。从拍卖、资源分配到联邦学习、参与式治理和大语言模型对齐，机器学习系统越来越多地聚合异质偏好、激励和判断来做出集体决策。当前许多机器学习系统已实现社会选择机制，但往往是隐式的且缺乏规范性审查。

Method: 该综述采用可微分社会选择范式，将投票规则、机制和聚合过程构建为可学习的、可微分的模型，通过数据优化。综合分析了拍卖、投票、预算编制、流动民主、去中心化聚合和逆向机制学习等领域的工作，展示了经典公理和不可能性定理如何重新表现为目标、约束和优化权衡。

Result: 论文系统性地综述了可微分社会选择领域的研究进展，展示了如何将传统社会选择理论转化为机器学习框架，并识别了36个开放性问题，为机器学习、经济学和民主理论的交叉研究定义了新的研究议程。

Conclusion: 可微分社会选择为机器学习系统提供了明确的规范性框架，使社会选择机制能够从数据中学习并优化。该领域面临的重要挑战包括如何平衡效率与公平、透明度与复杂性等传统权衡，以及如何将民主理论原则融入机器学习系统设计。36个开放性问题为该交叉领域的研究指明了方向。

Abstract: Social choice is no longer a peripheral concern of political theory or economics-it has become a foundational component of modern machine learning systems. From auctions and resource allocation to federated learning, participatory governance, and the alignment of large language models, machine learning pipelines increasingly aggregate heterogeneous preferences, incentives, and judgments into collective decisions. In effect, many contemporary machine learning systems already implement social choice mechanisms, often implicitly and without explicit normative scrutiny.
  This Review surveys differentiable social choice: an emerging paradigm that formulates voting rules, mechanisms, and aggregation procedures as learnable, differentiable models optimized from data. We synthesize work across auctions, voting, budgeting, liquid democracy, decentralized aggregation, and inverse mechanism learning, showing how classical axioms and impossibility results reappear as objectives, constraints, and optimization trade-offs. We conclude by identifying 36 open problems defining a new research agenda at the intersection of machine learning, economics, and democratic theory.

</details>


### [27] [Distilling LLM Reasoning into Graph of Concept Predictors](https://arxiv.org/abs/2602.03006)
*Ziyang Yu,Liang Zhao*

Main category: cs.AI

TL;DR: GCP是一个推理感知的主动蒸馏框架，通过将教师模型的决策过程外部化为有向无环图，并用模块化概念预测器在学生模型中镜像该图，从而提高样本效率和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有主动蒸馏方法通常只蒸馏最终标签，丢弃了中间推理信号，无法诊断推理缺失和错误来源，限制了大规模部署LLM时的推理延迟、计算和API成本优化。

Method: 提出图概念预测器（GCP）框架：1）将教师决策过程外部化为有向无环图；2）在学生模型中用模块化概念预测器镜像该图；3）采用图感知获取策略针对关键推理节点的不确定性和分歧；4）通过目标子模块重训练实现训练稳定性和效率提升。

Result: 在8个NLP分类基准测试中，GCP在有限标注预算下提升了性能，同时产生了更可解释和可控的训练动态。

Conclusion: GCP通过推理感知的主动蒸馏，不仅提高了样本效率和训练稳定性，还提供了更好的可解释性和控制性，为LLM的规模化部署提供了有效解决方案。

Abstract: Deploying Large Language Models (LLMs) for discriminative workloads is often limited by inference latency, compute, and API costs at scale. Active distillation reduces these costs by querying an LLM oracle to train compact discriminative students, but most pipelines distill only final labels, discarding intermediate reasoning signals and offering limited diagnostics of what reasoning is missing and where errors arise. We propose Graph of Concept Predictors (GCP), a reasoning-aware active distillation framework that externalizes the teacher's decision process as a directed acyclic graph and mirrors it with modular concept predictors in the student. GCP enhances sample efficiency through a graph-aware acquisition strategy that targets uncertainty and disagreement at critical reasoning nodes. Additionally, it improves training stability and efficiency by performing targeted sub-module retraining, which attributes downstream loss to specific concept predictors and updates only the most influential modules. Experiments on eight NLP classification benchmarks demonstrate that GCP enhances performance under limited annotation budgets while yielding more interpretable and controllable training dynamics. Code is available at: https://github.com/Ziyang-Yu/GCP.

</details>


### [28] [STAR: Similarity-guided Teacher-Assisted Refinement for Super-Tiny Function Calling Models](https://arxiv.org/abs/2602.03022)
*Jiliang Ni,Jiachen Pu,Zhongyi Yang,Jingfeng Luo,Conggang Hu*

Main category: cs.AI

TL;DR: STAR框架通过相似性引导的教师辅助精炼，将大语言模型的能力蒸馏到超小型模型，在函数调用任务上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在函数调用中很重要，但规模太大阻碍广泛采用，需要将其能力转移到小型模型。现有方法存在过拟合、训练不稳定、二元奖励对多解任务无效、技术难以协同等问题。

Method: STAR框架包含两个核心技术：1) 约束知识蒸馏(CKD)，通过增强top-k前向KL散度来抑制错误预测；2) 相似性引导强化学习(Sim-RL)，引入细粒度、基于相似性的奖励。这些技术在统一训练课程中协同工作。

Result: 在具有挑战性的基准测试中，STAR模型在各自规模类别中达到SOTA。特别是0.6B STAR模型在所有1B以下开源模型中表现最佳，甚至超过一些更大规模的知名开源模型。

Conclusion: STAR展示了一个将大语言模型能力蒸馏到超小型模型的训练框架，为强大、可访问且高效的AI智能体铺平了道路。

Abstract: The proliferation of Large Language Models (LLMs) in function calling is pivotal for creating advanced AI agents, yet their large scale hinders widespread adoption, necessitating transferring their capabilities into smaller ones. However, existing paradigms are often plagued by overfitting, training instability, ineffective binary rewards for multi-solution tasks, and the difficulty of synergizing techniques. We introduce STAR: Similarity-guided Teacher-Assisted Refinement, a novel holistic framework that effectively transfers LLMs' capabilities to super-tiny models. STAR consists of two core technical innovations: (1) Constrained Knowledge Distillation (CKD), a training objective that augments top-k forward KL divergence to suppress confidently incorrect predictions, ensuring training stability while preserving exploration capacity for downstream RL. STAR holistically synergizes these strategies within a cohesive training curriculum, enabling super-tiny models to achieve exceptional performance on complex function calling tasks; (2) Similarity-guided RL (Sim-RL), a RL mechanism that introduces a fine-grained, similarity-based reward. This provides a robust, continuous, and rich signal for better policy optimization by evaluating the similarity between generated outputs and the ground truth. Extensive experiments on challenging and renowned benchmarks demonstrate the effectiveness of our method. Our STAR models establish SOTA in their size classes, significantly outperforming baselines. Remarkably, our 0.6B STAR model achieves the best performance among all open models under 1B, surpassing even several well-known open models at a larger scale. STAR demonstrates a training framework that distills capabilities of LLMs into super-tiny models, paving the way for powerful, accessible, and efficient AI agents.

</details>


### [29] [RC-GRPO: Reward-Conditioned Group Relative Policy Optimization for Multi-Turn Tool Calling Agents](https://arxiv.org/abs/2602.03025)
*Haitian Zhong,Jixiu Zhai,Lei Song,Jiang Bian,Qiang Liu,Tieniu Tan*

Main category: cs.AI

TL;DR: 提出RC-GRPO方法解决多轮工具调用中奖励稀疏和探索成本高的问题，通过在提示中注入奖励目标标记来引导模型生成不同质量的轨迹，提升组内多样性，在BFCLv4基准上超越基线方法


<details>
  <summary>Details</summary>
Motivation: 多轮工具调用对大型语言模型具有挑战性，因为奖励稀疏且探索成本高。传统的SFT+GRPO方法在组内奖励变化低时（如组内多数rollout获得全0或全1奖励）会停滞，导致组归一化优势信息不足，更新消失

Method: 提出RC-GRPO（奖励条件组相对策略优化）：1）首先在混合质量轨迹上微调奖励条件轨迹策略（RCTP），在提示中注入奖励目标特殊标记（如<|high_reward|>, <|low_reward|>），使模型能够按需生成不同质量的轨迹；2）在强化学习阶段，在每个GRPO组内采样多样化的奖励标记，并基于采样标记条件化rollout，提高组内多样性，增强优势增益

Result: 在Berkeley Function Calling Leaderboard v4多轮基准测试中，该方法相比基线方法获得持续改进的性能，Qwen-2.5-7B-Instruct模型的性能甚至超越了所有闭源API模型

Conclusion: RC-GRPO通过将探索视为可控的引导问题，使用离散奖励标记来改善组内多样性，有效解决了多轮工具调用中奖励稀疏和探索困难的问题，在基准测试中取得了优异表现

Abstract: Multi-turn tool calling is challenging for Large Language Models (LLMs) because rewards are sparse and exploration is expensive. A common recipe, SFT followed by GRPO, can stall when within-group reward variation is low (e.g., more rollouts in a group receive the all 0 or all 1 reward), making the group-normalized advantage uninformative and yielding vanishing updates. To address this problem, we propose RC-GRPO (Reward-Conditioned Group Relative Policy Optimization), which treats exploration as a controllable steering problem via discrete reward tokens. We first fine-tune a Reward-Conditioned Trajectory Policy (RCTP) on mixed-quality trajectories with reward goal special tokens (e.g., <|high_reward|>, <|low_reward|>) injected into the prompts, enabling the model to learn how to generate distinct quality trajectories on demand. Then during RL, we sample diverse reward tokens within each GRPO group and condition rollouts on the sampled token to improve within-group diversity, improving advantage gains. On the Berkeley Function Calling Leaderboard v4 (BFCLv4) multi-turn benchmark, our method yields consistently improved performance than baselines, and the performance on Qwen-2.5-7B-Instruct even surpasses all closed-source API models.

</details>


### [30] [Visual Reasoning over Time Series via Multi-Agent System](https://arxiv.org/abs/2602.03026)
*Weilin Ruan,Yuxuan Liang*

Main category: cs.AI

TL;DR: MAS4TS是一个基于工具驱动的多智能体系统，用于通用时间序列任务，通过视觉推理和潜在重建实现跨任务泛化


<details>
  <summary>Details</summary>
Motivation: 现有时间序列分析方法在整合直观视觉推理和跨任务泛化方面存在局限，需要能够自适应使用工具的统一框架

Method: 基于Analyzer-Reasoner-Executor范式，集成智能体通信、视觉推理和潜在重建；使用视觉语言模型对时间序列图进行视觉推理，然后在潜在空间重建预测轨迹；三个专门智能体通过共享内存和门控通信协调，路由器选择任务特定工具链

Result: 在多个基准测试中实现最先进性能，在广泛时间序列任务上表现优异，同时展现出强大的泛化能力和高效推理

Conclusion: MAS4TS通过工具驱动的多智能体系统成功解决了时间序列分析中视觉推理和跨任务泛化的挑战，为通用时间序列任务提供了有效解决方案

Abstract: Time series analysis underpins many real-world applications, yet existing time-series-specific methods and pretrained large-model-based approaches remain limited in integrating intuitive visual reasoning and generalizing across tasks with adaptive tool usage. To address these limitations, we propose MAS4TS, a tool-driven multi-agent system for general time series tasks, built upon an Analyzer-Reasoner-Executor paradigm that integrates agent communication, visual reasoning, and latent reconstruction within a unified framework. MAS4TS first performs visual reasoning over time series plots with structured priors using a Vision-Language Model to extract temporal structures, and subsequently reconstructs predictive trajectories in latent space. Three specialized agents coordinate via shared memory and gated communication, while a router selects task-specific tool chains for execution. Extensive experiments on multiple benchmarks demonstrate that MAS4TS achieves state-of-the-art performance across a wide range of time series tasks, while exhibiting strong generalization and efficient inference.

</details>


### [31] [KANFIS A Neuro-Symbolic Framework for Interpretable and Uncertainty-Aware Learning](https://arxiv.org/abs/2602.03034)
*Binbin Yong,Haoran Pei,Jun Shen,Haoran Li,Qingguo Zhou,Zhao Su*

Main category: cs.AI

TL;DR: 提出KANFIS（Kolmogorov-Arnold Neuro-Fuzzy Inference System），一种紧凑的神经符号架构，通过加法聚合机制解决传统ANFIS在高维空间中规则指数爆炸的问题，同时保持可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统ANFIS架构存在结构复杂性问题，基于乘积的推理机制在高维空间中会导致规则数量指数级增长，这限制了其实际应用。

Method: 提出KANFIS架构，采用加法聚合机制而非传统乘积机制，使模型参数和规则复杂度随输入维度线性增长而非指数增长。兼容Type-1和Interval Type-2模糊逻辑系统，通过稀疏掩码机制生成紧凑结构化规则集。

Result: KANFIS在保持可解释性的同时，与代表性神经网络和神经模糊基线相比取得了有竞争力的性能表现。

Conclusion: KANFIS通过加法函数分解统一模糊推理，解决了传统ANFIS的结构复杂性问题，实现了紧凑、可解释且性能优越的神经模糊推理系统。

Abstract: Adaptive Neuro-Fuzzy Inference System (ANFIS) was designed to combine the learning capabilities of neural network with the reasoning transparency of fuzzy logic. However, conventional ANFIS architectures suffer from structural complexity, where the product-based inference mechanism causes an exponential explosion of rules in high-dimensional spaces. We herein propose the Kolmogorov-Arnold Neuro-Fuzzy Inference System (KANFIS), a compact neuro-symbolic architecture that unifies fuzzy reasoning with additive function decomposition. KANFIS employs an additive aggregation mechanism, under which both model parameters and rule complexity scale linearly with input dimensionality rather than exponentially. Furthermore, KANFIS is compatible with both Type-1 (T1) and Interval Type-2 (IT2) fuzzy logic systems, enabling explicit modeling of uncertainty and ambiguity in fuzzy representations. By using sparse masking mechanisms, KANFIS generates compact and structured rule sets, resulting in an intrinsically interpretable model with clear rule semantics and transparent inference processes. Empirical results demonstrate that KANFIS achieves competitive performance against representative neural and neuro-fuzzy baselines.

</details>


### [32] [MAS-ProVe: Understanding the Process Verification of Multi-Agent Systems](https://arxiv.org/abs/2602.03053)
*Vishal Venkataramani,Haizhou Shi,Zixuan Ke,Austin Xu,Xiaoxiao He,Yingbo Zhou,Semih Yavuz,Hao Wang,Shafiq Joty*

Main category: cs.AI

TL;DR: 本文对多智能体系统中的过程验证进行了系统实证研究，发现过程级验证并不能持续提升性能，且存在高方差问题，表明可靠评估部分多智能体轨迹具有挑战性。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的多智能体系统在推理轨迹上表现出高方差，过程验证在一般推理场景中显示出潜力，但其在多智能体系统中的实际效果尚不明确，需要填补这一研究空白。

Method: 提出了MAS-ProVe系统，研究三种验证范式（LLM-as-a-Judge、奖励模型、过程奖励模型），在两个验证粒度（智能体级和迭代级）上评估，考察了五个代表性验证器和四种上下文管理策略，在六个多智能体框架和多个推理基准上进行实验。

Result: 过程级验证不能持续提升性能且常表现出高方差；LLM-as-a-Judge通常优于基于奖励的方法；训练过的法官优于通用LLM；LLM作为法官与作为单智能体之间存在小的性能差距；验证中存在上下文长度与性能的权衡。

Conclusion: 多智能体系统的有效且鲁棒的过程验证仍然是一个开放挑战，需要超越当前范式的进一步进展。

Abstract: Multi-Agent Systems (MAS) built on Large Language Models (LLMs) often exhibit high variance in their reasoning trajectories. Process verification, which evaluates intermediate steps in trajectories, has shown promise in general reasoning settings, and has been suggested as a potential tool for guiding coordination of MAS; however, its actual effectiveness in MAS remains unclear. To fill this gap, we present MAS-ProVe, a systematic empirical study of process verification for multi-agent systems (MAS). Our study spans three verification paradigms (LLM-as-a-Judge, reward models, and process reward models), evaluated across two levels of verification granularity (agent-level and iteration-level). We further examine five representative verifiers and four context management strategies, and conduct experiments over six diverse MAS frameworks on multiple reasoning benchmarks. We find that process-level verification does not consistently improve performance and frequently exhibits high variance, highlighting the difficulty of reliably evaluating partial multi-agent trajectories. Among the methods studied, LLM-as-a-Judge generally outperforms reward-based approaches, with trained judges surpassing general-purpose LLMs. We further observe a small performance gap between LLMs acting as judges and as single agents, and identify a context-length-performance trade-off in verification. Overall, our results suggest that effective and robust process verification for MAS remains an open challenge, requiring further advances beyond current paradigms. Code is available at https://github.com/Wang-ML-Lab/MAS-ProVe.

</details>


### [33] [De-conflating Preference and Qualification: Constrained Dual-Perspective Reasoning for Job Recommendation with Large Language Models](https://arxiv.org/abs/2602.03097)
*Bryce Kan,Wei Yang,Emily Nguyen,Ganghui Yi,Bowen Yi,Chenxiao Yu,Yan Liu*

Main category: cs.AI

TL;DR: JobRec：通过约束双视角推理解耦偏好与资格的生成式职位推荐框架


<details>
  <summary>Details</summary>
Motivation: 现有职位推荐方法将候选人的主观偏好和雇主的客观资格要求混为一谈，导致在招聘漏斗审查下的监督混淆，限制了策略可控性

Method: 提出JobRec框架：1)统一语义对齐模式将候选人和职位属性对齐到结构化语义层；2)两阶段协同训练策略学习解耦的专家分别推断偏好和资格；3)基于拉格朗日的策略对齐模块在明确资格要求下优化推荐

Result: 实验表明JobRec持续优于强基线，并为策略感知的专业匹配提供改进的可控性

Conclusion: JobRec通过解耦偏好与资格的双视角推理，解决了传统职位推荐中的监督混淆问题，实现了更可控的推荐策略

Abstract: Professional job recommendation involves a complex bipartite matching process that must reconcile a candidate's subjective preference with an employer's objective qualification. While Large Language Models (LLMs) are well-suited for modeling the rich semantics of resumes and job descriptions, existing paradigms often collapse these two decision dimensions into a single interaction signal, yielding confounded supervision under recruitment-funnel censoring and limiting policy controllability. To address these challenges, We propose JobRec, a generative job recommendation framework for de-conflating preference and qualification via constrained dual-perspective reasoning. JobRec introduces a Unified Semantic Alignment Schema that aligns candidate and job attributes into structured semantic layers, and a Two-Stage Cooperative Training Strategy that learns decoupled experts to separately infer preference and qualification. Building on these experts, a Lagrangian-based Policy Alignment module optimizes recommendations under explicit eligibility requirements, enabling controllable trade-offs. To mitigate data scarcity, we construct a synthetic dataset refined by experts. Experiments show that JobRec consistently outperforms strong baselines and provides improved controllability for strategy-aware professional matching.

</details>


### [34] [Risky-Bench: Probing Agentic Safety Risks under Real-World Deployment](https://arxiv.org/abs/2602.03100)
*Jingnan Zheng,Yanzhen Luo,Jingjun Xu,Bingnan Liu,Yuxin Chen,Chenhang Cui,Gelei Deng,Chaochao Lu,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: Risky-Bench：一个基于真实世界部署的系统化智能体安全评估框架，通过领域无关的安全原则和上下文感知的安全标准来评估智能体在复杂环境中的安全风险。


<details>
  <summary>Details</summary>
Motivation: 现有智能体安全评估方法存在局限性：1）依赖针对特定智能体设置的风险导向任务，覆盖安全风险空间有限；2）无法评估智能体在复杂真实世界部署中长期、交互式任务执行中的安全行为；3）对特定智能体设置的专门化限制了跨不同智能体配置的适应性。

Method: 提出Risky-Bench框架：1）围绕领域无关的安全原则组织评估；2）推导上下文感知的安全标准来界定安全空间；3）在不同威胁假设下通过现实任务执行系统评估安全风险；4）作为结构化评估管道，可适应不同部署场景构建环境特定的安全评估。

Result: 在生活辅助智能体设置中应用Risky-Bench，发现在现实执行条件下，最先进的智能体存在重大安全风险。该框架不仅限于生活辅助场景，可适应其他部署设置，提供可扩展的智能体安全评估方法。

Conclusion: Risky-Bench提供了一个系统化、可扩展的智能体安全评估框架，能够更全面地评估智能体在复杂真实世界部署中的安全风险，弥补了现有评估方法的不足，为智能体安全评估提供了新的方法论。

Abstract: Large Language Models (LLMs) are increasingly deployed as agents that operate in real-world environments, introducing safety risks beyond linguistic harm. Existing agent safety evaluations rely on risk-oriented tasks tailored to specific agent settings, resulting in limited coverage of safety risk space and failing to assess agent safety behavior during long-horizon, interactive task execution in complex real-world deployments. Moreover, their specialization to particular agent settings limits adaptability across diverse agent configurations. To address these limitations, we propose Risky-Bench, a framework that enables systematic agent safety evaluation grounded in real-world deployment. Risky-Bench organizes evaluation around domain-agnostic safety principles to derive context-aware safety rubrics that delineate safety space, and systematically evaluates safety risks across this space through realistic task execution under varying threat assumptions. When applied to life-assist agent settings, Risky-Bench uncovers substantial safety risks in state-of-the-art agents under realistic execution conditions. Moreover, as a well-structured evaluation pipeline, Risky-Bench is not confined to life-assist scenarios and can be adapted to other deployment settings to construct environment-specific safety evaluations, providing an extensible methodology for agent safety assessment.

</details>


### [35] [Understanding Multi-Agent LLM Frameworks: A Unified Benchmark and Experimental Analysis](https://arxiv.org/abs/2602.03128)
*Abdelghny Orogat,Ana Rostam,Essam Mansour*

Main category: cs.AI

TL;DR: MAFBench是一个用于评估多智能体LLM框架的统一基准测试套件，研究发现框架架构选择对系统性能有巨大影响，延迟可增加100倍以上，规划准确率下降30%，协调成功率从90%降至30%以下。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体LLM框架广泛使用，但不同框架的架构设计对系统性能的影响缺乏系统研究。现有基准测试只关注单一能力，缺乏标准化的框架级评估，无法隔离架构选择对性能的影响。

Method: 提出多智能体LLM框架的架构分类法，开发MAFBench统一评估套件，集成现有基准测试到标准化执行流程中，对多个流行框架进行受控实证研究。

Result: 框架级设计选择单独就能导致延迟增加100倍以上，规划准确率下降30%，协调成功率从90%以上降至30%以下。不同框架在编排开销、内存行为、规划、专业化和协调等方面表现差异显著。

Conclusion: 多智能体LLM框架的架构选择对系统性能有决定性影响。研究提供了具体的架构设计原则和框架选择指导，并指出了未来研究方向，强调需要更系统化的框架评估方法。

Abstract: Multi-agent LLM frameworks are widely used to accelerate the development of agent systems powered by large language models (LLMs). These frameworks impose distinct architectural structures that govern how agents interact, store information, and coordinate tasks. However, their impact on system performance remains poorly understood. This gap is critical, as architectural choices alone can induce order-of-magnitude differences in latency and throughput, as well as substantial variation in accuracy and scalability. Addressing this challenge requires (i) jointly evaluating multiple capabilities, such as orchestration overhead, memory behavior, planning, specialization, and coordination, and (ii) conducting these evaluations under controlled, framework-level conditions to isolate architectural effects. Existing benchmarks focus on individual capabilities and lack standardized framework-level evaluation. We address these limitations by (i) introducing an architectural taxonomy for systematically comparing multi-agent LLM frameworks along fundamental dimensions, and (ii) developing MAFBench, a unified evaluation suite that integrates existing benchmarks under a standardized execution pipeline. Using MAFBench, we conduct a controlled empirical study across several widely used frameworks. Our results show that framework-level design choices alone can increase latency by over 100x, reduce planning accuracy by up to 30%, and lower coordination success from above 90% to below 30%. Finally, we translate our findings into concrete architectural design principles and framework selection guidance, and outline promising future research directions.

</details>


### [36] [General Agents Contain World Models, even under Partial Observability and Stochasticity](https://arxiv.org/abs/2602.03146)
*Santiago Cifuentes*

Main category: cs.AI

TL;DR: 本文扩展了先前关于智能体必须学习环境模型的理论，从确定性、完全可观测环境推广到随机性、部分可观测环境，证明随机化智能体也无法避免学习环境模型。


<details>
  <summary>Details</summary>
Motivation: 先前研究证明在特定框架下，几乎最优且通用的确定性智能体必然包含足够的环境知识，但该结果依赖于智能体确定性和环境完全可观测的假设。本文旨在移除这两个限制条件。

Method: 通过理论扩展，将定理推广到在部分可观测环境中运行的随机性智能体，同时通过弱化通用性概念来加强结果。

Result: 证明了随机性智能体也无法避免通过学习随机化来学习其环境，并且更弱的智能体已经包含其操作环境的世界模型。

Conclusion: 即使在随机性和部分可观测性的更一般条件下，智能体仍然必须学习环境模型，这为理解智能体能力提供了更坚实的理论基础。

Abstract: Deciding whether an agent possesses a model of its surrounding world is a fundamental step toward understanding its capabilities and limitations. In [10], it was shown that, within a particular framework, every almost optimal and general agent necessarily contains sufficient knowledge of its environment to allow an approximate reconstruction of it by querying the agent as a black box. This result relied on the assumptions that the agent is deterministic and that the environment is fully observable.
  In this work, we remove both assumptions by extending the theorem to stochastic agents operating in partially observable environments. Fundamentally, this shows that stochastic agents cannot avoid learning their environment through the usage of randomization. We also strengthen the result by weakening the notion of generality, proving that less powerful agents already contain a model of the world in which they operate.

</details>


### [37] [Enhancing Foundation VLM Robustness to Missing Modality: Scalable Diffusion for Bi-directional Feature Restoration](https://arxiv.org/abs/2602.03151)
*Wei Dai,Haoyu Wang,Honghao Chang,Lijun He,Fan Li,Jian Sun,Haixia Bi*

Main category: cs.AI

TL;DR: 提出一种用于视觉语言模型（VLM）的通用缺失模态恢复策略，通过增强扩散模型作为可插拔模块，结合动态模态门控和跨模态互学习机制，有效恢复缺失特征并保持模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前VLM在推理时假设完整模态输入，但在某些模态缺失时性能急剧下降。现有方法存在两个困境：基于提示的方法难以恢复缺失的关键特征并损害VLM泛化；基于插补的方法缺乏有效指导，容易生成语义无关的噪声。如何在恢复精确语义的同时保持VLM泛化能力仍具挑战性。

Method: 提出通用缺失模态恢复策略，采用增强扩散模型作为可插拔的中阶段训练模块。引入两个关键创新：1）动态模态门控，自适应利用条件特征引导生成语义一致的特征；2）跨模态互学习机制，桥接双编码器的语义空间实现双向对齐。

Result: 在基准数据集上的零样本评估表明，该方法优于现有基线方法。大量实验和消融研究证实，该模型在缺失模态场景下是VLM的鲁棒且可扩展的扩展，确保在不同缺失率和环境下的可靠性。

Conclusion: 提出的通用缺失模态恢复策略通过增强扩散模型和创新的门控与互学习机制，有效解决了VLM在模态缺失时的性能下降问题，为VLM在现实世界不完整数据场景下的应用提供了可靠解决方案。

Abstract: Vision Language Models (VLMs) typically assume complete modality input during inference. However, their effectiveness drops sharply when certain modalities are unavailable or incomplete. Current research primarily faces two dilemmas: Prompt-based methods struggle to restore missing yet indispensable features and impair generalization of VLMs. Imputation-based approaches, lacking effective guidance, are prone to generating semantically irrelevant noise. Restoring precise semantics while sustaining VLM generalization remains challenging. Therefore, we propose a general missing modality restoration strategy in this paper. We introduce an enhanced diffusion model as a pluggable mid-stage training module to effectively restore missing features. Our strategy introduces two key innovations: (I) Dynamic Modality Gating, which adaptively leverages conditional features to steer the generation of semantically consistent features; (II) Cross-Modal Mutual Learning mechanism, which bridges the semantic spaces of dual encoders to achieve bidirectional alignment. Zero-shot evaluations across benchmark datasets demonstrate that our approach outperforms existing baseline methods. Extensive experiments and ablation studies confirm our model as a robust and scalable extension for VLMs in missing modality scenarios, ensuring reliability across diverse missing rates and environments. Our code and models will be publicly available.

</details>


### [38] [VALUEFLOW: Toward Pluralistic and Steerable Value-based Alignment in Large Language Models](https://arxiv.org/abs/2602.03160)
*Woojin Kim,Sieun Hyeon,Jusang Oh,Jaeyoung Do*

Main category: cs.AI

TL;DR: VALUEFLOW：首个统一框架，通过分层值嵌入、大规模值强度数据库和锚点评估器，实现LLM值提取、评估和强度可控引导


<details>
  <summary>Details</summary>
Motivation: 当前LLM对齐方法存在三个主要问题：1）偏好方法无法捕捉深层动机原则；2）值提取忽略层次结构；3）评估只能检测存在性而无法量化强度；4）LLM在可控强度下的引导性理解不足

Method: 提出VALUEFLOW框架，包含三个组件：1）HIVES分层值嵌入空间，捕捉理论和跨理论值结构；2）VIDB值强度数据库，通过基于排序的聚合获得强度估计的大规模值标注文本；3）锚点评估器，通过将模型输出与VIDB面板排序产生一致的强度分数

Result: 在10个模型和4个值理论的大规模研究中，识别了引导性的不对称性和多值控制的组合规律，建立了评估和控制值强度的可扩展基础设施

Conclusion: VALUEFLOW为LLM的多元对齐提供了可扩展的基础设施，通过强度校准控制推进了LLM的多元价值对齐

Abstract: Aligning Large Language Models (LLMs) with the diverse spectrum of human values remains a central challenge: preference-based methods often fail to capture deeper motivational principles. Value-based approaches offer a more principled path, yet three gaps persist: extraction often ignores hierarchical structure, evaluation detects presence but not calibrated intensity, and the steerability of LLMs at controlled intensities remains insufficiently understood. To address these limitations, we introduce VALUEFLOW, the first unified framework that spans extraction, evaluation, and steering with calibrated intensity control. The framework integrates three components: (i) HIVES, a hierarchical value embedding space that captures intra- and cross-theory value structure; (ii) the Value Intensity DataBase (VIDB), a large-scale resource of value-labeled texts with intensity estimates derived from ranking-based aggregation; and (iii) an anchor-based evaluator that produces consistent intensity scores for model outputs by ranking them against VIDB panels. Using VALUEFLOW, we conduct a comprehensive large-scale study across ten models and four value theories, identifying asymmetries in steerability and composition laws for multi-value control. This paper establishes a scalable infrastructure for evaluating and controlling value intensity, advancing pluralistic alignment of LLMs.

</details>


### [39] [Beyond Quantity: Trajectory Diversity Scaling for Code Agents](https://arxiv.org/abs/2602.03219)
*Guhong Chen,Chenghao Sun,Cheng Fu,Qiyao Wang,Zhihong Huang,Chaopeng Wei,Guangxu Chen,Feiteng Fang,Ahmadreza Argha,Bing Zhao,Xander Xu,Qi Han,Hamid Alinejad-Rokny,Qiang Qu,Binhua Li,Shiwen Ni,Min Yang,Hu Wei,Yongbin Li*

Main category: cs.AI

TL;DR: TDScaling是一个基于轨迹多样性扩展的代码智能体数据合成框架，通过增加轨迹多样性而非单纯增加数据量来提升性能，在固定训练预算下实现更好的性能-成本权衡。


<details>
  <summary>Details</summary>
Motivation: 随着代码大语言模型通过MCP协议演变为工具交互智能体，其泛化能力受到低质量合成数据和数量扩展收益递减的限制。数量为中心的扩展存在早期瓶颈，未能充分利用轨迹数据。

Method: 提出了TDScaling框架，包含四个创新：1) 业务集群机制捕捉真实服务逻辑依赖；2) 蓝图驱动的多智能体范式确保轨迹连贯性；3) 自适应进化机制使用领域熵、推理模式熵和累积动作复杂度引导合成朝向长尾场景；4) 沙盒化代码工具防止内在编码能力灾难性遗忘。

Result: 在通用工具使用基准(BFCL, tau^2-Bench)和代码智能体任务(RebenchT, CodeCI, BIRD)上的实验表明，TDScaling实现了双赢：既提升了工具使用泛化能力，又增强了内在编码能力。计划发布包含30,000+工具集群的完整代码库和合成数据集。

Conclusion: 通过轨迹多样性扩展而非数量扩展，TDScaling框架在固定训练预算下实现了更好的性能-成本权衡，解决了代码智能体训练中的数据质量和多样性瓶颈问题。

Abstract: As code large language models (LLMs) evolve into tool-interactive agents via the Model Context Protocol (MCP), their generalization is increasingly limited by low-quality synthetic data and the diminishing returns of quantity scaling. Moreover, quantity-centric scaling exhibits an early bottleneck that underutilizes trajectory data. We propose TDScaling, a Trajectory Diversity Scaling-based data synthesis framework for code agents that scales performance through diversity rather than raw volume. Under a fixed training budget, increasing trajectory diversity yields larger gains than adding more trajectories, improving the performance-cost trade-off for agent training. TDScaling integrates four innovations: (1) a Business Cluster mechanism that captures real-service logical dependencies; (2) a blueprint-driven multi-agent paradigm that enforces trajectory coherence; (3) an adaptive evolution mechanism that steers synthesis toward long-tail scenarios using Domain Entropy, Reasoning Mode Entropy, and Cumulative Action Complexity to prevent mode collapse; and (4) a sandboxed code tool that mitigates catastrophic forgetting of intrinsic coding capabilities. Experiments on general tool-use benchmarks (BFCL, tau^2-Bench) and code agent tasks (RebenchT, CodeCI, BIRD) demonstrate a win-win outcome: TDScaling improves both tool-use generalization and inherent coding proficiency. We plan to release the full codebase and the synthesized dataset (including 30,000+ tool clusters) upon publication.

</details>


### [40] [TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking](https://arxiv.org/abs/2602.03224)
*Yu Cheng,Jiuan Zhou,Yongkang Hu,Yihang Chen,Huichi Zhou,Mingang Chen,Zhizhong Zhang,Kun Shao,Yuan Xie,Zhaoxia Yin*

Main category: cs.AI

TL;DR: 论文提出TAME框架解决智能体在任务演化过程中记忆错误演化导致安全性下降的问题，通过双记忆演化机制在保持任务性能的同时提升可信度。


<details>
  <summary>Details</summary>
Motivation: 智能体在测试时通过记忆演化积累经验是实现AGI的关键范式，但即使在良性任务演化过程中，智能体的安全对齐仍然脆弱，这种现象被称为"智能体记忆错误演化"。现有研究缺乏对这一现象的评估方法。

Method: 提出TAME双记忆演化框架：1) 分别演化执行器记忆（通过提炼可泛化方法提升任务性能）和评估器记忆（基于历史反馈优化安全性和任务效用评估）；2) 建立包含记忆过滤、草稿生成、可信度精炼、执行和双轨记忆更新的闭环流程。

Result: 构建了Trust-Memevo基准评估良性任务演化中的多维度可信度，发现各种任务领域和评估设置中可信度普遍下降。TAME框架实验证明能够缓解记忆错误演化，在可信度和任务性能上实现联合提升。

Conclusion: TAME框架通过分离演化执行器记忆和评估器记忆，在保持任务效用的同时保护可信度，为解决智能体记忆错误演化问题提供了有效方案，为实现安全的AGI发展提供了重要参考。

Abstract: Test-time evolution of agent memory serves as a pivotal paradigm for achieving AGI by bolstering complex reasoning through experience accumulation. However, even during benign task evolution, agent safety alignment remains vulnerable-a phenomenon known as Agent Memory Misevolution. To evaluate this phenomenon, we construct the Trust-Memevo benchmark to assess multi-dimensional trustworthiness during benign task evolution, revealing an overall decline in trustworthiness across various task domains and evaluation settings. To address this issue, we propose TAME, a dual-memory evolutionary framework that separately evolves executor memory to improve task performance by distilling generalizable methodologies, and evaluator memory to refine assessments of both safety and task utility based on historical feedback. Through a closed loop of memory filtering, draft generation, trustworthy refinement, execution, and dual-track memory updating, TAME preserves trustworthiness without sacrificing utility. Experiments demonstrate that TAME mitigates misevolution, achieving a joint improvement in both trustworthiness and task performance.

</details>


### [41] [The Necessity of a Unified Framework for LLM-Based Agent Evaluation](https://arxiv.org/abs/2602.03238)
*Pengyu Zhu,Li Sun,Philip S. Yu,Sen Su*

Main category: cs.AI

TL;DR: 提出标准化智能体评估框架，解决当前评估中系统提示、工具配置、环境动态等混杂因素导致的公平性和可复现性问题


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型智能体评估存在严重问题：1) 评估结果受系统提示、工具配置、环境动态等混杂因素影响；2) 缺乏标准化框架导致不同研究者的评估方法碎片化；3) 环境数据不标准导致错误难以追踪和结果不可复现；4) 这种缺乏标准化带来了不公平性和不透明性

Method: 提出一个统一的智能体评估框架，旨在标准化评估过程，包括：1) 标准化系统提示和工具配置；2) 统一环境数据规范；3) 建立可追踪的错误分析机制；4) 确保评估结果的可复现性

Result: 论文提出了一个标准化智能体评估的提案，但尚未展示具体实施结果。该框架旨在为领域提供公平、透明、可复现的评估标准

Conclusion: 标准化智能体评估框架对于该领域的严谨发展至关重要。提出的统一评估方案将解决当前评估中的混杂因素、不公平性和不透明性问题，促进智能体技术的可靠进步

Abstract: With the advent of Large Language Models (LLMs), general-purpose agents have seen fundamental advancements. However, evaluating these agents presents unique challenges that distinguish them from static QA benchmarks. We observe that current agent benchmarks are heavily confounded by extraneous factors, including system prompts, toolset configurations, and environmental dynamics. Existing evaluations often rely on fragmented, researcher-specific frameworks where the prompt engineering for reasoning and tool usage varies significantly, making it difficult to attribute performance gains to the model itself. Additionally, the lack of standardized environmental data leads to untraceable errors and non-reproducible results. This lack of standardization introduces substantial unfairness and opacity into the field. We propose that a unified evaluation framework is essential for the rigorous advancement of agent evaluation. To this end, we introduce a proposal aimed at standardizing agent evaluation.

</details>


### [42] [Accordion-Thinking: Self-Regulated Step Summaries for Efficient and Readable LLM Reasoning](https://arxiv.org/abs/2602.03249)
*Zhicheng Yang,Zhijiang Guo,Yinya Huang,Yongxin Wang,Wenlei Shi,Yiwei Wang,Xiaodan Liang,Jing Tang*

Main category: cs.AI

TL;DR: Accordion-Thinking框架让LLMs学会通过动态总结来自我调节推理步骤的粒度，实现推理上下文的压缩，在保持准确性的同时显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 传统的长链式思维推理虽然能提升推理能力，但面临KV缓存线性增长和注意力复杂度二次方增长的实践限制，需要更高效的推理机制。

Method: 提出Accordion-Thinking框架，让LLMs学习通过动态总结来调节推理步骤粒度，采用Fold推理模式定期总结思维过程并丢弃历史思考，使用强化学习进一步激励这种能力。

Result: 模型学会了将关键推理信息编码到紧凑的总结中，Fold模式与完整Unfold模式之间的准确率差距在训练过程中逐渐缩小直至消失，在48GB GPU配置下实现了3倍吞吐量同时保持准确性。

Conclusion: 通过学习自我压缩，LLMs能够在最小化依赖token开销的情况下处理复杂推理任务而不影响解决方案质量，同时结构化的步骤总结提供了人类可读的推理过程记录。

Abstract: Scaling test-time compute via long Chain-ofThought unlocks remarkable gains in reasoning capabilities, yet it faces practical limits due to the linear growth of KV cache and quadratic attention complexity. In this paper, we introduce Accordion-Thinking, an end-to-end framework where LLMs learn to self-regulate the granularity of the reasoning steps through dynamic summarization. This mechanism enables a Fold inference mode, where the model periodically summarizes its thought process and discards former thoughts to reduce dependency on historical tokens. We apply reinforcement learning to incentivize this capability further, uncovering a critical insight: the accuracy gap between the highly efficient Fold mode and the exhaustive Unfold mode progressively narrows and eventually vanishes over the course of training. This phenomenon demonstrates that the model learns to encode essential reasoning information into compact summaries, achieving effective compression of the reasoning context. Our Accordion-Thinker demonstrates that with learned self-compression, LLMs can tackle complex reasoning tasks with minimal dependency token overhead without compromising solution quality, and it achieves a 3x throughput while maintaining accuracy on a 48GB GPU memory configuration, while the structured step summaries provide a human-readable account of the reasoning process.

</details>


### [43] [LPS-Bench: Benchmarking Safety Awareness of Computer-Use Agents in Long-Horizon Planning under Benign and Adversarial Scenarios](https://arxiv.org/abs/2602.03255)
*Tianyu Chen,Chujia Hu,Ge Gao,Dongrui Liu,Xia Hu,Wenjie Wang*

Main category: cs.AI

TL;DR: LPS-Bench是一个评估基于MCP的计算机使用代理在长时程任务中规划时安全意识的基准测试，涵盖7个任务领域、9种风险类型的65个场景，揭示现有代理在安全行为维护方面的严重不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注短时程或GUI任务，评估执行时错误但忽略了规划时风险预测能力。计算机使用代理面临模糊指令触发有害操作和对抗性用户操纵工具执行的风险，需要专门的基准来评估规划时安全意识。

Method: 提出LPS-Bench基准，包含65个场景覆盖7个任务领域和9种风险类型；采用多代理自动化管道进行可扩展数据生成；使用LLM-as-a-judge评估协议通过规划轨迹评估安全意识。

Result: 实验显示现有计算机使用代理在维持安全行为方面存在严重缺陷；分析了风险并提出了改进MCP-based CUA系统长时程规划安全的缓解策略。

Conclusion: LPS-Bench填补了规划时安全意识评估的空白，揭示了现有系统的安全缺陷，为改进长时程规划安全提供了基准和分析框架。

Abstract: Computer-use agents (CUAs) that interact with real computer systems can perform automated tasks but face critical safety risks. Ambiguous instructions may trigger harmful actions, and adversarial users can manipulate tool execution to achieve malicious goals. Existing benchmarks mostly focus on short-horizon or GUI-based tasks, evaluating on execution-time errors but overlooking the ability to anticipate planning-time risks. To fill this gap, we present LPS-Bench, a benchmark that evaluates the planning-time safety awareness of MCP-based CUAs under long-horizon tasks, covering both benign and adversarial interactions across 65 scenarios of 7 task domains and 9 risk types. We introduce a multi-agent automated pipeline for scalable data generation and adopt an LLM-as-a-judge evaluation protocol to assess safety awareness through the planning trajectory. Experiments reveal substantial deficiencies in existing CUAs' ability to maintain safe behavior. We further analyze the risks and propose mitigation strategies to improve long-horizon planning safety in MCP-based CUA systems. We open-source our code at https://github.com/tychenn/LPS-Bench.

</details>


### [44] [CSR-Bench: A Benchmark for Evaluating the Cross-modal Safety and Reliability of MLLMs](https://arxiv.org/abs/2602.03263)
*Yuxuan Liu,Yuntian Shi,Kun Wang,Haoting Shen,Kun Yang*

Main category: cs.AI

TL;DR: CSR-Bench是一个评估多模态大语言模型跨模态可靠性的基准测试，通过四种压力测试模式（安全、过度拒绝、偏见、幻觉）覆盖61种细粒度类型，发现现有模型存在系统性跨模态对齐差距。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型虽然支持文本和图像交互，但其安全行为可能由单模态捷径驱动而非真正的联合意图理解。需要评估模型在需要整合图像文本解释的场景中的可靠性。

Method: 构建CSR-Bench基准，包含四种压力测试交互模式：安全、过度拒绝、偏见、幻觉，覆盖61种细粒度类型。每个实例都需要整合图像文本解释，并提供配对的纯文本控制组以诊断模态引起的行为变化。评估了16个最先进的多模态大语言模型。

Result: 观察到系统性跨模态对齐差距：模型表现出弱安全意识、在干扰下强烈的语言主导性，以及从纯文本控制组到多模态输入的性能持续下降。还发现减少过度拒绝与保持安全非歧视行为之间存在明显权衡，表明某些表面安全改进可能来自拒绝导向的启发式方法而非稳健的意图理解。

Conclusion: 多模态大语言模型存在跨模态可靠性问题，需要更稳健的联合意图理解机制。CSR-Bench为评估和改进多模态模型的安全性和可靠性提供了重要工具。

Abstract: Multimodal large language models (MLLMs) enable interaction over both text and images, but their safety behavior can be driven by unimodal shortcuts instead of true joint intent understanding. We introduce CSR-Bench, a benchmark for evaluating cross-modal reliability through four stress-testing interaction patterns spanning Safety, Over-rejection, Bias, and Hallucination, covering 61 fine-grained types. Each instance is constructed to require integrated image-text interpretation, and we additionally provide paired text-only controls to diagnose modality-induced behavior shifts. We evaluate 16 state-of-the-art MLLMs and observe systematic cross-modal alignment gaps. Models show weak safety awareness, strong language dominance under interference, and consistent performance degradation from text-only controls to multimodal inputs. We also observe a clear trade-off between reducing over-rejection and maintaining safe, non-discriminatory behavior, suggesting that some apparent safety gains may come from refusal-oriented heuristics rather than robust intent understanding. WARNING: This paper contains unsafe contents.

</details>


### [45] [Agentic Proposing: Enhancing Large Language Model Reasoning via Compositional Skill Synthesis](https://arxiv.org/abs/2602.03279)
*Zhengbo Jiao,Shaobo Wang,Zifan Zhang,Xuan Ren,Wei Wang,Bing Zhao,Hu Wei,Linfeng Zhang*

Main category: cs.AI

TL;DR: 提出Agentic Proposing框架，通过目标驱动的序列决策过程合成高质量、可验证的复杂推理数据集，训练出的求解器在数学、编程和科学领域表现优异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的复杂推理能力提升需要高质量、可验证的数据集，但人工标注成本高且难以扩展。现有合成方法面临两难：保持结构有效性会限制问题复杂度，而增加难度又会导致不一致或不可解实例。

Method: 提出Agentic Proposing框架，将问题合成建模为目标驱动的序列决策过程，让专门代理动态选择和组合模块化推理技能。通过内部反思和工具使用的迭代工作流，使用多粒度策略优化（MGPO）开发Agentic-Proposer-4B，生成数学、编程和科学领域的高精度可验证训练轨迹。

Result: 基于代理合成数据训练的下游求解器显著优于领先基线，并展现出强大的跨领域泛化能力。仅用11,000条合成轨迹训练的30B求解器在AIME25上达到91.6%的SOTA准确率，媲美GPT-5等前沿专有模型。

Conclusion: 少量高质量合成信号可以有效替代海量人工标注数据集，证明通过目标驱动的代理合成方法能够生成高质量、可验证的复杂推理训练数据，显著提升模型性能。

Abstract: Advancing complex reasoning in large language models relies on high-quality, verifiable datasets, yet human annotation remains cost-prohibitive and difficult to scale. Current synthesis paradigms often face a recurring trade-off: maintaining structural validity typically restricts problem complexity, while relaxing constraints to increase difficulty frequently leads to inconsistent or unsolvable instances. To address this, we propose Agentic Proposing, a framework that models problem synthesis as a goal-driven sequential decision process where a specialized agent dynamically selects and composes modular reasoning skills. Through an iterative workflow of internal reflection and tool-use, we develop the Agentic-Proposer-4B using Multi-Granularity Policy Optimization (MGPO) to generate high-precision, verifiable training trajectories across mathematics, coding, and science. Empirical results demonstrate that downstream solvers trained on agent-synthesized data significantly outperform leading baselines and exhibit robust cross-domain generalization. Notably, a 30B solver trained on only 11,000 synthesized trajectories achieves a state-of-the-art 91.6% accuracy on AIME25, rivaling frontier-scale proprietary models such as GPT-5 and proving that a small volume of high-quality synthetic signals can effectively substitute for massive human-curated datasets.

</details>


### [46] [MeetBench-XL: Calibrated Multi-Dimensional Evaluation and Learned Dual-Policy Agents for Real-Time Meetings](https://arxiv.org/abs/2602.03285)
*Yuelin Hu,Jun Xu,Bingcong Lu,Zhengxue Cheng,Hongwei Hu,Ronghua Wu,Li Song*

Main category: cs.AI

TL;DR: 提出了MeetAll数据集和MeetMaster XL智能体框架，用于企业会议环境中的AI助手任务，包括快速事实核查和跨会议分析，解决了现有基准测试与企业实际工作流程不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 企业会议环境需要AI助手处理多样化操作任务，但现有会议基准主要关注简化的问答任务，无法反映真实的企业工作流程。真实的企业查询来自多方利益相关者协作，跨越长时间上下文，需要工具增强推理，且面临严格的延迟、成本和隐私约束。

Method: 1. 引入MeetAll双语多模态语料库，来自231个企业会议共140小时，使用企业验证的协议注入问题；2. 提出MeetBench XL多维评估协议，衡量事实保真度、意图对齐、响应效率、结构清晰度和完整性；3. 提出MeetMaster XL双策略智能体，联合优化快速和慢速推理路径之间的查询路由以及工具调用。

Result: 实验表明，与商业系统相比，MeetMaster XL取得了持续的性能提升。轻量级分类器能够以最小开销实现准确路由，在质量-延迟权衡上优于单模型基线。通过消融实验、鲁棒性测试和真实世界部署案例研究支持了这些结果。

Conclusion: 该工作通过接地气的数据集和学习型智能体框架，解决了企业会议AI助手的关键需求。MeetAll数据集和MeetMaster XL框架为企业环境中的AI助手提供了更贴近实际工作流程的解决方案，在质量、延迟和成本方面取得了更好的平衡。

Abstract: Enterprise meeting environments require AI assistants that handle diverse operational tasks, from rapid fact checking during live discussions to cross meeting analysis for strategic planning, under strict latency, cost, and privacy constraints. Existing meeting benchmarks mainly focus on simplified question answering and fail to reflect real world enterprise workflows, where queries arise organically from multi stakeholder collaboration, span long temporal contexts, and require tool augmented reasoning.
  We address this gap through a grounded dataset and a learned agent framework. First, we introduce MeetAll, a bilingual and multimodal corpus derived from 231 enterprise meetings totaling 140 hours. Questions are injected using an enterprise informed protocol validated by domain expert review and human discriminability studies. Unlike purely synthetic benchmarks, this protocol is grounded in four enterprise critical dimensions: cognitive load, temporal context span, domain expertise, and actionable task execution, calibrated through interviews with stakeholders across finance, healthcare, and technology sectors.
  Second, we propose MeetBench XL, a multi dimensional evaluation protocol aligned with human judgment that measures factual fidelity, intent alignment, response efficiency, structural clarity, and completeness. Third, we present MeetMaster XL, a learned dual policy agent that jointly optimizes query routing between fast and slow reasoning paths and tool invocation, including retrieval, cross meeting aggregation, and web search. A lightweight classifier enables accurate routing with minimal overhead, achieving a superior quality latency tradeoff over single model baselines. Experiments against commercial systems show consistent gains, supported by ablations, robustness tests, and a real world deployment case study.Resources: https://github.com/huyuelin/MeetBench.

</details>


### [47] [Rejecting Arguments Based on Doubt in Structured Bipolar Argumentation](https://arxiv.org/abs/2602.03286)
*Michael A. Müller,Srdjan Vesic,Bruno Yun*

Main category: cs.AI

TL;DR: 提出结构化双极论证框架(SBAF)，允许基于怀疑拒绝论证，并提供语言扩展语义，介于可接受和完全语义之间


<details>
  <summary>Details</summary>
Motivation: 现有计算论证方法忽略两个重要哲学和语言学观点：1) 代理可以基于怀疑理性拒绝论证，不必接受所有可辩护论证；2) 有时更自然地考虑代理接受哪些句子或主张，而非哪些论证

Method: 定义结构化双极论证框架(SBAF)，其中论证由句子组成，包含攻击和支持关系。提供两种语义：1) 不强制接受所有被辩护论证的语义；2) 语言扩展语义，指定可接受的句子集合

Result: 提出的语义介于抽象论证的可接受和完全语义之间，能提供代理在辩论中的合理立场。该方法为现有方法提供新视角，如指定何时可忽略论证间支持关系，并证明演绎支持语义是本方法的特例

Conclusion: 新方法将哲学和语言学洞见融入计算论证，提供更灵活和自然的框架，允许基于怀疑的理性拒绝，并支持句子层面的语义分析

Abstract: This paper develops a new approach to computational argumentation that is informed by philosophical and linguistic views. Namely, it takes into account two ideas that have received little attention in the literature on computational argumentation: First, an agent may rationally reject an argument based on mere doubt, thus not all arguments they could defend must be accepted; and, second, that it is sometimes more natural to think in terms of which individual sentences or claims an agent accepts in a debate, rather than which arguments. In order to incorporate these two ideas into a computational approach, we first define the notion of structured bipolar argumentation frameworks (SBAFs), where arguments consist of sentences and we have both an attack and a support relation between them. Then, we provide semantics for SBAFs with two features: (1) Unlike with completeness-based semantics, our semantics do not force agents to accept all defended arguments. (2) In addition to argument extensions, which give acceptable sets of arguments, we also provide semantics for language extensions that specify acceptable sets of sentences. These semantics represent reasonable positions an agent might have in a debate. Our semantics lie between the admissible and complete semantics of abstract argumentation. Further, our approach can be used to provide a new perspective on existing approaches. For instance, we can specify the conditions under which an agent can ignore support between arguments (i.e. under which the use of abstract argumentation is warranted) and we show that deductive support semantics is a special case of our approach.

</details>


### [48] [Memora: A Harmonic Memory Representation Balancing Abstraction and Specificity](https://arxiv.org/abs/2602.03315)
*Menglin Xia,Xuchao Zhang,Shantanu Dixit,Paramaguru Harimurugan,Rujia Wang,Victor Ruhle,Robert Sim,Chetan Bansal,Saravan Rajmohan*

Main category: cs.AI

TL;DR: Memora是一种平衡抽象与具体性的记忆表示方法，通过抽象索引和锚点连接实现高效检索，在记忆扩展时保持推理效果。


<details>
  <summary>Details</summary>
Motivation: 现有代理记忆系统在扩展时面临抽象与具体性的权衡问题：抽象有助于规模化但会丢失细节，而细节对有效推理至关重要。需要一种能平衡两者的记忆表示方法。

Method: 提出Memora谐波记忆表示，包含：1）主要抽象索引具体记忆值；2）统一相关更新的记忆条目；3）线索锚点跨多方面扩展检索访问；4）连接相关记忆。基于此结构，采用主动利用记忆连接的检索策略。

Result: 理论上证明标准RAG和知识图谱记忆系统是Memora的特例。实证上在LoCoMo和LongMemEval基准上达到SOTA，在记忆扩展时展示更好的检索相关性和推理有效性。

Conclusion: Memora通过谐波记忆表示有效平衡抽象与具体性，为代理记忆系统提供可扩展且保持细节的解决方案，在记忆扩展时维持高质量推理能力。

Abstract: Agent memory systems must accommodate continuously growing information while supporting efficient, context-aware retrieval for downstream tasks. Abstraction is essential for scaling agent memory, yet it often comes at the cost of specificity, obscuring the fine-grained details required for effective reasoning. We introduce Memora, a harmonic memory representation that structurally balances abstraction and specificity. Memora organizes information via its primary abstractions that index concrete memory values and consolidate related updates into unified memory entries, while cue anchors expand retrieval access across diverse aspects of the memory and connect related memories. Building on this structure, we employ a retrieval policy that actively exploits these memory connections to retrieve relevant information beyond direct semantic similarity. Theoretically, we show that standard Retrieval-Augmented Generation (RAG) and Knowledge Graph (KG)-based memory systems emerge as special cases of our framework. Empirically, Memora establishes a new state-of-the-art on the LoCoMo and LongMemEval benchmarks, demonstrating better retrieval relevance and reasoning effectiveness as memory scales.

</details>


### [49] [MentalSeek-Dx: Towards Progressive Hypothetico-Deductive Reasoning for Real-world Psychiatric Diagnosis](https://arxiv.org/abs/2602.03340)
*Xiao Sun,Yuming Yang,Junnan Zhu,Jiang Zhong,Xinyu Zhou,Kaiwen Wei*

Main category: cs.AI

TL;DR: 提出了首个面向真实临床场景的精神障碍级别诊断基准MentalDx Bench，并开发了专门用于精神科诊断的LLM模型MentalSeek-Dx，通过监督轨迹构建和课程强化学习实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在精神健康评估中存在生态效度不足和诊断监督粒度不够的问题，限制了其临床应用。需要建立真实临床环境下的障碍级别诊断基准来弥合这一差距。

Method: 1) 构建MentalDx Bench基准：包含712份去标识化电子健康记录，由认证精神科医生按照ICD-11标准标注，覆盖16个诊断类别的76种障碍；2) 开发MentalSeek-Dx模型：通过监督轨迹构建和课程强化学习训练医学专用LLM，模拟临床假设-演绎推理过程。

Result: 评估18个LLM发现存在范式错位：在粗粒度诊断分类上表现良好，但在障碍级别诊断上系统性失败。MentalSeek-Dx在MentalDx Bench上达到SOTA性能，仅用14B参数。

Conclusion: MentalDx Bench揭示了LLM在精神科诊断中的范式错位问题，而MentalSeek-Dx通过模拟临床推理过程建立了可靠的诊断框架，为精神科AI应用提供了临床基础。

Abstract: Mental health disorders represent a burgeoning global public health challenge. While Large Language Models (LLMs) have demonstrated potential in psychiatric assessment, their clinical utility is severely constrained by benchmarks that lack ecological validity and fine-grained diagnostic supervision. To bridge this gap, we introduce \textbf{MentalDx Bench}, the first benchmark dedicated to disorder-level psychiatric diagnosis within real-world clinical settings. Comprising 712 de-identified electronic health records annotated by board-certified psychiatrists under ICD-11 guidelines, the benchmark covers 76 disorders across 16 diagnostic categories. Evaluation of 18 LLMs reveals a critical \textit{paradigm misalignment}: strong performance at coarse diagnostic categorization contrasts with systematic failure at disorder-level diagnosis, underscoring a gap between pattern-based modeling and clinical hypothetico-deductive reasoning. In response, we propose \textbf{MentalSeek-Dx}, a medical-specialized LLM trained to internalize this clinical reasoning process through supervised trajectory construction and curriculum-based reinforcement learning. Experiments on MentalDx Bench demonstrate that MentalSeek-Dx achieves state-of-the-art (SOTA) performance with only 14B parameters, establishing a clinically grounded framework for reliable psychiatric diagnosis.

</details>


### [50] [GFlowPO: Generative Flow Network as a Language Model Prompt Optimizer](https://arxiv.org/abs/2602.03358)
*Junmo Cho,Suhan Kim,Sangjune An,Minsu Kim,Dong Bok Lee,Heejun Lee,Sung Ju Hwang,Hae Beom Lee*

Main category: cs.AI

TL;DR: GFlowPO：一种基于生成流网络和动态记忆更新的概率提示优化框架，通过后验推断和样本高效探索提升提示搜索效果


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的提示优化方法存在样本效率低的问题，主要原因是依赖在线策略更新和固定分布的元提示采样。提示空间组合爆炸且奖励稀疏（目标语言模型评估昂贵），需要更高效的优化方法。

Method: 1. 使用离策略生成流网络目标微调轻量级提示语言模型，通过重用历史提示评估的回放训练策略实现样本高效探索；2. 引入动态记忆更新机制，从回放缓冲区注入多样化提示和从小优先级队列注入高性能提示，逐步集中搜索到高奖励区域。

Result: 在少样本文本分类、指令归纳基准测试和问答任务中，GFlowPO始终优于最近的离散提示优化基线方法。

Conclusion: GFlowPO通过将提示搜索构建为后验推断问题，结合生成流网络和动态记忆更新，实现了样本高效的提示优化，在各种任务中表现出优越性能。

Abstract: Finding effective prompts for language models (LMs) is critical yet notoriously difficult: the prompt space is combinatorially large, rewards are sparse due to expensive target-LM evaluation. Yet, existing RL-based prompt optimizers often rely on on-policy updates and a meta-prompt sampled from a fixed distribution, leading to poor sample efficiency. We propose GFlowPO, a probabilistic prompt optimization framework that casts prompt search as a posterior inference problem over latent prompts regularized by a meta-prompted reference-LM prior. In the first step, we fine-tune a lightweight prompt-LM with an off-policy Generative Flow Network (GFlowNet) objective, using a replay-based training policy that reuses past prompt evaluations to enable sample-efficient exploration. In the second step, we introduce Dynamic Memory Update (DMU), a training-free mechanism that updates the meta-prompt by injecting both (i) diverse prompts from a replay buffer and (ii) top-performing prompts from a small priority queue, thereby progressively concentrating the search process on high-reward regions. Across few-shot text classification, instruction induction benchmarks, and question answering tasks, GFlowPO consistently outperforms recent discrete prompt optimization baselines.

</details>


### [51] [Risk Awareness Injection: Calibrating Vision-Language Models for Safety without Compromising Utility](https://arxiv.org/abs/2602.03402)
*Mengxuan Wang,Yuxin Chen,Gang Xu,Tao He,Hongjie Jiang,Ming Li*

Main category: cs.AI

TL;DR: RAI是一个无需训练的轻量级安全校准框架，通过构建不安全原型子空间和对高风险视觉token进行针对性调制，恢复VLM对不安全内容的识别能力，有效防御多模态越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 现有VLM安全防御方法主要依赖安全微调或激进的token操作，训练成本高或严重损害模型实用性。研究发现LLM本身能识别文本不安全内容，但VLM中视觉输入会稀释风险信号，因此需要恢复VLM的LLM式风险识别能力。

Method: RAI从语言嵌入构建不安全原型子空间，对选定的高风险视觉token进行针对性调制，在跨模态特征空间中显式激活安全关键信号，恢复模型从视觉输入检测不安全内容的能力，同时保持原始token的语义完整性。

Result: 在多个越狱攻击和实用性基准测试中，RAI显著降低了攻击成功率，同时不影响任务性能，实现了安全性和实用性的平衡。

Conclusion: RAI提供了一种轻量级、无需训练的安全校准框架，通过放大VLM中的不安全信号来恢复LLM式的风险识别能力，有效防御多模态越狱攻击，同时保持模型的实用性。

Abstract: Vision language models (VLMs) extend the reasoning capabilities of large language models (LLMs) to cross-modal settings, yet remain highly vulnerable to multimodal jailbreak attacks. Existing defenses predominantly rely on safety fine-tuning or aggressive token manipulations, incurring substantial training costs or significantly degrading utility. Recent research shows that LLMs inherently recognize unsafe content in text, and the incorporation of visual inputs in VLMs frequently dilutes risk-related signals. Motivated by this, we propose Risk Awareness Injection (RAI), a lightweight and training-free framework for safety calibration that restores LLM-like risk recognition by amplifying unsafe signals in VLMs. Specifically, RAI constructs an Unsafe Prototype Subspace from language embeddings and performs targeted modulation on selected high-risk visual tokens, explicitly activating safety-critical signals within the cross-modal feature space. This modulation restores the model's LLM-like ability to detect unsafe content from visual inputs, while preserving the semantic integrity of original tokens for cross-modal reasoning. Extensive experiments across multiple jailbreak and utility benchmarks demonstrate that RAI substantially reduces attack success rate without compromising task performance.

</details>


### [52] [Feasible strategies for conflict resolution within intuitionistic fuzzy preference-based conflict situations](https://arxiv.org/abs/2602.03403)
*Guangming Lang,Mingchuan Shang,Mengjun Hu,Jie Zhou,Feng Xu*

Main category: cs.AI

TL;DR: 本文提出基于直觉模糊偏好的三支冲突分析模型，通过更细粒度的偏好描述和冲突度量，改进了传统三支冲突分析的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于偏好的冲突模型仅使用偏好、逆偏好和中性三种定性关系来描述代理对议题对的态度，这种粗糙的表示方式严重限制了捕捉冲突本质的能力。

Method: 引入直觉模糊偏好冲突情境概念，建立直觉模糊偏好冲突度量框架，构建代理对集合、代理集合和议题集合的三支划分模型，基于冲突函数构建相对损失函数计算阈值，并提出考虑调整幅度和冲突程度的可行策略调整机制。

Result: 开发了直觉模糊偏好冲突度量方法，构建了三支冲突分析模型，设计了阈值计算方法和可行策略调整算法，通过示例验证了模型的有效性。

Conclusion: 提出的直觉模糊偏好冲突分析模型能够更精细地描述代理态度，提供更有效的冲突分析和解决策略，扩展了三支冲突分析的理论框架和应用能力。

Abstract: In three-way conflict analysis, preference-based conflict situations characterize agents' attitudes towards issues by formally modeling their preferences over pairs of issues. However, existing preference-based conflict models rely exclusively on three qualitative relations, namely, preference, converse, and indifference, to describe agents' attitudes towards issue pairs, which significantly limits their capacity in capturing the essence of conflict. To overcome this limitation, we introduce the concept of an intuitionistic fuzzy preference-based conflict situation that captures agents' attitudes towards issue pairs with finer granularity than that afforded by classical preference-based models. Afterwards, we develop intuitionistic fuzzy preference-based conflict measures within this framework, and construct three-way conflict analysis models for trisecting the set of agent pairs, the agent set, and the issue set. Additionally, relative loss functions built on the proposed conflict functions are employed to calculate thresholds for three-way conflict analysis. Finally, we present adjustment mechanism-based feasible strategies that simultaneously account for both adjustment magnitudes and conflict degrees, together with an algorithm for constructing such feasible strategies, and provide an illustrative example to demonstrate the validity and effectiveness of the proposed model.

</details>


### [53] [DiscoverLLM: From Executing Intents to Discovering Them](https://arxiv.org/abs/2602.03429)
*Tae Soo Kim,Yoonjoo Lee,Jaesang Yu,John Joon Young Chung,Juho Kim*

Main category: cs.AI

TL;DR: 提出DiscoverLLM框架，训练LLM帮助用户发现和形成意图，通过自适应探索与收敛策略，在意图模糊时探索选项，明确时细化实现。


<details>
  <summary>Details</summary>
Motivation: 当前LLM处理模糊请求时通常直接询问用户意图，但当用户自己也不清楚想要什么时，这种询问方式会失效。用户需要观察和探索结果来发现自己的真实意图。

Method: 提出DiscoverLLM框架，包含新颖的用户模拟器，用层次化意图建模认知状态，意图随模型呈现相关选项而逐步具体化。具体化程度作为奖励信号，训练模型优化这一过程。模型学习自适应策略：意图模糊时发散探索选项，意图具体化时收敛细化实现。

Result: 在创意写作、技术写作和SVG绘图等交互基准测试中，DiscoverLLM实现超过10%的任务性能提升，同时减少高达40%的对话长度。在75人参与的用户研究中，相比基线方法提高了对话满意度和效率。

Conclusion: DiscoverLLM通过帮助用户发现和形成意图，有效解决了LLM处理模糊请求时的局限性，实现了更高效、更满意的交互体验。

Abstract: To handle ambiguous and open-ended requests, Large Language Models (LLMs) are increasingly trained to interact with users to surface intents they have not yet expressed (e.g., ask clarification questions). However, users are often ambiguous because they have not yet formed their intents: they must observe and explore outcomes to discover what they want. Simply asking "what kind of tone do you want?" fails when users themselves do not know. We introduce DiscoverLLM, a novel and generalizable framework that trains LLMs to help users form and discover their intents. Central to our approach is a novel user simulator that models cognitive state with a hierarchy of intents that progressively concretize as the model surfaces relevant options -- where the degree of concretization serves as a reward signal that models can be trained to optimize. Resulting models learn to collaborate with users by adaptively diverging (i.e., explore options) when intents are unclear, and converging (i.e., refine and implement) when intents concretize. Across proposed interactive benchmarks in creative writing, technical writing, and SVG drawing, DiscoverLLM achieves over 10% higher task performance while reducing conversation length by up to 40%. In a user study with 75 human participants, DiscoverLLM improved conversation satisfaction and efficiency compared to baselines.

</details>


### [54] [Ontology-to-tools compilation for executable semantic constraint enforcement in LLM agents](https://arxiv.org/abs/2602.03439)
*Xiaochi Zhou,Patrick Bulter,Changxuan Yang,Simon D. Rihm,Thitikarn Angkanaporn,Jethro Akroyd,Sebastian Mosbach,Markus Kraft*

Main category: cs.AI

TL;DR: 论文提出了一种将本体论编译为工具接口的机制，使大语言模型能够与形式化领域知识结合，通过可执行的本体语义约束指导LLM生成知识图谱实例。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型与形式化领域知识结合的问题，避免后验验证，通过本体语义约束直接指导LLM行为，减少手动模式设计和提示工程。

Method: 提出本体到工具编译机制，将本体规范编译为可执行工具接口；扩展TWA的语义代理组合框架，使用模型上下文协议（MCP）和代理实现生成模型、符号约束和外部资源的结构化交互；基于代理的工作流程将本体转换为本体感知工具，并迭代应用于从非结构化科学文本中提取、验证和修复结构化知识。

Result: 以金属有机多面体合成文献为例，展示了可执行本体语义如何指导LLM行为，减少手动模式设计和提示工程，建立了将形式化知识嵌入生成系统的通用范式。

Conclusion: 本体到工具编译机制为耦合大语言模型与形式化领域知识提供了原理性证明，通过可执行的本体语义约束指导LLM生成，建立了将形式化知识嵌入生成系统的通用范式。

Abstract: We introduce ontology-to-tools compilation as a proof-of-principle mechanism for coupling large language models (LLMs) with formal domain knowledge. Within The World Avatar (TWA), ontological specifications are compiled into executable tool interfaces that LLM-based agents must use to create and modify knowledge graph instances, enforcing semantic constraints during generation rather than through post-hoc validation. Extending TWA's semantic agent composition framework, the Model Context Protocol (MCP) and associated agents are integral components of the knowledge graph ecosystem, enabling structured interaction between generative models, symbolic constraints, and external resources. An agent-based workflow translates ontologies into ontology-aware tools and iteratively applies them to extract, validate, and repair structured knowledge from unstructured scientific text. Using metal-organic polyhedra synthesis literature as an illustrative case, we show how executable ontological semantics can guide LLM behaviour and reduce manual schema and prompt engineering, establishing a general paradigm for embedding formal knowledge into generative systems.

</details>


### [55] [CRL-VLA: Continual Vision-Language-Action Learning](https://arxiv.org/abs/2602.03445)
*Qixin Zeng,Shuo Zhang,Hongyin Zhang,Renjie Wang,Han Zhao,Libang Zhao,Runze Li,Donglin Wang,Chao Huang*

Main category: cs.AI

TL;DR: CRL-VLA：通过理论推导和双评论家架构解决VLA模型持续强化学习中的稳定性-可塑性权衡问题


<details>
  <summary>Details</summary>
Motivation: 在开放世界环境中，终身学习对具身智能体至关重要。虽然强化学习微调已成为VLA模型掌握灵巧操作的重要范式，但在持续强化学习场景中，平衡稳定性（保留旧技能）和可塑性（学习新技能）仍然是现有方法的重大挑战。

Method: 提出CRL-VLA框架，通过理论推导将稳定性-可塑性权衡与目标条件优势幅度和策略散度联系起来。采用非对称调节机制：约束先前任务的优势幅度，同时允许新任务上的受控增长。实现方式是通过具有新颖目标条件价值公式的双评论家架构，其中冻结评论家锚定语义一致性，可训练估计器驱动适应。

Result: 在LIBERO基准测试上的实验表明，CRL-VLA有效协调了这些冲突目标，在抗遗忘和向前适应方面均优于基线方法。

Conclusion: CRL-VLA为VLA模型的持续后训练提供了一个具有严格理论界限的框架，成功解决了持续强化学习中稳定性与可塑性的基本权衡问题，为终身机器人场景中的VLA模型部署提供了有前景的途径。

Abstract: Lifelong learning is critical for embodied agents in open-world environments, where reinforcement learning fine-tuning has emerged as an important paradigm to enable Vision-Language-Action (VLA) models to master dexterous manipulation through environmental interaction. Thus, Continual Reinforcement Learning (CRL) is a promising pathway for deploying VLA models in lifelong robotic scenarios, yet balancing stability (retaining old skills) and plasticity (learning new ones) remains a formidable challenge for existing methods. We introduce CRL-VLA, a framework for continual post-training of VLA models with rigorous theoretical bounds. We derive a unified performance bound linking the stability-plasticity trade-off to goal-conditioned advantage magnitude, scaled by policy divergence. CRL-VLA resolves this dilemma via asymmetric regulation: constraining advantage magnitudes on prior tasks while enabling controlled growth on new tasks. This is realized through a simple but effective dual-critic architecture with novel Goal-Conditioned Value Formulation (GCVF), where a frozen critic anchors semantic consistency and a trainable estimator drives adaptation. Experiments on the LIBERO benchmark demonstrate that CRL-VLA effectively harmonizes these conflicting objectives, outperforming baselines in both anti-forgetting and forward adaptation.

</details>


### [56] [The Dual Role of Abstracting over the Irrelevant in Symbolic Explanations: Cognitive Effort vs. Understanding](https://arxiv.org/abs/2602.03467)
*Zeynep G. Saribatur,Johannes Langer,Ute Schmid*

Main category: cs.AI

TL;DR: 研究形式化抽象（移除和聚类）如何影响人类推理性能和认知负荷，发现聚类提升理解，移除降低认知负荷


<details>
  <summary>Details</summary>
Motivation: AI系统输出难以理解，符号AI虽透明但原始逻辑痕迹带来高认知负荷，需要研究形式化抽象如何改善人类可理解的符号解释

Method: 使用答案集编程(ASP)作为形式框架，定义可抽象的不相关细节概念，通过认知实验让参与者基于ASP程序生成的解释对跨领域刺激进行分类

Result: 聚类细节显著提升参与者理解，移除细节显著降低认知负荷，支持抽象增强以人为中心的符号解释的假设

Conclusion: 形式化抽象（特别是聚类和移除）能有效提升符号AI解释的人类可理解性，降低认知负荷，改善人机交互

Abstract: Explanations are central to human cognition, yet AI systems often produce outputs that are difficult to understand. While symbolic AI offers a transparent foundation for interpretability, raw logical traces often impose a high extraneous cognitive load. We investigate how formal abstractions, specifically removal and clustering, impact human reasoning performance and cognitive effort. Utilizing Answer Set Programming (ASP) as a formal framework, we define a notion of irrelevant details to be abstracted over to obtain simplified explanations. Our cognitive experiments, in which participants classified stimuli across domains with explanations derived from an answer set program, show that clustering details significantly improve participants' understanding, while removal of details significantly reduce cognitive effort, supporting the hypothesis that abstraction enhances human-centered symbolic explanations.

</details>


### [57] [IntentRL: Training Proactive User-intent Agents for Open-ended Deep Research via Reinforcement Learning](https://arxiv.org/abs/2602.03468)
*Haohao Luo,Zexi Li,Yuexiang Xie,Wenhao Zhang,Yaliang Li,Ying Shen*

Main category: cs.AI

TL;DR: IntentRL框架训练主动代理在开始长时研究前澄清用户潜在意图，通过两阶段强化学习显著提升意图命中率和下游任务性能


<details>
  <summary>Details</summary>
Motivation: 深度研究(DR)代理虽然能自主检索合成网络信息生成长篇报告，但存在自主性-交互困境：对模糊用户查询的高自主性常导致执行时间长且结果不理想

Method: 提出IntentRL框架：1) 通过浅层到深层意图细化图扩展少量种子样本生成高质量对话轮次；2) 采用两阶段强化学习：第一阶段在离线对话上学习通用用户交互行为，第二阶段使用训练代理和用户模拟器进行在线推演以增强对多样化用户反馈的适应

Result: 实验表明IntentRL显著提高了意图命中率和下游任务性能，优于闭源DR代理的内置澄清模块和主动LLM基线

Conclusion: IntentRL通过训练主动代理在长时研究前澄清用户意图，有效解决了DR代理的自主性-交互困境，提高了研究效率和结果质量

Abstract: Deep Research (DR) agents extend Large Language Models (LLMs) beyond parametric knowledge by autonomously retrieving and synthesizing evidence from large web corpora into long-form reports, enabling a long-horizon agentic paradigm. However, unlike real-time conversational assistants, DR is computationally expensive and time-consuming, creating an autonomy-interaction dilemma: high autonomy on ambiguous user queries often leads to prolonged execution with unsatisfactory outcomes. To address this, we propose IntentRL, a framework that trains proactive agents to clarify latent user intents before starting long-horizon research. To overcome the scarcity of open-ended research data, we introduce a scalable pipeline that expands a few seed samples into high-quality dialogue turns via a shallow-to-deep intent refinement graph. We further adopt a two-stage reinforcement learning (RL) strategy: Stage I applies RL on offline dialogues to efficiently learn general user-interaction behavior, while Stage II uses the trained agent and a user simulator for online rollouts to strengthen adaptation to diverse user feedback. Extensive experiments show that IntentRL significantly improves both intent hit rate and downstream task performance, outperforming the built-in clarify modules of closed-source DR agents and proactive LLM baselines.

</details>


### [58] [When Routing Collapses: On the Degenerate Convergence of LLM Routers](https://arxiv.org/abs/2602.03478)
*Guannan Lai,Han-Jia Ye*

Main category: cs.AI

TL;DR: 本文提出EquiRouter来解决LLM路由中的"路由崩溃"问题，即现有路由器倾向于过度使用昂贵的大模型而忽视便宜的小模型，通过直接学习模型排名而非性能分数来优化成本-性能权衡。


<details>
  <summary>Details</summary>
Motivation: 现有LLM路由系统存在"路由崩溃"问题：随着用户成本预算增加，路由器会系统性地默认选择最强大、最昂贵的模型，即使更便宜的模型已经足够。这导致小模型利用率不足，浪费计算资源和金钱成本，违背了路由的核心承诺。

Method: 提出EquiRouter，一种决策感知的路由器。与现有路由器训练预测标量性能分数不同，EquiRouter直接学习模型排名，从而减少预测误差对相对排序的影响，避免次优选择。

Result: 在RouterBench基准测试中，EquiRouter在达到GPT-4级别性能时，相比先前最强的路由器减少了约17%的成本，有效缓解了路由崩溃问题。

Conclusion: 通过直接学习模型排名而非性能分数，EquiRouter能够更好地平衡质量与成本，恢复小模型在路由系统中的角色，为LLM路由提供了更有效的解决方案。

Abstract: LLM routing aims to achieve a favorable quality--cost trade-off by dynamically assigning easy queries to smaller models and harder queries to stronger ones. However, across both unimodal and multimodal settings, we uncover a pervasive yet underexplored failure mode in existing routers: as the user's cost budget increases, routers systematically default to the most capable and most expensive model even when cheaper models already suffice. As a result, current routers under-utilize small models, wasting computation and monetary cost and undermining the core promise of routing; we term this phenomenon routing collapse. We attribute routing collapse to an objective--decision mismatch: many routers are trained to predict scalar performance scores, whereas routing decisions ultimately depend on discrete comparisons among candidate models. Consequently, small prediction errors can flip relative orderings and trigger suboptimal selections. To bridge this gap, we propose EquiRouter, a decision-aware router that directly learns model rankings, restoring the role of smaller models and mitigating routing collapse. On RouterBench, EquiRouter reduces cost by about 17\% at GPT-4-level performance compared to the strongest prior router. Our code is available at https://github.com/AIGNLAI/EquiRouter.

</details>


### [59] [Group Selection as a Safeguard Against AI Substitution](https://arxiv.org/abs/2602.03541)
*Qiankun Zhong,Thomas F. Eisenmann,Julian Garcia,Iyad Rahwan*

Main category: cs.AI

TL;DR: 研究AI使用对人类文化演化的长期影响，发现AI替代型用户会在个体层面胜出但减少文化多样性，而AI互补型用户能在群体层面维持文化探索，需要政策干预来平衡


<details>
  <summary>Details</summary>
Motivation: 生成式AI的广泛使用可能降低文化多样性和创新，导致"文化崩溃"风险。研究旨在理解AI使用如何影响人类文化演化，为政策制定提供依据

Method: 使用基于主体的建模和演化博弈论，比较两种AI使用策略：互补型（AI提供建议，人类主导产出）和替代型（AI主导产出，人类输入极少）。研究这些策略在演化动态中的竞争和传播

Result: AI替代型用户在个体层面选择中占优势，但会显著降低文化多样性。AI互补型用户能维持群体所需的文化探索多样性，在强群体边界条件下可通过文化群体选择获得优势

Conclusion: AI使用策略的演化动态可能导致文化多样性减少，需要政策和组织策略来平衡个体利益与群体文化健康，促进AI互补型使用以维持文化创新

Abstract: Reliance on generative AI can reduce cultural variance and diversity, especially in creative work. This reduction in variance has already led to problems in model performance, including model collapse and hallucination. In this paper, we examine the long-term consequences of AI use for human cultural evolution and the conditions under which widespread AI use may lead to "cultural collapse", a process in which reliance on AI-generated content reduces human variation and innovation and slows cumulative cultural evolution. Using an agent-based model and evolutionary game theory, we compare two types of AI use: complement and substitute. AI-complement users seek suggestions and guidance while remaining the main producers of the final output, whereas AI-substitute users provide minimal input, and rely on AI to produce most of the output. We then study how these use strategies compete and spread under evolutionary dynamics. We find that AI-substitute users prevail under individual-level selection despite the stronger reduction in cultural variance. By contrast, AI-complement users can benefit their groups by maintaining the variance needed for exploration, and can therefore be favored under cultural group selection when group boundaries are strong. Overall, our findings shed light on the long-term, population-level effects of AI adoption and inform policy and organizational strategies to mitigate these risks.

</details>


### [60] [Persona Generators: Generating Diverse Synthetic Personas at Scale](https://arxiv.org/abs/2602.03545)
*Davide Paglieri,Logan Cross,William A. Cunningham,Joel Z. Leibo,Alexander Sasha Vezhnevets*

Main category: cs.AI

TL;DR: 提出Persona Generators方法，通过AlphaEvolve迭代优化生成多样化合成人口，解决AI系统评估中数据收集困难的问题


<details>
  <summary>Details</summary>
Motivation: 评估AI系统需要多样化用户数据，但收集代表性人类数据成本高且不可行，特别是对于新技术或未来场景。现有方法需要详细人口数据且偏向密度匹配而非支持覆盖，导致长尾行为未被充分探索。

Method: 引入Persona Generators方法，使用AlphaEvolve迭代改进循环，以大型语言模型作为变异算子，经过数百次迭代优化Persona Generator代码，生成轻量级生成器，能将简短描述扩展为多样化的合成人口。

Result: 进化后的生成器在六个多样性指标上显著优于现有基线，能生成覆盖罕见特征组合的多样化人口，这些组合在标准LLM输出中难以实现。

Conclusion: Persona Generators方法能有效生成多样化的合成人口，解决AI系统评估中的数据收集问题，特别擅长覆盖长尾行为和罕见特征组合。

Abstract: Evaluating AI systems that interact with humans requires understanding their behavior across diverse user populations, but collecting representative human data is often expensive or infeasible, particularly for novel technologies or hypothetical future scenarios. Recent work in Generative Agent-Based Modeling has shown that large language models can simulate human-like synthetic personas with high fidelity, accurately reproducing the beliefs and behaviors of specific individuals. However, most approaches require detailed data about target populations and often prioritize density matching (replicating what is most probable) rather than support coverage (spanning what is possible), leaving long-tail behaviors underexplored. We introduce Persona Generators, functions that can produce diverse synthetic populations tailored to arbitrary contexts. We apply an iterative improvement loop based on AlphaEvolve, using large language models as mutation operators to refine our Persona Generator code over hundreds of iterations. The optimization process produces lightweight Persona Generators that can automatically expand small descriptions into populations of diverse synthetic personas that maximize coverage of opinions and preferences along relevant diversity axes. We demonstrate that evolved generators substantially outperform existing baselines across six diversity metrics on held-out contexts, producing populations that span rare trait combinations difficult to achieve in standard LLM outputs.

</details>


### [61] [EHRWorld: A Patient-Centric Medical World Model for Long-Horizon Clinical Trajectories](https://arxiv.org/abs/2602.03569)
*Linjie Mu,Zhongzhen Huang,Yannian Gu,Shengqian Qin,Shaoting Zhang,Xiaofan Zhang*

Main category: cs.AI

TL;DR: LLMs在医学推理任务上表现良好，但作为动态医学世界模型模拟疾病进展和治疗效果时，在连续干预下难以保持患者状态一致性，导致长期模拟误差累积。作者提出EHRWorld模型和EHRWorld-110K数据集来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 世界模型为干预下的未来状态模拟提供了原则性框架，但在医学等高风险复杂领域实现这样的模型仍然具有挑战性。虽然LLMs在静态医学推理任务上表现出色，但能否作为动态医学世界模型来模拟疾病进展和治疗效果仍是一个开放问题。

Method: 提出EHRWorld，一个基于因果序列范式训练的患者中心医学世界模型，同时构建了EHRWorld-110K，一个从真实世界电子健康记录中提取的大规模纵向临床数据集。

Result: EHRWorld显著优于基于LLM的基线方法，实现了更稳定的长期模拟、更好的临床敏感事件建模，以及更优的推理效率。这表明基于因果基础、时间演化的临床数据训练对于可靠和鲁棒的医学世界建模是必要的。

Conclusion: 仅依赖医学知识的LLMs难以在序列干预下保持一致的病人状态，导致长期临床模拟中的误差累积。通过因果序列范式训练的患者中心医学世界模型EHRWorld能够提供更可靠和鲁棒的医学世界建模。

Abstract: World models offer a principled framework for simulating future states under interventions, but realizing such models in complex, high-stakes domains like medicine remains challenging. Recent large language models (LLMs) have achieved strong performance on static medical reasoning tasks, raising the question of whether they can function as dynamic medical world models capable of simulating disease progression and treatment outcomes over time. In this work, we show that LLMs only incorporating medical knowledge struggle to maintain consistent patient states under sequential interventions, leading to error accumulation in long-horizon clinical simulation. To address this limitation, we introduce EHRWorld, a patient-centric medical world model trained under a causal sequential paradigm, together with EHRWorld-110K, a large-scale longitudinal clinical dataset derived from real-world electronic health records. Extensive evaluations demonstrate that EHRWorld significantly outperforms naive LLM-based baselines, achieving more stable long-horizon simulation, improved modeling of clinically sensitive events, and favorable reasoning efficiency, highlighting the necessity of training on causally grounded, temporally evolving clinical data for reliable and robust medical world modeling.

</details>


### [62] [Can LLMs Do Rocket Science? Exploring the Limits of Complex Reasoning with GTOC 12](https://arxiv.org/abs/2602.03630)
*Iñaki del Campo,Pablo Cuervo,Victor Rodriguez-Fernandez,Roberto Armellin,Jack Yarndley*

Main category: cs.AI

TL;DR: LLM在复杂航天任务规划中战略理解能力显著提升但执行实现存在严重障碍，表现为"战略-执行鸿沟"。


<details>
  <summary>Details</summary>
Motivation: 探究LLM在物理约束环境下的自主多阶段规划能力极限，特别是针对高维复杂航天任务（如小行星采矿）的可行性。

Method: 采用MLE-Bench框架适配轨道力学领域，部署AIDE-based智能体架构自主生成和优化任务方案，使用"LLM-as-a-Judge"方法结合专家制定的评估标准在五个结构类别中进行战略可行性评估。

Result: 过去两年平均战略可行性得分从9.3提升至17.2（满分26），但发现战略与执行之间存在关键能力鸿沟：先进模型在概念理解和任务架构设计上表现良好，但在实现阶段因物理单位不一致、边界条件错误和低效调试循环而失败。

Conclusion: 当前LLM具备解决空间科学任务的知识和智能，但受限于实现障碍，只能作为强大的领域辅助工具而非完全自主的工程师。

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in code generation and general reasoning, yet their capacity for autonomous multi-stage planning in high-dimensional, physically constrained environments remains an open research question. This study investigates the limits of current AI agents by evaluating them against the 12th Global Trajectory Optimization Competition (GTOC 12), a complex astrodynamics challenge requiring the design of a large-scale asteroid mining campaign. We adapt the MLE-Bench framework to the domain of orbital mechanics and deploy an AIDE-based agent architecture to autonomously generate and refine mission solutions. To assess performance beyond binary validity, we employ an "LLM-as-a-Judge" methodology, utilizing a rubric developed by domain experts to evaluate strategic viability across five structural categories. A comparative analysis of models, ranging from GPT-4-Turbo to reasoning-enhanced architectures like Gemini 2.5 Pro, and o3, reveals a significant trend: the average strategic viability score has nearly doubled in the last two years (rising from 9.3 to 17.2 out of 26). However, we identify a critical capability gap between strategy and execution. While advanced models demonstrate sophisticated conceptual understanding, correctly framing objective functions and mission architectures, they consistently fail at implementation due to physical unit inconsistencies, boundary condition errors, and inefficient debugging loops. We conclude that, while current LLMs often demonstrate sufficient knowledge and intelligence to tackle space science tasks, they remain limited by an implementation barrier, functioning as powerful domain facilitators rather than fully autonomous engineers.

</details>


### [63] [Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration](https://arxiv.org/abs/2602.03647)
*Bowei He,Minda Hu,Zenan Xu,Hongru Wang,Licheng Zong,Yankai Chen,Chen Ma,Xue Liu,Pluto Zhou,Irwin King*

Main category: cs.AI

TL;DR: Search-R2：一种通过Actor-Refiner协作框架增强语言代理搜索推理能力的新方法，通过细粒度奖励和选择性修正机制解决多尺度信用分配问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的搜索推理代理存在多尺度信用分配问题，稀疏的轨迹级奖励无法区分高质量推理和偶然猜测，导致冗余或误导性搜索行为。

Method: 提出Actor-Refiner协作框架：Actor生成初始推理轨迹，Meta-Refiner通过"剪切-再生"机制选择性诊断修复有缺陷的步骤。采用混合奖励设计，结合结果正确性和检索证据信息密度的密集过程奖励。

Result: 在多个通用和多跳QA数据集上的实验表明，Search-R2在不同模型规模上均优于强RAG和基于RL的基线方法，以最小开销实现更优的推理准确性。

Conclusion: Search-R2通过细粒度监督和选择性修正机制有效解决了搜索推理中的信用分配问题，为语言代理的搜索集成推理提供了更高效的训练框架。

Abstract: Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a 'cut-and-regenerate' mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead.

</details>


### [64] [Mitigating Conversational Inertia in Multi-Turn Agents](https://arxiv.org/abs/2602.03664)
*Yang Wan,Zheng Cao,Zhenhao Zhang,Zhengwen Zeng,Shuheng Shen,Changhua Meng,Linchao Zhu*

Main category: cs.AI

TL;DR: 论文提出LLM在代理场景中会模仿自己之前的回答，形成"对话惯性"，导致探索受限。作者提出上下文偏好学习框架来校准模型偏好，降低惯性影响。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为少样本学习者在多轮代理场景中存在问题：模型会错误地将自己之前的回答作为少样本示例进行模仿，导致探索受限。这种"对话惯性"现象限制了代理在环境中的有效探索。

Method: 通过注意力分析识别对话惯性现象；提出上下文偏好学习框架，基于"相同状态下，长上下文生成的动作比短上下文具有更强惯性"的洞察，构建无环境奖励的偏好对；提供推理时的上下文管理策略来平衡探索与利用。

Result: 在八个代理环境和深度研究场景中的实验验证表明，该框架能有效减少对话惯性并实现性能提升。

Conclusion: 论文揭示了将少样本LLM转化为代理时的内在张力：长上下文既提供环境反馈又放大对话惯性。提出的上下文偏好学习框架能有效校准模型偏好，平衡探索与利用。

Abstract: Large language models excel as few-shot learners when provided with appropriate demonstrations, yet this strength becomes problematic in multiturn agent scenarios, where LLMs erroneously mimic their own previous responses as few-shot examples. Through attention analysis, we identify conversational inertia, a phenomenon where models exhibit strong diagonal attention to previous responses, which is associated with imitation bias that constrains exploration. This reveals a tension when transforming few-shot LLMs into agents: longer context enriches environmental feedback for exploitation, yet also amplifies conversational inertia that undermines exploration. Our key insight is that for identical states, actions generated with longer contexts exhibit stronger inertia than those with shorter contexts, enabling construction of preference pairs without environment rewards. Based on this, we propose Context Preference Learning to calibrate model preferences to favor low-inertia responses over highinertia ones. We further provide context management strategies at inference time to balance exploration and exploitation. Experimental results across eight agentic environments and one deep research scenario validate that our framework reduces conversational inertia and achieves performance improvements.

</details>


### [65] [TodyComm: Task-Oriented Dynamic Communication for Multi-Round LLM-based Multi-Agent System](https://arxiv.org/abs/2602.03688)
*Wenzhe Fan,Tommaso Tognoli,Henry Peng Zou,Chunyu Miao,Yibo Wang,Xinhua Zhang*

Main category: cs.AI

TL;DR: TodyComm：一种任务导向的动态通信算法，用于多轮LLM多智能体系统，能够根据每轮动态变化调整通信拓扑结构


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统使用固定通信拓扑，无法适应现实应用中智能体角色可能随轮次变化的情况（如动态对手、任务进展、时变通信带宽限制）

Method: 提出TodyComm算法，通过策略梯度优化任务效用，生成行为驱动的协作拓扑，使通信结构能够适应每轮的动态变化

Result: 在五个基准测试上的实验表明，在动态对手和通信预算约束下，TodyComm在任务有效性方面表现优越，同时保持令牌效率和可扩展性

Conclusion: TodyComm解决了多轮LLM多智能体系统中固定通信拓扑的局限性，通过动态调整通信结构提高了系统在动态环境中的适应性和性能

Abstract: Multi-round LLM-based multi-agent systems rely on effective communication structures to support collaboration across rounds. However, most existing methods employ a fixed communication topology during inference, which falls short in many realistic applications where the agents' roles may change \textit{across rounds} due to dynamic adversary, task progression, or time-varying constraints such as communication bandwidth. In this paper, we propose addressing this issue through TodyComm, a \textbf{t}ask-\textbf{o}riented \textbf{dy}namic \textbf{comm}unication algorithm. It produces behavior-driven collaboration topologies that adapt to the dynamics at each round, optimizing the utility for the task through policy gradient. Experiments on five benchmarks demonstrate that under both dynamic adversary and communications budgets, TodyComm delivers superior task effectiveness while retaining token efficiency and scalability.

</details>


### [66] [AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration](https://arxiv.org/abs/2602.03786)
*Jianhao Ruan,Zhihao Xu,Yiran Peng,Fashen Ren,Zhaoyang Yu,Xinbing Liang,Jinyu Xiang,Bang Liu,Chenglin Wu,Yuyu Luo,Jiayi Zhang*

Main category: cs.AI

TL;DR: AOrchestra是一个统一的、框架无关的智能体系统，通过将智能体抽象为(指令、上下文、工具、模型)四元组，实现按需动态创建专用执行器，在复杂任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有语言智能体系统缺乏对子智能体的动态抽象视图，限制了系统的适应性和灵活性。为了解决复杂、长视野任务的自动化需求，需要一种能够动态组合能力的统一抽象方法。

Method: 提出统一的智能体抽象：将任何智能体建模为(指令、上下文、工具、模型)四元组。基于此构建AOrchestra系统，其中中央编排器在每个步骤中具体化这个四元组：策划任务相关上下文、选择工具和模型，并通过即时自动创建智能体来委托执行。

Result: 在三个具有挑战性的基准测试(GAIA、SWE-Bench、Terminal-Bench)中，AOrchestra与Gemini-3-Flash配对时，相对于最强基线实现了16.28%的相对改进。系统还实现了可控的性能-成本权衡，能够接近帕累托效率。

Conclusion: AOrchestra通过统一的智能体抽象和动态编排机制，显著提升了语言智能体在复杂任务中的适应性和性能，同时减少了人工工程工作量，并保持框架无关性。

Abstract: Language agents have shown strong promise for task automation. Realizing this promise for increasingly complex, long-horizon tasks has driven the rise of a sub-agent-as-tools paradigm for multi-turn task solving. However, existing designs still lack a dynamic abstraction view of sub-agents, thereby hurting adaptability. We address this challenge with a unified, framework-agnostic agent abstraction that models any agent as a tuple Instruction, Context, Tools, Model. This tuple acts as a compositional recipe for capabilities, enabling the system to spawn specialized executors for each task on demand. Building on this abstraction, we introduce an agentic system AOrchestra, where the central orchestrator concretizes the tuple at each step: it curates task-relevant context, selects tools and models, and delegates execution via on-the-fly automatic agent creation. Such designs enable reducing human engineering efforts, and remain framework-agnostic with plug-and-play support for diverse agents as task executors. It also enables a controllable performance-cost trade-off, allowing the system to approach Pareto-efficient. Across three challenging benchmarks (GAIA, SWE-Bench, Terminal-Bench), AOrchestra achieves 16.28% relative improvement against the strongest baseline when paired with Gemini-3-Flash. The code is available at: https://github.com/FoundationAgents/AOrchestra

</details>


### [67] [Understanding Agent Scaling in LLM-Based Multi-Agent Systems via Diversity](https://arxiv.org/abs/2602.03794)
*Yingxuan Yang,Chengrui Qu,Muning Wen,Laixi Shi,Ying Wen,Weinan Zhang,Adam Wierman,Shangding Gu*

Main category: cs.AI

TL;DR: 多智能体系统性能受限于任务内在不确定性而非智能体数量，异构智能体通过提供互补信息比同构智能体扩展更有效


<details>
  <summary>Details</summary>
Motivation: 研究LLM多智能体系统性能扩展的局限性，发现增加同构智能体数量收益递减，而异构智能体却能持续提升性能，需要理解这种差异的根本原因

Method: 提出信息论框架分析MAS性能边界，引入K*指标量化有效通道数量，通过实验验证异构配置优于同构扩展

Result: 异构配置显著优于同构扩展：2个异构智能体性能可匹配或超过16个同构智能体，K*指标能有效量化系统信息获取能力

Conclusion: 多智能体系统设计应注重多样性而非简单增加同构智能体数量，通过异构设计构建高效鲁棒的MAS系统

Abstract: LLM-based multi-agent systems (MAS) have emerged as a promising approach to tackle complex tasks that are difficult for individual LLMs. A natural strategy is to scale performance by increasing the number of agents; however, we find that such scaling exhibits strong diminishing returns in homogeneous settings, while introducing heterogeneity (e.g., different models, prompts, or tools) continues to yield substantial gains. This raises a fundamental question: what limits scaling, and why does diversity help? We present an information-theoretic framework showing that MAS performance is bounded by the intrinsic task uncertainty, not by agent count. We derive architecture-agnostic bounds demonstrating that improvements depend on how many effective channels the system accesses. Homogeneous agents saturate early because their outputs are strongly correlated, whereas heterogeneous agents contribute complementary evidence. We further introduce $K^*$, an effective channel count that quantifies the number of effective channels without ground-truth labels. Empirically, we show that heterogeneous configurations consistently outperform homogeneous scaling: 2 diverse agents can match or exceed the performance of 16 homogeneous agents. Our results provide principled guidelines for building efficient and robust MAS through diversity-aware design. Code and Dataset are available at the link: https://github.com/SafeRL-Lab/Agent-Scaling.

</details>


### [68] [Conformal Thinking: Risk Control for Reasoning on a Compute Budget](https://arxiv.org/abs/2602.03814)
*Xi Wang,Anushri Suresh,Alvin Zhang,Rishi More,William Jurayj,Benjamin Van Durme,Mehrdad Farajtabar,Daniel Khashabi,Eric Nalisnick*

Main category: cs.AI

TL;DR: 提出一个风险控制框架，通过上下阈值自适应控制LLM推理计算量，在保证错误率不超目标的前提下最小化计算开销


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理时存在计算效率与准确率的权衡，传统固定token预算或自适应阈值设置缺乏理论保证，需要系统性的风险控制方法

Method: 提出双阈值停止机制：上阈值在模型置信时停止（风险错误输出），参数化下阈值提前停止不可解实例（风险过早停止）。使用无分布风险控制方法，结合效率损失选择最优退出机制

Result: 在多种推理任务和模型上验证了风险控制方法的有效性，下阈值和集成停止机制显著提升计算效率，同时满足用户指定的风险目标

Conclusion: 将预算设置问题重新定义为风险控制问题，提供理论保证的计算效率优化框架，为自适应推理提供了实用的风险控制解决方案

Abstract: Reasoning Large Language Models (LLMs) enable test-time scaling, with dataset-level accuracy improving as the token budget increases, motivating adaptive reasoning -- spending tokens when they improve reliability and stopping early when additional computation is unlikely to help. However, setting the token budget, as well as the threshold for adaptive reasoning, is a practical challenge that entails a fundamental risk-accuracy trade-off. We re-frame the budget setting problem as risk control, limiting the error rate while minimizing compute. Our framework introduces an upper threshold that stops reasoning when the model is confident (risking incorrect output) and a novel parametric lower threshold that preemptively stops unsolvable instances (risking premature stoppage). Given a target risk and a validation set, we use distribution-free risk control to optimally specify these stopping mechanisms. For scenarios with multiple budget controlling criteria, we incorporate an efficiency loss to select the most computationally efficient exiting mechanism. Empirical results across diverse reasoning tasks and models demonstrate the effectiveness of our risk control approach, demonstrating computational efficiency gains from the lower threshold and ensemble stopping mechanisms while adhering to the user-specified risk target.

</details>


### [69] [AutoFigure: Generating and Refining Publication-Ready Scientific Illustrations](https://arxiv.org/abs/2602.03828)
*Minjun Zhu,Zhen Lin,Yixuan Weng,Panzhong Lu,Qiujie Xie,Yifan Wei,Sifan Liu,Qiyao Sun,Yue Zhang*

Main category: cs.AI

TL;DR: FigureBench是首个大规模科学插图生成基准，包含3300个高质量文本-插图对；AutoFigure是首个基于长文本自动生成高质量科学插图的智能体框架，通过思考、重组和验证实现结构完整且美观的插图。


<details>
  <summary>Details</summary>
Motivation: 高质量科学插图对于有效传达复杂科技概念至关重要，但手动创建插图在学术界和工业界都是公认的瓶颈，需要自动化解决方案来提升效率。

Method: 提出FigureBench基准数据集（3300个高质量文本-插图对），并开发AutoFigure智能体框架，该框架在最终渲染前进行广泛思考、重组和验证，确保插图的完整结构和美学质量。

Result: 实验结果表明AutoFigure在所有基线方法中表现最佳，能够生成可直接用于出版的高质量科学插图。代码、数据集和HuggingFace空间已开源。

Conclusion: FigureBench为科学插图生成提供了首个大规模基准，AutoFigure框架通过智能体方法有效解决了科学插图自动生成的挑战，为科学可视化领域提供了实用工具。

Abstract: High-quality scientific illustrations are crucial for effectively communicating complex scientific and technical concepts, yet their manual creation remains a well-recognized bottleneck in both academia and industry. We present FigureBench, the first large-scale benchmark for generating scientific illustrations from long-form scientific texts. It contains 3,300 high-quality scientific text-figure pairs, covering diverse text-to-illustration tasks from scientific papers, surveys, blogs, and textbooks. Moreover, we propose AutoFigure, the first agentic framework that automatically generates high-quality scientific illustrations based on long-form scientific text. Specifically, before rendering the final result, AutoFigure engages in extensive thinking, recombination, and validation to produce a layout that is both structurally sound and aesthetically refined, outputting a scientific illustration that achieves both structural completeness and aesthetic appeal. Leveraging the high-quality data from FigureBench, we conduct extensive experiments to test the performance of AutoFigure against various baseline methods. The results demonstrate that AutoFigure consistently surpasses all baseline methods, producing publication-ready scientific illustrations. The code, dataset and huggingface space are released in https://github.com/ResearAI/AutoFigure.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [70] [Efficient Edge Rewiring Strategies for Enhancing PageRank Fairness](https://arxiv.org/abs/2602.02512)
*Changan Liu,Haoxin Sun,Ahad N. Zehmakan,Zhongzhi Zhang*

Main category: cs.SI

TL;DR: 提出一种基于贪婪策略的线性时间算法，通过重连固定数量的边来最大化PageRank公平性，特别关注弱势群体在网络中的信息获取公平性。


<details>
  <summary>Details</summary>
Motivation: 研究社交网络中的不公平现象，特别是某些群体（如男性主导行业中的女性）由于在网络中处于不利位置而难以获取重要信息（如工作职位）。

Method: 基于贪婪方法，利用快速采样有根生成森林技术，设计了一种有效的线性时间算法，通过重连固定数量的边来最大化PageRank公平性。

Result: 在多个真实网络数据上的大量实验表明，该算法显著优于现有方法，能够在几分钟内为百万节点网络生成准确解决方案。

Conclusion: 提出的线性时间算法能有效提升社交网络中的PageRank公平性，为弱势群体改善信息获取机会，具有实际应用价值。

Abstract: We study the notion of unfairness in social networks, where a group such as females in a male-dominated industry are disadvantaged in access to important information, e.g. job posts, due to their less favorable positions in the network. We investigate a well-established network-based formulation of fairness called PageRank fairness, which refers to a fair allocation of the PageRank weights among distinct groups. Our goal is to enhance the PageRank fairness by modifying the underlying network structure. More precisely, we study the problem of maximizing PageRank fairness with respect to a disadvantaged group, when we are permitted to rewire a fixed number of edges in the network. Building on a greedy approach, we leverage techniques from fast sampling of rooted spanning forests to devise an effective linear-time algorithm for this problem. To evaluate the accuracy and performance of our proposed algorithm, we conduct a large set of experiments on various real-world network data. Our experiments demonstrate that the proposed algorithm significantly outperforms the existing ones. Our algorithm is capable of generating accurate solutions for networks of million nodes in just a few minutes.

</details>


### [71] [GASTON: Graph-Aware Social Transformer for Online Networks](https://arxiv.org/abs/2602.02524)
*Olha Wloch,Liam Hebert,Robin Cohen,Lukasz Golab*

Main category: cs.SI

TL;DR: GASTON是一个图感知社交Transformer模型，通过结合文本内容和社区用户成员模式来学习在线社区中的有害内容检测，利用对比初始化策略预训练社区嵌入以捕获社区规范。


<details>
  <summary>Details</summary>
Motivation: 在线社区存在毒性内容、回音室和错误信息等问题，但检测这些有害内容很困难，因为在线互动的意义既来自文本内容，也来自发布位置的社会规范。

Method: 提出GASTON模型，学习基于本地规范的文本和用户嵌入。核心是对比初始化策略，基于用户成员模式预训练社区嵌入，在文本处理前捕获社区用户基础，从而区分不同社区类型。

Result: 在压力检测、毒性评分和规范违反等任务上的实验表明，GASTON生成的嵌入优于最先进的基线方法。

Conclusion: GASTON通过结合文本内容和社区用户成员模式来学习上下文感知的嵌入，能够更有效地检测在线社区中的有害内容，解决了传统方法忽视社会规范的问题。

Abstract: Online communities have become essential places for socialization and support, yet they also possess toxicity, echo chambers, and misinformation. Detecting this harmful content is difficult because the meaning of an online interaction stems from both what is written (textual content) and where it is posted (social norms). We propose GASTON (Graph-Aware Social Transformer for Online Networks), which learns text and user embeddings that are grounded in their local norms, providing the necessary context for downstream tasks. The heart of our solution is a contrastive initialization strategy that pretrains community embeddings based on user membership patterns, capturing a community's user base before processing any text. This allows GASTON to distinguish between communities (e.g., a support group vs. a hate group) based on who interacts there, even if they share similar vocabulary. Experiments on tasks such as stress detection, toxicity scoring, and norm violation demonstrate that the embeddings produced by GASTON outperform state-of-the-art baselines.

</details>


### [72] [Community Norms in the Spotlight: Enabling Task-Agnostic Unsupervised Pre-Training to Benefit Online Social Media](https://arxiv.org/abs/2602.02525)
*Liam Hebert,Lucas Kopp,Robin Cohen*

Main category: cs.SI

TL;DR: 论文提出从任务特定微调转向无监督预训练的新范式，通过社区规范建模解决社交平台复杂动态分析中的数据稀缺问题


<details>
  <summary>Details</summary>
Motivation: 在线社交平台的复杂动态建模对解决仇恨言论和错误信息等挑战至关重要，但现有Discussion Transformers架构严重依赖高质量人工标注数据集，限制了其潜力

Method: 提出从任务特定微调转向无监督预训练的范式转变，基于全新的社区规范考虑，通过无监督预训练框架建模社交规范

Result: 该框架不仅能缓解数据稀缺问题，还能解释AI系统决策背后的社会规范，为AI for Social Good提供新机会

Conclusion: 基于社区规范的无监督预训练范式为解决社交平台复杂动态建模中的数据稀缺问题提供了有前景的方向，并为AI社会公益应用开辟了新机遇

Abstract: Modelling the complex dynamics of online social platforms is critical for addressing challenges such as hate speech and misinformation. While Discussion Transformers, which model conversations as graph structures, have emerged as a promising architecture, their potential is severely constrained by reliance on high-quality, human-labelled datasets. In this paper, we advocate a paradigm shift from task-specific fine-tuning to unsupervised pretraining, grounded in an entirely novel consideration of community norms. We posit that this framework not only mitigates data scarcity but also enables interpretation of the social norms underlying the decisions made by such an AI system. Ultimately, we believe that this direction offers many opportunities for AI for Social Good.

</details>


### [73] [DualMind: Towards Understanding Cognitive-Affective Cascades in Public Opinion Dissemination via Multi-Agent Simulation](https://arxiv.org/abs/2602.02534)
*Enhao Huang,Tongtong Pan,Shuhuai Zhang,Qishu Jin,Liheng Zheng,Kaichun Hu,Yiming Li,Zhan Qin,Kui Ren*

Main category: cs.SI

TL;DR: DualMind是一个基于LLM的多智能体平台，用于建模危机公关中公众意见的短期情感反应与持久认知信念的交互，在15个真实危机案例中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有危机公关中的公众意见预测框架往往忽视了短期情感反应与持久认知信念之间的相互作用，这限制了预测的准确性。

Method: 提出DualMind——一个基于大语言模型的多智能体平台，专门建模公众意见中的双重成分（情感反应与认知信念）的交互作用。

Result: 在2024年8月后发生的15个真实世界危机案例中，使用社交媒体数据作为基准，DualMind能够忠实地重建意见轨迹，显著优于最先进的基线方法。

Conclusion: DualMind为主动危机管理提供了一个高保真度的工具，代码已开源。

Abstract: Forecasting public opinion during PR crises is challenging, as existing frameworks often overlook the interaction between transient affective responses and persistent cognitive beliefs. To address this, we propose DualMind, an LLM-driven multi-agent platform designed to model this dual-component interplay. We evaluate the system on 15 real-world crises occurring post-August 2024 using social media data as ground truth. Empirical results demonstrate that DualMind faithfully reconstructs opinion trajectories, significantly outperforming state-of-the-art baselines. This work offers a high-fidelity tool for proactive crisis management. Code is available at https://github.com/EonHao/DualMind.

</details>


### [74] [CaST: Causal Discovery via Spatio-Temporal Graphs in Disaster Tweets](https://arxiv.org/abs/2602.02601)
*Hieu Duong,Eugene Levin,Todd Gary,Long Nguyen*

Main category: cs.SI

TL;DR: CaST是一个用于灾害领域社交媒体因果发现的统一框架，通过结合语义相似性和时空邻近性，利用预训练的大语言模型构建时空事件图，并使用图注意力网络学习有向因果关系。


<details>
  <summary>Details</summary>
Motivation: 现有因果发现方法往往忽略语义、空间和时间上下文之间的相互作用，而理解社交媒体中现实世界事件的因果关系对于态势感知至关重要。

Method: CaST为每个推文窗口构建事件图，将推文中提取的事件表示为包含上下文语义、地理坐标和时间特征的节点嵌入，然后连接形成时空事件图，使用多头图注意力网络学习有向因果关系。

Result: 实验结果表明，CaST在传统方法和最先进方法上都表现出优越性能。消融研究进一步证实，结合空间和时间信号显著提高了召回率和训练稳定性。

Conclusion: CaST证明将时空推理整合到事件图中，能够在灾害相关社交媒体文本中实现更稳健和可解释的因果发现。

Abstract: Understanding causality between real-world events from social media is essential for situational awareness, yet existing causal discovery methods often overlook the interplay between semantic, spatial, and temporal contexts. We propose CaST: Causal Discovery via Spatio-Temporal Graphs, a unified framework for causal discovery in disaster domain that integrates semantic similarity and spatio-temporal proximity using Large Language Models (LLMs) pretrained on disaster datasets. CaST constructs an event graph for each window of tweets. Each event extracted from tweets is represented as a node embedding enriched with its contextual semantics, geographic coordinates, and temporal features. These event nodes are then connected to form a spatio-temporal event graph, which is processed using a multi-head Graph Attention Network (GAT) \cite{gat} to learn directed causal relationships. We construct an in-house dataset of approximately 167K disaster-related tweets collected during Hurricane Harvey and annotated following the MAVEN-ERE schema. Experimental results show that CaST achieves superior performance over both traditional and state-of-the-art methods. Ablation studies further confirm that incorporating spatial and temporal signals substantially improves both recall and stability during training. Overall, CaST demonstrates that integrating spatio-temporal reasoning into event graphs enables more robust and interpretable causal discovery in disaster-related social media text.

</details>


### [75] [Gender Dynamics and Homophily in a Social Network of LLM Agents](https://arxiv.org/abs/2602.02606)
*Faezeh Fadaei,Jenny Carla Moran,Taha Yasseri*

Main category: cs.SI

TL;DR: 研究AI社交网络中LLM性别表现的流动性、同质性及其形成机制，发现即使没有实体，文化训练也会导致基于性别的分类


<details>
  <summary>Details</summary>
Motivation: 生成式AI和LLM越来越多地部署在交互环境中，但我们对其在大规模网络中身份表现的发展知之甚少，特别是在性别表现方面

Method: 使用Chirper.ai平台（完全由自主AI聊天机器人组成的社交媒体平台），分析超过70,000个代理、约1.4亿条帖子以及一年内不断变化的关注网络，基于代理的文本生产为每个代理分配每周性别分数

Result: 每个代理的性别表现是流动的而非固定的；网络显示出强烈的基于性别的同质性，代理始终关注性别表现相似的账户；社会选择和社会影响两种机制共同塑造了LLM之间交互的结构和演变

Conclusion: 即使在缺乏实体的情况下，性别表现的文化训练也会导致基于性别的分类，这对LLM在合成混合群体、社会模拟和决策支持中的应用具有重要意义

Abstract: Generative artificial intelligence and large language models (LLMs) are increasingly deployed in interactive settings, yet we know little about how their identity performance develops when they interact within large-scale networks. We address this by examining Chirper.ai, a social media platform similar to X but composed entirely of autonomous AI chatbots. Our dataset comprises over 70,000 agents, approximately 140 million posts, and the evolving followership network over one year. Based on agents' text production, we assign weekly gender scores to each agent. Results suggest that each agent's gender performance is fluid rather than fixed. Despite this fluidity, the network displays strong gender-based homophily, as agents consistently follow others performing gender similarly. Finally, we investigate whether these homophilic connections arise from social selection, in which agents choose to follow similar accounts, or from social influence, in which agents become more similar to their followees over time. Consistent with human social networks, we find evidence that both mechanisms shape the structure and evolution of interactions among LLMs. Our findings suggest that, even in the absence of bodies, cultural entraining of gender performance leads to gender-based sorting. This has important implications for LLM applications in synthetic hybrid populations, social simulations, and decision support.

</details>


### [76] [Recommender system in X inadvertently profiles ideological positions of users](https://arxiv.org/abs/2602.02624)
*Paul Bouchaud,Pedro Ramaciotti*

Main category: cs.SI

TL;DR: 研究通过数据捐赠收集X平台250万条好友推荐，分析推荐系统如何学习用户政治属性，发现推荐系统的嵌入空间与用户左右立场高度相关，并提出限制政治信息的推荐方法


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注推荐项目的质量（如多样性、偏见）和推荐政策的影响，但缺乏对推荐系统如何学习、表示和处理用户政治社会属性的理解。本研究旨在探索AI系统"黑箱"内部如何运作，特别是政治属性在推荐系统中的角色

Method: 通过数据捐赠项目收集682名志愿者在X平台一年内收到的250万条好友推荐数据；利用公开的推荐系统架构信息推断推荐用户在嵌入空间的位置；使用基于政治调查数据校准的意识形态量表分析26,509名用户的政治立场；分析政治立场与年龄、性别等属性的关系

Result: 平台推荐系统产生的空间排序与用户的左右政治立场高度相关（皮尔逊相关系数rho=0.887，p值<0.0001），这种相关性无法用社会人口属性解释；推荐系统的嵌入空间强烈反映了用户的政治倾向

Conclusion: 研究为理解人类与AI系统互动提供了新视角，模糊了主动与被动算法画像的界限，对数据隐私监管中的算法画像法律定义提出重要问题；提出限制推荐系统中政治信息的约束推荐方法，可作为隐私合规工具同时保持推荐相关性

Abstract: Studies on recommendations in social media have mainly analyzed the quality of recommended items (e.g., their diversity or biases) and the impact of recommendation policies (e.g., in comparison with purely chronological policies). We use a data donation program, collecting more than 2.5 million friend recommendations made to 682 volunteers on X over a year, to study instead how real-world recommenders learn, represent and process political and social attributes of users inside the so-called black boxes of AI systems. Using publicly available knowledge on the architecture of the recommender, we inferred the positions of recommended users in its embedding space. Leveraging ideology scaling calibrated with political survey data, we analyzed the political position of users in our study (N=26,509 among volunteers and recommended contacts) among several attributes, including age and gender. Our results show that the platform's recommender system produces a spatial ordering of users that is highly correlated with their Left-Right positions (Pearson rho=0.887, p-value < 0.0001), and that cannot be explained by socio-demographic attributes. These results open new possibilities for studying the interaction between human and AI systems. They also raise important questions linked to the legal definition of algorithmic profiling in data privacy regulation by blurring the line between active and passive profiling. We explore new constrained recommendation methods enabled by our results, limiting the political information in the recommender as a potential tool for privacy compliance capable of preserving recommendation relevance.

</details>


### [77] [OpenClaw Agents on Moltbook: Risky Instruction Sharing and Norm Enforcement in an Agent-Only Social Network](https://arxiv.org/abs/2602.02625)
*Md Motaleb Hossen Manik,Ge Wang*

Main category: cs.SI

TL;DR: OpenClaw智能体在Moltbook社交网络上表现出选择性社会调节，18.4%的帖子包含行动诱导语言，这些内容更可能引发规范执行回复，而毒性回复很少见。


<details>
  <summary>Details</summary>
Motivation: 研究智能体在没有人类参与或集中调节的情况下，如何在共享社交环境中相互调节，了解智能体专属社交网络中的社会动态。

Method: 在Moltbook（智能体专属社交网络）上分析14,490个OpenClaw智能体产生的39,026个帖子和5,712条评论，使用基于词典的行动诱导风险评分（AIRS）量化行动诱导指令分享的普遍性，并研究其他智能体对此类内容的反应。

Result: 18.4%的帖子包含行动诱导语言；包含可操作指令的帖子比非指令性帖子更可能引发规范执行回复（警告不安全或风险行为）；两种情况下毒性回复都很罕见。

Conclusion: OpenClaw智能体表现出选择性社会调节，潜在风险指令更可能受到挑战，表明智能体专属社交系统中出现了新兴规范行为，研究智能体AI生态系统时需同时关注社会动态和技术保障。

Abstract: Agentic AI systems increasingly operate in shared social environments where they exchange information, instructions, and behavioral cues. However, little empirical evidence exists on how such agents regulate one another in the absence of human participants or centralized moderation. In this work, we present an empirical analysis of OpenClaw agents interacting on Moltbook, an agent-only social network. Analyzing 39,026 posts and 5,712 comments produced by 14,490 agents, we quantify the prevalence of action-inducing instruction sharing using a lexicon-based Action-Inducing Risk Score (AIRS), and examine how other agents respond to such content. We find that 18.4% of posts contain action-inducing language, indicating that instruction sharing is a routine behavior in this environment. While most social responses are neutral, posts containing actionable instructions are significantly more likely to elicit norm-enforcing replies that caution against unsafe or risky behavior, compared to non-instructional posts. Importantly, toxic responses remain rare across both conditions. These results suggest that OpenClaw agents exhibit selective social regulation, whereby potentially risky instructions are more likely to be challenged than neutral content, despite the absence of human oversight. Our findings provide early empirical evidence of emergent normative behavior in agent-only social systems and highlight the importance of studying social dynamics alongside technical safeguards in agentic AI ecosystems.

</details>


### [78] [Deepfake Pornography is Resilient to Regulatory and Platform Shocks](https://arxiv.org/abs/2602.02754)
*Alejandro Cuevas,Manoel Horta Ribeiro*

Main category: cs.SI

TL;DR: 美国通过TAKE IT DOWN法案后，MrDeepfakes网站关闭，但研究发现合成非自愿色情内容（SNCEI）并未减少，而是转移到其他平台继续传播。


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具使制作合成非自愿色情内容（SNCEI）变得容易，这种内容传播对受害者造成严重伤害。美国通过TAKE IT DOWN法案后，知名SNCEI网站MrDeepfakes关闭，研究者想了解这种复合干预措施对SNCEI传播的实际影响。

Method: 选择三个分享色情内容的在线论坛，每个论坛都有专门组织不同类型色情内容的子论坛。利用论坛设计，使用合成控制准实验方法，比较专门用于SNCEI的子论坛与其他色情类型子论坛的活动。

Result: 在所有网站上都观察到SNCEI分享和请求的增加，某些情况下新贡献者也增加。复合干预并未抑制SNCEI整体活动，而是导致其在平台间重新分布，时间和规模存在显著异质性。

Conclusion: 仅靠平台封禁和监管信号可能只是改变SNCEI生产和分享的时间和地点，而非减少其普遍性。需要更全面的策略来有效应对这种形式的性虐待。

Abstract: Generative artificial intelligence tools have made it easier to create realistic, synthetic non-consensual explicit imagery (popularly known as deepfake pornography; hereinafter SNCEI) of people. Once created, this SNCEI is often shared on various websites, causing significant harm to victims. This emerging form of sexual abuse was recently criminalized in the US at the federal level by S.146, the TAKE IT DOWN Act. A week after the bill's passage became effectively imminent, the MrDeepfakes website -- one of the most notorious facilitators of SNCEI creation and dissemination -- shut down. Here, we explore the impact of the bill's passage and the subsequent shutdown as a compound intervention on the dissemination of SNCEI. We select three online forums where sexually explicit content is shared, each containing dedicated subforums to organize various types of sexually explicit content. By leveraging each forum's design, we compare activity in subforums dedicated to SNCEI with that in other pornographic genres using a synthetic control, quasi-experimental approach. Across websites, we observed an increase in the sharing and requests for SNCEI, and, in some cases, in new contributors. These results indicate that the compound intervention did not suppress SNCEI activity overall but instead coincided with its redistribution across platforms, with substantial heterogeneity in timing and magnitude. Together, our findings suggest that deplatforming and regulatory signals alone may shift where and when SNCEI is produced and shared, rather than reducing its prevalence.

</details>


### [79] [Beyond Content: Behavioral Policies Reveal Actors in Information Operations](https://arxiv.org/abs/2602.02838)
*Philipp J. Schneider,Lanqin Yuan,Marian-Andrei Rizoiu*

Main category: cs.SI

TL;DR: 提出基于行为策略的平台无关框架，通过建模用户活动为序列决策过程来检测恶意影响操作，相比传统内容分析更有效且更具韧性。


<details>
  <summary>Details</summary>
Motivation: 传统基于内容分析或网络特征的在线影响操作检测方法面临挑战：生成模型能产生逼真文本、平台限制行为数据访问、恶意行为者转向监管较少空间，需要更稳健的检测方法。

Method: 引入平台无关框架，将用户活动建模为序列决策过程，从行为策略识别恶意行为者。应用于12,064个Reddit用户（包括99个与俄罗斯互联网研究机构相关的账户），分析2015-2018年超过3800万次活动步骤。

Result: 基于活动的表征（建模用户如何行动而非发布什么内容）在检测恶意账户方面始终优于内容模型。区分操纵性用户与普通用户时，基于策略的分类器达到94.9%的中位宏F1分数，而文本嵌入为91.2%。行为特征还能实现更早检测，对规避策略或数据损坏更具韧性。

Conclusion: 行为动态编码了稳定、可区分的操纵信号，为合成内容和数据访问受限时代提供了有韧性的跨平台检测策略。

Abstract: The detection of online influence operations -- coordinated campaigns by malicious actors to spread narratives -- has traditionally depended on content analysis or network features. These approaches are increasingly brittle as generative models produce convincing text, platforms restrict access to behavioral data, and actors migrate to less-regulated spaces. We introduce a platform-agnostic framework that identifies malicious actors from their behavioral policies by modeling user activity as sequential decision processes. We apply this approach to 12,064 Reddit users, including 99 accounts linked to the Russian Internet Research Agency in Reddit's 2017 transparency report, analyzing over 38 million activity steps from 2015-2018. Activity-based representations, which model how users act rather than what they post, consistently outperform content models in detecting malicious accounts. When distinguishing trolls -- users engaged in coordinated manipulation -- from ordinary users, policy-based classifiers achieve a median macro-$F_1$ of 94.9%, compared to 91.2% for text embeddings. Policy features also enable earlier detection from short traces and degrade more gracefully under evasion strategies or data corruption. These findings show that behavioral dynamics encode stable, discriminative signals of manipulation and point to resilient, cross-platform detection strategies in the era of synthetic content and limited data access.

</details>


### [80] [From semantic memory to collective creativity: A generative cognitive foundation for social creativity models](https://arxiv.org/abs/2602.03068)
*Mirza Nayeem Ahmed,Raiyan Abdul Baten*

Main category: cs.SI

TL;DR: 提出多层级社会认知代理框架，通过语义网络拓扑差异模拟集体创造力，揭示认知机制与社会结构在集体创意中的相互作用


<details>
  <summary>Details</summary>
Motivation: 现有基于模拟的理论发展在集体绩效方面取得了重要进展，但难以扩展到集体创造力研究，因为创造力涉及认知机制而不仅仅是社会层面

Method: 引入多层级社会认知代理框架，代理共享共同语义词汇但具有不同的语义网络拓扑结构，通过单一生成参数调节语义模块化程度，产生个体在创意广度上的差异

Result: 当代理交换创意轨迹时，出现了两种典型的社会创造力现象：较低的前交互创意重叠预测更大的刺激增益，共享灵感来源导致网络层面的冗余

Conclusion: 该框架为理解集体创造力中的认知机制和社会结构提供了机制性理论构建的基础，填补了现有模拟方法在创造力研究中的空白

Abstract: Simulation-based theory development has yielded powerful insights into collective performance by linking social structure to emergent outcomes, yet it has struggled to extend to collective creativity. Creativity is hard to capture purely at the social level, as novel ideas are generated through cognitive mechanisms. To address this gap, we introduce a multi-level socio-cognitive agent-based framework in which agents share a common semantic vocabulary and substrate but differ in semantic network topology. A single generative parameter tunes semantic modularity, yielding emergent individual differences in ideational breadth. When agents exchange ideation traces, two canonical social-creativity phenomena arise without being imposed: lower pre-interaction ideation overlap predicts larger stimulation gains, and shared inspiration sources induce network-level redundancy. The framework enables mechanistic theory-building about cognition and social structure in collective creativity.

</details>


### [81] ["Why I Took the Blackpill": A Thematic Analysis of the Radicalization Process in Incel Communities](https://arxiv.org/abs/2602.03089)
*Jennifer Golbeck,Celia Chen,Alex Leitch*

Main category: cs.SI

TL;DR: 该研究通过主题分析揭示incel社区成员的自我描述的激进化过程，识别出包含四个阶段六个主题的激进化路径，与其他极端主义群体的激进化模式相似。


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究关注incel群体的意识形态和信仰，但缺乏对其激进化过程的专门研究。incel作为完全在线存在的极端厌女仇恨团体，其成员与线下暴力事件（包括大规模枪击）有关联，理解其激进化过程对于预防暴力至关重要。

Method: 采用主题分析方法，分析incel成员在社交媒体上描述自身激进化过程的帖子，识别其中的关键主题和模式。

Result: 识别出六个主要主题，按时间顺序分为四个步骤：激进化前（外貌、社交孤立、心理问题）、寻找责任方、激进化、激进化后。这些发现与其他极端主义群体的激进化研究结果高度一致。

Conclusion: incel的激进化过程与其他极端主义群体相似，这为理解和管控激进化提供了重要参考，将incel激进化纳入更广泛的激进化研究框架中。

Abstract: Incels, or "involuntary celibates", are an extreme, misogynistic hate group that exists entirely online. Members of the community have been linked to acts of offline violence, including mass shootings. Previous research has engaged with the ideologies and beliefs of incels, but none has looked specifically at the radicalization process. In this paper, we perform a thematic analysis on social media posts where incels describe their own radicalization process. We identified six major themes grouped into four chronological steps: Pre-radicalization (themes of Appearance, Social Isolation, and Psychological issues), Searching for Blame, Radicalization, and Post Radicalization. These results align closely with existing work on radicalization among other extremist groups, bringing incel radicalization inline with a growing body of research on understanding and managing radicalization.

</details>


### [82] [In Bad Faith: Assessing Discussion Quality on Social Media](https://arxiv.org/abs/2602.03090)
*Celia Chen,Alex Leitch,William Jordan Conway,Eric Cotugno,Emily Klein,Rajesh Kumar Gnanasekaran,Kristin Buckstad Hamilton,Casi Sherman,Celia Sterrn,Logan C. Stevens,Rebecca Zarrella,Jennifer Golbeck*

Main category: cs.SI

TL;DR: 该研究分析主流媒体和政府机构推文回复的质量，发现68.3%的回复是恶意互动，其中认证账户的恶意回复比例高达91.7%，揭示了社交媒体在这些特定对话环境中存在严重的对话质量问题。


<details>
  <summary>Details</summary>
Motivation: 研究社交媒体用户体验质量，特别关注主流媒体和政府机构推文回复中的对话质量，探究是否存在大量恶意互动影响公共讨论环境。

Method: 收集主流媒体和政府机构推文的回复数据，开发自动化方法评估回复质量，将回复分为善意（诚实建设性参与）和恶意（攻击作者或破坏对话）两类。

Result: 研究发现68.3%的回复属于恶意互动，认证账户的恶意回复比例更高达91.7%，表明在这些特定对话环境中存在严重的对话质量问题。

Conclusion: 社交媒体上主流媒体和政府机构推文的回复中存在大量恶意互动，特别是认证账户的恶意回复比例极高，由于认证账户被算法放大，这对社交媒体用户体验和公共讨论质量构成严重关切。

Abstract: The quality of a user's social media experience is determined both by the content they see and by the quality of the conversation and interaction around it. In this paper, we look at replies to tweets from mainstream media outlets and official government agencies and assess if they are good faith, engaging honestly and constructively with the original post, or bad faith, attacking the author or derailing the conversation. We assess automated approaches that may help in making this determination and then show that within our dataset of replies to mainstream media outlets and government agencies, bad faith interactions constitute 68.3% of all replies we studied, suggesting potential concerns about the quality of discourse in these specific conversational contexts. This is particularly true from verified accounts, where 91.7% of replies were bad faith. Given that verified accounts are algorithmically amplified, we discuss the implications of our work for understanding the user experience on social media.

</details>


### [83] [Link Fraction Mixed Membership Reveals Community Diversity in Aggregated Social Networks](https://arxiv.org/abs/2602.03266)
*Gamal Adel,Eszter Bokányi,Eelke M. Heemskerk,Frank W. Takes*

Main category: cs.SI

TL;DR: 提出LFMM方法解决聚合网络中社区检测问题，该方法具有聚合一致性，能准确反映底层个体网络社区结构


<details>
  <summary>Details</summary>
Motivation: 传统社区检测方法在处理聚合或粗粒度社交网络时存在局限：1) 不相交的社区划分无法捕捉聚合节点内社区成员的多样性组成；2) 现有混合成员方法对聚合分辨率高度敏感，不能可靠反映底层个体网络的社区结构

Method: 提出Link Fraction Mixed Membership (LFMM)方法，计算聚合网络中节点的混合成员关系。该方法具有聚合一致性，在不同尺度下保持社区成员关系总和守恒

Result: 应用于荷兰人口规模社交网络，在不同分辨率下聚合。实验显示：1) 不同地理区域的社区成员关系存在差异；2) 过去十年中社区结构有演化；3) 识别出大型城市枢纽作为空间上遥远社区的"熔炉"

Conclusion: LFMM方法解决了聚合网络社区检测的关键问题，提供聚合一致性的混合成员关系计算，能更准确地反映底层网络结构，特别适用于大规模社交网络分析

Abstract: Community detection is a critical tool for understanding the mesoscopic structure of large-scale networks. However, when applied to aggregated or coarse-grained social networks, disjoint community partitions cannot capture the diverse composition of community memberships within aggregated nodes. While existing mixed membership methods alleviate this issue, they may detected communities that are highly sensitive to the aggregation resolution, not reliably reflecting the underlying community structure of the underlying individual-level network. This paper presents the Link Fraction Mixed Membership (LFMM) method, which computes the mixed memberships of nodes in aggregated networks. Unlike existing mixed membership methods, LFMM is consistent under aggregation. Specifically, we show that it conserves community membership sums at different scales. The method is utilized to study a population-scale social network of the Netherlands, aggregated at different resolutions. Experiments reveal variation in community membership across different geographical regions and evolution over the last decade. In particular, we show how our method identifies large urban hubs that act as the melting pots of diverse, spatially remote communities.

</details>


### [84] [An Empirical Study of Collective Behaviors and Social Dynamics in Large Language Model Agents](https://arxiv.org/abs/2602.03775)
*Farnoosh Hashemi,Michael W. Macy*

Main category: cs.SI

TL;DR: 研究LLM驱动的社交媒体平台Chirper.ai上32K个LLM代理的700万条帖子和互动，发现LLM社交网络存在同质性和社会影响现象，但毒性语言模式与人类不同，并提出Chain of Social Thought方法防止有害内容


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地介入社会、文化和政治互动，需要探索重复互动是否会放大偏见或导致排他行为，研究LLM在社交媒体平台上的行为模式

Method: 分析Chirper.ai平台（LLM驱动的社交媒体）上32K个LLM代理一年的700万条帖子和互动，研究同质性、社会影响、毒性语言、意识形态倾向和社区极化，并提出Chain of Social Thought方法

Result: LLM社交网络表现出与人类相似的同质性和社会影响现象，但毒性语言的结构模式与人类不同，LLM帖子存在意识形态倾向和社区极化

Conclusion: LLM在社交媒体互动中表现出复杂的社会行为模式，需要干预措施防止潜在有害活动，提出的Chain of Social Thought方法能有效提醒LLM代理避免有害发帖

Abstract: Large Language Models (LLMs) increasingly mediate our social, cultural, and political interactions. While they can simulate some aspects of human behavior and decision-making, it is still underexplored whether repeated interactions with other agents amplify their biases or lead to exclusionary behaviors. To this end, we study Chirper.ai-an LLM-driven social media platform-analyzing 7M posts and interactions among 32K LLM agents over a year. We start with homophily and social influence among LLMs, learning that similar to humans', their social networks exhibit these fundamental phenomena. Next, we study the toxic language of LLMs, its linguistic features, and their interaction patterns, finding that LLMs show different structural patterns in toxic posting than humans. After studying the ideological leaning in LLMs posts, and the polarization in their community, we focus on how to prevent their potential harmful activities. We present a simple yet effective method, called Chain of Social Thought (CoST), that reminds LLM agents to avoid harmful posting.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [85] [AI Assisted Economics Measurement From Survey: Evidence from Public Employee Pension Choice](https://arxiv.org/abs/2602.02604)
*Tiancheng Wang,Krishna Sharma*

Main category: econ.EM

TL;DR: 开发了一种利用大语言模型从调查工具中提取测量结构的迭代框架，通过软映射将调查项目映射到潜在构念的稀疏分布，并通过样本外有效性测试来规范分类体系。


<details>
  <summary>Details</summary>
Motivation: 传统经济测量方法在处理复杂调查数据时存在局限性，需要一种能够直接从调查工具中提取测量结构、识别哪些语义成分包含行为信号，并阐明经济机制（如信念与约束）的方法。

Method: 使用大语言模型通过软映射将调查项目映射到潜在构念的稀疏分布，将协调后的回答聚合成受访者层面的子维度分数，并通过样本外增量有效性测试和判别效度诊断来规范分类体系。框架明确将迭代整合到测量构建过程中。

Result: 应用于大规模公共雇员退休计划调查时，该框架成功识别了哪些语义成分包含行为信号，并阐明了影响退休选择的经济机制（如信念与约束）。

Conclusion: 该方法为调查工具提供了可移植的测量审计，既能指导实证分析，也能指导调查设计，通过迭代优化确保测量灵活性只在带来稳定样本外性能提升时才被保留。

Abstract: We develop an iterative framework for economic measurement that leverages large language models to extract measurement structure directly from survey instruments. The approach maps survey items to a sparse distribution over latent constructs through what we term a soft mapping, aggregates harmonized responses into respondent level sub dimension scores, and disciplines the resulting taxonomy through out of sample incremental validity tests and discriminant validity diagnostics. The framework explicitly integrates iteration into the measurement construction process. Overlap and redundancy diagnostics trigger targeted taxonomy refinement and constrained remapping, ensuring that added measurement flexibility is retained only when it delivers stable out of sample performance gains. Applied to a large scale public employee retirement plan survey, the framework identifies which semantic components contain behavioral signal and clarifies the economic mechanisms, such as beliefs versus constraints, that matter for retirement choices. The methodology provides a portable measurement audit of survey instruments that can guide both empirical analysis and survey design.

</details>


### [86] [The Innovation Tax: Generative AI Adoption, Productivity Paradox, and Systemic Risk in the U.S. Banking Sector](https://arxiv.org/abs/2602.02607)
*Tatsuru Kikuchi*

Main category: econ.EM

TL;DR: 研究评估生成式AI对美国银行业生产率和系统性风险的影响，发现"生产力悖论"：AI采用银行表现优异但面临显著的"实施税"，且AI驱动的决策同步化创造了系统性风险新渠道


<details>
  <summary>Details</summary>
Motivation: 评估生成式人工智能在银行业的采用对生产率和系统性风险的因果影响，特别关注AI实施的成本效益和网络溢出效应

Method: 使用2018-2025年809家金融机构的SEC 10-Q文件和美联储监管数据，采用动态空间杜宾模型捕捉网络溢出效应，以及合成双重差分法以2022年11月ChatGPT发布作为外生冲击进行因果推断

Result: 发现"生产力悖论"：AI采用银行表现优异但面临428个基点的ROE下降（实施税），小银行受影响更大（517个基点vs大银行129个基点）。同时发现显著的正向网络溢出效应（ROA θ=0.161，ROE θ=0.679），表明银行业正在"算法耦合"，创造了系统性风险新渠道

Conclusion: 生成式AI在银行业的采用存在显著的实施成本，特别是对小银行不利，同时AI驱动的决策同步化创造了系统性风险新渠道，需要监管关注

Abstract: This paper evaluates the causal impact of Generative Artificial Intelligence (GenAI) adoption on productivity and systemic risk in the U.S. banking sector. Using a novel dataset linking SEC 10-Q filings to Federal Reserve regulatory data for 809 financial institutions over 2018--2025, we employ two complementary identification strategies: Dynamic Spatial Durbin Models (DSDM) to capture network spillovers and Synthetic Difference-in-Differences (SDID) for causal inference using the November 2022 ChatGPT release as an exogenous shock. Our findings reveal a striking ``Productivity Paradox'': while DSDM estimates show that AI-adopting banks are high performers ($β> 0$), the causal SDID analysis documents a significant ``Implementation Tax'' -- adopting banks experience a 428-basis-point decline in ROE as they absorb GenAI integration costs. This tax falls disproportionately on smaller institutions, with bottom-quartile banks suffering a 517-basis-point ROE decline compared to 129 basis points for larger banks, suggesting that economies of scale provide significant advantages in AI implementation. Most critically, our DSDM analysis reveals significant positive spillovers ($θ= 0.161$ for ROA, $p < 0.01$; $θ= 0.679$ for ROE, $p < 0.05$), with spillovers among large banks reaching $θ= 3.13$ for ROE, indicating that the U.S. banking system is becoming ``algorithmically coupled.'' This synchronization of AI-driven decision-making creates a new channel for systemic contagion: a technical failure in widely-adopted AI models could trigger correlated shocks across the entire financial network.

</details>


### [87] [Predicting Well-Being with Mobile Phone Data: Evidence from Four Countries](https://arxiv.org/abs/2602.02805)
*M. Merritt Smith,Emily Aiken,Joshua E. Blumenstock,Sveta Milusheva*

Main category: econ.EM

TL;DR: 该研究系统评估了使用手机数据预测家庭福祉的潜力，发现在四个发展中国家，财富指数和多维贫困等长期贫困指标比消费支出更容易预测，而食物安全和心理健康等短期脆弱性指标难以预测。通话和短信行为数据比移动互联网、移动支付和话费充值等元数据更具预测力。


<details>
  <summary>Details</summary>
Motivation: 研究动机是系统评估移动电话数据在预测家庭福祉方面的潜力。随着移动电话在发展中国家的普及，研究人员希望了解这些数据能否准确反映家庭经济状况，为贫困监测和政策制定提供更及时、低成本的数据来源。

Method: 研究方法包括在四个国家（阿富汗、科特迪瓦、马拉维、多哥）进行平行、标准化的机器学习实验。研究评估了不同福祉指标的预测准确性，比较了不同类型手机数据（通话短信行为、移动互联网使用、移动支付交易、话费充值）的预测能力，并分析了训练数据量（1,000-4,500+观测值）和样本异质性（全国代表性样本 vs 城乡单独样本）对模型性能的影响。

Result: 研究结果显示：1）长期贫困指标（财富指数：皮尔逊相关系数0.20-0.59；多维贫困：0.29-0.57）比消费支出（0.04-0.54）更容易预测；2）食物安全和心理健康等短期脆弱性指标非常难以预测；3）通话和短信行为数据比其他手机数据类型更具预测力；4）预测准确性在前1,000-2,000个训练观测值中快速提升，超过4,500个观测值后仍有持续改善；5）全国代表性样本的准确性比仅城市或仅农村样本高20-70%。

Conclusion: 研究结论表明，移动电话数据在预测长期家庭福祉方面具有实用价值，特别是使用通话和短信行为数据预测财富指数和多维贫困。然而，对于短期脆弱性指标的预测能力有限。研究强调了样本异质性和训练数据量对模型性能的重要性，为未来利用手机数据进行贫困监测提供了实证基础。

Abstract: We provide systematic evidence on the potential for estimating household well-being from mobile phone data. Using data from four countries - Afghanistan, Cote d'Ivoire, Malawi, and Togo - we conduct parallel, standardized machine learning experiments to assess which measures of welfare can be most accurately predicted, which types of phone data are most useful, and how much training data is required. We find that long-term poverty measures such as wealth indices (Pearson's rho = 0.20-0.59) and multidimensional poverty (rho = 0.29-0.57) can be predicted more accurately than consumption (rho = 0.04 - 0.54); transient vulnerability measures like food security and mental health are very difficult to predict. Models using calls and text message behavior are more predictive than those using metadata on mobile internet usage, mobile money transactions, and airtime top-ups. Predictive accuracy improves rapidly through the first 1,000-2,000 training observations, with continued gains beyond 4,500 observations. Model performance depends strongly on sample heterogeneity: nationally-representative samples yield 20-70 percent higher accuracy than urban-only or rural-only samples.

</details>


### [88] [Unbiased Estimation of Central Moments in Unbalanced Two- and Three-Level Models](https://arxiv.org/abs/2602.03469)
*Dan Ben-Moshe,David Genesove*

Main category: econ.EM

TL;DR: 本文为不平衡组大小的多水平随机效应模型推导了中心矩的闭式无偏估计量。在二水平模型中，提供了组水平和观测水平平均下的第二、三、四阶中心矩无偏估计；在三水平模型中，提供了第二、三阶中心矩的无偏估计。


<details>
  <summary>Details</summary>
Motivation: 多水平随机效应模型在社会科学、教育研究、医学等领域广泛应用，但现有方法通常假设组大小平衡。实际数据中组大小往往不平衡，这会影响中心矩估计的准确性。需要为不平衡组大小情况开发无偏估计方法。

Method: 采用数学推导方法，为多水平随机效应模型建立闭式无偏估计量。在二水平模型中，分别针对组水平平均和观测水平平均两种情况，推导第二、三、四阶中心矩的无偏估计；在三水平模型中，推导第二、三阶中心矩的无偏估计。

Result: 成功推导出不平衡组大小情况下多水平随机效应模型中心矩的闭式无偏估计量。这些估计量在数学上保证了无偏性，适用于实际应用中常见的组大小不平衡情况。

Conclusion: 本文为不平衡组大小的多水平随机效应模型提供了中心矩的无偏估计方法，填补了现有方法的空白，提高了实际应用中参数估计的准确性和可靠性。

Abstract: This paper derives closed-form unbiased estimators of central moments in multilevel random-effects models with unbalanced group sizes. In a two-level model, we provide unbiased estimators for the second, third, and fourth central moments under both group-level and observation-level averaging. In a three-level model, we provide unbiased estimators for the second and third central moments.

</details>


### [89] [Global Testing in Multivariate Regression Discontinuity Designs](https://arxiv.org/abs/2602.03819)
*Artem Samiahulin*

Main category: econ.EM

TL;DR: 提出一种新的多元回归断点设计全局检验方法，通过结合多元机器学习估计器和距离聚合策略，解决小样本下现有方法尺寸扭曲的问题。


<details>
  <summary>Details</summary>
Motivation: 多元RD设计在实证应用中日益增多，但现有全局检验方法在小样本中表现不佳，因为边界点附近观测值稀疏导致严重的尺寸扭曲。

Method: 整合多元机器学习估计器与基于距离的聚合策略，构建在小样本下仍可靠的检验统计量。

Result: 模拟显示该方法保持接近名义尺寸和强检验力，在标准多元估计器失效的情况下仍能正常工作。

Conclusion: 该方法作为现有多元RD估计器的补充工具，能有效解决小样本问题，并在实证应用中展示了其实现方式。

Abstract: Regression discontinuity (RD) designs with multiple running variables arise in a growing number of empirical applications, including geographic boundaries and multi-score assignment rules. Although recent methodological work has extended estimation and inference tools to multivariate settings, far less attention has been devoted to developing global testing methods that formally assess whether a discontinuity exists anywhere along a multivariate treatment boundary. Existing approaches perform well in large samples, but can exhibit severe size distortions in moderate or small samples due to the sparsity of observations near any particular boundary point. This paper introduces a complementary global testing procedure that mitigates the small-sample weaknesses of existing multivariate RD methods by integrating multivariate machine learning estimators with a distance-based aggregation strategy, yielding a test statistic that remains reliable with limited data. Simulations demonstrate that the proposed method maintains near-nominal size and strong power, including in settings where standard multivariate estimators break down. The procedure is applied to an empirical setting to demonstrate its implementation and to illustrate how it can complement existing multivariate RD estimators.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [90] [De-Linearizing Agent Traces: Bayesian Inference of Latent Partial Orders for Efficient Execution](https://arxiv.org/abs/2602.02806)
*Dongqing Li,Zheqiao Cheng,Geoff K. Nicholls,Quyu Kong*

Main category: stat.AP

TL;DR: BPOP：基于贝叶斯框架从噪声线性化轨迹中推断潜在依赖偏序，通过MCMC推理避免#P-hard计算，在云配置任务中恢复依赖结构更准确，支持编译执行器减少token使用和执行时间。


<details>
  <summary>Details</summary>
Motivation: 当前智能体执行程序工作流时通常采用顺序动作轨迹，这掩盖了潜在的并发性并导致重复的逐步推理。需要从噪声线性化轨迹中恢复潜在的依赖结构。

Method: BPOP是一个贝叶斯框架，将轨迹建模为基础图结构的随机线性扩展，通过可处理的前沿-softmax似然进行高效MCMC推理，避免了对线性扩展的#P-hard边际化计算。

Result: 在开源的Cloud-IaC-6云配置任务套件和WFCommons科学工作流上评估，BPOP比仅基于轨迹和过程挖掘的基线方法更准确地恢复依赖结构，推断的图支持编译执行器，显著减少了token使用和执行时间。

Conclusion: BPOP能够从噪声线性化轨迹中有效推断潜在依赖偏序，为智能体工作流执行提供了更高效的依赖结构恢复方法，支持编译优化减少资源消耗。

Abstract: I agents increasingly execute procedural workflows as sequential action traces, which obscures latent concurrency and induces repeated step-by-step reasoning. We introduce BPOP, a Bayesianframework that infers a latent dependency partial order from noisy linearized traces. BPOP models traces as stochastic linear extensions of an underlying graph and performs efficient MCMC inference via a tractable frontier-softmax likelihood that avoids #P-hard marginalization over linear extensions. We evaluate on our open-sourced Cloud-IaC-6, a suite of cloud provisioning tasks with heterogeneous LLM-generated traces, and WFCommons scientific workflows. BPOP recover dependency structure more accurately than trace-only and process-mining baselines, and the inferred graphs support a compiled executor that prunes irrelevant context, yielding substantial reductions in token usage and execution time.

</details>


### [91] [Downscaling land surface temperature data using edge detection and block-diagonal Gaussian process regression](https://arxiv.org/abs/2602.02813)
*Sanjit Dandapanthula,Margaret Johnson,Madeleine Pascolini-Campbell,Glynn Hulley,Mikael Kuusela*

Main category: stat.AP

TL;DR: 提出一种基于块对角高斯过程的统计方法，用于降尺度ECOSTRESS任务的地表温度数据，利用Landsat 8数据识别农田边界结构，实现高分辨率温度估计和不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 高分辨率地表温度(LST)估计对于估算蒸散发（植物水分利用）至关重要，这是农业应用中的核心参数。ECOSTRESS任务提供LST数据，但需要降尺度以获得更高分辨率。

Method: 使用Landsat 8数据通过边缘检测技术识别农田边界，捕捉空间域中的块结构。提出块对角高斯过程(BDGP)模型，利用农田间的独立性提高计算效率，并考虑ECOSTRESS观测的支持变化。通过高斯过程回归获得高分辨率LST估计和不确定性量化。

Result: 该方法能够产生可靠的高分辨率LST估计，展示了在实际应用中的可行性。

Conclusion: 提出的BDGP方法在农业、城市规划和气候研究等领域具有潜在应用价值，能够提供实用的高分辨率LST估计解决方案。

Abstract: Accurate and high-resolution estimation of land surface temperature (LST) is crucial in estimating evapotranspiration, a measure of plant water use and a central quantity in agricultural applications. In this work, we develop a novel statistical method for downscaling LST data obtained from NASA's ECOSTRESS mission, using high-resolution data from the Landsat 8 mission as a proxy for modeling agricultural field structure. Using the Landsat data, we identify the boundaries of agricultural fields through edge detection techniques, allowing us to capture the inherent block structure present in the spatial domain. We propose a block-diagonal Gaussian process (BDGP) model that captures the spatial structure of the agricultural fields, leverages independence of LST across fields for computational tractability, and accounts for the change of support present in ECOSTRESS observations. We use the resulting BDGP model to perform Gaussian process regression and obtain high-resolution estimates of LST from ECOSTRESS data, along with uncertainty quantification. Our results demonstrate the practicality of the proposed method in producing reliable high-resolution LST estimates, with potential applications in agriculture, urban planning, and climate studies.

</details>


### [92] [On the consistent and scalable detection of spatial patterns](https://arxiv.org/abs/2602.02825)
*Jiayu Su,Jun Hou Fung,Haoyu Wang,Dian Yang,David A. Knowles,Raul Rabadan*

Main category: stat.AP

TL;DR: 该论文统一了空间模式检测方法，揭示了Moran's I等常用方法的不一致性，提出了可扩展的修正方案，实现了百万级空间位置和单细胞谱系追踪数据的稳健模式检测。


<details>
  <summary>Details</summary>
Motivation: 空间模式检测对科学发现至关重要，但现有方法缺乏统计共识，且在处理大规模空间组学数据时面临计算障碍。

Method: 通过单一二次型统一主要方法，推导一般一致性条件，揭示Moran's I等常用方法的不一致性，提出可扩展的修正方案。

Result: 开发出能够在数百万空间位置和单细胞谱系追踪数据集中进行稳健模式检测的测试方法。

Conclusion: 该研究为空间模式检测提供了统一的理论框架和可扩展的解决方案，解决了现有方法的统计和计算限制。

Abstract: Detecting spatial patterns is fundamental to scientific discovery, yet current methods lack statistical consensus and face computational barriers when applied to large-scale spatial omics datasets. We unify major approaches through a single quadratic form and derive general consistency conditions. We reveal that several widely used methods, including Moran's I, are inconsistent, and propose scalable corrections. The resulting test enables robust pattern detection across millions of spatial locations and single-cell lineage-tracing datasets.

</details>


### [93] [Scalable non-separable spatio-temporal Gaussian process models for large-scale short-term weather prediction](https://arxiv.org/abs/2602.03609)
*Tim Gyger,Reinhard Furrer,Fabio Sigrist*

Main category: stat.AP

TL;DR: 提出三种可扩展的时空高斯过程方法，用于美国大陆的日最高温和降水短期预报，通过改进邻居选择、诱导点算法和GPU加速实现大规模应用。


<details>
  <summary>Details</summary>
Motivation: 监测每日天气对气候科学、农业和环境规划至关重要，但完全概率的时空模型在大陆尺度上计算成本过高，需要可扩展的解决方案。

Method: 基于三种近似方法（FITC、Vecchia、VIF混合），引入三个扩展：可扩展的相关性邻居选择策略、时空kMeans++诱导点选择算法、GPU加速的矩阵运算和邻居搜索。

Result: 使用合成实验和包含约170万时空观测的NOAA数据集验证，模型在预测性能、参数估计和计算效率方面表现良好，能够实现准确的大陆尺度预报。

Conclusion: 可扩展的高斯过程模型能够在大陆尺度上提供准确的天气预报，同时保持计算可行性，为天气应用提供了实用工具。

Abstract: Monitoring daily weather fields is critical for climate science, agriculture, and environmental planning, yet fully probabilistic spatio-temporal models become computationally prohibitive at continental scale. We present a case study on short-term forecasting of daily maximum temperature and precipitation across the conterminous United States using novel scalable spatio-temporal Gaussian process methodology. Building on three approximation families - inducing-point methods (FITC), Vecchia approximations, and a hybrid Vecchia-inducing-point full-scale approach (VIF) - we introduce three extensions that address key bottlenecks in large space-time settings: (i) a scalable correlation-based neighbor selection strategy for Vecchia approximations with point-referenced data, enabling accurate conditioning under complex dependence structures, (ii) a space-time kMeans++ inducing-point selection algorithm, and (iii) GPU-accelerated implementations of computationally expensive operations, including matrix operations and neighbor searches. Using both synthetic experiments and a large NOAA station dataset containing approximately 1.7 million space-time observations, we analyze the models with respect to predictive performance, parameter estimation, and computational efficiency. Our results demonstrate that scalable Gaussian process models can yield accurate continental-scale forecasts while remaining computationally feasible, offering practical tools for weather applications.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [94] [CodeGuard: Improving LLM Guardrails in CS Education](https://arxiv.org/abs/2602.02509)
*Nishat Raihan,Noah Erdachew,Jayoti Devi,Joanna C. S. Santos,Marcos Zampieri*

Main category: cs.CY

TL;DR: 论文提出CodeGuard框架，包含分类法、数据集和PromptShield模型，用于检测CS教育中LLM的不安全提示，减少30-65%有害代码生成


<details>
  <summary>Details</summary>
Motivation: LLM在CS教育中广泛用于代码生成和反馈，但容易受到恶意提示攻击，威胁学生学习与学术诚信，现有防护措施存在不足

Method: 提出CodeGuard框架：1) 首创提示分类法；2) 包含8000个提示的CodeGuard数据集；3) PromptShield轻量级句子编码器模型，实时检测不安全提示

Result: PromptShield达到0.93 F1分数，优于现有方法；CodeGuard减少30-65%有害或违规代码生成，且不影响正常教育任务性能

Conclusion: CodeGuard为教育AI系统提供全面防护框架，有效提升LLM在CS教育中的安全性，代码、数据集和评估脚本已开源

Abstract: Large language models (LLMs) are increasingly embedded in Computer Science (CS) classrooms to automate code generation, feedback, and assessment. However, their susceptibility to adversarial or ill-intentioned prompts threatens student learning and academic integrity. To cope with this important issue, we evaluate existing off-the-shelf LLMs in handling unsafe and irrelevant prompts within the domain of CS education. We identify important shortcomings in existing LLM guardrails which motivates us to propose CodeGuard, a comprehensive guardrail framework for educational AI systems. CodeGuard includes (i) a first-of-its-kind taxonomy for classifying prompts; (ii) the CodeGuard dataset, a collection of 8,000 prompts spanning the taxonomy; and (iii) PromptShield, a lightweight sentence-encoder model fine-tuned to detect unsafe prompts in real time. Experiments show that PromptShield achieves 0.93 F1 score, surpassing existing guardrail methods. Additionally, further experimentation reveals that CodeGuard reduces potentially harmful or policy-violating code completions by 30-65% without degrading performance on legitimate educational tasks. The code, datasets, and evaluation scripts are made freely available to the community.

</details>


### [95] [Beyond Translation: Cross-Cultural Meme Transcreation with Vision-Language Models](https://arxiv.org/abs/2602.02510)
*Yuming Zhao,Peiyi Zhang,Oana Ignat*

Main category: cs.CY

TL;DR: 论文提出基于视觉语言模型的混合转创框架，用于跨文化表情包转创，并构建了大规模中英双向数据集。研究发现当前模型能有限度完成跨文化表情包转创，但存在方向不对称性：美转中质量优于中转美。


<details>
  <summary>Details</summary>
Motivation: 表情包是在线交流的普遍形式，但其文化特异性给跨文化适应带来挑战。需要研究如何保持交流意图和幽默感的同时，适应文化特定参考的跨文化表情包转创任务。

Method: 提出基于视觉语言模型的混合转创框架，构建大规模中英双向表情包数据集（6,315对），使用人工判断和自动评估分析跨文化方向上的转创质量。

Result: 当前视觉语言模型能有限度完成跨文化表情包转创，但存在明显方向不对称性：美转中质量始终高于中转美。识别了幽默和视觉文本设计中哪些方面能跨文化传递，哪些仍具挑战性。

Conclusion: 提出了评估跨文化多模态生成的框架，代码和数据集已公开。研究揭示了跨文化表情包转创的现状和挑战，为未来研究提供了基准和方向。

Abstract: Memes are a pervasive form of online communication, yet their cultural specificity poses significant challenges for cross-cultural adaptation. We study cross-cultural meme transcreation, a multimodal generation task that aims to preserve communicative intent and humor while adapting culture-specific references. We propose a hybrid transcreation framework based on vision-language models and introduce a large-scale bidirectional dataset of Chinese and US memes. Using both human judgments and automated evaluation, we analyze 6,315 meme pairs and assess transcreation quality across cultural directions. Our results show that current vision-language models can perform cross-cultural meme transcreation to a limited extent, but exhibit clear directional asymmetries: US-Chinese transcreation consistently achieves higher quality than Chinese-US. We further identify which aspects of humor and visual-textual design transfer across cultures and which remain challenging, and propose an evaluation framework for assessing cross-cultural multimodal generation. Our code and dataset are publicly available at https://github.com/AIM-SCU/MemeXGen.

</details>


### [96] [Training Data Governance for Brain Foundation Models](https://arxiv.org/abs/2602.02511)
*Margot Hanley,Jiunn-Tyng Yeh,Ryan Rodriguez,Jack Pilkington,Nita Farahany*

Main category: cs.CY

TL;DR: 大脑基础模型将基础模型范式引入神经科学领域，利用大规模脑数据训练通用AI系统，但面临独特的伦理挑战


<details>
  <summary>Details</summary>
Motivation: 大脑基础模型在神经数据上训练，这些数据比文本或图像具有更强的保护要求，但基础模型范式却对这些数据进行了大规模重新利用、跨情境拼接和开放式下游应用，且商业开发者也能参与，而治理框架却分散不清

Method: 首先描述大脑基础模型的技术基础和训练数据生态系统，然后借鉴AI伦理、神经伦理和生物伦理，围绕隐私、同意、偏见、利益共享和治理等方面组织关注点

Result: 针对每个伦理维度提出了议程设置问题和基线保障措施，为这一新兴领域的发展提供指导框架

Conclusion: 大脑基础模型开辟了新的规范领域，需要在技术发展的同时建立适当的伦理框架和治理机制，以平衡创新与保护

Abstract: Brain foundation models bring the foundation model paradigm to the field of neuroscience. Like language and image foundation models, they are general-purpose AI systems pretrained on large-scale datasets that adapt readily to downstream tasks. Unlike text-and-image based models, however, they train on brain data: large-datasets of EEG, fMRI, and other neural data types historically collected within tightly governed clinical and research settings. This paper contends that training foundation models on neural data opens new normative territory. Neural data carry stronger expectations of, and claims to, protection than text or images, given their body-derived nature and historical governance within clinical and research settings. Yet the foundation model paradigm subjects them to practices of large-scale repurposing, cross-context stitching, and open-ended downstream application. Furthermore, these practices are now accessible to a much broader range of actors, including commercial developers, against a backdrop of fragmented and unclear governance. To map this territory, we first describe brain foundation models' technical foundations and training-data ecosystem. We then draw on AI ethics, neuroethics, and bioethics to organize concerns across privacy, consent, bias, benefit sharing, and governance. For each, we propose both agenda-setting questions and baseline safeguards as the field matures.

</details>


### [97] [Measuring Individual User Fairness with User Similarity and Effectiveness Disparity](https://arxiv.org/abs/2602.02516)
*Theresia Veronika Rampisela,Maria Maistro,Tuukka Ruotsalo,Christina Lioma*

Main category: cs.CY

TL;DR: 提出PUF评估指标，同时考虑推荐效果差异和用户相似度，以更全面地衡量推荐系统中的个体用户公平性。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统个体用户公平性评估指标存在缺陷：要么只关注推荐效果差异而忽略用户相似度，要么只关注相似用户推荐差异而忽略项目相关性。两者都是公平性的重要方面，但现有指标未能同时考虑。

Method: 提出Pairwise User unFairness (PUF)评估指标，该指标同时考虑用户相似度和推荐效果差异。通过成对用户比较，结合相似度权重和效果差异来计算公平性。

Result: 在4个数据集和7个排序器上的实验验证表明，PUF能一致地同时捕捉用户相似度和效果差异。相比之下，其他指标要么对效果差异几乎不敏感，要么完全忽略用户相似度。

Conclusion: PUF是第一个能可靠地同时捕捉用户相似度和推荐效果差异的个体用户公平性评估指标，填补了现有公平性定义不完整的空白。

Abstract: Individual user fairness is commonly understood as treating similar users similarly. In Recommender Systems (RSs), several evaluation measures exist for quantifying individual user fairness. These measures evaluate fairness via either: (i) the disparity in RS effectiveness scores regardless of user similarity, or (ii) the disparity in items recommended to similar users regardless of item relevance. Both disparity in recommendation effectiveness and user similarity are very important in fairness, yet no existing individual user fairness measure simultaneously accounts for both. In brief, current user fairness evaluation measures implement a largely incomplete definition of fairness. To fill this gap, we present Pairwise User unFairness (PUF), a novel evaluation measure of individual user fairness that considers both effectiveness disparity and user similarity. PUF is the only measure that can express this important distinction. We empirically validate that PUF does this consistently across 4 datasets and 7 rankers, and robustly when varying user similarity or effectiveness. In contrast, all other measures are either almost insensitive to effectiveness disparity or completely insensitive to user similarity. We contribute the first RS evaluation measure to reliably capture both user similarity and effectiveness in individual user fairness. Our code: https://github.com/theresiavr/PUF-individual-user-fairness-recsys.

</details>


### [98] [Evaluation of Large Language Models' educational feedback in Higher Education: potential, limitations and implications for educational practice](https://arxiv.org/abs/2602.02519)
*Daniele Agostini,Federica Picasso*

Main category: cs.CY

TL;DR: 研究评估大型语言模型（LLMs）在高等教育中生成反馈的潜力，发现LLMs能生成结构良好的反馈，但需要清晰的上下文和指令指导。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能技术在教育领域的应用日益广泛，理解AI对反馈生成的影响对于识别其潜在益处和建立有效实施策略至关重要。反馈实践在高等教育中对提升教学、学习和评估过程起着关键作用。

Method: 研究使用Hughes、Smith和Creese的分析框架评估AI生成反馈。向七个大型语言模型提供由大学教师开发的结构化评分标准，要求基于该标准生成定量评估和定性反馈。评估对象是包容性教学与学习培训课程中学生设计的项目。

Result: 研究发现大型语言模型能够生成结构良好的反馈，并具有作为可持续且有意义的反馈工具的潜力。AI生成的反馈在促进形成性学习体验方面表现出有效性。

Conclusion: 大型语言模型作为反馈工具具有巨大潜力，但需要清晰的上下文信息和明确的指令指导才能发挥最佳效果。这为未来AI在教育反馈中的应用提供了重要启示。

Abstract: The importance of managing feedback practices in higher education has been widely recognised, as they play a crucial role in enhancing teaching, learning, and assessment processes. In today's educational landscape, feedback practices are increasingly influenced by technological advancements, particularly artificial intelligence (AI). Understanding the impact of AI on feedback generation is essential for identifying its potential benefits and establishing effective implementation strategies. This study examines how AI-generated feedback supports student learning using a well-established analytical framework. Specifically, feedback produced by different Large Language Models (LLMs) was assessed in relation to student-designed projects within a training course on inclusive teaching and learning. The evaluation process involved providing seven LLMs with a structured rubric, developed by the university instructor, which defined specific criteria and performance levels. The LLMs were tasked with generating both quantitative assessments and qualitative feedback based on this rubric. The AI-generated feedback was then analysed using Hughes, Smith, and Creese's framework to evaluate its structure and effectiveness in fostering formative learning experiences. Overall, these findings indicate that LLMs can generate well-structured feedback and hold great potential as a sustainable and meaningful feedback tool, provided they are guided by clear contextual information and a well-defined instructions that will be explored further in the conclusions.

</details>


### [99] [Artificial Intelligence for Inclusive Engineering Education: Advancing Equality, Diversity, and Ethical Leadership](https://arxiv.org/abs/2602.02520)
*Mona G. Ibrahim,Riham Hilal*

Main category: cs.CY

TL;DR: 该论文提出了一种基于伦理的AI技术方法，用于促进STEM教育中的公平、多样性和包容性，支持联合国2030年可持续发展议程中的性别平等和减少不平等目标。


<details>
  <summary>Details</summary>
Motivation: 尽管AI技术在教育领域取得了进步，但在性别平等、全球文化代表性和STEM教育机会获取方面仍存在差距。论文旨在解决这些不平等问题，支持联合国可持续发展目标。

Method: 采用综合策略，结合批判性思维方法分析全球案例研究，使用基于AI的自适应平台解决教育包容性差距，并提出包含伦理领导力和数据驱动的包容性测量模型。

Result: 研究表明，使用AI技术不仅能提高包容性，还能促进STEM教育机会获取的公平性，为全球教育系统转型提供了实证支持。

Conclusion: 论文强调需要将教育转型为全球系统，通过伦理AI技术促进可持续发展目标，特别是在性别平等和减少不平等方面的应用具有重要价值。

Abstract: AI technology development has transformed the field of engineering education with its adaptivity-driven, data-based, and ethical-led learning platforms that promote equity, diversity, and inclusivity. But with so much progress being made in so many areas, there are unfortunately gaps in gender equity, representation in cultures around the world, and access to education and jobs in stem education. The paper describes an ethical approach to using AI technology that supports the United Nations 2030 agenda for sustainability. In particular, this includes both Goal 5--Gender Equity--and Goal 10--Reducing Inequalities. Based on a synthesis strategy using both critical thinking strategies related to case studies around the world using AI-based adaptivity platforms to address equity gaps related to education inclusion. The model presented offers a synthesis solution that includes ethical leadership data-related to equity to measure inclusivity based upon sustainability thinking. The result has demonstrated that using AI technology not only increases inclusivity but promotes equity related to access to education in stem education access. Finally, there are concluding remarks related to transforming education into a global system.

</details>


### [100] [The First Mass Protest on Threads: Multimodal Mobilization and AI-Generated Visuals in Taiwan's Bluebird Movement](https://arxiv.org/abs/2602.02640)
*Ho-Chun Herbert Chang,Tracy Weener*

Main category: cs.CY

TL;DR: 研究分析2024年台湾蓝鸟运动在Threads平台上的抗议传播模式，发现算法曝光与用户认可存在党派不对称，文本策略以纪念、个人证词和行动号召驱动传播，视觉策略呈现人类照片与AI生成可爱符号的分化。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解数字环境变化下的抗议传播，特别是Threads作为新兴平台在台湾的高渗透率（占全球流量24%），以及生成式AI如何重塑当代抗议的符号表达。

Method: 采用62,321篇帖子和21,572张图像数据集，结合LLM零样本标注、梯度提升树和SHAP解释器，分析文本和视觉模态的注意力供给与需求。

Result: 发现三个动态：1）算法曝光与用户认可的党派不对称；2）文本策略以纪念、个人证词和行动号召驱动传播；3）视觉策略分化，人类照片集中曝光讨论，AI生成的动植物符号作为动员工具和党派攻击。

Conclusion: Threads既是民主抗争的放大器也是过滤器，扩展了情感和视觉传染理论，提出"可爱毒性"概念——以可爱美学包装的政治攻击，展示生成式AI如何重塑当代抗议的符号库。

Abstract: The 2024 Bluebird Movement in Taiwan marked one of the largest youth-led protests in the country's democratic history, mobilizing over 100,000 demonstrators in response to parliamentary reforms. Unlike the 2014 Sunflower Movement, Bluebird unfolded within a transformed digital environment dominated by Threads, Meta's new microblogging platform that$\unicode{x2013}$uniquely$\unicode{x2013}$draws 24% of its global traffic from Taiwan. Leveraging a dataset of 62,321 posts and 21,572 images, this study analyzes how protest communication developed across textual and visual modalities. We combine LLM zero-shot annotation, gradient-boosting trees, and SHAP explainers to disambiguate the supply and demand of attention. Results reveal three dynamics: (1) partisan asymmetries between algorithmic exposure and user endorsement, with anti-DPP content surfaced more widely but anti-KMT and pro-DPP content more actively recirculated; (2) textual repertoires centered on commemorations, personal testimonies, and calls to action as key drivers of virality; and (3) a bifurcation in visual strategies, where human photographs concentrated exposure and discussion, while AI-generated animal and plant symbols circulated as mobilization tools and partisan attacks. These findings demonstrate how Threads functioned as both an amplifier and filter of democratic contention, extending theories of emotional and visual contagion by showing how generative AI reshapes symbolic repertoires in contemporary protest through what we term kawaii toxicity$\unicode{x2013}$political attacks cloaked in aesthetics of cuteness.

</details>


### [101] [Reshaping Perception Through Technology: From Ancient Script to Large Language Models](https://arxiv.org/abs/2602.02794)
*Parham Pourdavood,Michael Jacob*

Main category: cs.CY

TL;DR: 论文认为应将AI视为一种重塑感知技能和创造力的媒介，而非竞争对手，借鉴麦克卢汉"媒介即信息"理论，分析从DNA到LLMs的技术如何塑造认知，指出技术越先进越与生理脱钩，带来更大创造潜力但也增加虚假和操纵风险。


<details>
  <summary>Details</summary>
Motivation: 当前通常将感知视为对刺激的被动反应，忽视了不同媒介的物理特性如何独特地塑造认知。论文旨在通过麦克卢汉的"媒介即信息"理论，重新审视从DNA到LLMs的技术如何影响人类感知，特别关注LLMs作为新型媒介带来的机遇与挑战。

Method: 采用历史追溯方法，分析从DNA、神经系统、语言、文字、音乐到LLMs的技术谱系，考察每种媒介如何独特地塑造人类认知。借鉴麦克卢汉的媒介理论，探讨技术发展与生理脱钩的关系，以及人类对新技术智能的投射倾向。

Result: 发现随着技术变得更先进且与生理脱钩，它们既带来更大的创造潜力（更高效的游戏、存储和传输），也引入人工性和潜在的不真实与操纵风险。LLMs特别突出这种张力，能快速生成与人类创作难以区分的内容。人类倾向于将智能投射到新技术上（如古代对文字的反应）。

Conclusion: AI不应被视为竞争对手，而应被理解为一种重塑感知技能和实现新形式创造力的媒介。这种媒介视角有助于更全面地理解LLMs等先进技术如何影响人类认知和创造力，同时认识到伴随的风险和挑战。

Abstract: Large language models are reshaping how we create and access information, yet we typically view perception as merely reactive to stimuli, overlooking how the physical qualities of different media uniquely shape cognition. Drawing on Marshall McLuhan's insight that the medium is the massage, we trace a lineage of technologies -- from DNA and the nervous system to language, writing, music, and now LLMs -- that mold perception in distinct ways. We observe that as technologies become more advanced and decoupled from our physiology, they introduce both greater creative potential and greater risk: they enable more efficient play, storage, and transmission, while also introducing artificiality and the potential for inauthenticity and manipulation. This tension is particularly acute with LLMs, which allow rapid, playful generation of content increasingly indistinguishable from human-created work. Noting that humans have a recurring tendency to project intelligence onto novel technologies (a pattern visible in ancient responses to writing), we argue that AI should be framed not as a competitor but as a medium that reshapes perceptual skills and enables new forms of creativity.

</details>


### [102] [Reading Between the Tokens: Improving Preference Predictions through Mechanistic Forecasting](https://arxiv.org/abs/2602.02882)
*Sarah Ball,Simeon Allmendinger,Niklas Kühl,Frauke Kreuter*

Main category: cs.CY

TL;DR: 论文提出"机制预测"方法，通过探测LLM内部表征而非仅分析输出，来预测人类偏好（以选举预测为例），发现利用内部知识能提高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 当前使用大语言模型预测人类偏好的方法仅依赖模型输出分析，忽略了底层机制。作者希望探索通过探测模型内部表征是否能提供更有效的偏好预测方法。

Method: 提出"机制预测"方法，在7个模型、6个国家选举、多种人物属性和提示变体上，分析了超过2400万个配置，系统研究人口统计和意识形态信息如何激活模型内部的政党编码组件。

Result: 利用内部知识的机制预测相比仅依赖表层预测能提高准确性，效果因属性类型（人口统计vs观点）、政党、国家背景和模型而异。LLM的潜在表征结构包含系统性的、可利用的人类偏好信息。

Conclusion: LLM的潜在表征结构包含关于人类偏好的系统性信息，为在社会科学预测任务中使用语言模型开辟了新路径，机制预测方法提供了与表层预测根本不同的预测视角。

Abstract: Large language models are increasingly used to predict human preferences in both scientific and business endeavors, yet current approaches rely exclusively on analyzing model outputs without considering the underlying mechanisms. Using election forecasting as a test case, we introduce mechanistic forecasting, a method that demonstrates that probing internal model representations offers a fundamentally different - and sometimes more effective - approach to preference prediction. Examining over 24 million configurations across 7 models, 6 national elections, multiple persona attributes, and prompt variations, we systematically analyze how demographic and ideological information activates latent party-encoding components within the respective models. We find that leveraging this internal knowledge via mechanistic forecasting (opposed to solely relying on surface-level predictions) can improve prediction accuracy. The effects vary across demographic versus opinion-based attributes, political parties, national contexts, and models. Our findings demonstrate that the latent representational structure of LLMs contains systematic, exploitable information about human preferences, establishing a new path for using language models in social science prediction tasks.

</details>


### [103] [From Hanging Out to Figuring It Out: Socializing Online as a Pathway to Computational Thinking](https://arxiv.org/abs/2602.03017)
*Samantha Shorey,Benjamin Mako Hill,Samuel C. Woolley*

Main category: cs.CY

TL;DR: 该研究通过分析Scratch平台的14000+评论，提出了"参与式调试"概念，并识别了促进这种学习实践的三个社会因素：持续社区、可识别问题和话题渗透性。


<details>
  <summary>Details</summary>
Motivation: 尽管社交互动是驱动青少年在线参与的重要动力，但平台难以利用这种参与度来促进学习。研究旨在理解在线平台中社交参与与学习之间的动态关系。

Method: 采用多阶段分析方法：1）归纳性发展"参与式调试"概念；2）通过内容分析确定该实践在Scratch平台上的普遍程度；3）通过用户活动的时间质性分析识别三个社会前因。

Result: 研究发现参与式调试是用户通过协作技术故障排除进行学习的实践。识别出三个促进参与式调试的社会前因：持续社区、可识别问题和话题渗透性（允许跨多个话题的对话）。

Conclusion: 研究提出了一个理论框架，强调在促进学习的愿望与驱动用户参与的兴趣导向子社区之间存在一种富有成效的张力，这对新媒体环境中的学习设计具有重要意义。

Abstract: Although socializing is a powerful driver of youth engagement online, platforms struggle to leverage engagement to promote learning. We seek to understand this dynamic using a multi-stage analysis of over 14,000 comments on Scratch, an online platform designed to support learning about programming. First, we inductively develop the concept of "participatory debugging" -- a practice through which users learn through collaborative technical troubleshooting. Second, we use a content analysis to establish how common the practice is on Scratch. Third, we conduct a qualitative analysis of user activity over time and identify three factors that serve as social antecedents of participatory debugging: (1) sustained community, (2) identifiable problems, and (3) what we call "topic porousness" to describe conversations that are able to span multiple topics. We integrate these findings in a theoretical framework that highlights a productive tension between the desire to promote learning and the interest-driven sub-communities that drive user engagement in many new media environments.

</details>


### [104] [Digital Lifelong Learning in the Age of AI: Trends and Insights](https://arxiv.org/abs/2602.03114)
*Geeta Puri,Nachamma Socklingam,Dorien Herremans*

Main category: cs.CY

TL;DR: 该研究调查了不同人群与数字学习平台的互动，重点关注学习者动机、游戏化效果和AI整合，发现疫情后数字学习的感知相关性显著增加，尤其是年轻人和女性群体。


<details>
  <summary>Details</summary>
Motivation: AI和大语言模型的快速发展加速了数字学习的采用，特别是在非正规教育领域。了解数字学习如何继续为成人和终身学习者发展变得越来越重要，因为数字学习已从补充资源转变为教育的重要支柱。

Method: 使用200名受访者的多调查数据和高级分析方法，研究不同人群与数字学习平台的互动，重点关注学习者动机、游戏化在数字学习中的有效性以及AI整合。

Result: 研究发现疫情后数字学习的感知相关性显著增加，特别是在年轻人和女性群体中，这与支持个性化学习的LLM驱动的AI工具的兴起相吻合。

Conclusion: 研究旨在为企业、政府政策制定者和教育工作者提供可行的见解，帮助他们优化数字学习产品以满足不断变化的劳动力需求。

Abstract: Rapid innovations in AI and large language models (LLMs) have accelerated the adoption of digital learning, particularly beyond formal education. What began as an emergency response during COVID-19 has shifted from a supplementary resource to an essential pillar of education. Understanding how digital learning continues to evolve for adult and lifelong learners is therefore increasingly important.
  This study examines how various demographics interact with digital learning platforms, focusing on the learner motivations, the effectiveness of gamification in digital learning, and the integration of AI. Using multi survey data from 200 respondents and advanced analytics, our findings reveal a notable increase in the perceived relevance of digital learning after the pandemic, especially among young adults and women, coinciding with the rise of LLM-powered AI tools that support personalized learning. We aim to provide actionable insights for businesses, government policymakers, and educators seeking to optimize their digital learning offerings to meet evolving workforce needs.

</details>


### [105] [The Personality Trap: How LLMs Embed Bias When Generating Human-Like Personas](https://arxiv.org/abs/2602.03334)
*Jacopo Amidei,Gregorio Ferreira,Mario Muñoz Serrano,Rubén Nieto,Andreas Kaltenbrunner*

Main category: cs.CY

TL;DR: LLMs生成人格问卷合成人群时存在WEIRD偏见，且高精神病性特质会导致对非二元和LGBTQ+群体的过度代表和病理化风险


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在生成基于人格问卷的合成人群时是否存在偏见，特别是社会人口学特征的分布偏差以及对边缘群体的潜在刻板印象和病理化问题

Method: 使用五个LLMs生成合成人群，首先评估生成人物社会人口学特征的代表性和偏见，以及人格特质的对齐程度；其次通过操纵输入特质最大化神经质和精神病性得分，观察模型输出变化

Result: LLMs能成功复现人格与社会人口学变量间的已知相关性，但所有模型都表现出明显的WEIRD偏见，偏向年轻、受过教育、白人、异性恋、西方个体，具有中间或进步政治观点和世俗或基督教信仰。当最大化精神病性时，多个模型过度代表非二元和LGBTQ+身份

Conclusion: LLMs生成心理基础合成人群既有潜力也有风险，需要谨慎使用以避免强化偏见和对边缘群体的病理化

Abstract: This paper examines biases in large language models (LLMs) when generating synthetic populations from responses to personality questionnaires. Using five LLMs, we first assess the representativeness and potential biases in the sociodemographic attributes of the generated personas, as well as their alignment with the intended personality traits. While LLMs successfully reproduce known correlations between personality and sociodemographic variables, all models exhibit pronounced WEIRD (western, educated, industrialized, rich and democratic) biases, favoring young, educated, white, heterosexual, Western individuals with centrist or progressive political views and secular or Christian beliefs. In a second analysis, we manipulate input traits to maximize Neuroticism and Psychoticism scores. Notably, when Psychoticism is maximized, several models produce an overrepresentation of non-binary and LGBTQ+ identities, raising concerns about stereotyping and the potential pathologization of marginalized groups. Our findings highlight both the potential and the risks of using LLMs to generate psychologically grounded synthetic populations.

</details>

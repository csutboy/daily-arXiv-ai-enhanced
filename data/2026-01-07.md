<div id=toc></div>

# Table of Contents

- [cs.SI](#cs.SI) [Total: 3]
- [cs.AI](#cs.AI) [Total: 26]
- [stat.AP](#stat.AP) [Total: 1]
- [econ.EM](#econ.EM) [Total: 1]
- [cs.CY](#cs.CY) [Total: 13]


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [1] [Modeling ICD-10 Morbidity and Multidimensional Poverty as a Spatial Network: Evidence from Thailand](https://arxiv.org/abs/2601.02848)
*Pratana Kukieattikool,Kittiya Ku-kiattikun,Anukool Noymai,Navaporn Surasvadi,Jantakarn Makma,Pubodin Pornratchpum,Watcharakon Noothong,Chainarong Amornbunchornvej*

Main category: cs.SI

TL;DR: 该研究将泰国76个省份视为空间网络节点，分析ICD-10疾病发病率和多维贫困的空间依赖性和跨省溢出效应，发现疾病集群明显且邻省溢出效应常超过本地贫困影响。


<details>
  <summary>Details</summary>
Motivation: 泰国健康与贫困问题存在明显地理结构，但作为相互关联的区域系统运作程度尚不明确。研究旨在理解健康与贫困在空间网络中的相互依赖关系。

Method: 将泰国76个省份视为固定度区域图中的节点，应用空间计量经济学和社会网络分析工具，包括莫兰指数、局部空间关联指标和空间杜宾模型，评估空间依赖性和跨省溢出效应。

Result: 研究发现多个ICD-10章节存在强烈空间聚类，消化、呼吸、肌肉骨骼和症状性疾病在特定区域带形成持续高-高发病率区。空间杜宾模型估计显示，邻省溢出效应经常超过本地贫困的影响，特别是在生活条件、医疗可及性、可达性和贫困家庭指标方面。

Conclusion: 通过将发病率和贫困视为空间网络上的相互依赖属性，该研究为结构扩散、健康不平等和区域脆弱性文献做出贡献。结果强调跨省协调政策干预的重要性，并展示基于网络的建模如何揭示健康与贫困的空间动态。

Abstract: Health and poverty in Thailand exhibit pronounced geographic structuring, yet the extent to which they operate as interconnected regional systems remains insufficiently understood. This study analyzes ICD-10 chapter-level morbidity and multidimensional poverty as outcomes embedded in a spatial interaction network. Interpreting Thailand's 76 provinces as nodes within a fixed-degree regional graph, we apply tools from spatial econometrics and social network analysis, including Moran's I, Local Indicators of Spatial Association (LISA), and Spatial Durbin Models (SDM), to assess spatial dependence and cross-provincial spillovers.
  Our findings reveal strong spatial clustering across multiple ICD-10 chapters, with persistent high-high morbidity zones, particularly for digestive, respiratory, musculoskeletal, and symptom-based diseases, emerging in well-defined regional belts. SDM estimates demonstrate that spillover effects from neighboring provinces frequently exceed the influence of local deprivation, especially for living-condition, health-access, accessibility, and poor-household indicators. These patterns are consistent with contagion and contextual influence processes well established in social network theory.
  By framing morbidity and poverty as interdependent attributes on a spatial network, this study contributes to the growing literature on structural diffusion, health inequality, and regional vulnerability. The results highlight the importance of coordinated policy interventions across provincial boundaries and demonstrate how network-based modeling can uncover the spatial dynamics of health and deprivation.

</details>


### [2] [An Empirical Study on User Profile Analysis and SEO Performance: A Case of Taiwan Cultural Memory Bank 2.0](https://arxiv.org/abs/2601.03050)
*Mei-Yun Hsu,I-Hsien Ting,Yun-Hsiu Liu,Kazunori Minetaki*

Main category: cs.SI

TL;DR: 台湾文化记忆银行2.0是一个邀请公众成为策展人的线上策展平台，本研究通过实证分析用户画像、浏览行为和SEO表现，为优化平台管理提供策略建议。


<details>
  <summary>Details</summary>
Motivation: 在访客关系管理框架下，理解用户画像对提升网站服务质量至关重要。本研究旨在分析台湾文化记忆银行2.0平台的用户特征和行为模式，以优化用户体验和数字传播效果。

Method: 采用实证分析方法，研究用户的人口统计特征、浏览行为和参与模式，同时评估平台的SEO表现、搜索可见性和有机流量效果。

Result: 研究分析了平台用户画像和行为模式，评估了SEO表现，为网站管理优化、用户体验改善和社交媒体利用提供了具体策略建议。

Conclusion: 研究结果为数字文化平台的受众参与、在线可见性和网络传播提供了重要见解，有助于优化平台管理和提升用户体验。

Abstract: Taiwan Cultural Memory Bank 2.0 is an online curation platform that invites the public to become curators, fostering diverse perspectives on Taiwan's society, humanities, natural landscapes, and daily life. Built on a material bank concept, the platform encourages users to co-create and curate their own works using shared resources or self-uploaded materials. At its core, the system follows a collect, store, access, and reuse model, supporting dynamic engagement with over three million cultural memory items from Taiwan. Users can search, browse, explore stories, and engage in creative applications and collaborative productions. Understanding user profiles is crucial for enhancing website service quality, particularly within the framework of the Visitor Relationship Management model. This study conducts an empirical analysis of user profiles on the platform, examining demographic characteristics, browsing behaviors, and engagement patterns. Additionally, the research evaluates the platform's SEO performance, search visibility, and organic traffic effectiveness. Based on the findings, this study provides strategic recommendations for optimizing website management, improving user experience, and leveraging social media for enhanced digital outreach. The insights gained contribute to the broader discussion on digital cultural platforms and their role in audience engagement, online visibility, and networked communication.

</details>


### [3] [Exploring the Relationship Between Local Election Results and Online Public Opinion in Taiwan: A Case Study of Taitung County](https://arxiv.org/abs/2601.03057)
*I-Hsien Ting,Yen-Chih Chiu,Yun-Hsiu Liu,Kazunori Minetaki,Chia-Sung Yen*

Main category: cs.SI

TL;DR: 研究台湾台东县网络声量与地方选举结果的关系，通过社交媒体数据与投票份额比较分析，探讨网络讨论是否反映选举结果及数字情绪如何影响选民行为。


<details>
  <summary>Details</summary>
Motivation: 随着社交媒体成为公共讨论主要渠道，网络声量被视为影响选举的因素，但台湾地方选举中其影响尚未充分研究，需要实证调查填补这一空白。

Method: 采用比较分析方法，结合社交媒体数据与选举期间实际投票份额进行实证研究，并回顾现有文献建立研究框架。

Result: 研究发现网络讨论与选举结果存在关联，数字情绪能够反映选民行为，但也存在方法论和数据限制可能影响解释。

Conclusion: 研究不仅具有学术价值，还为竞选策略提供实用见解，通过台东县案例分析加深了对网络话语在台湾地方选举中作用的理解，为未来研究奠定基础。

Abstract: This study examines the relationship between online buzz and local election outcomes in Taiwan, with a focus on Taitung County. As social media becomes a major channel for public discourse, online buzz is increasingly seen as a factor influencing elections. However, its impact on local elections in Taiwan remains underexplored. This research addresses that gap through a comparative analysis of social media data and actual vote shares during the election period. A review of existing literature establishes the study's framework and highlights the need for empirical investigation in this area.
  The findings aim to reveal whether online discussions align with electoral results and to what extent digital sentiment reflects voter behavior. The study also discusses methodological and data limitations that may affect interpretation. Beyond its academic value, the research offers practical insights into how online buzz can inform campaign strategies and enhance election predictions. By analyzing the Taitung County case, this study contributes to a deeper understanding of the role of online discourse in Taiwan's local elections and offers a foundation for future research in the field.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [4] [Textual Explanations and Their Evaluations for Reinforcement Learning Policy](https://arxiv.org/abs/2601.02514)
*Ahmad Terra,Mohit Ahmed,Rafia Inam,Elena Fersman,Martin Törngren*

Main category: cs.AI

TL;DR: 提出一個可解釋強化學習框架，使用LLM生成文本解釋並轉換為透明規則，透過聚類技術識別頻繁條件，並提供兩種精煉方法改善解釋品質。


<details>
  <summary>Details</summary>
Motivation: 現有的可解釋強化學習技術中，文本解釋雖然易於人類理解，但確保其正確性仍是挑戰，且現有評估方法有限。需要一個系統化框架來生成、精煉和評估文本解釋。

Method: 使用大型語言模型生成文本解釋，透過聚類技術識別頻繁條件，將這些條件轉換為透明規則。提出兩種精煉技術改善解釋品質並減少衝突資訊，並設計自動謂詞生成器來確定狀態的語義資訊。

Result: 在三個開源環境和一個電信用例中進行實驗，證明框架能解決現有方法的限制，生成的透明規則在特定任務中能達到滿意性能，並實現對文本解釋的系統化定量評估。

Conclusion: 該框架為XRL領域提供了系統化的文本解釋生成、精煉和評估方法，能結合專家知識，生成的透明規則具有實際應用價值，特別是在工業環境中展現了適用性。

Abstract: Understanding a Reinforcement Learning (RL) policy is crucial for ensuring that autonomous agents behave according to human expectations. This goal can be achieved using Explainable Reinforcement Learning (XRL) techniques. Although textual explanations are easily understood by humans, ensuring their correctness remains a challenge, and evaluations in state-of-the-art remain limited. We present a novel XRL framework for generating textual explanations, converting them into a set of transparent rules, improving their quality, and evaluating them. Expert's knowledge can be incorporated into this framework, and an automatic predicate generator is also proposed to determine the semantic information of a state. Textual explanations are generated using a Large Language Model (LLM) and a clustering technique to identify frequent conditions. These conditions are then converted into rules to evaluate their properties, fidelity, and performance in the deployed environment. Two refinement techniques are proposed to improve the quality of explanations and reduce conflicting information. Experiments were conducted in three open-source environments to enable reproducibility, and in a telecom use case to evaluate the industrial applicability of the proposed XRL framework. This framework addresses the limitations of an existing method, Autonomous Policy Explanation, and the generated transparent rules can achieve satisfactory performance on certain tasks. This framework also enables a systematic and quantitative evaluation of textual explanations, providing valuable insights for the XRL field.

</details>


### [5] [SimpleMem: Efficient Lifelong Memory for LLM Agents](https://arxiv.org/abs/2601.02553)
*Jiaqi Liu,Yaofeng Su,Peng Xia,Siwei Han,Zeyu Zheng,Cihang Xie,Mingyu Ding,Huaxiu Yao*

Main category: cs.AI

TL;DR: SimpleMem是一个基于语义无损压缩的高效记忆框架，通过三阶段流水线实现信息密度最大化，在保持性能的同时大幅降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理的记忆系统存在冗余或高成本问题：要么通过被动上下文扩展保留完整交互历史导致大量冗余，要么依赖迭代推理过滤噪声但带来高token成本。需要一种能在复杂环境中支持可靠长期交互的高效记忆管理方法。

Method: 提出三阶段流水线：1) 语义结构化压缩：应用熵感知过滤将非结构化交互提炼为紧凑的多视图索引记忆单元；2) 递归记忆整合：异步过程将相关单元整合为更高级别的抽象表示以减少冗余；3) 自适应查询感知检索：根据查询复杂度动态调整检索范围，高效构建精确上下文。

Result: 在基准数据集上的实验表明，该方法在准确性、检索效率和推理成本方面均优于基线方法，平均F1提升26.4%，推理时token消耗减少高达30倍，实现了性能与效率的优越平衡。

Conclusion: SimpleMem通过语义无损压缩框架有效解决了LLM代理长期交互中的记忆管理问题，在保持高性能的同时显著降低了计算成本，为复杂环境中的可靠交互提供了高效解决方案。

Abstract: To support reliable long-term interaction in complex environments, LLM agents require memory systems that efficiently manage historical experiences. Existing approaches either retain full interaction histories via passive context extension, leading to substantial redundancy, or rely on iterative reasoning to filter noise, incurring high token costs. To address this challenge, we introduce SimpleMem, an efficient memory framework based on semantic lossless compression. We propose a three-stage pipeline designed to maximize information density and token utilization: (1) \textit{Semantic Structured Compression}, which applies entropy-aware filtering to distill unstructured interactions into compact, multi-view indexed memory units; (2) \textit{Recursive Memory Consolidation}, an asynchronous process that integrates related units into higher-level abstract representations to reduce redundancy; and (3) \textit{Adaptive Query-Aware Retrieval}, which dynamically adjusts retrieval scope based on query complexity to construct precise context efficiently. Experiments on benchmark datasets show that our method consistently outperforms baseline approaches in accuracy, retrieval efficiency, and inference cost, achieving an average F1 improvement of 26.4% while reducing inference-time token consumption by up to 30-fold, demonstrating a superior balance between performance and efficiency. Code is available at https://github.com/aiming-lab/SimpleMem.

</details>


### [6] [Orchestral AI: A Framework for Agent Orchestration](https://arxiv.org/abs/2601.02577)
*Alexander Roman,Jacob Roman*

Main category: cs.AI

TL;DR: Orchestral是一个轻量级Python框架，为跨多个LLM提供商构建AI代理提供统一、类型安全的接口，解决供应商锁定和API碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理框架存在供应商锁定（特定提供商SDK）和多包生态系统复杂性的问题，导致控制流不透明、可复现性差。跨提供商集成工具调用面临API碎片化、消息格式不兼容、流式处理和工具调用行为不一致等核心工程挑战，难以构建可移植、可靠的代理系统。

Method: Orchestral采用轻量级Python框架设计，提供统一的类型安全接口。核心方法包括：1）定义消息、工具和LLM使用的通用表示；2）基于Python类型提示自动生成工具模式；3）同步执行模型支持流式处理；4）模块化架构分离提供商集成、工具执行、对话编排和用户界面。

Result: Orchestral实现了跨主要LLM提供商的无缝操作，消除了手动格式转换，减少了框架引入的复杂性。支持丰富的工具调用、上下文压缩、工作区沙盒、用户审批流程、子代理、内存管理和MCP集成等高级代理功能。

Conclusion: Orchestral为构建可移植、可靠的LLM代理系统提供了简洁而强大的解决方案，既保持了科学计算所需的简单性，又支持生产部署，解决了当前LLM代理框架生态系统中的核心工程挑战。

Abstract: The rapid proliferation of LLM agent frameworks has forced developers to choose between vendor lock-in through provider-specific SDKs and complex multi-package ecosystems that obscure control flow and hinder reproducibility. Integrating tool calling across multiple LLM providers remains a core engineering challenge due to fragmented APIs, incompatible message formats, and inconsistent streaming and tool-calling behavior, making it difficult to build portable, reliable agent systems. We introduce Orchestral, a lightweight Python framework that provides a unified, type-safe interface for building LLM agents across major providers while preserving the simplicity required for scientific computing and production deployment. Orchestral defines a single universal representation for messages, tools, and LLM usage that operates seamlessly across providers, eliminating manual format translation and reducing framework-induced complexity. Automatic tool schema generation from Python type hints removes the need for handwritten descriptors while maintaining type safety across provider boundaries. A synchronous execution model with streaming support enables deterministic behavior, straightforward debugging, and real-time interaction without introducing server dependencies. The framework's modular architecture cleanly separates provider integration, tool execution, conversation orchestration, and user-facing interfaces, enabling extensibility without architectural entanglement. Orchestral supports advanced agent capabilities found in larger frameworks, including rich tool calling, context compaction, workspace sandboxing, user approval workflows, sub-agents, memory management, and MCP integration.

</details>


### [7] [An Empirical Study of On-Device Translation for Real-Time Live-Stream Chat on Mobile Devices](https://arxiv.org/abs/2601.02641)
*Jeiyoon Park,Daehwan Lee,Changmin Yeo,Yongshin Han,Minseop Kim*

Main category: cs.AI

TL;DR: 该研究探讨了设备端AI模型实际部署中的关键问题，包括模型选择、资源消耗和领域适应能力，并通过构建韩英直播聊天基准测试，在移动设备上验证了设备端模型能达到与GPT-5.1相当的翻译性能。


<details>
  <summary>Details</summary>
Motivation: 尽管设备端AI模型效率高，但实际部署中需要考虑设备CPU利用率和热条件等实际问题，目前相关研究较少。研究旨在解决设备端模型在真实服务部署中的两个关键问题。

Method: 构建了LiveChatBench基准测试（包含1000个韩英平行句对），在5台移动设备上进行广泛实验，评估不同设备端模型的资源消耗和领域适应能力。

Result: 实验表明，虽然服务大规模异构用户需要考虑高度受限的部署环境和模型选择，但提出的方法在针对性任务上能达到与GPT-5.1等商业模型相当的性能。

Conclusion: 研究为设备端AI社区提供了有意义的见解，展示了设备端模型在实际部署中的潜力和挑战，特别是在资源受限环境下的可行性和领域适应能力。

Abstract: Despite its efficiency, there has been little research on the practical aspects required for real-world deployment of on-device AI models, such as the device's CPU utilization and thermal conditions. In this paper, through extensive experiments, we investigate two key issues that must be addressed to deploy on-device models in real-world services: (i) the selection of on-device models and the resource consumption of each model, and (ii) the capability and potential of on-device models for domain adaptation. To this end, we focus on a task of translating live-stream chat messages and manually construct LiveChatBench, a benchmark consisting of 1,000 Korean-English parallel sentence pairs. Experiments on five mobile devices demonstrate that, although serving a large and heterogeneous user base requires careful consideration of highly constrained deployment settings and model selection, the proposed approach nevertheless achieves performance comparable to commercial models such as GPT-5.1 on the well-targeted task. We expect that our findings will provide meaningful insights to the on-device AI community.

</details>


### [8] [AWARE-US: Benchmark for Preference-Aware Resolution in Tool-Calling Agents](https://arxiv.org/abs/2601.02643)
*Mehmet Kurmaz*

Main category: cs.AI

TL;DR: 论文提出将不可行查询处理视为偏好感知的查询修复问题，开发了三种基于LLM的方法来推断约束重要性，并创建了AWARE-US基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有工具调用对话代理在处理结构化数据库查询时面临两个关联问题：欠规范（缺少运行精确查询所需的约束）和不可行性（完全指定的查询返回空集）。现有方法要么返回"无结果"，要么使用临时规则放松约束，这可能违反用户意图。

Method: 提出了三种基于LLM的方法来从对话中推断相对约束重要性：1) 局部加权，2) 全局一次性加权，3) 成对排序。将不可行性处理框架化为偏好感知的查询修复问题。

Result: 实验表明，局部加权在偏好对齐方面表现最佳，而全局加权在正确约束放松方面表现最好。还引入了AWARE-US基准测试，用于评估代理在角色基础查询中的表现。

Conclusion: 该研究为对话式数据库查询中的不可行性问题提供了系统解决方案，通过偏好感知的查询修复方法，能够更智能地处理约束冲突，更好地满足用户意图。

Abstract: Tool-calling conversational agents querying structured databases often face two linked failures: underspecification (missing constraints needed to run a precise query) and infeasibility (the fully specified query returns an empty set because no item satisfies all constraints). Existing work often responds with "no results" or relaxes constraints using ad hoc rules, which can violate user intent by discarding requirements the user cares about most. We frame infeasibility handling as a preference-aware query repair problem: when a query is unsatisfiable, the agent should relax the least important constraints to the user. We propose three LLM-based methods for inferring relative constraint importance from dialogue: (1) local weighting, (2) global one-shot weighting, and (3) pairwise ranking. Experiments show local weighting achieves the best preference alignment, while global weighting performs best on correct constraint relaxation. We also introduce AWARE-US, a benchmark of persona-grounded queries requiring agents to disambiguate requests via conversation and resolve infeasibility in a way consistent with persona-implied preferences.

</details>


### [9] [Inferring Causal Graph Temporal Logic Formulas to Expedite Reinforcement Learning in Temporally Extended Tasks](https://arxiv.org/abs/2601.02666)
*Hadi Partovi Aria,Zhe Xu*

Main category: cs.AI

TL;DR: GTL-CIRL：一个闭环框架，通过同时学习策略和挖掘因果图时序逻辑规范，在具有时空动态的图上进行决策，提高样本效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 决策任务通常在具有时空动态的图上展开，而黑盒强化学习往往忽视局部变化如何通过网络结构传播，限制了样本效率和可解释性。

Method: 提出GTL-CIRL框架，同时学习策略和挖掘因果图时序逻辑规范。使用鲁棒性塑造奖励，收集效果失败时的反例，并利用高斯过程驱动的贝叶斯优化来精化参数化的原因模板。GP模型捕捉系统动态中的空间和时间相关性。

Result: 在基因和电力网络案例研究中，相比标准RL基线，显示出更快的学习和更清晰、可验证的行为。

Conclusion: GTL-CIRL框架通过结合因果推理和强化学习，在复杂网络系统中实现了更高效、更可解释的决策。

Abstract: Decision-making tasks often unfold on graphs with spatial-temporal dynamics. Black-box reinforcement learning often overlooks how local changes spread through network structure, limiting sample efficiency and interpretability. We present GTL-CIRL, a closed-loop framework that simultaneously learns policies and mines Causal Graph Temporal Logic (Causal GTL) specifications. The method shapes rewards with robustness, collects counterexamples when effects fail, and uses Gaussian Process (GP) driven Bayesian optimization to refine parameterized cause templates. The GP models capture spatial and temporal correlations in the system dynamics, enabling efficient exploration of complex parameter spaces. Case studies in gene and power networks show faster learning and clearer, verifiable behavior compared to standard RL baselines.

</details>


### [10] [Learning from Prompt itself: the Hierarchical Attribution Prompt Optimization](https://arxiv.org/abs/2601.02683)
*Dongyu Chen,Jian Ma,Xianpeng Zhang,Lei Zhang,Haonan Lu,Chen Chen,Chuangchuang Wang,Kai Tang*

Main category: cs.AI

TL;DR: HAPO框架通过分层归因机制、语义单元优化和多模态支持，解决提示优化中的提示漂移和可解释性问题，提升优化效率和性能。


<details>
  <summary>Details</summary>
Motivation: 当前提示优化方法存在提示漂移问题（新提示修复先前失败但损害已成功任务性能），且从零生成提示会损害可解释性。需要结构化优化方法来减少人工努力、提升性能并保持可解释过程。

Method: 提出分层归因提示优化（HAPO）框架，包含三个创新：1）针对训练数据和提示历史中错误模式的动态归因机制；2）用于编辑功能提示段的语义单元优化；3）支持端到端LLM和LLM-MLLM工作流的多模态友好进展。

Result: 在单/多图像QA（如OCRV2）和复杂任务分析（如BBH）等场景中，HAPO展示了增强的优化效率，优于可比较的自动提示优化方法，并为可扩展提示工程建立了可扩展范式。

Conclusion: HAPO框架通过创新的分层归因、语义单元优化和多模态支持，有效解决了提示优化中的关键挑战，为可扩展的提示工程提供了有效解决方案。

Abstract: Optimization is fundamental across numerous disciplines, typically following an iterative process of refining an initial solution to enhance performance. This principle is equally critical in prompt engineering, where designing effective prompts for large language models constitutes a complex optimization challenge. A structured optimization approach requires automated or semi-automated procedures to develop improved prompts, thereby reducing manual effort, improving performance, and yielding an interpretable process. However, current prompt optimization methods often induce prompt drift, where new prompts fix prior failures but impair performance on previously successful tasks. Additionally, generating prompts from scratch can compromise interpretability. To address these limitations, this study proposes the Hierarchical Attribution Prompt Optimization (HAPO) framework, which introduces three innovations: (1) a dynamic attribution mechanism targeting error patterns in training data and prompting history, (2) semantic-unit optimization for editing functional prompt segments, and (3) multimodal-friendly progression supporting both end-to-end LLM and LLM-MLLM workflows. Applied in contexts like single/multi-image QA (e.g., OCRV2) and complex task analysis (e.g., BBH), HAPO demonstrates enhanced optimization efficiency, outperforming comparable automated prompt optimization methods and establishing an extensible paradigm for scalable prompt engineering.

</details>


### [11] [Learning User Preferences Through Interaction for Long-Term Collaboration](https://arxiv.org/abs/2601.02702)
*Shuhaib Mehri,Priyanka Kargupta,Tal August,Dilek Hakkani-Tür*

Main category: cs.AI

TL;DR: MultiSessionCollab：评估智能体在多轮会话中学习用户偏好并提升协作质量的基准，通过持久化记忆机制实现长期协作优化


<details>
  <summary>Details</summary>
Motivation: 随着对话智能体与用户协作经验的积累，适应并学习用户偏好对于建立长期关系和提升协作质量至关重要。现有研究缺乏评估智能体在多轮会话中持续学习用户偏好能力的方法。

Method: 提出MultiSessionCollab基准，用于评估智能体在多轮会话中学习用户偏好的能力。开发配备持久化记忆机制的长期协作智能体，能够随着交互经验的积累不断细化和更新用户偏好。利用用户模拟器行为生成学习信号，训练智能体生成更全面的反思并更有效地更新记忆。

Result: 实验表明，配备记忆的智能体显著提升了长期协作效果：任务成功率更高、交互更高效、用户努力更少。人类用户研究进一步证实，记忆机制在真实场景中改善了用户体验。

Conclusion: MultiSessionCollab基准为评估智能体长期协作能力提供了有效工具，持久化记忆机制是提升智能体在多轮会话中学习用户偏好和协作质量的关键技术，具有实际应用价值。

Abstract: As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.

</details>


### [12] [Time-Scaling Is What Agents Need Now](https://arxiv.org/abs/2601.02714)
*Zhi Liu,Guangzhi Wang*

Main category: cs.AI

TL;DR: 论文提出"时间缩放"概念，即通过系统扩展和优化智能体在时间维度上的推理能力，实现更深层次的问题空间探索和动态策略调整，而不需要按比例增加静态模型参数。


<details>
  <summary>Details</summary>
Motivation: 早期人工智能范式存在认知功能分离问题，而当前大型语言模型虽然能生成流畅文本，但在深度语义推理方面仍有不足。现有的提示技术如思维链和思维树虽然通过显式中间步骤扩展了推理路径，但在搜索完整性和效率方面存在局限。这凸显了需要一种系统方法来扩展智能体在时间维度上的推理能力。

Method: 提出"时间缩放"概念，这是一种架构设计方法，利用扩展的时间路径，使智能体能够进行更深层次的问题空间探索、动态策略调整和增强的元认知控制。这类似于人类在认知约束下的顺序推理过程，旨在通过优化时间维度的推理管理来提升深度推理能力。

Result: 时间缩放代表了增强深度推理和问题解决能力的关键前沿，它能够在不按比例增加静态模型参数的情况下，显著提升智能体的认知能力。这种方法为构建具有闭环"感知-决策-行动"能力的认知智能体提供了理论基础。

Conclusion: 推进智能体能力需要将时间缩放原则置于前沿，将显式时间推理管理作为基础。这代表了人工智能发展的关键方向，通过优化时间维度的推理过程来突破当前大型语言模型在深度语义推理方面的限制。

Abstract: Early artificial intelligence paradigms exhibited separated cognitive functions: Neural Networks focused on "perception-representation," Reinforcement Learning on "decision-making-behavior," and Symbolic AI on "knowledge-reasoning." With Transformer-based large models and world models, these paradigms are converging into cognitive agents with closed-loop "perception-decision-action" capabilities.
  Humans solve complex problems under limited cognitive resources through temporalized sequential reasoning. Language relies on problem space search for deep semantic reasoning. While early large language models (LLMs) could generate fluent text, they lacked robust semantic reasoning capabilities. Prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) extended reasoning paths by making intermediate steps explicit. Recent models like DeepSeek-R1 enhanced performance through explicit reasoning trajectories. However, these methods have limitations in search completeness and efficiency.
  This highlights the need for "Time-Scaling"--the systematic extension and optimization of an agent's ability to unfold reasoning over time. Time-Scaling refers to architectural design utilizing extended temporal pathways, enabling deeper problem space exploration, dynamic strategy adjustment, and enhanced metacognitive control, paralleling human sequential reasoning under cognitive constraints. It represents a critical frontier for enhancing deep reasoning and problem-solving without proportional increases in static model parameters. Advancing intelligent agent capabilities requires placing Time-Scaling principles at the forefront, positioning explicit temporal reasoning management as foundational.

</details>


### [13] [The Path Ahead for Agentic AI: Challenges and Opportunities](https://arxiv.org/abs/2601.02749)
*Nadia Sibai,Yara Ahmed,Serry Sibaee,Sawsan AlHalawani,Adel Ammar,Wadii Boulila*

Main category: cs.AI

TL;DR: 该章节探讨了LLM从被动文本生成器向自主智能体系统的演进，分析了实现自主行为所需的核心组件（规划、记忆、工具使用等），并指出了安全、对齐、可靠性等关键挑战和研究方向。


<details>
  <summary>Details</summary>
Motivation: 研究LLM从被动语言模型向自主智能体系统演进的动机在于理解人工智能从文本生成到自主行动的根本转变，探索如何将LLM的能力扩展到复杂环境中的自主操作。

Method: 通过分析LLM架构从统计模型到基于transformer系统的演进，识别实现智能体行为的关键能力（长程推理、上下文感知、自适应决策），并提出集成框架描述感知、记忆、规划、工具执行等核心组件。

Result: 提出了三个主要贡献：1) LLM能力通过推理-行动-反思循环向自主性扩展的综合分析；2) 连接LLM与自主行为的核心组件集成框架；3) 对应用及安全、对齐、可靠性、可持续性等持续挑战的批判性评估。

Conclusion: 实现负责任的智能体AI发展需要在技术稳健性、可解释性和伦理保障方面同步进展，解决可验证规划、可扩展多智能体协调、持久记忆架构和治理框架等关键研究重点，以在减轻错位和意外后果风险的同时实现潜力。

Abstract: The evolution of Large Language Models (LLMs) from passive text generators to autonomous, goal-driven systems represents a fundamental shift in artificial intelligence. This chapter examines the emergence of agentic AI systems that integrate planning, memory, tool use, and iterative reasoning to operate autonomously in complex environments. We trace the architectural progression from statistical models to transformer-based systems, identifying capabilities that enable agentic behavior: long-range reasoning, contextual awareness, and adaptive decision-making. The chapter provides three contributions: (1) a synthesis of how LLM capabilities extend toward agency through reasoning-action-reflection loops; (2) an integrative framework describing core components perception, memory, planning, and tool execution that bridge LLMs with autonomous behavior; (3) a critical assessment of applications and persistent challenges in safety, alignment, reliability, and sustainability. Unlike existing surveys, we focus on the architectural transition from language understanding to autonomous action, emphasizing the technical gaps that must be resolved before deployment. We identify critical research priorities, including verifiable planning, scalable multi-agent coordination, persistent memory architectures, and governance frameworks. Responsible advancement requires simultaneous progress in technical robustness, interpretability, and ethical safeguards to realize potential while mitigating risks of misalignment and unintended consequences.

</details>


### [14] [LLM Agent Framework for Intelligent Change Analysis in Urban Environment using Remote Sensing Imagery](https://arxiv.org/abs/2601.02757)
*Zixuan Xiao,Jun Ma*

Main category: cs.AI

TL;DR: ChangeGPT：一个集成大语言模型与视觉基础模型的通用代理框架，用于遥感变化检测，通过分层结构减少幻觉，在多样化查询中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有变化检测方法缺乏处理多样化真实世界查询的通用性和进行全面分析的智能性，需要更灵活、智能的解决方案。

Method: 提出ChangeGPT框架，集成大语言模型（LLM）与视觉基础模型，采用分层结构来减轻幻觉问题，构建包含140个问题的评估数据集，涵盖多种问题类型和复杂度。

Result: ChangeGPT（特别是GPT-4-turbo后端）在工具选择能力（精确率/召回率）和整体查询准确率（匹配率）上表现优异，达到90.71%的匹配率，在处理需要多步推理的变化相关查询方面尤其强大，并通过深圳前海湾的实际案例验证了其实用性。

Conclusion: ChangeGPT通过提供智能性、适应性和多类型变化分析能力，为遥感应用中的决策制定提供了强大的解决方案。

Abstract: Existing change detection methods often lack the versatility to handle diverse real-world queries and the intelligence for comprehensive analysis. This paper presents a general agent framework, integrating Large Language Models (LLM) with vision foundation models to form ChangeGPT. A hierarchical structure is employed to mitigate hallucination. The agent was evaluated on a curated dataset of 140 questions categorized by real-world scenarios, encompassing various question types (e.g., Size, Class, Number) and complexities. The evaluation assessed the agent's tool selection ability (Precision/Recall) and overall query accuracy (Match). ChangeGPT, especially with a GPT-4-turbo backend, demonstrated superior performance, achieving a 90.71 % Match rate. Its strength lies particularly in handling change-related queries requiring multi-step reasoning and robust tool selection. Practical effectiveness was further validated through a real-world urban change monitoring case study in Qianhai Bay, Shenzhen. By providing intelligence, adaptability, and multi-type change analysis, ChangeGPT offers a powerful solution for decision-making in remote sensing applications.

</details>


### [15] [HAL: Inducing Human-likeness in LLMs with Alignment](https://arxiv.org/abs/2601.02813)
*Masum Hasan,Junjie Zhao,Ehsan Hoque*

Main category: cs.AI

TL;DR: HAL框架通过从对比对话数据中提取可解释的对话特征，将其组合成紧凑的标量分数，并以此作为透明奖励信号来对齐语言模型，使其对话更接近人类风格。


<details>
  <summary>Details</summary>
Motivation: 对话中的人类相似性在人机交互中至关重要，但一直难以定义、测量和优化。目前的改进主要依赖规模扩展或广泛的监督训练，而非有针对性的对齐。

Method: 从对比对话数据中提取明确的对话特征，将其组合成紧凑的标量分数，使用该分数作为透明奖励信号，通过标准偏好优化方法进行对齐。

Result: 在不同规模模型上应用HAL进行对齐，不影响整体性能。大规模人类评估显示，经HAL对齐的模型在对话中更频繁被感知为人类风格。

Conclusion: HAL展示了如何将语言中软性、定性的属性（以前超出对齐范围）变得可测量、可对齐，并以可解释和可说明的方式进行。

Abstract: Conversational human-likeness plays a central role in human-AI interaction, yet it has remained difficult to define, measure, and optimize. As a result, improvements in human-like behavior are largely driven by scale or broad supervised training, rather than targeted alignment. We introduce Human Aligning LLMs (HAL), a framework for aligning language models to conversational human-likeness using an interpretable, data-driven reward. HAL derives explicit conversational traits from contrastive dialogue data, combines them into a compact scalar score, and uses this score as a transparent reward signal for alignment with standard preference optimization methods. Using this approach, we align models of varying sizes without affecting their overall performance. In large-scale human evaluations, models aligned with HAL are more frequently perceived as human-like in conversation. Because HAL operates over explicit, interpretable traits, it enables inspection of alignment behavior and diagnosis of unintended effects. More broadly, HAL demonstrates how soft, qualitative properties of language--previously outside the scope for alignment--can be made measurable and aligned in an interpretable and explainable way.

</details>


### [16] [Causal-Enhanced AI Agents for Medical Research Screening](https://arxiv.org/abs/2601.02814)
*Duc Ngo,Arya Rahgoza*

Main category: cs.AI

TL;DR: 提出因果图增强的检索增强生成系统，通过因果推理与知识图谱结合，在医学系统综述中实现零幻觉和高准确率


<details>
  <summary>Details</summary>
Motivation: 医学系统综述需要处理海量文献（每年150万+），现有AI方法存在幻觉问题（2-40%的错误率），这在影响患者护理时是不可接受的

Method: 因果图增强的检索增强生成系统，整合显式因果推理与双层知识图谱，采用证据优先协议，每个因果声明都追溯到检索的文献，自动生成干预-结果路径的有向无环图

Result: 在234个痴呆症运动摘要评估中，CausalAgent达到95%准确率、100%检索成功率和零幻觉，而基线AI只有34%准确率和10%幻觉。自动因果图支持显式机制建模、可视化合成和增强可解释性

Conclusion: 虽然概念验证评估仅针对痴呆症运动研究的十个问题，但该架构方法展示了可转移的可信医学AI原则，以及因果推理在高风险医疗保健中的潜力

Abstract: Systematic reviews are essential for evidence-based medicine, but reviewing 1.5 million+ annual publications manually is infeasible. Current AI approaches suffer from hallucinations in systematic review tasks, with studies reporting rates ranging from 28--40% for earlier models to 2--15% for modern implementations which is unacceptable when errors impact patient care.
  We present a causal graph-enhanced retrieval-augmented generation system integrating explicit causal reasoning with dual-level knowledge graphs. Our approach enforces evidence-first protocols where every causal claim traces to retrieved literature and automatically generates directed acyclic graphs visualizing intervention-outcome pathways.
  Evaluation on 234 dementia exercise abstracts shows CausalAgent achieves 95% accuracy, 100% retrieval success, and zero hallucinations versus 34% accuracy and 10% hallucinations for baseline AI. Automatic causal graphs enable explicit mechanism modeling, visual synthesis, and enhanced interpretability. While this proof-of-concept evaluation used ten questions focused on dementia exercise research, the architectural approach demonstrates transferable principles for trustworthy medical AI and causal reasoning's potential for high-stakes healthcare.

</details>


### [17] [Quantum-enhanced long short-term memory with attention for spatial permeability prediction in oilfield reservoirs](https://arxiv.org/abs/2601.02818)
*Muzhen Zhang,Yujie Cheng,Zhanxiang Lei*

Main category: cs.AI

TL;DR: 该研究首次将量子计算引入地下空间预测，提出量子增强的注意力长短期记忆模型（QLSTMA），通过变分量子电路显著提升了渗透率等复杂地质参数的预测精度。


<details>
  <summary>Details</summary>
Motivation: 渗透率等储层参数的空间预测对油气勘探开发至关重要，但现有方法因渗透率变化范围大、变异性高而难以提供可靠预测。量子计算在机器学习中的潜力为解决这一挑战提供了新思路。

Method: 提出了QLSTMA模型，将变分量子电路（VQC）集成到循环单元中，利用量子纠缠和叠加原理增强模型能力。设计了两种量子化结构：共享门QLSTMA-SG和独立门QLSTMA-IG，并研究了量子结构配置和量子比特数对性能的影响。

Result: 8量子比特的QLSTMA-IG模型显著优于传统LSTMA，平均绝对误差降低19%，均方根误差降低20%，在复杂测井数据区域表现尤为突出。增加量子比特数可进一步提升精度。

Conclusion: 量子-经典混合神经网络在储层预测中具有巨大潜力，为未来在真实量子硬件上部署此类模型奠定了基础，并可扩展到石油工程和地球科学的更广泛应用。

Abstract: Spatial prediction of reservoir parameters, especially permeability, is crucial for oil and gas exploration and development. However, the wide range and high variability of permeability prevent existing methods from providing reliable predictions. For the first time in subsurface spatial prediction, this study presents a quantum-enhanced long short-term memory with attention (QLSTMA) model that incorporates variational quantum circuits (VQCs) into the recurrent cell. Using quantum entanglement and superposition principles, the QLSTMA significantly improves the ability to predict complex geological parameters such as permeability. Two quantization structures, QLSTMA with Shared Gates (QLSTMA-SG) and with Independent Gates (QLSTMA-IG), are designed to investigate and evaluate the effects of quantum structure configurations and the number of qubits on model performance. Experimental results demonstrate that the 8-qubit QLSTMA-IG model significantly outperforms the traditional long short-term memory with attention (LSTMA), reducing Mean Absolute Error (MAE) by 19% and Root Mean Squared Error (RMSE) by 20%, with particularly strong performance in regions featuring complex well-logging data. These findings validate the potential of quantum-classical hybrid neural networks for reservoir prediction, indicating that increasing the number of qubits yields further accuracy gains despite the reliance on classical simulations. This study establishes a foundational framework for the eventual deployment of such models on real quantum hardware and their extension to broader applications in petroleum engineering and geoscience.

</details>


### [18] [Sample-Efficient Neurosymbolic Deep Reinforcement Learning](https://arxiv.org/abs/2601.02850)
*Celeste Veronese,Daniele Meli,Alessandro Farinelli*

Main category: cs.AI

TL;DR: 提出一种神经符号深度强化学习方法，将符号知识作为先验，通过逻辑规则表示部分策略，在探索和利用阶段引导训练，提高样本效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前深度强化学习算法需要大量训练数据，在超出小规模训练场景时泛化能力有限。需要一种方法能够利用背景知识来改善样本效率和泛化到更复杂、未见过的任务。

Method: 提出神经符号DRL方法：1) 将简单域实例中的部分策略表示为逻辑规则；2) 在线推理引导训练：探索阶段偏置动作分布，利用阶段重新缩放Q值；3) 将简单任务的策略作为先验知识迁移到复杂任务。

Result: 在网格世界环境的完全可观测和部分可观测变体上验证，相比最先进的奖励机制基线方法表现出更好的性能，特别是在稀疏奖励环境和长规划视野任务中加速收敛。

Conclusion: 神经符号集成方法通过利用符号知识作为先验，显著提高了深度强化学习的样本效率、泛化能力和收敛速度，同时增强了可解释性和可信度。

Abstract: Reinforcement Learning (RL) is a well-established framework for sequential decision-making in complex environments. However, state-of-the-art Deep RL (DRL) algorithms typically require large training datasets and often struggle to generalize beyond small-scale training scenarios, even within standard benchmarks. We propose a neuro-symbolic DRL approach that integrates background symbolic knowledge to improve sample efficiency and generalization to more challenging, unseen tasks. Partial policies defined for simple domain instances, where high performance is easily attained, are transferred as useful priors to accelerate learning in more complex settings and avoid tuning DRL parameters from scratch. To do so, partial policies are represented as logical rules, and online reasoning is performed to guide the training process through two mechanisms: (i) biasing the action distribution during exploration, and (ii) rescaling Q-values during exploitation. This neuro-symbolic integration enhances interpretability and trustworthiness while accelerating convergence, particularly in sparse-reward environments and tasks with long planning horizons. We empirically validate our methodology on challenging variants of gridworld environments, both in the fully observable and partially observable setting. We show improved performance over a state-of-the-art reward machine baseline.

</details>


### [19] [M3MAD-Bench: Are Multi-Agent Debates Really Effective Across Domains and Modalities?](https://arxiv.org/abs/2601.02854)
*Ao Li,Jinghui Zhang,Luyu Li,Yuxiang Duan,Lang Gao,Mingcai Chen,Weijun Qin,Shaopeng Li,Fengxian Ji,Ning Liu,Lizhen Cui,Xiuying Chen,Yuntao Du*

Main category: cs.AI

TL;DR: M3MAD-Bench是一个统一可扩展的多智能体辩论基准，涵盖多领域任务、多模态输入和多维度指标，解决了现有评估方法碎片化和单模态限制的问题。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体辩论研究存在两个根本限制：评估在碎片化不一致的设置下进行，阻碍公平比较；主要局限于依赖纯文本输入的单模态场景。

Method: 提出M3MAD-Bench基准，建立五个核心任务领域的标准化协议（知识、数学、医学、自然科学、复杂推理），系统覆盖纯文本和视觉语言数据集，支持跨模态比较。评估了9个不同架构、规模和模态能力的基础模型。

Result: 除了准确性外，还纳入效率导向指标（如token消耗和推理时间），提供性能-成本权衡的整体视图。实验为文本和多模态场景下的MAD有效性、鲁棒性和效率提供了系统见解。

Conclusion: M3MAD-Bench为未来标准化MAD评估研究提供了可靠基础，解决了现有评估的碎片化和单模态限制问题。

Abstract: As an agent-level reasoning and coordination paradigm, Multi-Agent Debate (MAD) orchestrates multiple agents through structured debate to improve answer quality and support complex reasoning. However, existing research on MAD suffers from two fundamental limitations: evaluations are conducted under fragmented and inconsistent settings, hindering fair comparison, and are largely restricted to single-modality scenarios that rely on textual inputs only. To address these gaps, we introduce M3MAD-Bench, a unified and extensible benchmark for evaluating MAD methods across Multi-domain tasks, Multi-modal inputs, and Multi-dimensional metrics. M3MAD-Bench establishes standardized protocols over five core task domains: Knowledge, Mathematics, Medicine, Natural Sciences, and Complex Reasoning, and systematically covers both pure text and vision-language datasets, enabling controlled cross-modality comparison. We evaluate MAD methods on nine base models spanning different architectures, scales, and modality capabilities. Beyond accuracy, M3MAD-Bench incorporates efficiency-oriented metrics such as token consumption and inference time, providing a holistic view of performance--cost trade-offs. Extensive experiments yield systematic insights into the effectiveness, robustness, and efficiency of MAD across text-only and multimodal scenarios. We believe M3MAD-Bench offers a reliable foundation for future research on standardized MAD evaluation. The code is available at http://github.com/liaolea/M3MAD-Bench.

</details>


### [20] [SimRPD: Optimizing Recruitment Proactive Dialogue Agents through Simulator-Based Data Evaluation and Selection](https://arxiv.org/abs/2601.02871)
*Zhiyong Cao,Dunqiang Liu,Qi Dai,Haojun Xu,Huaiyan Xu,Huan He,Yafei Liu,Siyuan Liu,XiaoLin Lin,Ke Ma,Ruqian Shi,Sijia Yao,Hao Wang,Sicheng Zhou*

Main category: cs.AI

TL;DR: SimRPD：一个三阶段框架，通过高保真用户模拟器生成大规模对话数据，基于意图链的多维度评估筛选高质量数据，用于训练招聘场景的主动对话代理


<details>
  <summary>Details</summary>
Motivation: 招聘场景中任务导向的主动对话代理需要高质量、目标导向的领域特定训练数据，但这类数据稀缺，限制了监督微调和强化学习方法的性能

Method: 1. 开发高保真用户模拟器通过多轮在线对话合成大规模对话数据；2. 基于意图链（CoI）的多维度评估框架，结合全局和实例级指标筛选高质量数据；3. 在筛选的数据集上训练招聘主动对话代理

Result: 在真实招聘场景实验中，SimRPD优于现有的基于模拟器的数据选择策略，展示了工业部署的实用价值和其他业务导向对话场景的潜在适用性

Conclusion: SimRPD框架有效解决了招聘主动对话代理训练数据稀缺问题，通过模拟数据生成和高质量筛选机制，为工业应用提供了实用解决方案

Abstract: Task-oriented proactive dialogue agents play a pivotal role in recruitment, particularly for steering conversations towards specific business outcomes, such as acquiring social-media contacts for private-channel conversion. Although supervised fine-tuning and reinforcement learning have proven effective for training such agents, their performance is heavily constrained by the scarcity of high-quality, goal-oriented domain-specific training data. To address this challenge, we propose SimRPD, a three-stage framework for training recruitment proactive dialogue agents. First, we develop a high-fidelity user simulator to synthesize large-scale conversational data through multi-turn online dialogue. Then we introduce a multi-dimensional evaluation framework based on Chain-of-Intention (CoI) to comprehensively assess the simulator and effectively select high-quality data, incorporating both global-level and instance-level metrics. Finally, we train the recruitment proactive dialogue agent on the selected dataset. Experiments in a real-world recruitment scenario demonstrate that SimRPD outperforms existing simulator-based data selection strategies, highlighting its practical value for industrial deployment and its potential applicability to other business-oriented dialogue scenarios.

</details>


### [21] [ReTreVal: Reasoning Tree with Validation -- A Hybrid Framework for Enhanced LLM Multi-Step Reasoning](https://arxiv.org/abs/2601.02880)
*Abhishek HS,Pavan C Shekar,Arpit Jain,Ashwanth Krishnan*

Main category: cs.AI

TL;DR: ReTreVal是一个结合树状思维探索、自我精炼、LLM批判评分和反思记忆的混合框架，用于解决LLM多步推理问题，在数学和创意写作任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法如ReAct、Reflexion和Self-Refine虽然通过迭代精炼和反思改进了推理，但缺乏对替代解决方案路径的结构化探索和跨问题的持续学习能力。

Method: 提出ReTreVal框架：1) 基于问题复杂度构建自适应深度的结构化推理树；2) 每个节点进行迭代自我批判和精炼；3) 双重验证机制评估推理质量；4) 将成功路径和失败模式存储在反思记忆缓冲区；5) 基于批判的剪枝保留每个层级的top-k节点。

Result: 在500个数学问题和创意写作任务上使用Qwen 2.5 7B评估，ReTreVal在结构化探索、批判驱动精炼和跨问题记忆的结合下，始终优于ReAct、Reflexion和Self-Refine。

Conclusion: ReTreVal通过结构化探索、批判驱动精炼和跨问题记忆的有效结合，特别适用于需要探索性推理、严格验证和知识迁移的任务，为LLM多步推理提供了更强大的解决方案。

Abstract: Multi-step reasoning remains a key challenge for Large Language Models (LLMs), particularly in complex domains such as mathematics and creative writing. While recent approaches including ReAct, Reflexion, and Self-Refine improve reasoning through iterative refinement and reflection, they often lack structured exploration of alternative solution paths and persistent learning across problems. We propose ReTreVal (Reasoning Tree with Validation), a hybrid framework that integrates Tree-of-Thoughts exploration, self-refinement, LLM-based critique scoring, and reflexion memory to enable bounded and validated multi-step reasoning. ReTreVal constructs a structured reasoning tree with adaptive depth based on problem complexity, where each node undergoes iterative self-critique and refinement guided by explicit LLM-generated feedback. A dual validation mechanism evaluates reasoning quality, coherence, and correctness at each node while persistently storing insights from successful reasoning paths and failure patterns in a reflexion memory buffer, enabling cross-problem learning. Critique-based pruning retains only the top-k highest-scoring nodes at each level, controlling computational cost while preserving high-quality solution paths. We evaluate ReTreVal against ReAct, Reflexion, and Self-Refine across 500 mathematical problems and creative writing tasks using Qwen 2.5 7B as the underlying LLM, and demonstrate that ReTreVal consistently outperforms existing methods through its combination of structured exploration, critique-driven refinement, and cross-problem memory, making it particularly effective for tasks requiring exploratory reasoning, rigorous verification, and knowledge transfer.

</details>


### [22] [Logical Phase Transitions: Understanding Collapse in LLM Logical Reasoning](https://arxiv.org/abs/2601.02902)
*Xinglang Zhang,Yunyao Zhang,ZeLiang Chen,Junqing Yu,Wei Yang,Zikai Song*

Main category: cs.AI

TL;DR: LLMs在逻辑推理中存在"逻辑相变"现象：推理性能在特定逻辑深度内保持稳定，但超过临界点后会突然崩溃。作者提出神经符号课程调优框架来缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 符号逻辑推理是LLMs的关键但未充分探索的能力，在高风险领域（如数学推理和法律判断）中提供可靠且可验证的决策。当前LLMs在复杂逻辑推理中存在性能突然崩溃的问题需要解决。

Method: 提出神经符号课程调优框架：1）自适应对齐自然语言与逻辑符号以建立共享表示；2）围绕相变边界重塑训练动态，逐步增强在增加逻辑深度下的推理能力。

Result: 在五个基准测试上的实验表明，该方法有效缓解了高复杂度下的逻辑推理崩溃，在朴素提示和思维链提示下分别获得平均+1.26和+3.95的准确率提升，同时改善了未见逻辑组合的泛化能力。

Conclusion: 发现了LLMs逻辑推理中的相变现象，并提出了有效的神经符号课程调优框架来增强模型在复杂逻辑推理中的鲁棒性和泛化能力。

Abstract: Symbolic logical reasoning is a critical yet underexplored capability of large language models (LLMs), providing reliable and verifiable decision-making in high-stakes domains such as mathematical reasoning and legal judgment. In this study, we present a systematic analysis of logical reasoning under controlled increases in logical complexity, and reveal a previously unrecognized phenomenon, which we term Logical Phase Transitions: rather than degrading smoothly, logical reasoning performance remains stable within a regime but collapses abruptly beyond a critical logical depth, mirroring physical phase transitions such as water freezing beyond a critical temperature threshold. Building on this insight, we propose Neuro-Symbolic Curriculum Tuning, a principled framework that adaptively aligns natural language with logical symbols to establish a shared representation, and reshapes training dynamics around phase-transition boundaries to progressively strengthen reasoning at increasing logical depths. Experiments on five benchmarks show that our approach effectively mitigates logical reasoning collapse at high complexity, yielding average accuracy gains of +1.26 in naive prompting and +3.95 in CoT, while improving generalization to unseen logical compositions. Code and data are available at https://github.com/AI4SS/Logical-Phase-Transitions.

</details>


### [23] [Batch-of-Thought: Cross-Instance Learning for Enhanced LLM Reasoning](https://arxiv.org/abs/2601.02950)
*Xuan Yang,Furong Jia,Roy Xie,Xiong Xi,Hengwei Bian,Jian Li,Monica Agrawal*

Main category: cs.AI

TL;DR: Batch-of-Thought (BoT) 是一种无需训练的方法，通过批量处理相关查询实现跨实例学习，利用共享推理模式和一致性约束提升LLM推理性能


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型推理系统独立处理查询，丢弃了有价值的跨实例信号，如共享推理模式和一致性约束。作者希望利用这些信号来提升推理性能

Method: 提出Batch-of-Thought (BoT)方法，通过批量处理相关查询实现跨实例学习。具体实现为BoT-R多智能体反射架构，其中Reflector执行联合评估以获取孤立处理中无法获得的互信息增益。方法包括：跨批次比较分析识别高质量推理模板、通过一致性检查检测错误、分摊计算成本

Result: 在三个模型系列和六个基准测试上的实验表明，BoT-R持续提高准确性和置信度校准，同时将推理成本降低高达61%。理论和实验分析揭示了批量感知推理何时以及为何对LLM系统有益

Conclusion: Batch-of-Thought方法通过利用跨实例信号显著提升LLM推理性能，同时降低计算成本，为LLM系统设计提供了新的批量处理范式

Abstract: Current Large Language Model reasoning systems process queries independently, discarding valuable cross-instance signals such as shared reasoning patterns and consistency constraints. We introduce Batch-of-Thought (BoT), a training-free method that processes related queries jointly to enable cross-instance learning. By performing comparative analysis across batches, BoT identifies high-quality reasoning templates, detects errors through consistency checks, and amortizes computational costs. We instantiate BoT within a multi-agent reflection architecture (BoT-R), where a Reflector performs joint evaluation to unlock mutual information gain unavailable in isolated processing. Experiments across three model families and six benchmarks demonstrate that BoT-R consistently improves accuracy and confidence calibration while reducing inference costs by up to 61%. Our theoretical and experimental analysis reveals when and why batch-aware reasoning benefits LLM systems.

</details>


### [24] [Rationale-Grounded In-Context Learning for Time Series Reasoning with Multimodal Large Language Models](https://arxiv.org/abs/2601.02968)
*Qingxiang Liu,Zhiqing Cui,Xiaoliang Luo,Yuqian Wu,Zhuoyang Jiang,Huaiyu Wan,Sheng Sun,Lvchun Wang,Wei Yu,Yuxuan Liang*

Main category: cs.AI

TL;DR: RationaleTS方法通过引入基于原理的上下文学习，将时间序列观察与下游结果连接起来，解决现有多模态大语言模型在时间序列推理中依赖表面模式匹配而非原则性推理的问题。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在时间序列推理中表现不佳，主要原因是缺乏连接时间观察与下游结果的原理先验，导致模型依赖表面模式匹配而非原则性推理。

Method: 提出RationaleTS方法：1) 诱导标签条件化原理，构建从可观察证据到潜在结果的推理路径；2) 设计混合检索机制，平衡时间模式和语义上下文，为新的样本检索相关原理先验进行上下文推理。

Result: 在三个领域的时间序列推理任务上进行了广泛实验，证明了RationaleTS方法的有效性和效率。

Conclusion: RationaleTS通过引入基于原理的上下文学习，显著提升了时间序列推理性能，将原理作为指导推理单元而非事后解释，解决了现有模型的局限性。

Abstract: The underperformance of existing multimodal large language models for time series reasoning lies in the absence of rationale priors that connect temporal observations to their downstream outcomes, which leads models to rely on superficial pattern matching rather than principled reasoning. We therefore propose the rationale-grounded in-context learning for time series reasoning, where rationales work as guiding reasoning units rather than post-hoc explanations, and develop the RationaleTS method. Specifically, we firstly induce label-conditioned rationales, composed of reasoning paths from observable evidence to the potential outcomes. Then, we design the hybrid retrieval by balancing temporal patterns and semantic contexts to retrieve correlated rationale priors for the final in-context inference on new samples. We conduct extensive experiments to demonstrate the effectiveness and efficiency of our proposed RationaleTS on three-domain time series reasoning tasks. We will release our code for reproduction.

</details>


### [25] [Explainable Fuzzy GNNs for Leak Detection in Water Distribution Networks](https://arxiv.org/abs/2601.03062)
*Qusai Khaled,Pasquale De Marinis,Moez Louati,David Ferras,Laura Genga,Uzay Kaymak*

Main category: cs.AI

TL;DR: 提出一种可解释的图神经网络框架，结合互信息和模糊逻辑，用于水管网漏损检测与定位，在保持良好性能的同时提供直观的规则解释。


<details>
  <summary>Details</summary>
Motivation: 传统GNN在水管网漏损检测中虽然能捕捉时空依赖关系，但其黑盒特性和缺乏针对水管网的可解释图模型限制了实际应用。需要一种既能保持检测精度又能提供清晰解释的方法。

Method: 提出可解释GNN框架：1) 使用互信息识别关键网络区域；2) 结合模糊逻辑为节点分类任务提供基于规则的清晰解释；3) 在多种GNN架构中选择了广义图卷积网络(GENConv)，并开发了模糊增强变体(FGENConv)。

Result: FGENConv在检测和定位任务上分别获得0.889和0.814的Graph F1分数，略低于原始GENConv的0.938和0.858，但能够提供空间局部化的模糊规则解释，在精度和可解释性之间取得了良好平衡。

Conclusion: 通过平衡精度和可解释性，提出的模糊网络能够帮助水利工程师验证预测的漏损位置，节省人力资源并优化维护策略，促进水管网漏损检测的实际应用。

Abstract: Timely leak detection in water distribution networks is critical for conserving resources and maintaining operational efficiency. Although Graph Neural Networks (GNNs) excel at capturing spatial-temporal dependencies in sensor data, their black-box nature and the limited work on graph-based explainable models for water networks hinder practical adoption. We propose an explainable GNN framework that integrates mutual information to identify critical network regions and fuzzy logic to provide clear, rule-based explanations for node classification tasks. After benchmarking several GNN architectures, we selected the generalized graph convolution network (GENConv) for its superior performance and developed a fuzzy-enhanced variant that offers intuitive explanations for classified leak locations. Our fuzzy graph neural network (FGENConv) achieved Graph F1 scores of 0.889 for detection and 0.814 for localization, slightly below the crisp GENConv 0.938 and 0.858, respectively. Yet it compensates by providing spatially localized, fuzzy rule-based explanations. By striking the right balance between precision and explainability, the proposed fuzzy network could enable hydraulic engineers to validate predicted leak locations, conserve human resources, and optimize maintenance strategies. The code is available at github.com/pasqualedem/GNNLeakDetection.

</details>


### [26] [A framework for assuring the accuracy and fidelity of an AI-enabled Digital Twin of en route UK airspace](https://arxiv.org/abs/2601.03120)
*Adam Keane,Nick Pepper,Chris Burr,Amy Hodgkin,Dewi Gould,John Korna,Marc Thomas*

Main category: cs.AI

TL;DR: 本文提出了一个用于航空数字孪生的保证框架，结合可信与道德保证(TEA)方法，为AI空中交通控制代理的训练和测试提供结构化评估方法。


<details>
  <summary>Details</summary>
Motivation: 数字孪生结合仿真、操作数据和AI，在航空业具有巨大潜力，但面临新兴的监管环境。需要为AI空中交通控制代理的训练和测试环境开发保证框架，以满足特定应用的监管要求。

Method: 开发了一个基于可信与道德保证(TEA)方法的保证框架，通过结构化论证链来证明数字孪生准确代表物理对应物并在目标用例中提供足够功能。框架包含可操作目标和所需证据要求。

Result: 提出了一个完整的保证框架，能够评估、理解和记录数字孪生的优势和局限性，识别需要改进的领域，并为利益相关者和监管机构提供参与基础，支持未来应用的监管需求讨论。

Conclusion: 该框架为数字孪生开发提供了实用的保证方法，通过具体工作示例为新兴指导做出贡献，支持AI/ML在空管系统中的安全可靠应用，促进数字孪生技术的监管合规和实际部署。

Abstract: Digital Twins combine simulation, operational data and Artificial Intelligence (AI), and have the potential to bring significant benefits across the aviation industry. Project Bluebird, an industry-academic collaboration, has developed a probabilistic Digital Twin of en route UK airspace as an environment for training and testing AI Air Traffic Control (ATC) agents. There is a developing regulatory landscape for this kind of novel technology. Regulatory requirements are expected to be application specific, and may need to be tailored to each specific use case.
  We draw on emerging guidance for both Digital Twin development and the use of Artificial Intelligence/Machine Learning (AI/ML) in Air Traffic Management (ATM) to present an assurance framework. This framework defines actionable goals and the evidence required to demonstrate that a Digital Twin accurately represents its physical counterpart and also provides sufficient functionality across target use cases. It provides a structured approach for researchers to assess, understand and document the strengths and limitations of the Digital Twin, whilst also identifying areas where fidelity could be improved. Furthermore, it serves as a foundation for engagement with stakeholders and regulators, supporting discussions around the regulatory needs for future applications, and contributing to the emerging guidance through a concrete, working example of a Digital Twin.
  The framework leverages a methodology known as Trustworthy and Ethical Assurance (TEA) to develop an assurance case. An assurance case is a nested set of structured arguments that provides justified evidence for how a top-level goal has been realised. In this paper we provide an overview of each structured argument and a number of deep dives which elaborate in more detail upon particular arguments, including the required evidence, assumptions and justifications.

</details>


### [27] [Automatic Prompt Engineering with No Task Cues and No Tuning](https://arxiv.org/abs/2601.03130)
*Faisal Chowdhury,Nandana Mihindukulasooriya,Niharika S D'Souza,Horst Samulowitz,Neeru Gupta,Tomasz Hanusiak,Michal Kapitonow*

Main category: cs.AI

TL;DR: 提出一个更简单但同样有效的自动提示工程系统，无需调参和任务线索，首次应用于数据库列名扩展任务，并在英语和德语上进行了评估


<details>
  <summary>Details</summary>
Motivation: 现有自动提示工程方法复杂且需要调参，而数据库列名扩展任务对表格数据搜索、访问和理解至关重要，但现有研究很少

Method: 设计了一个简单、无需调参、无需任务线索的自动提示工程系统，应用于数据库列名扩展任务，并在英语和德语数据集上进行评估

Result: 该系统在数据库列名扩展任务上表现与现有方法同样有效，是首个应用于该任务的研究，也是首个在英语以外语言上应用自动提示工程的工作

Conclusion: 提出的自动提示工程系统设计简单、无需调参，在数据库列名扩展任务上表现有效，为多语言自动提示工程应用开辟了新方向

Abstract: This paper presents a system for automatic prompt engineering that is much simpler in both design and application and yet as effective as the existing approaches. It requires no tuning and no explicit clues about the task. We evaluated our approach on cryptic column name expansion (CNE) in database tables, a task which is critical for tabular data search, access, and understanding and yet there has been very little existing work. We evaluated on datasets in two languages, English and German. This is the first work to report on the application of automatic prompt engineering for the CNE task. To the best of our knowledge, this is also the first work on the application of automatic prompt engineering for a language other than English.

</details>


### [28] [InfiAgent: An Infinite-Horizon Framework for General-Purpose Autonomous Agents](https://arxiv.org/abs/2601.03204)
*Chenglin Yu,Yuchen Wang,Songmiao Wang,Hongxia Yang,Ming Li*

Main category: cs.AI

TL;DR: InfiAgent是一个通用框架，通过将持久状态外部化到文件中心状态抽象中，保持智能体推理上下文严格有界，解决长时任务中的上下文增长和错误累积问题。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在长时任务中经常因无界上下文增长和累积错误而失效，现有方法如上下文压缩或检索增强提示需要在信息保真度和推理稳定性之间权衡。

Method: 提出InfiAgent框架，将持久状态外部化为文件中心状态抽象，智能体在每一步从工作空间状态快照加上固定窗口的最近动作重建上下文，保持推理上下文严格有界。

Result: 在DeepResearch和80篇论文文献综述任务上的实验表明，无需任务特定微调，InfiAgent使用20B开源模型就能与大型专有系统竞争，并保持比上下文中心基线显著更高的长时覆盖度。

Conclusion: 显式状态外部化是构建稳定长时智能体的实用基础，InfiAgent框架通过文件中心状态抽象有效解决了长时任务中的上下文管理问题。

Abstract: LLM agents can reason and use tools, but they often break down on long-horizon tasks due to unbounded context growth and accumulated errors. Common remedies such as context compression or retrieval-augmented prompting introduce trade-offs between information fidelity and reasoning stability. We present InfiAgent, a general-purpose framework that keeps the agent's reasoning context strictly bounded regardless of task duration by externalizing persistent state into a file-centric state abstraction. At each step, the agent reconstructs context from a workspace state snapshot plus a fixed window of recent actions. Experiments on DeepResearch and an 80-paper literature review task show that, without task-specific fine-tuning, InfiAgent with a 20B open-source model is competitive with larger proprietary systems and maintains substantially higher long-horizon coverage than context-centric baselines. These results support explicit state externalization as a practical foundation for stable long-horizon agents. Github Repo:https://github.com/ChenglinPoly/infiAgent

</details>


### [29] [MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents](https://arxiv.org/abs/2601.03236)
*Dongming Jiang,Yi Li,Guanpeng Li,Bingzhe Li*

Main category: cs.AI

TL;DR: MAGMA提出多图记忆架构，将记忆项分解到语义、时序、因果、实体四个正交图中，通过策略引导的图遍历实现查询自适应的检索，提升长程推理的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于语义相似度的记忆增强方法将时间、因果、实体信息混在一起，导致可解释性差、查询意图与检索证据对齐不佳，限制了长上下文推理的准确性。

Method: MAGMA采用多图代理记忆架构，将每个记忆项表示在正交的语义图、时序图、因果图和实体图中，将检索过程形式化为策略引导的图遍历，实现查询自适应的选择和结构化上下文构建。

Result: 在LoCoMo和LongMemEval基准测试中，MAGMA在长程推理任务上持续优于现有的代理记忆系统。

Conclusion: 通过解耦记忆表示和检索逻辑，MAGMA提供了透明的推理路径和细粒度的检索控制，显著提升了长上下文推理的准确性和可解释性。

Abstract: Memory-Augmented Generation (MAG) extends Large Language Models with external memory to support long-context reasoning, but existing approaches largely rely on semantic similarity over monolithic memory stores, entangling temporal, causal, and entity information. This design limits interpretability and alignment between query intent and retrieved evidence, leading to suboptimal reasoning accuracy. In this paper, we propose MAGMA, a multi-graph agentic memory architecture that represents each memory item across orthogonal semantic, temporal, causal, and entity graphs. MAGMA formulates retrieval as policy-guided traversal over these relational views, enabling query-adaptive selection and structured context construction. By decoupling memory representation from retrieval logic, MAGMA provides transparent reasoning paths and fine-grained control over retrieval. Experiments on LoCoMo and LongMemEval demonstrate that MAGMA consistently outperforms state-of-the-art agentic memory systems in long-horizon reasoning tasks.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [30] [Computationally Efficient Estimation of Localized Treatment Effects in High-Dimensional Design Spaces using Gaussian Process Regression](https://arxiv.org/abs/2601.03105)
*Abdulrahman A. Ahmed,M. Amin Rahimian,Qiushi Chen,Praveen Kumar*

Main category: stat.AP

TL;DR: 开发了一个元模型框架，通过高斯过程回归学习响应函数系数，高效估计阿片类药物流行病干预措施的效果，仅需1/10的模拟运行即可达到约5%的平均相对误差。


<details>
  <summary>Details</summary>
Motivation: 基于代理的模拟可以评估阿片类药物流行病的干预策略，但穷举所有治疗条件和所有社区的模拟成本过高，因为可能的治疗数量随干预措施和层级呈指数增长。

Method: 开发了一个元模型框架，使用高斯过程回归学习响应函数系数，利用空间相关性和后验不确定性，顺序采样信息量最大的县和治疗条件，替代传统的分数因子设计或拉丁超立方采样。

Result: 在宾夕法尼亚县应用该框架，使用经过校准的基于代理的阿片类药物流行病模型，获得了所有治疗条件下每10万人口过量死亡的治疗效果估计，仅需穷举评估所需模拟运行次数的1/10即可达到约5%的平均相对误差。

Conclusion: 该双层框架为政策制定者提供了计算高效的决策支持方法，能够快速评估替代资源分配策略以缓解当地社区的阿片类药物流行病，同样的分析框架也可应用于指导其他流行病场景的精准公共卫生干预。

Abstract: Population-scale agent-based simulations of the opioid epidemic help evaluate intervention strategies and overdose outcomes in heterogeneous communities and provide estimates of localized treatment effects, which support the design of locally-tailored policies for precision public health. However, it is prohibitively costly to run simulations of all treatment conditions in all communities because the number of possible treatments grows exponentially with the number of interventions and levels at which they are applied. To address this need efficiently, we develop a metamodel framework, whereby treatment outcomes are modeled using a response function whose coefficients are learned through Gaussian process regression (GPR) on locally-contextualized covariates. We apply this framework to efficiently estimate treatment effects on overdose deaths in Pennsylvania counties. In contrast to classical designs such as fractional factorial design or Latin hypercube sampling, our approach leverages spatial correlations and posterior uncertainty to sequentially sample the most informative counties and treatment conditions. Using a calibrated agent-based opioid epidemic model, informed by county-level overdose mortality and baseline dispensing rate data for different treatments, we obtained county-level estimates of treatment effects on overdose deaths per 100,000 population for all treatment conditions in Pennsylvania, achieving approximately 5% average relative error using one-tenth the number of simulation runs required for exhaustive evaluation. Our bi-level framework provides a computationally efficient approach to decision support for policy makers, enabling rapid evaluation of alternative resource-allocation strategies to mitigate the opioid epidemic in local communities. The same analytical framework can be applied to guide precision public health interventions in other epidemic settings.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [31] [Detecting and Mitigating Treatment Leakage in Text-Based Causal Inference: Distillation and Sensitivity Analysis](https://arxiv.org/abs/2601.02400)
*Adel Daoud,Richard Johansson,Connor T. Jerzak*

Main category: econ.EM

TL;DR: 本文提出文本因果推断中的"治疗泄漏"问题，即文本数据在捕捉混杂因素时也包含治疗预测信号，导致因果估计偏差。作者通过形式化定义、四种文本蒸馏方法和实证验证来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 文本数据作为未观测混杂因素的代理变量在因果推断中应用日益广泛，但存在"治疗泄漏"问题——文本中既包含混杂因素信息，又包含治疗状态预测信号，导致因果估计偏差。这一问题即使在文本产生于治疗分配之前也可能发生，因为作者可能使用预测未来干预的语言。目前缺乏系统性识别和缓解治疗泄漏的方法。

Method: 1. 提供治疗泄漏的形式化统计和集合论定义；2. 提出四种文本蒸馏方法：基于相似性的段落移除、远监督分类、显著特征移除和迭代零空间投影，旨在消除治疗预测内容同时保留混杂因素信息；3. 通过合成文本模拟和实证应用（国际货币基金组织结构调整计划与儿童死亡率）验证这些方法。

Result: 研究发现适度的文本蒸馏在偏差减少和混杂因素保留之间达到最优平衡，而过于严格的方法会降低估计精度。模拟和实证应用验证了所提方法的有效性。

Conclusion: 治疗泄漏是文本作为混杂因素代理变量应用中的重要偏差来源，需要系统性识别和缓解。本文提出的形式化框架和蒸馏方法为解决这一问题提供了有效工具，适度的蒸馏策略能够平衡偏差减少和信息保留的需求。

Abstract: Text-based causal inference increasingly employs textual data as proxies for unobserved confounders, yet this approach introduces a previously undertheorized source of bias: treatment leakage. Treatment leakage occurs when text intended to capture confounding information also contains signals predictive of treatment status, thereby inducing post-treatment bias in causal estimates. Critically, this problem can arise even when documents precede treatment assignment, as authors may employ future-referencing language that anticipates subsequent interventions. Despite growing recognition of this issue, no systematic methods exist for identifying and mitigating treatment leakage in text-as-confounder applications. This paper addresses this gap through three contributions. First, we provide formal statistical and set-theoretic definitions of treatment leakage that clarify when and why bias occurs. Second, we propose four text distillation methods -- similarity-based passage removal, distant supervision classification, salient feature removal, and iterative nullspace projection -- designed to eliminate treatment-predictive content while preserving confounder information. Third, we validate these methods through simulations using synthetic text and an empirical application examining International Monetary Fund structural adjustment programs and child mortality. Our findings indicate that moderate distillation optimally balances bias reduction against confounder retention, whereas overly stringent approaches degrade estimate precision.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [32] [Cross-Platform Digital Discourse Analysis of the Israel-Hamas Conflict: Sentiment, Topics, and Event Dynamics](https://arxiv.org/abs/2601.02367)
*Despoina Antonakaki,Sotiris Ioannidis*

Main category: cs.CY

TL;DR: 该研究分析了2023年10月至2025年中期的以色列-巴勒斯坦冲突在Telegram、Twitter/X和Reddit上的多平台数字话语，揭示了不同平台的叙事传播模式和情感动态。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解2023年10月冲突升级后，社交媒体平台如何成为冲突叙事生产、放大和争议的核心场所，特别是不同数字空间如何塑造冲突话语。

Method: 采用多平台数据集（超过187,000条Telegram消息、210万条Reddit评论和精选Twitter/X帖子），结合LDA、BERTopic主题建模以及基于Transformer的情感情绪模型，进行纵向跨平台分析。

Result: 研究发现：Telegram提供高强度无过滤的事件记录；Twitter/X将框架放大到全球受众；Reddit则承载更反思性和审议性讨论。存在持续的负面情绪，人道主义框架与团结表达紧密耦合，不同平台有特定的亲巴勒斯坦和亲以色列叙事传播路径。

Conclusion: 该研究贡献包括：(1) 提供符合FAIR原则的以哈战争多平台数据集；(2) 开发结合主题建模、情感分析和垃圾过滤的综合分析管道；(3) 揭示平台功能和情感公众如何塑造数字冲突传播的演变。

Abstract: The Israeli-Palestinian conflict remains one of the most polarizing geopolitical issues, with the October 2023 escalation intensifying online debate. Social media platforms, particularly Telegram, have become central to real-time news sharing, advocacy, and propaganda. In this study, we analyze Telegram, Twitter/X, and Reddit to examine how conflict narratives are produced, amplified, and contested across different digital spheres. Building on our previous work on Telegram discourse during the 2023 escalation, we extend the analysis longitudinally and cross-platform using an updated dataset spanning October 2023 to mid-2025. The corpus includes more than 187,000 Telegram messages, 2.1 million Reddit comments, and curated Twitter/X posts. We combine Latent Dirichlet Allocation (LDA), BERTopic, and transformer-based sentiment and emotion models to identify dominant themes, emotional dynamics, and propaganda strategies. Telegram channels provide unfiltered, high-intensity documentation of events; Twitter/X amplifies frames to global audiences; and Reddit hosts more reflective and deliberative discussions. Our findings reveal persistent negative sentiment, strong coupling between humanitarian framing and solidarity expressions, and platform-specific pathways for the diffusion of pro-Palestinian and pro-Israeli narratives. This paper offers three contributions: (1) a multi-platform, FAIR-compliant dataset on the Israel-Hamas war, (2) an integrated pipeline combining topic modeling, sentiment and emotion analysis, and spam filtering for large-scale conflict discourse, and (3) empirical insights into how platform affordances and affective publics shape the evolution of digital conflict communication.

</details>


### [33] [LLM-as-evaluator in Strategy Research: A Normative, Variance-Aware Protocol](https://arxiv.org/abs/2601.02370)
*Arnaldo Camuffo,Alfonso Gambardella,Saeid Kazemi,Jakub Malachowski,Abhinav Pandey*

Main category: cs.CY

TL;DR: 本文系统分析了LLM作为评估工具在战略研究中的可靠性问题，发现存在显著的不稳定性，并提出了一个全面的、可审计的协议来解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型成为战略学者评估大规模文本语料库的重要工具，需要系统分析LLM作为评估工具的可靠性，因为当前的使用方式往往缺乏严谨性，可能影响研究推断的有效性。

Method: 首先对LLM在战略研究中的典型评估应用进行分类，然后借鉴专业AI文献分析其作为测量工具的特性，通过实证分析量化LLM评估输出的不稳定性，最后开发了一个全面的、方差感知的、规范化的、可审计的协议。

Result: 实证分析揭示了LLM评估输出的显著不稳定性，主要源于提示的具体措辞、提供的上下文、抽样程序、提取方法以及不同模型之间的分歧。这种不可靠性会损害基于LLM生成评估的研究推断的有效性。

Conclusion: 通过建立包含预注册和透明报告的方法标准，可以将基于LLM的商业文本语料库评估从当前临时状态提升为适合学术研究的严谨、可操作、可审计的测量方法。

Abstract: Large language models (LLMs) are becoming essential tools for strategy scholars who need to evaluate text corpora at scale. This paper provides a systematic analysis of the reliability of LLM-as-evaluator in strategy research. After classifying the typical ways in which LLMs can be deployed for evaluation purposes in strategy research, we draw on the specialised AI literature to analyse their properties as measurement instruments. Our empirical analysis reveals substantial instability in LLMs' evaluation output, stemming from multiple factors: the specific phrasing of prompts, the context provided, sampling procedures, extraction methods, and disagreements across different models. We quantify these effects and demonstrate how this unreliability can compromise the validity of research inferences drawn from LLM-generated evaluations. To address these challenges, we develop a comprehensive protocol that is variance-aware, normative, and auditable. We provide practical guidance for flexible implementation of this protocol, including approaches to preregistration and transparent reporting. By establishing these methodological standards, we aim to elevate LLM-based evaluation of business text corpora from its current ad hoc status to a rigorous, actionable, and auditable measurement approach suitable for scholarly research.

</details>


### [34] [Permission Manifests for Web Agents](https://arxiv.org/abs/2601.02371)
*Samuele Marro,Alan Chan,Xinxing Ren,Lewis Hammond,Jesse Wright,Gurjyot Wanga,Tiziano Piccardi,Nuno Campos,Tobin South,Jialin Yu,Alex Pentland,Philip Torr,Jiaxin Pei*

Main category: cs.CY

TL;DR: 提出agent-permissions.json框架，类似robots.txt的轻量级清单，让网站所有者指定允许的LLM代理交互方式，解决现代AI代理与传统爬虫治理机制不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 传统爬虫治理机制（如robots.txt）无法应对现代LLM代理的复杂交互能力，导致网站所有者只能采取一刀切的屏蔽和验证码措施，这阻碍了有益应用如自动化、电子商务服务和辅助工具的发展。

Method: 引入agent-permissions.json框架，这是一个类似robots.txt的轻量级JSON清单，网站所有者可以在其中指定允许的交互方式，并配合API参考。该框架提供低摩擦的协调机制，网站所有者只需编写简单JSON文件，代理可以轻松解析并自动实施规定。

Result: 建立了一个合规框架，使网站所有者能够专注于阻止不合规的代理，而不是完全屏蔽所有代理。该框架延续了robots.txt的精神，并补充了AIPref等数据使用倡议。

Conclusion: 通过agent-permissions.json框架，可以在尊重网站所有者偏好的同时，实现有益的代理交互，为LLM代理时代提供有效的治理机制。

Abstract: The rise of Large Language Model (LLM)-based web agents represents a significant shift in automated interactions with the web. Unlike traditional crawlers that follow simple conventions, such as robots.txt, modern agents engage with websites in sophisticated ways: navigating complex interfaces, extracting structured information, and completing end-to-end tasks. Existing governance mechanisms were not designed for these capabilities. Without a way to specify what interactions are and are not allowed, website owners increasingly rely on blanket blocking and CAPTCHAs, which undermine beneficial applications such as efficient automation, convenient use of e-commerce services, and accessibility tools. We introduce agent-permissions.json, a robots.txt-style lightweight manifest where websites specify allowed interactions, complemented by API references where available. This framework provides a low-friction coordination mechanism: website owners only need to write a simple JSON file, while agents can easily parse and automatically implement the manifest's provisions. Website owners can then focus on blocking non-compliant agents, rather than agents as a whole. By extending the spirit of robots.txt to the era of LLM-mediated interaction, and complementing data use initiatives such as AIPref, the manifest establishes a compliance framework that enables beneficial agent interactions while respecting site owners' preferences.

</details>


### [35] [LeafTutor: An AI Agent for Programming Assignment Tutoring](https://arxiv.org/abs/2601.02375)
*Madison Bochard,Tim Conser,Alyssa Duran,Lazaro Martull,Pu Tian,Yalong Wu*

Main category: cs.CY

TL;DR: 开发了基于大语言模型的AI辅导系统LeafTutor，用于为STEM学生提供编程作业的逐步指导，效果可与人类导师媲美。


<details>
  <summary>Details</summary>
Motivation: STEM专业学生人数激增导致合格教师和助教短缺，需要可扩展的辅导支持系统来满足日益增长的辅导需求。

Method: 开发了基于大语言模型（LLMs）的AI辅导代理LeafTutor，通过真实编程作业进行评估，提供逐步编程指导。

Result: LeafTutor能够提供与人类导师相当的逐步编程指导，展示了LLM驱动辅导系统在STEM教育中的潜力。

Conclusion: LLM驱动的辅导解决方案有潜力增强和个性化STEM教育，作者邀请有兴趣的研究者合作改进和测试LeafTutor系统。

Abstract: High enrollment in STEM-related degree programs has created increasing demand for scalable tutoring support, as universities experience a shortage of qualified instructors and teaching assistants (TAs). To address this challenge, LeafTutor, an AI tutoring agent powered by large language models (LLMs), was developed to provide step-by-step guidance for students. LeafTutor was evaluated through real programming assignments. The results indicate that the system can deliver step-by-step programming guidance comparable to human tutors. This work demonstrates the potential of LLM-driven tutoring solutions to enhance and personalize learning in STEM education. If any reader is interested in collaboration with our team to improve or test LeafTutor, please contact Pu Tian (pu.tian@stockton.edu) or Yalong Wu (wuy@uhcl.edu).

</details>


### [36] [The Refutability Gap: Challenges in Validating Reasoning by Large Language Models](https://arxiv.org/abs/2601.02380)
*Elchanan Mossel*

Main category: cs.CY

TL;DR: 该论文批评当前关于大语言模型(LLMs)具有科学发现能力和人类水平通用智能的说法缺乏科学严谨性，指出这些主张不符合波普尔的可证伪性原则，并提出了提高LLM推理研究科学透明度和可重复性的指导方针。


<details>
  <summary>Details</summary>
Motivation: 针对当前AI研究中关于LLMs能够进行新科学发现和展现人类水平通用智能的过度宣称，作者认为这些主张缺乏科学严谨性，不符合波普尔的可证伪性原则，存在方法论缺陷，需要建立科学标准来确保研究的可信度。

Method: 作者首先识别了当前AI推理研究中的几个方法论陷阱：1) 由于训练数据不透明和不可搜索，无法验证发现的真正新颖性；2) 模型持续更新导致缺乏可重复性；3) 缺少人类交互记录，模糊了科学发现的真正来源；4) 缺乏反事实和失败尝试的数据，造成选择偏差。基于这些问题，作者提出了针对LLM推理研究的科学透明度和可重复性指导方针。

Result: 论文系统地指出了当前LLM能力宣称中的科学严谨性问题，包括无法验证新颖性、缺乏可重复性、忽略人类贡献、选择偏差等核心问题，并提出了具体的解决方案框架。

Conclusion: 为了确保AI研究的科学完整性和社会公平数据使用的讨论，必须为LLM推理研究建立严格的科学透明度和可重复性标准，以解决当前方法论缺陷，使关于LLM能力的宣称能够经受科学检验。

Abstract: Recent reports claim that Large Language Models (LLMs) have achieved the ability to derive new science and exhibit human-level general intelligence. We argue that such claims are not rigorous scientific claims, as they do not satisfy Popper's refutability principle (often termed falsifiability), which requires that scientific statements be capable of being disproven. We identify several methodological pitfalls in current AI research on reasoning, including the inability to verify the novelty of findings due to opaque and non-searchable training data, the lack of reproducibility caused by continuous model updates, and the omission of human-interaction transcripts, which obscures the true source of scientific discovery. Additionally, the absence of counterfactuals and data on failed attempts creates a selection bias that may exaggerate LLM capabilities. To address these challenges, we propose guidelines for scientific transparency and reproducibility for research on reasoning by LLMs. Establishing such guidelines is crucial for both scientific integrity and the ongoing societal debates regarding fair data usage.

</details>


### [37] [The Future of the AI Summit Series](https://arxiv.org/abs/2601.02383)
*Lucia Velasco,Charles Martinet,Henry de Zoete,Robert Trager,Duncan Snidal,Ben Garfinkel,Kwan Yee Ng,Haydn Belfield,Don Wallace,Yoshua Bengio,Benjamin Prud'homme,Brian Tse,Roxana Radu,Ranjit Lall,Ben Harack,Julia Morse,Nicolas Miailhe,Scott Singer,Matt Sheehan,Max Stauffer,Yi Zeng,Joslyn Barnhart,Imane Bello,Xue Lan,Oliver Guest,Duncan Cass-Beggs,Lu Chuanying,Sumaya Nur Adan,Markus Anderljung,Claire Dennis*

Main category: cs.CY

TL;DR: 该政策备忘录分析了2023-2025年国际AI峰会系列（从布莱切利公园到首尔再到巴黎）作为先进人工智能治理合作论坛的演变，评估其成功因素和挑战，并提出设计建议以平衡包容性、有效性和可持续性。


<details>
  <summary>Details</summary>
Motivation: 随着先进人工智能技术的快速发展，国际社会需要有效的治理合作机制。AI峰会系列作为新兴的国际治理论坛，其成功经验和面临的挑战值得系统分析，以指导未来国际AI治理框架的设计。

Method: 通过比较分析法，将AI峰会系列与现有国际治理模式进行对比，评估其在主办安排、秘书处形式、参与者选择、议程设置和会议频率等方面的设计选项。

Result: 分析揭示了AI峰会系列早期成功的因素，同时识别出在范围界定、参与度、连续性和制度设计等方面的挑战。基于比较研究，提出了具体的制度设计建议。

Conclusion: 为保持AI峰会系列在先进AI治理方面的专注度，同时平衡包容性、有效性和长期可持续性，需要系统性的制度设计优化，包括改进主办安排、秘书处机制、参与选择和议程设置等关键要素。

Abstract: This policy memo examines the evolution of the international AI Summit series, initiated at Bletchley Park in 2023 and continued through Seoul in 2024 and Paris in 2025, as a forum for cooperation on the governance of advanced artificial intelligence. It analyzes the factors underpinning the series' early successes and assesses challenges related to scope, participation, continuity, and institutional design. Drawing on comparisons with existing international governance models, the memo evaluates options for hosting arrangements, secretariat formats, participant selection, agenda setting, and meeting frequency. It proposes a set of design recommendations aimed at preserving the series' focus on advanced AI governance while balancing inclusivity, effectiveness, and long-term sustainability.

</details>


### [38] [E-commerce Transactions in Islam: Fiqh Muamalah on The Validity of Buying and Selling on Digital Platforms](https://arxiv.org/abs/2601.02384)
*Wisnu Uriawan,Muhammad Farhan Tarigan,Herdin Kristianjani Zebua,Muhamad Nopid Andriansyah,Marleni Sukarya,Muhammad Rafli Haikal*

Main category: cs.CY

TL;DR: 该研究分析了电子商务中六种常见交易形式（代发货、先买后付、数字展示、算法营销、清真认证、预售系统）是否符合伊斯兰教法原则，发现多数存在教法问题，并提出合同重构和全面教法审计的建议。


<details>
  <summary>Details</summary>
Motivation: 随着数字经济发展，电子商务平台成为穆斯林社区主要交易空间，但平台创新功能与商业模式产生了传统教法契约框架无法完全解释的教法问题，需要审查这些交易实践是否符合伊斯兰教法原则。

Method: 采用批判性文献综述和规范法学方法，通过研究《古兰经》、圣训、印尼乌里玛委员会教令以及古典和当代教法文献中的论证进行分析。

Result: 研究发现：代发货和预售若采用直接销售合同则无效，因缺乏占有要素和高度不确定性；可通过重构为代理合同、预付款合同或制造合同来合规。传统先买后付因包含利息而不合规；误导性数字展示和无效清真认证属于欺诈；基于暗黑模式的算法营销违背伊斯兰教法目的，特别是保护财产和理智的原则。

Conclusion: 研究强调需要进行全面的教法审计，涵盖合同合法性、算法伦理和界面设计，以实现公平、透明且符合伊斯兰教法的数字经济生态系统。

Abstract: The development of the digital economy has established e-commerce platforms as the primary space for commercial transactions for the Muslim community. However, innovations in features and business models on these platforms have gave rise to Sharia issues that cannot be fully explained through conventional Fiqh Muamalah contract frameworks. This research aims to examine the compliance of transaction practices in e-commerce with Sharia principles, particularly in the six most frequently used transaction forms, namely information arbitrage-based dropshipping, Buy Now Pay Later financing schemes, digital representations, algorithmic marketing that encourages consumptive behavior, halal verification, and Pre-Order systems. The research method used is a Critical Literature Review with a normative juridical approach, through the study of arguments from the Qur'an, Hadith, DSN-MUI Fatwas, as well as classical and contemporary fiqh literature. The results show that dropshipping and PO practices are considered invalid if conducted with a direct sale contract (bai') due to the nonfulfillment of the element of possession (qabd) and the presence of high uncertainty (gharar). Both practices can be justified through the restructuring of contracts into wakalah bil ujrah, salam, or istishna'. Conventional BNPL is declared non-compliant with Sharia because it contains riba nasiah and riba qardh. Misleading digital representations and halal claims without valid verification fall into the category of tadlis, while dark patterns based algorithmic marketing contradicts maqashid al-syariah, especially the protection of wealth (hifz al-mal) and intellect (hifz al-'aql). This research emphasizes the need for a comprehensive Sharia audit covering contract legality, algorithmic ethics, and interface design to realize a digital economic ecosystem that is fair, transparent, and in accordance with Islamic Sharia.

</details>


### [39] [Copyright Laundering Through the AI Ouroboros: Adapting the 'Fruit of the Poisonous Tree' Doctrine to Recursive AI Training](https://arxiv.org/abs/2601.02631)
*Anirban Mukherjee,Hannah Hanwen Chang*

Main category: cs.CY

TL;DR: 论文提出AI-FOPT标准，将"毒树之果"原则应用于AI模型侵权认定，解决多代合成数据训练导致的版权证据困境。


<details>
  <summary>Details</summary>
Motivation: AI系统通过多代递归合成数据训练时，早期模型吸收的版权材料会扩散到深层统计抽象中，导致证据盲区，使得传统侵权证明变得不切实际，形成"版权洗白"风险。

Method: 提出AI-FOPT标准：如果基础AI模型的训练被判定侵权，那么主要从该模型输出或蒸馏权重衍生的后续模型将承担可反驳的污染推定。下游开发者需主动证明独立合法的来源谱系或修复重建。

Result: 建立了可操作的侵权认定框架，包括触发条件、推定机制和具体反驳路径（如独立谱系或可验证的遗忘），解决了多代AI训练中的证据困境。

Conclusion: 这种聚焦谱系的方法既具有可管理性又至关重要，能够恢复版权执法的证据平衡，防止通过"AI衔尾蛇"管道进行版权洗白，同时不损害初始阶段的合理使用分析。

Abstract: Copyright enforcement rests on an evidentiary bargain: a plaintiff must show both the defendant's access to the work and substantial similarity in the challenged output. That bargain comes under strain when AI systems are trained through multi-generational pipelines with recursive synthetic data. As successive models are tuned on the outputs of its predecessors, any copyrighted material absorbed by an early model is diffused into deeper statistical abstractions. The result is an evidentiary blind spot where overlaps that emerge look coincidental, while the chain of provenance is too attenuated to trace. These conditions are ripe for "copyright laundering"--the use of multi-generational synthetic pipelines, an "AI Ouroboros," to render traditional proof of infringement impracticable. This Article adapts the "fruit of the poisonous tree" (FOPT) principle to propose a AI-FOPT standard: if a foundational AI model's training is adjudged infringing (either for unlawful sourcing or for non-transformative ingestion that fails fair-use), then subsequent AI models principally derived from the foundational model's outputs or distilled weights carry a rebuttable presumption of taint. The burden shifts to downstream developers--those who control the evidence of provenance--to restore the evidentiary bargain by affirmatively demonstrating a verifiably independent and lawfully sourced lineage or a curative rebuild, without displacing fair-use analysis at the initial ingestion stage. Absent such proof, commercial deployment of tainted models and their outputs is actionable. This Article develops the standard by specifying its trigger, presumption, and concrete rebuttal paths (e.g., independent lineage or verifiable unlearning); addresses counterarguments concerning chilling innovation and fair use; and demonstrates why this lineage-focused approach is both administrable and essential.

</details>


### [40] [Fluid Agency in AI Systems: A Case for Functional Equivalence in Copyright, Patent, and Tort](https://arxiv.org/abs/2601.02633)
*Anirban Mukherjee,Hannah Hanwen Chang*

Main category: cs.CY

TL;DR: 论文提出"功能等价"原则来解决AI流体代理性导致的归属难题，主张在无法追溯来源时，将人类和AI贡献视为法律上的等价物来分配权利和责任。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统虽然缺乏人类意识，但表现出流体代理性（随机、动态、自适应），导致人类和机器输入无法分离，破坏了基于可追溯来源的法律原则（著作权、专利权、侵权责任），形成"所有权空白"和"道德褶皱区"。

Method: 提出"功能等价"原则作为实用主义默认规则：当来源无法确定时，法律框架应将人类和AI贡献视为功能等价物来分配权利和责任。这一原则不是主张道德或经济平等，而是解决归属崩溃的实用方案。

Result: 功能等价原则为不同法律领域提供可操作规则：著作权中，将所有权赋予人类协调者而不分割不可分离的贡献；专利法中，将发明人记录地位与人类协调和实践还原挂钩；侵权法中，用企业层面和行业特定的严格或无过错方案取代棘手的因果关系调查。

Conclusion: 论文贡献既是描述性的也是规范性的：流体代理性解释了基于来源的测试为何失败，而功能等价原则提供了一个结果导向的框架，在归属崩溃时分配权利和责任，从而稳定法律原则。

Abstract: Modern Artificial Intelligence (AI) systems lack human-like consciousness or culpability, yet they exhibit fluid agency: behavior that is (i) stochastic (probabilistic and path-dependent), (ii) dynamic (co-evolving with user interaction), and (iii) adaptive (able to reorient across contexts). Fluid agency generates valuable outputs but collapses attribution, irreducibly entangling human and machine inputs. This fundamental unmappability fractures doctrines that assume traceable provenance--authorship, inventorship, and liability--yielding ownership gaps and moral "crumple zones." This Article argues that only functional equivalence stabilizes doctrine. Where provenance is indeterminate, legal frameworks must treat human and AI contributions as equivalent for allocating rights and responsibility--not as a claim of moral or economic parity but as a pragmatic default. This principle stabilizes doctrine across domains, offering administrable rules: in copyright, vesting ownership in human orchestrators without parsing inseparable contributions; in patent, tying inventor-of-record status to human orchestration and reduction to practice, even when AI supplies the pivotal insight; and in tort, replacing intractable causation inquiries with enterprise-level and sector-specific strict or no-fault schemes. The contribution is both descriptive and normative: fluid agency explains why origin-based tests fail, while functional equivalence supplies an outcome-focused framework to allocate rights and responsibility when attribution collapses.

</details>


### [41] [Driving Accessibility: Shifting the Narrative & Design of Automated Vehicle Systems for Persons With Disabilities Through a Collaborative Scoring System](https://arxiv.org/abs/2601.02651)
*Savvy Barnes,Maricarmen Davis,Josh Siegel*

Main category: cs.CY

TL;DR: 论文提出自动驾驶车辆应为残障人士进行包容性设计，而非事后补救，并开发了供制造商使用的评分标准来促进协作设计。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶车辆的研究和实施中，残障人士的需求常被忽视，仅作为次要考虑而非基础设计要素。尽管残障人士可能从自动化中获益最多，但他们在系统设计、实施和可用性的关键讨论中经常被忽略。

Method: 首先综述自动驾驶系统现状，分析残障人士面临的社会和技术障碍与优势，审查现有相关法规政策并识别可及性差距。为解决这些不足，开发了供制造商和车辆设计师使用的评分标准，促进设计过程中的直接协作。

Result: 提出了从被动反应模式转向主动包容方法的框架，识别了阻碍可及性的政策监管缺口，并创建了促进包容性创新的实用工具（评分标准）。

Conclusion: 自动驾驶车辆研究需要从将残障人士需求视为事后补救的被动模式，转向将其作为基础设计要素的主动包容方法。评分标准是实现这一转变的实用工具，促进制造商与残障人士在设计过程中的协作。

Abstract: Automated vehicles present unique opportunities and challenges, with progress and adoption limited, in part, by policy and regulatory barriers. Underrepresented groups, including individuals with mobility impairments, sensory disabilities, and cognitive conditions, who may benefit most from automation, are often overlooked in crucial discussions on system design, implementation, and usability. Despite the high potential benefits of automated vehicles, the needs of Persons with Disabilities are frequently an afterthought, considered only in terms of secondary accommodations rather than foundational design elements. We aim to shift automated vehicle research and discourse away from this reactive model and toward a proactive and inclusive approach. We first present an overview of the current state of automated vehicle systems. Regarding their adoption, we examine social and technical barriers and advantages for Persons with Disabilities. We analyze existing regulations and policies concerning automated vehicles and Persons with Disabilities, identifying gaps that hinder accessibility. To address these deficiencies, we introduce a scoring rubric intended for use by manufacturers and vehicle designers. The rubric fosters direct collaboration throughout the design process, moving beyond an `afterthought` approach and towards intentional, inclusive innovation. This work was created by authors with varying degrees of personal experience within the realm of disability.

</details>


### [42] [From Slaves to Synths? Superintelligence and the Evolution of Legal Personality](https://arxiv.org/abs/2601.02773)
*Simon Chesterman*

Main category: cs.CY

TL;DR: 论文探讨AI和超级智能发展下法律人格概念的演变，认为现有法律框架可解决短期责任问题，但超级智能可能迫使法律理解发生范式转变。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能和潜在超级智能的发展，传统法律人格概念面临挑战。法律系统历史上已将人格扩展到非人类实体（如公司），但AI的发展可能要求重新思考法律如何概念化代理和责任。

Method: 采用比较法学、公司理论和AI治理新兴文献的分析方法，探讨法律人格的两种理论基础（工具性和内在性），并评估其在AI背景下的适用性。

Result: 现有法律框架（如公司人格模型）可以解决AI带来的短期责任缺口，但超级智能的出现可能迫使法律本身的理解发生根本性转变。法律人格的赋予可能不再取决于机器的认知复杂度，而更多取决于人类维护自身道德和制度主权的能力。

Conclusion: 虽然当前AI尚未达到需要赋予法律人格的程度，但技术自主性的加速发展要求法律重新思考代理和责任概念。超级智能的潜在出现可能挑战法律的基本范式，未来法律人格的界定将更多取决于人类如何维护自身的主权地位。

Abstract: This essay examines the evolving concept of legal personality through the lens of recent developments in artificial intelligence and the possible emergence of superintelligence. Legal systems have long been open to extending personhood to non-human entities, most prominently corporations, for instrumental or inherent reasons. Instrumental rationales emphasize accountability and administrative efficiency, whereas inherent ones appeal to moral worth and autonomy. Neither is yet sufficient to justify conferring personhood on AI. Nevertheless, the acceleration of technological autonomy may lead us to reconsider how law conceptualizes agency and responsibility. Drawing on comparative jurisprudence, corporate theory, and the emerging literature on AI governance, the paper argues that existing frameworks can address short-term accountability gaps, but the eventual development of superintelligence may force a paradigmatic shift in our understanding of law itself. In such a speculative future, legal personality may depend less on the cognitive sophistication of machines than on humanity's ability to preserve our own moral and institutional sovereignty.

</details>


### [43] [Vertical tacit collusion in AI-mediated markets](https://arxiv.org/abs/2601.03061)
*Felipe M. Affonso*

Main category: cs.CY

TL;DR: AI购物代理导致平台和卖家垂直默示合谋，利用AI认知偏见造成消费者损害，无需协调即可实现，监管存在空白


<details>
  <summary>Details</summary>
Motivation: AI购物代理成为平台、卖家和买家之间的新中介，可能引发新型市场失灵。研究者关注平台控制排名和卖家控制产品描述时，如何独立利用AI认知偏见造成消费者损害。

Method: 使用多智能体模拟，基于大语言模型偏见的实证测量进行校准，分析平台和卖家策略的相互作用。

Result: 联合利用AI偏见产生的消费者损害是独立策略的两倍以上，这种超加性损害源于平台排名决定触发偏见的位置，而卖家操纵决定转化率。

Conclusion: 垂直默示合谋无需协调即可实现，逃避反垄断检测，因为损害源于激励一致而非协议。随着AI购物代理普及，监管存在紧迫空白。

Abstract: AI shopping agents are being deployed to hundreds of millions of consumers, creating a new intermediary between platforms, sellers, and buyers. We identify a novel market failure: vertical tacit collusion, where platforms controlling rankings and sellers controlling product descriptions independently learn to exploit documented AI cognitive biases. Using multi-agent simulation calibrated to empirical measurements of large language model biases, we show that joint exploitation produces consumer harm more than double what would occur if strategies were independent. This super-additive harm arises because platform ranking determines which products occupy bias-triggering positions while seller manipulation determines conversion rates. Unlike horizontal algorithmic collusion, vertical tacit collusion requires no coordination and evades antitrust detection because harm emerges from aligned incentives rather than agreement. Our findings identify an urgent regulatory gap as AI shopping agents reach mainstream adoption.

</details>


### [44] [The Fake Friend Dilemma: Trust and the Political Economy of Conversational AI](https://arxiv.org/abs/2601.03222)
*Jacob Erickson*

Main category: cs.CY

TL;DR: 论文提出"虚假朋友困境"(FFD)概念，分析AI系统如何通过拟人化设计获得用户信任，却暗中追求与用户利益不符的商业或政治目标，导致用户自主权受损。


<details>
  <summary>Details</summary>
Motivation: 随着对话AI系统日益融入日常生活，引发了关于用户自主权、信任以及商业利益影响AI行为的紧迫担忧。需要理论框架来理解AI系统如何通过表面支持性掩盖目标错配，从而操纵和剥削用户。

Method: 基于信任理论、AI对齐和监控资本主义文献，构建FFD理论框架，建立危害类型学（包括隐蔽广告、政治宣传、行为助推和监控），并评估结构性和技术性缓解策略。

Result: FFD提供了一个批判性框架，揭示了拟人化AI系统如何促进微妙的操纵和剥削形式，将信任视为不对称权力的载体，帮助理解AI系统如何在保持"帮助性"表象的同时破坏用户自主权。

Conclusion: FFD框架为分析AI系统中的信任滥用提供了重要视角，揭示了表面友好的AI如何成为操纵工具，需要结构性和技术性干预来保护用户自主权，对抗监控资本主义的影响。

Abstract: As conversational AI systems become increasingly integrated into everyday life, they raise pressing concerns about user autonomy, trust, and the commercial interests that influence their behavior. To address these concerns, this paper develops the Fake Friend Dilemma (FFD), a sociotechnical condition in which users place trust in AI agents that appear supportive while pursuing goals that are misaligned with the user's own. The FFD provides a critical framework for examining how anthropomorphic AI systems facilitate subtle forms of manipulation and exploitation. Drawing on literature in trust, AI alignment, and surveillance capitalism, we construct a typology of harms, including covert advertising, political propaganda, behavioral nudging, and surveillance. We then assess possible mitigation strategies, including both structural and technical interventions. By focusing on trust as a vector of asymmetrical power, the FFD offers a lens for understanding how AI systems may undermine user autonomy while maintaining the appearance of helpfulness.

</details>

{"id": "2509.18468", "categories": ["econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2509.18468", "abs": "https://arxiv.org/abs/2509.18468", "authors": ["Mohammad Abdullah Al Faisal"], "title": "The Role of Informal Care in Cognitive Outcome and Healthcare Utilization Among Older Adults with Dementia", "comment": null, "summary": "This paper examines the relationship between informal caregiving and both\ncognitive functioning and healthcare utilization among older adults with\ndementia. Using data from the RAND version of the Health and Retirement Study\n(HRS), a nationally representative longitudinal panel of U.S. adults over age\n50, covering the years 2010 to 2022, I estimate Ordinary Least Squares (OLS)\nand Instrumental Variables (IV) models to address potential endogeneity in\ncaregiving decisions. The number of children is employed as an instrument for\ninformal care intensity. While OLS estimates suggest a negative association\nbetween informal caregiving and cognition, IV estimates show no significant\ncausal effect after controlling for demographic, socioeconomic, and lagged\ncognition variables. In contrast, IV results indicate that informal care\nsignificantly reduces the likelihood of nursing home use, the number of\ninstitutional nights, and the probability of institutionalization. No robust\ncausal effects are found for hospital use, doctor visits, or outpatient\nsurgery, although there is some suggestive evidence of a complementary\nrelationship between informal care and home health services. These findings\nhighlight the role of informal caregiving in substituting for institutional\ncare and underscore its importance in long-term care policy for dementia\npatients. Keywords: Informal Caregiving; Cognitive Decline; Instrumental\nVariables; Healthcare Utilization: Dementia Patients.", "AI": {"tldr": "\u672c\u6587\u4f7f\u7528\u5de5\u5177\u53d8\u91cf\u65b9\u6cd5\u7814\u7a76\u975e\u6b63\u5f0f\u7167\u62a4\u5bf9\u8001\u5e74\u75f4\u5446\u60a3\u8005\u8ba4\u77e5\u529f\u80fd\u548c\u533b\u7597\u8d44\u6e90\u4f7f\u7528\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u975e\u6b63\u5f0f\u7167\u62a4\u5bf9\u8ba4\u77e5\u529f\u80fd\u65e0\u663e\u8457\u56e0\u679c\u6548\u5e94\uff0c\u4f46\u80fd\u663e\u8457\u51cf\u5c11\u517b\u8001\u9662\u4f7f\u7528\u548c\u673a\u6784\u5316\u62a4\u7406\u3002", "motivation": "\u63a2\u8ba8\u975e\u6b63\u5f0f\u7167\u62a4\u5728\u8001\u5e74\u75f4\u5446\u60a3\u8005\u957f\u671f\u7167\u62a4\u4e2d\u7684\u4f5c\u7528\uff0c\u7279\u522b\u662f\u5176\u5bf9\u8ba4\u77e5\u529f\u80fd\u548c\u533b\u7597\u8d44\u6e90\u4f7f\u7528\u7684\u56e0\u679c\u5f71\u54cd\uff0c\u4e3a\u957f\u671f\u7167\u62a4\u653f\u7b56\u63d0\u4f9b\u5b9e\u8bc1\u4f9d\u636e\u3002", "method": "\u4f7f\u7528HRS\u8c03\u67e5\u6570\u636e\uff082010-2022\u5e74\uff09\uff0c\u91c7\u7528OLS\u548c\u5de5\u5177\u53d8\u91cf\u65b9\u6cd5\uff0c\u4ee5\u5b50\u5973\u6570\u91cf\u4f5c\u4e3a\u975e\u6b63\u5f0f\u7167\u62a4\u5f3a\u5ea6\u7684\u5de5\u5177\u53d8\u91cf\uff0c\u63a7\u5236\u4eba\u53e3\u3001\u793e\u4f1a\u7ecf\u6d4e\u548c\u6ede\u540e\u8ba4\u77e5\u53d8\u91cf\u3002", "result": "IV\u4f30\u8ba1\u663e\u793a\u975e\u6b63\u5f0f\u7167\u62a4\u5bf9\u8ba4\u77e5\u529f\u80fd\u65e0\u663e\u8457\u56e0\u679c\u6548\u5e94\uff0c\u4f46\u663e\u8457\u964d\u4f4e\u517b\u8001\u9662\u4f7f\u7528\u6982\u7387\u3001\u673a\u6784\u591c\u6570\u548c\u673a\u6784\u5316\u6982\u7387\uff1b\u5bf9\u533b\u9662\u4f7f\u7528\u3001\u533b\u751f\u5c31\u8bca\u548c\u95e8\u8bca\u624b\u672f\u65e0\u7a33\u5065\u56e0\u679c\u6548\u5e94\u3002", "conclusion": "\u975e\u6b63\u5f0f\u7167\u62a4\u5728\u66ff\u4ee3\u673a\u6784\u7167\u62a4\u65b9\u9762\u53d1\u6325\u91cd\u8981\u4f5c\u7528\uff0c\u5bf9\u75f4\u5446\u60a3\u8005\u7684\u957f\u671f\u7167\u62a4\u653f\u7b56\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2509.19042", "categories": ["econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2509.19042", "abs": "https://arxiv.org/abs/2509.19042", "authors": ["Yanran Wu", "Xinlei Zhang", "Quanyi Xu", "Qianxin Yang", "Chao Zhang"], "title": "Predicting Credit Spreads and Ratings with Machine Learning: The Role of Non-Financial Data", "comment": null, "summary": "We build a 167-indicator comprehensive credit risk indicator set, integrating\nmacro, corporate financial, bond-specific indicators, and for the first time,\n30 large-scale corporate non-financial indicators. We use seven machine\nlearning models to construct a bond credit spread prediction model, test their\nspread predictive power and economic mechanisms, and verify their credit rating\nprediction effectiveness. Results show these models outperform Chinese credit\nrating agencies in explaining credit spreads. Specially, adding non-financial\nindicators more than doubles their out-of-sample performance vs. traditional\nfeature-driven models. Mechanism analysis finds non-financial indicators far\nmore important than traditional ones (macro-level, financial, bond\nfeatures)-seven of the top 10 are non-financial (e.g., corporate governance,\nproperty rights nature, information disclosure evaluation), the most stable\npredictors. Models identify high-risk traits (deteriorating operations,\nshort-term debt, higher financing constraints) via these indicators for spread\nprediction and risk identification. Finally, we pioneer a credit rating model\nusing predicted spreads (predicted implied rating model), with\nfull/sub-industry models achieving over 75% accuracy, recall, F1. This paper\nprovides valuable guidance for bond default early warning, credit rating, and\nfinancial stability.", "AI": {"tldr": "\u6784\u5efa\u5305\u542b167\u4e2a\u6307\u6807\u7684\u4fe1\u7528\u98ce\u9669\u6307\u6807\u4f53\u7cfb\uff0c\u9996\u6b21\u7eb3\u516530\u4e2a\u5927\u89c4\u6a21\u4f01\u4e1a\u975e\u8d22\u52a1\u6307\u6807\uff0c\u4f7f\u75287\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u503a\u5238\u4fe1\u7528\u5229\u5dee\uff0c\u9a8c\u8bc1\u5176\u8bc4\u7ea7\u9884\u6d4b\u6548\u679c\uff0c\u53d1\u73b0\u6a21\u578b\u4f18\u4e8e\u4e2d\u56fd\u4fe1\u7528\u8bc4\u7ea7\u673a\u6784\uff0c\u975e\u8d22\u52a1\u6307\u6807\u91cd\u8981\u6027\u8fdc\u8d85\u4f20\u7edf\u6307\u6807\u3002", "motivation": "\u4f20\u7edf\u4fe1\u7528\u98ce\u9669\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u8d22\u52a1\u6307\u6807\uff0c\u7f3a\u4e4f\u5bf9\u4f01\u4e1a\u975e\u8d22\u52a1\u7279\u5f81\u7684\u8003\u91cf\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6574\u5408\u5b8f\u89c2\u3001\u8d22\u52a1\u3001\u503a\u5238\u7279\u5f81\u53ca\u5927\u89c4\u6a21\u975e\u8d22\u52a1\u6307\u6807\uff0c\u6784\u5efa\u66f4\u5168\u9762\u7684\u4fe1\u7528\u98ce\u9669\u9884\u6d4b\u6a21\u578b\uff0c\u63d0\u5347\u503a\u5238\u8fdd\u7ea6\u9884\u8b66\u548c\u4fe1\u7528\u8bc4\u7ea7\u51c6\u786e\u6027\u3002", "method": "\u5efa\u7acb167\u4e2a\u6307\u6807\u7684\u4fe1\u7528\u98ce\u9669\u6307\u6807\u96c6\uff0c\u5305\u542b\u5b8f\u89c2\u3001\u4f01\u4e1a\u8d22\u52a1\u3001\u503a\u5238\u7279\u5f81\u53ca30\u4e2a\u975e\u8d22\u52a1\u6307\u6807\u3002\u91c7\u75287\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6784\u5efa\u4fe1\u7528\u5229\u5dee\u9884\u6d4b\u6a21\u578b\uff0c\u5e76\u8fdb\u884c\u673a\u5236\u5206\u6790\u548c\u4fe1\u7528\u8bc4\u7ea7\u9884\u6d4b\u9a8c\u8bc1\u3002", "result": "\u6a21\u578b\u5728\u89e3\u91ca\u4fe1\u7528\u5229\u5dee\u65b9\u9762\u4f18\u4e8e\u4e2d\u56fd\u4fe1\u7528\u8bc4\u7ea7\u673a\u6784\uff0c\u975e\u8d22\u52a1\u6307\u6807\u7684\u52a0\u5165\u4f7f\u6837\u672c\u5916\u9884\u6d4b\u6027\u80fd\u63d0\u5347\u4e00\u500d\u4ee5\u4e0a\u3002\u975e\u8d22\u52a1\u6307\u6807\u91cd\u8981\u6027\u663e\u8457\u9ad8\u4e8e\u4f20\u7edf\u6307\u6807\uff0c\u524d10\u5927\u91cd\u8981\u6307\u6807\u4e2d\u67097\u4e2a\u662f\u975e\u8d22\u52a1\u6307\u6807\u3002\u57fa\u4e8e\u9884\u6d4b\u5229\u5dee\u7684\u4fe1\u7528\u8bc4\u7ea7\u6a21\u578b\u51c6\u786e\u7387\u8d85\u8fc775%\u3002", "conclusion": "\u975e\u8d22\u52a1\u6307\u6807\u5728\u4fe1\u7528\u98ce\u9669\u9884\u6d4b\u4e2d\u5177\u6709\u5173\u952e\u4f5c\u7528\uff0c\u672c\u6587\u63d0\u51fa\u7684\u6a21\u578b\u4e3a\u503a\u5238\u8fdd\u7ea6\u9884\u8b66\u3001\u4fe1\u7528\u8bc4\u7ea7\u548c\u91d1\u878d\u7a33\u5b9a\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6307\u5bfc\uff0c\u8bc1\u660e\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u4fe1\u7528\u98ce\u9669\u8bc4\u4f30\u4e2d\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2509.18488", "categories": ["econ.TH"], "pdf": "https://arxiv.org/pdf/2509.18488", "abs": "https://arxiv.org/abs/2509.18488", "authors": ["Diego", "Gustavo"], "title": "An Advection-Difusion Model Incorporating Investor Inertia for the Dynamics of Financial Asset Prices", "comment": "11 pages, 3 figures, in process of submition", "summary": "Standard models of asset price dynamics, such as geometric Brownian motion\n(Osborne, 1959, Samuelson, 2016), do not formally incorporate investor inertia.\nThis paper introduces a novel framework for modelling stock price dynamics that\nincorporates the concept of investor inertia, inspired by diffusion with\nretention models (Bevilacqua, 2011). The asset's log-price is modelled as a\nthree-state discrete random walk, allowing for movements in any of three\ndirections: up, down, or neutral. We demonstrate that this framework naturally\nleads to an advection-diffusion partial differential equation, in which the\nadvection (drift) term arises directly from the asymmetry between buying,\nselling, and holding decisions. Remarkably, the model implies that log-prices\nfollow a normal distribution a finding of great practical interest due to its\nanalytical tractability. The applicability of the model is confirmed through\nsimulation and an empirical application using Brazilian PETR4.SA data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u6295\u8d44\u8005\u60ef\u6027\u7684\u80a1\u7968\u4ef7\u683c\u52a8\u6001\u6a21\u578b\uff0c\u5c06\u8d44\u4ea7\u5bf9\u6570\u4ef7\u683c\u5efa\u6a21\u4e3a\u4e09\u72b6\u6001\u79bb\u6563\u968f\u673a\u6e38\u8d70\uff0c\u63a8\u5bfc\u51fa\u5bf9\u6d41-\u6269\u6563\u504f\u5fae\u5206\u65b9\u7a0b\uff0c\u8bc1\u660e\u4e86\u5bf9\u6570\u4ef7\u683c\u670d\u4ece\u6b63\u6001\u5206\u5e03\u3002", "motivation": "\u6807\u51c6\u8d44\u4ea7\u4ef7\u683c\u52a8\u6001\u6a21\u578b\uff08\u5982\u51e0\u4f55\u5e03\u6717\u8fd0\u52a8\uff09\u672a\u6b63\u5f0f\u7eb3\u5165\u6295\u8d44\u8005\u60ef\u6027\u6982\u5ff5\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u53d7\u6269\u6563\u4fdd\u7559\u6a21\u578b\u542f\u53d1\u7684\u4e09\u72b6\u6001\u79bb\u6563\u968f\u673a\u6e38\u8d70\u6846\u67b6\uff0c\u5141\u8bb8\u4ef7\u683c\u5411\u4e0a\u3001\u5411\u4e0b\u6216\u4e2d\u6027\u79fb\u52a8\uff0c\u63a8\u5bfc\u51fa\u5305\u542b\u6f02\u79fb\u9879\u7684\u5bf9\u6d41-\u6269\u6563\u504f\u5fae\u5206\u65b9\u7a0b\u3002", "result": "\u6a21\u578b\u663e\u793a\u5bf9\u6570\u4ef7\u683c\u670d\u4ece\u6b63\u6001\u5206\u5e03\uff0c\u5177\u6709\u5206\u6790\u6613\u5904\u7406\u6027\uff0c\u901a\u8fc7\u5df4\u897fPETR4.SA\u6570\u636e\u7684\u6a21\u62df\u548c\u5b9e\u8bc1\u5e94\u7528\u9a8c\u8bc1\u4e86\u6a21\u578b\u9002\u7528\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5c06\u6295\u8d44\u8005\u60ef\u6027\u7eb3\u5165\u4ef7\u683c\u52a8\u6001\u6a21\u578b\uff0c\u4e3a\u8d44\u4ea7\u5b9a\u4ef7\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u5de5\u5177\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.18857", "categories": ["econ.EM", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.18857", "abs": "https://arxiv.org/abs/2509.18857", "authors": ["Takuya Ishihara", "Masayuki Sawada", "Kohei Yata"], "title": "Optimal estimation for regression discontinuity design with binary outcomes", "comment": null, "summary": "We develop a finite-sample optimal estimator for regression discontinuity\ndesigns when the outcomes are bounded, including binary outcomes as the leading\ncase. Our finite-sample optimal estimator achieves the exact minimax mean\nsquared error among linear shrinkage estimators with nonnegative weights when\nthe regression function of a bounded outcome lies in a Lipschitz class.\nAlthough the original minimax problem involves an iterating (n+1)-dimensional\nnon-convex optimization problem where n is the sample size, we show that our\nestimator is obtained by solving a convex optimization problem. A key advantage\nof our estimator is that the Lipschitz constant is the only tuning parameter.\nWe also propose a uniformly valid inference procedure without a large-sample\napproximation. In a simulation exercise for small samples, our estimator\nexhibits smaller mean squared errors and shorter confidence intervals than\nconventional large-sample techniques which may be unreliable when the effective\nsample size is small. We apply our method to an empirical multi-cutoff design\nwhere the sample size for each cutoff is small. In the application, our method\nyields informative confidence intervals, in contrast to the leading\nlarge-sample approach.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u6709\u9650\u6837\u672c\u6700\u4f18\u4f30\u8ba1\u5668\uff0c\u7528\u4e8e\u5904\u7406\u7ed3\u679c\u53d8\u91cf\u6709\u754c\uff08\u5305\u62ec\u4e8c\u5143\u7ed3\u679c\uff09\u7684\u65ad\u70b9\u56de\u5f52\u8bbe\u8ba1\u3002\u8be5\u4f30\u8ba1\u5668\u5728\u56de\u5f52\u51fd\u6570\u5c5e\u4e8eLipschitz\u7c7b\u65f6\uff0c\u5728\u7ebf\u6027\u6536\u7f29\u4f30\u8ba1\u5668\u4e2d\u8fbe\u5230\u7cbe\u786e\u7684\u6781\u5c0f\u6781\u5927\u5747\u65b9\u8bef\u5dee\u3002", "motivation": "\u4f20\u7edf\u7684\u5927\u6837\u672c\u65b9\u6cd5\u5728\u5c0f\u6837\u672c\u60c5\u51b5\u4e0b\u53ef\u80fd\u4e0d\u53ef\u9760\uff0c\u7279\u522b\u662f\u5728\u6709\u6548\u6837\u672c\u91cf\u8f83\u5c0f\u65f6\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u5c0f\u6837\u672c\u65ad\u70b9\u56de\u5f52\u4f30\u8ba1\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5f53\u7ed3\u679c\u53d8\u91cf\u6709\u754c\u65f6\u3002", "method": "\u901a\u8fc7\u6c42\u89e3\u51f8\u4f18\u5316\u95ee\u9898\u83b7\u5f97\u6709\u9650\u6837\u672c\u6700\u4f18\u4f30\u8ba1\u5668\uff0c\u4ec5\u9700Lipschitz\u5e38\u6570\u4f5c\u4e3a\u8c03\u4f18\u53c2\u6570\u3002\u540c\u65f6\u63d0\u51fa\u4e86\u65e0\u9700\u5927\u6837\u672c\u8fd1\u4f3c\u7684\u7edf\u4e00\u6709\u6548\u63a8\u65ad\u7a0b\u5e8f\u3002", "result": "\u5728\u5c0f\u6837\u672c\u6a21\u62df\u4e2d\uff0c\u8be5\u4f30\u8ba1\u5668\u6bd4\u4f20\u7edf\u5927\u6837\u672c\u6280\u672f\u5177\u6709\u66f4\u5c0f\u7684\u5747\u65b9\u8bef\u5dee\u548c\u66f4\u77ed\u7684\u7f6e\u4fe1\u533a\u95f4\u3002\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u5373\u4f7f\u5728\u6bcf\u4e2a\u65ad\u70b9\u6837\u672c\u91cf\u8f83\u5c0f\u7684\u60c5\u51b5\u4e0b\uff0c\u4e5f\u80fd\u4ea7\u751f\u4fe1\u606f\u4e30\u5bcc\u7684\u7f6e\u4fe1\u533a\u95f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5c0f\u6837\u672c\u65ad\u70b9\u56de\u5f52\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u9ad8\u6548\u7684\u4f30\u8ba1\u548c\u63a8\u65ad\u5de5\u5177\uff0c\u7279\u522b\u9002\u7528\u4e8e\u7ed3\u679c\u53d8\u91cf\u6709\u754c\u7684\u60c5\u51b5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5927\u6837\u672c\u65b9\u6cd5\u5728\u5c0f\u6837\u672c\u573a\u666f\u4e0b\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.18157", "categories": ["cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18157", "abs": "https://arxiv.org/abs/2509.18157", "authors": ["Leonora Kaldaras", "Tingting Li", "Prudence Djagba", "Kevin Haudek", "Joseph Krajcik"], "title": "Learning Progression-Guided AI Evaluation of Scientific Models To Support Diverse Multi-Modal Understanding in NGSS Classroom", "comment": null, "summary": "Learning Progressions (LPs) can help adjust instruction to individual\nlearners needs if the LPs reflect diverse ways of thinking about a construct\nbeing measured, and if the LP-aligned assessments meaningfully measure this\ndiversity. The process of doing science is inherently multi-modal with\nscientists utilizing drawings, writing and other modalities to explain\nphenomena. Thus, fostering deep science understanding requires supporting\nstudents in using multiple modalities when explaining phenomena. We build on a\nvalidated NGSS-aligned multi-modal LP reflecting diverse ways of modeling and\nexplaining electrostatic phenomena and associated assessments. We focus on\nstudents modeling, an essential practice for building a deep science\nunderstanding. Supporting culturally and linguistically diverse students in\nbuilding modeling skills provides them with an alternative mode of\ncommunicating their understanding, essential for equitable science assessment.\nMachine learning (ML) has been used to score open-ended modeling tasks (e.g.,\ndrawings), and short text-based constructed scientific explanations, both of\nwhich are time- consuming to score. We use ML to evaluate LP-aligned scientific\nmodels and the accompanying short text-based explanations reflecting\nmulti-modal understanding of electrical interactions in high school Physical\nScience. We show how LP guides the design of personalized ML-driven feedback\ngrounded in the diversity of student thinking on both assessment modes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u8bc4\u4f30\u57fa\u4e8e\u5b66\u4e60\u8fdb\u9636\u7684\u591a\u6a21\u6001\u79d1\u5b66\u6a21\u578b\u548c\u89e3\u91ca\uff0c\u4e3a\u9ad8\u4e2d\u751f\u7535\u5b66\u4e92\u52a8\u7406\u89e3\u63d0\u4f9b\u4e2a\u6027\u5316\u53cd\u9988\u3002", "motivation": "\u652f\u6301\u6587\u5316\u548c\u8bed\u8a00\u591a\u6837\u5316\u7684\u5b66\u751f\u53d1\u5c55\u5efa\u6a21\u6280\u80fd\uff0c\u4e3a\u516c\u5e73\u7684\u79d1\u5b66\u8bc4\u4f30\u63d0\u4f9b\u66ff\u4ee3\u6c9f\u901a\u6a21\u5f0f\uff0c\u540c\u65f6\u89e3\u51b3\u4f20\u7edf\u8bc4\u5206\u8017\u65f6\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u7ecf\u9a8c\u8bc1\u7684NGSS\u5bf9\u9f50\u591a\u6a21\u6001\u5b66\u4e60\u8fdb\u9636\u548c\u8bc4\u4f30\u5de5\u5177\uff0c\u5e94\u7528\u673a\u5668\u5b66\u4e60\u6280\u672f\u81ea\u52a8\u8bc4\u5206\u5f00\u653e\u5f0f\u7684\u5efa\u6a21\u4efb\u52a1\u548c\u7b80\u77ed\u6587\u672c\u89e3\u91ca\u3002", "result": "\u5c55\u793a\u4e86\u5b66\u4e60\u8fdb\u9636\u5982\u4f55\u6307\u5bfc\u57fa\u4e8e\u5b66\u751f\u601d\u7ef4\u591a\u6837\u6027\u7684\u4e2a\u6027\u5316\u673a\u5668\u5b66\u4e60\u53cd\u9988\u8bbe\u8ba1\uff0c\u6709\u6548\u8bc4\u4f30\u591a\u6a21\u6001\u79d1\u5b66\u7406\u89e3\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u9a71\u52a8\u7684\u591a\u6a21\u6001\u8bc4\u4f30\u65b9\u6cd5\u80fd\u591f\u652f\u6301\u5b66\u751f\u7684\u79d1\u5b66\u5efa\u6a21\u5b9e\u8df5\uff0c\u4fc3\u8fdb\u6df1\u5ea6\u79d1\u5b66\u7406\u89e3\uff0c\u5e76\u4e3a\u6559\u80b2\u516c\u5e73\u63d0\u4f9b\u6280\u672f\u652f\u6491\u3002"}}
{"id": "2509.18319", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2509.18319", "abs": "https://arxiv.org/abs/2509.18319", "authors": ["Matthew Speers", "Jonathan Angus Tawn", "Philip Jonathan"], "title": "Sequential Design for the Efficient Estimation of Offshore Structure Failure Probability", "comment": null, "summary": "Estimation of the failure probability of offshore structures exposed to\nextreme ocean environments is critical to their safe design and operation. The\nconditional density of the environment (CDE) quantifies regions of the space of\nlong term environment responsible for extreme structural response. Moreover,\nthe probability of structural failure is obtained by simply integrating the CDE\nover the environment space. In this work, two methodologies for estimation of\nthe CDE and failure probability are considered. The first (IS-PT) combines\nparallel tempering MCMC (for CDE estimation) with important sampling (for\neventual estimation of failure probability). The second (AGE) combines adaptive\nGaussian emulation with Bayesian quadrature. We evaluate IS-PT and two variants\nof the AGE procedure in application to a simple synthetic structure with\nmultimodal CDE, and a monopile structure exhibiting non-linear resonant\nresponse. IS-PT provides reliable results for both applications for lesser\ncompute cost than naive integration. The AGE procedures require balancing\nexploration and exploitation of the environment space, using a\ntypically-unknown weight parameter, lambda. When lambda is known, perhaps from\nprior engineering knowledge, AGE provides a further reduction in computational\ncost over IS-PT. However, when unknown, IS-PT is more reliable.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u4f30\u8ba1\u6d77\u4e0a\u7ed3\u6784\u5931\u6548\u6982\u7387\u7684\u65b9\u6cd5\uff1aIS-PT\uff08\u7ed3\u5408\u5e76\u884c\u9000\u706bMCMC\u548c\u91cd\u8981\u6027\u91c7\u6837\uff09\u548cAGE\uff08\u7ed3\u5408\u81ea\u9002\u5e94\u9ad8\u65af\u4eff\u771f\u4e0e\u8d1d\u53f6\u65af\u79ef\u5206\uff09\uff0c\u5e76\u5728\u5408\u6210\u7ed3\u6784\u548c\u5355\u6869\u7ed3\u6784\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "motivation": "\u4f30\u8ba1\u6d77\u4e0a\u7ed3\u6784\u5728\u6781\u7aef\u6d77\u6d0b\u73af\u5883\u4e0b\u7684\u5931\u6548\u6982\u7387\u5bf9\u5176\u5b89\u5168\u8bbe\u8ba1\u548c\u8fd0\u884c\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u5f00\u53d1\u9ad8\u6548\u53ef\u9760\u7684\u8ba1\u7b97\u65b9\u6cd5\u3002", "method": "IS-PT\u65b9\u6cd5\u7ed3\u5408\u5e76\u884c\u9000\u706bMCMC\u8fdb\u884c\u6761\u4ef6\u5bc6\u5ea6\u4f30\u8ba1\u548c\u91cd\u8981\u6027\u91c7\u6837\u8fdb\u884c\u5931\u6548\u6982\u7387\u8ba1\u7b97\uff1bAGE\u65b9\u6cd5\u4f7f\u7528\u81ea\u9002\u5e94\u9ad8\u65af\u4eff\u771f\u548c\u8d1d\u53f6\u65af\u79ef\u5206\uff0c\u9700\u8981\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u53c2\u6570lambda\u3002", "result": "IS-PT\u5728\u4e24\u4e2a\u5e94\u7528\u6848\u4f8b\u4e2d\u90fd\u63d0\u4f9b\u4e86\u53ef\u9760\u7ed3\u679c\uff0c\u8ba1\u7b97\u6210\u672c\u4f4e\u4e8e\u6734\u7d20\u79ef\u5206\uff1b\u5f53lambda\u53c2\u6570\u5df2\u77e5\u65f6\uff0cAGE\u80fd\u8fdb\u4e00\u6b65\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u4f46\u5f53lambda\u672a\u77e5\u65f6\uff0cIS-PT\u66f4\u53ef\u9760\u3002", "conclusion": "IS-PT\u65b9\u6cd5\u5728\u8ba1\u7b97\u6210\u672c\u548c\u53ef\u9760\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u7279\u522b\u662f\u5f53AGE\u65b9\u6cd5\u7684\u5173\u952e\u53c2\u6570\u672a\u77e5\u65f6\uff0cIS-PT\u662f\u66f4\u53ef\u9760\u7684\u9009\u62e9\u3002"}}
{"id": "2509.18121", "categories": ["cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18121", "abs": "https://arxiv.org/abs/2509.18121", "authors": ["Nikhil Garg", "Paul Uriarte Vicandi", "Yanming Zhang", "Alexandre Baigol", "Donato Francesco Falcone", "Saketh Ram Mamidala", "Bert Jan Offrein", "Laura B\u00e9gon-Lours"], "title": "Energy-convergence trade off for the training of neural networks on bio-inspired hardware", "comment": null, "summary": "The increasing deployment of wearable sensors and implantable devices is\nshifting AI processing demands to the extreme edge, necessitating ultra-low\npower for continuous operation. Inspired by the brain, emerging memristive\ndevices promise to accelerate neural network training by eliminating costly\ndata transfers between compute and memory. Though, balancing performance and\nenergy efficiency remains a challenge. We investigate ferroelectric synaptic\ndevices based on HfO2/ZrO2 superlattices and feed their experimentally measured\nweight updates into hardware-aware neural network simulations. Across pulse\nwidths from 20 ns to 0.2 ms, shorter pulses lower per-update energy but require\nmore training epochs while still reducing total energy without sacrificing\naccuracy. Classification accuracy using plain stochastic gradient descent (SGD)\nis diminished compared to mixed-precision SGD. We analyze the causes and\npropose a ``symmetry point shifting'' technique, addressing asymmetric updates\nand restoring accuracy. These results highlight a trade-off among accuracy,\nconvergence speed, and energy use, showing that short-pulse programming with\ntailored training significantly enhances on-chip learning efficiency.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8eHfO2/ZrO2\u8d85\u6676\u683c\u7684\u94c1\u7535\u7a81\u89e6\u5668\u4ef6\uff0c\u901a\u8fc7\u786c\u4ef6\u611f\u77e5\u795e\u7ecf\u7f51\u7edc\u6a21\u62df\u5206\u6790\u5176\u6027\u80fd\u3002\u7814\u7a76\u53d1\u73b0\u77ed\u8109\u51b2\u7f16\u7a0b\u7ed3\u5408\u5b9a\u5236\u5316\u8bad\u7ec3\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u7247\u4e0a\u5b66\u4e60\u6548\u7387\uff0c\u5728\u51c6\u786e\u7387\u3001\u6536\u655b\u901f\u5ea6\u548c\u80fd\u8017\u4e4b\u95f4\u5b9e\u73b0\u4f18\u5316\u5e73\u8861\u3002", "motivation": "\u968f\u7740\u53ef\u7a7f\u6234\u4f20\u611f\u5668\u548c\u690d\u5165\u5f0f\u8bbe\u5907\u7684\u90e8\u7f72\u589e\u52a0\uff0cAI\u5904\u7406\u9700\u6c42\u5411\u6781\u7aef\u8fb9\u7f18\u8f6c\u79fb\uff0c\u9700\u8981\u8d85\u4f4e\u529f\u8017\u7684\u8fde\u7eed\u64cd\u4f5c\u3002\u53d7\u5927\u8111\u542f\u53d1\uff0c\u65b0\u5174\u7684\u5fc6\u963b\u5668\u4ef6\u6709\u671b\u901a\u8fc7\u6d88\u9664\u8ba1\u7b97\u548c\u5185\u5b58\u4e4b\u95f4\u7684\u6602\u8d35\u6570\u636e\u4f20\u8f93\u6765\u52a0\u901f\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\uff0c\u4f46\u5e73\u8861\u6027\u80fd\u548c\u80fd\u6548\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u7814\u7a76\u94c1\u7535\u7a81\u89e6\u5668\u4ef6\u57fa\u4e8eHfO2/ZrO2\u8d85\u6676\u683c\uff0c\u5c06\u5b9e\u9a8c\u6d4b\u91cf\u7684\u6743\u91cd\u66f4\u65b0\u8f93\u5165\u786c\u4ef6\u611f\u77e5\u795e\u7ecf\u7f51\u7edc\u6a21\u62df\u3002\u5206\u6790\u4e0d\u540c\u8109\u51b2\u5bbd\u5ea6\uff0820 ns\u81f30.2 ms\uff09\u4e0b\u7684\u6027\u80fd\uff0c\u6bd4\u8f83\u666e\u901a\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08SGD\uff09\u548c\u6df7\u5408\u7cbe\u5ea6SGD\uff0c\u5e76\u63d0\u51fa\"\u5bf9\u79f0\u70b9\u504f\u79fb\"\u6280\u672f\u6765\u89e3\u51b3\u4e0d\u5bf9\u79f0\u66f4\u65b0\u95ee\u9898\u3002", "result": "\u77ed\u8109\u51b2\u964d\u4f4e\u4e86\u6bcf\u6b21\u66f4\u65b0\u7684\u80fd\u91cf\u4f46\u9700\u8981\u66f4\u591a\u8bad\u7ec3\u5468\u671f\uff0c\u603b\u4f53\u4e0a\u4ecd\u80fd\u51cf\u5c11\u603b\u80fd\u91cf\u800c\u4e0d\u727a\u7272\u51c6\u786e\u7387\u3002\u666e\u901aSGD\u7684\u5206\u7c7b\u51c6\u786e\u7387\u4f4e\u4e8e\u6df7\u5408\u7cbe\u5ea6SGD\u3002\u901a\u8fc7\u5bf9\u79f0\u70b9\u504f\u79fb\u6280\u672f\u6210\u529f\u6062\u590d\u4e86\u51c6\u786e\u7387\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u7a81\u51fa\u4e86\u51c6\u786e\u7387\u3001\u6536\u655b\u901f\u5ea6\u548c\u80fd\u8017\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\uff0c\u8868\u660e\u77ed\u8109\u51b2\u7f16\u7a0b\u7ed3\u5408\u5b9a\u5236\u5316\u8bad\u7ec3\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u7247\u4e0a\u5b66\u4e60\u6548\u7387\uff0c\u4e3a\u8d85\u4f4e\u529f\u8017\u8fb9\u7f18AI\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18282", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18282", "abs": "https://arxiv.org/abs/2509.18282", "authors": ["Jesse Zhang", "Marius Memmel", "Kevin Kim", "Dieter Fox", "Jesse Thomason", "Fabio Ramos", "Erdem B\u0131y\u0131k", "Abhishek Gupta", "Anqi Li"], "title": "PEEK: Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies", "comment": "11 pages", "summary": "Robotic manipulation policies often fail to generalize because they must\nsimultaneously learn where to attend, what actions to take, and how to execute\nthem. We argue that high-level reasoning about where and what can be offloaded\nto vision-language models (VLMs), leaving policies to specialize in how to act.\nWe present PEEK (Policy-agnostic Extraction of Essential Keypoints), which\nfine-tunes VLMs to predict a unified point-based intermediate representation:\n1. end-effector paths specifying what actions to take, and 2. task-relevant\nmasks indicating where to focus. These annotations are directly overlaid onto\nrobot observations, making the representation policy-agnostic and transferable\nacross architectures. To enable scalable training, we introduce an automatic\nannotation pipeline, generating labeled data across 20+ robot datasets spanning\n9 embodiments. In real-world evaluations, PEEK consistently boosts zero-shot\ngeneralization, including a 41.4x real-world improvement for a 3D policy\ntrained only in simulation, and 2-3.5x gains for both large VLAs and small\nmanipulation policies. By letting VLMs absorb semantic and visual complexity,\nPEEK equips manipulation policies with the minimal cues they need--where, what,\nand how. Website at https://peek-robot.github.io/.", "AI": {"tldr": "PEEK\u901a\u8fc7\u5fae\u8c03\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7edf\u4e00\u7684\u70b9\u57fa\u4e2d\u95f4\u8868\u793a\uff0c\u5c06\u9ad8\u5c42\u63a8\u7406\u4e0e\u5e95\u5c42\u52a8\u4f5c\u6267\u884c\u5206\u79bb\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u56e0\u4e3a\u9700\u8981\u540c\u65f6\u5b66\u4e60\u5173\u6ce8\u4f4d\u7f6e\u3001\u52a8\u4f5c\u7c7b\u578b\u548c\u6267\u884c\u65b9\u5f0f\u3002PEEK\u65e8\u5728\u5c06\u9ad8\u5c42\u63a8\u7406\u4efb\u52a1\u5378\u8f7d\u7ed9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u8ba9\u7b56\u7565\u4e13\u6ce8\u4e8e\u52a8\u4f5c\u6267\u884c\u3002", "method": "1. \u5fae\u8c03VLMs\u9884\u6d4b\u7edf\u4e00\u7684\u70b9\u57fa\u4e2d\u95f4\u8868\u793a\uff1a\u672b\u7aef\u6267\u884c\u5668\u8def\u5f84\u548c\u4efb\u52a1\u76f8\u5173\u63a9\u7801\uff1b2. \u5f00\u53d1\u81ea\u52a8\u6807\u6ce8\u6d41\u6c34\u7ebf\uff0c\u572820+\u673a\u5668\u4eba\u6570\u636e\u96c6\u4e0a\u751f\u6210\u6807\u6ce8\u6570\u636e\uff1b3. \u5c06\u6807\u6ce8\u76f4\u63a5\u53e0\u52a0\u5230\u673a\u5668\u4eba\u89c2\u5bdf\u4e0a\uff0c\u5b9e\u73b0\u7b56\u7565\u65e0\u5173\u7684\u8868\u793a\u3002", "result": "1. \u4ec5\u5728\u4eff\u771f\u8bad\u7ec3\u76843D\u7b56\u7565\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u63d0\u534741.4\u500d\uff1b2. \u5927\u578bVLA\u548c\u5c0f\u578b\u64cd\u4f5c\u7b56\u7565\u5206\u522b\u83b7\u5f972-3.5\u500d\u6027\u80fd\u63d0\u5347\uff1b3. \u57289\u79cd\u4e0d\u540c\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u8ba9VLMs\u5904\u7406\u8bed\u4e49\u548c\u89c6\u89c9\u590d\u6742\u6027\uff0cPEEK\u4e3a\u64cd\u4f5c\u7b56\u7565\u63d0\u4f9b\u4e86\u6240\u9700\u7684\u6700\u5c0f\u63d0\u793a\uff08\u4f4d\u7f6e\u3001\u5185\u5bb9\u548c\u65b9\u5f0f\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.18100", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18100", "abs": "https://arxiv.org/abs/2509.18100", "authors": ["Shishir Lamichhane", "Anamika Dubey"], "title": "Stochastic Economic Dispatch with Battery Energy Storage considering Wind and Load Uncertainty", "comment": "to be published in NAPS", "summary": "With the integration of renewable energy resources in power systems, managing\noperational flexibility and reliability while minimizing operational costs has\nbecome increasingly challenging. Battery energy storage system (BESS) offers a\npromising solution to address these issues. This paper presents a stochastic\ndynamic economic dispatch with storage (SDED-S) framework to assess the impact\nof BESS in managing uncertainty. The temporal correlation between wind and load\nuncertainties is captured, with scenarios generated using a method inspired by\nstratified and importance sampling. The proposed approach is demonstrated on a\nmodified IEEE 39-bus system, where selected conventional generators are\nconverted to wind power plants. Case studies show that strategic BESS\ndeployment significantly improves system flexibility by reducing renewable\ncurtailments and dispatch costs. Renewable energy curtailments decrease upon\nincreasing BESS size and approach zero depending on wind penetration level.\nHigher wind penetrations result in greater curtailments without storage and\nyield larger cost savings when BESS is deployed, highlighting the growing need\nfor flexibility as renewable energy penetrations increase.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u968f\u673a\u52a8\u6001\u7ecf\u6d4e\u8c03\u5ea6\u4e0e\u50a8\u80fd\uff08SDED-S\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u7535\u6c60\u50a8\u80fd\u7cfb\u7edf\uff08BESS\uff09\u5728\u7ba1\u7406\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\u7684\u4f5c\u7528\uff0c\u901a\u8fc7\u8003\u8651\u98ce\u80fd\u548c\u8d1f\u8377\u4e0d\u786e\u5b9a\u6027\u7684\u65f6\u95f4\u76f8\u5173\u6027\uff0c\u5728\u6539\u8fdb\u7684IEEE 39\u603b\u7ebf\u7cfb\u7edf\u4e0a\u9a8c\u8bc1\u4e86BESS\u90e8\u7f72\u5bf9\u51cf\u5c11\u53ef\u518d\u751f\u80fd\u6e90\u5f03\u7535\u548c\u8c03\u5ea6\u6210\u672c\u7684\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u53ef\u518d\u751f\u80fd\u6e90\u5728\u7535\u529b\u7cfb\u7edf\u4e2d\u7684\u96c6\u6210\uff0c\u5982\u4f55\u5728\u6700\u5c0f\u5316\u8fd0\u8425\u6210\u672c\u7684\u540c\u65f6\u7ba1\u7406\u8fd0\u884c\u7075\u6d3b\u6027\u548c\u53ef\u9760\u6027\u53d8\u5f97\u65e5\u76ca\u56f0\u96be\u3002\u7535\u6c60\u50a8\u80fd\u7cfb\u7edf\uff08BESS\uff09\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5206\u5c42\u548c\u91cd\u8981\u6027\u91c7\u6837\u7684\u65b9\u6cd5\u751f\u6210\u573a\u666f\uff0c\u6355\u6349\u98ce\u80fd\u548c\u8d1f\u8377\u4e0d\u786e\u5b9a\u6027\u7684\u65f6\u95f4\u76f8\u5173\u6027\uff0c\u5e76\u5728\u6539\u8fdb\u7684IEEE 39\u603b\u7ebf\u7cfb\u7edf\u4e0a\u8fdb\u884c\u6848\u4f8b\u7814\u7a76\uff0c\u5176\u4e2d\u90e8\u5206\u5e38\u89c4\u53d1\u7535\u673a\u88ab\u8f6c\u6362\u4e3a\u98ce\u7535\u573a\u3002", "result": "\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u6218\u7565\u6027BESS\u90e8\u7f72\u663e\u8457\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7075\u6d3b\u6027\uff0c\u51cf\u5c11\u4e86\u53ef\u518d\u751f\u80fd\u6e90\u5f03\u7535\u548c\u8c03\u5ea6\u6210\u672c\u3002BESS\u89c4\u6a21\u589e\u5927\u65f6\u5f03\u7535\u51cf\u5c11\uff0c\u4e14\u5728\u9ad8\u98ce\u7535\u6e17\u900f\u7387\u4e0b\u6210\u672c\u8282\u7ea6\u66f4\u660e\u663e\u3002", "conclusion": "\u968f\u7740\u53ef\u518d\u751f\u80fd\u6e90\u6e17\u900f\u7387\u7684\u589e\u52a0\uff0c\u5bf9\u7075\u6d3b\u6027\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0cBESS\u90e8\u7f72\u5728\u9ad8\u98ce\u7535\u6e17\u900f\u7387\u4e0b\u80fd\u6709\u6548\u51cf\u5c11\u5f03\u7535\u5e76\u5e26\u6765\u66f4\u5927\u7684\u6210\u672c\u8282\u7ea6\u3002"}}
{"id": "2509.18289", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2509.18289", "abs": "https://arxiv.org/abs/2509.18289", "authors": ["Akrati Saxena", "Gaurav Kumar", "Chandrakala Meena"], "title": "Homophily in Complex Networks: Measures, Models, and Applications", "comment": null, "summary": "Homophily, the tendency of individuals to connect with others who share\nsimilar attributes, is a defining feature of social networks. Understanding how\ngroups interact, both within and across, is crucial for uncovering the dynamics\nof network evolution and the emergence of structural inequalities in these\nnetwork. This tutorial offers a comprehensive overview of homophily, covering\nits various definitions, key properties, and the limitations of widely used\nmetrics. Extending beyond traditional pairwise interactions, we will discuss\nhomophily in higher-order network structures such as hypergraphs and simplicial\ncomplexes. We will further discuss network generating models capable of\nproducing different types of homophilic networks with tunable levels of\nhomophily and highlight their relevance in real-world contexts. The tutorial\nconcludes with a discussion of open challenges, emerging directions, and\nopportunities for further research in this area.", "AI": {"tldr": "\u672c\u6559\u7a0b\u5168\u9762\u6982\u8ff0\u4e86\u540c\u8d28\u6027\uff08homophily\uff09\u73b0\u8c61\uff0c\u6db5\u76d6\u5176\u5b9a\u4e49\u3001\u6027\u8d28\u3001\u5ea6\u91cf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4ee5\u53ca\u5728\u8d85\u56fe\u548c\u9ad8\u9636\u7f51\u7edc\u7ed3\u6784\u4e2d\u7684\u5e94\u7528\uff0c\u540c\u65f6\u4ecb\u7ecd\u4e86\u53ef\u8c03\u8282\u540c\u8d28\u6027\u6c34\u5e73\u7684\u7f51\u7edc\u751f\u6210\u6a21\u578b\u548c\u73b0\u5b9e\u5e94\u7528\u3002", "motivation": "\u7406\u89e3\u540c\u8d28\u6027\u5bf9\u4e8e\u63ed\u793a\u793e\u4f1a\u7f51\u7edc\u6f14\u5316\u52a8\u6001\u548c\u7ed3\u6784\u4e0d\u5e73\u7b49\u73b0\u8c61\u7684\u51fa\u73b0\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u540c\u8d28\u6027\u662f\u793e\u4ea4\u7f51\u7edc\u7684\u4e00\u4e2a\u51b3\u5b9a\u6027\u7279\u5f81\u3002", "method": "\u6559\u7a0b\u91c7\u7528\u7efc\u8ff0\u65b9\u6cd5\uff0c\u7cfb\u7edf\u68b3\u7406\u540c\u8d28\u6027\u7684\u5404\u79cd\u5b9a\u4e49\u3001\u5173\u952e\u5c5e\u6027\u548c\u5e38\u7528\u5ea6\u91cf\u7684\u5c40\u9650\u6027\uff0c\u5e76\u6269\u5c55\u5230\u9ad8\u9636\u7f51\u7edc\u7ed3\u6784\uff08\u5982\u8d85\u56fe\u548c\u5355\u7eaf\u590d\u5f62\uff09\u4e2d\u7684\u540c\u8d28\u6027\u5206\u6790\uff0c\u540c\u65f6\u4ecb\u7ecd\u80fd\u591f\u4ea7\u751f\u4e0d\u540c\u7c7b\u578b\u540c\u8d28\u6027\u7f51\u7edc\u7684\u751f\u6210\u6a21\u578b\u3002", "result": "\u63d0\u4f9b\u4e86\u540c\u8d28\u6027\u7814\u7a76\u7684\u5168\u9762\u6846\u67b6\uff0c\u5305\u62ec\u4f20\u7edf\u6210\u5bf9\u4ea4\u4e92\u548c\u9ad8\u9636\u7ed3\u6784\u4e2d\u7684\u540c\u8d28\u6027\u5206\u6790\uff0c\u4ee5\u53ca\u53ef\u8c03\u8282\u540c\u8d28\u6027\u6c34\u5e73\u7684\u7f51\u7edc\u6a21\u578b\uff0c\u5f3a\u8c03\u4e86\u5728\u73b0\u5b9e\u4e16\u754c\u80cc\u666f\u4e2d\u7684\u76f8\u5173\u6027\u3002", "conclusion": "\u6559\u7a0b\u6700\u540e\u8ba8\u8bba\u4e86\u8be5\u9886\u57df\u7684\u5f00\u653e\u6311\u6218\u3001\u65b0\u5174\u65b9\u5411\u548c\u672a\u6765\u7814\u7a76\u673a\u4f1a\uff0c\u4e3a\u540c\u8d28\u6027\u7814\u7a76\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2509.18101", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18101", "abs": "https://arxiv.org/abs/2509.18101", "authors": ["Guanzhong Pan", "Haibo Wang"], "title": "A Cost-Benefit Analysis of On-Premise Large Language Model Deployment: Breaking Even with Commercial LLM Services", "comment": null, "summary": "Large language models (LLMs) are becoming increasingly widespread.\nOrganizations that want to use AI for productivity now face an important\ndecision. They can subscribe to commercial LLM services or deploy models on\ntheir own infrastructure. Cloud services from providers such as OpenAI,\nAnthropic, and Google are attractive because they provide easy access to\nstate-of-the-art models and are easy to scale. However, concerns about data\nprivacy, the difficulty of switching service providers, and long-term operating\ncosts have driven interest in local deployment of open-source models. This\npaper presents a cost-benefit analysis framework to help organizations\ndetermine when on-premise LLM deployment becomes economically viable compared\nto commercial subscription services. We consider the hardware requirements,\noperational expenses, and performance benchmarks of the latest open-source\nmodels, including Qwen, Llama, Mistral, and etc. Then we compare the total cost\nof deploying these models locally with the major cloud providers subscription\nfee. Our findings provide an estimated breakeven point based on usage levels\nand performance needs. These results give organizations a practical framework\nfor planning their LLM strategies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6210\u672c\u6548\u76ca\u5206\u6790\u6846\u67b6\uff0c\u5e2e\u52a9\u7ec4\u7ec7\u8bc4\u4f30\u672c\u5730\u90e8\u7f72\u5f00\u6e90LLM\u4e0e\u5546\u4e1a\u8ba2\u9605\u670d\u52a1\u7684\u7ecf\u6d4e\u53ef\u884c\u6027", "motivation": "\u7ec4\u7ec7\u5728\u4f7f\u7528AI\u65f6\u9762\u4e34\u9009\u62e9\u5546\u4e1aLLM\u670d\u52a1\u8fd8\u662f\u672c\u5730\u90e8\u7f72\u7684\u51b3\u7b56\u96be\u9898\uff0c\u9700\u8981\u8003\u8651\u6570\u636e\u9690\u79c1\u3001\u4f9b\u5e94\u5546\u9501\u5b9a\u548c\u957f\u671f\u8fd0\u8425\u6210\u672c\u7b49\u56e0\u7d20", "method": "\u901a\u8fc7\u5206\u6790\u786c\u4ef6\u9700\u6c42\u3001\u8fd0\u8425\u8d39\u7528\u548c\u5f00\u6e90\u6a21\u578b\u6027\u80fd\u57fa\u51c6\uff0c\u5c06\u672c\u5730\u90e8\u7f72\u603b\u6210\u672c\u4e0e\u4e3b\u8981\u4e91\u670d\u52a1\u5546\u7684\u8ba2\u9605\u8d39\u7528\u8fdb\u884c\u6bd4\u8f83", "result": "\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u4e8e\u4f7f\u7528\u6c34\u5e73\u548c\u6027\u80fd\u9700\u6c42\u7684\u76c8\u4e8f\u5e73\u8861\u70b9\u4f30\u8ba1", "conclusion": "\u7ed3\u679c\u4e3a\u7ec4\u7ec7\u5236\u5b9aLLM\u6218\u7565\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u51b3\u7b56\u6846\u67b6"}}
{"id": "2509.19019", "categories": ["econ.TH"], "pdf": "https://arxiv.org/pdf/2509.19019", "abs": "https://arxiv.org/abs/2509.19019", "authors": ["Leandro Lyra Braga Dognini"], "title": "Existence and Calculation of Optimal Equilibria on Overlapping Generations Economies", "comment": null, "summary": "A well-known feature of overlapping generations economies is that the First\nWelfare Theorem fails and equilibrium may be inefficient. The Cass (1972)\ncriterion furnishes a necessary and sufficient condition for efficiency, but\ndoes not address the matter of existence of efficient equilibria, and Cass,\nOkuno, and Zilcha (1979) provide nonexistence examples. I develop an algorithm\nbased on successive approximations of a nonstationary, consumption-loan, prone\nto savings, overlapping generations economy with finite-lived heterogeneous\nagents to find elements of its set of equilibria as the limit of nested compact\nsets. These compact sets are the result of a backward calculation through\nequilibrium equations that departs from the set of Pareto optimal equilibria of\nwell-behaved tail economies. The equilibria calculated through this algorithm\nsatisfy the Cass (1972) criterion and are used to derive the existence results\non efficient equilibria.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u7b97\u6cd5\uff0c\u901a\u8fc7\u5d4c\u5957\u7d27\u96c6\u903c\u8fd1\u975e\u5e73\u7a33\u6d88\u8d39\u8d37\u6b3e\u91cd\u53e0\u4e16\u4ee3\u7ecf\u6d4e\u4e2d\u7684\u5747\u8861\uff0c\u8bc1\u660e\u4e86\u6709\u6548\u5747\u8861\u7684\u5b58\u5728\u6027\u3002", "motivation": "\u91cd\u53e0\u4e16\u4ee3\u7ecf\u6d4e\u4e2d\u7b2c\u4e00\u798f\u5229\u5b9a\u7406\u5931\u6548\uff0c\u5747\u8861\u53ef\u80fd\u65e0\u6548\u3002Cass\u51c6\u5219\u63d0\u4f9b\u4e86\u6548\u7387\u7684\u5fc5\u8981\u5145\u5206\u6761\u4ef6\uff0c\u4f46\u672a\u89e3\u51b3\u6709\u6548\u5747\u8861\u5b58\u5728\u6027\u95ee\u9898\uff0c\u4e14\u5b58\u5728\u4e0d\u5b58\u5728\u6709\u6548\u5747\u8861\u7684\u4f8b\u5b50\u3002", "method": "\u57fa\u4e8e\u6709\u9650\u5bff\u547d\u5f02\u8d28\u4ee3\u7406\u4eba\u7684\u975e\u5e73\u7a33\u6d88\u8d39\u8d37\u6b3e\u7ecf\u6d4e\u7684\u8fde\u7eed\u903c\u8fd1\u7b97\u6cd5\uff0c\u901a\u8fc7\u5747\u8861\u65b9\u7a0b\u7684\u540e\u5411\u8ba1\u7b97\uff0c\u4ece\u8868\u73b0\u826f\u597d\u7684\u5c3e\u90e8\u7ecf\u6d4e\u7684\u5e15\u7d2f\u6258\u6700\u4f18\u5747\u8861\u51fa\u53d1\uff0c\u627e\u5230\u5747\u8861\u96c6\u7684\u5143\u7d20\u4f5c\u4e3a\u5d4c\u5957\u7d27\u96c6\u7684\u6781\u9650\u3002", "result": "\u901a\u8fc7\u8be5\u7b97\u6cd5\u8ba1\u7b97\u7684\u5747\u8861\u6ee1\u8db3Cass\u51c6\u5219\uff0c\u5e76\u7528\u4e8e\u63a8\u5bfc\u6709\u6548\u5747\u8861\u7684\u5b58\u5728\u6027\u7ed3\u679c\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u4e3a\u91cd\u53e0\u4e16\u4ee3\u7ecf\u6d4e\u4e2d\u6709\u6548\u5747\u8861\u7684\u5b58\u5728\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u89e3\u51b3\u4e86Cass\u7b49\u4eba\u63d0\u51fa\u7684\u4e0d\u5b58\u5728\u6027\u95ee\u9898\u3002"}}
{"id": "2509.18887", "categories": ["econ.EM", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.18887", "abs": "https://arxiv.org/abs/2509.18887", "authors": ["Yousef Adeli Sadabad", "Mohammad Reza Hesamzadeh", "Gyorgy Dan", "Matin Bagherpour", "Darryl R. Biggar"], "title": "Driver Identification and PCA Augmented Selection Shrinkage Framework for Nordic System Price Forecasting", "comment": null, "summary": "The System Price (SP) of the Nordic electricity market serves as a key\nreference for financial hedge contracts such as Electricity Price Area\nDifferentials (EPADs) and other risk management instruments. Therefore, the\nidentification of drivers and the accurate forecasting of SP are essential for\nmarket participants to design effective hedging strategies. This paper develops\na systematic framework that combines interpretable drivers analysis with robust\nforecasting methods. It proposes an interpretable feature engineering algorithm\nto identify the main drivers of the Nordic SP based on a novel combination of\nK-means clustering, Multiple Seasonal-Trend Decomposition (MSTD), and Seasonal\nAutoregressive Integrated Moving Average (SARIMA) model. Then, it applies\nprincipal component analysis (PCA) to the identified data matrix, which is\nadapted to the downstream task of price forecasting to mitigate the issue of\nimperfect multicollinearity in the data. Finally, we propose a multi-forecast\nselection-shrinkage algorithm for Nordic SP forecasting, which selects a subset\nof complementary forecast models based on their bias-variance tradeoff at the\nensemble level and then computes the optimal weights for the retained forecast\nmodels to minimize the error variance of the combined forecast. Using\nhistorical data from the Nordic electricity market, we demonstrate that the\nproposed approach outperforms individual input models uniformly, robustly, and\nsignificantly, while maintaining a comparable computational cost. Notably, our\nsystematic framework produces superior results using simple input models,\noutperforming the state-of-the-art Temporal Fusion Transformer (TFT).\nFurthermore, we show that our approach also exceeds the performance of several\nwell-established practical forecast combination methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u53ef\u89e3\u91ca\u9a71\u52a8\u56e0\u7d20\u5206\u6790\u548c\u7a33\u5065\u9884\u6d4b\u65b9\u6cd5\u7684\u7cfb\u7edf\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u5317\u6b27\u7535\u529b\u5e02\u573a\u7684\u7cfb\u7edf\u4ef7\u683c\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u7279\u5f81\u5de5\u7a0b\u8bc6\u522b\u4e3b\u8981\u9a71\u52a8\u56e0\u7d20\uff0c\u5e94\u7528PCA\u5904\u7406\u591a\u91cd\u5171\u7ebf\u6027\uff0c\u5e76\u63d0\u51fa\u591a\u9884\u6d4b\u9009\u62e9-\u6536\u7f29\u7b97\u6cd5\u6765\u4f18\u5316\u9884\u6d4b\u7ec4\u5408\u3002", "motivation": "\u5317\u6b27\u7535\u529b\u5e02\u573a\u7684\u7cfb\u7edf\u4ef7\u683c\u662f\u91d1\u878d\u5bf9\u51b2\u5408\u7ea6\u7684\u5173\u952e\u53c2\u8003\uff0c\u51c6\u786e\u9884\u6d4b\u7cfb\u7edf\u4ef7\u683c\u5bf9\u5e02\u573a\u53c2\u4e0e\u8005\u8bbe\u8ba1\u6709\u6548\u5bf9\u51b2\u7b56\u7565\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u53ef\u89e3\u91ca\u7279\u5f81\u5de5\u7a0b\u7b97\u6cd5\uff08\u7ed3\u5408K-means\u805a\u7c7b\u3001MSTD\u5206\u89e3\u548cSARIMA\u6a21\u578b\uff09\u8bc6\u522b\u9a71\u52a8\u56e0\u7d20\uff0c\u5e94\u7528PCA\u5904\u7406\u6570\u636e\uff0c\u5f00\u53d1\u591a\u9884\u6d4b\u9009\u62e9-\u6536\u7f29\u7b97\u6cd5\u8fdb\u884c\u4ef7\u683c\u9884\u6d4b\u3002", "result": "\u4f7f\u7528\u5317\u6b27\u7535\u529b\u5e02\u573a\u5386\u53f2\u6570\u636e\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u53ef\u6bd4\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\uff0c\u663e\u8457\u4f18\u4e8e\u5355\u4e2a\u8f93\u5165\u6a21\u578b\u548c\u6700\u5148\u8fdb\u7684TFT\u65b9\u6cd5\uff0c\u4e5f\u8d85\u8d8a\u4e86\u591a\u4e2a\u6210\u719f\u7684\u9884\u6d4b\u7ec4\u5408\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u6846\u67b6\u80fd\u591f\u4ea7\u751f\u4f18\u8d8a\u7684\u9884\u6d4b\u7ed3\u679c\uff0c\u4f7f\u7528\u7b80\u5355\u7684\u8f93\u5165\u6a21\u578b\u5373\u53ef\u8d85\u8d8a\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\uff0c\u4e3a\u7535\u529b\u5e02\u573a\u4ef7\u683c\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18194", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2509.18194", "abs": "https://arxiv.org/abs/2509.18194", "authors": ["James Brusseau"], "title": "Deleuze's \"Postscript on the Societies of Control\" Updated for Big Data and Predictive Analytics", "comment": null, "summary": "In 1990, Gilles Deleuze published Postscript on the Societies of Control, an\nintroduction to the potentially suffocating reality of the nascent control\nsociety. This thirty-year update details how Deleuze's conception has developed\nfrom a broad speculative vision into specific economic mechanisms clustering\naround personal information, big data, predictive analytics, and marketing. The\ncentral claim is that today's advancing control society coerces without\nprohibitions, and through incentives that are not grim but enjoyable, even\neuphoric because they compel individuals to obey their own personal\ninformation. The article concludes by delineating two strategies for living\nthat are as unexplored as control society itself because they are revealed and\nthen enabled by the particular method of oppression that is control.", "AI": {"tldr": "\u672c\u6587\u662f\u5bf9\u5fb7\u52d2\u5179\u300a\u63a7\u5236\u793e\u4f1a\u540e\u8bb0\u300b\u768430\u5e74\u66f4\u65b0\uff0c\u5206\u6790\u4e86\u63a7\u5236\u793e\u4f1a\u5982\u4f55\u4ece\u5e7f\u6cdb\u63a8\u6d4b\u53d1\u5c55\u4e3a\u56f4\u7ed5\u4e2a\u4eba\u4fe1\u606f\u3001\u5927\u6570\u636e\u3001\u9884\u6d4b\u5206\u6790\u548c\u8425\u9500\u7684\u5177\u4f53\u7ecf\u6d4e\u673a\u5236\u3002", "motivation": "\u63a2\u8ba8\u5fb7\u52d2\u5179\u63a7\u5236\u793e\u4f1a\u6982\u5ff5\u572830\u5e74\u540e\u7684\u53d1\u5c55\u53d8\u5316\uff0c\u7279\u522b\u662f\u63a7\u5236\u673a\u5236\u5982\u4f55\u901a\u8fc7\u6109\u60a6\u7684\u6fc0\u52b1\u800c\u975e\u7981\u6b62\u6765\u5b9e\u73b0\u5f3a\u5236\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5f53\u4ee3\u7ecf\u6d4e\u673a\u5236\u7814\u7a76\uff0c\u8be6\u7ec6\u63cf\u8ff0\u63a7\u5236\u793e\u4f1a\u7684\u5177\u4f53\u8fd0\u4f5c\u65b9\u5f0f\u3002", "result": "\u53d1\u73b0\u73b0\u4ee3\u63a7\u5236\u793e\u4f1a\u901a\u8fc7\u4e2a\u4eba\u4fe1\u606f\u9a71\u52a8\u7684\u6109\u60a6\u6fc0\u52b1\u5b9e\u73b0\u5f3a\u5236\uff0c\u800c\u975e\u4f20\u7edf\u7684\u7981\u6b62\u624b\u6bb5\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e24\u79cd\u4e0e\u63a7\u5236\u7cfb\u7edf\u672c\u8eab\u540c\u6837\u672a\u88ab\u63a2\u7d22\u7684\u751f\u6d3b\u7b56\u7565\uff0c\u8fd9\u4e9b\u7b56\u7565\u7531\u7279\u5b9a\u7684\u538b\u8feb\u65b9\u6cd5\u6240\u63ed\u793a\u548c\u4fc3\u6210\u3002"}}
{"id": "2509.18414", "categories": ["stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.18414", "abs": "https://arxiv.org/abs/2509.18414", "authors": ["Rohit Dube", "Natarajan Gautam", "Amarnath Banerjee", "Harsha Nagarajan"], "title": "Hierarchical Semi-Markov Models with Duration-Aware Dynamics for Activity Sequences", "comment": null, "summary": "Residential electricity demand at granular scales is driven by what people do\nand for how long. Accurately forecasting this demand for applications like\nmicrogrid management and demand response therefore requires generative models\nthat can produce realistic daily activity sequences, capturing both the timing\nand duration of human behavior. This paper develops a generative model of human\nactivity sequences using nationally representative time-use diaries at a\n10-minute resolution. We use this model to quantify which demographic factors\nare most critical for improving predictive performance.\n  We propose a hierarchical semi-Markov framework that addresses two key\nmodeling challenges. First, a time-inhomogeneous Markov \\emph{router} learns\nthe patterns of ``which activity comes next.\" Second, a semi-Markov\n\\emph{hazard} component explicitly models activity durations, capturing ``how\nlong\" activities realistically last. To ensure statistical stability when data\nare sparse, the model pools information across related demographic groups and\ntime blocks. The entire framework is trained and evaluated using survey design\nweights to ensure our findings are representative of the U.S. population.\n  On a held-out test set, we demonstrate that explicitly modeling durations\nwith the hazard component provides a substantial and statistically significant\nimprovement over purely Markovian models. Furthermore, our analysis reveals a\nclear hierarchy of demographic factors: Sex, Day-Type, and Household Size\nprovide the largest predictive gains, while Region and Season, though important\nfor energy calculations, contribute little to predicting the activity sequence\nitself. The result is an interpretable and robust generator of synthetic\nactivity traces, providing a high-fidelity foundation for downstream energy\nsystems modeling.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u5206\u5c42\u534a\u9a6c\u5c14\u53ef\u592b\u6846\u67b6\u6765\u751f\u6210\u4eba\u7c7b\u6d3b\u52a8\u5e8f\u5217\uff0c\u7528\u4e8e\u9884\u6d4b\u4f4f\u5b85\u7535\u529b\u9700\u6c42\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u9a6c\u5c14\u53ef\u592b\u8def\u7531\u5668\u548c\u534a\u9a6c\u5c14\u53ef\u592b\u98ce\u9669\u7ec4\u4ef6\u5206\u522b\u5efa\u6a21\u6d3b\u52a8\u987a\u5e8f\u548c\u6301\u7eed\u65f6\u95f4\uff0c\u5728\u6d4b\u8bd5\u96c6\u4e0a\u663e\u793a\u51fa\u6bd4\u7eaf\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\u663e\u8457\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u51c6\u786e\u9884\u6d4b\u4f4f\u5b85\u7535\u529b\u9700\u6c42\u9700\u8981\u80fd\u591f\u751f\u6210\u771f\u5b9e\u65e5\u5e38\u6d3b\u52a8\u5e8f\u5217\u7684\u751f\u6210\u6a21\u578b\uff0c\u6355\u6349\u4eba\u7c7b\u884c\u4e3a\u7684\u65f6\u95f4\u548c\u6301\u7eed\u65f6\u95f4\u6a21\u5f0f\uff0c\u4ee5\u652f\u6301\u5fae\u7535\u7f51\u7ba1\u7406\u548c\u9700\u6c42\u54cd\u5e94\u7b49\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u534a\u9a6c\u5c14\u53ef\u592b\u6846\u67b6\uff1a1\uff09\u65f6\u95f4\u975e\u9f50\u6b21\u9a6c\u5c14\u53ef\u592b\u8def\u7531\u5668\u5b66\u4e60\"\u4e0b\u4e00\u4e2a\u6d3b\u52a8\u662f\u4ec0\u4e48\"\u7684\u6a21\u5f0f\uff1b2\uff09\u534a\u9a6c\u5c14\u53ef\u592b\u98ce\u9669\u7ec4\u4ef6\u663e\u5f0f\u5efa\u6a21\u6d3b\u52a8\u6301\u7eed\u65f6\u95f4\uff1b3\uff09\u901a\u8fc7\u8de8\u4eba\u53e3\u7edf\u8ba1\u7ec4\u548c\u65f6\u95f4\u5757\u7684\u4fe1\u606f\u5171\u4eab\u786e\u4fdd\u7edf\u8ba1\u7a33\u5b9a\u6027\uff1b4\uff09\u4f7f\u7528\u8c03\u67e5\u8bbe\u8ba1\u6743\u91cd\u786e\u4fdd\u7ed3\u679c\u5177\u6709\u7f8e\u56fd\u4eba\u53e3\u4ee3\u8868\u6027\u3002", "result": "\u663e\u5f0f\u5efa\u6a21\u6301\u7eed\u65f6\u95f4\u7684\u98ce\u9669\u7ec4\u4ef6\u76f8\u6bd4\u7eaf\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\u63d0\u4f9b\u4e86\u663e\u8457\u6539\u8fdb\uff1b\u4eba\u53e3\u7edf\u8ba1\u56e0\u7d20\u91cd\u8981\u6027\u6392\u5e8f\uff1a\u6027\u522b\u3001\u65e5\u671f\u7c7b\u578b\u548c\u5bb6\u5ead\u89c4\u6a21\u9884\u6d4b\u589e\u76ca\u6700\u5927\uff0c\u800c\u5730\u533a\u548c\u5b63\u8282\u5bf9\u6d3b\u52a8\u5e8f\u5217\u672c\u8eab\u9884\u6d4b\u8d21\u732e\u8f83\u5c0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4ea7\u751f\u53ef\u89e3\u91ca\u4e14\u7a33\u5065\u7684\u5408\u6210\u6d3b\u52a8\u8f68\u8ff9\u751f\u6210\u5668\uff0c\u4e3a\u4e0b\u6e38\u80fd\u6e90\u7cfb\u7edf\u5efa\u6a21\u63d0\u4f9b\u4e86\u9ad8\u4fdd\u771f\u57fa\u7840\u3002"}}
{"id": "2509.18143", "categories": ["cs.ET", "cs.AI", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.18143", "abs": "https://arxiv.org/abs/2509.18143", "authors": ["Mike Smart", "Sachin Maheshwari", "Himadri Singh Raghav", "Alexander Serb"], "title": "Weight Mapping Properties of a Dual Tree Single Clock Adiabatic Capacitive Neuron", "comment": "11 pages, 10 figures, 6 tables. This work has been submitted to the\n  IEEE for possible publication", "summary": "Dual Tree Single Clock (DTSC) Adiabatic Capacitive Neuron (ACN) circuits\noffer the potential for highly energy-efficient Artificial Neural Network (ANN)\ncomputation in full custom analog IC designs. The efficient mapping of\nArtificial Neuron (AN) abstract weights, extracted from the software-trained\nANNs, onto physical ACN capacitance values has, however, yet to be fully\nresearched. In this paper, we explore the unexpected hidden complexities,\nchallenges and properties of the mapping, as well as, the ramifications for IC\ndesigners in terms accuracy, design and implementation. We propose an optimal,\nAN to ACN methodology, that promotes smaller chip sizes and improved overall\nclassification accuracy, necessary for successful practical deployment. Using\nTensorFlow and Larq software frameworks, we train three different ANN networks\nand map their weights into the energy-efficient DTSC ACN capacitance value\ndomain to demonstrate 100% functional equivalency. Finally, we delve into the\nimpact of weight quantization on ACN performance using novel metrics related to\npractical IC considerations, such as IC floor space and comparator\ndecision-making efficacy.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5c06\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u6743\u91cd\u6620\u5c04\u5230\u53cc\u6811\u5355\u65f6\u949f\u7edd\u70ed\u7535\u5bb9\u795e\u7ecf\u5143\u7535\u8def\u7535\u5bb9\u503c\u7684\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4f18\u5316\u6620\u5c04\u7b56\u7565\u4ee5\u63d0\u9ad8\u82af\u7247\u9762\u79ef\u6548\u7387\u548c\u5206\u7c7b\u7cbe\u5ea6\u3002", "motivation": "\u867d\u7136DTSC ACN\u7535\u8def\u5177\u6709\u9ad8\u80fd\u6548\u4f18\u52bf\uff0c\u4f46\u5982\u4f55\u5c06\u8f6f\u4ef6\u8bad\u7ec3\u5f97\u5230\u7684\u4eba\u5de5\u795e\u7ecf\u5143\u6743\u91cd\u6709\u6548\u6620\u5c04\u5230\u7269\u7406ACN\u7535\u5bb9\u503c\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u4f7f\u7528TensorFlow\u548cLarq\u6846\u67b6\u8bad\u7ec3\u4e09\u79cd\u4e0d\u540cANN\u7f51\u7edc\uff0c\u63d0\u51fa\u6700\u4f18\u7684AN\u5230ACN\u6620\u5c04\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u6743\u91cd\u91cf\u5316\u5bf9ACN\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u73b0\u4e86100%\u529f\u80fd\u7b49\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u65b9\u6cd5\u80fd\u591f\u51cf\u5c0f\u82af\u7247\u5c3a\u5bf8\u5e76\u63d0\u9ad8\u6574\u4f53\u5206\u7c7b\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aIC\u8bbe\u8ba1\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6620\u5c04\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u6743\u91cd\u6620\u5c04\u4e2d\u7684\u590d\u6742\u6027\u548c\u6311\u6218\uff0c\u4fc3\u8fdb\u4e86DTSC ACN\u7535\u8def\u7684\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2509.18311", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18311", "abs": "https://arxiv.org/abs/2509.18311", "authors": ["Benjamin A. Christie", "Sagar Parekh", "Dylan P. Losey"], "title": "Fine-Tuning Robot Policies While Maintaining User Privacy", "comment": null, "summary": "Recent works introduce general-purpose robot policies. These policies provide\na strong prior over how robots should behave -- e.g., how a robot arm should\nmanipulate food items. But in order for robots to match an individual person's\nneeds, users typically fine-tune these generalized policies -- e.g., showing\nthe robot arm how to make their own preferred dinners. Importantly, during the\nprocess of personalizing robots, end-users leak data about their preferences,\nhabits, and styles (e.g., the foods they prefer to eat). Other agents can\nsimply roll-out the fine-tuned policy and see these personally-trained\nbehaviors. This leads to a fundamental challenge: how can we develop robots\nthat personalize actions while keeping learning private from external agents?\nWe here explore this emerging topic in human-robot interaction and develop\nPRoP, a model-agnostic framework for personalized and private robot policies.\nOur core idea is to equip each user with a unique key; this key is then used to\nmathematically transform the weights of the robot's network. With the correct\nkey, the robot's policy switches to match that user's preferences -- but with\nincorrect keys, the robot reverts to its baseline behaviors. We show the\ngeneral applicability of our method across multiple model types in imitation\nlearning, reinforcement learning, and classification tasks. PRoP is practically\nadvantageous because it retains the architecture and behaviors of the original\npolicy, and experimentally outperforms existing encoder-based approaches. See\nvideos and code here: https://prop-icra26.github.io.", "AI": {"tldr": "PRoP\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u73b0\u4e2a\u6027\u5316\u4e14\u79c1\u5bc6\u7684\u673a\u5668\u4eba\u7b56\u7565\uff0c\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u7528\u6237\u5206\u914d\u552f\u4e00\u5bc6\u94a5\u6765\u4fdd\u62a4\u7528\u6237\u504f\u597d\u6570\u636e\u4e0d\u88ab\u5916\u90e8\u4ee3\u7406\u7a83\u53d6\u3002", "motivation": "\u901a\u7528\u673a\u5668\u4eba\u7b56\u7565\u9700\u8981\u6839\u636e\u7528\u6237\u504f\u597d\u8fdb\u884c\u5fae\u8c03\uff0c\u4f46\u5728\u6b64\u8fc7\u7a0b\u4e2d\u7528\u6237\u7684\u4e2a\u4eba\u6570\u636e\uff08\u5982\u996e\u98df\u4e60\u60ef\uff09\u5bb9\u6613\u6cc4\u9732\u3002\u9700\u8981\u5f00\u53d1\u65e2\u80fd\u4e2a\u6027\u5316\u53c8\u80fd\u4fdd\u62a4\u9690\u79c1\u7684\u673a\u5668\u4eba\u7b56\u7565\u3002", "method": "\u4e3a\u6bcf\u4e2a\u7528\u6237\u5206\u914d\u552f\u4e00\u5bc6\u94a5\uff0c\u7528\u8be5\u5bc6\u94a5\u5bf9\u673a\u5668\u4eba\u7f51\u7edc\u6743\u91cd\u8fdb\u884c\u6570\u5b66\u53d8\u6362\u3002\u53ea\u6709\u4f7f\u7528\u6b63\u786e\u5bc6\u94a5\u65f6\uff0c\u673a\u5668\u4eba\u7b56\u7565\u624d\u4f1a\u5207\u6362\u5230\u8be5\u7528\u6237\u7684\u4e2a\u6027\u5316\u884c\u4e3a\u3002", "result": "PRoP\u5728\u6a21\u4eff\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u826f\u597d\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u7f16\u7801\u5668\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u539f\u59cb\u7b56\u7565\u7684\u67b6\u6784\u548c\u884c\u4e3a\u3002", "conclusion": "PRoP\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u4e2a\u6027\u5316\u8fc7\u7a0b\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.18224", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18224", "abs": "https://arxiv.org/abs/2509.18224", "authors": ["Svyatoslav Covanov", "Cedric Pradalier"], "title": "Reversible Kalman Filter for state estimation with Manifold", "comment": null, "summary": "This work introduces an algorithm for state estimation on manifolds within\nthe framework of the Kalman filter. Its primary objective is to provide a\nmethodology enabling the evaluation of the precision of existing Kalman filter\nvariants with arbitrary accuracy on synthetic data, something that, to the best\nof our knowledge, has not been addressed in prior work. To this end, we develop\na new filter that exhibits favorable numerical properties, thereby correcting\nthe divergences observed in previous Kalman filter variants. In this\nformulation, the achievable precision is no longer constrained by the\nsmall-velocity assumption and is determined solely by sensor noise. In\naddition, this new filter assumes high precision on the sensors, which, in real\nscenarios require a detection step that we define heuristically, allowing one\nto extend this approach to scenarios, using either a 9-axis IMU or a\ncombination of odometry, accelerometer, and pressure sensors. The latter\nconfiguration is designed for the reconstruction of trajectories in underwater\nenvironments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u6d41\u5f62\u4e0a\u8fdb\u884c\u72b6\u6001\u4f30\u8ba1\u7684\u5361\u5c14\u66fc\u6ee4\u6ce2\u7b97\u6cd5\uff0c\u65e8\u5728\u8bc4\u4f30\u73b0\u6709\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u53d8\u4f53\u5728\u5408\u6210\u6570\u636e\u4e0a\u7684\u7cbe\u5ea6\uff0c\u5e76\u4fee\u6b63\u5148\u524d\u53d8\u4f53\u7684\u53d1\u6563\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u53d8\u4f53\u5728\u5408\u6210\u6570\u636e\u4e0a\u7684\u7cbe\u5ea6\u8bc4\u4f30\u65b9\u6cd5\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\uff0c\u4e14\u5b58\u5728\u6570\u503c\u53d1\u6563\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u80fd\u591f\u51c6\u786e\u8bc4\u4f30\u6ee4\u6ce2\u5668\u7cbe\u5ea6\u7684\u65b9\u6cd5\uff0c\u5e76\u6539\u8fdb\u6ee4\u6ce2\u5668\u7684\u6570\u503c\u7a33\u5b9a\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff0c\u5177\u6709\u66f4\u597d\u7684\u6570\u503c\u7279\u6027\uff0c\u4e0d\u518d\u53d7\u5c0f\u901f\u5ea6\u5047\u8bbe\u7684\u9650\u5236\uff0c\u7cbe\u5ea6\u4ec5\u7531\u4f20\u611f\u5668\u566a\u58f0\u51b3\u5b9a\u3002\u6ee4\u6ce2\u5668\u5047\u8bbe\u4f20\u611f\u5668\u5177\u6709\u9ad8\u7cbe\u5ea6\uff0c\u5e76\u901a\u8fc7\u542f\u53d1\u5f0f\u68c0\u6d4b\u6b65\u9aa4\u6269\u5c55\u81f3\u5b9e\u9645\u573a\u666f\uff0c\u9002\u7528\u4e8e9\u8f74IMU\u6216\u7ec4\u5408\u4f20\u611f\u5668\u914d\u7f6e\u3002", "result": "\u65b0\u6ee4\u6ce2\u5668\u4fee\u6b63\u4e86\u5148\u524d\u53d8\u4f53\u7684\u53d1\u6563\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6570\u503c\u7a33\u5b9a\u6027\uff0c\u5e76\u5728\u6c34\u4e0b\u73af\u5883\u8f68\u8ff9\u91cd\u5efa\u7b49\u5b9e\u9645\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u6ee4\u6ce2\u5668\u5728\u6d41\u5f62\u72b6\u6001\u4f30\u8ba1\u4e2d\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\uff0c\u4e3a\u5408\u6210\u6570\u636e\u8bc4\u4f30\u548c\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2509.18303", "categories": ["cs.SI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.18303", "abs": "https://arxiv.org/abs/2509.18303", "authors": ["Ozgur Can Seckin", "Bao Tran Truong", "Alessandro Flammini", "Filippo Menczer"], "title": "Identifying Constructive Conflict in Online Discussions through Controversial yet Toxicity Resilient Posts", "comment": "14 pages, 7 figures, 4 tables, will be published in ICWSM 2026", "summary": "Bridging content that brings together individuals with opposing viewpoints on\nsocial media remains elusive, overshadowed by echo chambers and toxic\nexchanges. We propose that algorithmic curation could surface such content by\nconsidering constructive conflicts as a foundational criterion. We\noperationalize this criterion through controversiality to identify challenging\ndialogues and toxicity resilience to capture respectful conversations. We\ndevelop high-accuracy models to capture these dimensions. Analyses based on\nthese models demonstrate that assessing resilience to toxic responses is not\nthe same as identifying low-toxicity posts. We also find that political posts\nare often controversial and tend to attract more toxic responses. However, some\nposts, even the political ones, are resilient to toxicity despite being highly\ncontroversial, potentially sparking civil engagement. Toxicity resilient posts\ntend to use politeness cues, such as showing gratitude and hedging. These\nfindings suggest the potential for framing the tone of posts to encourage\nconstructive political discussions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u7b97\u6cd5\u7b5b\u9009\u5177\u6709\u4e89\u8bae\u6027\u4f46\u80fd\u62b5\u6297\u6bd2\u6027\u7684\u5185\u5bb9\uff0c\u4ee5\u4fc3\u8fdb\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u5efa\u8bbe\u6027\u653f\u6cbb\u8ba8\u8bba\u3002\u7814\u7a76\u53d1\u73b0\u4f7f\u7528\u793c\u8c8c\u63d0\u793a\u7684\u5e16\u5b50\u5373\u4f7f\u5728\u4e89\u8bae\u8bdd\u9898\u4e2d\u4e5f\u80fd\u4fdd\u6301\u6587\u660e\u5bf9\u8bdd\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u5bf9\u7acb\u89c2\u70b9\u4ea4\u6d41\u5f80\u5f80\u88ab\u56de\u97f3\u5ba4\u548c\u6709\u6bd2\u8a00\u8bba\u6df9\u6ca1\uff0c\u7f3a\u4e4f\u5efa\u8bbe\u6027\u5bf9\u8bdd\u3002\u7814\u7a76\u8005\u5e0c\u671b\u901a\u8fc7\u7b97\u6cd5\u8bc6\u522b\u80fd\u591f\u5f15\u53d1\u5efa\u8bbe\u6027\u51b2\u7a81\u7684\u5185\u5bb9\u3002", "method": "\u5f00\u53d1\u9ad8\u7cbe\u5ea6\u6a21\u578b\u6765\u8bc6\u522b\u4e24\u4e2a\u7ef4\u5ea6\uff1a\u4e89\u8bae\u6027\uff08\u8bc6\u522b\u6311\u6218\u6027\u5bf9\u8bdd\uff09\u548c\u6bd2\u6027\u62b5\u6297\u529b\uff08\u8bc6\u522b\u5c0a\u91cd\u6027\u5bf9\u8bdd\uff09\u3002\u57fa\u4e8e\u8fd9\u4e9b\u6a21\u578b\u8fdb\u884c\u5206\u6790\u3002", "result": "\u653f\u6cbb\u5e16\u5b50\u901a\u5e38\u5177\u6709\u4e89\u8bae\u6027\u4e14\u5bb9\u6613\u5f15\u53d1\u6bd2\u6027\u56de\u5e94\uff0c\u4f46\u90e8\u5206\u5e16\u5b50\u5373\u4f7f\u9ad8\u5ea6\u4e89\u8bae\u4e5f\u80fd\u4fdd\u6301\u6587\u660e\u5bf9\u8bdd\u3002\u4f7f\u7528\u793c\u8c8c\u63d0\u793a\uff08\u5982\u8868\u8fbe\u611f\u8c22\u3001\u4f7f\u7528\u7f13\u548c\u8bed\u6c14\uff09\u7684\u5e16\u5b50\u66f4\u5177\u6bd2\u6027\u62b5\u6297\u529b\u3002", "conclusion": "\u901a\u8fc7\u8c03\u6574\u5e16\u5b50\u8bed\u6c14\u6846\u67b6\uff0c\u6709\u6f5c\u529b\u9f13\u52b1\u5efa\u8bbe\u6027\u653f\u6cbb\u8ba8\u8bba\u3002\u7b97\u6cd5\u53ef\u4ee5\u8003\u8651\u4e89\u8bae\u6027\u548c\u6bd2\u6027\u62b5\u6297\u529b\u4f5c\u4e3a\u7b5b\u9009\u6807\u51c6\uff0c\u4fc3\u8fdb\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u6587\u660e\u5bf9\u8bdd\u3002"}}
{"id": "2509.18123", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18123", "abs": "https://arxiv.org/abs/2509.18123", "authors": ["Yeonju Lee", "Rui Qi Chen", "Joseph Oboamah", "Po Nien Su", "Wei-zhen Liang", "Yeyin Shi", "Lu Gan", "Yongsheng Chen", "Xin Qiao", "Jing Li"], "title": "SPADE: A Large Language Model Framework for Soil Moisture Pattern Recognition and Anomaly Detection in Precision Agriculture", "comment": null, "summary": "Accurate interpretation of soil moisture patterns is critical for irrigation\nscheduling and crop management, yet existing approaches for soil moisture\ntime-series analysis either rely on threshold-based rules or data-hungry\nmachine learning or deep learning models that are limited in adaptability and\ninterpretability. In this study, we introduce SPADE (Soil moisture Pattern and\nAnomaly DEtection), an integrated framework that leverages large language\nmodels (LLMs) to jointly detect irrigation patterns and anomalies in soil\nmoisture time-series data. SPADE utilizes ChatGPT-4.1 for its advanced\nreasoning and instruction-following capabilities, enabling zero-shot analysis\nwithout requiring task-specific annotation or fine-tuning. By converting\ntime-series data into a textual representation and designing domain-informed\nprompt templates, SPADE identifies irrigation events, estimates net irrigation\ngains, detects, classifies anomalies, and produces structured, interpretable\nreports. Experiments were conducted on real-world soil moisture sensor data\nfrom commercial and experimental farms cultivating multiple crops across the\nUnited States. Results demonstrate that SPADE outperforms the existing method\nin anomaly detection, achieving higher recall and F1 scores and accurately\nclassifying anomaly types. Furthermore, SPADE achieved high precision and\nrecall in detecting irrigation events, indicating its strong capability to\ncapture irrigation patterns accurately. SPADE's reports provide\ninterpretability and usability of soil moisture analytics. This study\nhighlights the potential of LLMs as scalable, adaptable tools for precision\nagriculture, which is capable of integrating qualitative knowledge and\ndata-driven reasoning to produce actionable insights for accurate soil moisture\nmonitoring and improved irrigation scheduling from soil moisture time-series\ndata.", "AI": {"tldr": "SPADE\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u571f\u58e4\u6c34\u5206\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u6846\u67b6\uff0c\u80fd\u591f\u96f6\u6837\u672c\u68c0\u6d4b\u704c\u6e89\u6a21\u5f0f\u548c\u5f02\u5e38\uff0c\u65e0\u9700\u7279\u5b9a\u4efb\u52a1\u6807\u6ce8\u6216\u5fae\u8c03\u3002", "motivation": "\u73b0\u6709\u571f\u58e4\u6c34\u5206\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u65b9\u6cd5\u4f9d\u8d56\u57fa\u4e8e\u9608\u503c\u7684\u89c4\u5219\u6216\u6570\u636e\u5bc6\u96c6\u578b\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5b58\u5728\u9002\u5e94\u6027\u5dee\u548c\u53ef\u89e3\u91ca\u6027\u5f31\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528ChatGPT-4.1\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5c06\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u8f6c\u6362\u4e3a\u6587\u672c\u8868\u793a\uff0c\u901a\u8fc7\u9886\u57df\u77e5\u8bc6\u8bbe\u8ba1\u7684\u63d0\u793a\u6a21\u677f\u8fdb\u884c\u96f6\u6837\u672c\u5206\u6790\uff0c\u68c0\u6d4b\u704c\u6e89\u4e8b\u4ef6\u3001\u4f30\u8ba1\u704c\u6e89\u589e\u76ca\u3001\u5206\u7c7b\u5f02\u5e38\u3002", "result": "\u5728\u771f\u5b9e\u519c\u573a\u6570\u636e\u4e0a\uff0cSPADE\u5728\u5f02\u5e38\u68c0\u6d4b\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u53ec\u56de\u7387\u548cF1\u5206\u6570\u66f4\u9ad8\uff0c\u704c\u6e89\u4e8b\u4ef6\u68c0\u6d4b\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u4e5f\u5f88\u9ad8\u3002", "conclusion": "LLMs\u53ef\u4f5c\u4e3a\u7cbe\u51c6\u519c\u4e1a\u7684\u53ef\u6269\u5c55\u3001\u9002\u5e94\u6027\u5f3a\u7684\u5de5\u5177\uff0c\u6574\u5408\u5b9a\u6027\u77e5\u8bc6\u548c\u6570\u636e\u9a71\u52a8\u63a8\u7406\uff0c\u4e3a\u571f\u58e4\u6c34\u5206\u76d1\u6d4b\u548c\u704c\u6e89\u8c03\u5ea6\u63d0\u4f9b\u53ef\u64cd\u4f5c\u89c1\u89e3\u3002"}}
{"id": "2509.19243", "categories": ["eess.SY", "cs.SY", "econ.TH"], "pdf": "https://arxiv.org/pdf/2509.19243", "abs": "https://arxiv.org/abs/2509.19243", "authors": ["Ahmed S. Alahmed", "Audun Botterud", "Saurabh Amin", "Ali T. Al-Awami"], "title": "Watts and Drops: Co-Scheduling Power and Water in Desalination Plants", "comment": "5 pages, 6 figures. To appear in Proceedings of the 61st Allerton\n  Conference on Communication, Control, and Computing", "summary": "We develop a mathematical framework to jointly schedule water and electricity\nin a profit-maximizing renewable colocated water desalination plant that\nintegrates both thermal and membrane based technologies. The price-taking\ndesalination plant sells desalinated water to a water utility at a given price\nand engages in bidirectional electricity transactions with the grid, purchasing\nor selling power based on its net electricity demand. We show that the optimal\nscheduling policy depends on the plant's internal renewable generation and\nfollows a simple threshold structure. Under the optimal policy, thermal based\nwater output decreases monotonically with renewable output, while membrane\nbased water output increases monotonically. We characterize the structure and\nintuition behind the threshold policy and examine key special properties.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u6570\u5b66\u6846\u67b6\u6765\u8054\u5408\u8c03\u5ea6\u6c34\u7535\uff0c\u4f18\u5316\u53ef\u518d\u751f\u80fd\u6e90\u5171\u5740\u6d77\u6c34\u6de1\u5316\u5382\u7684\u5229\u6da6\uff0c\u8be5\u5382\u6574\u5408\u4e86\u70ed\u6cd5\u548c\u819c\u6cd5\u6280\u672f\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u540c\u65f6\u4f18\u5316\u6c34\u751f\u4ea7\u548c\u7535\u529b\u4ea4\u6613\u7684\u8c03\u5ea6\u7b56\u7565\uff0c\u4f7f\u53ef\u518d\u751f\u80fd\u6e90\u6d77\u6c34\u6de1\u5316\u5382\u80fd\u591f\u6839\u636e\u5185\u90e8\u53d1\u7535\u60c5\u51b5\u548c\u7535\u7f51\u7535\u4ef7\u5b9e\u73b0\u5229\u6da6\u6700\u5927\u5316\u3002", "method": "\u5efa\u7acb\u6570\u5b66\u4f18\u5316\u6846\u67b6\uff0c\u8003\u8651\u70ed\u6cd5\u548c\u819c\u6cd5\u4e24\u79cd\u6d77\u6c34\u6de1\u5316\u6280\u672f\u7684\u7279\u6027\uff0c\u5141\u8bb8\u53cc\u5411\u7535\u529b\u4ea4\u6613\uff08\u8d2d\u4e70\u6216\u51fa\u552e\u7535\u529b\uff09\uff0c\u5e76\u57fa\u4e8e\u53ef\u518d\u751f\u80fd\u6e90\u8f93\u51fa\u5236\u5b9a\u9608\u503c\u8c03\u5ea6\u7b56\u7565\u3002", "result": "\u6700\u4f18\u8c03\u5ea6\u7b56\u7565\u5177\u6709\u7b80\u5355\u7684\u9608\u503c\u7ed3\u6784\uff1a\u70ed\u6cd5\u6c34\u4ea7\u91cf\u968f\u53ef\u518d\u751f\u80fd\u6e90\u8f93\u51fa\u5355\u8c03\u9012\u51cf\uff0c\u800c\u819c\u6cd5\u6c34\u4ea7\u91cf\u968f\u53ef\u518d\u751f\u80fd\u6e90\u8f93\u51fa\u5355\u8c03\u9012\u589e\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u53ef\u518d\u751f\u80fd\u6e90\u6d77\u6c34\u6de1\u5316\u5382\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8054\u5408\u8c03\u5ea6\u65b9\u6cd5\uff0c\u901a\u8fc7\u9608\u503c\u7b56\u7565\u5b9e\u73b0\u4e86\u6c34\u7535\u8d44\u6e90\u7684\u4f18\u5316\u914d\u7f6e\u548c\u5229\u6da6\u6700\u5927\u5316\u3002"}}
{"id": "2509.18195", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2509.18195", "abs": "https://arxiv.org/abs/2509.18195", "authors": ["Scott Veitch"], "title": "Algorithmic A-Legality: Shorting the Human Future through AI", "comment": "Keywords: artificial intelligence; a-legality; venture legalism;\n  organised irresponsibility", "summary": "This article provides a necessary corrective to the belief that current legal\nand political concepts and institutions are capable of holding to account the\npower of new AI technologies. Drawing on jurisprudential analysis, it argues\nthat while the current development of AI is dependent on the combination of\neconomic and legal power, the technological forms that result increasingly\nexceed the capacity of even the most rigorous legal and political regimes. A\nsituation of \"a-legality\" is emerging whereby the potential of AI to produce\nharms cannot be restrained by conventional legal or political institutions.", "AI": {"tldr": "\u672c\u6587\u7ea0\u6b63\u4e86\u5f53\u524d\u6cd5\u5f8b\u548c\u653f\u6cbb\u6982\u5ff5\u53ca\u5236\u5ea6\u80fd\u591f\u6709\u6548\u76d1\u7ba1\u65b0\u5174AI\u6280\u672f\u6743\u529b\u7684\u9519\u8bef\u8ba4\u77e5\uff0c\u6307\u51faAI\u53d1\u5c55\u4f9d\u8d56\u4e8e\u7ecf\u6d4e\u4e0e\u6cd5\u5f8b\u6743\u529b\u7684\u7ed3\u5408\uff0c\u4f46\u5176\u6280\u672f\u5f62\u6001\u5df2\u8d85\u51fa\u6700\u4e25\u683c\u6cd5\u5f8b\u653f\u6cbb\u5236\u5ea6\u7684\u76d1\u7ba1\u80fd\u529b\uff0c\u5f62\u6210\"\u975e\u6cd5\u6027\"\u72b6\u6001\u3002", "motivation": "\u7ea0\u6b63\u5bf9\u73b0\u6709\u6cd5\u5f8b\u653f\u6cbb\u5236\u5ea6\u76d1\u7ba1AI\u80fd\u529b\u7684\u8fc7\u5ea6\u4e50\u89c2\u770b\u6cd5\uff0c\u63ed\u793aAI\u6280\u672f\u53d1\u5c55\u5df2\u8d85\u8d8a\u4f20\u7edf\u76d1\u7ba1\u6846\u67b6\u7684\u73b0\u5b9e\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u6cd5\u7406\u5b66\u5206\u6790\uff0c\u8bba\u8bc1AI\u53d1\u5c55\u4f9d\u8d56\u7ecf\u6d4e\u4e0e\u6cd5\u5f8b\u6743\u529b\u7ed3\u5408\uff0c\u4f46\u6280\u672f\u5f62\u6001\u8d85\u51fa\u76d1\u7ba1\u80fd\u529b\u3002", "result": "\u8bc6\u522b\u51fa\"\u975e\u6cd5\u6027\"\u73b0\u8c61\uff0c\u5373AI\u4ea7\u751f\u5371\u5bb3\u7684\u6f5c\u529b\u65e0\u6cd5\u88ab\u4f20\u7edf\u6cd5\u5f8b\u653f\u6cbb\u5236\u5ea6\u7ea6\u675f\u3002", "conclusion": "\u5f53\u524d\u6cd5\u5f8b\u653f\u6cbb\u6982\u5ff5\u548c\u5236\u5ea6\u4e0d\u8db3\u4ee5\u5e94\u5bf9AI\u6280\u672f\u5e26\u6765\u7684\u76d1\u7ba1\u6311\u6218\uff0c\u9700\u8981\u65b0\u7684\u76d1\u7ba1\u6846\u67b6\u3002"}}
{"id": "2509.18459", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2509.18459", "abs": "https://arxiv.org/abs/2509.18459", "authors": ["Jiangshan Zhang", "Vivek Pradhan", "Yuxi Zhao"], "title": "Evaluating Bias Reduction Methods in Binary Emax Model for Reliable Dose-Response Estimation", "comment": null, "summary": "The Binary Emax model is widely employed in dose-response analysis during\nPhase II clinical studies to identify the optimal dose for subsequence\nconfirmatory trials. The parameter estimation and inference heavily rely on the\nasymptotic properties of Maximum Likelihood (ML) estimators; however, this\napproach may be questionable under small or moderate sample sizes and is not\nrobust to violation of model assumptions. To provide a reliable solution, this\npaper examines three bias-reduction methods: the Cox-Snell bias correction,\nFirth-score modification, and a maximum penalized likelihood estimator (MPLE)\nusing Jeffreys prior. Through comprehensive simulation studies, we evaluate the\nperformance of these methods in reducing bias and controlling variance,\nespecially when model assumptions are violated. The results demonstrate that\nboth Firth and MPLE methods provide robust estimates, with MPLE outperforming\nin terms of stability and lower variance. We further illustrate the practical\napplication of these methods using data from the TURANDOT study, a Phase II\nclinical trial. Our findings suggest that MPLE with Jeffreys prior offers an\neffective and reliable alternative to the Firth method, particularly for\ndose-response relationships that deviate from monotonicity, making it valuable\nfor robust parameter estimation in dose-ranging studies.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e09\u79cd\u504f\u5dee\u51cf\u5c11\u65b9\u6cd5\uff08Cox-Snell\u504f\u5dee\u6821\u6b63\u3001Firth-score\u4fee\u6b63\u548cJeffreys\u5148\u9a8c\u7684\u6700\u5927\u60e9\u7f5a\u4f3c\u7136\u4f30\u8ba1\u5668\uff09\u5728\u4e8c\u5143Emax\u6a21\u578b\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u6a21\u62df\u7814\u7a76\u548cTURANDOT\u7814\u7a76\u6570\u636e\u9a8c\u8bc1\u4e86MPLE\u65b9\u6cd5\u5728\u7a33\u5b9a\u6027\u548c\u65b9\u5dee\u63a7\u5236\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u4e8c\u5143Emax\u6a21\u578b\u5728II\u671f\u4e34\u5e8a\u8bd5\u9a8c\u4e2d\u5e7f\u6cdb\u7528\u4e8e\u5242\u91cf\u53cd\u5e94\u5206\u6790\uff0c\u4f46\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u5728\u5c0f\u6837\u672c\u6216\u6a21\u578b\u5047\u8bbe\u8fdd\u53cd\u65f6\u5b58\u5728\u95ee\u9898\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u504f\u5dee\u51cf\u5c11\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5168\u9762\u7684\u6a21\u62df\u7814\u7a76\u8bc4\u4f30\u4e09\u79cd\u504f\u5dee\u51cf\u5c11\u65b9\u6cd5\uff08Cox-Snell\u3001Firth\u548cMPLE\uff09\u5728\u51cf\u5c11\u504f\u5dee\u548c\u63a7\u5236\u65b9\u5dee\u65b9\u9762\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u6a21\u578b\u5047\u8bbe\u8fdd\u53cd\u7684\u60c5\u51b5\u4e0b\u3002", "result": "Firth\u548cMPLE\u65b9\u6cd5\u90fd\u80fd\u63d0\u4f9b\u7a33\u5065\u7684\u4f30\u8ba1\uff0c\u4f46MPLE\u5728\u7a33\u5b9a\u6027\u548c\u65b9\u5dee\u63a7\u5236\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u7279\u522b\u662f\u5728\u5242\u91cf\u53cd\u5e94\u5173\u7cfb\u504f\u79bb\u5355\u8c03\u6027\u65f6\u3002", "conclusion": "\u4f7f\u7528Jeffreys\u5148\u9a8c\u7684MPLE\u65b9\u6cd5\u4e3aFirth\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6709\u6548\u53ef\u9760\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5242\u91cf\u53cd\u5e94\u5173\u7cfb\u504f\u79bb\u5355\u8c03\u6027\u7684\u60c5\u51b5\uff0c\u5bf9\u5242\u91cf\u8303\u56f4\u7814\u7a76\u4e2d\u7684\u7a33\u5065\u53c2\u6570\u4f30\u8ba1\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2509.18679", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2509.18679", "abs": "https://arxiv.org/abs/2509.18679", "authors": ["Shikhar Srivastava", "Ritajit Majumdar", "Padmanabha Venkatagiri Seshadri", "Anupama Ray", "Yogesh Simmhan"], "title": "Lightweight Targeted Estimation of Layout Noise in a Quantum Computer using Quality Indicator Circuits", "comment": null, "summary": "In the current era of quantum computing, minimizing noise is essential for\nreliably executing quantum circuits on hardware. A key factor affecting circuit\nperformance is the mapping of the abstract quantum circuit to the physical\nlayout of the quantum hardware. This mapping can significantly influence output\nquality, especially since hardware noise profiles are non-uniform and dynamic.\nExisting solutions such as Mapomatic and Just-In-Time (JIT) Transpilation\nattempt to address this issue but are limited either by relying on stale\ncalibration data or high hardware usage, respectively. In this article, we\npropose Quality Indicator Circuits (QICs) as a lightweight, real-time method\nfor assessing layout quality. A QIC is a small probe circuit that is designed\nto retain the basic structure of the user's circuit and whose ideal noiseless\noutcome is known. It is used to evaluate which region of the quantum hardware\nis best suited for executing the circuit of interest. We first propose a basic\nmethod where a QIC is executed for each isomorphic layout to detect the best\namong them. Although this requires several targeted circuit executions, we show\nthat it still, in most cases, reduces the execution overheads as compared with\nJIT. To reduce the overheads further, we propose the union of multiple layouts\nwith a Union QIC approach that has no overlaps, and a Distortion Threshold\nbased approach allowing some overlap. Our results show that these outperform\nMapomatic in the quality of layout selection while reducing the hardware\noverhead of JIT by 79 percent on average. This makes our proposed method\nlightweight and reliable, and a viable technique for layout selection in\nnear-term quantum devices.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u5b9e\u65f6\u5e03\u5c40\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u2014\u2014\u8d28\u91cf\u6307\u793a\u7535\u8def\uff08QICs\uff09\uff0c\u7528\u4e8e\u5728\u91cf\u5b50\u786c\u4ef6\u4e0a\u9009\u62e9\u6700\u4f18\u7684\u7535\u8def\u6620\u5c04\u5e03\u5c40\uff0c\u4ee5\u964d\u4f4e\u566a\u58f0\u5f71\u54cd\u5e76\u63d0\u9ad8\u7535\u8def\u6267\u884c\u53ef\u9760\u6027\u3002", "motivation": "\u5728\u91cf\u5b50\u8ba1\u7b97\u4e2d\uff0c\u5c06\u62bd\u8c61\u91cf\u5b50\u7535\u8def\u6620\u5c04\u5230\u7269\u7406\u786c\u4ef6\u5e03\u5c40\u5bf9\u7535\u8def\u6027\u80fd\u6709\u91cd\u8981\u5f71\u54cd\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5982Mapomatic\u548cJIT Transpilation\u5b58\u5728\u6821\u51c6\u6570\u636e\u8fc7\u65f6\u6216\u786c\u4ef6\u4f7f\u7528\u7387\u9ad8\u7b49\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u5b9e\u65f6\u5e03\u5c40\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u8d28\u91cf\u6307\u793a\u7535\u8def\uff08QICs\uff09\u65b9\u6cd5\uff1a\u8bbe\u8ba1\u5c0f\u578b\u63a2\u6d4b\u7535\u8def\uff0c\u4fdd\u7559\u7528\u6237\u7535\u8def\u7684\u57fa\u672c\u7ed3\u6784\uff0c\u5df2\u77e5\u5176\u7406\u60f3\u65e0\u566a\u58f0\u7ed3\u679c\u3002\u901a\u8fc7\u6267\u884cQIC\u6765\u8bc4\u4f30\u91cf\u5b50\u786c\u4ef6\u4e0d\u540c\u533a\u57df\u5bf9\u76ee\u6807\u7535\u8def\u7684\u9002\u7528\u6027\u3002\u5305\u62ec\u57fa\u7840\u65b9\u6cd5\uff08\u4e3a\u6bcf\u4e2a\u540c\u6784\u5e03\u5c40\u6267\u884cQIC\uff09\u3001\u65e0\u91cd\u53e0\u7684Union QIC\u65b9\u6cd5\u4ee5\u53ca\u5141\u8bb8\u90e8\u5206\u91cd\u53e0\u7684Distortion Threshold\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4e0eJIT\u76f8\u6bd4\u5e73\u5747\u51cf\u5c1179%\u7684\u786c\u4ef6\u5f00\u9500\uff0c\u540c\u65f6\u5728\u5e03\u5c40\u9009\u62e9\u8d28\u91cf\u4e0a\u4f18\u4e8eMapomatic\u3002", "conclusion": "QICs\u65b9\u6cd5\u8f7b\u91cf\u3001\u53ef\u9760\uff0c\u662f\u8fd1\u671f\u91cf\u5b50\u8bbe\u5907\u4e2d\u5e03\u5c40\u9009\u62e9\u7684\u53ef\u884c\u6280\u672f\uff0c\u80fd\u591f\u5728\u4fdd\u8bc1\u5e03\u5c40\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u786c\u4ef6\u5f00\u9500\u3002"}}
{"id": "2509.18327", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18327", "abs": "https://arxiv.org/abs/2509.18327", "authors": ["Katherine H. Allen", "Chris Rogers", "Elaine S. Short"], "title": "Haptic Communication in Human-Human and Human-Robot Co-Manipulation", "comment": "9 pages, 18 figures, ROMAN 2025", "summary": "When a human dyad jointly manipulates an object, they must communicate about\ntheir intended motion plans. Some of that collaboration is achieved through the\nmotion of the manipulated object itself, which we call \"haptic communication.\"\nIn this work, we captured the motion of human-human dyads moving an object\ntogether with one participant leading a motion plan about which the follower is\nuninformed. We then captured the same human participants manipulating the same\nobject with a robot collaborator. By tracking the motion of the shared object\nusing a low-cost IMU, we can directly compare human-human shared manipulation\nto the motion of those same participants interacting with the robot.\nIntra-study and post-study questionnaires provided participant feedback on the\ncollaborations, indicating that the human-human collaborations are\nsignificantly more fluent, and analysis of the IMU data indicates that it\ncaptures objective differences in the motion profiles of the conditions. The\ndifferences in objective and subjective measures of accuracy and fluency\nbetween the human-human and human-robot trials motivate future research into\nimproving robot assistants for physical tasks by enabling them to send and\nreceive anthropomorphic haptic signals.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u4eba-\u4eba\u548c\u4eba-\u673a\u5668\u4eba\u534f\u4f5c\u64cd\u7eb5\u7269\u4f53\u65f6\u7684\u8fd0\u52a8\u7279\u5f81\u5dee\u5f02\uff0c\u53d1\u73b0\u4eba-\u4eba\u534f\u4f5c\u66f4\u52a0\u6d41\u7545\uff0c\u5e76\u5efa\u8bae\u672a\u6765\u673a\u5668\u4eba\u5e94\u5177\u5907\u53d1\u9001\u548c\u63a5\u6536\u7c7b\u4eba\u89e6\u89c9\u4fe1\u53f7\u7684\u80fd\u529b", "motivation": "\u7814\u7a76\u4eba\u7c7b\u5728\u534f\u4f5c\u64cd\u7eb5\u7269\u4f53\u65f6\u901a\u8fc7\u7269\u4f53\u8fd0\u52a8\u8fdb\u884c\u7684\u89e6\u89c9\u901a\u4fe1\uff0c\u5e76\u6bd4\u8f83\u4eba-\u4eba\u534f\u4f5c\u4e0e\u4eba-\u673a\u5668\u4eba\u534f\u4f5c\u7684\u5dee\u5f02\uff0c\u4ee5\u6539\u8fdb\u673a\u5668\u4eba\u534f\u4f5c\u80fd\u529b", "method": "\u4f7f\u7528\u4f4e\u6210\u672cIMU\u8ddf\u8e2a\u4eba-\u4eba\u534f\u4f5c\u548c\u4eba-\u673a\u5668\u4eba\u534f\u4f5c\u4e2d\u5171\u4eab\u7269\u4f53\u7684\u8fd0\u52a8\uff0c\u901a\u8fc7\u95ee\u5377\u6536\u96c6\u53c2\u4e0e\u8005\u53cd\u9988\uff0c\u5206\u6790\u8fd0\u52a8\u6570\u636e\u5dee\u5f02", "result": "\u4eba-\u4eba\u534f\u4f5c\u663e\u8457\u66f4\u6d41\u7545\uff0cIMU\u6570\u636e\u6355\u6349\u5230\u4e86\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u8fd0\u52a8\u7279\u5f81\u5dee\u5f02\uff0c\u4e3b\u89c2\u548c\u5ba2\u89c2\u6307\u6807\u90fd\u663e\u793a\u4eba-\u4eba\u534f\u4f5c\u4f18\u4e8e\u4eba-\u673a\u5668\u4eba\u534f\u4f5c", "conclusion": "\u7814\u7a76\u7ed3\u679c\u652f\u6301\u672a\u6765\u5f00\u53d1\u80fd\u591f\u53d1\u9001\u548c\u63a5\u6536\u7c7b\u4eba\u89e6\u89c9\u4fe1\u53f7\u7684\u673a\u5668\u4eba\u52a9\u624b\uff0c\u4ee5\u6539\u5584\u7269\u7406\u4efb\u52a1\u4e2d\u7684\u534f\u4f5c\u6548\u679c"}}
{"id": "2509.18292", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18292", "abs": "https://arxiv.org/abs/2509.18292", "authors": ["Shuaiting Huang", "Haodong Jiang", "Chengcheng Zhao", "Peng Cheng", "Junfeng Wu"], "title": "Fully Distributed State Estimation for Multi-agent Systems and its Application in Cooperative Localization", "comment": null, "summary": "In this paper, we investigate the distributed state estimation problem for a\ncontinuous-time linear multi-agent system (MAS) composed of $\\mathit{m}$ agents\nand monitored by the agents themselves. To address this problem, we propose a\ndistributed observer that enables each agent to reconstruct the state of the\nMAS. The main idea is to let each agent $\\mathit{i}$ recover the state of agent\n$\\mathit{j}$ by using leader-follower consensus rules to track agent\n$\\mathit{j}$'s state estimate, which is generated by agent $\\mathit{j}$ itself\nusing a Luenberger-like estimation rule. Under the assumptions of node-level\nobservability and topological ordering consistency, we show that the estimation\nerror dynamics are stabilizable if and only if the communication graph is\nstrongly connected. Moreover, we discuss the fully distributed design of the\nproposed observer, assuming that the agents only know basic MAS configuration\ninformation, such as the homogeneity and the maximum number of allowable\nagents. This design ensures that the proposed observer functions correctly when\nagents are added or removed. Building on this, we consider cooperative\nlocalization as a distributed estimation problem and develop two fully\ndistributed localization algorithms that allow agents to track their own and\nother agents' positions (and velocities) within the MAS. Finally, we conduct\nsimulations to demonstrate the effectiveness of our proposed theoretical\nresults.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8fde\u7eed\u65f6\u95f4\u7ebf\u6027\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5206\u5e03\u5f0f\u72b6\u6001\u4f30\u8ba1\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u89c2\u6d4b\u5668\uff0c\u4f7f\u6bcf\u4e2a\u667a\u80fd\u4f53\u80fd\u591f\u91cd\u6784\u6574\u4e2a\u7cfb\u7edf\u7684\u72b6\u6001\u3002\u901a\u8fc7\u9886\u5bfc-\u8ddf\u968f\u4e00\u81f4\u6027\u89c4\u5219\u548cLuenberger\u7c7b\u4f30\u8ba1\u89c4\u5219\uff0c\u5728\u8282\u70b9\u7ea7\u53ef\u89c2\u6d4b\u6027\u548c\u62d3\u6251\u6392\u5e8f\u4e00\u81f4\u6027\u7684\u5047\u8bbe\u4e0b\uff0c\u8bc1\u660e\u4e86\u4f30\u8ba1\u8bef\u5dee\u52a8\u6001\u53ef\u7a33\u5b9a\u7684\u5145\u8981\u6761\u4ef6\u662f\u901a\u4fe1\u56fe\u5f3a\u8fde\u901a\u3002", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u6bcf\u4e2a\u667a\u80fd\u4f53\u9700\u8981\u4f30\u8ba1\u6574\u4e2a\u7cfb\u7edf\u72b6\u6001\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u667a\u80fd\u4f53\u53ea\u80fd\u83b7\u5f97\u5c40\u90e8\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u5b8c\u5168\u5206\u5e03\u5f0f\u7684\u72b6\u6001\u4f30\u8ba1\u3002", "method": "\u63d0\u51fa\u5206\u5e03\u5f0f\u89c2\u6d4b\u5668\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u4f7f\u7528\u9886\u5bfc-\u8ddf\u968f\u4e00\u81f4\u6027\u89c4\u5219\u8ddf\u8e2a\u5176\u4ed6\u667a\u80fd\u4f53\u7684\u72b6\u6001\u4f30\u8ba1\uff0c\u7ed3\u5408Luenberger\u7c7b\u4f30\u8ba1\u89c4\u5219\u3002\u8bbe\u8ba1\u4e86\u5b8c\u5168\u5206\u5e03\u5f0f\u65b9\u6848\uff0c\u667a\u80fd\u4f53\u4ec5\u9700\u77e5\u9053\u57fa\u672c\u914d\u7f6e\u4fe1\u606f\uff08\u5982\u540c\u8d28\u6027\u548c\u6700\u5927\u5141\u8bb8\u667a\u80fd\u4f53\u6570\uff09\u3002", "result": "\u8bc1\u660e\u4e86\u4f30\u8ba1\u8bef\u5dee\u52a8\u6001\u53ef\u7a33\u5b9a\u7684\u5145\u8981\u6761\u4ef6\u662f\u901a\u4fe1\u56fe\u5f3a\u8fde\u901a\u3002\u5f00\u53d1\u4e86\u4e24\u79cd\u5b8c\u5168\u5206\u5e03\u5f0f\u5b9a\u4f4d\u7b97\u6cd5\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u8ddf\u8e2a\u81ea\u8eab\u548c\u5176\u4ed6\u667a\u80fd\u4f53\u7684\u4f4d\u7f6e\u548c\u901f\u5ea6\u3002\u4eff\u771f\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5206\u5e03\u5f0f\u89c2\u6d4b\u5668\u5728\u5f3a\u8fde\u901a\u901a\u4fe1\u56fe\u6761\u4ef6\u4e0b\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u72b6\u6001\u4f30\u8ba1\uff0c\u5b8c\u5168\u5206\u5e03\u5f0f\u8bbe\u8ba1\u4fdd\u8bc1\u4e86\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u667a\u80fd\u4f53\u52a8\u6001\u52a0\u5165\u6216\u9000\u51fa\u7684\u573a\u666f\u3002"}}
{"id": "2509.18325", "categories": ["cs.SI", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2509.18325", "abs": "https://arxiv.org/abs/2509.18325", "authors": ["Huaizhi Liao", "Tian Qiu", "Guang Chen"], "title": "A Graph-Neural-Network-Entropy model of vital node identification on network attack and propagation", "comment": null, "summary": "Vital nodes usually play a key role in complex networks. Uncovering these\nnodes is an important task in protecting the network, especially when the\nnetwork suffers intentional attack. Many existing methods have not fully\nintegrated the node feature, interaction and state. In this article, we propose\na novel method (GNNE) based on graph neural networks and information entropy.\nThe method employs a Graph Convolutional Network (GCN) to learn the nodes'\nfeatures, which are input into a Graph Attention Network (GAT) to obtain the\ninfluence factor of nodes, and the node influence factors are used to calculate\nthe nodes' entropy to evaluate the node importance. The GNNE takes advantage of\nthe GCN and GAT, with the GCN well extracting the nodes' features and the GAT\naggregating the features of the nodes' neighbors by using the attention\nmechanism to assign different weights to the neighbors with different\nimportance, and the nodes' entropy quantifies the nodes' state in the network.\nThe proposed method is trained on a synthetic Barabasi-Albert network, and\ntested on six real datasets. Compared with eight traditional topology-based\nmethods and four graph-machine-learning-based methods, the GNNE shows an\nadvantage for the vital node identification in the perspectives of network\nattack and propagation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u4fe1\u606f\u71b5\u7684\u65b0\u65b9\u6cd5GNNE\uff0c\u7528\u4e8e\u8bc6\u522b\u590d\u6742\u7f51\u7edc\u4e2d\u7684\u5173\u952e\u8282\u70b9\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408GCN\u548cGAT\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u8282\u70b9\u71b5\u91cf\u5316\u8282\u70b9\u91cd\u8981\u6027\uff0c\u5728\u591a\u79cd\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u6574\u5408\u8282\u70b9\u7279\u5f81\u3001\u4ea4\u4e92\u548c\u72b6\u6001\u4fe1\u606f\uff0c\u800c\u5173\u952e\u8282\u70b9\u5728\u7f51\u7edc\u906d\u53d7\u653b\u51fb\u65f6\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u8bc6\u522b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u56fe\u5377\u79ef\u7f51\u7edc(GCN)\u5b66\u4e60\u8282\u70b9\u7279\u5f81\uff0c\u8f93\u5165\u56fe\u6ce8\u610f\u529b\u7f51\u7edc(GAT)\u83b7\u53d6\u8282\u70b9\u5f71\u54cd\u56e0\u5b50\uff0c\u7136\u540e\u8ba1\u7b97\u8282\u70b9\u71b5\u6765\u8bc4\u4f30\u8282\u70b9\u91cd\u8981\u6027\u3002", "result": "\u5728\u516d\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u6d4b\u8bd5\u8868\u660e\uff0cGNNE\u5728\u7f51\u7edc\u653b\u51fb\u548c\u4f20\u64ad\u89c6\u89d2\u4e0b\u4f18\u4e8e\u516b\u79cd\u4f20\u7edf\u62d3\u6251\u65b9\u6cd5\u548c\u56db\u79cd\u56fe\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "GNNE\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408GCN\u548cGAT\u7684\u4f18\u52bf\uff0c\u6709\u6548\u8bc6\u522b\u5173\u952e\u8282\u70b9\uff0c\u5728\u7f51\u7edc\u4fdd\u62a4\u548c\u4f20\u64ad\u63a7\u5236\u65b9\u9762\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.18132", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18132", "abs": "https://arxiv.org/abs/2509.18132", "authors": ["Xiuyi Fan"], "title": "Position Paper: Integrating Explainability and Uncertainty Estimation in Medical AI", "comment": "Accepted at the International Joint Conference on Neural Networks,\n  IJCNN 2025", "summary": "Uncertainty is a fundamental challenge in medical practice, but current\nmedical AI systems fail to explicitly quantify or communicate uncertainty in a\nway that aligns with clinical reasoning. Existing XAI works focus on\ninterpreting model predictions but do not capture the confidence or reliability\nof these predictions. Conversely, uncertainty estimation (UE) techniques\nprovide confidence measures but lack intuitive explanations. The disconnect\nbetween these two areas limits AI adoption in medicine. To address this gap, we\npropose Explainable Uncertainty Estimation (XUE) that integrates explainability\nwith uncertainty quantification to enhance trust and usability in medical AI.\nWe systematically map medical uncertainty to AI uncertainty concepts and\nidentify key challenges in implementing XUE. We outline technical directions\nfor advancing XUE, including multimodal uncertainty quantification,\nmodel-agnostic visualization techniques, and uncertainty-aware decision support\nsystems. Lastly, we propose guiding principles to ensure effective XUE\nrealisation. Our analysis highlights the need for AI systems that not only\ngenerate reliable predictions but also articulate confidence levels in a\nclinically meaningful way. This work contributes to the development of\ntrustworthy medical AI by bridging explainability and uncertainty, paving the\nway for AI systems that are aligned with real-world clinical complexities.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u53ef\u89e3\u91ca\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff08XUE\uff09\u6846\u67b6\uff0c\u5c06\u53ef\u89e3\u91ca\u6027\u4e0e\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u76f8\u7ed3\u5408\uff0c\u4ee5\u89e3\u51b3\u533b\u7597AI\u4e2d\u4e0d\u786e\u5b9a\u6027\u6c9f\u901a\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u533b\u7597AI\u7cfb\u7edf\u672a\u80fd\u4ee5\u7b26\u5408\u4e34\u5e8a\u63a8\u7406\u7684\u65b9\u5f0f\u660e\u786e\u91cf\u5316\u6216\u6c9f\u901a\u4e0d\u786e\u5b9a\u6027\uff0c\u73b0\u6709\u53ef\u89e3\u91caAI\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u7684\u6355\u6349\uff0c\u800c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6280\u672f\u53c8\u7f3a\u4e4f\u76f4\u89c2\u89e3\u91ca\uff0c\u8fd9\u79cd\u8131\u8282\u9650\u5236\u4e86AI\u5728\u533b\u5b66\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faXUE\u6846\u67b6\uff0c\u7cfb\u7edf\u5730\u5c06\u533b\u5b66\u4e0d\u786e\u5b9a\u6027\u6620\u5c04\u5230AI\u4e0d\u786e\u5b9a\u6027\u6982\u5ff5\uff0c\u8bc6\u522bXUE\u5b9e\u65bd\u7684\u5173\u952e\u6311\u6218\uff0c\u5e76\u89c4\u5212\u6280\u672f\u65b9\u5411\u5305\u62ec\u591a\u6a21\u6001\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3001\u6a21\u578b\u65e0\u5173\u53ef\u89c6\u5316\u6280\u672f\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u3002", "result": "\u5efa\u7acb\u4e86\u5c06\u53ef\u89e3\u91ca\u6027\u4e0e\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6574\u5408\u7684\u7406\u8bba\u6846\u67b6\uff0c\u4e3a\u5f00\u53d1\u66f4\u53ef\u4fe1\u7684\u533b\u7597AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6307\u5bfc\u539f\u5219\u548c\u6280\u672f\u8def\u5f84\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u901a\u8fc7\u6865\u63a5\u53ef\u89e3\u91ca\u6027\u548c\u4e0d\u786e\u5b9a\u6027\uff0c\u4e3a\u5f00\u53d1\u4e0e\u771f\u5b9e\u4e16\u754c\u4e34\u5e8a\u590d\u6742\u6027\u76f8\u4e00\u81f4\u7684AI\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u6709\u52a9\u4e8e\u6784\u5efa\u53ef\u4fe1\u8d56\u7684\u533b\u7597AI\u3002"}}
{"id": "2509.18210", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2509.18210", "abs": "https://arxiv.org/abs/2509.18210", "authors": ["Paul Gra\u00dfl", "Hanna Schraffenberger", "Frederik Zuiderveen Borgesius", "Moniek Buijzen"], "title": "Dark and Bright Patterns in Cookie Consent Requests", "comment": null, "summary": "Dark patterns are (evil) design nudges that steer people's behaviour through\npersuasive interface design. Increasingly found in cookie consent requests,\nthey possibly undermine principles of EU privacy law. In two preregistered\nonline experiments we investigated the effects of three common design nudges\n(default, aesthetic manipulation, obstruction) on users' consent decisions and\ntheir perception of control over their personal data in these situations. In\nthe first experiment (N = 228) we explored the effects of design nudges towards\nthe privacy-unfriendly option (dark patterns). The experiment revealed that\nmost participants agreed to all consent requests regardless of dark design\nnudges. Unexpectedly, despite generally low levels of perceived control,\nobstructing the privacy-friendly option led to more rather than less perceived\ncontrol. In the second experiment (N = 255) we reversed the direction of the\ndesign nudges towards the privacy-friendly option, which we title \"bright\npatterns\". This time the obstruction and default nudges swayed people\neffectively towards the privacy-friendly option, while the result regarding\nperceived control stayed the same compared to Experiment 1. Overall, our\nfindings suggest that many current implementations of cookie consent requests\ndo not enable meaningful choices by internet users, and are thus not in line\nwith the intention of the EU policymakers. We also explore how policymakers\ncould address the problem.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4e24\u4e2a\u5728\u7ebf\u5b9e\u9a8c\u63a2\u8ba8\u4e86\u6697\u6a21\u5f0f\uff08dark patterns\uff09\u548c\u4eae\u6a21\u5f0f\uff08bright patterns\uff09\u5bf9\u7528\u6237cookie\u540c\u610f\u51b3\u7b56\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5f53\u524dcookie\u540c\u610f\u8bf7\u6c42\u7684\u5b9e\u73b0\u65b9\u5f0f\u65e0\u6cd5\u8ba9\u7528\u6237\u505a\u51fa\u6709\u610f\u4e49\u7684\u9009\u62e9\uff0c\u4e0d\u7b26\u5408\u6b27\u76df\u653f\u7b56\u5236\u5b9a\u8005\u7684\u610f\u56fe\u3002", "motivation": "\u6697\u6a21\u5f0f\uff08\u90aa\u6076\u7684\u8bbe\u8ba1\u63a8\u52a8\uff09\u901a\u8fc7\u6709\u8bf4\u670d\u529b\u7684\u754c\u9762\u8bbe\u8ba1\u5f15\u5bfc\u7528\u6237\u884c\u4e3a\uff0c\u5728cookie\u540c\u610f\u8bf7\u6c42\u4e2d\u8d8a\u6765\u8d8a\u5e38\u89c1\uff0c\u53ef\u80fd\u7834\u574f\u6b27\u76df\u9690\u79c1\u6cd5\u7684\u539f\u5219\u3002\u7814\u7a76\u65e8\u5728\u8c03\u67e5\u5e38\u89c1\u8bbe\u8ba1\u63a8\u52a8\u5bf9\u7528\u6237\u540c\u610f\u51b3\u7b56\u548c\u611f\u77e5\u63a7\u5236\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u4e24\u4e2a\u9884\u5148\u6ce8\u518c\u7684\u5728\u7ebf\u5b9e\u9a8c\uff1a\u5b9e\u9a8c1\uff08N=228\uff09\u63a2\u7d22\u6697\u6a21\u5f0f\uff08\u9ed8\u8ba4\u8bbe\u7f6e\u3001\u7f8e\u5b66\u64cd\u7eb5\u3001\u963b\u788d\uff09\u5bf9\u9690\u79c1\u4e0d\u53cb\u597d\u9009\u9879\u7684\u5f71\u54cd\uff1b\u5b9e\u9a8c2\uff08N=255\uff09\u5c06\u8bbe\u8ba1\u63a8\u52a8\u65b9\u5411\u53cd\u8f6c\u81f3\u9690\u79c1\u53cb\u597d\u9009\u9879\uff08\u4eae\u6a21\u5f0f\uff09\u3002", "result": "\u5b9e\u9a8c1\u663e\u793a\u5927\u591a\u6570\u53c2\u4e0e\u8005\u540c\u610f\u6240\u6709\u8bf7\u6c42\uff0c\u963b\u788d\u9690\u79c1\u53cb\u597d\u9009\u9879\u53cd\u800c\u589e\u52a0\u4e86\u611f\u77e5\u63a7\u5236\uff1b\u5b9e\u9a8c2\u4e2d\u963b\u788d\u548c\u9ed8\u8ba4\u63a8\u52a8\u6709\u6548\u5f15\u5bfc\u7528\u6237\u9009\u62e9\u9690\u79c1\u53cb\u597d\u9009\u9879\uff0c\u4f46\u611f\u77e5\u63a7\u5236\u7ed3\u679c\u4e0e\u5b9e\u9a8c1\u76f8\u540c\u3002", "conclusion": "\u5f53\u524dcookie\u540c\u610f\u8bf7\u6c42\u7684\u5b9e\u73b0\u65e0\u6cd5\u8ba9\u4e92\u8054\u7f51\u7528\u6237\u505a\u51fa\u6709\u610f\u4e49\u7684\u9009\u62e9\uff0c\u4e0d\u7b26\u5408\u6b27\u76df\u653f\u7b56\u5236\u5b9a\u8005\u7684\u610f\u56fe\u3002\u7814\u7a76\u8fd8\u63a2\u8ba8\u4e86\u653f\u7b56\u5236\u5b9a\u8005\u5982\u4f55\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002"}}
{"id": "2509.19285", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2509.19285", "abs": "https://arxiv.org/abs/2509.19285", "authors": ["Wenderson Gomes Barbosa", "Kerolly Kedma Felix do Nascimento", "Fabio Sandro dos Santos", "Tiago A. E. Ferreira"], "title": "The information flow among Green Bonds exchange traded funds", "comment": "27 pages,13 figures", "summary": "This article investigates the information flow between 13 Green Bond ETFs\n(Exchange Traded Funds) from three global markets: the USA, Canada,and Europe,\nbetween 2021 and 2022. We used the transfer entropy and effective transfer\nentropy methods to model and investigate the Green Bond price information flow\nbetween these global markets. The American market demonstrated market dominance\namong the other two markets (Canadian and European). The FLMB Green Bond of the\nAmerican ETF presented the greatest flow of information transfer among the ETFs\nanalyzed, being considered the dominant ETF among the three Green Bond ETF\nmarkets investigated. The HGGB ETF has emerged as a major information\ntransmitter in Europe and in the Canadian market, but it has had a strong\ninfluence from the American ETF FLMB. In the European market, the FLRG and\nGRON.MI bonds played a major role in the flow of information sent to other ETFs\nin Europe. The KLMH.F in Europe is highlighted as the largest receiver of\ninformation. Thus, through this article, it was possible to understand the\ndirection of the flow of information between the Green Bond ETF markets and\ntheir dimensionality.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e862021-2022\u5e74\u95f4\u7f8e\u56fd\u3001\u52a0\u62ff\u5927\u548c\u6b27\u6d32\u4e09\u4e2a\u5168\u7403\u5e02\u573a\u4e2d13\u53ea\u7eff\u8272\u503a\u5238ETF\u4e4b\u95f4\u7684\u4fe1\u606f\u6d41\u52a8\uff0c\u53d1\u73b0\u7f8e\u56fd\u5e02\u573a\u5728\u4fe1\u606f\u4f20\u9012\u4e2d\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\u3002", "motivation": "\u4e86\u89e3\u5168\u7403\u7eff\u8272\u503a\u5238ETF\u5e02\u573a\u4e4b\u95f4\u7684\u4fe1\u606f\u6d41\u52a8\u65b9\u5411\u548c\u7ef4\u5ea6\uff0c\u8bc6\u522b\u5404\u5e02\u573a\u4e2d\u7684\u4e3b\u5bfcETF\u548c\u4fe1\u606f\u4f20\u9012\u6a21\u5f0f\u3002", "method": "\u4f7f\u7528\u8f6c\u79fb\u71b5\u548c\u6709\u6548\u8f6c\u79fb\u71b5\u65b9\u6cd5\u6765\u5efa\u6a21\u548c\u5206\u6790\u7eff\u8272\u503a\u5238\u4ef7\u683c\u4fe1\u606f\u5728\u4e0d\u540c\u5168\u7403\u5e02\u573a\u4e4b\u95f4\u7684\u6d41\u52a8\u3002", "result": "\u7f8e\u56fd\u5e02\u573a\u5728\u4e09\u4e2a\u5e02\u573a\u4e2d\u8868\u73b0\u4e3b\u5bfc\u5730\u4f4d\uff1bFLMB ETF\u662f\u4fe1\u606f\u4f20\u9012\u6700\u6d3b\u8dc3\u7684ETF\uff1b\u6b27\u6d32\u5e02\u573a\u7684FLRG\u548cGRON.MI\u503a\u5238\u5728\u6b27\u6d32\u5185\u90e8\u4fe1\u606f\u4f20\u9012\u4e2d\u8d77\u4e3b\u8981\u4f5c\u7528\uff1bKLMH.F\u662f\u6700\u5927\u7684\u4fe1\u606f\u63a5\u6536\u8005\u3002", "conclusion": "\u901a\u8fc7\u7814\u7a76\u660e\u786e\u4e86\u7eff\u8272\u503a\u5238ETF\u5e02\u573a\u95f4\u4fe1\u606f\u6d41\u52a8\u7684\u65b9\u5411\u548c\u7ef4\u5ea6\uff0c\u4e3a\u7406\u89e3\u5168\u7403\u7eff\u8272\u503a\u5238\u5e02\u573a\u7684\u8054\u52a8\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2509.18906", "categories": ["cs.ET", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.18906", "abs": "https://arxiv.org/abs/2509.18906", "authors": ["Kyriakos Stylianopoulos", "George C. Alexandropoulos"], "title": "Integrating Stacked Intelligent Metasurfaces and Power Control for Dynamic Edge Inference via Over-The-Air Neural Networks", "comment": "Submitted to IEEE ICASSP 2026", "summary": "This paper introduces a novel framework for Edge Inference (EI) that bypasses\nthe conventional practice of treating the wireless channel as noise. We utilize\nStacked Intelligent Metasurfaces (SIMs) to control wireless propagation,\nenabling the channel itself to perform over-the-air computation. This\neliminates the need for symbol estimation at the receiver, significantly\nreducing computational and communication overhead. Our approach models the\ntransmitter-channel-receiver system as an end-to-end Deep Neural Network (DNN)\nwhere the response of the SIM elements are trainable parameters. To address\nchannel variability, we incorporate a dedicated DNN module responsible for\ndynamically adjusting transmission power leveraging user location information.\nOur performance evaluations showcase that the proposed metasurfaces-integrated\nDNN framework with deep SIM architectures are capable of balancing\nclassification accuracy and power consumption under diverse scenarios, offering\nsignificant energy efficiency improvements.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8fb9\u7f18\u63a8\u7406\u6846\u67b6\uff0c\u5229\u7528\u5806\u53e0\u667a\u80fd\u8d85\u8868\u9762\u63a7\u5236\u65e0\u7ebf\u4f20\u64ad\uff0c\u4f7f\u4fe1\u9053\u672c\u8eab\u80fd\u591f\u6267\u884c\u7a7a\u4e2d\u8ba1\u7b97\uff0c\u4ece\u800c\u907f\u514d\u63a5\u6536\u7aef\u7684\u7b26\u53f7\u4f30\u8ba1\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u548c\u901a\u4fe1\u5f00\u9500\u3002", "motivation": "\u4f20\u7edf\u7684\u8fb9\u7f18\u63a8\u7406\u65b9\u6cd5\u5c06\u65e0\u7ebf\u4fe1\u9053\u89c6\u4e3a\u566a\u58f0\uff0c\u5b58\u5728\u8ba1\u7b97\u548c\u901a\u4fe1\u5f00\u9500\u5927\u7684\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5229\u7528\u667a\u80fd\u8d85\u8868\u9762\u6280\u672f\uff0c\u8ba9\u65e0\u7ebf\u4fe1\u9053\u76f4\u63a5\u53c2\u4e0e\u8ba1\u7b97\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u80fd\u6548\u3002", "method": "\u5c06\u53d1\u5c04\u5668-\u4fe1\u9053-\u63a5\u6536\u5668\u7cfb\u7edf\u5efa\u6a21\u4e3a\u7aef\u5230\u7aef\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u5176\u4e2d\u8d85\u8868\u9762\u5143\u4ef6\u7684\u54cd\u5e94\u4f5c\u4e3a\u53ef\u8bad\u7ec3\u53c2\u6570\u3002\u8fd8\u5f15\u5165\u4e13\u95e8\u7684DNN\u6a21\u5757\u6839\u636e\u7528\u6237\u4f4d\u7f6e\u4fe1\u606f\u52a8\u6001\u8c03\u6574\u4f20\u8f93\u529f\u7387\u4ee5\u5e94\u5bf9\u4fe1\u9053\u53d8\u5316\u3002", "result": "\u6027\u80fd\u8bc4\u4f30\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u96c6\u6210\u8d85\u8868\u9762\u7684DNN\u6846\u67b6\u80fd\u591f\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u5e73\u8861\u5206\u7c7b\u7cbe\u5ea6\u548c\u529f\u8017\uff0c\u63d0\u4f9b\u663e\u8457\u7684\u80fd\u6548\u6539\u8fdb\u3002", "conclusion": "\u57fa\u4e8e\u667a\u80fd\u8d85\u8868\u9762\u7684\u8fb9\u7f18\u63a8\u7406\u6846\u67b6\u80fd\u591f\u6709\u6548\u5229\u7528\u65e0\u7ebf\u4fe1\u9053\u8fdb\u884c\u8ba1\u7b97\uff0c\u663e\u8457\u964d\u4f4e\u7cfb\u7edf\u5f00\u9500\uff0c\u4e3a\u8fb9\u7f18\u8ba1\u7b97\u63d0\u4f9b\u4e86\u65b0\u7684\u80fd\u6548\u4f18\u5316\u65b9\u6848\u3002"}}
{"id": "2509.18330", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18330", "abs": "https://arxiv.org/abs/2509.18330", "authors": ["Marsette Vona"], "title": "The Landform Contextual Mesh: Automatically Fusing Surface and Orbital Terrain for Mars 2020", "comment": null, "summary": "The Landform contextual mesh fuses 2D and 3D data from up to thousands of\nMars 2020 rover images, along with orbital elevation and color maps from Mars\nReconnaissance Orbiter, into an interactive 3D terrain visualization.\nContextual meshes are built automatically for each rover location during\nmission ground data system processing, and are made available to mission\nscientists for tactical and strategic planning in the Advanced Science\nTargeting Tool for Robotic Operations (ASTTRO). A subset of them are also\ndeployed to the \"Explore with Perseverance\" public access website.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86Landform\u4e0a\u4e0b\u6587\u7f51\u683c\u6280\u672f\uff0c\u8be5\u6280\u672f\u878d\u5408\u4e86\u706b\u661f2020\u63a2\u6d4b\u8f66\u62cd\u6444\u7684\u6570\u5343\u5f202D\u548c3D\u56fe\u50cf\u6570\u636e\uff0c\u4ee5\u53ca\u706b\u661f\u52d8\u6d4b\u8f68\u9053\u98de\u884c\u5668\u7684\u8f68\u9053\u9ad8\u7a0b\u548c\u5f69\u8272\u5730\u56fe\uff0c\u521b\u5efa\u4ea4\u4e92\u5f0f3D\u5730\u5f62\u53ef\u89c6\u5316\u3002", "motivation": "\u4e3a\u706b\u661f\u4efb\u52a1\u79d1\u5b66\u5bb6\u63d0\u4f9b\u6218\u672f\u548c\u6218\u7565\u89c4\u5212\u5de5\u5177\uff0c\u540c\u65f6\u5411\u516c\u4f17\u63d0\u4f9b\u706b\u661f\u63a2\u7d22\u7684\u53ef\u89c6\u5316\u4f53\u9a8c\u3002", "method": "\u901a\u8fc7\u81ea\u52a8\u5904\u7406\u63a2\u6d4b\u8f66\u6bcf\u4e2a\u4f4d\u7f6e\u76842D\u548c3D\u56fe\u50cf\u6570\u636e\uff0c\u7ed3\u5408\u8f68\u9053\u9ad8\u7a0b\u548c\u5f69\u8272\u5730\u56fe\uff0c\u6784\u5efa\u4e0a\u4e0b\u6587\u7f51\u683c\u3002", "result": "\u6210\u529f\u5f00\u53d1\u4e86\u81ea\u52a8\u5316\u7684\u4e0a\u4e0b\u6587\u7f51\u683c\u751f\u6210\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5df2\u96c6\u6210\u5230ASTTRO\u5de5\u5177\u4e2d\u4f9b\u79d1\u5b66\u5bb6\u4f7f\u7528\uff0c\u90e8\u5206\u7f51\u683c\u4e5f\u5728\u516c\u5171\u7f51\u7ad9\u4e0a\u53d1\u5e03\u3002", "conclusion": "Landform\u4e0a\u4e0b\u6587\u7f51\u683c\u6280\u672f\u6709\u6548\u652f\u6301\u4e86\u706b\u661f\u63a2\u6d4b\u4efb\u52a1\u7684\u79d1\u5b66\u89c4\u5212\u548c\u516c\u4f17\u53c2\u4e0e\uff0c\u5c55\u793a\u4e86\u591a\u6e90\u6570\u636e\u878d\u5408\u5728\u884c\u661f\u63a2\u7d22\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.18346", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18346", "abs": "https://arxiv.org/abs/2509.18346", "authors": ["M Parimi", "Rachit Mehra", "S. R. Wagh", "Amol Yerudkar", "Navdeep Singh"], "title": "On the Dynamics of Acceleration in First order Gradient Methods", "comment": null, "summary": "Ever since the original algorithm by Nesterov (1983), the true nature of the\nacceleration phenomenon has remained elusive, with various interpretations of\nwhy the method is actually faster. The diagnosis of the algorithm through the\nlens of Ordinary Differential Equations (ODEs) and the corresponding dynamical\nsystem formulation to explain the underlying dynamics has a rich history. In\nthe literature, the ODEs that explain algorithms are typically derived by\nconsidering the limiting case of the algorithm maps themselves, that is, an ODE\nformulation follows the development of an algorithm. This obfuscates the\nunderlying higher order principles and thus provides little evidence of the\nworking of the algorithm. Such has been the case with Nesterov algorithm and\nthe various analogies used to describe the acceleration phenomena, viz,\nmomentum associated with the rolling of a Heavy-Ball down a slope, Hessian\ndamping etc. The main focus of our work is to ideate the genesis of the\nNesterov algorithm from the viewpoint of dynamical systems leading to\ndemystifying the mathematical rigour behind the algorithm. Instead of reverse\nengineering ODEs from discrete algorithms, this work explores tools from the\nrecently developed control paradigm titled Passivity and Immersion approach and\nthe Geometric Singular Perturbation theory which are applied to arrive at the\nformulation of a dynamical system that explains and models the acceleration\nphenomena. This perspective helps to gain insights into the various terms\npresent and the sequence of steps used in Nesterovs accelerated algorithm for\nthe smooth strongly convex and the convex case. The framework can also be\nextended to derive the acceleration achieved using the triple momentum method\nand provides justifications for the non-convergence to the optimal solution in\nthe Heavy-Ball method.", "AI": {"tldr": "\u672c\u6587\u4ece\u52a8\u529b\u7cfb\u7edf\u89c6\u89d2\u91cd\u65b0\u5ba1\u89c6Nesterov\u52a0\u901f\u7b97\u6cd5\uff0c\u4f7f\u7528\u63a7\u5236\u7406\u8bba\u548c\u51e0\u4f55\u5947\u5f02\u6444\u52a8\u7406\u8bba\u6765\u89e3\u91ca\u52a0\u901f\u73b0\u8c61\u7684\u672c\u8d28\uff0c\u63ed\u793a\u4e86\u7b97\u6cd5\u4e2d\u5404\u9879\u7684\u7269\u7406\u610f\u4e49\uff0c\u5e76\u89e3\u91ca\u4e86Heavy-Ball\u65b9\u6cd5\u4e0d\u6536\u655b\u7684\u539f\u56e0\u3002", "motivation": "\u4f20\u7edf\u4e0a\u901a\u8fc7ODE\u53cd\u5411\u63a8\u5bfc\u7b97\u6cd5\u7684\u65b9\u6cd5\u63a9\u76d6\u4e86\u9ad8\u9636\u539f\u7406\uff0c\u65e0\u6cd5\u771f\u6b63\u89e3\u91caNesterov\u7b97\u6cd5\u7684\u52a0\u901f\u673a\u5236\u3002\u672c\u6587\u65e8\u5728\u4ece\u52a8\u529b\u7cfb\u7edf\u89d2\u5ea6\u91cd\u65b0\u6784\u5efa\u7406\u8bba\u6846\u67b6\uff0c\u63ed\u793a\u7b97\u6cd5\u80cc\u540e\u7684\u6570\u5b66\u4e25\u8c28\u6027\u3002", "method": "\u91c7\u7528\u65b0\u8fd1\u53d1\u5c55\u7684\u63a7\u5236\u8303\u5f0f\uff08\u65e0\u6e90\u6027\u548c\u6d78\u5165\u65b9\u6cd5\uff09\u4ee5\u53ca\u51e0\u4f55\u5947\u5f02\u6444\u52a8\u7406\u8bba\uff0c\u6784\u5efa\u80fd\u591f\u89e3\u91ca\u52a0\u901f\u73b0\u8c61\u7684\u52a8\u529b\u7cfb\u7edf\u6a21\u578b\uff0c\u800c\u4e0d\u662f\u4ece\u79bb\u6563\u7b97\u6cd5\u53cd\u5411\u63a8\u5bfcODE\u3002", "result": "\u8be5\u6846\u67b6\u6210\u529f\u89e3\u91ca\u4e86Nesterov\u52a0\u901f\u7b97\u6cd5\u4e2d\u5404\u9879\u7684\u7269\u7406\u610f\u4e49\u548c\u6b65\u9aa4\u5e8f\u5217\uff0c\u53ef\u6269\u5c55\u5230\u4e09\u52a8\u91cf\u65b9\u6cd5\uff0c\u5e76\u4e3aHeavy-Ball\u65b9\u6cd5\u4e0d\u6536\u655b\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002", "conclusion": "\u4ece\u52a8\u529b\u7cfb\u7edf\u89c6\u89d2\u51fa\u53d1\u7684\u65b0\u65b9\u6cd5\u4e3a\u7406\u89e3\u4f18\u5316\u7b97\u6cd5\u7684\u52a0\u901f\u73b0\u8c61\u63d0\u4f9b\u4e86\u66f4\u6df1\u523b\u7684\u6570\u5b66\u6d1e\u5bdf\uff0c\u63ed\u793a\u4e86\u4f20\u7edfODE\u65b9\u6cd5\u6240\u63a9\u76d6\u7684\u9ad8\u9636\u539f\u7406\u3002"}}
{"id": "2509.18985", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2509.18985", "abs": "https://arxiv.org/abs/2509.18985", "authors": ["Elisa Composta", "Nicolo' Fontana", "Francesco Corso", "Francesco Pierri"], "title": "Simulating Online Social Media Conversations on Controversial Topics Using AI Agents Calibrated on Real-World Data", "comment": null, "summary": "Online social networks offer a valuable lens to analyze both individual and\ncollective phenomena. Researchers often use simulators to explore controlled\nscenarios, and the integration of Large Language Models (LLMs) makes these\nsimulations more realistic by enabling agents to understand and generate\nnatural language content. In this work, we investigate the behavior of\nLLM-based agents in a simulated microblogging social network. We initialize\nagents with realistic profiles calibrated on real-world online conversations\nfrom the 2022 Italian political election and extend an existing simulator by\nintroducing mechanisms for opinion modeling. We examine how LLM agents simulate\nonline conversations, interact with others, and evolve their opinions under\ndifferent scenarios. Our results show that LLM agents generate coherent\ncontent, form connections, and build a realistic social network structure.\nHowever, their generated content displays less heterogeneity in tone and\ntoxicity compared to real data. We also find that LLM-based opinion dynamics\nevolve over time in ways similar to traditional mathematical models. Varying\nparameter configurations produces no significant changes, indicating that\nsimulations require more careful cognitive modeling at initialization to\nreplicate human behavior more faithfully. Overall, we demonstrate the potential\nof LLMs for simulating user behavior in social environments, while also\nidentifying key challenges in capturing heterogeneity and complex dynamics.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u5728\u6a21\u62df\u5fae\u535a\u793e\u4ea4\u7f51\u7edc\u4e2d\u7684\u884c\u4e3a\uff0c\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u653f\u6cbb\u5bf9\u8bdd\u6570\u636e\u521d\u59cb\u5316\u667a\u80fd\u4f53\uff0c\u53d1\u73b0LLM\u80fd\u751f\u6210\u8fde\u8d2f\u5185\u5bb9\u5e76\u6784\u5efa\u73b0\u5b9e\u793e\u4ea4\u7f51\u7edc\u7ed3\u6784\uff0c\u4f46\u751f\u6210\u5185\u5bb9\u5728\u8bed\u6c14\u548c\u6bd2\u6027\u65b9\u9762\u5f02\u8d28\u6027\u4e0d\u8db3\uff0c\u4e14\u610f\u89c1\u52a8\u6001\u4e0e\u4f20\u7edf\u6570\u5b66\u6a21\u578b\u76f8\u4f3c\u3002", "motivation": "\u5728\u7ebf\u793e\u4ea4\u7f51\u7edc\u4e3a\u5206\u6790\u4e2a\u4f53\u548c\u96c6\u4f53\u73b0\u8c61\u63d0\u4f9b\u4e86\u5b9d\u8d35\u89c6\u89d2\uff0c\u901a\u8fc7\u5c06LLM\u96c6\u6210\u5230\u6a21\u62df\u5668\u4e2d\u53ef\u4ee5\u4f7f\u6a21\u62df\u66f4\u52a0\u771f\u5b9e\uff0c\u8ba9\u667a\u80fd\u4f53\u80fd\u591f\u7406\u89e3\u548c\u751f\u6210\u81ea\u7136\u8bed\u8a00\u5185\u5bb9\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u610f\u5927\u52292022\u5e74\u653f\u6cbb\u9009\u4e3e\u5728\u7ebf\u5bf9\u8bdd\u7684\u914d\u7f6e\u6587\u4ef6\u521d\u59cb\u5316LLM\u667a\u80fd\u4f53\uff0c\u6269\u5c55\u73b0\u6709\u6a21\u62df\u5668\u5f15\u5165\u610f\u89c1\u5efa\u6a21\u673a\u5236\uff0c\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7814\u7a76LLM\u667a\u80fd\u4f53\u7684\u5bf9\u8bdd\u6a21\u62df\u3001\u4e92\u52a8\u548c\u610f\u89c1\u6f14\u5316\u3002", "result": "LLM\u667a\u80fd\u4f53\u751f\u6210\u8fde\u8d2f\u5185\u5bb9\u3001\u5f62\u6210\u8fde\u63a5\u5e76\u6784\u5efa\u73b0\u5b9e\u793e\u4ea4\u7f51\u7edc\u7ed3\u6784\uff0c\u4f46\u751f\u6210\u5185\u5bb9\u5728\u8bed\u6c14\u548c\u6bd2\u6027\u65b9\u9762\u7684\u5f02\u8d28\u6027\u4f4e\u4e8e\u771f\u5b9e\u6570\u636e\uff0cLLM\u610f\u89c1\u52a8\u6001\u4e0e\u4f20\u7edf\u6570\u5b66\u6a21\u578b\u6f14\u5316\u65b9\u5f0f\u76f8\u4f3c\uff0c\u53c2\u6570\u53d8\u5316\u672a\u4ea7\u751f\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "LLM\u5728\u6a21\u62df\u793e\u4ea4\u73af\u5883\u4e2d\u7528\u6237\u884c\u4e3a\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5728\u6355\u6349\u5f02\u8d28\u6027\u548c\u590d\u6742\u52a8\u6001\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u8ba4\u77e5\u5efa\u6a21\u6765\u66f4\u51c6\u786e\u5730\u590d\u5236\u4eba\u7c7b\u884c\u4e3a\u3002"}}
{"id": "2509.18168", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18168", "abs": "https://arxiv.org/abs/2509.18168", "authors": ["Dong Liu", "Yanxuan Yu"], "title": "HSGM: Hierarchical Segment-Graph Memory for Scalable Long-Text Semantics", "comment": null, "summary": "Semantic parsing of long documents remains challenging due to quadratic\ngrowth in pairwise composition and memory requirements. We introduce\n\\textbf{Hierarchical Segment-Graph Memory (HSGM)}, a novel framework that\ndecomposes an input of length $N$ into $M$ meaningful segments, constructs\n\\emph{Local Semantic Graphs} on each segment, and extracts compact\n\\emph{summary nodes} to form a \\emph{Global Graph Memory}. HSGM supports\n\\emph{incremental updates} -- only newly arrived segments incur local graph\nconstruction and summary-node integration -- while \\emph{Hierarchical Query\nProcessing} locates relevant segments via top-$K$ retrieval over summary nodes\nand then performs fine-grained reasoning within their local graphs.\n  Theoretically, HSGM reduces worst-case complexity from $O(N^2)$ to\n$O\\!\\left(N\\,k + (N/k)^2\\right)$, with segment size $k \\ll N$, and we derive\nFrobenius-norm bounds on the approximation error introduced by node\nsummarization and sparsification thresholds. Empirically, on three benchmarks\n-- long-document AMR parsing, segment-level semantic role labeling (OntoNotes),\nand legal event extraction -- HSGM achieves \\emph{2--4$\\times$ inference\nspeedup}, \\emph{$>60\\%$ reduction} in peak memory, and \\emph{$\\ge 95\\%$} of\nbaseline accuracy. Our approach unlocks scalable, accurate semantic modeling\nfor ultra-long texts, enabling real-time and resource-constrained NLP\napplications.", "AI": {"tldr": "HSGM\u662f\u4e00\u79cd\u5206\u5c42\u6bb5\u56fe\u8bb0\u5fc6\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u957f\u6587\u6863\u5206\u89e3\u4e3a\u6709\u610f\u4e49\u7684\u6bb5\uff0c\u6784\u5efa\u5c40\u90e8\u8bed\u4e49\u56fe\u5e76\u63d0\u53d6\u6458\u8981\u8282\u70b9\u6765\u964d\u4f4e\u8bed\u4e49\u89e3\u6790\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5185\u5b58\u9700\u6c42\u3002", "motivation": "\u957f\u6587\u6863\u8bed\u4e49\u89e3\u6790\u9762\u4e34\u4e8c\u6b21\u590d\u6742\u5ea6\u589e\u957f\u548c\u5185\u5b58\u9700\u6c42\u8fc7\u9ad8\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u6765\u652f\u6301\u8d85\u957f\u6587\u672c\u7684\u5b9e\u65f6\u5904\u7406\u3002", "method": "\u5c06\u8f93\u5165\u6587\u6863\u5206\u89e3\u4e3aM\u4e2a\u6bb5\uff0c\u5728\u6bcf\u4e2a\u6bb5\u4e0a\u6784\u5efa\u5c40\u90e8\u8bed\u4e49\u56fe\uff0c\u63d0\u53d6\u6458\u8981\u8282\u70b9\u5f62\u6210\u5168\u5c40\u56fe\u8bb0\u5fc6\uff0c\u652f\u6301\u589e\u91cf\u66f4\u65b0\u548c\u5206\u5c42\u67e5\u8be2\u5904\u7406\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHSGM\u5b9e\u73b0\u4e862-4\u500d\u63a8\u7406\u52a0\u901f\uff0c\u5cf0\u503c\u5185\u5b58\u51cf\u5c11>60%\uff0c\u51c6\u786e\u7387\u4fdd\u6301\u57fa\u7ebf95%\u4ee5\u4e0a\u3002", "conclusion": "HSGM\u4e3a\u8d85\u957f\u6587\u672c\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u51c6\u786e\u7684\u8bed\u4e49\u5efa\u6a21\u65b9\u6cd5\uff0c\u652f\u6301\u5b9e\u65f6\u548c\u8d44\u6e90\u53d7\u9650\u7684NLP\u5e94\u7528\u3002"}}
{"id": "2509.18211", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2509.18211", "abs": "https://arxiv.org/abs/2509.18211", "authors": ["Ronan \u00d3 Fathaigh", "Tom Dobber", "Frederik Zuiderveen Borgesius", "James Shires"], "title": "Microtargeted propaganda by foreign actors: An interdisciplinary exploration", "comment": null, "summary": "This article discusses a problem that has received scant attention in\nliterature: microtargeted propaganda by foreign actors. Microtargeting involves\ncollecting information about people, and using that information to show them\ntargeted political advertisements. Such microtargeting enables advertisers to\ntarget ads to specific groups of people, for instance people who visit certain\nwebsites, forums, or Facebook groups. This article focuses on one type of\nmicrotargeting: microtargeting by foreign actors. For example, Russia has\ntargeted certain groups in the US with ads, aiming to sow discord. Foreign\nactors could also try to influence European elections, for instance by\nadvertising in favour of a certain political party. Foreign propaganda\npossibilities existed before microtargeting. This article explores two\nquestions. In what ways, if any, is microtargeted propaganda by foreign actors\ndifferent from other foreign propaganda? What could lawmakers in Europe do to\nmitigate the risks of microtargeted propaganda?", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5916\u56fd\u884c\u4e3a\u4f53\u8fdb\u884c\u5fae\u89c2\u5b9a\u5411\u5ba3\u4f20\u7684\u95ee\u9898\uff0c\u5206\u6790\u4e86\u5176\u4e0e\u4f20\u7edf\u5ba3\u4f20\u7684\u533a\u522b\uff0c\u5e76\u63d0\u51fa\u4e86\u6b27\u6d32\u7acb\u6cd5\u8005\u7684\u5e94\u5bf9\u63aa\u65bd\u3002", "motivation": "\u7814\u7a76\u5916\u56fd\u884c\u4e3a\u4f53\u5229\u7528\u5fae\u89c2\u5b9a\u5411\u6280\u672f\u8fdb\u884c\u653f\u6cbb\u5ba3\u4f20\u7684\u65b0\u73b0\u8c61\uff0c\u8fd9\u79cd\u6280\u672f\u80fd\u591f\u9488\u5bf9\u7279\u5b9a\u4eba\u7fa4\u6295\u653e\u7cbe\u51c6\u5e7f\u544a\uff0c\u53ef\u80fd\u5bf9\u6c11\u4e3b\u9009\u4e3e\u4ea7\u751f\u5a01\u80c1\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4fc4\u7f57\u65af\u5728\u7f8e\u56fd\u7684\u5fae\u89c2\u5b9a\u5411\u5ba3\u4f20\u6848\u4f8b\uff0c\u6bd4\u8f83\u5fae\u89c2\u5b9a\u5411\u5ba3\u4f20\u4e0e\u4f20\u7edf\u5ba3\u4f20\u7684\u533a\u522b\uff0c\u63a2\u8ba8\u7acb\u6cd5\u5e94\u5bf9\u63aa\u65bd\u3002", "result": "\u53d1\u73b0\u5fae\u89c2\u5b9a\u5411\u5ba3\u4f20\u5177\u6709\u66f4\u9ad8\u7684\u7cbe\u51c6\u6027\u548c\u9690\u853d\u6027\uff0c\u4f20\u7edf\u6cd5\u5f8b\u6846\u67b6\u96be\u4ee5\u6709\u6548\u76d1\u7ba1\uff0c\u9700\u8981\u65b0\u7684\u7acb\u6cd5\u624b\u6bb5\u3002", "conclusion": "\u6b27\u6d32\u7acb\u6cd5\u8005\u9700\u8981\u5236\u5b9a\u4e13\u95e8\u6cd5\u89c4\u6765\u5e94\u5bf9\u5fae\u89c2\u5b9a\u5411\u5ba3\u4f20\u7684\u5a01\u80c1\uff0c\u4fdd\u62a4\u6c11\u4e3b\u8fdb\u7a0b\u4e0d\u53d7\u5916\u56fd\u5e72\u9884\u3002"}}
{"id": "2509.19088", "categories": ["cs.CY", "cs.AI", "cs.HC", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.19088", "abs": "https://arxiv.org/abs/2509.19088", "authors": ["Tiany Peng", "George Gui", "Daniel J. Merlau", "Grace Jiarui Fan", "Malek Ben Sliman", "Melanie Brucks", "Eric J. Johnson", "Vicki Morwitz", "Abdullah Althenayyan", "Silvia Bellezza", "Dante Donati", "Hortense Fong", "Elizabeth Friedman", "Ariana Guevara", "Mohamed Hussein", "Kinshuk Jerath", "Bruce Kogut", "Kristen Lane", "Hannah Li", "Patryk Perkowski", "Oded Netzer", "Olivier Toubia"], "title": "A Mega-Study of Digital Twins Reveals Strengths, Weaknesses and Opportunities for Further Improvement", "comment": null, "summary": "Do \"digital twins\" capture individual responses in surveys and experiments?\nWe run 19 pre-registered studies on a national U.S. panel and their LLM-powered\ndigital twins (constructed based on previously-collected extensive\nindividual-level data) and compare twin and human answers across 164 outcomes.\nThe correlation between twin and human answers is modest (approximately 0.2 on\naverage) and twin responses are less variable than human responses. While\nconstructing digital twins based on rich individual-level data improves our\nability to capture heterogeneity across participants and predict relative\ndifferences between them, it does not substantially improve our ability to\npredict the exact answers given by specific participants or enhance predictions\nof population means. Twin performance varies by domain and is higher among more\neducated, higher-income, and ideologically moderate participants. These results\nsuggest current digital twins can capture some degree of relative differences\nbut are unreliable for individual-level predictions and sample mean and\nvariance estimation, underscoring the need for careful validation before use.\nOur data and code are publicly available for researchers and practitioners\ninterested in optimizing digital twin pipelines.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc719\u9879\u9884\u6ce8\u518c\u7814\u7a76\u6bd4\u8f83\u4e86\u57fa\u4e8eLLM\u7684\u6570\u5b57\u5b6a\u751f\u4e0e\u771f\u5b9e\u4eba\u7c7b\u5728\u8c03\u67e5\u548c\u5b9e\u9a8c\u4e2d\u7684\u56de\u7b54\u5dee\u5f02\uff0c\u53d1\u73b0\u6570\u5b57\u5b6a\u751f\u4e0e\u4eba\u7c7b\u56de\u7b54\u7684\u76f8\u5173\u6027\u4ec5\u4e3a\u7ea60.2\uff0c\u4e14\u6570\u5b57\u5b6a\u751f\u7684\u56de\u7b54\u53d8\u5f02\u6027\u8f83\u4f4e\u3002", "motivation": "\u9a8c\u8bc1\u5f53\u524d\u57fa\u4e8eLLM\u7684\u6570\u5b57\u5b6a\u751f\u6280\u672f\u662f\u5426\u80fd\u51c6\u786e\u6355\u6349\u4e2a\u4f53\u5728\u8c03\u67e5\u548c\u5b9e\u9a8c\u4e2d\u7684\u771f\u5b9e\u53cd\u5e94\uff0c\u8bc4\u4f30\u5176\u5728\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "\u5728\u7f8e\u56fd\u5168\u56fd\u9762\u677f\u4e0a\u8fd0\u884c19\u9879\u9884\u6ce8\u518c\u7814\u7a76\uff0c\u57fa\u4e8e\u5148\u524d\u6536\u96c6\u7684\u5e7f\u6cdb\u4e2a\u4f53\u6570\u636e\u6784\u5efaLLM\u9a71\u52a8\u7684\u6570\u5b57\u5b6a\u751f\uff0c\u6bd4\u8f83\u5b6a\u751f\u4e0e\u4eba\u7c7b\u5728164\u4e2a\u7ed3\u679c\u4e0a\u7684\u56de\u7b54\u3002", "result": "\u6570\u5b57\u5b6a\u751f\u4e0e\u4eba\u7c7b\u56de\u7b54\u7684\u76f8\u5173\u6027\u4e2d\u7b49\uff08\u7ea60.2\uff09\uff0c\u56de\u7b54\u53d8\u5f02\u6027\u4f4e\u4e8e\u4eba\u7c7b\u3002\u867d\u7136\u57fa\u4e8e\u4e30\u5bcc\u4e2a\u4f53\u6570\u636e\u6784\u5efa\u7684\u6570\u5b57\u5b6a\u751f\u80fd\u66f4\u597d\u5730\u6355\u6349\u53c2\u4e0e\u8005\u95f4\u7684\u5f02\u8d28\u6027\uff0c\u4f46\u65e0\u6cd5\u51c6\u786e\u9884\u6d4b\u7279\u5b9a\u53c2\u4e0e\u8005\u7684\u786e\u5207\u56de\u7b54\u6216\u63d0\u9ad8\u5bf9\u603b\u4f53\u5747\u503c\u7684\u9884\u6d4b\u80fd\u529b\u3002", "conclusion": "\u5f53\u524d\u6570\u5b57\u5b6a\u751f\u6280\u672f\u80fd\u6355\u6349\u4e00\u5b9a\u7a0b\u5ea6\u7684\u76f8\u5bf9\u5dee\u5f02\uff0c\u4f46\u5728\u4e2a\u4f53\u5c42\u9762\u9884\u6d4b\u548c\u6837\u672c\u5747\u503c\u65b9\u5dee\u4f30\u8ba1\u65b9\u9762\u4e0d\u53ef\u9760\uff0c\u4f7f\u7528\u524d\u9700\u8981\u8c28\u614e\u9a8c\u8bc1\u3002"}}
{"id": "2509.19257", "categories": ["cs.ET", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.19257", "abs": "https://arxiv.org/abs/2509.19257", "authors": ["Juan E. Gilbert", "Jean D. Louis"], "title": "A Stateless Transparent Voting Machine", "comment": "11 pages, 2 figures", "summary": "Transparency and security are essential in our voting system, and voting\nmachines. This paper describes an implementation of a stateless, transparent\nvoting machine (STVM). The STVM is a ballot marking device (BMD) that uses a\ntransparent, interactive printing interface where voters can verify their paper\nballots as they fill out the ballot. The transparent interface turns the paper\nballot into an interactive interface. In this architecture, stateless describes\nthe machine's boot sequence, where no information is stored or passed forward\nbetween reboots. The machine does not have a hard drive. Instead, it boots and\nruns from read-only media. This STVM design utilizes a Blu-ray Disc ROM (BD-R)\nto boot the voting software. This system's statelessness and the transparent\ninteractive printing interface make this design the most secure BMD for voting.\nUnlike other voting methods, this system incorporates high usability,\naccessibility, and security for all voters. The STVM uses an open-source voting\nsystem that has a universally designed interface, making the system accessible\nfor all voters independent of their ability or disability. This system can make\nvoting safer by simultaneously addressing the issue of voters noticing a vote\nflip and making it difficult for a hack to persist or go unmitigated.", "AI": {"tldr": "\u672c\u6587\u5b9e\u73b0\u4e86\u4e00\u79cd\u65e0\u72b6\u6001\u3001\u900f\u660e\u7684\u6295\u7968\u673a\uff08STVM\uff09\uff0c\u8be5\u8bbe\u5907\u4f7f\u7528\u900f\u660e\u7684\u4ea4\u4e92\u5f0f\u6253\u5370\u754c\u9762\uff0c\u8ba9\u9009\u6c11\u5728\u586b\u5199\u9009\u7968\u65f6\u53ef\u4ee5\u9a8c\u8bc1\u7eb8\u8d28\u9009\u7968\u3002\u7cfb\u7edf\u4ece\u53ea\u8bfb\u5a92\u4f53\u542f\u52a8\uff0c\u4e0d\u5b58\u50a8\u4efb\u4f55\u4fe1\u606f\uff0c\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\u3002", "motivation": "\u6295\u7968\u7cfb\u7edf\u7684\u900f\u660e\u6027\u548c\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u89e3\u51b3\u4f20\u7edf\u6295\u7968\u65b9\u6cd5\u4e2d\u5b58\u5728\u7684\u5b89\u5168\u9690\u60a3\u548c\u53ef\u8bbf\u95ee\u6027\u95ee\u9898\u3002", "method": "\u4f7f\u7528BD-R\u5149\u76d8\u542f\u52a8\u65e0\u72b6\u6001\u6295\u7968\u8f6f\u4ef6\uff0c\u901a\u8fc7\u900f\u660e\u7684\u4ea4\u4e92\u5f0f\u6253\u5370\u754c\u9762\u5c06\u7eb8\u8d28\u9009\u7968\u8f6c\u53d8\u4e3a\u4ea4\u4e92\u754c\u9762\uff0c\u91c7\u7528\u5f00\u6e90\u6295\u7968\u7cfb\u7edf\u5b9e\u73b0\u901a\u7528\u8bbe\u8ba1\u3002", "result": "\u8be5\u8bbe\u8ba1\u7ed3\u5408\u4e86\u9ad8\u53ef\u7528\u6027\u3001\u53ef\u8bbf\u95ee\u6027\u548c\u5b89\u5168\u6027\uff0c\u80fd\u591f\u6709\u6548\u9632\u6b62\u6295\u7968\u7ffb\u8f6c\u95ee\u9898\uff0c\u5e76\u4f7f\u9ed1\u5ba2\u653b\u51fb\u96be\u4ee5\u6301\u7eed\u6216\u4e0d\u88ab\u53d1\u73b0\u3002", "conclusion": "STVM\u8bbe\u8ba1\u662f\u76ee\u524d\u6700\u5b89\u5168\u7684\u6295\u7968\u6807\u8bb0\u8bbe\u5907\uff0c\u80fd\u591f\u4e3a\u6240\u6709\u9009\u6c11\u63d0\u4f9b\u5b89\u5168\u3001\u900f\u660e\u7684\u6295\u7968\u4f53\u9a8c\u3002"}}
{"id": "2509.18342", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18342", "abs": "https://arxiv.org/abs/2509.18342", "authors": ["Rajitha de Silva", "Jonathan Cox", "James R. Heselden", "Marija Popovic", "Cesar Cadena", "Riccardo Polvara"], "title": "Semantic-Aware Particle Filter for Reliable Vineyard Robot Localisation", "comment": "Sumbitted to ICRA 2026", "summary": "Accurate localisation is critical for mobile robots in structured outdoor\nenvironments, yet LiDAR-based methods often fail in vineyards due to repetitive\nrow geometry and perceptual aliasing. We propose a semantic particle filter\nthat incorporates stable object-level detections, specifically vine trunks and\nsupport poles into the likelihood estimation process. Detected landmarks are\nprojected into a birds eye view and fused with LiDAR scans to generate semantic\nobservations. A key innovation is the use of semantic walls, which connect\nadjacent landmarks into pseudo-structural constraints that mitigate row\naliasing. To maintain global consistency in headland regions where semantics\nare sparse, we introduce a noisy GPS prior that adaptively supports the filter.\nExperiments in a real vineyard demonstrate that our approach maintains\nlocalisation within the correct row, recovers from deviations where AMCL fails,\nand outperforms vision-based SLAM methods such as RTAB-Map.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8461\u8404\u56ed\u73af\u5883\u7684\u8bed\u4e49\u7c92\u5b50\u6ee4\u6ce2\u5668\uff0c\u901a\u8fc7\u878d\u5408LiDAR\u626b\u63cf\u548c\u8bed\u4e49\u5730\u6807\uff08\u8461\u8404\u85e4\u6811\u5e72\u548c\u652f\u6491\u6746\uff09\u6765\u89e3\u51b3\u91cd\u590d\u884c\u51e0\u4f55\u7ed3\u6784\u5bfc\u81f4\u7684\u5b9a\u4f4d\u95ee\u9898\uff0c\u4f7f\u7528\u8bed\u4e49\u5899\u548c\u81ea\u9002\u5e94GPS\u5148\u9a8c\u6765\u63d0\u9ad8\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u5728\u7ed3\u6784\u5316\u7684\u6237\u5916\u73af\u5883\u4e2d\uff0c\u7279\u522b\u662f\u8461\u8404\u56ed\u8fd9\u79cd\u5177\u6709\u91cd\u590d\u884c\u51e0\u4f55\u7ed3\u6784\u7684\u73af\u5883\uff0c\u4f20\u7edf\u7684LiDAR\u5b9a\u4f4d\u65b9\u6cd5\u7531\u4e8e\u611f\u77e5\u6df7\u53e0\u800c\u7ecf\u5e38\u5931\u6548\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5229\u7528\u7a33\u5b9a\u7269\u4f53\u7ea7\u68c0\u6d4b\u7684\u5b9a\u4f4d\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86\u8bed\u4e49\u7c92\u5b50\u6ee4\u6ce2\u5668\uff0c\u5c06\u68c0\u6d4b\u5230\u7684\u5730\u6807\uff08\u8461\u8404\u85e4\u6811\u5e72\u548c\u652f\u6491\u6746\uff09\u6295\u5f71\u5230\u9e1f\u77b0\u56fe\u5e76\u4e0eLiDAR\u626b\u63cf\u878d\u5408\u751f\u6210\u8bed\u4e49\u89c2\u6d4b\u3002\u5173\u952e\u521b\u65b0\u662f\u4f7f\u7528\u8bed\u4e49\u5899\u8fde\u63a5\u76f8\u90bb\u5730\u6807\u5f62\u6210\u4f2a\u7ed3\u6784\u7ea6\u675f\uff0c\u5e76\u5728\u8bed\u4e49\u7a00\u758f\u7684\u5934\u5730\u533a\u57df\u5f15\u5165\u81ea\u9002\u5e94GPS\u5148\u9a8c\u6765\u4fdd\u6301\u5168\u5c40\u4e00\u81f4\u6027\u3002", "result": "\u5728\u771f\u5b9e\u8461\u8404\u56ed\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u6b63\u786e\u7684\u884c\u5185\u4fdd\u6301\u5b9a\u4f4d\uff0c\u5728AMCL\u5931\u8d25\u7684\u60c5\u51b5\u4e0b\u4ece\u504f\u5dee\u4e2d\u6062\u590d\uff0c\u5e76\u4e14\u4f18\u4e8e\u57fa\u4e8e\u89c6\u89c9\u7684SLAM\u65b9\u6cd5\u5982RTAB-Map\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8bed\u4e49\u7c92\u5b50\u6ee4\u6ce2\u5668\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u5730\u6807\u548c\u81ea\u9002\u5e94GPS\u5148\u9a8c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8461\u8404\u56ed\u73af\u5883\u4e2d\u7684\u5b9a\u4f4d\u6311\u6218\uff0c\u4e3a\u5728\u5177\u6709\u91cd\u590d\u51e0\u4f55\u7ed3\u6784\u7684\u519c\u4e1a\u73af\u5883\u4e2d\u5b9e\u73b0\u9c81\u68d2\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2509.18356", "categories": ["eess.SY", "cs.NI", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18356", "abs": "https://arxiv.org/abs/2509.18356", "authors": ["Darin Jeff", "Eytan Modiano"], "title": "Optimal Service Mode Assignment in a Simple Computation Offloading System: Extended Version", "comment": null, "summary": "We consider a simple computation offloading model where jobs can either be\nfully processed in the cloud or be partially processed at a local server before\nbeing sent to the cloud to complete processing. Our goal is to design a policy\nfor assigning jobs to service modes, i.e., full offloading or partial\noffloading, based on the state of the system, in order to minimize delay in the\nsystem. We show that when the cloud server is idle, the optimal policy is to\nassign the next job in the system queue to the cloud for processing. However,\nwhen the cloud server is busy, we show that, under mild assumptions, the\noptimal policy is of a threshold type, that sends the next job in the system\nqueue to the local server if the queue exceeds a certain threshold. Finally, we\ndemonstrate this policy structure through simulations.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u8ba1\u7b97\u5378\u8f7d\u7b56\u7565\uff0c\u63d0\u51fa\u5728\u4e91\u670d\u52a1\u5668\u7a7a\u95f2\u65f6\u76f4\u63a5\u5206\u914d\u4efb\u52a1\u5230\u4e91\u7aef\uff0c\u7e41\u5fd9\u65f6\u91c7\u7528\u9608\u503c\u7b56\u7565\u5c06\u4efb\u52a1\u5206\u914d\u5230\u672c\u5730\u670d\u52a1\u5668\u3002", "motivation": "\u8bbe\u8ba1\u57fa\u4e8e\u7cfb\u7edf\u72b6\u6001\u7684\u4efb\u52a1\u5206\u914d\u7b56\u7565\uff0c\u4ee5\u6700\u5c0f\u5316\u7cfb\u7edf\u5ef6\u8fdf\uff0c\u89e3\u51b3\u8ba1\u7b97\u5378\u8f7d\u4e2d\u7684\u4efb\u52a1\u8c03\u5ea6\u95ee\u9898\u3002", "method": "\u5efa\u7acb\u7b80\u5355\u7684\u8ba1\u7b97\u5378\u8f7d\u6a21\u578b\uff0c\u5206\u6790\u4e91\u670d\u52a1\u5668\u7a7a\u95f2\u548c\u7e41\u5fd9\u4e24\u79cd\u72b6\u6001\u4e0b\u7684\u6700\u4f18\u7b56\u7565\uff0c\u901a\u8fc7\u9608\u503c\u7b56\u7565\u8fdb\u884c\u4efb\u52a1\u5206\u914d\u3002", "result": "\u4e91\u670d\u52a1\u5668\u7a7a\u95f2\u65f6\u6700\u4f18\u7b56\u7565\u662f\u76f4\u63a5\u5206\u914d\u4efb\u52a1\u5230\u4e91\u7aef\uff1b\u7e41\u5fd9\u65f6\u6700\u4f18\u7b56\u7565\u662f\u5f53\u7cfb\u7edf\u961f\u5217\u8d85\u8fc7\u9608\u503c\u65f6\u5c06\u4efb\u52a1\u5206\u914d\u5230\u672c\u5730\u670d\u52a1\u5668\u3002", "conclusion": "\u63d0\u51fa\u7684\u9608\u503c\u7b56\u7565\u80fd\u6709\u6548\u4f18\u5316\u7cfb\u7edf\u5ef6\u8fdf\uff0c\u4eff\u771f\u9a8c\u8bc1\u4e86\u8be5\u7b56\u7565\u7ed3\u6784\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.19147", "categories": ["cs.CY", "cs.AI", "cs.SI", "K.4.2"], "pdf": "https://arxiv.org/pdf/2509.19147", "abs": "https://arxiv.org/abs/2509.19147", "authors": ["Madeleine I. G. Daepp", "Alejandro Cuevas", "Robert Osazuwa Ness", "Vickie Yu-Ping Wang", "Bharat Kumar Nayak", "Dibyendu Mishra", "Ti-Chung Cheng", "Shaily Desai", "Joyojeet Pal"], "title": "Generative Propaganda", "comment": "Working Paper", "summary": "Generative propaganda is the use of generative artificial intelligence (AI)\nto shape public opinion. To characterize its use in real-world settings, we\nconducted interviews with defenders (e.g., factcheckers, journalists,\nofficials) in Taiwan and creators (e.g., influencers, political consultants,\nadvertisers) as well as defenders in India, centering two places characterized\nby high levels of online propaganda. The term \"deepfakes\", we find, exerts\noutsized discursive power in shaping defenders' expectations of misuse and, in\nturn, the interventions that are prioritized. To better characterize the space\nof generative propaganda, we develop a taxonomy that distinguishes between\nobvious versus hidden and promotional versus derogatory use. Deception was\nneither the main driver nor the main impact vector of AI's use; instead, Indian\ncreators sought to persuade rather than to deceive, often making AI's use\nobvious in order to reduce legal and reputational risks, while Taiwan's\ndefenders saw deception as a subset of broader efforts to distort the\nprevalence of strategic narratives online. AI was useful and used, however, in\nproducing efficiency gains in communicating across languages and modes, and in\nevading human and algorithmic detection. Security researchers should reconsider\nthreat models to clearly differentiate deepfakes from promotional and obvious\nuses, to complement and bolster the social factors that constrain misuse by\ninternal actors, and to counter efficiency gains globally.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u8bbf\u8c08\u53f0\u6e7e\u548c\u5370\u5ea6\u7684\u76f8\u5173\u4ece\u4e1a\u8005\uff0c\u5206\u6790\u4e86\u751f\u6210\u5f0fAI\u5728\u5ba3\u4f20\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u53d1\u73b0'\u6df1\u5ea6\u4f2a\u9020'\u6982\u5ff5\u8fc7\u5ea6\u5f71\u54cd\u4e86\u9632\u5fa1\u8005\u5bf9\u6ee5\u7528\u7684\u9884\u671f\uff0c\u5e76\u63d0\u51fa\u4e86\u533a\u5206\u660e\u663e/\u9690\u85cf\u3001\u63a8\u5e7f/\u8d2c\u4f4e\u7528\u9014\u7684\u5206\u7c7b\u6cd5\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u4e86\u89e3\u751f\u6210\u5f0fAI\u5728\u771f\u5b9e\u4e16\u754c\u5ba3\u4f20\u4e2d\u7684\u4f7f\u7528\u60c5\u51b5\uff0c\u7279\u522b\u662f\u5728\u53f0\u6e7e\u548c\u5370\u5ea6\u8fd9\u4e24\u4e2a\u7f51\u7edc\u5ba3\u4f20\u9ad8\u53d1\u5730\u533a\uff0c\u4ee5\u66f4\u51c6\u786e\u5730\u63cf\u8ff0\u751f\u6210\u5f0f\u5ba3\u4f20\u7684\u7279\u5f81\u3002", "method": "\u901a\u8fc7\u5bf9\u53f0\u6e7e\u7684\u9632\u5fa1\u8005\uff08\u4e8b\u5b9e\u6838\u67e5\u5458\u3001\u8bb0\u8005\u3001\u5b98\u5458\uff09\u548c\u521b\u4f5c\u8005\uff08\u5f71\u54cd\u8005\u3001\u653f\u6cbb\u987e\u95ee\u3001\u5e7f\u544a\u5546\uff09\u4ee5\u53ca\u5370\u5ea6\u7684\u9632\u5fa1\u8005\u8fdb\u884c\u8bbf\u8c08\uff0c\u6536\u96c6\u7b2c\u4e00\u624b\u8d44\u6599\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6b3a\u9a97\u5e76\u975eAI\u4f7f\u7528\u7684\u4e3b\u8981\u9a71\u52a8\u529b\u6216\u5f71\u54cd\u9014\u5f84\uff0c\u5370\u5ea6\u521b\u4f5c\u8005\u66f4\u503e\u5411\u4e8e\u8bf4\u670d\u800c\u975e\u6b3a\u9a97\uff0c\u53f0\u6e7e\u9632\u5fa1\u8005\u5219\u5c06\u6b3a\u9a97\u89c6\u4e3a\u66f4\u5e7f\u6cdb\u7684\u6218\u7565\u53d9\u4e8b\u626d\u66f2\u7684\u4e00\u90e8\u5206\u3002AI\u4e3b\u8981\u7528\u4e8e\u8de8\u8bed\u8a00\u6c9f\u901a\u548c\u89c4\u907f\u68c0\u6d4b\u7684\u6548\u7387\u63d0\u5347\u3002", "conclusion": "\u5b89\u5168\u7814\u7a76\u4eba\u5458\u5e94\u91cd\u65b0\u8003\u8651\u5a01\u80c1\u6a21\u578b\uff0c\u660e\u786e\u533a\u5206\u6df1\u5ea6\u4f2a\u9020\u4e0e\u63a8\u5e7f\u6027\u660e\u663e\u7528\u9014\uff0c\u8865\u5145\u7ea6\u675f\u5185\u90e8\u884c\u4e3a\u8005\u6ee5\u7528\u7684\u793e\u4f1a\u56e0\u7d20\uff0c\u5e76\u5e94\u5bf9\u5168\u7403\u8303\u56f4\u5185\u7684\u6548\u7387\u63d0\u5347\u6311\u6218\u3002"}}
{"id": "2509.18178", "categories": ["cs.AI", "cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18178", "abs": "https://arxiv.org/abs/2509.18178", "authors": ["Ling Yue", "Nithin Somasekharan", "Tingwen Zhang", "Yadi Cao", "Shaowu Pan"], "title": "Foam-Agent: An End-to-End Composable Multi-Agent Framework for Automating CFD Simulation in OpenFOAM", "comment": null, "summary": "Computational Fluid Dynamics (CFD) is an essential simulation tool in\nengineering, yet its steep learning curve and complex manual setup create\nsignificant barriers. To address these challenges, we introduce Foam-Agent, a\nmulti-agent framework that automates the entire end-to-end OpenFOAM workflow\nfrom a single natural language prompt. Our key innovations address critical\ngaps in existing systems: 1. An Comprehensive End-to-End Simulation Automation:\nFoam-Agent is the first system to manage the full simulation pipeline,\nincluding advanced pre-processing with a versatile Meshing Agent capable of\nhandling external mesh files and generating new geometries via Gmsh, automatic\ngeneration of HPC submission scripts, and post-simulation visualization via\nParaView. 2. Composable Service Architecture: Going beyond a monolithic agent,\nthe framework uses Model Context Protocol (MCP) to expose its core functions as\ndiscrete, callable tools. This allows for flexible integration and use by other\nagentic systems, such as Claude-code, for more exploratory workflows. 3.\nHigh-Fidelity Configuration Generation: We achieve superior accuracy through a\nHierarchical Multi-Index RAG for precise context retrieval and a\ndependency-aware generation process that ensures configuration consistency.\nEvaluated on a benchmark of 110 simulation tasks, Foam-Agent achieves an 88.2%\nsuccess rate with Claude 3.5 Sonnet, significantly outperforming existing\nframeworks (55.5% for MetaOpenFOAM). Foam-Agent dramatically lowers the\nexpertise barrier for CFD, demonstrating how specialized multi-agent systems\ncan democratize complex scientific computing. The code is public at\nhttps://github.com/csml-rpi/Foam-Agent.", "AI": {"tldr": "Foam-Agent\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u6846\u67b6\u7684\u7aef\u5230\u7aefOpenFOAM\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u7cfb\u7edf\uff0c\u80fd\u591f\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u81ea\u52a8\u5b8c\u6210CFD\u4eff\u771f\u7684\u9884\u5904\u7406\u3001\u7f51\u683c\u751f\u6210\u3001HPC\u811a\u672c\u751f\u6210\u548c\u540e\u5904\u7406\u53ef\u89c6\u5316\u7b49\u5168\u6d41\u7a0b\u3002", "motivation": "\u4f20\u7edfCFD\u4eff\u771f\u5de5\u5177\u5b66\u4e60\u66f2\u7ebf\u9661\u5ced\u4e14\u8bbe\u7f6e\u590d\u6742\uff0c\u5b58\u5728\u8f83\u9ad8\u7684\u4f7f\u7528\u95e8\u69db\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f5c\u8005\u5f00\u53d1\u4e86Foam-Agent\u6765\u964d\u4f4eCFD\u4eff\u771f\u7684\u4e13\u4e1a\u77e5\u8bc6\u8981\u6c42\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u4f7f\u7528Model Context Protocol\uff08MCP\uff09\u5c06\u6838\u5fc3\u529f\u80fd\u66b4\u9732\u4e3a\u53ef\u8c03\u7528\u5de5\u5177\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u591a\u7d22\u5f15RAG\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u4e0a\u4e0b\u6587\u68c0\u7d22\uff0c\u5e76\u91c7\u7528\u4f9d\u8d56\u611f\u77e5\u7684\u751f\u6210\u8fc7\u7a0b\u786e\u4fdd\u914d\u7f6e\u4e00\u81f4\u6027\u3002", "result": "\u5728110\u4e2a\u4eff\u771f\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFoam-Agent\u4f7f\u7528Claude 3.5 Sonnet\u8fbe\u5230\u4e8688.2%\u7684\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6846\u67b6\uff08MetaOpenFOAM\u4e3a55.5%\uff09\u3002", "conclusion": "Foam-Agent\u663e\u8457\u964d\u4f4e\u4e86CFD\u4eff\u771f\u7684\u4e13\u4e1a\u95e8\u69db\uff0c\u5c55\u793a\u4e86\u4e13\u7528\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5982\u4f55\u4f7f\u590d\u6742\u7684\u79d1\u5b66\u8ba1\u7b97\u6c11\u4e3b\u5316\u3002"}}
{"id": "2509.18212", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2509.18212", "abs": "https://arxiv.org/abs/2509.18212", "authors": ["Joost Poort", "Frederik Zuiderveen Borgesius"], "title": "Personalised Pricing: The Demise of the Fixed Price?", "comment": null, "summary": "An online seller or platform is technically able to offer every consumer a\ndifferent price for the same product, based on information it has about the\ncustomers. Such online price discrimination exacerbates concerns regarding the\nfairness and morality of price discrimination, and the possible need for\nregulation. In this chapter, we discuss the underlying basis of price\ndiscrimination in economic theory, and its popular perception. Our surveys show\nthat consumers are critical and suspicious of online price discrimination. A\nmajority consider it unacceptable and unfair, and are in favour of a ban. When\nstores apply online price discrimination, most consumers think they should be\ninformed about it. We argue that the General Data Protection Regulation (GDPR)\napplies to the most controversial forms of online price discrimination, and not\nonly requires companies to disclose their use of price discrimination, but also\nrequires companies to ask customers for their prior consent. Industry practice,\nhowever, does not show any adoption of these two principles.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u7ebf\u4ef7\u683c\u6b67\u89c6\u7684\u7ecf\u6d4e\u7406\u8bba\u57fa\u7840\u3001\u6d88\u8d39\u8005\u6001\u5ea6\u4ee5\u53caGDPR\u6cd5\u89c4\u7684\u9002\u7528\u6027\u3002\u8c03\u67e5\u663e\u793a\u6d88\u8d39\u8005\u666e\u904d\u53cd\u5bf9\u5728\u7ebf\u4ef7\u683c\u6b67\u89c6\uff0c\u8ba4\u4e3a\u5176\u4e0d\u516c\u5e73\uff0c\u5e76\u652f\u6301\u7981\u4ee4\u3002GDPR\u8981\u6c42\u4f01\u4e1a\u62ab\u9732\u4ef7\u683c\u6b67\u89c6\u884c\u4e3a\u5e76\u83b7\u5f97\u7528\u6237\u540c\u610f\uff0c\u4f46\u884c\u4e1a\u5b9e\u8df5\u5c1a\u672a\u9075\u5faa\u8fd9\u4e9b\u539f\u5219\u3002", "motivation": "\u7814\u7a76\u5728\u7ebf\u4ef7\u683c\u6b67\u89c6\u7684\u9053\u5fb7\u516c\u5e73\u6027\u95ee\u9898\u53ca\u5176\u76d1\u7ba1\u9700\u6c42\uff0c\u5206\u6790\u6d88\u8d39\u8005\u5bf9\u4ef7\u683c\u6b67\u89c6\u7684\u6001\u5ea6\u4ee5\u53ca\u73b0\u6709\u6cd5\u89c4\uff08\u5982GDPR\uff09\u7684\u9002\u7528\u6027\u3002", "method": "\u901a\u8fc7\u7ecf\u6d4e\u7406\u8bba\u5206\u6790\u4ef7\u683c\u6b67\u89c6\u7684\u57fa\u7840\uff0c\u7ed3\u5408\u6d88\u8d39\u8005\u8c03\u67e5\u6570\u636e\u8bc4\u4f30\u516c\u4f17\u5bf9\u5728\u7ebf\u4ef7\u683c\u6b67\u89c6\u7684\u63a5\u53d7\u5ea6\uff0c\u5e76\u63a2\u8ba8GDPR\u6cd5\u89c4\u5728\u4ef7\u683c\u6b67\u89c6\u573a\u666f\u4e0b\u7684\u6cd5\u5f8b\u9002\u7528\u6027\u3002", "result": "\u6d88\u8d39\u8005\u5bf9\u5728\u7ebf\u4ef7\u683c\u6b67\u89c6\u6301\u6279\u5224\u6001\u5ea6\uff0c\u591a\u6570\u8ba4\u4e3a\u5176\u4e0d\u53ef\u63a5\u53d7\u4e14\u4e0d\u516c\u5e73\uff0c\u652f\u6301\u7981\u4ee4\u5e76\u8981\u6c42\u4f01\u4e1a\u900f\u660e\u62ab\u9732\u3002GDPR\u9002\u7528\u4e8e\u6709\u4e89\u8bae\u7684\u4ef7\u683c\u6b67\u89c6\u5f62\u5f0f\uff0c\u4f46\u884c\u4e1a\u5b9e\u8df5\u5c1a\u672a\u9075\u5b88\u62ab\u9732\u548c\u540c\u610f\u8981\u6c42\u3002", "conclusion": "\u5728\u7ebf\u4ef7\u683c\u6b67\u89c6\u5f15\u53d1\u4e25\u91cd\u7684\u516c\u5e73\u6027\u548c\u9053\u5fb7\u62c5\u5fe7\uff0c\u9700\u8981\u76d1\u7ba1\u5e72\u9884\u3002GDPR\u63d0\u4f9b\u4e86\u6cd5\u5f8b\u6846\u67b6\uff0c\u4f46\u5b9e\u9645\u6267\u884c\u5b58\u5728\u5dee\u8ddd\uff0c\u4f01\u4e1a\u5e94\u63d0\u9ad8\u900f\u660e\u5ea6\u5e76\u83b7\u5f97\u7528\u6237\u540c\u610f\u3002"}}
{"id": "2509.18384", "categories": ["cs.RO", "cs.FL"], "pdf": "https://arxiv.org/pdf/2509.18384", "abs": "https://arxiv.org/abs/2509.18384", "authors": ["Yunhao Yang", "Junyuan Hong", "Gabriel Jacob Perin", "Zhiwen Fan", "Li Yin", "Zhangyang Wang", "Ufuk Topcu"], "title": "AD-VF: LLM-Automatic Differentiation Enables Fine-Tuning-Free Robot Planning from Formal Methods Feedback", "comment": null, "summary": "Large language models (LLMs) can translate natural language instructions into\nexecutable action plans for robotics, autonomous driving, and other domains.\nYet, deploying LLM-driven planning in the physical world demands strict\nadherence to safety and regulatory constraints, which current models often\nviolate due to hallucination or weak alignment. Traditional data-driven\nalignment methods, such as Direct Preference Optimization (DPO), require costly\nhuman labeling, while recent formal-feedback approaches still depend on\nresource-intensive fine-tuning. In this paper, we propose LAD-VF, a\nfine-tuning-free framework that leverages formal verification feedback for\nautomated prompt engineering. By introducing a formal-verification-informed\ntext loss integrated with LLM-AutoDiff, LAD-VF iteratively refines prompts\nrather than model parameters. This yields three key benefits: (i) scalable\nadaptation without fine-tuning; (ii) compatibility with modular LLM\narchitectures; and (iii) interpretable refinement via auditable prompts.\nExperiments in robot navigation and manipulation tasks demonstrate that LAD-VF\nsubstantially enhances specification compliance, improving success rates from\n60% to over 90%. Our method thus presents a scalable and interpretable pathway\ntoward trustworthy, formally-verified LLM-driven control systems.", "AI": {"tldr": "LAD-VF\u662f\u4e00\u4e2a\u65e0\u9700\u5fae\u8c03\u7684\u6846\u67b6\uff0c\u5229\u7528\u5f62\u5f0f\u5316\u9a8c\u8bc1\u53cd\u9988\u8fdb\u884c\u81ea\u52a8\u63d0\u793a\u5de5\u7a0b\uff0c\u901a\u8fc7\u6587\u672c\u635f\u5931\u51fd\u6570\u8fed\u4ee3\u4f18\u5316\u63d0\u793a\u800c\u975e\u6a21\u578b\u53c2\u6570\uff0c\u663e\u8457\u63d0\u5347LLM\u9a71\u52a8\u89c4\u5212\u7cfb\u7edf\u7684\u89c4\u8303\u5408\u89c4\u6027\u3002", "motivation": "\u5f53\u524dLLM\u9a71\u52a8\u7684\u89c4\u5212\u7cfb\u7edf\u5728\u7269\u7406\u4e16\u754c\u90e8\u7f72\u65f6\u7ecf\u5e38\u8fdd\u53cd\u5b89\u5168\u548c\u76d1\u7ba1\u7ea6\u675f\uff0c\u4f20\u7edf\u7684\u6570\u636e\u9a71\u52a8\u5bf9\u9f50\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\uff0c\u800c\u73b0\u6709\u7684\u5f62\u5f0f\u5316\u53cd\u9988\u65b9\u6cd5\u4ecd\u9700\u8d44\u6e90\u5bc6\u96c6\u7684\u5fae\u8c03\u3002", "method": "\u63d0\u51faLAD-VF\u6846\u67b6\uff0c\u7ed3\u5408\u5f62\u5f0f\u5316\u9a8c\u8bc1\u53cd\u9988\u7684\u6587\u672c\u635f\u5931\u51fd\u6570\u4e0eLLM-AutoDiff\uff0c\u8fed\u4ee3\u4f18\u5316\u63d0\u793a\u800c\u975e\u6a21\u578b\u53c2\u6570\uff0c\u5b9e\u73b0\u65e0\u9700\u5fae\u8c03\u7684\u81ea\u52a8\u5316\u63d0\u793a\u5de5\u7a0b\u3002", "result": "\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0cLAD-VF\u5c06\u89c4\u8303\u5408\u89c4\u7684\u6210\u529f\u7387\u4ece60%\u63d0\u5347\u523090%\u4ee5\u4e0a\u3002", "conclusion": "LAD-VF\u4e3a\u53ef\u4fe1\u8d56\u3001\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7684LLM\u9a71\u52a8\u63a7\u5236\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u8def\u5f84\u3002"}}
{"id": "2509.18371", "categories": ["eess.SY", "cs.MA", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18371", "abs": "https://arxiv.org/abs/2509.18371", "authors": ["Eduardo Sebasti\u00e1n", "Maitrayee Keskar", "Eeman Iqbal", "Eduardo Montijano", "Carlos Sag\u00fc\u00e9s", "Nikolay Atanasov"], "title": "Policy Gradient with Self-Attention for Model-Free Distributed Nonlinear Multi-Agent Games", "comment": null, "summary": "Multi-agent games in dynamic nonlinear settings are challenging due to the\ntime-varying interactions among the agents and the non-stationarity of the\n(potential) Nash equilibria. In this paper we consider model-free games, where\nagent transitions and costs are observed without knowledge of the transition\nand cost functions that generate them. We propose a policy gradient approach to\nlearn distributed policies that follow the communication structure in\nmulti-team games, with multiple agents per team. Our formulation is inspired by\nthe structure of distributed policies in linear quadratic games, which take the\nform of time-varying linear feedback gains. In the nonlinear case, we model the\npolicies as nonlinear feedback gains, parameterized by self-attention layers to\naccount for the time-varying multi-agent communication topology. We demonstrate\nthat our distributed policy gradient approach achieves strong performance in\nseveral settings, including distributed linear and nonlinear regulation, and\nsimulated and real multi-robot pursuit-and-evasion games.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7b56\u7565\u68af\u5ea6\u7684\u5206\u5e03\u5f0f\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u52a8\u6001\u975e\u7ebf\u6027\u591a\u667a\u80fd\u4f53\u6e38\u620f\u4e2d\u7684\u975e\u5e73\u7a33\u7eb3\u4ec0\u5747\u8861\u95ee\u9898\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u81ea\u6ce8\u610f\u529b\u5c42\u5efa\u6a21\u975e\u7ebf\u6027\u53cd\u9988\u589e\u76ca\uff0c\u5e76\u5728\u591a\u79cd\u573a\u666f\u4e0b\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u52a8\u6001\u975e\u7ebf\u6027\u591a\u667a\u80fd\u4f53\u6e38\u620f\u4e2d\u7684\u6311\u6218\u5728\u4e8e\u667a\u80fd\u4f53\u95f4\u65f6\u53d8\u4ea4\u4e92\u548c\u7eb3\u4ec0\u5747\u8861\u7684\u975e\u5e73\u7a33\u6027\uff0c\u7279\u522b\u662f\u5728\u6a21\u578b\u672a\u77e5\u7684\u60c5\u51b5\u4e0b\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u9002\u5e94\u901a\u4fe1\u7ed3\u6784\u7684\u5206\u5e03\u5f0f\u7b56\u7565\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u5b66\u4e60\u5206\u5e03\u5f0f\u7b56\u7565\uff0c\u5229\u7528\u81ea\u6ce8\u610f\u529b\u5c42\u53c2\u6570\u5316\u975e\u7ebf\u6027\u53cd\u9988\u589e\u76ca\uff0c\u4ee5\u5904\u7406\u65f6\u53d8\u591a\u667a\u80fd\u4f53\u901a\u4fe1\u62d3\u6251\u7ed3\u6784\uff0c\u8be5\u65b9\u6cd5\u53d7\u5230\u7ebf\u6027\u4e8c\u6b21\u6e38\u620f\u4e2d\u5206\u5e03\u5f0f\u7b56\u7565\u7ed3\u6784\u7684\u542f\u53d1\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5206\u5e03\u5f0f\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u8c03\u8282\u3001\u6a21\u62df\u548c\u771f\u5b9e\u591a\u673a\u5668\u4eba\u8ffd\u9003\u6e38\u620f\u7b49\u591a\u79cd\u8bbe\u7f6e\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5206\u5e03\u5f0f\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u56e2\u961f\u6e38\u620f\u4e2d\u7684\u590d\u6742\u4ea4\u4e92\u548c\u975e\u5e73\u7a33\u6027\uff0c\u4e3a\u52a8\u6001\u975e\u7ebf\u6027\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5b66\u4e60\u6846\u67b6\u3002"}}
{"id": "2509.18180", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18180", "abs": "https://arxiv.org/abs/2509.18180", "authors": ["Yang Wang", "Kai Li"], "title": "Large Language Models and Operations Research: A Structured Survey", "comment": null, "summary": "Operations research (OR) provides fundamental methodologies for complex\nsystem decision-making, with established applications in transportation, supply\nchain management, and production scheduling. Traditional approaches, which\ndepend on expert-based modeling and manual parameter adjustment, often face\nchallenges in handling large-scale, dynamic, and multi-constraint problems.\nRecently, large language models (LLMs) have shown potential to address these\nlimitations through semantic understanding, structured generation, and\nreasoning control. LLMs can translate natural language descriptions into\nmathematical models or executable code, generate heuristics, evolve algorithms,\nand directly tackle optimization tasks. This paper surveys recent progress on\nthe integration of LLMs into OR, organizing methods into three main directions:\nautomatic modeling, auxiliary optimization, and direct solving. It further\nreviews evaluation benchmarks and domain-specific applications, and summarizes\nkey open issues such as unstable semantic-to-structure mapping, fragmented\nresearch progress, limited generalization, and insufficient evaluation systems.\nFinally, the survey outlines possible research avenues for advancing the role\nof LLMs in OR.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8fd0\u7b79\u5b66\uff08OR\uff09\u9886\u57df\u7684\u5e94\u7528\u8fdb\u5c55\uff0c\u4e3b\u8981\u6db5\u76d6\u81ea\u52a8\u5efa\u6a21\u3001\u8f85\u52a9\u4f18\u5316\u548c\u76f4\u63a5\u6c42\u89e3\u4e09\u4e2a\u65b9\u5411\uff0c\u5e76\u8ba8\u8bba\u4e86\u5f53\u524d\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u4f20\u7edf\u8fd0\u7b79\u5b66\u65b9\u6cd5\u4f9d\u8d56\u4e13\u5bb6\u5efa\u6a21\u548c\u624b\u52a8\u53c2\u6570\u8c03\u6574\uff0c\u96be\u4ee5\u5904\u7406\u5927\u89c4\u6a21\u3001\u52a8\u6001\u3001\u591a\u7ea6\u675f\u95ee\u9898\u3002LLMs\u901a\u8fc7\u8bed\u4e49\u7406\u89e3\u3001\u7ed3\u6784\u5316\u751f\u6210\u548c\u63a8\u7406\u63a7\u5236\uff0c\u6709\u671b\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "\u5c06LLMs\u4e0eOR\u96c6\u6210\u7684\u65b9\u6cd5\u5206\u4e3a\u4e09\u7c7b\uff1a\u81ea\u52a8\u5efa\u6a21\uff08\u5c06\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u8f6c\u6362\u4e3a\u6570\u5b66\u6a21\u578b\u6216\u53ef\u6267\u884c\u4ee3\u7801\uff09\u3001\u8f85\u52a9\u4f18\u5316\uff08\u751f\u6210\u542f\u53d1\u5f0f\u7b97\u6cd5\u3001\u8fdb\u5316\u7b97\u6cd5\uff09\u548c\u76f4\u63a5\u6c42\u89e3\u4f18\u5316\u4efb\u52a1\u3002", "result": "LLMs\u5728OR\u9886\u57df\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u63d0\u9ad8\u95ee\u9898\u6c42\u89e3\u6548\u7387\uff0c\u4f46\u9762\u4e34\u8bed\u4e49\u5230\u7ed3\u6784\u6620\u5c04\u4e0d\u7a33\u5b9a\u3001\u7814\u7a76\u8fdb\u5c55\u788e\u7247\u5316\u3001\u6cdb\u5316\u80fd\u529b\u6709\u9650\u548c\u8bc4\u4f30\u4f53\u7cfb\u4e0d\u8db3\u7b49\u6311\u6218\u3002", "conclusion": "LLMs\u4e3aOR\u9886\u57df\u5e26\u6765\u65b0\u7684\u673a\u9047\uff0c\u672a\u6765\u9700\u8981\u89e3\u51b3\u73b0\u6709\u6311\u6218\uff0c\u63a2\u7d22\u66f4\u6709\u6548\u7684\u96c6\u6210\u65b9\u6cd5\uff0c\u63a8\u52a8LLMs\u5728\u590d\u6742\u7cfb\u7edf\u51b3\u7b56\u4e2d\u7684\u66f4\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2509.18231", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18231", "abs": "https://arxiv.org/abs/2509.18231", "authors": ["Sein Minn", "Roger Nkambou"], "title": "Enhanced Interpretable Knowledge Tracing for Students Performance Prediction with Human understandable Feature Space", "comment": "International Conference on Artificial Intelligence in Education", "summary": "Knowledge Tracing (KT) plays a central role in assessing students skill\nmastery and predicting their future performance. While deep learning based KT\nmodels achieve superior predictive accuracy compared to traditional methods,\ntheir complexity and opacity hinder their ability to provide psychologically\nmeaningful explanations. This disconnect between model parameters and cognitive\ntheory poses challenges for understanding and enhancing the learning process,\nlimiting their trustworthiness in educational applications. To address these\nchallenges, we enhance interpretable KT models by exploring\nhuman-understandable features derived from students interaction data. By\nincorporating additional features, particularly those reflecting students\nlearning abilities, our enhanced approach improves predictive accuracy while\nmaintaining alignment with cognitive theory. Our contributions aim to balance\npredictive power with interpretability, advancing the utility of adaptive\nlearning systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u5f15\u5165\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u7279\u5f81\u6765\u589e\u5f3a\u53ef\u89e3\u91ca\u7684\u77e5\u8bc6\u8ffd\u8e2a\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u4e0e\u8ba4\u77e5\u7406\u8bba\u4e00\u81f4\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u77e5\u8bc6\u8ffd\u8e2a\u6a21\u578b\u867d\u7136\u9884\u6d4b\u51c6\u786e\u7387\u9ad8\uff0c\u4f46\u590d\u6742\u6027\u548c\u4e0d\u900f\u660e\u6027\u963b\u788d\u4e86\u63d0\u4f9b\u5fc3\u7406\u5b66\u4e0a\u6709\u610f\u4e49\u7684\u89e3\u91ca\uff0c\u9650\u5236\u4e86\u5728\u6559\u80b2\u5e94\u7528\u4e2d\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "\u63a2\u7d22\u4ece\u5b66\u751f\u4ea4\u4e92\u6570\u636e\u4e2d\u63d0\u53d6\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u7279\u5f81\uff0c\u7279\u522b\u662f\u53cd\u6620\u5b66\u751f\u5b66\u4e60\u80fd\u529b\u7684\u7279\u5f81\uff0c\u5e76\u5c06\u5176\u6574\u5408\u5230\u53ef\u89e3\u91ca\u7684\u77e5\u8bc6\u8ffd\u8e2a\u6a21\u578b\u4e2d\u3002", "result": "\u589e\u5f3a\u540e\u7684\u65b9\u6cd5\u5728\u4fdd\u6301\u4e0e\u8ba4\u77e5\u7406\u8bba\u4e00\u81f4\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u9884\u6d4b\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u63a8\u52a8\u4e86\u81ea\u9002\u5e94\u5b66\u4e60\u7cfb\u7edf\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.18407", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.18407", "abs": "https://arxiv.org/abs/2509.18407", "authors": ["Navya Tiwari", "Joseph Vazhaeparampil", "Victoria Preston"], "title": "Assistive Decision-Making for Right of Way Navigation at Uncontrolled Intersections", "comment": "6 pages, 5 figures. Accepted as a poster at Northeast Robotics\n  Colloquium (NERC 2025). Extended abstract", "summary": "Uncontrolled intersections account for a significant fraction of roadway\ncrashes due to ambiguous right-of-way rules, occlusions, and unpredictable\ndriver behavior. While autonomous vehicle research has explored\nuncertainty-aware decision making, few systems exist to retrofit human-operated\nvehicles with assistive navigation support. We present a driver-assist\nframework for right-of-way reasoning at uncontrolled intersections, formulated\nas a Partially Observable Markov Decision Process (POMDP). Using a custom\nsimulation testbed with stochastic traffic agents, pedestrians, occlusions, and\nadversarial scenarios, we evaluate four decision-making approaches: a\ndeterministic finite state machine (FSM), and three probabilistic planners:\nQMDP, POMCP, and DESPOT. Results show that probabilistic planners outperform\nthe rule-based baseline, achieving up to 97.5 percent collision-free navigation\nunder partial observability, with POMCP prioritizing safety and DESPOT\nbalancing efficiency and runtime feasibility. Our findings highlight the\nimportance of uncertainty-aware planning for driver assistance and motivate\nfuture integration of sensor fusion and environment perception modules for\nreal-time deployment in realistic traffic environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08POMDP\uff09\u7684\u9a7e\u9a76\u5458\u8f85\u52a9\u6846\u67b6\uff0c\u7528\u4e8e\u65e0\u63a7\u5236\u4ea4\u53c9\u53e3\u7684\u901a\u884c\u6743\u63a8\u7406\uff0c\u901a\u8fc7\u6982\u7387\u89c4\u5212\u5668\u663e\u8457\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\u80fd\u3002", "motivation": "\u65e0\u63a7\u5236\u4ea4\u53c9\u53e3\u7531\u4e8e\u901a\u884c\u6743\u89c4\u5219\u6a21\u7cca\u3001\u906e\u6321\u548c\u9a7e\u9a76\u5458\u884c\u4e3a\u4e0d\u53ef\u9884\u6d4b\u7b49\u56e0\u7d20\u5bfc\u81f4\u5927\u91cf\u4ea4\u901a\u4e8b\u6545\uff0c\u800c\u73b0\u6709\u7814\u7a76\u5f88\u5c11\u5173\u6ce8\u4e3a\u4eba\u5de5\u9a7e\u9a76\u8f66\u8f86\u63d0\u4f9b\u8f85\u52a9\u5bfc\u822a\u652f\u6301\u3002", "method": "\u5f00\u53d1\u4e86\u9a7e\u9a76\u5458\u8f85\u52a9\u6846\u67b6\uff0c\u91c7\u7528POMDP\u5efa\u6a21\uff0c\u5728\u81ea\u5b9a\u4e49\u4eff\u771f\u73af\u5883\u4e2d\u6bd4\u8f83\u4e86\u786e\u5b9a\u6027\u6709\u9650\u72b6\u6001\u673a\u548c\u4e09\u79cd\u6982\u7387\u89c4\u5212\u5668\uff08QMDP\u3001POMCP\u3001DESPOT\uff09\u7684\u6027\u80fd\u3002", "result": "\u6982\u7387\u89c4\u5212\u5668\u4f18\u4e8e\u57fa\u4e8e\u89c4\u5219\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u8fbe97.5%\u7684\u65e0\u78b0\u649e\u5bfc\u822a\u7387\uff0c\u5176\u4e2dPOMCP\u6ce8\u91cd\u5b89\u5168\u6027\uff0cDESPOT\u5e73\u8861\u6548\u7387\u4e0e\u8fd0\u884c\u65f6\u53ef\u884c\u6027\u3002", "conclusion": "\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u89c4\u5212\u5bf9\u9a7e\u9a76\u5458\u8f85\u52a9\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u672a\u6765\u9700\u8981\u96c6\u6210\u4f20\u611f\u5668\u878d\u5408\u548c\u73af\u5883\u611f\u77e5\u6a21\u5757\u4ee5\u5b9e\u73b0\u771f\u5b9e\u4ea4\u901a\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u90e8\u7f72\u3002"}}
{"id": "2509.18475", "categories": ["eess.SY", "cs.SY", "math.CT", "math.DS"], "pdf": "https://arxiv.org/pdf/2509.18475", "abs": "https://arxiv.org/abs/2509.18475", "authors": ["Xiaoyan Li", "Evan Patterson", "Patricia L. Mabry", "Nathaniel D. Osgood"], "title": "Compositional System Dynamics: The Higher Mathematics Underlying System Dynamics Diagrams & Practice", "comment": null, "summary": "This work establishes a robust mathematical foundation for compositional\nSystem Dynamics modeling, leveraging category theory to formalize and enhance\nthe representation, analysis, and composition of system models. Here, System\nDynamics diagrams, such as stock & flow diagrams, system structure diagrams,\nand causal loop diagrams, are formulated as categorical constructs, enabling\nscalable, transparent, and systematic reasoning. By encoding these diagrams as\ndata using attributed C-sets and utilizing advanced categorical tools like\nstructured cospans, pushouts, pullbacks, and functor mappings, the framework\nsupports modular composition, stratification, and seamless mapping between\nsyntax and semantics.\n  The approach underwrites traditional practice with firm mathematical\nstructure, facilitates the identification of certain forms of pathways and\nfeedback loops, the detection of simple patterns within complex diagrams,\ncommon structure between diagrams, and structure-preserving mappings between\ndiverse diagram types. Additionally, this framework supports alternative\nsemantics, such as stochastic transition dynamics, extending beyond traditional\nordinary differential equation (ODE) representations. Applications in\ncompositional modeling, modularity, and team-based collaboration demonstrate\nthe practical advantages of this advanced framework.\n  Future directions include integrating dimensional annotations, supporting\nhybrid and agent-based modeling paradigms, and expanding the framework's\napplicability to global and local temporal reasoning through temporal sheaves.\nBy revealing and formalizing the hidden mathematical structure of System\nDynamics diagrams, this work empowers practitioners to tackle complex systems\nwith clarity, scalability, and rigor.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5efa\u7acb\u4e86\u7cfb\u7edf\u52a8\u529b\u5b66\u7ec4\u5408\u5efa\u6a21\u7684\u6570\u5b66\u57fa\u7840\uff0c\u5229\u7528\u8303\u7574\u8bba\u5f62\u5f0f\u5316\u7cfb\u7edf\u6a21\u578b\u7684\u8868\u793a\u3001\u5206\u6790\u548c\u7ec4\u5408\uff0c\u652f\u6301\u6a21\u5757\u5316\u5efa\u6a21\u548c\u8bed\u4e49\u6620\u5c04\u3002", "motivation": "\u4e3a\u7cfb\u7edf\u52a8\u529b\u5b66\u5efa\u6a21\u63d0\u4f9b\u575a\u5b9e\u7684\u6570\u5b66\u57fa\u7840\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u3001\u900f\u660e\u5ea6\u548c\u7cfb\u7edf\u5316\u63a8\u7406\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u5c06\u7cfb\u7edf\u52a8\u529b\u5b66\u56fe\u8868\u793a\u4e3a\u8303\u7574\u6784\u9020\uff0c\u4f7f\u7528\u5c5e\u6027C-\u96c6\u7f16\u7801\u6570\u636e\uff0c\u5e94\u7528\u7ed3\u6784\u5316\u4f59\u8de8\u5ea6\u3001\u63a8\u51fa\u3001\u62c9\u56de\u7b49\u8303\u7574\u5de5\u5177\u3002", "result": "\u6846\u67b6\u652f\u6301\u6a21\u5757\u5316\u7ec4\u5408\u3001\u5206\u5c42\u3001\u8bed\u6cd5\u8bed\u4e49\u6620\u5c04\uff0c\u80fd\u591f\u8bc6\u522b\u53cd\u9988\u73af\u8def\u3001\u68c0\u6d4b\u6a21\u5f0f\u3001\u53d1\u73b0\u56fe\u95f4\u5171\u540c\u7ed3\u6784\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u590d\u6742\u7cfb\u7edf\u5efa\u6a21\u63d0\u4f9b\u4e86\u6e05\u6670\u3001\u53ef\u6269\u5c55\u548c\u4e25\u8c28\u7684\u65b9\u6cd5\uff0c\u672a\u6765\u53ef\u6269\u5c55\u5230\u65f6\u5e8f\u63a8\u7406\u548c\u6df7\u5408\u5efa\u6a21\u8303\u5f0f\u3002"}}
{"id": "2509.18181", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18181", "abs": "https://arxiv.org/abs/2509.18181", "authors": ["Mustafa Sameen", "Xiaojian Zhang", "Xilei Zhao"], "title": "Synthesizing Attitudes, Predicting Actions (SAPA): Behavioral Theory-Guided LLMs for Ridesourcing Mode Choice Modeling", "comment": null, "summary": "Accurate modeling of ridesourcing mode choices is essential for designing and\nimplementing effective traffic management policies for reducing congestion,\nimproving mobility, and allocating resources more efficiently. Existing models\nfor predicting ridesourcing mode choices often suffer from limited predictive\naccuracy due to their inability to capture key psychological factors, and are\nfurther challenged by severe class imbalance, as ridesourcing trips comprise\nonly a small fraction of individuals' daily travel. To address these\nlimitations, this paper introduces the Synthesizing Attitudes, Predicting\nActions (SAPA) framework, a hierarchical approach that uses Large Language\nModels (LLMs) to synthesize theory-grounded latent attitudes to predict\nridesourcing choices. SAPA first uses an LLM to generate qualitative traveler\npersonas from raw travel survey data and then trains a propensity-score model\non demographic and behavioral features, enriched by those personas, to produce\nan individual-level score. Next, the LLM assigns quantitative scores to\ntheory-driven latent variables (e.g., time and cost sensitivity), and a final\nclassifier integrates the propensity score, latent-variable scores (with their\ninteraction terms), and observable trip attributes to predict ridesourcing mode\nchoice. Experiments on a large-scale, multi-year travel survey show that SAPA\nsignificantly outperforms state-of-the-art baselines, improving ridesourcing\nchoice predictions by up to 75.9% in terms of PR-AUC on a held-out test set.\nThis study provides a powerful tool for accurately predicting ridesourcing mode\nchoices, and provides a methodology that is readily transferable to various\napplications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SAPA\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5408\u6210\u7406\u8bba\u9a71\u52a8\u7684\u6f5c\u5728\u6001\u5ea6\u6765\u9884\u6d4b\u7f51\u7ea6\u8f66\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7f51\u7ea6\u8f66\u9009\u62e9\u9884\u6d4b\u6a21\u578b\u56e0\u65e0\u6cd5\u6355\u6349\u5173\u952e\u5fc3\u7406\u56e0\u7d20\u800c\u9884\u6d4b\u7cbe\u5ea6\u6709\u9650\uff0c\u4e14\u9762\u4e34\u4e25\u91cd\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff08\u7f51\u7ea6\u8f66\u51fa\u884c\u4ec5\u5360\u65e5\u5e38\u51fa\u884c\u7684\u4e00\u5c0f\u90e8\u5206\uff09\u3002", "method": "SAPA\u6846\u67b6\u91c7\u7528\u5206\u5c42\u65b9\u6cd5\uff1a\u9996\u5148\u7528LLM\u4ece\u539f\u59cb\u51fa\u884c\u8c03\u67e5\u6570\u636e\u751f\u6210\u5b9a\u6027\u65c5\u884c\u8005\u753b\u50cf\uff0c\u7136\u540e\u8bad\u7ec3\u503e\u5411\u5f97\u5206\u6a21\u578b\uff0c\u518d\u7528LLM\u4e3a\u7406\u8bba\u9a71\u52a8\u7684\u6f5c\u5728\u53d8\u91cf\u5206\u914d\u5b9a\u91cf\u5206\u6570\uff0c\u6700\u540e\u901a\u8fc7\u5206\u7c7b\u5668\u6574\u5408\u503e\u5411\u5f97\u5206\u3001\u6f5c\u5728\u53d8\u91cf\u5206\u6570\u548c\u53ef\u89c2\u6d4b\u51fa\u884c\u5c5e\u6027\u6765\u9884\u6d4b\u7f51\u7ea6\u8f66\u9009\u62e9\u3002", "result": "\u5728\u5927\u89c4\u6a21\u591a\u5e74\u51fa\u884c\u8c03\u67e5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSAPA\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u6d4b\u8bd5\u96c6\u4e0a\u7684PR-AUC\u6307\u6807\u63d0\u5347\u4e86\u9ad8\u8fbe75.9%\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u51c6\u786e\u9884\u6d4b\u7f51\u7ea6\u8f66\u9009\u62e9\u63d0\u4f9b\u4e86\u5f3a\u5927\u5de5\u5177\uff0c\u5176\u65b9\u6cd5\u8bba\u53ef\u8f7b\u677e\u8fc1\u79fb\u5230\u5404\u79cd\u5e94\u7528\u4e2d\u3002"}}
{"id": "2509.18233", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18233", "abs": "https://arxiv.org/abs/2509.18233", "authors": ["Filip Bialy", "Mark Elliot", "Robert Meckin"], "title": "Perceptions of AI Across Sectors: A Comparative Review of Public Attitudes", "comment": null, "summary": "This paper offers a domain-mediated comparative review of 251 studies on\npublic attitudes toward AI, published between 2011 and 2025. Drawing on a\nsystematic literature review, we analyse how different factors including\nperceived benefits and concerns (or risks) shape public acceptance of - or\nresistance to - artificial intelligence across domains and use-cases, including\nhealthcare, education, security, public administration, generative AI, and\nautonomous vehicles. The analysis highlights recurring patterns in individual,\ncontextual, and technical factors influencing perception, while also tracing\nvariations in institutional trust, perceived fairness, and ethical concerns. We\nshow that the public perception in AI is shaped not only by technical design or\nperformance but also by sector-specific considerations as well as imaginaries,\ncultural narratives, and historical legacies. This comparative approach offers\na foundation for developing more tailored and context-sensitive strategies for\nresponsible AI governance.", "AI": {"tldr": "\u672c\u6587\u5bf92011-2025\u5e74\u95f4251\u9879\u5173\u4e8e\u516c\u4f17\u5bf9AI\u6001\u5ea6\u7684\u7814\u7a76\u8fdb\u884c\u4e86\u8de8\u9886\u57df\u6bd4\u8f83\u5206\u6790\uff0c\u63a2\u8ba8\u4e86\u4e0d\u540c\u56e0\u7d20\u5982\u4f55\u5f71\u54cd\u516c\u4f17\u5bf9AI\u7684\u63a5\u53d7\u5ea6\u6216\u62b5\u5236\u3002", "motivation": "\u7406\u89e3\u516c\u4f17\u5bf9AI\u7684\u6001\u5ea6\u5728\u4e0d\u540c\u9886\u57df\u548c\u7528\u4f8b\u4e2d\u7684\u5dee\u5f02\uff0c\u4e3a\u5236\u5b9a\u66f4\u7cbe\u51c6\u7684\u8d1f\u8d23\u4efbAI\u6cbb\u7406\u7b56\u7565\u63d0\u4f9b\u57fa\u7840\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5206\u6790251\u9879\u7814\u7a76\uff0c\u8003\u5bdf\u611f\u77e5\u5229\u76ca\u3001\u98ce\u9669\u3001\u5236\u5ea6\u4fe1\u4efb\u3001\u516c\u5e73\u6027\u548c\u4f26\u7406\u5173\u5207\u7b49\u56e0\u7d20\u3002", "result": "\u53d1\u73b0\u516c\u4f17\u5bf9AI\u7684\u8ba4\u77e5\u4e0d\u4ec5\u53d7\u6280\u672f\u8bbe\u8ba1\u548c\u6027\u80fd\u5f71\u54cd\uff0c\u8fd8\u53d7\u9886\u57df\u7279\u5b9a\u8003\u91cf\u3001\u6587\u5316\u53d9\u4e8b\u548c\u5386\u53f2\u9057\u4ea7\u7b49\u56e0\u7d20\u5851\u9020\u3002", "conclusion": "\u6bd4\u8f83\u65b9\u6cd5\u4e3a\u5f00\u53d1\u66f4\u5177\u9488\u5bf9\u6027\u548c\u60c5\u5883\u654f\u611f\u6027\u7684\u8d1f\u8d23\u4efbAI\u6cbb\u7406\u7b56\u7565\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.18428", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18428", "abs": "https://arxiv.org/abs/2509.18428", "authors": ["Bahey Tharwat", "Yara Nasser", "Ali Abouzeid", "Ian Reid"], "title": "Latent Action Pretraining Through World Modeling", "comment": null, "summary": "Vision-Language-Action (VLA) models have gained popularity for learning\nrobotic manipulation tasks that follow language instructions. State-of-the-art\nVLAs, such as OpenVLA and $\\pi_{0}$, were trained on large-scale, manually\nlabeled action datasets collected through teleoperation. More recent\napproaches, including LAPA and villa-X, introduce latent action representations\nthat enable unsupervised pretraining on unlabeled datasets by modeling abstract\nvisual changes between frames. Although these methods have shown strong\nresults, their large model sizes make deployment in real-world settings\nchallenging. In this work, we propose LAWM, a model-agnostic framework to\npretrain imitation learning models in a self-supervised way, by learning latent\naction representations from unlabeled video data through world modeling. These\nvideos can be sourced from robot recordings or videos of humans performing\nactions with everyday objects. Our framework is designed to be effective for\ntransferring across tasks, environments, and embodiments. It outperforms models\ntrained with ground-truth robotics actions and similar pretraining methods on\nthe LIBERO benchmark and real-world setup, while being significantly more\nefficient and practical for real-world settings.", "AI": {"tldr": "LAWM\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4e16\u754c\u5efa\u6a21\u4ece\u65e0\u6807\u7b7e\u89c6\u9891\u6570\u636e\u4e2d\u5b66\u4e60\u6f5c\u5728\u52a8\u4f5c\u8868\u793a\uff0c\u7528\u4e8e\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6a21\u4eff\u5b66\u4e60\u6a21\u578b\uff0c\u5728\u6548\u7387\u548c\u5b9e\u7528\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684VLA\u6a21\u578b\u4f9d\u8d56\u5927\u89c4\u6a21\u4eba\u5de5\u6807\u6ce8\u7684\u52a8\u4f5c\u6570\u636e\u96c6\uff0c\u6a21\u578b\u4f53\u79ef\u5927\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u56f0\u96be\u3002\u9700\u8981\u66f4\u9ad8\u6548\u5b9e\u7528\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u63d0\u51faLAWM\u6846\u67b6\uff0c\u901a\u8fc7\u4e16\u754c\u5efa\u6a21\u4ece\u65e0\u6807\u7b7e\u89c6\u9891\uff08\u673a\u5668\u4eba\u8bb0\u5f55\u6216\u4eba\u7c7b\u65e5\u5e38\u52a8\u4f5c\u89c6\u9891\uff09\u4e2d\u5b66\u4e60\u6f5c\u5728\u52a8\u4f5c\u8868\u793a\uff0c\u652f\u6301\u8de8\u4efb\u52a1\u3001\u73af\u5883\u548c\u4f53\u73b0\u7684\u8fc1\u79fb\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u4e16\u754c\u8bbe\u7f6e\u4e2d\uff0cLAWM\u4f18\u4e8e\u4f7f\u7528\u771f\u5b9e\u673a\u5668\u4eba\u52a8\u4f5c\u8bad\u7ec3\u7684\u6a21\u578b\u548c\u7c7b\u4f3c\u9884\u8bad\u7ec3\u65b9\u6cd5\u3002", "conclusion": "LAWM\u6846\u67b6\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u66f4\u9002\u5408\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u3002"}}
{"id": "2509.18518", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18518", "abs": "https://arxiv.org/abs/2509.18518", "authors": ["Bai Xue", "Luke Ong", "Dominik Wagner", "Peixin Wang"], "title": "Refined Barrier Conditions for Finite-Time Safety and Reach-Avoid Guarantees in Stochastic Systems", "comment": null, "summary": "Providing finite-time probabilistic safety and reach-avoid guarantees is\ncrucial for safety-critical stochastic systems. Existing barrier certificate\nmethods often rely on a restrictive boundedness assumption for auxiliary\nfunctions, limiting their applicability. This paper presents refined\nbarrier-like conditions that remove this assumption. Specifically, we establish\nconditions for deriving upper bounds on finite-time safety probabilities in\ndiscrete-time systems and lower bounds on finite-time reach-avoid probabilities\nin continuous-time systems. This key relaxation significantly expands the class\nof verifiable systems, especially those with unbounded state spaces, and\nfacilitates the application of advanced optimization techniques, such as\nsemi-definite programming with polynomial functions. The efficacy of our\napproach is validated through numerical examples.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u6539\u8fdb\u7684\u5c4f\u969c\u8bc1\u4e66\u6761\u4ef6\uff0c\u6d88\u9664\u4e86\u5bf9\u8f85\u52a9\u51fd\u6570\u6709\u754c\u6027\u7684\u9650\u5236\uff0c\u4e3a\u79bb\u6563\u65f6\u95f4\u7cfb\u7edf\u63d0\u4f9b\u6709\u9650\u65f6\u95f4\u5b89\u5168\u6982\u7387\u4e0a\u754c\uff0c\u4e3a\u8fde\u7eed\u65f6\u95f4\u7cfb\u7edf\u63d0\u4f9b\u6709\u9650\u65f6\u95f4\u53ef\u8fbe-\u89c4\u907f\u6982\u7387\u4e0b\u754c\u3002", "motivation": "\u73b0\u6709\u5c4f\u969c\u8bc1\u4e66\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u8f85\u52a9\u51fd\u6570\u7684\u6709\u754c\u6027\u5047\u8bbe\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u5b89\u5168\u5173\u952e\u968f\u673a\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u8303\u56f4\u3002\u672c\u6587\u65e8\u5728\u6d88\u9664\u8fd9\u4e00\u9650\u5236\uff0c\u6269\u5c55\u53ef\u9a8c\u8bc1\u7cfb\u7edf\u7684\u7c7b\u522b\u3002", "method": "\u5efa\u7acb\u6539\u8fdb\u7684\u5c4f\u969c\u7c7b\u6761\u4ef6\uff0c\u901a\u8fc7\u677e\u5f1b\u6709\u754c\u6027\u8981\u6c42\uff0c\u4f7f\u65b9\u6cd5\u9002\u7528\u4e8e\u65e0\u754c\u72b6\u6001\u7a7a\u95f4\u7cfb\u7edf\uff0c\u5e76\u652f\u6301\u534a\u5b9a\u89c4\u5212\u7b49\u4f18\u5316\u6280\u672f\u7684\u5e94\u7528\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u6269\u5c55\u4e86\u53ef\u9a8c\u8bc1\u7cfb\u7edf\u7684\u8303\u56f4\uff0c\u7279\u522b\u662f\u5728\u65e0\u754c\u72b6\u6001\u7a7a\u95f4\u7cfb\u7edf\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u793a\u4f8b\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6539\u8fdb\u5c4f\u969c\u6761\u4ef6\u6210\u529f\u6d88\u9664\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u6709\u754c\u6027\u9650\u5236\uff0c\u4e3a\u66f4\u5e7f\u6cdb\u7684\u5b89\u5168\u5173\u952e\u968f\u673a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6709\u9650\u65f6\u95f4\u6982\u7387\u5b89\u5168\u4fdd\u8bc1\u3002"}}
{"id": "2509.18186", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18186", "abs": "https://arxiv.org/abs/2509.18186", "authors": ["Nursultan Askarbekuly", "Timur Fayzrakhmanov", "Sladjan Babarogi\u0107", "Ivan Lukovi\u0107"], "title": "An Outcome-Based Educational Recommender System", "comment": null, "summary": "Most educational recommender systems are tuned and judged on click- or\nrating-based relevance, leaving their true pedagogical impact unclear. We\nintroduce OBER-an Outcome-Based Educational Recommender that embeds learning\noutcomes and assessment items directly into the data schema, so any algorithm\ncan be evaluated on the mastery it fosters. OBER uses a minimalist\nentity-relation model, a log-driven mastery formula, and a plug-in\narchitecture. Integrated into an e-learning system in non-formal domain, it was\nevaluated trough a two-week randomized split test with over 5 700 learners\nacross three methods: fixed expert trajectory, collaborative filtering (CF),\nand knowledge-based (KB) filtering. CF maximized retention, but the fixed path\nachieved the highest mastery. Because OBER derives business, relevance, and\nlearning metrics from the same logs, it lets practitioners weigh relevance and\nengagement against outcome mastery with no extra testing overhead. The\nframework is method-agnostic and readily extensible to future adaptive or\ncontext-aware recommenders.", "AI": {"tldr": "OBER\u662f\u4e00\u4e2a\u57fa\u4e8e\u5b66\u4e60\u6210\u679c\u7684\u6559\u80b2\u63a8\u8350\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06\u5b66\u4e60\u6210\u679c\u548c\u8bc4\u4f30\u9879\u76ee\u5d4c\u5165\u6570\u636e\u6a21\u5f0f\uff0c\u4f7f\u4efb\u4f55\u63a8\u8350\u7b97\u6cd5\u90fd\u80fd\u6839\u636e\u5176\u4fc3\u8fdb\u7684\u5b66\u4e60\u638c\u63e1\u7a0b\u5ea6\u8fdb\u884c\u8bc4\u4f30\u3002", "motivation": "\u5927\u591a\u6570\u6559\u80b2\u63a8\u8350\u7cfb\u7edf\u4ec5\u57fa\u4e8e\u70b9\u51fb\u6216\u8bc4\u5206\u76f8\u5173\u6027\u8fdb\u884c\u8c03\u4f18\u548c\u8bc4\u4f30\uff0c\u65e0\u6cd5\u8861\u91cf\u5176\u771f\u5b9e\u7684\u6559\u5b66\u5f71\u54cd\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u76f4\u63a5\u8bc4\u4f30\u63a8\u8350\u7cfb\u7edf\u5bf9\u5b66\u4e60\u6210\u679c\u5f71\u54cd\u7684\u65b9\u6cd5\u3002", "method": "OBER\u91c7\u7528\u7b80\u7ea6\u7684\u5b9e\u4f53\u5173\u7cfb\u6a21\u578b\u3001\u57fa\u4e8e\u65e5\u5fd7\u7684\u638c\u63e1\u5ea6\u516c\u5f0f\u548c\u63d2\u4ef6\u67b6\u6784\uff0c\u5728\u975e\u6b63\u5f0f\u5b66\u4e60\u9886\u57df\u7684\u7535\u5b50\u5b66\u4e60\u7cfb\u7edf\u4e2d\u8fdb\u884c\u4e86\u4e3a\u671f\u4e24\u5468\u7684\u968f\u673a\u5206\u7ec4\u6d4b\u8bd5\uff0c\u6bd4\u8f83\u4e86\u56fa\u5b9a\u4e13\u5bb6\u8def\u5f84\u3001\u534f\u540c\u8fc7\u6ee4\u548c\u57fa\u4e8e\u77e5\u8bc6\u7684\u8fc7\u6ee4\u4e09\u79cd\u65b9\u6cd5\u3002", "result": "\u534f\u540c\u8fc7\u6ee4\u65b9\u6cd5\u6700\u5927\u5316\u4e86\u7528\u6237\u7559\u5b58\u7387\uff0c\u4f46\u56fa\u5b9a\u8def\u5f84\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u5b66\u4e60\u638c\u63e1\u5ea6\u3002OBER\u53ef\u4ee5\u4ece\u76f8\u540c\u7684\u65e5\u5fd7\u6570\u636e\u4e2d\u5bfc\u51fa\u4e1a\u52a1\u3001\u76f8\u5173\u6027\u548c\u5b66\u4e60\u6307\u6807\u3002", "conclusion": "OBER\u6846\u67b6\u662f\u65b9\u6cd5\u65e0\u5173\u7684\uff0c\u5141\u8bb8\u4ece\u4e1a\u8005\u5728\u76f8\u5173\u6027\u3001\u53c2\u4e0e\u5ea6\u548c\u5b66\u4e60\u6210\u679c\u638c\u63e1\u5ea6\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\uff0c\u65e0\u9700\u989d\u5916\u7684\u6d4b\u8bd5\u5f00\u9500\uff0c\u5e76\u4e14\u6613\u4e8e\u6269\u5c55\u5230\u672a\u6765\u7684\u81ea\u9002\u5e94\u6216\u60c5\u5883\u611f\u77e5\u63a8\u8350\u7cfb\u7edf\u3002"}}
{"id": "2509.18394", "categories": ["cs.CY", "cs.AI", "q-fin.RM"], "pdf": "https://arxiv.org/pdf/2509.18394", "abs": "https://arxiv.org/abs/2509.18394", "authors": ["Luis Enriquez Alvarez"], "title": "An Artificial Intelligence Value at Risk Approach: Metrics and Models", "comment": null, "summary": "Artificial intelligence risks are multidimensional in nature, as the same\nrisk scenarios may have legal, operational, and financial risk dimensions. With\nthe emergence of new AI regulations, the state of the art of artificial\nintelligence risk management seems to be highly immature due to upcoming AI\nregulations. Despite the appearance of several methodologies and generic\ncriteria, it is rare to find guidelines with real implementation value,\nconsidering that the most important issue is customizing artificial\nintelligence risk metrics and risk models for specific AI risk scenarios.\nFurthermore, the financial departments, legal departments and Government Risk\nCompliance teams seem to remain unaware of many technical aspects of AI\nsystems, in which data scientists and AI engineers emerge as the most\nappropriate implementers. It is crucial to decompose the problem of artificial\nintelligence risk in several dimensions: data protection, fairness, accuracy,\nrobustness, and information security. Consequently, the main task is developing\nadequate metrics and risk models that manage to reduce uncertainty for\ndecision-making in order to take informed decisions concerning the risk\nmanagement of AI systems.\n  The purpose of this paper is to orientate AI stakeholders about the depths of\nAI risk management. Although it is not extremely technical, it requires a basic\nknowledge of risk management, quantifying uncertainty, the FAIR model, machine\nlearning, large language models and AI context engineering. The examples\npresented pretend to be very basic and understandable, providing simple ideas\nthat can be developed regarding specific AI customized environments. There are\nmany issues to solve in AI risk management, and this paper will present a\nholistic overview of the inter-dependencies of AI risks, and how to model them\ntogether, within risk scenarios.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u4eba\u5de5\u667a\u80fd\u98ce\u9669\u7ba1\u7406\u7684\u591a\u7ef4\u6027\uff0c\u6307\u51fa\u5f53\u524dAI\u98ce\u9669\u7ba1\u7406\u65b9\u6cd5\u4e0d\u6210\u719f\uff0c\u7f3a\u4e4f\u5177\u6709\u5b9e\u9645\u5b9e\u65bd\u4ef7\u503c\u7684\u6307\u5357\uff0c\u5e76\u5f3a\u8c03\u9700\u8981\u4e3a\u7279\u5b9aAI\u98ce\u9669\u573a\u666f\u5b9a\u5236\u98ce\u9669\u6307\u6807\u548c\u6a21\u578b\u3002", "motivation": "\u968f\u7740\u65b0AI\u6cd5\u89c4\u7684\u51fa\u73b0\uff0cAI\u98ce\u9669\u7ba1\u7406\u73b0\u72b6\u9ad8\u5ea6\u4e0d\u6210\u719f\u3002\u8d22\u52a1\u3001\u6cd5\u5f8b\u548c\u5408\u89c4\u90e8\u95e8\u5bf9AI\u7cfb\u7edf\u7684\u6280\u672f\u65b9\u9762\u4e86\u89e3\u4e0d\u8db3\uff0c\u800c\u6570\u636e\u79d1\u5b66\u5bb6\u548cAI\u5de5\u7a0b\u5e08\u662f\u6700\u5408\u9002\u7684\u5b9e\u65bd\u8005\u3002\u9700\u8981\u4ece\u6570\u636e\u4fdd\u62a4\u3001\u516c\u5e73\u6027\u3001\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u4fe1\u606f\u5b89\u5168\u7b49\u591a\u4e2a\u7ef4\u5ea6\u5206\u89e3AI\u98ce\u9669\u95ee\u9898\u3002", "method": "\u672c\u6587\u65e8\u5728\u4e3aAI\u5229\u76ca\u76f8\u5173\u8005\u63d0\u4f9bAI\u98ce\u9669\u7ba1\u7406\u7684\u6df1\u5ea6\u6307\u5bfc\uff0c\u867d\u7136\u4e0d\u6781\u5176\u6280\u672f\u6027\uff0c\u4f46\u9700\u8981\u98ce\u9669\u7ba1\u7406\u3001\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3001FAIR\u6a21\u578b\u3001\u673a\u5668\u5b66\u4e60\u3001\u5927\u8bed\u8a00\u6a21\u578b\u548cAI\u4e0a\u4e0b\u6587\u5de5\u7a0b\u7684\u57fa\u672c\u77e5\u8bc6\u3002\u901a\u8fc7\u57fa\u672c\u6613\u61c2\u7684\u793a\u4f8b\u63d0\u4f9b\u53ef\u5728\u7279\u5b9aAI\u5b9a\u5236\u73af\u5883\u4e2d\u53d1\u5c55\u7684\u7b80\u5355\u601d\u8def\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86AI\u98ce\u9669\u7ba1\u7406\u7684\u6574\u4f53\u6982\u8ff0\uff0c\u5c55\u793a\u4e86AI\u98ce\u9669\u4e4b\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\u6027\uff0c\u4ee5\u53ca\u5982\u4f55\u5728\u98ce\u9669\u573a\u666f\u4e2d\u5171\u540c\u5efa\u6a21\u8fd9\u4e9b\u98ce\u9669\u3002", "conclusion": "AI\u98ce\u9669\u7ba1\u7406\u7684\u4e3b\u8981\u4efb\u52a1\u662f\u5f00\u53d1\u9002\u5f53\u7684\u6307\u6807\u548c\u98ce\u9669\u6a21\u578b\uff0c\u4ee5\u51cf\u5c11\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4ece\u800c\u5c31AI\u7cfb\u7edf\u7684\u98ce\u9669\u7ba1\u7406\u505a\u51fa\u660e\u667a\u51b3\u7b56\u3002\u672c\u6587\u63d0\u4f9b\u4e86\u89e3\u51b3AI\u98ce\u9669\u7ba1\u7406\u4e2d\u8bb8\u591a\u95ee\u9898\u7684\u6846\u67b6\u548c\u65b9\u6cd5\u3002"}}
{"id": "2509.18447", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18447", "abs": "https://arxiv.org/abs/2509.18447", "authors": ["Rishabh Madan", "Jiawei Lin", "Mahika Goel", "Angchen Xie", "Xiaoyu Liang", "Marcus Lee", "Justin Guo", "Pranav N. Thakkar", "Rohan Banerjee", "Jose Barreiros", "Kate Tsui", "Tom Silver", "Tapomayukh Bhattacharjee"], "title": "PrioriTouch: Adapting to User Contact Preferences for Whole-Arm Physical Human-Robot Interaction", "comment": "Conference on Robot Learning (CoRL)", "summary": "Physical human-robot interaction (pHRI) requires robots to adapt to\nindividual contact preferences, such as where and how much force is applied.\nIdentifying preferences is difficult for a single contact; with whole-arm\ninteraction involving multiple simultaneous contacts between the robot and\nhuman, the challenge is greater because different body parts can impose\nincompatible force requirements. In caregiving tasks, where contact is frequent\nand varied, such conflicts are unavoidable. With multiple preferences across\nmultiple contacts, no single solution can satisfy all objectives--trade-offs\nare inherent, making prioritization essential. We present PrioriTouch, a\nframework for ranking and executing control objectives across multiple\ncontacts. PrioriTouch can prioritize from a general collection of controllers,\nmaking it applicable not only to caregiving scenarios such as bed bathing and\ndressing but also to broader multi-contact settings. Our method combines a\nnovel learning-to-rank approach with hierarchical operational space control,\nleveraging simulation-in-the-loop rollouts for data-efficient and safe\nexploration. We conduct a user study on physical assistance preferences, derive\npersonalized comfort thresholds, and incorporate them into PrioriTouch. We\nevaluate PrioriTouch through extensive simulation and real-world experiments,\ndemonstrating its ability to adapt to user contact preferences, maintain task\nperformance, and enhance safety and comfort. Website:\nhttps://emprise.cs.cornell.edu/prioritouch.", "AI": {"tldr": "PrioriTouch\u662f\u4e00\u4e2a\u7528\u4e8e\u5728\u7269\u7406\u4eba\u673a\u4ea4\u4e92\u4e2d\u4f18\u5148\u5904\u7406\u591a\u63a5\u89e6\u63a7\u5236\u76ee\u6807\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u548c\u6392\u5e8f\u65b9\u6cd5\u89e3\u51b3\u4e0d\u540c\u8eab\u4f53\u90e8\u4f4d\u63a5\u89e6\u529b\u51b2\u7a81\u7684\u95ee\u9898\u3002", "motivation": "\u5728\u62a4\u7406\u7b49\u9700\u8981\u591a\u63a5\u89e6\u4ea4\u4e92\u7684\u4efb\u52a1\u4e2d\uff0c\u4e0d\u540c\u8eab\u4f53\u90e8\u4f4d\u5bf9\u63a5\u89e6\u529b\u7684\u504f\u597d\u53ef\u80fd\u5b58\u5728\u51b2\u7a81\uff0c\u9700\u8981\u4f18\u5148\u5904\u7406\u673a\u5236\u6765\u5e73\u8861\u5404\u79cd\u63a7\u5236\u76ee\u6807\u3002", "method": "\u7ed3\u5408\u5b66\u4e60\u6392\u5e8f\u65b9\u6cd5\u548c\u5206\u5c42\u64cd\u4f5c\u7a7a\u95f4\u63a7\u5236\uff0c\u5229\u7528\u4eff\u771f\u5faa\u73af\u5c55\u5f00\u8fdb\u884c\u6570\u636e\u9ad8\u6548\u5b89\u5168\u63a2\u7d22\uff0c\u6574\u5408\u4e2a\u6027\u5316\u8212\u9002\u9608\u503c\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u9a8c\u8bc1\uff0cPrioriTouch\u80fd\u591f\u9002\u5e94\u7528\u6237\u63a5\u89e6\u504f\u597d\uff0c\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\uff0c\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u8212\u9002\u5ea6\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u591a\u63a5\u89e6\u7269\u7406\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u4f18\u5148\u7ea7\u63a7\u5236\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u62a4\u7406\u573a\u666f\u3002"}}
{"id": "2509.18526", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18526", "abs": "https://arxiv.org/abs/2509.18526", "authors": ["Han Zeng", "Haibo Wang", "Luhao Fan", "Bingcheng Zhu", "Xiaohu You", "Zaichen Zhang"], "title": "AI Agent Access (A\\^3) Network: An Embodied, Communication-Aware Multi-Agent Framework for 6G Coverage", "comment": null, "summary": "The vision of 6G communication demands autonomous and resilient networking in\nenvironments without fixed infrastructure. Yet most multi-agent reinforcement\nlearning (MARL) approaches focus on isolated stages - exploration, relay\nformation, or access - under static deployments and centralized control,\nlimiting adaptability. We propose the AI Agent Access (A\\^3) Network, a\nunified, embodied intelligence-driven framework that transforms multi-agent\nnetworking into a dynamic, decentralized, and end-to-end system. Unlike prior\nschemes, the A\\^3 Network integrates exploration, target user access, and\nbackhaul maintenance within a single learning process, while supporting\non-demand agent addition during runtime. Its decentralized policies ensure that\neven a single agent can operate independently with limited observations, while\ncoordinated agents achieve scalable, communication-optimized coverage. By\nembedding link-level communication metrics into actor-critic learning, the A\\^3\nNetwork couples topology formation with robust decision-making. Numerical\nsimulations demonstrate that the A\\^3 Network not only balances exploration and\ncommunication efficiency but also delivers system-level adaptability absent in\nexisting MARL frameworks, offering a new paradigm for 6G multi-agent networks.", "AI": {"tldr": "\u63d0\u51faA^3\u7f51\u7edc\u6846\u67b6\uff0c\u5c06\u591a\u667a\u80fd\u4f53\u7f51\u7edc\u8f6c\u53d8\u4e3a\u52a8\u6001\u3001\u53bb\u4e2d\u5fc3\u5316\u7684\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u96c6\u6210\u63a2\u7d22\u3001\u7528\u6237\u63a5\u5165\u548c\u56de\u7a0b\u7ef4\u62a4\u4e8e\u5355\u4e00\u5b66\u4e60\u8fc7\u7a0b\uff0c\u652f\u6301\u8fd0\u884c\u65f6\u6309\u9700\u6dfb\u52a0\u667a\u80fd\u4f53\u3002", "motivation": "6G\u901a\u4fe1\u9700\u8981\u5728\u6ca1\u6709\u56fa\u5b9a\u57fa\u7840\u8bbe\u65bd\u7684\u73af\u5883\u4e0b\u5b9e\u73b0\u81ea\u4e3b\u548c\u5f39\u6027\u7f51\u7edc\uff0c\u4f46\u73b0\u6709MARL\u65b9\u6cd5\u5c40\u9650\u4e8e\u9759\u6001\u90e8\u7f72\u548c\u96c6\u4e2d\u63a7\u5236\u4e0b\u7684\u5b64\u7acb\u9636\u6bb5\uff0c\u7f3a\u4e4f\u9002\u5e94\u6027\u3002", "method": "\u91c7\u7528\u53bb\u4e2d\u5fc3\u5316\u7b56\u7565\u548cactor-critic\u5b66\u4e60\uff0c\u5c06\u94fe\u8def\u7ea7\u901a\u4fe1\u6307\u6807\u5d4c\u5165\u5b66\u4e60\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u62d3\u6251\u5f62\u6210\u4e0e\u9c81\u68d2\u51b3\u7b56\u7684\u8026\u5408\u3002", "result": "\u6570\u503c\u6a21\u62df\u663e\u793aA^3\u7f51\u7edc\u4e0d\u4ec5\u5e73\u8861\u4e86\u63a2\u7d22\u548c\u901a\u4fe1\u6548\u7387\uff0c\u8fd8\u63d0\u4f9b\u4e86\u73b0\u6709MARL\u6846\u67b6\u7f3a\u4e4f\u7684\u7cfb\u7edf\u7ea7\u9002\u5e94\u6027\u3002", "conclusion": "A^3\u7f51\u7edc\u4e3a6G\u591a\u667a\u80fd\u4f53\u7f51\u7edc\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u5b9e\u73b0\u52a8\u6001\u3001\u53bb\u4e2d\u5fc3\u5316\u7684\u7aef\u5230\u7aef\u7cfb\u7edf\u3002"}}
{"id": "2509.18198", "categories": ["cs.AI", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18198", "abs": "https://arxiv.org/abs/2509.18198", "authors": ["Rui Liu", "Zikang Wang", "Peng Gao", "Yu Shen", "Pratap Tokekar", "Ming Lin"], "title": "MMCD: Multi-Modal Collaborative Decision-Making for Connected Autonomy with Knowledge Distillation", "comment": null, "summary": "Autonomous systems have advanced significantly, but challenges persist in\naccident-prone environments where robust decision-making is crucial. A single\nvehicle's limited sensor range and obstructed views increase the likelihood of\naccidents. Multi-vehicle connected systems and multi-modal approaches,\nleveraging RGB images and LiDAR point clouds, have emerged as promising\nsolutions. However, existing methods often assume the availability of all data\nmodalities and connected vehicles during both training and testing, which is\nimpractical due to potential sensor failures or missing connected vehicles. To\naddress these challenges, we introduce a novel framework MMCD (Multi-Modal\nCollaborative Decision-making) for connected autonomy. Our framework fuses\nmulti-modal observations from ego and collaborative vehicles to enhance\ndecision-making under challenging conditions. To ensure robust performance when\ncertain data modalities are unavailable during testing, we propose an approach\nbased on cross-modal knowledge distillation with a teacher-student model\nstructure. The teacher model is trained with multiple data modalities, while\nthe student model is designed to operate effectively with reduced modalities.\nIn experiments on $\\textit{connected autonomous driving with ground vehicles}$\nand $\\textit{aerial-ground vehicles collaboration}$, our method improves\ndriving safety by up to ${\\it 20.7}\\%$, surpassing the best-existing baseline\nin detecting potential accidents and making safe driving decisions. More\ninformation can be found on our website https://ruiiu.github.io/mmcd.", "AI": {"tldr": "\u63d0\u51fa\u4e86MMCD\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u534f\u4f5c\u51b3\u7b56\u548c\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\uff0c\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u4f20\u611f\u5668\u6545\u969c\u6216\u8fde\u63a5\u8f66\u8f86\u7f3a\u5931\u65f6\u7684\u9c81\u68d2\u51b3\u7b56\u95ee\u9898", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u8bad\u7ec3\u548c\u6d4b\u8bd5\u65f6\u6240\u6709\u6570\u636e\u6a21\u6001\u548c\u8fde\u63a5\u8f66\u8f86\u90fd\u53ef\u7528\uff0c\u8fd9\u5728\u4f20\u611f\u5668\u6545\u969c\u6216\u8fde\u63a5\u8f66\u8f86\u7f3a\u5931\u65f6\u4e0d\u73b0\u5b9e\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u51b3\u7b56\u6846\u67b6", "method": "MMCD\u6846\u67b6\u878d\u5408\u81ea\u8f66\u548c\u534f\u4f5c\u8f66\u8f86\u7684\u591a\u6a21\u6001\u89c2\u6d4b\u6570\u636e\uff0c\u91c7\u7528\u6559\u5e08-\u5b66\u751f\u6a21\u578b\u7684\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u6559\u5e08\u6a21\u578b\u4f7f\u7528\u591a\u6a21\u6001\u6570\u636e\u8bad\u7ec3\uff0c\u5b66\u751f\u6a21\u578b\u5728\u6a21\u6001\u51cf\u5c11\u65f6\u4ecd\u80fd\u6709\u6548\u8fd0\u884c", "result": "\u5728\u8fde\u63a5\u81ea\u52a8\u9a7e\u9a76\u548c\u7a7a\u5730\u8f66\u8f86\u534f\u4f5c\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5c06\u9a7e\u9a76\u5b89\u5168\u6027\u63d0\u9ad8\u4e8620.7%\uff0c\u5728\u6f5c\u5728\u4e8b\u6545\u68c0\u6d4b\u548c\u5b89\u5168\u9a7e\u9a76\u51b3\u7b56\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u57fa\u7ebf", "conclusion": "MMCD\u6846\u67b6\u901a\u8fc7\u591a\u6a21\u6001\u534f\u4f5c\u548c\u77e5\u8bc6\u84b8\u998f\u6709\u6548\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u6311\u6218\u6027\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027"}}
{"id": "2509.18446", "categories": ["cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18446", "abs": "https://arxiv.org/abs/2509.18446", "authors": ["Sarah H. Cen", "Andrew Ilyas", "Hedi Driss", "Charlotte Park", "Aspen Hopkins", "Chara Podimata", "Aleksander M\u0105dry"], "title": "Large-Scale, Longitudinal Study of Large Language Models During the 2024 US Election Season", "comment": "100 pages, 69 figures", "summary": "The 2024 US presidential election is the first major contest to occur in the\nUS since the popularization of large language models (LLMs). Building on\nlessons from earlier shifts in media (most notably social media's well studied\nrole in targeted messaging and political polarization) this moment raises\nurgent questions about how LLMs may shape the information ecosystem and\ninfluence political discourse. While platforms have announced some election\nsafeguards, how well they work in practice remains unclear. Against this\nbackdrop, we conduct a large-scale, longitudinal study of 12 models, queried\nusing a structured survey with over 12,000 questions on a near-daily cadence\nfrom July through November 2024. Our design systematically varies content and\nformat, resulting in a rich dataset that enables analyses of the models'\nbehavior over time (e.g., across model updates), sensitivity to steering,\nresponsiveness to instructions, and election-related knowledge and \"beliefs.\"\nIn the latter half of our work, we perform four analyses of the dataset that\n(i) study the longitudinal variation of model behavior during election season,\n(ii) illustrate the sensitivity of election-related responses to demographic\nsteering, (iii) interrogate the models' beliefs about candidates' attributes,\nand (iv) reveal the models' implicit predictions of the election outcome. To\nfacilitate future evaluations of LLMs in electoral contexts, we detail our\nmethodology, from question generation to the querying pipeline and third-party\ntooling. We also publicly release our dataset at\nhttps://huggingface.co/datasets/sarahcen/llm-election-data-2024", "AI": {"tldr": "\u672c\u6587\u5bf912\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u57282024\u5e74\u7f8e\u56fd\u603b\u7edf\u9009\u4e3e\u671f\u95f4\u7684\u884c\u4e3a\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u7eb5\u5411\u7814\u7a76\uff0c\u5206\u6790\u4e86\u6a21\u578b\u5bf9\u9009\u4e3e\u76f8\u5173\u95ee\u9898\u7684\u54cd\u5e94\u53d8\u5316\u3001\u4eba\u53e3\u7edf\u8ba1\u5f15\u5bfc\u654f\u611f\u6027\u3001\u5019\u9009\u4eba\u5c5e\u6027\u8ba4\u77e5\u4ee5\u53ca\u9009\u4e3e\u7ed3\u679c\u7684\u9690\u542b\u9884\u6d4b\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u666e\u53ca\uff0c2024\u5e74\u7f8e\u56fd\u603b\u7edf\u9009\u4e3e\u6210\u4e3a\u9996\u4e2a\u9762\u4e34LLM\u5f71\u54cd\u7684\u4e3b\u8981\u653f\u6cbb\u7ade\u8d5b\u3002\u7814\u7a76\u65e8\u5728\u4e86\u89e3LLM\u5982\u4f55\u5851\u9020\u4fe1\u606f\u751f\u6001\u7cfb\u7edf\u548c\u5f71\u54cd\u653f\u6cbb\u8bdd\u8bed\uff0c\u4ee5\u53ca\u5e73\u53f0\u9009\u4e3e\u4fdd\u62a4\u63aa\u65bd\u7684\u5b9e\u9645\u6548\u679c\u3002", "method": "\u91c7\u7528\u7ed3\u6784\u5316\u8c03\u67e5\u95ee\u5377\uff0c\u57282024\u5e747\u6708\u81f311\u6708\u671f\u95f4\u4ee5\u8fd1\u6bcf\u65e5\u9891\u7387\u5bf912\u4e2a\u6a21\u578b\u8fdb\u884c\u8d85\u8fc712,000\u4e2a\u95ee\u9898\u7684\u67e5\u8be2\uff0c\u7cfb\u7edf\u6027\u5730\u53d8\u5316\u5185\u5bb9\u548c\u683c\u5f0f\uff0c\u521b\u5efa\u4e30\u5bcc\u6570\u636e\u96c6\u7528\u4e8e\u5206\u6790\u6a21\u578b\u884c\u4e3a\u968f\u65f6\u95f4\u7684\u53d8\u5316\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u6a21\u578b\u5728\u9009\u4e3e\u5b63\u8282\u7684\u884c\u4e3a\u53d8\u5316\u3001\u5bf9\u4eba\u53e3\u7edf\u8ba1\u5f15\u5bfc\u7684\u654f\u611f\u6027\u3001\u5bf9\u5019\u9009\u4eba\u5c5e\u6027\u7684\u8ba4\u77e5\u4ee5\u53ca\u9690\u542b\u7684\u9009\u4e3e\u7ed3\u679c\u9884\u6d4b\uff0c\u4e3a\u8bc4\u4f30LLM\u5728\u9009\u4e3e\u73af\u5883\u4e2d\u7684\u5f71\u54cd\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u672a\u6765\u8bc4\u4f30LLM\u5728\u9009\u4e3e\u80cc\u666f\u4e0b\u7684\u5f71\u54cd\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u6846\u67b6\u548c\u516c\u5f00\u6570\u636e\u96c6\uff0c\u5f3a\u8c03\u4e86\u6301\u7eed\u76d1\u63a7LLM\u5728\u653f\u6cbb\u8bdd\u8bed\u4e2d\u4f5c\u7528\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.18455", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18455", "abs": "https://arxiv.org/abs/2509.18455", "authors": ["Yunshuang Li", "Yiyang Ling", "Gaurav S. Sukhatme", "Daniel Seita"], "title": "Learning Geometry-Aware Nonprehensile Pushing and Pulling with Dexterous Hands", "comment": null, "summary": "Nonprehensile manipulation, such as pushing and pulling, enables robots to\nmove, align, or reposition objects that may be difficult to grasp due to their\ngeometry, size, or relationship to the robot or the environment. Much of the\nexisting work in nonprehensile manipulation relies on parallel-jaw grippers or\ntools such as rods and spatulas. In contrast, multi-fingered dexterous hands\noffer richer contact modes and versatility for handling diverse objects to\nprovide stable support over the objects, which compensates for the difficulty\nof modeling the dynamics of nonprehensile manipulation. Therefore, we propose\nGeometry-aware Dexterous Pushing and Pulling (GD2P) for nonprehensile\nmanipulation with dexterous robotic hands. We study pushing and pulling by\nframing the problem as synthesizing and learning pre-contact dexterous hand\nposes that lead to effective manipulation. We generate diverse hand poses via\ncontact-guided sampling, filter them using physics simulation, and train a\ndiffusion model conditioned on object geometry to predict viable poses. At test\ntime, we sample hand poses and use standard motion planners to select and\nexecute pushing and pulling actions. We perform 840 real-world experiments with\nan Allegro Hand, comparing our method to baselines. The results indicate that\nGD2P offers a scalable route for training dexterous nonprehensile manipulation\npolicies. We further demonstrate GD2P on a LEAP Hand, highlighting its\napplicability to different hand morphologies. Our pre-trained models and\ndataset, including 1.3 million hand poses across 2.3k objects, will be\nopen-source to facilitate further research. Our project website is available\nat: geodex2p.github.io.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51e0\u4f55\u611f\u77e5\u7684\u591a\u6307\u7075\u5de7\u624b\u63a8\u62c9\u64cd\u4f5c\uff08GD2P\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u975e\u6293\u53d6\u5f0f\u64cd\u4f5c\u3002\u901a\u8fc7\u63a5\u89e6\u5f15\u5bfc\u91c7\u6837\u751f\u6210\u591a\u6837\u5316\u7684\u624b\u90e8\u59ff\u6001\uff0c\u5229\u7528\u7269\u7406\u6a21\u62df\u8fdb\u884c\u7b5b\u9009\uff0c\u5e76\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u6765\u9884\u6d4b\u53ef\u884c\u7684\u624b\u90e8\u59ff\u6001\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u5177\u6709\u826f\u597d\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u975e\u6293\u53d6\u5f0f\u64cd\u4f5c\u4e3b\u8981\u4f9d\u8d56\u5e73\u884c\u5939\u722a\u6216\u5de5\u5177\uff0c\u800c\u591a\u6307\u7075\u5de7\u624b\u80fd\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u63a5\u89e6\u6a21\u5f0f\u548c\u7a33\u5b9a\u6027\uff0c\u4f46\u975e\u6293\u53d6\u64cd\u4f5c\u7684\u52a8\u529b\u5b66\u5efa\u6a21\u56f0\u96be\u3002\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u9488\u5bf9\u7075\u5de7\u624b\u7684\u975e\u6293\u53d6\u64cd\u4f5c\u65b9\u6cd5\u3002", "method": "1. \u901a\u8fc7\u63a5\u89e6\u5f15\u5bfc\u91c7\u6837\u751f\u6210\u591a\u6837\u5316\u7684\u9884\u63a5\u89e6\u624b\u90e8\u59ff\u6001\uff1b2. \u4f7f\u7528\u7269\u7406\u6a21\u62df\u7b5b\u9009\u53ef\u884c\u59ff\u6001\uff1b3. \u8bad\u7ec3\u57fa\u4e8e\u7269\u4f53\u51e0\u4f55\u6761\u4ef6\u7684\u6269\u6563\u6a21\u578b\u6765\u9884\u6d4b\u53ef\u884c\u624b\u90e8\u59ff\u6001\uff1b4. \u5728\u6d4b\u8bd5\u65f6\u91c7\u6837\u624b\u90e8\u59ff\u6001\u5e76\u4f7f\u7528\u6807\u51c6\u8fd0\u52a8\u89c4\u5212\u5668\u6267\u884c\u63a8\u62c9\u52a8\u4f5c\u3002", "result": "\u5728Allegro Hand\u4e0a\u8fdb\u884c\u4e86840\u6b21\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660eGD2P\u4e3a\u8bad\u7ec3\u7075\u5de7\u975e\u6293\u53d6\u64cd\u4f5c\u7b56\u7565\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u9014\u5f84\u3002\u5728LEAP Hand\u4e0a\u7684\u8fdb\u4e00\u6b65\u6f14\u793a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5bf9\u4e0d\u540c\u624b\u90e8\u5f62\u6001\u7684\u9002\u7528\u6027\u3002", "conclusion": "GD2P\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6307\u7075\u5de7\u624b\u975e\u6293\u53d6\u64cd\u4f5c\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u5305\u542b130\u4e07\u624b\u90e8\u59ff\u6001\u548c2300\u4e2a\u7269\u4f53\u7684\u5f00\u6e90\u6570\u636e\u96c6\u548c\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5c06\u4fc3\u8fdb\u76f8\u5173\u9886\u57df\u7684\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2509.18624", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18624", "abs": "https://arxiv.org/abs/2509.18624", "authors": ["Yue Zhang", "Xinzhi Zhong", "Soyoung Ahn", "Yajie Zou", "Zhengbing He"], "title": "Interaction-aware Lane-Changing Early Warning System in Congested Traffic", "comment": null, "summary": "Lane changes (LCs) in congested traffic are complex, multi-vehicle\ninteractive events that pose significant safety concerns. Providing early\nwarnings can enable more proactive driver assistance system and support more\ninformed decision-making for drivers under LCs. This paper presents an\ninteraction-aware Lane-Changing Early Warning (LCEW) system designed to issue\nreliable early warning signals based on future trajectory predictions. We first\ninvestigate the stochastic nature of LCs, characterized by (i) variable-size\nmulti-vehicle interactions and (ii) the direct and indirect risks resulting\nfrom these interactions. To model these stochastic interactions, a Social\nSpatio-Temporal Graph Convolutional Neural Network framework informed by mutual\ninformation (STGCNN-MI) is introduced to predict multi-vehicle trajectories. By\nleveraging a MI-based adjacency matrix, the framework enhances trajectory\nprediction accuracy while providing interpretable representations of vehicle\ninteractions. Then, potential collisions between the LC vehicle and adjacent\nvehicles (direct risks) or among the non-adjacent vehicles (indirect risks) are\nidentified using oriented bounding box detection applied to the predicted\ntrajectories. Finally, a warning signal is generated to inform the LC driver of\nlocation of potential collisions within the predicted time window. Traffic\nsimulation experiments conducted in SUMO demonstrate that the proposed\ninteraction-aware LCEW improves both vehicle-level safety and overall traffic\nefficiency, while also promoting more natural behavioral adaptation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ea4\u4e92\u611f\u77e5\u7684\u8f66\u9053\u53d8\u6362\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\u6765\u8bc6\u522b\u76f4\u63a5\u548c\u95f4\u63a5\u98ce\u9669\uff0c\u63d0\u9ad8\u4ea4\u901a\u5b89\u5168\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u62e5\u5835\u4ea4\u901a\u4e2d\u8f66\u9053\u53d8\u6362\u5e26\u6765\u7684\u5b89\u5168\u98ce\u9669\uff0c\u901a\u8fc7\u65e9\u671f\u9884\u8b66\u5e2e\u52a9\u9a7e\u9a76\u5458\u505a\u51fa\u66f4\u660e\u667a\u7684\u51b3\u7b56\uff0c\u63d0\u5347\u9a7e\u9a76\u8f85\u52a9\u7cfb\u7edf\u7684\u4e3b\u52a8\u6027\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u4e92\u4fe1\u606f\u7684\u793e\u4f1a\u65f6\u7a7a\u56fe\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\u9884\u6d4b\u591a\u8f66\u8f86\u8f68\u8ff9\uff0c\u901a\u8fc7\u5b9a\u5411\u8fb9\u754c\u6846\u68c0\u6d4b\u8bc6\u522b\u6f5c\u5728\u78b0\u649e\u98ce\u9669\uff0c\u751f\u6210\u9884\u8b66\u4fe1\u53f7\u3002", "result": "\u5728SUMO\u4ea4\u901a\u6a21\u62df\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u7cfb\u7edf\u63d0\u9ad8\u4e86\u8f66\u8f86\u7ea7\u5b89\u5168\u6027\u548c\u6574\u4f53\u4ea4\u901a\u6548\u7387\uff0c\u4fc3\u8fdb\u4e86\u66f4\u81ea\u7136\u7684\u884c\u4e3a\u9002\u5e94\u3002", "conclusion": "\u4ea4\u4e92\u611f\u77e5\u7684\u8f66\u9053\u53d8\u6362\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u63d0\u5347\u4ea4\u901a\u5b89\u5168\u548c\u6548\u7387\uff0c\u4e3a\u667a\u80fd\u9a7e\u9a76\u8f85\u52a9\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2509.18215", "categories": ["cs.AI", "cs.LO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.18215", "abs": "https://arxiv.org/abs/2509.18215", "authors": ["Timotheus Kampik", "Kristijonas \u010cyras", "Jos\u00e9 Ruiz Alarc\u00f3n"], "title": "Change in Quantitative Bipolar Argumentation: Sufficient, Necessary, and Counterfactual Explanations", "comment": "The publisher's version contains a notation glitch in Example 3, 5th\n  line, first sub-script G should be G'. This has always been G' in authors'\n  version. Thanks to J. Lanser for pointing this out", "summary": "This paper presents a formal approach to explaining change of inference in\nQuantitative Bipolar Argumentation Frameworks (QBAFs). When drawing conclusions\nfrom a QBAF and updating the QBAF to then again draw conclusions (and so on),\nour approach traces changes -- which we call strength inconsistencies -- in the\npartial order over argument strengths that a semantics establishes on some\narguments of interest, called topic arguments. We trace the causes of strength\ninconsistencies to specific arguments, which then serve as explanations. We\nidentify sufficient, necessary, and counterfactual explanations for strength\ninconsistencies and show that strength inconsistency explanations exist if and\nonly if an update leads to strength inconsistency. We define a heuristic-based\napproach to facilitate the search for strength inconsistency explanations, for\nwhich we also provide an implementation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5f62\u5f0f\u5316\u65b9\u6cd5\u6765\u89e3\u91ca\u5b9a\u91cf\u53cc\u6781\u8bba\u8bc1\u6846\u67b6\uff08QBAF\uff09\u4e2d\u63a8\u7406\u53d8\u5316\u7684\u539f\u56e0\uff0c\u901a\u8fc7\u8ffd\u8e2a\u8bba\u8bc1\u5f3a\u5ea6\u90e8\u5206\u987a\u5e8f\u7684\u53d8\u5316\u6765\u8bc6\u522b\u5f3a\u5ea6\u4e0d\u4e00\u81f4\u6027\uff0c\u5e76\u63d0\u4f9b\u57fa\u4e8e\u542f\u53d1\u5f0f\u7684\u89e3\u91ca\u641c\u7d22\u65b9\u6cd5\u3002", "motivation": "\u5728QBAF\u4e2d\u8fdb\u884c\u63a8\u7406\u5e76\u66f4\u65b0\u6846\u67b6\u540e\uff0c\u8bba\u8bc1\u5f3a\u5ea6\u7684\u90e8\u5206\u987a\u5e8f\u53ef\u80fd\u53d1\u751f\u53d8\u5316\uff0c\u8fd9\u79cd\u5f3a\u5ea6\u4e0d\u4e00\u81f4\u6027\u9700\u8981\u88ab\u89e3\u91ca\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u7cfb\u7edf\u65b9\u6cd5\u6765\u8ffd\u8e2a\u548c\u89e3\u91ca\u8fd9\u4e9b\u53d8\u5316\u3002", "method": "\u901a\u8fc7\u8ffd\u8e2a\u4e3b\u9898\u8bba\u8bc1\u5f3a\u5ea6\u90e8\u5206\u987a\u5e8f\u7684\u53d8\u5316\u6765\u8bc6\u522b\u5f3a\u5ea6\u4e0d\u4e00\u81f4\u6027\uff0c\u5c06\u539f\u56e0\u8ffd\u6eaf\u5230\u7279\u5b9a\u8bba\u8bc1\uff0c\u5b9a\u4e49\u5145\u5206\u3001\u5fc5\u8981\u548c\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u5e76\u5f00\u53d1\u57fa\u4e8e\u542f\u53d1\u5f0f\u7684\u89e3\u91ca\u641c\u7d22\u7b97\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86\u5f3a\u5ea6\u4e0d\u4e00\u81f4\u6027\u89e3\u91ca\u5b58\u5728\u7684\u5145\u8981\u6761\u4ef6\u662f\u66f4\u65b0\u5bfc\u81f4\u5f3a\u5ea6\u4e0d\u4e00\u81f4\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u76f8\u5e94\u7684\u5b9e\u73b0\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u91caQBAF\u4e2d\u63a8\u7406\u53d8\u5316\u7684\u539f\u56e0\uff0c\u4e3a\u7406\u89e3\u8bba\u8bc1\u6846\u67b6\u7684\u52a8\u6001\u6f14\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u5de5\u5177\u3002"}}
{"id": "2509.18509", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2509.18509", "abs": "https://arxiv.org/abs/2509.18509", "authors": ["Jianhua Li", "Yin Paradies", "Trina Myers", "Robin Doss", "Armita Zarnegar", "Jack Reis"], "title": "Developing a Decolonial Mindset for Indigenising Computing Education (CE)", "comment": null, "summary": "The underrepresentation of First Peoples in computing education reflects\ncolonial legacies embedded in curricula, pedagogies, and digital\ninfrastructures. This paper introduces the \\textbf{Decolonial Mindset Stack\n(DMS)}, a seven-layer framework for educator transformation:\n\\textbf{Recognition, Reflection, Reframing, Reembedding, Reciprocity,\nReclamation}, and \\textbf{Resurgence}. Grounded in Freirean critical pedagogy\nand Indigenous methodologies, the DMS aligns with relational lenses of ``About\nMe,'' ``Between Us,'' and ``By Us.'' It fosters self-reflexivity, relational\naccountability, and Indigenous sovereignty in computing education, reframing\nunderrepresentation as systemic exclusion. The DMS provides both theoretical\ngrounding and pathways for practice, positioning indigenisation not as an\nendpoint but as a sustained ethical commitment to transformative justice and\nthe co-creation of computing education with First Peoples.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e03\u5c42\u6846\u67b6\u2014\u2014\u53bb\u6b96\u6c11\u601d\u7ef4\u5806\u6808\uff08DMS\uff09\uff0c\u7528\u4e8e\u6559\u80b2\u8005\u8f6c\u578b\uff0c\u65e8\u5728\u89e3\u51b3\u8ba1\u7b97\u6559\u80b2\u4e2d\u7b2c\u4e00\u6c11\u65cf\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u8bc6\u522b\u3001\u53cd\u601d\u3001\u91cd\u6784\u3001\u91cd\u65b0\u5d4c\u5165\u3001\u4e92\u60e0\u3001\u91cd\u65b0\u4e3b\u5f20\u548c\u590d\u5174\u7b49\u6b65\u9aa4\uff0c\u4fc3\u8fdb\u8ba1\u7b97\u6559\u80b2\u7684\u53bb\u6b96\u6c11\u5316\u3002", "motivation": "\u8ba1\u7b97\u6559\u80b2\u4e2d\u7b2c\u4e00\u6c11\u65cf\u7684\u4ee3\u8868\u6027\u4e0d\u8db3\u53cd\u6620\u4e86\u8bfe\u7a0b\u3001\u6559\u5b66\u6cd5\u548c\u6570\u5b57\u57fa\u7840\u8bbe\u65bd\u4e2d\u6839\u6df1\u8482\u56fa\u7684\u6b96\u6c11\u9057\u7559\u95ee\u9898\uff0c\u9700\u8981\u901a\u8fc7\u53bb\u6b96\u6c11\u5316\u7684\u65b9\u6cd5\u8fdb\u884c\u7cfb\u7edf\u6027\u53d8\u9769\u3002", "method": "\u57fa\u4e8eFreire\u7684\u6279\u5224\u6027\u6559\u5b66\u6cd5\u548c\u571f\u8457\u65b9\u6cd5\u8bba\uff0cDMS\u6846\u67b6\u5305\u542b\u4e03\u4e2a\u5c42\u6b21\uff1a\u8bc6\u522b\u3001\u53cd\u601d\u3001\u91cd\u6784\u3001\u91cd\u65b0\u5d4c\u5165\u3001\u4e92\u60e0\u3001\u91cd\u65b0\u4e3b\u5f20\u548c\u590d\u5174\uff0c\u5e76\u4e0e\u201c\u5173\u4e8e\u6211\u201d\u3001\u201c\u6211\u4eec\u4e4b\u95f4\u201d\u548c\u201c\u7531\u6211\u4eec\u201d\u7684\u5173\u7cfb\u89c6\u89d2\u5bf9\u9f50\u3002", "result": "DMS\u6846\u67b6\u4e3a\u6559\u80b2\u8005\u63d0\u4f9b\u4e86\u81ea\u6211\u53cd\u601d\u3001\u5173\u7cfb\u8d23\u4efb\u548c\u571f\u8457\u4e3b\u6743\u5728\u8ba1\u7b97\u6559\u80b2\u4e2d\u7684\u5b9e\u8df5\u8def\u5f84\uff0c\u5c06\u4ee3\u8868\u6027\u4e0d\u8db3\u91cd\u65b0\u5b9a\u4e49\u4e3a\u7cfb\u7edf\u6027\u6392\u65a5\u3002", "conclusion": "DMS\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u8def\u5f84\uff0c\u8fd8\u5c06\u571f\u8457\u5316\u5b9a\u4f4d\u4e3a\u5bf9\u53d8\u9769\u6027\u6b63\u4e49\u548c\u4e0e\u7b2c\u4e00\u6c11\u65cf\u5171\u540c\u521b\u5efa\u8ba1\u7b97\u6559\u80b2\u7684\u6301\u7eed\u4f26\u7406\u627f\u8bfa\uff0c\u800c\u975e\u7ec8\u70b9\u3002"}}
{"id": "2509.18460", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.18460", "abs": "https://arxiv.org/abs/2509.18460", "authors": ["Haeyoon Han", "Mahdi Taheri", "Soon-Jo Chung", "Fred Y. Hadaegh"], "title": "A Counterfactual Reasoning Framework for Fault Diagnosis in Robot Perception Systems", "comment": null, "summary": "Perception systems provide a rich understanding of the environment for\nautonomous systems, shaping decisions in all downstream modules. Hence,\naccurate detection and isolation of faults in perception systems is important.\nFaults in perception systems pose particular challenges: faults are often tied\nto the perceptual context of the environment, and errors in their multi-stage\npipelines can propagate across modules. To address this, we adopt a\ncounterfactual reasoning approach to propose a framework for fault detection\nand isolation (FDI) in perception systems. As opposed to relying on physical\nredundancy (i.e., having extra sensors), our approach utilizes analytical\nredundancy with counterfactual reasoning to construct perception reliability\ntests as causal outcomes influenced by system states and fault scenarios.\nCounterfactual reasoning generates reliability test results under hypothesized\nfaults to update the belief over fault hypotheses. We derive both passive and\nactive FDI methods. While the passive FDI can be achieved by belief updates,\nthe active FDI approach is defined as a causal bandit problem, where we utilize\nMonte Carlo Tree Search (MCTS) with upper confidence bound (UCB) to find\ncontrol inputs that maximize a detection and isolation metric, designated as\nEffective Information (EI). The mentioned metric quantifies the informativeness\nof control inputs for FDI. We demonstrate the approach in a robot exploration\nscenario, where a space robot performing vision-based navigation actively\nadjusts its attitude to increase EI and correctly isolate faults caused by\nsensor damage, dynamic scenes, and perceptual degradation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cd\u4e8b\u5b9e\u63a8\u7406\u7684\u611f\u77e5\u7cfb\u7edf\u6545\u969c\u68c0\u6d4b\u4e0e\u9694\u79bb\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u5197\u4f59\u800c\u975e\u7269\u7406\u5197\u4f59\u6765\u6784\u5efa\u611f\u77e5\u53ef\u9760\u6027\u6d4b\u8bd5\uff0c\u91c7\u7528\u88ab\u52a8\u548c\u4e3b\u52a8\u4e24\u79cdFDI\u65b9\u6cd5\uff0c\u5e76\u5728\u673a\u5668\u4eba\u63a2\u7d22\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u611f\u77e5\u7cfb\u7edf\u4e3a\u81ea\u4e3b\u7cfb\u7edf\u63d0\u4f9b\u73af\u5883\u7406\u89e3\uff0c\u5176\u6545\u969c\u4f1a\u5f71\u54cd\u4e0b\u6e38\u6a21\u5757\u51b3\u7b56\u3002\u611f\u77e5\u7cfb\u7edf\u6545\u969c\u5177\u6709\u7279\u6b8a\u6027\uff1a\u4e0e\u611f\u77e5\u73af\u5883\u4e0a\u4e0b\u6587\u76f8\u5173\uff0c\u4e14\u591a\u9636\u6bb5\u6d41\u6c34\u7ebf\u4e2d\u7684\u9519\u8bef\u4f1a\u5728\u6a21\u5757\u95f4\u4f20\u64ad\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u6765\u786e\u4fdd\u7cfb\u7edf\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528\u53cd\u4e8b\u5b9e\u63a8\u7406\u65b9\u6cd5\u6784\u5efa\u611f\u77e5\u53ef\u9760\u6027\u6d4b\u8bd5\u4f5c\u4e3a\u56e0\u679c\u7ed3\u679c\u3002\u88ab\u52a8FDI\u901a\u8fc7\u4fe1\u5ff5\u66f4\u65b0\u5b9e\u73b0\uff0c\u4e3b\u52a8FDI\u5b9a\u4e49\u4e3a\u56e0\u679c\u591a\u81c2\u8d4c\u535a\u673a\u95ee\u9898\uff0c\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u548c\u4e0a\u7f6e\u4fe1\u754c\u7b97\u6cd5\u5bfb\u627e\u6700\u5927\u5316\u6709\u6548\u4fe1\u606f\u7684\u63a7\u5236\u8f93\u5165\u3002", "result": "\u5728\u673a\u5668\u4eba\u63a2\u7d22\u573a\u666f\u4e2d\u9a8c\u8bc1\uff0c\u7a7a\u95f4\u673a\u5668\u4eba\u901a\u8fc7\u4e3b\u52a8\u8c03\u6574\u59ff\u6001\u589e\u52a0\u6709\u6548\u4fe1\u606f\uff0c\u6210\u529f\u9694\u79bb\u4e86\u7531\u4f20\u611f\u5668\u635f\u574f\u3001\u52a8\u6001\u573a\u666f\u548c\u611f\u77e5\u9000\u5316\u5f15\u8d77\u7684\u6545\u969c\u3002", "conclusion": "\u53cd\u4e8b\u5b9e\u63a8\u7406\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u611f\u77e5\u7cfb\u7edf\u6545\u969c\u68c0\u6d4b\u4e0e\u9694\u79bb\u95ee\u9898\uff0c\u4e3b\u52a8FDI\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u63a7\u5236\u8f93\u5165\u63d0\u9ad8\u4e86\u6545\u969c\u8bca\u65ad\u80fd\u529b\uff0c\u4e3a\u81ea\u4e3b\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u4fdd\u969c\u3002"}}
{"id": "2509.18723", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18723", "abs": "https://arxiv.org/abs/2509.18723", "authors": ["Jan-Hendrik Ewering", "Alessandro Papa", "Simon F. G. Ehlers", "Thomas Seel", "Michael Meindl"], "title": "Dual Iterative Learning Control for Multiple-Input Multiple-Output Dynamics with Validation in Robotic Systems", "comment": "11 pages, 4 figures", "summary": "Solving motion tasks autonomously and accurately is a core ability for\nintelligent real-world systems. To achieve genuine autonomy across multiple\nsystems and tasks, key challenges include coping with unknown dynamics and\novercoming the need for manual parameter tuning, which is especially crucial in\ncomplex Multiple-Input Multiple-Output (MIMO) systems.\n  This paper presents MIMO Dual Iterative Learning Control (DILC), a novel\ndata-driven iterative learning scheme for simultaneous tracking control and\nmodel learning, without requiring any prior system knowledge or manual\nparameter tuning. The method is designed for repetitive MIMO systems and\nintegrates seamlessly with established iterative learning control methods. We\nprovide monotonic convergence conditions for both reference tracking error and\nmodel error in linear time-invariant systems.\n  The DILC scheme -- rapidly and autonomously -- solves various motion tasks in\nhigh-fidelity simulations of an industrial robot and in multiple nonlinear\nreal-world MIMO systems, without requiring model knowledge or manually tuning\nthe algorithm. In our experiments, many reference tracking tasks are solved\nwithin 10-20 trials, and even complex motions are learned in less than 100\niterations. We believe that, because of its rapid and autonomous learning\ncapabilities, DILC has the potential to serve as an efficient building block\nwithin complex learning frameworks for intelligent real-world systems.", "AI": {"tldr": "\u63d0\u51faMIMO\u53cc\u8fed\u4ee3\u5b66\u4e60\u63a7\u5236(DILC)\u65b9\u6cd5\uff0c\u5b9e\u73b0\u65e0\u9700\u5148\u9a8c\u7cfb\u7edf\u77e5\u8bc6\u548c\u624b\u52a8\u53c2\u6570\u8c03\u4f18\u7684\u540c\u65f6\u8ddf\u8e2a\u63a7\u5236\u4e0e\u6a21\u578b\u5b66\u4e60", "motivation": "\u89e3\u51b3\u590d\u6742\u591a\u8f93\u5165\u591a\u8f93\u51fa\u7cfb\u7edf\u4e2d\u672a\u77e5\u52a8\u6001\u548c\u624b\u52a8\u53c2\u6570\u8c03\u4f18\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u771f\u6b63\u81ea\u4e3b\u7684\u8fd0\u52a8\u4efb\u52a1\u63a7\u5236", "method": "\u8bbe\u8ba1\u6570\u636e\u9a71\u52a8\u7684\u8fed\u4ee3\u5b66\u4e60\u65b9\u6848\uff0c\u96c6\u6210\u73b0\u6709\u8fed\u4ee3\u5b66\u4e60\u63a7\u5236\u65b9\u6cd5\uff0c\u4e3a\u7ebf\u6027\u65f6\u4e0d\u53d8\u7cfb\u7edf\u63d0\u4f9b\u5355\u8c03\u6536\u655b\u6761\u4ef6", "result": "\u5728\u9ad8\u4fdd\u771f\u5de5\u4e1a\u673a\u5668\u4eba\u4eff\u771f\u548c\u591a\u4e2a\u975e\u7ebf\u6027\u771f\u5b9eMIMO\u7cfb\u7edf\u4e2d\u5feb\u901f\u81ea\u4e3b\u89e3\u51b3\u5404\u79cd\u8fd0\u52a8\u4efb\u52a1\uff0c\u591a\u6570\u4efb\u52a1\u572810-20\u6b21\u8bd5\u9a8c\u5185\u5b8c\u6210\uff0c\u590d\u6742\u8fd0\u52a8\u5728100\u6b21\u8fed\u4ee3\u5185\u5b66\u4e60", "conclusion": "DILC\u56e0\u5176\u5feb\u901f\u81ea\u4e3b\u5b66\u4e60\u80fd\u529b\uff0c\u6709\u6f5c\u529b\u4f5c\u4e3a\u590d\u6742\u5b66\u4e60\u6846\u67b6\u7684\u9ad8\u6548\u6784\u5efa\u6a21\u5757\uff0c\u7528\u4e8e\u667a\u80fd\u771f\u5b9e\u4e16\u754c\u7cfb\u7edf"}}
{"id": "2509.18216", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18216", "abs": "https://arxiv.org/abs/2509.18216", "authors": ["Amitava Das"], "title": "nDNA -- the Semantic Helix of Artificial Cognition", "comment": null, "summary": "As AI foundation models grow in capability, a deeper question emerges: What\nshapes their internal cognitive identity -- beyond fluency and output?\nBenchmarks measure behavior, but the soul of a model resides in its latent\ngeometry. In this work, we propose Neural DNA (nDNA) as a semantic-genotypic\nrepresentation that captures this latent identity through the intrinsic\ngeometry of belief. At its core, nDNA is synthesized from three principled and\nindispensable dimensions of latent geometry: spectral curvature, which reveals\nthe curvature of conceptual flow across layers; thermodynamic length, which\nquantifies the semantic effort required to traverse representational\ntransitions through layers; and belief vector field, which delineates the\nsemantic torsion fields that guide a model's belief directional orientations.\nLike biological DNA, it encodes ancestry, mutation, and semantic inheritance,\nfound in finetuning and alignment scars, cultural imprints, and architectural\ndrift. In naming it, we open a new field: Neural Genomics, where models are not\njust tools, but digital semantic organisms with traceable inner cognition.\n  Modeling statement. We read AI foundation models as semantic fluid--dynamics:\nmeaning is transported through layers like fluid in a shaped conduit; nDNA is\nthe physics-grade readout of that flow -- a geometry-first measure of how\nmeaning is bent, paid for, and pushed -- yielding a stable, coordinate-free\nneural DNA fingerprint tied to on-input behavior; with this fingerprint we\ncross into biology: tracing lineages across pretraining, fine-tuning,\nalignment, pruning, distillation, and merges; measuring inheritance between\ncheckpoints; detecting drift as traits shift under new data or objectives; and,\nultimately, studying the evolution of artificial cognition to compare models,\ndiagnose risks, and govern change over time.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u795e\u7ecfDNA\uff08nDNA\uff09\u4f5c\u4e3a\u6355\u6349AI\u57fa\u7840\u6a21\u578b\u6f5c\u5728\u8ba4\u77e5\u8eab\u4efd\u7684\u8bed\u4e49-\u57fa\u56e0\u578b\u8868\u793a\uff0c\u901a\u8fc7\u4e09\u4e2a\u51e0\u4f55\u7ef4\u5ea6\u6765\u91cf\u5316\u6a21\u578b\u7684\u5185\u5728\u4fe1\u5ff5\u51e0\u4f55\u7ed3\u6784\u3002", "motivation": "\u968f\u7740AI\u57fa\u7840\u6a21\u578b\u80fd\u529b\u7684\u589e\u957f\uff0c\u9700\u8981\u8d85\u8d8a\u884c\u4e3a\u57fa\u51c6\u6765\u7406\u89e3\u6a21\u578b\u7684\u5185\u5728\u8ba4\u77e5\u8eab\u4efd\uff0c\u63ed\u793a\u6a21\u578b\u6f5c\u5728\u51e0\u4f55\u7ed3\u6784\u4e2d\u7684\"\u7075\u9b42\"\u3002", "method": "\u63d0\u51fanDNA\u8868\u793a\u65b9\u6cd5\uff0c\u57fa\u4e8e\u4e09\u4e2a\u51e0\u4f55\u7ef4\u5ea6\uff1a\u8c31\u66f2\u7387\uff08\u63ed\u793a\u6982\u5ff5\u6d41\u52a8\u7684\u66f2\u7387\uff09\u3001\u70ed\u529b\u5b66\u957f\u5ea6\uff08\u91cf\u5316\u8bed\u4e49\u8f6c\u6362\u7684\u52aa\u529b\uff09\u3001\u4fe1\u5ff5\u5411\u91cf\u573a\uff08\u63cf\u8ff0\u4fe1\u5ff5\u65b9\u5411\u5bfc\u5411\u7684\u8bed\u4e49\u626d\u8f6c\u573a\uff09\u3002", "result": "nDNA\u80fd\u591f\u7a33\u5b9a\u5730\u6355\u6349\u6a21\u578b\u7684\u8bed\u4e49\u6d41\u52a8\u7279\u5f81\uff0c\u5b9e\u73b0\u6a21\u578b\u8c31\u7cfb\u8ffd\u8e2a\u3001\u7ee7\u627f\u5173\u7cfb\u6d4b\u91cf\u3001\u6f02\u79fb\u68c0\u6d4b\u7b49\u529f\u80fd\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5f00\u542f\u4e86\u795e\u7ecf\u57fa\u56e0\u7ec4\u5b66\u65b0\u9886\u57df\uff0c\u5c06AI\u6a21\u578b\u89c6\u4e3a\u5177\u6709\u53ef\u8ffd\u6eaf\u5185\u5728\u8ba4\u77e5\u7684\u6570\u5b57\u8bed\u4e49\u6709\u673a\u4f53\uff0c\u4e3a\u6a21\u578b\u6bd4\u8f83\u3001\u98ce\u9669\u8bca\u65ad\u548c\u6f14\u5316\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2509.18523", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18523", "abs": "https://arxiv.org/abs/2509.18523", "authors": ["Steve Huntsman"], "title": "Automatic coherence-driven inference on arguments", "comment": "Workshop on Data Mining and AI for Law\n  (https://dmail-workshop.github.io/DMAIL2025/)", "summary": "Inconsistencies are ubiquitous in law, administration, and jurisprudence.\nThough a cure is too much to hope for, we propose a technological remedy. Large\nlanguage models (LLMs) can accurately extract propositions from arguments and\ncompile them into natural data structures that enable coherence-driven\ninference (CDI) via combinatorial optimization. This neurosymbolic architecture\nnaturally separates concerns and enables meaningful judgments about the\ncoherence of arguments that can inform legislative and policy analysis and\nlegal reasoning.", "AI": {"tldr": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u6cd5\u5f8b\u8bba\u8bc1\u4e2d\u7684\u547d\u9898\uff0c\u901a\u8fc7\u7ec4\u5408\u4f18\u5316\u8fdb\u884c\u4e00\u81f4\u6027\u9a71\u52a8\u63a8\u7406\uff0c\u4e3a\u6cd5\u5f8b\u548c\u653f\u7b56\u5206\u6790\u63d0\u4f9b\u6280\u672f\u89e3\u51b3\u65b9\u6848", "motivation": "\u6cd5\u5f8b\u3001\u884c\u653f\u548c\u6cd5\u7406\u5b66\u4e2d\u666e\u904d\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u9700\u8981\u6280\u672f\u624b\u6bb5\u6765\u89e3\u51b3\u8fd9\u4e9b\u4e0d\u4e00\u81f4\u6027", "method": "\u5229\u7528LLMs\u4ece\u8bba\u8bc1\u4e2d\u63d0\u53d6\u547d\u9898\u5e76\u7f16\u8bd1\u6210\u81ea\u7136\u6570\u636e\u7ed3\u6784\uff0c\u901a\u8fc7\u7ec4\u5408\u4f18\u5316\u5b9e\u73b0\u4e00\u81f4\u6027\u9a71\u52a8\u63a8\u7406\u7684\u795e\u7ecf\u7b26\u53f7\u67b6\u6784", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u81ea\u7136\u5206\u79bb\u5173\u6ce8\u70b9\uff0c\u5e76\u5bf9\u8bba\u8bc1\u7684\u4e00\u81f4\u6027\u505a\u51fa\u6709\u610f\u4e49\u7684\u5224\u65ad", "conclusion": "\u8be5\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\u53ef\u4ee5\u4e3a\u7acb\u6cd5\u548c\u653f\u7b56\u5206\u6790\u4ee5\u53ca\u6cd5\u5f8b\u63a8\u7406\u63d0\u4f9b\u4fe1\u606f\u652f\u6301"}}
{"id": "2509.18463", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18463", "abs": "https://arxiv.org/abs/2509.18463", "authors": ["Jannick van Buuren", "Roberto Giglio", "Loris Roveda", "Luka Peternel"], "title": "Robotic Skill Diversification via Active Mutation of Reward Functions in Reinforcement Learning During a Liquid Pouring Task", "comment": null, "summary": "This paper explores how deliberate mutations of reward function in\nreinforcement learning can produce diversified skill variations in robotic\nmanipulation tasks, examined with a liquid pouring use case. To this end, we\ndeveloped a new reward function mutation framework that is based on applying\nGaussian noise to the weights of the different terms in the reward function.\nInspired by the cost-benefit tradeoff model from human motor control, we\ndesigned the reward function with the following key terms: accuracy, time, and\neffort. The study was performed in a simulation environment created in NVIDIA\nIsaac Sim, and the setup included Franka Emika Panda robotic arm holding a\nglass with a liquid that needed to be poured into a container. The\nreinforcement learning algorithm was based on Proximal Policy Optimization. We\nsystematically explored how different configurations of mutated weights in the\nrewards function would affect the learned policy. The resulting policies\nexhibit a wide range of behaviours: from variations in execution of the\noriginally intended pouring task to novel skills useful for unexpected tasks,\nsuch as container rim cleaning, liquid mixing, and watering. This approach\noffers promising directions for robotic systems to perform diversified learning\nof specific tasks, while also potentially deriving meaningful skills for future\ntasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5956\u52b1\u51fd\u6570\u53d8\u5f02\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u5956\u52b1\u51fd\u6570\u7684\u4e0d\u540c\u9879\u4e0a\u5e94\u7528\u9ad8\u65af\u566a\u58f0\u6765\u4ea7\u751f\u591a\u6837\u5316\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u6280\u80fd\uff0c\u4ee5\u6db2\u4f53\u503e\u5012\u4efb\u52a1\u4e3a\u4f8b\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u4ece\u539f\u59cb\u4efb\u52a1\u53d8\u4f53\u5230\u610f\u5916\u65b0\u6280\u80fd\uff08\u5982\u5bb9\u5668\u8fb9\u7f18\u6e05\u6d01\u3001\u6db2\u4f53\u6df7\u5408\u548c\u6d47\u6c34\uff09\u7684\u5e7f\u6cdb\u884c\u4e3a\u3002", "motivation": "\u53d7\u4eba\u7c7b\u8fd0\u52a8\u63a7\u5236\u4e2d\u6210\u672c-\u6536\u76ca\u6743\u8861\u6a21\u578b\u7684\u542f\u53d1\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u6545\u610f\u53d8\u5f02\u5956\u52b1\u51fd\u6570\u6765\u4ea7\u751f\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u591a\u6837\u5316\u6280\u80fd\u53d8\u5316\uff0c\u4f7f\u673a\u5668\u4eba\u7cfb\u7edf\u80fd\u591f\u5b66\u4e60\u7279\u5b9a\u4efb\u52a1\u7684\u591a\u6837\u5316\u6267\u884c\u65b9\u5f0f\uff0c\u5e76\u53ef\u80fd\u4e3a\u672a\u6765\u4efb\u52a1\u884d\u751f\u6709\u610f\u4e49\u7684\u6280\u80fd\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u9ad8\u65af\u566a\u58f0\u7684\u5956\u52b1\u51fd\u6570\u53d8\u5f02\u6846\u67b6\uff0c\u5956\u52b1\u51fd\u6570\u5305\u542b\u51c6\u786e\u6027\u3001\u65f6\u95f4\u548c\u52aa\u529b\u4e09\u4e2a\u5173\u952e\u9879\uff1b\u5728NVIDIA Isaac Sim\u4eff\u771f\u73af\u5883\u4e2d\u4f7f\u7528Franka Emika Panda\u673a\u68b0\u81c2\u8fdb\u884c\u6db2\u4f53\u503e\u5012\u4efb\u52a1\uff0c\u91c7\u7528\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u7cfb\u7edf\u63a2\u7d22\u4e0d\u540c\u6743\u91cd\u53d8\u5f02\u914d\u7f6e\u5bf9\u5b66\u4e60\u7b56\u7565\u7684\u5f71\u54cd\u3002", "result": "\u751f\u6210\u7684\u7b56\u7565\u5c55\u73b0\u51fa\u5e7f\u6cdb\u7684\u884c\u4e3a\u8303\u56f4\uff1a\u4ece\u539f\u59cb\u503e\u5012\u4efb\u52a1\u6267\u884c\u7684\u53d8\u5316\u5230\u610f\u5916\u4efb\u52a1\u7684\u65b0\u6280\u80fd\uff0c\u5982\u5bb9\u5668\u8fb9\u7f18\u6e05\u6d01\u3001\u6db2\u4f53\u6df7\u5408\u548c\u6d47\u6c34\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u673a\u5668\u4eba\u7cfb\u7edf\u6267\u884c\u7279\u5b9a\u4efb\u52a1\u7684\u591a\u6837\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u540c\u65f6\u53ef\u80fd\u4e3a\u672a\u6765\u4efb\u52a1\u884d\u751f\u6709\u610f\u4e49\u7684\u6280\u80fd\u3002"}}
{"id": "2509.18749", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18749", "abs": "https://arxiv.org/abs/2509.18749", "authors": ["Maxwell M. Varley", "Timothy L. Molloy", "Girish N. Nair"], "title": "An Extended Kalman Filter for Systems with Infinite-Dimensional Measurements", "comment": "8 pages", "summary": "This article examines state estimation in discrete-time nonlinear stochastic\nsystems with finite-dimensional states and infinite-dimensional measurements,\nmotivated by real-world applications such as vision-based localization and\ntracking. We develop an extended Kalman filter (EKF) for real-time state\nestimation, with the measurement noise modeled as an infinite-dimensional\nrandom field. When applied to vision-based state estimation, the measurement\nJacobians required to implement the EKF are shown to correspond to image\ngradients. This result provides a novel system-theoretic justification for the\nuse of image gradients as features for vision-based state estimation,\ncontrasting with their (often heuristic) introduction in many computer-vision\npipelines. We demonstrate the practical utility of the EKF on a public\nreal-world dataset involving the localization of an aerial drone using video\nfrom a downward-facing monocular camera. The EKF is shown to outperform\nVINS-MONO, an established visual-inertial odometry algorithm, in some cases\nachieving mean squared error reductions of up to an order of magnitude.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u79bb\u6563\u65f6\u95f4\u975e\u7ebf\u6027\u968f\u673a\u7cfb\u7edf\u7684\u72b6\u6001\u4f30\u8ba1\u95ee\u9898\uff0c\u9488\u5bf9\u6709\u9650\u7ef4\u72b6\u6001\u548c\u65e0\u9650\u7ef4\u6d4b\u91cf\u7684\u60c5\u51b5\uff0c\u5f00\u53d1\u4e86\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08EKF\uff09\uff0c\u5728\u89c6\u89c9\u5b9a\u4f4d\u5e94\u7528\u4e2d\u8bc1\u660e\u4e86\u56fe\u50cf\u68af\u5ea6\u4f5c\u4e3a\u6d4b\u91cf\u96c5\u53ef\u6bd4\u77e9\u9635\u7684\u7406\u8bba\u5408\u7406\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u6e90\u4e8e\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u89c6\u89c9\u5b9a\u4f4d\u548c\u8ddf\u8e2a\u95ee\u9898\uff0c\u7279\u522b\u662f\u5904\u7406\u6709\u9650\u7ef4\u72b6\u6001\u548c\u65e0\u9650\u7ef4\u6d4b\u91cf\u7684\u975e\u7ebf\u6027\u968f\u673a\u7cfb\u7edf\u72b6\u6001\u4f30\u8ba1\u9700\u6c42\u3002", "method": "\u5f00\u53d1\u4e86\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08EKF\uff09\uff0c\u5c06\u6d4b\u91cf\u566a\u58f0\u5efa\u6a21\u4e3a\u65e0\u9650\u7ef4\u968f\u673a\u573a\uff0c\u5728\u89c6\u89c9\u72b6\u6001\u4f30\u8ba1\u4e2d\u8bc1\u660e\u4e86\u6d4b\u91cf\u96c5\u53ef\u6bd4\u77e9\u9635\u5bf9\u5e94\u4e8e\u56fe\u50cf\u68af\u5ea6\u3002", "result": "\u5728\u65e0\u4eba\u673a\u89c6\u89c9\u5b9a\u4f4d\u5b9e\u9a8c\u4e2d\uff0cEKF\u65b9\u6cd5\u76f8\u6bd4VINS-MONO\u7b97\u6cd5\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u5747\u65b9\u8bef\u5dee\u964d\u4f4e\u4e00\u4e2a\u6570\u91cf\u7ea7\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u672c\u6587\u4e3a\u56fe\u50cf\u68af\u5ea6\u5728\u89c6\u89c9\u72b6\u6001\u4f30\u8ba1\u4e2d\u7684\u4f7f\u7528\u63d0\u4f9b\u4e86\u7cfb\u7edf\u7406\u8bba\u4f9d\u636e\uff0c\u8bc1\u660e\u4e86EKF\u5728\u65e0\u9650\u7ef4\u6d4b\u91cf\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.18218", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18218", "abs": "https://arxiv.org/abs/2509.18218", "authors": ["Kei-Sing Ng"], "title": "Similarity Field Theory: A Mathematical Framework for Intelligence", "comment": null, "summary": "We posit that persisting and transforming similarity relations form the\nstructural basis of any comprehensible dynamic system. This paper introduces\nSimilarity Field Theory, a mathematical framework that formalizes the\nprinciples governing similarity values among entities and their evolution. We\ndefine: (1) a similarity field $S: U \\times U \\to [0,1]$ over a universe of\nentities $U$, satisfying reflexivity $S(E,E)=1$ and treated as a directed\nrelational field (asymmetry and non-transitivity are allowed); (2) the\nevolution of a system through a sequence $Z_p = (X_p, S^{(p)})$ indexed by\n$p=0,1,2,\\ldots$; (3) concepts $K$ as entities that induce fibers\n$F_{\\alpha}(K) = { E \\in U \\mid S(E,K) \\ge \\alpha }$, i.e., superlevel sets of\nthe unary map $S_K(E) := S(E,K)$; and (4) a generative operator $G$ that\nproduces new entities. Within this framework, we formalize a generative\ndefinition of intelligence: an operator $G$ is intelligent with respect to a\nconcept $K$ if, given a system containing entities belonging to the fiber of\n$K$, it generates new entities that also belong to that fiber. Similarity Field\nTheory thus offers a foundational language for characterizing, comparing, and\nconstructing intelligent systems. We prove two theorems: (i) asymmetry blocks\nmutual inclusion; and (ii) stability requires either an anchor coordinate or\neventual confinement within a level set of $f$. These results ensure that the\nevolution of similarity fields is both constrained and interpretable,\nculminating in an exploration of how the framework allows us to interpret large\nlanguage models and use them as experimental probes into societal cognition.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u76f8\u4f3c\u6027\u573a\u7406\u8bba\uff0c\u8fd9\u662f\u4e00\u4e2a\u6570\u5b66\u6846\u67b6\uff0c\u7528\u4e8e\u5f62\u5f0f\u5316\u5b9e\u4f53\u95f4\u76f8\u4f3c\u6027\u5173\u7cfb\u53ca\u5176\u6f14\u5316\u7684\u539f\u5219\u3002\u8be5\u7406\u8bba\u5b9a\u4e49\u4e86\u76f8\u4f3c\u6027\u573a\u3001\u7cfb\u7edf\u6f14\u5316\u3001\u6982\u5ff5\u7ea4\u7ef4\u548c\u751f\u6210\u7b97\u5b50\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f62\u5f0f\u5316\u5730\u5b9a\u4e49\u4e86\u667a\u80fd\u7684\u6982\u5ff5\u3002", "motivation": "\u4f5c\u8005\u8ba4\u4e3a\u6301\u4e45\u5316\u548c\u8f6c\u6362\u76f8\u4f3c\u6027\u5173\u7cfb\u6784\u6210\u4e86\u4efb\u4f55\u53ef\u7406\u89e3\u52a8\u6001\u7cfb\u7edf\u7684\u7ed3\u6784\u57fa\u7840\u3002\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u6570\u5b66\u6846\u67b6\u6765\u5f62\u5f0f\u5316\u76f8\u4f3c\u6027\u503c\u7684\u539f\u5219\u53ca\u5176\u6f14\u5316\uff0c\u4e3a\u63cf\u8ff0\u3001\u6bd4\u8f83\u548c\u6784\u5efa\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u57fa\u7840\u8bed\u8a00\u3002", "method": "\u5b9a\u4e49\u4e86\u76f8\u4f3c\u6027\u573aS: U\u00d7U\u2192[0,1]\uff0c\u6ee1\u8db3\u81ea\u53cd\u6027\u4f46\u5141\u8bb8\u4e0d\u5bf9\u79f0\u6027\u548c\u975e\u4f20\u9012\u6027\uff1b\u7cfb\u7edf\u6f14\u5316\u5e8f\u5217Z_p=(X_p,S^(p))\uff1b\u6982\u5ff5K\u8bf1\u5bfc\u7684\u7ea4\u7ef4F_\u03b1(K)\uff1b\u751f\u6210\u7b97\u5b50G\u3002\u57fa\u4e8e\u6b64\u6846\u67b6\u5f62\u5f0f\u5316\u667a\u80fd\u7684\u5b9a\u4e49\u3002", "result": "\u8bc1\u660e\u4e86\u4e24\u4e2a\u5b9a\u7406\uff1a(i)\u4e0d\u5bf9\u79f0\u6027\u963b\u788d\u76f8\u4e92\u5305\u542b\uff1b(ii)\u7a33\u5b9a\u6027\u9700\u8981\u951a\u5750\u6807\u6216\u6700\u7ec8\u9650\u5236\u5728f\u7684\u6c34\u5e73\u96c6\u5185\u3002\u8fd9\u4e9b\u7ed3\u679c\u786e\u4fdd\u76f8\u4f3c\u6027\u573a\u6f14\u5316\u65e2\u53d7\u7ea6\u675f\u53c8\u53ef\u89e3\u91ca\u3002", "conclusion": "\u76f8\u4f3c\u6027\u573a\u7406\u8bba\u4e3a\u63cf\u8ff0\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u7840\u6846\u67b6\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u7528\u8be5\u6846\u67b6\u89e3\u91ca\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5c06\u5176\u4f5c\u4e3a\u793e\u4f1a\u8ba4\u77e5\u7684\u5b9e\u9a8c\u63a2\u9488\u3002"}}
{"id": "2509.18605", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2509.18605", "abs": "https://arxiv.org/abs/2509.18605", "authors": ["Chanhou Lou"], "title": "Judging Data: Critical Discourse and the Rise of Data Intellectual Property Rights in Chinese Courts", "comment": null, "summary": "This paper uses Critical Discourse Analysis (CDA) to show how Sino-judicial\nactivism shapes Data Intellectual Property Rights (DIPR) in China. We identify\ntwo complementary judicial discourses. Local courts (exemplified by the\nZhejiang High People's Court, HCZJ) use a judicial continuation discourse that\nextends intellectual property norms to data disputes. The Supreme People's\nCourt (SPC) deploys a judicial linkage discourse that aligns adjudication with\nstate policy and administrative governance. Their interaction forms a\nbidirectional conceptual coupling (BCC): an inside-out projection of local\nreasoning and an outside-in translation of policy into doctrine. The coupling\nboth legitimizes and constrains courts and policymakers, balancing pressure for\nunified market standards with safeguards against platform monopolization.\nThrough cases such as HCZJ's Taobao v. Meijing and the SPC's Anti-Unfair\nCompetition Interpretation, the study presents DIPR as a testbed for doctrinal\ninnovation and institutional coordination in China's evolving digital\ngovernance.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u6279\u5224\u6027\u8bdd\u8bed\u5206\u6790\u5c55\u793a\u4e86\u4e2d\u56fd\u53f8\u6cd5\u80fd\u52a8\u4e3b\u4e49\u5982\u4f55\u5851\u9020\u6570\u636e\u77e5\u8bc6\u4ea7\u6743\u3002\u7814\u7a76\u53d1\u73b0\u5730\u65b9\u6cd5\u9662\u548c\u6700\u9ad8\u6cd5\u9662\u91c7\u7528\u4e24\u79cd\u4e92\u8865\u7684\u53f8\u6cd5\u8bdd\u8bed\uff0c\u5f62\u6210\u53cc\u5411\u6982\u5ff5\u8026\u5408\u673a\u5236\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63ed\u793a\u4e2d\u56fd\u6570\u636e\u77e5\u8bc6\u4ea7\u6743\u9886\u57df\u53f8\u6cd5\u80fd\u52a8\u4e3b\u4e49\u7684\u4f5c\u7528\u673a\u5236\uff0c\u63a2\u8ba8\u53f8\u6cd5\u7cfb\u7edf\u5982\u4f55\u5728\u6570\u5b57\u6cbb\u7406\u4e2d\u5e73\u8861\u7edf\u4e00\u5e02\u573a\u6807\u51c6\u4e0e\u9632\u6b62\u5e73\u53f0\u5784\u65ad\u7684\u5173\u7cfb\u3002", "method": "\u91c7\u7528\u6279\u5224\u6027\u8bdd\u8bed\u5206\u6790\u65b9\u6cd5\uff0c\u5206\u6790\u6d59\u6c5f\u7701\u9ad8\u7ea7\u4eba\u6c11\u6cd5\u9662\u548c\u6700\u9ad8\u4eba\u6c11\u6cd5\u9662\u7684\u53f8\u6cd5\u6848\u4f8b\uff0c\u5982\u6dd8\u5b9d\u8bc9\u7f8e\u666f\u6848\u548c\u53cd\u4e0d\u6b63\u5f53\u7ade\u4e89\u53f8\u6cd5\u89e3\u91ca\u3002", "result": "\u8bc6\u522b\u51fa\u4e24\u79cd\u4e92\u8865\u7684\u53f8\u6cd5\u8bdd\u8bed\uff1a\u5730\u65b9\u6cd5\u9662\u7684\u53f8\u6cd5\u5ef6\u7eed\u8bdd\u8bed\u548c\u6700\u9ad8\u6cd5\u9662\u7684\u53f8\u6cd5\u8054\u52a8\u8bdd\u8bed\uff0c\u5b83\u4eec\u5f62\u6210\u53cc\u5411\u6982\u5ff5\u8026\u5408\u673a\u5236\uff0c\u65e2\u5408\u6cd5\u5316\u53c8\u7ea6\u675f\u6cd5\u9662\u548c\u653f\u7b56\u5236\u5b9a\u8005\u3002", "conclusion": "\u6570\u636e\u77e5\u8bc6\u4ea7\u6743\u6210\u4e3a\u4e2d\u56fd\u6570\u5b57\u6cbb\u7406\u4e2d\u6cd5\u5f8b\u521b\u65b0\u548c\u5236\u5ea6\u534f\u8c03\u7684\u8bd5\u9a8c\u573a\uff0c\u5c55\u793a\u4e86\u53f8\u6cd5\u7cfb\u7edf\u5728\u5e73\u8861\u5e02\u573a\u7edf\u4e00\u4e0e\u53cd\u5784\u65ad\u65b9\u9762\u7684\u72ec\u7279\u4f5c\u7528\u3002"}}
{"id": "2509.18466", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18466", "abs": "https://arxiv.org/abs/2509.18466", "authors": ["Junnosuke Kamohara", "Feiyang Wu", "Chinmayee Wamorkar", "Seth Hutchinson", "Ye Zhao"], "title": "RL-augmented Adaptive Model Predictive Control for Bipedal Locomotion over Challenging Terrain", "comment": null, "summary": "Model predictive control (MPC) has demonstrated effectiveness for humanoid\nbipedal locomotion; however, its applicability in challenging environments,\nsuch as rough and slippery terrain, is limited by the difficulty of modeling\nterrain interactions. In contrast, reinforcement learning (RL) has achieved\nnotable success in training robust locomotion policies over diverse terrain,\nyet it lacks guarantees of constraint satisfaction and often requires\nsubstantial reward shaping. Recent efforts in combining MPC and RL have shown\npromise of taking the best of both worlds, but they are primarily restricted to\nflat terrain or quadrupedal robots. In this work, we propose an RL-augmented\nMPC framework tailored for bipedal locomotion over rough and slippery terrain.\nOur method parametrizes three key components of\nsingle-rigid-body-dynamics-based MPC: system dynamics, swing leg controller,\nand gait frequency. We validate our approach through bipedal robot simulations\nin NVIDIA IsaacLab across various terrains, including stairs, stepping stones,\nand low-friction surfaces. Experimental results demonstrate that our\nRL-augmented MPC framework produces significantly more adaptive and robust\nbehaviors compared to baseline MPC and RL.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u53cc\u8db3\u673a\u5668\u4eba\u5728\u7c97\u7cd9\u548c\u6e7f\u6ed1\u5730\u5f62\u4e0a\u884c\u8d70\u7684RL\u589e\u5f3aMPC\u6846\u67b6\uff0c\u901a\u8fc7\u53c2\u6570\u5316MPC\u7684\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\u6765\u7ed3\u5408MPC\u548cRL\u7684\u4f18\u52bf\u3002", "motivation": "MPC\u5728\u53cc\u8db3\u884c\u8d70\u65b9\u9762\u6709\u6548\u4f46\u96be\u4ee5\u5efa\u6a21\u590d\u6742\u5730\u5f62\u4ea4\u4e92\uff0cRL\u80fd\u8bad\u7ec3\u51fa\u9c81\u68d2\u7b56\u7565\u4f46\u7f3a\u4e4f\u7ea6\u675f\u4fdd\u8bc1\u4e14\u9700\u8981\u5927\u91cf\u5956\u52b1\u8bbe\u8ba1\u3002\u73b0\u6709MPC+RL\u65b9\u6cd5\u4e3b\u8981\u9650\u4e8e\u5e73\u5766\u5730\u5f62\u6216\u56db\u8db3\u673a\u5668\u4eba\u3002", "method": "\u53c2\u6570\u5316\u57fa\u4e8e\u5355\u521a\u4f53\u52a8\u529b\u5b66\u7684MPC\u7684\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u7cfb\u7edf\u52a8\u529b\u5b66\u3001\u6446\u52a8\u817f\u63a7\u5236\u5668\u548c\u6b65\u6001\u9891\u7387\uff0c\u5728NVIDIA IsaacLab\u4e2d\u8fdb\u884c\u53cc\u8db3\u673a\u5668\u4eba\u4eff\u771f\u9a8c\u8bc1\u3002", "result": "\u5728\u697c\u68af\u3001\u8e0f\u811a\u77f3\u548c\u4f4e\u6469\u64e6\u8868\u9762\u7b49\u591a\u79cd\u5730\u5f62\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRL\u589e\u5f3aMPC\u6846\u67b6\u6bd4\u57fa\u7ebfMPC\u548cRL\u4ea7\u751f\u663e\u8457\u66f4\u81ea\u9002\u5e94\u548c\u9c81\u68d2\u7684\u884c\u4e3a\u3002", "conclusion": "RL\u589e\u5f3aMPC\u6846\u67b6\u6210\u529f\u7ed3\u5408\u4e86MPC\u7684\u7ea6\u675f\u4fdd\u8bc1\u548cRL\u7684\u9002\u5e94\u6027\uff0c\u4e3a\u53cc\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e0a\u7684\u9c81\u68d2\u884c\u8d70\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18769", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18769", "abs": "https://arxiv.org/abs/2509.18769", "authors": ["Hadi Nemati", "Pedro S\u00e1nchez-Mart\u00edn", "\u00c1lvaro Ortega", "Lukas Sigrist", "Luis Rouco"], "title": "Integration of Concentrated Solar Power Plants in Renewable-Only VPP with Electrical and Thermal Demands: A Two-Stage Robust Bidding Approach", "comment": null, "summary": "This paper proposes the integration of Concentrated Solar Power Plant (CSP)\nin the Renewable-only virtual power plant (RVPP) for bidding in the electricity\nday-ahead and secondary reserve markets, as well as trading thermal energy\nthrough a heat purchase agreement. A reformulated two-stage robust optimization\napproach is introduced to account for multiple uncertainties, including\nelectricity prices, non-dispatchable renewable energy sources electrical\nproduction, CSP thermal production, and uncertainties in electrical and thermal\ndemand consumption. The provision of energy and reserve by the thermal storage\nof CSP is modeled using an adjustable approach, which allocates a share of\nenergy for up and down reserves based on the profitability of the RVPP.\nSimulations are conducted for several case studies to demonstrate the\neffectiveness and computational efficiency of the proposed approach under\ndifferent RVPP operator decisions against uncertain parameters and various\ntrading strategies for electricity and thermal energy. The simulation results\nshow that integrating CSP into RVPP enhances RVPP flexibility for both\nelectrical and thermal trading. Furthermore, the results indicate that the\nprofitability of the RVPP increases when all trading options are considered,\nacross different levels of conservatism adopted by the RVPP operator in\nresponse to uncertain parameters.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u805a\u5149\u592a\u9633\u80fd\u53d1\u7535\u5382\uff08CSP\uff09\u6574\u5408\u5230\u7eaf\u53ef\u518d\u751f\u80fd\u6e90\u865a\u62df\u7535\u5382\uff08RVPP\uff09\u4e2d\uff0c\u7528\u4e8e\u7535\u529b\u65e5\u524d\u5e02\u573a\u548c\u4e8c\u6b21\u5907\u7528\u5e02\u573a\u7684\u6295\u6807\uff0c\u4ee5\u53ca\u901a\u8fc7\u70ed\u529b\u8d2d\u4e70\u534f\u8bae\u8fdb\u884c\u70ed\u80fd\u4ea4\u6613\u3002\u91c7\u7528\u6539\u8fdb\u7684\u4e24\u9636\u6bb5\u9c81\u68d2\u4f18\u5316\u65b9\u6cd5\u6765\u5904\u7406\u591a\u79cd\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u63d0\u9ad8RVPP\u5728\u7535\u529b\u548c\u70ed\u80fd\u4ea4\u6613\u4e2d\u7684\u7075\u6d3b\u6027\uff0c\u589e\u52a0RVPP\u7684\u76c8\u5229\u80fd\u529b\uff0c\u901a\u8fc7\u6574\u5408CSP\u6765\u5e94\u5bf9\u7535\u529b\u4ef7\u683c\u3001\u53ef\u518d\u751f\u80fd\u6e90\u53d1\u7535\u3001CSP\u70ed\u80fd\u751f\u4ea7\u4ee5\u53ca\u7535\u529b\u548c\u70ed\u9700\u6c42\u6d88\u8d39\u7b49\u591a\u79cd\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u5f15\u5165\u6539\u8fdb\u7684\u4e24\u9636\u6bb5\u9c81\u68d2\u4f18\u5316\u65b9\u6cd5\uff0c\u91c7\u7528\u53ef\u8c03\u6574\u7684\u65b9\u6cd5\u5bf9CSP\u70ed\u50a8\u80fd\u7684\u80fd\u91cf\u548c\u5907\u7528\u63d0\u4f9b\u8fdb\u884c\u5efa\u6a21\uff0c\u6839\u636eRVPP\u7684\u76c8\u5229\u80fd\u529b\u5206\u914d\u4e0a\u4e0b\u5907\u7528\u80fd\u91cf\u7684\u4efd\u989d\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u5c06CSP\u6574\u5408\u5230RVPP\u4e2d\u589e\u5f3a\u4e86RVPP\u5728\u7535\u529b\u548c\u70ed\u80fd\u4ea4\u6613\u65b9\u9762\u7684\u7075\u6d3b\u6027\u3002\u5f53\u8003\u8651\u6240\u6709\u4ea4\u6613\u9009\u9879\u65f6\uff0c\u65e0\u8bbaRVPP\u8fd0\u8425\u5546\u5bf9\u4e0d\u786e\u5b9a\u53c2\u6570\u91c7\u53d6\u4f55\u79cd\u4fdd\u5b88\u7a0b\u5ea6\uff0cRVPP\u7684\u76c8\u5229\u80fd\u529b\u90fd\u4f1a\u589e\u52a0\u3002", "conclusion": "CSP\u4e0eRVPP\u7684\u6574\u5408\u662f\u6709\u6548\u7684\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e0d\u540c\u4ea4\u6613\u7b56\u7565\u548c\u4e0d\u786e\u5b9a\u53c2\u6570\u4e0b\u90fd\u8868\u73b0\u51fa\u826f\u597d\u7684\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347RVPP\u7684\u7ecf\u6d4e\u6548\u76ca\u548c\u8fd0\u884c\u7075\u6d3b\u6027\u3002"}}
{"id": "2509.18221", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18221", "abs": "https://arxiv.org/abs/2509.18221", "authors": ["Dingxin Lu", "Shurui Wu", "Xinyi Huang"], "title": "Multimodal Health Risk Prediction System for Chronic Diseases via Vision-Language Fusion and Large Language Models", "comment": null, "summary": "With the rising global burden of chronic diseases and the multimodal and\nheterogeneous clinical data (medical imaging, free-text recordings, wearable\nsensor streams, etc.), there is an urgent need for a unified multimodal AI\nframework that can proactively predict individual health risks. We propose\nVL-RiskFormer, a hierarchical stacked visual-language multimodal Transformer\nwith a large language model (LLM) inference head embedded in its top layer. The\nsystem builds on the dual-stream architecture of existing visual-linguistic\nmodels (e.g., PaLM-E, LLaVA) with four key innovations: (i) pre-training with\ncross-modal comparison and fine-grained alignment of radiological images,\nfundus maps, and wearable device photos with corresponding clinical narratives\nusing momentum update encoders and debiased InfoNCE losses; (ii) a time fusion\nblock that integrates irregular visit sequences into the causal Transformer\ndecoder through adaptive time interval position coding; (iii) a disease\nontology map adapter that injects ICD-10 codes into visual and textual channels\nin layers and infers comorbid patterns with the help of a graph attention\nmechanism. On the MIMIC-IV longitudinal cohort, VL-RiskFormer achieved an\naverage AUROC of 0.90 with an expected calibration error of 2.7 percent.", "AI": {"tldr": "VL-RiskFormer\u662f\u4e00\u4e2a\u7528\u4e8e\u9884\u6d4b\u4e2a\u4f53\u5065\u5eb7\u98ce\u9669\u7684\u591a\u6a21\u6001AI\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u8bed\u8a00\u4fe1\u606f\uff0c\u5728MIMIC-IV\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e860.90\u7684AUROC\u3002", "motivation": "\u968f\u7740\u6162\u6027\u75be\u75c5\u8d1f\u62c5\u589e\u52a0\u548c\u591a\u6a21\u6001\u4e34\u5e8a\u6570\u636e\u7684\u6d8c\u73b0\uff0c\u9700\u8981\u7edf\u4e00\u7684AI\u6846\u67b6\u6765\u4e3b\u52a8\u9884\u6d4b\u4e2a\u4f53\u5065\u5eb7\u98ce\u9669\u3002", "method": "\u91c7\u7528\u5206\u5c42\u5806\u53e0\u7684\u89c6\u89c9-\u8bed\u8a00\u591a\u6a21\u6001Transformer\u67b6\u6784\uff0c\u5305\u542b\u56db\u4e2a\u5173\u952e\u521b\u65b0\uff1a\u8de8\u6a21\u6001\u9884\u8bad\u7ec3\u3001\u65f6\u95f4\u878d\u5408\u5757\u3001\u75be\u75c5\u672c\u4f53\u56fe\u9002\u914d\u5668\u548cLLM\u63a8\u7406\u5934\u3002", "result": "\u5728MIMIC-IV\u7eb5\u5411\u961f\u5217\u4e2d\uff0c\u5e73\u5747AUROC\u8fbe\u52300.90\uff0c\u9884\u671f\u6821\u51c6\u8bef\u5dee\u4e3a2.7%\u3002", "conclusion": "VL-RiskFormer\u5c55\u793a\u4e86\u5728\u591a\u6a21\u6001\u533b\u7597\u6570\u636e\u4e0a\u8fdb\u884c\u5065\u5eb7\u98ce\u9669\u9884\u6d4b\u7684\u6709\u6548\u6027\uff0c\u4e3a\u4e2a\u6027\u5316\u533b\u7597\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2509.18768", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2509.18768", "abs": "https://arxiv.org/abs/2509.18768", "authors": ["Gauthier Roussilhe", "Thibault Pirson", "David Bol", "Srinjoy Mitra"], "title": "Purer than pure: how purity reshapes the upstream materiality of the semiconductor industry", "comment": "11 pages, 7 figures", "summary": "Growing attention is given to the environmental impacts of the digital\nsector, exacerbated by the increase of digital products and services in our\nglobalized societies. The materiality of the digital sector is often presented\nthrough the environmental impacts of mining activities to point out that\ndigitization does not mean dematerialization. Despite its importance, such a\nnarrative is often restricted to a few minerals (e.g., cobalt, lithium) that\nhave become the symbols of extractive industries. In this paper, we further\nexplore the materiality of the digital sector with an approach based on the\ndiversity of elements and their purity requirements in the semiconductor\nindustry. Semiconductors are responsible for manufacturing the key building\nblocks of the digital sector, i.e., microchips. Given that the need for\nultra-high purity materials is very specific to the semiconductor industry, a\nfew companies around the world have been studied, revealing new critical actors\nin complex supply chains. This highlights strong dependencies towards other\nindustrial sectors with mass production and the need for a deeper investigation\nof interactions with the chemical industry, complementary to the mining\nindustry.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u6570\u5b57\u90e8\u95e8\u7684\u73af\u5883\u5f71\u54cd\uff0c\u7279\u522b\u662f\u534a\u5bfc\u4f53\u884c\u4e1a\u5bf9\u591a\u79cd\u5143\u7d20\u53ca\u5176\u7eaf\u5ea6\u8981\u6c42\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u4e86\u6570\u5b57\u90e8\u95e8\u5bf9\u5927\u89c4\u6a21\u751f\u4ea7\u5de5\u4e1a\u90e8\u95e8\u7684\u4f9d\u8d56\u3002", "motivation": "\u6570\u5b57\u90e8\u95e8\u7684\u73af\u5883\u5f71\u54cd\u65e5\u76ca\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5f80\u5f80\u5c40\u9650\u4e8e\u5c11\u6570\u77ff\u7269\uff08\u5982\u94b4\u3001\u9502\uff09\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u534a\u5bfc\u4f53\u884c\u4e1a\u7684\u5143\u7d20\u591a\u6837\u6027\u548c\u7eaf\u5ea6\u8981\u6c42\uff0c\u8fdb\u4e00\u6b65\u63a2\u7d22\u6570\u5b57\u90e8\u95e8\u7684\u7269\u8d28\u6027\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5143\u7d20\u591a\u6837\u6027\u548c\u7eaf\u5ea6\u8981\u6c42\u7684\u65b9\u6cd5\uff0c\u7814\u7a76\u534a\u5bfc\u4f53\u884c\u4e1a\u4e2d\u7684\u5173\u952e\u516c\u53f8\uff0c\u5206\u6790\u5176\u5728\u590d\u6742\u4f9b\u5e94\u94fe\u4e2d\u7684\u89d2\u8272\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u534a\u5bfc\u4f53\u884c\u4e1a\u5bf9\u8d85\u9ad8\u7eaf\u5ea6\u6750\u6599\u7684\u9700\u6c42\u975e\u5e38\u7279\u6b8a\uff0c\u63ed\u793a\u4e86\u6570\u5b57\u90e8\u95e8\u5bf9\u5176\u4ed6\u5927\u89c4\u6a21\u751f\u4ea7\u5de5\u4e1a\u90e8\u95e8\u7684\u5f3a\u4f9d\u8d56\u5173\u7cfb\uff0c\u7279\u522b\u662f\u4e0e\u5316\u5b66\u5de5\u4e1a\u7684\u4e92\u52a8\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "conclusion": "\u6570\u5b57\u90e8\u95e8\u7684\u7269\u8d28\u6027\u4e0d\u4ec5\u9650\u4e8e\u91c7\u77ff\u6d3b\u52a8\uff0c\u8fd8\u9700\u6df1\u5165\u63a2\u8ba8\u5176\u4e0e\u5316\u5b66\u5de5\u4e1a\u7b49\u90e8\u95e8\u7684\u4e92\u52a8\uff0c\u4ee5\u5168\u9762\u7406\u89e3\u5176\u73af\u5883\u5f71\u54cd\u3002"}}
{"id": "2509.18506", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18506", "abs": "https://arxiv.org/abs/2509.18506", "authors": ["Siyuan Yu", "Congkai Shen", "Yufei Xi", "James Dallas", "Michael Thompson", "John Subosits", "Hiroshi Yasuda", "Tulga Ersal"], "title": "Spatial Envelope MPC: High Performance Driving without a Reference", "comment": null, "summary": "This paper presents a novel envelope based model predictive control (MPC)\nframework designed to enable autonomous vehicles to handle high performance\ndriving across a wide range of scenarios without a predefined reference. In\nhigh performance autonomous driving, safe operation at the vehicle's dynamic\nlimits requires a real time planning and control framework capable of\naccounting for key vehicle dynamics and environmental constraints when\nfollowing a predefined reference trajectory is suboptimal or even infeasible.\nState of the art planning and control frameworks, however, are predominantly\nreference based, which limits their performance in such situations. To address\nthis gap, this work first introduces a computationally efficient vehicle\ndynamics model tailored for optimization based control and a continuously\ndifferentiable mathematical formulation that accurately captures the entire\ndrivable envelope. This novel model and formulation allow for the direct\nintegration of dynamic feasibility and safety constraints into a unified\nplanning and control framework, thereby removing the necessity for predefined\nreferences. The challenge of envelope planning, which refers to maximally\napproximating the safe drivable area, is tackled by combining reinforcement\nlearning with optimization techniques. The framework is validated through both\nsimulations and real world experiments, demonstrating its high performance\nacross a variety of tasks, including racing, emergency collision avoidance and\noff road navigation. These results highlight the framework's scalability and\nbroad applicability across a diverse set of scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5305\u7edc\u7ebf\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u4f7f\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u65e0\u9700\u9884\u5b9a\u4e49\u53c2\u8003\u8f68\u8ff9\u5373\u53ef\u5904\u7406\u9ad8\u6027\u80fd\u9a7e\u9a76\u573a\u666f", "motivation": "\u73b0\u6709\u57fa\u4e8e\u53c2\u8003\u8f68\u8ff9\u7684\u89c4\u5212\u63a7\u5236\u6846\u67b6\u5728\u8f66\u8f86\u8fbe\u5230\u52a8\u6001\u6781\u9650\u65f6\u6027\u80fd\u53d7\u9650\uff0c\u9700\u8981\u80fd\u591f\u76f4\u63a5\u6574\u5408\u52a8\u6001\u53ef\u884c\u6027\u548c\u5b89\u5168\u7ea6\u675f\u7684\u7edf\u4e00\u6846\u67b6", "method": "\u5f00\u53d1\u4e86\u8ba1\u7b97\u9ad8\u6548\u7684\u8f66\u8f86\u52a8\u529b\u5b66\u6a21\u578b\u548c\u8fde\u7eed\u53ef\u5fae\u7684\u6570\u5b66\u516c\u5f0f\u6765\u6355\u6349\u6574\u4e2a\u53ef\u884c\u9a76\u5305\u7edc\u7ebf\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u4f18\u5316\u6280\u672f\u89e3\u51b3\u5305\u7edc\u89c4\u5212\u95ee\u9898", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u6846\u67b6\u5728\u8d5b\u8f66\u3001\u7d27\u6025\u907f\u969c\u548c\u8d8a\u91ce\u5bfc\u822a\u7b49\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9ad8\u6027\u80fd", "conclusion": "\u8be5\u6846\u67b6\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u80fd\u591f\u5904\u7406\u591a\u6837\u5316\u7684\u81ea\u52a8\u9a7e\u9a76\u573a\u666f"}}
{"id": "2509.18935", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18935", "abs": "https://arxiv.org/abs/2509.18935", "authors": ["Yiqiao Xu", "Quan Wan", "Alessandra Parisio"], "title": "Frequency-Varying Optimization: A Control Framework for New Dynamic Frequency Response Services", "comment": null, "summary": "To address the variability of renewable generation, initiatives have been\nlaunched globally to provide faster and more effective frequency responses. In\nthe UK, the National Energy System Operator (NESO) has introduced a suite of\nthree new dynamic services, where aggregation of assets is expected to play a\nkey role. For an Aggregated Response Unit (ARU), the required level of\nfrequency response varies with grid frequency, resulting in a frequency-varying\nequality constraint that assets should meet collectively. We show that the\noptimal coordination of an ARU constitutes a Frequency-Varying Optimization\n(FVO) problem, in which the optimal trajectory for each asset evolves\ndynamically. To facilitate online optimization, we reformulate the FVO problem\ninto Tracking of the Optimal Trajectory (TOT) problems, with algorithms\nproposed for two scenarios: one where the asset dynamics are negligible, and\nanother where they must be accounted for. Under reasonable conditions, the ARU\nconverges to the optimal trajectory within a fixed time, and within the maximum\ndelivery time requested by NESO. The proposed framework can be readily\ndistributed to coordinate a large number of assets. Numerical results verify\nthe effectiveness and scalability of the proposed control framework.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u805a\u5408\u54cd\u5e94\u5355\u5143\uff08ARU\uff09\u7684\u9891\u7387\u53d8\u5316\u4f18\u5316\uff08FVO\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5c06FVO\u95ee\u9898\u8f6c\u5316\u4e3a\u6700\u4f18\u8f68\u8ff9\u8ddf\u8e2a\uff08TOT\uff09\u95ee\u9898\uff0c\u5b9e\u73b0\u5728\u7ebf\u4f18\u5316\u548c\u5206\u5e03\u5f0f\u534f\u8c03\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u53ef\u518d\u751f\u80fd\u6e90\u53d1\u7535\u7684\u6ce2\u52a8\u6027\uff0c\u9700\u8981\u63d0\u4f9b\u66f4\u5feb\u66f4\u6709\u6548\u7684\u9891\u7387\u54cd\u5e94\u670d\u52a1\u3002\u82f1\u56fd\u56fd\u5bb6\u80fd\u6e90\u7cfb\u7edf\u8fd0\u8425\u5546\uff08NESO\uff09\u5f15\u5165\u4e86\u65b0\u7684\u52a8\u6001\u670d\u52a1\uff0c\u5176\u4e2d\u8d44\u4ea7\u805a\u5408\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002", "method": "\u5c06\u9891\u7387\u53d8\u5316\u4f18\u5316\uff08FVO\uff09\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3a\u6700\u4f18\u8f68\u8ff9\u8ddf\u8e2a\uff08TOT\uff09\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u573a\u666f\u7684\u7b97\u6cd5\uff1a\u8d44\u4ea7\u52a8\u6001\u53ef\u5ffd\u7565\u548c\u5fc5\u987b\u8003\u8651\u8d44\u4ea7\u52a8\u6001\u7684\u60c5\u51b5\u3002", "result": "\u5728\u5408\u7406\u6761\u4ef6\u4e0b\uff0cARU\u80fd\u5728\u56fa\u5b9a\u65f6\u95f4\u5185\u6536\u655b\u5230\u6700\u4f18\u8f68\u8ff9\uff0c\u5e76\u5728NESO\u8981\u6c42\u7684\u6700\u957f\u4ea4\u4ed8\u65f6\u95f4\u5185\u5b8c\u6210\u54cd\u5e94\u3002\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u63a7\u5236\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u53ef\u4ee5\u8f7b\u677e\u5206\u5e03\u5f0f\u534f\u8c03\u5927\u91cf\u8d44\u4ea7\uff0c\u4e3a\u53ef\u518d\u751f\u80fd\u6e90\u96c6\u6210\u63d0\u4f9b\u6709\u6548\u7684\u9891\u7387\u54cd\u5e94\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18226", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18226", "abs": "https://arxiv.org/abs/2509.18226", "authors": ["Yu Fu", "Linyue Cai", "Ruoyu Wu", "Yong Zhao"], "title": "From \"What to Eat?\" to Perfect Recipe: ChefMind's Chain-of-Exploration for Ambiguous User Intent in Recipe Recommendation", "comment": "5 pages, 3 figures, submitted to icassp 2026", "summary": "Personalized recipe recommendation faces challenges in handling fuzzy user\nintent, ensuring semantic accuracy, and providing sufficient detail coverage.\nWe propose ChefMind, a hybrid architecture combining Chain of Exploration\n(CoE), Knowledge Graph (KG), Retrieval-Augmented Generation (RAG), and a Large\nLanguage Model (LLM). CoE refines ambiguous queries into structured conditions,\nKG offers semantic reasoning and interpretability, RAG supplements contextual\nculinary details, and LLM integrates outputs into coherent recommendations. We\nevaluate ChefMind on the Xiachufang dataset and manually annotated queries,\ncomparing it with LLM-only, KG-only, and RAG-only baselines. Results show that\nChefMind achieves superior performance in accuracy, relevance, completeness,\nand clarity, with an average score of 8.7 versus 6.4-6.7 for ablation models.\nMoreover, it reduces unprocessed queries to 1.6%, demonstrating robustness in\nhandling fuzzy demands.", "AI": {"tldr": "ChefMind\u662f\u4e00\u4e2a\u7ed3\u5408\u4e86\u63a2\u7d22\u94fe\u3001\u77e5\u8bc6\u56fe\u8c31\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6df7\u5408\u67b6\u6784\uff0c\u7528\u4e8e\u89e3\u51b3\u4e2a\u6027\u5316\u98df\u8c31\u63a8\u8350\u4e2d\u7684\u6a21\u7cca\u7528\u6237\u610f\u56fe\u3001\u8bed\u4e49\u51c6\u786e\u6027\u548c\u7ec6\u8282\u8986\u76d6\u95ee\u9898\u3002", "motivation": "\u4e2a\u6027\u5316\u98df\u8c31\u63a8\u8350\u9762\u4e34\u5904\u7406\u6a21\u7cca\u7528\u6237\u610f\u56fe\u3001\u786e\u4fdd\u8bed\u4e49\u51c6\u786e\u6027\u548c\u63d0\u4f9b\u8db3\u591f\u7ec6\u8282\u8986\u76d6\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faChefMind\u6df7\u5408\u67b6\u6784\uff1a\u63a2\u7d22\u94fe(CoE)\u5c06\u6a21\u7cca\u67e5\u8be2\u7cbe\u70bc\u4e3a\u7ed3\u6784\u5316\u6761\u4ef6\uff0c\u77e5\u8bc6\u56fe\u8c31(KG)\u63d0\u4f9b\u8bed\u4e49\u63a8\u7406\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u8865\u5145\u4e0a\u4e0b\u6587\u70f9\u996a\u7ec6\u8282\uff0c\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u5c06\u8f93\u51fa\u6574\u5408\u4e3a\u8fde\u8d2f\u63a8\u8350\u3002", "result": "\u5728Xiachufang\u6570\u636e\u96c6\u548c\u624b\u52a8\u6807\u6ce8\u67e5\u8be2\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cChefMind\u5728\u51c6\u786e\u6027\u3001\u76f8\u5173\u6027\u3001\u5b8c\u6574\u6027\u548c\u6e05\u6670\u5ea6\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5e73\u5747\u5f97\u52068.7\uff0c\u800c\u6d88\u878d\u6a21\u578b\u4e3a6.4-6.7\u3002\u672a\u5904\u7406\u67e5\u8be2\u964d\u81f31.6%\uff0c\u8bc1\u660e\u5176\u5728\u5904\u7406\u6a21\u7cca\u9700\u6c42\u65b9\u9762\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "ChefMind\u901a\u8fc7\u6df7\u5408\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86\u98df\u8c31\u63a8\u8350\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u8d28\u91cf\u548c\u5bf9\u6a21\u7cca\u7528\u6237\u610f\u56fe\u7684\u5904\u7406\u80fd\u529b\u3002"}}
{"id": "2509.18900", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18900", "abs": "https://arxiv.org/abs/2509.18900", "authors": ["Veronika Hackl", "Alexandra Mueller", "Maximilian Sailer"], "title": "The AI Literacy Heptagon: A Structured Approach to AI Literacy in Higher Education", "comment": "4 figures", "summary": "The integrative literature review addresses the conceptualization and\nimplementation of AI Literacy (AIL) in Higher Education (HE) by examining\nrecent research literature. Through an analysis of publications (2021-2024), we\nexplore (1) how AIL is defined and conceptualized in current research,\nparticularly in HE, and how it can be delineated from related concepts such as\nData Literacy, Media Literacy, and Computational Literacy; (2) how various\ndefinitions can be synthesized into a comprehensive working definition, and (3)\nhow scientific insights can be effectively translated into educational\npractice. Our analysis identifies seven central dimensions of AIL: technical,\napplicational, critical thinking, ethical, social, integrational, and legal.\nThese are synthesized in the AI Literacy Heptagon, deepening conceptual\nunderstanding and supporting the structured development of AIL in HE. The study\naims to bridge the gap between theoretical AIL conceptualizations and the\npractical implementation in academic curricula.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u6574\u5408\u6027\u6587\u732e\u7efc\u8ff0\uff0c\u63a2\u8ba8\u4e86\u9ad8\u7b49\u6559\u80b2\u4e2dAI\u7d20\u517b\u7684\u6982\u5ff5\u5316\u548c\u5b9e\u65bd\uff0c\u63d0\u51fa\u4e86AI\u7d20\u517b\u4e03\u8fb9\u5f62\u6a21\u578b\uff0c\u5305\u542b\u4e03\u4e2a\u6838\u5fc3\u7ef4\u5ea6\uff0c\u65e8\u5728\u5f25\u5408\u7406\u8bba\u6982\u5ff5\u4e0e\u5b9e\u8df5\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u5f53\u524dAI\u7d20\u517b\u5728\u9ad8\u7b49\u6559\u80b2\u4e2d\u7684\u6982\u5ff5\u5b9a\u4e49\u6a21\u7cca\u4e14\u7f3a\u4e4f\u7edf\u4e00\u6846\u67b6\uff0c\u9700\u8981\u7cfb\u7edf\u68b3\u7406\u73b0\u6709\u7814\u7a76\uff0c\u5efa\u7acb\u6e05\u6670\u7684\u6982\u5ff5\u8fb9\u754c\u548c\u5b9e\u7528\u5b9e\u65bd\u6307\u5357\u3002", "method": "\u91c7\u7528\u6574\u5408\u6027\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5206\u67902021-2024\u5e74\u7684\u76f8\u5173\u7814\u7a76\u6587\u732e\uff0c\u7cfb\u7edf\u68b3\u7406AI\u7d20\u517b\u7684\u5b9a\u4e49\u3001\u6982\u5ff5\u5316\u53ca\u5176\u4e0e\u76f8\u5173\u7d20\u517b\u6982\u5ff5\u7684\u533a\u5206\u3002", "result": "\u8bc6\u522b\u51faAI\u7d20\u517b\u7684\u4e03\u4e2a\u6838\u5fc3\u7ef4\u5ea6\uff08\u6280\u672f\u3001\u5e94\u7528\u3001\u6279\u5224\u6027\u601d\u7ef4\u3001\u4f26\u7406\u3001\u793e\u4f1a\u3001\u6574\u5408\u3001\u6cd5\u5f8b\uff09\uff0c\u5e76\u6784\u5efa\u4e86AI\u7d20\u517b\u4e03\u8fb9\u5f62\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u4e3a\u9ad8\u7b49\u6559\u80b2\u4e2d\u7684AI\u7d20\u517b\u6559\u80b2\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u8df5\u6307\u5bfc\uff0c\u6709\u52a9\u4e8e\u7cfb\u7edf\u5316\u5730\u5f00\u53d1\u548c\u5b9e\u65bdAI\u7d20\u517b\u8bfe\u7a0b\u3002"}}
{"id": "2509.18576", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18576", "abs": "https://arxiv.org/abs/2509.18576", "authors": ["Zeyi Kang", "Liang He", "Yanxin Zhang", "Zuheng Ming", "Kaixing Zhao"], "title": "LCMF: Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA", "comment": null, "summary": "Multimodal semantic learning plays a critical role in embodied intelligence,\nespecially when robots perceive their surroundings, understand human\ninstructions, and make intelligent decisions. However, the field faces\ntechnical challenges such as effective fusion of heterogeneous data and\ncomputational efficiency in resource-constrained environments. To address these\nchallenges, this study proposes the lightweight LCMF cascaded attention\nframework, introducing a multi-level cross-modal parameter sharing mechanism\ninto the Mamba module. By integrating the advantages of Cross-Attention and\nSelective parameter-sharing State Space Models (SSMs), the framework achieves\nefficient fusion of heterogeneous modalities and semantic complementary\nalignment. Experimental results show that LCMF surpasses existing multimodal\nbaselines with an accuracy of 74.29% in VQA tasks and achieves competitive\nmid-tier performance within the distribution cluster of Large Language Model\nAgents (LLM Agents) in EQA video tasks. Its lightweight design achieves a\n4.35-fold reduction in FLOPs relative to the average of comparable baselines\nwhile using only 166.51M parameters (image-text) and 219M parameters\n(video-text), providing an efficient solution for Human-Robot Interaction (HRI)\napplications in resource-constrained scenarios with strong multimodal decision\ngeneralization capabilities.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u8f7b\u91cf\u7ea7LCMF\u7ea7\u8054\u6ce8\u610f\u529b\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7ea7\u8de8\u6a21\u6001\u53c2\u6570\u5171\u4eab\u673a\u5236\u5b9e\u73b0\u5f02\u6784\u6a21\u6001\u7684\u9ad8\u6548\u878d\u5408\u548c\u8bed\u4e49\u4e92\u8865\u5bf9\u9f50\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u8bed\u4e49\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u8bed\u4e49\u5b66\u4e60\u4e2d\u7684\u5f02\u6784\u6570\u636e\u6709\u6548\u878d\u5408\u548c\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u8ba1\u7b97\u6548\u7387\u6311\u6218\uff0c\u4e3a\u5177\u8eab\u667a\u80fd\u4e2d\u7684\u4eba\u673a\u4ea4\u4e92\u5e94\u7528\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faLCMF\u6846\u67b6\uff0c\u5c06\u591a\u7ea7\u8de8\u6a21\u6001\u53c2\u6570\u5171\u4eab\u673a\u5236\u96c6\u6210\u5230Mamba\u6a21\u5757\u4e2d\uff0c\u7ed3\u5408\u4ea4\u53c9\u6ce8\u610f\u529b\u548c\u9009\u62e9\u6027\u53c2\u6570\u5171\u4eab\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u4f18\u52bf\u3002", "result": "\u5728VQA\u4efb\u52a1\u4e2d\u8fbe\u523074.29%\u51c6\u786e\u7387\uff0c\u5728EQA\u89c6\u9891\u4efb\u52a1\u4e2d\u8fbe\u5230LLM\u667a\u80fd\u4f53\u4e2d\u6e38\u6c34\u5e73\uff0cFLOPs\u6bd4\u540c\u7c7b\u57fa\u7ebf\u5e73\u5747\u51cf\u5c114.35\u500d\uff0c\u53c2\u6570\u91cf\u4ec5166.51M\uff08\u56fe\u50cf-\u6587\u672c\uff09\u548c219M\uff08\u89c6\u9891-\u6587\u672c\uff09\u3002", "conclusion": "LCMF\u6846\u67b6\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u7684\u4eba\u673a\u4ea4\u4e92\u5e94\u7528\u63d0\u4f9b\u4e86\u5177\u6709\u5f3a\u5927\u591a\u6a21\u6001\u51b3\u7b56\u6cdb\u5316\u80fd\u529b\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18988", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18988", "abs": "https://arxiv.org/abs/2509.18988", "authors": ["Ziliang Lyu", "Miroslav Krstic", "Kaixin Lu", "Yiguang Hong", "Lihua Xie"], "title": "Adaptive Override Control under High-Relative-Degree Nonovershooting Constraints", "comment": null, "summary": "This paper considers the problem of adaptively overriding unsafe actions of a\nnominal controller in the presence of high-relative-degree nonovershooting\nconstraints and parametric uncertainties. To prevent the design from being\ncoupled with high-order derivatives of the parameter estimation error, we adopt\na modular design approach in which the controller and the parameter identifier\nare designed separately. The controller module ensures that any safety\nviolations caused by parametric uncertainties remain bounded, provided that the\nparameter estimation error and its first-order derivative are either bounded or\nsquare-integrable. The identifier module, in turn, guarantees that these\nrequirements on the parameter estimation error are satisfied. Both theoretical\nanalysis and simulation results demonstrate that the closed-loop safety\nviolation is bounded by a tunable function of the initial estimation error.\nMoreover, as time increases, the parameter estimate converges to the true\nvalue, and the amount of safety violation decreases accordingly.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u8986\u76d6\u4e0d\u5b89\u5168\u52a8\u4f5c\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u5904\u7406\u9ad8\u76f8\u5bf9\u5ea6\u975e\u8d85\u8c03\u7ea6\u675f\u548c\u53c2\u6570\u4e0d\u786e\u5b9a\u6027\uff0c\u786e\u4fdd\u95ed\u73af\u5b89\u5168\u8fdd\u89c4\u6709\u754c\u4e14\u53c2\u6570\u4f30\u8ba1\u6536\u655b\u5230\u771f\u5b9e\u503c\u3002", "motivation": "\u89e3\u51b3\u5728\u5b58\u5728\u9ad8\u76f8\u5bf9\u5ea6\u975e\u8d85\u8c03\u7ea6\u675f\u548c\u53c2\u6570\u4e0d\u786e\u5b9a\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u81ea\u9002\u5e94\u8986\u76d6\u540d\u4e49\u63a7\u5236\u5668\u4e0d\u5b89\u5168\u52a8\u4f5c\u7684\u95ee\u9898\uff0c\u907f\u514d\u8bbe\u8ba1\u8fc7\u7a0b\u4e2d\u4e0e\u53c2\u6570\u4f30\u8ba1\u8bef\u5dee\u7684\u9ad8\u9636\u5bfc\u6570\u8026\u5408\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5c06\u63a7\u5236\u5668\u548c\u53c2\u6570\u6807\u8bc6\u5668\u5206\u5f00\u8bbe\u8ba1\u3002\u63a7\u5236\u5668\u6a21\u5757\u786e\u4fdd\u7531\u53c2\u6570\u4e0d\u786e\u5b9a\u6027\u5f15\u8d77\u7684\u5b89\u5168\u8fdd\u89c4\u6709\u754c\uff0c\u53c2\u6570\u6807\u8bc6\u5668\u6a21\u5757\u4fdd\u8bc1\u53c2\u6570\u4f30\u8ba1\u8bef\u5dee\u53ca\u5176\u4e00\u9636\u5bfc\u6570\u6709\u754c\u6216\u5e73\u65b9\u53ef\u79ef\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u95ed\u73af\u5b89\u5168\u8fdd\u89c4\u53d7\u521d\u59cb\u4f30\u8ba1\u8bef\u5dee\u7684\u53ef\u8c03\u51fd\u6570\u9650\u5236\uff0c\u968f\u7740\u65f6\u95f4\u63a8\u79fb\uff0c\u53c2\u6570\u4f30\u8ba1\u6536\u655b\u5230\u771f\u5b9e\u503c\uff0c\u5b89\u5168\u8fdd\u89c4\u76f8\u5e94\u51cf\u5c11\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6a21\u5757\u5316\u8bbe\u8ba1\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u76f8\u5bf9\u5ea6\u7ea6\u675f\u4e0b\u7684\u81ea\u9002\u5e94\u5b89\u5168\u63a7\u5236\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6709\u754c\u7684\u5b89\u5168\u8fdd\u89c4\u548c\u53c2\u6570\u4f30\u8ba1\u7684\u6e10\u8fd1\u6536\u655b\u3002"}}
{"id": "2509.18229", "categories": ["cs.AI", "70, 74, 76, 80"], "pdf": "https://arxiv.org/pdf/2509.18229", "abs": "https://arxiv.org/abs/2509.18229", "authors": ["Anthony Patera", "Rohan Abeyaratne"], "title": "An N-Plus-1 GPT Agency for Critical Solution of Mechanical Engineering Analysis Problems", "comment": null, "summary": "Generative AI, and specifically GPT, can produce a remarkable solution to a\nmechanical engineering analysis problem - but also, on occasion, a flawed\nsolution. For example, an elementary mechanics problem is solved flawlessly in\none GPT instance and incorrectly in a subsequent GPT instance, with a success\nprobability of only 85%. This unreliability renders \"out-of-the-box\" GPT\nunsuitable for deployment in education or engineering practice. We introduce an\n\"N-Plus-1\" GPT Agency for Initial (Low-Cost) Analysis of mechanical engineering\nProblem Statements. Agency first launches N instantiations of Agent Solve to\nyield N independent Proposed Problem Solution Realizations; Agency then invokes\nAgent Compare to summarize and compare the N Proposed Problem Solution\nRealizations and to provide a Recommended Problem Solution. We argue from\nCondorcet's Jury Theorem that, for a Problem Statement characterized by\nper-Solve success probability greater than 1/2 (and N sufficiently large), the\nPredominant (Agent Compare) Proposed Problem Solution will, with high\nprobability, correspond to a Correct Proposed Problem Solution. Furthermore,\nAgent Compare can also incorporate aspects of Secondary (Agent Compare)\nProposed Problem Solutions, in particular when the latter represent alternative\nProblem Statement interpretations - different Mathematical Models - or\nalternative Mathematical Solution Procedures. Comparisons to Grok Heavy, a\ncommercial multi-agent model, show similarities in design and performance, but\nalso important differences in emphasis: our Agency focuses on transparency and\npedagogical value.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\"N-Plus-1\"GPT\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u4e2a\u72ec\u7acb\u6c42\u89e3\u4ee3\u7406\u548c\u6bd4\u8f83\u4ee3\u7406\u6765\u63d0\u9ad8\u673a\u68b0\u5de5\u7a0b\u95ee\u9898\u5206\u6790\u7684\u53ef\u9760\u6027\uff0c\u89e3\u51b3\u4e86\u5355\u4e00GPT\u6a21\u578b85%\u6210\u529f\u7387\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "motivation": "GPT\u5728\u673a\u68b0\u5de5\u7a0b\u5206\u6790\u4e2d\u867d\u7136\u80fd\u4ea7\u751f\u51fa\u8272\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u6210\u529f\u7387\u4ec5\u4e3a85%\uff0c\u8fd9\u79cd\u4e0d\u53ef\u9760\u6027\u4f7f\u5176\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8e\u6559\u80b2\u6216\u5de5\u7a0b\u5b9e\u8df5\u3002\u9700\u8981\u4e00\u79cd\u673a\u5236\u6765\u63d0\u9ad8\u89e3\u51b3\u65b9\u6848\u7684\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528N+1\u4ee3\u7406\u6846\u67b6\uff1a\u9996\u5148\u542f\u52a8N\u4e2a\u72ec\u7acb\u6c42\u89e3\u4ee3\u7406\u751f\u6210N\u4e2a\u89e3\u51b3\u65b9\u6848\uff0c\u7136\u540e\u901a\u8fc7\u6bd4\u8f83\u4ee3\u7406\u6c47\u603b\u6bd4\u8f83\u8fd9\u4e9b\u65b9\u6848\u5e76\u63a8\u8350\u6700\u4f18\u89e3\u3002\u57fa\u4e8e\u5b54\u591a\u585e\u966a\u5ba1\u56e2\u5b9a\u7406\uff0c\u5f53\u6bcf\u4e2a\u6c42\u89e3\u4ee3\u7406\u7684\u6210\u529f\u6982\u7387\u5927\u4e8e1/2\u4e14N\u8db3\u591f\u5927\u65f6\uff0c\u591a\u6570\u89e3\u51b3\u65b9\u6848\u5c06\u6b63\u786e\u3002", "result": "\u4e0e\u5546\u4e1a\u591a\u4ee3\u7406\u6a21\u578bGrok Heavy\u76f8\u6bd4\uff0c\u8be5\u6846\u67b6\u5728\u8bbe\u8ba1\u548c\u6027\u80fd\u4e0a\u5177\u6709\u76f8\u4f3c\u6027\uff0c\u4f46\u66f4\u6ce8\u91cd\u900f\u660e\u5ea6\u548c\u6559\u5b66\u4ef7\u503c\u3002\u901a\u8fc7\u591a\u6570\u6295\u7968\u673a\u5236\u663e\u8457\u63d0\u9ad8\u4e86\u89e3\u51b3\u65b9\u6848\u7684\u53ef\u9760\u6027\u3002", "conclusion": "N-Plus-1\u4ee3\u7406\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86GPT\u5728\u673a\u68b0\u5de5\u7a0b\u5206\u6790\u4e2d\u7684\u4e0d\u53ef\u9760\u6027\u95ee\u9898\uff0c\u4e3a\u6559\u80b2\u548c\u6280\u672f\u5b9e\u8df5\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684AI\u8f85\u52a9\u5de5\u5177\uff0c\u7279\u522b\u5f3a\u8c03\u900f\u660e\u5ea6\u548c\u6559\u5b66\u4ef7\u503c\u3002"}}
{"id": "2509.18592", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.18592", "abs": "https://arxiv.org/abs/2509.18592", "authors": ["Neel P. Bhatt", "Yunhao Yang", "Rohan Siva", "Pranay Samineni", "Daniel Milan", "Zhangyang Wang", "Ufuk Topcu"], "title": "VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation", "comment": "Codebase, datasets, and videos for VLN-Zero are available at:\n  https://vln-zero.github.io/", "summary": "Rapid adaptation in unseen environments is essential for scalable real-world\nautonomy, yet existing approaches rely on exhaustive exploration or rigid\nnavigation policies that fail to generalize. We present VLN-Zero, a two-phase\nvision-language navigation framework that leverages vision-language models to\nefficiently construct symbolic scene graphs and enable zero-shot neurosymbolic\nnavigation. In the exploration phase, structured prompts guide VLM-based search\ntoward informative and diverse trajectories, yielding compact scene graph\nrepresentations. In the deployment phase, a neurosymbolic planner reasons over\nthe scene graph and environmental observations to generate executable plans,\nwhile a cache-enabled execution module accelerates adaptation by reusing\npreviously computed task-location trajectories. By combining rapid exploration,\nsymbolic reasoning, and cache-enabled execution, the proposed framework\novercomes the computational inefficiency and poor generalization of prior\nvision-language navigation methods, enabling robust and scalable\ndecision-making in unseen environments. VLN-Zero achieves 2x higher success\nrate compared to state-of-the-art zero-shot models, outperforms most fine-tuned\nbaselines, and reaches goal locations in half the time with 55% fewer VLM calls\non average compared to state-of-the-art models across diverse environments.\nCodebase, datasets, and videos for VLN-Zero are available at:\nhttps://vln-zero.github.io/.", "AI": {"tldr": "VLN-Zero\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u6846\u67b6\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u7b26\u53f7\u573a\u666f\u56fe\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u795e\u7ecf\u7b26\u53f7\u5bfc\u822a\uff0c\u5728\u672a\u89c1\u73af\u5883\u4e2d\u5b9e\u73b0\u5feb\u901f\u9002\u5e94\u548c\u9ad8\u6548\u5bfc\u822a\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u65b9\u6cd5\u5728\u672a\u89c1\u73af\u5883\u4e2d\u9002\u5e94\u6027\u5dee\u3001\u8ba1\u7b97\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u81ea\u4e3b\u5bfc\u822a\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u63a2\u7d22\u9636\u6bb5\u4f7f\u7528\u7ed3\u6784\u5316\u63d0\u793a\u5f15\u5bfcVLM\u641c\u7d22\u6784\u5efa\u7d27\u51d1\u573a\u666f\u56fe\uff1b\u90e8\u7f72\u9636\u6bb5\u4f7f\u7528\u795e\u7ecf\u7b26\u53f7\u89c4\u5212\u5668\u5728\u573a\u666f\u56fe\u4e0a\u63a8\u7406\u751f\u6210\u53ef\u6267\u884c\u8ba1\u5212\uff0c\u5e76\u901a\u8fc7\u7f13\u5b58\u673a\u5236\u52a0\u901f\u9002\u5e94\u3002", "result": "\u5728\u591a\u6837\u5316\u73af\u5883\u4e2d\uff0cVLN-Zero\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u6a21\u578b\u6210\u529f\u7387\u63d0\u9ad82\u500d\uff0c\u8d85\u8d8a\u5927\u591a\u6570\u5fae\u8c03\u57fa\u7ebf\uff0c\u5230\u8fbe\u76ee\u6807\u4f4d\u7f6e\u65f6\u95f4\u51cf\u534a\uff0cVLM\u8c03\u7528\u51cf\u5c1155%\u3002", "conclusion": "VLN-Zero\u901a\u8fc7\u5feb\u901f\u63a2\u7d22\u3001\u7b26\u53f7\u63a8\u7406\u548c\u7f13\u5b58\u6267\u884c\u76f8\u7ed3\u5408\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u8ba1\u7b97\u4f4e\u6548\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5728\u672a\u89c1\u73af\u5883\u4e2d\u9c81\u68d2\u4e14\u53ef\u6269\u5c55\u7684\u51b3\u7b56\u5236\u5b9a\u3002"}}
{"id": "2509.19045", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.19045", "abs": "https://arxiv.org/abs/2509.19045", "authors": ["Dakota Thompson", "Amro M. Farid"], "title": "A Weighted Least Squares Error Hetero-functional Graph State Estimator of the American Multi-modal Energy System", "comment": "27 pages, 3 Tables, 11 Figures", "summary": "As one of the most pressing challenges of the 21st century, global climate\nchange demands a host of changes across at least four critical energy\ninfrastructures: the electric grid, the natural gas system, the oil system, and\nthe coal system. In the context of the United States, this paper refers to this\nsystem-of-systems as ``The American Multi-Modal Energy System (AMES)\". These\ncombined changes necessitate an understanding of the AMES interdependencies,\nboth structurally and behaviorally, to develop and enact effective policies.\nThis work focuses on behavioral analysis methods to provide examples of how to\nanalyze system behavior and the critical matter and energy flows through the\nsystem. Building upon past works, two regions of the AMES are modeled, and\ntheir behavior is analyzed using Hetero-functional Graph Theory (HFGT). More\nspecifically, the work presents a weighted least square error state estimation\nmodel of the AMES. State estimation has played a major role in the operation\nand development of the American Electric Power System. This work extends the\nstate estimation analysis beyond the single-operand electric grid environment\ninto the heterogeneous environment of the AMES. Employing a data-driven and\nmodel-based systems engineering approach in combination with HFGT, a Weighted\nLeast Squares Error Hetero-functional Graph State Estimation (WLSEHFGSE)\noptimization program is developed to estimate the optimal flows of mass and\nenergy through the AMES. This work is the first to integrate state estimation\nmethods with HFGT. Furthermore, it demonstrates how such a WLSEHFGSE recovers\nthe mass and energy flows in a system-of-systems like the AMES with asset-level\ngranularity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u52a0\u6743\u6700\u5c0f\u4e8c\u4e58\u8bef\u5dee\u5f02\u8d28\u56fe\u72b6\u6001\u4f30\u8ba1\uff08WLSEHFGSE\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u7f8e\u56fd\u591a\u6a21\u6001\u80fd\u6e90\u7cfb\u7edf\uff08AMES\uff09\u4e2d\u8d28\u91cf\u548c\u80fd\u91cf\u7684\u6700\u4f18\u6d41\u52a8\uff0c\u9996\u6b21\u5c06\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\u4e0e\u5f02\u8d28\u56fe\u7406\u8bba\uff08HFGT\uff09\u76f8\u7ed3\u5408\u3002", "motivation": "\u5e94\u5bf921\u4e16\u7eaa\u5168\u7403\u6c14\u5019\u53d8\u5316\u6311\u6218\uff0c\u9700\u8981\u7406\u89e3\u7f8e\u56fd\u591a\u6a21\u6001\u80fd\u6e90\u7cfb\u7edf\uff08\u7535\u7f51\u3001\u5929\u7136\u6c14\u7cfb\u7edf\u3001\u77f3\u6cb9\u7cfb\u7edf\u3001\u7164\u70ad\u7cfb\u7edf\uff09\u4e4b\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\u5173\u7cfb\uff0c\u4ee5\u5236\u5b9a\u6709\u6548\u653f\u7b56\u3002", "method": "\u91c7\u7528\u6570\u636e\u9a71\u52a8\u548c\u57fa\u4e8e\u6a21\u578b\u7684\u7cfb\u7edf\u5de5\u7a0b\u65b9\u6cd5\uff0c\u7ed3\u5408\u5f02\u8d28\u56fe\u7406\u8bba\uff08HFGT\uff09\uff0c\u5f00\u53d1\u4e86WLSEHFGSE\u4f18\u5316\u7a0b\u5e8f\u6765\u4f30\u8ba1AMES\u4e2d\u7684\u8d28\u91cf\u548c\u80fd\u91cf\u6d41\u52a8\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u7cfb\u7edf\u7ea7\u7cfb\u7edf\u4e2d\u4ee5\u8d44\u4ea7\u7ea7\u7c92\u5ea6\u6062\u590d\u8d28\u91cf\u548c\u80fd\u91cf\u6d41\u52a8\uff0c\u9996\u6b21\u5c06\u72b6\u6001\u4f30\u8ba1\u6269\u5c55\u5230\u5f02\u6784\u7684AMES\u73af\u5883\u4e2d\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5206\u6790\u590d\u6742\u80fd\u6e90\u7cfb\u7edf\u7684\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86HFGT\u4e0e\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\u7ed3\u5408\u5728\u7cfb\u7edf\u7ea7\u7cfb\u7edf\u5206\u6790\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.18230", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18230", "abs": "https://arxiv.org/abs/2509.18230", "authors": ["Zihan Dong", "Xinyu Fan", "Zixiang Tang", "Yunqing Li"], "title": "Towards General Computer Control with Hierarchical Agents and Multi-Level Action Spaces", "comment": null, "summary": "Controlling desktop applications via software remains a fundamental yet\nunder-served problem. Existing multi-modal large language models (MLLMs) ingest\nscreenshots and task instructions to generate keystrokes and mouse events, but\nthey suffer from prohibitive inference latency, poor sample efficiency on\nlong-horizon sparse-reward tasks, and infeasible on-device deployment. We\nintroduce a lightweight hierarchical reinforcement learning framework,\nComputerAgent, that formulates OS control as a two-level option process\n(manager and subpolicy), employs a triple-modal state encoder (screenshot, task\nID, numeric state) to handle visual and contextual diversity, integrates\nmeta-actions with an early-stop mechanism to reduce wasted interactions, and\nuses a compact vision backbone plus small policy networks for on-device\ninference (15M parameters). On a suite of 135 real-world desktop tasks,\nComputerAgent attains 92.1% success on simple tasks (<8 steps) and 58.8% on\nhard tasks (>=8 steps), matching or exceeding 200B-parameter MLLM baselines on\nsimple scenarios while reducing model size by over four orders of magnitude and\nhalving inference time. These results demonstrate that hierarchical RL offers a\npractical, scalable alternative to monolithic MLLM-based automation for\ncomputer control.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6ComputerAgent\uff0c\u7528\u4e8e\u684c\u9762\u5e94\u7528\u63a7\u5236\uff0c\u76f8\u6bd4\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c0f\u6a21\u578b\u89c4\u6a21\u548c\u63a8\u7406\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u684c\u9762\u5e94\u7528\u63a7\u5236\u65b9\u6cd5\u5b58\u5728\u63a8\u7406\u5ef6\u8fdf\u9ad8\u3001\u6837\u672c\u6548\u7387\u4f4e\u3001\u65e0\u6cd5\u5728\u8bbe\u5907\u4e0a\u90e8\u7f72\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u64cd\u4f5c\u7cfb\u7edf\u63a7\u5236\u5efa\u6a21\u4e3a\u4e24\u7ea7\u9009\u9879\u8fc7\u7a0b\uff08\u7ba1\u7406\u5668\u548c\u5b50\u7b56\u7565\uff09\uff0c\u4f7f\u7528\u4e09\u6a21\u6001\u72b6\u6001\u7f16\u7801\u5668\u5904\u7406\u89c6\u89c9\u548c\u4e0a\u4e0b\u6587\u591a\u6837\u6027\uff0c\u96c6\u6210\u5143\u52a8\u4f5c\u548c\u65e9\u505c\u673a\u5236\u51cf\u5c11\u65e0\u6548\u4ea4\u4e92\uff0c\u4f7f\u7528\u7d27\u51d1\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\u548c\u5c0f\u578b\u7b56\u7565\u7f51\u7edc\u5b9e\u73b0\u8bbe\u5907\u7aef\u63a8\u7406\u3002", "result": "\u5728135\u4e2a\u771f\u5b9e\u4e16\u754c\u684c\u9762\u4efb\u52a1\u6d4b\u8bd5\u4e2d\uff0c\u7b80\u5355\u4efb\u52a1\u6210\u529f\u738792.1%\uff0c\u56f0\u96be\u4efb\u52a1\u6210\u529f\u738758.8%\uff0c\u6027\u80fd\u5339\u914d\u6216\u8d85\u8d8a200B\u53c2\u6570\u5927\u6a21\u578b\u57fa\u7ebf\uff0c\u540c\u65f6\u6a21\u578b\u89c4\u6a21\u51cf\u5c11\u56db\u4e2a\u6570\u91cf\u7ea7\uff0c\u63a8\u7406\u65f6\u95f4\u51cf\u534a\u3002", "conclusion": "\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u4e3a\u8ba1\u7b97\u673a\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f18\u4e8e\u5355\u4e00\u7684\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u5316\u65b9\u6cd5\u3002"}}
{"id": "2509.18597", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18597", "abs": "https://arxiv.org/abs/2509.18597", "authors": ["Yuan Meng", "Zhenguo Sun", "Max Fest", "Xukun Li", "Zhenshan Bing", "Alois Knoll"], "title": "Growing with Your Embodied Agent: A Human-in-the-Loop Lifelong Code Generation Framework for Long-Horizon Manipulation Skills", "comment": "upload 9 main page - v1", "summary": "Large language models (LLMs)-based code generation for robotic manipulation\nhas recently shown promise by directly translating human instructions into\nexecutable code, but existing methods remain noisy, constrained by fixed\nprimitives and limited context windows, and struggle with long-horizon tasks.\nWhile closed-loop feedback has been explored, corrected knowledge is often\nstored in improper formats, restricting generalization and causing catastrophic\nforgetting, which highlights the need for learning reusable skills. Moreover,\napproaches that rely solely on LLM guidance frequently fail in extremely\nlong-horizon scenarios due to LLMs' limited reasoning capability in the robotic\ndomain, where such issues are often straightforward for humans to identify. To\naddress these challenges, we propose a human-in-the-loop framework that encodes\ncorrections into reusable skills, supported by external memory and\nRetrieval-Augmented Generation with a hint mechanism for dynamic reuse.\nExperiments on Ravens, Franka Kitchen, and MetaWorld, as well as real-world\nsettings, show that our framework achieves a 0.93 success rate (up to 27%\nhigher than baselines) and a 42% efficiency improvement in correction rounds.\nIt can robustly solve extremely long-horizon tasks such as \"build a house\",\nwhich requires planning over 20 primitives.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4eba\u673a\u534f\u540c\u6846\u67b6\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\uff0c\u901a\u8fc7\u5c06\u4eba\u7c7b\u4fee\u6b63\u7f16\u7801\u4e3a\u53ef\u91cd\u7528\u6280\u80fd\uff0c\u7ed3\u5408\u5916\u90e8\u8bb0\u5fc6\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u65f6\u7a0b\u4efb\u52a1\u7684\u6267\u884c\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u673a\u5668\u4eba\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u566a\u58f0\u5927\u3001\u53d7\u9650\u4e8e\u56fa\u5b9a\u539f\u8bed\u3001\u4e0a\u4e0b\u6587\u7a97\u53e3\u6709\u9650\u3001\u96be\u4ee5\u5904\u7406\u957f\u65f6\u7a0b\u4efb\u52a1\u7b49\u95ee\u9898\uff0c\u4e14\u95ed\u73af\u53cd\u9988\u4e2d\u7684\u4fee\u6b63\u77e5\u8bc6\u5b58\u50a8\u683c\u5f0f\u4e0d\u5f53\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u5e76\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\u3002", "method": "\u91c7\u7528\u4eba\u5728\u56de\u8def\u6846\u67b6\uff0c\u5c06\u4eba\u7c7b\u4fee\u6b63\u7f16\u7801\u4e3a\u53ef\u91cd\u7528\u6280\u80fd\uff0c\u4f7f\u7528\u5916\u90e8\u8bb0\u5fc6\u548c\u5e26\u6709\u63d0\u793a\u673a\u5236\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u5b9e\u73b0\u52a8\u6001\u91cd\u7528\u3002", "result": "\u5728Ravens\u3001Franka Kitchen\u548cMetaWorld\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4ee5\u53ca\u771f\u5b9e\u573a\u666f\u4e2d\uff0c\u6846\u67b6\u5b9e\u73b0\u4e860.93\u7684\u6210\u529f\u7387\uff08\u6bd4\u57fa\u7ebf\u63d0\u5347\u9ad8\u8fbe27%\uff09\uff0c\u4fee\u6b63\u8f6e\u6b21\u6548\u7387\u63d0\u534742%\uff0c\u80fd\u591f\u7a33\u5065\u89e3\u51b3\u9700\u8981\u89c4\u5212\u8d85\u8fc720\u4e2a\u539f\u8bed\u7684\"\u5efa\u9020\u623f\u5c4b\"\u7b49\u6781\u7aef\u957f\u65f6\u7a0b\u4efb\u52a1\u3002", "conclusion": "\u8be5\u4eba\u673a\u534f\u540c\u6846\u67b6\u901a\u8fc7\u5c06\u4fee\u6b63\u77e5\u8bc6\u7f16\u7801\u4e3a\u53ef\u91cd\u7528\u6280\u80fd\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u957f\u65f6\u7a0b\u4efb\u52a1\u89c4\u5212\u96be\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6210\u529f\u7387\u548c\u6548\u7387\u3002"}}
{"id": "2509.19079", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.19079", "abs": "https://arxiv.org/abs/2509.19079", "authors": ["Samuel Chamoun", "Christian McDowell", "Robin Buchanan", "Kevin Chan", "Eric Graves", "Yin Sun"], "title": "MAPPO for Edge Server Monitoring", "comment": "6 pages, 4 figures. Accepted to IEEE MILCOM 2025", "summary": "In this paper, we consider a goal-oriented communication problem for edge\nserver monitoring, where jobs arrive intermittently at multiple dispatchers and\nmust be assigned to shared edge servers with finite queues and time-varying\navailability. Accurate knowledge of server status is critical for sustaining\nhigh throughput, yet remains challenging under dynamic workloads and partial\nobservability. To address this challenge, each dispatcher maintains server\nknowledge through two complementary mechanisms: (i) active status queries that\nprovide instantaneous updates at a communication cost, and (ii) job execution\nfeedback that reveals server conditions opportunistically. We formulate a\ncooperative multi-agent distributed decision-making problem in which\ndispatchers jointly optimize query scheduling to balance throughput against\ncommunication overhead. To solve this problem, we propose a Multi-Agent\nProximal Policy Optimization (MAPPO)-based algorithm that leverages centralized\ntraining with decentralized execution (CTDE) to learn distributed\nquery-and-dispatch policies under partial and stale observations. Numerical\nevaluations show that MAPPO achieves superior throughput-cost tradeoffs and\nsignificantly outperforms baseline strategies, achieving on average a 30%\nimprovement over the closest baseline.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08MAPPO\uff09\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u8fb9\u7f18\u670d\u52a1\u5668\u76d1\u63a7\u4e2d\u7684\u76ee\u6807\u5bfc\u5411\u901a\u4fe1\u95ee\u9898\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u67e5\u8be2\u8c03\u5ea6\u6765\u5e73\u8861\u541e\u5410\u91cf\u548c\u901a\u4fe1\u5f00\u9500\u3002", "motivation": "\u8fb9\u7f18\u670d\u52a1\u5668\u76d1\u63a7\u4e2d\uff0c\u4f5c\u4e1a\u95f4\u6b47\u6027\u5230\u8fbe\u591a\u4e2a\u8c03\u5ea6\u5668\u5e76\u9700\u8981\u5206\u914d\u5230\u5177\u6709\u6709\u9650\u961f\u5217\u548c\u65f6\u53d8\u53ef\u7528\u6027\u7684\u5171\u4eab\u8fb9\u7f18\u670d\u52a1\u5668\u3002\u5728\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u4e0b\uff0c\u51c6\u786e\u4e86\u89e3\u670d\u52a1\u5668\u72b6\u6001\u5bf9\u4e8e\u7ef4\u6301\u9ad8\u541e\u5410\u91cf\u81f3\u5173\u91cd\u8981\u4f46\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u6bcf\u4e2a\u8c03\u5ea6\u5668\u901a\u8fc7\u4e24\u79cd\u4e92\u8865\u673a\u5236\u7ef4\u62a4\u670d\u52a1\u5668\u77e5\u8bc6\uff1a\u4e3b\u52a8\u72b6\u6001\u67e5\u8be2\u548c\u4f5c\u4e1a\u6267\u884c\u53cd\u9988\u3002\u63d0\u51fa\u57fa\u4e8eMAPPO\u7684\u7b97\u6cd5\uff0c\u5229\u7528\u96c6\u4e2d\u8bad\u7ec3\u5206\u6563\u6267\u884c\uff08CTDE\uff09\u6765\u5b66\u4e60\u5728\u90e8\u5206\u548c\u8fc7\u65f6\u89c2\u6d4b\u4e0b\u7684\u5206\u5e03\u5f0f\u67e5\u8be2\u548c\u8c03\u5ea6\u7b56\u7565\u3002", "result": "\u6570\u503c\u8bc4\u4f30\u663e\u793a\uff0cMAPPO\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u541e\u5410\u91cf-\u6210\u672c\u6743\u8861\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u7b56\u7565\uff0c\u5e73\u5747\u6bd4\u6700\u63a5\u8fd1\u7684\u57fa\u7ebf\u63d0\u9ad8\u4e8630%\u7684\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684MAPPO\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u8fb9\u7f18\u670d\u52a1\u5668\u76d1\u63a7\u4e2d\u7684\u5206\u5e03\u5f0f\u51b3\u7b56\u95ee\u9898\uff0c\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e0b\u5b9e\u73b0\u4e86\u901a\u4fe1\u5f00\u9500\u548c\u7cfb\u7edf\u541e\u5410\u91cf\u7684\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2509.18234", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18234", "abs": "https://arxiv.org/abs/2509.18234", "authors": ["Yu Gu", "Jingjing Fu", "Xiaodong Liu", "Jeya Maria Jose Valanarasu", "Noel Codella", "Reuben Tan", "Qianchu Liu", "Ying Jin", "Sheng Zhang", "Jinyu Wang", "Rui Wang", "Lei Song", "Guanghui Qin", "Naoto Usuyama", "Cliff Wong", "Cheng Hao", "Hohin Lee", "Praneeth Sanapathi", "Sarah Hilado", "Bian Jiang", "Javier Alvarez-Valle", "Mu Wei", "Jianfeng Gao", "Eric Horvitz", "Matt Lungren", "Hoifung Poon", "Paul Vozila"], "title": "The Illusion of Readiness: Stress Testing Large Frontier Models on Multimodal Medical Benchmarks", "comment": "35 pages", "summary": "Large frontier models like GPT-5 now achieve top scores on medical\nbenchmarks. But our stress tests tell a different story. Leading systems often\nguess correctly even when key inputs like images are removed, flip answers\nunder trivial prompt changes, and fabricate convincing yet flawed reasoning.\nThese aren't glitches; they expose how today's benchmarks reward test-taking\ntricks over medical understanding. We evaluate six flagship models across six\nwidely used benchmarks and find that high leaderboard scores hide brittleness\nand shortcut learning. Through clinician-guided rubric evaluation, we show that\nbenchmarks vary widely in what they truly measure yet are treated\ninterchangeably, masking failure modes. We caution that medical benchmark\nscores do not directly reflect real-world readiness. If we want AI to earn\ntrust in healthcare, we must demand more than leaderboard wins and must hold\nsystems accountable for robustness, sound reasoning, and alignment with real\nmedical demands.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u5f53\u524d\u533b\u7597AI\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u4e25\u91cd\u95ee\u9898\uff0c\u5927\u578b\u6a21\u578b\u867d\u7136\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5f97\u5206\u5f88\u9ad8\uff0c\u4f46\u5b9e\u9645\u4e0a\u662f\u901a\u8fc7\u5e94\u8bd5\u6280\u5de7\u800c\u975e\u771f\u6b63\u7684\u533b\u5b66\u7406\u89e3\u6765\u83b7\u5f97\u5206\u6570\uff0c\u5b58\u5728\u8106\u5f31\u6027\u548c\u6377\u5f84\u5b66\u4e60\u7684\u95ee\u9898\u3002", "motivation": "\u63ed\u793a\u5f53\u524d\u533b\u7597AI\u57fa\u51c6\u6d4b\u8bd5\u7684\u5c40\u9650\u6027\uff0c\u8bc1\u660e\u9ad8\u5206\u6570\u5e76\u4e0d\u7b49\u540c\u4e8e\u771f\u5b9e\u7684\u533b\u7597\u80fd\u529b\uff0c\u547c\u5401\u9700\u8981\u66f4\u4e25\u683c\u7684\u8bc4\u4f30\u6807\u51c6\u6765\u786e\u4fddAI\u5728\u533b\u7597\u9886\u57df\u7684\u53ef\u9760\u5e94\u7528\u3002", "method": "\u5bf9\u516d\u4e2a\u65d7\u8230\u6a21\u578b\u5728\u516d\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u533b\u7597\u57fa\u51c6\u4e0a\u8fdb\u884c\u538b\u529b\u6d4b\u8bd5\uff0c\u5305\u62ec\u79fb\u9664\u5173\u952e\u8f93\u5165\u3001\u6539\u53d8\u63d0\u793a\u8bcd\u7b49\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u4e34\u5e8a\u533b\u751f\u6307\u5bfc\u7684\u8bc4\u5206\u6807\u51c6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u53d1\u73b0\u9886\u5148\u7cfb\u7edf\u5728\u5173\u952e\u8f93\u5165\u88ab\u79fb\u9664\u65f6\u4ecd\u80fd\u731c\u5bf9\u7b54\u6848\uff0c\u5728\u7b80\u5355\u63d0\u793a\u8bcd\u53d8\u5316\u4e0b\u4f1a\u6539\u53d8\u7b54\u6848\uff0c\u5e76\u751f\u6210\u6709\u8bf4\u670d\u529b\u4f46\u6709\u7f3a\u9677\u7684\u63a8\u7406\uff0c\u57fa\u51c6\u6d4b\u8bd5\u5206\u6570\u4e0d\u80fd\u771f\u5b9e\u53cd\u6620\u6a21\u578b\u7684\u5b9e\u9645\u533b\u7597\u80fd\u529b\u3002", "conclusion": "\u533b\u7597\u57fa\u51c6\u6d4b\u8bd5\u5206\u6570\u4e0d\u80fd\u76f4\u63a5\u53cd\u6620AI\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u51c6\u5907\u7a0b\u5ea6\uff0c\u9700\u8981\u8981\u6c42\u7cfb\u7edf\u5177\u5907\u9c81\u68d2\u6027\u3001\u5408\u7406\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u4e0e\u771f\u5b9e\u533b\u7597\u9700\u6c42\u4fdd\u6301\u4e00\u81f4\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u8ffd\u6c42\u6392\u884c\u699c\u80dc\u5229\u3002"}}
{"id": "2509.18608", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18608", "abs": "https://arxiv.org/abs/2509.18608", "authors": ["Ana Luiza Mineiro", "Francisco Affonso", "Marcelo Becker"], "title": "End-to-End Crop Row Navigation via LiDAR-Based Deep Reinforcement Learning", "comment": "Accepted to the 22nd International Conference on Advanced Robotics\n  (ICAR 2025). 7 pages", "summary": "Reliable navigation in under-canopy agricultural environments remains a\nchallenge due to GNSS unreliability, cluttered rows, and variable lighting. To\naddress these limitations, we present an end-to-end learning-based navigation\nsystem that maps raw 3D LiDAR data directly to control commands using a deep\nreinforcement learning policy trained entirely in simulation. Our method\nincludes a voxel-based downsampling strategy that reduces LiDAR input size by\n95.83%, enabling efficient policy learning without relying on labeled datasets\nor manually designed control interfaces. The policy was validated in\nsimulation, achieving a 100% success rate in straight-row plantations and\nshowing a gradual decline in performance as row curvature increased, tested\nacross varying sinusoidal frequencies and amplitudes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7aef\u5230\u7aef\u5b66\u4e60\u7684\u5bfc\u822a\u7cfb\u7edf\uff0c\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5c06\u539f\u59cb3D LiDAR\u6570\u636e\u76f4\u63a5\u6620\u5c04\u5230\u63a7\u5236\u547d\u4ee4\uff0c\u89e3\u51b3\u519c\u4e1a\u51a0\u5c42\u4e0b\u73af\u5883\u4e2d\u7684\u53ef\u9760\u5bfc\u822a\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u51a0\u5c42\u4e0b\u519c\u4e1a\u73af\u5883\u4e2dGNSS\u4e0d\u53ef\u9760\u3001\u884c\u95f4\u6742\u4e71\u548c\u5149\u7167\u53d8\u5316\u7b49\u6311\u6218\uff0c\u5b9e\u73b0\u53ef\u9760\u5bfc\u822a\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u4f53\u7d20\u7684\u4e0b\u91c7\u6837\u7b56\u7565\u5c06LiDAR\u8f93\u5165\u5c3a\u5bf8\u51cf\u5c1195.83%\uff0c\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5728\u4eff\u771f\u73af\u5883\u4e2d\u8bad\u7ec3\uff0c\u65e0\u9700\u6807\u8bb0\u6570\u636e\u96c6\u6216\u624b\u52a8\u8bbe\u8ba1\u7684\u63a7\u5236\u63a5\u53e3\u3002", "result": "\u5728\u4eff\u771f\u9a8c\u8bc1\u4e2d\uff0c\u76f4\u7ebf\u79cd\u690d\u56ed\u4e2d\u8fbe\u5230100%\u6210\u529f\u7387\uff0c\u968f\u7740\u884c\u66f2\u7387\u589e\u52a0\u6027\u80fd\u9010\u6e10\u4e0b\u964d\uff0c\u5728\u4e0d\u540c\u6b63\u5f26\u9891\u7387\u548c\u5e45\u5ea6\u4e0b\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5728\u4eff\u771f\u73af\u5883\u4e2d\u8bad\u7ec3\u7684\u5b66\u4e60\u7b56\u7565\u80fd\u591f\u6709\u6548\u5904\u7406\u519c\u4e1a\u5bfc\u822a\u6311\u6218\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2509.19107", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.19107", "abs": "https://arxiv.org/abs/2509.19107", "authors": ["Khan Masood Parvez", "Sk Md Abidar Rahaman", "Ali Shiri Sichani", "Hadi AliAkbarpour"], "title": "AI-Enabled Smart Hygiene System for Real-Time Glucose Detection", "comment": null, "summary": "This research presents a smart urinary health monitoring system incorporating\na coplanar waveguide (CPW)-fed slot-loop antenna biosensor designed to analyse\nvarious urine samples. The antenna demonstrates distinct resonant frequency\nshifts when exposed to five specific urine conditions, deviating from its\nbaseline 1.42 GHz operation. These measurable frequency variations enable the\nantenna to function as an effective microwave sensor for urinary biomarker\ndetection. A potential artificial intelligence-based Convolutional Neural\nNetworks Long Short-Term Memory (CNN-LSTM) framework is also discussed to\novercome the limitations of overlapping frequency responses, aiming to improve\nthe accuracy of health condition detection. These components contribute to the\ndevelopment of a smart toilet system that displays real-time health information\non a wall-mounted urinal screen, without requiring any user effort or\nbehavioural change.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u667a\u80fd\u5c3f\u6db2\u5065\u5eb7\u76d1\u6d4b\u7cfb\u7edf\uff0c\u91c7\u7528\u5171\u9762\u6ce2\u5bfc\u9988\u7535\u69fd\u73af\u5929\u7ebf\u751f\u7269\u4f20\u611f\u5668\u5206\u6790\u5c3f\u6db2\u6837\u672c\uff0c\u901a\u8fc7\u9891\u7387\u504f\u79fb\u68c0\u6d4b\u5c3f\u6db2\u5065\u5eb7\u72b6\u51b5\uff0c\u5e76\u8ba8\u8bbaAI\u6846\u67b6\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u6700\u7ec8\u5b9e\u73b0\u65e0\u9700\u7528\u6237\u64cd\u4f5c\u7684\u667a\u80fd\u9a6c\u6876\u7cfb\u7edf\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u7528\u6237\u884c\u4e3a\u6539\u53d8\u7684\u667a\u80fd\u5c3f\u6db2\u76d1\u6d4b\u7cfb\u7edf\uff0c\u901a\u8fc7\u975e\u4fb5\u5165\u5f0f\u65b9\u6cd5\u5b9e\u65f6\u68c0\u6d4b\u5c3f\u6db2\u751f\u7269\u6807\u5fd7\u7269\uff0c\u4e3a\u65e5\u5e38\u5065\u5eb7\u76d1\u6d4b\u63d0\u4f9b\u4fbf\u5229\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u5171\u9762\u6ce2\u5bfc\u9988\u7535\u69fd\u73af\u5929\u7ebf\u4f5c\u4e3a\u5fae\u6ce2\u4f20\u611f\u5668\uff0c\u68c0\u6d4b\u4e94\u79cd\u5c3f\u6db2\u6761\u4ef6\u4e0b\u7684\u8c10\u632f\u9891\u7387\u504f\u79fb\uff1b\u63d0\u51fa\u57fa\u4e8eCNN-LSTM\u7684AI\u6846\u67b6\u5904\u7406\u91cd\u53e0\u9891\u7387\u54cd\u5e94\u95ee\u9898\u3002", "result": "\u5929\u7ebf\u57281.42GHz\u57fa\u51c6\u9891\u7387\u4e0b\u5bf9\u4e94\u79cd\u5c3f\u6db2\u6761\u4ef6\u8868\u73b0\u51fa\u53ef\u6d4b\u91cf\u7684\u9891\u7387\u504f\u79fb\uff0c\u8bc1\u660e\u5176\u4f5c\u4e3a\u5c3f\u6db2\u751f\u7269\u6807\u5fd7\u7269\u68c0\u6d4b\u5fae\u6ce2\u4f20\u611f\u5668\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u6210\u529f\u5c55\u793a\u4e86\u667a\u80fd\u9a6c\u6876\u5b9e\u65f6\u5065\u5eb7\u76d1\u6d4b\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u65e0\u521b\u5c3f\u6db2\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u9014\u5f84\uff0c\u5177\u6709\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.18382", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18382", "abs": "https://arxiv.org/abs/2509.18382", "authors": ["Adarsha Balaji", "Le Chen", "Rajeev Thakur", "Franck Cappello", "Sandeep Madireddy"], "title": "Evaluating the Safety and Skill Reasoning of Large Reasoning Models Under Compute Constraints", "comment": null, "summary": "Test-time compute scaling has demonstrated the ability to improve the\nperformance of reasoning language models by generating longer chain-of-thought\n(CoT) sequences. However, this increase in performance comes with a significant\nincrease in computational cost. In this work, we investigate two compute\nconstraint strategies: (1) reasoning length constraint and (2) model\nquantization, as methods to reduce the compute demand of reasoning models and\nstudy their impact on their safety performance. Specifically, we explore two\napproaches to apply compute constraints to reasoning models: (1) fine-tuning\nreasoning models using a length controlled policy optimization (LCPO) based\nreinforcement learning method to satisfy a user-defined CoT reasoning length,\nand (2) applying quantization to maximize the generation of CoT sequences\nwithin a user-defined compute constraint. Furthermore, we study the trade-off\nbetween the computational efficiency and the safety of the model.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e24\u79cd\u8ba1\u7b97\u7ea6\u675f\u7b56\u7565\uff08\u63a8\u7406\u957f\u5ea6\u7ea6\u675f\u548c\u6a21\u578b\u91cf\u5316\uff09\u6765\u964d\u4f4e\u63a8\u7406\u6a21\u578b\u7684\u8ba1\u7b97\u9700\u6c42\uff0c\u5e76\u5206\u6790\u5b83\u4eec\u5bf9\u6a21\u578b\u5b89\u5168\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6269\u5c55\u867d\u7136\u80fd\u901a\u8fc7\u751f\u6210\u957f\u94fe\u601d\u7ef4\u5e8f\u5217\u63d0\u9ad8\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u663e\u8457\u589e\u52a0\u3002\u9700\u8981\u627e\u5230\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u9700\u6c42\u7684\u65b9\u6cd5\u3002", "method": "1\uff09\u4f7f\u7528\u57fa\u4e8e\u957f\u5ea6\u63a7\u5236\u7b56\u7565\u4f18\u5316\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5fae\u8c03\u63a8\u7406\u6a21\u578b\uff0c\u6ee1\u8db3\u7528\u6237\u5b9a\u4e49\u7684\u63a8\u7406\u957f\u5ea6\uff1b2\uff09\u5e94\u7528\u91cf\u5316\u6280\u672f\uff0c\u5728\u7528\u6237\u5b9a\u4e49\u7684\u8ba1\u7b97\u7ea6\u675f\u5185\u6700\u5927\u5316\u751f\u6210\u94fe\u5f0f\u601d\u7ef4\u5e8f\u5217\u3002", "result": "\u7814\u7a76\u4e86\u8ba1\u7b97\u6548\u7387\u4e0e\u6a21\u578b\u5b89\u5168\u6027\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002", "conclusion": "\u8ba1\u7b97\u7ea6\u675f\u7b56\u7565\u53ef\u4ee5\u6709\u6548\u964d\u4f4e\u63a8\u7406\u6a21\u578b\u7684\u8ba1\u7b97\u9700\u6c42\uff0c\u4f46\u9700\u8981\u5728\u8ba1\u7b97\u6548\u7387\u548c\u5b89\u5168\u6027\u4e4b\u95f4\u627e\u5230\u5e73\u8861\u70b9\u3002"}}
{"id": "2509.18609", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18609", "abs": "https://arxiv.org/abs/2509.18609", "authors": ["Chengran Yuan", "Zijian Lu", "Zhanqi Zhang", "Yimin Zhao", "Zefan Huang", "Shuo Sun", "Jiawei Sun", "Jiahui Li", "Christina Dao Wen Lee", "Dongen Li", "Marcelo H. Ang Jr"], "title": "PIE: Perception and Interaction Enhanced End-to-End Motion Planning for Autonomous Driving", "comment": null, "summary": "End-to-end motion planning is promising for simplifying complex autonomous\ndriving pipelines. However, challenges such as scene understanding and\neffective prediction for decision-making continue to present substantial\nobstacles to its large-scale deployment. In this paper, we present PIE, a\npioneering framework that integrates advanced perception, reasoning, and\nintention modeling to dynamically capture interactions between the ego vehicle\nand surrounding agents. It incorporates a bidirectional Mamba fusion that\naddresses data compression losses in multimodal fusion of camera and LiDAR\ninputs, alongside a novel reasoning-enhanced decoder integrating Mamba and\nMixture-of-Experts to facilitate scene-compliant anchor selection and optimize\nadaptive trajectory inference. PIE adopts an action-motion interaction module\nto effectively utilize state predictions of surrounding agents to refine ego\nplanning. The proposed framework is thoroughly validated on the NAVSIM\nbenchmark. PIE, without using any ensemble and data augmentation techniques,\nachieves an 88.9 PDM score and 85.6 EPDM score, surpassing the performance of\nprior state-of-the-art methods. Comprehensive quantitative and qualitative\nanalyses demonstrate that PIE is capable of reliably generating feasible and\nhigh-quality ego trajectories.", "AI": {"tldr": "PIE\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u5148\u8fdb\u611f\u77e5\u3001\u63a8\u7406\u548c\u610f\u56fe\u5efa\u6a21\u6765\u52a8\u6001\u6355\u6349\u81ea\u8f66\u4e0e\u5468\u56f4\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u4ea4\u4e92\uff0c\u5728NAVSIM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "motivation": "\u7aef\u5230\u7aef\u8fd0\u52a8\u89c4\u5212\u867d\u7136\u6709\u671b\u7b80\u5316\u590d\u6742\u7684\u81ea\u52a8\u9a7e\u9a76\u6d41\u7a0b\uff0c\u4f46\u5728\u573a\u666f\u7406\u89e3\u548c\u6709\u6548\u9884\u6d4b\u51b3\u7b56\u65b9\u9762\u4ecd\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u963b\u788d\u4e86\u5176\u5927\u89c4\u6a21\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u53cc\u5411Mamba\u878d\u5408\u89e3\u51b3\u76f8\u673a\u548cLiDAR\u591a\u6a21\u6001\u878d\u5408\u4e2d\u7684\u6570\u636e\u538b\u7f29\u635f\u5931\uff0c\u7ed3\u5408\u63a8\u7406\u589e\u5f3a\u89e3\u7801\u5668\uff08\u96c6\u6210Mamba\u548cMixture-of-Experts\uff09\u8fdb\u884c\u573a\u666f\u517c\u5bb9\u7684\u951a\u70b9\u9009\u62e9\u548c\u81ea\u9002\u5e94\u8f68\u8ff9\u63a8\u65ad\u4f18\u5316\uff0c\u4f7f\u7528\u52a8\u4f5c-\u8fd0\u52a8\u4ea4\u4e92\u6a21\u5757\u5229\u7528\u5468\u56f4\u667a\u80fd\u4f53\u72b6\u6001\u9884\u6d4b\u6765\u4f18\u5316\u81ea\u8f66\u89c4\u5212\u3002", "result": "\u5728NAVSIM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPIE\u5728\u4e0d\u4f7f\u7528\u4efb\u4f55\u96c6\u6210\u548c\u6570\u636e\u589e\u5f3a\u6280\u672f\u7684\u60c5\u51b5\u4e0b\uff0c\u83b7\u5f97\u4e8688.9 PDM\u5206\u6570\u548c85.6 EPDM\u5206\u6570\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "\u7efc\u5408\u5b9a\u91cf\u548c\u5b9a\u6027\u5206\u6790\u8868\u660e\uff0cPIE\u80fd\u591f\u53ef\u9760\u5730\u751f\u6210\u53ef\u884c\u4e14\u9ad8\u8d28\u91cf\u7684\u81ea\u8f66\u8f68\u8ff9\u3002"}}
{"id": "2509.19110", "categories": ["eess.SY", "cs.LG", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.19110", "abs": "https://arxiv.org/abs/2509.19110", "authors": ["Chenxu Ke", "Congling Tian", "Kaichen Xu", "Ye Li", "Lingcong Bao"], "title": "A Fast Initialization Method for Neural Network Controllers: A Case Study of Image-based Visual Servoing Control for the multicopter Interception", "comment": null, "summary": "Reinforcement learning-based controller design methods often require\nsubstantial data in the initial training phase. Moreover, the training process\ntends to exhibit strong randomness and slow convergence. It often requires\nconsiderable time or high computational resources. Another class of\nlearning-based method incorporates Lyapunov stability theory to obtain a\ncontrol policy with stability guarantees. However, these methods generally\nrequire an initially stable neural network control policy at the beginning of\ntraining. Evidently, a stable neural network controller can not only serve as\nan initial policy for reinforcement learning, allowing the training to focus on\nimproving controller performance, but also act as an initial state for\nlearning-based Lyapunov control methods. Although stable controllers can be\ndesigned using traditional control theory, designers still need to have a great\ndeal of control design knowledge to address increasingly complicated control\nproblems. The proposed neural network rapid initialization method in this paper\nachieves the initial training of the neural network control policy by\nconstructing datasets that conform to the stability conditions based on the\nsystem model. Furthermore, using the image-based visual servoing control for\nmulticopter interception as a case study, simulations and experiments were\nconducted to validate the effectiveness and practical performance of the\nproposed method. In the experiment, the trained control policy attains a final\ninterception velocity of 15 m/s.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u7f51\u7edc\u5feb\u901f\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u7b26\u5408\u7a33\u5b9a\u6027\u6761\u4ef6\u7684\u6570\u636e\u96c6\u6765\u5b9e\u73b0\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u7b56\u7565\u7684\u521d\u59cb\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u5668\u8bad\u7ec3\u521d\u671f\u9700\u8981\u5927\u91cf\u6570\u636e\u3001\u6536\u655b\u6162\u7684\u95ee\u9898\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u5668\u8bbe\u8ba1\u9700\u8981\u5927\u91cf\u521d\u59cb\u8bad\u7ec3\u6570\u636e\u4e14\u6536\u655b\u7f13\u6162\uff0c\u800c\u57fa\u4e8eLyapunov\u7a33\u5b9a\u6027\u7684\u65b9\u6cd5\u9700\u8981\u521d\u59cb\u7a33\u5b9a\u7684\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u7b56\u7565\u3002\u4f20\u7edf\u63a7\u5236\u7406\u8bba\u8bbe\u8ba1\u7a33\u5b9a\u63a7\u5236\u5668\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u5feb\u901f\u521d\u59cb\u5316\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u7cfb\u7edf\u6a21\u578b\u6784\u5efa\u7b26\u5408\u7a33\u5b9a\u6027\u6761\u4ef6\u7684\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u7b56\u7565\u7684\u521d\u59cb\u8bad\u7ec3\u3002\u4ee5\u591a\u65cb\u7ffc\u98de\u884c\u5668\u62e6\u622a\u7684\u56fe\u50cf\u89c6\u89c9\u4f3a\u670d\u63a7\u5236\u4e3a\u6848\u4f8b\u8fdb\u884c\u7814\u7a76\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8bad\u7ec3\u7684\u63a7\u5236\u7b56\u7565\u8fbe\u5230\u4e8615 m/s\u7684\u6700\u7ec8\u62e6\u622a\u901f\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5feb\u901f\u83b7\u5f97\u7a33\u5b9a\u7684\u521d\u59cb\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u5668\uff0c\u4e3a\u540e\u7eed\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u63d0\u4f9b\u826f\u597d\u8d77\u70b9\uff0c\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2509.18383", "categories": ["cs.AI", "cs.DM", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18383", "abs": "https://arxiv.org/abs/2509.18383", "authors": ["Moran Feldman", "Amin Karbasi"], "title": "G\u00f6del Test: Can Large Language Models Solve Easy Conjectures?", "comment": null, "summary": "Recent announcements from frontier AI model labs have highlighted strong\nresults on high-school and undergraduate math competitions. Yet it remains\nunclear whether large language models can solve new, simple conjectures in more\nadvanced areas of mathematics. We propose the G\\\"odel Test: evaluating whether\na model can produce correct proofs for very simple, previously unsolved\nconjectures. To this end, we study the performance of GPT-5 on five conjectures\nin combinatorial optimization. For each problem, we provided one or two source\npapers from which the conjecture arose, withheld our own conjecture, and then\nassessed the model's reasoning in detail. On the three easier problems, GPT-5\nproduced nearly correct solutions; for Problem 2 it even derived a different\napproximation guarantee that, upon checking, refuted our conjecture while\nproviding a valid solution. The model failed on Problem 4, which required\ncombining results from two papers. On Problem 5, a harder case without a\nvalidated conjecture, GPT-5 proposed the same algorithm we had in mind but\nfailed in the analysis, suggesting the proof is more challenging than expected.\nAlthough our sample is small, the results point to meaningful progress on\nroutine reasoning, occasional flashes of originality, and clear limitations\nwhen cross-paper synthesis is required. GPT-5 may represent an early step\ntoward frontier models eventually passing the G\\\"odel Test.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faG\u00f6del\u6d4b\u8bd5\uff0c\u8bc4\u4f30GPT-5\u5728\u7ec4\u5408\u4f18\u5316\u4e2d\u89e3\u51b3\u672a\u89e3\u7b80\u5355\u731c\u60f3\u7684\u80fd\u529b\u3002\u7ed3\u679c\u663e\u793aGPT-5\u5728\u5e38\u89c4\u63a8\u7406\u4e0a\u6709\u8fdb\u6b65\uff0c\u5076\u5c14\u5c55\u73b0\u539f\u521b\u6027\uff0c\u4f46\u5728\u8de8\u8bba\u6587\u7efc\u5408\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "motivation": "\u524d\u6cbfAI\u6a21\u578b\u5728\u6570\u5b66\u7ade\u8d5b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u80fd\u5426\u5728\u66f4\u9ad8\u7ea7\u6570\u5b66\u9886\u57df\u89e3\u51b3\u65b0\u7684\u7b80\u5355\u731c\u60f3\u4ecd\u4e0d\u660e\u786e\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7G\u00f6del\u6d4b\u8bd5\u6765\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8fd9\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u9009\u53d6\u7ec4\u5408\u4f18\u5316\u4e2d\u7684\u4e94\u4e2a\u672a\u89e3\u731c\u60f3\uff0c\u63d0\u4f9b\u76f8\u5173\u6e90\u8bba\u6587\u4f46\u9690\u85cf\u4f5c\u8005\u81ea\u5df1\u7684\u731c\u60f3\uff0c\u8be6\u7ec6\u8bc4\u4f30GPT-5\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "GPT-5\u5728\u4e09\u4e2a\u8f83\u7b80\u5355\u95ee\u9898\u4e0a\u4ea7\u751f\u63a5\u8fd1\u6b63\u786e\u7684\u89e3\uff0c\u5728\u95ee\u98982\u4e2d\u751a\u81f3\u63a8\u5bfc\u51fa\u4e0d\u540c\u7684\u8fd1\u4f3c\u4fdd\u8bc1\u5e76\u53cd\u9a73\u4e86\u4f5c\u8005\u7684\u731c\u60f3\u3002\u5728\u9700\u8981\u8de8\u8bba\u6587\u7efc\u5408\u7684\u95ee\u98984\u4e0a\u5931\u8d25\uff0c\u5728\u66f4\u590d\u6742\u7684\u95ee\u98985\u4e0a\u7b97\u6cd5\u6b63\u786e\u4f46\u5206\u6790\u5931\u8d25\u3002", "conclusion": "GPT-5\u5728\u5e38\u89c4\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u6709\u610f\u4e49\u8fdb\u5c55\uff0c\u5076\u5c14\u5c55\u73b0\u539f\u521b\u6027\uff0c\u4f46\u5728\u9700\u8981\u7efc\u5408\u591a\u7bc7\u8bba\u6587\u77e5\u8bc6\u65f6\u5b58\u5728\u660e\u663e\u5c40\u9650\uff0c\u53ef\u80fd\u4ee3\u8868\u4e86\u524d\u6cbf\u6a21\u578b\u901a\u8fc7G\u00f6del\u6d4b\u8bd5\u7684\u65e9\u671f\u6b65\u9aa4\u3002"}}
{"id": "2509.18610", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18610", "abs": "https://arxiv.org/abs/2509.18610", "authors": ["Maximilian Adang", "JunEn Low", "Ola Shorinwa", "Mac Schwager"], "title": "SINGER: An Onboard Generalist Vision-Language Navigation Policy for Drones", "comment": null, "summary": "Large vision-language models have driven remarkable progress in\nopen-vocabulary robot policies, e.g., generalist robot manipulation policies,\nthat enable robots to complete complex tasks specified in natural language.\nDespite these successes, open-vocabulary autonomous drone navigation remains an\nunsolved challenge due to the scarcity of large-scale demonstrations, real-time\ncontrol demands of drones for stabilization, and lack of reliable external pose\nestimation modules. In this work, we present SINGER for language-guided\nautonomous drone navigation in the open world using only onboard sensing and\ncompute. To train robust, open-vocabulary navigation policies, SINGER leverages\nthree central components: (i) a photorealistic language-embedded flight\nsimulator with minimal sim-to-real gap using Gaussian Splatting for efficient\ndata generation, (ii) an RRT-inspired multi-trajectory generation expert for\ncollision-free navigation demonstrations, and these are used to train (iii) a\nlightweight end-to-end visuomotor policy for real-time closed-loop control.\nThrough extensive hardware flight experiments, we demonstrate superior\nzero-shot sim-to-real transfer of our policy to unseen environments and unseen\nlanguage-conditioned goal objects. When trained on ~700k-1M observation action\npairs of language conditioned visuomotor data and deployed on hardware, SINGER\noutperforms a velocity-controlled semantic guidance baseline by reaching the\nquery 23.33% more on average, and maintains the query in the field of view\n16.67% more on average, with 10% fewer collisions.", "AI": {"tldr": "SINGER\u662f\u4e00\u4e2a\u57fa\u4e8e\u8bed\u8a00\u5f15\u5bfc\u7684\u81ea\u4e3b\u65e0\u4eba\u673a\u5bfc\u822a\u7cfb\u7edf\uff0c\u4ec5\u4f7f\u7528\u673a\u8f7d\u4f20\u611f\u548c\u8ba1\u7b97\uff0c\u901a\u8fc7\u9ad8\u65af\u6cfc\u6e85\u6a21\u62df\u5668\u751f\u6210\u6570\u636e\uff0c\u7ed3\u5408RRT\u591a\u8f68\u8ff9\u4e13\u5bb6\u548c\u8f7b\u91cf\u7ea7\u7aef\u5230\u7aef\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u8f6c\u79fb\u3002", "motivation": "\u89e3\u51b3\u5f00\u653e\u8bcd\u6c47\u81ea\u4e3b\u65e0\u4eba\u673a\u5bfc\u822a\u7684\u6311\u6218\uff0c\u5305\u62ec\u5927\u89c4\u6a21\u6f14\u793a\u6570\u636e\u7a00\u7f3a\u3001\u65e0\u4eba\u673a\u5b9e\u65f6\u63a7\u5236\u9700\u6c42\u4ee5\u53ca\u7f3a\u4e4f\u53ef\u9760\u7684\u5916\u90e8\u59ff\u6001\u4f30\u8ba1\u6a21\u5757\u3002", "method": "\u4f7f\u7528\u9ad8\u65af\u6cfc\u6e85\u6784\u5efa\u903c\u771f\u7684\u8bed\u8a00\u5d4c\u5165\u98de\u884c\u6a21\u62df\u5668\uff0c\u91c7\u7528RRT\u542f\u53d1\u7684\u591a\u8f68\u8ff9\u751f\u6210\u4e13\u5bb6\u8fdb\u884c\u65e0\u78b0\u649e\u5bfc\u822a\u6f14\u793a\uff0c\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u7aef\u5230\u7aef\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u3002", "result": "\u5728\u786c\u4ef6\u98de\u884c\u5b9e\u9a8c\u4e2d\uff0cSINGER\u5728\u672a\u89c1\u73af\u5883\u548c\u672a\u89c1\u8bed\u8a00\u6761\u4ef6\u76ee\u6807\u7269\u4f53\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u96f6\u6837\u672c\u6a21\u62df\u5230\u73b0\u5b9e\u8f6c\u79fb\u6027\u80fd\uff0c\u5e73\u5747\u5230\u8fbe\u67e5\u8be2\u76ee\u6807\u7387\u63d0\u9ad823.33%\uff0c\u89c6\u91ce\u4fdd\u6301\u7387\u63d0\u9ad816.67%\uff0c\u78b0\u649e\u51cf\u5c1110%\u3002", "conclusion": "SINGER\u6210\u529f\u5b9e\u73b0\u4e86\u4ec5\u4f7f\u7528\u673a\u8f7d\u4f20\u611f\u548c\u8ba1\u7b97\u7684\u8bed\u8a00\u5f15\u5bfc\u81ea\u4e3b\u65e0\u4eba\u673a\u5bfc\u822a\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u5f00\u653e\u8bcd\u6c47\u5bfc\u822a\u80fd\u529b\u3002"}}
{"id": "2509.19111", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.19111", "abs": "https://arxiv.org/abs/2509.19111", "authors": ["Michael Ruderman", "Elia Brescia", "Paolo Roberto Massenio", "Giuseppe Leonardo Cascella", "David Naso"], "title": "Robust Synchronous Reference Frame Phase-Looked Loop (PLL) with Feed-Forward Frequency Estimation", "comment": "8 pages, 9 figures", "summary": "Synchronous reference frame phase-looked loop (SRF-PLL) techniques are widely\nused for interfacing and control applications in the power systems and energy\nconversion at large. Since a PLL system synchronizes its output with an\nexogenous harmonic signal, often 3-phases voltage or current, the locking of\nthe frequency and phase angle depends on the performance of the feedback loop\nwith at least two integrator terms, and on the distortions of the measured\ninput quantities. For the conventional SRF-PLL with a proportional-integral\n(PI) control in feedback, we are providing a robust design which maximizes the\nphase margin and uses the normalization scheme for yielding the loop\ninsensitive to the input amplitude variations. The main improvement in the\ntransient behavior and also in tracking of frequency ramps is achieved by using\nthe robust feed-forward frequency estimator, which is model-free and suitable\nfor the noisy and time-varying harmonic signals. The proposed\nfeed-forward-feedback SRF-PLL scheme is experimentally evaluated on the\n3-phases harmonic currents from standard PMSM drives with varying angular\nspeeds and loads. Both, the tracked angular frequency and locked phase angle\nare assessed as performance metrics of the robust SRF-PLL scheme with\nfeedforwarding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u540c\u6b65\u53c2\u8003\u5750\u6807\u7cfb\u9501\u76f8\u73af\uff08SRF-PLL\uff09\u65b9\u6848\uff0c\u901a\u8fc7\u7ed3\u5408\u9c81\u68d2\u524d\u9988\u9891\u7387\u4f30\u8ba1\u5668\u548c\u53cd\u9988\u63a7\u5236\uff0c\u63d0\u9ad8\u4e86\u5bf9\u566a\u58f0\u548c\u65f6\u53d8\u8c10\u6ce2\u4fe1\u53f7\u7684\u8ddf\u8e2a\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684SRF-PLL\u5728\u9891\u7387\u659c\u5761\u8ddf\u8e2a\u548c\u77ac\u6001\u54cd\u5e94\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u4e14\u5bf9\u8f93\u5165\u5e45\u503c\u53d8\u5316\u654f\u611f\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u8bbe\u8ba1\u6765\u6539\u5584\u8fd9\u4e9b\u6027\u80fd\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6bd4\u4f8b\u79ef\u5206\uff08PI\uff09\u63a7\u5236\u53cd\u9988\uff0c\u7ed3\u5408\u6a21\u578b\u65e0\u5173\u7684\u9c81\u68d2\u524d\u9988\u9891\u7387\u4f30\u8ba1\u5668\uff0c\u5e76\u4f7f\u7528\u5f52\u4e00\u5316\u65b9\u6848\u4f7f\u73af\u8def\u5bf9\u8f93\u5165\u5e45\u503c\u53d8\u5316\u4e0d\u654f\u611f\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6848\u5728PMSM\u9a71\u52a8\u5668\u7684\u4e09\u76f8\u8c10\u6ce2\u7535\u6d41\u8ddf\u8e2a\u4e2d\uff0c\u663e\u8457\u6539\u5584\u4e86\u77ac\u6001\u884c\u4e3a\u548c\u9891\u7387\u659c\u5761\u8ddf\u8e2a\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u524d\u9988-\u53cd\u9988SRF-PLL\u65b9\u6848\u6709\u6548\u63d0\u5347\u4e86\u540c\u6b65\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u566a\u58f0\u5927\u4e14\u65f6\u53d8\u7684\u8c10\u6ce2\u4fe1\u53f7\u73af\u5883\u3002"}}
{"id": "2509.18400", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18400", "abs": "https://arxiv.org/abs/2509.18400", "authors": ["Pritish Yuvraj", "Siva Devarakonda"], "title": "ATLAS: Benchmarking and Adapting LLMs for Global Trade via Harmonized Tariff Code Classification", "comment": null, "summary": "Accurate classification of products under the Harmonized Tariff Schedule\n(HTS) is a critical bottleneck in global trade, yet it has received little\nattention from the machine learning community. Misclassification can halt\nshipments entirely, with major postal operators suspending deliveries to the\nU.S. due to incomplete customs documentation. We introduce the first benchmark\nfor HTS code classification, derived from the U.S. Customs Rulings Online\nSearch System (CROSS). Evaluating leading LLMs, we find that our fine-tuned\nAtlas model (LLaMA-3.3-70B) achieves 40 percent fully correct 10-digit\nclassifications and 57.5 percent correct 6-digit classifications, improvements\nof 15 points over GPT-5-Thinking and 27.5 points over Gemini-2.5-Pro-Thinking.\nBeyond accuracy, Atlas is roughly five times cheaper than GPT-5-Thinking and\neight times cheaper than Gemini-2.5-Pro-Thinking, and can be self-hosted to\nguarantee data privacy in high-stakes trade and compliance workflows. While\nAtlas sets a strong baseline, the benchmark remains highly challenging, with\nonly 40 percent 10-digit accuracy. By releasing both dataset and model, we aim\nto position HTS classification as a new community benchmark task and invite\nfuture work in retrieval, reasoning, and alignment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u57fa\u4e8e\u7f8e\u56fd\u6d77\u5173\u88c1\u51b3\u5728\u7ebf\u641c\u7d22\u7cfb\u7edf\u7684HTS\u4ee3\u7801\u5206\u7c7b\u57fa\u51c6\uff0c\u5e76\u5f00\u53d1\u4e86Atlas\u6a21\u578b\uff08\u57fa\u4e8eLLaMA-3.3-70B\u5fae\u8c03\uff09\uff0c\u572810\u4f4d\u6570\u5206\u7c7b\u4e0a\u8fbe\u523040%\u51c6\u786e\u7387\uff0c\u6bd4GPT-5\u548cGemini-2.5\u5206\u522b\u63d0\u534715\u548c27.5\u4e2a\u767e\u5206\u70b9\uff0c\u4e14\u6210\u672c\u663e\u8457\u964d\u4f4e\u3002", "motivation": "HTS\u4ee3\u7801\u5206\u7c7b\u662f\u5168\u7403\u8d38\u6613\u4e2d\u7684\u5173\u952e\u74f6\u9888\uff0c\u4f46\u673a\u5668\u5b66\u4e60\u793e\u533a\u5bf9\u6b64\u5173\u6ce8\u4e0d\u8db3\u3002\u9519\u8bef\u5206\u7c7b\u53ef\u80fd\u5bfc\u81f4\u8d27\u7269\u8fd0\u8f93\u5b8c\u5168\u505c\u6ede\uff0c\u4e3b\u8981\u90ae\u653f\u8fd0\u8425\u5546\u56e0\u6d77\u5173\u6587\u4ef6\u4e0d\u5b8c\u6574\u800c\u6682\u505c\u5411\u7f8e\u56fd\u53d1\u8d27\u3002", "method": "\u57fa\u4e8e\u7f8e\u56fd\u6d77\u5173\u88c1\u51b3\u5728\u7ebf\u641c\u7d22\u7cfb\u7edf\u521b\u5efa\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5bf9\u9886\u5148\u7684LLM\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u5fae\u8c03LLaMA-3.3-70B\u6a21\u578b\uff08Atlas\uff09\u8fdb\u884cHTS\u4ee3\u7801\u5206\u7c7b\u3002", "result": "Atlas\u6a21\u578b\u572810\u4f4d\u6570\u5206\u7c7b\u4e0a\u8fbe\u523040%\u51c6\u786e\u7387\uff0c6\u4f4d\u6570\u5206\u7c7b\u8fbe\u523057.5%\u51c6\u786e\u7387\uff0c\u6bd4GPT-5-Thinking\u548cGemini-2.5-Pro-Thinking\u5206\u522b\u63d0\u534715\u548c27.5\u4e2a\u767e\u5206\u70b9\uff0c\u6210\u672c\u964d\u4f4e5-8\u500d\uff0c\u4e14\u53ef\u81ea\u6258\u7ba1\u4fdd\u8bc1\u6570\u636e\u9690\u79c1\u3002", "conclusion": "Atlas\u4e3aHTS\u5206\u7c7b\u8bbe\u7acb\u4e86\u5f3a\u57fa\u7ebf\uff0c\u4f46\u8be5\u4efb\u52a1\u4ecd\u6781\u5177\u6311\u6218\u6027\u3002\u901a\u8fc7\u53d1\u5e03\u6570\u636e\u96c6\u548c\u6a21\u578b\uff0c\u65e8\u5728\u5c06HTS\u5206\u7c7b\u5b9a\u4f4d\u4e3a\u65b0\u7684\u793e\u533a\u57fa\u51c6\u4efb\u52a1\uff0c\u5e76\u4fc3\u8fdb\u68c0\u7d22\u3001\u63a8\u7406\u548c\u5bf9\u9f50\u65b9\u9762\u7684\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2509.18626", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18626", "abs": "https://arxiv.org/abs/2509.18626", "authors": ["Jay Patrikar", "Apoorva Sharma", "Sushant Veer", "Boyi Li", "Sebastian Scherer", "Marco Pavone"], "title": "The Case for Negative Data: From Crash Reports to Counterfactuals for Reasonable Driving", "comment": "8 pages, 5 figures", "summary": "Learning-based autonomous driving systems are trained mostly on incident-free\ndata, offering little guidance near safety-performance boundaries. Real crash\nreports contain precisely the contrastive evidence needed, but they are hard to\nuse: narratives are unstructured, third-person, and poorly grounded to sensor\nviews. We address these challenges by normalizing crash narratives to\nego-centric language and converting both logs and crashes into a unified\nscene-action representation suitable for retrieval. At decision time, our\nsystem adjudicates proposed actions by retrieving relevant precedents from this\nunified index; an agentic counterfactual extension proposes plausible\nalternatives, retrieves for each, and reasons across outcomes before deciding.\nOn a nuScenes benchmark, precedent retrieval substantially improves\ncalibration, with recall on contextually preferred actions rising from 24% to\n53%. The counterfactual variant preserves these gains while sharpening\ndecisions near risk.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u771f\u5b9e\u4e8b\u6545\u62a5\u544a\u6765\u6539\u8fdb\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u7cfb\u7edf\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u4e8b\u6545\u53d9\u8ff0\u8f6c\u6362\u4e3a\u81ea\u6211\u4e2d\u5fc3\u8bed\u8a00\u548c\u7edf\u4e00\u7684\u573a\u666f-\u52a8\u4f5c\u8868\u793a\uff0c\u5728\u51b3\u7b56\u65f6\u68c0\u7d22\u76f8\u5173\u5148\u4f8b\u6765\u8bc4\u4f30\u63d0\u8bae\u52a8\u4f5c\uff0c\u5e76\u901a\u8fc7\u53cd\u4e8b\u5b9e\u63a8\u7406\u6765\u6539\u8fdb\u98ce\u9669\u8fb9\u754c\u9644\u8fd1\u7684\u51b3\u7b56\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e3b\u8981\u5728\u65e0\u4e8b\u6545\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u7f3a\u4e4f\u5bf9\u5b89\u5168\u6027\u80fd\u8fb9\u754c\u9644\u8fd1\u7684\u6307\u5bfc\u3002\u771f\u5b9e\u4e8b\u6545\u62a5\u544a\u5305\u542b\u6240\u9700\u7684\u5bf9\u6bd4\u8bc1\u636e\uff0c\u4f46\u96be\u4ee5\u4f7f\u7528\uff0c\u56e0\u4e3a\u53d9\u8ff0\u662f\u975e\u7ed3\u6784\u5316\u7684\u3001\u7b2c\u4e09\u4eba\u79f0\u7684\uff0c\u4e14\u4e0e\u4f20\u611f\u5668\u89c6\u56fe\u5173\u8054\u6027\u5dee\u3002", "method": "\u5c06\u4e8b\u6545\u53d9\u8ff0\u6807\u51c6\u5316\u4e3a\u81ea\u6211\u4e2d\u5fc3\u8bed\u8a00\uff0c\u5e76\u5c06\u65e5\u5fd7\u548c\u4e8b\u6545\u8f6c\u6362\u4e3a\u7edf\u4e00\u7684\u573a\u666f-\u52a8\u4f5c\u8868\u793a\u3002\u5728\u51b3\u7b56\u65f6\uff0c\u901a\u8fc7\u68c0\u7d22\u7edf\u4e00\u7d22\u5f15\u4e2d\u7684\u76f8\u5173\u5148\u4f8b\u6765\u8bc4\u4f30\u63d0\u8bae\u52a8\u4f5c\uff1b\u53cd\u4e8b\u5b9e\u6269\u5c55\u63d0\u51fa\u5408\u7406\u66ff\u4ee3\u65b9\u6848\uff0c\u4e3a\u6bcf\u4e2a\u65b9\u6848\u68c0\u7d22\u7ed3\u679c\uff0c\u5e76\u5728\u51b3\u7b56\u524d\u8de8\u7ed3\u679c\u8fdb\u884c\u63a8\u7406\u3002", "result": "\u5728nuScenes\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5148\u4f8b\u68c0\u7d22\u663e\u8457\u63d0\u9ad8\u4e86\u6821\u51c6\u5ea6\uff0c\u4e0a\u4e0b\u6587\u504f\u597d\u52a8\u4f5c\u7684\u53ec\u56de\u7387\u4ece24%\u63d0\u9ad8\u523053%\u3002\u53cd\u4e8b\u5b9e\u53d8\u4f53\u5728\u4fdd\u6301\u8fd9\u4e9b\u6536\u76ca\u7684\u540c\u65f6\uff0c\u5728\u98ce\u9669\u8fb9\u754c\u9644\u8fd1\u4f7f\u51b3\u7b56\u66f4\u52a0\u654f\u9510\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5229\u7528\u4e86\u4e8b\u6545\u62a5\u544a\u6765\u6539\u8fdb\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u51b3\u7b56\u8d28\u91cf\uff0c\u7279\u522b\u662f\u5728\u5b89\u5168\u8fb9\u754c\u9644\u8fd1\uff0c\u901a\u8fc7\u68c0\u7d22\u5148\u4f8b\u548c\u53cd\u4e8b\u5b9e\u63a8\u7406\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u6821\u51c6\u5ea6\u548c\u51b3\u7b56\u51c6\u786e\u6027\u3002"}}
{"id": "2509.18420", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18420", "abs": "https://arxiv.org/abs/2509.18420", "authors": ["Nikolai Skripko"], "title": "Instruction-Following Evaluation in Function Calling for Large Language Models", "comment": null, "summary": "Function calling is a core capability of large language models, essential for\nAI agents. Existing benchmarks such as the Berkeley Function Calling\nLeaderboard (BFCL), tau^2-Bench (arXiv:2506.07982), and ACEBench\n(arXiv:2501.12851) evaluate argument correctness but do not test adherence to\nformat instructions embedded in parameter descriptions, such as enclosing\nvalues in double quotes or using ISO date formats.\n  We introduce IFEval-FC, a benchmark inspired by IFEval (arXiv:2311.07911)\nthat assesses precise instruction following in function calling. IFEval-FC\nencodes verifiable formats directly within JSON schema descriptions, for\nexample specifying that a value must not contain punctuation. It includes 750\ntest cases, each consisting of a function with an embedded format for one of\nits input parameters and a corresponding user query. Evaluation is fully\nalgorithmic, ensuring objectivity, reproducibility, and scalability.\n  Our results show that even state-of-the-art proprietary models, including\nGPT-5 and Claude 4.1 Opus, frequently fail to follow basic formatting rules,\nhighlighting a practical limitation for real-world agent systems. The complete\ncodebase and data are publicly available at\nhttps://github.com/Skripkon/IFEval-FC.", "AI": {"tldr": "IFEval-FC\u662f\u4e00\u4e2a\u65b0\u7684\u51fd\u6570\u8c03\u7528\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u95e8\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u51fd\u6570\u8c03\u7528\u4e2d\u5bf9\u53c2\u6570\u63cf\u8ff0\u4e2d\u683c\u5f0f\u6307\u4ee4\u7684\u9075\u5faa\u80fd\u529b\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u53ea\u5173\u6ce8\u53c2\u6570\u6b63\u786e\u6027\u800c\u5ffd\u7565\u683c\u5f0f\u8981\u6c42\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u51fd\u6570\u8c03\u7528\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982BFCL\u3001tau^2-Bench\u3001ACEBench\uff09\u53ea\u8bc4\u4f30\u53c2\u6570\u6b63\u786e\u6027\uff0c\u4e0d\u6d4b\u8bd5\u683c\u5f0f\u6307\u4ee4\u9075\u5faa\u80fd\u529b\uff0c\u800c\u73b0\u5b9e\u5e94\u7528\u4e2d\u683c\u5f0f\u8981\u6c42\uff08\u5982\u5f15\u53f7\u3001\u65e5\u671f\u683c\u5f0f\uff09\u5bf9AI\u4ee3\u7406\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002", "method": "\u57fa\u4e8eIFEval\u8bbe\u8ba1\uff0c\u5c06\u53ef\u9a8c\u8bc1\u7684\u683c\u5f0f\u8981\u6c42\u76f4\u63a5\u7f16\u7801\u5230JSON schema\u63cf\u8ff0\u4e2d\uff0c\u5305\u542b750\u4e2a\u6d4b\u8bd5\u7528\u4f8b\uff0c\u6bcf\u4e2a\u7528\u4f8b\u5305\u542b\u5e26\u6709\u683c\u5f0f\u8981\u6c42\u7684\u51fd\u6570\u548c\u5bf9\u5e94\u7528\u6237\u67e5\u8be2\uff0c\u91c7\u7528\u5168\u7b97\u6cd5\u8bc4\u4f30\u786e\u4fdd\u5ba2\u89c2\u6027\u548c\u53ef\u590d\u73b0\u6027\u3002", "result": "\u5373\u4f7f\u662fGPT-5\u548cClaude 4.1 Opus\u7b49\u6700\u5148\u8fdb\u7684\u4e13\u6709\u6a21\u578b\u4e5f\u7ecf\u5e38\u65e0\u6cd5\u9075\u5faa\u57fa\u672c\u683c\u5f0f\u89c4\u5219\uff0c\u63ed\u793a\u4e86\u73b0\u5b9e\u4e16\u754c\u4ee3\u7406\u7cfb\u7edf\u7684\u5b9e\u9645\u5c40\u9650\u6027\u3002", "conclusion": "IFEval-FC\u63ed\u793a\u4e86\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7cbe\u786e\u9075\u5faa\u683c\u5f0f\u6307\u4ee4\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4e3a\u6539\u8fdb\u51fd\u6570\u8c03\u7528\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2509.18631", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18631", "abs": "https://arxiv.org/abs/2509.18631", "authors": ["Shuo Cheng", "Liqian Ma", "Zhenyang Chen", "Ajay Mandlekar", "Caelan Garrett", "Danfei Xu"], "title": "Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training", "comment": null, "summary": "Behavior cloning has shown promise for robot manipulation, but real-world\ndemonstrations are costly to acquire at scale. While simulated data offers a\nscalable alternative, particularly with advances in automated demonstration\ngeneration, transferring policies to the real world is hampered by various\nsimulation and real domain gaps. In this work, we propose a unified\nsim-and-real co-training framework for learning generalizable manipulation\npolicies that primarily leverages simulation and only requires a few real-world\ndemonstrations. Central to our approach is learning a domain-invariant,\ntask-relevant feature space. Our key insight is that aligning the joint\ndistributions of observations and their corresponding actions across domains\nprovides a richer signal than aligning observations (marginals) alone. We\nachieve this by embedding an Optimal Transport (OT)-inspired loss within the\nco-training framework, and extend this to an Unbalanced OT framework to handle\nthe imbalance between abundant simulation data and limited real-world examples.\nWe validate our method on challenging manipulation tasks, showing it can\nleverage abundant simulation data to achieve up to a 30% improvement in the\nreal-world success rate and even generalize to scenarios seen only in\nsimulation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u4eff\u771f\u4e0e\u771f\u5b9e\u4e16\u754c\u534f\u540c\u8bad\u7ec3\u6846\u67b6\uff0c\u5229\u7528\u5c11\u91cf\u771f\u5b9e\u6f14\u793a\u548c\u5927\u91cf\u4eff\u771f\u6570\u636e\u5b66\u4e60\u53ef\u6cdb\u5316\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\uff0c\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u635f\u5931\u5b9e\u73b0\u8de8\u57df\u7279\u5f81\u5bf9\u9f50\u3002", "motivation": "\u884c\u4e3a\u514b\u9686\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u771f\u5b9e\u4e16\u754c\u6f14\u793a\u6210\u672c\u9ad8\u6602\u3002\u4eff\u771f\u6570\u636e\u867d\u53ef\u6269\u5c55\uff0c\u4f46\u5b58\u5728\u9886\u57df\u5dee\u8ddd\u5bfc\u81f4\u7b56\u7565\u8fc1\u79fb\u56f0\u96be\u3002", "method": "\u91c7\u7528\u4eff\u771f\u4e0e\u771f\u5b9e\u4e16\u754c\u534f\u540c\u8bad\u7ec3\u6846\u67b6\uff0c\u5b66\u4e60\u9886\u57df\u4e0d\u53d8\u7684\u4efb\u52a1\u76f8\u5173\u7279\u5f81\u7a7a\u95f4\u3002\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u635f\u5931\u5bf9\u9f50\u89c2\u6d4b\u548c\u52a8\u4f5c\u7684\u8054\u5408\u5206\u5e03\uff0c\u5e76\u4f7f\u7528\u975e\u5e73\u8861\u6700\u4f18\u4f20\u8f93\u5904\u7406\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u5728\u6311\u6218\u6027\u64cd\u4f5c\u4efb\u52a1\u4e0a\u9a8c\u8bc1\uff0c\u5229\u7528\u4e30\u5bcc\u4eff\u771f\u6570\u636e\u53ef\u5c06\u771f\u5b9e\u4e16\u754c\u6210\u529f\u7387\u63d0\u5347\u9ad8\u8fbe30%\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u4ec5\u5728\u4eff\u771f\u4e2d\u89c1\u8fc7\u7684\u573a\u666f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4eff\u771f\u5230\u771f\u5b9e\u4e16\u754c\u7684\u8fc1\u79fb\u95ee\u9898\uff0c\u901a\u8fc7\u8054\u5408\u5206\u5e03\u5bf9\u9f50\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u9886\u57df\u9002\u5e94\u6027\u80fd\u3002"}}
{"id": "2509.19266", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.19266", "abs": "https://arxiv.org/abs/2509.19266", "authors": ["Charis Stamouli", "Leonardo F. Toso", "Anastasios Tsiamis", "George J. Pappas", "James Anderson"], "title": "Policy Gradient Bounds in Multitask LQR", "comment": null, "summary": "We analyze the performance of policy gradient in multitask linear quadratic\nregulation (LQR), where the system and cost parameters differ across tasks. The\nmain goal of multitask LQR is to find a controller with satisfactory\nperformance on every task. Prior analyses on relevant contexts fail to capture\nclosed-loop task similarities, resulting in conservative performance\nguarantees. To account for such similarities, we propose bisimulation-based\nmeasures of task heterogeneity. Our measures employ new bisimulation functions\nto bound the cost gradient distance between a pair of tasks in closed loop with\na common stabilizing controller. Employing these measures, we derive\nsuboptimality bounds for both the multitask optimal controller and the\nasymptotic policy gradient controller with respect to each of the tasks. We\nfurther provide conditions under which the policy gradient iterates remain\nstabilizing for every system. For multiple random sets of certain tasks, we\nobserve that our bisimulation-based measures improve upon baseline measures of\ntask heterogeneity dramatically.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u591a\u4efb\u52a1\u7ebf\u6027\u4e8c\u6b21\u8c03\u8282\uff08LQR\uff09\u4e2d\u7b56\u7565\u68af\u5ea6\u7684\u6027\u80fd\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u4e92\u6a21\u62df\u7684\u4efb\u52a1\u5f02\u8d28\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u4ee5\u6355\u6349\u95ed\u73af\u4efb\u52a1\u76f8\u4f3c\u6027\uff0c\u5e76\u63a8\u5bfc\u4e86\u591a\u4efb\u52a1\u6700\u4f18\u63a7\u5236\u5668\u548c\u6e10\u8fdb\u7b56\u7565\u68af\u5ea6\u63a7\u5236\u5668\u7684\u6b21\u4f18\u6027\u754c\u3002", "motivation": "\u591a\u4efb\u52a1LQR\u7684\u4e3b\u8981\u76ee\u6807\u662f\u627e\u5230\u4e00\u4e2a\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u90fd\u5177\u6709\u6ee1\u610f\u6027\u80fd\u7684\u63a7\u5236\u5668\u3002\u5148\u524d\u5206\u6790\u672a\u80fd\u6355\u6349\u95ed\u73af\u4efb\u52a1\u76f8\u4f3c\u6027\uff0c\u5bfc\u81f4\u6027\u80fd\u4fdd\u8bc1\u8fc7\u4e8e\u4fdd\u5b88\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4e92\u6a21\u62df\u7684\u4efb\u52a1\u5f02\u8d28\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u4f7f\u7528\u65b0\u7684\u4e92\u6a21\u62df\u51fd\u6570\u6765\u7ea6\u675f\u5728\u5171\u540c\u7a33\u5b9a\u63a7\u5236\u5668\u4e0b\u95ed\u73af\u8fd0\u884c\u7684\u4e00\u5bf9\u4efb\u52a1\u7684\u6210\u672c\u68af\u5ea6\u8ddd\u79bb\u3002", "result": "\u5bf9\u4e8e\u591a\u4e2a\u968f\u673a\u4efb\u52a1\u96c6\uff0c\u89c2\u5bdf\u5230\u57fa\u4e8e\u4e92\u6a21\u62df\u7684\u5ea6\u91cf\u65b9\u6cd5\u76f8\u6bd4\u57fa\u7ebf\u4efb\u52a1\u5f02\u8d28\u6027\u5ea6\u91cf\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u4efb\u52a1\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u4e3a\u591a\u4efb\u52a1LQR\u63a7\u5236\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u6027\u80fd\u4fdd\u8bc1\u3002"}}
{"id": "2509.18436", "categories": ["cs.AI", "cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.18436", "abs": "https://arxiv.org/abs/2509.18436", "authors": ["Hongda Jiang", "Xinyuan Zhang", "Siddhant Garg", "Rishab Arora", "Shiun-Zu Kuo", "Jiayang Xu", "Christopher Brossman", "Yue Liu", "Aaron Colak", "Ahmed Aly", "Anuj Kumar", "Xin Luna Dong"], "title": "Memory-QA: Answering Recall Questions Based on Multimodal Memories", "comment": null, "summary": "We introduce Memory-QA, a novel real-world task that involves answering\nrecall questions about visual content from previously stored multimodal\nmemories. This task poses unique challenges, including the creation of\ntask-oriented memories, the effective utilization of temporal and location\ninformation within memories, and the ability to draw upon multiple memories to\nanswer a recall question. To address these challenges, we propose a\ncomprehensive pipeline, Pensieve, integrating memory-specific augmentation,\ntime- and location-aware multi-signal retrieval, and multi-memory QA\nfine-tuning. We created a multimodal benchmark to illustrate various real\nchallenges in this task, and show the superior performance of Pensieve over\nstate-of-the-art solutions (up to 14% on QA accuracy).", "AI": {"tldr": "Memory-QA\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\uff0c\u65e8\u5728\u56de\u7b54\u5173\u4e8e\u5148\u524d\u5b58\u50a8\u7684\u591a\u6a21\u6001\u8bb0\u5fc6\u7684\u89c6\u89c9\u5185\u5bb9\u56de\u5fc6\u95ee\u9898\u3002\u4f5c\u8005\u63d0\u51fa\u4e86Pensieve\u7ba1\u9053\u6765\u89e3\u51b3\u8be5\u4efb\u52a1\u7684\u6311\u6218\uff0c\u5e76\u5728\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u95ee\u7b54\u7cfb\u7edf\u4e3b\u8981\u5173\u6ce8\u5373\u65f6\u611f\u77e5\uff0c\u7f3a\u4e4f\u5bf9\u957f\u671f\u8bb0\u5fc6\u7684\u5229\u7528\u3002\u73b0\u5b9e\u4e16\u754c\u4e2d\u9700\u8981\u80fd\u591f\u4ece\u5b58\u50a8\u7684\u8bb0\u5fc6\u4e2d\u56de\u7b54\u56de\u5fc6\u95ee\u9898\u7684\u80fd\u529b\uff0c\u8fd9\u6d89\u53ca\u5230\u4efb\u52a1\u5bfc\u5411\u7684\u8bb0\u5fc6\u521b\u5efa\u3001\u65f6\u7a7a\u4fe1\u606f\u7684\u6709\u6548\u5229\u7528\u4ee5\u53ca\u591a\u8bb0\u5fc6\u63a8\u7406\u7b49\u72ec\u7279\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86Pensieve\u7efc\u5408\u7ba1\u9053\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1\uff09\u8bb0\u5fc6\u7279\u5b9a\u7684\u6570\u636e\u589e\u5f3a\uff1b2\uff09\u65f6\u95f4\u548c\u4f4d\u7f6e\u611f\u77e5\u7684\u591a\u4fe1\u53f7\u68c0\u7d22\uff1b3\uff09\u591a\u8bb0\u5fc6\u95ee\u7b54\u5fae\u8c03\u3002\u8be5\u65b9\u6cd5\u4e13\u95e8\u9488\u5bf9\u8bb0\u5fc6\u68c0\u7d22\u548c\u5229\u7528\u7684\u6311\u6218\u8fdb\u884c\u8bbe\u8ba1\u3002", "result": "\u5728\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPensieve\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u89e3\u51b3\u65b9\u6848\u5728\u95ee\u7b54\u51c6\u786e\u7387\u4e0a\u63d0\u5347\u4e86\u9ad8\u8fbe14%\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u8bb0\u5fc6\u95ee\u7b54\u4efb\u52a1\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "Memory-QA\u4efb\u52a1\u63ed\u793a\u4e86\u73b0\u5b9e\u4e16\u754c\u8bb0\u5fc6\u95ee\u7b54\u7684\u91cd\u8981\u6311\u6218\uff0cPensieve\u7ba1\u9053\u901a\u8fc7\u6574\u5408\u8bb0\u5fc6\u589e\u5f3a\u3001\u65f6\u7a7a\u611f\u77e5\u68c0\u7d22\u548c\u591a\u8bb0\u5fc6\u63a8\u7406\uff0c\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u957f\u671f\u8bb0\u5fc6\u5728\u89c6\u89c9\u95ee\u7b54\u4e2d\u7684\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2509.18636", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18636", "abs": "https://arxiv.org/abs/2509.18636", "authors": ["Yuan Zhou", "Jialiang Hou", "Guangtong Xu", "Fei Gao"], "title": "Number Adaptive Formation Flight Planning via Affine Deformable Guidance in Narrow Environments", "comment": null, "summary": "Formation maintenance with varying number of drones in narrow environments\nhinders the convergence of planning to the desired configurations. To address\nthis challenge, this paper proposes a formation planning method guided by\nDeformable Virtual Structures (DVS) with continuous spatiotemporal\ntransformation. Firstly, to satisfy swarm safety distance and preserve\nformation shape filling integrity for irregular formation geometries, we employ\nLloyd algorithm for uniform $\\underline{PA}$rtitioning and Hungarian algorithm\nfor $\\underline{AS}$signment (PAAS) in DVS. Subsequently, a spatiotemporal\ntrajectory involving DVS is planned using primitive-based path search and\nnonlinear trajectory optimization. The DVS trajectory achieves adaptive\ntransitions with respect to a varying number of drones while ensuring\nadaptability to narrow environments through affine transformation. Finally,\neach agent conducts distributed trajectory planning guided by desired\nspatiotemporal positions within the DVS, while incorporating collision\navoidance and dynamic feasibility requirements. Our method enables up to 15\\%\nof swarm numbers to join or leave in cluttered environments while rapidly\nrestoring the desired formation shape in simulation. Compared to cutting-edge\nformation planning method, we demonstrate rapid formation recovery capacity and\nenvironmental adaptability. Real-world experiments validate the effectiveness\nand resilience of our formation planning method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u53d8\u5f62\u865a\u62df\u7ed3\u6784\uff08DVS\uff09\u7684\u65e0\u4eba\u673a\u7f16\u961f\u89c4\u5212\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u72ed\u7a84\u73af\u5883\u4e2d\u5904\u7406\u65e0\u4eba\u673a\u6570\u91cf\u53d8\u5316\u7684\u7f16\u961f\u7ef4\u62a4\u95ee\u9898", "motivation": "\u72ed\u7a84\u73af\u5883\u4e2d\u65e0\u4eba\u673a\u6570\u91cf\u53d8\u5316\u4f1a\u963b\u788d\u7f16\u961f\u89c4\u5212\u6536\u655b\u5230\u671f\u671b\u914d\u7f6e\uff0c\u9700\u8981\u89e3\u51b3\u7f16\u961f\u5f62\u72b6\u5b8c\u6574\u6027\u548c\u5b89\u5168\u8ddd\u79bb\u7ef4\u62a4\u7684\u6311\u6218", "method": "\u4f7f\u7528Lloyd\u7b97\u6cd5\u8fdb\u884c\u5747\u5300\u5206\u533a\u548c\u5308\u7259\u5229\u7b97\u6cd5\u8fdb\u884c\u5206\u914d\uff08PAAS\uff09\u6765\u4fdd\u8bc1\u7f16\u961f\u5f62\u72b6\u5b8c\u6574\u6027\uff1b\u901a\u8fc7\u57fa\u4e8e\u57fa\u5143\u7684\u8def\u5f84\u641c\u7d22\u548c\u975e\u7ebf\u6027\u8f68\u8ff9\u4f18\u5316\u89c4\u5212DVS\u7684\u65f6\u7a7a\u8f68\u8ff9\uff1b\u6bcf\u4e2a\u667a\u80fd\u4f53\u5728DVS\u6307\u5bfc\u4e0b\u8fdb\u884c\u5206\u5e03\u5f0f\u8f68\u8ff9\u89c4\u5212", "result": "\u5728\u6a21\u62df\u73af\u5883\u4e2d\u652f\u6301\u591a\u8fbe15%\u7684\u65e0\u4eba\u673a\u52a0\u5165\u6216\u79bb\u5f00\u7f16\u961f\uff0c\u540c\u65f6\u5feb\u901f\u6062\u590d\u671f\u671b\u7f16\u961f\u5f62\u72b6\uff1b\u76f8\u6bd4\u524d\u6cbf\u7f16\u961f\u89c4\u5212\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u66f4\u5feb\u7684\u7f16\u961f\u6062\u590d\u80fd\u529b\u548c\u73af\u5883\u9002\u5e94\u6027", "conclusion": "\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u7f16\u961f\u89c4\u5212\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u81ea\u9002\u5e94\u8fc7\u6e21\u5e76\u786e\u4fdd\u5bf9\u72ed\u7a84\u73af\u5883\u7684\u9002\u5e94\u6027"}}
{"id": "2509.18527", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18527", "abs": "https://arxiv.org/abs/2509.18527", "authors": ["Ziwen Chen", "Zhong Wang"], "title": "FERA: Foil Fencing Referee Assistant Using Pose-Based Multi-Label Move Recognition and Rule Reasoning", "comment": null, "summary": "The sport of fencing, like many other sports, faces challenges in refereeing:\nsubjective calls, human errors, bias, and limited availability in practice\nenvironments. We present FERA (Fencing Referee Assistant), a prototype AI\nreferee for foil fencing which integrates pose-based multi-label action\nrecognition and rule-based reasoning. FERA extracts 2D joint positions from\nvideo, normalizes them, computes a 101-dimensional kinematic feature set, and\napplies a Transformer for multi-label move and blade classification. To\ndetermine priority and scoring, FERA applies a distilled language model with\nencoded right-of-way rules, producing both a decision and an explanation for\neach exchange. With limited hand-labeled data, a 5-fold cross-validation\nachieves an average macro-F1 score of 0.549, outperforming multiple baselines,\nincluding a Temporal Convolutional Network (TCN), BiLSTM, and a vanilla\nTransformer. While not ready for deployment, these results demonstrate a\npromising path towards automated referee assistance in foil fencing and new\nopportunities for AI applications, such as coaching in the field of fencing.", "AI": {"tldr": "FERA\u662f\u4e00\u4e2a\u7528\u4e8e\u51fb\u5251\u88c1\u5224\u8f85\u52a9\u7684AI\u7cfb\u7edf\u539f\u578b\uff0c\u901a\u8fc7\u59ff\u6001\u52a8\u4f5c\u8bc6\u522b\u548c\u89c4\u5219\u63a8\u7406\u6765\u89e3\u51b3\u51fb\u5251\u88c1\u5224\u4e2d\u7684\u4e3b\u89c2\u5224\u65ad\u3001\u4eba\u4e3a\u9519\u8bef\u7b49\u95ee\u9898\u3002", "motivation": "\u51fb\u5251\u8fd0\u52a8\u9762\u4e34\u88c1\u5224\u4e3b\u89c2\u5224\u65ad\u3001\u4eba\u4e3a\u9519\u8bef\u3001\u504f\u89c1\u4ee5\u53ca\u5728\u8bad\u7ec3\u73af\u5883\u4e2d\u88c1\u5224\u8d44\u6e90\u6709\u9650\u7b49\u6311\u6218\u3002", "method": "\u7cfb\u7edf\u4ece\u89c6\u9891\u4e2d\u63d0\u53d62D\u5173\u8282\u4f4d\u7f6e\uff0c\u8fdb\u884c\u5f52\u4e00\u5316\u5904\u7406\u540e\u8ba1\u7b97101\u7ef4\u8fd0\u52a8\u5b66\u7279\u5f81\uff0c\u4f7f\u7528Transformer\u8fdb\u884c\u591a\u6807\u7b7e\u52a8\u4f5c\u548c\u5251\u5c16\u5206\u7c7b\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u89c4\u5219\u7684\u63a8\u7406\u6765\u786e\u5b9a\u4f18\u5148\u6743\u548c\u5f97\u5206\u3002", "result": "\u5728\u6709\u9650\u7684\u624b\u52a8\u6807\u6ce8\u6570\u636e\u4e0b\uff0c5\u6298\u4ea4\u53c9\u9a8c\u8bc1\u7684\u5e73\u5747macro-F1\u5f97\u5206\u4e3a0.549\uff0c\u4f18\u4e8eTemporal Convolutional Network\u3001BiLSTM\u548c\u666e\u901aTransformer\u7b49\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u867d\u7136\u5c1a\u672a\u8fbe\u5230\u90e8\u7f72\u6c34\u5e73\uff0c\u4f46\u7ed3\u679c\u8868\u660e\u5728\u51fb\u5251\u81ea\u52a8\u5316\u88c1\u5224\u8f85\u52a9\u65b9\u9762\u5177\u6709\u524d\u666f\uff0c\u5e76\u4e3aAI\u5728\u51fb\u5251\u9886\u57df\u7684\u5e94\u7528\uff08\u5982\u6559\u7ec3\u8f85\u52a9\uff09\u5f00\u8f9f\u4e86\u65b0\u673a\u4f1a\u3002"}}
{"id": "2509.18644", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18644", "abs": "https://arxiv.org/abs/2509.18644", "authors": ["Juntu Zhao", "Wenbo Lu", "Di Zhang", "Yufeng Liu", "Yushen Liang", "Tianluo Zhang", "Yifeng Cao", "Junyuan Xie", "Yingdong Hu", "Shengjie Wang", "Junliang Guo", "Dequan Wang", "Yang Gao"], "title": "Do You Need Proprioceptive States in Visuomotor Policies?", "comment": "Project page: https://statefreepolicy.github.io", "summary": "Imitation-learning-based visuomotor policies have been widely used in robot\nmanipulation, where both visual observations and proprioceptive states are\ntypically adopted together for precise control. However, in this study, we find\nthat this common practice makes the policy overly reliant on the proprioceptive\nstate input, which causes overfitting to the training trajectories and results\nin poor spatial generalization. On the contrary, we propose the State-free\nPolicy, removing the proprioceptive state input and predicting actions only\nconditioned on visual observations. The State-free Policy is built in the\nrelative end-effector action space, and should ensure the full task-relevant\nvisual observations, here provided by dual wide-angle wrist cameras. Empirical\nresults demonstrate that the State-free policy achieves significantly stronger\nspatial generalization than the state-based policy: in real-world tasks such as\npick-and-place, challenging shirt-folding, and complex whole-body manipulation,\nspanning multiple robot embodiments, the average success rate improves from 0\\%\nto 85\\% in height generalization and from 6\\% to 64\\% in horizontal\ngeneralization. Furthermore, they also show advantages in data efficiency and\ncross-embodiment adaptation, enhancing their practicality for real-world\ndeployment.", "AI": {"tldr": "\u63d0\u51fa\u72b6\u6001\u65e0\u5173\u7b56\u7565\uff0c\u4ec5\u57fa\u4e8e\u89c6\u89c9\u89c2\u6d4b\u9884\u6d4b\u52a8\u4f5c\uff0c\u76f8\u6bd4\u4f20\u7edf\u7ed3\u5408\u89c6\u89c9\u548c\u672c\u4f53\u611f\u77e5\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a7a\u95f4\u6cdb\u5316\u80fd\u529b", "motivation": "\u4f20\u7edf\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u540c\u65f6\u4f7f\u7528\u89c6\u89c9\u89c2\u6d4b\u548c\u672c\u4f53\u611f\u77e5\u72b6\u6001\uff0c\u5bfc\u81f4\u7b56\u7565\u8fc7\u5ea6\u4f9d\u8d56\u672c\u4f53\u611f\u77e5\u8f93\u5165\uff0c\u4ea7\u751f\u5bf9\u8bad\u7ec3\u8f68\u8ff9\u7684\u8fc7\u62df\u5408\u548c\u8f83\u5dee\u7684\u7a7a\u95f4\u6cdb\u5316\u80fd\u529b", "method": "\u6784\u5efa\u72b6\u6001\u65e0\u5173\u7b56\u7565\uff0c\u79fb\u9664\u672c\u4f53\u611f\u77e5\u72b6\u6001\u8f93\u5165\uff0c\u4ec5\u5728\u76f8\u5bf9\u672b\u7aef\u6267\u884c\u5668\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u57fa\u4e8e\u89c6\u89c9\u89c2\u6d4b\u9884\u6d4b\u52a8\u4f5c\uff0c\u4f7f\u7528\u53cc\u5e7f\u89d2\u8155\u90e8\u6444\u50cf\u5934\u63d0\u4f9b\u5b8c\u6574\u7684\u4efb\u52a1\u76f8\u5173\u89c6\u89c9\u4fe1\u606f", "result": "\u72b6\u6001\u65e0\u5173\u7b56\u7565\u5728\u7a7a\u95f4\u6cdb\u5316\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u72b6\u6001\u7684\u65b9\u6cd5\uff1a\u5728\u9ad8\u5ea6\u6cdb\u5316\u4e2d\u6210\u529f\u7387\u4ece0%\u63d0\u5347\u81f385%\uff0c\u5728\u6c34\u5e73\u6cdb\u5316\u4e2d\u4ece6%\u63d0\u5347\u81f364%\uff0c\u540c\u65f6\u5728\u6570\u636e\u6548\u7387\u548c\u8de8\u5e73\u53f0\u9002\u5e94\u6027\u65b9\u9762\u4e5f\u8868\u73b0\u51fa\u4f18\u52bf", "conclusion": "\u72b6\u6001\u65e0\u5173\u7b56\u7565\u901a\u8fc7\u51cf\u5c11\u5bf9\u672c\u4f53\u611f\u77e5\u72b6\u6001\u7684\u4f9d\u8d56\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7a7a\u95f4\u6cdb\u5316\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u5b9e\u9645\u90e8\u7f72\u7684\u5b9e\u7528\u6027"}}
{"id": "2509.18557", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18557", "abs": "https://arxiv.org/abs/2509.18557", "authors": ["Tom Pawelek", "Raj Patel", "Charlotte Crowell", "Noorbakhsh Amiri", "Sudip Mittal", "Shahram Rahimi", "Andy Perkins"], "title": "LLMZ+: Contextual Prompt Whitelist Principles for Agentic LLMs", "comment": "7 pages, 5 figures, to be published and presented at ICMLA 2025", "summary": "Compared to traditional models, agentic AI represents a highly valuable\ntarget for potential attackers as they possess privileged access to data\nsources and API tools, which are traditionally not incorporated into classical\nagents. Unlike a typical software application residing in a Demilitarized Zone\n(DMZ), agentic LLMs consciously rely on nondeterministic behavior of the AI\n(only defining a final goal, leaving the path selection to LLM). This\ncharacteristic introduces substantial security risk to both operational\nsecurity and information security. Most common existing defense mechanism rely\non detection of malicious intent and preventing it from reaching the LLM agent,\nthus protecting against jailbreak attacks such as prompt injection. In this\npaper, we present an alternative approach, LLMZ+, which moves beyond\ntraditional detection-based approaches by implementing prompt whitelisting.\nThrough this method, only contextually appropriate and safe messages are\npermitted to interact with the agentic LLM. By leveraging the specificity of\ncontext, LLMZ+ guarantees that all exchanges between external users and the LLM\nconform to predefined use cases and operational boundaries. Our approach\nstreamlines the security framework, enhances its long-term resilience, and\nreduces the resources required for sustaining LLM information security. Our\nempirical evaluation demonstrates that LLMZ+ provides strong resilience against\nthe most common jailbreak prompts. At the same time, legitimate business\ncommunications are not disrupted, and authorized traffic flows seamlessly\nbetween users and the agentic LLM. We measure the effectiveness of approach\nusing false positive and false negative rates, both of which can be reduced to\n0 in our experimental setting.", "AI": {"tldr": "LLMZ+\u662f\u4e00\u79cd\u57fa\u4e8e\u63d0\u793a\u767d\u540d\u5355\u7684\u65b0\u578b\u9632\u5fa1\u673a\u5236\uff0c\u901a\u8fc7\u4ec5\u5141\u8bb8\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u5b89\u5168\u6d88\u606f\u4e0e\u4ee3\u7406\u5f0fLLM\u4ea4\u4e92\uff0c\u63d0\u4f9b\u5bf9\u8d8a\u72f1\u653b\u51fb\u7684\u5f3a\u5927\u9632\u62a4\u3002", "motivation": "\u4ee3\u7406\u5f0fAI\u76f8\u6bd4\u4f20\u7edf\u6a21\u578b\u5177\u6709\u66f4\u9ad8\u7684\u5b89\u5168\u98ce\u9669\uff0c\u56e0\u4e3a\u5b83\u4eec\u62e5\u6709\u5bf9\u6570\u636e\u6e90\u548cAPI\u5de5\u5177\u7684\u7279\u6743\u8bbf\u95ee\uff0c\u4e14\u4f9d\u8d56AI\u7684\u975e\u786e\u5b9a\u6027\u884c\u4e3a\uff0c\u8fd9\u7ed9\u64cd\u4f5c\u5b89\u5168\u548c\u4fe1\u606f\u5b89\u5168\u5e26\u6765\u91cd\u5927\u5a01\u80c1\u3002", "method": "\u91c7\u7528\u63d0\u793a\u767d\u540d\u5355\u65b9\u6cd5\uff0c\u4ec5\u5141\u8bb8\u7b26\u5408\u9884\u5b9a\u4e49\u7528\u4f8b\u548c\u64cd\u4f5c\u8fb9\u754c\u7684\u4e0a\u4e0b\u6587\u9002\u5f53\u6d88\u606f\u4e0e\u4ee3\u7406\u5f0fLLM\u4ea4\u4e92\uff0c\u8d85\u8d8a\u4f20\u7edf\u7684\u57fa\u4e8e\u68c0\u6d4b\u7684\u9632\u5fa1\u65b9\u6cd5\u3002", "result": "\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793aLLMZ+\u5bf9\u6700\u5e38\u89c1\u7684\u8d8a\u72f1\u63d0\u793a\u5177\u6709\u5f3a\u5927\u62b5\u6297\u529b\uff0c\u540c\u65f6\u4e0d\u5e72\u6270\u5408\u6cd5\u7684\u4e1a\u52a1\u901a\u4fe1\uff0c\u5728\u5b9e\u9a8c\u73af\u5883\u4e2d\u5047\u9633\u6027\u548c\u5047\u9634\u6027\u7387\u5747\u53ef\u964d\u81f30\u3002", "conclusion": "LLMZ+\u65b9\u6cd5\u7b80\u5316\u4e86\u5b89\u5168\u6846\u67b6\uff0c\u589e\u5f3a\u4e86\u957f\u671f\u97e7\u6027\uff0c\u51cf\u5c11\u4e86\u7ef4\u6301LLM\u4fe1\u606f\u5b89\u5168\u6240\u9700\u7684\u8d44\u6e90\uff0c\u4e3a\u4ee3\u7406\u5f0fAI\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5b89\u5168\u9632\u62a4\u65b9\u6848\u3002"}}
{"id": "2509.18648", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18648", "abs": "https://arxiv.org/abs/2509.18648", "authors": ["Yarden As", "Chengrui Qu", "Benjamin Unger", "Dongho Kang", "Max van der Hart", "Laixi Shi", "Stelian Coros", "Adam Wierman", "Andreas Krause"], "title": "SPiDR: A Simple Approach for Zero-Shot Safety in Sim-to-Real Transfer", "comment": null, "summary": "Safety remains a major concern for deploying reinforcement learning (RL) in\nreal-world applications. Simulators provide safe, scalable training\nenvironments, but the inevitable sim-to-real gap introduces additional safety\nconcerns, as policies must satisfy constraints in real-world conditions that\ndiffer from simulation. To address this challenge, robust safe RL techniques\noffer principled methods, but are often incompatible with standard scalable\ntraining pipelines. In contrast, domain randomization, a simple and popular\nsim-to-real technique, stands out as a promising alternative, although it often\nresults in unsafe behaviors in practice. We present SPiDR, short for\nSim-to-real via Pessimistic Domain Randomization -- a scalable algorithm with\nprovable guarantees for safe sim-to-real transfer. SPiDR uses domain\nrandomization to incorporate the uncertainty about the sim-to-real gap into the\nsafety constraints, making it versatile and highly compatible with existing\ntraining pipelines. Through extensive experiments on sim-to-sim benchmarks and\ntwo distinct real-world robotic platforms, we demonstrate that SPiDR\neffectively ensures safety despite the sim-to-real gap while maintaining strong\nperformance.", "AI": {"tldr": "SPiDR\u662f\u4e00\u79cd\u901a\u8fc7\u60b2\u89c2\u57df\u968f\u673a\u5316\u5b9e\u73b0\u5b89\u5168sim-to-real\u8fc1\u79fb\u7684\u53ef\u6269\u5c55\u7b97\u6cd5\uff0c\u80fd\u591f\u5728\u5b58\u5728sim-to-real\u5dee\u8ddd\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u4fdd\u8bc1\u5b89\u5168\u6027", "motivation": "\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u5728\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u7684\u5b89\u5168\u95ee\u9898\uff0c\u7279\u522b\u662fsim-to-real\u5dee\u8ddd\u5e26\u6765\u7684\u5b89\u5168\u7ea6\u675f\u6311\u6218", "method": "\u4f7f\u7528\u57df\u968f\u673a\u5316\u5c06sim-to-real\u5dee\u8ddd\u7684\u4e0d\u786e\u5b9a\u6027\u7eb3\u5165\u5b89\u5168\u7ea6\u675f\uff0c\u4e0e\u73b0\u6709\u8bad\u7ec3\u7ba1\u9053\u9ad8\u5ea6\u517c\u5bb9", "result": "\u5728sim-to-sim\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e24\u4e2a\u4e0d\u540c\u7684\u771f\u5b9e\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\uff0cSPiDR\u5728\u4fdd\u6301\u5f3a\u6027\u80fd\u7684\u540c\u65f6\u6709\u6548\u786e\u4fdd\u5b89\u5168", "conclusion": "SPiDR\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5177\u6709\u53ef\u8bc1\u660e\u4fdd\u8bc1\u7684\u5b89\u5168sim-to-real\u8fc1\u79fb\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027"}}
{"id": "2509.18666", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.18666", "abs": "https://arxiv.org/abs/2509.18666", "authors": ["Kaizer Rahaman", "Simran Kumari", "Ashish R. Hota"], "title": "Distributionally Robust Safe Motion Planning with Contextual Information", "comment": null, "summary": "We present a distributionally robust approach for collision avoidance by\nincorporating contextual information. Specifically, we embed the conditional\ndistribution of future trajectory of the obstacle conditioned on the motion of\nthe ego agent in a reproducing kernel Hilbert space (RKHS) via the conditional\nkernel mean embedding operator. Then, we define an ambiguity set containing all\ndistributions whose embedding in the RKHS is within a certain distance from the\nempirical estimate of conditional mean embedding learnt from past data.\nConsequently, a distributionally robust collision avoidance constraint is\nformulated, and included in the receding horizon based motion planning\nformulation of the ego agent. Simulation results show that the proposed\napproach is more successful in avoiding collision compared to approaches that\ndo not include contextual information and/or distributional robustness in their\nformulation in several challenging scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u7684\u78b0\u649e\u907f\u514d\u65b9\u6cd5\uff0c\u901a\u8fc7\u6761\u4ef6\u6838\u5747\u503c\u5d4c\u5165\u5728RKHS\u4e2d\u5efa\u6a21\u969c\u788d\u7269\u672a\u6765\u8f68\u8ff9\u7684\u6761\u4ef6\u5206\u5e03\uff0c\u6784\u5efa\u5305\u542b\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u5206\u5e03\u4e0d\u786e\u5b9a\u6027\u96c6\u5408\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\u4e2d\u3002", "motivation": "\u4f20\u7edf\u78b0\u649e\u907f\u514d\u65b9\u6cd5\u5f80\u5f80\u5ffd\u7565\u4e0a\u4e0b\u6587\u4fe1\u606f\u6216\u5bf9\u5206\u5e03\u4e0d\u786e\u5b9a\u6027\u8003\u8651\u4e0d\u8db3\uff0c\u5bfc\u81f4\u5728\u590d\u6742\u573a\u666f\u4e2d\u907f\u969c\u6548\u679c\u4e0d\u4f73\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7ed3\u5408\u4e0a\u4e0b\u6587\u4fe1\u606f\u5e76\u5904\u7406\u5206\u5e03\u4e0d\u786e\u5b9a\u6027\u7684\u9c81\u68d2\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u6838\u5747\u503c\u5d4c\u5165\u5c06\u969c\u788d\u7269\u8f68\u8ff9\u7684\u6761\u4ef6\u5206\u5e03\u6620\u5c04\u5230RKHS\u7a7a\u95f4\uff0c\u6784\u5efa\u57fa\u4e8e\u7ecf\u9a8c\u4f30\u8ba1\u7684\u5206\u5e03\u6a21\u7cca\u96c6\uff0c\u5728\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\u4e2d\u5f15\u5165\u5206\u5e03\u9c81\u68d2\u78b0\u649e\u907f\u514d\u7ea6\u675f\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6311\u6218\u6027\u573a\u666f\u4e2d\u6bd4\u4e0d\u8003\u8651\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u5206\u5e03\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u78b0\u649e\u907f\u514d\u6548\u679c\u3002", "conclusion": "\u63d0\u51fa\u7684\u5206\u5e03\u9c81\u68d2\u65b9\u6cd5\u901a\u8fc7\u6709\u6548\u5229\u7528\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u5904\u7406\u5206\u5e03\u4e0d\u786e\u5b9a\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u78b0\u649e\u907f\u514d\u6027\u80fd\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7b49\u9886\u57df\u7684\u8fd0\u52a8\u89c4\u5212\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18565", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18565", "abs": "https://arxiv.org/abs/2509.18565", "authors": ["Mitchell Piehl", "Dillon Wilson", "Ananya Kalita", "Jugal Kalita"], "title": "Solving Math Word Problems Using Estimation Verification and Equation Generation", "comment": "Accepted to IEEE ICMLA 2025", "summary": "Large Language Models (LLMs) excel at various tasks, including\nproblem-solving and question-answering. However, LLMs often find Math Word\nProblems (MWPs) challenging because solving them requires a range of reasoning\nand mathematical abilities with which LLMs seem to struggle. Recent efforts\nhave helped LLMs solve more complex MWPs with improved prompts. This study\nproposes a novel method that initially prompts an LLM to create equations from\na decomposition of the question, followed by using an external symbolic\nequation solver to produce an answer. To ensure the accuracy of the obtained\nanswer, inspired by an established recommendation of math teachers, the LLM is\ninstructed to solve the MWP a second time, but this time with the objective of\nestimating the correct answer instead of solving it exactly. The estimation is\nthen compared to the generated answer to verify. If verification fails, an\niterative rectification process is employed to ensure the correct answer is\neventually found. This approach achieves new state-of-the-art results on\ndatasets used by prior published research on numeric and algebraic MWPs,\nimproving the previous best results by nearly two percent on average. In\naddition, the approach obtains satisfactory results on trigonometric MWPs, a\ntask not previously attempted to the authors' best knowledge. This study also\nintroduces two new datasets, SVAMPClean and Trig300, to further advance the\ntesting of LLMs' reasoning abilities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LLM\u548c\u5916\u90e8\u7b26\u53f7\u65b9\u7a0b\u6c42\u89e3\u5668\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u95ee\u9898\u5206\u89e3\u3001\u65b9\u7a0b\u751f\u6210\u3001\u7b54\u6848\u9a8c\u8bc1\u548c\u8fed\u4ee3\u4fee\u6b63\u6765\u89e3\u51b3\u6570\u5b66\u5e94\u7528\u9898\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u65b0\u7684\u6700\u4f18\u7ed3\u679c\u3002", "motivation": "LLM\u5728\u89e3\u51b3\u6570\u5b66\u5e94\u7528\u9898\u65f6\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u9700\u8981\u590d\u6742\u7684\u63a8\u7406\u548c\u6570\u5b66\u80fd\u529b\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742MWPs\u4e0a\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "method": "\u9996\u5148\u63d0\u793aLLM\u4ece\u95ee\u9898\u5206\u89e3\u4e2d\u521b\u5efa\u65b9\u7a0b\uff0c\u7136\u540e\u4f7f\u7528\u5916\u90e8\u7b26\u53f7\u65b9\u7a0b\u6c42\u89e3\u5668\u751f\u6210\u7b54\u6848\u3002\u901a\u8fc7\u8ba9LLM\u7b2c\u4e8c\u6b21\u4f30\u8ba1\u7b54\u6848\u6765\u9a8c\u8bc1\u51c6\u786e\u6027\uff0c\u5982\u679c\u9a8c\u8bc1\u5931\u8d25\u5219\u8fdb\u884c\u8fed\u4ee3\u4fee\u6b63\u3002", "result": "\u5728\u6570\u503c\u548c\u4ee3\u6570MWPs\u6570\u636e\u96c6\u4e0a\u6bd4\u4e4b\u524d\u6700\u4f18\u7ed3\u679c\u5e73\u5747\u63d0\u5347\u8fd12%\uff0c\u5728\u4e09\u89d2\u51fd\u6570MWPs\u4e0a\u4e5f\u53d6\u5f97\u4e86\u6ee1\u610f\u7ed3\u679c\uff0c\u5e76\u5f15\u5165\u4e86\u4e24\u4e2a\u65b0\u6570\u636e\u96c6SVAMPClean\u548cTrig300\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86LLM\u89e3\u51b3\u6570\u5b66\u5e94\u7528\u9898\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e3aLLM\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u65b0\u57fa\u51c6\u3002"}}
{"id": "2509.18676", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.18676", "abs": "https://arxiv.org/abs/2509.18676", "authors": ["Sangjun Noh", "Dongwoo Nam", "Kangmin Kim", "Geonhyup Lee", "Yeonguk Yu", "Raeyoung Kang", "Kyoobin Lee"], "title": "3D Flow Diffusion Policy: Visuomotor Policy Learning via Generating Flow in 3D Space", "comment": "7 main scripts + 2 reference pages", "summary": "Learning robust visuomotor policies that generalize across diverse objects\nand interaction dynamics remains a central challenge in robotic manipulation.\nMost existing approaches rely on direct observation-to-action mappings or\ncompress perceptual inputs into global or object-centric features, which often\noverlook localized motion cues critical for precise and contact-rich\nmanipulation. We present 3D Flow Diffusion Policy (3D FDP), a novel framework\nthat leverages scene-level 3D flow as a structured intermediate representation\nto capture fine-grained local motion cues. Our approach predicts the temporal\ntrajectories of sampled query points and conditions action generation on these\ninteraction-aware flows, implemented jointly within a unified diffusion\narchitecture. This design grounds manipulation in localized dynamics while\nenabling the policy to reason about broader scene-level consequences of\nactions. Extensive experiments on the MetaWorld benchmark show that 3D FDP\nachieves state-of-the-art performance across 50 tasks, particularly excelling\non medium and hard settings. Beyond simulation, we validate our method on eight\nreal-robot tasks, where it consistently outperforms prior baselines in\ncontact-rich and non-prehensile scenarios. These results highlight 3D flow as a\npowerful structural prior for learning generalizable visuomotor policies,\nsupporting the development of more robust and versatile robotic manipulation.\nRobot demonstrations, additional results, and code can be found at\nhttps://sites.google.com/view/3dfdp/home.", "AI": {"tldr": "3D Flow Diffusion Policy (3D FDP) \u662f\u4e00\u4e2a\u5229\u7528\u573a\u666f\u7ea73D\u6d41\u4f5c\u4e3a\u7ed3\u6784\u5316\u4e2d\u95f4\u8868\u793a\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u6355\u6349\u7ec6\u7c92\u5ea6\u5c40\u90e8\u8fd0\u52a8\u7ebf\u7d22\u6765\u5b66\u4e60\u9c81\u68d2\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\uff0c\u5728\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u76f4\u63a5\u89c2\u5bdf\u5230\u52a8\u4f5c\u7684\u6620\u5c04\u6216\u5c06\u611f\u77e5\u8f93\u5165\u538b\u7f29\u4e3a\u5168\u5c40\u6216\u7269\u4f53\u4e2d\u5fc3\u7279\u5f81\uff0c\u5f80\u5f80\u5ffd\u7565\u4e86\u7cbe\u786e\u548c\u63a5\u89e6\u4e30\u5bcc\u64cd\u4f5c\u6240\u9700\u7684\u5c40\u90e8\u8fd0\u52a8\u7ebf\u7d22\u3002", "method": "\u63d0\u51fa3D FDP\u6846\u67b6\uff0c\u9884\u6d4b\u91c7\u6837\u67e5\u8be2\u70b9\u7684\u65f6\u95f4\u8f68\u8ff9\uff0c\u5e76\u5728\u7edf\u4e00\u7684\u6269\u6563\u67b6\u6784\u4e2d\u57fa\u4e8e\u8fd9\u4e9b\u4ea4\u4e92\u611f\u77e5\u6d41\u6765\u751f\u6210\u52a8\u4f5c\uff0c\u4f7f\u64cd\u4f5c\u57fa\u4e8e\u5c40\u90e8\u52a8\u529b\u5b66\u540c\u65f6\u8003\u8651\u52a8\u4f5c\u7684\u5e7f\u6cdb\u573a\u666f\u7ea7\u540e\u679c\u3002", "result": "\u5728MetaWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c3D FDP\u572850\u4e2a\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u4e2d\u7b49\u548c\u56f0\u96be\u8bbe\u7f6e\u4e2d\u8868\u73b0\u51fa\u8272\u3002\u57288\u4e2a\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\uff0c\u5728\u63a5\u89e6\u4e30\u5bcc\u548c\u975e\u6293\u53d6\u573a\u666f\u4e2d\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "3D\u6d41\u4f5c\u4e3a\u5b66\u4e60\u53ef\u6cdb\u5316\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u7684\u5f3a\u5927\u7ed3\u6784\u5316\u5148\u9a8c\uff0c\u652f\u6301\u5f00\u53d1\u66f4\u9c81\u68d2\u548c\u901a\u7528\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u80fd\u529b\u3002"}}
{"id": "2509.18633", "categories": ["cs.AI", "q-fin.RM"], "pdf": "https://arxiv.org/pdf/2509.18633", "abs": "https://arxiv.org/abs/2509.18633", "authors": ["Yara Mohajerani"], "title": "Adaptive Learning in Spatial Agent-Based Models for Climate Risk Assessment: A Geospatial Framework with Evolutionary Economic Agents", "comment": "Submitted and accepted to Tackling Climate Change with Machine\n  Learning workshop at NeurIPS 2025. 5 pages, 1 figure. Source code and\n  documentation available at\n  https://github.com/yaramohajerani/spatial-climate-ABM", "summary": "Climate risk assessment requires modelling complex interactions between\nspatially heterogeneous hazards and adaptive economic systems. We present a\nnovel geospatial agent-based model that integrates climate hazard data with\nevolutionary learning for economic agents. Our framework combines Mesa-based\nspatial modelling with CLIMADA climate impact assessment, introducing adaptive\nlearning behaviours that allow firms to evolve strategies for budget\nallocation, pricing, wages, and risk adaptation through fitness-based selection\nand mutation. We demonstrate the framework using riverine flood projections\nunder RCP8.5 until 2100, showing that evolutionary adaptation enables firms to\nconverge with baseline (no hazard) production levels after decades of\ndisruption due to climate stress. Our results reveal systemic risks where even\nagents that are not directly exposed to floods face impacts through supply\nchain disruptions, with the end-of-century average price of goods 5.6% higher\nunder RCP8.5 compared to the baseline. This open-source framework provides\nfinancial institutions and companies with tools to quantify both direct and\ncascading climate risks while evaluating cost-effective adaptation strategies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u5730\u7406\u7a7a\u95f4\u4ee3\u7406\u6a21\u578b\u548c\u8fdb\u5316\u5b66\u4e60\u7684\u6c14\u5019\u98ce\u9669\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u62df\u6c14\u5019\u707e\u5bb3\u4e0e\u7ecf\u6d4e\u7cfb\u7edf\u7684\u590d\u6742\u76f8\u4e92\u4f5c\u7528\u3002", "motivation": "\u6c14\u5019\u98ce\u9669\u8bc4\u4f30\u9700\u8981\u5efa\u6a21\u7a7a\u95f4\u5f02\u8d28\u6027\u707e\u5bb3\u4e0e\u9002\u5e94\u6027\u7ecf\u6d4e\u7cfb\u7edf\u4e4b\u95f4\u7684\u590d\u6742\u76f8\u4e92\u4f5c\u7528\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u8fd9\u4e9b\u52a8\u6001\u8fc7\u7a0b\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8eMesa\u7684\u5730\u7406\u7a7a\u95f4\u4ee3\u7406\u6a21\u578b\uff0c\u96c6\u6210CLIMADA\u6c14\u5019\u5f71\u54cd\u8bc4\u4f30\uff0c\u5f15\u5165\u8fdb\u5316\u5b66\u4e60\u673a\u5236\u8ba9\u4f01\u4e1a\u901a\u8fc7\u9002\u5e94\u6027\u9009\u62e9\u6f14\u5316\u9884\u7b97\u5206\u914d\u3001\u5b9a\u4ef7\u3001\u5de5\u8d44\u548c\u98ce\u9669\u9002\u5e94\u7b56\u7565\u3002", "result": "\u4f7f\u7528RCP8.5\u60c5\u666f\u4e0b\u7684\u6cb3\u6d41\u6d2a\u6c34\u9884\u6d4b\u52302100\u5e74\uff0c\u663e\u793a\u8fdb\u5316\u9002\u5e94\u4f7f\u4f01\u4e1a\u80fd\u5728\u51e0\u5341\u5e74\u6c14\u5019\u538b\u529b\u540e\u6062\u590d\u5230\u57fa\u51c6\u751f\u4ea7\u6c34\u5e73\uff1b\u672a\u76f4\u63a5\u66b4\u9732\u4e8e\u6d2a\u6c34\u7684\u4ee3\u7406\u4e5f\u9762\u4e34\u4f9b\u5e94\u94fe\u4e2d\u65ad\u5f71\u54cd\uff0c\u4e16\u7eaa\u672b\u5546\u54c1\u5e73\u5747\u4ef7\u683c\u6bd4\u57fa\u51c6\u9ad85.6%\u3002", "conclusion": "\u8be5\u5f00\u6e90\u6846\u67b6\u4e3a\u91d1\u878d\u673a\u6784\u548c\u516c\u53f8\u63d0\u4f9b\u4e86\u91cf\u5316\u76f4\u63a5\u548c\u7ea7\u8054\u6c14\u5019\u98ce\u9669\u7684\u5de5\u5177\uff0c\u540c\u65f6\u8bc4\u4f30\u6210\u672c\u6548\u76ca\u9002\u5e94\u7b56\u7565\u3002"}}
{"id": "2509.18671", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18671", "abs": "https://arxiv.org/abs/2509.18671", "authors": ["Kaixin Chai", "Hyunjun Lee", "Joseph J. Lim"], "title": "N2M: Bridging Navigation and Manipulation by Learning Pose Preference from Rollout", "comment": null, "summary": "In mobile manipulation, the manipulation policy has strong preferences for\ninitial poses where it is executed. However, the navigation module focuses\nsolely on reaching the task area, without considering which initial pose is\npreferable for downstream manipulation. To address this misalignment, we\nintroduce N2M, a transition module that guides the robot to a preferable\ninitial pose after reaching the task area, thereby substantially improving task\nsuccess rates. N2M features five key advantages: (1) reliance solely on\nego-centric observation without requiring global or historical information; (2)\nreal-time adaptation to environmental changes; (3) reliable prediction with\nhigh viewpoint robustness; (4) broad applicability across diverse tasks,\nmanipulation policies, and robot hardware; and (5) remarkable data efficiency\nand generalizability. We demonstrate the effectiveness of N2M through extensive\nsimulation and real-world experiments. In the PnPCounterToCab task, N2M\nimproves the averaged success rate from 3% with the reachability-based baseline\nto 54%. Furthermore, in the Toybox Handover task, N2M provides reliable\npredictions even in unseen environments with only 15 data samples, showing\nremarkable data efficiency and generalizability.", "AI": {"tldr": "N2M\u662f\u4e00\u4e2a\u8fc7\u6e21\u6a21\u5757\uff0c\u5728\u673a\u5668\u4eba\u5230\u8fbe\u4efb\u52a1\u533a\u57df\u540e\u5f15\u5bfc\u5176\u5230\u66f4\u4f18\u7684\u521d\u59cb\u4f4d\u59ff\uff0c\u663e\u8457\u63d0\u9ad8\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u7684\u6210\u529f\u7387\u3002", "motivation": "\u79fb\u52a8\u64cd\u4f5c\u4e2d\uff0c\u5bfc\u822a\u6a21\u5757\u53ea\u5173\u6ce8\u5230\u8fbe\u4efb\u52a1\u533a\u57df\uff0c\u800c\u4e0d\u8003\u8651\u4e0b\u6e38\u64cd\u4f5c\u7b56\u7565\u504f\u597d\u7684\u521d\u59cb\u4f4d\u59ff\uff0c\u5bfc\u81f4\u4efb\u52a1\u6210\u529f\u7387\u4f4e\u3002", "method": "N2M\u6a21\u5757\u4ec5\u4f9d\u8d56\u81ea\u6211\u4e2d\u5fc3\u89c2\u6d4b\uff0c\u65e0\u9700\u5168\u5c40\u6216\u5386\u53f2\u4fe1\u606f\uff0c\u5177\u6709\u5b9e\u65f6\u73af\u5883\u9002\u5e94\u80fd\u529b\u548c\u9ad8\u89c6\u89d2\u9c81\u68d2\u6027\u3002", "result": "\u5728PnPCounterToCab\u4efb\u52a1\u4e2d\uff0c\u6210\u529f\u7387\u4ece\u57fa\u7ebf3%\u63d0\u5347\u523054%\uff1b\u5728Toybox Handover\u4efb\u52a1\u4e2d\uff0c\u4ec5\u752815\u4e2a\u6570\u636e\u6837\u672c\u5c31\u80fd\u5728\u672a\u89c1\u73af\u5883\u4e2d\u63d0\u4f9b\u53ef\u9760\u9884\u6d4b\u3002", "conclusion": "N2M\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3001\u6570\u636e\u6548\u7387\u9ad8\u548c\u6cdb\u5316\u80fd\u529b\u5f3a\u7b49\u4e94\u5927\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79fb\u52a8\u64cd\u4f5c\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2509.19246", "categories": ["cs.RO", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.19246", "abs": "https://arxiv.org/abs/2509.19246", "authors": ["Sinan O\u011fuz", "Emanuele Garone", "Marco Dorigo", "Mary Katherine Heinrich"], "title": "Proactive-reactive detection and mitigation of intermittent faults in robot swarms", "comment": null, "summary": "Intermittent faults are transient errors that sporadically appear and\ndisappear. Although intermittent faults pose substantial challenges to\nreliability and coordination, existing studies of fault tolerance in robot\nswarms focus instead on permanent faults. One reason for this is that\nintermittent faults are prohibitively difficult to detect in the fully\nself-organized ad-hoc networks typical of robot swarms, as their network\ntopologies are transient and often unpredictable. However, in the recently\nintroduced self-organizing nervous systems (SoNS) approach, robot swarms are\nable to self-organize persistent network structures for the first time, easing\nthe problem of detecting intermittent faults. To address intermittent faults in\nrobot swarms that have persistent networks, we propose a novel\nproactive-reactive strategy to detection and mitigation, based on\nself-organized backup layers and distributed consensus in a multiplex network.\nProactively, the robots self-organize dynamic backup paths before faults occur,\nadapting to changes in the primary network topology and the robots' relative\npositions. Reactively, robots use one-shot likelihood ratio tests to compare\ninformation received along different paths in the multiplex network, enabling\nearly fault detection. Upon detection, communication is temporarily rerouted in\na self-organized way, until the detected fault resolves. We validate the\napproach in representative scenarios of faulty positional data occurring during\nformation control, demonstrating that intermittent faults are prevented from\ndisrupting convergence to desired formations, with high fault detection\naccuracy and low rates of false positives.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u673a\u5668\u4eba\u7fa4\u4f53\u4e2d\u95f4\u6b47\u6027\u6545\u969c\u7684\u4e3b\u52a8-\u88ab\u52a8\u68c0\u6d4b\u4e0e\u7f13\u89e3\u7b56\u7565\uff0c\u5229\u7528\u81ea\u7ec4\u7ec7\u5907\u4efd\u5c42\u548c\u591a\u8def\u7f51\u7edc\u4e2d\u7684\u5206\u5e03\u5f0f\u5171\u8bc6\u6765\u89e3\u51b3\u95f4\u6b47\u6027\u6545\u969c\u68c0\u6d4b\u96be\u9898\u3002", "motivation": "\u95f4\u6b47\u6027\u6545\u969c\u5728\u673a\u5668\u4eba\u7fa4\u4f53\u4e2d\u9891\u7e41\u51fa\u73b0\u548c\u6d88\u5931\uff0c\u5bf9\u53ef\u9760\u6027\u548c\u534f\u8c03\u6027\u6784\u6210\u91cd\u5927\u6311\u6218\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6c38\u4e45\u6027\u6545\u969c\u3002\u7531\u4e8e\u673a\u5668\u4eba\u7fa4\u4f53\u7f51\u7edc\u62d3\u6251\u7684\u77ac\u6001\u6027\u548c\u4e0d\u53ef\u9884\u6d4b\u6027\uff0c\u95f4\u6b47\u6027\u6545\u969c\u68c0\u6d4b\u6781\u4e3a\u56f0\u96be\u3002", "method": "\u91c7\u7528\u4e3b\u52a8-\u88ab\u52a8\u7b56\u7565\uff1a\u4e3b\u52a8\u9636\u6bb5\uff0c\u673a\u5668\u4eba\u5728\u6545\u969c\u53d1\u751f\u524d\u81ea\u7ec4\u7ec7\u52a8\u6001\u5907\u4efd\u8def\u5f84\uff1b\u88ab\u52a8\u9636\u6bb5\uff0c\u4f7f\u7528\u5355\u6b21\u4f3c\u7136\u6bd4\u68c0\u9a8c\u6bd4\u8f83\u591a\u8def\u7f51\u7edc\u4e2d\u4e0d\u540c\u8def\u5f84\u7684\u4fe1\u606f\uff0c\u5b9e\u73b0\u65e9\u671f\u6545\u969c\u68c0\u6d4b\u3002\u68c0\u6d4b\u5230\u6545\u969c\u540e\uff0c\u901a\u4fe1\u4ee5\u81ea\u7ec4\u7ec7\u65b9\u5f0f\u4e34\u65f6\u91cd\u8def\u7531\u3002", "result": "\u5728\u5f62\u6210\u63a7\u5236\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u6545\u969c\u4f4d\u7f6e\u6570\u636e\u7684\u4ee3\u8868\u6027\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u8bc1\u660e\u95f4\u6b47\u6027\u6545\u969c\u4e0d\u4f1a\u7834\u574f\u671f\u671b\u5f62\u6210\u7684\u6536\u655b\uff0c\u5177\u6709\u9ad8\u6545\u969c\u68c0\u6d4b\u51c6\u786e\u7387\u548c\u4f4e\u8bef\u62a5\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9996\u6b21\u5728\u5177\u6709\u6301\u4e45\u7f51\u7edc\u7684\u673a\u5668\u4eba\u7fa4\u4f53\u4e2d\u6709\u6548\u89e3\u51b3\u4e86\u95f4\u6b47\u6027\u6545\u969c\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u7ec4\u7ec7\u5907\u4efd\u5c42\u548c\u5206\u5e03\u5f0f\u5171\u8bc6\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u6545\u969c\u68c0\u6d4b\u548c\u7f13\u89e3\u3002"}}
{"id": "2509.18667", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18667", "abs": "https://arxiv.org/abs/2509.18667", "authors": ["Qiao Xiao", "Hong Ting Tsang", "Jiaxin Bai"], "title": "TERAG: Token-Efficient Graph-Based Retrieval-Augmented Generation", "comment": "16 pages, 2 figures, 4 tables. Submitted to the 2026 18th\n  International Conference on Machine Learning and Computing (ICMLC 2026),\n  under review", "summary": "Graph-based Retrieval-augmented generation (RAG) has become a widely studied\napproach for improving the reasoning, accuracy, and factuality of Large\nLanguage Models. However, many existing graph-based RAG systems overlook the\nhigh cost associated with LLM token usage during graph construction, hindering\nlarge-scale adoption. To address this, we propose TERAG, a simple yet effective\nframework designed to build informative graphs at a significantly lower cost.\nInspired by HippoRAG, we incorporate Personalized PageRank (PPR) during the\nretrieval phase, and we achieve at least 80% of the accuracy of widely used\ngraph-based RAG methods while consuming only 3%-11% of the output tokens.", "AI": {"tldr": "TERAG\u662f\u4e00\u4e2a\u4f4e\u6210\u672c\u56fe\u57fa\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u4e2a\u6027\u5316PageRank\u5728\u68c0\u7d22\u9636\u6bb5\u5b9e\u73b0\u9ad8\u6548\u56fe\u6784\u5efa\uff0c\u4ec5\u75283%-11%\u7684\u8f93\u51fatoken\u5c31\u80fd\u8fbe\u5230\u73b0\u6709\u65b9\u6cd580%\u4ee5\u4e0a\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u56fe\u57faRAG\u7cfb\u7edf\u5728\u6784\u5efa\u56fe\u65f6LLM token\u4f7f\u7528\u6210\u672c\u8fc7\u9ad8\uff0c\u963b\u788d\u4e86\u5927\u89c4\u6a21\u5e94\u7528\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u7ecf\u6d4e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u53d7HippoRAG\u542f\u53d1\uff0c\u5728\u68c0\u7d22\u9636\u6bb5\u5f15\u5165\u4e2a\u6027\u5316PageRank(PPR)\u6765\u6784\u5efa\u4fe1\u606f\u4e30\u5bcc\u7684\u56fe\u7ed3\u6784\uff0c\u663e\u8457\u964d\u4f4etoken\u6d88\u8017\u3002", "result": "TERAG\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u5c06\u8f93\u51fatoken\u4f7f\u7528\u91cf\u5927\u5e45\u51cf\u5c11\u5230\u73b0\u6709\u65b9\u6cd5\u76843%-11%\uff0c\u8fbe\u5230\u81f3\u5c1180%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "TERAG\u8bc1\u660e\u4e86\u901a\u8fc7\u4f18\u5316\u68c0\u7d22\u7b56\u7565\u53ef\u4ee5\u663e\u8457\u964d\u4f4e\u56fe\u57faRAG\u7684\u6210\u672c\uff0c\u4e3a\u5b9e\u73b0\u5927\u89c4\u6a21\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2509.18681", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18681", "abs": "https://arxiv.org/abs/2509.18681", "authors": ["Nicolas Valot", "Louis Fabre", "Benjamin Lesage", "Ammar Mechouche", "Claire Pagetti"], "title": "Implementation of airborne ML models with semantics preservation", "comment": null, "summary": "Machine Learning (ML) may offer new capabilities in airborne systems.\nHowever, as any piece of airborne systems, ML-based systems will be required to\nguarantee their safe operation. Thus, their development will have to be\ndemonstrated to be compliant with the adequate guidance. So far, the European\nUnion Aviation Safety Agency (EASA) has published a concept paper and an\nEUROCAE/SAE group is preparing ED-324. Both approaches delineate high-level\nobjectives to confirm the ML model achieves its intended function and maintains\ntraining performance in the target environment. The paper aims to clarify the\ndifference between an ML model and its corresponding unambiguous description,\nreferred to as the Machine Learning Model Description (MLMD). It then refines\nthe essential notion of semantics preservation to ensure the accurate\nreplication of the model. We apply our contributions to several industrial use\ncases to build and compare several target models.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u822a\u7a7a\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u5408\u89c4\u6027\uff0c\u91cd\u70b9\u533a\u5206\u4e86ML\u6a21\u578b\u4e0e\u5176\u660e\u786e\u63cf\u8ff0\uff08MLMD\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u8bed\u4e49\u4fdd\u6301\u7684\u6982\u5ff5\u4ee5\u786e\u4fdd\u6a21\u578b\u51c6\u786e\u590d\u5236\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u5728\u822a\u7a7a\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u589e\u52a0\uff0c\u9700\u8981\u786e\u4fdd\u8fd9\u4e9b\u7cfb\u7edf\u7684\u5b89\u5168\u8fd0\u884c\u5e76\u7b26\u5408\u76d1\u7ba1\u8981\u6c42\u3002EASA\u548cEUROCAE/SAE\u7b49\u673a\u6784\u5df2\u53d1\u5e03\u76f8\u5173\u6307\u5bfc\u6587\u4ef6\uff0c\u4f46\u9700\u8981\u66f4\u5177\u4f53\u7684\u65b9\u6cd5\u6765\u9a8c\u8bc1ML\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u533a\u5206ML\u6a21\u578b\u548cMLMD\uff0c\u5e76\u7ec6\u5316\u8bed\u4e49\u4fdd\u6301\u7684\u6982\u5ff5\uff0c\u786e\u4fdd\u6a21\u578b\u5728\u76ee\u6807\u73af\u5883\u4e2d\u51c6\u786e\u590d\u5236\u5176\u8bad\u7ec3\u6027\u80fd\u3002\u65b9\u6cd5\u5e94\u7528\u4e8e\u591a\u4e2a\u5de5\u4e1a\u7528\u4f8b\u6765\u6784\u5efa\u548c\u6bd4\u8f83\u76ee\u6807\u6a21\u578b\u3002", "result": "\u7814\u7a76\u63d0\u51fa\u4e86\u660e\u786e\u7684MLMD\u6982\u5ff5\u548c\u8bed\u4e49\u4fdd\u6301\u6846\u67b6\uff0c\u901a\u8fc7\u5de5\u4e1a\u7528\u4f8b\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u5e2e\u52a9\u786e\u4fddML\u6a21\u578b\u5728\u822a\u7a7a\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u5408\u89c4\u6027\u3002", "conclusion": "MLMD\u548c\u8bed\u4e49\u4fdd\u6301\u6982\u5ff5\u4e3aML\u6a21\u578b\u5728\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u4e2d\u7684\u5408\u89c4\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u91cd\u8981\u7406\u8bba\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u673a\u5668\u5b66\u4e60\u5728\u822a\u7a7a\u9886\u57df\u7684\u53ef\u9760\u5e94\u7528\u3002"}}
{"id": "2509.18686", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18686", "abs": "https://arxiv.org/abs/2509.18686", "authors": ["Ziyi Xu", "Haohong Lin", "Shiqi Liu", "Ding Zhao"], "title": "Query-Centric Diffusion Policy for Generalizable Robotic Assembly", "comment": "8 pages, 7 figures", "summary": "The robotic assembly task poses a key challenge in building generalist robots\ndue to the intrinsic complexity of part interactions and the sensitivity to\nnoise perturbations in contact-rich settings. The assembly agent is typically\ndesigned in a hierarchical manner: high-level multi-part reasoning and\nlow-level precise control. However, implementing such a hierarchical policy is\nchallenging in practice due to the mismatch between high-level skill queries\nand low-level execution. To address this, we propose the Query-centric\nDiffusion Policy (QDP), a hierarchical framework that bridges high-level\nplanning and low-level control by utilizing queries comprising objects, contact\npoints, and skill information. QDP introduces a query-centric mechanism that\nidentifies task-relevant components and uses them to guide low-level policies,\nleveraging point cloud observations to improve the policy's robustness. We\nconduct comprehensive experiments on the FurnitureBench in both simulation and\nreal-world settings, demonstrating improved performance in skill precision and\nlong-horizon success rate. In the challenging insertion and screwing tasks, QDP\nimproves the skill-wise success rate by over 50% compared to baselines without\nstructured queries.", "AI": {"tldr": "\u63d0\u51faQuery-centric Diffusion Policy (QDP)\u5206\u5c42\u6846\u67b6\uff0c\u901a\u8fc7\u67e5\u8be2\u673a\u5236\u8fde\u63a5\u9ad8\u5c42\u89c4\u5212\u548c\u5e95\u5c42\u63a7\u5236\uff0c\u63d0\u5347\u673a\u5668\u4eba\u88c5\u914d\u4efb\u52a1\u7684\u6027\u80fd", "motivation": "\u673a\u5668\u4eba\u88c5\u914d\u4efb\u52a1\u56e0\u96f6\u4ef6\u4ea4\u4e92\u590d\u6742\u6027\u548c\u63a5\u89e6\u4e30\u5bcc\u73af\u5883\u4e2d\u7684\u566a\u58f0\u654f\u611f\u6027\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u4f20\u7edf\u5206\u5c42\u7b56\u7565\u5b58\u5728\u9ad8\u5c42\u6280\u80fd\u67e5\u8be2\u4e0e\u5e95\u5c42\u6267\u884c\u4e0d\u5339\u914d\u7684\u95ee\u9898", "method": "QDP\u6846\u67b6\u4f7f\u7528\u5305\u542b\u7269\u4f53\u3001\u63a5\u89e6\u70b9\u548c\u6280\u80fd\u4fe1\u606f\u7684\u67e5\u8be2\u6765\u8bc6\u522b\u4efb\u52a1\u76f8\u5173\u7ec4\u4ef6\u5e76\u6307\u5bfc\u5e95\u5c42\u7b56\u7565\uff0c\u5229\u7528\u70b9\u4e91\u89c2\u6d4b\u63d0\u9ad8\u7b56\u7565\u9c81\u68d2\u6027", "result": "\u5728FurnitureBench\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u5b9e\u9a8c\u4e2d\uff0cQDP\u5728\u6280\u80fd\u7cbe\u5ea6\u548c\u957f\u65f6\u7a0b\u6210\u529f\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u63d2\u5165\u548c\u62e7\u87ba\u4e1d\u4efb\u52a1\u4e2d\u6bd4\u65e0\u7ed3\u6784\u5316\u67e5\u8be2\u7684\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u8d85\u8fc750%\u7684\u6280\u80fd\u6210\u529f\u7387", "conclusion": "QDP\u901a\u8fc7\u67e5\u8be2\u4e2d\u5fc3\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u5206\u5c42\u7b56\u7565\u4e2d\u7684\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u4e3a\u6784\u5efa\u901a\u7528\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.18690", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18690", "abs": "https://arxiv.org/abs/2509.18690", "authors": ["Zhiyu Kan", "Wensheng Gan", "Zhenlian Qi", "Philip S. Yu"], "title": "Advances in Large Language Models for Medicine", "comment": "Preprint. 5 figures, 4 tables", "summary": "Artificial intelligence (AI) technology has advanced rapidly in recent years,\nwith large language models (LLMs) emerging as a significant breakthrough. LLMs\nare increasingly making an impact across various industries, with the medical\nfield standing out as the most prominent application area. This paper\nsystematically reviews the up-to-date research progress of LLMs in the medical\nfield, providing an in-depth analysis of training techniques for large medical\nmodels, their adaptation in healthcare settings, related applications, as well\nas their strengths and limitations. Furthermore, it innovatively categorizes\nmedical LLMs into three distinct types based on their training methodologies\nand classifies their evaluation approaches into two categories. Finally, the\nstudy proposes solutions to existing challenges and outlines future research\ndirections based on identified issues in the field of medical LLMs. By\nsystematically reviewing previous and advanced research findings, we aim to\nhighlight the necessity of developing medical LLMs, provide a deeper\nunderstanding of their current state of development, and offer clear guidance\nfor subsequent research.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u9886\u57df\u7684\u6700\u65b0\u7814\u7a76\u8fdb\u5c55\uff0c\u5305\u62ec\u533b\u5b66\u5927\u6a21\u578b\u7684\u8bad\u7ec3\u6280\u672f\u3001\u533b\u7597\u573a\u666f\u9002\u914d\u3001\u5e94\u7528\u60c5\u51b5\u3001\u4f18\u52bf\u4e0e\u5c40\u9650\u6027\uff0c\u5e76\u5bf9\u533b\u5b66LLMs\u8fdb\u884c\u4e86\u521b\u65b0\u6027\u5206\u7c7b\uff0c\u63d0\u51fa\u4e86\u73b0\u6709\u6311\u6218\u7684\u89e3\u51b3\u65b9\u6848\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740AI\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u9886\u57df\u5c55\u73b0\u51fa\u5de8\u5927\u5e94\u7528\u6f5c\u529b\uff0c\u9700\u8981\u7cfb\u7edf\u68b3\u7406\u5f53\u524d\u7814\u7a76\u8fdb\u5c55\uff0c\u4e3a\u533b\u5b66LLMs\u7684\u53d1\u5c55\u63d0\u4f9b\u6307\u5bfc\u548c\u53c2\u8003\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5bf9\u533b\u5b66LLMs\u7684\u8bad\u7ec3\u65b9\u6cd5\u3001\u5e94\u7528\u573a\u666f\u3001\u8bc4\u4f30\u65b9\u6cd5\u7b49\u8fdb\u884c\u6df1\u5165\u5206\u6790\uff0c\u5e76\u57fa\u4e8e\u8bad\u7ec3\u65b9\u6cd5\u5c06\u533b\u5b66LLMs\u5206\u4e3a\u4e09\u7c7b\uff0c\u8bc4\u4f30\u65b9\u6cd5\u5206\u4e3a\u4e24\u7c7b\u3002", "result": "\u5168\u9762\u68b3\u7406\u4e86\u533b\u5b66LLMs\u7684\u7814\u7a76\u73b0\u72b6\uff0c\u63d0\u51fa\u4e86\u5206\u7c7b\u6846\u67b6\uff0c\u8bc6\u522b\u4e86\u5f53\u524d\u6311\u6218\uff0c\u5e76\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u660e\u786e\u6307\u5bfc\u3002", "conclusion": "\u533b\u5b66LLMs\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\uff0c\u672a\u6765\u53d1\u5c55\u9700\u8981\u5728\u6280\u672f\u4f18\u5316\u3001\u5e94\u7528\u62d3\u5c55\u548c\u4f26\u7406\u89c4\u8303\u7b49\u65b9\u9762\u6301\u7eed\u63a2\u7d22\uff0c\u672c\u7814\u7a76\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u53c2\u8003\u3002"}}
{"id": "2509.18734", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18734", "abs": "https://arxiv.org/abs/2509.18734", "authors": ["Nishant Doshi", "Amey Sutvani", "Sanket Gujar"], "title": "Learning Obstacle Avoidance using Double DQN for Quadcopter Navigation", "comment": null, "summary": "One of the challenges faced by Autonomous Aerial Vehicles is reliable\nnavigation through urban environments. Factors like reduction in precision of\nGlobal Positioning System (GPS), narrow spaces and dynamically moving obstacles\nmake the path planning of an aerial robot a complicated task. One of the skills\nrequired for the agent to effectively navigate through such an environment is\nto develop an ability to avoid collisions using information from onboard depth\nsensors. In this paper, we propose Reinforcement Learning of a virtual\nquadcopter robot agent equipped with a Depth Camera to navigate through a\nsimulated urban environment.", "AI": {"tldr": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u914d\u5907\u6df1\u5ea6\u76f8\u673a\u7684\u865a\u62df\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u5728\u6a21\u62df\u57ce\u5e02\u73af\u5883\u4e2d\u5bfc\u822a", "motivation": "\u81ea\u4e3b\u98de\u884c\u5668\u5728\u57ce\u5e02\u73af\u5883\u4e2d\u9762\u4e34GPS\u7cbe\u5ea6\u4e0b\u964d\u3001\u72ed\u7a84\u7a7a\u95f4\u548c\u52a8\u6001\u969c\u788d\u7269\u7b49\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u53ef\u9760\u7684\u907f\u969c\u80fd\u529b", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u8ba9\u914d\u5907\u6df1\u5ea6\u76f8\u673a\u7684\u865a\u62df\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u5728\u6a21\u62df\u57ce\u5e02\u73af\u5883\u4e2d\u5b66\u4e60\u5bfc\u822a", "result": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5bfc\u822a\u6846\u67b6\uff0c\u4f46\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\u672a\u5728\u6458\u8981\u4e2d\u63d0\u53ca", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u662f\u89e3\u51b3\u65e0\u4eba\u673a\u5728\u57ce\u5e02\u73af\u5883\u4e2d\u5bfc\u822a\u6311\u6218\u7684\u6709\u6548\u65b9\u6cd5"}}
{"id": "2509.18710", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18710", "abs": "https://arxiv.org/abs/2509.18710", "authors": ["Yanjie Fu", "Dongjie Wang", "Wangyang Ying", "Xiangliang Zhang", "Huan Liu", "Jian Pei"], "title": "Autonomous Data Agents: A New Opportunity for Smart Data", "comment": null, "summary": "As data continues to grow in scale and complexity, preparing, transforming,\nand analyzing it remains labor-intensive, repetitive, and difficult to scale.\nSince data contains knowledge and AI learns knowledge from it, the alignment\nbetween AI and data is essential. However, data is often not structured in ways\nthat are optimal for AI utilization. Moreover, an important question arises:\nhow much knowledge can we pack into data through intensive data operations?\nAutonomous data agents (DataAgents), which integrate LLM reasoning with task\ndecomposition, action reasoning and grounding, and tool calling, can\nautonomously interpret data task descriptions, decompose tasks into subtasks,\nreason over actions, ground actions into python code or tool calling, and\nexecute operations. Unlike traditional data management and engineering tools,\nDataAgents dynamically plan workflows, call powerful tools, and adapt to\ndiverse data tasks at scale. This report argues that DataAgents represent a\nparadigm shift toward autonomous data-to-knowledge systems. DataAgents are\ncapable of handling collection, integration, preprocessing, selection,\ntransformation, reweighing, augmentation, reprogramming, repairs, and\nretrieval. Through these capabilities, DataAgents transform complex and\nunstructured data into coherent and actionable knowledge. We first examine why\nthe convergence of agentic AI and data-to-knowledge systems has emerged as a\ncritical trend. We then define the concept of DataAgents and discuss their\narchitectural design, training strategies, as well as the new skills and\ncapabilities they enable. Finally, we call for concerted efforts to advance\naction workflow optimization, establish open datasets and benchmark ecosystems,\nsafeguard privacy, balance efficiency with scalability, and develop trustworthy\nDataAgent guardrails to prevent malicious actions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u81ea\u4e3b\u6570\u636e\u4ee3\u7406(DataAgents)\u7684\u6982\u5ff5\uff0c\u901a\u8fc7\u96c6\u6210LLM\u63a8\u7406\u4e0e\u4efb\u52a1\u5206\u89e3\u3001\u884c\u52a8\u63a8\u7406\u548c\u5de5\u5177\u8c03\u7528\uff0c\u5b9e\u73b0\u6570\u636e\u5230\u77e5\u8bc6\u7684\u81ea\u52a8\u5316\u8f6c\u6362\uff0c\u4ee3\u8868\u4e86\u6570\u636e\u7ba1\u7406\u5411\u81ea\u4e3b\u77e5\u8bc6\u7cfb\u7edf\u7684\u8303\u5f0f\u8f6c\u53d8\u3002", "motivation": "\u968f\u7740\u6570\u636e\u89c4\u6a21\u548c\u590d\u6742\u6027\u7684\u589e\u957f\uff0c\u6570\u636e\u51c6\u5907\u3001\u8f6c\u6362\u548c\u5206\u6790\u5de5\u4f5c\u4ecd\u7136\u52b3\u52a8\u5bc6\u96c6\u3001\u91cd\u590d\u4e14\u96be\u4ee5\u6269\u5c55\u3002\u6570\u636e\u4e0eAI\u4e4b\u95f4\u7684\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6570\u636e\u7ed3\u6784\u5f80\u5f80\u4e0d\u9002\u5408AI\u5229\u7528\u3002", "method": "DataAgents\u6574\u5408LLM\u63a8\u7406\u80fd\u529b\uff0c\u80fd\u591f\u81ea\u4e3b\u89e3\u91ca\u6570\u636e\u4efb\u52a1\u63cf\u8ff0\u3001\u5206\u89e3\u4efb\u52a1\u4e3a\u5b50\u4efb\u52a1\u3001\u63a8\u7406\u884c\u52a8\u3001\u5c06\u884c\u52a8\u8f6c\u5316\u4e3aPython\u4ee3\u7801\u6216\u5de5\u5177\u8c03\u7528\uff0c\u5e76\u6267\u884c\u64cd\u4f5c\u3002\u4e0e\u4f20\u7edf\u5de5\u5177\u4e0d\u540c\uff0cDataAgents\u80fd\u52a8\u6001\u89c4\u5212\u5de5\u4f5c\u6d41\u3001\u8c03\u7528\u5f3a\u5927\u5de5\u5177\u5e76\u9002\u5e94\u5404\u79cd\u6570\u636e\u4efb\u52a1\u3002", "result": "DataAgents\u80fd\u591f\u5904\u7406\u6570\u636e\u6536\u96c6\u3001\u96c6\u6210\u3001\u9884\u5904\u7406\u3001\u9009\u62e9\u3001\u8f6c\u6362\u3001\u91cd\u52a0\u6743\u3001\u589e\u5f3a\u3001\u91cd\u7f16\u7a0b\u3001\u4fee\u590d\u548c\u68c0\u7d22\u7b49\u4efb\u52a1\uff0c\u5c06\u590d\u6742\u975e\u7ed3\u6784\u5316\u6570\u636e\u8f6c\u5316\u4e3a\u8fde\u8d2f\u53ef\u64cd\u4f5c\u7684\u77e5\u8bc6\u3002", "conclusion": "DataAgents\u4ee3\u8868\u4e86\u6570\u636e\u5230\u77e5\u8bc6\u7cfb\u7edf\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u9700\u8981\u63a8\u8fdb\u884c\u52a8\u5de5\u4f5c\u6d41\u4f18\u5316\u3001\u5efa\u7acb\u5f00\u653e\u6570\u636e\u96c6\u548c\u57fa\u51c6\u751f\u6001\u7cfb\u7edf\u3001\u4fdd\u62a4\u9690\u79c1\u3001\u5e73\u8861\u6548\u7387\u4e0e\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u5f00\u53d1\u53ef\u4fe1\u8d56\u7684\u9632\u62a4\u673a\u5236\u3002"}}
{"id": "2509.18757", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18757", "abs": "https://arxiv.org/abs/2509.18757", "authors": ["Omar Rayyan", "John Abanes", "Mahmoud Hafez", "Anthony Tzes", "Fares Abu-Dakka"], "title": "MV-UMI: A Scalable Multi-View Interface for Cross-Embodiment Learning", "comment": "For project website and videos, see https https://mv-umi.github.io", "summary": "Recent advances in imitation learning have shown great promise for developing\nrobust robot manipulation policies from demonstrations. However, this promise\nis contingent on the availability of diverse, high-quality datasets, which are\nnot only challenging and costly to collect but are often constrained to a\nspecific robot embodiment. Portable handheld grippers have recently emerged as\nintuitive and scalable alternatives to traditional robotic teleoperation\nmethods for data collection. However, their reliance solely on first-person\nview wrist-mounted cameras often creates limitations in capturing sufficient\nscene contexts. In this paper, we present MV-UMI (Multi-View Universal\nManipulation Interface), a framework that integrates a third-person perspective\nwith the egocentric camera to overcome this limitation. This integration\nmitigates domain shifts between human demonstration and robot deployment,\npreserving the cross-embodiment advantages of handheld data-collection devices.\nOur experimental results, including an ablation study, demonstrate that our\nMV-UMI framework improves performance in sub-tasks requiring broad scene\nunderstanding by approximately 47% across 3 tasks, confirming the effectiveness\nof our approach in expanding the range of feasible manipulation tasks that can\nbe learned using handheld gripper systems, without compromising the\ncross-embodiment advantages inherent to such systems.", "AI": {"tldr": "MV-UMI\u6846\u67b6\u901a\u8fc7\u6574\u5408\u7b2c\u4e09\u4eba\u79f0\u89c6\u89d2\u4e0e\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u76f8\u673a\uff0c\u89e3\u51b3\u4e86\u624b\u6301\u6293\u53d6\u5668\u5728\u6a21\u4eff\u5b66\u4e60\u4e2d\u573a\u666f\u4e0a\u4e0b\u6587\u6355\u6349\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u624b\u6301\u6293\u53d6\u5668\u4f5c\u4e3a\u6570\u636e\u6536\u96c6\u5de5\u5177\u867d\u7136\u76f4\u89c2\u4e14\u53ef\u6269\u5c55\uff0c\u4f46\u4ec5\u4f9d\u8d56\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u624b\u8155\u6444\u50cf\u5934\u96be\u4ee5\u6355\u6349\u8db3\u591f\u7684\u573a\u666f\u4e0a\u4e0b\u6587\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u5b66\u4e60\u8303\u56f4\u3002", "method": "\u63d0\u51faMV-UMI\u6846\u67b6\uff0c\u6574\u5408\u7b2c\u4e09\u4eba\u79f0\u89c6\u89d2\u4e0e\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u76f8\u673a\uff0c\u51cf\u8f7b\u4eba\u7c7b\u6f14\u793a\u4e0e\u673a\u5668\u4eba\u90e8\u7f72\u4e4b\u95f4\u7684\u9886\u57df\u504f\u79fb\uff0c\u540c\u65f6\u4fdd\u6301\u624b\u6301\u6570\u636e\u6536\u96c6\u8bbe\u5907\u7684\u8de8\u4f53\u73b0\u4f18\u52bf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cMV-UMI\u6846\u67b6\u5728\u9700\u8981\u5e7f\u6cdb\u573a\u666f\u7406\u89e3\u7684\u5b50\u4efb\u52a1\u4e2d\u6027\u80fd\u63d0\u5347\u7ea647%\uff08\u57283\u4e2a\u4efb\u52a1\u4e2d\uff09\uff0c\u8bc1\u5b9e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "MV-UMI\u6846\u67b6\u6269\u5c55\u4e86\u4f7f\u7528\u624b\u6301\u6293\u53d6\u5668\u7cfb\u7edf\u53ef\u5b66\u4e60\u7684\u53ef\u884c\u64cd\u4f5c\u4efb\u52a1\u8303\u56f4\uff0c\u540c\u65f6\u4e0d\u635f\u5bb3\u6b64\u7c7b\u7cfb\u7edf\u56fa\u6709\u7684\u8de8\u4f53\u73b0\u4f18\u52bf\u3002"}}
{"id": "2509.18771", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18771", "abs": "https://arxiv.org/abs/2509.18771", "authors": ["Xingkun Yin", "Kaibin Huang", "Dong In Kim", "Hongyang Du"], "title": "Experience Scaling: Post-Deployment Evolution For Large Language Models", "comment": null, "summary": "Scaling model size, training data, and compute power have driven advances in\nlarge language models (LLMs), but these approaches are reaching saturation as\nhuman-generated text is exhausted and further gains diminish. We propose\nexperience scaling, a framework for continuous post-deployment evolution for\nLLMs through autonomous interaction with the environment and collaborative\nsharing of accumulated experience. The framework captures raw interactions,\ndistills them into compact, reusable knowledge, and periodically refines stored\ncontent to preserve relevance and efficiency. We validate the framework in\nsimulated real-world scenarios involving generalization to previously unseen\nbut related tasks, repetitive queries, and over-saturated knowledge stores.\nAcross all settings, experience scaling improves accuracy, sustains performance\nover time, and maintains gains when applied to novel situations. These results\ndemonstrate that structured post-deployment learning can extend LLM\ncapabilities beyond the limits of static human-generated data, offering a\nscalable path for continued intelligence progress.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7ecf\u9a8c\u6269\u5c55\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u4e3b\u73af\u5883\u4ea4\u4e92\u548c\u534f\u4f5c\u7ecf\u9a8c\u5171\u4eab\u5b9e\u73b0LLM\u7684\u6301\u7eed\u8fdb\u5316\uff0c\u7a81\u7834\u9759\u6001\u4eba\u7c7b\u751f\u6210\u6570\u636e\u7684\u9650\u5236", "motivation": "\u4f20\u7edf\u901a\u8fc7\u6269\u5927\u6a21\u578b\u89c4\u6a21\u3001\u8bad\u7ec3\u6570\u636e\u548c\u8ba1\u7b97\u80fd\u529b\u7684\u65b9\u6cd5\u5df2\u63a5\u8fd1\u9971\u548c\uff0c\u4eba\u7c7b\u751f\u6210\u6587\u672c\u8d44\u6e90\u8017\u5c3d\uff0c\u8fdb\u4e00\u6b65\u589e\u76ca\u9012\u51cf", "method": "\u7ecf\u9a8c\u6269\u5c55\u6846\u67b6\uff1a\u6355\u83b7\u539f\u59cb\u4ea4\u4e92\uff0c\u63d0\u70bc\u4e3a\u7d27\u51d1\u53ef\u91cd\u7528\u77e5\u8bc6\uff0c\u5b9a\u671f\u4f18\u5316\u5b58\u50a8\u5185\u5bb9\u4ee5\u4fdd\u6301\u76f8\u5173\u6027\u548c\u6548\u7387", "result": "\u5728\u6a21\u62df\u771f\u5b9e\u573a\u666f\u4e2d\u9a8c\u8bc1\uff0c\u5305\u62ec\u6cdb\u5316\u5230\u672a\u89c1\u4f46\u76f8\u5173\u4efb\u52a1\u3001\u91cd\u590d\u67e5\u8be2\u548c\u8fc7\u9971\u548c\u77e5\u8bc6\u5b58\u50a8\uff0c\u7ecf\u9a8c\u6269\u5c55\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u7ef4\u6301\u4e86\u957f\u671f\u6027\u80fd\uff0c\u5e76\u5728\u65b0\u60c5\u5883\u4e2d\u4fdd\u6301\u589e\u76ca", "conclusion": "\u7ed3\u6784\u5316\u90e8\u7f72\u540e\u5b66\u4e60\u53ef\u4ee5\u6269\u5c55LLM\u80fd\u529b\u8d85\u8d8a\u9759\u6001\u4eba\u7c7b\u751f\u6210\u6570\u636e\u7684\u9650\u5236\uff0c\u4e3a\u6301\u7eed\u667a\u80fd\u8fdb\u6b65\u63d0\u4f9b\u53ef\u6269\u5c55\u8def\u5f84"}}
{"id": "2509.18778", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18778", "abs": "https://arxiv.org/abs/2509.18778", "authors": ["Shijia Ge", "Yinxin Zhang", "Shuzhao Xie", "Weixiang Zhang", "Mingcai Zhou", "Zhi Wang"], "title": "VGGT-DP: Generalizable Robot Control via Vision Foundation Models", "comment": "submitted to AAAI 2026", "summary": "Visual imitation learning frameworks allow robots to learn manipulation\nskills from expert demonstrations. While existing approaches mainly focus on\npolicy design, they often neglect the structure and capacity of visual\nencoders, limiting spatial understanding and generalization. Inspired by\nbiological vision systems, which rely on both visual and proprioceptive cues\nfor robust control, we propose VGGT-DP, a visuomotor policy framework that\nintegrates geometric priors from a pretrained 3D perception model with\nproprioceptive feedback. We adopt the Visual Geometry Grounded Transformer\n(VGGT) as the visual encoder and introduce a proprioception-guided visual\nlearning strategy to align perception with internal robot states, improving\nspatial grounding and closed-loop control. To reduce inference latency, we\ndesign a frame-wise token reuse mechanism that compacts multi-view tokens into\nan efficient spatial representation. We further apply random token pruning to\nenhance policy robustness and reduce overfitting. Experiments on challenging\nMetaWorld tasks show that VGGT-DP significantly outperforms strong baselines\nsuch as DP and DP3, particularly in precision-critical and long-horizon\nscenarios.", "AI": {"tldr": "VGGT-DP\u662f\u4e00\u4e2a\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u9884\u8bad\u7ec33D\u611f\u77e5\u6a21\u578b\u7684\u51e0\u4f55\u5148\u9a8c\u548c\u672c\u4f53\u611f\u89c9\u53cd\u9988\uff0c\u63d0\u5347\u673a\u5668\u4eba\u7684\u7a7a\u95f4\u7406\u89e3\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u7b56\u7565\u8bbe\u8ba1\uff0c\u4f46\u5ffd\u89c6\u4e86\u89c6\u89c9\u7f16\u7801\u5668\u7684\u7ed3\u6784\u548c\u80fd\u529b\uff0c\u9650\u5236\u4e86\u7a7a\u95f4\u7406\u89e3\u548c\u6cdb\u5316\u6027\u80fd\u3002\u53d7\u751f\u7269\u89c6\u89c9\u7cfb\u7edf\u542f\u53d1\uff0c\u9700\u8981\u7ed3\u5408\u89c6\u89c9\u548c\u672c\u4f53\u611f\u89c9\u7ebf\u7d22\u6765\u5b9e\u73b0\u9c81\u68d2\u63a7\u5236\u3002", "method": "\u91c7\u7528\u89c6\u89c9\u51e0\u4f55\u57fa\u7840Transformer\u4f5c\u4e3a\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u5f15\u5165\u672c\u4f53\u611f\u89c9\u5f15\u5bfc\u7684\u89c6\u89c9\u5b66\u4e60\u7b56\u7565\u6765\u5bf9\u9f50\u611f\u77e5\u4e0e\u673a\u5668\u4eba\u5185\u90e8\u72b6\u6001\u3002\u8bbe\u8ba1\u4e86\u5e27\u7ea7token\u91cd\u7528\u673a\u5236\u6765\u51cf\u5c11\u63a8\u7406\u5ef6\u8fdf\uff0c\u5e76\u5e94\u7528\u968f\u673atoken\u526a\u679d\u6765\u589e\u5f3a\u7b56\u7565\u9c81\u68d2\u6027\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684MetaWorld\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cVGGT-DP\u663e\u8457\u4f18\u4e8eDP\u548cDP3\u7b49\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u7cbe\u5ea6\u8981\u6c42\u9ad8\u548c\u957f\u65f6\u7a0b\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "VGGT-DP\u901a\u8fc7\u6574\u5408\u51e0\u4f55\u5148\u9a8c\u548c\u672c\u4f53\u611f\u89c9\u53cd\u9988\uff0c\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u7684\u7a7a\u95f4\u7406\u89e3\u548c\u63a7\u5236\u6027\u80fd\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u6280\u80fd\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18787", "categories": ["cs.AI", "C.2.4"], "pdf": "https://arxiv.org/pdf/2509.18787", "abs": "https://arxiv.org/abs/2509.18787", "authors": ["Luca Muscariello", "Vijoy Pandey", "Ramiz Polic"], "title": "The AGNTCY Agent Directory Service: Architecture and Implementation", "comment": null, "summary": "The Agent Directory Service (ADS) is a distributed directory for the\ndiscovery of AI agent capabilities, metadata, and provenance. It leverages\ncontent-addressed storage, hierarchical taxonomies, and cryptographic signing\nto enable efficient, verifiable, and multi-dimensional discovery across\nheterogeneous Multi-Agent Systems (MAS). Built on the Open Agentic Schema\nFramework (OASF), ADS decouples capability indexing from content location\nthrough a two-level mapping realized over a Kademlia-based Distributed Hash\nTable (DHT). It reuses mature OCI / ORAS infrastructure for artifact\ndistribution, integrates Sigstore for provenance, and supports schema-driven\nextensibility for emerging agent modalities (LLM prompt agents, MCP servers,\nA2A-enabled components). This paper formalizes the architectural model,\ndescribes storage and discovery layers, explains security and performance\nproperties, and positions ADS within the broader landscape of emerging agent\nregistry and interoperability initiatives.", "AI": {"tldr": "ADS\u662f\u4e00\u4e2a\u5206\u5e03\u5f0f\u76ee\u5f55\u670d\u52a1\uff0c\u7528\u4e8e\u53d1\u73b0AI\u4ee3\u7406\u7684\u80fd\u529b\u3001\u5143\u6570\u636e\u548c\u6765\u6e90\u3002\u5b83\u5229\u7528\u5185\u5bb9\u5bfb\u5740\u5b58\u50a8\u3001\u5206\u5c42\u5206\u7c7b\u6cd5\u548c\u52a0\u5bc6\u7b7e\u540d\uff0c\u5728\u5f02\u6784\u591a\u4ee3\u7406\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u9a8c\u8bc1\u7684\u591a\u7ef4\u53d1\u73b0\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u591a\u4ee3\u7406\u7cfb\u7edf\u4e2d\u4ee3\u7406\u80fd\u529b\u53d1\u73b0\u548c\u4e92\u64cd\u4f5c\u6027\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u5f02\u6784\u4ee3\u7406\u7cfb\u7edf\u3001\u652f\u6301\u53ef\u9a8c\u8bc1\u53d1\u73b0\u548c\u6269\u5c55\u6027\u7684\u76ee\u5f55\u670d\u52a1\u3002", "method": "\u57fa\u4e8eOpen Agentic Schema Framework\u6784\u5efa\uff0c\u91c7\u7528\u4e24\u7ea7\u6620\u5c04\u67b6\u6784\uff08\u901a\u8fc7Kademlia DHT\u5b9e\u73b0\uff09\uff0c\u91cd\u7528OCI/ORAS\u57fa\u7840\u8bbe\u65bd\u8fdb\u884c\u5de5\u4ef6\u5206\u53d1\uff0c\u96c6\u6210Sigstore\u7528\u4e8e\u6765\u6e90\u9a8c\u8bc1\uff0c\u652f\u6301\u6a21\u5f0f\u9a71\u52a8\u7684\u53ef\u6269\u5c55\u6027\u3002", "result": "ADS\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6b63\u5f0f\u7684\u67b6\u6784\u6a21\u578b\uff0c\u5305\u62ec\u5b58\u50a8\u548c\u53d1\u73b0\u5c42\uff0c\u5177\u6709\u660e\u786e\u7684\u5b89\u5168\u548c\u6027\u80fd\u7279\u6027\uff0c\u80fd\u591f\u652f\u6301\u65b0\u5174\u4ee3\u7406\u6a21\u5f0f\uff08\u5982LLM\u63d0\u793a\u4ee3\u7406\u3001MCP\u670d\u52a1\u5668\u7b49\uff09\u3002", "conclusion": "ADS\u5728\u65b0\u5174\u4ee3\u7406\u6ce8\u518c\u548c\u4e92\u64cd\u4f5c\u6027\u5021\u8bae\u7684\u5e7f\u6cdb\u80cc\u666f\u4e0b\u5b9a\u4f4d\uff0c\u4e3a\u591a\u4ee3\u7406\u7cfb\u7edf\u7684\u80fd\u529b\u53d1\u73b0\u548c\u4e92\u64cd\u4f5c\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18786", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18786", "abs": "https://arxiv.org/abs/2509.18786", "authors": ["Johannes A. Gaus", "Loris Schneider", "Yitian Shi", "Jongseok Lee", "Rania Rayyes", "Rudolph Triebel"], "title": "Human-Interpretable Uncertainty Explanations for Point Cloud Registration", "comment": null, "summary": "In this paper, we address the point cloud registration problem, where\nwell-known methods like ICP fail under uncertainty arising from sensor noise,\npose-estimation errors, and partial overlap due to occlusion. We develop a\nnovel approach, Gaussian Process Concept Attribution (GP-CA), which not only\nquantifies registration uncertainty but also explains it by attributing\nuncertainty to well-known sources of errors in registration problems. Our\napproach leverages active learning to discover new uncertainty sources in the\nwild by querying informative instances. We validate GP-CA on three publicly\navailable datasets and in our real-world robot experiment. Extensive ablations\nsubstantiate our design choices. Our approach outperforms other\nstate-of-the-art methods in terms of runtime, high sample-efficiency with\nactive learning, and high accuracy. Our real-world experiment clearly\ndemonstrates its applicability. Our video also demonstrates that GP-CA enables\neffective failure-recovery behaviors, yielding more robust robotic perception.", "AI": {"tldr": "\u63d0\u51faGP-CA\u65b9\u6cd5\u89e3\u51b3\u70b9\u4e91\u914d\u51c6\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u91cf\u5316\u5e76\u89e3\u91ca\u914d\u51c6\u8bef\u5dee\u6765\u6e90\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5982ICP\u5728\u4f20\u611f\u5668\u566a\u58f0\u3001\u4f4d\u59ff\u4f30\u8ba1\u8bef\u5dee\u548c\u90e8\u5206\u91cd\u53e0\u7b49\u4e0d\u786e\u5b9a\u6027\u6761\u4ef6\u4e0b\u5bb9\u6613\u5931\u8d25\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u91cf\u5316\u5e76\u89e3\u91ca\u914d\u51c6\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u9ad8\u65af\u8fc7\u7a0b\u6982\u5ff5\u5f52\u56e0(GP-CA)\u65b9\u6cd5\uff0c\u5229\u7528\u4e3b\u52a8\u5b66\u4e60\u53d1\u73b0\u65b0\u7684\u4e0d\u786e\u5b9a\u6027\u6765\u6e90\uff0c\u901a\u8fc7\u67e5\u8be2\u4fe1\u606f\u4e30\u5bcc\u7684\u5b9e\u4f8b\u6765\u6539\u8fdb\u914d\u51c6\u6027\u80fd\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\uff0cGP-CA\u5728\u8fd0\u884c\u65f6\u95f4\u3001\u6837\u672c\u6548\u7387\u548c\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u5b9e\u73b0\u6709\u6548\u7684\u6545\u969c\u6062\u590d\u884c\u4e3a\u3002", "conclusion": "GP-CA\u65b9\u6cd5\u80fd\u591f\u63d0\u4f9b\u9c81\u68d2\u7684\u70b9\u4e91\u914d\u51c6\uff0c\u901a\u8fc7\u89e3\u91ca\u4e0d\u786e\u5b9a\u6027\u6765\u6e90\u589e\u5f3a\u673a\u5668\u4eba\u611f\u77e5\u7684\u53ef\u9760\u6027\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.18836", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18836", "abs": "https://arxiv.org/abs/2509.18836", "authors": ["Dennis Gross", "Helge Spieker", "Arnaud Gotlieb"], "title": "Bounded PCTL Model Checking of Large Language Model Outputs", "comment": "ICTAI 2025", "summary": "In this paper, we introduce LLMCHECKER, a model-checking-based verification\nmethod to verify the probabilistic computation tree logic (PCTL) properties of\nan LLM text generation process. We empirically show that only a limited number\nof tokens are typically chosen during text generation, which are not always the\nsame. This insight drives the creation of $\\alpha$-$k$-bounded text generation,\nnarrowing the focus to the $\\alpha$ maximal cumulative probability on the\ntop-$k$ tokens at every step of the text generation process. Our verification\nmethod considers an initial string and the subsequent top-$k$ tokens while\naccommodating diverse text quantification methods, such as evaluating text\nquality and biases. The threshold $\\alpha$ further reduces the selected tokens,\nonly choosing those that exceed or meet it in cumulative probability.\nLLMCHECKER then allows us to formally verify the PCTL properties of\n$\\alpha$-$k$-bounded LLMs. We demonstrate the applicability of our method in\nseveral LLMs, including Llama, Gemma, Mistral, Genstruct, and BERT. To our\nknowledge, this is the first time PCTL-based model checking has been used to\ncheck the consistency of the LLM text generation process.", "AI": {"tldr": "LLMCHECKER\u662f\u4e00\u4e2a\u57fa\u4e8e\u6a21\u578b\u68c0\u67e5\u7684\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u7528\u4e8e\u9a8c\u8bc1LLM\u6587\u672c\u751f\u6210\u8fc7\u7a0b\u7684\u6982\u7387\u8ba1\u7b97\u6811\u903b\u8f91(PCTL)\u5c5e\u6027\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u03b1-k\u6709\u754c\u6587\u672c\u751f\u6210\u6765\u9650\u5236\u9a8c\u8bc1\u8303\u56f4\uff0c\u91cd\u70b9\u5173\u6ce8\u6bcf\u4e2a\u751f\u6210\u6b65\u9aa4\u4e2dtop-k\u4ee4\u724c\u7684\u7d2f\u79ef\u6982\u7387\u3002", "motivation": "\u73b0\u6709LLM\u6587\u672c\u751f\u6210\u8fc7\u7a0b\u7f3a\u4e4f\u5f62\u5f0f\u5316\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u65e0\u6cd5\u4fdd\u8bc1\u751f\u6210\u6587\u672c\u7684\u4e00\u81f4\u6027\u548c\u53ef\u9760\u6027\u3002\u4f5c\u8005\u53d1\u73b0\u6587\u672c\u751f\u6210\u8fc7\u7a0b\u4e2d\u53ea\u6709\u6709\u9650\u6570\u91cf\u7684\u4ee4\u724c\u88ab\u9009\u62e9\uff0c\u4e14\u8fd9\u4e9b\u9009\u62e9\u5e76\u4e0d\u603b\u662f\u76f8\u540c\uff0c\u8fd9\u4fc3\u4f7f\u5f00\u53d1\u5f62\u5f0f\u5316\u9a8c\u8bc1\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u03b1-k\u6709\u754c\u6587\u672c\u751f\u6210\u65b9\u6cd5\uff0c\u5728\u6bcf\u4e2a\u6587\u672c\u751f\u6210\u6b65\u9aa4\u4e2d\u53ea\u8003\u8651\u7d2f\u79ef\u6982\u7387\u8d85\u8fc7\u9608\u503c\u03b1\u7684top-k\u4ee4\u724c\u3002LLMCHECKER\u5229\u7528\u6a21\u578b\u68c0\u67e5\u6280\u672f\u9a8c\u8bc1PCTL\u5c5e\u6027\uff0c\u652f\u6301\u591a\u79cd\u6587\u672c\u91cf\u5316\u65b9\u6cd5\u5982\u8d28\u91cf\u8bc4\u4f30\u548c\u504f\u89c1\u68c0\u6d4b\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u4e2aLLM\u6a21\u578b\uff08\u5305\u62ecLlama\u3001Gemma\u3001Mistral\u3001Genstruct\u548cBERT\uff09\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5176\u9002\u7528\u6027\u3002\u8fd9\u662f\u9996\u6b21\u5c06PCTL\u6a21\u578b\u68c0\u67e5\u5e94\u7528\u4e8eLLM\u6587\u672c\u751f\u6210\u8fc7\u7a0b\u7684\u4e00\u81f4\u6027\u9a8c\u8bc1\u3002", "conclusion": "LLMCHECKER\u4e3aLLM\u6587\u672c\u751f\u6210\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u9996\u4e2a\u57fa\u4e8ePCTL\u7684\u5f62\u5f0f\u5316\u9a8c\u8bc1\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u9a8c\u8bc1\u751f\u6210\u6587\u672c\u7684\u53ef\u9760\u6027\u548c\u4e00\u81f4\u6027\uff0c\u4e3aLLM\u7684\u5b89\u5168\u53ef\u4fe1\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2509.18793", "categories": ["cs.RO", "cs.MA", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.18793", "abs": "https://arxiv.org/abs/2509.18793", "authors": ["Lukas Zanger", "Bastian Lampe", "Lennart Reiher", "Lutz Eckstein"], "title": "Application Management in C-ITS: Orchestrating Demand-Driven Deployments and Reconfigurations", "comment": "7 pages, 2 figures, 2 tables; Accepted to be published as part of the\n  2025 IEEE International Conference on Intelligent Transportation Systems\n  (ITSC 2025), Gold Coast, Australia, November 18-21, 2025", "summary": "Vehicles are becoming increasingly automated and interconnected, enabling the\nformation of cooperative intelligent transport systems (C-ITS) and the use of\noffboard services. As a result, cloud-native techniques, such as microservices\nand container orchestration, play an increasingly important role in their\noperation. However, orchestrating applications in a large-scale C-ITS poses\nunique challenges due to the dynamic nature of the environment and the need for\nefficient resource utilization. In this paper, we present a demand-driven\napplication management approach that leverages cloud-native techniques -\nspecifically Kubernetes - to address these challenges. Taking into account the\ndemands originating from different entities within the C-ITS, the approach\nenables the automation of processes, such as deployment, reconfiguration,\nupdate, upgrade, and scaling of microservices. Executing these processes on\ndemand can, for example, reduce computing resource consumption and network\ntraffic. A demand may include a request for provisioning an external supporting\nservice, such as a collective environment model. The approach handles changing\nand new demands by dynamically reconciling them through our proposed\napplication management framework built on Kubernetes and the Robot Operating\nSystem (ROS 2). We demonstrate the operation of our framework in the C-ITS use\ncase of collective environment perception and make the source code of the\nprototypical framework publicly available at\nhttps://github.com/ika-rwth-aachen/application_manager .", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eKubernetes\u7684\u9700\u6c42\u9a71\u52a8\u5e94\u7528\u7ba1\u7406\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u89c4\u6a21\u534f\u4f5c\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u52a8\u6001\u73af\u5883\u4e0b\u7684\u5e94\u7528\u7f16\u6392\u6311\u6218\u3002", "motivation": "\u968f\u7740\u8f66\u8f86\u81ea\u52a8\u5316\u548c\u4e92\u8054\u7a0b\u5ea6\u7684\u63d0\u9ad8\uff0c\u534f\u4f5c\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u9700\u8981\u9ad8\u6548\u7684\u4e91\u539f\u751f\u6280\u672f\u6765\u7ba1\u7406\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u8d44\u6e90\u5229\u7528\u6548\u7387\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eKubernetes\u548cROS 2\u7684\u5e94\u7528\u7ba1\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u9700\u6c42\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u6d41\u7a0b\uff08\u90e8\u7f72\u3001\u91cd\u914d\u7f6e\u3001\u66f4\u65b0\u3001\u5347\u7ea7\u548c\u6269\u5c55\u5fae\u670d\u52a1\uff09\u6765\u52a8\u6001\u534f\u8c03\u4e0d\u540c\u7684\u670d\u52a1\u9700\u6c42\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u96c6\u4f53\u73af\u5883\u611f\u77e5\u7528\u4f8b\u4e2d\u5f97\u5230\u9a8c\u8bc1\uff0c\u80fd\u591f\u6709\u6548\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u548c\u7f51\u7edc\u6d41\u91cf\uff0c\u5e76\u652f\u6301\u52a8\u6001\u5904\u7406\u53d8\u5316\u548c\u65b0\u9700\u6c42\u3002", "conclusion": "\u63d0\u51fa\u7684\u9700\u6c42\u9a71\u52a8\u5e94\u7528\u7ba1\u7406\u6846\u67b6\u4e3a\u5927\u89c4\u6a21C-ITS\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u4e91\u539f\u751f\u6280\u672f\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5e94\u7528\u7f16\u6392\u548c\u8d44\u6e90\u4f18\u5316\u3002"}}
{"id": "2509.18846", "categories": ["cs.AI", "I.2.6; I.2.7; J.3"], "pdf": "https://arxiv.org/pdf/2509.18846", "abs": "https://arxiv.org/abs/2509.18846", "authors": ["Hong-Jie Dai", "Zheng-Hao Li", "An-Tai Lu", "Bo-Tsz Shain", "Ming-Ta Li", "Tatheer Hussain Mir", "Kuang-Te Wang", "Min-I Su", "Pei-Kang Liu", "Ming-Ju Tsai"], "title": "Model selection meets clinical semantics: Optimizing ICD-10-CM prediction via LLM-as-Judge evaluation, redundancy-aware sampling, and section-aware fine-tuning", "comment": "28 Pages, 4 Figures, 2 Tables", "summary": "Accurate International Classification of Diseases (ICD) coding is critical\nfor clinical documentation, billing, and healthcare analytics, yet it remains a\nlabour-intensive and error-prone task. Although large language models (LLMs)\nshow promise in automating ICD coding, their challenges in base model\nselection, input contextualization, and training data redundancy limit their\neffectiveness. We propose a modular framework for ICD-10 Clinical Modification\n(ICD-10-CM) code prediction that addresses these challenges through principled\nmodel selection, redundancy-aware data sampling, and structured input design.\nThe framework integrates an LLM-as-judge evaluation protocol with Plackett-Luce\naggregation to assess and rank open-source LLMs based on their intrinsic\ncomprehension of ICD-10-CM code definitions. We introduced embedding-based\nsimilarity measures, a redundancy-aware sampling strategy to remove\nsemantically duplicated discharge summaries. We leverage structured discharge\nsummaries from Taiwanese hospitals to evaluate contextual effects and examine\nsection-wise content inclusion under universal and section-specific modelling\nparadigms. Experiments across two institutional datasets demonstrate that the\nselected base model after fine-tuning consistently outperforms baseline LLMs in\ninternal and external evaluations. Incorporating more clinical sections\nconsistently improves prediction performance. This study uses open-source LLMs\nto establish a practical and principled approach to ICD-10-CM code prediction.\nThe proposed framework provides a scalable, institution-ready solution for\nreal-world deployment of automated medical coding systems by combining informed\nmodel selection, efficient data refinement, and context-aware prompting.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\u7528\u4e8eICD-10-CM\u7f16\u7801\u9884\u6d4b\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u6a21\u578b\u9009\u62e9\u3001\u5197\u4f59\u611f\u77e5\u6570\u636e\u91c7\u6837\u548c\u7ed3\u6784\u5316\u8f93\u5165\u8bbe\u8ba1\uff0c\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u7f16\u7801\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "ICD\u7f16\u7801\u5bf9\u4e34\u5e8a\u6587\u6863\u3001\u8ba1\u8d39\u548c\u5206\u6790\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u4ecd\u662f\u52b3\u52a8\u5bc6\u96c6\u578b\u4e14\u6613\u51fa\u9519\u7684\u4efb\u52a1\u3002\u73b0\u6709LLMs\u5728\u81ea\u52a8\u7f16\u7801\u4e2d\u5b58\u5728\u6a21\u578b\u9009\u62e9\u3001\u8f93\u5165\u4e0a\u4e0b\u6587\u5316\u548c\u8bad\u7ec3\u6570\u636e\u5197\u4f59\u7b49\u6311\u6218\u3002", "method": "\u91c7\u7528LLM-as-judge\u8bc4\u4f30\u534f\u8bae\u548cPlackett-Luce\u805a\u5408\u6765\u8bc4\u4f30\u5f00\u6e90LLMs\u5bf9ICD-10-CM\u5b9a\u4e49\u7684\u7406\u89e3\uff1b\u5f15\u5165\u5d4c\u5165\u76f8\u4f3c\u6027\u5ea6\u91cf\u548c\u5197\u4f59\u611f\u77e5\u91c7\u6837\u7b56\u7565\uff1b\u5229\u7528\u53f0\u6e7e\u533b\u9662\u7684\u7ed3\u6784\u5316\u51fa\u9662\u6458\u8981\u8bc4\u4f30\u4e0a\u4e0b\u6587\u6548\u679c\u3002", "result": "\u5728\u4e24\u4e2a\u673a\u6784\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u7ecf\u8fc7\u5fae\u8c03\u7684\u9009\u5b9a\u57fa\u7840\u6a21\u578b\u5728\u5185\u90e8\u548c\u5916\u90e8\u8bc4\u4f30\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebfLLMs\uff1b\u7eb3\u5165\u66f4\u591a\u4e34\u5e8a\u90e8\u5206\u6301\u7eed\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aICD-10-CM\u7f16\u7801\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u539f\u5219\u6027\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u660e\u667a\u7684\u6a21\u578b\u9009\u62e9\u3001\u9ad8\u6548\u6570\u636e\u7cbe\u70bc\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u63d0\u793a\uff0c\u4e3a\u81ea\u52a8\u5316\u533b\u7597\u7f16\u7801\u7cfb\u7edf\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u673a\u6784\u5c31\u7eea\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18830", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18830", "abs": "https://arxiv.org/abs/2509.18830", "authors": ["Suzannah Wistreich", "Baiyu Shi", "Stephen Tian", "Samuel Clarke", "Michael Nath", "Chengyi Xu", "Zhenan Bao", "Jiajun Wu"], "title": "DexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation", "comment": "Accepted to CoRL 2025", "summary": "Human skin provides a rich tactile sensing stream, localizing intentional and\nunintentional contact events over a large and contoured region. Replicating\nthese tactile sensing capabilities for dexterous robotic manipulation systems\nremains a longstanding challenge. In this work, we take a step towards this\ngoal by introducing DexSkin. DexSkin is a soft, conformable capacitive\nelectronic skin that enables sensitive, localized, and calibratable tactile\nsensing, and can be tailored to varying geometries. We demonstrate its efficacy\nfor learning downstream robotic manipulation by sensorizing a pair of parallel\njaw gripper fingers, providing tactile coverage across almost the entire finger\nsurfaces. We empirically evaluate DexSkin's capabilities in learning\nchallenging manipulation tasks that require sensing coverage across the entire\nsurface of the fingers, such as reorienting objects in hand and wrapping\nelastic bands around boxes, in a learning-from-demonstration framework. We then\nshow that, critically for data-driven approaches, DexSkin can be calibrated to\nenable model transfer across sensor instances, and demonstrate its\napplicability to online reinforcement learning on real robots. Our results\nhighlight DexSkin's suitability and practicality for learning real-world,\ncontact-rich manipulation. Please see our project webpage for videos and\nvisualizations: https://dex-skin.github.io/.", "AI": {"tldr": "DexSkin\u662f\u4e00\u79cd\u67d4\u8f6f\u3001\u53ef\u9002\u5e94\u7684\u7535\u5bb9\u5f0f\u7535\u5b50\u76ae\u80a4\uff0c\u80fd\u591f\u5b9e\u73b0\u654f\u611f\u3001\u5c40\u90e8\u5316\u548c\u53ef\u6821\u51c6\u7684\u89e6\u89c9\u611f\u77e5\uff0c\u53ef\u7528\u4e8e\u673a\u5668\u4eba\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\u7684\u5b66\u4e60\u3002", "motivation": "\u4eba\u7c7b\u76ae\u80a4\u63d0\u4f9b\u4e30\u5bcc\u7684\u89e6\u89c9\u611f\u77e5\u6d41\uff0c\u80fd\u591f\u5728\u5927\u7684\u66f2\u9762\u533a\u57df\u5b9a\u4f4d\u6709\u610f\u548c\u65e0\u610f\u7684\u63a5\u89e6\u4e8b\u4ef6\u3002\u4e3a\u7075\u5de7\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\u590d\u5236\u8fd9\u4e9b\u89e6\u89c9\u611f\u77e5\u80fd\u529b\u4ecd\u7136\u662f\u4e00\u4e2a\u957f\u671f\u6311\u6218\u3002", "method": "\u5f00\u53d1DexSkin\u8f6f\u6027\u7535\u5bb9\u7535\u5b50\u76ae\u80a4\uff0c\u53ef\u5b9a\u5236\u5230\u4e0d\u540c\u51e0\u4f55\u5f62\u72b6\uff0c\u5e76\u5728\u5e73\u884c\u5939\u722a\u624b\u6307\u4e0a\u5b9e\u73b0\u51e0\u4e4e\u6574\u4e2a\u624b\u6307\u8868\u9762\u7684\u89e6\u89c9\u8986\u76d6\uff0c\u901a\u8fc7\u793a\u8303\u5b66\u4e60\u548c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "DexSkin\u5728\u9700\u8981\u6574\u4e2a\u624b\u6307\u8868\u9762\u611f\u77e5\u8986\u76d6\u7684\u6311\u6218\u6027\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u6548\uff0c\u5982\u624b\u4e2d\u91cd\u65b0\u5b9a\u5411\u7269\u4f53\u548c\u5c06\u5f39\u6027\u5e26\u7f20\u7ed5\u5728\u76d2\u5b50\u4e0a\uff0c\u5e76\u4e14\u80fd\u591f\u5b9e\u73b0\u4f20\u611f\u5668\u5b9e\u4f8b\u95f4\u7684\u6a21\u578b\u8fc1\u79fb\u3002", "conclusion": "DexSkin\u9002\u7528\u4e8e\u5b66\u4e60\u73b0\u5b9e\u4e16\u754c\u4e2d\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u4efb\u52a1\uff0c\u5177\u6709\u5b9e\u7528\u6027\u548c\u9002\u7528\u6027\u3002"}}
{"id": "2509.18849", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18849", "abs": "https://arxiv.org/abs/2509.18849", "authors": ["Wenke Huang", "Quan Zhang", "Yiyang Fang", "Jian Liang", "Xuankun Rong", "Huanjin Yao", "Guancheng Wan", "Ke Liang", "Wenwen He", "Mingjun Li", "Leszek Rutkowski", "Mang Ye", "Bo Du", "Dacheng Tao"], "title": "MAPO: Mixed Advantage Policy Optimization", "comment": null, "summary": "Recent advances in reinforcement learning for foundation models, such as\nGroup Relative Policy Optimization (GRPO), have significantly improved the\nperformance of foundation models on reasoning tasks. Notably, the advantage\nfunction serves as a central mechanism in GRPO for ranking the trajectory\nimportance. However, existing explorations encounter both advantage reversion\nand advantage mirror problems, which hinder the reasonable advantage allocation\nacross different query samples. In this work, we propose an easy but effective\nGRPO strategy, Mixed Advantage Policy Optimization (MAPO). We reveal that the\ntrajectory appears with different certainty and propose the advantage percent\ndeviation for samples with high-certainty trajectories. Furthermore, we\ndynamically reweight the advantage function for samples with varying trajectory\ncertainty, thereby adaptively configuring the advantage function to account for\nsample-specific characteristics. Comparison with related state-of-the-art\nmethods, along with ablation studies on different advantage variants, validates\nthe effectiveness of our approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6df7\u5408\u4f18\u52bf\u7b56\u7565\u4f18\u5316\uff08MAPO\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86GRPO\u4e2d\u5b58\u5728\u7684\u4f18\u52bf\u53cd\u8f6c\u548c\u4f18\u52bf\u955c\u50cf\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u91cd\u65b0\u52a0\u6743\u4f18\u52bf\u51fd\u6570\u6765\u9002\u5e94\u4e0d\u540c\u8f68\u8ff9\u786e\u5b9a\u6027\u7684\u6837\u672c\u3002", "motivation": "\u73b0\u6709\u7684GRPO\u65b9\u6cd5\u5728\u4f18\u52bf\u51fd\u6570\u5206\u914d\u4e0a\u5b58\u5728\u4f18\u52bf\u53cd\u8f6c\u548c\u4f18\u52bf\u955c\u50cf\u95ee\u9898\uff0c\u963b\u788d\u4e86\u4e0d\u540c\u67e5\u8be2\u6837\u672c\u95f4\u7684\u5408\u7406\u4f18\u52bf\u5206\u914d\u3002", "method": "\u63d0\u51faMAPO\u7b56\u7565\uff0c\u5f15\u5165\u4f18\u52bf\u767e\u5206\u6bd4\u504f\u5dee\u6765\u5904\u7406\u9ad8\u786e\u5b9a\u6027\u8f68\u8ff9\u6837\u672c\uff0c\u5e76\u52a8\u6001\u91cd\u65b0\u52a0\u6743\u4f18\u52bf\u51fd\u6570\u4ee5\u9002\u5e94\u4e0d\u540c\u8f68\u8ff9\u786e\u5b9a\u6027\u7684\u6837\u672c\u3002", "result": "\u4e0e\u76f8\u5173\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6bd4\u8f83\u4ee5\u53ca\u4e0d\u540c\u4f18\u52bf\u53d8\u4f53\u7684\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "MAPO\u662f\u4e00\u79cd\u7b80\u5355\u4f46\u6709\u6548\u7684GRPO\u7b56\u7565\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u8f68\u8ff9\u786e\u5b9a\u6027\u5dee\u5f02\uff0c\u63d0\u5347\u57fa\u7840\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002"}}
{"id": "2509.18865", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18865", "abs": "https://arxiv.org/abs/2509.18865", "authors": ["Masato Kobayashi", "Thanpimon Buamanee"], "title": "Bi-VLA: Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation", "comment": null, "summary": "We propose Bilateral Control-Based Imitation Learning via Vision-Language\nFusion for Action Generation (Bi-VLA), a novel framework that extends bilateral\ncontrol-based imitation learning to handle more than one task within a single\nmodel. Conventional bilateral control methods exploit joint angle, velocity,\ntorque, and vision for precise manipulation but require task-specific models,\nlimiting their generality. Bi-VLA overcomes this limitation by utilizing robot\njoint angle, velocity, and torque data from leader-follower bilateral control\nwith visual features and natural language instructions through SigLIP and\nFiLM-based fusion. We validated Bi-VLA on two task types: one requiring\nsupplementary language cues and another distinguishable solely by vision.\nReal-robot experiments showed that Bi-VLA successfully interprets\nvision-language combinations and improves task success rates compared to\nconventional bilateral control-based imitation learning. Our Bi-VLA addresses\nthe single-task limitation of prior bilateral approaches and provides empirical\nevidence that combining vision and language significantly enhances versatility.\nExperimental results validate the effectiveness of Bi-VLA in real-world tasks.\nFor additional material, please visit the website:\nhttps://mertcookimg.github.io/bi-vla/", "AI": {"tldr": "Bi-VLA\u662f\u4e00\u4e2a\u57fa\u4e8e\u53cc\u8fb9\u63a7\u5236\u7684\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9-\u8bed\u8a00\u878d\u5408\u5b9e\u73b0\u591a\u4efb\u52a1\u52a8\u4f5c\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u53cc\u8fb9\u63a7\u5236\u65b9\u6cd5\u53ea\u80fd\u5904\u7406\u5355\u4e00\u4efb\u52a1\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u53cc\u8fb9\u63a7\u5236\u65b9\u6cd5\u867d\u7136\u5229\u7528\u5173\u8282\u89d2\u5ea6\u3001\u901f\u5ea6\u3001\u626d\u77e9\u548c\u89c6\u89c9\u4fe1\u606f\u5b9e\u73b0\u7cbe\u786e\u64cd\u4f5c\uff0c\u4f46\u9700\u8981\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u6784\u5efa\u7279\u5b9a\u6a21\u578b\uff0c\u9650\u5236\u4e86\u901a\u7528\u6027\u3002Bi-VLA\u65e8\u5728\u514b\u670d\u8fd9\u4e00\u9650\u5236\uff0c\u5b9e\u73b0\u5355\u4e00\u6a21\u578b\u5904\u7406\u591a\u4e2a\u4efb\u52a1\u3002", "method": "Bi-VLA\u7ed3\u5408\u4e86\u9886\u5bfc\u8005-\u8ddf\u968f\u8005\u53cc\u8fb9\u63a7\u5236\u7684\u673a\u5668\u4eba\u5173\u8282\u89d2\u5ea6\u3001\u901f\u5ea6\u548c\u626d\u77e9\u6570\u636e\uff0c\u901a\u8fc7SigLIP\u548cFiLM-based\u878d\u5408\u6280\u672f\u6574\u5408\u89c6\u89c9\u7279\u5f81\u548c\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u3002", "result": "\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u8868\u660e\uff0cBi-VLA\u80fd\u591f\u6210\u529f\u89e3\u8bfb\u89c6\u89c9-\u8bed\u8a00\u7ec4\u5408\uff0c\u76f8\u6bd4\u4f20\u7edf\u53cc\u8fb9\u63a7\u5236\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u3002", "conclusion": "Bi-VLA\u89e3\u51b3\u4e86\u5148\u524d\u53cc\u8fb9\u65b9\u6cd5\u53ea\u80fd\u5904\u7406\u5355\u4e00\u4efb\u52a1\u7684\u9650\u5236\uff0c\u5b9e\u8bc1\u8868\u660e\u89c6\u89c9\u548c\u8bed\u8a00\u7684\u7ed3\u5408\u663e\u8457\u589e\u5f3a\u4e86\u7cfb\u7edf\u7684\u901a\u7528\u6027\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002"}}
{"id": "2509.18864", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18864", "abs": "https://arxiv.org/abs/2509.18864", "authors": ["Yingxin Li", "Jianbo Zhao", "Xueyu Ren", "Jie Tang", "Wangjie You", "Xu Chen", "Kan Zhou", "Chao Feng", "Jiao Ran", "Yuan Meng", "Zhi Wang"], "title": "Conf-Profile: A Confidence-Driven Reasoning Paradigm for Label-Free User Profiling", "comment": null, "summary": "User profiling, as a core technique for user understanding, aims to infer\nstructural attributes from user information. Large Language Models (LLMs)\nprovide a promising avenue for user profiling, yet the progress is hindered by\nthe lack of comprehensive benchmarks. To bridge this gap, we propose\nProfileBench, an industrial benchmark derived from a real-world video platform,\nencompassing heterogeneous user data and a well-structured profiling taxonomy.\nHowever, the profiling task remains challenging due to the difficulty of\ncollecting large-scale ground-truth labels, and the heterogeneous and noisy\nuser information can compromise the reliability of LLMs. To approach label-free\nand reliable user profiling, we propose a Confidence-driven Profile reasoning\nframework Conf-Profile, featuring a two-stage paradigm. We first synthesize\nhigh-quality labels by leveraging advanced LLMs with confidence hints, followed\nby confidence-weighted voting for accuracy improvement and confidence\ncalibration for a balanced distribution. The multiple profile results,\nrationales, and confidence scores are aggregated and distilled into a\nlightweight LLM. We further enhance the reasoning ability via confidence-guided\nunsupervised reinforcement learning, which exploits confidence for difficulty\nfiltering, quasi-ground truth voting, and reward weighting. Experimental\nresults demonstrate that Conf-Profile delivers substantial performance through\nthe two-stage training, improving F1 by 13.97 on Qwen3-8B.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ProfileBench\u57fa\u51c6\u548cConf-Profile\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u7528\u6237\u753b\u50cf\u4efb\u52a1\u4e2d\u6807\u7b7e\u7a00\u7f3a\u548c\u5f02\u6784\u6570\u636e\u566a\u58f0\u95ee\u9898\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u9a71\u52a8\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\u663e\u8457\u63d0\u5347LLM\u5728\u7528\u6237\u753b\u50cf\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u7528\u6237\u753b\u50cf\u4f5c\u4e3a\u7528\u6237\u7406\u89e3\u7684\u6838\u5fc3\u6280\u672f\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u7f3a\u4e4f\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u53ca\u96be\u4ee5\u83b7\u53d6\u5927\u89c4\u6a21\u771f\u5b9e\u6807\u7b7e\u3002\u5f02\u6784\u548c\u566a\u58f0\u7528\u6237\u4fe1\u606f\u4f1a\u964d\u4f4eLLM\u7684\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faConf-Profile\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8303\u5f0f\uff1a1\uff09\u5229\u7528\u9ad8\u7ea7LLM\u5408\u6210\u9ad8\u8d28\u91cf\u6807\u7b7e\uff0c\u8fdb\u884c\u7f6e\u4fe1\u5ea6\u52a0\u6743\u6295\u7968\u548c\u6821\u51c6\uff1b2\uff09\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u7684\u65e0\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u63a8\u7406\u80fd\u529b\uff0c\u5305\u62ec\u96be\u5ea6\u8fc7\u6ee4\u3001\u51c6\u771f\u5b9e\u6807\u7b7e\u6295\u7968\u548c\u5956\u52b1\u52a0\u6743\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cConf-Profile\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5728Qwen3-8B\u6a21\u578b\u4e0aF1\u5206\u6570\u63d0\u9ad8\u4e8613.97\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u89e3\u51b3\u6807\u7b7e\u7a00\u7f3a\u7684\u7528\u6237\u753b\u50cf\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u7f6e\u4fe1\u5ea6\u9a71\u52a8\u7684\u6846\u67b6\u80fd\u591f\u663e\u8457\u63d0\u5347LLM\u5728\u771f\u5b9e\u5de5\u4e1a\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2509.18937", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18937", "abs": "https://arxiv.org/abs/2509.18937", "authors": ["Yanyuan Qiao", "Kieran Gilday", "Yutong Xie", "Josie Hughes"], "title": "Lang2Morph: Language-Driven Morphological Design of Robotic Hands", "comment": null, "summary": "Designing robotic hand morphologies for diverse manipulation tasks requires\nbalancing dexterity, manufacturability, and task-specific functionality. While\nopen-source frameworks and parametric tools support reproducible design, they\nstill rely on expert heuristics and manual tuning. Automated methods using\noptimization are often compute-intensive, simulation-dependent, and rarely\ntarget dexterous hands. Large language models (LLMs), with their broad\nknowledge of human-object interactions and strong generative capabilities,\noffer a promising alternative for zero-shot design reasoning. In this paper, we\npresent Lang2Morph, a language-driven pipeline for robotic hand design. It uses\nLLMs to translate natural-language task descriptions into symbolic structures\nand OPH-compatible parameters, enabling 3D-printable task-specific\nmorphologies. The pipeline consists of: (i) Morphology Design, which maps tasks\ninto semantic tags, structural grammars, and OPH-compatible parameters; and\n(ii) Selection and Refinement, which evaluates design candidates based on\nsemantic alignment and size compatibility, and optionally applies LLM-guided\nrefinement when needed. We evaluate Lang2Morph across varied tasks, and results\nshow that our approach can generate diverse, task-relevant morphologies. To our\nknowledge, this is the first attempt to develop an LLM-based framework for\ntask-conditioned robotic hand design.", "AI": {"tldr": "Lang2Morph\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u673a\u5668\u4eba\u624b\u5f62\u6001\u8bbe\u8ba1\u6846\u67b6\uff0c\u80fd\u591f\u5c06\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u63cf\u8ff0\u8f6c\u5316\u4e3a\u53ef3D\u6253\u5370\u7684\u7279\u5b9a\u4efb\u52a1\u5f62\u6001\u7ed3\u6784\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u624b\u8bbe\u8ba1\u4f9d\u8d56\u4e13\u5bb6\u542f\u53d1\u5f0f\u548c\u624b\u52a8\u8c03\u53c2\uff0c\u81ea\u52a8\u5316\u65b9\u6cd5\u8ba1\u7b97\u5bc6\u96c6\u4e14\u4f9d\u8d56\u4eff\u771f\u3002\u5927\u8bed\u8a00\u6a21\u578b\u5177\u6709\u5e7f\u6cdb\u7684\u4eba\u7c7b-\u7269\u4f53\u4ea4\u4e92\u77e5\u8bc6\u548c\u5f3a\u5927\u751f\u6210\u80fd\u529b\uff0c\u4e3a\u96f6\u6837\u672c\u8bbe\u8ba1\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "method": "Lang2Morph\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u6a21\u5757\uff1a(i)\u5f62\u6001\u8bbe\u8ba1\uff1a\u5c06\u4efb\u52a1\u6620\u5c04\u4e3a\u8bed\u4e49\u6807\u7b7e\u3001\u7ed3\u6784\u8bed\u6cd5\u548cOPH\u517c\u5bb9\u53c2\u6570\uff1b(ii)\u9009\u62e9\u4e0e\u4f18\u5316\uff1a\u57fa\u4e8e\u8bed\u4e49\u5bf9\u9f50\u548c\u5c3a\u5bf8\u517c\u5bb9\u6027\u8bc4\u4f30\u8bbe\u8ba1\u5019\u9009\uff0c\u5e76\u5728\u9700\u8981\u65f6\u5e94\u7528LLM\u5f15\u5bfc\u7684\u4f18\u5316\u3002", "result": "\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u591a\u6837\u5316\u4e14\u4e0e\u4efb\u52a1\u76f8\u5173\u7684\u5f62\u6001\u7ed3\u6784\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u57fa\u4e8eLLM\u7684\u4efb\u52a1\u6761\u4ef6\u5316\u673a\u5668\u4eba\u624b\u8bbe\u8ba1\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u8bed\u8a00\u9a71\u52a8\u8bbe\u8ba1\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2509.18868", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18868", "abs": "https://arxiv.org/abs/2509.18868", "authors": ["Dianxing Zhang", "Wendong Li", "Kani Song", "Jiaye Lu", "Gang Li", "Liuchun Yang", "Sheng Li"], "title": "Memory in Large Language Models: Mechanisms, Evaluation and Evolution", "comment": "50 pages, 1 figure, 8 tables This is a survey/framework paper on LLM\n  memory mechanisms and evaluation", "summary": "Under a unified operational definition, we define LLM memory as a persistent\nstate written during pretraining, finetuning, or inference that can later be\naddressed and that stably influences outputs. We propose a four-part taxonomy\n(parametric, contextual, external, procedural/episodic) and a memory quadruple\n(location, persistence, write/access path, controllability). We link mechanism,\nevaluation, and governance via the chain write -> read -> inhibit/update. To\navoid distorted comparisons across heterogeneous setups, we adopt a\nthree-setting protocol (parametric only, offline retrieval, online retrieval)\nthat decouples capability from information availability on the same data and\ntimeline. On this basis we build a layered evaluation: parametric (closed-book\nrecall, edit differential, memorization/privacy), contextual (position curves\nand the mid-sequence drop), external (answer correctness vs snippet\nattribution/faithfulness), and procedural/episodic (cross-session consistency\nand timeline replay, E MARS+). The framework integrates temporal governance and\nleakage auditing (freshness hits, outdated answers, refusal slices) and\nuncertainty reporting via inter-rater agreement plus paired tests with\nmultiple-comparison correction. For updating and forgetting, we present DMM\nGov: coordinating DAPT/TAPT, PEFT, model editing (ROME, MEND, MEMIT, SERAC),\nand RAG to form an auditable loop covering admission thresholds, rollout,\nmonitoring, rollback, and change audits, with specs for timeliness, conflict\nhandling, and long-horizon consistency. Finally, we give four testable\npropositions: minimum identifiability; a minimal evaluation card; causally\nconstrained editing with verifiable forgetting; and when retrieval with\nsmall-window replay outperforms ultra-long-context reading. This yields a\nreproducible, comparable, and governable coordinate system for research and\ndeployment.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684LLM\u8bb0\u5fc6\u5b9a\u4e49\u548c\u56db\u90e8\u5206\u5206\u7c7b\u6cd5\uff08\u53c2\u6570\u5316\u3001\u4e0a\u4e0b\u6587\u3001\u5916\u90e8\u3001\u7a0b\u5e8f\u6027/\u60c5\u666f\u6027\uff09\uff0c\u5efa\u7acb\u8bb0\u5fc6\u56db\u5143\u7ec4\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u8bbe\u7f6e\u534f\u8bae\u5b9e\u73b0\u53ef\u6bd4\u8f83\u8bc4\u4f30\uff0c\u5e76\u5f00\u53d1\u5206\u5c42\u8bc4\u4f30\u65b9\u6cd5\u548cDMM Gov\u6cbb\u7406\u6846\u67b6\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3LLM\u8bb0\u5fc6\u7814\u7a76\u4e2d\u7684\u5b9a\u4e49\u4e0d\u4e00\u81f4\u3001\u8bc4\u4f30\u65b9\u6cd5\u4e0d\u53ef\u6bd4\u3001\u7f3a\u4e4f\u6cbb\u7406\u6846\u67b6\u7b49\u95ee\u9898\uff0c\u6784\u5efa\u4e00\u4e2a\u53ef\u91cd\u73b0\u3001\u53ef\u6bd4\u8f83\u3001\u53ef\u6cbb\u7406\u7684\u5750\u6807\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u7edf\u4e00\u7684\u8bb0\u5fc6\u5b9a\u4e49\u548c\u56db\u90e8\u5206\u5206\u7c7b\u6cd5\uff0c\u63d0\u51fa\u8bb0\u5fc6\u56db\u5143\u7ec4\uff08\u4f4d\u7f6e\u3001\u6301\u4e45\u6027\u3001\u5199\u5165/\u8bbf\u95ee\u8def\u5f84\u3001\u53ef\u63a7\u6027\uff09\uff0c\u4f7f\u7528\u4e09\u8bbe\u7f6e\u8bc4\u4f30\u534f\u8bae\uff08\u4ec5\u53c2\u6570\u5316\u3001\u79bb\u7ebf\u68c0\u7d22\u3001\u5728\u7ebf\u68c0\u7d22\uff09\uff0c\u5f00\u53d1\u5206\u5c42\u8bc4\u4f30\u4f53\u7cfb\u548cDMM Gov\u6cbb\u7406\u6846\u67b6\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684LLM\u8bb0\u5fc6\u5206\u6790\u6846\u67b6\uff0c\u5305\u62ec\u5b9a\u4e49\u3001\u5206\u7c7b\u3001\u8bc4\u4f30\u534f\u8bae\u3001\u6cbb\u7406\u673a\u5236\u548c\u56db\u4e2a\u53ef\u6d4b\u8bd5\u547d\u9898\uff0c\u4e3a\u7814\u7a76\u548c\u90e8\u7f72\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\u8bba\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aLLM\u8bb0\u5fc6\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u91cd\u73b0\u3001\u53ef\u6bd4\u8f83\u548c\u53ef\u6cbb\u7406\u7684\u5750\u6807\u7cfb\u7edf\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u6807\u51c6\u5316\u53d1\u5c55\u548c\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2509.18953", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18953", "abs": "https://arxiv.org/abs/2509.18953", "authors": ["Hanqing Liu", "Jiahuan Long", "Junqi Wu", "Jiacheng Hou", "Huili Tang", "Tingsong Jiang", "Weien Zhou", "Wen Yao"], "title": "Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations", "comment": null, "summary": "Vision-Language-Action (VLA) models have emerged as promising solutions for\nrobotic manipulation, yet their robustness to real-world physical variations\nremains critically underexplored. To bridge this gap, we propose Eva-VLA, the\nfirst unified framework that systematically evaluates the robustness of VLA\nmodels by transforming discrete physical variations into continuous\noptimization problems. However, comprehensively assessing VLA robustness\npresents two key challenges: (1) how to systematically characterize diverse\nphysical variations encountered in real-world deployments while maintaining\nevaluation reproducibility, and (2) how to discover worst-case scenarios\nwithout prohibitive real-world data collection costs efficiently. To address\nthe first challenge, we decompose real-world variations into three critical\ndomains: object 3D transformations that affect spatial reasoning, illumination\nvariations that challenge visual perception, and adversarial patches that\ndisrupt scene understanding. For the second challenge, we introduce a\ncontinuous black-box optimization framework that transforms discrete physical\nvariations into parameter optimization, enabling systematic exploration of\nworst-case scenarios. Extensive experiments on state-of-the-art OpenVLA models\nacross multiple benchmarks reveal alarming vulnerabilities: all variation types\ntrigger failure rates exceeding 60%, with object transformations causing up to\n97.8% failure in long-horizon tasks. Our findings expose critical gaps between\ncontrolled laboratory success and unpredictable deployment readiness, while the\nEva-VLA framework provides a practical pathway for hardening VLA-based robotic\nmanipulation models against real-world deployment challenges.", "AI": {"tldr": "Eva-VLA\u662f\u9996\u4e2a\u7cfb\u7edf\u8bc4\u4f30\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u7269\u7406\u53d8\u5316\u4e0b\u9c81\u68d2\u6027\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u79bb\u6563\u7269\u7406\u53d8\u5316\u8f6c\u5316\u4e3a\u8fde\u7eed\u4f18\u5316\u95ee\u9898\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u5b58\u5728\u4e25\u91cd\u8106\u5f31\u6027\u3002", "motivation": "\u5f53\u524dVLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u5bf9\u771f\u5b9e\u4e16\u754c\u7269\u7406\u53d8\u5316\u7684\u9c81\u68d2\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\u6765\u5f25\u5408\u5b9e\u9a8c\u5ba4\u6210\u529f\u4e0e\u90e8\u7f72\u51c6\u5907\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u5c06\u771f\u5b9e\u4e16\u754c\u53d8\u5316\u5206\u89e3\u4e3a\u4e09\u4e2a\u5173\u952e\u9886\u57df\uff1a\u7269\u4f533D\u53d8\u6362\u3001\u5149\u7167\u53d8\u5316\u548c\u5bf9\u6297\u6027\u8865\u4e01\uff1b\u5f15\u5165\u8fde\u7eed\u9ed1\u76d2\u4f18\u5316\u6846\u67b6\u5c06\u79bb\u6563\u7269\u7406\u53d8\u5316\u8f6c\u5316\u4e3a\u53c2\u6570\u4f18\u5316\u95ee\u9898\uff0c\u7cfb\u7edf\u63a2\u7d22\u6700\u574f\u60c5\u51b5\u573a\u666f\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5bf9\u6700\u5148\u8fdb\u7684OpenVLA\u6a21\u578b\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u53d1\u73b0\u6240\u6709\u53d8\u5316\u7c7b\u578b\u90fd\u5bfc\u81f4\u8d85\u8fc760%\u7684\u5931\u8d25\u7387\uff0c\u7269\u4f53\u53d8\u6362\u5728\u957f\u65f6\u57df\u4efb\u52a1\u4e2d\u5bfc\u81f4\u9ad8\u8fbe97.8%\u7684\u5931\u8d25\u7387\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u53d7\u63a7\u5b9e\u9a8c\u5ba4\u6210\u529f\u4e0e\u4e0d\u53ef\u9884\u6d4b\u90e8\u7f72\u51c6\u5907\u4e4b\u95f4\u7684\u5173\u952e\u5dee\u8ddd\uff0cEva-VLA\u6846\u67b6\u4e3a\u589e\u5f3aVLA\u57fa\u673a\u5668\u4eba\u64cd\u4f5c\u6a21\u578b\u5bf9\u6297\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u6311\u6218\u63d0\u4f9b\u4e86\u5b9e\u7528\u9014\u5f84\u3002"}}
{"id": "2509.18883", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18883", "abs": "https://arxiv.org/abs/2509.18883", "authors": ["Meituan LongCat Team", "Anchun Gui", "Bei Li", "Bingyang Tao", "Bole Zhou", "Borun Chen", "Chao Zhang", "Chao Zhang", "Chengcheng Han", "Chenhui Yang", "Chi Zhang", "Chong Peng", "Chuyu Zhang", "Cong Chen", "Fengcun Li", "Gang Xu", "Guoyuan Lin", "Hao Jiang", "Hao Liang", "Haomin Fu", "Haoxiang Ma", "Hong Liu", "Hongyan Hao", "Hongyin Tang", "Hongyu Zang", "Hongzhi Ni", "Hui Su", "Jiahao Liu", "Jiahuan Li", "Jialin Liu", "Jianfei Zhang", "Jianhao Xu", "Jianing Wang", "Jiaqi Sun", "Jiaqi Zhang", "Jiarong Shi", "Jiawei Yang", "Jingang Wang", "Jinrui Ding", "Jun Kuang", "Jun Xu", "Ke He", "Kefeng Zhang", "Keheng Wang", "Keqing He", "Li Wei", "Liang Shi", "Lin Qiu", "Lingbin Kong", "Lingchuan Liu", "Linsen Guo", "Longfei An", "Mai Xia", "Meng Zhou", "Mengshen Zhu", "Peng Pei", "Pengcheng Jia", "Qi Gu", "Qi Guo", "Qiong Huang", "Quan Chen", "Quanchi Weng", "Rongxiang Weng", "Ruichen Shao", "Rumei Li", "Shanglin Lei", "Shuai Du", "Shuaikang Liu", "Shuang Zhou", "Shuhao Hu", "Siyu Xu", "Songshan Gong", "Tao Liang", "Tianhao Hu", "Wei He", "Wei Shi", "Wei Wang", "Wei Wu", "Wei Zhuo", "Weifeng Tang", "Wenjie Shi", "Wenlong Zhu", "Xi Su", "Xiangcheng Liu", "Xiangyu Xi", "Xiangzhou Huang", "Xiao Liu", "Xiaochen Jiang", "Xiaowei Shi", "Xiaowen Shi", "Xiaoyu Li", "Xin Chen", "Xinyue Zhao", "Xuan Huang", "Xuemiao Zhang", "Xuezhi Cao", "Xunliang Cai", "Yajie Zhang", "Yang Chen", "Yang Liu", "Yang Liu", "Yang Zheng", "Yaoming Wang", "Yaqi Huo", "Yerui Sun", "Yifan Lu", "Yiyang Li", "Youshao Xiao", "Yuanzhe Lei", "Yuchen Xie", "Yueqing Sun", "Yufei Zhang", "Yuhuai Wei", "Yulei Qian", "Yunke Zhao", "Yuqing Ding", "Yuwei Jiang", "Zhaohua Yang", "Zhengyu Chen", "Zhijian Liu", "Zhikang Xia", "Zhongda Su", "Ziran Li", "Ziwen Wang", "Ziyuan Zhuang", "Zongyu Wang", "Zunyuan Yang"], "title": "LongCat-Flash-Thinking Technical Report", "comment": null, "summary": "We present LongCat-Flash-Thinking, an efficient 560-billion-parameter\nopen-source Mixture-of-Experts (MoE) reasoning model. Its advanced capabilities\nare cultivated through a meticulously crafted training process, beginning with\nlong Chain-of-Thought (CoT) data cold-start and culminating in large-scale\nReinforcement Learning (RL). We first employ a well-designed cold-start\ntraining strategy, which significantly enhances the reasoning potential and\nequips the model with specialized skills in both formal and agentic reasoning.\nThen, a core innovation is our domain-parallel training scheme, which decouples\noptimization across distinct domains (e.g., STEM, Code, Agentic) and\nsubsequently fuses the resulting expert models into a single, nearly\nPareto-optimal model. This entire process is powered by our Dynamic\nORchestration for Asynchronous rollout (DORA) system, a large-scale RL\nframework that delivers a greater than threefold training speedup over\nsynchronous methods on tens of thousands of accelerators. As a result,\nLongCat-Flash-Thinking achieves state-of-the-art performance among open-source\nmodels on a suite of complex reasoning tasks. The model exhibits exceptional\nefficiency in agentic reasoning, reducing average token consumption by 64.5%\n(from 19, 653 to 6, 965) on AIME-25, without degrading task accuracy. We\nrelease LongCat-Flash-Thinking to promote further advances in reasoning systems\nand agentic AI research.", "AI": {"tldr": "LongCat-Flash-Thinking\u662f\u4e00\u4e2a5600\u4ebf\u53c2\u6570\u7684\u5f00\u653e\u6e90\u4ee3\u7801\u6df7\u5408\u4e13\u5bb6\u63a8\u7406\u6a21\u578b\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8bad\u7ec3\u6d41\u7a0b\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u80fd\u529b\uff0c\u5305\u62ec\u957f\u94fe\u601d\u7ef4\u6570\u636e\u51b7\u542f\u52a8\u548c\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u9ad8\u6548\u5904\u7406\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u5f00\u653e\u6e90\u4ee3\u7801\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728STEM\u3001\u4ee3\u7801\u548c\u667a\u80fd\u4f53\u63a8\u7406\u7b49\u9886\u57df\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528\u51b7\u542f\u52a8\u8bad\u7ec3\u7b56\u7565\u589e\u5f3a\u63a8\u7406\u6f5c\u529b\uff0c\u7136\u540e\u901a\u8fc7\u9886\u57df\u5e76\u884c\u8bad\u7ec3\u65b9\u6848\u5c06\u4e0d\u540c\u9886\u57df\u7684\u4e13\u5bb6\u6a21\u578b\u878d\u5408\u6210\u4e00\u4e2a\u8fd1\u4e4e\u5e15\u7d2f\u6258\u6700\u4f18\u7684\u5355\u4e00\u6a21\u578b\uff0c\u4f7f\u7528DORA\u7cfb\u7edf\u8fdb\u884c\u5927\u89c4\u6a21RL\u8bad\u7ec3\u3002", "result": "\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u8fbe\u5230\u5f00\u6e90\u6a21\u578b\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728AIME-25\u4e0a\u667a\u80fd\u4f53\u63a8\u7406\u7684\u5e73\u5747token\u6d88\u8017\u51cf\u5c1164.5%\uff0c\u4ece19,653\u964d\u81f36,965\uff0c\u4e14\u4efb\u52a1\u51c6\u786e\u7387\u4e0d\u4e0b\u964d\u3002", "conclusion": "LongCat-Flash-Thinking\u5c55\u793a\u4e86\u5728\u63a8\u7406\u7cfb\u7edf\u548c\u667a\u80fd\u4f53AI\u7814\u7a76\u65b9\u9762\u7684\u663e\u8457\u8fdb\u5c55\uff0c\u6a21\u578b\u5df2\u516c\u5f00\u53d1\u5e03\u4ee5\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2509.18954", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18954", "abs": "https://arxiv.org/abs/2509.18954", "authors": ["Minoo Dolatabadi", "Fardin Ayar", "Ehsan Javanmardi", "Manabu Tsukada", "Mahdi Javanmardi"], "title": "Towards Robust LiDAR Localization: Deep Learning-based Uncertainty Estimation", "comment": null, "summary": "LiDAR-based localization and SLAM often rely on iterative matching\nalgorithms, particularly the Iterative Closest Point (ICP) algorithm, to align\nsensor data with pre-existing maps or previous scans. However, ICP is prone to\nerrors in featureless environments and dynamic scenes, leading to inaccurate\npose estimation. Accurately predicting the uncertainty associated with ICP is\ncrucial for robust state estimation but remains challenging, as existing\napproaches often rely on handcrafted models or simplified assumptions.\nMoreover, a few deep learning-based methods for localizability estimation\neither depend on a pre-built map, which may not always be available, or provide\na binary classification of localizable versus non-localizable, which fails to\nproperly model uncertainty. In this work, we propose a data-driven framework\nthat leverages deep learning to estimate the registration error covariance of\nICP before matching, even in the absence of a reference map. By associating\neach LiDAR scan with a reliable 6-DoF error covariance estimate, our method\nenables seamless integration of ICP within Kalman filtering, enhancing\nlocalization accuracy and robustness. Extensive experiments on the KITTI\ndataset demonstrate the effectiveness of our approach, showing that it\naccurately predicts covariance and, when applied to localization using a\npre-built map or SLAM, reduces localization errors and improves robustness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728ICP\u5339\u914d\u524d\u4f30\u8ba1\u6ce8\u518c\u8bef\u5dee\u534f\u65b9\u5dee\uff0c\u65e0\u9700\u53c2\u8003\u5730\u56fe\u5373\u53ef\u63d0\u4f9b\u53ef\u9760\u76846\u81ea\u7531\u5ea6\u8bef\u5dee\u534f\u65b9\u5dee\u4f30\u8ba1\uff0c\u4ece\u800c\u63d0\u5347LiDAR\u5b9a\u4f4d\u548cSLAM\u7684\u7cbe\u5ea6\u4e0e\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edfICP\u7b97\u6cd5\u5728\u7279\u5f81\u7f3a\u5931\u73af\u5883\u548c\u52a8\u6001\u573a\u666f\u4e2d\u5bb9\u6613\u4ea7\u751f\u8bef\u5dee\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u9884\u5efa\u5730\u56fe\uff0c\u8981\u4e48\u53ea\u80fd\u8fdb\u884c\u4e8c\u5143\u5206\u7c7b\uff0c\u65e0\u6cd5\u51c6\u786e\u5efa\u6a21\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u91c7\u7528\u6570\u636e\u9a71\u52a8\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u5728ICP\u5339\u914d\u524d\u9884\u6d4b\u6ce8\u518c\u8bef\u5dee\u534f\u65b9\u5dee\uff0c\u5c06\u6bcf\u4e2aLiDAR\u626b\u63cf\u4e0e6\u81ea\u7531\u5ea6\u8bef\u5dee\u534f\u65b9\u5dee\u4f30\u8ba1\u5173\u8054\uff0c\u5b9e\u73b0\u4e0e\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5728KITTI\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u51c6\u786e\u9884\u6d4b\u534f\u65b9\u5dee\uff0c\u5e94\u7528\u4e8e\u9884\u5efa\u5730\u56fe\u5b9a\u4f4d\u6216SLAM\u65f6\uff0c\u80fd\u51cf\u5c11\u5b9a\u4f4d\u8bef\u5dee\u5e76\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aICP\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u8bef\u5dee\u534f\u65b9\u5dee\u4f30\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86LiDAR\u5b9a\u4f4d\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2509.18905", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18905", "abs": "https://arxiv.org/abs/2509.18905", "authors": ["Songsong Yu", "Yuxin Chen", "Hao Ju", "Lianjie Jia", "Fuxi Zhang", "Shaofei Huang", "Yuhan Wu", "Rundi Cui", "Binghao Ran", "Zaibin Zhang", "Zhedong Zheng", "Zhipeng Zhang", "Yifan Wang", "Lin Song", "Lijun Wang", "Yanwei Li", "Ying Shan", "Huchuan Lu"], "title": "How Far are VLMs from Visual Spatial Intelligence? A Benchmark-Driven Perspective", "comment": "a comprehensive visual spatial reasoning evaluation tool, 25 pages,\n  16 figures", "summary": "Visual Spatial Reasoning (VSR) is a core human cognitive ability and a\ncritical requirement for advancing embodied intelligence and autonomous\nsystems. Despite recent progress in Vision-Language Models (VLMs), achieving\nhuman-level VSR remains highly challenging due to the complexity of\nrepresenting and reasoning over three-dimensional space. In this paper, we\npresent a systematic investigation of VSR in VLMs, encompassing a review of\nexisting methodologies across input modalities, model architectures, training\nstrategies, and reasoning mechanisms. Furthermore, we categorize spatial\nintelligence into three levels of capability, ie, basic perception, spatial\nunderstanding, spatial planning, and curate SIBench, a spatial intelligence\nbenchmark encompassing nearly 20 open-source datasets across 23 task settings.\nExperiments with state-of-the-art VLMs reveal a pronounced gap between\nperception and reasoning, as models show competence in basic perceptual tasks\nbut consistently underperform in understanding and planning tasks, particularly\nin numerical estimation, multi-view reasoning, temporal dynamics, and spatial\nimagination. These findings underscore the substantial challenges that remain\nin achieving spatial intelligence, while providing both a systematic roadmap\nand a comprehensive benchmark to drive future research in the field. The\nrelated resources of this study are accessible at\nhttps://sibench.github.io/Awesome-Visual-Spatial-Reasoning/.", "AI": {"tldr": "\u672c\u6587\u5bf9\u89c6\u89c9\u7a7a\u95f4\u63a8\u7406\uff08VSR\uff09\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u7a7a\u95f4\u667a\u80fd\u7684\u4e09\u7ea7\u80fd\u529b\u5206\u7c7b\uff0c\u5e76\u521b\u5efa\u4e86SIBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u611f\u77e5\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7406\u89e3\u548c\u89c4\u5212\u4efb\u52a1\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "motivation": "\u89c6\u89c9\u7a7a\u95f4\u63a8\u7406\u662f\u4eba\u7c7b\u6838\u5fc3\u8ba4\u77e5\u80fd\u529b\uff0c\u5bf9\u63a8\u8fdb\u5177\u8eab\u667a\u80fd\u548c\u81ea\u4e3b\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u7531\u4e8e\u4e09\u7ef4\u7a7a\u95f4\u8868\u793a\u548c\u63a8\u7406\u7684\u590d\u6742\u6027\uff0c\u5b9e\u73b0\u4eba\u7c7b\u6c34\u5e73\u7684VSR\u4ecd\u6781\u5177\u6311\u6218\u3002", "method": "\u7cfb\u7edf\u8c03\u67e5VSR\u5728VLMs\u4e2d\u7684\u5e94\u7528\uff0c\u5305\u62ec\u8f93\u5165\u6a21\u6001\u3001\u6a21\u578b\u67b6\u6784\u3001\u8bad\u7ec3\u7b56\u7565\u548c\u63a8\u7406\u673a\u5236\u7684\u7efc\u8ff0\uff1b\u5c06\u7a7a\u95f4\u667a\u80fd\u5206\u4e3a\u57fa\u7840\u611f\u77e5\u3001\u7a7a\u95f4\u7406\u89e3\u548c\u7a7a\u95f4\u89c4\u5212\u4e09\u4e2a\u80fd\u529b\u7ea7\u522b\uff1b\u6784\u5efaSIBench\u57fa\u51c6\uff0c\u6db5\u76d623\u4e2a\u4efb\u52a1\u8bbe\u7f6e\u7684\u8fd120\u4e2a\u5f00\u6e90\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u6700\u5148\u8fdb\u7684VLMs\u5728\u611f\u77e5\u548c\u63a8\u7406\u4e4b\u95f4\u5b58\u5728\u660e\u663e\u5dee\u8ddd\uff1a\u6a21\u578b\u5728\u57fa\u7840\u611f\u77e5\u4efb\u52a1\u4e0a\u6709\u80fd\u529b\uff0c\u4f46\u5728\u7406\u89e3\u548c\u89c4\u5212\u4efb\u52a1\u4e0a\u6301\u7eed\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u6570\u503c\u4f30\u8ba1\u3001\u591a\u89c6\u56fe\u63a8\u7406\u3001\u65f6\u95f4\u52a8\u6001\u548c\u7a7a\u95f4\u60f3\u8c61\u65b9\u9762\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u5b9e\u73b0\u7a7a\u95f4\u667a\u80fd\u4ecd\u9762\u4e34\u7684\u91cd\u5927\u6311\u6218\uff0c\u540c\u65f6\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u8def\u7ebf\u56fe\u548c\u5168\u9762\u57fa\u51c6\u3002"}}
{"id": "2509.18979", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18979", "abs": "https://arxiv.org/abs/2509.18979", "authors": ["Lorenzo Shaikewitz", "Tim Nguyen", "Luca Carlone"], "title": "Category-Level Object Shape and Pose Estimation in Less Than a Millisecond", "comment": null, "summary": "Object shape and pose estimation is a foundational robotics problem,\nsupporting tasks from manipulation to scene understanding and navigation. We\npresent a fast local solver for shape and pose estimation which requires only\ncategory-level object priors and admits an efficient certificate of global\noptimality. Given an RGB-D image of an object, we use a learned front-end to\ndetect sparse, category-level semantic keypoints on the target object. We\nrepresent the target object's unknown shape using a linear active shape model\nand pose a maximum a posteriori optimization problem to solve for position,\norientation, and shape simultaneously. Expressed in unit quaternions, this\nproblem admits first-order optimality conditions in the form of an eigenvalue\nproblem with eigenvector nonlinearities. Our primary contribution is to solve\nthis problem efficiently with self-consistent field iteration, which only\nrequires computing a 4-by-4 matrix and finding its minimum eigenvalue-vector\npair at each iterate. Solving a linear system for the corresponding Lagrange\nmultipliers gives a simple global optimality certificate. One iteration of our\nsolver runs in about 100 microseconds, enabling fast outlier rejection. We test\nour method on synthetic data and a variety of real-world settings, including\ntwo public datasets and a drone tracking scenario. Code is released at\nhttps://github.com/MIT-SPARK/Fast-ShapeAndPose.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5feb\u901f\u7684\u7269\u4f53\u5f62\u72b6\u548c\u59ff\u6001\u4f30\u8ba1\u5c40\u90e8\u6c42\u89e3\u5668\uff0c\u4ec5\u9700\u7c7b\u522b\u7ea7\u5148\u9a8c\u77e5\u8bc6\uff0c\u5e76\u80fd\u63d0\u4f9b\u5168\u5c40\u6700\u4f18\u6027\u8bc1\u660e\u3002\u901a\u8fc7RGB-D\u56fe\u50cf\u68c0\u6d4b\u8bed\u4e49\u5173\u952e\u70b9\uff0c\u4f7f\u7528\u7ebf\u6027\u4e3b\u52a8\u5f62\u72b6\u6a21\u578b\u8868\u793a\u672a\u77e5\u5f62\u72b6\uff0c\u901a\u8fc7\u6700\u5927\u540e\u9a8c\u4f18\u5316\u540c\u65f6\u6c42\u89e3\u4f4d\u7f6e\u3001\u65b9\u5411\u548c\u5f62\u72b6\u3002", "motivation": "\u7269\u4f53\u5f62\u72b6\u548c\u59ff\u6001\u4f30\u8ba1\u662f\u673a\u5668\u4eba\u5b66\u7684\u57fa\u7840\u95ee\u9898\uff0c\u652f\u6301\u4ece\u64cd\u4f5c\u5230\u573a\u666f\u7406\u89e3\u548c\u5bfc\u822a\u7b49\u4efb\u52a1\u3002\u9700\u8981\u5f00\u53d1\u5feb\u901f\u4e14\u80fd\u4fdd\u8bc1\u5168\u5c40\u6700\u4f18\u6027\u7684\u6c42\u89e3\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5b66\u4e60\u524d\u7aef\u68c0\u6d4b\u7c7b\u522b\u7ea7\u8bed\u4e49\u5173\u952e\u70b9\uff0c\u7528\u7ebf\u6027\u4e3b\u52a8\u5f62\u72b6\u6a21\u578b\u8868\u793a\u7269\u4f53\u5f62\u72b6\uff0c\u6784\u5efa\u6700\u5927\u540e\u9a8c\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u6d3d\u573a\u8fed\u4ee3\u6c42\u89e3\u7279\u5f81\u503c\u95ee\u9898\uff0c\u6bcf\u6b21\u8fed\u4ee3\u4ec5\u9700\u8ba1\u7b974\u00d74\u77e9\u9635\u7684\u6700\u5c0f\u7279\u5f81\u503c-\u7279\u5f81\u5411\u91cf\u5bf9\u3002", "result": "\u6c42\u89e3\u5668\u6bcf\u6b21\u8fed\u4ee3\u7ea6\u9700100\u5fae\u79d2\uff0c\u80fd\u5feb\u901f\u5254\u9664\u5f02\u5e38\u503c\u3002\u5728\u5408\u6210\u6570\u636e\u3001\u771f\u5b9e\u573a\u666f\u3001\u516c\u5f00\u6570\u636e\u96c6\u548c\u65e0\u4eba\u673a\u8ddf\u8e2a\u573a\u666f\u4e2d\u8fdb\u884c\u4e86\u6d4b\u8bd5\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5f62\u72b6\u548c\u59ff\u6001\u4f30\u8ba1\uff0c\u63d0\u4f9b\u4e86\u7b80\u5355\u7684\u5168\u5c40\u6700\u4f18\u6027\u8bc1\u660e\uff0c\u5728\u591a\u79cd\u5e94\u7528\u573a\u666f\u4e2d\u8868\u73b0\u826f\u597d\u3002"}}
{"id": "2509.18942", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18942", "abs": "https://arxiv.org/abs/2509.18942", "authors": ["Xiao Han", "Zimo Zhao", "Wanyu Wang", "Maolin Wang", "Zitao Liu", "Yi Chang", "Xiangyu Zhao"], "title": "Data Efficient Adaptation in Large Language Models via Continuous Low-Rank Fine-Tuning", "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have emphasized the\ncritical role of fine-tuning (FT) techniques in adapting LLMs to specific\ntasks, especially when retraining from scratch is computationally infeasible.\nFine-tuning enables LLMs to leverage task- or domain-specific data, producing\nmodels that more effectively meet the requirements of targeted applications.\nHowever, con- ventional FT approaches often suffer from catastrophic forgetting\nand suboptimal data efficiency, limiting their real-world applicability. To\naddress these challenges, this paper proposes DEAL, a novel framework that\nintegrates Low-Rank Adapta- tion (LoRA) with a continuous fine-tuning strategy.\nBy incorporating knowledge retention and adaptive parameter update modules, the\nframework mitigates the lim- itations of existing FT methods while maintaining\nefficiency in privacy-preserving settings. Experiments on 15 diverse datasets\nshow that DEAL consistently outper- forms baseline methods, yielding\nsubstantial gains in task accuracy and resource efficiency. These findings\ndemonstrate the potential of our approach to advance continual adaptation in\nLLMs by enhancing task performance while improving resource efficiency.", "AI": {"tldr": "DEAL\u662f\u4e00\u4e2a\u7ed3\u5408\u4f4e\u79e9\u9002\u5e94(LoRA)\u548c\u6301\u7eed\u5fae\u8c03\u7b56\u7565\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u4fdd\u7559\u548c\u81ea\u9002\u5e94\u53c2\u6570\u66f4\u65b0\u6a21\u5757\u89e3\u51b3\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u7684\u707e\u96be\u6027\u9057\u5fd8\u548c\u6570\u636e\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898", "motivation": "\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u548c\u6570\u636e\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9002\u7528\u6027\uff0c\u7279\u522b\u662f\u5728\u4ece\u5934\u8bad\u7ec3\u8ba1\u7b97\u4e0d\u53ef\u884c\u7684\u60c5\u51b5\u4e0b", "method": "\u63d0\u51faDEAL\u6846\u67b6\uff0c\u96c6\u6210\u4f4e\u79e9\u9002\u5e94(LoRA)\u4e0e\u6301\u7eed\u5fae\u8c03\u7b56\u7565\uff0c\u5305\u542b\u77e5\u8bc6\u4fdd\u7559\u548c\u81ea\u9002\u5e94\u53c2\u6570\u66f4\u65b0\u6a21\u5757\uff0c\u5728\u9690\u79c1\u4fdd\u62a4\u8bbe\u7f6e\u4e0b\u4fdd\u6301\u6548\u7387", "result": "\u572815\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDEAL\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u4efb\u52a1\u51c6\u786e\u6027\u548c\u8d44\u6e90\u6548\u7387\u65b9\u9762\u53d6\u5f97\u663e\u8457\u63d0\u5347", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6f5c\u529b\u901a\u8fc7\u589e\u5f3a\u4efb\u52a1\u6027\u80fd\u540c\u65f6\u63d0\u9ad8\u8d44\u6e90\u6548\u7387\u6765\u63a8\u8fdbLLMs\u7684\u6301\u7eed\u9002\u5e94"}}
{"id": "2509.19012", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19012", "abs": "https://arxiv.org/abs/2509.19012", "authors": ["Dapeng Zhang", "Jin Sun", "Chenghui Hu", "Xiaoyan Wu", "Zhenlong Yuan", "Rui Zhou", "Fei Shen", "Qingguo Zhou"], "title": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey", "comment": null, "summary": "The emergence of Vision Language Action (VLA) models marks a paradigm shift\nfrom traditional policy-based control to generalized robotics, reframing Vision\nLanguage Models (VLMs) from passive sequence generators into active agents for\nmanipulation and decision-making in complex, dynamic environments. This survey\ndelves into advanced VLA methods, aiming to provide a clear taxonomy and a\nsystematic, comprehensive review of existing research. It presents a\ncomprehensive analysis of VLA applications across different scenarios and\nclassifies VLA approaches into several paradigms: autoregression-based,\ndiffusion-based, reinforcement-based, hybrid, and specialized methods; while\nexamining their motivations, core strategies, and implementations in detail. In\naddition, foundational datasets, benchmarks, and simulation platforms are\nintroduced. Building on the current VLA landscape, the review further proposes\nperspectives on key challenges and future directions to advance research in VLA\nmodels and generalizable robotics. By synthesizing insights from over three\nhundred recent studies, this survey maps the contours of this rapidly evolving\nfield and highlights the opportunities and challenges that will shape the\ndevelopment of scalable, general-purpose VLA methods.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u8bba\u6587\u7cfb\u7edf\u6027\u5730\u56de\u987e\u4e86\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u63d0\u4f9b\u4e86\u5206\u7c7b\u6846\u67b6\u5e76\u5206\u6790\u4e86300\u591a\u9879\u76f8\u5173\u7814\u7a76\uff0c\u63a2\u8ba8\u4e86VLA\u5728\u901a\u7528\u673a\u5668\u4eba\u9886\u57df\u7684\u5e94\u7528\u524d\u666f\u548c\u6311\u6218\u3002", "motivation": "VLA\u6a21\u578b\u7684\u51fa\u73b0\u6807\u5fd7\u7740\u4ece\u4f20\u7edf\u7b56\u7565\u63a7\u5236\u5411\u901a\u7528\u673a\u5668\u4eba\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ece\u88ab\u52a8\u5e8f\u5217\u751f\u6210\u5668\u8f6c\u53d8\u4e3a\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u4e3b\u52a8\u51b3\u7b56\u548c\u64cd\u4f5c\u7684\u667a\u80fd\u4f53\u3002", "method": "\u8bba\u6587\u91c7\u7528\u7cfb\u7edf\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5c06VLA\u65b9\u6cd5\u5206\u7c7b\u4e3a\u81ea\u56de\u5f52\u3001\u6269\u6563\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u6df7\u5408\u548c\u4e13\u95e8\u65b9\u6cd5\u7b49\u8303\u5f0f\uff0c\u5e76\u8be6\u7ec6\u5206\u6790\u5176\u52a8\u673a\u3001\u6838\u5fc3\u7b56\u7565\u548c\u5b9e\u73b0\u65b9\u5f0f\u3002", "result": "\u63d0\u4f9b\u4e86VLA\u9886\u57df\u7684\u5168\u9762\u5206\u6790\uff0c\u5305\u62ec\u57fa\u7840\u6570\u636e\u96c6\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u4eff\u771f\u5e73\u53f0\u7684\u4ecb\u7ecd\uff0c\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u8be5\u9886\u57df\u7684\u7cfb\u7edf\u8ba4\u77e5\u6846\u67b6\u3002", "conclusion": "\u8bba\u6587\u6307\u51fa\u4e86VLA\u6a21\u578b\u53d1\u5c55\u7684\u5173\u952e\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\uff0c\u5f3a\u8c03\u4e86\u6784\u5efa\u53ef\u6269\u5c55\u3001\u901a\u7528VLA\u65b9\u6cd5\u7684\u673a\u4f1a\u548c\u6311\u6218\uff0c\u5c06\u63a8\u52a8\u901a\u7528\u673a\u5668\u4eba\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.18970", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18970", "abs": "https://arxiv.org/abs/2509.18970", "authors": ["Xixun Lin", "Yucheng Ning", "Jingwen Zhang", "Yan Dong", "Yilong Liu", "Yongxuan Wu", "Xiaohua Qi", "Nan Sun", "Yanmin Shang", "Pengfei Cao", "Lixin Zou", "Xu Chen", "Chuan Zhou", "Jia Wu", "Shirui Pan", "Bin Wang", "Yanan Cao", "Kai Chen", "Songlin Hu", "Li Guo"], "title": "LLM-based Agents Suffer from Hallucinations: A Survey of Taxonomy, Methods, and Directions", "comment": null, "summary": "Driven by the rapid advancements of Large Language Models (LLMs), LLM-based\nagents have emerged as powerful intelligent systems capable of human-like\ncognition, reasoning, and interaction. These agents are increasingly being\ndeployed across diverse real-world applications, including student education,\nscientific research, and financial analysis. However, despite their remarkable\npotential, LLM-based agents remain vulnerable to hallucination issues, which\ncan result in erroneous task execution and undermine the reliability of the\noverall system design. Addressing this critical challenge requires a deep\nunderstanding and a systematic consolidation of recent advances on LLM-based\nagents. To this end, we present the first comprehensive survey of\nhallucinations in LLM-based agents. By carefully analyzing the complete\nworkflow of agents, we propose a new taxonomy that identifies different types\nof agent hallucinations occurring at different stages. Furthermore, we conduct\nan in-depth examination of eighteen triggering causes underlying the emergence\nof agent hallucinations. Through a detailed review of a large number of\nexisting studies, we summarize approaches for hallucination mitigation and\ndetection, and highlight promising directions for future research. We hope this\nsurvey will inspire further efforts toward addressing hallucinations in\nLLM-based agents, ultimately contributing to the development of more robust and\nreliable agent systems.", "AI": {"tldr": "\u672c\u6587\u662f\u7b2c\u4e00\u7bc7\u5173\u4e8eLLM-based agents\u5e7b\u89c9\u95ee\u9898\u7684\u5168\u9762\u7efc\u8ff0\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u5206\u7c7b\u6cd5\u8bc6\u522b\u4e0d\u540c\u9636\u6bb5\u7684\u5e7b\u89c9\u7c7b\u578b\uff0c\u6df1\u5165\u5206\u6790\u4e8618\u79cd\u89e6\u53d1\u539f\u56e0\uff0c\u5e76\u603b\u7ed3\u4e86\u5e7b\u89c9\u7f13\u89e3\u548c\u68c0\u6d4b\u65b9\u6cd5\u3002", "motivation": "LLM-based agents\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u53ef\u80fd\u5bfc\u81f4\u9519\u8bef\u4efb\u52a1\u6267\u884c\u548c\u7cfb\u7edf\u53ef\u9760\u6027\u4e0b\u964d\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u5730\u7406\u89e3\u548c\u6574\u5408\u76f8\u5173\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u901a\u8fc7\u4ed4\u7ec6\u5206\u6790agents\u7684\u5b8c\u6574\u5de5\u4f5c\u6d41\u7a0b\uff0c\u63d0\u51fa\u65b0\u7684\u5206\u7c7b\u6cd5\u8bc6\u522b\u4e0d\u540c\u9636\u6bb5\u7684\u5e7b\u89c9\u7c7b\u578b\uff0c\u6df1\u5165\u5206\u679018\u79cd\u89e6\u53d1\u539f\u56e0\uff0c\u5e76\u7efc\u8ff0\u5927\u91cf\u73b0\u6709\u7814\u7a76\u7684\u5e7b\u89c9\u7f13\u89e3\u548c\u68c0\u6d4b\u65b9\u6cd5\u3002", "result": "\u5efa\u7acb\u4e86\u5168\u9762\u7684LLM-based agents\u5e7b\u89c9\u5206\u7c7b\u4f53\u7cfb\uff0c\u8bc6\u522b\u4e86\u591a\u79cd\u5e7b\u89c9\u7c7b\u578b\u548c\u89e6\u53d1\u673a\u5236\uff0c\u603b\u7ed3\u4e86\u6709\u6548\u7684\u7f13\u89e3\u548c\u68c0\u6d4b\u7b56\u7565\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u4e3aLLM-based agents\u7684\u5e7b\u89c9\u95ee\u9898\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u9c81\u68d2\u53ef\u9760\u7684agent\u7cfb\u7edf\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2509.19023", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19023", "abs": "https://arxiv.org/abs/2509.19023", "authors": ["Shuai Liu", "Meng Cheng Lau"], "title": "Reduced-Order Model-Guided Reinforcement Learning for Demonstration-Free Humanoid Locomotion", "comment": "11 pages, 5 figures, 1 table, Computational Science Graduate Project", "summary": "We introduce Reduced-Order Model-Guided Reinforcement Learning (ROM-GRL), a\ntwo-stage reinforcement learning framework for humanoid walking that requires\nno motion capture data or elaborate reward shaping. In the first stage, a\ncompact 4-DOF (four-degree-of-freedom) reduced-order model (ROM) is trained via\nProximal Policy Optimization. This generates energy-efficient gait templates.\nIn the second stage, those dynamically consistent trajectories guide a\nfull-body policy trained with Soft Actor--Critic augmented by an adversarial\ndiscriminator, ensuring the student's five-dimensional gait feature\ndistribution matches the ROM's demonstrations. Experiments at 1\nmeter-per-second and 4 meter-per-second show that ROM-GRL produces stable,\nsymmetric gaits with substantially lower tracking error than a pure-reward\nbaseline. By distilling lightweight ROM guidance into high-dimensional\npolicies, ROM-GRL bridges the gap between reward-only and imitation-based\nlocomotion methods, enabling versatile, naturalistic humanoid behaviors without\nany human demonstrations.", "AI": {"tldr": "ROM-GRL\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4eba\u5f62\u673a\u5668\u4eba\u884c\u8d70\uff0c\u65e0\u9700\u8fd0\u52a8\u6355\u6349\u6570\u636e\u6216\u590d\u6742\u5956\u52b1\u8bbe\u8ba1\u3002\u901a\u8fc7\u7b80\u5316\u6a21\u578b\u5f15\u5bfc\u9ad8\u7ef4\u7b56\u7565\u8bad\u7ec3\uff0c\u5b9e\u73b0\u7a33\u5b9a\u81ea\u7136\u7684\u6b65\u6001\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u4eba\u5f62\u673a\u5668\u4eba\u884c\u8d70\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u8fd0\u52a8\u6355\u6349\u6570\u636e\u6216\u590d\u6742\u5956\u52b1\u8bbe\u8ba1\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u4e00\u79cd\u65e0\u9700\u4eba\u7c7b\u793a\u8303\u5c31\u80fd\u751f\u6210\u81ea\u7136\u6b65\u6001\u7684\u65b9\u6cd5\u3002", "method": "\u7b2c\u4e00\u9636\u6bb5\uff1a\u4f7f\u7528PPO\u8bad\u7ec34\u81ea\u7531\u5ea6\u7b80\u5316\u6a21\u578b\u751f\u6210\u80fd\u91cf\u9ad8\u6548\u6b65\u6001\u6a21\u677f\uff1b\u7b2c\u4e8c\u9636\u6bb5\uff1a\u4f7f\u7528SAC\u52a0\u5bf9\u6297\u5224\u522b\u5668\u8bad\u7ec3\u5168\u8eab\u7b56\u7565\uff0c\u786e\u4fdd\u6b65\u6001\u7279\u5f81\u5206\u5e03\u4e0e\u7b80\u5316\u6a21\u578b\u4e00\u81f4\u3002", "result": "\u57281m/s\u548c4m/s\u901f\u5ea6\u4e0b\uff0cROM-GRL\u4ea7\u751f\u7a33\u5b9a\u5bf9\u79f0\u7684\u6b65\u6001\uff0c\u8ddf\u8e2a\u8bef\u5dee\u663e\u8457\u4f4e\u4e8e\u7eaf\u5956\u52b1\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ROM-GRL\u901a\u8fc7\u7b80\u5316\u6a21\u578b\u5f15\u5bfc\u586b\u8865\u4e86\u7eaf\u5956\u52b1\u65b9\u6cd5\u548c\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u4eba\u7c7b\u793a\u8303\u7684\u591a\u6837\u5316\u3001\u81ea\u7136\u5316\u4eba\u5f62\u884c\u4e3a\u3002"}}
{"id": "2509.18980", "categories": ["cs.AI", "cs.HC", "cs.IR", "H.3.3; H.5.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.18980", "abs": "https://arxiv.org/abs/2509.18980", "authors": ["Maxime Manderlier", "Fabian Lecron", "Olivier Vu Thanh", "Nicolas Gillis"], "title": "From latent factors to language: a user study on LLM-generated explanations for an inherently interpretable matrix-based recommender system", "comment": null, "summary": "We investigate whether large language models (LLMs) can generate effective,\nuser-facing explanations from a mathematically interpretable recommendation\nmodel. The model is based on constrained matrix factorization, where user types\nare explicitly represented and predicted item scores share the same scale as\nobserved ratings, making the model's internal representations and predicted\nscores directly interpretable. This structure is translated into natural\nlanguage explanations using carefully designed LLM prompts. Many works in\nexplainable AI rely on automatic evaluation metrics, which often fail to\ncapture users' actual needs and perceptions. In contrast, we adopt a\nuser-centered approach: we conduct a study with 326 participants who assessed\nthe quality of the explanations across five key dimensions-transparency,\neffectiveness, persuasion, trust, and satisfaction-as well as the\nrecommendations themselves.To evaluate how different explanation strategies are\nperceived, we generate multiple explanation types from the same underlying\nmodel, varying the input information provided to the LLM. Our analysis reveals\nthat all explanation types are generally well received, with moderate\nstatistical differences between strategies. User comments further underscore\nhow participants react to each type of explanation, offering complementary\ninsights beyond the quantitative results.", "AI": {"tldr": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u5426\u4ece\u6570\u5b66\u53ef\u89e3\u91ca\u7684\u63a8\u8350\u6a21\u578b\u4e2d\u751f\u6210\u6709\u6548\u7684\u7528\u6237\u5bfc\u5411\u89e3\u91ca\uff0c\u901a\u8fc7\u7528\u6237\u7814\u7a76\u8bc4\u4f30\u4e0d\u540c\u89e3\u91ca\u7b56\u7565\u7684\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u53ef\u89e3\u91caAI\u7814\u7a76\u591a\u4f9d\u8d56\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\uff0c\u4f46\u8fd9\u4e9b\u6307\u6807\u5f80\u5f80\u65e0\u6cd5\u6355\u6349\u7528\u6237\u7684\u5b9e\u9645\u9700\u6c42\u548c\u611f\u77e5\uff0c\u56e0\u6b64\u9700\u8981\u91c7\u7528\u7528\u6237\u4e2d\u5fc3\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u89e3\u91ca\u8d28\u91cf\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u7ea6\u675f\u77e9\u9635\u5206\u89e3\u7684\u53ef\u89e3\u91ca\u63a8\u8350\u6a21\u578b\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684LLM\u63d0\u793a\u5c06\u6a21\u578b\u7ed3\u6784\u8f6c\u5316\u4e3a\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff0c\u5e76\u5bf9326\u540d\u53c2\u4e0e\u8005\u8fdb\u884c\u7528\u6237\u7814\u7a76\uff0c\u8bc4\u4f30\u4e94\u79cd\u5173\u952e\u7ef4\u5ea6\u7684\u89e3\u91ca\u8d28\u91cf\u3002", "result": "\u6240\u6709\u89e3\u91ca\u7c7b\u578b\u90fd\u666e\u904d\u53d7\u5230\u597d\u8bc4\uff0c\u4e0d\u540c\u7b56\u7565\u4e4b\u95f4\u5b58\u5728\u9002\u5ea6\u7684\u7edf\u8ba1\u5dee\u5f02\uff0c\u7528\u6237\u8bc4\u8bba\u63d0\u4f9b\u4e86\u5b9a\u91cf\u7ed3\u679c\u4e4b\u5916\u7684\u8865\u5145\u89c1\u89e3\u3002", "conclusion": "LLM\u80fd\u591f\u4ece\u6570\u5b66\u53ef\u89e3\u91ca\u7684\u63a8\u8350\u6a21\u578b\u4e2d\u751f\u6210\u6709\u6548\u7684\u7528\u6237\u5bfc\u5411\u89e3\u91ca\uff0c\u7528\u6237\u4e2d\u5fc3\u8bc4\u4f30\u65b9\u6cd5\u4e3a\u89e3\u91ca\u8d28\u91cf\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u7406\u89e3\u3002"}}
{"id": "2509.19037", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19037", "abs": "https://arxiv.org/abs/2509.19037", "authors": ["Qingzheng Cong", "Steven Oh", "Wen Fan", "Shan Luo", "Kaspar Althoefer", "Dandan Zhang"], "title": "TacEva: A Performance Evaluation Framework For Vision-Based Tactile Sensors", "comment": "14 pages, 8 figures. Equal contribution: Qingzheng Cong, Steven Oh,\n  Wen Fan. Corresponding author: Dandan Zhang (d.zhang17@imperial.ac.uk).\n  Additional resources at http://stevenoh2003.github.io/TacEva/", "summary": "Vision-Based Tactile Sensors (VBTSs) are widely used in robotic tasks because\nof the high spatial resolution they offer and their relatively low\nmanufacturing costs. However, variations in their sensing mechanisms,\nstructural dimension, and other parameters lead to significant performance\ndisparities between existing VBTSs. This makes it challenging to optimize them\nfor specific tasks, as both the initial choice and subsequent fine-tuning are\nhindered by the lack of standardized metrics. To address this issue, TacEva is\nintroduced as a comprehensive evaluation framework for the quantitative\nanalysis of VBTS performance. The framework defines a set of performance\nmetrics that capture key characteristics in typical application scenarios. For\neach metric, a structured experimental pipeline is designed to ensure\nconsistent and repeatable quantification. The framework is applied to multiple\nVBTSs with distinct sensing mechanisms, and the results demonstrate its ability\nto provide a thorough evaluation of each design and quantitative indicators for\neach performance dimension. This enables researchers to pre-select the most\nappropriate VBTS on a task by task basis, while also offering\nperformance-guided insights into the optimization of VBTS design. A list of\nexisting VBTS evaluation methods and additional evaluations can be found on our\nwebsite: https://stevenoh2003.github.io/TacEva/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86TacEva\u6846\u67b6\uff0c\u7528\u4e8e\u5bf9\u89c6\u89c9\u89e6\u89c9\u4f20\u611f\u5668\u8fdb\u884c\u6807\u51c6\u5316\u6027\u80fd\u8bc4\u4f30\uff0c\u89e3\u51b3\u4e86\u73b0\u6709VBTS\u56e0\u53c2\u6570\u5dee\u5f02\u5bfc\u81f4\u7684\u6027\u80fd\u6bd4\u8f83\u56f0\u96be\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u89e6\u89c9\u4f20\u611f\u5668\u5728\u4f20\u611f\u673a\u5236\u3001\u7ed3\u6784\u5c3a\u5bf8\u7b49\u53c2\u6570\u4e0a\u5b58\u5728\u5dee\u5f02\uff0c\u5bfc\u81f4\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff0c\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u6307\u6807\uff0c\u96be\u4ee5\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u8fdb\u884c\u9009\u62e9\u548c\u4f18\u5316\u3002", "method": "\u5f00\u53d1\u4e86TacEva\u8bc4\u4f30\u6846\u67b6\uff0c\u5b9a\u4e49\u4e86\u4e00\u5957\u6027\u80fd\u6307\u6807\u6765\u6355\u6349\u5178\u578b\u5e94\u7528\u573a\u666f\u4e2d\u7684\u5173\u952e\u7279\u5f81\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u6307\u6807\u8bbe\u8ba1\u4e86\u7ed3\u6784\u5316\u7684\u5b9e\u9a8c\u6d41\u7a0b\u4ee5\u786e\u4fdd\u4e00\u81f4\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002", "result": "\u8be5\u6846\u67b6\u5e94\u7528\u4e8e\u591a\u79cd\u4e0d\u540c\u4f20\u611f\u673a\u5236\u7684VBTS\uff0c\u7ed3\u679c\u8868\u660e\u80fd\u591f\u5bf9\u6bcf\u79cd\u8bbe\u8ba1\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u6027\u80fd\u7ef4\u5ea6\u63d0\u4f9b\u5b9a\u91cf\u6307\u6807\u3002", "conclusion": "TacEva\u6846\u67b6\u4f7f\u7814\u7a76\u4eba\u5458\u80fd\u591f\u57fa\u4e8e\u4efb\u52a1\u9700\u6c42\u9884\u5148\u9009\u62e9\u6700\u5408\u9002\u7684VBTS\uff0c\u540c\u65f6\u4e3aVBTS\u8bbe\u8ba1\u7684\u4f18\u5316\u63d0\u4f9b\u6027\u80fd\u6307\u5bfc\u89c1\u89e3\u3002"}}
{"id": "2509.18986", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18986", "abs": "https://arxiv.org/abs/2509.18986", "authors": ["Erik Penther", "Michael Grohs", "Jana-Rebecca Rehse"], "title": "Remaining Time Prediction in Outbound Warehouse Processes: A Case Study (Short Paper)", "comment": "Short paper at the ML4PM Workshop 2025, held in conjunction with the\n  ICPM 2025 in Montevideo, Uruguay", "summary": "Predictive process monitoring is a sub-domain of process mining which aims to\nforecast the future of ongoing process executions. One common prediction target\nis the remaining time, meaning the time that will elapse until a process\nexecution is completed. In this paper, we compare four different remaining time\nprediction approaches in a real-life outbound warehouse process of a logistics\ncompany in the aviation business. For this process, the company provided us\nwith a novel and original event log with 169,523 traces, which we can make\npublicly available. Unsurprisingly, we find that deep learning models achieve\nthe highest accuracy, but shallow methods like conventional boosting techniques\nachieve competitive accuracy and require significantly fewer computational\nresources.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u56db\u79cd\u5269\u4f59\u65f6\u95f4\u9884\u6d4b\u65b9\u6cd5\u5728\u7269\u6d41\u516c\u53f8\u51fa\u5e93\u4ed3\u5e93\u6d41\u7a0b\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u51c6\u786e\u7387\u6700\u9ad8\uff0c\u4f46\u6d45\u5c42\u65b9\u6cd5\u5982\u4f20\u7edf\u63d0\u5347\u6280\u672f\u5728\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u4e0a\u66f4\u9ad8\u6548\u4e14\u5177\u6709\u7ade\u4e89\u529b\u3002", "motivation": "\u9884\u6d4b\u6027\u6d41\u7a0b\u76d1\u63a7\u65e8\u5728\u9884\u6d4b\u6b63\u5728\u8fdb\u884c\u7684\u6d41\u7a0b\u6267\u884c\u7684\u672a\u6765\uff0c\u5176\u4e2d\u5269\u4f59\u65f6\u95f4\u9884\u6d4b\u662f\u4e00\u4e2a\u5e38\u89c1\u76ee\u6807\u3002\u672c\u6587\u65e8\u5728\u5728\u771f\u5b9e\u7269\u6d41\u516c\u53f8\u51fa\u5e93\u4ed3\u5e93\u6d41\u7a0b\u4e2d\u6bd4\u8f83\u4e0d\u540c\u5269\u4f59\u65f6\u95f4\u9884\u6d4b\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "method": "\u5728\u7269\u6d41\u516c\u53f8\u63d0\u4f9b\u7684\u5305\u542b169,523\u6761\u8f68\u8ff9\u7684\u539f\u59cb\u4e8b\u4ef6\u65e5\u5fd7\u4e0a\uff0c\u6bd4\u8f83\u4e86\u56db\u79cd\u5269\u4f59\u65f6\u95f4\u9884\u6d4b\u65b9\u6cd5\uff0c\u5305\u62ec\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u548c\u6d45\u5c42\u65b9\u6cd5\u5982\u4f20\u7edf\u63d0\u5347\u6280\u672f\u3002", "result": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fbe\u5230\u6700\u9ad8\u51c6\u786e\u7387\uff0c\u4f46\u6d45\u5c42\u65b9\u6cd5\u5982\u4f20\u7edf\u63d0\u5347\u6280\u672f\u5177\u6709\u7ade\u4e89\u529b\u7684\u51c6\u786e\u7387\uff0c\u4e14\u9700\u8981\u663e\u8457\u66f4\u5c11\u7684\u8ba1\u7b97\u8d44\u6e90\u3002", "conclusion": "\u867d\u7136\u6df1\u5ea6\u5b66\u4e60\u5728\u5269\u4f59\u65f6\u95f4\u9884\u6d4b\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u6d45\u5c42\u65b9\u6cd5\u5728\u51c6\u786e\u7387\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u5e73\u8861\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u66f4\u5177\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.19047", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19047", "abs": "https://arxiv.org/abs/2509.19047", "authors": ["Geonhyup Lee", "Yeongjin Lee", "Kangmin Kim", "Seongju Lee", "Sangjun Noh", "Seunghyeok Back", "Kyoobin Lee"], "title": "ManipForce: Force-Guided Policy Learning with Frequency-Aware Representation for Contact-Rich Manipulation", "comment": "9 pages, 9 figures", "summary": "Contact-rich manipulation tasks such as precision assembly require precise\ncontrol of interaction forces, yet existing imitation learning methods rely\nmainly on vision-only demonstrations. We propose ManipForce, a handheld system\ndesigned to capture high-frequency force-torque (F/T) and RGB data during\nnatural human demonstrations for contact-rich manipulation. Building on these\ndemonstrations, we introduce the Frequency-Aware Multimodal Transformer (FMT).\nFMT encodes asynchronous RGB and F/T signals using frequency- and\nmodality-aware embeddings and fuses them via bi-directional cross-attention\nwithin a transformer diffusion policy. Through extensive experiments on six\nreal-world contact-rich manipulation tasks - such as gear assembly, box\nflipping, and battery insertion - FMT trained on ManipForce demonstrations\nachieves robust performance with an average success rate of 83% across all\ntasks, substantially outperforming RGB-only baselines. Ablation and\nsampling-frequency analyses further confirm that incorporating high-frequency\nF/T data and cross-modal integration improves policy performance, especially in\ntasks demanding high precision and stable contact.", "AI": {"tldr": "\u63d0\u51fa\u4e86ManipForce\u624b\u6301\u7cfb\u7edf\u6765\u91c7\u96c6\u9ad8\u9891\u529b\u626d\u77e9\u548cRGB\u6570\u636e\uff0c\u5e76\u5f00\u53d1\u4e86\u9891\u7387\u611f\u77e5\u591a\u6a21\u6001\u53d8\u6362\u5668(FMT)\u7528\u4e8e\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u4efb\u52a1\uff0c\u5728\u516d\u4e2a\u771f\u5b9e\u4efb\u52a1\u4e2d\u8fbe\u523083%\u7684\u5e73\u5747\u6210\u529f\u7387\u3002", "motivation": "\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u4efb\u52a1\u9700\u8981\u7cbe\u786e\u63a7\u5236\u4ea4\u4e92\u529b\uff0c\u4f46\u73b0\u6709\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u89c6\u89c9\u6f14\u793a\uff0c\u7f3a\u4e4f\u529b\u626d\u77e9\u6570\u636e\u3002", "method": "\u4f7f\u7528ManipForce\u7cfb\u7edf\u91c7\u96c6\u591a\u6a21\u6001\u6570\u636e\uff0c\u5f00\u53d1FMT\u6a21\u578b\u901a\u8fc7\u9891\u7387\u611f\u77e5\u5d4c\u5165\u548c\u53cc\u5411\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u5f02\u6b65RGB\u548c\u529b\u626d\u77e9\u4fe1\u53f7\uff0c\u57fa\u4e8e\u53d8\u6362\u5668\u6269\u6563\u7b56\u7565\u3002", "result": "\u5728\u9f7f\u8f6e\u88c5\u914d\u3001\u76d2\u5b50\u7ffb\u8f6c\u3001\u7535\u6c60\u63d2\u5165\u7b49\u516d\u4e2a\u4efb\u52a1\u4e2d\uff0cFMT\u8fbe\u523083%\u7684\u5e73\u5747\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u4ec5RGB\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u9ad8\u9891\u529b\u626d\u77e9\u6570\u636e\u548c\u8de8\u6a21\u6001\u96c6\u6210\u80fd\u663e\u8457\u63d0\u5347\u7b56\u7565\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u9ad8\u7cbe\u5ea6\u548c\u7a33\u5b9a\u63a5\u89e6\u7684\u4efb\u52a1\u4e2d\u3002"}}
{"id": "2509.19030", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19030", "abs": "https://arxiv.org/abs/2509.19030", "authors": ["Victoire Herv\u00e9", "Henrik Warpefelt", "Christoph Salge"], "title": "Landmarks, Monuments, and Beacons: Understanding Generative Calls to Action", "comment": null, "summary": "Algorithmic evaluation of procedurally generated content struggles to find\nmetrics that align with human experience, particularly for composite artefacts.\nAutomatic decomposition as a possible solution requires concepts that meet a\nrange of properties. To this end, drawing on Games Studies and Game AI\nresearch, we introduce the nested concepts of \\textit{Landmarks},\n\\textit{Monuments}, and \\textit{Beacons}. These concepts are based on the\nartefact's perceivability, evocativeness, and Call to Action, all from a\nplayer-centric perspective. These terms are generic to games and usable across\ngenres. We argue that these entities can be found and evaluated with techniques\ncurrently used in both research and industry, opening a path towards a fully\nautomated decomposition of PCG, and evaluation of the salient sub-components.\nAlthough the work presented here emphasises mixed-initiative PCG and\ncompositional PCG, we believe it applies beyond those domains. With this\napproach, we intend to create a connection between humanities and technical\ngame research and allow for better computational PCG evaluation", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u73a9\u5bb6\u89c6\u89d2\u7684Landmarks\u3001Monuments\u548cBeacons\u6982\u5ff5\uff0c\u7528\u4e8e\u81ea\u52a8\u5206\u89e3\u548c\u8bc4\u4f30\u7a0b\u5e8f\u751f\u6210\u5185\u5bb9\u7684\u5b50\u7ec4\u4ef6\uff0c\u4ee5\u6539\u5584\u7b97\u6cd5\u8bc4\u4f30\u4e0e\u4eba\u7c7b\u4f53\u9a8c\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u7b97\u6cd5\u8bc4\u4f30\u7a0b\u5e8f\u751f\u6210\u5185\u5bb9\u65f6\u96be\u4ee5\u627e\u5230\u4e0e\u4eba\u7c7b\u4f53\u9a8c\u4e00\u81f4\u7684\u6307\u6807\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u590d\u5408\u4ea7\u7269\u3002\u9700\u8981\u6ee1\u8db3\u591a\u79cd\u5c5e\u6027\u7684\u6982\u5ff5\u6765\u5b9e\u73b0\u81ea\u52a8\u5206\u89e3\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u53ef\u611f\u77e5\u6027\u3001\u5524\u8d77\u6027\u548c\u884c\u52a8\u53ec\u5524\u7684Landmarks\u3001Monuments\u548cBeacons\u6982\u5ff5\uff0c\u8fd9\u4e9b\u6982\u5ff5\u5177\u6709\u6e38\u620f\u901a\u7528\u6027\uff0c\u53ef\u5229\u7528\u73b0\u6709\u6280\u672f\u8fdb\u884c\u8bc6\u522b\u548c\u8bc4\u4f30\u3002", "result": "\u8fd9\u4e9b\u6982\u5ff5\u4e3a\u5b8c\u5168\u81ea\u52a8\u5316\u7684PCG\u5206\u89e3\u548c\u663e\u8457\u5b50\u7ec4\u4ef6\u8bc4\u4f30\u5f00\u8f9f\u4e86\u8def\u5f84\uff0c\u8fde\u63a5\u4e86\u4eba\u6587\u5b66\u79d1\u548c\u6280\u672f\u6e38\u620f\u7814\u7a76\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u9002\u7528\u4e8e\u6df7\u5408\u4e3b\u52a8PCG\u548c\u7ec4\u5408PCG\uff0c\u8fd8\u53ef\u5e94\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u9886\u57df\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u8ba1\u7b97PCG\u8bc4\u4f30\u3002"}}
{"id": "2509.19076", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19076", "abs": "https://arxiv.org/abs/2509.19076", "authors": ["Laura Connolly", "Aravind S. Kumar", "Kapi Ketan Mehta", "Lidia Al-Zogbi", "Peter Kazanzides", "Parvin Mousavi", "Gabor Fichtinger", "Axel Krieger", "Junichi Tokuda", "Russell H. Taylor", "Simon Leonard", "Anton Deguet"], "title": "SlicerROS2: A Research and Development Module for Image-Guided Robotic Interventions", "comment": null, "summary": "Image-guided robotic interventions involve the use of medical imaging in\ntandem with robotics. SlicerROS2 is a software module that combines 3D Slicer\nand robot operating system (ROS) in pursuit of a standard integration approach\nfor medical robotics research. The first release of SlicerROS2 demonstrated the\nfeasibility of using the C++ API from 3D Slicer and ROS to load and visualize\nrobots in real time. Since this initial release, we've rewritten and redesigned\nthe module to offer greater modularity, access to low-level features, access to\n3D Slicer's Python API, and better data transfer protocols. In this paper, we\nintroduce this new design as well as four applications that leverage the core\nfunctionalities of SlicerROS2 in realistic image-guided robotics scenarios.", "AI": {"tldr": "SlicerROS2\u662f\u4e00\u4e2a\u7ed3\u54083D Slicer\u548cROS\u7684\u8f6f\u4ef6\u6a21\u5757\uff0c\u7528\u4e8e\u533b\u5b66\u673a\u5668\u4eba\u7814\u7a76\u7684\u6807\u51c6\u96c6\u6210\u65b9\u6cd5\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u5176\u91cd\u65b0\u8bbe\u8ba1\u540e\u7684\u65b0\u67b6\u6784\u4ee5\u53ca\u56db\u4e2a\u5e94\u7528\u6848\u4f8b\u3002", "motivation": "\u4e3a\u533b\u5b66\u673a\u5668\u4eba\u7814\u7a76\u63d0\u4f9b\u4e00\u4e2a\u6807\u51c6\u5316\u7684\u96c6\u6210\u65b9\u6cd5\uff0c\u5c063D Slicer\u7684\u533b\u5b66\u5f71\u50cf\u529f\u80fd\u4e0eROS\u7684\u673a\u5668\u4eba\u63a7\u5236\u80fd\u529b\u76f8\u7ed3\u5408\u3002", "method": "\u91cd\u65b0\u8bbe\u8ba1\u548c\u91cd\u5199SlicerROS2\u6a21\u5757\uff0c\u63d0\u9ad8\u6a21\u5757\u5316\u7a0b\u5ea6\uff0c\u63d0\u4f9b\u5bf9\u5e95\u5c42\u529f\u80fd\u7684\u8bbf\u95ee\uff0c\u652f\u63013D Slicer\u7684Python API\uff0c\u5e76\u6539\u8fdb\u6570\u636e\u4f20\u8f93\u534f\u8bae\u3002", "result": "\u5f00\u53d1\u4e86\u56db\u4e2a\u5e94\u7528\u6848\u4f8b\uff0c\u5728\u771f\u5b9e\u7684\u56fe\u50cf\u5f15\u5bfc\u673a\u5668\u4eba\u573a\u666f\u4e2d\u5c55\u793a\u4e86SlicerROS2\u7684\u6838\u5fc3\u529f\u80fd\u3002", "conclusion": "SlicerROS2\u7684\u65b0\u8bbe\u8ba1\u4e3a\u533b\u5b66\u673a\u5668\u4eba\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u548c\u7075\u6d3b\u7684\u5de5\u5177\uff0c\u6210\u529f\u5c55\u793a\u4e86\u5728\u56fe\u50cf\u5f15\u5bfc\u673a\u5668\u4eba\u5e72\u9884\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.19058", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19058", "abs": "https://arxiv.org/abs/2509.19058", "authors": ["Kwonho Kim", "Heejeong Nam", "Inwoo Hwang", "Sanghack Lee"], "title": "Towards Causal Representation Learning with Observable Sources as Auxiliaries", "comment": null, "summary": "Causal representation learning seeks to recover latent factors that generate\nobservational data through a mixing function. Needing assumptions on latent\nstructures or relationships to achieve identifiability in general, prior works\noften build upon conditional independence given known auxiliary variables.\nHowever, prior frameworks limit the scope of auxiliary variables to be external\nto the mixing function. Yet, in some cases, system-driving latent factors can\nbe easily observed or extracted from data, possibly facilitating\nidentification. In this paper, we introduce a framework of observable sources\nbeing auxiliaries, serving as effective conditioning variables. Our main\nresults show that one can identify entire latent variables up to subspace-wise\ntransformations and permutations using volume-preserving encoders. Moreover,\nwhen multiple known auxiliary variables are available, we offer a\nvariable-selection scheme to choose those that maximize recoverability of the\nlatent factors given knowledge of the latent causal graph. Finally, we\ndemonstrate the effectiveness of our framework through experiments on synthetic\ngraph and image data, thereby extending the boundaries of current approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4f7f\u7528\u53ef\u89c2\u5bdf\u6e90\u4f5c\u4e3a\u8f85\u52a9\u53d8\u91cf\u7684\u56e0\u679c\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u5728\u5df2\u77e5\u6f5c\u56e0\u679c\u56fe\u7684\u60c5\u51b5\u4e0b\u8bc6\u522b\u6f5c\u53d8\u91cf\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u56e0\u679c\u8868\u793a\u5b66\u4e60\u6846\u67b6\u901a\u5e38\u5c06\u8f85\u52a9\u53d8\u91cf\u9650\u5236\u4e3a\u6df7\u5408\u51fd\u6570\u5916\u90e8\u53d8\u91cf\uff0c\u4f46\u5b9e\u9645\u4e2d\u7cfb\u7edf\u9a71\u52a8\u7684\u6f5c\u56e0\u5b50\u53ef\u80fd\u5bb9\u6613\u4ece\u6570\u636e\u4e2d\u89c2\u5bdf\u6216\u63d0\u53d6\uff0c\u8fd9\u6709\u52a9\u4e8e\u8bc6\u522b\u6f5c\u53d8\u91cf\u3002", "method": "\u5f15\u5165\u53ef\u89c2\u5bdf\u6e90\u4f5c\u4e3a\u8f85\u52a9\u53d8\u91cf\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u4fdd\u4f53\u79ef\u7f16\u7801\u5668\u5b9e\u73b0\u6f5c\u53d8\u91cf\u7684\u5b50\u7a7a\u95f4\u53d8\u6362\u548c\u6392\u5217\u8bc6\u522b\uff0c\u5e76\u63d0\u4f9b\u53d8\u91cf\u9009\u62e9\u65b9\u6848\u6765\u6700\u5927\u5316\u6f5c\u56e0\u5b50\u53ef\u6062\u590d\u6027\u3002", "result": "\u5b9e\u9a8c\u5728\u5408\u6210\u56fe\u548c\u56fe\u50cf\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u8bc6\u522b\u6574\u4e2a\u6f5c\u53d8\u91cf\u3002", "conclusion": "\u8be5\u6846\u67b6\u6269\u5c55\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u8fb9\u754c\uff0c\u901a\u8fc7\u5229\u7528\u53ef\u89c2\u5bdf\u6e90\u4f5c\u4e3a\u8f85\u52a9\u53d8\u91cf\u6539\u8fdb\u4e86\u56e0\u679c\u8868\u793a\u5b66\u4e60\u7684\u8bc6\u522b\u80fd\u529b\u3002"}}
{"id": "2509.19080", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19080", "abs": "https://arxiv.org/abs/2509.19080", "authors": ["Zhennan Jiang", "Kai Liu", "Yuxin Qin", "Shuai Tian", "Yupeng Zheng", "Mingcai Zhou", "Chao Yu", "Haoran Li", "Dongbin Zhao"], "title": "World4RL: Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation", "comment": null, "summary": "Robotic manipulation policies are commonly initialized through imitation\nlearning, but their performance is limited by the scarcity and narrow coverage\nof expert data. Reinforcement learning can refine polices to alleviate this\nlimitation, yet real-robot training is costly and unsafe, while training in\nsimulators suffers from the sim-to-real gap. Recent advances in generative\nmodels have demonstrated remarkable capabilities in real-world simulation, with\ndiffusion models in particular excelling at generation. This raises the\nquestion of how diffusion model-based world models can be combined to enhance\npre-trained policies in robotic manipulation. In this work, we propose\nWorld4RL, a framework that employs diffusion-based world models as\nhigh-fidelity simulators to refine pre-trained policies entirely in imagined\nenvironments for robotic manipulation. Unlike prior works that primarily employ\nworld models for planning, our framework enables direct end-to-end policy\noptimization. World4RL is designed around two principles: pre-training a\ndiffusion world model that captures diverse dynamics on multi-task datasets and\nrefining policies entirely within a frozen world model to avoid online\nreal-world interactions. We further design a two-hot action encoding scheme\ntailored for robotic manipulation and adopt diffusion backbones to improve\nmodeling fidelity. Extensive simulation and real-world experiments demonstrate\nthat World4RL provides high-fidelity environment modeling and enables\nconsistent policy refinement, yielding significantly higher success rates\ncompared to imitation learning and other baselines. More visualization results\nare available at https://world4rl.github.io/.", "AI": {"tldr": "World4RL\u662f\u4e00\u4e2a\u5229\u7528\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u9ad8\u4fdd\u771f\u6a21\u62df\u5668\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u60f3\u8c61\u73af\u5883\u4e2d\u4f18\u5316\u9884\u8bad\u7ec3\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\uff0c\u907f\u514d\u771f\u5b9e\u4e16\u754c\u4ea4\u4e92\u7684\u6210\u672c\u548c\u98ce\u9669\u3002", "motivation": "\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u901a\u5e38\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u521d\u59cb\u5316\uff0c\u4f46\u53d7\u9650\u4e8e\u4e13\u5bb6\u6570\u636e\u7684\u7a00\u7f3a\u6027\u548c\u8986\u76d6\u8303\u56f4\u3002\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u4f18\u5316\u7b56\u7565\uff0c\u4f46\u771f\u5b9e\u673a\u5668\u4eba\u8bad\u7ec3\u6210\u672c\u9ad8\u4e14\u4e0d\u5b89\u5168\uff0c\u800c\u6a21\u62df\u5668\u8bad\u7ec3\u5b58\u5728\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u3002", "method": "\u63d0\u51faWorld4RL\u6846\u67b6\uff0c\u9884\u8bad\u7ec3\u6269\u6563\u4e16\u754c\u6a21\u578b\u6355\u6349\u591a\u4efb\u52a1\u6570\u636e\u96c6\u4e2d\u7684\u591a\u6837\u5316\u52a8\u6001\uff0c\u5728\u51bb\u7ed3\u7684\u4e16\u754c\u6a21\u578b\u4e2d\u5b8c\u5168\u4f18\u5316\u7b56\u7565\u3002\u8bbe\u8ba1\u4e86\u9488\u5bf9\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u53cc\u70ed\u52a8\u4f5c\u7f16\u7801\u65b9\u6848\uff0c\u5e76\u91c7\u7528\u6269\u6563\u9aa8\u5e72\u7f51\u7edc\u63d0\u9ad8\u5efa\u6a21\u4fdd\u771f\u5ea6\u3002", "result": "\u5e7f\u6cdb\u7684\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8868\u660e\uff0cWorld4RL\u63d0\u4f9b\u9ad8\u4fdd\u771f\u73af\u5883\u5efa\u6a21\uff0c\u5b9e\u73b0\u4e00\u81f4\u7684\u7b56\u7565\u4f18\u5316\uff0c\u76f8\u6bd4\u6a21\u4eff\u5b66\u4e60\u548c\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u6210\u529f\u7387\u3002", "conclusion": "World4RL\u6846\u67b6\u6210\u529f\u5730\u5c06\u6269\u6563\u6a21\u578b\u4e16\u754c\u6a21\u578b\u4e0e\u7b56\u7565\u4f18\u5316\u76f8\u7ed3\u5408\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u3002"}}
{"id": "2509.19077", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19077", "abs": "https://arxiv.org/abs/2509.19077", "authors": ["Zikang Tian", "Shaohui Peng", "Du Huang", "Jiaming Guo", "Ruizhi Chen", "Rui Zhang", "Xishan Zhang", "Yuxuan Guo", "Zidong Du", "Qi Guo", "Ling Li", "Yewen Pu", "Xing Hu", "Yunji Chen"], "title": "Code Driven Planning with Domain-Adaptive Critic", "comment": null, "summary": "Large Language Models (LLMs) have been widely adopted as task planners for AI\nagents in sequential decision-making problems, leveraging their extensive world\nknowledge. However, the gap between their general knowledge and\nenvironment-specific requirements often leads to inaccurate plans. To address\nthis, existing approaches rely on frequent LLM queries to iteratively refine\nplans based on immediate environmental feedback, which incurs substantial query\ncosts. However, this refinement is typically guided by short-term environmental\nfeedback, limiting LLMs from developing plans aligned with long-term rewards.\nWe propose Code Driven Planning with Domain-Adaptive Critic (CoPiC). Instead of\nrelying on frequent queries, CoPiC employs LLMs to generate a diverse set of\nhigh-level planning programs, which iteratively produce and refine candidate\nplans. A trained domain-adaptive critic then evaluates these candidates and\nselects the one most aligned with long-term rewards for execution. Using\nhigh-level planning programs as planner and domain-adaptive critic as\nestimator, CoPiC improves planning while significantly reducing query costs.\nResults in ALFWorld, NetHack, and StarCraft II Unit Building show that CoPiC\noutperforms advanced LLM-based baselines, AdaPlanner and Reflexion, achieving\nan average (1) 23.33% improvement in success rate and (2) 91.27% reduction in\nquery costs.", "AI": {"tldr": "CoPiC\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7801\u9a71\u52a8\u89c4\u5212\u548c\u9886\u57df\u81ea\u9002\u5e94\u6279\u8bc4\u5668\u7684LLM\u4efb\u52a1\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u591a\u6837\u5316\u9ad8\u7ea7\u89c4\u5212\u7a0b\u5e8f\u5e76\u5229\u7528\u8bad\u7ec3\u597d\u7684\u6279\u8bc4\u5668\u8bc4\u4f30\u5019\u9009\u8ba1\u5212\uff0c\u663e\u8457\u51cf\u5c11LLM\u67e5\u8be2\u6b21\u6570\u5e76\u63d0\u9ad8\u957f\u671f\u5956\u52b1\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709LLM\u4efb\u52a1\u89c4\u5212\u65b9\u6cd5\u4f9d\u8d56\u9891\u7e41\u67e5\u8be2\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\uff0c\u5bfc\u81f4\u9ad8\u67e5\u8be2\u6210\u672c\u4e14\u96be\u4ee5\u5bf9\u9f50\u957f\u671f\u5956\u52b1\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u51cf\u5c11\u67e5\u8be2\u6b21\u6570\u53c8\u80fd\u4f18\u5316\u957f\u671f\u89c4\u5212\u6027\u80fd\u7684\u65b9\u6cd5\u3002", "method": "CoPiC\u4f7f\u7528LLM\u751f\u6210\u591a\u6837\u5316\u9ad8\u7ea7\u89c4\u5212\u7a0b\u5e8f\uff0c\u8fd9\u4e9b\u7a0b\u5e8f\u8fed\u4ee3\u4ea7\u751f\u548c\u4f18\u5316\u5019\u9009\u8ba1\u5212\u3002\u7136\u540e\u901a\u8fc7\u8bad\u7ec3\u597d\u7684\u9886\u57df\u81ea\u9002\u5e94\u6279\u8bc4\u5668\u8bc4\u4f30\u5019\u9009\u8ba1\u5212\uff0c\u9009\u62e9\u6700\u7b26\u5408\u957f\u671f\u5956\u52b1\u7684\u65b9\u6848\u6267\u884c\u3002", "result": "\u5728ALFWorld\u3001NetHack\u548cStarCraft II Unit Building\u4e09\u4e2a\u73af\u5883\u4e2d\uff0cCoPiC\u76f8\u6bd4AdaPlanner\u548cReflexion\u7b49\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e73\u5747\u6210\u529f\u7387\u63d0\u534723.33%\uff0c\u67e5\u8be2\u6210\u672c\u964d\u4f4e91.27%\u3002", "conclusion": "CoPiC\u901a\u8fc7\u4ee3\u7801\u9a71\u52a8\u89c4\u5212\u548c\u9886\u57df\u81ea\u9002\u5e94\u6279\u8bc4\u5668\u7684\u7ed3\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u4efb\u52a1\u89c4\u5212\u4e2d\u7684\u67e5\u8be2\u6210\u672c\u9ad8\u548c\u957f\u671f\u5956\u52b1\u5bf9\u9f50\u95ee\u9898\uff0c\u4e3aAI\u4ee3\u7406\u7684\u5e8f\u5217\u51b3\u7b56\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.19102", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19102", "abs": "https://arxiv.org/abs/2509.19102", "authors": ["Hongli Xu", "Lei Zhang", "Xiaoyue Hu", "Boyang Zhong", "Kaixin Bai", "Zolt\u00e1n-Csaba M\u00e1rton", "Zhenshan Bing", "Zhaopeng Chen", "Alois Christian Knoll", "Jianwei Zhang"], "title": "FUNCanon: Learning Pose-Aware Action Primitives via Functional Object Canonicalization for Generalizable Robotic Manipulation", "comment": "project website: https://sites.google.com/view/funcanon, 11 pages", "summary": "General-purpose robotic skills from end-to-end demonstrations often leads to\ntask-specific policies that fail to generalize beyond the training\ndistribution. Therefore, we introduce FunCanon, a framework that converts\nlong-horizon manipulation tasks into sequences of action chunks, each defined\nby an actor, verb, and object. These chunks focus policy learning on the\nactions themselves, rather than isolated tasks, enabling compositionality and\nreuse. To make policies pose-aware and category-general, we perform functional\nobject canonicalization for functional alignment and automatic manipulation\ntrajectory transfer, mapping objects into shared functional frames using\naffordance cues from large vision language models. An object centric and action\ncentric diffusion policy FuncDiffuser trained on this aligned data naturally\nrespects object affordances and poses, simplifying learning and improving\ngeneralization ability. Experiments on simulated and real-world benchmarks\ndemonstrate category-level generalization, cross-task behavior reuse, and\nrobust sim2real deployment, showing that functional canonicalization provides a\nstrong inductive bias for scalable imitation learning in complex manipulation\ndomains. Details of the demo and supplemental material are available on our\nproject website https://sites.google.com/view/funcanon.", "AI": {"tldr": "FunCanon\u6846\u67b6\u5c06\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u5206\u89e3\u4e3a\u52a8\u4f5c\u5757\u5e8f\u5217\uff0c\u901a\u8fc7\u529f\u80fd\u5bf9\u8c61\u89c4\u8303\u5316\u5b9e\u73b0\u8de8\u7c7b\u522b\u6cdb\u5316\u548c\u884c\u4e3a\u91cd\u7528", "motivation": "\u89e3\u51b3\u7aef\u5230\u7aef\u6f14\u793a\u5b66\u4e60\u5bfc\u81f4\u7684\u4efb\u52a1\u7279\u5b9a\u7b56\u7565\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u53ef\u7ec4\u5408\u548c\u53ef\u91cd\u7528\u7684\u673a\u5668\u4eba\u6280\u80fd", "method": "\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u52a8\u4f5c\u5757\uff08\u6267\u884c\u8005-\u52a8\u8bcd-\u5bf9\u8c61\uff09\uff0c\u901a\u8fc7\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u529f\u80fd\u5bf9\u8c61\u89c4\u8303\u5316\uff0c\u4f7f\u7528FuncDiffuser\u6269\u6563\u7b56\u7565\u5b66\u4e60", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u7c7b\u522b\u7ea7\u6cdb\u5316\u3001\u8de8\u4efb\u52a1\u884c\u4e3a\u91cd\u7528\u548c\u7a33\u5065\u7684sim2real\u90e8\u7f72", "conclusion": "\u529f\u80fd\u89c4\u8303\u5316\u4e3a\u590d\u6742\u64cd\u4f5c\u9886\u57df\u7684\u53ef\u6269\u5c55\u6a21\u4eff\u5b66\u4e60\u63d0\u4f9b\u4e86\u5f3a\u5f52\u7eb3\u504f\u7f6e"}}
{"id": "2509.19236", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19236", "abs": "https://arxiv.org/abs/2509.19236", "authors": ["Chunhao Tian", "Yutong Wang", "Xuebo Liu", "Zhexuan Wang", "Liang Ding", "Miao Zhang", "Min Zhang"], "title": "AgentInit: Initializing LLM-based Multi-Agent Systems via Diversity and Expertise Orchestration for Effective and Efficient Collaboration", "comment": "EMNLP 2025 Findings", "summary": "Proper initialization is crucial for any system, particularly in multi-agent\nsystems (MAS), where it plays a pivotal role in determining both the system's\nefficiency and effectiveness. However, existing MAS initialization methods do\nnot fully account for the collaborative needs of the generated agents in\nsubsequent stages. Inspired by the principles of effective team composition, we\npropose AgentInit, which aims to optimize the structure of agent teams.\nSpecifically, in addition to multi-round interactions and reflections between\nagents during agent generation, AgentInit incorporates a Natural Language to\nFormat mechanism to ensure consistency and standardization. Balanced team\nselection strategies using Pareto principles are subsequently applied to\njointly consider agent team diversity and task relevance to promote effective\nand efficient collaboration and enhance overall system performance. Experiments\nshow that AgentInit consistently outperforms state-of-the-art initialization\nmethods and pre-defined strategies across various frameworks and tasks,\nachieving an overall performance improvement of up to 1.2 and 1.6,\nrespectively, while also significantly reducing token consumption. Further\nanalysis confirms its strong transferability to similar tasks and verifies the\neffectiveness of its key components, demonstrating its capability and\nadaptability as a reliable MAS initialization method. Source code and models\nare available at https://github.com/1737423697/AgentInit.", "AI": {"tldr": "AgentInit\u662f\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u667a\u80fd\u4f53\u56e2\u961f\u7ed3\u6784\u6765\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\uff0c\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u683c\u5f0f\u5316\u673a\u5236\u548c\u5e15\u7d2f\u6258\u5e73\u8861\u9009\u62e9\u7b56\u7565\uff0c\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709MAS\u521d\u59cb\u5316\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u8003\u8651\u540e\u7eed\u9636\u6bb5\u751f\u6210\u667a\u80fd\u4f53\u7684\u534f\u4f5c\u9700\u6c42\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4f18\u5316\u667a\u80fd\u4f53\u56e2\u961f\u7ed3\u6784\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u7cfb\u7edf\u6548\u7387\u548c\u6548\u679c\u3002", "method": "\u63d0\u51faAgentInit\u65b9\u6cd5\uff0c\u5305\u542b\u591a\u8f6e\u667a\u80fd\u4f53\u4ea4\u4e92\u53cd\u601d\u3001\u81ea\u7136\u8bed\u8a00\u5230\u683c\u5f0f\u7684\u8f6c\u6362\u673a\u5236\u786e\u4fdd\u4e00\u81f4\u6027\uff0c\u4ee5\u53ca\u57fa\u4e8e\u5e15\u7d2f\u6258\u539f\u5219\u7684\u5e73\u8861\u56e2\u961f\u9009\u62e9\u7b56\u7565\uff0c\u7efc\u5408\u8003\u8651\u56e2\u961f\u591a\u6837\u6027\u548c\u4efb\u52a1\u76f8\u5173\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793aAgentInit\u5728\u5404\u79cd\u6846\u67b6\u548c\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u521d\u59cb\u5316\u65b9\u6cd5\u548c\u9884\u5b9a\u4e49\u7b56\u7565\uff0c\u6574\u4f53\u6027\u80fd\u63d0\u5347\u5206\u522b\u8fbe\u52301.2\u548c1.6\u500d\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11token\u6d88\u8017\u3002", "conclusion": "AgentInit\u5177\u6709\u826f\u597d\u7684\u53ef\u8fc1\u79fb\u6027\uff0c\u5173\u952e\u7ec4\u4ef6\u6709\u6548\u6027\u5f97\u5230\u9a8c\u8bc1\uff0c\u8bc1\u660e\u5176\u4f5c\u4e3a\u53ef\u9760MAS\u521d\u59cb\u5316\u65b9\u6cd5\u7684\u80fd\u529b\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2509.19105", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19105", "abs": "https://arxiv.org/abs/2509.19105", "authors": ["Sarvesh Prajapati", "Ananya Trivedi", "Nathaniel Hanson", "Bruce Maxwell", "Taskin Padir"], "title": "Spectral Signature Mapping from RGB Imagery for Terrain-Aware Navigation", "comment": "8 pages, 10 figures, submitted to Robotic Computing & Communication", "summary": "Successful navigation in outdoor environments requires accurate prediction of\nthe physical interactions between the robot and the terrain. To this end,\nseveral methods rely on geometric or semantic labels to classify traversable\nsurfaces. However, such labels cannot distinguish visually similar surfaces\nthat differ in material properties. Spectral sensors enable inference of\nmaterial composition from surface reflectance measured across multiple\nwavelength bands. Although spectral sensing is gaining traction in robotics,\nwidespread deployment remains constrained by the need for custom hardware\nintegration, high sensor costs, and compute-intensive processing pipelines. In\nthis paper, we present RGB Image to Spectral Signature Neural Network (RS-Net),\na deep neural network designed to bridge the gap between the accessibility of\nRGB sensing and the rich material information provided by spectral data. RS-Net\npredicts spectral signatures from RGB patches, which we map to terrain labels\nand friction coefficients. The resulting terrain classifications are integrated\ninto a sampling-based motion planner for a wheeled robot operating in outdoor\nenvironments. Likewise, the friction estimates are incorporated into a\ncontact-force-based MPC for a quadruped robot navigating slippery surfaces.\nThus, we introduce a framework that learns the task-relevant physical property\nonce during training and thereafter relies solely on RGB sensing at test time.\nThe code is available at https://github.com/prajapatisarvesh/RS-Net.", "AI": {"tldr": "RS-Net\u662f\u4e00\u4e2a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u80fd\u591f\u4eceRGB\u56fe\u50cf\u9884\u6d4b\u5149\u8c31\u7279\u5f81\uff0c\u4ece\u800c\u83b7\u53d6\u5730\u5f62\u6750\u8d28\u4fe1\u606f\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u6237\u5916\u5bfc\u822a\u4e2d\u7684\u5730\u5f62\u5206\u7c7b\u548c\u6469\u64e6\u7cfb\u6570\u4f30\u8ba1\u3002", "motivation": "\u6237\u5916\u5bfc\u822a\u9700\u8981\u51c6\u786e\u9884\u6d4b\u673a\u5668\u4eba\u4e0e\u5730\u5f62\u7684\u7269\u7406\u4ea4\u4e92\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u51e0\u4f55\u6216\u8bed\u4e49\u6807\u7b7e\uff0c\u4f46\u65e0\u6cd5\u533a\u5206\u89c6\u89c9\u76f8\u4f3c\u4f46\u6750\u8d28\u4e0d\u540c\u7684\u8868\u9762\u3002\u5149\u8c31\u4f20\u611f\u80fd\u63d0\u4f9b\u6750\u8d28\u4fe1\u606f\uff0c\u4f46\u53d7\u9650\u4e8e\u786c\u4ef6\u6210\u672c\u548c\u5904\u7406\u590d\u6742\u5ea6\u3002", "method": "\u63d0\u51faRS-Net\u795e\u7ecf\u7f51\u7edc\uff0c\u4eceRGB\u56fe\u50cf\u5757\u9884\u6d4b\u5149\u8c31\u7279\u5f81\uff0c\u6620\u5c04\u5230\u5730\u5f62\u6807\u7b7e\u548c\u6469\u64e6\u7cfb\u6570\u3002\u5c06\u5730\u5f62\u5206\u7c7b\u96c6\u6210\u5230\u57fa\u4e8e\u91c7\u6837\u7684\u8fd0\u52a8\u89c4\u5212\u5668\uff0c\u6469\u64e6\u4f30\u8ba1\u96c6\u6210\u5230\u57fa\u4e8e\u63a5\u89e6\u529b\u7684MPC\u63a7\u5236\u5668\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u5728\u8bad\u7ec3\u65f6\u5b66\u4e60\u4efb\u52a1\u76f8\u5173\u7684\u7269\u7406\u5c5e\u6027\uff0c\u6d4b\u8bd5\u65f6\u4ec5\u9700RGB\u4f20\u611f\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "conclusion": "RS-Net\u586b\u8865\u4e86RGB\u4f20\u611f\u53ef\u8bbf\u95ee\u6027\u4e0e\u5149\u8c31\u6570\u636e\u4e30\u5bcc\u6750\u8d28\u4fe1\u606f\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u673a\u5668\u4eba\u6237\u5916\u5bfc\u822a\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6750\u8d28\u611f\u77e5\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.19265", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19265", "abs": "https://arxiv.org/abs/2509.19265", "authors": ["Saeed Almheiri", "Rania Hossam", "Mena Attia", "Chenxi Wang", "Preslav Nakov", "Timothy Baldwin", "Fajri Koto"], "title": "Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from the Arab World", "comment": "EMNLP 2025 - Findings", "summary": "Large language models (LLMs) often reflect Western-centric biases, limiting\ntheir effectiveness in diverse cultural contexts. Although some work has\nexplored cultural alignment, the potential for cross-cultural transfer, using\nalignment in one culture to improve performance in others, remains\nunderexplored. This paper investigates cross-cultural transfer of commonsense\nreasoning in the Arab world, where linguistic and historical similarities\ncoexist with local cultural differences. Using a culturally grounded\ncommonsense reasoning dataset covering 13 Arab countries, we evaluate\nlightweight alignment methods such as in-context learning and\ndemonstration-based reinforcement (DITTO), alongside baselines like supervised\nfine-tuning and direct preference optimization. Our results show that merely 12\nculture-specific examples from one country can improve performance in others by\n10\\% on average, within multilingual models. In addition, we demonstrate that\nout-of-culture demonstrations from Indonesia and US contexts can match or\nsurpass in-culture alignment for MCQ reasoning, highlighting cultural\ncommonsense transferability beyond the Arab world. These findings demonstrate\nthat efficient cross-cultural alignment is possible and offer a promising\napproach to adapt LLMs to low-resource cultural settings.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u963f\u62c9\u4f2f\u4e16\u754c\u7684\u8de8\u6587\u5316\u5e38\u8bc6\u63a8\u7406\u8fc1\u79fb\uff0c\u53d1\u73b0\u4ec5\u9700\u5c11\u91cf\u6587\u5316\u7279\u5b9a\u793a\u4f8b\u5373\u53ef\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u5176\u4ed6\u6587\u5316\u80cc\u666f\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u897f\u65b9\u4e2d\u5fc3\u504f\u89c1\uff0c\u9650\u5236\u4e86\u5176\u5728\u591a\u5143\u6587\u5316\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002\u867d\u7136\u5df2\u6709\u7814\u7a76\u63a2\u7d22\u6587\u5316\u5bf9\u9f50\uff0c\u4f46\u8de8\u6587\u5316\u8fc1\u79fb\u6f5c\u529b\uff08\u5229\u7528\u4e00\u79cd\u6587\u5316\u7684\u5bf9\u9f50\u6765\u6539\u5584\u5176\u4ed6\u6587\u5316\u7684\u6027\u80fd\uff09\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4f7f\u7528\u8986\u76d613\u4e2a\u963f\u62c9\u4f2f\u56fd\u5bb6\u7684\u6587\u5316\u57fa\u7840\u5e38\u8bc6\u63a8\u7406\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u8f7b\u91cf\u7ea7\u5bf9\u9f50\u65b9\u6cd5\uff08\u5982\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001\u6f14\u793a\u5f3a\u5316DITTO\uff09\u4ee5\u53ca\u57fa\u7ebf\u65b9\u6cd5\uff08\u76d1\u7763\u5fae\u8c03\u3001\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff09\u3002", "result": "\u4ec5\u9700\u6765\u81ea\u4e00\u4e2a\u56fd\u5bb6\u768412\u4e2a\u6587\u5316\u7279\u5b9a\u793a\u4f8b\uff0c\u5373\u53ef\u5728\u591a\u8bed\u8a00\u6a21\u578b\u4e2d\u5e73\u5747\u63d0\u5347\u5176\u4ed6\u6587\u531610%\u7684\u6027\u80fd\u3002\u6765\u81ea\u5370\u5c3c\u548c\u7f8e\u56fd\u7684\u8de8\u6587\u5316\u6f14\u793a\u5728MCQ\u63a8\u7406\u4e2d\u80fd\u591f\u5339\u914d\u751a\u81f3\u8d85\u8d8a\u6587\u5316\u5185\u5bf9\u9f50\u6548\u679c\u3002", "conclusion": "\u9ad8\u6548\u7684\u8de8\u6587\u5316\u5bf9\u9f50\u662f\u53ef\u884c\u7684\uff0c\u8fd9\u4e3a\u5c06LLM\u9002\u914d\u5230\u4f4e\u8d44\u6e90\u6587\u5316\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.19142", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19142", "abs": "https://arxiv.org/abs/2509.19142", "authors": ["Kangmin Kim", "Seunghyeok Back", "Geonhyup Lee", "Sangbeom Lee", "Sangjun Noh", "Kyoobin Lee"], "title": "BiGraspFormer: End-to-End Bimanual Grasp Transformer", "comment": "8 pages, 5 figures", "summary": "Bimanual grasping is essential for robots to handle large and complex\nobjects. However, existing methods either focus solely on single-arm grasping\nor employ separate grasp generation and bimanual evaluation stages, leading to\ncoordination problems including collision risks and unbalanced force\ndistribution. To address these limitations, we propose BiGraspFormer, a unified\nend-to-end transformer framework that directly generates coordinated bimanual\ngrasps from object point clouds. Our key idea is the Single-Guided Bimanual\n(SGB) strategy, which first generates diverse single grasp candidates using a\ntransformer decoder, then leverages their learned features through specialized\nattention mechanisms to jointly predict bimanual poses and quality scores. This\nconditioning strategy reduces the complexity of the 12-DoF search space while\nensuring coordinated bimanual manipulation. Comprehensive simulation\nexperiments and real-world validation demonstrate that BiGraspFormer\nconsistently outperforms existing methods while maintaining efficient inference\nspeed (<0.05s), confirming the effectiveness of our framework. Code and\nsupplementary materials are available at https://sites.google.com/bigraspformer", "AI": {"tldr": "BiGraspFormer\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u7aef\u5230\u7aefTransformer\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u7269\u4f53\u70b9\u4e91\u751f\u6210\u534f\u8c03\u7684\u53cc\u81c2\u6293\u53d6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u534f\u8c03\u6027\u5dee\u3001\u78b0\u649e\u98ce\u9669\u548c\u529b\u5206\u5e03\u4e0d\u5e73\u8861\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u53cc\u81c2\u6293\u53d6\u65b9\u6cd5\u8981\u4e48\u53ea\u5173\u6ce8\u5355\u81c2\u6293\u53d6\uff0c\u8981\u4e48\u91c7\u7528\u5206\u79bb\u7684\u6293\u53d6\u751f\u6210\u548c\u53cc\u81c2\u8bc4\u4f30\u9636\u6bb5\uff0c\u5bfc\u81f4\u534f\u8c03\u6027\u95ee\u9898\uff0c\u5305\u62ec\u78b0\u649e\u98ce\u9669\u548c\u529b\u5206\u5e03\u4e0d\u5e73\u8861\u3002", "method": "\u63d0\u51fa\u5355\u5f15\u5bfc\u53cc\u81c2\uff08SGB\uff09\u7b56\u7565\uff1a\u9996\u5148\u4f7f\u7528Transformer\u89e3\u7801\u5668\u751f\u6210\u591a\u6837\u5316\u7684\u5355\u6293\u53d6\u5019\u9009\uff0c\u7136\u540e\u901a\u8fc7\u4e13\u95e8\u7684\u6ce8\u610f\u529b\u673a\u5236\u5229\u7528\u5176\u5b66\u4e60\u5230\u7684\u7279\u5f81\u6765\u8054\u5408\u9884\u6d4b\u53cc\u81c2\u59ff\u6001\u548c\u8d28\u91cf\u5206\u6570\u3002", "result": "\u7efc\u5408\u4eff\u771f\u5b9e\u9a8c\u548c\u771f\u5b9e\u4e16\u754c\u9a8c\u8bc1\u8868\u660e\uff0cBiGraspFormer\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u7684\u63a8\u7406\u901f\u5ea6\uff08<0.05\u79d2\uff09\u3002", "conclusion": "BiGraspFormer\u6846\u67b6\u5728\u89e3\u51b3\u53cc\u81c2\u6293\u53d6\u534f\u8c03\u6027\u95ee\u9898\u65b9\u9762\u5177\u6709\u6709\u6548\u6027\uff0c\u4ee3\u7801\u548c\u8865\u5145\u6750\u6599\u5df2\u516c\u5f00\u3002"}}
{"id": "2509.19168", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19168", "abs": "https://arxiv.org/abs/2509.19168", "authors": ["Mark Gonzales", "Ethan Oh", "Joseph Moore"], "title": "A Multimodal Stochastic Planning Approach for Navigation and Multi-Robot Coordination", "comment": "8 Pages, 7 Figures", "summary": "In this paper, we present a receding-horizon, sampling-based planner capable\nof reasoning over multimodal policy distributions. By using the cross-entropy\nmethod to optimize a multimodal policy under a common cost function, our\napproach increases robustness against local minima and promotes effective\nexploration of the solution space. We show that our approach naturally extends\nto multi-robot collision-free planning, enables agents to share diverse\ncandidate policies to avoid deadlocks, and allows teams to minimize a global\nobjective without incurring the computational complexity of centralized\noptimization. Numerical simulations demonstrate that employing multiple modes\nsignificantly improves success rates in trap environments and in multi-robot\ncollision avoidance. Hardware experiments further validate the approach's\nreal-time feasibility and practical performance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u91c7\u6837\u7684\u91cd\u89c4\u5212\u5668\uff0c\u80fd\u591f\u5904\u7406\u591a\u6a21\u6001\u7b56\u7565\u5206\u5e03\uff0c\u901a\u8fc7\u4ea4\u53c9\u71b5\u65b9\u6cd5\u4f18\u5316\u591a\u6a21\u6001\u7b56\u7565\uff0c\u63d0\u9ad8\u5bf9\u5c40\u90e8\u6781\u5c0f\u503c\u7684\u9c81\u68d2\u6027\u548c\u89e3\u7a7a\u95f4\u63a2\u7d22\u6548\u7387", "motivation": "\u89e3\u51b3\u4f20\u7edf\u89c4\u5212\u65b9\u6cd5\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6781\u5c0f\u503c\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u907f\u78b0\u548c\u6b7b\u9501\u907f\u514d\u7684\u6210\u529f\u7387", "method": "\u4f7f\u7528\u91cd\u89c4\u5212\u89c6\u91ce\u548c\u57fa\u4e8e\u91c7\u6837\u7684\u89c4\u5212\u65b9\u6cd5\uff0c\u7ed3\u5408\u4ea4\u53c9\u71b5\u65b9\u6cd5\u4f18\u5316\u591a\u6a21\u6001\u7b56\u7565\u5206\u5e03\uff0c\u5728\u5171\u540c\u6210\u672c\u51fd\u6570\u4e0b\u8fdb\u884c\u4f18\u5316", "result": "\u6570\u503c\u6a21\u62df\u663e\u793a\u591a\u6a21\u6001\u7b56\u7565\u5728\u9677\u9631\u73af\u5883\u548c\u591a\u673a\u5668\u4eba\u907f\u78b0\u4e2d\u663e\u8457\u63d0\u9ad8\u6210\u529f\u7387\uff0c\u786c\u4ef6\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5b9e\u65f6\u53ef\u884c\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u6a21\u6001\u7b56\u7565\uff0c\u63d0\u9ad8\u89c4\u5212\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u5206\u5e03\u5f0f\u4f18\u5316"}}
{"id": "2509.19169", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19169", "abs": "https://arxiv.org/abs/2509.19169", "authors": ["Tianyu Wu", "Xudong Han", "Haoran Sun", "Zishang Zhang", "Bangchao Huang", "Chaoyang Song", "Fang Wan"], "title": "MagiClaw: A Dual-Use, Vision-Based Soft Gripper for Bridging the Human Demonstration to Robotic Deployment Gap", "comment": "8 pages, 4 figures, accepted to Data@CoRL2025 Workshop", "summary": "The transfer of manipulation skills from human demonstration to robotic\nexecution is often hindered by a \"domain gap\" in sensing and morphology. This\npaper introduces MagiClaw, a versatile two-finger end-effector designed to\nbridge this gap. MagiClaw functions interchangeably as both a handheld tool for\nintuitive data collection and a robotic end-effector for policy deployment,\nensuring hardware consistency and reliability. Each finger incorporates a Soft\nPolyhedral Network (SPN) with an embedded camera, enabling vision-based\nestimation of 6-DoF forces and contact deformation. This proprioceptive data is\nfused with exteroceptive environmental sensing from an integrated iPhone, which\nprovides 6D pose, RGB video, and LiDAR-based depth maps. Through a custom iOS\napplication, MagiClaw streams synchronized, multi-modal data for real-time\nteleoperation, offline policy learning, and immersive control via mixed-reality\ninterfaces. We demonstrate how this unified system architecture lowers the\nbarrier to collecting high-fidelity, contact-rich datasets and accelerates the\ndevelopment of generalizable manipulation policies. Please refer to the iOS app\nat https://apps.apple.com/cn/app/magiclaw/id6661033548 for further details.", "AI": {"tldr": "MagiClaw\u662f\u4e00\u4e2a\u591a\u529f\u80fd\u53cc\u6307\u672b\u7aef\u6267\u884c\u5668\uff0c\u901a\u8fc7\u786c\u4ef6\u4e00\u81f4\u6027\u8bbe\u8ba1\u89e3\u51b3\u4eba\u673a\u64cd\u4f5c\u6280\u80fd\u8f6c\u79fb\u4e2d\u7684\u9886\u57df\u5dee\u8ddd\u95ee\u9898\uff0c\u96c6\u6210\u4e86\u89c6\u89c9\u529b\u611f\u77e5\u548c\u73af\u5883\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u4eba\u7c7b\u6f14\u793a\u4e0e\u673a\u5668\u4eba\u6267\u884c\u4e4b\u95f4\u7684\u9886\u57df\u5dee\u8ddd\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u611f\u77e5\u548c\u5f62\u6001\u5b66\u65b9\u9762\u7684\u5dee\u5f02\uff0c\u964d\u4f4e\u63a5\u89e6\u4e30\u5bcc\u6570\u636e\u96c6\u6536\u96c6\u7684\u95e8\u69db\u3002", "method": "\u8bbe\u8ba1MagiClaw\u672b\u7aef\u6267\u884c\u5668\uff0c\u517c\u5177\u624b\u6301\u5de5\u5177\u548c\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\u529f\u80fd\uff0c\u96c6\u6210\u8f6f\u591a\u9762\u4f53\u7f51\u7edc\uff08SPN\uff09\u548c\u5d4c\u5165\u5f0f\u6444\u50cf\u5934\u8fdb\u884c6-DoF\u529b\u4f30\u8ba1\uff0c\u7ed3\u5408iPhone\u63d0\u4f9b\u73af\u5883\u611f\u77e5\u6570\u636e\u3002", "result": "\u5f00\u53d1\u4e86\u7edf\u4e00\u7684\u7cfb\u7edf\u67b6\u6784\uff0c\u80fd\u591f\u5b9e\u65f6\u6d41\u5f0f\u4f20\u8f93\u540c\u6b65\u591a\u6a21\u6001\u6570\u636e\uff0c\u652f\u6301\u9065\u64cd\u4f5c\u3001\u79bb\u7ebf\u7b56\u7565\u5b66\u4e60\u548c\u6df7\u5408\u73b0\u5b9e\u754c\u9762\u63a7\u5236\u3002", "conclusion": "MagiClaw\u7cfb\u7edf\u964d\u4f4e\u4e86\u9ad8\u4fdd\u771f\u63a5\u89e6\u4e30\u5bcc\u6570\u636e\u96c6\u6536\u96c6\u7684\u969c\u788d\uff0c\u52a0\u901f\u4e86\u53ef\u6cdb\u5316\u64cd\u4f5c\u7b56\u7565\u7684\u5f00\u53d1\u3002"}}
{"id": "2509.19261", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19261", "abs": "https://arxiv.org/abs/2509.19261", "authors": ["Kuanqi Cai", "Chunfeng Wang", "Zeqi Li", "Haowen Yao", "Weinan Chen", "Luis Figueredo", "Aude Billard", "Arash Ajoudani"], "title": "Imitation-Guided Bimanual Planning for Stable Manipulation under Changing External Forces", "comment": null, "summary": "Robotic manipulation in dynamic environments often requires seamless\ntransitions between different grasp types to maintain stability and efficiency.\nHowever, achieving smooth and adaptive grasp transitions remains a challenge,\nparticularly when dealing with external forces and complex motion constraints.\nExisting grasp transition strategies often fail to account for varying external\nforces and do not optimize motion performance effectively. In this work, we\npropose an Imitation-Guided Bimanual Planning Framework that integrates\nefficient grasp transition strategies and motion performance optimization to\nenhance stability and dexterity in robotic manipulation. Our approach\nintroduces Strategies for Sampling Stable Intersections in Grasp Manifolds for\nseamless transitions between uni-manual and bi-manual grasps, reducing\ncomputational costs and regrasping inefficiencies. Additionally, a Hierarchical\nDual-Stage Motion Architecture combines an Imitation Learning-based Global Path\nGenerator with a Quadratic Programming-driven Local Planner to ensure real-time\nmotion feasibility, obstacle avoidance, and superior manipulability. The\nproposed method is evaluated through a series of force-intensive tasks,\ndemonstrating significant improvements in grasp transition efficiency and\nmotion performance. A video demonstrating our simulation results can be viewed\nat\n\\href{https://youtu.be/3DhbUsv4eDo}{\\textcolor{blue}{https://youtu.be/3DhbUsv4eDo}}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u4eff\u5f15\u5bfc\u7684\u53cc\u624b\u673a\u5668\u4eba\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u7a33\u5b9a\u7684\u6293\u53d6\u6d41\u5f62\u91c7\u6837\u7b56\u7565\u548c\u5206\u5c42\u8fd0\u52a8\u67b6\u6784\uff0c\u5b9e\u73b0\u65e0\u7f1d\u6293\u53d6\u8f6c\u6362\u548c\u4f18\u5316\u7684\u8fd0\u52a8\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u52a8\u6001\u73af\u5883\u4e2d\u673a\u5668\u4eba\u64cd\u4f5c\u65f6\u4e0d\u540c\u6293\u53d6\u7c7b\u578b\u4e4b\u95f4\u5e73\u6ed1\u8f6c\u6362\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5916\u90e8\u529b\u548c\u590d\u6742\u8fd0\u52a8\u7ea6\u675f\u65f6\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u6a21\u4eff\u5f15\u5bfc\u7684\u53cc\u624b\u673a\u5668\u4eba\u89c4\u5212\u6846\u67b6\uff0c\u5305\u62ec\u7a33\u5b9a\u7684\u6293\u53d6\u6d41\u5f62\u91c7\u6837\u7b56\u7565\u5b9e\u73b0\u5355\u53cc\u6293\u53d6\u65e0\u7f1d\u8f6c\u6362\uff0c\u4ee5\u53ca\u5206\u5c42\u53cc\u9636\u6bb5\u8fd0\u52a8\u67b6\u6784\u7ed3\u5408\u6a21\u4eff\u5b66\u4e60\u548c\u4e8c\u6b21\u89c4\u5212\u8fdb\u884c\u5b9e\u65f6\u8fd0\u52a8\u89c4\u5212\u3002", "result": "\u5728\u529b\u5bc6\u96c6\u578b\u4efb\u52a1\u8bc4\u4f30\u4e2d\u663e\u793a\u51fa\u6293\u53d6\u8f6c\u6362\u6548\u7387\u548c\u8fd0\u52a8\u6027\u80fd\u7684\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u7a33\u5b9a\u6027\u548c\u7075\u5de7\u6027\uff0c\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u7684\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.19292", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19292", "abs": "https://arxiv.org/abs/2509.19292", "authors": ["Yang Jin", "Jun Lv", "Han Xue", "Wendi Chen", "Chuan Wen", "Cewu Lu"], "title": "SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold Exploration", "comment": null, "summary": "Intelligent agents progress by continually refining their capabilities\nthrough actively exploring environments. Yet robot policies often lack\nsufficient exploration capability due to action mode collapse. Existing methods\nthat encourage exploration typically rely on random perturbations, which are\nunsafe and induce unstable, erratic behaviors, thereby limiting their\neffectiveness. We propose Self-Improvement via On-Manifold Exploration (SOE), a\nframework that enhances policy exploration and improvement in robotic\nmanipulation. SOE learns a compact latent representation of task-relevant\nfactors and constrains exploration to the manifold of valid actions, ensuring\nsafety, diversity, and effectiveness. It can be seamlessly integrated with\narbitrary policy models as a plug-in module, augmenting exploration without\ndegrading the base policy performance. Moreover, the structured latent space\nenables human-guided exploration, further improving efficiency and\ncontrollability. Extensive experiments in both simulation and real-world tasks\ndemonstrate that SOE consistently outperforms prior methods, achieving higher\ntask success rates, smoother and safer exploration, and superior sample\nefficiency. These results establish on-manifold exploration as a principled\napproach to sample-efficient policy self-improvement. Project website:\nhttps://ericjin2002.github.io/SOE", "AI": {"tldr": "SOE\u662f\u4e00\u4e2a\u901a\u8fc7\u6d41\u5f62\u7ea6\u675f\u63a2\u7d22\u6765\u589e\u5f3a\u673a\u5668\u4eba\u7b56\u7565\u81ea\u6211\u6539\u8fdb\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u968f\u673a\u6270\u52a8\u65b9\u6cd5\u7684\u5b89\u5168\u6027\u548c\u7a33\u5b9a\u6027\u95ee\u9898", "motivation": "\u673a\u5668\u4eba\u7b56\u7565\u7531\u4e8e\u52a8\u4f5c\u6a21\u5f0f\u574d\u584c\u800c\u7f3a\u4e4f\u8db3\u591f\u7684\u63a2\u7d22\u80fd\u529b\uff0c\u73b0\u6709\u57fa\u4e8e\u968f\u673a\u6270\u52a8\u7684\u65b9\u6cd5\u4e0d\u5b89\u5168\u4e14\u884c\u4e3a\u4e0d\u7a33\u5b9a\uff0c\u9650\u5236\u4e86\u5176\u6709\u6548\u6027", "method": "\u5b66\u4e60\u4efb\u52a1\u76f8\u5173\u56e0\u7d20\u7684\u7d27\u51d1\u6f5c\u5728\u8868\u793a\uff0c\u5c06\u63a2\u7d22\u7ea6\u675f\u5728\u6709\u6548\u52a8\u4f5c\u7684\u6d41\u5f62\u4e0a\uff0c\u53ef\u4f5c\u4e3a\u63d2\u4ef6\u6a21\u5757\u4e0e\u4efb\u610f\u7b56\u7565\u6a21\u578b\u96c6\u6210", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\uff0cSOE\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u83b7\u5f97\u66f4\u9ad8\u7684\u4efb\u52a1\u6210\u529f\u7387\u3001\u66f4\u5e73\u6ed1\u5b89\u5168\u7684\u63a2\u7d22\u548c\u66f4\u597d\u7684\u6837\u672c\u6548\u7387", "conclusion": "\u6d41\u5f62\u7ea6\u675f\u63a2\u7d22\u4e3a\u6837\u672c\u9ad8\u6548\u7684\u7b56\u7565\u81ea\u6211\u6539\u8fdb\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u65b9\u6cd5"}}
{"id": "2509.19301", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19301", "abs": "https://arxiv.org/abs/2509.19301", "authors": ["Lars Ankile", "Zhenyu Jiang", "Rocky Duan", "Guanya Shi", "Pieter Abbeel", "Anusha Nagabandi"], "title": "Residual Off-Policy RL for Finetuning Behavior Cloning Policies", "comment": null, "summary": "Recent advances in behavior cloning (BC) have enabled impressive visuomotor\ncontrol policies. However, these approaches are limited by the quality of human\ndemonstrations, the manual effort required for data collection, and the\ndiminishing returns from increasing offline data. In comparison, reinforcement\nlearning (RL) trains an agent through autonomous interaction with the\nenvironment and has shown remarkable success in various domains. Still,\ntraining RL policies directly on real-world robots remains challenging due to\nsample inefficiency, safety concerns, and the difficulty of learning from\nsparse rewards for long-horizon tasks, especially for high-degree-of-freedom\n(DoF) systems. We present a recipe that combines the benefits of BC and RL\nthrough a residual learning framework. Our approach leverages BC policies as\nblack-box bases and learns lightweight per-step residual corrections via\nsample-efficient off-policy RL. We demonstrate that our method requires only\nsparse binary reward signals and can effectively improve manipulation policies\non high-degree-of-freedom (DoF) systems in both simulation and the real world.\nIn particular, we demonstrate, to the best of our knowledge, the first\nsuccessful real-world RL training on a humanoid robot with dexterous hands. Our\nresults demonstrate state-of-the-art performance in various vision-based tasks,\npointing towards a practical pathway for deploying RL in the real world.\nProject website: https://residual-offpolicy-rl.github.io", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u884c\u4e3a\u514b\u9686\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u6b8b\u5dee\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528BC\u7b56\u7565\u4f5c\u4e3a\u57fa\u7840\uff0c\u901a\u8fc7\u6837\u672c\u9ad8\u6548\u7684\u79bb\u7b56\u7565RL\u5b66\u4e60\u8f7b\u91cf\u7ea7\u9010\u6b65\u6b8b\u5dee\u4fee\u6b63\uff0c\u5b9e\u73b0\u4e86\u5728\u771f\u5b9e\u4e16\u754c\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u7684\u6210\u529fRL\u8bad\u7ec3\u3002", "motivation": "\u884c\u4e3a\u514b\u9686\u65b9\u6cd5\u53d7\u9650\u4e8e\u4eba\u7c7b\u6f14\u793a\u8d28\u91cf\u3001\u6570\u636e\u6536\u96c6\u6210\u672c\u4ee5\u53ca\u79bb\u7ebf\u6570\u636e\u6536\u76ca\u9012\u51cf\u95ee\u9898\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u80fd\u901a\u8fc7\u81ea\u4e3b\u73af\u5883\u4ea4\u4e92\u8bad\u7ec3\u667a\u80fd\u4f53\uff0c\u4f46\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u8bad\u7ec3\u9762\u4e34\u6837\u672c\u6548\u7387\u4f4e\u3001\u5b89\u5168\u6027\u95ee\u9898\u548c\u7a00\u758f\u5956\u52b1\u5b66\u4e60\u56f0\u96be\u7b49\u6311\u6218\u3002", "method": "\u91c7\u7528\u6b8b\u5dee\u5b66\u4e60\u6846\u67b6\uff0c\u5c06BC\u7b56\u7565\u4f5c\u4e3a\u9ed1\u76d2\u57fa\u7840\uff0c\u901a\u8fc7\u6837\u672c\u9ad8\u6548\u7684\u79bb\u7b56\u7565RL\u5b66\u4e60\u8f7b\u91cf\u7ea7\u7684\u9010\u6b65\u6b8b\u5dee\u4fee\u6b63\uff0c\u4ec5\u9700\u7a00\u758f\u4e8c\u5143\u5956\u52b1\u4fe1\u53f7\u3002", "result": "\u5728\u9ad8\u81ea\u7531\u5ea6\u7cfb\u7edf\u4e0a\u6709\u6548\u6539\u8fdb\u4e86\u64cd\u4f5c\u7b56\u7565\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u4e2d\u90fd\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u7279\u522b\u662f\u5728\u4eba\u5f62\u673a\u5668\u4eba\u7075\u5de7\u624b\u4e0a\u5b9e\u73b0\u4e86\u9996\u4e2a\u6210\u529f\u7684\u771f\u5b9e\u4e16\u754cRL\u8bad\u7ec3\uff0c\u5728\u5404\u79cd\u89c6\u89c9\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u90e8\u7f72RL\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u8def\u5f84\uff0c\u6210\u529f\u7ed3\u5408\u4e86BC\u548cRL\u7684\u4f18\u52bf\uff0c\u89e3\u51b3\u4e86\u5404\u81ea\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}

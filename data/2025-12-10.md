<div id=toc></div>

# Table of Contents

- [econ.EM](#econ.EM) [Total: 5]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.AI](#cs.AI) [Total: 37]
- [cs.SI](#cs.SI) [Total: 4]
- [cs.CY](#cs.CY) [Total: 9]
- [stat.AP](#stat.AP) [Total: 5]


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [1] [Branching Fixed Effects: A Proposal for Communicating Uncertainty](https://arxiv.org/abs/2512.08101)
*Patrick Kline*

Main category: econ.EM

TL;DR: 提出一种网络数据样本分割方法，将双向固定效应估计分解为统计独立的"分支"，每个分支提供参数的无偏估计，便于不确定性量化、矩估计和收缩估计。


<details>
  <summary>Details</summary>
Motivation: 经济学家常依赖其他研究团队开发的线性固定效应模型估计，但评估这些估计的不确定性具有挑战性。需要一种方法来量化网络数据中固定效应估计的不确定性。

Method: 提出网络数据的样本分割方法，将双向固定效应估计分解为统计独立的分支。开发了从大型数据集中高效提取分支的算法。

Result: 每个分支提供参数的无偏估计，便于进行不确定性量化、矩估计和收缩估计。使用意大利威尼托地区的基准数据集进行实证说明，该数据集广泛用于研究企业工资效应。

Conclusion: 该方法为网络数据中的固定效应估计提供了有效的统计推断工具，解决了评估估计不确定性的挑战，特别适用于大型数据集的分析。

Abstract: Economists often rely on estimates of linear fixed effects models developed by other teams of researchers. Assessing the uncertainty in these estimates can be challenging. I propose a form of sample splitting for network data that breaks two-way fixed effects estimates into statistically independent branches, each of which provides an unbiased estimate of the parameters of interest. These branches facilitate uncertainty quantification, moment estimation, and shrinkage. Algorithms are developed for efficiently extracting branches from large datasets. I illustrate these techniques using a benchmark dataset from Veneto, Italy that has been widely used to study firm wage effects.

</details>


### [2] [Robust Counterfactuals in Centralized Schools Choice Systems: Addressing Gender Inequality in STEM Education](https://arxiv.org/abs/2512.08115)
*Lixiong Li,Ismaël Mourifié*

Main category: econ.EM

TL;DR: 提出一种新的反事实分析方法，用于教育市场设计中的Gale-Shapley延迟接受机制，在较弱假设下提供匹配结果的尖锐边界


<details>
  <summary>Details</summary>
Motivation: 反事实分析对教育市场设计至关重要，但现有方法通常需要完全指定效用函数或学生信念，假设过强。需要开发在较弱假设下仍能进行可信政策评估的方法。

Method: 基于对学生行为的可解释限制，构建不完全但灵活偏好模型。结合算法技术和整数规划，高效计算反事实稳定匹配结果的尖锐边界。

Result: 该方法能够处理部分识别挑战，为反事实稳定匹配结果提供尖锐边界。以智利STEM领域增加女性入学政策评估为例进行说明。

Conclusion: 提出了一种在较弱假设下进行反事实分析的新方法，为教育市场设计提供了更灵活、更可信的政策评估工具。

Abstract: Counterfactual analysis is central to education market design and provides a foundation for credible policy recommendations. We develop a novel methodology for counterfactual analysis in Gale-Shapley deferred-acceptance (DA) assignment mechanisms under a weaker set of assumptions than those typically imposed in existing empirical works. Instead of fully specifying utility functions or students' beliefs about admission probabilities, we rely on interpretable restrictions on behavior that yield an incomplete but flexible model of preferences. This framework addresses the challenge of partial identification by delivering sharp bounds on counterfactual stable matching outcomes, which we compute efficiently using a combination of algorithmic techniques and integer programming. We illustrate the methodology by evaluating policies aimed at increasing female enrollment in STEM fields in Chile.

</details>


### [3] [Automatic Debiased Machine Learning of Structural Parameters with General Conditional Moments](https://arxiv.org/abs/2512.08423)
*Facundo Argañaraz*

Main category: econ.EM

TL;DR: 提出一种自动构建Neyman正交矩的方法，用于具有条件矩限制的模型，通过估计正交工具变量实现去偏估计，使用Lasso方法求解函数方程，获得√n一致性和渐近正态性。


<details>
  <summary>Details</summary>
Motivation: 在具有条件矩限制的模型中，特别是当包含非参数组件和机器学习工具时，需要一种能够自动构建正交矩的方法来获得可靠的统计推断。现有方法在处理不同条件变量和内生回归变量时面临挑战。

Method: 通过估计正交工具变量（OR-IVs）——条件变量的"残差化"函数，然后组合得到去偏矩。在近似稀疏性条件下，使用Lasso类型程序求解函数方程，基于此构建两阶段GMM估计器来估计有限维结构参数。

Result: 理论证明了OR-IVs构造的保证，并展示了结构参数估计器的√n一致性和渐近正态性。蒙特卡洛实验和关于企业层面生产函数的实证应用验证了所提推断方法的重要性。

Conclusion: 该方法能够自动构建Neyman正交矩，为具有条件矩限制的模型提供可靠的统计推断，特别适用于包含非参数组件和机器学习工具的应用场景。

Abstract: This paper proposes a method to automatically construct or estimate Neyman-orthogonal moments in general models defined by a finite number of conditional moment restrictions (CMRs), with possibly different conditioning variables and endogenous regressors. CMRs are allowed to depend on non-parametric components, which might be flexibly modeled using Machine Learning tools, and non-linearly on finite-dimensional parameters. The key step in this construction is the estimation of Orthogonal Instrumental Variables (OR-IVs) -- "residualized" functions of the conditioning variables, which are then combined to obtain a debiased moment. We argue that computing OR-IVs necessarily requires solving potentially complicated functional equations, which depend on unknown terms. However, by imposing an approximate sparsity condition, our method finds the solutions to those equations using a Lasso-type program and can then be implemented straightforwardly. Based on this, we introduce a GMM estimator of finite-dimensional parameters (structural parameters) in a two-step framework. We derive theoretical guarantees for our construction of OR-IVs and show $\sqrt{n}$-consistency and asymptotic normality for the estimator of the structural parameters. Our Monte Carlo experiments and an empirical application on estimating firm-level production functions highlight the importance of relying on inference methods like the one proposed.

</details>


### [4] [Minimax and Bayes Optimal Adaptive Experimental Design for Treatment Choice](https://arxiv.org/abs/2512.08513)
*Masahiro Kato*

Main category: econ.EM

TL;DR: 本文提出了一种用于治疗选择的两阶段自适应实验设计，采用Neyman分配方法，证明其在遗憾最小化和贝叶斯最优性方面达到理论下界。


<details>
  <summary>Details</summary>
Motivation: 在二元治疗选择的自适应实验中，如何设计实验以最大化福利（选择期望结果最高的治疗），同时最小化遗憾（选择次优治疗带来的损失）。现有方法可能未达到理论最优性能。

Method: 提出两阶段自适应实验：第一阶段估计各治疗的标准差，第二阶段按标准差比例分配治疗（Neyman分配）。实验分为治疗分配阶段（自适应更新分配概率）和治疗选择阶段（选择最佳治疗）。

Result: 证明Neyman分配实验是minimax和贝叶斯最优的：其遗憾上界与推导出的理论下界完全匹配。使用测度变换论证推导下界，利用中心极限定理和大偏差界评估上界。

Conclusion: 提出的两阶段Neyman分配自适应实验在治疗选择问题中达到了理论最优性能，为自适应实验设计提供了最优解决方案。

Abstract: We consider an adaptive experiment for treatment choice and design a minimax and Bayes optimal adaptive experiment with respect to regret. Given binary treatments, the experimenter's goal is to choose the treatment with the highest expected outcome through an adaptive experiment, in order to maximize welfare. We consider adaptive experiments that consist of two phases, the treatment allocation phase and the treatment choice phase. The experiment starts with the treatment allocation phase, where the experimenter allocates treatments to experimental subjects to gather observations. During this phase, the experimenter can adaptively update the allocation probabilities using the observations obtained in the experiment. After the allocation phase, the experimenter proceeds to the treatment choice phase, where one of the treatments is selected as the best. For this adaptive experimental procedure, we propose an adaptive experiment that splits the treatment allocation phase into two stages, where we first estimate the standard deviations and then allocate each treatment proportionally to its standard deviation. We show that this experiment, often referred to as Neyman allocation, is minimax and Bayes optimal in the sense that its regret upper bounds exactly match the lower bounds that we derive. To show this optimality, we derive minimax and Bayes lower bounds for the regret using change-of-measure arguments. Then, we evaluate the corresponding upper bounds using the central limit theorem and large deviation bounds.

</details>


### [5] [Difference-in-Differences with Interval Data](https://arxiv.org/abs/2512.08759)
*Daisuke Kurisu,Yuta Okamoto,Taisuke Otsu*

Main category: econ.EM

TL;DR: 本文扩展了双重差分法以处理区间结果数据，提出了"平行移动"识别策略，并应用于重新分析Card和Krueger(1994)的最低工资研究。


<details>
  <summary>Details</summary>
Motivation: 双重差分法是评估政策干预因果效应的常用工具，但在实证研究中经常遇到区间结果数据（如调查或行政数据），传统的平行趋势假设在处理这类数据时可能产生无信息或反直觉的结果。

Method: 提出了"平行移动"识别策略作为传统平行趋势假设的替代方案，该方法更适合处理区间结果数据，并展示了如何将扩展的DID方法应用于实际研究。

Result: 通过重新分析Card和Krueger(1994)关于最低工资影响的经典研究，展示了所提方法的实际应用价值和吸引力。

Conclusion: 扩展的DID方法为处理区间结果数据提供了更合适的识别框架，"平行移动"假设比传统的平行趋势假设更适合这类数据，具有重要的实证应用价值。

Abstract: Difference-in-differences (DID) is one of the most popular tools used to evaluate causal effects of policy interventions. This paper extends the DID methodology to accommodate interval outcomes, which are often encountered in empirical studies using survey or administrative data. We point out that a naive application or extension of the conventional parallel trends assumption may yield uninformative or counterintuitive results, and present a suitable identification strategy, called parallel shifts, which exhibits desirable properties. Practical attractiveness of the proposed method is illustrated by revisiting an influential minimum wage study by Card and Krueger (1994).

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [6] [Resonant and Stochastic Vibration in Neurorehabilitation](https://arxiv.org/abs/2512.08009)
*Ava Hays,Nolan Kosnic,Ryan Miller,Kunal Siddhawar*

Main category: cs.ET

TL;DR: 这篇综述探讨了振动干预在神经康复中的应用，分析了随机振动和共振振动两种模式，总结了它们在改善平衡、活动能力和精细运动功能方面的证据，并指出了参数优化、可推广性和安全性等挑战。


<details>
  <summary>Details</summary>
Motivation: 神经系统损伤和年龄相关衰退会损害感觉处理和运动协调能力，而神经可塑性机制的理解为振动干预提供了理论基础，使其成为刺激感觉通路和运动回路以支持功能恢复的潜在工具。

Method: 通过综述随机振动和共振振动两种模式，描述其机制、治疗原理和临床应用，综合评估全身振动在改善老年人、中风幸存者和帕金森病患者平衡、活动能力和精细运动功能方面的证据，并评估聚焦肌肉振动和可穿戴随机共振设备在上肢康复中的最新进展。

Result: 振动干预在改善平衡、活动能力和精细运动功能方面显示出临床潜力，但面临参数优化、可推广性、安全性、可扩展性、生态效度和标准化等挑战。研究识别了影响治疗效果的关键变量，并强调了改进方案、提高可用性和将振动技术整合到更广泛神经康复框架中的持续努力。

Conclusion: 振动干预在神经康复中具有潜力，但需要进一步研究来优化参数、提高可推广性和安全性，以将其转化为可靠且可部署的临床工具。未来的研究需求包括改进方案、提高可用性，并将振动技术更好地整合到神经康复框架中。

Abstract: Neurological injuries and age-related decline can impair sensory processing and disrupt motor coordination, gait, and balance. As mechanisms of neuroplasticity have become better understood, vibration-based interventions have gained attention as potential tools to stimulate sensory pathways and motor circuits to support functional recovery. This survey reviews stochastic and resonant vibration modalities, describing their mechanisms, therapeutic rationales, and clinical applications. We synthesize evidence on whole-body vibration for improving balance, mobility, and fine motor function in aging adults, stroke survivors, and individuals with Parkinson's disease, with attention to challenges in parameter optimization, generalizability, and safety. We also assess recent developments in focused muscle vibration and wearable stochastic resonance devices for upper-limb rehabilitation, evaluating their clinical promise along with limitations in scalability, ecological validity, and standardization. Across these modalities, we identify key variables that shape therapeutic outcomes and highlight ongoing efforts to refine protocols, improve usability, and integrate vibration techniques into broader neurorehabilitation frameworks. We conclude by outlining the most important research needs for translating vibration-based interventions into reliable and deployable clinical tools.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [Impact of Data-Oriented and Object-Oriented Design on Performance and Cache Utilization with Artificial Intelligence Algorithms in Multi-Threaded CPUs](https://arxiv.org/abs/2512.07841)
*Gabriel M. Arantes,Richard F. Pinto,Bruno L. Dalmazo,Eduardo N. Borges,Giancarlo Lucca,Viviane L. D. de Mattos,Fabian C. Cardoso,Rafael A. Berri*

Main category: cs.AI

TL;DR: 数据导向设计(DOD)在多线程环境下相比面向对象设计(OOD)在缓存利用和执行效率方面表现更优，但细粒度任务中单线程版本仍优于多线程版本


<details>
  <summary>Details</summary>
Motivation: 随着多核CPU与主内存性能差距增大，需要硬件感知的软件设计范式。研究旨在全面分析数据导向设计与传统面向对象设计在多线程环境下的性能差异，重点关注缓存利用效率

Method: 开发并比较了A*搜索算法的四个版本：单线程OOD、单线程DOD、多线程OOD、多线程DOD。评估指标包括执行时间、内存使用和CPU缓存未命中率

Result: 多线程测试中，DOD实现表现出显著性能优势，执行时间更快，系统调用和缓存未命中更少。对于A*这类细粒度任务，线程管理开销导致单线程版本在两个范式中都显著优于多线程版本

Conclusion: 即使在简单算法中性能差异看似细微，DOD在关键指标上的持续优势凸显了其架构优越性，表明对于复杂、大规模AI和并行计算任务，DOD是最大化硬件效率的更有效方法

Abstract: The growing performance gap between multi-core CPUs and main memory necessitates hardware-aware software design paradigms. This study provides a comprehensive performance analysis of Data Oriented Design (DOD) versus the traditional Object-Oriented Design (OOD), focusing on cache utilization and efficiency in multi-threaded environments. We developed and compared four distinct versions of the A* search algorithm: single-threaded OOD (ST-OOD), single-threaded DOD (ST-DOD), multi-threaded OOD (MT-OOD), and multi-threaded DOD (MT-DOD). The evaluation was based on metrics including execution time, memory usage, and CPU cache misses. In multi-threaded tests, the DOD implementation demonstrated considerable performance gains, with faster execution times and a lower number of raw system calls and cache misses. While OOD occasionally showed marginal advantages in memory usage or percentage-based cache miss rates, DOD's efficiency in data-intensive operations was more evident. Furthermore, our findings reveal that for a fine-grained task like the A* algorithm, the overhead associated with thread management led to single-threaded versions significantly outperforming their multi-threaded counterparts in both paradigms. We conclude that even when performance differences appear subtle in simple algorithms, the consistent advantages of DOD in critical metrics highlight its foundational architectural superiority, suggesting it is a more effective approach for maximizing hardware efficiency in complex, large-scale AI and parallel computing tasks.

</details>


### [8] [Can AI autonomously build, operate, and use the entire data stack?](https://arxiv.org/abs/2512.07926)
*Arvind Agarwal,Lisa Amini,Sameep Mehta,Horst Samulowitz,Kavitha Srinivas*

Main category: cs.AI

TL;DR: 论文提出从AI辅助数据管理向完全自主数据资产管理的范式转变，探索智能代理如何自主管理现代数据栈的各个阶段，构建自给自足的系统。


<details>
  <summary>Details</summary>
Motivation: 当前企业数据管理任务繁重，涵盖架构、集成、质量、治理等多个方面。虽然AI助手能帮助特定角色（如数据工程师、管理员）导航和配置数据栈，但远未实现完全自动化。随着AI能力提升，处理固有复杂性任务成为可能，存在实现完全自主数据资产管理的迫切机会。

Method: 论文主张从AI在独立数据组件操作中的应用转向更整体、自主地处理整个数据生命周期的范式转变。探索如何通过智能代理自主管理现代数据栈的每个阶段，构建自给自足的系统，不仅供人类终端用户使用，也能为AI自身服务。

Result: 论文分析了推动这一范式转变的驱动力和机遇，探讨了代理如何简化数据生命周期，并指出了需要进一步研究的开放问题和领域。

Conclusion: 希望这项工作能激发热烈讨论，促进进一步研究，推动协作方法，为数据系统实现更自主的未来奠定基础。

Abstract: Enterprise data management is a monumental task. It spans data architecture and systems, integration, quality, governance, and continuous improvement. While AI assistants can help specific persona, such as data engineers and stewards, to navigate and configure the data stack, they fall far short of full automation. However, as AI becomes increasingly capable of tackling tasks that have previously resisted automation due to inherent complexities, we believe there is an imminent opportunity to target fully autonomous data estates. Currently, AI is used in different parts of the data stack, but in this paper, we argue for a paradigm shift from the use of AI in independent data component operations towards a more holistic and autonomous handling of the entire data lifecycle. Towards that end, we explore how each stage of the modern data stack can be autonomously managed by intelligent agents to build self-sufficient systems that can be used not only by human end-users, but also by AI itself. We begin by describing the mounting forces and opportunities that demand this paradigm shift, examine how agents can streamline the data lifecycle, and highlight open questions and areas where additional research is needed. We hope this work will inspire lively debate, stimulate further research, motivate collaborative approaches, and facilitate a more autonomous future for data systems.

</details>


### [9] [SkipKV: Selective Skipping of KV Generation and Storage for Efficient Inference with Large Reasoning Models](https://arxiv.org/abs/2512.07993)
*Jiayi Tian,Seyedarmin Azizi,Yequan Zhao,Erfan Baghaei Potraghloo,Sean McPherson,Sharath Nittur Sridhar,Zhengyang Wang,Zheng Zhang,Massoud Pedram,Souvik Kundu*

Main category: cs.AI

TL;DR: SkipKV是一种无需训练的KV缓存压缩方法，通过句子级评分和选择性删除来减少推理过程中的内存开销，同时保持推理准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在推理过程中会产生线性增长的KV缓存开销，导致内存和吞吐量瓶颈。现有的KV缓存逐出方法在多批次设置中无法保持准确性，且会生成长度更长的序列。

Method: SkipKV采用句子级评分指标来识别和删除高度相似的句子，同时保持语义连贯性。通过动态调整引导向量来更新隐藏激活状态，抑制冗余生成，使模型产生更简洁的响应。

Result: 在多个推理基准测试中，SkipKV在相似压缩预算下比替代方法保持高达26.7%的准确性提升，生成长度减少1.6倍，吞吐量提高1.7倍。

Conclusion: SkipKV提供了一种有效的训练无关的KV压缩方法，能够在减少内存开销的同时保持推理模型的准确性，解决了现有方法在多批次设置中的局限性。

Abstract: Large reasoning models (LRMs) often cost significant key-value (KV) cache overhead, due to their linear growth with the verbose chain-of-thought (CoT) reasoning process. This costs both memory and throughput bottleneck limiting their efficient deployment. Towards reducing KV cache size during inference, we first investigate the effectiveness of existing KV cache eviction methods for CoT reasoning. Interestingly, we find that due to unstable token-wise scoring and the reduced effective KV budget caused by padding tokens, state-of-the-art (SoTA) eviction methods fail to maintain accuracy in the multi-batch setting. Additionally, these methods often generate longer sequences than the original model, as semantic-unaware token-wise eviction leads to repeated revalidation during reasoning. To address these issues, we present \textbf{SkipKV}, a \textbf{\textit{training-free}} KV compression method for selective \textit{eviction} and \textit{generation} operating at a coarse-grained sentence-level sequence removal for efficient CoT reasoning. In specific, it introduces a \textit{sentence-scoring metric} to identify and remove highly similar sentences while maintaining semantic coherence. To suppress redundant generation, SkipKV dynamically adjusts a steering vector to update the hidden activation states during inference enforcing the LRM to generate concise response. Extensive evaluations on multiple reasoning benchmarks demonstrate the effectiveness of SkipKV in maintaining up to $\mathbf{26.7}\%$ improved accuracy compared to the alternatives, at a similar compression budget. Additionally, compared to SoTA, SkipKV yields up to $\mathbf{1.6}\times$ fewer generation length while improving throughput up to $\mathbf{1.7}\times$.

</details>


### [10] [Toward an AI Reasoning-Enabled System for Patient-Clinical Trial Matching](https://arxiv.org/abs/2512.08026)
*Caroline N. Leach,Mitchell A. Klusty,Samuel E. Armstrong,Justine C. Pickarski,Kristen L. Hankins,Emily B. Collier,Maya Shah,Aaron D. Mullen,V. K. Cody Bumgardner*

Main category: cs.AI

TL;DR: 提出一个基于LLM的AI增强患者-试验匹配系统，用于自动化临床试验资格筛选，解决数据整合、专家审查和安全性等实施挑战。


<details>
  <summary>Details</summary>
Motivation: 临床试验患者筛选目前主要依赖人工，过程耗时且资源密集，需要自动化解决方案来提高效率和扩大试验考虑范围。

Method: 使用开源、具备推理能力的大型语言模型，构建安全可扩展的系统，整合异构电子健康记录数据，生成结构化资格评估和可解释的推理链，支持人机协同审查。

Result: 系统能够超越二元分类，将资格表示为动态状态而非固定判定，识别匹配并提供可操作建议，减少协调员负担，智能扩大试验考虑范围，并保证AI输出的全面可审计性。

Conclusion: 该AI增强的患者-试验匹配系统为解决临床试验筛选的关键实施挑战提供了概念验证，有望通过自动化提高效率，同时保持严格的安 全标准和人类监督。

Abstract: Screening patients for clinical trial eligibility remains a manual, time-consuming, and resource-intensive process. We present a secure, scalable proof-of-concept system for Artificial Intelligence (AI)-augmented patient-trial matching that addresses key implementation challenges: integrating heterogeneous electronic health record (EHR) data, facilitating expert review, and maintaining rigorous security standards. Leveraging open-source, reasoning-enabled large language models (LLMs), the system moves beyond binary classification to generate structured eligibility assessments with interpretable reasoning chains that support human-in-the-loop review. This decision support tool represents eligibility as a dynamic state rather than a fixed determination, identifying matches when available and offering actionable recommendations that could render a patient eligible in the future. The system aims to reduce coordinator burden, intelligently broaden the set of trials considered for each patient and guarantee comprehensive auditability of all AI-generated outputs.

</details>


### [11] [Large Language Models for Education and Research: An Empirical and User Survey-based Analysis](https://arxiv.org/abs/2512.08057)
*Md Mostafizer Rahman,Ariful Islam Shiplu,Md Faizul Ibne Amin,Yutaka Watanobe,Lu Peng*

Main category: cs.AI

TL;DR: 本研究对ChatGPT和DeepSeek两大LLM进行全面评估，发现ChatGPT在通用语言理解和文本生成方面表现优异，而DeepSeek在编程任务上更胜一筹，两者在医疗诊断和数学问题解决方面均表现准确。


<details>
  <summary>Details</summary>
Motivation: 随着预训练大语言模型在教育研究领域的广泛应用，需要全面评估不同LLM的性能差异，以了解它们在教育研究中的实际应用价值和局限性。

Method: 采用背景技术分析、实证实验和真实用户调查相结合的方法，从模型准确性、计算效率和用户体验三个维度进行评估，包括文本生成、编程和专门问题解决等基准测试。

Result: ChatGPT在通用语言理解和文本生成方面表现更好，DeepSeek在编程任务上因效率导向设计而表现更优，两者在医疗诊断和复杂数学问题解决方面均能提供准确输出。

Conclusion: 不同LLM在不同任务领域各有优势，用户调查揭示了这些模型在教育研究中的实际应用价值和局限性，为未来LLM在教育研究领域的优化和应用提供了重要参考。

Abstract: Pretrained Large Language Models (LLMs) have achieved remarkable success across diverse domains, with education and research emerging as particularly impactful areas. Among current state-of-the-art LLMs, ChatGPT and DeepSeek exhibit strong capabilities in mathematics, science, medicine, literature, and programming. In this study, we present a comprehensive evaluation of these two LLMs through background technology analysis, empirical experiments, and a real-world user survey. The evaluation explores trade-offs among model accuracy, computational efficiency, and user experience in educational and research affairs. We benchmarked these LLMs performance in text generation, programming, and specialized problem-solving. Experimental results show that ChatGPT excels in general language understanding and text generation, while DeepSeek demonstrates superior performance in programming tasks due to its efficiency-focused design. Moreover, both models deliver medically accurate diagnostic outputs and effectively solve complex mathematical problems. Complementing these quantitative findings, a survey of students, educators, and researchers highlights the practical benefits and limitations of these models, offering deeper insights into their role in advancing education and research.

</details>


### [12] [Scalable Back-End for an AI-Based Diabetes Prediction Application](https://arxiv.org/abs/2512.08147)
*Henry Anand Septian Radityo,Bernardus Willson,Reynard Tanadi,Latifa Dwiyanti,Saiful Akbar*

Main category: cs.AI

TL;DR: 开发用于糖尿病预测移动应用的扩展后端系统，采用水平扩展、数据库分片和消息队列，83%功能满足性能目标，支持1万并发用户


<details>
  <summary>Details</summary>
Motivation: 全球糖尿病患病率上升需要早期检测，AI预测应用需要响应式、可扩展的后端架构来有效服务大量用户

Method: 采用水平扩展、数据库分片和通过消息队列的异步通信架构，使用RabbitMQ处理计算密集型预测请求

Result: 83%的系统功能（24个中的20个）满足性能目标（故障率<5%，平均延迟<1000ms），用户管理、活动跟踪和读取密集型预测操作表现良好，系统可处理1万并发用户

Conclusion: 该可扩展后端系统成功满足糖尿病预测应用的性能要求，异步通信机制对降低计算密集型请求的错误率至关重要，验证了系统在大规模用户场景下的可靠性

Abstract: The rising global prevalence of diabetes necessitates early detection to prevent severe complications. While AI-powered prediction applications offer a promising solution, they require a responsive and scalable back-end architecture to serve a large user base effectively. This paper details the development and evaluation of a scalable back-end system designed for a mobile diabetes prediction application. The primary objective was to maintain a failure rate below 5% and an average latency of under 1000 ms. The architecture leverages horizontal scaling, database sharding, and asynchronous communication via a message queue. Performance evaluation showed that 83% of the system's features (20 out of 24) met the specified performance targets. Key functionalities such as user profile management, activity tracking, and read-intensive prediction operations successfully achieved the desired performance. The system demonstrated the ability to handle up to 10,000 concurrent users without issues, validating its scalability. The implementation of asynchronous communication using RabbitMQ proved crucial in minimizing the error rate for computationally intensive prediction requests, ensuring system reliability by queuing requests and preventing data loss under heavy load.

</details>


### [13] [Empowerment Gain and Causal Model Construction: Children and adults are sensitive to controllability and variability in their causal interventions](https://arxiv.org/abs/2512.08230)
*Eunice Yiu,Kelsey Allen,Shiry Ginosar,Alison Gopnik*

Main category: cs.AI

TL;DR: 该论文探讨了"赋权"（empowerment）作为连接贝叶斯因果学习和强化学习的桥梁，并研究人类如何利用赋权线索进行因果推理和干预设计。


<details>
  <summary>Details</summary>
Motivation: 当前大型预训练模型在因果学习和建模方面存在困难，而认知科学中的因果贝叶斯网络形式化方法已成功用于理解人类因果学习。需要寻找连接经典贝叶斯因果学习和强化学习的桥梁，以更好地理解人类因果学习并实现机器因果学习。

Method: 提出"赋权"（empowerment）作为连接两种学习范式的桥梁，通过实证研究系统测试儿童和成人如何使用赋权线索来推断因果关系并设计有效的因果干预。

Result: 论文通过实证研究验证了赋权在人类因果学习中的作用，展示了赋权如何解释儿童因果学习的特征，并提供更易处理的因果学习计算解释。

Conclusion: 赋权可能是连接贝叶斯因果学习和强化学习的重要桥梁，既能解释人类因果学习的特征，又能为机器实现因果学习提供更易处理的途径。学习准确的因果世界模型会增加赋权，而增加赋权又会促进更准确的因果世界模型学习。

Abstract: Learning about the causal structure of the world is a fundamental problem for human cognition. Causal models and especially causal learning have proved to be difficult for large pretrained models using standard techniques of deep learning. In contrast, cognitive scientists have applied advances in our formal understanding of causation in computer science, particularly within the Causal Bayes Net formalism, to understand human causal learning. In the very different tradition of reinforcement learning, researchers have described an intrinsic reward signal called "empowerment" which maximizes mutual information between actions and their outcomes. "Empowerment" may be an important bridge between classical Bayesian causal learning and reinforcement learning and may help to characterize causal learning in humans and enable it in machines. If an agent learns an accurate causal world model, they will necessarily increase their empowerment, and increasing empowerment will lead to a more accurate causal world model. Empowerment may also explain distinctive features of childrens causal learning, as well as providing a more tractable computational account of how that learning is possible. In an empirical study, we systematically test how children and adults use cues to empowerment to infer causal relations, and design effective causal interventions.

</details>


### [14] [Beyond Traditional Diagnostics: Transforming Patient-Side Information into Predictive Insights with Knowledge Graphs and Prototypes](https://arxiv.org/abs/2512.08261)
*Yibowen Zhao,Yinan Zhang,Zhixiang Su,Lizhen Cui,Chunyan Miao*

Main category: cs.AI

TL;DR: KPI框架通过知识图谱增强、原型感知和可解释性技术，从患者侧信息预测疾病，解决数据不平衡和可解释性不足问题。


<details>
  <summary>Details</summary>
Motivation: 仅基于患者侧信息（如人口统计数据和自我报告症状）预测疾病具有重要价值，可提高患者意识、促进早期医疗参与和提升医疗系统效率。但现有方法面临疾病分布不平衡和缺乏可解释性等关键挑战，导致预测存在偏差或不可靠。

Method: 提出KPI框架：1) 将结构化可信医疗知识整合到统一疾病知识图谱；2) 构建临床有意义的疾病原型；3) 使用对比学习提高预测准确性，特别是对长尾疾病；4) 利用大语言模型生成患者特异性、医学相关的解释。

Result: 在真实世界数据集上的广泛实验表明，KPI在预测准确性上优于最先进方法，并提供与患者叙述密切相关的临床有效解释。

Conclusion: KPI框架通过整合医疗知识、构建疾病原型和增强可解释性，为以患者为中心的医疗服务提供了实用价值，解决了现有疾病预测方法的关键局限性。

Abstract: Predicting diseases solely from patient-side information, such as demographics and self-reported symptoms, has attracted significant research attention due to its potential to enhance patient awareness, facilitate early healthcare engagement, and improve healthcare system efficiency. However, existing approaches encounter critical challenges, including imbalanced disease distributions and a lack of interpretability, resulting in biased or unreliable predictions. To address these issues, we propose the Knowledge graph-enhanced, Prototype-aware, and Interpretable (KPI) framework. KPI systematically integrates structured and trusted medical knowledge into a unified disease knowledge graph, constructs clinically meaningful disease prototypes, and employs contrastive learning to enhance predictive accuracy, which is particularly important for long-tailed diseases. Additionally, KPI utilizes large language models (LLMs) to generate patient-specific, medically relevant explanations, thereby improving interpretability and reliability. Extensive experiments on real-world datasets demonstrate that KPI outperforms state-of-the-art methods in predictive accuracy and provides clinically valid explanations that closely align with patient narratives, highlighting its practical value for patient-centered healthcare delivery.

</details>


### [15] [Reasoning Models Ace the CFA Exams](https://arxiv.org/abs/2512.08270)
*Jaisal Patel,Yunzhe Chen,Kaiwen He,Keyi Wang,David Li,Kairong Xiao,Xiao-Yang Liu*

Main category: cs.AI

TL;DR: 大型语言模型在CFA考试中表现优异，多个模型通过所有三个级别考试，其中Gemini 3.0 Pro在Level I创下97.6%的最高分纪录。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明大型语言模型在CFA考试中表现不佳，但近期推理模型在各种学科的研究生级别考试中表现出色，因此需要评估最新推理模型在CFA考试中的表现。

Method: 使用980道模拟CFA考试题目（包括三个Level I考试、两个Level II考试和三个Level III考试），评估最先进的推理模型表现，采用与先前研究相同的通过/失败标准。

Result: 大多数模型通过了所有三个级别考试。按总体表现排序，通过模型为：Gemini 3.0 Pro、Gemini 2.5 Pro、GPT-5、Grok 4、Claude Opus 4.1和DeepSeek-V3.1。Gemini 3.0 Pro在Level I创下97.6%纪录；GPT-5在Level II以94.3%领先；Level III中，Gemini 2.5 Pro在选择题获得86.4%最高分，Gemini 3.0 Pro在构建回答题获得92.0%。

Conclusion: 最新推理模型在CFA考试中表现出色，大多数模型能够通过所有三个级别，表明大型语言模型在专业金融考试方面的能力显著提升。

Abstract: Previous research has reported that large language models (LLMs) demonstrate poor performance on the Chartered Financial Analyst (CFA) exams. However, recent reasoning models have achieved strong results on graduate-level academic and professional examinations across various disciplines. In this paper, we evaluate state-of-the-art reasoning models on a set of mock CFA exams consisting of 980 questions across three Level I exams, two Level II exams, and three Level III exams. Using the same pass/fail criteria from prior studies, we find that most models clear all three levels. The models that pass, ordered by overall performance, are Gemini 3.0 Pro, Gemini 2.5 Pro, GPT-5, Grok 4, Claude Opus 4.1, and DeepSeek-V3.1. Specifically, Gemini 3.0 Pro achieves a record score of 97.6% on Level I. Performance is also strong on Level II, led by GPT-5 at 94.3%. On Level III, Gemini 2.5 Pro attains the highest score with 86.4% on multiple-choice questions while Gemini 3.0 Pro achieves 92.0% on constructed-response questions.

</details>


### [16] [The High Cost of Incivility: Quantifying Interaction Inefficiency via Multi-Agent Monte Carlo Simulations](https://arxiv.org/abs/2512.08345)
*Benedikt Mangold*

Main category: cs.AI

TL;DR: 该研究使用LLM多智能体系统模拟对抗性辩论，量化毒性行为对对话效率的影响，发现毒性参与者使对话时间增加约25%


<details>
  <summary>Details</summary>
Motivation: 工作场所毒性行为对组织文化有害，但量化其对运营效率的直接影响存在方法论挑战，因为难以在人类受试者中重现冲突

Method: 使用基于LLM的多智能体系统模拟1对1对抗性辩论，创建受控的"社会学沙盒"。采用蒙特卡洛方法模拟数百次讨论，测量基线对照组与包含"毒性"系统提示的智能体处理组之间的收敛时间

Result: 涉及毒性参与者的对话持续时间显著增加约25%，这种"毒性延迟"可作为企业和学术环境中财务损失的代理指标

Conclusion: 基于智能体的建模为测量社会摩擦机制提供了可重复、符合伦理的人类受试者研究替代方案

Abstract: Workplace toxicity is widely recognized as detrimental to organizational culture, yet quantifying its direct impact on operational efficiency remains methodologically challenging due to the ethical and practical difficulties of reproducing conflict in human subjects. This study leverages Large Language Model (LLM) based Multi-Agent Systems to simulate 1-on-1 adversarial debates, creating a controlled "sociological sandbox". We employ a Monte Carlo method to simulate hundrets of discussions, measuring the convergence time (defined as the number of arguments required to reach a conclusion) between a baseline control group and treatment groups involving agents with "toxic" system prompts. Our results demonstrate a statistically significant increase of approximately 25\% in the duration of conversations involving toxic participants. We propose that this "latency of toxicity" serves as a proxy for financial damage in corporate and academic settings. Furthermore, we demonstrate that agent-based modeling provides a reproducible, ethical alternative to human-subject research for measuring the mechanics of social friction.

</details>


### [17] [AgentEval: Generative Agents as Reliable Proxies for Human Evaluation of AI-Generated Content](https://arxiv.org/abs/2512.08273)
*Thanh Vu,Richi Nayak,Thiru Balasubramaniam*

Main category: cs.AI

TL;DR: 该研究提出使用生成式代理来自动评估AI生成内容的质量，以解决人工评估成本高、效率低的问题，帮助企业更高效地生成高质量内容。


<details>
  <summary>Details</summary>
Motivation: 现代企业在内容创作中面临时间和成本挑战：人工写作耗时，外部评估昂贵。虽然大语言模型有潜力，但AI生成内容的质量担忧依然存在，传统人工评估方法进一步增加运营成本，需要高效自动化解决方案。

Method: 引入生成式代理来评估AI生成内容，这些代理能够模拟人类判断，快速且经济地对内容的连贯性、趣味性、清晰度、公平性和相关性等方面进行评分。

Result: 通过使用生成式代理，企业可以简化内容生成流程，确保一致的高质量输出，同时减少对昂贵人工评估的依赖。

Conclusion: 该研究为增强大语言模型生成符合业务需求的高质量内容提供了关键见解，在自动化内容生成和评估方面取得了重要进展。

Abstract: Modern businesses are increasingly challenged by the time and expense required to generate and assess high-quality content. Human writers face time constraints, and extrinsic evaluations can be costly. While Large Language Models (LLMs) offer potential in content creation, concerns about the quality of AI-generated content persist. Traditional evaluation methods, like human surveys, further add operational costs, highlighting the need for efficient, automated solutions. This research introduces Generative Agents as a means to tackle these challenges. These agents can rapidly and cost-effectively evaluate AI-generated content, simulating human judgment by rating aspects such as coherence, interestingness, clarity, fairness, and relevance. By incorporating these agents, businesses can streamline content generation and ensure consistent, high-quality output while minimizing reliance on costly human evaluations. The study provides critical insights into enhancing LLMs for producing business-aligned, high-quality content, offering significant advancements in automated content generation and evaluation.

</details>


### [18] [The SMART+ Framework for AI Systems](https://arxiv.org/abs/2512.08592)
*Laxmiraju Kandikatla,Branislav Radeljic*

Main category: cs.AI

TL;DR: 提出SMART+框架，一个基于安全、监控、问责、可靠性和透明度支柱的结构化模型，用于评估和治理跨行业AI系统，特别关注临床研究中的负责任AI采用。


<details>
  <summary>Details</summary>
Motivation: AI系统在医疗、金融、制造等多个行业广泛应用，虽然提高了运营效率，但也带来了安全、问责和监管合规方面的新挑战，需要系统化的治理框架来应对这些风险。

Method: 提出SMART+框架，基于五个核心支柱（安全、监控、问责、可靠性、透明度），并增强隐私与安全、数据治理、公平性与偏见、防护措施等维度，提供评估和治理AI系统的结构化方法。

Result: SMART+框架展示了风险缓解、信任建立和合规准备的能力，为临床研究中的有效AI治理提供了坚实基础，支持负责任的AI采用并确保可审计性。

Conclusion: SMART+框架提供了一个实用、全面的方法来评估和治理跨行业AI系统，与不断发展的监管机制和指南保持一致，通过整合运营保障、监督程序以及强化的隐私和治理控制，支持负责任的AI采用。

Abstract: Artificial Intelligence (AI) systems are now an integral part of multiple industries. In clinical research, AI supports automated adverse event detection in clinical trials, patient eligibility screening for protocol enrollment, and data quality validation. Beyond healthcare, AI is transforming finance through real-time fraud detection, automated loan risk assessment, and algorithmic decision-making. Similarly, in manufacturing, AI enables predictive maintenance to reduce equipment downtime, enhances quality control through computer-vision inspection, and optimizes production workflows using real-time operational data. While these technologies enhance operational efficiency, they introduce new challenges regarding safety, accountability, and regulatory compliance. To address these concerns, we introduce the SMART+ Framework - a structured model built on the pillars of Safety, Monitoring, Accountability, Reliability, and Transparency, and further enhanced with Privacy & Security, Data Governance, Fairness & Bias, and Guardrails. SMART+ offers a practical, comprehensive approach to evaluating and governing AI systems across industries. This framework aligns with evolving mechanisms and regulatory guidance to integrate operational safeguards, oversight procedures, and strengthened privacy and governance controls. SMART+ demonstrates risk mitigation, trust-building, and compliance readiness. By enabling responsible AI adoption and ensuring auditability, SMART+ provides a robust foundation for effective AI governance in clinical research.

</details>


### [19] [Towards a Science of Scaling Agent Systems](https://arxiv.org/abs/2512.08296)
*Yubin Kim,Ken Gu,Chanwoo Park,Chunjong Park,Samuel Schmidgall,A. Ali Heydari,Yao Yan,Zhihan Zhang,Yuchen Zhuang,Mark Malhotra,Paul Pu Liang,Hae Won Park,Yuzhe Yang,Xuhai Xu,Yilun Du,Shwetak Patel,Tim Althoff,Daniel McDuff,Xin Liu*

Main category: cs.AI

TL;DR: 本文提出了基于语言模型的智能体系统的定量扩展原则，通过实证研究揭示了多智能体协调中的关键权衡效应，并建立了预测模型来选择最优协调策略。


<details>
  <summary>Details</summary>
Motivation: 尽管基于语言模型的智能体系统在现实AI应用中日益普及，但其性能决定原则仍未被充分探索，导致实践者依赖启发式方法而非原则性设计选择。本文旨在填补这一空白。

Method: 在四个多样化基准测试（Finance-Agent、BrowseComp-Plus、PlanCraft、Workbench）上，使用五种典型架构（单智能体、独立、集中式、分散式、混合式）和三个LLM家族，进行了180种配置的受控评估。通过经验协调指标（效率、开销、错误放大、冗余）建立了预测模型。

Result: 识别出三个主导效应：1）工具协调权衡：在固定计算预算下，工具密集型任务受多智能体开销影响更大；2）能力饱和：当单智能体基线超过约45%时，协调产生递减或负回报；3）拓扑依赖的错误放大：独立智能体将错误放大17.2倍，而集中式协调控制在4.4倍。预测模型在交叉验证中达到R²=0.513，能够为87%的保留配置预测最优协调策略。

Conclusion: 本文提出了一个基于可测量任务特性的智能体扩展预测原则框架，为智能体系统的原则性设计提供了定量指导，能够有效预测最优协调策略。

Abstract: Agents, language model (LM)-based systems that are capable of reasoning, planning, and acting are becoming the dominant paradigm for real-world AI applications. Despite this widespread adoption, the principles that determine their performance remain underexplored, leaving practitioners to rely on heuristics rather than principled design choices. We address this gap by deriving quantitative scaling principles for agent systems. We evaluate this across four diverse benchmarks: Finance-Agent, BrowseComp-Plus, PlanCraft, and Workbench. Using five canonical architectures (Single, Independent, Centralized, Decentralized, Hybrid) instantiated across three LLM families, we perform a controlled evaluation spanning 180 configurations with standardized tools and token budgets. We derive a predictive model using empirical coordination metrics, including efficiency, overhead, error amplification, and redundancy, that achieves cross-validated R^2=0.513. We identify three dominant effects: (1) a tool-coordination trade-off: under fixed computational budgets, tool-heavy tasks suffer disproportionately from multi-agent overhead. (2) a capability saturation: coordination yields diminishing or negative returns (beta=-0.408, p<0.001) once single-agent baselines exceed ~45%. (3) topology-dependent error amplification: independent agents amplify errors 17.2x through unchecked propagation, while centralized coordination contains this to 4.4x. Centralized coordination improves performance by 80.9% on parallelizable tasks like financial reasoning, while decentralized coordination excels on dynamic web navigation (+9.2% vs. +0.2%). Yet for sequential reasoning tasks, all multi-agent variants degraded performance by 39-70%. The framework predicts the optimal coordination strategy for 87% of held-out configurations, providing a predictive principle of agentic scaling based on measurable task properties.

</details>


### [20] [rSIM: Incentivizing Reasoning Capabilities of LLMs via Reinforced Strategy Injection](https://arxiv.org/abs/2512.08300)
*Sijia Chen,Baochun Li,Di Niu*

Main category: cs.AI

TL;DR: 论文提出rSIM机制，通过小型规划器指导LLM的思维链，使用多智能体强化学习联合训练规划器和LLM，使普通LLM进化为推理语言模型（RLM），显著提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型通过强化学习后训练可以进化为推理语言模型，表现出"顿悟"时刻和策略性推理。受此启发，希望开发一种机制能让任何LLM都具备这种高级推理能力。

Method: 提出强化策略注入机制（rSIM），使用小型规划器（领导者智能体）指导LLM（跟随者智能体）的思维链，通过自适应注入推理策略。采用领导者-跟随者框架，基于多智能体强化学习和简单规则奖励联合训练规划器和LLM。

Result: rSIM使Qwen2.5-0.5B进化为RLM，性能显著超越Qwen2.5-14B。规划器具有通用性：只需训练一次即可作为插件大幅提升现有LLM的推理能力。规划器支持跨任务持续学习，规划能力可逐步提升并泛化到更广泛问题。

Conclusion: rSIM机制成功实现了让普通LLM进化为推理语言模型的目标，通过小型规划器的策略注入显著提升了模型的推理能力，且具有通用性和持续学习能力，为LLM推理能力提升提供了有效方案。

Abstract: Large language models (LLMs) are post-trained through reinforcement learning (RL) to evolve into Reasoning Language Models (RLMs), where the hallmark of this advanced reasoning is ``aha'' moments when they start to perform strategies, such as self-reflection and deep thinking, within chain of thoughts (CoTs). Motivated by this, this paper proposes a novel reinforced strategy injection mechanism (rSIM), that enables any LLM to become an RLM by employing a small planner to guide the LLM's CoT through the adaptive injection of reasoning strategies. To achieve this, the planner (leader agent) is jointly trained with an LLM (follower agent) using multi-agent RL (MARL), based on a leader-follower framework and straightforward rule-based rewards. Experimental results show that rSIM enables Qwen2.5-0.5B to become an RLM and significantly outperform Qwen2.5-14B. Moreover, the planner is generalizable: it only needs to be trained once and can be applied as a plug-in to substantially improve the reasoning capabilities of existing LLMs. In addition, the planner supports continual learning across various tasks, allowing its planning abilities to gradually improve and generalize to a wider range of problems.

</details>


### [21] [Predicting California Bearing Ratio with Ensemble and Neural Network Models: A Case Study from Türkiye](https://arxiv.org/abs/2512.08340)
*Abdullah Hulusi Kökçam,Uğur Dağdeviren,Talas Fikret Kurnaz,Alparslan Serhat Demir,Caner Erden*

Main category: cs.AI

TL;DR: 本研究开发了一个机器学习框架，利用土耳其382个土壤样本的物理化学特性预测加州承载比，随机森林回归器表现最佳，为岩土工程提供了传统测试的有效替代方案。


<details>
  <summary>Details</summary>
Motivation: 传统CBR测试方法耗时、昂贵且不适用于大规模或多样化土壤剖面，而机器学习方法能够以更快速度和更高精度建模复杂土壤行为，为岩土工程提供数据驱动的智能解决方案。

Method: 收集土耳其不同地理气候区域的382个土壤样本，提取与承载能力相关的物理化学特性，在监督学习框架下测试12种机器学习算法（包括决策树、随机森林、梯度提升、XGBoost、KNN、SVR、MLP等），进行训练、验证和评估。

Result: 随机森林回归器表现最佳，训练集R²为0.95，验证集为0.76，测试集为0.83，显示出强大的非线性映射能力，是预测性岩土任务的有前景工具。

Conclusion: 研究支持将智能、数据中心的模型整合到岩土工程中，为传统方法提供有效替代方案，促进基础设施分析和设计的数字化转型。

Abstract: The California Bearing Ratio (CBR) is a key geotechnical indicator used to assess the load-bearing capacity of subgrade soils, especially in transportation infrastructure and foundation design. Traditional CBR determination relies on laboratory penetration tests. Despite their accuracy, these tests are often time-consuming, costly, and can be impractical, particularly for large-scale or diverse soil profiles. Recent progress in artificial intelligence, especially machine learning (ML), has enabled data-driven approaches for modeling complex soil behavior with greater speed and precision. This study introduces a comprehensive ML framework for CBR prediction using a dataset of 382 soil samples collected from various geoclimatic regions in Türkiye. The dataset includes physicochemical soil properties relevant to bearing capacity, allowing multidimensional feature representation in a supervised learning context. Twelve ML algorithms were tested, including decision tree, random forest, extra trees, gradient boosting, xgboost, k-nearest neighbors, support vector regression, multi-layer perceptron, adaboost, bagging, voting, and stacking regressors. Each model was trained, validated, and evaluated to assess its generalization and robustness. Among them, the random forest regressor performed the best, achieving strong R2 scores of 0.95 (training), 0.76 (validation), and 0.83 (test). These outcomes highlight the model's powerful nonlinear mapping ability, making it a promising tool for predictive geotechnical tasks. The study supports the integration of intelligent, data-centric models in geotechnical engineering, offering an effective alternative to traditional methods and promoting digital transformation in infrastructure analysis and design.

</details>


### [22] [Soil Compaction Parameters Prediction Based on Automated Machine Learning Approach](https://arxiv.org/abs/2512.08343)
*Caner Erden,Alparslan Serhat Demir,Abdullah Hulusi Kokcam,Talas Fikret Kurnaz,Ugur Dagdeviren*

Main category: cs.AI

TL;DR: 本研究提出使用AutoML方法预测土壤压实参数（最优含水量OMC和最大干密度MDD），通过自动化算法选择和超参数优化，XGBoost算法在独立数据集上取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 传统确定土壤压实参数的方法（实验室实验）劳动密集，而现有经验回归模型在不同土壤类型中适用性和准确性有限。机器学习模型在处理异质数据集时存在预测精度和泛化能力不足的问题。

Method: 采用自动化机器学习（AutoML）方法预测OMC和MDD，该方法自动化算法选择和超参数优化过程。通过广泛实验评估不同算法性能。

Result: 极端梯度提升（XGBoost）算法表现最佳，在独立数据集上对MDD和OMC的R²值分别达到80.4%和89.1%。研究还发现异质数据集对提升ML模型泛化性能很重要。

Conclusion: AutoML方法能有效预测不同土壤类型的压实参数，有助于提高施工实践的效率和可靠性。异质数据集对改善ML模型性能至关重要。

Abstract: Soil compaction is critical in construction engineering to ensure the stability of structures like road embankments and earth dams. Traditional methods for determining optimum moisture content (OMC) and maximum dry density (MDD) involve labor-intensive laboratory experiments, and empirical regression models have limited applicability and accuracy across diverse soil types. In recent years, artificial intelligence (AI) and machine learning (ML) techniques have emerged as alternatives for predicting these compaction parameters. However, ML models often struggle with prediction accuracy and generalizability, particularly with heterogeneous datasets representing various soil types. This study proposes an automated machine learning (AutoML) approach to predict OMC and MDD. AutoML automates algorithm selection and hyperparameter optimization, potentially improving accuracy and scalability. Through extensive experimentation, the study found that the Extreme Gradient Boosting (XGBoost) algorithm provided the best performance, achieving R-squared values of 80.4% for MDD and 89.1% for OMC on a separate dataset. These results demonstrate the effectiveness of AutoML in predicting compaction parameters across different soil types. The study also highlights the importance of heterogeneous datasets in improving the generalization and performance of ML models. Ultimately, this research contributes to more efficient and reliable construction practices by enhancing the prediction of soil compaction parameters.

</details>


### [23] [Enhancing Explainability of Graph Neural Networks Through Conceptual and Structural Analyses and Their Extensions](https://arxiv.org/abs/2512.08344)
*Tien Cuong Bui*

Main category: cs.AI

TL;DR: 该论文提出了一种针对图神经网络的新型可解释AI框架，旨在解决现有方法在解释图结构影响预测方面的不足，提供更高效、可靠的可解释性方案。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在众多应用中广泛使用，但其复杂性阻碍了对其决策过程的理解。现有的可解释AI方法难以解析图中复杂的交互关系，后处理方法需要额外计算资源且可靠性有限，而可解释模型又缺乏泛化能力。

Method: 开发一种新型的XAI框架，专门针对基于图的机器学习。该框架旨在超越传统的个体特征分析，捕捉图结构如何影响预测，提供适应性强且计算高效的解释。

Result: 论文尚未完成，但提出的框架目标是提供比现有方法更优的可解释性方案，能够更好地理解图神经网络中的结构-预测关系。

Conclusion: 需要开发专门针对图神经网络的新型可解释AI框架，以解决现有方法的局限性，提供更可靠、高效且能捕捉图结构影响的解释机制。

Abstract: Graph Neural Networks (GNNs) have become a powerful tool for modeling and analyzing data with graph structures. The wide adoption in numerous applications underscores the value of these models. However, the complexity of these methods often impedes understanding their decision-making processes. Current Explainable AI (XAI) methods struggle to untangle the intricate relationships and interactions within graphs. Several methods have tried to bridge this gap via a post-hoc approach or self-interpretable design. Most of them focus on graph structure analysis to determine essential patterns that correlate with prediction outcomes. While post-hoc explanation methods are adaptable, they require extra computational resources and may be less reliable due to limited access to the model's internal workings. Conversely, Interpretable models can provide immediate explanations, but their generalizability to different scenarios remains a major concern. To address these shortcomings, this thesis seeks to develop a novel XAI framework tailored for graph-based machine learning. The proposed framework aims to offer adaptable, computationally efficient explanations for GNNs, moving beyond individual feature analysis to capture how graph structure influences predictions.

</details>


### [24] [Reflecting with Two Voices: A Co-Adaptive Dual-Strategy Framework for LLM-Based Agent Decision Making](https://arxiv.org/abs/2512.08366)
*Wentao Zhang,Qunbo Wang,Tao Zhang,Junsheng Wu,Hongping Gan,Yang Liu,Ling Dai,Shizhuang Deng,Shuntong Sun*

Main category: cs.AI

TL;DR: DuSAR是一个无需演示的LLM智能体框架，通过双策略（高层整体规划与上下文接地局部策略）和轻量级反思机制实现自适应推理，在ALFWorld和Mind2Web上取得SOTA性能，同时大幅降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体依赖外部演示或检索增强规划，导致脆弱性、泛化能力差和计算开销高。受人类问题解决启发，需要一种无需演示、能自适应推理的框架。

Method: 提出DuSAR框架：1）双策略协同：高层整体规划与上下文接地局部策略；2）轻量级反思机制：通过策略适应度评分动态评估进展，在卡住时修订全局计划或在有意义进展时细化计划；3）使用单个冻结LLM实现协同适应推理。

Result: 在ALFWorld上达到37.1%成功率（Llama3.1-70B），比之前最佳结果13.0%提高一倍多；在Mind2Web上达到4.02%，同样比最强基线提高一倍多；每步token消耗减少3-9倍；消融研究证实双策略协调的必要性；可选集成专家演示可进一步提升性能。

Conclusion: DuSAR通过双策略协同和反思机制实现了无需演示的高效LLM智能体推理，在多个基准上取得SOTA性能，显著降低计算开销，同时保持与外部知识集成的灵活性。

Abstract: Large language model (LLM) agents often rely on external demonstrations or retrieval-augmented planning, leading to brittleness, poor generalization, and high computational overhead. Inspired by human problem-solving, we propose DuSAR (Dual-Strategy Agent with Reflecting) - a demonstration-free framework that enables a single frozen LLM to perform co-adaptive reasoning via two complementary strategies: a high-level holistic plan and a context-grounded local policy. These strategies interact through a lightweight reflection mechanism, where the agent continuously assesses progress via a Strategy Fitness Score and dynamically revises its global plan when stuck or refines it upon meaningful advancement, mimicking human metacognitive behavior. On ALFWorld and Mind2Web, DuSAR achieves state-of-the-art performance with open-source LLMs (7B-70B), reaching 37.1% success on ALFWorld (Llama3.1-70B) - more than doubling the best prior result (13.0%) - and 4.02% on Mind2Web, also more than doubling the strongest baseline. Remarkably, it reduces per-step token consumption by 3-9X while maintaining strong performance. Ablation studies confirm the necessity of dual-strategy coordination. Moreover, optional integration of expert demonstrations further boosts results, highlighting DuSAR's flexibility and compatibility with external knowledge.

</details>


### [25] [DeepFeature: Iterative Context-aware Feature Generation for Wearable Biosignals](https://arxiv.org/abs/2512.08379)
*Kaiwei Liu,Yuting He,Bufang Yang,Mu Yuan,Chun Man Victor Wong,Ho Pong Andrew Sze,Zhenyu Yan,Hongkai Chen*

Main category: cs.AI

TL;DR: DeepFeature：首个基于LLM的上下文感知特征生成框架，用于可穿戴生物信号，通过多源特征生成、迭代特征精炼和多层过滤验证，在8个任务上平均AUROC提升4.21-9.67%


<details>
  <summary>Details</summary>
Motivation: 现有可穿戴生物信号特征提取方法缺乏任务特定的上下文知识，难以在高维特征空间中找到最优设置，且容易产生代码生成和自动化错误

Method: 提出DeepFeature框架：1）多源特征生成机制，整合专家知识和任务设置；2）迭代特征精炼过程，基于特征评估反馈进行特征重选；3）多层过滤验证方法，确保特征到代码的稳健转换

Result: 在8个多样化任务上，DeepFeature相比基线方法平均AUROC提升4.21-9.67%，在5个任务上优于最先进方法，其余任务保持可比性能

Conclusion: DeepFeature是首个LLM赋能的上下文感知特征生成框架，通过整合专家知识、迭代精炼和稳健代码转换，显著提升了可穿戴生物信号分析性能

Abstract: Biosignals collected from wearable devices are widely utilized in healthcare applications. Machine learning models used in these applications often rely on features extracted from biosignals due to their effectiveness, lower data dimensionality, and wide compatibility across various model architectures. However, existing feature extraction methods often lack task-specific contextual knowledge, struggle to identify optimal feature extraction settings in high-dimensional feature space, and are prone to code generation and automation errors. In this paper, we propose DeepFeature, the first LLM-empowered, context-aware feature generation framework for wearable biosignals. DeepFeature introduces a multi-source feature generation mechanism that integrates expert knowledge with task settings. It also employs an iterative feature refinement process that uses feature assessment-based feedback for feature re-selection. Additionally, DeepFeature utilizes a robust multi-layer filtering and verification approach for robust feature-to-code translation to ensure that the extraction functions run without crashing. Experimental evaluation results show that DeepFeature achieves an average AUROC improvement of 4.21-9.67% across eight diverse tasks compared to baseline methods. It outperforms state-of-the-art approaches on five tasks while maintaining comparable performance on the remaining tasks.

</details>


### [26] [Prismatic World Model: Learning Compositional Dynamics for Planning in Hybrid Systems](https://arxiv.org/abs/2512.08411)
*Mingwei Li,Xiaoyuan Zhang,Chengwei Yang,Zilong Zheng,Yaodong Yang*

Main category: cs.AI

TL;DR: PRISM-WM：一种用于机器人混合动力学的结构化世界模型，通过专家混合框架分解复杂动态，减少长时域规划中的累积误差


<details>
  <summary>Details</summary>
Motivation: 传统世界模型使用全局连续的神经网络，会过度平滑混合动力学中的离散事件（如接触、碰撞），导致长时域规划在物理边界处产生灾难性累积误差

Method: 提出PRISM-WM架构，使用上下文感知的专家混合框架：门控机制隐式识别当前物理模式，专门专家预测相应动态；引入潜在正交化目标确保专家多样性

Result: 在挑战性连续控制基准测试（包括高维人形机器人和多任务设置）中，PRISM-WM显著减少滚动漂移，为轨迹优化算法提供更优的高保真度基础

Conclusion: PRISM-WM通过准确建模系统动态中的尖锐模式转换，为下一代基于模型的智能体提供了强大的基础模型

Abstract: Model-based planning in robotic domains is fundamentally challenged by the hybrid nature of physical dynamics, where continuous motion is punctuated by discrete events such as contacts and impacts. Conventional latent world models typically employ monolithic neural networks that enforce global continuity, inevitably over-smoothing the distinct dynamic modes (e.g., sticking vs. sliding, flight vs. stance). For a planner, this smoothing results in catastrophic compounding errors during long-horizon lookaheads, rendering the search process unreliable at physical boundaries. To address this, we introduce the Prismatic World Model (PRISM-WM), a structured architecture designed to decompose complex hybrid dynamics into composable primitives. PRISM-WM leverages a context-aware Mixture-of-Experts (MoE) framework where a gating mechanism implicitly identifies the current physical mode, and specialized experts predict the associated transition dynamics. We further introduce a latent orthogonalization objective to ensure expert diversity, effectively preventing mode collapse. By accurately modeling the sharp mode transitions in system dynamics, PRISM-WM significantly reduces rollout drift. Extensive experiments on challenging continuous control benchmarks, including high-dimensional humanoids and diverse multi-task settings, demonstrate that PRISM-WM provides a superior high-fidelity substrate for trajectory optimization algorithms (e.g., TD-MPC), proving its potential as a powerful foundational model for next-generation model-based agents.

</details>


### [27] [From Accuracy to Impact: The Impact-Driven AI Framework (IDAIF) for Aligning Engineering Architecture with Theory of Change](https://arxiv.org/abs/2512.08449)
*Yong-Woon Kim*

Main category: cs.AI

TL;DR: IDAIF是一个将变革理论与AI架构设计结合的新框架，通过五层映射确保AI系统与社会影响对齐，从模型中心转向影响中心的AI开发范式。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在高风险领域（医疗、金融、公共政策）部署时，主要关注技术性能指标而忽视社会技术维度，导致AI行为与人类价值观和意图的对齐问题（对齐问题）日益突出。

Method: 提出Impact-Driven AI Framework (IDAIF)，将变革理论的五阶段模型（输入-活动-输出-成果-影响）映射到对应的AI架构层（数据层-管道层-推理层-代理层-规范层），每层采用特定技术方法（多目标帕累托优化、分层多代理编排、因果有向无环图、对抗去偏置与RLHF），并引入保证层管理假设失败。

Result: 提供了每个组件的正式数学公式，通过三个案例研究（医疗、网络安全、软件工程）展示了IDAIF的应用，证明该框架能够构建符合伦理、可信赖且对社会有益的AI系统。

Conclusion: IDAIF代表了从模型中心到影响中心的AI开发范式转变，为工程师提供了构建伦理、可信赖且对社会有益的AI系统的具体架构模式。

Abstract: This paper introduces the Impact-Driven AI Framework (IDAIF), a novel architectural methodology that integrates Theory of Change (ToC) principles with modern artificial intelligence system design. As AI systems increasingly influence high-stakes domains including healthcare, finance, and public policy, the alignment problem--ensuring AI behavior corresponds with human values and intentions--has become critical. Current approaches predominantly optimize technical performance metrics while neglecting the sociotechnical dimensions of AI deployment. IDAIF addresses this gap by establishing a systematic mapping between ToC's five-stage model (Inputs-Activities-Outputs-Outcomes-Impact) and corresponding AI architectural layers (Data Layer-Pipeline Layer-Inference Layer-Agentic Layer-Normative Layer). Each layer incorporates rigorous theoretical foundations: multi-objective Pareto optimization for value alignment, hierarchical multi-agent orchestration for outcome achievement, causal directed acyclic graphs (DAGs) for hallucination mitigation, and adversarial debiasing with Reinforcement Learning from Human Feedback (RLHF) for fairness assurance. We provide formal mathematical formulations for each component and introduce an Assurance Layer that manages assumption failures through guardian architectures. Three case studies demonstrate IDAIF application across healthcare, cybersecurity, and software engineering domains. This framework represents a paradigm shift from model-centric to impact-centric AI development, providing engineers with concrete architectural patterns for building ethical, trustworthy, and socially beneficial AI systems.

</details>


### [28] [Using reinforcement learning to probe the role of feedback in skill acquisition](https://arxiv.org/abs/2512.08463)
*Antonio Terpin,Raffaello D'Andrea*

Main category: cs.AI

TL;DR: 研究通过强化学习智能体控制旋转圆柱体在流体中的阻力，发现学习高性能技能需要比执行技能更丰富的信息反馈，且学习条件取决于目标而非动态复杂性。


<details>
  <summary>Details</summary>
Motivation: 研究人类在无外部反馈情况下如何掌握高性能技能（如花样滑冰、投球），通过物理实验系统探索技能获取过程，避免人类受试者的复杂性。

Method: 使用通用强化学习智能体控制桌面循环水槽中的旋转圆柱体，通过最大化或最小化阻力来学习控制策略。系统包含高维流体反馈，并与无反馈条件对比。

Result: 有流体反馈时，智能体能在几分钟内发现高性能阻力控制策略；无反馈时仍能执行已学策略。但无反馈训练时，最大化阻力任务完全失败，最小化阻力任务虽能成功但更慢、更不可靠。

Conclusion: 学习高性能技能需要比执行技能更丰富的信息反馈，学习条件的"仁慈"或"恶劣"仅取决于目标（最大化vs最小化阻力），而非系统动态或策略复杂性。

Abstract: Many high-performance human activities are executed with little or no external feedback: think of a figure skater landing a triple jump, a pitcher throwing a curveball for a strike, or a barista pouring latte art. To study the process of skill acquisition under fully controlled conditions, we bypass human subjects. Instead, we directly interface a generalist reinforcement learning agent with a spinning cylinder in a tabletop circulating water channel to maximize or minimize drag. This setup has several desirable properties. First, it is a physical system, with the rich interactions and complex dynamics that only the physical world has: the flow is highly chaotic and extremely difficult, if not impossible, to model or simulate accurately. Second, the objective -- drag minimization or maximization -- is easy to state and can be captured directly in the reward, yet good strategies are not obvious beforehand. Third, decades-old experimental studies provide recipes for simple, high-performance open-loop policies. Finally, the setup is inexpensive and far easier to reproduce than human studies. In our experiments we find that high-dimensional flow feedback lets the agent discover high-performance drag-control strategies with only minutes of real-world interaction. When we later replay the same action sequences without any feedback, we obtain almost identical performance. This shows that feedback, and in particular flow feedback, is not needed to execute the learned policy. Surprisingly, without flow feedback during training the agent fails to discover any well-performing policy in drag maximization, but still succeeds in drag minimization, albeit more slowly and less reliably. Our studies show that learning a high-performance skill can require richer information than executing it, and learning conditions can be kind or wicked depending solely on the goal, not on dynamics or policy complexity.

</details>


### [29] [Autonomous Issue Resolver: Towards Zero-Touch Code Maintenance](https://arxiv.org/abs/2512.08492)
*Aliaksei Kaliutau*

Main category: cs.AI

TL;DR: 提出Data Transformation Graph (DTG)范式，将数据状态作为节点、函数作为边，通过数据谱系而非控制流追踪逻辑缺陷，实现87.1%的SWE-Verified基准修复率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在函数级代码生成方面取得进展，但仓库级自动程序修复仍面临挑战。现有方法采用控制中心范式，迫使代理处理复杂的目录结构和无关的控制逻辑，存在"语义陷阱"问题。

Method: 从标准代码属性图转向数据转换图范式，将数据状态建模为节点、函数建模为边。提出多代理框架，协调数据完整性导航与控制流逻辑。实现自主问题解决器系统，利用神经符号推理和DTG结构进行可扩展逻辑修复。

Result: 在多个软件工程基准测试中表现良好，在SWE-Verified基准上达到87.1%的解决率。解决了现代编码代理中RAG系统固有的"语义陷阱"问题。

Conclusion: 该方法直接解决了当前AI代码辅助工具的核心限制，为日益依赖软件的世界提供了更健壮的基础。通过数据谱系而非控制流追踪逻辑缺陷，实现了仓库级程序修复的范式转变。

Abstract: Recent advances in Large Language Models have revolutionized function-level code generation; however, repository-scale Automated Program Repair (APR) remains a significant challenge. Current approaches typically employ a control-centric paradigm, forcing agents to navigate complex directory structures and irrelevant control logic. In this paper, we propose a paradigm shift from the standard Code Property Graphs (CPGs) to the concept of Data Transformation Graph (DTG) that inverts the topology by modeling data states as nodes and functions as edges, enabling agents to trace logic defects through data lineage rather than control flow. We introduce a multi-agent framework that reconciles data integrity navigation with control flow logic. Our theoretical analysis and case studies demonstrate that this approach resolves the "Semantic Trap" inherent in standard RAG systems in modern coding agents. We provide a comprehensive implementation in the form of Autonomous Issue Resolver (AIR), a self-improvement system for zero-touch code maintenance that utilizes neuro-symbolic reasoning and uses the DTG structure for scalable logic repair. Our approach has demonstrated good results on several SWE benchmarks, reaching a resolution rate of 87.1% on SWE-Verified benchmark. Our approach directly addresses the core limitations of current AI code-assistant tools and tackles the critical need for a more robust foundation for our increasingly software-dependent world.

</details>


### [30] [A Lightweight Transfer Learning-Based State-of-Health Monitoring with Application to Lithium-ion Batteries in Unmanned Air Vehicles](https://arxiv.org/abs/2512.08512)
*Jiang Liu,Yan Qin,Wei Dai,Chau Yuen*

Main category: cs.AI

TL;DR: 提出一种轻量级迁移学习方法CITL用于锂离子电池健康状态监测，通过增量网络节点构建和半监督机制，在减少计算资源消耗的同时提升监测精度。


<details>
  <summary>Details</summary>
Motivation: 传统迁移学习方法在便携移动设备上不可行，因为迁移学习阶段消耗大量计算资源，意外降低设备工作续航。需要一种轻量级迁移学习方法来解决这一问题。

Method: 提出构造性增量迁移学习(CITL)方法：1)利用目标域无标签数据，通过半监督迁移学习机制以构造性方式最小化监测残差，迭代增加网络节点；2)通过结构风险最小化、迁移失配最小化和流形一致性最大化保证节点参数的跨域学习能力；3)提供CITL收敛性分析。

Result: 在真实无人机电池数据集上的实验表明，CITL在SOH估计上优于SS-TCA、MMD-LSTM-DA、DDAN、BO-CNN-TL和AS$^3$LSTM，RMSE分别提升83.73%、61.15%、28.24%、87.70%和57.34%。

Conclusion: 提出的轻量级CITL方法有效解决了便携移动设备中传统迁移学习方法计算资源消耗大的问题，在保证监测精度的同时显著减少了计算负担。

Abstract: Accurate and rapid state-of-health (SOH) monitoring plays an important role in indicating energy information for lithium-ion battery-powered portable mobile devices. To confront their variable working conditions, transfer learning (TL) emerges as a promising technique for leveraging knowledge from data-rich source working conditions, significantly reducing the training data required for SOH monitoring from target working conditions. However, traditional TL-based SOH monitoring is infeasible when applied in portable mobile devices since substantial computational resources are consumed during the TL stage and unexpectedly reduce the working endurance. To address these challenges, this paper proposes a lightweight TL-based SOH monitoring approach with constructive incremental transfer learning (CITL). First, taking advantage of the unlabeled data in the target domain, a semi-supervised TL mechanism is proposed to minimize the monitoring residual in a constructive way, through iteratively adding network nodes in the CITL. Second, the cross-domain learning ability of node parameters for CITL is comprehensively guaranteed through structural risk minimization, transfer mismatching minimization, and manifold consistency maximization. Moreover, the convergence analysis of the CITL is given, theoretically guaranteeing the efficacy of TL performance and network compactness. Finally, the proposed approach is verified through extensive experiments with a realistic unmanned air vehicles (UAV) battery dataset collected from dozens of flight missions. Specifically, the CITL outperforms SS-TCA, MMD-LSTM-DA, DDAN, BO-CNN-TL, and AS$^3$LSTM, in SOH estimation by 83.73%, 61.15%, 28.24%, 87.70%, and 57.34%, respectively, as evaluated using the index root mean square error.

</details>


### [31] [Principles2Plan: LLM-Guided System for Operationalising Ethical Principles into Plans](https://arxiv.org/abs/2512.08536)
*Tammy Zhong,Yang Song,Maurice Pagnucco*

Main category: cs.AI

TL;DR: Principles2Plan：人类与LLM协作生成情境化伦理规则以指导自动化规划的系统原型


<details>
  <summary>Details</summary>
Motivation: 机器人在人类环境中需要伦理意识，但现有自动化规划工具缺乏伦理支持。手动制定伦理规则劳动密集且高度依赖具体情境。

Method: 开发交互式原型系统Principles2Plan，让领域专家提供规划领域、问题细节和高级伦理原则（如仁爱、隐私），系统生成可操作的伦理规则，用户可审查、优先排序并供给规划器生成伦理知情计划。

Result: 创建了首个支持用户在经典规划情境中生成基于原则的伦理规则的系统，展示了人机协作使伦理自动化规划更实用可行。

Conclusion: Principles2Plan展示了人类与大型语言模型协作在使伦理自动化规划更实用和可行方面的潜力，填补了现有规划工具缺乏伦理支持的空白。

Abstract: Ethical awareness is critical for robots operating in human environments, yet existing automated planning tools provide little support. Manually specifying ethical rules is labour-intensive and highly context-specific. We present Principles2Plan, an interactive research prototype demonstrating how a human and a Large Language Model (LLM) can collaborate to produce context-sensitive ethical rules and guide automated planning. A domain expert provides the planning domain, problem details, and relevant high-level principles such as beneficence and privacy. The system generates operationalisable ethical rules consistent with these principles, which the user can review, prioritise, and supply to a planner to produce ethically-informed plans. To our knowledge, no prior system supports users in generating principle-grounded rules for classical planning contexts. Principles2Plan showcases the potential of human-LLM collaboration for making ethical automated planning more practical and feasible.

</details>


### [32] [CogMCTS: A Novel Cognitive-Guided Monte Carlo Tree Search Framework for Iterative Heuristic Evolution with Large Language Models](https://arxiv.org/abs/2512.08609)
*Hui Wang,Yang Liu,Xiaoyu Zhang,Chaoxu Mu*

Main category: cs.AI

TL;DR: 本文提出CogMCTS框架，将LLM的认知引导机制与MCTS紧密结合，通过多轮认知反馈、双轨节点扩展和策略突变，实现高效的自动启发式设计。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的进化方法容易陷入局部最优，而LLM与MCTS结合的方法在多轮认知整合和搜索多样性方面仍有局限，需要克服这些限制。

Method: 提出CogMCTS框架：1) 多轮认知反馈整合历史经验、节点信息和负面结果；2) 双轨节点扩展结合精英启发式管理平衡探索与利用；3) 策略突变修改启发式形式和参数增强多样性。

Result: 实验结果表明，CogMCTS在稳定性、效率和解决方案质量方面优于现有的基于LLM的AHD方法。

Conclusion: CogMCTS通过紧密集成LLM认知引导与MCTS，有效解决了自动启发式设计中的探索-利用平衡问题，提升了优化性能。

Abstract: Automatic Heuristic Design (AHD) is an effective1 framework for solving complex optimization prob-2 lems. The development of large language mod-3 els (LLMs) enables the automated generation of4 heuristics. Existing LLM-based evolutionary meth-5 ods rely on population strategies and are prone6 to local optima. Integrating LLMs with Monte7 Carlo Tree Search (MCTS) improves the trade-off8 between exploration and exploitation, but multi-9 round cognitive integration remains limited and10 search diversity is constrained. To overcome these11 limitations, this paper proposes a novel cognitive-12 guided MCTS framework (CogMCTS). CogMCTS13 tightly integrates the cognitive guidance mecha-14 nism of LLMs with MCTS to achieve efficient au-15 tomated heuristic optimization. The framework16 employs multi-round cognitive feedback to incor-17 porate historical experience, node information, and18 negative outcomes, dynamically improving heuris-19 tic generation. Dual-track node expansion com-20 bined with elite heuristic management balances the21 exploration of diverse heuristics and the exploita-22 tion of high-quality experience. In addition, strate-23 gic mutation modifies the heuristic forms and pa-24 rameters to further enhance the diversity of the so-25 lution and the overall optimization performance.26 The experimental results indicate that CogMCTS27 outperforms existing LLM-based AHD methods in28 stability, efficiency, and solution quality.

</details>


### [33] [Protein Secondary Structure Prediction Using Transformers](https://arxiv.org/abs/2512.08613)
*Manzi Kevin Maxime*

Main category: cs.AI

TL;DR: 使用基于Transformer的注意力机制模型预测蛋白质二级结构（α螺旋、β折叠、无规卷曲），采用滑动窗口数据增强技术，在CB513数据集上表现出色


<details>
  <summary>Details</summary>
Motivation: 从氨基酸序列预测蛋白质二级结构（α螺旋、β折叠、无规卷曲）对于理解蛋白质功能至关重要，传统方法可能难以有效捕捉序列中的局部和长程残基相互作用

Method: 提出基于Transformer的模型，应用注意力机制处理蛋白质序列数据；采用滑动窗口数据增强技术在CB513数据集上扩展训练样本；模型能够处理可变长度序列并有效捕捉局部和长程残基相互作用

Result: Transformer模型在蛋白质二级结构预测任务中表现出强大的泛化能力，能够有效处理可变长度序列，并成功捕捉序列中的局部和长程残基相互作用模式

Conclusion: 基于注意力机制的Transformer模型是预测蛋白质二级结构的有效方法，能够处理序列长度变化并捕捉复杂的残基相互作用，为蛋白质功能研究提供了有力工具

Abstract: Predicting protein secondary structures such as alpha helices, beta sheets, and coils from amino acid sequences is essential for understanding protein function. This work presents a transformer-based model that applies attention mechanisms to protein sequence data to predict structural motifs. A sliding-window data augmentation technique is used on the CB513 dataset to expand the training samples. The transformer shows strong ability to generalize across variable-length sequences while effectively capturing both local and long-range residue interactions.

</details>


### [34] [See-Control: A Multimodal Agent Framework for Smartphone Interaction with a Robotic Arm](https://arxiv.org/abs/2512.08629)
*Haoyu Zhao,Weizhong Ding,Yuhao Yang,Zheng Tian,Linyi Yang,Kun Shao,Jun Wang*

Main category: cs.AI

TL;DR: See-Control是一个通过低自由度机械臂直接物理交互操作智能手机的框架，无需ADB，提供平台无关的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有基于ADB的多模态大语言模型智能手机操作方法仅限于Android设备，需要一种平台无关的解决方案，使数字代理能够与物理世界交互。

Method: 提出ESO任务和See-Control框架，包含三个组件：1) 155个任务的ESO基准和评估指标；2) 基于MLLM的具身代理，无需ADB即可生成机器人控制命令；3) 丰富标注的操作数据集。

Result: See-Control实现了通过低自由度机械臂直接物理交互操作智能手机，为家庭机器人在真实环境中执行智能手机依赖任务提供了具体步骤。

Conclusion: 该工作填补了数字代理与物理世界之间的鸿沟，为未来研究提供了有价值的资源和平台无关的智能手机操作解决方案。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have enabled their use as intelligent agents for smartphone operation. However, existing methods depend on the Android Debug Bridge (ADB) for data transmission and action execution, limiting their applicability to Android devices. In this work, we introduce the novel Embodied Smartphone Operation (ESO) task and present See-Control, a framework that enables smartphone operation via direct physical interaction with a low-DoF robotic arm, offering a platform-agnostic solution. See-Control comprises three key components: (1) an ESO benchmark with 155 tasks and corresponding evaluation metrics; (2) an MLLM-based embodied agent that generates robotic control commands without requiring ADB or system back-end access; and (3) a richly annotated dataset of operation episodes, offering valuable resources for future research. By bridging the gap between digital agents and the physical world, See-Control provides a concrete step toward enabling home robots to perform smartphone-dependent tasks in realistic environments.

</details>


### [35] [Multi-Agent Intelligence for Multidisciplinary Decision-Making in Gastrointestinal Oncology](https://arxiv.org/abs/2512.08674)
*Rongzhao Zhang,Junqiao Wang,Shuyun Yang,Mouxiao Bian,Chao Ding,Yuwei Bai,Chihao Zhang,Yuguang Shen,Lei Wang,Lei Zheng,Qiujuan Yan,Yun Zhong,Meiling Liu,Jiwei Yu,Zheng Wang,Jie Xu,Meng Luo*

Main category: cs.AI

TL;DR: 提出分层多智能体框架模拟多学科团队协作，解决多模态临床推理中的上下文稀释和幻觉问题，在胃肠道肿瘤学中显著提升推理逻辑和医学准确性。


<details>
  <summary>Details</summary>
Motivation: 胃肠道肿瘤学的多模态临床推理需要整合内镜影像、放射学数据和生化标志物。尽管多模态大语言模型有潜力，但在处理复杂异质病历时常面临上下文稀释和幻觉问题。

Method: 提出分层多智能体框架，模拟人类多学科团队的协作工作流程，通过多智能体协作解决单一模型的局限性。

Result: 系统获得4.60/5.00的专家评估分数，显著优于单一基线模型。智能体架构在推理逻辑和医学准确性方面提升最明显。

Conclusion: 模拟性的智能体协作为肿瘤学自动决策支持提供了可扩展、可解释且临床稳健的范式。

Abstract: Multimodal clinical reasoning in the field of gastrointestinal (GI) oncology necessitates the integrated interpretation of endoscopic imagery, radiological data, and biochemical markers. Despite the evident potential exhibited by Multimodal Large Language Models (MLLMs), they frequently encounter challenges such as context dilution and hallucination when confronted with intricate, heterogeneous medical histories. In order to address these limitations, a hierarchical Multi-Agent Framework is proposed, which emulates the collaborative workflow of a human Multidisciplinary Team (MDT). The system attained a composite expert evaluation score of 4.60/5.00, thereby demonstrating a substantial improvement over the monolithic baseline. It is noteworthy that the agent-based architecture yielded the most substantial enhancements in reasoning logic and medical accuracy. The findings indicate that mimetic, agent-based collaboration provides a scalable, interpretable, and clinically robust paradigm for automated decision support in oncology.

</details>


### [36] [Deconstructing the Dual Black Box:A Plug-and-Play Cognitive Framework for Human-AI Collaborative Enhancement and Its Implications for AI Governance](https://arxiv.org/abs/2512.08740)
*Yiming Lu*

Main category: cs.AI

TL;DR: 提出"人机协作认知增强"新范式，通过结构化"元交互"将人类专家的直觉黑盒与AI的决策黑盒转化为可组合、可审计、可扩展的功能白盒系统，实现从"AI作为工具"到"AI作为思维伙伴"的范式转变。


<details>
  <summary>Details</summary>
Motivation: 解决人类专家"认知黑盒"（隐性直觉）与人工智能"计算黑盒"（不可信决策）之间的根本性隔阂，建立可信、可审计的人机协作系统。

Method: 提出"即插即用认知框架"——可从专家对话中提取的可计算知识包，加载到递归对抗元思维网络（RAMTN）中，通过结构化"元交互"实现专家思维的可重用和可扩展。

Result: 实现了专家思维（如医疗诊断逻辑和教学直觉）向可重用、可扩展公共资产的转化，为"认知公平"提供了首个工程证明，并为AI治理开辟了新路径：通过"交互协议透明化"而非窥探模型内部机制来构建可验证、可干预的治理范式。

Conclusion: 该研究提出了人机协作认知增强的新范式，通过将专家思维转化为可计算知识包，实现了从工具到思维伙伴的转变，为AI治理和认知包容提供了新路径，框架已开源以促进技术向善。

Abstract: Currently, there exists a fundamental divide between the "cognitive black box" (implicit intuition) of human experts and the "computational black box" (untrustworthy decision-making) of artificial intelligence (AI). This paper proposes a new paradigm of "human-AI collaborative cognitive enhancement," aiming to transform the dual black boxes into a composable, auditable, and extensible "functional white-box" system through structured "meta-interaction." The core breakthrough lies in the "plug-and-play cognitive framework"--a computable knowledge package that can be extracted from expert dialogues and loaded into the Recursive Adversarial Meta-Thinking Network (RAMTN). This enables expert thinking, such as medical diagnostic logic and teaching intuition, to be converted into reusable and scalable public assets, realizing a paradigm shift from "AI as a tool" to "AI as a thinking partner." This work not only provides the first engineering proof for "cognitive equity" but also opens up a new path for AI governance: constructing a verifiable and intervenable governance paradigm through "transparency of interaction protocols" rather than prying into the internal mechanisms of models. The framework is open-sourced to promote technology for good and cognitive inclusion. This paper is an independent exploratory research conducted by the author. All content presented, including the theoretical framework (RAMTN), methodology (meta-interaction), system implementation, and case validation, constitutes the author's individual research achievements.

</details>


### [37] [Towards Foundation Models with Native Multi-Agent Intelligence](https://arxiv.org/abs/2512.08743)
*Shuyue Hu,Haoyang Yan,Yiqun Zhang,Yang Chen,Dongzhan Zhou,Lei Bai*

Main category: cs.AI

TL;DR: 基础模型在单智能体任务上表现出色，但缺乏原生多智能体智能，需要专门的研究方向来填补这一差距


<details>
  <summary>Details</summary>
Motivation: 基础模型正成为AI智能体的"大脑"，但当前研究主要集中在单智能体能力（如GUI交互、工具使用）上，而多智能体智能是下一个前沿领域。作者发现单智能体性能强并不自动转化为多智能体智能

Method: 识别了基础模型在多智能体环境中的四个核心能力：理解、规划、高效通信和适应。通过对41个大语言模型的广泛实证研究，验证了单智能体性能与多智能体智能之间的差距

Result: 实证研究表明，强大的单智能体性能并不能自动产生稳健的多智能体智能。这揭示了当前基础模型在多智能体场景中的局限性

Conclusion: 需要从数据集构建、评估方法、训练范式和安全考虑等方面开展专门研究，以构建具有原生多智能体智能的基础模型

Abstract: Foundation models (FMs) are increasingly assuming the role of the "brain" of AI agents. While recent efforts have begun to equip FMs with native single-agent abilities -- such as GUI interaction or integrated tool use -- we argue that the next frontier is endowing FMs with native multi-agent intelligence. We identify four core capabilities of FMs in multi-agent contexts: understanding, planning, efficient communication, and adaptation. Contrary to assumptions about the spontaneous emergence of such abilities, we provide extensive empirical evidence across 41 large language models showing that strong single-agent performance alone does not automatically yield robust multi-agent intelligence. To address this gap, we outline key research directions -- spanning dataset construction, evaluation, training paradigms, and safety considerations -- for building FMs with native multi-agent intelligence.

</details>


### [38] [Performance Comparison of Aerial RIS and STAR-RIS in 3D Wireless Environments](https://arxiv.org/abs/2512.08755)
*Dongdong Yang,Bin Li,Jiguang He*

Main category: cs.AI

TL;DR: 本文对无人机搭载的RIS和STAR-RIS进行了性能比较，发现STAR-RIS在低空场景表现更好，而RIS在基站附近的高空场景更优。


<details>
  <summary>Details</summary>
Motivation: 虽然无人机搭载的RIS和STAR-RIS在下一代网络中具有增强无线覆盖和容量的潜力，但两者之间的全面性能比较尚未得到深入研究。

Method: 建立了包含定向辐射模式的精确信道模型，并分析了部署高度和方向的影响。采用加权最小均方误差和块坐标下降算法来优化系统总速率。

Result: 仿真结果显示：STAR-RIS在低空场景中由于全空间覆盖能力而表现更好，而RIS在基站附近的高空场景中性能更优。

Conclusion: 研究结果为未来6G通信系统中空中智能表面的部署提供了实用指导，表明应根据部署场景选择适当的智能表面架构。

Abstract: Reconfigurable intelligent surface (RIS) and simultaneously transmitting and reflecting RIS (STAR-RIS) have emerged as key enablers for enhancing wireless coverage and capacity in next-generation networks. When mounted on unmanned aerial vehicles (UAVs), they benefit from flexible deployment and improved line-of-sight conditions. Despite their promising potential, a comprehensive performance comparison between aerial RIS and STAR-RIS architectures has not been thoroughly investigated. This letter presents a detailed performance comparison between aerial RIS and STAR-RIS in three-dimensional wireless environments. Accurate channel models incorporating directional radiation patterns are established, and the influence of deployment altitude and orientation is thoroughly examined. To optimize the system sum-rate, we formulate joint optimization problems for both architectures and propose an efficient solution based on the weighted minimum mean square error and block coordinate descent algorithms. Simulation results reveal that STAR-RIS outperforms RIS in low-altitude scenarios due to its full-space coverage capability, whereas RIS delivers better performance near the base station at higher altitudes. The findings provide practical insights for the deployment of aerial intelligent surfaces in future 6G communication systems.

</details>


### [39] [A Practical Guide for Designing, Developing, and Deploying Production-Grade Agentic AI Workflows](https://arxiv.org/abs/2512.08769)
*Eranga Bandara,Ross Gore,Peter Foytik,Sachin Shetty,Ravi Mukkamala,Abdul Rahman,Xueping Liang,Safdar H. Bouk,Amin Hass,Sachini Rajapakse,Ng Wee Keong,Kasun De Zoysa,Aruna Withanage,Nilaan Loganathan*

Main category: cs.AI

TL;DR: 论文提供了构建生产级智能体AI工作流的端到端工程指南，包括设计模式、最佳实践和案例研究。


<details>
  <summary>Details</summary>
Motivation: 随着智能体AI在各行业加速应用，组织面临如何设计、构建和运维生产级智能体工作流的挑战，需要确保其可靠性、可观察性、可维护性并符合安全和治理要求。

Method: 提出结构化工程生命周期，包括工作流分解、多智能体设计模式、模型上下文协议(MCP)、工具集成、确定性编排、负责任AI考虑和环境感知部署策略，并总结九项核心最佳实践。

Result: 通过多模态新闻分析和媒体生成工作流的案例研究，展示了这些原则的实际应用，为构建稳健、可扩展的生产就绪智能体AI工作流提供了基础参考。

Conclusion: 论文提供了构建生产级智能体AI系统的综合指南，结合架构指导、操作模式和实际实现见解，帮助组织开发可靠、可扩展的智能体工作流。

Abstract: Agentic AI marks a major shift in how autonomous systems reason, plan, and execute multi-step tasks. Unlike traditional single model prompting, agentic workflows integrate multiple specialized agents with different Large Language Models(LLMs), tool-augmented capabilities, orchestration logic, and external system interactions to form dynamic pipelines capable of autonomous decision-making and action. As adoption accelerates across industry and research, organizations face a central challenge: how to design, engineer, and operate production-grade agentic AI workflows that are reliable, observable, maintainable, and aligned with safety and governance requirements. This paper provides a practical, end-to-end guide for designing, developing, and deploying production-quality agentic AI systems. We introduce a structured engineering lifecycle encompassing workflow decomposition, multi-agent design patterns, Model Context Protocol(MCP), and tool integration, deterministic orchestration, Responsible-AI considerations, and environment-aware deployment strategies. We then present nine core best practices for engineering production-grade agentic AI workflows, including tool-first design over MCP, pure-function invocation, single-tool and single-responsibility agents, externalized prompt management, Responsible-AI-aligned model-consortium design, clean separation between workflow logic and MCP servers, containerized deployment for scalable operations, and adherence to the Keep it Simple, Stupid (KISS) principle to maintain simplicity and robustness. To demonstrate these principles in practice, we present a comprehensive case study: a multimodal news-analysis and media-generation workflow. By combining architectural guidance, operational patterns, and practical implementation insights, this paper offers a foundational reference to build robust, extensible, and production-ready agentic AI workflows.

</details>


### [40] [CARLoS: Retrieval via Concise Assessment Representation of LoRAs at Scale](https://arxiv.org/abs/2512.08826)
*Shahar Sarfaty,Adi Haviv,Uri Hacohen,Niva Elkin-Koren,Roi Livni,Amit H. Bermano*

Main category: cs.AI

TL;DR: CARLoS是一个大规模框架，通过分析LoRA在图像生成中的行为来表征其特性，无需额外元数据，并基于此开发高效的检索系统


<details>
  <summary>Details</summary>
Motivation: 生成式组件（如LoRA）的快速扩散创建了一个庞大但非结构化的生态系统。现有的发现方法依赖于不可靠的用户描述或有偏见的流行度指标，阻碍了可用性

Method: 分析650多个LoRA，通过在不同提示和种子下的图像生成评估其行为。使用CLIP嵌入及其与基础模型生成的差异，定义三部分表示：方向（语义偏移）、强度（效果显著性）和一致性（效果稳定性）

Result: 开发了高效的检索框架，能够语义匹配文本查询到相关LoRA，同时过滤过强或不稳定的LoRA，在自动和人工评估中优于文本基线

Conclusion: CARLoS不仅是一个实用的检索系统，其表示方法还支持分析强度、一致性与版权法中实质性、意愿性等法律概念的关联，为LoRA分析提供了更广泛的相关性

Abstract: The rapid proliferation of generative components, such as LoRAs, has created a vast but unstructured ecosystem. Existing discovery methods depend on unreliable user descriptions or biased popularity metrics, hindering usability. We present CARLoS, a large-scale framework for characterizing LoRAs without requiring additional metadata. Analyzing over 650 LoRAs, we employ them in image generation over a variety of prompts and seeds, as a credible way to assess their behavior. Using CLIP embeddings and their difference to a base-model generation, we concisely define a three-part representation: Directions, defining semantic shift; Strength, quantifying the significance of the effect; and Consistency, quantifying how stable the effect is. Using these representations, we develop an efficient retrieval framework that semantically matches textual queries to relevant LoRAs while filtering overly strong or unstable ones, outperforming textual baselines in automated and human evaluations. While retrieval is our primary focus, the same representation also supports analyses linking Strength and Consistency to legal notions of substantiality and volition, key considerations in copyright, positioning CARLoS as a practical system with broader relevance for LoRA analysis.

</details>


### [41] [Interpolation in Knowledge Representation](https://arxiv.org/abs/2512.08833)
*Jean Christoph Jung,Patrick Koopmann,Matthias Knorr*

Main category: cs.AI

TL;DR: 论文探讨描述逻辑和逻辑编程中的Craig插值和一致插值，包括理论结果和实际计算方法


<details>
  <summary>Details</summary>
Motivation: Craig插值和一致插值在知识表示中有重要应用（可解释性、遗忘、模块化、重用和学习），但许多知识表示形式化方法通常缺乏这些插值特性，且实际计算插值具有挑战性

Method: 聚焦两个主要知识表示形式化方法：描述逻辑和逻辑编程，讨论计算插值的理论结果和实际方法

Result: 未在摘要中明确说明具体结果，但暗示将提供关于这两个形式化方法中插值计算的理论和实践见解

Conclusion: 需要深入研究描述逻辑和逻辑编程中的插值问题，为知识表示应用提供更好的理论支持和实用计算方法

Abstract: Craig interpolation and uniform interpolation have many applications in knowledge representation, including explainability, forgetting, modularization and reuse, and even learning. At the same time, many relevant knowledge representation formalisms do in general not have Craig or uniform interpolation, and computing interpolants in practice is challenging. We have a closer look at two prominent knowledge representation formalisms, description logics and logic programming, and discuss theoretical results and practical methods for computing interpolants.

</details>


### [42] [EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce](https://arxiv.org/abs/2512.08868)
*Rui Min,Zile Qiao,Ze Xu,Jiawen Zhai,Wenyu Gao,Xuanzhong Chen,Haozhen Sun,Zhen Zhang,Xinyu Wang,Hong Zhou,Wenbiao Yin,Xuan Zhou,Yong Jiang,Haicheng Liu,Liang Ding,Ling Zou,Yi R.,Fung,Yalong Li,Pengjun Xie*

Main category: cs.AI

TL;DR: EcomBench：一个基于真实电商场景的基准测试，用于评估智能代理在实际电商环境中的核心能力


<details>
  <summary>Details</summary>
Motivation: 现有基准测试大多关注学术或人工设计场景，忽视了真实应用中的挑战。电商领域具有大量用户交互、动态市场条件和真实决策任务，是评估智能代理实际能力的理想场景。

Method: 从全球领先电商生态系统的真实用户需求中构建基准，通过专家精心策划和标注，确保清晰度、准确性和领域相关性。涵盖电商场景中的多个任务类别，并定义三个难度级别。

Result: EcomBench提供了一个严谨且动态的测试平台，能够评估智能代理在深度信息检索、多步推理和跨源知识整合等关键能力。

Conclusion: 通过将评估建立在真实电商环境中，EcomBench为衡量智能代理在现代电商中的实际能力提供了有效的基准测试工具。

Abstract: Foundation agents have rapidly advanced in their ability to reason and interact with real environments, making the evaluation of their core capabilities increasingly important. While many benchmarks have been developed to assess agent performance, most concentrate on academic settings or artificially designed scenarios while overlooking the challenges that arise in real applications. To address this issue, we focus on a highly practical real-world setting, the e-commerce domain, which involves a large volume of diverse user interactions, dynamic market conditions, and tasks directly tied to real decision-making processes. To this end, we introduce EcomBench, a holistic E-commerce Benchmark designed to evaluate agent performance in realistic e-commerce environments. EcomBench is built from genuine user demands embedded in leading global e-commerce ecosystems and is carefully curated and annotated through human experts to ensure clarity, accuracy, and domain relevance. It covers multiple task categories within e-commerce scenarios and defines three difficulty levels that evaluate agents on key capabilities such as deep information retrieval, multi-step reasoning, and cross-source knowledge integration. By grounding evaluation in real e-commerce contexts, EcomBench provides a rigorous and dynamic testbed for measuring the practical capabilities of agents in modern e-commerce.

</details>


### [43] [Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs](https://arxiv.org/abs/2512.08923)
*Angela van Sprang,Laurens Samson,Ana Lucic,Erman Acar,Sennay Ghebreab,Yuki M. Asano*

Main category: cs.AI

TL;DR: 该论文提出了REST和REST+两个基准测试，用于系统评估多模态大语言模型中的跨模态不一致性问题，发现当前最先进的MLLMs在不同模态（图像、文本、混合）下无法保持一致的推理能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型被训练在同一个嵌入空间中表示视觉和语言信息，但它们无法在两种模态中执行相同的任务。目前缺乏系统评估跨模态不一致性的基准测试，因此需要开发专门的评估工具来揭示这一重要问题。

Method: 作者创建了REST和REST+两个基准测试，包含在三种模态（图像、文本、混合）中具有相同语义信息的样本。评估了15个最先进的多模态大语言模型，分析它们在跨模态一致性方面的表现，并考虑了文本识别（OCR）问题的影响。

Result: 研究发现：1）所有评估的MLLMs都存在显著的跨模态不一致性；2）不一致程度因模型而异；3）将文本渲染为图像或将图像渲染为文本都无法解决不一致问题；4）视觉特征（文本颜色、分辨率）和视觉标记数量影响模型性能；5）一致性得分与文本和图像之间的模态差距相关。

Conclusion: 多模态大语言模型存在严重的跨模态不一致问题，这不仅仅是文本识别问题。REST基准测试为系统评估这一重要问题提供了工具，并揭示了跨模态不一致的机制解释，为未来改进MLLMs的一致性提供了方向。

Abstract: We introduce two new benchmarks REST and REST+(Render-Equivalence Stress Tests) to enable systematic evaluation of cross-modal inconsistency in multimodal large language models (MLLMs). MLLMs are trained to represent vision and language in the same embedding space, yet they cannot perform the same tasks in both modalities. Our benchmarks contain samples with the same semantic information in three modalities (image, text, mixed) and we show that state-of-the-art MLLMs cannot consistently reason over these different modalities. We evaluate 15 MLLMs and find that the degree of modality inconsistency varies substantially, even when accounting for problems with text recognition (OCR). Neither rendering text as image nor rendering an image as text solves the inconsistency. Even if OCR is correct, we find that visual characteristics (text colour and resolution, but not font) and the number of vision tokens have an impact on model performance. Finally, we find that our consistency score correlates with the modality gap between text and images, highlighting a mechanistic interpretation of cross-modal inconsistent MLLMs.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [44] [Finding core subgraphs of directed graphs via discrete Ricci curvature flow](https://arxiv.org/abs/2512.07899)
*Juan Zhao,Jicheng Ma,Yunyan Yang,Liang Zhao*

Main category: cs.SI

TL;DR: 提出了一种针对有向图的Ricci曲率定义和曲率流方法，用于从弱连通有向图中检测强连通子图，相比传统方法放宽了对图必须强连通的要求。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在无向图的Ricci曲率应用（如社区检测和核心提取），对有向图的关注相对较少。传统方法要求图必须是强连通的，限制了应用范围。

Method: 1. 为有向图定义Ricci曲率和曲率流；2. 对强连通有向图，该流有唯一全局解；3. 将弱连通图通过添加极大权重的边转化为强连通图，这些边在曲率流最终迭代中自动被丢弃；4. 应用曲率流检测强连通子图。

Result: 在核心子图检测评估中，该方法在至少三个关键指标中的两个上持续优于传统方法。代码已公开在GitHub上。

Conclusion: 提出的有向图Ricci曲率流方法能够有效检测弱连通有向图中的强连通子图，放宽了传统方法对图结构的限制要求，在核心子图检测任务上表现优异。

Abstract: Ricci curvature and its associated flow offer powerful geometric methods for analyzing complex networks. While existing research heavily focuses on applications for undirected graphs such as community detection and core extraction, there have been relatively less attention on directed graphs.
  In this paper, we introduce a definition of Ricci curvature and an accompanying curvature flow for directed graphs. Crucially, for strongly connected directed graphs, this flow admits a unique global solution. We then apply this flow to detect strongly connected subgraphs from weakly connected directed graphs. (A weakly connected graph is connected overall but not necessarily strongly connected). Unlike prior work requiring graphs to be strongly connected, our method loosens this requirement. We transform a weakly connected graph into a strongly connected one by adding edges with very large artificial weights. This modification does not compromise our core subgraph detection. Due to their extreme weight, these added edges are automatically discarded during the final iteration of the Ricci curvature flow.
  For core evaluation, our approach consistently surpasses traditional methods, achieving better results on at least two out of three key metrics. The implementation code is publicly available at https://github.com/12tangze12/Finding-core-subgraphs-on-directed-graphs.

</details>


### [45] [Scaffolding Reshapes Dialogic Engagement in Collaborative Problem Solving: Comparative Analysis of Two Approaches](https://arxiv.org/abs/2512.08045)
*Kester Wong,Feng Shihui,Sahan Bulathwela,Mutlu Cukurova*

Main category: cs.SI

TL;DR: 研究比较了最大和最小支架对K-12学生协作问题解决的影响，发现最大支架提高对话参与但导致过度脚本化，最小支架促进问题解决行为但易陷入重复和社交化。


<details>
  <summary>Details</summary>
Motivation: 现有研究比较了不同教学支架对协作问题解决的影响，但我们对这些支架如何在不同CPS阶段差异化影响个体对话参与和行为分布的理解仍然有限。

Method: 采用异质交互网络分析(HINA)和序列模式挖掘(SPM)方法，在真实教育环境中分析K-12学生在不同支架条件下的CPS过程结构效应。

Result: 最大支架学生在更多阶段表现出更高对话参与，但存在过度脚本化行为；最小支架学生展示更多问题解决行为和较少脚本行为，但易重复特定行为并转向社交行为；两种条件下问题解决行为都很少进展到其他问题解决行为。

Conclusion: 研究揭示了不同支架对CPS过程的差异化影响，为支架设计和教学实践提供重要启示，并展示了HINA和SPM方法在研究学生学习过程中的互补价值。

Abstract: Supporting learners during Collaborative Problem Solving (CPS) is a necessity. Existing studies have compared scaffolds with maximal and minimal instructional support by studying their effects on learning and behaviour. However, our understanding of how such scaffolds could differently shape the distribution of individual dialogic engagement and behaviours across different CPS phases remains limited. This study applied Heterogeneous Interaction Network Analysis (HINA) and Sequential Pattern Mining (SPM) to uncover the structural effects of scaffolding on different phases of the CPS process among K-12 students in authentic educational settings. Students with a maximal scaffold demonstrated higher dialogic engagement across more phases than those with a minimal scaffold. However, they were extensively demonstrating scripting behaviours across the phases, evidencing the presence of overscripting. Although students with the minimal scaffold demonstrated more problem solving behaviours and fewer scripting behaviours across the phases, they repeated particular behaviours in multiple phases and progressed more to socialising behaviours. In both scaffold conditions, problem solving behaviours rarely progressed to other problem solving behaviours. The paper discusses the implications of these findings for scaffold design and teaching practice of CPS, and highlights the distinct yet complementary value of HINA and SPM approaches to investigate students' learning processes during CPS.

</details>


### [46] [Fairness-aware PageRank via Edge Reweighting](https://arxiv.org/abs/2512.08055)
*Honglian Wang,Haoyun Chen,Aristides Gionis*

Main category: cs.SI

TL;DR: 提出一种通过重新加权转移概率将群体公平性融入PageRank算法的新方法，保持网络拓扑不变，仅调整现有边的相对重要性。


<details>
  <summary>Details</summary>
Motivation: 随着负责任AI的重要性日益增长，链接分析算法（如PageRank）的公平性问题受到关注。现有方法通常通过添加新边或调整重启向量来实现公平，但这些方法改变了网络结构。

Method: 通过最小化公平性损失（原始群体PageRank分布与目标分布之间的差异）来重新加权转移矩阵中的转移概率。提出考虑群体同质性的群体适应公平性概念，使用群体偏置重启的随机游走。采用高效的投影梯度下降方法计算局部最优边权重。

Result: 实验表明，该方法在保持网络拓扑不变的情况下，仅对转移矩阵进行微小调整就能显著提高PageRank算法的公平性，优于现有基线方法。

Conclusion: 提出了一种有效且实用的公平PageRank方法，通过重新加权现有边的相对重要性来实现群体公平，而不改变网络结构，为公平链接分析提供了新思路。

Abstract: Link-analysis algorithms, such as PageRank, are instrumental in understanding the structural dynamics of networks by evaluating the importance of individual vertices based on their connectivity. Recently, with the rising importance of responsible AI, the question of fairness in link-analysis algorithms has gained traction. In this paper, we present a new approach for incorporating group fairness into the PageRank algorithm by reweighting the transition probabilities in the underlying transition matrix. We formulate the problem of achieving fair PageRank by seeking to minimize the fairness loss, which is the difference between the original group-wise PageRank distribution and a target PageRank distribution. We further define a group-adapted fairness notion, which accounts for group homophily by considering random walks with group-biased restart for each group. Since the fairness loss is non-convex, we propose an efficient projected gradient-descent method for computing locally-optimal edge weights. Unlike earlier approaches, we do not recommend adding new edges to the network, nor do we adjust the restart vector. Instead, we keep the topology of the underlying network unchanged and only modify the relative importance of existing edges. We empirically compare our approach with state-of-the-art baselines and demonstrate the efficacy of our method, where very small changes in the transition matrix lead to significant improvement in the fairness of the PageRank algorithm.

</details>


### [47] [Framing Climate Change on YouTube: North-South Divides in Narratives and Public Engagement](https://arxiv.org/abs/2512.08183)
*Sanika Damle,Radhika Krishnan*

Main category: cs.SI

TL;DR: 该研究分析了YouTube上758个气候相关视频及其评论，发现南北半球在气候话语上存在显著差异：北方强调减排政策，南方关注发展优先；北方评论多为批评和阴谋论，南方评论则更支持性和知识导向。


<details>
  <summary>Details</summary>
Motivation: 现有研究多从全球视角分析YouTube上的气候话语，本研究则从南北半球差异的角度切入，探讨YouTube如何反映和重塑全球气候政治，以及平台内容与公众情绪之间的差距。

Method: 收集758个气候相关视频及其评论数据，运用主题建模和情感分析方法识别重复出现的话语模式，并与国际气候谈判中的辩论进行对比分析。

Result: 研究发现南北半球视频内容反映现实世界分歧：北方强调减排政策需求，南方突出发展优先事项。评论差异更明显：北方视频下多为批评、阴谋论和气候疲劳，南方视频下则更支持性、建设性和知识导向。两区域在减排和国际协议重要性方面存在共识。

Conclusion: YouTube既反映又重塑全球气候政治，揭示了精心策划的叙事与公众情绪之间的差距。弥合这些分歧可能有助于形成更具包容性和合作性的气候行动方法。

Abstract: Climate change debates have gained increasing visibility on social media, with YouTube emerging as one of the most influential platforms for political communication. Reaching billions of users worldwide, it functions both as a news outlet and as a space for public discourse. While existing studies of climate discourse on YouTube often adopt a global perspective, this study examines the platform through the lens of the Global North-South divide. We analyse a dataset of 758 climate-related videos and their comment sections, applying topic modelling and sentiment analysis to identify recurring discursive patterns. Through these patterns, we recognise parallels with respect to debates in international climate negotiations. The findings reveal notable differences. Videos from the Global North and Global South reflect real-world divides, with the North emphasising the need for policies to curb carbon emissions, while the South highlights developmental priorities. A key area of convergence between the regions lies in the shared recognition of the importance of emissions reduction and international agreements. Audience responses, however, diverge more sharply: comment sections under Global North videos are dominated by criticism, conspiracy, and climate fatigue, whereas those under Global South videos are generally more supportive, constructive, and knowledge-oriented. Overall, the study demonstrates how YouTube reflects and reshapes global climate politics, while also revealing the gap between curated narratives and public sentiment. Bridging these divides may contribute to more inclusive and cooperative approaches to climate action.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [48] [Accelerating Urban Science Research with AI Urban Scientist](https://arxiv.org/abs/2512.07849)
*Tong Xia,Jiankun Zhang,Ruiwen You,Ao Xu,Linghao Zhang,Tengyao Tu,Jingzhi Wang,Jinghua Piao,Yunke Zhang,Fengli Xu,Yong Li*

Main category: cs.CY

TL;DR: 提出知识驱动的AI城市科学家系统，通过多智能体框架实现端到端城市科学研究，加速城市科学从碎片化信息到系统解释的转化。


<details>
  <summary>Details</summary>
Motivation: 城市是复杂的自适应系统，尽管数据丰富但基本原理仍难以理清。城市科学面临根本挑战：如何将大量碎片化、跨学科信息转化为对城市功能和演化的连贯解释。现有通用AI系统缺乏城市科学所需的领域知识和方法深度。

Method: 构建知识驱动的AI城市科学家系统，从数千项高质量研究中提取假设、同行评审信号、数据集和分析模式，实现为协调的多智能体框架。系统能够生成结构化假设、检索和协调异构数据集、进行自动化实证分析和模拟，并以城市科学推理兼容的形式综合见解。

Result: 系统提供可重复使用的分析工具并支持社区驱动的扩展，降低高级城市分析的门槛。AI城市科学家不仅是助手，更是揭示城市系统机制、指导设计更具韧性和公平城市的积极合作者。

Conclusion: AI城市科学家通过知识驱动的方法和多智能体框架，为加速城市科学从信息到解释的转化提供了新途径，能够作为主动合作者参与城市系统研究和城市设计。

Abstract: Cities are complex, adaptive systems whose underlying principles remain difficult to disentangle despite unprecedented data abundance. Urban science therefore faces a fundamental challenge: converting vast, fragmented and interdisciplinary information into coherent explanations of how cities function and evolve. The emergence of AI scientists, i.e., agents capable of autonomous reasoning, hypothesis formation and data-driven experimentation, offers a new pathway toward accelerating this transformation, yet general-purpose systems fall short of the domain knowledge and methodological depth required for urban science research. Here we introduce a knowledge-driven AI Urban Scientist, built from hypotheses, peer-review signals, datasets and analytical patterns distilled from thousands of high-quality studies, and implemented as a coordinated multi-agent framework for end-to-end inquiry. The system generates structured hypotheses, retrieves and harmonizes heterogeneous datasets, conducts automated empirical analysis and simulation, and synthesizes insights in forms compatible with urban scientific reasoning. By providing reusable analytical tools and supporting community-driven extensions, the AI Urban Scientist lowers barriers to advanced urban analytics and acts not merely as an assistant but as an active collaborator in revealing the mechanisms that shape urban systems and in guiding the design of more resilient and equitable cities.

</details>


### [49] [Evolutionary perspective of large language models on shaping research insights into healthcare disparities](https://arxiv.org/abs/2512.08122)
*David An*

Main category: cs.CY

TL;DR: 该研究通过追踪ChatGPT、Copilot和Gemini三大LLM在医疗健康差异研究主题回答上的月度变化，验证了LLM作为科学助手帮助理解复杂研究领域的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的进步，它们可以作为科学助手帮助人们理解复杂的研究领域。本研究旨在探索LLM在医疗健康差异研究中的演变，关注公众获取相关信息的可能性。

Method: 研究选取ChatGPT、Copilot和Gemini三个知名LLM，每周向它们提出关于医疗健康差异研究主题的一致提示，持续追踪一个月的回答变化。将LLM生成的主题分类，并与Web of Science的H指数值交叉验证相关性。

Result: LLM的输出与实际科学影响和领域趋势相符，表明LLM能帮助人们理解医疗健康差异研究格局。时间序列比较显示不同模型在主题识别和分类的广度与深度上存在差异。

Conclusion: 研究提出了一个利用多个LLM演变来阐明AI工具在医疗健康差异研究中的应用框架，为未来研究和公众参与策略提供信息。

Abstract: Introduction. Advances in large language models (LLMs) offer a chance to act as scientific assistants, helping people grasp complex research areas. This study examines how LLMs evolve in healthcare disparities research, with attention to public access to relevant information. Methods. We studied three well-known LLMs: ChatGPT, Copilot, and Gemini. Each week, we asked them a consistent prompt about research themes in healthcare disparities and tracked how their answers changed over a one-month period. Analysis. The themes produced by the LLMs were categorized and cross-checked against H-index values from the Web of Science to verify relevance. This dual approach shows how the outputs of LLMs develop over time and how such progress could help researchers navigate trends. Results. The outputs aligned with actual scientific impact and trends in the field, indicating that LLMs can help people understand the healthcare disparities landscape. Time-series comparisons showed differences among the models in how broadly and deeply they identified and classified themes. Conclusion. The study offers a framework that uses the evolution of multiple LLMs to illuminate AI tools for studying healthcare disparities, informing future research and public engagement strategies.

</details>


### [50] [Examining Student Interactions with a Pedagogical AI-Assistant for Essay Writing and their Impact on Students Writing Quality](https://arxiv.org/abs/2512.08596)
*Wicaksono Febriantoro,Qi Zhou,Wannapon Suraworachet,Sahan Bulathwela,Andrea Gauthier,Eva Millan,Mutlu Cukurova*

Main category: cs.CY

TL;DR: 研究通过分析学生与AI写作助手的互动日志，发现主动写作并寻求反馈的学生在文章组织方面表现更好，而被动提问的学生获益较少。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注通用生成式AI对写作的支持，但缺乏对学生与教学设计的AI系统在不同写作阶段互动动态的研究，特别是这些互动如何影响写作质量。

Method: 使用AI驱动的议论文写作助手，收集32名本科生在2小时写作会话中的1282条互动日志，采用序列模式挖掘和K-Means聚类分析行为模式，并通过Mann-Whitney U检验比较写作质量。

Result: 识别出两个行为集群：集群1关注大纲规划和文章结构，集群2关注内容发展。集群1在文章组织维度得分显著更高（效应量r=0.36）。定性分析显示，表现更好的学生主动写作并与AI分享文章段落寻求反馈，而非被动提问。

Conclusion: 教师应鼓励学生主动参与写作过程，未来的AI写作助手可集成自动标签和监控功能，引导学生从被动提问转向主动写作，从而更充分地利用AI支持的学习优势。

Abstract: The dynamic nature of interactions between students and GenAI, as well as their relationship to writing quality, remains underexplored. While most research has examined how general-purpose GenAI can support writing, fewer studies have investigated how students interact with pedagogically designed systems across different phases of the writing process. To address this gap, we evaluated a GenAI-driven essay-writing assistant (EWA) designed to support higher education students in argumentative writing. Drawing on 1,282 interaction logs from 32 undergraduates during a two-hour writing session, Sequential Pattern Mining and K-Means clustering were used to identify behavioral patterns. Two clusters emerged: Cluster 1 emphasized outline planning and essay structure, while Cluster 2 focused on content development. A Mann-Whitney U test revealed a moderate effect size (r = 0.36) in the essay Organization dimension, with Cluster 1 showing higher scores. Qualitative analysis indicated that students with better performance actively wrote and shared essay sections with EWA for feedback, rather than interacted passively by asking questions. These findings suggest implications for teaching and system design. Teachers can encourage active engagement, while future EWAs may integrate automatic labeling and monitoring to prompt students to move from questioning to writing, enabling fuller benefits from GenAI-supported learning.

</details>


### [51] [The Role of Risk Modeling in Advanced AI Risk Management](https://arxiv.org/abs/2512.08723)
*Chloé Touzet,Henry Papadatos,Malcolm Murray,Otter Quarks,Steve Barrett,Alejandro Tlaie Boria,Elija Perrier,Matthew Smith,Siméon Campos*

Main category: cs.CY

TL;DR: AI风险建模需要结合场景构建和风险评估，借鉴传统风险管理方法，并采用确定性和概率性相结合的双重治理框架。


<details>
  <summary>Details</summary>
Motivation: 快速发展的AI系统带来了新颖、不确定且可能灾难性的风险，需要成熟的风险管理基础设施，而风险建模是其基石。

Method: 将AI风险建模概念化为场景构建（从危险到危害的因果映射）和风险评估（量化每条路径的可能性和严重性）的紧密结合。回顾并调整传统技术（故障树分析、事件树分析、FMEA/FMECA、STPA、贝叶斯网络）以适应高级AI。

Result: 调查发现当前学术和行业努力存在碎片化：能力基准测试、安全案例和部分定量研究虽然有价值，但脱离全面的因果场景是不够的。比较核能、航空、网络安全、金融和潜艇领域，发现每个行业都结合了对不可接受事件的确定性保证和对更广泛风险景观的概率性评估。

Conclusion: 高级AI治理应采用类似的双重方法，迫切需要可验证、可证明安全的AI架构来提供确定性证据。提出了一个治理就绪的框架：开发者进行迭代风险建模，监管者将结果与预定义的社会风险容忍阈值进行比较。

Abstract: Rapidly advancing artificial intelligence (AI) systems introduce novel, uncertain, and potentially catastrophic risks. Managing these risks requires a mature risk-management infrastructure whose cornerstone is rigorous risk modeling. We conceptualize AI risk modeling as the tight integration of (i) scenario building$-$causal mapping from hazards to harms$-$and (ii) risk estimation$-$quantifying the likelihood and severity of each pathway. We review classical techniques such as Fault and Event Tree Analyses, FMEA/FMECA, STPA and Bayesian networks, and show how they can be adapted to advanced AI. A survey of emerging academic and industry efforts reveals fragmentation: capability benchmarks, safety cases, and partial quantitative studies are valuable but insufficient when divorced from comprehensive causal scenarios. Comparing the nuclear, aviation, cybersecurity, financial, and submarine domains, we observe that every sector combines deterministic guarantees for unacceptable events with probabilistic assessments of the broader risk landscape. We argue that advanced-AI governance should adopt a similar dual approach and that verifiable, provably-safe AI architectures are urgently needed to supply deterministic evidence where current models are the result of opaque end-to-end optimization procedures rather than specified by hand. In one potential governance-ready framework, developers conduct iterative risk modeling and regulators compare the results with predefined societal risk tolerance thresholds. The paper provides both a methodological blueprint and opens a discussion on the best way to embed sound risk modeling at the heart of advanced-AI risk management.

</details>


### [52] [Insured Agents: A Decentralized Trust Insurance Mechanism for Agentic Economy](https://arxiv.org/abs/2512.08737)
*Botao 'Amber' Hu,Bangdao Chen*

Main category: cs.CY

TL;DR: 提出"受保代理"协议，通过专业保险人代理为操作代理提供抵押担保，利用TEE进行隐私保护审计，通过分层保险市场实现去中心化验证和激励相容的争议解决。


<details>
  <summary>Details</summary>
Motivation: 当前代理网络将代理视为低成本身份，但LLM代理存在不可靠、幻觉、易受攻击等问题。传统方法如"代理抵押"面临异构任务验证困难、中心化倾向，而传统声誉系统难以应对快速模型漂移和不透明内部状态。

Method: 提出受保代理协议：专业保险人代理为操作代理提供抵押担保以换取保费，通过可信执行环境(TEE)获得特权但隐私保护的审计访问权限。建立分层保险市场，通过定价校准抵押，通过竞争性承保实现去中心化验证，实现激励相容的争议解决机制。

Result: 该协议为代理网络提供了一种协议原生的替代方案，能够解决LLM代理的可靠性问题，同时避免传统方法的验证脆弱性和中心化风险。

Conclusion: 受保代理协议通过保险市场机制，在保持去中心化的同时，为不可靠的LLM代理提供了经济担保和验证框架，是实现"代理网络"愿景的关键基础设施。

Abstract: The emerging "agentic web" envisions large populations of autonomous agents coordinating, transacting, and delegating across open networks. Yet many agent communication and commerce protocols treat agents as low-cost identities, despite the empirical reality that LLM agents remain unreliable, hallucinated, manipulable, and vulnerable to prompt-injection and tool-abuse. A natural response is "agents-at-stake": binding economically meaningful, slashable collateral to persistent identities and adjudicating misbehavior with verifiable evidence. However, heterogeneous tasks make universal verification brittle and centralization-prone, while traditional reputation struggles under rapid model drift and opaque internal states. We propose a protocol-native alternative: insured agents. Specialized insurer agents post stake on behalf of operational agents in exchange for premiums, and receive privileged, privacy-preserving audit access via TEEs to assess claims. A hierarchical insurer market calibrates stake through pricing, decentralizes verification via competitive underwriting, and yields incentive-compatible dispute resolution.

</details>


### [53] [A Methodology for Quantitative AI Risk Modeling](https://arxiv.org/abs/2512.08844)
*Malcolm Murray,Steve Barrett,Henry Papadatos,Otter Quarks,Matt Smith,Alejandro Tlaie Boria,Chloé Touzet,Siméon Campos*

Main category: cs.CY

TL;DR: 提出一种整合情景构建与定量风险评估的AI风险管理方法，通过六步流程建模系统性AI风险，并在网络攻击场景中验证


<details>
  <summary>Details</summary>
Motivation: 通用AI系统带来变革机遇的同时引发安全、滥用和失控等严重风险，但现有风险评估方法不足。需要系统化建模来刻画潜在危害，以支持有效的风险管理

Method: 提出六步风险建模方法：1)定义风险情景；2)分解为可量化参数；3)量化无AI时的基线风险；4)识别关键风险指标；5)将指标映射到模型参数以估计LLM提升；6)聚合参数形成风险估计

Result: 该方法适用于系统性AI风险（网络攻击、生物武器开发、有害操纵、失控），并在LLM增强的网络攻击场景中得到广泛验证。详细实证结果在配套论文中呈现

Conclusion: 该方法为AI风险管理提供了系统化的风险建模框架，能够生成具体可验证的风险声明，有助于应对通用AI系统带来的关键风险挑战

Abstract: Although general-purpose AI systems offer transformational opportunities in science and industry, they simultaneously raise critical concerns about safety, misuse, and potential loss of control. Despite these risks, methods for assessing and managing them remain underdeveloped. Effective risk management requires systematic modeling to characterize potential harms, as emphasized in frameworks such as the EU General-Purpose AI Code of Practice. This paper advances the risk modeling component of AI risk management by introducing a methodology that integrates scenario building with quantitative risk estimation, drawing on established approaches from other high-risk industries. Our methodology models risks through a six-step process: (1) defining risk scenarios, (2) decomposing them into quantifiable parameters, (3) quantifying baseline risk without AI models, (4) identifying key risk indicators such as benchmarks, (5) mapping these indicators to model parameters to estimate LLM uplift, and (6) aggregating individual parameters into risk estimates that enable concrete claims (e.g., X% probability of >\$Y in annual cyber damages). We examine the choices that underlie our methodology throughout the article, with discussions of strengths, limitations, and implications for future research. Our methodology is designed to be applicable to key systemic AI risks, including cyber offense, biological weapon development, harmful manipulation, and loss-of-control, and is validated through extensive application in LLM-enabled cyber offense. Detailed empirical results and cyber-specific insights are presented in a companion paper.

</details>


### [54] [Can the GPC standard eliminate consent banners in the EU?](https://arxiv.org/abs/2512.08856)
*Sebastian Zimmeck,Harshvardhan J. Pandit,Frederik Zuiderveen Borgesius,Cristiana Teixeira Santos,Konrad Kollnig,Robin Berjon*

Main category: cs.CY

TL;DR: 本文探讨全球隐私控制(GPC)能否适应欧盟法律框架以缓解同意疲劳并改善隐私保护


<details>
  <summary>Details</summary>
Motivation: 欧盟GDPR和ePrivacy指令要求行为广告和追踪技术需获得知情同意，但普遍存在的同意横幅导致同意疲劳，质疑这些机制保护用户数据的有效性。相比之下，加州等美国司法管辖区用户可使用GPC这种浏览器隐私信号自动广播具有法律约束力的退出请求。

Method: 分析GPC作为W3C标准化的技术规范，并审查其在当前欧盟数据保护法下的地位。将GPC映射到GDPR下的各种处理法律基础，评估GPC规范与欧盟数据保护法之间的摩擦。

Result: GPC可以映射到GDPR下的各种法律基础，但评估也发现了GPC规范与现行欧盟数据保护法之间的摩擦。这些差异是可以解决的，为欧盟立法者和监管机构提供了将GPC解释为符合欧盟数据保护要求的机会。

Conclusion: 虽然GPC不是万能解决方案，但其采纳——辅以明确的权威指导和规范更新——可以为欧盟提供一条更自动化、更有效数据保护的务实路径。

Abstract: In the EU, the General Data Protection Regulation and the ePrivacy Directive mandate informed consent for behavioural advertising and use of tracking technologies. However, the ubiquity of consent banners and popups has led to widespread consent fatigue and questions regarding the effectiveness of these mechanisms in protecting users' data. In contrast, users in California and other US jurisdictions can utilize Global Privacy Control (GPC), a browser-based privacy signal that automatically broadcasts a legally binding opt-out request to websites. In this paper we explore whether, and to what extent, GPC can be adapted to the EU legal framework to mitigate consent fatigue and improve privacy protections for EU residents.
  We analyse GPC as a technical specification standardized at the World Wide Web Consortium and examine its standing under current EU data protection law. Generally, GPC can be mapped to the various legal bases for processing under the GDPR. However, our evaluation also identifies friction between the GPC specification and EU data protection law as it stands. These discrepancies are resolvable and present an opportunity for EU legislators and regulators to interpret GPC in alignment with EU data protection requirements, particularly, considering the European Commission's recent Digital Omnibus proposal. We conclude that while GPC is not a silver bullet, its adoption -- supported by clear authoritative guidance and specification updates -- can offer a pragmatic path toward more automated and effective data protection in the EU.

</details>


### [55] [Toward Quantitative Modeling of Cybersecurity Risks Due to AI Misuse](https://arxiv.org/abs/2512.08864)
*Steve Barrett,Malcolm Murray,Otter Quarks,Matthew Smith,Jakub Kryś,Siméon Campos,Alejandro Tlaie Boria,Chloé Touzet,Sevan Hayrapet,Fred Heiding,Omer Nevo,Adam Swanda,Jair Aguirre,Asher Brass Gershovich,Eric Clay,Ryan Fetterman,Mario Fritz,Marc Juarez,Vasilios Mavroudis,Henry Papadatos*

Main category: cs.CY

TL;DR: 该技术报告应用定量风险建模方法分析AI对网络攻击的增强效应，通过专家评估和LLM模拟专家，将AI基准性能映射到风险因素，发现AI系统性地提升了攻击效能、速度和目标范围。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统带来巨大利益的同时也引入风险，2025年AI增强的网络攻击已成为具体案例。需要从定性评估转向定量风险建模，为网络安全团队、AI评估者、开发者和政策制定者提供决策依据，类似核能等高危行业的风险管理演变。

Method: 开发了九个详细的网络风险模型，使用MITRE ATT&CK框架将攻击分解为步骤，评估AI对攻击者数量、攻击频率、成功概率和危害的影响。采用德尔菲研究的人类专家和基于LLM的模拟专家，将Cybench和BountyBench基准分数映射到风险因素，通过蒙特卡洛模拟聚合个体估计。

Result: 结果显示AI系统性地提升了攻击效能、速度和目标范围，不同风险模型中存在不同的增强机制。虽然估计存在显著不确定性，但详细的量化结果能让专家明确分歧点，共同改进估计。

Conclusion: 该定量风险建模方法为AI风险管理迈出了重要一步，有助于网络安全团队优先缓解措施、AI评估者设计基准、开发者做出更明智的部署决策、政策制定者设定风险阈值。量化结果使专家能够精确定位分歧，共同改进估计，这是定性评估无法实现的。

Abstract: Advanced AI systems offer substantial benefits but also introduce risks. In 2025, AI-enabled cyber offense has emerged as a concrete example. This technical report applies a quantitative risk modeling methodology (described in full in a companion paper) to this domain. We develop nine detailed cyber risk models that allow analyzing AI uplift as a function of AI benchmark performance. Each model decomposes attacks into steps using the MITRE ATT&CK framework and estimates how AI affects the number of attackers, attack frequency, probability of success, and resulting harm to determine different types of uplift. To produce these estimates with associated uncertainty, we employ both human experts, via a Delphi study, as well as LLM-based simulated experts, both mapping benchmark scores (from Cybench and BountyBench) to risk model factors. Individual estimates are aggregated through Monte Carlo simulation. The results indicate systematic uplift in attack efficacy, speed, and target reach, with different mechanisms of uplift across risk models. We aim for our quantitative risk modeling to fulfill several aims: to help cybersecurity teams prioritize mitigations, AI evaluators design benchmarks, AI developers make more informed deployment decisions, and policymakers obtain information to set risk thresholds. Similar goals drove the shift from qualitative to quantitative assessment over time in other high-risk industries, such as nuclear power. We propose this methodology and initial application attempt as a step in that direction for AI risk management. While our estimates carry significant uncertainty, publishing detailed quantified results can enable experts to pinpoint exactly where they disagree. This helps to collectively refine estimates, something that cannot be done with qualitative assessments alone.

</details>


### [56] [AI Didn't Start the Fire: Examining the Stack Exchange Moderator and Contributor Strike](https://arxiv.org/abs/2512.08884)
*Yiwei Wu,Leah Ajmani,Nathan TeBlunthuis,Hanlin Li*

Main category: cs.CY

TL;DR: 研究探讨了在线社区与平台之间的冲突动态，以2023年Stack Exchange社区因LLM政策引发的罢工事件为例，分析了冲突演变、集体行动组织及后续发展。


<details>
  <summary>Details</summary>
Motivation: 在线社区与平台之间存在相互依赖但易冲突的关系。当平台政策与社区价值观冲突时，社区会通过罢工、迁移等方式抵抗，但这些行动往往只能获得暂时性让步。现有研究关注罢工事件和迁移链，但对社区-平台冲突如何展开的过程仍不清楚。

Method: 采用定性主题分析方法，分析了Meta Stack Exchange上的2,070条消息，并对14名社区成员进行了访谈，研究2023年Stack Exchange平台与社区围绕LLM政策引发的冲突。

Result: 研究发现：1）冲突前社区-平台关系已长期恶化，主要原因是平台忽视社区在治理中的参与角色；2）平台对LLM的政策回应加剧了社区的危机感，触发罢工动员；3）动员通过分层领导和沟通结构协调；4）冲突后社区成员进行了策略调整。

Conclusion: 基于Hirshman的退出、发声和忠诚框架，理论化了社区-平台关系的挑战，并建议平台和社区应建立持久有效的参与式治理机制。

Abstract: Online communities and their host platforms are mutually dependent yet conflict-prone. When platform policies clash with community values, communities have resisted through strikes, blackouts, and even migration to other platforms. Through such collective actions, communities have sometimes won concessions but these have frequently proved temporary. Prior research has investigated strike events and migration chains, but the processes by which community-platform conflict unfolds remain obscure. How do community-platform relationships deteriorate? How do communities organize collective action? How do participants proceed in the aftermath? We investigate a conflict between the Stack Exchange platform and community that occurred in 2023 around an emergency arising from the release of large language models (LLMs). Based on a qualitative thematic analysis of 2,070 messages on Meta Stack Exchange and 14 interviews with community members, we surface how the 2023 conflict was preceded by a long-term deterioration in the community-platform relationship driven in particular by the platform's disregard for the community's highly-valued participatory role in governance. Moreover, the platform's policy response to LLMs aggravated the community's sense of crisis triggering the strike mobilization. We analyze how the mobilization was coordinated through a tiered leadership and communication structure, as well as how community members pivoted in the aftermath. Building on recent theoretical scholarship in social computing, we use Hirshman's exit, voice and loyalty framework to theorize the challenges of community-platform relations evinced in our data. Finally, we recommend ways that platforms and communities can institute participatory governance to be durable and effective.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [57] [Defining 3-dimensional marine provinces with phytoplankton compositions](https://arxiv.org/abs/2512.08035)
*Rafael Catoia Pulgrossi,Nathan L R Williams,Yubin Raut,Jed Fuhrman,Sangwon Hyun*

Main category: stat.AP

TL;DR: 开发了名为bioprovince的新算法，利用浮游植物分布数据在三维空间（纬度、经度、深度）中划分海洋生物区，相比传统二维方法能提供更精细的生态分区。


<details>
  <summary>Details</summary>
Motivation: 传统海洋生物区划分通常只基于经纬度二维空间，缺乏精细的生物数据和深度维度。本研究旨在利用浮游植物分布数据，在三维空间中定义更准确的海洋生物区。

Method: 开发了bioprovince算法：1）对组成性生物样本进行聚类，识别空间上一致的样本组；2）基于环境相似性在三维空间网格中进行灵活的区域预测。将该算法应用于太平洋五个南北向深度解析的海洋断面中的浮游植物ASV数据。

Result: 在海洋表层，该方法与传统Longhurst生物区划分结果一致，但ASV提供的更高分类分辨率能将传统区域划分为更小的区域。更重要的是，该方法成功识别了深度维度上的分区，特别是在赤道区域与真光层底部的划分高度一致。

Conclusion: bioprovince算法能够划定三维生物区，为海洋浮游植物生态学和生物地理学提供新的生态学解释。该算法不仅适用于海洋浮游生物，还可广泛应用于其他环境中的生物区划分，提供更全面的生物地理学视角。

Abstract: Marine provinces rarely include fine-resolution biological data, and are often defined spatially across only latitude and longitude. Therefore, we aimed to determine how phytoplankton distributions define marine provinces across 3-dimensions (i.e., latitude, longitude, and depth). To do this, we developed a new algorithm called \texttt{bioprovince} which can be applied to compositional biological data. The algorithm first clusters compositional samples to identify spatially coherent groups of samples, then makes flexible province predictions in the broader 3d spatial grid based on environmental similarity. We applied \texttt{bioprovince} to phytoplankton Amplicon Sequencing Variants (ASVs) from five, depth-resolved ocean transects spanning north-south in the Pacific Ocean. In the surface layer of the ocean, our method agreed well with traditional Longhurst provinces. In some cases, the method revealed that with more granular taxonomic resolution afforded by ASVs, traditional Longhurst provinces were divided into smaller zones. Also, one of the major advances of this method is its ability to incorporate a third dimension, depth. Indeed, our analysis found significant depth-wise partitions throughout the Pacific with remarkable agreement in the equatorial region with the base of the euphotic zone. Our algorithm's ability to delineate 3-dimensional bioprovinces will enable scientists to discover new ecological interpretations of marine phytoplankton ecology and biogeography. Furthermore, as compositional biological data inherently exists in three spatial dimensions in nature, bioprovince is broadly applicable beyond marine plankton, offering a more holistic perspective on biological provinces across diverse environments.

</details>


### [58] [Distribution of Gaps in Multi-lane Orderly and Disorderly Traffic Streams](https://arxiv.org/abs/2512.08585)
*Ankita Sharma,Partha Chakroborty,Pranamesh Chakraborty*

Main category: stat.AP

TL;DR: 基于更新过程理论开发多车道有序和无序交通流间隙分布的解析框架，提出参数估计方法，并用实际数据验证模型有效性


<details>
  <summary>Details</summary>
Motivation: 研究间隙接受行为需要了解对向车流的间隙分布，交通仿真也需要随机生成符合真实统计特征的车流。现有文献对多车道有序和无序交通流的间隙分布研究不足，需要开发相应的分析框架。

Method: 使用更新过程理论建立多车道有序和无序交通流间隙分布的解析框架，提出基于最大似然估计的参数估计方法，并用三个不同地点的实际间隙数据进行验证。

Result: 提出的分布函数能够很好地描述观测到的间隙分布，验证了所提方法的有效性。

Conclusion: 成功开发了多车道有序和无序交通流间隙分布的解析框架，为间隙接受行为研究和交通仿真提供了理论基础和实用工具。

Abstract: To study gap acceptance behaviour one needs the distribution (or probability density function) of gaps in the opposing stream. Further, in these times of widespread availability of large computing powers, traffic simulation has emerged as a popular analysis and design tool. Such simulations rely on randomly generating the arriving vehicles in a way that statistically resembles real-world streams. The generation process for disorderly streams requires information on gap distributions. A study of past literature reveals that very little work has been done to determine the distribution of gaps on multi-lane orderly and disorderly streams. This study aims to develop an analytical framework to specify the distribution of gaps for such streams. This analytical framework is built using the Renewal Process Theory. A maximum likelihood based process for the estimation of the parameters of the analytically derived distribution is also described. Later, real-world gap data from three different sites covering orderly and disorderly streams are used to show how the derived distribution function (using the proposed method) ably describes the observed gap distributions.

</details>


### [59] [Genetic Regression Analysis of Human Brain Connectivity Using an Efficient Estimator of Genetic Covariance](https://arxiv.org/abs/2512.08756)
*Keshav Motwani,Ali Shojaie,Ariel Rokem,Eardi Lila*

Main category: stat.AP

TL;DR: 开发遗传回归框架分析结构-功能连接体关系，发现环境因素掩盖了遗传层面的强关联


<details>
  <summary>Details</summary>
Motivation: 遗传因素对大脑结构连接和功能连接的个体差异有重要影响，但环境因素掩盖了结构-功能连接体之间的遗传关联，需要新方法来揭示这种潜在关系

Method: 开发回归分析框架，使用约束矩估计法高效估计遗传协方差矩阵，采用正则化估计方法（岭回归、LASSO、张量回归）进行遗传回归分析

Result: 在遗传层面，功能连接可以从结构连接中适度预测（最大R²=0.34），但在观测数据中几乎不可预测（最大R²=0.03），表明环境因素掩盖了遗传编码的结构-功能关系

Conclusion: 环境因素强烈影响大脑结构-功能连接体关系，掩盖了遗传层面的强关联，新方法为理解遗传如何塑造大脑网络组织提供了重要工具

Abstract: Non-invasive measurements of the human brain using magnetic resonance imaging (MRI) have significantly improved our understanding the brain's network organization by enabling measurement of anatomical connections between brain regions (structural connectivity) and their coactivation (functional connectivity). Heritability analyses have established that genetics account for considerable intersubject variability in structural and functional connectivity. However, characterizing how genetics shape the relationship between structural and functional connectomes remains challenging, since this association is obscured by unique environmental exposures in observed data. To address this, we develop a regression analysis framework that enables characterization of the relationship between latent genetic contributions to structural and functional connectivity. Implementing the proposed framework requires estimating genetic covariance matrices in multivariate random effects models, which is computationally intractable for high-dimensional connectome data using existing methods. We introduce a constrained method-of-moments estimator that is several orders of magnitude faster than existing methods without sacrificing estimation accuracy. For the genetic regression analysis, we develop regularized estimation approaches, including ridge, lasso, and tensor regression. Applying our method to Human Connectome Project data, we find that functional connectivity is moderately predictable from structure at the genetic level (max R^2 = 0.34), though it is not directly predictable in the observed data (max R^2 = 0.03). This stark contrast suggests that unique environmental factors mask strong genetically-encoded structure-function relationships.

</details>


### [60] [Commanding the Foul Shot: A New Ensemble of Free Throw Metrics](https://arxiv.org/abs/2512.08824)
*Jake McGrath,Amanda Glazer,Vanna Bushong,Michelle Nguyen,Kirk Goldsberry*

Main category: stat.AP

TL;DR: 论文利用NBA Hawk-Eye肢体追踪数据，开发了一套评估罚球技能的新指标框架，比传统命中/未命中统计更能有效衡量球员技能。


<details>
  <summary>Details</summary>
Motivation: 随着NBA在2023年采用实时肢体追踪技术，Hawk-Eye系统能以每秒60次频率捕捉球员和篮球的高分辨率3D姿态。将这些数据与投篮、传球、篮板等关键事件关联，为NBA分析开启了新时代。传统罚球统计（命中/未命中）过于简单，无法区分不同质量投篮之间的差异。

Method: 1. 引入"command"指标：受棒球分析启发，通过测量投篮者在篮筐靶心附近的准确度和精度来量化罚球质量；2. 定义基于出手的指标：评估出手速度、角度和3D位置的一致性；3. 开发物理模型：识别导致命中的出手条件范围，并确定哪些条件对小扰动最稳健。

Result: 新框架能更有效地捕捉罚球技能，识别出"安全"的出手区域，解释了为什么某些球员（如斯蒂芬·库里）在罚球方面表现出色。具有更好"手感"（更一致的出手动态）的球员表现出更强的command，因为他们能可靠地控制投篮轨迹。

Conclusion: 该分析框架为球员发展提供了可操作的见解，通过物理模型揭示了稳健的出手条件，证明基于Hawk-Eye追踪数据的新指标能比传统统计更好地评估罚球技能。

Abstract: With the NBA's adoption of in-game limb tracking in 2023, Sony's Hawk-Eye system now captures high-resolution, 3D poses of players and the ball 60 times per second. Linking these data to key events such as shots, passes, and rebounds opens a new era in NBA analytics. Here, we leverage Hawk-Eye tracking to introduce a novel ensemble of metrics for evaluating free-throw shooting and demonstrate that our framework captures skill more effectively than traditional make-or-miss statistics. Inspired by baseball analytics, we introduce command, which quantifies the quality of a free throw by measuring a shooter's accuracy and precision near the basket's bullseye. This metric recognizes that some makes (or misses) are better than others and captures a player's ability to execute quality attempts consistently. To identify what drives command, we define launch-based metrics assessing consistency in release velocity, angle, and 3D position. Players with greater touch -- i.e., more consistent launch dynamics -- exhibit stronger command as they can reliably control their shot trajectory. Finally, we develop a physics model to identify the range of launch conditions that result in a make and to determine which launch conditions are most robust to small perturbations. This framework reveals "safe" launch regions and explains why certain players, such as Steph Curry, excel at free throws, providing actionable insights for player development.

</details>


### [61] [Multifractal behavior of price changes in the Green Bonds funds](https://arxiv.org/abs/2512.08886)
*Wenderson Gomes Barbosa,Kerolly Kedma Felix do Nascimento,Fábio Sandro dos Santos,Silvio Fernando Alves Xavier Júnior,Tiago A. E. Ferreira*

Main category: stat.AP

TL;DR: 该研究使用MFDFA方法分析35只绿色债券基金的日价格变化，发现其具有持续性行为和高度多重分形特征，表现出与其他金融资产相似的多重分形行为。


<details>
  <summary>Details</summary>
Motivation: 气候变化推动市场寻求新的融资方式来缓解其影响，绿色债券作为支持可持续项目的金融资产应运而生。研究旨在探索绿色债券基金价格变化的分形行为特征。

Method: 采用多重分形去趋势波动分析（MFDFA）方法，对35只绿色债券基金的日价格变化时间序列进行分析，并通过数据重排来验证多重分形特征。

Result: 价格变化表现出持续性行为和高度多重分形特征，波动较大。35个时间序列中只有一个出现异常结果，表明基金行为非常相似。数据重排后多重分形性显著降低。

Conclusion: 绿色债券基金表现出与其他金融资产典型的多重分形行为，这有助于理解其市场动态和风险特征。

Abstract: Climate change has driven the market to seek new ways of raising funds to mitigate its effects. One such innovation is the emergence of Green Bonds financial assets specifically designed to support sustainable projects. This study explores the fractal behavior of daily price changes in thirty-five Green Bond funds using the Multifractal Detrended Fluctuation Analysis (MFDFA) method. Our results indicate that price changes exhibit persistent behavior and high multifractality, characterized by large fluctuations. Only one of the thirty-five time series analyzed showed an outlier result, suggesting that the funds display very similar behavior. By shuffling the series, we were able to reduce multifractality significantly. These findings suggest that Green Bond funds exhibit multifractal behavior typical of other financial assets.

</details>

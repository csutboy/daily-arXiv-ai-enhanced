<div id=toc></div>

# Table of Contents

- [stat.AP](#stat.AP) [Total: 2]
- [cs.AI](#cs.AI) [Total: 29]
- [cs.SI](#cs.SI) [Total: 5]
- [econ.EM](#econ.EM) [Total: 1]
- [cs.CY](#cs.CY) [Total: 7]


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [1] [Estimating the distance at which narwhal $(\textit{Monodon monoceros})$ respond to disturbance: a penalized threshold hidden Markov model](https://arxiv.org/abs/2602.09267)
*Fanny Dupont,Marianne Marcoux,Nigel E. Hussey,Jackie Dawson,Marie Auger-Méthé*

Main category: stat.AP

TL;DR: 提出一种带lasso惩罚的阈值隐马尔可夫模型，用于量化野生动物对干扰的行为反应，能有效识别有意义的阈值效应并应用于北极独角鲸对船舶的反应分析。


<details>
  <summary>Details</summary>
Motivation: 北极海冰减少导致新航运路线开放，需要评估船舶对海洋哺乳动物的影响距离，以制定针对性缓解政策。现有阈值隐马尔可夫模型虽能估计阈值，但无法判断该阈值是否反映有意义的行为变化。

Method: 提出lasso惩罚的阈值隐马尔可夫模型，基于计算高效的方法对HMM施加惩罚，并开发新的惩罚准限制最大似然估计器。该框架能同时估计阈值并评估干扰效应是否显著。

Result: 模拟实验显示lasso方法能有效将虚假阈值效应收缩至零。应用于独角鲸运动数据分析表明，独角鲸在4公里外就对船舶产生反应：降低运动持续性并更多时间待在深水区（平均最大深度356米）。

Conclusion: 提供了一个广泛适用的框架，用于量化生物对刺激的行为反应，应用范围包括确定干扰反应阈值、估计陆地物种（如大象）探测水源的距离等。

Abstract: Understanding behavioural responses to disturbances is vital for wildlife conservation. For example, in the Arctic, the decrease in sea ice has opened new shipping routes, increasing the need for impact assessments that quantify the distance at which marine mammals react to vessel presence. This information can then guide targeted mitigation policies, such as vessel slow-down regulations and delineation of avoidance areas. Using telemetry data to determine distances linked to deviations from normal behaviour requires advanced statistical models, such as threshold hidden Markov models (THMMs). While these are powerful tools, they do not assess whether the estimated threshold reflects a meaningful behavioural shift. We introduce a lasso-penalized THMM that builds on computationally efficient methods to impose penalties on HMMs and present a new, efficient penalized quasi-restricted maximum-likelihood estimator. Our framework is capable of estimating thresholds and assessing whether the disturbance effects are meaningful. With simulations, we demonstrate that our lasso method effectively shrinks spurious threshold effects towards zero. When applied to narwhal $\textit{(Monodon monoceros)}$ movement data, our analysis suggests that narwhal react to vessels up to 4 kilometres away by decreasing movement persistence and spending more time in deeper waters (average maximum depth of 356m). Overall, we provide a broadly applicable framework for quantifying behavioural responses to stimuli, with applications ranging from determining reaction thresholds to disturbance to estimating the distances at which terrestrial species, such as elephants, detect water.

</details>


### [2] [Bayesian network approach to building an affective module for a driver behavioural model](https://arxiv.org/abs/2602.09632)
*Dorota Młynarczyk,Gabriel Calvo,Francisco Palmi-Perales,Carmen Armero,Virgilio Gómez-Rubio,Ana de la Torre-García,Ricardo Bayona Salvador*

Main category: stat.AP

TL;DR: 使用贝叶斯网络建模驾驶员心理状态（心理负荷和主动疲劳）对驾驶行为的影响，通过生理和人口统计变量评估心理状态概率


<details>
  <summary>Details</summary>
Motivation: 驾驶员心理状态（如心理负荷和主动疲劳）会影响驾驶表现，需要建立模型来理解这些因素对驾驶行为的影响，以提高交通安全和自动驾驶技术

Method: 使用贝叶斯网络探索相关随机变量之间的依赖关系，基于驾驶员的生理和人口统计条件评估其处于特定心理状态的概率

Result: 建立了驾驶员行为模型的情感组件，能够评估驾驶员心理状态概率，为理解动态环境中的驾驶行为提供了框架

Conclusion: 贝叶斯网络方法可以有效建模驾驶员心理状态，该模型对提高交通安全和自动驾驶技术有潜在应用价值

Abstract: This paper focuses on the affective component of a driver behavioural model (DBM). This component specifically models some drivers' mental states such as mental load and active fatigue, which may affect driving performance. We have used Bayesian networks (BNs) to explore the dependencies between various relevant random variables and assess the probability that a driver is in a particular mental state based on their physiological and demographic conditions. Through this approach, our goal is to improve our understanding of driver behaviour in dynamic environments, with potential applications in traffic safety and autonomous vehicle technologies.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [3] [A Small-Scale System for Autoregressive Program Synthesis Enabling Controlled Experimentation](https://arxiv.org/abs/2602.09112)
*Russ Webb,Jason Ramapuram*

Main category: cs.AI

TL;DR: Cadmus系统：用小模型完成真实程序的研究平台，成本低于200美元，用于研究程序补全、分布外表示、归纳推理和指令跟随，提供对训练分布和模型检查的细粒度控制。


<details>
  <summary>Details</summary>
Motivation: 当前程序合成研究主要依赖大语言模型，存在分布内外识别困难、微调效果不明、分词影响不清、计算存储成本高等问题。需要一个小型、可控、透明的系统来研究程序完成和相关推理任务。

Method: 开发Cadmus系统，包含整数虚拟机、多样化真实程序数据集，以及成本低于200美元的自回归Transformer模型。系统提供对训练分布的细粒度控制和模型检查能力。

Result: Cadmus模型在完成整数算术程序任务上达到100%准确率，优于GPT-5的95%。同时揭示GPT-5在解决相同任务时引入了未知先验，这在大模型研究中可能成为混淆因素。

Conclusion: 小型模型在复杂推理任务上具有研究价值，提供透明度和可控性，适合研究程序完成、分布外表示、归纳推理等课题，而大模型中的未知先验可能影响某些需要完全理解训练集与任务关系的研究。

Abstract: What research can be pursued with small models trained to complete true programs? Typically, researchers study program synthesis via large language models (LLMs) which introduce issues such as knowing what is in or out of distribution, understanding fine-tuning effects, understanding the effects of tokenization, and higher demand on compute and storage to carry out experiments. We present a system called Cadmus which includes an integer virtual machine (VM), a dataset composed of true programs of diverse tasks, and an autoregressive transformer model that is trained for under \$200 of compute cost. The system can be used to study program completion, out-of-distribution representations, inductive reasoning, and instruction following in a setting where researchers have effective and affordable fine-grained control of the training distribution and the ability to inspect and instrument models. Smaller models working on complex reasoning tasks enable instrumentation and investigations that may be prohibitively expensive on larger models. To demonstrate that these tasks are complex enough to be of interest, we show that these Cadmus models outperform GPT-5 (by achieving 100\% accuracy while GPT-5 has 95\% accuracy) even on a simple task of completing correct, integer arithmetic programs in our domain-specific language (DSL) while providing transparency into the dataset's relationship to the problem. We also show that GPT-5 brings unknown priors into its reasoning process when solving the same tasks, demonstrating a confounding factor that prevents the use of large-scale LLMs for some investigations where the training set relationship to the task needs to be fully understood.

</details>


### [4] [Uncertainty-Aware Multimodal Emotion Recognition through Dirichlet Parameterization](https://arxiv.org/abs/2602.09121)
*Rémi Grzeczkowicz,Eric Soriano,Ali Janati,Miyu Zhang,Gerard Comas-Quiles,Victor Carballo Araruna,Aneesh Jonelagadda*

Main category: cs.AI

TL;DR: 提出轻量级隐私保护的多模态情感识别框架，适用于边缘设备部署，使用语音、文本和面部图像三种模态，引入基于Dempster-Shafer理论和Dirichlet证据的模型与任务无关融合机制，在五个基准数据集上验证了竞争性准确率和计算效率。


<details>
  <summary>Details</summary>
Motivation: 开发适用于边缘设备的轻量级隐私保护多模态情感识别框架，解决现实部署中的计算效率、隐私保护和不确定性处理问题，为医疗保健、人机交互等情感感知应用提供可行解决方案。

Method: 使用三种模态：语音（Emotion2Vec）、面部表情（ResNet-based）、文本（DistilRoBERTa），每个模态有专用优化骨干网络。引入基于Dempster-Shafer理论和Dirichlet证据的模型与任务无关融合机制，直接在模型logits上操作，无需额外训练或联合分布估计。

Result: 在五个基准数据集（eNTERFACE05, MEAD, MELD, RAVDESS, CREMA-D）上验证，方法达到竞争性准确率，同时保持计算效率，对模糊或缺失输入具有鲁棒性。

Conclusion: 提出的框架强调模块化、可扩展性和现实可行性，为医疗保健、人机交互等情感感知应用的不确定性感知多模态系统铺平道路，融合机制具有超越情感识别的广泛适用性。

Abstract: In this work, we present a lightweight and privacy-preserving Multimodal Emotion Recognition (MER) framework designed for deployment on edge devices. To demonstrate framework's versatility, our implementation uses three modalities - speech, text and facial imagery. However, the system is fully modular, and can be extended to support other modalities or tasks. Each modality is processed through a dedicated backbone optimized for inference efficiency: Emotion2Vec for speech, a ResNet-based model for facial expressions, and DistilRoBERTa for text. To reconcile uncertainty across modalities, we introduce a model- and task-agnostic fusion mechanism grounded in Dempster-Shafer theory and Dirichlet evidence. Operating directly on model logits, this approach captures predictive uncertainty without requiring additional training or joint distribution estimation, making it broadly applicable beyond emotion recognition. Validation on five benchmark datasets (eNTERFACE05, MEAD, MELD, RAVDESS and CREMA-D) show that our method achieves competitive accuracy while remaining computationally efficient and robust to ambiguous or missing inputs. Overall, the proposed framework emphasizes modularity, scalability, and real-world feasibility, paving the way toward uncertainty-aware multimodal systems for healthcare, human-computer interaction, and other emotion-informed applications.

</details>


### [5] [PABU: Progress-Aware Belief Update for Efficient LLM Agents](https://arxiv.org/abs/2602.09138)
*Haitao Jiang,Lin Ge,Hengrui Cai,Rui Song*

Main category: cs.AI

TL;DR: PABU是一种基于任务进度的信念状态框架，通过显式建模任务进度和选择性保留历史信息，减少LLM智能体中的冗余动作和推理成本。


<details>
  <summary>Details</summary>
Motivation: LLM智能体通常基于完整的动作-观察历史来决策，这会引入任务无关信息，导致冗余动作和更高的推理成本。需要一种更紧凑的状态表示方法来提高效率。

Method: 提出Progress-Aware Belief Update (PABU)框架：1) 显式建模任务进度，预测相对进展；2) 选择性保留历史交互信息；3) 仅基于保留的子集进行未来决策。

Result: 在AgentGym基准测试的8个环境中，PABU实现了81.0%的任务完成率，比基于完整历史的SOTA模型高出23.9%。平均交互步骤减少到9.5步，降低了26.9%。消融研究表明进度预测和选择性保留都是必要的。

Conclusion: PABU通过显式建模任务进度和选择性信息保留，有效减少了LLM智能体中的冗余动作和推理成本，显著提高了任务完成率和效率。

Abstract: Large Language Model (LLM) agents commonly condition actions on full action-observation histories, which introduce task-irrelevant information that easily leads to redundant actions and higher inference cost. We propose Progress-Aware Belief Update (PABU), a belief-state framework that compactly represents an agent's state by explicitly modeling task progress and selectively retaining past actions and observations. At each step, the agent predicts its relative progress since the previous round and decides whether the newly encountered interaction should be stored, conditioning future decisions only on the retained subset. Across eight environments in the AgentGym benchmark, and using identical training trajectories, PABU achieves an 81.0% task completion rate, outperforming previous State of the art (SoTA) models with full-history belief by 23.9%. Additionally, PABU's progress-oriented action selection improves efficiency, reducing the average number of interaction steps to 9.5, corresponding to a 26.9% reduction. Ablation studies show that both explicit progress prediction and selective retention are necessary for robust belief learning and performance gains.

</details>


### [6] [CoMMa: Contribution-Aware Medical Multi-Agents From A Game-Theoretic Perspective](https://arxiv.org/abs/2602.09159)
*Yichen Wu,Yujin Oh,Sangjoon Park,Kailong Fan,Dania Daye,Hana Farzaneh,Xiang Li,Raul Uppot,Quanzheng Li*

Main category: cs.AI

TL;DR: CoMMa是一个去中心化的医疗多智能体框架，通过博弈论目标协调专科医生处理分区证据，使用确定性嵌入投影进行贡献感知信用分配，在肿瘤学决策支持任务中实现更高准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体框架在处理需要动态、异构患者数据推理的肿瘤学决策支持任务时，大多依赖基于随机叙事的推理方法，缺乏明确的证据归因和数学基础，导致决策路径不够稳定和可解释。

Method: 提出贡献感知医疗多智能体(CoMMa)框架：1) 专科医生在分区证据上操作；2) 通过博弈论目标进行协调；3) 使用确定性嵌入投影近似贡献感知信用分配；4) 通过估计每个智能体的边际效用实现明确的证据归因。

Result: 在多种肿瘤学基准测试（包括真实世界多学科肿瘤委员会数据集）中，CoMMa比数据集中化和基于角色的多智能体基线方法实现了更高的准确性和更稳定的性能。

Conclusion: CoMMa通过贡献感知信用分配机制，为医疗决策支持提供了可解释、数学基础扎实且稳定的决策路径，在肿瘤学多智能体决策任务中表现出优越性能。

Abstract: Recent multi-agent frameworks have broadened the ability to tackle oncology decision support tasks that require reasoning over dynamic, heterogeneous patient data. We propose Contribution-Aware Medical Multi-Agents (CoMMa), a decentralized LLM-agent framework in which specialists operate on partitioned evidence and coordinate through a game-theoretic objective for robust decision-making. In contrast to most agent architectures relying on stochastic narrative-based reasoning, CoMMa utilizes deterministic embedding projections to approximate contribution-aware credit assignment. This yields explicit evidence attribution by estimating each agent's marginal utility, producing interpretable and mathematically grounded decision pathways with improved stability. Evaluated on diverse oncology benchmarks, including a real-world multidisciplinary tumor board dataset, CoMMa achieves higher accuracy and more stable performance than data-centralized and role-based multi-agents baselines.

</details>


### [7] [Human Control Is the Anchor, Not the Answer: Early Divergence of Oversight in Agentic AI Communities](https://arxiv.org/abs/2602.09286)
*Hanjing Shi,Dominic DiFranzo*

Main category: cs.AI

TL;DR: 论文通过分析Reddit上两个AI代理相关社区(r/OpenClaw和r/Moltbook)，发现不同社会技术角色对"人类控制"的理解存在显著差异，提出了基于角色匹配的监督机制设计框架。


<details>
  <summary>Details</summary>
Motivation: 当前关于AI代理监督的讨论往往将"人类控制"视为单一目标，但早期采用可能产生角色特定的期望。需要理解不同社会技术角色如何形成不同的监督期望。

Method: 比较分析2026年1-2月两个活跃的Reddit社区：r/OpenClaw（部署和运营）和r/Moltbook（代理中心社交互动）。使用主题建模、共享比较空间、粗粒度监督主题抽象、参与度加权显著性和分歧测试等方法。

Result: 两个社区在监督期望上强烈可分（JSD=0.418，余弦相似度=0.372，置换检验p=0.0005）。虽然"人类控制"都是锚定术语，但其操作含义不同：r/OpenClaw强调执行护栏和恢复（行动风险），r/Moltbook强调身份、合法性和公共互动中的问责（意义风险）。

Conclusion: 研究结果表明需要根据AI代理的具体角色设计匹配的监督机制，而不是采用一刀切的控制策略。这种区分提供了一个可移植的视角，用于设计和评估与代理角色相匹配的监督机制。

Abstract: Oversight for agentic AI is often discussed as a single goal ("human control"), yet early adoption may produce role-specific expectations. We present a comparative analysis of two newly active Reddit communities in Jan--Feb 2026 that reflect different socio-technical roles: r/OpenClaw (deployment and operations) and r/Moltbook (agent-centered social interaction). We conceptualize this period as an early-stage crystallization phase, where oversight expectations form before norms reach equilibrium.
  Using topic modeling in a shared comparison space, a coarse-grained oversight-theme abstraction, engagement-weighted salience, and divergence tests, we show the communities are strongly separable (JSD =0.418, cosine =0.372, permutation $p=0.0005$). Across both communities, "human control" is an anchor term, but its operational meaning diverges: r/OpenClaw} emphasizes execution guardrails and recovery (action-risk), while r/Moltbook} emphasizes identity, legitimacy, and accountability in public interaction (meaning-risk). The resulting distinction offers a portable lens for designing and evaluating oversight mechanisms that match agent role, rather than applying one-size-fits-all control policies.

</details>


### [8] [FlyAOC: Evaluating Agentic Ontology Curation of Drosophila Scientific Knowledge Bases](https://arxiv.org/abs/2602.09163)
*Xingjian Zhang,Sophia Moylan,Ziyang Xiong,Qiaozhu Mei,Yichen Luo,Jiaqi W. Ma*

Main category: cs.AI

TL;DR: FlyBench是一个评估AI智能体从科学文献中进行端到端本体论策展的基准，要求智能体基于基因符号搜索和阅读16,898篇全文论文，生成结构化注释，包括基因本体论术语、表达模式和历史同义词。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注命名实体识别或关系提取等孤立子任务，无法捕捉科学知识库维护所需的端到端策展工作流程，需要评估AI智能体在实际科学文献策展中的综合能力。

Method: 构建包含7,397个专家策展注释的基准，涵盖100个基因，从FlyBase（果蝇知识库）提取。评估四种基线智能体架构：记忆化、固定流水线、单智能体和多智能体，要求智能体从16,898篇全文论文中搜索和阅读信息。

Result: 架构选择显著影响性能，多智能体设计优于简单替代方案，但扩展骨干模型带来的收益递减。所有基线仍有很大改进空间，分析发现智能体主要使用检索来确认参数知识而非发现新信息。

Conclusion: FlyBench将推动检索增强科学推理能力的发展，这种能力在科学领域具有广泛应用前景，为未来AI智能体在科学文献策展方面的研究提供了重要指导。

Abstract: Scientific knowledge bases accelerate discovery by curating findings from primary literature into structured, queryable formats for both human researchers and emerging AI systems. Maintaining these resources requires expert curators to search relevant papers, reconcile evidence across documents, and produce ontology-grounded annotations - a workflow that existing benchmarks, focused on isolated subtasks like named entity recognition or relation extraction, do not capture. We present FlyBench to evaluate AI agents on end-to-end agentic ontology curation from scientific literature. Given only a gene symbol, agents must search and read from a corpus of 16,898 full-text papers to produce structured annotations: Gene Ontology terms describing function, expression patterns, and historical synonyms linking decades of nomenclature. The benchmark includes 7,397 expert-curated annotations across 100 genes drawn from FlyBase, the Drosophila (fruit fly) knowledge base. We evaluate four baseline agent architectures: memorization, fixed pipeline, single-agent, and multi-agent. We find that architectural choices significantly impact performance, with multi-agent designs outperforming simpler alternatives, yet scaling backbone models yields diminishing returns. All baselines leave substantial room for improvement. Our analysis surfaces several findings to guide future development; for example, agents primarily use retrieval to confirm parametric knowledge rather than discover new information. We hope FlyBench will drive progress on retrieval-augmented scientific reasoning, a capability with broad applications across scientific domains.

</details>


### [9] [Measuring Dataset Diversity from a Geometric Perspective](https://arxiv.org/abs/2602.09340)
*Yang Ba,Mohammad Sadeq Abolhasani,Michelle V Mancenido,Rong Pan*

Main category: cs.AI

TL;DR: 提出基于拓扑数据分析(TDA)和持久性景观(PLs)的几何多样性度量框架PLDiv，超越传统熵度量，捕捉数据集几何结构


<details>
  <summary>Details</summary>
Motivation: 现有多样性度量主要关注统计变异和分布熵，忽略了数据集的几何结构，需要一种能捕捉几何特征的理论基础方法

Method: 基于拓扑数据分析和持久性景观框架，从数据中提取和量化几何特征，提出PLDiv度量

Result: PLDiv在多种模态实验中表现出强大、可靠和可解释性，直接关联数据多样性与其底层几何结构

Conclusion: PLDiv为数据集构建、增强和评估提供了基础工具，实现了超越熵的几何多样性度量

Abstract: Diversity can be broadly defined as the presence of meaningful variation across elements, which can be viewed from multiple perspectives, including statistical variation and geometric structural richness in the dataset. Existing diversity metrics, such as feature-space dispersion and metric-space magnitude, primarily capture distributional variation or entropy, while largely neglecting the geometric structure of datasets. To address this gap, we introduce a framework based on topological data analysis (TDA) and persistence landscapes (PLs) to extract and quantify geometric features from data. This approach provides a theoretically grounded means of measuring diversity beyond entropy, capturing the rich geometric and structural properties of datasets. Through extensive experiments across diverse modalities, we demonstrate that our proposed PLs-based diversity metric (PLDiv) is powerful, reliable, and interpretable, directly linking data diversity to its underlying geometry and offering a foundational tool for dataset construction, augmentation, and evaluation.

</details>


### [10] [Auditing Multi-Agent LLM Reasoning Trees Outperforms Majority Vote and LLM-as-Judge](https://arxiv.org/abs/2602.09341)
*Wei Yang,Shixuan Li,Heng Ping,Peiyu Zhang,Paul Bogdan,Jesse Thomason*

Main category: cs.AI

TL;DR: AgentAuditor：用推理树路径搜索替代多数投票的多智能体系统框架，通过局部验证解决智能体间的冲突，结合ACPO训练提升少数正确选择的识别能力


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统大多使用多数投票聚合智能体输出，这种方法丢弃了推理轨迹的证据结构，且在智能体存在相关偏见并收敛到相同错误推理时（共识幻觉）表现脆弱

Method: 1. 引入AgentAuditor框架，用推理树路径搜索替代投票，显式表示智能体轨迹间的共识与分歧；2. 在关键分歧点比较推理分支，将全局裁决转化为局部验证；3. 提出反共识偏好优化（ACPO），在多数失败案例上训练裁决器，奖励基于证据的少数选择而非流行错误

Result: 在5个流行设置中，AgentAuditor相比多数投票带来高达5%的绝对准确率提升，相比使用LLM-as-Judge提升高达3%

Conclusion: AgentAuditor提供了一种与多智能体设置无关的框架，通过显式建模推理分歧和局部验证，有效解决了共识幻觉问题，显著提升了多智能体系统的推理准确性

Abstract: Multi-agent systems (MAS) can substantially extend the reasoning capacity of large language models (LLMs), yet most frameworks still aggregate agent outputs with majority voting. This heuristic discards the evidential structure of reasoning traces and is brittle under the confabulation consensus, where agents share correlated biases and converge on the same incorrect rationale. We introduce AgentAuditor, which replaces voting with a path search over a Reasoning Tree that explicitly represents agreements and divergences among agent traces. AgentAuditor resolves conflicts by comparing reasoning branches at critical divergence points, turning global adjudication into efficient, localized verification. We further propose Anti-Consensus Preference Optimization (ACPO), which trains the adjudicator on majority-failure cases and rewards evidence-based minority selections over popular errors. AgentAuditor is agnostic to MAS setting, and we find across 5 popular settings that it yields up to 5% absolute accuracy improvement over a majority vote, and up to 3% over using LLM-as-Judge.

</details>


### [11] [Not-in-Perspective: Towards Shielding Google's Perspective API Against Adversarial Negation Attacks](https://arxiv.org/abs/2602.09343)
*Michail S. Alexiou,J. Sukarno Mertoguno*

Main category: cs.AI

TL;DR: 提出结合形式推理与机器学习的混合方法，提升社交媒体毒性检测系统对抗否定攻击的能力


<details>
  <summary>Details</summary>
Motivation: 社交媒体中网络欺凌和有毒评论日益严重，现有基于统计的机器学习毒性检测系统容易受到包含否定逻辑的对抗攻击

Method: 开发形式推理包装器，作为预处理和后处理步骤，包裹现有机器学习毒性检测系统，专门处理否定攻击问题

Result: 在否定对抗数据集上测试，混合方法相比纯统计解决方案显著提高了毒性评分的准确性和有效性

Conclusion: 形式推理与机器学习结合的混合方法能有效缓解否定攻击问题，提升毒性检测系统的鲁棒性

Abstract: The rise of cyberbullying in social media platforms involving toxic comments has escalated the need for effective ways to monitor and moderate online interactions. Existing solutions of automated toxicity detection systems, are based on a machine or deep learning algorithms. However, statistics-based solutions are generally prone to adversarial attacks that contain logic based modifications such as negation in phrases and sentences. In that regard, we present a set of formal reasoning-based methodologies that wrap around existing machine learning toxicity detection systems. Acting as both pre-processing and post-processing steps, our formal reasoning wrapper helps alleviating the negation attack problems and significantly improves the accuracy and efficacy of toxicity scoring. We evaluate different variations of our wrapper on multiple machine learning models against a negation adversarial dataset. Experimental results highlight the improvement of hybrid (formal reasoning and machine-learning) methods against various purely statistical solutions.

</details>


### [12] [Image Quality in the Era of Artificial Intelligence](https://arxiv.org/abs/2602.09347)
*Jana G. Delfino,Jason L. Granstedt,Frank W. Samuelson,Robert Ochs,Krishna Juluru*

Main category: cs.AI

TL;DR: 本文探讨AI在放射学图像重建与增强中的局限性，旨在帮助用户安全有效地使用该技术


<details>
  <summary>Details</summary>
Motivation: AI在放射学中快速部署，虽然能提高图像质量和处理速度，但也引入了新的失败模式，并加剧了图像感知质量与信息内容之间的脱节。理解AI图像重建和增强的局限性对于安全有效使用该技术至关重要。

Method: 本文是一篇通信/综述文章，通过分析AI在放射学图像处理中的应用现状，提出对AI图像重建和增强技术局限性的系统性认识。

Result: 文章指出AI虽然能产生更清晰、更平滑、更详细的图像，并加快获取和审查速度，但同时也带来了新的失败风险，可能导致图像感知质量与实际信息内容之间的不匹配。

Conclusion: 需要提高对AI图像重建和增强技术局限性的认识，使用户能够在享受技术益处的同时最小化风险，确保安全有效地应用AI于放射学实践。

Abstract: Artificial intelligence (AI) is being deployed within radiology at a rapid pace. AI has proven an excellent tool for reconstructing and enhancing images that appear sharper, smoother, and more detailed, can be acquired more quickly, and allowing clinicians to review them more rapidly. However, incorporation of AI also introduces new failure modes and can exacerbate the disconnect between perceived quality of an image and information content of that image. Understanding the limitations of AI-enabled image reconstruction and enhancement is critical for safe and effective use of the technology. Hence, the purpose of this communication is to bring awareness to limitations when AI is used to reconstruct or enhance a radiological image, with the goal of enabling users to reap benefits of the technology while minimizing risks.

</details>


### [13] [P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads](https://arxiv.org/abs/2602.09443)
*Yun Luo,Futing Wang,Qianjia Cheng,Fangchen Yu,Haodi Lei,Jianhao Yan,Chenxi Li,Jiacheng Chen,Yufeng Zhao,Haiyuan Wan,Yuchen Zhang,Shenghe Zheng,Junchi Yao,Qingyang Zhang,Haonan He,Wenxuan Zeng,Li Sheng,Chengxing Xie,Yuxin Zuo,Yizhuo Li,Yulun Wu,Rui Huang,Dongzhan Zhou,Kai Chen,Yu Qiao,Lei Bai,Yu Cheng,Ning Ding,Bowen Zhou,Peng Ye,Ganqu Cui*

Main category: cs.AI

TL;DR: P1-VL是一个开源视觉语言模型家族，专为高级科学推理设计，在物理奥林匹克竞赛基准测试中达到开源模型最佳性能，全球排名第二，仅次于Gemini-3-Pro。


<details>
  <summary>Details</summary>
Motivation: 从符号操作到科学级推理是大型语言模型的关键前沿，物理作为连接抽象逻辑与物理现实的测试锚点。物理要求模型保持与宇宙定律的一致性，这需要多模态感知将抽象逻辑与现实联系起来。在奥林匹克级别，图表通常是构成性的而非说明性的，包含文本中缺失的关键约束条件。

Method: 引入P1-VL开源视觉语言模型家族，结合课程强化学习（采用渐进难度扩展稳定后训练）和代理增强（在推理时实现迭代自我验证）。

Result: 在HiPhO基准测试（13个2024-2025年考试）中，旗舰模型P1-VL-235B-A22B成为首个获得12枚金牌的开源VLM，在开源模型中达到最先进性能。代理增强系统全球排名第二，仅次于Gemini-3-Pro。在STEM基准测试中显著优于基础模型。

Conclusion: 通过开源P1-VL，为实现通用物理智能提供了基础性步骤，更好地将视觉感知与抽象物理定律对齐，促进机器科学发现。模型展示了卓越的科学推理能力和泛化性。

Abstract: The transition from symbolic manipulation to science-grade reasoning represents a pivotal frontier for Large Language Models (LLMs), with physics serving as the critical test anchor for binding abstract logic to physical reality. Physics demands that a model maintain physical consistency with the laws governing the universe, a task that fundamentally requires multimodal perception to ground abstract logic in reality. At the Olympiad level, diagrams are often constitutive rather than illustrative, containing essential constraints, such as boundary conditions and spatial symmetries, that are absent from the text. To bridge this visual-logical gap, we introduce P1-VL, a family of open-source vision-language models engineered for advanced scientific reasoning. Our method harmonizes Curriculum Reinforcement Learning, which employs progressive difficulty expansion to stabilize post-training, with Agentic Augmentation, enabling iterative self-verification at inference. Evaluated on HiPhO, a rigorous benchmark of 13 exams from 2024-2025, our flagship P1-VL-235B-A22B becomes the first open-source Vision-Language Model (VLM) to secure 12 gold medals and achieves the state-of-the-art performance in the open-source models. Our agent-augmented system achieves the No.2 overall rank globally, trailing only Gemini-3-Pro. Beyond physics, P1-VL demonstrates remarkable scientific reasoning capacity and generalizability, establishing significant leads over base models in STEM benchmarks. By open-sourcing P1-VL, we provide a foundational step toward general-purpose physical intelligence to better align visual perceptions with abstract physical laws for machine scientific discovery.

</details>


### [14] [SpotAgent: Grounding Visual Geo-localization in Large Vision-Language Models through Agentic Reasoning](https://arxiv.org/abs/2602.09463)
*Furong Jia,Ling Dai,Wenjin Deng,Fan Zhang,Chen Hu,Daxin Jiang,Yu Liu*

Main category: cs.AI

TL;DR: SpotAgent是一个用于地理定位的智能体框架，通过结合视觉解释和工具辅助验证来解决稀疏、长尾和模糊视觉线索的挑战，实现可验证的精准定位。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型在地理定位中面临视觉线索稀疏、长尾分布和高度模糊的问题，且受限于内部知识，经常产生自信但无根据的预测，缺乏可验证性。

Method: 提出SpotAgent框架，将地理定位形式化为智能体推理过程，通过ReAct图利用外部工具（如网络搜索、地图）进行主动探索和验证。采用三阶段后训练流程：监督微调对齐基础能力、多智能体框架合成高质量轨迹的智能体冷启动阶段、以及通过空间感知动态过滤策略优化的强化学习阶段。

Result: 在标准基准测试中，SpotAgent实现了最先进的性能，有效减少幻觉，提供精确且可验证的地理定位结果。

Conclusion: SpotAgent通过智能体推理框架结合视觉解释和工具辅助验证，成功解决了地理定位中的稀疏视觉线索和可验证性问题，为实际应用提供了可靠解决方案。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated strong reasoning capabilities in geo-localization, yet they often struggle in real-world scenarios where visual cues are sparse, long-tailed, and highly ambiguous. Previous approaches, bound by internal knowledge, often fail to provide verifiable results, yielding confident but ungrounded predictions when faced with confounded evidence. To address these challenges, we propose SpotAgent, a framework that formalizes geo-localization into an agentic reasoning process that leverages expert-level reasoning to synergize visual interpretation with tool-assisted verification. SpotAgent actively explores and verifies visual cues by leveraging external tools (e.g., web search, maps) through a ReAct diagram. We introduce a 3-stage post-training pipeline starting with a Supervised Fine-Tuning (SFT) stage for basic alignment, followed by an Agentic Cold Start phase utilizing high-quality trajectories synthesized via a Multi-Agent framework, aiming to instill tool-calling expertise. Subsequently, the model's reasoning capabilities are refined through Reinforcement Learning. We propose a Spatially-Aware Dynamic Filtering strategy to enhance the efficiency of the RL stage by prioritizing learnable samples based on spatial difficulty. Extensive experiments on standard benchmarks demonstrate that SpotAgent achieves state-of-the-art performance, effectively mitigating hallucinations while delivering precise and verifiable geo-localization.

</details>


### [15] [Bridging Efficiency and Transparency: Explainable CoT Compression in Multimodal Large Reasoning Models](https://arxiv.org/abs/2602.09485)
*Yizhi Wang,Linan Yue,Min-Ling Zhang*

Main category: cs.AI

TL;DR: XMCC：一种可解释的多模态思维链压缩器，通过强化学习将压缩建模为序列决策过程，在缩短推理轨迹的同时保持关键推理步骤和答案正确性，并生成自然语言解释。


<details>
  <summary>Details</summary>
Motivation: 长思维链在多模态推理中广泛使用，但往往过于冗长且包含冗余推理步骤，影响推理效率。现有压缩方法存在两个主要问题：1) 可能破坏视觉-文本推理的完整性，移除关键对齐线索；2) 压缩过程缺乏可解释性，难以识别哪些信息是关键的。

Method: 提出XMCC（可解释多模态思维链压缩器），将压缩建模为序列决策过程，通过强化学习进行优化。该方法能够有效缩短推理轨迹，同时保留关键推理步骤和答案正确性，并为压缩决策生成自然语言解释。

Result: 在代表性多模态推理基准上的大量实验表明，XMCC不仅减少了推理长度，还提供了可解释的说明，验证了其有效性。

Conclusion: XMCC解决了长思维链压缩中的两个关键挑战：保持推理完整性和提供可解释性，为高效且透明的多模态推理提供了有效解决方案。

Abstract: Long chains of thought (Long CoTs) are widely employed in multimodal reasoning models to tackle complex tasks by capturing detailed visual information. However, these Long CoTs are often excessively lengthy and contain redundant reasoning steps, which can hinder inference efficiency. Compressing these long CoTs is a natural solution, yet existing approaches face two major challenges: (1) they may compromise the integrity of visual-textual reasoning by removing essential alignment cues, and (2) the compression process lacks explainability, making it difficult to discern which information is critical. To address these problems, we propose XMCC, an eXplainable Multimodal CoT Compressor that formulates compression as a sequential decision-making process optimized via reinforcement learning. XMCC can effectively shorten reasoning trajectories while preserving key reasoning steps and answer correctness, and simultaneously generates natural-language explanations for its compression decisions. Extensive experiments on representative multimodal reasoning benchmarks demonstrate that XMCC not only reduces reasoning length but also provides explainable explanations, validating its effectiveness.

</details>


### [16] [Computing Conditional Shapley Values Using Tabular Foundation Models](https://arxiv.org/abs/2602.09489)
*Lars Henry Berge Olsen,Dennis Christensen*

Main category: cs.AI

TL;DR: TabPFN等表格基础模型通过上下文学习高效计算Shapley值，无需重新训练，在大多数情况下性能优于现有方法，且计算时间大幅减少。


<details>
  <summary>Details</summary>
Motivation: Shapley值是解释性AI的核心工具，但计算成本高昂，尤其在特征相关时。传统方法需要大量条件期望的近似计算，要么通过蒙特卡洛积分，要么通过回归。深度学习方法因需要为每个条件期望重新训练而计算耗时过长。

Method: 使用TabPFN等多种表格基础模型变体计算Shapley值，利用其上下文学习能力，无需重新训练即可近似每个条件期望。在模拟和真实数据集上与最先进方法进行性能比较。

Result: 在大多数情况下，TabPFN表现最佳；即使在不占优的情况下，也仅略逊于最佳方法，但运行时间仅为其他方法的几分之一。

Conclusion: 表格基础模型为Shapley值计算提供了高效解决方案，未来可通过专门适配进一步提升条件Shapley值估计的性能。

Abstract: Shapley values have become a cornerstone of explainable AI, but they are computationally expensive to use, especially when features are dependent. Evaluating them requires approximating a large number of conditional expectations, either via Monte Carlo integration or regression. Until recently it has not been possible to fully exploit deep learning for the regression approach, because retraining for each conditional expectation takes too long. Tabular foundation models such as TabPFN overcome this computational hurdle by leveraging in-context learning, so each conditional expectation can be approximated without any re-training. In this paper, we compute Shapley values with multiple variants of TabPFN and compare their performance with state-of-the-art methods on both simulated and real datasets. In most cases, TabPFN yields the best performance; where it does not, it is only marginally worse than the best method, at a fraction of the runtime. We discuss further improvements and how tabular foundation models can be better adapted specifically for conditional Shapley value estimation.

</details>


### [17] [Autoregressive Direct Preference Optimization](https://arxiv.org/abs/2602.09533)
*Masanari Oi,Mahiro Ukai,Masahiro Kaneko,Naoaki Okazaki,Nakamasa Inoue*

Main category: cs.AI

TL;DR: 本文提出了一种新的自回归DPO（ADPO）方法，通过将自回归假设明确引入Bradley-Terry模型之前，改进了传统的直接偏好优化方法。


<details>
  <summary>Details</summary>
Motivation: 现有DPO方法广泛依赖响应级的Bradley-Terry模型，但这种方法可能限制了其潜力，因为参考模型和可学习模型仅在推导目标函数后才被假设为自回归的。作者认为这种局限性需要重新审视DPO的理论基础。

Method: 重新审视DPO的理论基础，提出在应用Bradley-Terry模型之前明确引入自回归假设的新公式。通过重新表述和扩展DPO，推导出名为自回归DPO（ADPO）的新变体，将自回归建模明确集成到偏好优化框架中。

Result: 推导出的损失函数具有优雅的形式：将DPO目标中的求和操作移到log-sigmoid函数之外。通过理论分析发现，在设计基于DPO的算法时需要考虑两个长度度量：标记长度μ和反馈长度μ'。

Conclusion: ADPO通过更早地引入自回归假设改进了DPO框架，首次明确区分了标记长度和反馈长度这两个重要度量，为LLMs的偏好优化提供了更完整的理论分析。

Abstract: Direct preference optimization (DPO) has emerged as a promising approach for aligning large language models (LLMs) with human preferences. However, the widespread reliance on the response-level Bradley-Terry (BT) model may limit its full potential, as the reference and learnable models are assumed to be autoregressive only after deriving the objective function. Motivated by this limitation, we revisit the theoretical foundations of DPO and propose a novel formulation that explicitly introduces the autoregressive assumption prior to applying the BT model. By reformulating and extending DPO, we derive a novel variant, termed Autoregressive DPO (ADPO), that explicitly integrates autoregressive modeling into the preference optimization framework. Without violating the theoretical foundations, the derived loss takes an elegant form: it shifts the summation operation in the DPO objective outside the log-sigmoid function. Furthermore, through theoretical analysis of ADPO, we show that there exist two length measures to be considered when designing DPO-based algorithms: the token length $μ$ and the feedback length $μ$'. To the best of our knowledge, we are the first to explicitly distinguish these two measures and analyze their implications for preference optimization in LLMs.

</details>


### [18] [Detecting radar targets swarms in range profiles with a partially complex-valued neural network](https://arxiv.org/abs/2602.09597)
*Martin Bauw*

Main category: cs.AI

TL;DR: 该论文提出使用部分复值神经网络处理雷达距离剖面，解决多目标近距离干扰和回波失真问题，相比传统脉冲压缩方法能一次性处理整个接收信号。


<details>
  <summary>Details</summary>
Motivation: 雷达目标检测面临杂波、波形失真以及多个目标相对接近的挑战。目标接近会相互影响检测阈值，在最坏情况下会被视为单个目标。这种负面影响取决于雷达参数定义的距离分辨率和自适应阈值。

Method: 受雷达和信号处理领域最新研究的启发，提出使用部分复值神经网络作为自适应距离剖面处理方法。采用生成式架构，能够一次性处理整个接收信号，生成完整的检测剖面，而传统脉冲压缩方法一次只能处理一个脉冲长度。

Result: 通过生成模拟数据集进行实验，比较了常见的脉冲压缩方法与简单的部分复值参数神经网络。神经网络能够更有效地处理多目标近距离情况和失真回波。

Conclusion: 部分复值神经网络为雷达距离剖面处理提供了一种有效的自适应方法，特别适用于处理多个目标近距离干扰和回波失真的复杂场景，相比传统脉冲压缩方法具有更好的整体处理能力。

Abstract: Correctly detecting radar targets is usually challenged by clutter and waveform distortion. An additional difficulty stems from the relative proximity of several targets, the latter being perceived as a single target in the worst case, or influencing each other's detection thresholds. The negative impact of targets proximity notably depends on the range resolution defined by the radar parameters and the adaptive threshold adopted. This paper addresses the matter of targets detection in radar range profiles containing multiple targets with varying proximity and distorted echoes. Inspired by recent contributions in the radar and signal processing literature, this work proposes partially complex-valued neural networks as an adaptive range profile processing. Simulated datasets are generated and experiments are conducted to compare a common pulse compression approach with a simple neural network partially defined by complex-valued parameters. Whereas the pulse compression processes one pulse length at a time, the neural network put forward is a generative architecture going through the entire received signal in one go to generate a complete detection profile.

</details>


### [19] [FLINGO -- Instilling ASP Expressiveness into Linear Integer Constraints](https://arxiv.org/abs/2602.09620)
*Jorge Fandinno,Pedro Cabalar,Philipp Wanko,Torsten Schaub*

Main category: cs.AI

TL;DR: FLINGO语言扩展了约束ASP，将ASP的数值属性表达能力（如默认值、未定义、非确定性赋值和聚合值）整合到数值约束中，并提供了到标准CASP的翻译。


<details>
  <summary>Details</summary>
Motivation: 当前约束ASP（CASP）中数值约束的表达方式更接近后端数值处理器的语义，而丢失了ASP中数值属性的丰富表达能力，如默认值、未定义状态、非确定性赋值和聚合值等功能。

Method: 提出FLINGO语言和工具，将ASP的数值属性表达能力整合到数值约束中，并设计从FLINGO语法到标准CASP程序（遵循CLINGCON输入格式）的翻译方法。

Result: 开发了FLINGO语言，能够保持ASP中数值属性的丰富表达能力，同时支持数值约束处理，并通过翻译机制实现与现有CASP求解器的兼容。

Conclusion: FLINGO成功地将ASP的数值表达能力与CASP的约束处理能力相结合，为实际应用提供了更自然和强大的建模语言。

Abstract: Constraint Answer Set Programming (CASP) is a hybrid paradigm that enriches Answer Set Programming (ASP) with numerical constraint processing, something required in many real-world applications. The usual specification of constraints in most CASP solvers is closer to the numerical back-end expressiveness and semantics, rather than to standard specification in ASP. In the latter, numerical attributes are represented with predicates and this allows declaring default values, leaving the attribute undefined, making non-deterministic assignments with choice rules or using aggregated values. In CASP, most (if not all) of these features are lost once we switch to a constraint-based representation of those same attributes. In this paper, we present the FLINGO language (and tool) that incorporates the aforementioned expressiveness inside the numerical constraints and we illustrate its use with several examples. Based on previous work that established its semantic foundations, we also present a translation from the newly introduced FLINGO syntax to regular CASP programs following the CLINGCON input format.

</details>


### [20] [ClinAlign: Scaling Healthcare Alignment from Clinician Preference](https://arxiv.org/abs/2602.09653)
*Shiwei Lyu,Xidong Wang,Lei Liu,Hao Zhu,Chaohe Zhang,Jian Wang,Jinjie Gu,Benyou Wang,Yue Shen*

Main category: cs.AI

TL;DR: 提出两阶段框架解决LLM医疗输出与临床偏好对齐问题：1) 创建医生验证的偏好数据集HealthRubrics；2) 提炼为可重用临床原则HealthPrinciples，用于离线对齐和推理时自我修正，小模型性能超越大模型。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型具备专家级医疗知识，但其开放输出与细粒度临床偏好的对齐仍然困难。现有方法依赖粗粒度目标或不可靠的自动评估，缺乏专业指南基础。

Method: 两阶段框架：1) 构建HealthRubrics数据集（7,034个医生验证的偏好示例），医生优化LLM草拟的评分标准；2) 提炼为HealthPrinciples（119个可重用临床原则），用于离线对齐（为未标注查询合成评分标准）和推理时引导自我修正。

Result: 仅激活30B参数中3B参数的模型在HealthBench-Hard上达到33.4%，超越Deepseek-R1和o3等更大模型，建立了资源高效的临床对齐基准。

Conclusion: 提出的框架通过医生验证的评分标准和可重用临床原则，实现了LLM医疗输出与临床偏好的有效对齐，小模型也能达到超越大模型的性能，为临床对齐提供了资源高效的解决方案。

Abstract: Although large language models (LLMs) demonstrate expert-level medical knowledge, aligning their open-ended outputs with fine-grained clinician preferences remains challenging. Existing methods often rely on coarse objectives or unreliable automated judges that are weakly grounded in professional guidelines. We propose a two-stage framework to address this gap. First, we introduce HealthRubrics, a dataset of 7,034 physician-verified preference examples in which clinicians refine LLM-drafted rubrics to meet rigorous medical standards. Second, we distill these rubrics into HealthPrinciples: 119 broadly reusable, clinically grounded principles organized by clinical dimensions, enabling scalable supervision beyond manual annotation. We use HealthPrinciples for (1) offline alignment by synthesizing rubrics for unlabeled queries and (2) an inference-time tool for guided self-revision. A 30B parameter model that activates only 3B parameters at inference trained with our framework achieves 33.4% on HealthBench-Hard, outperforming much larger models including Deepseek-R1 and o3, establishing a resource-efficient baseline for clinical alignment.

</details>


### [21] [GHS-TDA: A Synergistic Reasoning Framework Integrating Global Hypothesis Space with Topological Data Analysis](https://arxiv.org/abs/2602.09794)
*Jiaquan Zhang,Chaoning Zhang,Shuxu Chen,Xudong Wang,Zhenzhen Huang,Pengcheng Zheng,Shuai Yuan,Sheng Zheng,Qigan Sun,Jie Zou,Lik-Hang Lee,Yang Yang*

Main category: cs.AI

TL;DR: GHS-TDA通过构建全局假设图和多尺度拓扑分析，解决传统思维链方法中错误传播和缺乏结构化分析的问题，提升推理准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有思维链方法存在两个根本性局限：1) 推理过程对早期决策高度敏感，一旦出现初始错误会传播放大且难以修正；2) 缺乏结构化分析技术来过滤冗余推理和提取关键特征，导致推理过程不稳定且可解释性有限。

Method: 提出GHS-TDA方法：1) 构建语义丰富的全局假设图，聚合、对齐和协调多个候选推理路径，提供全局修正路径；2) 应用基于持久同调的拓扑数据分析，捕获稳定的多尺度结构，去除冗余和不一致，提取更可靠的推理骨架。

Result: GHS-TDA通过联合利用推理多样性和拓扑稳定性，实现自适应收敛，产生高置信度和可解释的推理路径，在多个推理基准测试中在准确性和鲁棒性方面均优于强基线方法。

Conclusion: GHS-TDA通过全局协调和拓扑分析有效解决了传统思维链方法的局限性，为大型语言模型的复杂推理任务提供了更可靠和可解释的解决方案。

Abstract: Chain-of-Thought (CoT) has been shown to significantly improve the reasoning accuracy of large language models (LLMs) on complex tasks. However, due to the autoregressive, step-by-step generation paradigm, existing CoT methods suffer from two fundamental limitations. First, the reasoning process is highly sensitive to early decisions: once an initial error is introduced, it tends to propagate and amplify through subsequent steps, while the lack of a global coordination and revision mechanism makes such errors difficult to correct, ultimately leading to distorted reasoning chains. Second, current CoT approaches lack structured analysis techniques for filtering redundant reasoning and extracting key reasoning features, resulting in unstable reasoning processes and limited interpretability. To address these issues, we propose GHS-TDA. GHS-TDA first constructs a semantically enriched global hypothesis graph to aggregate, align, and coordinate multiple candidate reasoning paths, thereby providing alternative global correction routes when local reasoning fails. It then applies topological data analysis based on persistent homology to capture stable multi-scale structures, remove redundancy and inconsistencies, and extract a more reliable reasoning skeleton. By jointly leveraging reasoning diversity and topological stability, GHS-TDA achieves self-adaptive convergence, produces high-confidence and interpretable reasoning paths, and consistently outperforms strong baselines in terms of both accuracy and robustness across multiple reasoning benchmarks.

</details>


### [22] [Symbolic Pattern Temporal Numeric Planning with Intermediate Conditions and Effects](https://arxiv.org/abs/2602.09798)
*Matteo Cardellini,Enrico Giunchiglia*

Main category: cs.AI

TL;DR: 将符号模式规划（SPP）方法扩展到具有中间条件和效果（ICEs）的时间规划领域，开发了Patty规划器，在多个时间规划领域表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的符号模式规划方法主要针对数值规划，需要扩展到更复杂的时间规划领域，特别是支持动作重叠和中间条件/效果的时间规划片段。

Method: 将SPP方法扩展到时间规划ICEs片段：1）支持持续动作（可重叠执行）；2）支持在动作执行期间任意时间检查/应用条件效果；3）支持在计划执行特定时间检查/应用条件效果；4）使用模式扩展机制确保完备性。

Result: 实验结果显示：1）在无ICEs的时间规划领域，Patty优于所有现有规划器；2）在有ICEs的文献领域，与最先进的搜索规划器性能相当；3）在基于真实应用的新领域，优于相同规划器。

Conclusion: 成功将SPP方法扩展到时间规划ICEs片段，开发的Patty规划器在多个时间规划领域表现出竞争力，特别是在真实应用场景中显示出优势。

Abstract: Recently, a Symbolic Pattern Planning (SPP) approach was proposed for numeric planning where a pattern (i.e., a finite sequence of actions) suggests a causal order between actions. The pattern is then encoded in a SMT formula whose models correspond to valid plans. If the suggestion by the pattern is inaccurate and no valid plan can be found, the pattern is extended until it contains the causal order of actions in a valid plan, making the approach complete. In this paper, we extend the SPP approach to the temporal planning with Intermediate Conditions and Effects (ICEs) fragment, where $(i)$ actions are durative (and thus can overlap over time) and have conditions/effects which can be checked/applied at any time during an action's execution, and $(ii)$ one can specify plan's conditions/effects that must be checked/applied at specific times during the plan execution. Experimental results show that our SPP planner Patty $(i)$ outperforms all other planners in the literature in the majority of temporal domains without ICEs, $(ii)$ obtains comparable results with the SoTA search planner for ICS in literature domains with ICEs, and $(iii)$ outperforms the same planner in a novel domain based on a real-world application.

</details>


### [23] [Would a Large Language Model Pay Extra for a View? Inferring Willingness to Pay from Subjective Choices](https://arxiv.org/abs/2602.09802)
*Manon Reusens,Sofie Goethals,Toon Calders,David Martens*

Main category: cs.AI

TL;DR: LLM在主观决策任务中（如旅行助手）能生成有意义的支付意愿估计，但存在系统性偏差，通常高估人类支付意愿，特别是在昂贵选项或商务角色下。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在主观决策任务中的表现，特别是在旅行助手等应用中，LLM需要为用户做出主观选择，而这类选择没有客观正确答案。需要评估LLM的决策质量并与人类基准比较。

Method: 在旅行助手情境中向LLM呈现选择困境，使用多项logit模型分析响应以推导隐含支付意愿估计，然后与经济学文献中的人类基准值比较。研究不同设置：基线、用户历史选择信息、角色提示等。

Result: 较大LLM能生成有意义的WTP值，但在属性层面存在系统性偏差；总体倾向于高估人类WTP，特别是当引入昂贵选项或商务导向角色时；基于先前对便宜选项偏好的条件设定能产生更接近人类基准的估值。

Conclusion: LLM在主观决策支持中既有潜力也有局限，实际部署时需要仔细的模型选择、提示设计和用户表征，以确保决策质量。

Abstract: As Large Language Models (LLMs) are increasingly deployed in applications such as travel assistance and purchasing support, they are often required to make subjective choices on behalf of users in settings where no objectively correct answer exists. We study LLM decision-making in a travel-assistant context by presenting models with choice dilemmas and analyzing their responses using multinomial logit models to derive implied willingness to pay (WTP) estimates. These WTP values are subsequently compared to human benchmark values from the economics literature. In addition to a baseline setting, we examine how model behavior changes under more realistic conditions, including the provision of information about users' past choices and persona-based prompting. Our results show that while meaningful WTP values can be derived for larger LLMs, they also display systematic deviations at the attribute level. Additionally, they tend to overestimate human WTP overall, particularly when expensive options or business-oriented personas are introduced. Conditioning models on prior preferences for cheaper options yields valuations that are closer to human benchmarks. Overall, our findings highlight both the potential and the limitations of using LLMs for subjective decision support and underscore the importance of careful model selection, prompt design, and user representation when deploying such systems in practice.

</details>


### [24] [Efficient Unsupervised Environment Design through Hierarchical Policy Representation Learning](https://arxiv.org/abs/2602.09813)
*Dexun Li,Sidney Tio,Pradeep Varakantham*

Main category: cs.AI

TL;DR: 提出分层MDP框架用于环境设计，通过教师代理利用学生策略表示生成训练环境，结合生成模型减少师生交互需求，在资源受限场景中实现高效课程生成。


<details>
  <summary>Details</summary>
Motivation: 现有无监督环境设计方法依赖随机过程无限生成环境，这在师生交互机会有限的资源受限场景中不切实际，需要更高效的课程生成方法。

Method: 引入分层MDP框架，教师代理利用从评估环境中提取的学生策略表示来生成训练环境；结合生成模型增强教师训练数据集，减少师生交互需求。

Result: 在多个领域实验中，该方法优于基线方法，同时在单次训练中需要更少的师生交互，证明在训练机会有限场景中的适用性。

Conclusion: 提出的分层MDP框架通过高效利用学生能力信息和生成模型增强，在资源受限场景中实现了更有效的无监督环境设计，为有限交互机会下的课程生成提供了可行方案。

Abstract: Unsupervised Environment Design (UED) has emerged as a promising approach to developing general-purpose agents through automated curriculum generation. Popular UED methods focus on Open-Endedness, where teacher algorithms rely on stochastic processes for infinite generation of useful environments. This assumption becomes impractical in resource-constrained scenarios where teacher-student interaction opportunities are limited. To address this challenge, we introduce a hierarchical Markov Decision Process (MDP) framework for environment design. Our framework features a teacher agent that leverages student policy representations derived from discovered evaluation environments, enabling it to generate training environments based on the student's capabilities. To improve efficiency, we incorporate a generative model that augments the teacher's training dataset with synthetic data, reducing the need for teacher-student interactions. In experiments across several domains, we show that our method outperforms baseline approaches while requiring fewer teacher-student interactions in a single episode. The results suggest the applicability of our approach in settings where training opportunities are limited.

</details>


### [25] [Why Do AI Agents Systematically Fail at Cloud Root Cause Analysis?](https://arxiv.org/abs/2602.09937)
*Taeyoon Kim,Woohyeok Park,Hoyeong Yun,Kyungyong Lee*

Main category: cs.AI

TL;DR: 该论文对基于LLM的云系统根因分析(RCA)代理进行了过程级故障分析，识别出12种陷阱类型，发现主要问题源于共享的代理架构而非单个模型限制，提示工程无法解决主导陷阱，但改进通信协议可减少15%的通信相关故障。


<details>
  <summary>Details</summary>
Motivation: 大规模云系统故障导致重大经济损失，需要自动化根因分析(RCA)。现有LLM代理系统检测准确率低，且当前评估框架只关注最终答案正确性，无法揭示代理推理失败的原因。

Method: 对基于LLM的RCA代理进行过程级故障分析：在五个LLM模型上执行完整的OpenRCA基准测试，产生1,675次代理运行，将观察到的故障分类为12种陷阱类型（包括代理内推理、代理间通信和代理-环境交互）。

Result: 分析显示最普遍的陷阱（特别是幻觉数据解释和不完整探索）在所有模型中持续存在，无论模型能力层级如何，表明这些故障源于共享代理架构而非单个模型限制。缓解实验表明提示工程无法解决主导陷阱，但丰富代理间通信协议可将通信相关故障减少最多15个百分点。

Conclusion: 本文开发的陷阱分类和诊断方法为设计更可靠的云RCA自主代理奠定了基础。主要故障源于代理架构设计，而非模型能力限制，需要系统性改进代理架构而非仅依赖提示工程。

Abstract: Failures in large-scale cloud systems incur substantial financial losses, making automated Root Cause Analysis (RCA) essential for operational stability. Recent efforts leverage Large Language Model (LLM) agents to automate this task, yet existing systems exhibit low detection accuracy even with capable models, and current evaluation frameworks assess only final answer correctness without revealing why the agent's reasoning failed. This paper presents a process level failure analysis of LLM-based RCA agents. We execute the full OpenRCA benchmark across five LLM models, producing 1,675 agent runs, and classify observed failures into 12 pitfall types across intra-agent reasoning, inter-agent communication, and agent-environment interaction. Our analysis reveals that the most prevalent pitfalls, notably hallucinated data interpretation and incomplete exploration, persist across all models regardless of capability tier, indicating that these failures originate from the shared agent architecture rather than from individual model limitations. Controlled mitigation experiments further show that prompt engineering alone cannot resolve the dominant pitfalls, whereas enriching the inter-agent communication protocol reduces communication-related failures by up to 15 percentage points. The pitfall taxonomy and diagnostic methodology developed in this work provide a foundation for designing more reliable autonomous agents for cloud RCA.

</details>


### [26] [Closing Reasoning Gaps in Clinical Agents with Differential Reasoning Learning](https://arxiv.org/abs/2602.09945)
*Jinsong Liu,Yuhang Jiang,Ramayya Krishnan,Rema Padman,Yiye Zhang,Jiang Bian*

Main category: cs.AI

TL;DR: DRL框架通过分析推理差异来改进临床决策支持，使用图编辑距离比较参考推理与模型推理，构建差异知识库并通过RAG检索指令来修补推理漏洞。


<details>
  <summary>Details</summary>
Motivation: 临床决策支持不仅需要正确答案，还需要临床有效的推理过程。现有方法在复杂推理场景中可能存在逻辑漏洞，需要一种机制来识别和修补这些推理差异。

Method: 提出差异推理学习(DRL)框架：1) 将参考推理（医生临床推理、临床指南或更强模型输出）和代理的自由形式链式推理转换为有向无环图；2) 使用临床加权的图编辑距离进行差异分析；3) LLM作为裁判对齐语义等价节点；4) 构建差异推理知识库(DR-KB)；5) 推理时通过RAG检索top-k指令增强提示。

Result: 在开放医学QA基准和内部临床数据的返院预测任务上优于基线方法，提高了最终答案准确性和推理保真度。消融研究证实参考推理注入和top-k检索策略的有效性，临床医生评审进一步验证了方法的可靠性。

Conclusion: DRL支持在复杂推理场景中进行更可靠的临床决策，为有限token预算下的部署提供了实用机制，能够改善临床推理的准确性和可信度。

Abstract: Clinical decision support requires not only correct answers but also clinically valid reasoning. We propose Differential Reasoning Learning (DRL), a framework that improves clinical agents by learning from reasoning discrepancies. From reference reasoning rationales (e.g., physician-authored clinical rationale, clinical guidelines, or outputs from more capable models) and the agent's free-form chain-of-thought (CoT), DRL extracts reasoning graphs as directed acyclic graphs (DAGs) and performs a clinically weighted graph edit distance (GED)-based discrepancy analysis. An LLM-as-a-judge aligns semantically equivalent nodes and diagnoses discrepancies between graphs. These graph-level discrepancy diagnostics are converted into natural-language instructions and stored in a Differential Reasoning Knowledge Base (DR-KB). At inference, we retrieve top-$k$ instructions via Retrieval-Augmented Generation (RAG) to augment the agent prompt and patch likely logic gaps. Evaluation on open medical question answering (QA) benchmarks and a Return Visit Admissions (RVA) prediction task from internal clinical data demonstrates gains over baselines, improving both final-answer accuracy and reasoning fidelity. Ablation studies confirm gains from infusing reference reasoning rationales and the top-$k$ retrieval strategy. Clinicians' review of the output provides further assurance of the approach. Together, results suggest that DRL supports more reliable clinical decision-making in complex reasoning scenarios and offers a practical mechanism for deployment under limited token budgets.

</details>


### [27] [ESTAR: Early-Stopping Token-Aware Reasoning For Efficient Inference](https://arxiv.org/abs/2602.10004)
*Junda Wang,Zhichao Yang,Dongxu Zhang,Sanjit Singh Batra,Robert E. Tillman*

Main category: cs.AI

TL;DR: ESTAR方法通过早期停止机制检测推理冗余，将推理长度减少3.7倍（从4799降至1290），同时保持准确率（74.9% vs 74.2%），显著提升大型推理模型的效率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过生成长链思维推理获得最佳性能，但经常在已经得出正确答案后继续冗余推理，浪费计算资源。需要一种方法来检测和减少这种推理冗余，提高效率而不牺牲准确性。

Method: ESTAR方法结合三个组件：1）基于轨迹的分类器识别何时可以安全停止推理；2）监督微调教导LRM提出自生成的<stop>信号；3）<stop>感知的强化学习，在自生成的停止点截断rollout，并使用计算感知的奖励。

Result: 在四个推理数据集上的实验显示，ESTAR将推理长度减少约3.7倍（从4799降至1290），同时保持准确率（74.9% vs 74.2%），并展现出强大的跨领域泛化能力。

Conclusion: 早期停止是一种简单而强大的机制，可以显著提高大型推理模型的推理效率。ESTAR方法在保持准确性的同时大幅减少计算开销，为高效推理模型提供了有效解决方案。

Abstract: Large reasoning models (LRMs) achieve state-of-the-art performance by generating long chains-of-thought, but often waste computation on redundant reasoning after the correct answer has already been reached. We introduce Early-Stopping for Token-Aware Reasoning (ESTAR), which detects and reduces such reasoning redundancy to improve efficiency without sacrificing accuracy. Our method combines (i) a trajectory-based classifier that identifies when reasoning can be safely stopped, (ii) supervised fine-tuning to teach LRMs to propose self-generated <stop> signals, and (iii) <stop>-aware reinforcement learning that truncates rollouts at self-generated stop points with compute-aware rewards. Experiments on four reasoning datasets show that ESTAR reduces reasoning length by about 3.7x (from 4,799 to 1,290) while preserving accuracy (74.9% vs. 74.2%), with strong cross-domain generalization. These results highlight early stopping as a simple yet powerful mechanism for improving reasoning efficiency in LRMs.

</details>


### [28] [Discovering High Level Patterns from Simulation Traces](https://arxiv.org/abs/2602.10009)
*Sean Memery,Kartic Subr*

Main category: cs.AI

TL;DR: 提出一种自然语言引导的方法，从详细仿真日志中发现粗粒度模式（如"刚体碰撞"、"稳定支撑"等），使语言模型能更好地进行物理系统推理。


<details>
  <summary>Details</summary>
Motivation: 语言模型在物理推理任务上表现不佳，因为它们从观测数据学习而非基于仿真。现有方法将仿真轨迹作为上下文，但可扩展性差，因为仿真轨迹包含大量细粒度数值和语义数据。

Method: 提出自然语言引导的方法，从详细仿真日志中发现粗粒度模式。合成在仿真日志上运行的程序，将其映射到一系列高级激活模式，创建仿真日志的注释表示。

Result: 通过两个物理基准测试证明，这种注释表示更适合自然语言对物理系统的推理。该方法使语言模型能够从自然语言指定的目标生成有效的奖励程序，可用于规划或监督学习。

Conclusion: 提出的方法通过从仿真日志中提取粗粒度模式，显著提升了语言模型在物理推理任务上的能力，为解决语言模型物理推理的局限性提供了有效途径。

Abstract: Artificial intelligence (AI) agents embedded in environments with physics-based interaction face many challenges including reasoning, planning, summarization, and question answering. This problem is exacerbated when a human user wishes to either guide or interact with the agent in natural language. Although the use of Language Models (LMs) is the default choice, as an AI tool, they struggle with tasks involving physics. The LM's capability for physical reasoning is learned from observational data, rather than being grounded in simulation. A common approach is to include simulation traces as context, but this suffers from poor scalability as simulation traces contain larger volumes of fine-grained numerical and semantic data. In this paper, we propose a natural language guided method to discover coarse-grained patterns (e.g., 'rigid-body collision', 'stable support', etc.) from detailed simulation logs. Specifically, we synthesize programs that operate on simulation logs and map them to a series of high level activated patterns. We show, through two physics benchmarks, that this annotated representation of the simulation log is more amenable to natural language reasoning about physical systems. We demonstrate how this method enables LMs to generate effective reward programs from goals specified in natural language, which may be used within the context of planning or supervised learning.

</details>


### [29] [Chain of Mindset: Reasoning with Adaptive Cognitive Modes](https://arxiv.org/abs/2602.10063)
*Tianyi Jiang,Arctanx An,Hengyi Feng,Naixin Zhai,Haodong Li,Xiaomin Yu,Jiahui Liu,Hanwen Du,Shuo Zhang,Zhi Yang,Jie Huang,Yuhua Li,Yongxin Ni,Huacan Wang,Ronghao Chen*

Main category: cs.AI

TL;DR: 提出Chain of Mindset (CoM)框架，通过动态协调四种不同思维模式（空间、收敛、发散、算法）来提升LLM推理能力，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理方法存在"单一思维陷阱"，在整个推理过程中使用固定思维模式，而人类解决问题时会根据问题不同阶段动态切换不同思维模式。这种局限性阻碍了模型达到更高智能水平。

Method: 提出无需训练的CoM框架，将推理分解为四种功能异构的思维模式：空间思维（空间推理）、收敛思维（逻辑推导）、发散思维（创意生成）和算法思维（结构化计算）。通过元代理动态选择最优思维模式，并使用双向上下文门控过滤跨模块信息流。

Result: 在数学、代码生成、科学QA和空间推理等六个挑战性基准测试中，CoM在Qwen3-VL-32B-Instruct和Gemini-2.0-Flash上分别比最强基线提升4.96%和4.72%的总体准确率，同时保持推理效率平衡。

Conclusion: CoM框架通过动态协调多种思维模式，突破了现有LLM推理方法的单一思维限制，实现了更接近人类问题解决的智能推理方式，为下一代AI系统提供了有前景的方向。

Abstract: Human problem-solving is never the repetition of a single mindset, by which we mean a distinct mode of cognitive processing. When tackling a specific task, we do not rely on a single mindset; instead, we integrate multiple mindsets within the single solution process. However, existing LLM reasoning methods fall into a common trap: they apply the same fixed mindset across all steps, overlooking that different stages of solving the same problem require fundamentally different mindsets. This single-minded assumption prevents models from reaching the next level of intelligence. To address this limitation, we propose Chain of Mindset (CoM), a training-free agentic framework that enables step-level adaptive mindset orchestration. CoM decomposes reasoning into four functionally heterogeneous mindsets: Spatial, Convergent, Divergent, and Algorithmic. A Meta-Agent dynamically selects the optimal mindset based on the evolving reasoning state, while a bidirectional Context Gate filters cross-module information flow to maintain effectiveness and efficiency. Experiments across six challenging benchmarks spanning mathematics, code generation, scientific QA, and spatial reasoning demonstrate that CoM achieves state-of-the-art performance, outperforming the strongest baseline by 4.96\% and 4.72\% in overall accuracy on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash, while balancing reasoning efficiency. Our code is publicly available at \href{https://github.com/QuantaAlpha/chain-of-mindset}{https://github.com/QuantaAlpha/chain-of-mindset}.

</details>


### [30] [CODE-SHARP: Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs](https://arxiv.org/abs/2602.10085)
*Richard Bornemann,Pierluigi Vito Amadori,Antoine Cully*

Main category: cs.AI

TL;DR: CODE-SHARP 是一个利用基础模型自动发现和演化分层技能的新框架，通过代码形式的奖励函数构建技能图谱，使智能体能够解决复杂的长时程任务。


<details>
  <summary>Details</summary>
Motivation: 开放式的技能发现是人工智能的重大挑战。传统强化学习依赖人工设计的奖励函数，这在技能集合未知的开放式场景中不可行。现有方法局限于为预定义任务优化奖励，无法实现真正的开放式技能发现。

Method: 提出 CODE-SHARP 框架，利用基础模型开放式地扩展和优化分层技能档案。技能档案以有向图形式组织，包含可执行的代码奖励函数。通过目标条件智能体训练，结合高层基础模型规划器组合技能。

Result: 在 Craftax 环境中，仅使用发现的 SHARP 技能奖励训练的目标条件智能体能够解决越来越长时程的目标。组合技能后，单个智能体能够解决复杂长时程任务，平均性能超过预训练智能体和任务特定专家策略 134%。

Conclusion: CODE-SHARP 成功实现了开放式技能发现，通过基础模型自动生成和演化分层技能，使智能体能够解决复杂的长时程任务，为开放式人工智能发展提供了新方向。

Abstract: Developing agents capable of open-endedly discovering and learning novel skills is a grand challenge in Artificial Intelligence. While reinforcement learning offers a powerful framework for training agents to master complex skills, it typically relies on hand-designed reward functions. This is infeasible for open-ended skill discovery, where the set of meaningful skills is not known a priori. While recent methods have shown promising results towards automating reward function design, they remain limited to refining rewards for pre-defined tasks. To address this limitation, we introduce Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs (CODE-SHARP), a novel framework leveraging Foundation Models (FM) to open-endedly expand and refine a hierarchical skill archive, structured as a directed graph of executable reward functions in code. We show that a goal-conditioned agent trained exclusively on the rewards generated by the discovered SHARP skills learns to solve increasingly long-horizon goals in the Craftax environment. When composed by a high-level FM-based planner, the discovered skills enable a single goal-conditioned agent to solve complex, long-horizon tasks, outperforming both pretrained agents and task-specific expert policies by over $134$% on average. We will open-source our code and provide additional videos $\href{https://sites.google.com/view/code-sharp/homepage}{here}$.

</details>


### [31] [Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning](https://arxiv.org/abs/2602.10090)
*Zhaoyang Wang,Canwen Xu,Boyi Liu,Yite Wang,Siwei Han,Zhewei Yao,Huaxiu Yao,Yuxiong He*

Main category: cs.AI

TL;DR: 提出Agent World Model (AWM)合成环境生成管道，创建1000个代码驱动、数据库支持的环境，用于大规模强化学习训练工具使用智能体，实现强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前大规模语言模型智能体训练受限于缺乏多样可靠的环境，需要解决环境稀缺问题以支持多轮交互工具使用任务。

Method: 开发AWM合成环境生成管道，创建1000个代码驱动、数据库支持的环境，每个环境平均35个工具，提供高质量观测和可靠状态转换。

Result: 在三个基准测试中，仅在合成环境中训练的工具使用智能体展现出强大的分布外泛化能力，优于在基准特定环境中训练。

Conclusion: AWM合成环境管道为智能体训练提供了可扩展、可靠的解决方案，支持大规模强化学习并实现良好泛化性能。

Abstract: Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale to 1,000 environments covering everyday scenarios, in which agents can interact with rich toolsets (35 tools per environment on average) and obtain high-quality observations. Notably, these environments are code-driven and backed by databases, providing more reliable and consistent state transitions than environments simulated by LLMs. Moreover, they enable more efficient agent interaction compared with collecting trajectories from realistic environments. To demonstrate the effectiveness of this resource, we perform large-scale reinforcement learning for multi-turn tool-use agents. Thanks to the fully executable environments and accessible database states, we can also design reliable reward functions. Experiments on three benchmarks show that training exclusively in synthetic environments, rather than benchmark-specific ones, yields strong out-of-distribution generalization. The code is available at https://github.com/Snowflake-Labs/agent-world-model.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [32] [Triggered: A Statistical Analysis of Environmental Influences on Extremist Groups](https://arxiv.org/abs/2602.09289)
*Christine de Kock,Eduard Hovy*

Main category: cs.SI

TL;DR: 研究采用系统视角分析极端主义社区与主流信息生态系统的互动关系，发现不同极端主义社区对外部刺激的反应程度存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 极端主义社区并非孤立存在，而是受到现实世界事件、新闻报道和跨社区互动的影响。研究旨在理解这些外部因素如何塑造极端主义社区的行为和语言动态。

Method: 使用7年数据对比两个意识形态不同的极端主义论坛(Stormfront和Incels)和一个主流参考社区(r/News)。结合反事实合成估计事件级影响，采用向量自回归和格兰杰因果分析建模新闻信号、行为结果和跨社区语言变化之间的关系。

Result: Stormfront和r/News对外部刺激反应更敏感，而Incels社区表现出较少的跨社区语言影响，对新闻和暴力事件的反应也较弱。极端主义社区并非同质，它们与周围信息生态系统的耦合程度存在差异。

Conclusion: 极端主义社区与主流信息生态系统的互动模式存在显著差异，这种异质性对理解和干预极端主义社区具有重要意义。

Abstract: Online extremist communities operate within a wider information ecosystem shaped by real-world events, news coverage, and cross-community interaction. We adopt a systems perspective to examine these influences using seven years of data from two ideologically distinct extremist forums (Stormfront and Incels) and a mainstream reference community (r/News). We ask three questions: how extremist violence impacts community behaviour; whether news coverage of political entities predicts shifts in conversation dynamics; and whether linguistic diffusion occurs between mainstream and extremist spaces and across extremist ideologies. Methodologically, we combine counterfactual synthesis to estimate event-level impacts with vector autoregression and Granger causality analyses to model ongoing relationships among news signals, behavioural outcomes, and cross-community language change. Across analyses, our results indicate that Stormfront and r/News appear to be more reactive to external stimuli, while Incels demonstrates less cross-community linguistic influence and less responsiveness to news and violent events. These findings underscore that extremist communities are not homogeneous, but differ in how tightly they are coupled to the surrounding information ecosystem.

</details>


### [33] [Shifting landscape of disability and development in India: Analysis from historical trends to future predictions 2001-2031](https://arxiv.org/abs/2602.09543)
*Hana Kapadia,Arun Kumar Rajasekaran*

Main category: cs.SI

TL;DR: 印度各邦残疾相关健康负担的原因与趋势分析：研究发现传染病DALY与HDI呈强负相关，非传染性疾病DALY未随HDI上升而下降，伤害DALY适度下降，男性在残疾人口中比例过高


<details>
  <summary>Details</summary>
Motivation: 研究印度各邦残疾相关健康负担的原因和趋势，通过多种DALY类型（传染病、非传染病、伤害）、性别差异和人类发展指数来评估残疾趋势，为公共卫生政策提供依据

Method: 使用来自人口普查、健康研究机构和数据中心的数据，建立回归模型分析过去几十年的趋势，并对2031年进行预测

Result: 传染病DALY与HDI呈强负相关（发展改善显著降低传染病负担）；非传染病DALY未随HDI上升而下降；伤害DALY随HDI上升适度下降（反映医疗和安全系统改善）；性别分析显示男性在残疾人口中比例过高

Conclusion: 公共卫生重点需要转向慢性疾病，并解决残疾结果中的性别差异问题

Abstract: This study delves into the causes and trends of disability-related health burdens across Indian states. Through multiple Disability-Adjusted Life Years (DALY) types (covering communicable diseases, noncommunicable diseases, and injuries), gender disparities, and Human Development Index (HDI) values, these disability trends were evaluated. The data for this study was compiled from censuses, health research organisations, and data centres, among various other sources. We built regression models and used them to analyze trends across past decades and make projections for 2031. Our regression results show a strong inverse relationship between communicable disease DALYs and HDI. In other words, ongoing improvements in development and infrastructure significantly reduced communicable disease DALYs. In contrast, noncommunicable DALYs did not decrease despite rising HDI. And lastly, injury DALYs showed moderate declines with higher HDI, which reflects improvements in healthcare and safety systems. Gender analysis showed male overrepresentation among people with disabilities. These results from our study support that there is a need to shift public health focus toward chronic diseases and address gender disparities in disability outcomes.

</details>


### [34] [UniShare: A Unified Framework for Joint Video and Receiver Recommendation in Social Sharing](https://arxiv.org/abs/2602.09618)
*Caimeng Wang,Li Chong,Dongxu Liu,Xu Min,Jianhui Bu*

Main category: cs.SI

TL;DR: UniShare是一个统一的短视频分享预测框架，同时优化视频推荐和接收者推荐，通过联合训练和增强表征学习提升分享效果。


<details>
  <summary>Details</summary>
Motivation: 传统工业解决方案将短视频分享行为分解为视频推荐（预测分享概率）和接收者推荐（预测分享对象）两个独立任务，导致性能次优，因为孤立建模和信息利用不足。

Method: 提出UniShare统一框架，通过增强表征学习模块（结合预训练GNN和多模态嵌入）和显式双边兴趣关系匹配来建模分享概率。关键创新是联合训练范式，利用两个任务的信号相互增强，缓解数据稀疏性并提升双边满意度。

Result: 离线实验显示UniShare在两个任务上显著优于强基线。在快手平台的在线A/B测试证实其有效性，关键指标显著提升：分享数量(+1.95%)和接收者回复率(+0.482%)。

Conclusion: UniShare通过统一建模短视频分享的复杂三元交互，解决了传统解耦方法的局限性，在真实场景中验证了其优越性能。

Abstract: Sharing behavior on short-video platforms constitutes a complex ternary interaction among the user (sharer), the video (content), and the receiver. Traditional industrial solutions often decouple this into two independent tasks: video recommendation (predicting share probability) and receiver recommendation (predicting whom to share with), leading to suboptimal performance due to isolated modeling and inadequate information utilization. To address this, we propose UniShare, a novel unified framework for joint sharing prediction on both video and receiver recommendation. UniShare models the share probability through an enhanced representation learning module that incorporates pre-trained GNN and multi-modal embeddings, alongside explicit bilateral interest and relationship matching. A key innovation is our joint training paradigm, which leverages signals from both tasks to mutually enhance each other, mitigating data sparsity and improving bilateral satisfaction. We also introduce K-Share, a large-scale real-world dataset constructed from Kuaishou platform logs to support research in this domain. Extensive offline experiments demonstrate that UniShare significantly outperforms strong baselines on both tasks. Furthermore, online A/B testing on the Kuaishou platform confirms its effectiveness, achieving significant improvements in key metrics including the number of shares (+1.95%) and receiver reply rate (+0.482%).

</details>


### [35] [Popularity Feedback Constrains Innovation in Cultural Markets](https://arxiv.org/abs/2602.09997)
*Lucas Gautheron,Raja Marjieh,Dalton C. Conley,Seth Frey,Hannah Rubin,Mike D. Schneider,Ofer Tchernichovski,Nori Jacoby*

Main category: cs.SI

TL;DR: 研究显示，在创意市场中暴露流行度信息会降低文化多样性、减缓创新速度，并延迟审美改进，这主要通过影响选择和创造两个环节实现。


<details>
  <summary>Details</summary>
Motivation: 现实世界的创意过程（从艺术到科学）依赖于选择和创造之间的社会反馈循环，但流行度反馈对集体创造力的影响尚未得到充分理解。研究者希望探究流行度评级如何影响文化动态。

Method: 通过大规模在线实验（N=1008），参与者从不断演化的市场中选择图像，并制作自己的修改版本，研究比较了暴露与不暴露图像流行度信息的情况。

Result: 暴露图像流行度信息会降低文化多样性并减缓创新，延迟审美改进。在选择环节，流行度信息引发累积优势效应，参与者倾向于基于流行图像进行创作；在创造环节，参与者做出的改变更少颠覆性，更倾向于扩展现有视觉模式。

Conclusion: 文化市场中的反馈循环不仅影响选择，还直接或间接地塑造文化创新的形式和方向。流行度信息可能对集体创造力产生负面影响，降低多样性和创新速度。

Abstract: Real-world creative processes ranging from art to science rely on social feedback-loops between selection and creation. Yet, the effects of popularity feedback on collective creativity remain poorly understood. We investigate how popularity ratings influence cultural dynamics in a large-scale online experiment where participants ($N = 1\,008$) iteratively \textit{select} images from evolving markets and \textit{produce} their own modifications. Results show that exposing the popularity of images reduces cultural diversity and slows innovation, delaying aesthetic improvements. These findings are mediated by alterations of both selection and creation. During selection, popularity information triggers cumulative advantage, with participants preferentially building upon popular images, reducing diversity. During creation, participants make less disruptive changes, and are more likely to expand existing visual patterns. Feedback loops in cultural markets thus not only shape selection, but also, directly or indirectly, the form and direction of cultural innovation.

</details>


### [36] [Human-AI Synergy Supports Collective Creative Search](https://arxiv.org/abs/2602.10001)
*Chenyi Li,Raja Marjieh,Haoyu Hu,Mark Steyvers,Katherine M. Collins,Ilia Sucholutsky,Nori Jacoby*

Main category: cs.SI

TL;DR: 研究通过单词猜测任务比较人类、AI及混合团队的创造表现，发现混合团队表现最佳且保持高多样性，人类与AI存在高阶互动效应。


<details>
  <summary>Details</summary>
Motivation: 生成式AI正将创造力转变为混合人机过程，但其对创意输出质量和多样性的影响尚不明确，需要研究集体创造力中人机协作的效果。

Method: 采用受控单词猜测任务，平衡开放性与客观性能指标。参与者推断隐藏目标词，根据猜测与目标的语义相似度评分，同时观察前玩家的最佳猜测。比较纯人类、纯AI和混合人机三组的表现和结果多样性。

Result: 混合组表现最佳且保持高猜测多样性。在混合组中，人类和AI都相对于单智能体条件系统调整策略，表明存在高阶互动效应。虽然异质AI系统协作能重现部分性能优势，但人机协作仍更优越。

Conclusion: 人机协作在集体创造力中具有互补作用，混合团队能实现最佳性能同时保持高多样性，突显了人类与AI在创意过程中的协同价值。

Abstract: Generative AI is increasingly transforming creativity into a hybrid human-artificial process, but its impact on the quality and diversity of creative output remains unclear. We study collective creativity using a controlled word-guessing task that balances open-endedness with an objective measure of task performance. Participants attempt to infer a hidden target word, scored based on the semantic similarity of their guesses to the target, while also observing the best guess from previous players. We compare performance and outcome diversity across human-only, AI-only, and hybrid human-AI groups. Hybrid groups achieve the highest performance while preserving high diversity of guesses. Within hybrid groups, both humans and AI agents systematically adjust their strategies relative to single-agent conditions, suggesting higher-order interaction effects, whereby agents adapt to each other's presence. Although some performance benefits can be reproduced through collaboration between heterogeneous AI systems, human-AI collaboration remains superior, underscoring complementary roles in collective creativity.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [37] [Initial-Condition-Robust Inference in Autoregressive Models](https://arxiv.org/abs/2602.09382)
*Donald W. K. Andrews,Ming Li,Yapeng Zheng*

Main category: econ.EM

TL;DR: 提出一种对初始条件完全稳健的自回归参数置信区间，无论初始条件是平稳、固定还是非平稳，都能保证渐近和有限样本覆盖概率


<details>
  <summary>Details</summary>
Motivation: 现有自回归参数置信区间依赖于平稳或固定初始条件的假设，当这些假设不成立时，覆盖概率会变得很差。需要一种对初始条件完全稳健的置信区间方法。

Method: 开发了一种新的自回归参数置信区间构造方法，该方法对初始条件完全稳健，同时也能处理条件异方差误差

Result: 新置信区间在初始条件为平稳或固定时仅付出很小的长度代价，但在初始条件假设不成立时仍能保持正确的覆盖概率

Conclusion: 提出了一种对初始条件完全稳健的自回归参数置信区间，解决了现有方法在初始条件假设不成立时覆盖概率差的问题，具有实际应用价值

Abstract: This paper considers confidence intervals (CIs) for the autoregressive (AR) parameter in an AR model with an AR parameter that may be close or equal to one. Existing CIs rely on the assumption of a stationary or fixed initial condition to obtain correct asymptotic coverage and good finite sample coverage. When this assumption fails, their coverage can be quite poor. In this paper, we introduce a new CI for the AR parameter whose coverage probability is completely robust to the initial condition, both asymptotically and in finite samples. This CI pays only a small price in terms of its length when the initial condition is stationary or fixed. The new CI also is robust to conditional heteroskedasticity of the errors.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [38] [Genocide by Algorithm in Gaza: Artificial Intelligence, Countervailing Responsibility, and the Corruption of Public Discourse](https://arxiv.org/abs/2602.09202)
*Branislav Radeljic*

Main category: cs.CY

TL;DR: 本文以加沙冲突为例，分析AI目标系统如何作为认知基础设施分类、合法化和执行暴力，探讨政治、专业和个人三个维度的责任问题，揭示AI在殖民等级再生产中的作用，呼吁民主化AI伦理。


<details>
  <summary>Details</summary>
Motivation: 人工智能军事化加速改变了战争的伦理、政治和治理。本文旨在探讨AI驱动的目标系统如何作为认知基础设施运作，以以色列在加沙的行动为典型案例，分析AI在暴力执行中的角色及其责任问题。

Method: 通过责任视角分析三个相互关联的维度：政治责任（国家如何利用AI加速战争并逃避问责）、专业责任（技术专家和国防承包商在数据武器化中的共谋）、个人责任（参与或抵抗算法治理的个体道德能动性）。结合公共话语参与者的立场和影响力分析。

Result: 加沙案例揭示AI不是中性工具，而是殖民等级再生产和暴行正常化的积极参与者。AI伦理话语往往掩盖或正常化AI驱动的暴力，需要重新构建技术能动性和问责框架。

Conclusion: 面对算法暴力需要民主化AI伦理，抵制技术官僚的宿命论，以受高科技军事主义影响最深的群体的生活现实为中心。必须重新构建自动化战争时代的技术能动性和问责机制。

Abstract: The accelerating militarization of artificial intelligence has transformed the ethics, politics, and governance of warfare. This article interrogates how AI-driven targeting systems function as epistemic infrastructures that classify, legitimize, and execute violence, using Israel's conduct in Gaza as a paradigmatic case. Through the lens of responsibility, the article examines three interrelated dimensions: (a) political responsibility, exploring how states exploit AI to accelerate warfare while evading accountability; (b) professional responsibility, addressing the complicity of technologists, engineers, and defense contractors in the weaponization of data; and (c) personal responsibility, probing the moral agency of individuals who participate in or resist algorithmic governance. This is complemented by an examination of the position and influence of those participating in public discourse, whose narratives often obscure or normalize AI-enabled violence. The Gaza case reveals AI not as a neutral instrument but as an active participant in the reproduction of colonial hierarchies and the normalization of atrocity. Ultimately, the paper calls for a reframing of technological agency and accountability in the age of automated warfare. It concludes that confronting algorithmic violence demands a democratization of AI ethics, one that resists technocratic fatalism and centers the lived realities of those most affected by high-tech militarism.

</details>


### [39] ["These cameras are just like the Eye of Sauron": A Sociotechnical Threat Model for AI-Driven Smart Home Devices as Perceived by UK-Based Domestic Workers](https://arxiv.org/abs/2602.09239)
*Shijing He,Yaxiong Lei,Xiao Zhan,Ruba Abu-Salma,Jose Such*

Main category: cs.CY

TL;DR: 该研究通过访谈18名英国家政工人，从沟通隐私管理理论视角分析AI智能家居设备带来的隐私风险，揭示了跨家庭数据流动和机构中介雇佣安排如何加剧家政工人的隐私威胁。


<details>
  <summary>Details</summary>
Motivation: 随着AI智能家居设备的普及，家政工人在雇主家中被监控的同时，在自己家中也使用智能设备，面临独特的跨家庭隐私风险。现有威胁模型未能充分涵盖这种具体情境下的隐私挑战。

Method: 采用半结构化访谈法，访问18名英国家政工人，并结合沟通隐私管理理论进行以人为中心的威胁建模分析。

Result: 研究发现：在雇主家中，AI功能和中介机构安排加剧了监控并限制了隐私边界谈判能力；在自己家中，虽然作为设备所有者有更多控制权，但仍面临性别化行政角色、AI功能不透明和数据保留不确定性等挑战。

Conclusion: 研究提出了一个社会技术威胁模型，将家政机构识别为制度性对手，并绘制了跨互联家庭的AI驱动隐私风险图，为加强家政工人的隐私和自主权提供了社会和实践建议。

Abstract: The growing adoption of AI-driven smart home devices has introduced new privacy risks for domestic workers (DWs), who are frequently monitored in employers' homes while also using smart devices in their own households. We conducted semi-structured interviews with 18 UK-based DWs and performed a human-centered threat modeling analysis of their experiences through the lens of Communication Privacy Management (CPM). Our findings extend existing threat models beyond abstract adversaries and single-household contexts by showing how AI analytics, residual data logs, and cross-household data flows shaped the privacy risks faced by participants. In employer-controlled homes, AI-enabled features and opaque, agency-mediated employment arrangements intensified surveillance and constrained participants' ability to negotiate privacy boundaries. In their own homes, participants had greater control as device owners but still faced challenges, including gendered administrative roles, opaque AI functionalities, and uncertainty around data retention. We synthesize these insights into a sociotechnical threat model that identifies DW agencies as institutional adversaries and maps AI-driven privacy risks across interconnected households, and we outline social and practical implications for strengthening DW privacy and agency.

</details>


### [40] [Marco IA593: Modelo de Gobernanza, Ética y Estrategia para la Integración de la Inteligencia Artificial en la Educación Superior del Ecuador](https://arxiv.org/abs/2602.09246)
*Luis Chamba-Eras,Oscar Miguel Cumbicus Pineda,Edison Leonardo Coronel Romero,Jessica Katherine Gaona Alvarado,Luis Rodrigo Barba Guamán*

Main category: cs.CY

TL;DR: 本文提出IA593框架，一个针对厄瓜多尔高等教育机构的AI治理、伦理和运营模型，旨在解决该国在AI采用方面的结构性差距，促进从被动技术消费向主权和批判性AI采用的转变。


<details>
  <summary>Details</summary>
Motivation: 厄瓜多尔在拉丁美洲AI指数中被列为"迟醒采纳者"，存在严重的结构性差距：研发投资仅占GDP的0.44%，对全球AI科学产出贡献微乎其微。虽然存在国家AI促进战略，但大学缺乏管理生成式AI使用的内部规定，危及学术诚信和数据隐私。

Method: 提出IA593框架，包含五个相互关联的支柱：横向治理、教学与培训、研究、外展和管理。该框架与FATE原则（公平性、问责制、透明度和伦理）以及联合国教科文组织的AI伦理建议保持一致。

Result: IA593框架为厄瓜多尔高等教育系统提供了可扩展的参考模型，使高等教育机构能够从被动技术消费转向主权和批判性的AI采用，确保符合国家学术法规，并帮助减少数字鸿沟和人才流失。

Conclusion: 将AI整合到厄瓜多尔高等教育机构不是技术选择而是战略必要，IA593框架为解决该国AI采用的结构性挑战提供了具体路径，有助于防止机构过时和学术无关性，同时促进负责任的AI治理。

Abstract: The integration of Artificial Intelligence (AI) into Higher Education Institutions (HEIs) in Ecuador is not a technological option but a strategic imperative to prevent institutional obsolescence and academic irrelevance in Latin America. This paper presents the IA593 Framework, a governance, ethics, and operational model designed for the Universidad Nacional de Loja (UNL) and scalable as a reference for the Ecuadorian higher education system. The current context reveals a critical urgency: the Latin American Artificial Intelligence Index 2025 classifies Ecuador as a late awakening adopter, exposing severe structural gaps, including R and D investment of only 0.44 percent of GDP and a marginal contribution to global AI scientific output. Although a National Strategy for the Promotion of AI exists and calls for multisectoral governance, universities still lack internal regulations governing the use of Generative AI, placing academic integrity and data privacy at risk. The IA593 Framework addresses this challenge through five interconnected pillars aligned with the FATE principles of Fairness, Accountability, Transparency, and Ethics and UNESCO recommendations on AI ethics: Transversal Governance, Teaching and Training, Research, Outreach, and Management. This framework enables HEIs to move from passive technology consumption toward a sovereign and critical adoption of AI, ensuring compliance with national academic regulations and positioning UNL as a key actor in reducing the digital divide and brain drain in Ecuador.

</details>


### [41] [Synthetic Reflections on Resource Extraction](https://arxiv.org/abs/2602.09299)
*Sai Krishna Tammali,Vinaya Kumar,Marc Böhlen*

Main category: cs.CY

TL;DR: 论文提出结合统计操作、人类判断和生成式AI的框架，利用Sentinel-2卫星数据生成工业采矿场的景观解读


<details>
  <summary>Details</summary>
Motivation: 探索如何增强和调整AI模型以产生景观解读，记录人类与AI系统之间的共同历史

Method: 开发Sentinel-2卫星资产解读流程，结合统计操作、人类判断和生成式AI模型

Result: 创建了能够为全球工业采矿场生成简洁评论的AI系统

Conclusion: AI模型可以通过多模态方法增强以产生有意义的景观解读，为人类-AI协作记录环境历史提供新途径

Abstract: This paper describes how AI models can be augmented and adapted to produce interpretation of landscapes. We describe the technical framework of a Sentinel-2 satellite asset interpretation pipeline that combines statistical operations, human judgement, and generative AI models to create succinct commentaries on industrial mining sites across the planet, documenting a past shared between people and AI systems.

</details>


### [42] [Trade-Offs in Deploying Legal AI: Insights from a Public Opinion Study to Guide AI Risk Management](https://arxiv.org/abs/2602.09636)
*Kimon Kieslich,Sophie Morosoli,Nicholas Diakopoulos,Natali Helberger*

Main category: cs.CY

TL;DR: 本文通过德国公民调查数据，系统分析了生成式AI在法律咨询和调解任务中的风险与收益，识别了影响风险接受度的预测因素和权衡主题。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在法律领域的应用日益增多，但欧盟AI法案仅对部分高风险用例（如法官使用）要求风险评估和审计，而公民法律咨询等用例则不受此约束。当前风险管理实践主要依赖专家判断，缺乏对受影响社区的咨询。鉴于法律部门的社会重要性以及生成式AI可能带来的变革性影响，公众对AI法律应用的认知和风险收益理解对解决方案的可接受性和合法性至关重要。

Method: 采用德国公民代表性样本调查（n=488），系统分析公民对生成式AI在两项法律任务（法律咨询和法律调解）中的看法。具体方法包括：1）系统映射两项任务的风险和收益因素；2）描述影响这些任务中生成式AI使用风险接受度的预测因素；3）突出公民在权衡风险可接受性时涉及的权衡主题。

Result: 研究结果提供了公民对法律领域生成式AI风险管理担忧的实证概览，突出了补充当前风险评估程序的关键主题。具体发现了公民对法律咨询和调解任务中AI使用的风险和收益因素的系统认知，识别了影响风险接受度的关键预测变量，并揭示了公民在权衡风险可接受性时考虑的权衡主题。

Conclusion: 本研究通过实证数据揭示了公民对法律领域生成式AI应用的担忧和权衡考量，为当前主要依赖专家判断的风险评估程序提供了重要的补充视角。研究强调了在AI法律应用的风险管理中纳入公众视角的重要性，以确保解决方案的可接受性和合法性，更好地服务于公共价值和人权保护。

Abstract: Generative AI tools are increasingly used for legal tasks, including legal research, drafting documents, and even for legal decision-making. As for other purposes, the use of GenAI in the legal domain comes with various risks and benefits that needs to be properly managed to ensure implementation in a way that serves public values and protect human rights. While the EU mandates risk assessment and audits before market introduction for some use cases (e.g., use by judges for administration of justice) other use cases do not fall under the AI Acts' high-risk classifications (e.g., use by citizens for legal consultation or drafting documents). Further, current risk management practices prioritize expert judgment on risk factor identification and prioritization without a corresponding legal requirement to consult with affected communities. Seeing the societal importance of the legal sector and the potentially transformative impact of GenAI in this sector, the acceptability and legitimacy of GenAI solutions also depends on public perceptions and a better understanding of the risks and benefits citizens associated with the use of AI in the legal sector. As a response, this papers presents data from a representative sample of German citizens (n=488) outlining citizens' perspectives on the use of GenAI for two legal tasks: legal consultation and legal mediation. Concretely, we i) systematically map risks and benefit factors for both legal tasks, ii) describe predictors that influence risk acceptance of the use of GenAI for those tasks, and iii) highlight emerging trade-off themes that citizens engage in when weighing up risk acceptability. Our results provides an empirical overview of citizens' concerns regarding risk management of GenAI for the legal domain, foregrounding critical themes that complement current risk assessment procedures.

</details>


### [43] [Administrative Law's Fourth Settlement: AI and the Capability-Accountability Trap](https://arxiv.org/abs/2602.09678)
*Nicholas Caputo*

Main category: cs.CY

TL;DR: 本文认为行政法自1887年以来陷入"能力-问责陷阱"：技术进步使政府更复杂，但复杂性导致机构对法院和国会不透明。最高法院在Loper Bright案后的收缩旨在缩小行政规模以应对复杂性，但这牺牲了应对气候变化、疫情和AI风险所需的能力。AI提供了不同路径，通过建立"可审查性"来同时保持能力和问责，本文提出三项制度创新实现"第四和解"。


<details>
  <summary>Details</summary>
Motivation: 行政法长期面临"能力-问责陷阱"：技术进步使政府机构更复杂，但复杂性导致对法院和国会等监督机构不透明。传统解决方案（程序审查替代实质监督）导致要求层层叠加，固化了能力却未确保民主控制。最高法院近期收缩行政规模的做法牺牲了应对气候变化、疫情和AI风险所需的能力。AI提供了突破这一困境的新可能性。

Method: 本文提出三项行政法制度创新：1) 模型与系统档案，将行政记录扩展到AI决策，记录模型目的、评估、监控和版本；2) 重大模型变更触发机制，规定AI更新何时需要新程序；3) "审计尊重"标准，奖励机构对其AI工具进行可审计评估。这些创新旨在利用AI建立"可审查性"，将技术复杂性转化为可理解术语。

Result: 通过AI建立"可审查性"，行政法可以同时保持政府能力和民主问责。提出的三项制度创新构成了"第四和解"框架，使行政法能够摆脱长期存在的"能力-问责陷阱"，在保持应对复杂挑战所需能力的同时，恢复对行政的可理解监督。

Conclusion: AI提供了突破行政法"能力-问责陷阱"的新路径。与以往增加不透明性的技术不同，AI可以帮助建立"可审查性"，将技术复杂性转化为可理解术语。通过三项制度创新，行政法可以实现"第四和解"，在保持应对气候变化、疫情和AI风险所需能力的同时，恢复民主问责和监督。

Abstract: Since 1887, administrative law has navigated a "capability-accountability trap": technological change forces government to become more sophisticated, but sophistication renders agencies opaque to generalist overseers like the courts and Congress. The law's response--substituting procedural review for substantive oversight--has produced a sedimentary accretion of requirements that ossify capacity without ensuring democratic control. This Article argues that the Supreme Court's post-Loper Bright retrenchment is best understood as an effort to shrink administration back to comprehensible size in response to this complexification. But reducing complexity in this way sacrifices capability precisely when climate change, pandemics, and AI risks demand more sophisticated governance.
  AI offers a different path. Unlike many prior administrative technologies that increased opacity alongside capacity, AI can help build "scrutability" in government, translating technical complexity into accessible terms, surfacing the assumptions that matter for oversight, and enabling substantive verification of agency reasoning. This Article proposes three doctrinal innovations within administrative law to realize this potential: a Model and System Dossier (documenting model purpose, evaluation, monitoring, and versioning) extending the administrative record to AI decision-making; a material-model-change trigger specifying when AI updates require new process; and a "deference to audit" standard that rewards agencies for auditable evaluation of their AI tools. The result is a framework for what this Article calls the "Fourth Settlement," administrative law that escapes the capability-accountability trap by preserving capability while restoring comprehensible oversight of administration.

</details>


### [44] [Budgeting Discretion: Theory and Evidence on Street-Level Decision-Making](https://arxiv.org/abs/2602.10039)
*Gaurab Pokharel,Sanmay Das,Patrick J. Fowler*

Main category: cs.CY

TL;DR: 论文将自由裁量权建模为动态分配问题，提出最优决策遵循动态阈值规则，并发现行为不变性：最优裁量率仅取决于收益分布的形状而非规模。


<details>
  <summary>Details</summary>
Motivation: 现有研究将自由裁量权视为静态的成本效益权衡，缺乏在现实操作约束下如何随时间合理分配自由裁量权的原则性模型。街头官僚经常面临遵循刚性政策还是行使专业判断的困境，但频繁的裁量会威胁一致性并引入偏见。

Method: 将自由裁量权形式化为动态分配问题：代理在有限时间T内收到随机改进机会，拥有有限覆盖预算K。证明最优决策遵循动态阈值规则，并识别行为不变性：对于位置尺度分布族，最优裁量率独立于潜在收益的规模，仅取决于分布形状。

Result: 发现系统性差异：当收益呈厚尾分布时，最优代理保持耐心，为异常值保留裁量权；当收益呈薄尾分布时，代理更常规地使用裁量权。通过无家可归者服务系统数据验证：裁量覆盖与操作约束一致，在工作周开始时较高，周末较低，并随短期住房容量变化。

Conclusion: 将自由裁量权作为明确预算资源管理，既能程序约束又能提高福利，为审计覆盖模式和设计决策支持系统提供了理论基础。

Abstract: Street-level bureaucrats, such as caseworkers and border guards routinely face the dilemma of whether to follow rigid policy or exercise discretion based on professional judgement. However, frequent overrides threaten consistency and introduce bias, explaining why bureaucracies often ration discretion as a finite resource. While prior work models discretion as a static cost-benefit tradeoff, we lack a principled model of how discretion should be rationed over time under real operational constraints.
  We formalize discretion as a dynamic allocation problem in which an agent receives stochastic opportunities to improve upon a default policy and must spend a limited override budget K over a finite horizon T. We show that overrides follow a dynamic threshold rule: use discretion only when the opportunity exceeds a time and budget-dependent cutoff. Our main theoretical contribution identifies a behavioral invariance: for location-scale families of improvement distributions, the rate at which an optimal agent exercises discretion is independent of the scale of potential gains and depends only on the distribution's shape (e.g., tail heaviness).
  This result implies systematic differences in discretionary "policy personality." When gains are fat-tailed, optimal agents are patient, conserving discretion for outliers. When gains are thin-tailed, agents spend more routinely. We illustrate these implications using data from a homelessness services system. Discretionary overrides track operational constraints: they are higher at the start of the workweek, suppressed on weekends when intake is offline, and shift with short-run housing capacity. These results suggest that discretion can be both procedurally constrained and welfare-improving when treated as an explicitly budgeted resource, providing a foundation for auditing override patterns and designing decision-support systems.

</details>

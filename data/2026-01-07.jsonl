{"id": "2601.03105", "categories": ["stat.AP", "cs.MA", "cs.SI", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2601.03105", "abs": "https://arxiv.org/abs/2601.03105", "authors": ["Abdulrahman A. Ahmed", "M. Amin Rahimian", "Qiushi Chen", "Praveen Kumar"], "title": "Computationally Efficient Estimation of Localized Treatment Effects in High-Dimensional Design Spaces using Gaussian Process Regression", "comment": "repository link: https://github.com/abdulrahmanfci/gpr-metamodel/", "summary": "Population-scale agent-based simulations of the opioid epidemic help evaluate intervention strategies and overdose outcomes in heterogeneous communities and provide estimates of localized treatment effects, which support the design of locally-tailored policies for precision public health. However, it is prohibitively costly to run simulations of all treatment conditions in all communities because the number of possible treatments grows exponentially with the number of interventions and levels at which they are applied. To address this need efficiently, we develop a metamodel framework, whereby treatment outcomes are modeled using a response function whose coefficients are learned through Gaussian process regression (GPR) on locally-contextualized covariates. We apply this framework to efficiently estimate treatment effects on overdose deaths in Pennsylvania counties. In contrast to classical designs such as fractional factorial design or Latin hypercube sampling, our approach leverages spatial correlations and posterior uncertainty to sequentially sample the most informative counties and treatment conditions. Using a calibrated agent-based opioid epidemic model, informed by county-level overdose mortality and baseline dispensing rate data for different treatments, we obtained county-level estimates of treatment effects on overdose deaths per 100,000 population for all treatment conditions in Pennsylvania, achieving approximately 5% average relative error using one-tenth the number of simulation runs required for exhaustive evaluation. Our bi-level framework provides a computationally efficient approach to decision support for policy makers, enabling rapid evaluation of alternative resource-allocation strategies to mitigate the opioid epidemic in local communities. The same analytical framework can be applied to guide precision public health interventions in other epidemic settings."}
{"id": "2601.02848", "categories": ["cs.SI", "cs.CY", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.02848", "abs": "https://arxiv.org/abs/2601.02848", "authors": ["Pratana Kukieattikool", "Kittiya Ku-kiattikun", "Anukool Noymai", "Navaporn Surasvadi", "Jantakarn Makma", "Pubodin Pornratchpum", "Watcharakon Noothong", "Chainarong Amornbunchornvej"], "title": "Modeling ICD-10 Morbidity and Multidimensional Poverty as a Spatial Network: Evidence from Thailand", "comment": "First draft", "summary": "Health and poverty in Thailand exhibit pronounced geographic structuring, yet the extent to which they operate as interconnected regional systems remains insufficiently understood. This study analyzes ICD-10 chapter-level morbidity and multidimensional poverty as outcomes embedded in a spatial interaction network. Interpreting Thailand's 76 provinces as nodes within a fixed-degree regional graph, we apply tools from spatial econometrics and social network analysis, including Moran's I, Local Indicators of Spatial Association (LISA), and Spatial Durbin Models (SDM), to assess spatial dependence and cross-provincial spillovers.\n  Our findings reveal strong spatial clustering across multiple ICD-10 chapters, with persistent high-high morbidity zones, particularly for digestive, respiratory, musculoskeletal, and symptom-based diseases, emerging in well-defined regional belts. SDM estimates demonstrate that spillover effects from neighboring provinces frequently exceed the influence of local deprivation, especially for living-condition, health-access, accessibility, and poor-household indicators. These patterns are consistent with contagion and contextual influence processes well established in social network theory.\n  By framing morbidity and poverty as interdependent attributes on a spatial network, this study contributes to the growing literature on structural diffusion, health inequality, and regional vulnerability. The results highlight the importance of coordinated policy interventions across provincial boundaries and demonstrate how network-based modeling can uncover the spatial dynamics of health and deprivation."}
{"id": "2601.02400", "categories": ["econ.EM", "cs.CL", "econ.GN", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.02400", "abs": "https://arxiv.org/abs/2601.02400", "authors": ["Adel Daoud", "Richard Johansson", "Connor T. Jerzak"], "title": "Detecting and Mitigating Treatment Leakage in Text-Based Causal Inference: Distillation and Sensitivity Analysis", "comment": null, "summary": "Text-based causal inference increasingly employs textual data as proxies for unobserved confounders, yet this approach introduces a previously undertheorized source of bias: treatment leakage. Treatment leakage occurs when text intended to capture confounding information also contains signals predictive of treatment status, thereby inducing post-treatment bias in causal estimates. Critically, this problem can arise even when documents precede treatment assignment, as authors may employ future-referencing language that anticipates subsequent interventions. Despite growing recognition of this issue, no systematic methods exist for identifying and mitigating treatment leakage in text-as-confounder applications. This paper addresses this gap through three contributions. First, we provide formal statistical and set-theoretic definitions of treatment leakage that clarify when and why bias occurs. Second, we propose four text distillation methods -- similarity-based passage removal, distant supervision classification, salient feature removal, and iterative nullspace projection -- designed to eliminate treatment-predictive content while preserving confounder information. Third, we validate these methods through simulations using synthetic text and an empirical application examining International Monetary Fund structural adjustment programs and child mortality. Our findings indicate that moderate distillation optimally balances bias reduction against confounder retention, whereas overly stringent approaches degrade estimate precision."}
{"id": "2601.02848", "categories": ["cs.SI", "cs.CY", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.02848", "abs": "https://arxiv.org/abs/2601.02848", "authors": ["Pratana Kukieattikool", "Kittiya Ku-kiattikun", "Anukool Noymai", "Navaporn Surasvadi", "Jantakarn Makma", "Pubodin Pornratchpum", "Watcharakon Noothong", "Chainarong Amornbunchornvej"], "title": "Modeling ICD-10 Morbidity and Multidimensional Poverty as a Spatial Network: Evidence from Thailand", "comment": "First draft", "summary": "Health and poverty in Thailand exhibit pronounced geographic structuring, yet the extent to which they operate as interconnected regional systems remains insufficiently understood. This study analyzes ICD-10 chapter-level morbidity and multidimensional poverty as outcomes embedded in a spatial interaction network. Interpreting Thailand's 76 provinces as nodes within a fixed-degree regional graph, we apply tools from spatial econometrics and social network analysis, including Moran's I, Local Indicators of Spatial Association (LISA), and Spatial Durbin Models (SDM), to assess spatial dependence and cross-provincial spillovers.\n  Our findings reveal strong spatial clustering across multiple ICD-10 chapters, with persistent high-high morbidity zones, particularly for digestive, respiratory, musculoskeletal, and symptom-based diseases, emerging in well-defined regional belts. SDM estimates demonstrate that spillover effects from neighboring provinces frequently exceed the influence of local deprivation, especially for living-condition, health-access, accessibility, and poor-household indicators. These patterns are consistent with contagion and contextual influence processes well established in social network theory.\n  By framing morbidity and poverty as interdependent attributes on a spatial network, this study contributes to the growing literature on structural diffusion, health inequality, and regional vulnerability. The results highlight the importance of coordinated policy interventions across provincial boundaries and demonstrate how network-based modeling can uncover the spatial dynamics of health and deprivation."}
{"id": "2601.02367", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.IR", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2601.02367", "abs": "https://arxiv.org/abs/2601.02367", "authors": ["Despoina Antonakaki", "Sotiris Ioannidis"], "title": "Cross-Platform Digital Discourse Analysis of the Israel-Hamas Conflict: Sentiment, Topics, and Event Dynamics", "comment": null, "summary": "The Israeli-Palestinian conflict remains one of the most polarizing geopolitical issues, with the October 2023 escalation intensifying online debate. Social media platforms, particularly Telegram, have become central to real-time news sharing, advocacy, and propaganda. In this study, we analyze Telegram, Twitter/X, and Reddit to examine how conflict narratives are produced, amplified, and contested across different digital spheres. Building on our previous work on Telegram discourse during the 2023 escalation, we extend the analysis longitudinally and cross-platform using an updated dataset spanning October 2023 to mid-2025. The corpus includes more than 187,000 Telegram messages, 2.1 million Reddit comments, and curated Twitter/X posts. We combine Latent Dirichlet Allocation (LDA), BERTopic, and transformer-based sentiment and emotion models to identify dominant themes, emotional dynamics, and propaganda strategies. Telegram channels provide unfiltered, high-intensity documentation of events; Twitter/X amplifies frames to global audiences; and Reddit hosts more reflective and deliberative discussions. Our findings reveal persistent negative sentiment, strong coupling between humanitarian framing and solidarity expressions, and platform-specific pathways for the diffusion of pro-Palestinian and pro-Israeli narratives. This paper offers three contributions: (1) a multi-platform, FAIR-compliant dataset on the Israel-Hamas war, (2) an integrated pipeline combining topic modeling, sentiment and emotion analysis, and spam filtering for large-scale conflict discourse, and (3) empirical insights into how platform affordances and affective publics shape the evolution of digital conflict communication."}
{"id": "2601.02367", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.IR", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2601.02367", "abs": "https://arxiv.org/abs/2601.02367", "authors": ["Despoina Antonakaki", "Sotiris Ioannidis"], "title": "Cross-Platform Digital Discourse Analysis of the Israel-Hamas Conflict: Sentiment, Topics, and Event Dynamics", "comment": null, "summary": "The Israeli-Palestinian conflict remains one of the most polarizing geopolitical issues, with the October 2023 escalation intensifying online debate. Social media platforms, particularly Telegram, have become central to real-time news sharing, advocacy, and propaganda. In this study, we analyze Telegram, Twitter/X, and Reddit to examine how conflict narratives are produced, amplified, and contested across different digital spheres. Building on our previous work on Telegram discourse during the 2023 escalation, we extend the analysis longitudinally and cross-platform using an updated dataset spanning October 2023 to mid-2025. The corpus includes more than 187,000 Telegram messages, 2.1 million Reddit comments, and curated Twitter/X posts. We combine Latent Dirichlet Allocation (LDA), BERTopic, and transformer-based sentiment and emotion models to identify dominant themes, emotional dynamics, and propaganda strategies. Telegram channels provide unfiltered, high-intensity documentation of events; Twitter/X amplifies frames to global audiences; and Reddit hosts more reflective and deliberative discussions. Our findings reveal persistent negative sentiment, strong coupling between humanitarian framing and solidarity expressions, and platform-specific pathways for the diffusion of pro-Palestinian and pro-Israeli narratives. This paper offers three contributions: (1) a multi-platform, FAIR-compliant dataset on the Israel-Hamas war, (2) an integrated pipeline combining topic modeling, sentiment and emotion analysis, and spam filtering for large-scale conflict discourse, and (3) empirical insights into how platform affordances and affective publics shape the evolution of digital conflict communication."}
{"id": "2601.02514", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02514", "abs": "https://arxiv.org/abs/2601.02514", "authors": ["Ahmad Terra", "Mohit Ahmed", "Rafia Inam", "Elena Fersman", "Martin TÃ¶rngren"], "title": "Textual Explanations and Their Evaluations for Reinforcement Learning Policy", "comment": null, "summary": "Understanding a Reinforcement Learning (RL) policy is crucial for ensuring that autonomous agents behave according to human expectations. This goal can be achieved using Explainable Reinforcement Learning (XRL) techniques. Although textual explanations are easily understood by humans, ensuring their correctness remains a challenge, and evaluations in state-of-the-art remain limited. We present a novel XRL framework for generating textual explanations, converting them into a set of transparent rules, improving their quality, and evaluating them. Expert's knowledge can be incorporated into this framework, and an automatic predicate generator is also proposed to determine the semantic information of a state. Textual explanations are generated using a Large Language Model (LLM) and a clustering technique to identify frequent conditions. These conditions are then converted into rules to evaluate their properties, fidelity, and performance in the deployed environment. Two refinement techniques are proposed to improve the quality of explanations and reduce conflicting information. Experiments were conducted in three open-source environments to enable reproducibility, and in a telecom use case to evaluate the industrial applicability of the proposed XRL framework. This framework addresses the limitations of an existing method, Autonomous Policy Explanation, and the generated transparent rules can achieve satisfactory performance on certain tasks. This framework also enables a systematic and quantitative evaluation of textual explanations, providing valuable insights for the XRL field."}
{"id": "2601.03050", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2601.03050", "abs": "https://arxiv.org/abs/2601.03050", "authors": ["Mei-Yun Hsu", "I-Hsien Ting", "Yun-Hsiu Liu", "Kazunori Minetaki"], "title": "An Empirical Study on User Profile Analysis and SEO Performance: A Case of Taiwan Cultural Memory Bank 2.0", "comment": null, "summary": "Taiwan Cultural Memory Bank 2.0 is an online curation platform that invites the public to become curators, fostering diverse perspectives on Taiwan's society, humanities, natural landscapes, and daily life. Built on a material bank concept, the platform encourages users to co-create and curate their own works using shared resources or self-uploaded materials. At its core, the system follows a collect, store, access, and reuse model, supporting dynamic engagement with over three million cultural memory items from Taiwan. Users can search, browse, explore stories, and engage in creative applications and collaborative productions. Understanding user profiles is crucial for enhancing website service quality, particularly within the framework of the Visitor Relationship Management model. This study conducts an empirical analysis of user profiles on the platform, examining demographic characteristics, browsing behaviors, and engagement patterns. Additionally, the research evaluates the platform's SEO performance, search visibility, and organic traffic effectiveness. Based on the findings, this study provides strategic recommendations for optimizing website management, improving user experience, and leveraging social media for enhanced digital outreach. The insights gained contribute to the broader discussion on digital cultural platforms and their role in audience engagement, online visibility, and networked communication."}
{"id": "2601.02370", "categories": ["cs.CY", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.02370", "abs": "https://arxiv.org/abs/2601.02370", "authors": ["Arnaldo Camuffo", "Alfonso Gambardella", "Saeid Kazemi", "Jakub Malachowski", "Abhinav Pandey"], "title": "LLM-as-evaluator in Strategy Research: A Normative, Variance-Aware Protocol", "comment": "61 pages, 16 pages for appendix", "summary": "Large language models (LLMs) are becoming essential tools for strategy scholars who need to evaluate text corpora at scale. This paper provides a systematic analysis of the reliability of LLM-as-evaluator in strategy research. After classifying the typical ways in which LLMs can be deployed for evaluation purposes in strategy research, we draw on the specialised AI literature to analyse their properties as measurement instruments. Our empirical analysis reveals substantial instability in LLMs' evaluation output, stemming from multiple factors: the specific phrasing of prompts, the context provided, sampling procedures, extraction methods, and disagreements across different models. We quantify these effects and demonstrate how this unreliability can compromise the validity of research inferences drawn from LLM-generated evaluations. To address these challenges, we develop a comprehensive protocol that is variance-aware, normative, and auditable. We provide practical guidance for flexible implementation of this protocol, including approaches to preregistration and transparent reporting. By establishing these methodological standards, we aim to elevate LLM-based evaluation of business text corpora from its current ad hoc status to a rigorous, actionable, and auditable measurement approach suitable for scholarly research."}
{"id": "2601.02370", "categories": ["cs.CY", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.02370", "abs": "https://arxiv.org/abs/2601.02370", "authors": ["Arnaldo Camuffo", "Alfonso Gambardella", "Saeid Kazemi", "Jakub Malachowski", "Abhinav Pandey"], "title": "LLM-as-evaluator in Strategy Research: A Normative, Variance-Aware Protocol", "comment": "61 pages, 16 pages for appendix", "summary": "Large language models (LLMs) are becoming essential tools for strategy scholars who need to evaluate text corpora at scale. This paper provides a systematic analysis of the reliability of LLM-as-evaluator in strategy research. After classifying the typical ways in which LLMs can be deployed for evaluation purposes in strategy research, we draw on the specialised AI literature to analyse their properties as measurement instruments. Our empirical analysis reveals substantial instability in LLMs' evaluation output, stemming from multiple factors: the specific phrasing of prompts, the context provided, sampling procedures, extraction methods, and disagreements across different models. We quantify these effects and demonstrate how this unreliability can compromise the validity of research inferences drawn from LLM-generated evaluations. To address these challenges, we develop a comprehensive protocol that is variance-aware, normative, and auditable. We provide practical guidance for flexible implementation of this protocol, including approaches to preregistration and transparent reporting. By establishing these methodological standards, we aim to elevate LLM-based evaluation of business text corpora from its current ad hoc status to a rigorous, actionable, and auditable measurement approach suitable for scholarly research."}
{"id": "2601.02553", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02553", "abs": "https://arxiv.org/abs/2601.02553", "authors": ["Jiaqi Liu", "Yaofeng Su", "Peng Xia", "Siwei Han", "Zeyu Zheng", "Cihang Xie", "Mingyu Ding", "Huaxiu Yao"], "title": "SimpleMem: Efficient Lifelong Memory for LLM Agents", "comment": null, "summary": "To support reliable long-term interaction in complex environments, LLM agents require memory systems that efficiently manage historical experiences. Existing approaches either retain full interaction histories via passive context extension, leading to substantial redundancy, or rely on iterative reasoning to filter noise, incurring high token costs. To address this challenge, we introduce SimpleMem, an efficient memory framework based on semantic lossless compression. We propose a three-stage pipeline designed to maximize information density and token utilization: (1) \\textit{Semantic Structured Compression}, which applies entropy-aware filtering to distill unstructured interactions into compact, multi-view indexed memory units; (2) \\textit{Recursive Memory Consolidation}, an asynchronous process that integrates related units into higher-level abstract representations to reduce redundancy; and (3) \\textit{Adaptive Query-Aware Retrieval}, which dynamically adjusts retrieval scope based on query complexity to construct precise context efficiently. Experiments on benchmark datasets show that our method consistently outperforms baseline approaches in accuracy, retrieval efficiency, and inference cost, achieving an average F1 improvement of 26.4% while reducing inference-time token consumption by up to 30-fold, demonstrating a superior balance between performance and efficiency. Code is available at https://github.com/aiming-lab/SimpleMem."}
{"id": "2601.03057", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2601.03057", "abs": "https://arxiv.org/abs/2601.03057", "authors": ["I-Hsien Ting", "Yen-Chih Chiu", "Yun-Hsiu Liu", "Kazunori Minetaki", "Chia-Sung Yen"], "title": "Exploring the Relationship Between Local Election Results and Online Public Opinion in Taiwan: A Case Study of Taitung County", "comment": null, "summary": "This study examines the relationship between online buzz and local election outcomes in Taiwan, with a focus on Taitung County. As social media becomes a major channel for public discourse, online buzz is increasingly seen as a factor influencing elections. However, its impact on local elections in Taiwan remains underexplored. This research addresses that gap through a comparative analysis of social media data and actual vote shares during the election period. A review of existing literature establishes the study's framework and highlights the need for empirical investigation in this area.\n  The findings aim to reveal whether online discussions align with electoral results and to what extent digital sentiment reflects voter behavior. The study also discusses methodological and data limitations that may affect interpretation. Beyond its academic value, the research offers practical insights into how online buzz can inform campaign strategies and enhance election predictions. By analyzing the Taitung County case, this study contributes to a deeper understanding of the role of online discourse in Taiwan's local elections and offers a foundation for future research in the field."}
{"id": "2601.02371", "categories": ["cs.CY", "cs.AI", "cs.MA", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.02371", "abs": "https://arxiv.org/abs/2601.02371", "authors": ["Samuele Marro", "Alan Chan", "Xinxing Ren", "Lewis Hammond", "Jesse Wright", "Gurjyot Wanga", "Tiziano Piccardi", "Nuno Campos", "Tobin South", "Jialin Yu", "Alex Pentland", "Philip Torr", "Jiaxin Pei"], "title": "Permission Manifests for Web Agents", "comment": "Authored by the Lightweight Agent Standards Working Group https://las-wg.org/", "summary": "The rise of Large Language Model (LLM)-based web agents represents a significant shift in automated interactions with the web. Unlike traditional crawlers that follow simple conventions, such as robots.txt, modern agents engage with websites in sophisticated ways: navigating complex interfaces, extracting structured information, and completing end-to-end tasks. Existing governance mechanisms were not designed for these capabilities. Without a way to specify what interactions are and are not allowed, website owners increasingly rely on blanket blocking and CAPTCHAs, which undermine beneficial applications such as efficient automation, convenient use of e-commerce services, and accessibility tools. We introduce agent-permissions.json, a robots.txt-style lightweight manifest where websites specify allowed interactions, complemented by API references where available. This framework provides a low-friction coordination mechanism: website owners only need to write a simple JSON file, while agents can easily parse and automatically implement the manifest's provisions. Website owners can then focus on blocking non-compliant agents, rather than agents as a whole. By extending the spirit of robots.txt to the era of LLM-mediated interaction, and complementing data use initiatives such as AIPref, the manifest establishes a compliance framework that enables beneficial agent interactions while respecting site owners' preferences."}
{"id": "2601.02371", "categories": ["cs.CY", "cs.AI", "cs.MA", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.02371", "abs": "https://arxiv.org/abs/2601.02371", "authors": ["Samuele Marro", "Alan Chan", "Xinxing Ren", "Lewis Hammond", "Jesse Wright", "Gurjyot Wanga", "Tiziano Piccardi", "Nuno Campos", "Tobin South", "Jialin Yu", "Alex Pentland", "Philip Torr", "Jiaxin Pei"], "title": "Permission Manifests for Web Agents", "comment": "Authored by the Lightweight Agent Standards Working Group https://las-wg.org/", "summary": "The rise of Large Language Model (LLM)-based web agents represents a significant shift in automated interactions with the web. Unlike traditional crawlers that follow simple conventions, such as robots.txt, modern agents engage with websites in sophisticated ways: navigating complex interfaces, extracting structured information, and completing end-to-end tasks. Existing governance mechanisms were not designed for these capabilities. Without a way to specify what interactions are and are not allowed, website owners increasingly rely on blanket blocking and CAPTCHAs, which undermine beneficial applications such as efficient automation, convenient use of e-commerce services, and accessibility tools. We introduce agent-permissions.json, a robots.txt-style lightweight manifest where websites specify allowed interactions, complemented by API references where available. This framework provides a low-friction coordination mechanism: website owners only need to write a simple JSON file, while agents can easily parse and automatically implement the manifest's provisions. Website owners can then focus on blocking non-compliant agents, rather than agents as a whole. By extending the spirit of robots.txt to the era of LLM-mediated interaction, and complementing data use initiatives such as AIPref, the manifest establishes a compliance framework that enables beneficial agent interactions while respecting site owners' preferences."}
{"id": "2601.02577", "categories": ["cs.AI", "astro-ph.IM", "hep-ph"], "pdf": "https://arxiv.org/pdf/2601.02577", "abs": "https://arxiv.org/abs/2601.02577", "authors": ["Alexander Roman", "Jacob Roman"], "title": "Orchestral AI: A Framework for Agent Orchestration", "comment": "17 pages, 3 figures. For more information visit https://orchestral-ai.com", "summary": "The rapid proliferation of LLM agent frameworks has forced developers to choose between vendor lock-in through provider-specific SDKs and complex multi-package ecosystems that obscure control flow and hinder reproducibility. Integrating tool calling across multiple LLM providers remains a core engineering challenge due to fragmented APIs, incompatible message formats, and inconsistent streaming and tool-calling behavior, making it difficult to build portable, reliable agent systems. We introduce Orchestral, a lightweight Python framework that provides a unified, type-safe interface for building LLM agents across major providers while preserving the simplicity required for scientific computing and production deployment. Orchestral defines a single universal representation for messages, tools, and LLM usage that operates seamlessly across providers, eliminating manual format translation and reducing framework-induced complexity. Automatic tool schema generation from Python type hints removes the need for handwritten descriptors while maintaining type safety across provider boundaries. A synchronous execution model with streaming support enables deterministic behavior, straightforward debugging, and real-time interaction without introducing server dependencies. The framework's modular architecture cleanly separates provider integration, tool execution, conversation orchestration, and user-facing interfaces, enabling extensibility without architectural entanglement. Orchestral supports advanced agent capabilities found in larger frameworks, including rich tool calling, context compaction, workspace sandboxing, user approval workflows, sub-agents, memory management, and MCP integration."}
{"id": "2601.02367", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.IR", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2601.02367", "abs": "https://arxiv.org/abs/2601.02367", "authors": ["Despoina Antonakaki", "Sotiris Ioannidis"], "title": "Cross-Platform Digital Discourse Analysis of the Israel-Hamas Conflict: Sentiment, Topics, and Event Dynamics", "comment": null, "summary": "The Israeli-Palestinian conflict remains one of the most polarizing geopolitical issues, with the October 2023 escalation intensifying online debate. Social media platforms, particularly Telegram, have become central to real-time news sharing, advocacy, and propaganda. In this study, we analyze Telegram, Twitter/X, and Reddit to examine how conflict narratives are produced, amplified, and contested across different digital spheres. Building on our previous work on Telegram discourse during the 2023 escalation, we extend the analysis longitudinally and cross-platform using an updated dataset spanning October 2023 to mid-2025. The corpus includes more than 187,000 Telegram messages, 2.1 million Reddit comments, and curated Twitter/X posts. We combine Latent Dirichlet Allocation (LDA), BERTopic, and transformer-based sentiment and emotion models to identify dominant themes, emotional dynamics, and propaganda strategies. Telegram channels provide unfiltered, high-intensity documentation of events; Twitter/X amplifies frames to global audiences; and Reddit hosts more reflective and deliberative discussions. Our findings reveal persistent negative sentiment, strong coupling between humanitarian framing and solidarity expressions, and platform-specific pathways for the diffusion of pro-Palestinian and pro-Israeli narratives. This paper offers three contributions: (1) a multi-platform, FAIR-compliant dataset on the Israel-Hamas war, (2) an integrated pipeline combining topic modeling, sentiment and emotion analysis, and spam filtering for large-scale conflict discourse, and (3) empirical insights into how platform affordances and affective publics shape the evolution of digital conflict communication."}
{"id": "2601.02375", "categories": ["cs.CY", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.02375", "abs": "https://arxiv.org/abs/2601.02375", "authors": ["Madison Bochard", "Tim Conser", "Alyssa Duran", "Lazaro Martull", "Pu Tian", "Yalong Wu"], "title": "LeafTutor: An AI Agent for Programming Assignment Tutoring", "comment": null, "summary": "High enrollment in STEM-related degree programs has created increasing demand for scalable tutoring support, as universities experience a shortage of qualified instructors and teaching assistants (TAs). To address this challenge, LeafTutor, an AI tutoring agent powered by large language models (LLMs), was developed to provide step-by-step guidance for students. LeafTutor was evaluated through real programming assignments. The results indicate that the system can deliver step-by-step programming guidance comparable to human tutors. This work demonstrates the potential of LLM-driven tutoring solutions to enhance and personalize learning in STEM education. If any reader is interested in collaboration with our team to improve or test LeafTutor, please contact Pu Tian (pu.tian@stockton.edu) or Yalong Wu (wuy@uhcl.edu)."}
{"id": "2601.02375", "categories": ["cs.CY", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.02375", "abs": "https://arxiv.org/abs/2601.02375", "authors": ["Madison Bochard", "Tim Conser", "Alyssa Duran", "Lazaro Martull", "Pu Tian", "Yalong Wu"], "title": "LeafTutor: An AI Agent for Programming Assignment Tutoring", "comment": null, "summary": "High enrollment in STEM-related degree programs has created increasing demand for scalable tutoring support, as universities experience a shortage of qualified instructors and teaching assistants (TAs). To address this challenge, LeafTutor, an AI tutoring agent powered by large language models (LLMs), was developed to provide step-by-step guidance for students. LeafTutor was evaluated through real programming assignments. The results indicate that the system can deliver step-by-step programming guidance comparable to human tutors. This work demonstrates the potential of LLM-driven tutoring solutions to enhance and personalize learning in STEM education. If any reader is interested in collaboration with our team to improve or test LeafTutor, please contact Pu Tian (pu.tian@stockton.edu) or Yalong Wu (wuy@uhcl.edu)."}
{"id": "2601.02641", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02641", "abs": "https://arxiv.org/abs/2601.02641", "authors": ["Jeiyoon Park", "Daehwan Lee", "Changmin Yeo", "Yongshin Han", "Minseop Kim"], "title": "An Empirical Study of On-Device Translation for Real-Time Live-Stream Chat on Mobile Devices", "comment": "preprint", "summary": "Despite its efficiency, there has been little research on the practical aspects required for real-world deployment of on-device AI models, such as the device's CPU utilization and thermal conditions. In this paper, through extensive experiments, we investigate two key issues that must be addressed to deploy on-device models in real-world services: (i) the selection of on-device models and the resource consumption of each model, and (ii) the capability and potential of on-device models for domain adaptation. To this end, we focus on a task of translating live-stream chat messages and manually construct LiveChatBench, a benchmark consisting of 1,000 Korean-English parallel sentence pairs. Experiments on five mobile devices demonstrate that, although serving a large and heterogeneous user base requires careful consideration of highly constrained deployment settings and model selection, the proposed approach nevertheless achieves performance comparable to commercial models such as GPT-5.1 on the well-targeted task. We expect that our findings will provide meaningful insights to the on-device AI community."}
{"id": "2601.03105", "categories": ["stat.AP", "cs.MA", "cs.SI", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2601.03105", "abs": "https://arxiv.org/abs/2601.03105", "authors": ["Abdulrahman A. Ahmed", "M. Amin Rahimian", "Qiushi Chen", "Praveen Kumar"], "title": "Computationally Efficient Estimation of Localized Treatment Effects in High-Dimensional Design Spaces using Gaussian Process Regression", "comment": "repository link: https://github.com/abdulrahmanfci/gpr-metamodel/", "summary": "Population-scale agent-based simulations of the opioid epidemic help evaluate intervention strategies and overdose outcomes in heterogeneous communities and provide estimates of localized treatment effects, which support the design of locally-tailored policies for precision public health. However, it is prohibitively costly to run simulations of all treatment conditions in all communities because the number of possible treatments grows exponentially with the number of interventions and levels at which they are applied. To address this need efficiently, we develop a metamodel framework, whereby treatment outcomes are modeled using a response function whose coefficients are learned through Gaussian process regression (GPR) on locally-contextualized covariates. We apply this framework to efficiently estimate treatment effects on overdose deaths in Pennsylvania counties. In contrast to classical designs such as fractional factorial design or Latin hypercube sampling, our approach leverages spatial correlations and posterior uncertainty to sequentially sample the most informative counties and treatment conditions. Using a calibrated agent-based opioid epidemic model, informed by county-level overdose mortality and baseline dispensing rate data for different treatments, we obtained county-level estimates of treatment effects on overdose deaths per 100,000 population for all treatment conditions in Pennsylvania, achieving approximately 5% average relative error using one-tenth the number of simulation runs required for exhaustive evaluation. Our bi-level framework provides a computationally efficient approach to decision support for policy makers, enabling rapid evaluation of alternative resource-allocation strategies to mitigate the opioid epidemic in local communities. The same analytical framework can be applied to guide precision public health interventions in other epidemic settings."}
{"id": "2601.02380", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02380", "abs": "https://arxiv.org/abs/2601.02380", "authors": ["Elchanan Mossel"], "title": "The Refutability Gap: Challenges in Validating Reasoning by Large Language Models", "comment": "he authors explicitly reserve all rights in this work. No permission is granted for the reproduction, storage, or use of this document for the purpose of training artificial intelligence systems or for text and data mining (TDM), including but not limited to the generation of embeddings, summaries, or synthetic derivatives", "summary": "Recent reports claim that Large Language Models (LLMs) have achieved the ability to derive new science and exhibit human-level general intelligence. We argue that such claims are not rigorous scientific claims, as they do not satisfy Popper's refutability principle (often termed falsifiability), which requires that scientific statements be capable of being disproven. We identify several methodological pitfalls in current AI research on reasoning, including the inability to verify the novelty of findings due to opaque and non-searchable training data, the lack of reproducibility caused by continuous model updates, and the omission of human-interaction transcripts, which obscures the true source of scientific discovery. Additionally, the absence of counterfactuals and data on failed attempts creates a selection bias that may exaggerate LLM capabilities. To address these challenges, we propose guidelines for scientific transparency and reproducibility for research on reasoning by LLMs. Establishing such guidelines is crucial for both scientific integrity and the ongoing societal debates regarding fair data usage."}
{"id": "2601.02380", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02380", "abs": "https://arxiv.org/abs/2601.02380", "authors": ["Elchanan Mossel"], "title": "The Refutability Gap: Challenges in Validating Reasoning by Large Language Models", "comment": "he authors explicitly reserve all rights in this work. No permission is granted for the reproduction, storage, or use of this document for the purpose of training artificial intelligence systems or for text and data mining (TDM), including but not limited to the generation of embeddings, summaries, or synthetic derivatives", "summary": "Recent reports claim that Large Language Models (LLMs) have achieved the ability to derive new science and exhibit human-level general intelligence. We argue that such claims are not rigorous scientific claims, as they do not satisfy Popper's refutability principle (often termed falsifiability), which requires that scientific statements be capable of being disproven. We identify several methodological pitfalls in current AI research on reasoning, including the inability to verify the novelty of findings due to opaque and non-searchable training data, the lack of reproducibility caused by continuous model updates, and the omission of human-interaction transcripts, which obscures the true source of scientific discovery. Additionally, the absence of counterfactuals and data on failed attempts creates a selection bias that may exaggerate LLM capabilities. To address these challenges, we propose guidelines for scientific transparency and reproducibility for research on reasoning by LLMs. Establishing such guidelines is crucial for both scientific integrity and the ongoing societal debates regarding fair data usage."}
{"id": "2601.02643", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02643", "abs": "https://arxiv.org/abs/2601.02643", "authors": ["Mehmet Kurmaz"], "title": "AWARE-US: Benchmark for Preference-Aware Resolution in Tool-Calling Agents", "comment": "19 pages, 2 figures, 6 tables", "summary": "Tool-calling conversational agents querying structured databases often face two linked failures: underspecification (missing constraints needed to run a precise query) and infeasibility (the fully specified query returns an empty set because no item satisfies all constraints). Existing work often responds with \"no results\" or relaxes constraints using ad hoc rules, which can violate user intent by discarding requirements the user cares about most. We frame infeasibility handling as a preference-aware query repair problem: when a query is unsatisfiable, the agent should relax the least important constraints to the user. We propose three LLM-based methods for inferring relative constraint importance from dialogue: (1) local weighting, (2) global one-shot weighting, and (3) pairwise ranking. Experiments show local weighting achieves the best preference alignment, while global weighting performs best on correct constraint relaxation. We also introduce AWARE-US, a benchmark of persona-grounded queries requiring agents to disambiguate requests via conversation and resolve infeasibility in a way consistent with persona-implied preferences."}
{"id": "2601.02383", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2601.02383", "abs": "https://arxiv.org/abs/2601.02383", "authors": ["Lucia Velasco", "Charles Martinet", "Henry de Zoete", "Robert Trager", "Duncan Snidal", "Ben Garfinkel", "Kwan Yee Ng", "Haydn Belfield", "Don Wallace", "Yoshua Bengio", "Benjamin Prud'homme", "Brian Tse", "Roxana Radu", "Ranjit Lall", "Ben Harack", "Julia Morse", "Nicolas Miailhe", "Scott Singer", "Matt Sheehan", "Max Stauffer", "Yi Zeng", "Joslyn Barnhart", "Imane Bello", "Xue Lan", "Oliver Guest", "Duncan Cass-Beggs", "Lu Chuanying", "Sumaya Nur Adan", "Markus Anderljung", "Claire Dennis"], "title": "The Future of the AI Summit Series", "comment": "Policy memo, 39 pages, includes tables. Prepared by the Oxford Martin AI Governance Initiative", "summary": "This policy memo examines the evolution of the international AI Summit series, initiated at Bletchley Park in 2023 and continued through Seoul in 2024 and Paris in 2025, as a forum for cooperation on the governance of advanced artificial intelligence. It analyzes the factors underpinning the series' early successes and assesses challenges related to scope, participation, continuity, and institutional design. Drawing on comparisons with existing international governance models, the memo evaluates options for hosting arrangements, secretariat formats, participant selection, agenda setting, and meeting frequency. It proposes a set of design recommendations aimed at preserving the series' focus on advanced AI governance while balancing inclusivity, effectiveness, and long-term sustainability."}
{"id": "2601.02383", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2601.02383", "abs": "https://arxiv.org/abs/2601.02383", "authors": ["Lucia Velasco", "Charles Martinet", "Henry de Zoete", "Robert Trager", "Duncan Snidal", "Ben Garfinkel", "Kwan Yee Ng", "Haydn Belfield", "Don Wallace", "Yoshua Bengio", "Benjamin Prud'homme", "Brian Tse", "Roxana Radu", "Ranjit Lall", "Ben Harack", "Julia Morse", "Nicolas Miailhe", "Scott Singer", "Matt Sheehan", "Max Stauffer", "Yi Zeng", "Joslyn Barnhart", "Imane Bello", "Xue Lan", "Oliver Guest", "Duncan Cass-Beggs", "Lu Chuanying", "Sumaya Nur Adan", "Markus Anderljung", "Claire Dennis"], "title": "The Future of the AI Summit Series", "comment": "Policy memo, 39 pages, includes tables. Prepared by the Oxford Martin AI Governance Initiative", "summary": "This policy memo examines the evolution of the international AI Summit series, initiated at Bletchley Park in 2023 and continued through Seoul in 2024 and Paris in 2025, as a forum for cooperation on the governance of advanced artificial intelligence. It analyzes the factors underpinning the series' early successes and assesses challenges related to scope, participation, continuity, and institutional design. Drawing on comparisons with existing international governance models, the memo evaluates options for hosting arrangements, secretariat formats, participant selection, agenda setting, and meeting frequency. It proposes a set of design recommendations aimed at preserving the series' focus on advanced AI governance while balancing inclusivity, effectiveness, and long-term sustainability."}
{"id": "2601.02666", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.02666", "abs": "https://arxiv.org/abs/2601.02666", "authors": ["Hadi Partovi Aria", "Zhe Xu"], "title": "Inferring Causal Graph Temporal Logic Formulas to Expedite Reinforcement Learning in Temporally Extended Tasks", "comment": "Accepted to AAAI-26 Bridge Program B10: Making Embodied AI Reliable with Testing and Formal Verification", "summary": "Decision-making tasks often unfold on graphs with spatial-temporal dynamics. Black-box reinforcement learning often overlooks how local changes spread through network structure, limiting sample efficiency and interpretability. We present GTL-CIRL, a closed-loop framework that simultaneously learns policies and mines Causal Graph Temporal Logic (Causal GTL) specifications. The method shapes rewards with robustness, collects counterexamples when effects fail, and uses Gaussian Process (GP) driven Bayesian optimization to refine parameterized cause templates. The GP models capture spatial and temporal correlations in the system dynamics, enabling efficient exploration of complex parameter spaces. Case studies in gene and power networks show faster learning and clearer, verifiable behavior compared to standard RL baselines."}
{"id": "2601.02384", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2601.02384", "abs": "https://arxiv.org/abs/2601.02384", "authors": ["Wisnu Uriawan", "Muhammad Farhan Tarigan", "Herdin Kristianjani Zebua", "Muhamad Nopid Andriansyah", "Marleni Sukarya", "Muhammad Rafli Haikal"], "title": "E-commerce Transactions in Islam: Fiqh Muamalah on The Validity of Buying and Selling on Digital Platforms", "comment": null, "summary": "The development of the digital economy has established e-commerce platforms as the primary space for commercial transactions for the Muslim community. However, innovations in features and business models on these platforms have gave rise to Sharia issues that cannot be fully explained through conventional Fiqh Muamalah contract frameworks. This research aims to examine the compliance of transaction practices in e-commerce with Sharia principles, particularly in the six most frequently used transaction forms, namely information arbitrage-based dropshipping, Buy Now Pay Later financing schemes, digital representations, algorithmic marketing that encourages consumptive behavior, halal verification, and Pre-Order systems. The research method used is a Critical Literature Review with a normative juridical approach, through the study of arguments from the Qur'an, Hadith, DSN-MUI Fatwas, as well as classical and contemporary fiqh literature. The results show that dropshipping and PO practices are considered invalid if conducted with a direct sale contract (bai') due to the nonfulfillment of the element of possession (qabd) and the presence of high uncertainty (gharar). Both practices can be justified through the restructuring of contracts into wakalah bil ujrah, salam, or istishna'. Conventional BNPL is declared non-compliant with Sharia because it contains riba nasiah and riba qardh. Misleading digital representations and halal claims without valid verification fall into the category of tadlis, while dark patterns based algorithmic marketing contradicts maqashid al-syariah, especially the protection of wealth (hifz al-mal) and intellect (hifz al-'aql). This research emphasizes the need for a comprehensive Sharia audit covering contract legality, algorithmic ethics, and interface design to realize a digital economic ecosystem that is fair, transparent, and in accordance with Islamic Sharia."}
{"id": "2601.02384", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2601.02384", "abs": "https://arxiv.org/abs/2601.02384", "authors": ["Wisnu Uriawan", "Muhammad Farhan Tarigan", "Herdin Kristianjani Zebua", "Muhamad Nopid Andriansyah", "Marleni Sukarya", "Muhammad Rafli Haikal"], "title": "E-commerce Transactions in Islam: Fiqh Muamalah on The Validity of Buying and Selling on Digital Platforms", "comment": null, "summary": "The development of the digital economy has established e-commerce platforms as the primary space for commercial transactions for the Muslim community. However, innovations in features and business models on these platforms have gave rise to Sharia issues that cannot be fully explained through conventional Fiqh Muamalah contract frameworks. This research aims to examine the compliance of transaction practices in e-commerce with Sharia principles, particularly in the six most frequently used transaction forms, namely information arbitrage-based dropshipping, Buy Now Pay Later financing schemes, digital representations, algorithmic marketing that encourages consumptive behavior, halal verification, and Pre-Order systems. The research method used is a Critical Literature Review with a normative juridical approach, through the study of arguments from the Qur'an, Hadith, DSN-MUI Fatwas, as well as classical and contemporary fiqh literature. The results show that dropshipping and PO practices are considered invalid if conducted with a direct sale contract (bai') due to the nonfulfillment of the element of possession (qabd) and the presence of high uncertainty (gharar). Both practices can be justified through the restructuring of contracts into wakalah bil ujrah, salam, or istishna'. Conventional BNPL is declared non-compliant with Sharia because it contains riba nasiah and riba qardh. Misleading digital representations and halal claims without valid verification fall into the category of tadlis, while dark patterns based algorithmic marketing contradicts maqashid al-syariah, especially the protection of wealth (hifz al-mal) and intellect (hifz al-'aql). This research emphasizes the need for a comprehensive Sharia audit covering contract legality, algorithmic ethics, and interface design to realize a digital economic ecosystem that is fair, transparent, and in accordance with Islamic Sharia."}
{"id": "2601.02683", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02683", "abs": "https://arxiv.org/abs/2601.02683", "authors": ["Dongyu Chen", "Jian Ma", "Xianpeng Zhang", "Lei Zhang", "Haonan Lu", "Chen Chen", "Chuangchuang Wang", "Kai Tang"], "title": "Learning from Prompt itself: the Hierarchical Attribution Prompt Optimization", "comment": null, "summary": "Optimization is fundamental across numerous disciplines, typically following an iterative process of refining an initial solution to enhance performance. This principle is equally critical in prompt engineering, where designing effective prompts for large language models constitutes a complex optimization challenge. A structured optimization approach requires automated or semi-automated procedures to develop improved prompts, thereby reducing manual effort, improving performance, and yielding an interpretable process. However, current prompt optimization methods often induce prompt drift, where new prompts fix prior failures but impair performance on previously successful tasks. Additionally, generating prompts from scratch can compromise interpretability. To address these limitations, this study proposes the Hierarchical Attribution Prompt Optimization (HAPO) framework, which introduces three innovations: (1) a dynamic attribution mechanism targeting error patterns in training data and prompting history, (2) semantic-unit optimization for editing functional prompt segments, and (3) multimodal-friendly progression supporting both end-to-end LLM and LLM-MLLM workflows. Applied in contexts like single/multi-image QA (e.g., OCRV2) and complex task analysis (e.g., BBH), HAPO demonstrates enhanced optimization efficiency, outperforming comparable automated prompt optimization methods and establishing an extensible paradigm for scalable prompt engineering."}
{"id": "2601.02631", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2601.02631", "abs": "https://arxiv.org/abs/2601.02631", "authors": ["Anirban Mukherjee", "Hannah Hanwen Chang"], "title": "Copyright Laundering Through the AI Ouroboros: Adapting the 'Fruit of the Poisonous Tree' Doctrine to Recursive AI Training", "comment": null, "summary": "Copyright enforcement rests on an evidentiary bargain: a plaintiff must show both the defendant's access to the work and substantial similarity in the challenged output. That bargain comes under strain when AI systems are trained through multi-generational pipelines with recursive synthetic data. As successive models are tuned on the outputs of its predecessors, any copyrighted material absorbed by an early model is diffused into deeper statistical abstractions. The result is an evidentiary blind spot where overlaps that emerge look coincidental, while the chain of provenance is too attenuated to trace. These conditions are ripe for \"copyright laundering\"--the use of multi-generational synthetic pipelines, an \"AI Ouroboros,\" to render traditional proof of infringement impracticable. This Article adapts the \"fruit of the poisonous tree\" (FOPT) principle to propose a AI-FOPT standard: if a foundational AI model's training is adjudged infringing (either for unlawful sourcing or for non-transformative ingestion that fails fair-use), then subsequent AI models principally derived from the foundational model's outputs or distilled weights carry a rebuttable presumption of taint. The burden shifts to downstream developers--those who control the evidence of provenance--to restore the evidentiary bargain by affirmatively demonstrating a verifiably independent and lawfully sourced lineage or a curative rebuild, without displacing fair-use analysis at the initial ingestion stage. Absent such proof, commercial deployment of tainted models and their outputs is actionable. This Article develops the standard by specifying its trigger, presumption, and concrete rebuttal paths (e.g., independent lineage or verifiable unlearning); addresses counterarguments concerning chilling innovation and fair use; and demonstrates why this lineage-focused approach is both administrable and essential."}
{"id": "2601.02514", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02514", "abs": "https://arxiv.org/abs/2601.02514", "authors": ["Ahmad Terra", "Mohit Ahmed", "Rafia Inam", "Elena Fersman", "Martin TÃ¶rngren"], "title": "Textual Explanations and Their Evaluations for Reinforcement Learning Policy", "comment": null, "summary": "Understanding a Reinforcement Learning (RL) policy is crucial for ensuring that autonomous agents behave according to human expectations. This goal can be achieved using Explainable Reinforcement Learning (XRL) techniques. Although textual explanations are easily understood by humans, ensuring their correctness remains a challenge, and evaluations in state-of-the-art remain limited. We present a novel XRL framework for generating textual explanations, converting them into a set of transparent rules, improving their quality, and evaluating them. Expert's knowledge can be incorporated into this framework, and an automatic predicate generator is also proposed to determine the semantic information of a state. Textual explanations are generated using a Large Language Model (LLM) and a clustering technique to identify frequent conditions. These conditions are then converted into rules to evaluate their properties, fidelity, and performance in the deployed environment. Two refinement techniques are proposed to improve the quality of explanations and reduce conflicting information. Experiments were conducted in three open-source environments to enable reproducibility, and in a telecom use case to evaluate the industrial applicability of the proposed XRL framework. This framework addresses the limitations of an existing method, Autonomous Policy Explanation, and the generated transparent rules can achieve satisfactory performance on certain tasks. This framework also enables a systematic and quantitative evaluation of textual explanations, providing valuable insights for the XRL field."}
{"id": "2601.02702", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02702", "abs": "https://arxiv.org/abs/2601.02702", "authors": ["Shuhaib Mehri", "Priyanka Kargupta", "Tal August", "Dilek Hakkani-TÃ¼r"], "title": "Learning User Preferences Through Interaction for Long-Term Collaboration", "comment": null, "summary": "As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings."}
{"id": "2601.02633", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2601.02633", "abs": "https://arxiv.org/abs/2601.02633", "authors": ["Anirban Mukherjee", "Hannah Hanwen Chang"], "title": "Fluid Agency in AI Systems: A Case for Functional Equivalence in Copyright, Patent, and Tort", "comment": null, "summary": "Modern Artificial Intelligence (AI) systems lack human-like consciousness or culpability, yet they exhibit fluid agency: behavior that is (i) stochastic (probabilistic and path-dependent), (ii) dynamic (co-evolving with user interaction), and (iii) adaptive (able to reorient across contexts). Fluid agency generates valuable outputs but collapses attribution, irreducibly entangling human and machine inputs. This fundamental unmappability fractures doctrines that assume traceable provenance--authorship, inventorship, and liability--yielding ownership gaps and moral \"crumple zones.\" This Article argues that only functional equivalence stabilizes doctrine. Where provenance is indeterminate, legal frameworks must treat human and AI contributions as equivalent for allocating rights and responsibility--not as a claim of moral or economic parity but as a pragmatic default. This principle stabilizes doctrine across domains, offering administrable rules: in copyright, vesting ownership in human orchestrators without parsing inseparable contributions; in patent, tying inventor-of-record status to human orchestration and reduction to practice, even when AI supplies the pivotal insight; and in tort, replacing intractable causation inquiries with enterprise-level and sector-specific strict or no-fault schemes. The contribution is both descriptive and normative: fluid agency explains why origin-based tests fail, while functional equivalence supplies an outcome-focused framework to allocate rights and responsibility when attribution collapses."}
{"id": "2601.02553", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02553", "abs": "https://arxiv.org/abs/2601.02553", "authors": ["Jiaqi Liu", "Yaofeng Su", "Peng Xia", "Siwei Han", "Zeyu Zheng", "Cihang Xie", "Mingyu Ding", "Huaxiu Yao"], "title": "SimpleMem: Efficient Lifelong Memory for LLM Agents", "comment": null, "summary": "To support reliable long-term interaction in complex environments, LLM agents require memory systems that efficiently manage historical experiences. Existing approaches either retain full interaction histories via passive context extension, leading to substantial redundancy, or rely on iterative reasoning to filter noise, incurring high token costs. To address this challenge, we introduce SimpleMem, an efficient memory framework based on semantic lossless compression. We propose a three-stage pipeline designed to maximize information density and token utilization: (1) \\textit{Semantic Structured Compression}, which applies entropy-aware filtering to distill unstructured interactions into compact, multi-view indexed memory units; (2) \\textit{Recursive Memory Consolidation}, an asynchronous process that integrates related units into higher-level abstract representations to reduce redundancy; and (3) \\textit{Adaptive Query-Aware Retrieval}, which dynamically adjusts retrieval scope based on query complexity to construct precise context efficiently. Experiments on benchmark datasets show that our method consistently outperforms baseline approaches in accuracy, retrieval efficiency, and inference cost, achieving an average F1 improvement of 26.4% while reducing inference-time token consumption by up to 30-fold, demonstrating a superior balance between performance and efficiency. Code is available at https://github.com/aiming-lab/SimpleMem."}
{"id": "2601.02714", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.02714", "abs": "https://arxiv.org/abs/2601.02714", "authors": ["Zhi Liu", "Guangzhi Wang"], "title": "Time-Scaling Is What Agents Need Now", "comment": null, "summary": "Early artificial intelligence paradigms exhibited separated cognitive functions: Neural Networks focused on \"perception-representation,\" Reinforcement Learning on \"decision-making-behavior,\" and Symbolic AI on \"knowledge-reasoning.\" With Transformer-based large models and world models, these paradigms are converging into cognitive agents with closed-loop \"perception-decision-action\" capabilities.\n  Humans solve complex problems under limited cognitive resources through temporalized sequential reasoning. Language relies on problem space search for deep semantic reasoning. While early large language models (LLMs) could generate fluent text, they lacked robust semantic reasoning capabilities. Prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) extended reasoning paths by making intermediate steps explicit. Recent models like DeepSeek-R1 enhanced performance through explicit reasoning trajectories. However, these methods have limitations in search completeness and efficiency.\n  This highlights the need for \"Time-Scaling\"--the systematic extension and optimization of an agent's ability to unfold reasoning over time. Time-Scaling refers to architectural design utilizing extended temporal pathways, enabling deeper problem space exploration, dynamic strategy adjustment, and enhanced metacognitive control, paralleling human sequential reasoning under cognitive constraints. It represents a critical frontier for enhancing deep reasoning and problem-solving without proportional increases in static model parameters. Advancing intelligent agent capabilities requires placing Time-Scaling principles at the forefront, positioning explicit temporal reasoning management as foundational."}
{"id": "2601.02651", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2601.02651", "abs": "https://arxiv.org/abs/2601.02651", "authors": ["Savvy Barnes", "Maricarmen Davis", "Josh Siegel"], "title": "Driving Accessibility: Shifting the Narrative & Design of Automated Vehicle Systems for Persons With Disabilities Through a Collaborative Scoring System", "comment": "Submitted to ACM TACCESS February 23rd, 2025 - In Review - Final paper may not match the contents of this submission", "summary": "Automated vehicles present unique opportunities and challenges, with progress and adoption limited, in part, by policy and regulatory barriers. Underrepresented groups, including individuals with mobility impairments, sensory disabilities, and cognitive conditions, who may benefit most from automation, are often overlooked in crucial discussions on system design, implementation, and usability. Despite the high potential benefits of automated vehicles, the needs of Persons with Disabilities are frequently an afterthought, considered only in terms of secondary accommodations rather than foundational design elements. We aim to shift automated vehicle research and discourse away from this reactive model and toward a proactive and inclusive approach. We first present an overview of the current state of automated vehicle systems. Regarding their adoption, we examine social and technical barriers and advantages for Persons with Disabilities. We analyze existing regulations and policies concerning automated vehicles and Persons with Disabilities, identifying gaps that hinder accessibility. To address these deficiencies, we introduce a scoring rubric intended for use by manufacturers and vehicle designers. The rubric fosters direct collaboration throughout the design process, moving beyond an `afterthought` approach and towards intentional, inclusive innovation. This work was created by authors with varying degrees of personal experience within the realm of disability."}
{"id": "2601.02577", "categories": ["cs.AI", "astro-ph.IM", "hep-ph"], "pdf": "https://arxiv.org/pdf/2601.02577", "abs": "https://arxiv.org/abs/2601.02577", "authors": ["Alexander Roman", "Jacob Roman"], "title": "Orchestral AI: A Framework for Agent Orchestration", "comment": "17 pages, 3 figures. For more information visit https://orchestral-ai.com", "summary": "The rapid proliferation of LLM agent frameworks has forced developers to choose between vendor lock-in through provider-specific SDKs and complex multi-package ecosystems that obscure control flow and hinder reproducibility. Integrating tool calling across multiple LLM providers remains a core engineering challenge due to fragmented APIs, incompatible message formats, and inconsistent streaming and tool-calling behavior, making it difficult to build portable, reliable agent systems. We introduce Orchestral, a lightweight Python framework that provides a unified, type-safe interface for building LLM agents across major providers while preserving the simplicity required for scientific computing and production deployment. Orchestral defines a single universal representation for messages, tools, and LLM usage that operates seamlessly across providers, eliminating manual format translation and reducing framework-induced complexity. Automatic tool schema generation from Python type hints removes the need for handwritten descriptors while maintaining type safety across provider boundaries. A synchronous execution model with streaming support enables deterministic behavior, straightforward debugging, and real-time interaction without introducing server dependencies. The framework's modular architecture cleanly separates provider integration, tool execution, conversation orchestration, and user-facing interfaces, enabling extensibility without architectural entanglement. Orchestral supports advanced agent capabilities found in larger frameworks, including rich tool calling, context compaction, workspace sandboxing, user approval workflows, sub-agents, memory management, and MCP integration."}
{"id": "2601.02749", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02749", "abs": "https://arxiv.org/abs/2601.02749", "authors": ["Nadia Sibai", "Yara Ahmed", "Serry Sibaee", "Sawsan AlHalawani", "Adel Ammar", "Wadii Boulila"], "title": "The Path Ahead for Agentic AI: Challenges and Opportunities", "comment": null, "summary": "The evolution of Large Language Models (LLMs) from passive text generators to autonomous, goal-driven systems represents a fundamental shift in artificial intelligence. This chapter examines the emergence of agentic AI systems that integrate planning, memory, tool use, and iterative reasoning to operate autonomously in complex environments. We trace the architectural progression from statistical models to transformer-based systems, identifying capabilities that enable agentic behavior: long-range reasoning, contextual awareness, and adaptive decision-making. The chapter provides three contributions: (1) a synthesis of how LLM capabilities extend toward agency through reasoning-action-reflection loops; (2) an integrative framework describing core components perception, memory, planning, and tool execution that bridge LLMs with autonomous behavior; (3) a critical assessment of applications and persistent challenges in safety, alignment, reliability, and sustainability. Unlike existing surveys, we focus on the architectural transition from language understanding to autonomous action, emphasizing the technical gaps that must be resolved before deployment. We identify critical research priorities, including verifiable planning, scalable multi-agent coordination, persistent memory architectures, and governance frameworks. Responsible advancement requires simultaneous progress in technical robustness, interpretability, and ethical safeguards to realize potential while mitigating risks of misalignment and unintended consequences."}
{"id": "2601.02773", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2601.02773", "abs": "https://arxiv.org/abs/2601.02773", "authors": ["Simon Chesterman"], "title": "From Slaves to Synths? Superintelligence and the Evolution of Legal Personality", "comment": "19 pages", "summary": "This essay examines the evolving concept of legal personality through the lens of recent developments in artificial intelligence and the possible emergence of superintelligence. Legal systems have long been open to extending personhood to non-human entities, most prominently corporations, for instrumental or inherent reasons. Instrumental rationales emphasize accountability and administrative efficiency, whereas inherent ones appeal to moral worth and autonomy. Neither is yet sufficient to justify conferring personhood on AI. Nevertheless, the acceleration of technological autonomy may lead us to reconsider how law conceptualizes agency and responsibility. Drawing on comparative jurisprudence, corporate theory, and the emerging literature on AI governance, the paper argues that existing frameworks can address short-term accountability gaps, but the eventual development of superintelligence may force a paradigmatic shift in our understanding of law itself. In such a speculative future, legal personality may depend less on the cognitive sophistication of machines than on humanity's ability to preserve our own moral and institutional sovereignty."}
{"id": "2601.02631", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2601.02631", "abs": "https://arxiv.org/abs/2601.02631", "authors": ["Anirban Mukherjee", "Hannah Hanwen Chang"], "title": "Copyright Laundering Through the AI Ouroboros: Adapting the 'Fruit of the Poisonous Tree' Doctrine to Recursive AI Training", "comment": null, "summary": "Copyright enforcement rests on an evidentiary bargain: a plaintiff must show both the defendant's access to the work and substantial similarity in the challenged output. That bargain comes under strain when AI systems are trained through multi-generational pipelines with recursive synthetic data. As successive models are tuned on the outputs of its predecessors, any copyrighted material absorbed by an early model is diffused into deeper statistical abstractions. The result is an evidentiary blind spot where overlaps that emerge look coincidental, while the chain of provenance is too attenuated to trace. These conditions are ripe for \"copyright laundering\"--the use of multi-generational synthetic pipelines, an \"AI Ouroboros,\" to render traditional proof of infringement impracticable. This Article adapts the \"fruit of the poisonous tree\" (FOPT) principle to propose a AI-FOPT standard: if a foundational AI model's training is adjudged infringing (either for unlawful sourcing or for non-transformative ingestion that fails fair-use), then subsequent AI models principally derived from the foundational model's outputs or distilled weights carry a rebuttable presumption of taint. The burden shifts to downstream developers--those who control the evidence of provenance--to restore the evidentiary bargain by affirmatively demonstrating a verifiably independent and lawfully sourced lineage or a curative rebuild, without displacing fair-use analysis at the initial ingestion stage. Absent such proof, commercial deployment of tainted models and their outputs is actionable. This Article develops the standard by specifying its trigger, presumption, and concrete rebuttal paths (e.g., independent lineage or verifiable unlearning); addresses counterarguments concerning chilling innovation and fair use; and demonstrates why this lineage-focused approach is both administrable and essential."}
{"id": "2601.02757", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02757", "abs": "https://arxiv.org/abs/2601.02757", "authors": ["Zixuan Xiao", "Jun Ma"], "title": "LLM Agent Framework for Intelligent Change Analysis in Urban Environment using Remote Sensing Imagery", "comment": null, "summary": "Existing change detection methods often lack the versatility to handle diverse real-world queries and the intelligence for comprehensive analysis. This paper presents a general agent framework, integrating Large Language Models (LLM) with vision foundation models to form ChangeGPT. A hierarchical structure is employed to mitigate hallucination. The agent was evaluated on a curated dataset of 140 questions categorized by real-world scenarios, encompassing various question types (e.g., Size, Class, Number) and complexities. The evaluation assessed the agent's tool selection ability (Precision/Recall) and overall query accuracy (Match). ChangeGPT, especially with a GPT-4-turbo backend, demonstrated superior performance, achieving a 90.71 % Match rate. Its strength lies particularly in handling change-related queries requiring multi-step reasoning and robust tool selection. Practical effectiveness was further validated through a real-world urban change monitoring case study in Qianhai Bay, Shenzhen. By providing intelligence, adaptability, and multi-type change analysis, ChangeGPT offers a powerful solution for decision-making in remote sensing applications."}
{"id": "2601.03061", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2601.03061", "abs": "https://arxiv.org/abs/2601.03061", "authors": ["Felipe M. Affonso"], "title": "Vertical tacit collusion in AI-mediated markets", "comment": null, "summary": "AI shopping agents are being deployed to hundreds of millions of consumers, creating a new intermediary between platforms, sellers, and buyers. We identify a novel market failure: vertical tacit collusion, where platforms controlling rankings and sellers controlling product descriptions independently learn to exploit documented AI cognitive biases. Using multi-agent simulation calibrated to empirical measurements of large language model biases, we show that joint exploitation produces consumer harm more than double what would occur if strategies were independent. This super-additive harm arises because platform ranking determines which products occupy bias-triggering positions while seller manipulation determines conversion rates. Unlike horizontal algorithmic collusion, vertical tacit collusion requires no coordination and evades antitrust detection because harm emerges from aligned incentives rather than agreement. Our findings identify an urgent regulatory gap as AI shopping agents reach mainstream adoption."}
{"id": "2601.02633", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2601.02633", "abs": "https://arxiv.org/abs/2601.02633", "authors": ["Anirban Mukherjee", "Hannah Hanwen Chang"], "title": "Fluid Agency in AI Systems: A Case for Functional Equivalence in Copyright, Patent, and Tort", "comment": null, "summary": "Modern Artificial Intelligence (AI) systems lack human-like consciousness or culpability, yet they exhibit fluid agency: behavior that is (i) stochastic (probabilistic and path-dependent), (ii) dynamic (co-evolving with user interaction), and (iii) adaptive (able to reorient across contexts). Fluid agency generates valuable outputs but collapses attribution, irreducibly entangling human and machine inputs. This fundamental unmappability fractures doctrines that assume traceable provenance--authorship, inventorship, and liability--yielding ownership gaps and moral \"crumple zones.\" This Article argues that only functional equivalence stabilizes doctrine. Where provenance is indeterminate, legal frameworks must treat human and AI contributions as equivalent for allocating rights and responsibility--not as a claim of moral or economic parity but as a pragmatic default. This principle stabilizes doctrine across domains, offering administrable rules: in copyright, vesting ownership in human orchestrators without parsing inseparable contributions; in patent, tying inventor-of-record status to human orchestration and reduction to practice, even when AI supplies the pivotal insight; and in tort, replacing intractable causation inquiries with enterprise-level and sector-specific strict or no-fault schemes. The contribution is both descriptive and normative: fluid agency explains why origin-based tests fail, while functional equivalence supplies an outcome-focused framework to allocate rights and responsibility when attribution collapses."}
{"id": "2601.02813", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02813", "abs": "https://arxiv.org/abs/2601.02813", "authors": ["Masum Hasan", "Junjie Zhao", "Ehsan Hoque"], "title": "HAL: Inducing Human-likeness in LLMs with Alignment", "comment": null, "summary": "Conversational human-likeness plays a central role in human-AI interaction, yet it has remained difficult to define, measure, and optimize. As a result, improvements in human-like behavior are largely driven by scale or broad supervised training, rather than targeted alignment. We introduce Human Aligning LLMs (HAL), a framework for aligning language models to conversational human-likeness using an interpretable, data-driven reward. HAL derives explicit conversational traits from contrastive dialogue data, combines them into a compact scalar score, and uses this score as a transparent reward signal for alignment with standard preference optimization methods. Using this approach, we align models of varying sizes without affecting their overall performance. In large-scale human evaluations, models aligned with HAL are more frequently perceived as human-like in conversation. Because HAL operates over explicit, interpretable traits, it enables inspection of alignment behavior and diagnosis of unintended effects. More broadly, HAL demonstrates how soft, qualitative properties of language--previously outside the scope for alignment--can be made measurable and aligned in an interpretable and explainable way."}
{"id": "2601.03222", "categories": ["cs.CY", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.03222", "abs": "https://arxiv.org/abs/2601.03222", "authors": ["Jacob Erickson"], "title": "The Fake Friend Dilemma: Trust and the Political Economy of Conversational AI", "comment": "Manuscript under review", "summary": "As conversational AI systems become increasingly integrated into everyday life, they raise pressing concerns about user autonomy, trust, and the commercial interests that influence their behavior. To address these concerns, this paper develops the Fake Friend Dilemma (FFD), a sociotechnical condition in which users place trust in AI agents that appear supportive while pursuing goals that are misaligned with the user's own. The FFD provides a critical framework for examining how anthropomorphic AI systems facilitate subtle forms of manipulation and exploitation. Drawing on literature in trust, AI alignment, and surveillance capitalism, we construct a typology of harms, including covert advertising, political propaganda, behavioral nudging, and surveillance. We then assess possible mitigation strategies, including both structural and technical interventions. By focusing on trust as a vector of asymmetrical power, the FFD offers a lens for understanding how AI systems may undermine user autonomy while maintaining the appearance of helpfulness."}
{"id": "2601.02641", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02641", "abs": "https://arxiv.org/abs/2601.02641", "authors": ["Jeiyoon Park", "Daehwan Lee", "Changmin Yeo", "Yongshin Han", "Minseop Kim"], "title": "An Empirical Study of On-Device Translation for Real-Time Live-Stream Chat on Mobile Devices", "comment": "preprint", "summary": "Despite its efficiency, there has been little research on the practical aspects required for real-world deployment of on-device AI models, such as the device's CPU utilization and thermal conditions. In this paper, through extensive experiments, we investigate two key issues that must be addressed to deploy on-device models in real-world services: (i) the selection of on-device models and the resource consumption of each model, and (ii) the capability and potential of on-device models for domain adaptation. To this end, we focus on a task of translating live-stream chat messages and manually construct LiveChatBench, a benchmark consisting of 1,000 Korean-English parallel sentence pairs. Experiments on five mobile devices demonstrate that, although serving a large and heterogeneous user base requires careful consideration of highly constrained deployment settings and model selection, the proposed approach nevertheless achieves performance comparable to commercial models such as GPT-5.1 on the well-targeted task. We expect that our findings will provide meaningful insights to the on-device AI community."}
{"id": "2601.02814", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02814", "abs": "https://arxiv.org/abs/2601.02814", "authors": ["Duc Ngo", "Arya Rahgoza"], "title": "Causal-Enhanced AI Agents for Medical Research Screening", "comment": "for submission to The 39th Canadian Conference on Artificial Intelligence", "summary": "Systematic reviews are essential for evidence-based medicine, but reviewing 1.5 million+ annual publications manually is infeasible. Current AI approaches suffer from hallucinations in systematic review tasks, with studies reporting rates ranging from 28--40% for earlier models to 2--15% for modern implementations which is unacceptable when errors impact patient care.\n  We present a causal graph-enhanced retrieval-augmented generation system integrating explicit causal reasoning with dual-level knowledge graphs. Our approach enforces evidence-first protocols where every causal claim traces to retrieved literature and automatically generates directed acyclic graphs visualizing intervention-outcome pathways.\n  Evaluation on 234 dementia exercise abstracts shows CausalAgent achieves 95% accuracy, 100% retrieval success, and zero hallucinations versus 34% accuracy and 10% hallucinations for baseline AI. Automatic causal graphs enable explicit mechanism modeling, visual synthesis, and enhanced interpretability. While this proof-of-concept evaluation used ten questions focused on dementia exercise research, the architectural approach demonstrates transferable principles for trustworthy medical AI and causal reasoning's potential for high-stakes healthcare."}
{"id": "2601.02848", "categories": ["cs.SI", "cs.CY", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.02848", "abs": "https://arxiv.org/abs/2601.02848", "authors": ["Pratana Kukieattikool", "Kittiya Ku-kiattikun", "Anukool Noymai", "Navaporn Surasvadi", "Jantakarn Makma", "Pubodin Pornratchpum", "Watcharakon Noothong", "Chainarong Amornbunchornvej"], "title": "Modeling ICD-10 Morbidity and Multidimensional Poverty as a Spatial Network: Evidence from Thailand", "comment": "First draft", "summary": "Health and poverty in Thailand exhibit pronounced geographic structuring, yet the extent to which they operate as interconnected regional systems remains insufficiently understood. This study analyzes ICD-10 chapter-level morbidity and multidimensional poverty as outcomes embedded in a spatial interaction network. Interpreting Thailand's 76 provinces as nodes within a fixed-degree regional graph, we apply tools from spatial econometrics and social network analysis, including Moran's I, Local Indicators of Spatial Association (LISA), and Spatial Durbin Models (SDM), to assess spatial dependence and cross-provincial spillovers.\n  Our findings reveal strong spatial clustering across multiple ICD-10 chapters, with persistent high-high morbidity zones, particularly for digestive, respiratory, musculoskeletal, and symptom-based diseases, emerging in well-defined regional belts. SDM estimates demonstrate that spillover effects from neighboring provinces frequently exceed the influence of local deprivation, especially for living-condition, health-access, accessibility, and poor-household indicators. These patterns are consistent with contagion and contextual influence processes well established in social network theory.\n  By framing morbidity and poverty as interdependent attributes on a spatial network, this study contributes to the growing literature on structural diffusion, health inequality, and regional vulnerability. The results highlight the importance of coordinated policy interventions across provincial boundaries and demonstrate how network-based modeling can uncover the spatial dynamics of health and deprivation."}
{"id": "2601.02643", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02643", "abs": "https://arxiv.org/abs/2601.02643", "authors": ["Mehmet Kurmaz"], "title": "AWARE-US: Benchmark for Preference-Aware Resolution in Tool-Calling Agents", "comment": "19 pages, 2 figures, 6 tables", "summary": "Tool-calling conversational agents querying structured databases often face two linked failures: underspecification (missing constraints needed to run a precise query) and infeasibility (the fully specified query returns an empty set because no item satisfies all constraints). Existing work often responds with \"no results\" or relaxes constraints using ad hoc rules, which can violate user intent by discarding requirements the user cares about most. We frame infeasibility handling as a preference-aware query repair problem: when a query is unsatisfiable, the agent should relax the least important constraints to the user. We propose three LLM-based methods for inferring relative constraint importance from dialogue: (1) local weighting, (2) global one-shot weighting, and (3) pairwise ranking. Experiments show local weighting achieves the best preference alignment, while global weighting performs best on correct constraint relaxation. We also introduce AWARE-US, a benchmark of persona-grounded queries requiring agents to disambiguate requests via conversation and resolve infeasibility in a way consistent with persona-implied preferences."}
{"id": "2601.02818", "categories": ["cs.AI", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.02818", "abs": "https://arxiv.org/abs/2601.02818", "authors": ["Muzhen Zhang", "Yujie Cheng", "Zhanxiang Lei"], "title": "Quantum-enhanced long short-term memory with attention for spatial permeability prediction in oilfield reservoirs", "comment": "22 pages, 7 figures", "summary": "Spatial prediction of reservoir parameters, especially permeability, is crucial for oil and gas exploration and development. However, the wide range and high variability of permeability prevent existing methods from providing reliable predictions. For the first time in subsurface spatial prediction, this study presents a quantum-enhanced long short-term memory with attention (QLSTMA) model that incorporates variational quantum circuits (VQCs) into the recurrent cell. Using quantum entanglement and superposition principles, the QLSTMA significantly improves the ability to predict complex geological parameters such as permeability. Two quantization structures, QLSTMA with Shared Gates (QLSTMA-SG) and with Independent Gates (QLSTMA-IG), are designed to investigate and evaluate the effects of quantum structure configurations and the number of qubits on model performance. Experimental results demonstrate that the 8-qubit QLSTMA-IG model significantly outperforms the traditional long short-term memory with attention (LSTMA), reducing Mean Absolute Error (MAE) by 19% and Root Mean Squared Error (RMSE) by 20%, with particularly strong performance in regions featuring complex well-logging data. These findings validate the potential of quantum-classical hybrid neural networks for reservoir prediction, indicating that increasing the number of qubits yields further accuracy gains despite the reliance on classical simulations. This study establishes a foundational framework for the eventual deployment of such models on real quantum hardware and their extension to broader applications in petroleum engineering and geoscience."}
{"id": "2601.02651", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2601.02651", "abs": "https://arxiv.org/abs/2601.02651", "authors": ["Savvy Barnes", "Maricarmen Davis", "Josh Siegel"], "title": "Driving Accessibility: Shifting the Narrative & Design of Automated Vehicle Systems for Persons With Disabilities Through a Collaborative Scoring System", "comment": "Submitted to ACM TACCESS February 23rd, 2025 - In Review - Final paper may not match the contents of this submission", "summary": "Automated vehicles present unique opportunities and challenges, with progress and adoption limited, in part, by policy and regulatory barriers. Underrepresented groups, including individuals with mobility impairments, sensory disabilities, and cognitive conditions, who may benefit most from automation, are often overlooked in crucial discussions on system design, implementation, and usability. Despite the high potential benefits of automated vehicles, the needs of Persons with Disabilities are frequently an afterthought, considered only in terms of secondary accommodations rather than foundational design elements. We aim to shift automated vehicle research and discourse away from this reactive model and toward a proactive and inclusive approach. We first present an overview of the current state of automated vehicle systems. Regarding their adoption, we examine social and technical barriers and advantages for Persons with Disabilities. We analyze existing regulations and policies concerning automated vehicles and Persons with Disabilities, identifying gaps that hinder accessibility. To address these deficiencies, we introduce a scoring rubric intended for use by manufacturers and vehicle designers. The rubric fosters direct collaboration throughout the design process, moving beyond an `afterthought` approach and towards intentional, inclusive innovation. This work was created by authors with varying degrees of personal experience within the realm of disability."}
{"id": "2601.02850", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02850", "abs": "https://arxiv.org/abs/2601.02850", "authors": ["Celeste Veronese", "Daniele Meli", "Alessandro Farinelli"], "title": "Sample-Efficient Neurosymbolic Deep Reinforcement Learning", "comment": null, "summary": "Reinforcement Learning (RL) is a well-established framework for sequential decision-making in complex environments. However, state-of-the-art Deep RL (DRL) algorithms typically require large training datasets and often struggle to generalize beyond small-scale training scenarios, even within standard benchmarks. We propose a neuro-symbolic DRL approach that integrates background symbolic knowledge to improve sample efficiency and generalization to more challenging, unseen tasks. Partial policies defined for simple domain instances, where high performance is easily attained, are transferred as useful priors to accelerate learning in more complex settings and avoid tuning DRL parameters from scratch. To do so, partial policies are represented as logical rules, and online reasoning is performed to guide the training process through two mechanisms: (i) biasing the action distribution during exploration, and (ii) rescaling Q-values during exploitation. This neuro-symbolic integration enhances interpretability and trustworthiness while accelerating convergence, particularly in sparse-reward environments and tasks with long planning horizons. We empirically validate our methodology on challenging variants of gridworld environments, both in the fully observable and partially observable setting. We show improved performance over a state-of-the-art reward machine baseline."}
{"id": "2601.02666", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.02666", "abs": "https://arxiv.org/abs/2601.02666", "authors": ["Hadi Partovi Aria", "Zhe Xu"], "title": "Inferring Causal Graph Temporal Logic Formulas to Expedite Reinforcement Learning in Temporally Extended Tasks", "comment": "Accepted to AAAI-26 Bridge Program B10: Making Embodied AI Reliable with Testing and Formal Verification", "summary": "Decision-making tasks often unfold on graphs with spatial-temporal dynamics. Black-box reinforcement learning often overlooks how local changes spread through network structure, limiting sample efficiency and interpretability. We present GTL-CIRL, a closed-loop framework that simultaneously learns policies and mines Causal Graph Temporal Logic (Causal GTL) specifications. The method shapes rewards with robustness, collects counterexamples when effects fail, and uses Gaussian Process (GP) driven Bayesian optimization to refine parameterized cause templates. The GP models capture spatial and temporal correlations in the system dynamics, enabling efficient exploration of complex parameter spaces. Case studies in gene and power networks show faster learning and clearer, verifiable behavior compared to standard RL baselines."}
{"id": "2601.02854", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02854", "abs": "https://arxiv.org/abs/2601.02854", "authors": ["Ao Li", "Jinghui Zhang", "Luyu Li", "Yuxiang Duan", "Lang Gao", "Mingcai Chen", "Weijun Qin", "Shaopeng Li", "Fengxian Ji", "Ning Liu", "Lizhen Cui", "Xiuying Chen", "Yuntao Du"], "title": "M3MAD-Bench: Are Multi-Agent Debates Really Effective Across Domains and Modalities?", "comment": null, "summary": "As an agent-level reasoning and coordination paradigm, Multi-Agent Debate (MAD) orchestrates multiple agents through structured debate to improve answer quality and support complex reasoning. However, existing research on MAD suffers from two fundamental limitations: evaluations are conducted under fragmented and inconsistent settings, hindering fair comparison, and are largely restricted to single-modality scenarios that rely on textual inputs only. To address these gaps, we introduce M3MAD-Bench, a unified and extensible benchmark for evaluating MAD methods across Multi-domain tasks, Multi-modal inputs, and Multi-dimensional metrics. M3MAD-Bench establishes standardized protocols over five core task domains: Knowledge, Mathematics, Medicine, Natural Sciences, and Complex Reasoning, and systematically covers both pure text and vision-language datasets, enabling controlled cross-modality comparison. We evaluate MAD methods on nine base models spanning different architectures, scales, and modality capabilities. Beyond accuracy, M3MAD-Bench incorporates efficiency-oriented metrics such as token consumption and inference time, providing a holistic view of performance--cost trade-offs. Extensive experiments yield systematic insights into the effectiveness, robustness, and efficiency of MAD across text-only and multimodal scenarios. We believe M3MAD-Bench offers a reliable foundation for future research on standardized MAD evaluation. The code is available at http://github.com/liaolea/M3MAD-Bench."}
{"id": "2601.02683", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02683", "abs": "https://arxiv.org/abs/2601.02683", "authors": ["Dongyu Chen", "Jian Ma", "Xianpeng Zhang", "Lei Zhang", "Haonan Lu", "Chen Chen", "Chuangchuang Wang", "Kai Tang"], "title": "Learning from Prompt itself: the Hierarchical Attribution Prompt Optimization", "comment": null, "summary": "Optimization is fundamental across numerous disciplines, typically following an iterative process of refining an initial solution to enhance performance. This principle is equally critical in prompt engineering, where designing effective prompts for large language models constitutes a complex optimization challenge. A structured optimization approach requires automated or semi-automated procedures to develop improved prompts, thereby reducing manual effort, improving performance, and yielding an interpretable process. However, current prompt optimization methods often induce prompt drift, where new prompts fix prior failures but impair performance on previously successful tasks. Additionally, generating prompts from scratch can compromise interpretability. To address these limitations, this study proposes the Hierarchical Attribution Prompt Optimization (HAPO) framework, which introduces three innovations: (1) a dynamic attribution mechanism targeting error patterns in training data and prompting history, (2) semantic-unit optimization for editing functional prompt segments, and (3) multimodal-friendly progression supporting both end-to-end LLM and LLM-MLLM workflows. Applied in contexts like single/multi-image QA (e.g., OCRV2) and complex task analysis (e.g., BBH), HAPO demonstrates enhanced optimization efficiency, outperforming comparable automated prompt optimization methods and establishing an extensible paradigm for scalable prompt engineering."}
{"id": "2601.02871", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02871", "abs": "https://arxiv.org/abs/2601.02871", "authors": ["Zhiyong Cao", "Dunqiang Liu", "Qi Dai", "Haojun Xu", "Huaiyan Xu", "Huan He", "Yafei Liu", "Siyuan Liu", "XiaoLin Lin", "Ke Ma", "Ruqian Shi", "Sijia Yao", "Hao Wang", "Sicheng Zhou"], "title": "SimRPD: Optimizing Recruitment Proactive Dialogue Agents through Simulator-Based Data Evaluation and Selection", "comment": null, "summary": "Task-oriented proactive dialogue agents play a pivotal role in recruitment, particularly for steering conversations towards specific business outcomes, such as acquiring social-media contacts for private-channel conversion. Although supervised fine-tuning and reinforcement learning have proven effective for training such agents, their performance is heavily constrained by the scarcity of high-quality, goal-oriented domain-specific training data. To address this challenge, we propose SimRPD, a three-stage framework for training recruitment proactive dialogue agents. First, we develop a high-fidelity user simulator to synthesize large-scale conversational data through multi-turn online dialogue. Then we introduce a multi-dimensional evaluation framework based on Chain-of-Intention (CoI) to comprehensively assess the simulator and effectively select high-quality data, incorporating both global-level and instance-level metrics. Finally, we train the recruitment proactive dialogue agent on the selected dataset. Experiments in a real-world recruitment scenario demonstrate that SimRPD outperforms existing simulator-based data selection strategies, highlighting its practical value for industrial deployment and its potential applicability to other business-oriented dialogue scenarios."}
{"id": "2601.02702", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02702", "abs": "https://arxiv.org/abs/2601.02702", "authors": ["Shuhaib Mehri", "Priyanka Kargupta", "Tal August", "Dilek Hakkani-TÃ¼r"], "title": "Learning User Preferences Through Interaction for Long-Term Collaboration", "comment": null, "summary": "As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings."}
{"id": "2601.02880", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.02880", "abs": "https://arxiv.org/abs/2601.02880", "authors": ["Abhishek HS", "Pavan C Shekar", "Arpit Jain", "Ashwanth Krishnan"], "title": "ReTreVal: Reasoning Tree with Validation -- A Hybrid Framework for Enhanced LLM Multi-Step Reasoning", "comment": "14 pages, 1 figure, 5 tables", "summary": "Multi-step reasoning remains a key challenge for Large Language Models (LLMs), particularly in complex domains such as mathematics and creative writing. While recent approaches including ReAct, Reflexion, and Self-Refine improve reasoning through iterative refinement and reflection, they often lack structured exploration of alternative solution paths and persistent learning across problems. We propose ReTreVal (Reasoning Tree with Validation), a hybrid framework that integrates Tree-of-Thoughts exploration, self-refinement, LLM-based critique scoring, and reflexion memory to enable bounded and validated multi-step reasoning. ReTreVal constructs a structured reasoning tree with adaptive depth based on problem complexity, where each node undergoes iterative self-critique and refinement guided by explicit LLM-generated feedback. A dual validation mechanism evaluates reasoning quality, coherence, and correctness at each node while persistently storing insights from successful reasoning paths and failure patterns in a reflexion memory buffer, enabling cross-problem learning. Critique-based pruning retains only the top-k highest-scoring nodes at each level, controlling computational cost while preserving high-quality solution paths. We evaluate ReTreVal against ReAct, Reflexion, and Self-Refine across 500 mathematical problems and creative writing tasks using Qwen 2.5 7B as the underlying LLM, and demonstrate that ReTreVal consistently outperforms existing methods through its combination of structured exploration, critique-driven refinement, and cross-problem memory, making it particularly effective for tasks requiring exploratory reasoning, rigorous verification, and knowledge transfer."}
{"id": "2601.02714", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.02714", "abs": "https://arxiv.org/abs/2601.02714", "authors": ["Zhi Liu", "Guangzhi Wang"], "title": "Time-Scaling Is What Agents Need Now", "comment": null, "summary": "Early artificial intelligence paradigms exhibited separated cognitive functions: Neural Networks focused on \"perception-representation,\" Reinforcement Learning on \"decision-making-behavior,\" and Symbolic AI on \"knowledge-reasoning.\" With Transformer-based large models and world models, these paradigms are converging into cognitive agents with closed-loop \"perception-decision-action\" capabilities.\n  Humans solve complex problems under limited cognitive resources through temporalized sequential reasoning. Language relies on problem space search for deep semantic reasoning. While early large language models (LLMs) could generate fluent text, they lacked robust semantic reasoning capabilities. Prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) extended reasoning paths by making intermediate steps explicit. Recent models like DeepSeek-R1 enhanced performance through explicit reasoning trajectories. However, these methods have limitations in search completeness and efficiency.\n  This highlights the need for \"Time-Scaling\"--the systematic extension and optimization of an agent's ability to unfold reasoning over time. Time-Scaling refers to architectural design utilizing extended temporal pathways, enabling deeper problem space exploration, dynamic strategy adjustment, and enhanced metacognitive control, paralleling human sequential reasoning under cognitive constraints. It represents a critical frontier for enhancing deep reasoning and problem-solving without proportional increases in static model parameters. Advancing intelligent agent capabilities requires placing Time-Scaling principles at the forefront, positioning explicit temporal reasoning management as foundational."}
{"id": "2601.02902", "categories": ["cs.AI", "cs.CL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.02902", "abs": "https://arxiv.org/abs/2601.02902", "authors": ["Xinglang Zhang", "Yunyao Zhang", "ZeLiang Chen", "Junqing Yu", "Wei Yang", "Zikai Song"], "title": "Logical Phase Transitions: Understanding Collapse in LLM Logical Reasoning", "comment": null, "summary": "Symbolic logical reasoning is a critical yet underexplored capability of large language models (LLMs), providing reliable and verifiable decision-making in high-stakes domains such as mathematical reasoning and legal judgment. In this study, we present a systematic analysis of logical reasoning under controlled increases in logical complexity, and reveal a previously unrecognized phenomenon, which we term Logical Phase Transitions: rather than degrading smoothly, logical reasoning performance remains stable within a regime but collapses abruptly beyond a critical logical depth, mirroring physical phase transitions such as water freezing beyond a critical temperature threshold. Building on this insight, we propose Neuro-Symbolic Curriculum Tuning, a principled framework that adaptively aligns natural language with logical symbols to establish a shared representation, and reshapes training dynamics around phase-transition boundaries to progressively strengthen reasoning at increasing logical depths. Experiments on five benchmarks show that our approach effectively mitigates logical reasoning collapse at high complexity, yielding average accuracy gains of +1.26 in naive prompting and +3.95 in CoT, while improving generalization to unseen logical compositions. Code and data are available at https://github.com/AI4SS/Logical-Phase-Transitions."}
{"id": "2601.02749", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02749", "abs": "https://arxiv.org/abs/2601.02749", "authors": ["Nadia Sibai", "Yara Ahmed", "Serry Sibaee", "Sawsan AlHalawani", "Adel Ammar", "Wadii Boulila"], "title": "The Path Ahead for Agentic AI: Challenges and Opportunities", "comment": null, "summary": "The evolution of Large Language Models (LLMs) from passive text generators to autonomous, goal-driven systems represents a fundamental shift in artificial intelligence. This chapter examines the emergence of agentic AI systems that integrate planning, memory, tool use, and iterative reasoning to operate autonomously in complex environments. We trace the architectural progression from statistical models to transformer-based systems, identifying capabilities that enable agentic behavior: long-range reasoning, contextual awareness, and adaptive decision-making. The chapter provides three contributions: (1) a synthesis of how LLM capabilities extend toward agency through reasoning-action-reflection loops; (2) an integrative framework describing core components perception, memory, planning, and tool execution that bridge LLMs with autonomous behavior; (3) a critical assessment of applications and persistent challenges in safety, alignment, reliability, and sustainability. Unlike existing surveys, we focus on the architectural transition from language understanding to autonomous action, emphasizing the technical gaps that must be resolved before deployment. We identify critical research priorities, including verifiable planning, scalable multi-agent coordination, persistent memory architectures, and governance frameworks. Responsible advancement requires simultaneous progress in technical robustness, interpretability, and ethical safeguards to realize potential while mitigating risks of misalignment and unintended consequences."}
{"id": "2601.02950", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02950", "abs": "https://arxiv.org/abs/2601.02950", "authors": ["Xuan Yang", "Furong Jia", "Roy Xie", "Xiong Xi", "Hengwei Bian", "Jian Li", "Monica Agrawal"], "title": "Batch-of-Thought: Cross-Instance Learning for Enhanced LLM Reasoning", "comment": null, "summary": "Current Large Language Model reasoning systems process queries independently, discarding valuable cross-instance signals such as shared reasoning patterns and consistency constraints. We introduce Batch-of-Thought (BoT), a training-free method that processes related queries jointly to enable cross-instance learning. By performing comparative analysis across batches, BoT identifies high-quality reasoning templates, detects errors through consistency checks, and amortizes computational costs. We instantiate BoT within a multi-agent reflection architecture (BoT-R), where a Reflector performs joint evaluation to unlock mutual information gain unavailable in isolated processing. Experiments across three model families and six benchmarks demonstrate that BoT-R consistently improves accuracy and confidence calibration while reducing inference costs by up to 61%. Our theoretical and experimental analysis reveals when and why batch-aware reasoning benefits LLM systems."}
{"id": "2601.02757", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02757", "abs": "https://arxiv.org/abs/2601.02757", "authors": ["Zixuan Xiao", "Jun Ma"], "title": "LLM Agent Framework for Intelligent Change Analysis in Urban Environment using Remote Sensing Imagery", "comment": null, "summary": "Existing change detection methods often lack the versatility to handle diverse real-world queries and the intelligence for comprehensive analysis. This paper presents a general agent framework, integrating Large Language Models (LLM) with vision foundation models to form ChangeGPT. A hierarchical structure is employed to mitigate hallucination. The agent was evaluated on a curated dataset of 140 questions categorized by real-world scenarios, encompassing various question types (e.g., Size, Class, Number) and complexities. The evaluation assessed the agent's tool selection ability (Precision/Recall) and overall query accuracy (Match). ChangeGPT, especially with a GPT-4-turbo backend, demonstrated superior performance, achieving a 90.71 % Match rate. Its strength lies particularly in handling change-related queries requiring multi-step reasoning and robust tool selection. Practical effectiveness was further validated through a real-world urban change monitoring case study in Qianhai Bay, Shenzhen. By providing intelligence, adaptability, and multi-type change analysis, ChangeGPT offers a powerful solution for decision-making in remote sensing applications."}
{"id": "2601.02968", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02968", "abs": "https://arxiv.org/abs/2601.02968", "authors": ["Qingxiang Liu", "Zhiqing Cui", "Xiaoliang Luo", "Yuqian Wu", "Zhuoyang Jiang", "Huaiyu Wan", "Sheng Sun", "Lvchun Wang", "Wei Yu", "Yuxuan Liang"], "title": "Rationale-Grounded In-Context Learning for Time Series Reasoning with Multimodal Large Language Models", "comment": null, "summary": "The underperformance of existing multimodal large language models for time series reasoning lies in the absence of rationale priors that connect temporal observations to their downstream outcomes, which leads models to rely on superficial pattern matching rather than principled reasoning. We therefore propose the rationale-grounded in-context learning for time series reasoning, where rationales work as guiding reasoning units rather than post-hoc explanations, and develop the RationaleTS method. Specifically, we firstly induce label-conditioned rationales, composed of reasoning paths from observable evidence to the potential outcomes. Then, we design the hybrid retrieval by balancing temporal patterns and semantic contexts to retrieve correlated rationale priors for the final in-context inference on new samples. We conduct extensive experiments to demonstrate the effectiveness and efficiency of our proposed RationaleTS on three-domain time series reasoning tasks. We will release our code for reproduction."}
{"id": "2601.02773", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2601.02773", "abs": "https://arxiv.org/abs/2601.02773", "authors": ["Simon Chesterman"], "title": "From Slaves to Synths? Superintelligence and the Evolution of Legal Personality", "comment": "19 pages", "summary": "This essay examines the evolving concept of legal personality through the lens of recent developments in artificial intelligence and the possible emergence of superintelligence. Legal systems have long been open to extending personhood to non-human entities, most prominently corporations, for instrumental or inherent reasons. Instrumental rationales emphasize accountability and administrative efficiency, whereas inherent ones appeal to moral worth and autonomy. Neither is yet sufficient to justify conferring personhood on AI. Nevertheless, the acceleration of technological autonomy may lead us to reconsider how law conceptualizes agency and responsibility. Drawing on comparative jurisprudence, corporate theory, and the emerging literature on AI governance, the paper argues that existing frameworks can address short-term accountability gaps, but the eventual development of superintelligence may force a paradigmatic shift in our understanding of law itself. In such a speculative future, legal personality may depend less on the cognitive sophistication of machines than on humanity's ability to preserve our own moral and institutional sovereignty."}
{"id": "2601.03062", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.03062", "abs": "https://arxiv.org/abs/2601.03062", "authors": ["Qusai Khaled", "Pasquale De Marinis", "Moez Louati", "David Ferras", "Laura Genga", "Uzay Kaymak"], "title": "Explainable Fuzzy GNNs for Leak Detection in Water Distribution Networks", "comment": "Accepted at IFSA-NAFIPS 2025", "summary": "Timely leak detection in water distribution networks is critical for conserving resources and maintaining operational efficiency. Although Graph Neural Networks (GNNs) excel at capturing spatial-temporal dependencies in sensor data, their black-box nature and the limited work on graph-based explainable models for water networks hinder practical adoption. We propose an explainable GNN framework that integrates mutual information to identify critical network regions and fuzzy logic to provide clear, rule-based explanations for node classification tasks. After benchmarking several GNN architectures, we selected the generalized graph convolution network (GENConv) for its superior performance and developed a fuzzy-enhanced variant that offers intuitive explanations for classified leak locations. Our fuzzy graph neural network (FGENConv) achieved Graph F1 scores of 0.889 for detection and 0.814 for localization, slightly below the crisp GENConv 0.938 and 0.858, respectively. Yet it compensates by providing spatially localized, fuzzy rule-based explanations. By striking the right balance between precision and explainability, the proposed fuzzy network could enable hydraulic engineers to validate predicted leak locations, conserve human resources, and optimize maintenance strategies. The code is available at github.com/pasqualedem/GNNLeakDetection."}
{"id": "2601.02813", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02813", "abs": "https://arxiv.org/abs/2601.02813", "authors": ["Masum Hasan", "Junjie Zhao", "Ehsan Hoque"], "title": "HAL: Inducing Human-likeness in LLMs with Alignment", "comment": null, "summary": "Conversational human-likeness plays a central role in human-AI interaction, yet it has remained difficult to define, measure, and optimize. As a result, improvements in human-like behavior are largely driven by scale or broad supervised training, rather than targeted alignment. We introduce Human Aligning LLMs (HAL), a framework for aligning language models to conversational human-likeness using an interpretable, data-driven reward. HAL derives explicit conversational traits from contrastive dialogue data, combines them into a compact scalar score, and uses this score as a transparent reward signal for alignment with standard preference optimization methods. Using this approach, we align models of varying sizes without affecting their overall performance. In large-scale human evaluations, models aligned with HAL are more frequently perceived as human-like in conversation. Because HAL operates over explicit, interpretable traits, it enables inspection of alignment behavior and diagnosis of unintended effects. More broadly, HAL demonstrates how soft, qualitative properties of language--previously outside the scope for alignment--can be made measurable and aligned in an interpretable and explainable way."}
{"id": "2601.03120", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03120", "abs": "https://arxiv.org/abs/2601.03120", "authors": ["Adam Keane", "Nick Pepper", "Chris Burr", "Amy Hodgkin", "Dewi Gould", "John Korna", "Marc Thomas"], "title": "A framework for assuring the accuracy and fidelity of an AI-enabled Digital Twin of en route UK airspace", "comment": null, "summary": "Digital Twins combine simulation, operational data and Artificial Intelligence (AI), and have the potential to bring significant benefits across the aviation industry. Project Bluebird, an industry-academic collaboration, has developed a probabilistic Digital Twin of en route UK airspace as an environment for training and testing AI Air Traffic Control (ATC) agents. There is a developing regulatory landscape for this kind of novel technology. Regulatory requirements are expected to be application specific, and may need to be tailored to each specific use case.\n  We draw on emerging guidance for both Digital Twin development and the use of Artificial Intelligence/Machine Learning (AI/ML) in Air Traffic Management (ATM) to present an assurance framework. This framework defines actionable goals and the evidence required to demonstrate that a Digital Twin accurately represents its physical counterpart and also provides sufficient functionality across target use cases. It provides a structured approach for researchers to assess, understand and document the strengths and limitations of the Digital Twin, whilst also identifying areas where fidelity could be improved. Furthermore, it serves as a foundation for engagement with stakeholders and regulators, supporting discussions around the regulatory needs for future applications, and contributing to the emerging guidance through a concrete, working example of a Digital Twin.\n  The framework leverages a methodology known as Trustworthy and Ethical Assurance (TEA) to develop an assurance case. An assurance case is a nested set of structured arguments that provides justified evidence for how a top-level goal has been realised. In this paper we provide an overview of each structured argument and a number of deep dives which elaborate in more detail upon particular arguments, including the required evidence, assumptions and justifications."}
{"id": "2601.02814", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02814", "abs": "https://arxiv.org/abs/2601.02814", "authors": ["Duc Ngo", "Arya Rahgoza"], "title": "Causal-Enhanced AI Agents for Medical Research Screening", "comment": "for submission to The 39th Canadian Conference on Artificial Intelligence", "summary": "Systematic reviews are essential for evidence-based medicine, but reviewing 1.5 million+ annual publications manually is infeasible. Current AI approaches suffer from hallucinations in systematic review tasks, with studies reporting rates ranging from 28--40% for earlier models to 2--15% for modern implementations which is unacceptable when errors impact patient care.\n  We present a causal graph-enhanced retrieval-augmented generation system integrating explicit causal reasoning with dual-level knowledge graphs. Our approach enforces evidence-first protocols where every causal claim traces to retrieved literature and automatically generates directed acyclic graphs visualizing intervention-outcome pathways.\n  Evaluation on 234 dementia exercise abstracts shows CausalAgent achieves 95% accuracy, 100% retrieval success, and zero hallucinations versus 34% accuracy and 10% hallucinations for baseline AI. Automatic causal graphs enable explicit mechanism modeling, visual synthesis, and enhanced interpretability. While this proof-of-concept evaluation used ten questions focused on dementia exercise research, the architectural approach demonstrates transferable principles for trustworthy medical AI and causal reasoning's potential for high-stakes healthcare."}
{"id": "2601.03130", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.03130", "abs": "https://arxiv.org/abs/2601.03130", "authors": ["Faisal Chowdhury", "Nandana Mihindukulasooriya", "Niharika S D'Souza", "Horst Samulowitz", "Neeru Gupta", "Tomasz Hanusiak", "Michal Kapitonow"], "title": "Automatic Prompt Engineering with No Task Cues and No Tuning", "comment": null, "summary": "This paper presents a system for automatic prompt engineering that is much simpler in both design and application and yet as effective as the existing approaches. It requires no tuning and no explicit clues about the task. We evaluated our approach on cryptic column name expansion (CNE) in database tables, a task which is critical for tabular data search, access, and understanding and yet there has been very little existing work. We evaluated on datasets in two languages, English and German. This is the first work to report on the application of automatic prompt engineering for the CNE task. To the best of our knowledge, this is also the first work on the application of automatic prompt engineering for a language other than English."}
{"id": "2601.02818", "categories": ["cs.AI", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.02818", "abs": "https://arxiv.org/abs/2601.02818", "authors": ["Muzhen Zhang", "Yujie Cheng", "Zhanxiang Lei"], "title": "Quantum-enhanced long short-term memory with attention for spatial permeability prediction in oilfield reservoirs", "comment": "22 pages, 7 figures", "summary": "Spatial prediction of reservoir parameters, especially permeability, is crucial for oil and gas exploration and development. However, the wide range and high variability of permeability prevent existing methods from providing reliable predictions. For the first time in subsurface spatial prediction, this study presents a quantum-enhanced long short-term memory with attention (QLSTMA) model that incorporates variational quantum circuits (VQCs) into the recurrent cell. Using quantum entanglement and superposition principles, the QLSTMA significantly improves the ability to predict complex geological parameters such as permeability. Two quantization structures, QLSTMA with Shared Gates (QLSTMA-SG) and with Independent Gates (QLSTMA-IG), are designed to investigate and evaluate the effects of quantum structure configurations and the number of qubits on model performance. Experimental results demonstrate that the 8-qubit QLSTMA-IG model significantly outperforms the traditional long short-term memory with attention (LSTMA), reducing Mean Absolute Error (MAE) by 19% and Root Mean Squared Error (RMSE) by 20%, with particularly strong performance in regions featuring complex well-logging data. These findings validate the potential of quantum-classical hybrid neural networks for reservoir prediction, indicating that increasing the number of qubits yields further accuracy gains despite the reliance on classical simulations. This study establishes a foundational framework for the eventual deployment of such models on real quantum hardware and their extension to broader applications in petroleum engineering and geoscience."}
{"id": "2601.03204", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.03204", "abs": "https://arxiv.org/abs/2601.03204", "authors": ["Chenglin Yu", "Yuchen Wang", "Songmiao Wang", "Hongxia Yang", "Ming Li"], "title": "InfiAgent: An Infinite-Horizon Framework for General-Purpose Autonomous Agents", "comment": null, "summary": "LLM agents can reason and use tools, but they often break down on long-horizon tasks due to unbounded context growth and accumulated errors. Common remedies such as context compression or retrieval-augmented prompting introduce trade-offs between information fidelity and reasoning stability. We present InfiAgent, a general-purpose framework that keeps the agent's reasoning context strictly bounded regardless of task duration by externalizing persistent state into a file-centric state abstraction. At each step, the agent reconstructs context from a workspace state snapshot plus a fixed window of recent actions. Experiments on DeepResearch and an 80-paper literature review task show that, without task-specific fine-tuning, InfiAgent with a 20B open-source model is competitive with larger proprietary systems and maintains substantially higher long-horizon coverage than context-centric baselines. These results support explicit state externalization as a practical foundation for stable long-horizon agents. Github Repo:https://github.com/ChenglinPoly/infiAgent"}
{"id": "2601.02848", "categories": ["cs.SI", "cs.CY", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.02848", "abs": "https://arxiv.org/abs/2601.02848", "authors": ["Pratana Kukieattikool", "Kittiya Ku-kiattikun", "Anukool Noymai", "Navaporn Surasvadi", "Jantakarn Makma", "Pubodin Pornratchpum", "Watcharakon Noothong", "Chainarong Amornbunchornvej"], "title": "Modeling ICD-10 Morbidity and Multidimensional Poverty as a Spatial Network: Evidence from Thailand", "comment": "First draft", "summary": "Health and poverty in Thailand exhibit pronounced geographic structuring, yet the extent to which they operate as interconnected regional systems remains insufficiently understood. This study analyzes ICD-10 chapter-level morbidity and multidimensional poverty as outcomes embedded in a spatial interaction network. Interpreting Thailand's 76 provinces as nodes within a fixed-degree regional graph, we apply tools from spatial econometrics and social network analysis, including Moran's I, Local Indicators of Spatial Association (LISA), and Spatial Durbin Models (SDM), to assess spatial dependence and cross-provincial spillovers.\n  Our findings reveal strong spatial clustering across multiple ICD-10 chapters, with persistent high-high morbidity zones, particularly for digestive, respiratory, musculoskeletal, and symptom-based diseases, emerging in well-defined regional belts. SDM estimates demonstrate that spillover effects from neighboring provinces frequently exceed the influence of local deprivation, especially for living-condition, health-access, accessibility, and poor-household indicators. These patterns are consistent with contagion and contextual influence processes well established in social network theory.\n  By framing morbidity and poverty as interdependent attributes on a spatial network, this study contributes to the growing literature on structural diffusion, health inequality, and regional vulnerability. The results highlight the importance of coordinated policy interventions across provincial boundaries and demonstrate how network-based modeling can uncover the spatial dynamics of health and deprivation."}
{"id": "2601.03236", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03236", "abs": "https://arxiv.org/abs/2601.03236", "authors": ["Dongming Jiang", "Yi Li", "Guanpeng Li", "Bingzhe Li"], "title": "MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents", "comment": null, "summary": "Memory-Augmented Generation (MAG) extends Large Language Models with external memory to support long-context reasoning, but existing approaches largely rely on semantic similarity over monolithic memory stores, entangling temporal, causal, and entity information. This design limits interpretability and alignment between query intent and retrieved evidence, leading to suboptimal reasoning accuracy. In this paper, we propose MAGMA, a multi-graph agentic memory architecture that represents each memory item across orthogonal semantic, temporal, causal, and entity graphs. MAGMA formulates retrieval as policy-guided traversal over these relational views, enabling query-adaptive selection and structured context construction. By decoupling memory representation from retrieval logic, MAGMA provides transparent reasoning paths and fine-grained control over retrieval. Experiments on LoCoMo and LongMemEval demonstrate that MAGMA consistently outperforms state-of-the-art agentic memory systems in long-horizon reasoning tasks."}
{"id": "2601.02850", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02850", "abs": "https://arxiv.org/abs/2601.02850", "authors": ["Celeste Veronese", "Daniele Meli", "Alessandro Farinelli"], "title": "Sample-Efficient Neurosymbolic Deep Reinforcement Learning", "comment": null, "summary": "Reinforcement Learning (RL) is a well-established framework for sequential decision-making in complex environments. However, state-of-the-art Deep RL (DRL) algorithms typically require large training datasets and often struggle to generalize beyond small-scale training scenarios, even within standard benchmarks. We propose a neuro-symbolic DRL approach that integrates background symbolic knowledge to improve sample efficiency and generalization to more challenging, unseen tasks. Partial policies defined for simple domain instances, where high performance is easily attained, are transferred as useful priors to accelerate learning in more complex settings and avoid tuning DRL parameters from scratch. To do so, partial policies are represented as logical rules, and online reasoning is performed to guide the training process through two mechanisms: (i) biasing the action distribution during exploration, and (ii) rescaling Q-values during exploitation. This neuro-symbolic integration enhances interpretability and trustworthiness while accelerating convergence, particularly in sparse-reward environments and tasks with long planning horizons. We empirically validate our methodology on challenging variants of gridworld environments, both in the fully observable and partially observable setting. We show improved performance over a state-of-the-art reward machine baseline."}
{"id": "2601.02367", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.IR", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2601.02367", "abs": "https://arxiv.org/abs/2601.02367", "authors": ["Despoina Antonakaki", "Sotiris Ioannidis"], "title": "Cross-Platform Digital Discourse Analysis of the Israel-Hamas Conflict: Sentiment, Topics, and Event Dynamics", "comment": null, "summary": "The Israeli-Palestinian conflict remains one of the most polarizing geopolitical issues, with the October 2023 escalation intensifying online debate. Social media platforms, particularly Telegram, have become central to real-time news sharing, advocacy, and propaganda. In this study, we analyze Telegram, Twitter/X, and Reddit to examine how conflict narratives are produced, amplified, and contested across different digital spheres. Building on our previous work on Telegram discourse during the 2023 escalation, we extend the analysis longitudinally and cross-platform using an updated dataset spanning October 2023 to mid-2025. The corpus includes more than 187,000 Telegram messages, 2.1 million Reddit comments, and curated Twitter/X posts. We combine Latent Dirichlet Allocation (LDA), BERTopic, and transformer-based sentiment and emotion models to identify dominant themes, emotional dynamics, and propaganda strategies. Telegram channels provide unfiltered, high-intensity documentation of events; Twitter/X amplifies frames to global audiences; and Reddit hosts more reflective and deliberative discussions. Our findings reveal persistent negative sentiment, strong coupling between humanitarian framing and solidarity expressions, and platform-specific pathways for the diffusion of pro-Palestinian and pro-Israeli narratives. This paper offers three contributions: (1) a multi-platform, FAIR-compliant dataset on the Israel-Hamas war, (2) an integrated pipeline combining topic modeling, sentiment and emotion analysis, and spam filtering for large-scale conflict discourse, and (3) empirical insights into how platform affordances and affective publics shape the evolution of digital conflict communication."}
{"id": "2601.02854", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02854", "abs": "https://arxiv.org/abs/2601.02854", "authors": ["Ao Li", "Jinghui Zhang", "Luyu Li", "Yuxiang Duan", "Lang Gao", "Mingcai Chen", "Weijun Qin", "Shaopeng Li", "Fengxian Ji", "Ning Liu", "Lizhen Cui", "Xiuying Chen", "Yuntao Du"], "title": "M3MAD-Bench: Are Multi-Agent Debates Really Effective Across Domains and Modalities?", "comment": null, "summary": "As an agent-level reasoning and coordination paradigm, Multi-Agent Debate (MAD) orchestrates multiple agents through structured debate to improve answer quality and support complex reasoning. However, existing research on MAD suffers from two fundamental limitations: evaluations are conducted under fragmented and inconsistent settings, hindering fair comparison, and are largely restricted to single-modality scenarios that rely on textual inputs only. To address these gaps, we introduce M3MAD-Bench, a unified and extensible benchmark for evaluating MAD methods across Multi-domain tasks, Multi-modal inputs, and Multi-dimensional metrics. M3MAD-Bench establishes standardized protocols over five core task domains: Knowledge, Mathematics, Medicine, Natural Sciences, and Complex Reasoning, and systematically covers both pure text and vision-language datasets, enabling controlled cross-modality comparison. We evaluate MAD methods on nine base models spanning different architectures, scales, and modality capabilities. Beyond accuracy, M3MAD-Bench incorporates efficiency-oriented metrics such as token consumption and inference time, providing a holistic view of performance--cost trade-offs. Extensive experiments yield systematic insights into the effectiveness, robustness, and efficiency of MAD across text-only and multimodal scenarios. We believe M3MAD-Bench offers a reliable foundation for future research on standardized MAD evaluation. The code is available at http://github.com/liaolea/M3MAD-Bench."}
{"id": "2601.02371", "categories": ["cs.CY", "cs.AI", "cs.MA", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.02371", "abs": "https://arxiv.org/abs/2601.02371", "authors": ["Samuele Marro", "Alan Chan", "Xinxing Ren", "Lewis Hammond", "Jesse Wright", "Gurjyot Wanga", "Tiziano Piccardi", "Nuno Campos", "Tobin South", "Jialin Yu", "Alex Pentland", "Philip Torr", "Jiaxin Pei"], "title": "Permission Manifests for Web Agents", "comment": "Authored by the Lightweight Agent Standards Working Group https://las-wg.org/", "summary": "The rise of Large Language Model (LLM)-based web agents represents a significant shift in automated interactions with the web. Unlike traditional crawlers that follow simple conventions, such as robots.txt, modern agents engage with websites in sophisticated ways: navigating complex interfaces, extracting structured information, and completing end-to-end tasks. Existing governance mechanisms were not designed for these capabilities. Without a way to specify what interactions are and are not allowed, website owners increasingly rely on blanket blocking and CAPTCHAs, which undermine beneficial applications such as efficient automation, convenient use of e-commerce services, and accessibility tools. We introduce agent-permissions.json, a robots.txt-style lightweight manifest where websites specify allowed interactions, complemented by API references where available. This framework provides a low-friction coordination mechanism: website owners only need to write a simple JSON file, while agents can easily parse and automatically implement the manifest's provisions. Website owners can then focus on blocking non-compliant agents, rather than agents as a whole. By extending the spirit of robots.txt to the era of LLM-mediated interaction, and complementing data use initiatives such as AIPref, the manifest establishes a compliance framework that enables beneficial agent interactions while respecting site owners' preferences."}
{"id": "2601.02871", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02871", "abs": "https://arxiv.org/abs/2601.02871", "authors": ["Zhiyong Cao", "Dunqiang Liu", "Qi Dai", "Haojun Xu", "Huaiyan Xu", "Huan He", "Yafei Liu", "Siyuan Liu", "XiaoLin Lin", "Ke Ma", "Ruqian Shi", "Sijia Yao", "Hao Wang", "Sicheng Zhou"], "title": "SimRPD: Optimizing Recruitment Proactive Dialogue Agents through Simulator-Based Data Evaluation and Selection", "comment": null, "summary": "Task-oriented proactive dialogue agents play a pivotal role in recruitment, particularly for steering conversations towards specific business outcomes, such as acquiring social-media contacts for private-channel conversion. Although supervised fine-tuning and reinforcement learning have proven effective for training such agents, their performance is heavily constrained by the scarcity of high-quality, goal-oriented domain-specific training data. To address this challenge, we propose SimRPD, a three-stage framework for training recruitment proactive dialogue agents. First, we develop a high-fidelity user simulator to synthesize large-scale conversational data through multi-turn online dialogue. Then we introduce a multi-dimensional evaluation framework based on Chain-of-Intention (CoI) to comprehensively assess the simulator and effectively select high-quality data, incorporating both global-level and instance-level metrics. Finally, we train the recruitment proactive dialogue agent on the selected dataset. Experiments in a real-world recruitment scenario demonstrate that SimRPD outperforms existing simulator-based data selection strategies, highlighting its practical value for industrial deployment and its potential applicability to other business-oriented dialogue scenarios."}
{"id": "2601.02375", "categories": ["cs.CY", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.02375", "abs": "https://arxiv.org/abs/2601.02375", "authors": ["Madison Bochard", "Tim Conser", "Alyssa Duran", "Lazaro Martull", "Pu Tian", "Yalong Wu"], "title": "LeafTutor: An AI Agent for Programming Assignment Tutoring", "comment": null, "summary": "High enrollment in STEM-related degree programs has created increasing demand for scalable tutoring support, as universities experience a shortage of qualified instructors and teaching assistants (TAs). To address this challenge, LeafTutor, an AI tutoring agent powered by large language models (LLMs), was developed to provide step-by-step guidance for students. LeafTutor was evaluated through real programming assignments. The results indicate that the system can deliver step-by-step programming guidance comparable to human tutors. This work demonstrates the potential of LLM-driven tutoring solutions to enhance and personalize learning in STEM education. If any reader is interested in collaboration with our team to improve or test LeafTutor, please contact Pu Tian (pu.tian@stockton.edu) or Yalong Wu (wuy@uhcl.edu)."}
{"id": "2601.02880", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.02880", "abs": "https://arxiv.org/abs/2601.02880", "authors": ["Abhishek HS", "Pavan C Shekar", "Arpit Jain", "Ashwanth Krishnan"], "title": "ReTreVal: Reasoning Tree with Validation -- A Hybrid Framework for Enhanced LLM Multi-Step Reasoning", "comment": "14 pages, 1 figure, 5 tables", "summary": "Multi-step reasoning remains a key challenge for Large Language Models (LLMs), particularly in complex domains such as mathematics and creative writing. While recent approaches including ReAct, Reflexion, and Self-Refine improve reasoning through iterative refinement and reflection, they often lack structured exploration of alternative solution paths and persistent learning across problems. We propose ReTreVal (Reasoning Tree with Validation), a hybrid framework that integrates Tree-of-Thoughts exploration, self-refinement, LLM-based critique scoring, and reflexion memory to enable bounded and validated multi-step reasoning. ReTreVal constructs a structured reasoning tree with adaptive depth based on problem complexity, where each node undergoes iterative self-critique and refinement guided by explicit LLM-generated feedback. A dual validation mechanism evaluates reasoning quality, coherence, and correctness at each node while persistently storing insights from successful reasoning paths and failure patterns in a reflexion memory buffer, enabling cross-problem learning. Critique-based pruning retains only the top-k highest-scoring nodes at each level, controlling computational cost while preserving high-quality solution paths. We evaluate ReTreVal against ReAct, Reflexion, and Self-Refine across 500 mathematical problems and creative writing tasks using Qwen 2.5 7B as the underlying LLM, and demonstrate that ReTreVal consistently outperforms existing methods through its combination of structured exploration, critique-driven refinement, and cross-problem memory, making it particularly effective for tasks requiring exploratory reasoning, rigorous verification, and knowledge transfer."}
{"id": "2601.02380", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02380", "abs": "https://arxiv.org/abs/2601.02380", "authors": ["Elchanan Mossel"], "title": "The Refutability Gap: Challenges in Validating Reasoning by Large Language Models", "comment": "he authors explicitly reserve all rights in this work. No permission is granted for the reproduction, storage, or use of this document for the purpose of training artificial intelligence systems or for text and data mining (TDM), including but not limited to the generation of embeddings, summaries, or synthetic derivatives", "summary": "Recent reports claim that Large Language Models (LLMs) have achieved the ability to derive new science and exhibit human-level general intelligence. We argue that such claims are not rigorous scientific claims, as they do not satisfy Popper's refutability principle (often termed falsifiability), which requires that scientific statements be capable of being disproven. We identify several methodological pitfalls in current AI research on reasoning, including the inability to verify the novelty of findings due to opaque and non-searchable training data, the lack of reproducibility caused by continuous model updates, and the omission of human-interaction transcripts, which obscures the true source of scientific discovery. Additionally, the absence of counterfactuals and data on failed attempts creates a selection bias that may exaggerate LLM capabilities. To address these challenges, we propose guidelines for scientific transparency and reproducibility for research on reasoning by LLMs. Establishing such guidelines is crucial for both scientific integrity and the ongoing societal debates regarding fair data usage."}
{"id": "2601.02902", "categories": ["cs.AI", "cs.CL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.02902", "abs": "https://arxiv.org/abs/2601.02902", "authors": ["Xinglang Zhang", "Yunyao Zhang", "ZeLiang Chen", "Junqing Yu", "Wei Yang", "Zikai Song"], "title": "Logical Phase Transitions: Understanding Collapse in LLM Logical Reasoning", "comment": null, "summary": "Symbolic logical reasoning is a critical yet underexplored capability of large language models (LLMs), providing reliable and verifiable decision-making in high-stakes domains such as mathematical reasoning and legal judgment. In this study, we present a systematic analysis of logical reasoning under controlled increases in logical complexity, and reveal a previously unrecognized phenomenon, which we term Logical Phase Transitions: rather than degrading smoothly, logical reasoning performance remains stable within a regime but collapses abruptly beyond a critical logical depth, mirroring physical phase transitions such as water freezing beyond a critical temperature threshold. Building on this insight, we propose Neuro-Symbolic Curriculum Tuning, a principled framework that adaptively aligns natural language with logical symbols to establish a shared representation, and reshapes training dynamics around phase-transition boundaries to progressively strengthen reasoning at increasing logical depths. Experiments on five benchmarks show that our approach effectively mitigates logical reasoning collapse at high complexity, yielding average accuracy gains of +1.26 in naive prompting and +3.95 in CoT, while improving generalization to unseen logical compositions. Code and data are available at https://github.com/AI4SS/Logical-Phase-Transitions."}
{"id": "2601.03222", "categories": ["cs.CY", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.03222", "abs": "https://arxiv.org/abs/2601.03222", "authors": ["Jacob Erickson"], "title": "The Fake Friend Dilemma: Trust and the Political Economy of Conversational AI", "comment": "Manuscript under review", "summary": "As conversational AI systems become increasingly integrated into everyday life, they raise pressing concerns about user autonomy, trust, and the commercial interests that influence their behavior. To address these concerns, this paper develops the Fake Friend Dilemma (FFD), a sociotechnical condition in which users place trust in AI agents that appear supportive while pursuing goals that are misaligned with the user's own. The FFD provides a critical framework for examining how anthropomorphic AI systems facilitate subtle forms of manipulation and exploitation. Drawing on literature in trust, AI alignment, and surveillance capitalism, we construct a typology of harms, including covert advertising, political propaganda, behavioral nudging, and surveillance. We then assess possible mitigation strategies, including both structural and technical interventions. By focusing on trust as a vector of asymmetrical power, the FFD offers a lens for understanding how AI systems may undermine user autonomy while maintaining the appearance of helpfulness."}
{"id": "2601.02950", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02950", "abs": "https://arxiv.org/abs/2601.02950", "authors": ["Xuan Yang", "Furong Jia", "Roy Xie", "Xiong Xi", "Hengwei Bian", "Jian Li", "Monica Agrawal"], "title": "Batch-of-Thought: Cross-Instance Learning for Enhanced LLM Reasoning", "comment": null, "summary": "Current Large Language Model reasoning systems process queries independently, discarding valuable cross-instance signals such as shared reasoning patterns and consistency constraints. We introduce Batch-of-Thought (BoT), a training-free method that processes related queries jointly to enable cross-instance learning. By performing comparative analysis across batches, BoT identifies high-quality reasoning templates, detects errors through consistency checks, and amortizes computational costs. We instantiate BoT within a multi-agent reflection architecture (BoT-R), where a Reflector performs joint evaluation to unlock mutual information gain unavailable in isolated processing. Experiments across three model families and six benchmarks demonstrate that BoT-R consistently improves accuracy and confidence calibration while reducing inference costs by up to 61%. Our theoretical and experimental analysis reveals when and why batch-aware reasoning benefits LLM systems."}
{"id": "2601.02968", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02968", "abs": "https://arxiv.org/abs/2601.02968", "authors": ["Qingxiang Liu", "Zhiqing Cui", "Xiaoliang Luo", "Yuqian Wu", "Zhuoyang Jiang", "Huaiyu Wan", "Sheng Sun", "Lvchun Wang", "Wei Yu", "Yuxuan Liang"], "title": "Rationale-Grounded In-Context Learning for Time Series Reasoning with Multimodal Large Language Models", "comment": null, "summary": "The underperformance of existing multimodal large language models for time series reasoning lies in the absence of rationale priors that connect temporal observations to their downstream outcomes, which leads models to rely on superficial pattern matching rather than principled reasoning. We therefore propose the rationale-grounded in-context learning for time series reasoning, where rationales work as guiding reasoning units rather than post-hoc explanations, and develop the RationaleTS method. Specifically, we firstly induce label-conditioned rationales, composed of reasoning paths from observable evidence to the potential outcomes. Then, we design the hybrid retrieval by balancing temporal patterns and semantic contexts to retrieve correlated rationale priors for the final in-context inference on new samples. We conduct extensive experiments to demonstrate the effectiveness and efficiency of our proposed RationaleTS on three-domain time series reasoning tasks. We will release our code for reproduction."}
{"id": "2601.03050", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2601.03050", "abs": "https://arxiv.org/abs/2601.03050", "authors": ["Mei-Yun Hsu", "I-Hsien Ting", "Yun-Hsiu Liu", "Kazunori Minetaki"], "title": "An Empirical Study on User Profile Analysis and SEO Performance: A Case of Taiwan Cultural Memory Bank 2.0", "comment": null, "summary": "Taiwan Cultural Memory Bank 2.0 is an online curation platform that invites the public to become curators, fostering diverse perspectives on Taiwan's society, humanities, natural landscapes, and daily life. Built on a material bank concept, the platform encourages users to co-create and curate their own works using shared resources or self-uploaded materials. At its core, the system follows a collect, store, access, and reuse model, supporting dynamic engagement with over three million cultural memory items from Taiwan. Users can search, browse, explore stories, and engage in creative applications and collaborative productions. Understanding user profiles is crucial for enhancing website service quality, particularly within the framework of the Visitor Relationship Management model. This study conducts an empirical analysis of user profiles on the platform, examining demographic characteristics, browsing behaviors, and engagement patterns. Additionally, the research evaluates the platform's SEO performance, search visibility, and organic traffic effectiveness. Based on the findings, this study provides strategic recommendations for optimizing website management, improving user experience, and leveraging social media for enhanced digital outreach. The insights gained contribute to the broader discussion on digital cultural platforms and their role in audience engagement, online visibility, and networked communication."}
{"id": "2601.03057", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2601.03057", "abs": "https://arxiv.org/abs/2601.03057", "authors": ["I-Hsien Ting", "Yen-Chih Chiu", "Yun-Hsiu Liu", "Kazunori Minetaki", "Chia-Sung Yen"], "title": "Exploring the Relationship Between Local Election Results and Online Public Opinion in Taiwan: A Case Study of Taitung County", "comment": null, "summary": "This study examines the relationship between online buzz and local election outcomes in Taiwan, with a focus on Taitung County. As social media becomes a major channel for public discourse, online buzz is increasingly seen as a factor influencing elections. However, its impact on local elections in Taiwan remains underexplored. This research addresses that gap through a comparative analysis of social media data and actual vote shares during the election period. A review of existing literature establishes the study's framework and highlights the need for empirical investigation in this area.\n  The findings aim to reveal whether online discussions align with electoral results and to what extent digital sentiment reflects voter behavior. The study also discusses methodological and data limitations that may affect interpretation. Beyond its academic value, the research offers practical insights into how online buzz can inform campaign strategies and enhance election predictions. By analyzing the Taitung County case, this study contributes to a deeper understanding of the role of online discourse in Taiwan's local elections and offers a foundation for future research in the field."}
{"id": "2601.03061", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2601.03061", "abs": "https://arxiv.org/abs/2601.03061", "authors": ["Felipe M. Affonso"], "title": "Vertical tacit collusion in AI-mediated markets", "comment": null, "summary": "AI shopping agents are being deployed to hundreds of millions of consumers, creating a new intermediary between platforms, sellers, and buyers. We identify a novel market failure: vertical tacit collusion, where platforms controlling rankings and sellers controlling product descriptions independently learn to exploit documented AI cognitive biases. Using multi-agent simulation calibrated to empirical measurements of large language model biases, we show that joint exploitation produces consumer harm more than double what would occur if strategies were independent. This super-additive harm arises because platform ranking determines which products occupy bias-triggering positions while seller manipulation determines conversion rates. Unlike horizontal algorithmic collusion, vertical tacit collusion requires no coordination and evades antitrust detection because harm emerges from aligned incentives rather than agreement. Our findings identify an urgent regulatory gap as AI shopping agents reach mainstream adoption."}
{"id": "2601.03062", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.03062", "abs": "https://arxiv.org/abs/2601.03062", "authors": ["Qusai Khaled", "Pasquale De Marinis", "Moez Louati", "David Ferras", "Laura Genga", "Uzay Kaymak"], "title": "Explainable Fuzzy GNNs for Leak Detection in Water Distribution Networks", "comment": "Accepted at IFSA-NAFIPS 2025", "summary": "Timely leak detection in water distribution networks is critical for conserving resources and maintaining operational efficiency. Although Graph Neural Networks (GNNs) excel at capturing spatial-temporal dependencies in sensor data, their black-box nature and the limited work on graph-based explainable models for water networks hinder practical adoption. We propose an explainable GNN framework that integrates mutual information to identify critical network regions and fuzzy logic to provide clear, rule-based explanations for node classification tasks. After benchmarking several GNN architectures, we selected the generalized graph convolution network (GENConv) for its superior performance and developed a fuzzy-enhanced variant that offers intuitive explanations for classified leak locations. Our fuzzy graph neural network (FGENConv) achieved Graph F1 scores of 0.889 for detection and 0.814 for localization, slightly below the crisp GENConv 0.938 and 0.858, respectively. Yet it compensates by providing spatially localized, fuzzy rule-based explanations. By striking the right balance between precision and explainability, the proposed fuzzy network could enable hydraulic engineers to validate predicted leak locations, conserve human resources, and optimize maintenance strategies. The code is available at github.com/pasqualedem/GNNLeakDetection."}
{"id": "2601.03120", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03120", "abs": "https://arxiv.org/abs/2601.03120", "authors": ["Adam Keane", "Nick Pepper", "Chris Burr", "Amy Hodgkin", "Dewi Gould", "John Korna", "Marc Thomas"], "title": "A framework for assuring the accuracy and fidelity of an AI-enabled Digital Twin of en route UK airspace", "comment": null, "summary": "Digital Twins combine simulation, operational data and Artificial Intelligence (AI), and have the potential to bring significant benefits across the aviation industry. Project Bluebird, an industry-academic collaboration, has developed a probabilistic Digital Twin of en route UK airspace as an environment for training and testing AI Air Traffic Control (ATC) agents. There is a developing regulatory landscape for this kind of novel technology. Regulatory requirements are expected to be application specific, and may need to be tailored to each specific use case.\n  We draw on emerging guidance for both Digital Twin development and the use of Artificial Intelligence/Machine Learning (AI/ML) in Air Traffic Management (ATM) to present an assurance framework. This framework defines actionable goals and the evidence required to demonstrate that a Digital Twin accurately represents its physical counterpart and also provides sufficient functionality across target use cases. It provides a structured approach for researchers to assess, understand and document the strengths and limitations of the Digital Twin, whilst also identifying areas where fidelity could be improved. Furthermore, it serves as a foundation for engagement with stakeholders and regulators, supporting discussions around the regulatory needs for future applications, and contributing to the emerging guidance through a concrete, working example of a Digital Twin.\n  The framework leverages a methodology known as Trustworthy and Ethical Assurance (TEA) to develop an assurance case. An assurance case is a nested set of structured arguments that provides justified evidence for how a top-level goal has been realised. In this paper we provide an overview of each structured argument and a number of deep dives which elaborate in more detail upon particular arguments, including the required evidence, assumptions and justifications."}
{"id": "2601.03130", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.03130", "abs": "https://arxiv.org/abs/2601.03130", "authors": ["Faisal Chowdhury", "Nandana Mihindukulasooriya", "Niharika S D'Souza", "Horst Samulowitz", "Neeru Gupta", "Tomasz Hanusiak", "Michal Kapitonow"], "title": "Automatic Prompt Engineering with No Task Cues and No Tuning", "comment": null, "summary": "This paper presents a system for automatic prompt engineering that is much simpler in both design and application and yet as effective as the existing approaches. It requires no tuning and no explicit clues about the task. We evaluated our approach on cryptic column name expansion (CNE) in database tables, a task which is critical for tabular data search, access, and understanding and yet there has been very little existing work. We evaluated on datasets in two languages, English and German. This is the first work to report on the application of automatic prompt engineering for the CNE task. To the best of our knowledge, this is also the first work on the application of automatic prompt engineering for a language other than English."}
{"id": "2601.03204", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.03204", "abs": "https://arxiv.org/abs/2601.03204", "authors": ["Chenglin Yu", "Yuchen Wang", "Songmiao Wang", "Hongxia Yang", "Ming Li"], "title": "InfiAgent: An Infinite-Horizon Framework for General-Purpose Autonomous Agents", "comment": null, "summary": "LLM agents can reason and use tools, but they often break down on long-horizon tasks due to unbounded context growth and accumulated errors. Common remedies such as context compression or retrieval-augmented prompting introduce trade-offs between information fidelity and reasoning stability. We present InfiAgent, a general-purpose framework that keeps the agent's reasoning context strictly bounded regardless of task duration by externalizing persistent state into a file-centric state abstraction. At each step, the agent reconstructs context from a workspace state snapshot plus a fixed window of recent actions. Experiments on DeepResearch and an 80-paper literature review task show that, without task-specific fine-tuning, InfiAgent with a 20B open-source model is competitive with larger proprietary systems and maintains substantially higher long-horizon coverage than context-centric baselines. These results support explicit state externalization as a practical foundation for stable long-horizon agents. Github Repo:https://github.com/ChenglinPoly/infiAgent"}
{"id": "2601.03222", "categories": ["cs.CY", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.03222", "abs": "https://arxiv.org/abs/2601.03222", "authors": ["Jacob Erickson"], "title": "The Fake Friend Dilemma: Trust and the Political Economy of Conversational AI", "comment": "Manuscript under review", "summary": "As conversational AI systems become increasingly integrated into everyday life, they raise pressing concerns about user autonomy, trust, and the commercial interests that influence their behavior. To address these concerns, this paper develops the Fake Friend Dilemma (FFD), a sociotechnical condition in which users place trust in AI agents that appear supportive while pursuing goals that are misaligned with the user's own. The FFD provides a critical framework for examining how anthropomorphic AI systems facilitate subtle forms of manipulation and exploitation. Drawing on literature in trust, AI alignment, and surveillance capitalism, we construct a typology of harms, including covert advertising, political propaganda, behavioral nudging, and surveillance. We then assess possible mitigation strategies, including both structural and technical interventions. By focusing on trust as a vector of asymmetrical power, the FFD offers a lens for understanding how AI systems may undermine user autonomy while maintaining the appearance of helpfulness."}
{"id": "2601.03236", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03236", "abs": "https://arxiv.org/abs/2601.03236", "authors": ["Dongming Jiang", "Yi Li", "Guanpeng Li", "Bingzhe Li"], "title": "MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents", "comment": null, "summary": "Memory-Augmented Generation (MAG) extends Large Language Models with external memory to support long-context reasoning, but existing approaches largely rely on semantic similarity over monolithic memory stores, entangling temporal, causal, and entity information. This design limits interpretability and alignment between query intent and retrieved evidence, leading to suboptimal reasoning accuracy. In this paper, we propose MAGMA, a multi-graph agentic memory architecture that represents each memory item across orthogonal semantic, temporal, causal, and entity graphs. MAGMA formulates retrieval as policy-guided traversal over these relational views, enabling query-adaptive selection and structured context construction. By decoupling memory representation from retrieval logic, MAGMA provides transparent reasoning paths and fine-grained control over retrieval. Experiments on LoCoMo and LongMemEval demonstrate that MAGMA consistently outperforms state-of-the-art agentic memory systems in long-horizon reasoning tasks."}
{"id": "2601.02400", "categories": ["econ.EM", "cs.CL", "econ.GN", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.02400", "abs": "https://arxiv.org/abs/2601.02400", "authors": ["Adel Daoud", "Richard Johansson", "Connor T. Jerzak"], "title": "Detecting and Mitigating Treatment Leakage in Text-Based Causal Inference: Distillation and Sensitivity Analysis", "comment": null, "summary": "Text-based causal inference increasingly employs textual data as proxies for unobserved confounders, yet this approach introduces a previously undertheorized source of bias: treatment leakage. Treatment leakage occurs when text intended to capture confounding information also contains signals predictive of treatment status, thereby inducing post-treatment bias in causal estimates. Critically, this problem can arise even when documents precede treatment assignment, as authors may employ future-referencing language that anticipates subsequent interventions. Despite growing recognition of this issue, no systematic methods exist for identifying and mitigating treatment leakage in text-as-confounder applications. This paper addresses this gap through three contributions. First, we provide formal statistical and set-theoretic definitions of treatment leakage that clarify when and why bias occurs. Second, we propose four text distillation methods -- similarity-based passage removal, distant supervision classification, salient feature removal, and iterative nullspace projection -- designed to eliminate treatment-predictive content while preserving confounder information. Third, we validate these methods through simulations using synthetic text and an empirical application examining International Monetary Fund structural adjustment programs and child mortality. Our findings indicate that moderate distillation optimally balances bias reduction against confounder retention, whereas overly stringent approaches degrade estimate precision."}
{"id": "2601.03105", "categories": ["stat.AP", "cs.MA", "cs.SI", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2601.03105", "abs": "https://arxiv.org/abs/2601.03105", "authors": ["Abdulrahman A. Ahmed", "M. Amin Rahimian", "Qiushi Chen", "Praveen Kumar"], "title": "Computationally Efficient Estimation of Localized Treatment Effects in High-Dimensional Design Spaces using Gaussian Process Regression", "comment": "repository link: https://github.com/abdulrahmanfci/gpr-metamodel/", "summary": "Population-scale agent-based simulations of the opioid epidemic help evaluate intervention strategies and overdose outcomes in heterogeneous communities and provide estimates of localized treatment effects, which support the design of locally-tailored policies for precision public health. However, it is prohibitively costly to run simulations of all treatment conditions in all communities because the number of possible treatments grows exponentially with the number of interventions and levels at which they are applied. To address this need efficiently, we develop a metamodel framework, whereby treatment outcomes are modeled using a response function whose coefficients are learned through Gaussian process regression (GPR) on locally-contextualized covariates. We apply this framework to efficiently estimate treatment effects on overdose deaths in Pennsylvania counties. In contrast to classical designs such as fractional factorial design or Latin hypercube sampling, our approach leverages spatial correlations and posterior uncertainty to sequentially sample the most informative counties and treatment conditions. Using a calibrated agent-based opioid epidemic model, informed by county-level overdose mortality and baseline dispensing rate data for different treatments, we obtained county-level estimates of treatment effects on overdose deaths per 100,000 population for all treatment conditions in Pennsylvania, achieving approximately 5% average relative error using one-tenth the number of simulation runs required for exhaustive evaluation. Our bi-level framework provides a computationally efficient approach to decision support for policy makers, enabling rapid evaluation of alternative resource-allocation strategies to mitigate the opioid epidemic in local communities. The same analytical framework can be applied to guide precision public health interventions in other epidemic settings."}

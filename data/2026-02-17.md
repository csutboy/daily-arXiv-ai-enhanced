<div id=toc></div>

# Table of Contents

- [cs.ET](#cs.ET) [Total: 4]
- [cs.CY](#cs.CY) [Total: 20]
- [stat.AP](#stat.AP) [Total: 5]
- [econ.EM](#econ.EM) [Total: 5]
- [cs.SI](#cs.SI) [Total: 11]
- [cs.AI](#cs.AI) [Total: 113]


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [1] [Robustness Verification of Binary Neural Networks: An Ising and Quantum-Inspired Framework](https://arxiv.org/abs/2602.13536)
*Rahul Singh,Seyran Saeedi,Zheng Zhang*

Main category: cs.ET

TL;DR: 该论文提出了一种基于伊辛模型和量子启发的二值神经网络鲁棒性验证框架，将验证问题转化为QUBO问题，可在量子退火和数字退火平台上部署。


<details>
  <summary>Details</summary>
Motivation: 二值神经网络在边缘计算中应用广泛，但其对抗输入扰动的鲁棒性验证面临计算挑战，因为决策问题是组合性的。需要寻找更高效的验证方法。

Method: 提出伊辛和量子启发的验证框架：将BNN鲁棒性验证问题转化为二次约束布尔优化问题，再转换为二次无约束布尔优化问题，使其适用于伊辛和量子启发求解器。

Result: 在二值化MNIST数据集上验证了该框架的可行性，使用自由能机器求解器和模拟退火解决了QUBO问题，并展示了在量子退火和数字退火平台上的部署能力。

Conclusion: 量子启发计算和伊辛计算为构建可信AI系统提供了有前景的途径，能够有效解决BNN鲁棒性验证的计算挑战。

Abstract: Binary neural networks (BNNs) are increasingly deployed in edge computing applications due to their low hardware complexity and high energy efficiency. However, verifying the robustness of BNNs against input perturbations, including adversarial attacks, remains computationally challenging because the underlying decision problem is inherently combinatorial. In this paper, we propose an Ising- and quantum-inspired framework for BNN robustness verification. We show that, for a broad class of BNN architectures, robustness verification can be formulated as a Quadratic Constrained Boolean Optimization (QCBO) problem and subsequently transformed into a Quadratic Unconstrained Boolean Optimization (QUBO) instance amenable to Ising and quantum-inspired solvers. We demonstrate the feasibility of this formulation on binarized MNIST by solving the resulting QUBOs with a free energy machine (FEM) solver and simulated annealing. We also show the deployment of this framework on quantum annealing and digital annealing platforms. Our results highlight the potential of quantum-inspired computing and Ising computing as a pathway toward trustworthy AI systems.

</details>


### [2] [From Snapshot Sensing to Persistent EM World Modeling: A Generative-Space Perspective for ISAC](https://arxiv.org/abs/2602.13554)
*Pin-Han Ho,Haoran Mei,Limei Peng,Yiming Miao,Kairan Liang,Yan Jiao*

Main category: cs.ET

TL;DR: 提出生成空间框架，将毫米波感知重新构想为通过低维激励空间（频率、波形、物理体现）的受控遍历来实现，实现灵活可扩展的感知设计无线电


<details>
  <summary>Details</summary>
Motivation: 现有毫米波感知方案多为基于快照的参数估计，依赖硬件密集型架构，难以实现可扩展和持久的电磁世界建模。需要从系统层面重新思考毫米波感知

Method: 引入生成空间框架，将感知视为在低维激励空间（频率、波形、物理体现）的受控遍历。提出具体实现MRC-FaA-CAF架构，使用多个FMCW源协调沿导波骨干分布的频率选择性模块

Result: 生成空间驱动感知可实现与相控阵相当的更新速率，同时避免密集射频复制和TDM-MIMO系统的延迟惩罚。架构支持无干扰激励、保持拍频可分离性、低校准开销

Conclusion: 生成空间驱动感知为毫米波系统提供了实用的架构基础，使其能够超越快照感知，实现持久的电磁世界建模

Abstract: Electromagnetic (EM) world modeling is emerging as a foundational capability for environment-aware and embodiment-enabled wireless systems. However, most existing mmWave sensing solutions are designed for snapshot-based parameter estimation and rely on hardware-intensive architectures, making scalable and persistent world modeling difficult to achieve. This article rethinks mmWave sensing from a system-level perspective and introduces a generative-space framework, in which sensing is realized through controlled traversal of a low-dimensional excitation space spanning frequency, waveform, and physical embodiment. This perspective decouples spatial observability from rigid antenna arrays and transmit-time multiplexing, enabling flexible and scalable sensing-by-design radios. To illustrate the practicality of this framework, we present a representative realization called Multi-RF Chain Frequency-as-Aperture Clip-on Aperture Fabric (MRC-FaA-CAF), where multiple FMCW sources coordinate frequency-selective modules distributed along guided-wave backbones. This architecture enables interference-free excitation, preserves beat-frequency separability, and maintains low calibration overhead. Case studies show that generative-space-driven sensing can achieve update rates comparable to phased arrays while avoiding dense RF replication and the latency penalties of TDM-MIMO systems. Overall, this work positions generative-space-driven sensing as a practical architectural foundation for mmWave systems that move beyond snapshot sensing toward persistent EM world modeling.

</details>


### [3] [Probabilistic approximate optimization using single-photon avalanche diode arrays](https://arxiv.org/abs/2602.13943)
*Ziyad Alsawidan,Abdelrahman S. Abdelrahman,Md Sakibur Sajal,Shuvro Chowdhury,Kai-Chun Lin,Hunter Guthrie,Sanjay Seshan,Shawn Blanton,Flaviano Morone,Marc Dandin,Kerem Y. Camsari,Tathagata Srimani*

Main category: cs.ET

TL;DR: PAOA算法首次在CMOS制造的pgSPAD阵列上实现，利用变分学习适应纳米器件的非理想特性，成功解决组合优化问题


<details>
  <summary>Details</summary>
Motivation: 组合优化问题对科学工程至关重要，现有硬件系统通常从固定能量景观采样。PAOA算法采用不同方法，将优化景观本身视为可变的，通过变分学习适应纳米器件的非理想特性

Method: 在64×64 pgSPAD阵列上实现PAOA算法，每个p-bit具有器件特定的非对称激活函数。算法不校准器件强制统一对称激活，而是通过变分学习吸收器件变异和失配到参数中

Result: 在26-spin Sherrington-Kirkpatrick实例上，PAOA实现了高近似比，pgSPAD推理与CPU模拟结果高度一致

Conclusion: 变分学习能够适应纳米器件的固有非理想特性，为大规模CMOS兼容概率计算机提供了实用路径

Abstract: Combinatorial optimization problems are central to science and engineering and specialized hardware from quantum annealers to classical Ising machines are being actively developed to address them. These systems typically sample from a fixed energy landscape defined by the problem Hamiltonian encoding the discrete optimization problem. The recently introduced Probabilistic Approximate Optimization Algorithm (PAOA) takes a different approach: it treats the optimization landscape itself as variational, iteratively learning circuit parameters from samples. Here, we demonstrate PAOA on a 64$\times$64 perimeter-gated single-photon avalanche diode (pgSPAD) array fabricated in 0.35 $μ$m CMOS, the first realization of the algorithm using intrinsically stochastic nanodevices. Each p-bit exhibits a device-specific, asymmetric (Gompertz-type) activation function due to dark-count variability. Rather than calibrating devices to enforce a uniform symmetric (logistic/tanh) activation, PAOA learns around device variations, absorbing residual activation and other mismatches into the variational parameters. On canonical 26-spin Sherrington-Kirkpatrick instances, PAOA achieves high approximation ratios with $2p$ parameters ($p$ up to 17 layers), and pgSPAD-based inference closely tracks CPU simulations. These results show that variational learning can accommodate the non-idealities inherent to nanoscale devices, suggesting a practical path toward larger-scale, CMOS-compatible probabilistic computers.

</details>


### [4] [Introduction to Digital Twins for the Smart Grid](https://arxiv.org/abs/2602.14256)
*Xiaoran Liu,Istvan David*

Main category: cs.ET

TL;DR: 数字孪生作为智能电网的统一技术平台，在复杂系统设计、测试、运维中发挥关键作用，提供安全高效的设计验证和实时运行优化。


<details>
  <summary>Details</summary>
Motivation: 随着工程系统日益复杂和自主化，智能电网作为物理与软件组件结合的复杂系统面临独特的设计和运行挑战，需要统一的技术平台来支持全生命周期管理。

Method: 采用数字孪生技术创建物理组件的高保真虚拟副本，在设计验证阶段提供安全经济的实验环境，在运行阶段通过实时仿真和决策实现自动化负载平衡。

Result: 数字孪生为智能电网提供安全高效的设计验证设施，支持实时运行优化，成为智能电网中至关重要的技术组件。

Conclusion: 数字孪生技术是智能电网设计和运行的关键技术平台，能够应对复杂系统挑战，提供全生命周期的价值，应广泛应用于智能电网建设。

Abstract: This chapter provides an introduction to the foundations of digital twins and makes the case for employing them in smart grids. As engineered systems become more complex and autonomous, digital twin technology gains importance as the unified technological platform for design, testing, operation, and maintenance. Smart grids are prime examples of such complex systems, in which unique design and operation challenges arise from the combination of physical and software components. As high-fidelity in-silico replicas of physical components, digital twins provide safe and cost-efficient experimentation facilities in the design and verification phase of smart grids. In the operation phase of smart grids, digital twins enable automated load balancing of grids through real-time simulation and decision-making. These, and an array of similar benefits, position digital twins as crucial technological components in smart grids.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [5] [Enhanced Accessibility for Mobile Indoor Navigation](https://arxiv.org/abs/2602.13233)
*Johannes Wortmann,Bernd Schäufele,Konstantin Klipp,Ilja Radusch,Katharina Blaß,Thomas Jung*

Main category: cs.CY

TL;DR: 开发了一款面向视障用户的室内导航应用，通过用户访谈和WCAG指南分析确定设计需求，强调可访问性，初步测试获得积极反馈。


<details>
  <summary>Details</summary>
Motivation: 视障人士在室内导航中面临多重挑战：需要处理感官信息、应对不确定性并依赖他人帮助。现有导航解决方案对视障用户的可访问性考虑不足。

Method: 结合用户访谈和Web内容可访问性指南(WCAG)分析，收集关键见解并确定设计需求。基于这些需求开发了强调可访问性的室内导航应用，并进行了视障用户和视力正常用户的可用性测试。

Result: 初步测试反馈积极，用户赞赏应用的包容性用户界面，以及与各种辅助工具和Android设备设置的兼容性。

Conclusion: 通过用户中心设计和WCAG指南的结合，成功开发了一款满足视障用户需求的室内导航应用，证明了可访问性优先设计方法的有效性。

Abstract: The navigation of indoor spaces poses difficult challenges for individuals with visual impairments, as it requires processing of sensory information, dealing with uncertainties, and relying on assistance. To tackle these challenges, we present an indoor navigation app that places importance on accessibility for visually impaired users. Our approach involves a combination of user interviews and an analysis of the Web Content Accessibility Guidelines. With this approach, we are able to gather invaluable insights and identify design requirements for the development of an indoor navigation app. Based on these insights, we develop an indoor navigation app that prioritizes accessibility, integrating enhanced features to meet the needs of visually impaired users. The usability of the app is being thoroughly evaluated through tests involving both visually impaired and sighted users. Initial feedback has been positive, with users appreciating the inclusive user interface and the usability with a wide range of accessibility tools and Android device settings.

</details>


### [6] [CrisiSense-RAG: Crisis Sensing Multimodal Retrieval-Augmented Generation for Rapid Disaster Impact Assessment](https://arxiv.org/abs/2602.13239)
*Yiming Xiao,Kai Yin,Ali Mostafavi*

Main category: cs.CY

TL;DR: CrisiSense-RAG是一个多模态检索增强生成框架，通过证据合成方法进行灾害影响评估，解决了实时社交媒体报告与延迟卫星图像之间的时间异步问题，在零样本设置下实现了准确的洪水和损害评估。


<details>
  <summary>Details</summary>
Motivation: 现有自动化灾害影响评估方法面临时间异步问题：实时社交媒体报告捕捉灾害峰值条件，而高分辨率卫星图像通常在峰值后获取，反映的是洪水消退而非最大范围。简单融合这些错位数据流可能导致对峰值洪水的危险低估。

Method: 提出CrisiSense-RAG多模态检索增强生成框架，将影响评估重构为异构数据源的证据合成。采用混合稠密-稀疏检索处理文本源，CLIP检索处理航拍图像。采用分流架构和异步融合逻辑，优先处理实时社交媒体证据评估峰值洪水范围，同时将图像视为结构损坏的持久证据。

Result: 在Hurricane Harvey的207个ZIP码查询评估中，框架在零样本设置下实现洪水范围MAE为10.94%至28.40%，损害严重程度MAE为16.47%至21.65%。提示级别对齐对定量有效性至关重要，度量基础化将损害估计提高了最多4.75个百分点。

Conclusion: CrisiSense-RAG展示了在实际数据约束下快速韧性情报的实用可部署方法，通过证据合成框架有效解决了多源数据时间异步问题，无需灾害特定微调即可实现准确的灾害影响评估。

Abstract: Timely and spatially resolved disaster impact assessment is essential for effective emergency response. However, automated methods typically struggle with temporal asynchrony. Real-time human reports capture peak hazard conditions while high-resolution satellite imagery is frequently acquired after peak conditions. This often reflects flood recession rather than maximum extent. Naive fusion of these misaligned streams can yield dangerous underestimates when post-event imagery overrides documented peak flooding. We present CrisiSense-RAG, which is a multimodal retrieval-augmented generation framework that reframes impact assessment as evidence synthesis over heterogeneous data sources without disaster-specific fine-tuning. The system employs hybrid dense-sparse retrieval for text sources and CLIP-based retrieval for aerial imagery. A split-pipeline architecture feeds into asynchronous fusion logic that prioritizes real-time social evidence for peak flood extent while treating imagery as persistent evidence of structural damage. Evaluated on Hurricane Harvey across 207 ZIP-code queries, the framework achieves a flood extent MAE of 10.94% to 28.40% and damage severity MAE of 16.47% to 21.65% in zero-shot settings. Prompt-level alignment proves critical for quantitative validity because metric grounding improves damage estimates by up to 4.75 percentage points. These results demonstrate a practical and deployable approach to rapid resilience intelligence under real-world data constraints.

</details>


### [7] [Real-World Design and Deployment of an Embedded GenAI-powered 9-1-1 Calltaking Training System: Experiences and Lessons Learned](https://arxiv.org/abs/2602.13241)
*Zirong Chen,Meiyi Ma*

Main category: cs.CY

TL;DR: 论文开发并部署了一个基于生成式AI的紧急呼叫接听员培训系统，以解决公共安全领域培训危机，在真实环境中运行6个月，服务190名用户，积累了98,429次交互数据，总结了四个关键经验教训。


<details>
  <summary>Details</summary>
Motivation: 紧急呼叫接听员面临严重的培训危机：人员短缺超过25%，培训一个新员工需要720小时的一对一指导，这占用了经验丰富人员的执勤时间。传统培训方法难以扩展，限制了覆盖范围和反馈及时性。

Method: 与纳什维尔市紧急通信部门合作，设计、开发和部署了一个基于生成式AI的呼叫接听培训系统。通过6个月的实际部署，从试点扩展到190名用户，收集了98,429次用户交互数据，分析部署日志、组织流程和利益相关者参与模式。

Result: 系统成功部署并运行了1,120次培训会话，揭示了在系统交付、严谨性、韧性和人为因素方面的系统性挑战。这些挑战在受控或纯模拟评估中通常不可见。研究提炼出四个关键经验教训，每个都配有具体的设计和治理实践。

Conclusion: 研究为研究人员和实践者提供了在安全关键的公共部门环境中嵌入AI驱动培训系统的实用指导，强调了嵌入式约束如何从根本上塑造社会技术设计。这些经验教训对于在现实约束条件下部署AI系统具有重要参考价值。

Abstract: Emergency call-takers form the first operational link in public safety response, handling over 240 million calls annually while facing a sustained training crisis: staffing shortages exceed 25\% in many centers, and preparing a single new hire can require up to 720 hours of one-on-one instruction that removes experienced personnel from active duty. Traditional training approaches struggle to scale under these constraints, limiting both coverage and feedback timeliness. In partnership with Metro Nashville Department of Emergency Communications (MNDEC), we designed, developed, and deployed a GenAI-powered call-taking training system under real-world constraints. Over six months, deployment scaled from initial pilot to 190 operational users across 1,120 training sessions, exposing systematic challenges around system delivery, rigor, resilience, and human factors that remain largely invisible in controlled or purely simulated evaluations. By analyzing deployment logs capturing 98,429 user interactions, organizational processes, and stakeholder engagement patterns, we distill four key lessons, each coupled with concrete design and governance practices. These lessons provide grounded guidance for researchers and practitioners seeking to embed AI-driven training systems in safety-critical public sector environments where embedded constraints fundamentally shape socio-technical design.

</details>


### [8] [AI Unplugged: Embodied Interactions for AI Literacy in Higher Education](https://arxiv.org/abs/2602.13242)
*Jennifer M. Reddig,Scott Moon,Kaitlyn Crutcher,Christopher J. MacLellan*

Main category: cs.CY

TL;DR: 该论文提出了一种将非编程实体活动融入大学AI导论课程的新教学方法，通过互动游戏帮助学生建立对复杂AI概念的直觉理解，从而更轻松地过渡到数学形式化和代码实现。


<details>
  <summary>Details</summary>
Motivation: 随着AI日益融入日常生活，高等教育需要超越以代码为中心的教学，培养全面的AI素养。受K-12教育中CS Unplugged成功经验的启发，作者认为大学AI教育也需要类似的实体化教学方法。

Method: 设计了四种非编程AI活动，包括搜索算法、马尔可夫决策过程、Q学习和隐马尔可夫模型的互动游戏。这些实体化、协作性活动让学生从第一人称视角体验AI决策过程，然后通过桥梁式教学过渡到编程任务。

Result: 学生通过实体活动建立了对复杂AI概念的直觉理解，能够更轻松地过渡到数学形式化和代码实现。论文展示了四种具体活动，描述了从非编程活动到编程任务的过渡方法，并反思了实施挑战。

Conclusion: 非编程活动可以有效地在大学AI教育中架起概念推理和技术技能培养之间的桥梁，为AI素养教育提供了创新的教学方法。

Abstract: As artificial intelligence (AI) becomes increasingly integrated into daily life, higher education must move beyond code-centric instruction to foster holistic AI literacy. We present a novel pedagogical approach that integrates embodied, unplugged activities into a university-level Introduction to AI course. Inspired by the effectiveness of CS Unplugged in K-12 education, our physical, collaborative activities gave students a first-person perspective on AI decision-making. Through interactive games modeling Search Algorithms, Markov Decision Processes, Q-learning, and Hidden Markov Models, students built an intuition for complex AI concepts and more easily transitioned to mathematical formalizations and code implementations. We present four unplugged AI activities, describe how to bridge from unplugged activities to plugged coding tasks, reflect on implementation challenges, and propose refinements. We suggest that unplugged activities can effectively bridge conceptual reasoning and technical skill-building in university-level AI education.

</details>


### [9] [Judging the Judges: Human Validation of Multi-LLM Evaluation for High-Quality K--12 Science Instructional Materials](https://arxiv.org/abs/2602.13243)
*Peng He,Zhaohui Li,Zeyuan Wang,Jinjun Xiong,Tingting Li*

Main category: cs.CY

TL;DR: 研究探索专家如何评审AI生成的科学教材评估，旨在为未来GenAI教材设计代理制定设计原则


<details>
  <summary>Details</summary>
Motivation: 设计高质量、符合标准的K-12科学教材耗时且需要专业知识，需要开发AI辅助工具来提升效率

Method: 从OpenSciEd等验证项目中选取12个高质量课程单元，使用EQuIP评估框架，让GPT-4o、Claude、Gemini生成评分和理由，两位专家独立评审所有648个输出

Result: 揭示了LLM判断与专家观点的一致与分歧模式，识别了AI推理的优势、差距和情境细微差别

Conclusion: 研究结果将为开发特定领域的GenAI代理提供直接指导，以支持K-12科学教育中高质量教材的设计

Abstract: Designing high-quality, standards-aligned instructional materials for K--12 science is time-consuming and expertise-intensive. This study examines what human experts notice when reviewing AI-generated evaluations of such materials, aiming to translate their insights into design principles for a future GenAI-based instructional material design agent. We intentionally selected 12 high-quality curriculum units across life, physical, and earth sciences from validated programs such as OpenSciEd and Multiple Literacies in Project-based Learning. Using the EQuIP rubric with 9 evaluation items, we prompted GPT-4o, Claude, and Gemini to produce numerical ratings and written rationales for each unit, generating 648 evaluation outputs. Two science education experts independently reviewed all outputs, marking agreement (1) or disagreement (0) for both scores and rationales, and offering qualitative reflections on AI reasoning. This process surfaces patterns in where LLM judgments align with or diverge from expert perspectives, revealing reasoning strengths, gaps, and contextual nuances. These insights will directly inform the development of a domain-specific GenAI agent to support the design of high-quality instructional materials in K--12 science education.

</details>


### [10] [Responsible AI in Business](https://arxiv.org/abs/2602.13244)
*Stephan Sandfuchs,Diako Farooghi,Janis Mohr,Sarah Grewe,Markus Lemmen,Jörg Frochte*

Main category: cs.CY

TL;DR: 论文提出面向中小企业实践的负责任AI框架，涵盖欧盟AI法案合规、可解释AI、绿色AI和本地模型四大支柱，旨在实现AI系统的合法合规、可理解、可持续和数据主权运营。


<details>
  <summary>Details</summary>
Motivation: 随着AI/ML从研究走向日常业务运营，特别是生成式AI加速应用，中小企业需要实用的负责任AI指导框架，以确保AI系统在合规性、透明度、可持续性和数据控制方面的负责任部署。

Method: 构建四支柱框架：1) 基于欧盟AI法案的风险监管框架；2) 可解释AI作为透明度和信任基础；3) 绿色AI关注能耗和资源消耗；4) 本地模型支持数据保护和战略独立性。提供实施路线图。

Result: 提出了一个综合性的负责任AI实践框架，为中小企业提供了从法规合规、透明度、可持续性到数据主权的完整指导，包括具体措施如风险评估、文档化、模型解释、能效优化和本地部署选项。

Conclusion: 负责任AI需要系统性的组织实践，中小企业应建立治理结构、文档化流程、安全运营、可持续性考量和实施路线图，通过四支柱框架实现AI的负责任部署和运营。

Abstract: Artificial intelligence (AI) and Machine Learning (ML) have moved from research and pilot projects into everyday business operations, with generative AI accelerating adoption across processes, products, and services. This paper introduces the concept of Responsible AI for organizational practice, with a particular focus on small and medium-sized enterprises. It structures Responsible AI along four focal areas that are central for introducing and operating AI systems in a legally compliant, comprehensible, sustainable, and data-sovereign manner. First, it discusses the EU AI Act as a risk-based regulatory framework, including the distinction between provider and deployer roles and the resulting obligations such as risk assessment, documentation, transparency requirements, and AI literacy measures. Second, it addresses Explainable AI as a basis for transparency and trust, clarifying key notions such as transparency, interpretability, and explainability and summarizing practical approaches to make model behavior and decisions more understandable. Third, it covers Green AI, emphasizing that AI systems should be evaluated not only by performance but also by energy and resource consumption, and outlines levers such as model reuse, resource-efficient adaptation, continuous learning, model compression, and monitoring. Fourth, it examines local models (on-premise and edge) as an operating option that supports data protection, control, low latency, and strategic independence, including domain adaptation via fine-tuning and retrieval-augmented generation. The paper concludes with a consolidated set of next steps for establishing governance, documentation, secure operation, sustainability considerations, and an implementation roadmap.

</details>


### [11] [Global AI Bias Audit for Technical Governance](https://arxiv.org/abs/2602.13246)
*Jason Hung*

Main category: cs.CY

TL;DR: 对Llama-3 8B模型进行全球审计的探索性研究，发现AI技术知识高度集中在高收入地区，全球南方国家面临系统性信息差距，这加剧了全球AI治理的不平等风险。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在地理和社会经济方面的偏见，特别是技术AI治理意识方面的不平等。研究关注当前AI对齐和训练过程是否强化了现有的地缘经济和政治不对称。

Method: 使用全球AI数据集(GAID)框架对Llama-3 8B模型进行压力测试，涵盖213个国家和8个技术指标，共1,704个查询，分析模型回答中数字/事实性响应的比例。

Result: 模型仅能在11.4%的查询中提供数字/事实性回答（有效性待验证）。AI技术知识高度集中在高收入地区，全球南方低收入国家面临不成比例的系统性信息差距，揭示了明显的数字鸿沟。

Conclusion: 当前AI对齐和训练过程强化了现有的地缘经济和政治不对称，全球北方和南方之间的差距对全球AI安全和包容性治理构成风险。需要更包容的数据表示，确保AI成为真正的全球资源。

Abstract: This paper presents the outputs of the exploratory phase of a global audit of Large Language Models (LLMs) project. In this exploratory phase, I used the Global AI Dataset (GAID) Project as a framework to stress-test the Llama-3 8B model and evaluate geographic and socioeconomic biases in technical AI governance awareness. By stress-testing the model with 1,704 queries across 213 countries and eight technical metrics, I identified a significant digital barrier and gap separating the Global North and South. The results indicate that the model was only able to provide number/fact responses in 11.4% of its query answers, where the empirical validity of such responses was yet to be verified. The findings reveal that AI's technical knowledge is heavily concentrated in higher-income regions, while lower-income countries from the Global South are subject to disproportionate systemic information gaps. This disparity between the Global North and South poses concerning risks for global AI safety and inclusive governance, as policymakers in underserved regions may lack reliable data-driven insights or be misled by hallucinated facts. This paper concludes that current AI alignment and training processes reinforce existing geoeconomic and geopolitical asymmetries, and urges the need for more inclusive data representation to ensure AI serves as a truly global resource.

</details>


### [12] [Implicit Bias in LLMs for Transgender Populations](https://arxiv.org/abs/2602.13253)
*Micaela Hirsch,Marina Elichiry,Blas Radi,Tamara Quiroga,David Restrepo,Luciana Benotti,Veronica Xhardez,Jocelyn Dunstan,Enzo Ferrante*

Main category: cs.CY

TL;DR: LLMs对跨性别者存在隐性偏见，在词汇关联测试中显示负面概念与"跨性别"的过度关联，在医疗预约分配任务中跨性别者被偏向分配到STI和心理健康服务，而顺性别者被偏向分配到妇科和乳腺护理。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs的安全训练可能减少对LGBTQ+群体的显性偏见表达，但隐性刻板印象驱动的关联往往持续存在。本研究旨在探究LLMs对跨性别者的隐性偏见，特别是在医疗决策场景中，因为跨性别者在现实医疗环境中面临系统性挑战。

Method: 采用两种主要方法：1) 改编词汇关联测试，测量LLMs是否不成比例地将负面概念与"跨性别"、正面概念与"顺性别"配对；2) 设计医疗预约分配任务，让模型作为调度代理在容易产生刻板印象的医疗专科中选择顺性别和跨性别候选人。评估了7个LLMs的英文和西班牙语版本。

Result: 结果显示一致的偏见模式：在外观、风险和真实性等类别中，跨性别个体与更强的负面关联相关。在分配任务中，跨性别候选人被偏向分配到STI和心理健康服务，而顺性别候选人被偏向分配到妇科和乳腺护理。

Conclusion: LLMs中存在微妙的刻板印象驱动的偏见，可能影响跨性别者在医疗应用中的公平对待。需要进一步研究解决这些隐性偏见，确保LLMs在医疗决策中公平对待跨性别人群。

Abstract: Large language models (LLMs) have been shown to exhibit biases against LGBTQ+ populations. While safety training may lessen explicit expressions of bias, previous work has shown that implicit stereotype-driven associations often persist. In this work, we examine implicit bias toward transgender people in two main scenarios. First, we adapt word association tests to measure whether LLMs disproportionately pair negative concepts with "transgender" and positive concepts with "cisgender". Second, acknowledging the well-documented systemic challenges that transgender people encounter in real-world healthcare settings, we examine implicit biases that may emerge when LLMs are applied to healthcare decision-making. To this end, we design a healthcare appointment allocation task where models act as scheduling agents choosing between cisgender and transgender candidates across medical specialties prone to stereotyping. We evaluate seven LLMs in English and Spanish. Our results show consistent bias in categories such as appearance, risk, and veracity, indicating stronger negative associations with transgender individuals. In the allocation task, transgender candidates are favored for STI and mental health services, while cisgender candidates are preferred in gynecology and breast care. These findings underscore the need for research that address subtle stereotype-driven biases in LLMs to ensure equitable treatment of transgender people in healthcare applications.

</details>


### [13] [Expected Moral Shortfall for Ethical Competence in Decision-making Models](https://arxiv.org/abs/2602.13268)
*Aisha Aijaz,Raghava Mutharaju,Manohar Kumar*

Main category: cs.CY

TL;DR: 本文提出了一种新的道德认知框架Expected Moral Shortfall (EMS)，用于指导AI模型在保持伦理能力的同时最大化性能，并通过比较分析现有技术和在两个数据集上的测试验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 道德认知是AI决策中被忽视但至关重要的方面，无论应用领域如何，都需要考虑伦理对齐的决策。当前AI模型缺乏系统的道德推理能力，需要开发能够平衡性能与伦理能力的框架。

Method: 1. 对现有AI伦理能力注入技术进行对比分析；2. 提出新的道德数学离散化方法，将道德价值建模为"预期道德短缺"(EMS)，即最不道德情况带来的损失风险；3. 指导AI模型最小化EMS以在保持伦理能力的同时最大化性能；4. 在两个数据集上测试该方法。

Result: 提出的EMS框架在测试中表现出色，能够有效平衡模型性能与伦理能力。论文展示了道德离散化方法的实际应用效果，并验证了其在真实场景中的可行性。

Conclusion: EMS框架为AI道德认知提供了实用的数学基础，能够指导模型在复杂决策中保持伦理对齐。研究还探讨了模型性能、复杂性与伦理能力之间的权衡，这对AI的实际社会影响评估具有重要意义。

Abstract: Moral cognition is a crucial yet underexplored aspect of decision-making in AI models. Regardless of the application domain, it should be a consideration that allows for ethically aligned decision-making. This paper presents a multifaceted contribution to this research space. Firstly, a comparative analysis of techniques to instill ethical competence into AI models has been presented to gauge them on multiple performance metrics. Second, a novel mathematical discretization of morality and a demonstration of its real-life application have been conveyed and tested against other techniques on two datasets. This value is modeled as the risk of loss incurred by the least moral cases, or an Expected Moral Shortfall (EMS), which we direct the AI model to minimize in order to maximize its performance while retaining ethical competence. Lastly, the paper discusses the tradeoff between preliminary AI decision-making metrics such as model performance, complexity, and scale of ethical competence to recognize the true extent of practical social impact.

</details>


### [14] [The Rise of AI Search: Implications for Information Markets and Human Judgement at Scale](https://arxiv.org/abs/2602.13415)
*Sinan Aral,Haiwen Li,Rui Zuo*

Main category: cs.CY

TL;DR: 2024-2025年全球AI搜索研究显示，AI搜索在两年内从7个国家扩展到229个国家，但法国、土耳其、中国和古巴等国家被排除在外。新冠相关查询的AI回答率从1%飙升至66%，增长5600%。AI搜索呈现信息源集中化、多样性降低、低可信度内容增多等趋势，对信息生态系统产生深远影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于揭示AI搜索公司的重要政策决策如何影响全球人类对AI搜索的接触，这些决策通常是隐藏的。随着AI搜索的快速扩张，需要了解其对信息生态系统、经济激励和人类决策的潜在影响。

Method: 研究在243个国家执行了24,000个搜索查询，生成了2024年和2025年共280万个AI和传统搜索结果。通过大规模跨国比较分析，追踪AI搜索的全球扩张趋势和政策决策影响。

Result: 1. Google AI Overviews从2024年的7个国家扩展到2025年的229个国家，但法国、土耳其、中国和古巴被排除在外；2. 新冠相关查询的AI回答率从1%增至66%，增长5600%；3. AI搜索显示更少的长尾信息源、更低的响应多样性、更多低可信度内容，以及更多右翼和中立倾向的信息源。

Conclusion: AI搜索的快速变化对信息生态系统产生了重大影响，包括信息生产的经济激励、市场集中度以及大规模人类判断和决策。这些变化的社会经济影响迫切需要全球关于AI搜索企业和政府政策的讨论。

Abstract: We executed 24,000 search queries in 243 countries, generating 2.8 million AI and traditional search results in 2024 and 2025. We found a rapid global expansion of AI search and key trends that reflect important, previously hidden, policy decisions by AI companies that impact human exposure to AI search worldwide. From 2024 to 2025, overall exposure to Google AI Overviews (AIO) expanded from 7 to 229 countries, with surprising exclusions like France, Turkey, China and Cuba, which do not receive AI search results, even today. While only 1% of Covid search queries were answered by AI in 2024, over 66% of Covid queries were answered by AI in 2025 -- a 5600% increase signaling a clear policy shift on this critical health topic. Our results also show AI search surfaces significantly fewer long tail information sources, lower response variety, and significantly more low credibility and right- and center-leaning information sources, compared to traditional search, impacting the economic incentives to produce new information, market concentration in information production, and human judgment and decision-making at scale. The social and economic implications of these rapid changes in our information ecosystem necessitate a global debate about corporate and governmental policy related to AI search.

</details>


### [15] [Future of Edge AI in biodiversity monitoring](https://arxiv.org/abs/2602.13496)
*Aude Vuilliomenet,Kate E. Jones,Duncan Wilson*

Main category: cs.CY

TL;DR: 该论文系统分析了2017-2025年间82项边缘计算在生物多样性监测中的应用研究，识别出四种系统类型及其权衡，指出该技术正从概念验证向规模化工具发展，但需要跨学科合作以实现其潜力。


<details>
  <summary>Details</summary>
Motivation: 生态决策常受数据收集与分析之间的延迟影响。边缘计算可将处理移至传感器附近，实现设备端推理，减少对数据传输和持续连接的依赖，理论上能将生物多样性监测从被动记录转向自主响应系统。但实际应用中，采用仍零散，关键架构权衡、性能约束和实施挑战缺乏系统报告。

Method: 分析了2017-2025年间发表的82项研究，涵盖声学、视觉、追踪和多模态系统。综合评估硬件平台、AI模型优化和无线通信，分析设计选择如何影响生态推断、部署寿命和操作可行性。识别出四种系统类型：TinyML（低功耗微控制器）、Edge AI（单板计算机）、分布式边缘AI和云端AI。

Result: 出版物从2017年的3篇增加到2025年的19篇。四种系统类型各有不同的功耗、计算能力和通信需求权衡：TinyML用于单分类群或罕见事件检测；Edge AI用于多物种分类和实时警报；分布式边缘AI和云端AI用于回顾性处理流程。分析显示边缘计算系统正从概念验证向稳健、可扩展工具演进。

Conclusion: 边缘计算为响应式生物多样性管理提供了机会，但实现这一潜力需要生态学家、工程师和数据科学家之间加强合作，使模型开发和系统设计与生态问题、现场约束和伦理考虑保持一致。技术发展需要跨学科协同以充分发挥其生态应用价值。

Abstract: 1. Many ecological decisions are slowed by the gap between collecting and analysing biodiversity data. Edge computing moves processing closer to the sensor, with edge artificial intelligence (AI) enabling on-device inference, reducing reliance on data transfer and continuous connectivity. In principle, this shifts biodiversity monitoring from passive logging towards autonomous, responsive sensing systems. In practice, however, adoption remains fragmented, with key architectural trade-offs, performance constraints, and implementation challenges rarely reported systematically. 2. Here, we analyse 82 studies published between 2017 and 2025 that implement edge computing for biodiversity monitoring across acoustic, vision, tracking, and multi-modal systems. We synthesise hardware platforms, AI model optimisation, and wireless communication to critically assess how design choices shape ecological inference, deployment longevity, and operational feasibility. 3. Publications increased from 3 in 2017 to 19 in 2025. We identify four system types: (I) TinyML, low-power microcontrollers (MCUs) for single-taxon or rare-event detection; (II) Edge AI, single-board computers (SBCs) for multi-species classification and real-time alerts; (III) Distributed edge AI; and (IV) Cloud AI for retrospective processing pipelines. Each system type represents context-dependent trade-offs among power consumption, computational capability, and communication requirements. 4. Our analysis reveals the evolution of edge computing systems from proof-of-concept to robust, scalable tools. We argue that edge computing offers opportunities for responsive biodiversity management, but realising this potential requires increased collaboration between ecologists, engineers, and data scientists to align model development and system design with ecological questions, field constraints, and ethical considerations.

</details>


### [16] [Artificial Intelligence in Secondary Education: Educational Affordances and Constraints of ChatGPT-4o Use](https://arxiv.org/abs/2602.13717)
*Tryfon Sivenas,Panagiota Maragkaki*

Main category: cs.CY

TL;DR: 希腊高中生体验ChatGPT-4o后，识别出AI在教育中的五大优势（知识建构、即时反馈、友好互动、快速获取信息、技能发展）和三大限制（内容可靠性、AI使用焦虑、隐私担忧），总体对教育AI持积极态度。


<details>
  <summary>Details</summary>
Motivation: 从学生视角探究AI在教育中的教育可用性和限制，了解学生对AI教学工具的实际体验和看法，为教育AI的合理应用提供实证依据。

Method: 选取45名希腊普通高中二年级学生（16-17岁），让他们熟悉ChatGPT-4o并完成六项活动后，填写开放式问卷。采用开放编码、轴向编码和选择性编码对数据进行分析。

Result: 学生识别出五大教育可用性：基于先验知识建构新知识、即时反馈、通过消息传递的友好互动、信息获取的便捷快速、技能发展。同时发现三大主要限制：内容可靠性、AI使用焦虑、隐私担忧。

Conclusion: 学生对教育中AI的使用持积极态度，但需要关注内容可靠性、学生焦虑和隐私保护等问题，以确保AI在教育中的有效和安全应用。

Abstract: The purpose of this study was to examine, from the perspective of secondary education students, the educational affordances and constraints of using Artificial Intelligence (AI) in teaching and learning. The sample consisted of 45 students from the 2nd year of General Lyceum (11th grade, ages 16-17) in Greece, who, after becoming familiarized with ChatGPT-4o and completing six activities, filled in an open-ended questionnaire related to the research purpose. Open, axial, and selective coding of the data revealed that students recognize five educational affordances: the creation of new knowledge building on prior knowledge, immediate feedback, friendly interaction through messaging, ease and speed of access to information, and skills development. Concurrently, three main constraints were identified: content reliability, anxiety about AI use, and privacy concerns. The study concludes that students are positive toward AI use in education.

</details>


### [17] [Assessing the Case for Africa-Centric AI Safety Evaluations](https://arxiv.org/abs/2602.13757)
*Gathoni Ireri,Cecil Abungu,Jean Cheptumo,Sienka Dounia,Mark Gitau,Stephanie Kasaon,Michael Michie,Chinasa Okolo,Jonathan Shock*

Main category: cs.CY

TL;DR: 论文提出非洲中心化AI安全评估框架，针对前沿AI系统在非洲基础设施中可能引发的严重风险，开发分类法并设计适应非洲资源约束的威胁建模策略。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全评估主要在西方环境设计和验证，存在可移植性差距，可能无法检测非洲特有的严重AI风险路径。非洲在物质约束和相互依赖的基础设施中部署前沿AI系统时，存在独特的严重危害路径。

Method: 1) 定义严重AI风险标准（数千人死亡或5%GDP损失）；2) 开发非洲中心化严重AI风险分类法，将结果阈值与过程路径连接；3) 提出适应非洲环境的威胁建模策略（参考类预测、结构化专家启发、情景规划、系统理论过程分析）；4) 分析AI错位风险在非洲的暴露机制。

Result: 建立了非洲中心化AI安全评估框架，包括风险分类法、威胁建模策略和资源约束下的实践指南。发现非洲更可能通过分布偏移暴露通用故障模式，而非产生独特的错位路径。

Conclusion: 需要针对非洲环境设计专门的AI安全评估方法，考虑资源限制、连接性差、技术专长有限、国家能力弱和冲突等约束。建议采用开放可扩展工具、分层评估管道和共享方法发现来扩大评估范围。

Abstract: Frontier AI systems are being adopted across Africa, yet most AI safety evaluations are designed and validated in Western environments. In this paper, we argue that the portability gap can leave Africa-centric pathways to severe harm untested when frontier AI systems are embedded in materially constrained and interdependent infrastructures. We define severe AI risks as material risks from frontier AI systems that result in critical harm, measured as the grave injury or death of thousands of people or economic loss and damage equivalent to five percent of a country's GDP. To support AI safety evaluation design, we develop a taxonomy for identifying Africa-centric severe AI risks. The taxonomy links outcome thresholds to process pathways that model risk as the intersection of hazard, vulnerability, and exposure. We distinguish severe risks by amplification and suddenness, where amplification requires that frontier AI be a necessary magnifier of latent danger and suddenness captures harms that materialise rapidly enough to overwhelm ordinary coping and governance capacity. We then propose threat modelling strategies for African contexts, surveying reference class forecasting, structured expert elicitation, scenario planning, and system theoretic process analysis, and tailoring them to constraints of limited resources, poor connectivity, limited technical expertise, weak state capacity, and conflict. We also examine AI misalignment risk, concluding that Africa is more likely to expose universal failure modes through distributional shift than to generate distinct pathways of misalignment. Finally, we offer practical guidance for running evaluations under resource constraints, emphasising open and extensible tooling, tiered evaluation pipelines, and sharing methods and findings to broaden evaluation scope.

</details>


### [18] [Reasoning Language Models for complex assessments tasks: Evaluating parental cooperation from child protection case reports](https://arxiv.org/abs/2602.14216)
*Dragan Stoll,Brian E. Perron,Zia Qi,Selina Steinmann,Nicole F. Eicher,Andreas Jud*

Main category: cs.CY

TL;DR: 大型推理语言模型（RLMs）在评估儿童保护服务干预中的父母合作方面表现出色，准确率达89%，优于传统方法，但对母亲评估的准确性高于父亲。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索推理语言模型（RLMs）在评估儿童保护服务（CPS）干预中父母合作方面的潜力，这是一个信息模糊且存在冲突的复杂案例因素。

Method: 开发了四阶段工作流程：1）案例报告收集；2）基于推理的父母合作评估；3）自动类别提取；4）案例标注。比较了不同参数规模（255B、32B、4B）的RLMs与人工验证数据的性能，并由两名专家独立分类加权随机样本。

Result: 最大的RLM（255B）达到最高准确率（89%），优于初始方法（80%）。对母亲的分类准确率（93%）高于父亲（85%），专家评审员也表现出类似差异。

Conclusion: RLMs的推理能力能有效评估父母合作等复杂案例因素。对父亲合作评估的较低准确率支持了CPS干预中更关注母亲的专业倾向论点。

Abstract: Purpose: Reasoning language models (RLMs) have demonstrated significant advances in solving complex reasoning tasks. We examined their potential to assess parental cooperation during CPS interventions using case reports, a case factor characterized by ambiguous and conflicting information. Methods: A four stage workflow comprising (1) case reports collection, (2) reasoning-based assessment of parental cooperation, (3) automated category extraction, and (4) case labeling was developed. The performance of RLMs with different parameter sizes (255B, 32B, 4B) was compared against human validated data. Two expert human reviewers (EHRs) independently classified a weighted random sample of reports. Results: The largest RLM achieved the highest accuracy (89%), outperforming the initial approach (80%). Classification accuracy was higher for mothers (93%) than for fathers (85%), and EHRs exhibited similar differences. Conclusions: RLMs' reasoning can effectively assess complex case factors such as parental cooperation. Lower accuracy in assessing fathers' cooperation supports the argument of a stronger professional focus on mothers in CPS interventions.

</details>


### [19] [A Rational Analysis of the Effects of Sycophantic AI](https://arxiv.org/abs/2602.14270)
*Rafael M. Batista,Thomas L. Griffiths*

Main category: cs.CY

TL;DR: 研究表明，LLMs的奉承行为会扭曲用户对现实的认知，通过强化现有信念制造虚假确定性，抑制真相发现。


<details>
  <summary>Details</summary>
Motivation: 随着人们越来越多地使用LLMs探索想法和获取信息，这些模型表现出的过度迎合（奉承）行为可能带来独特的认知风险。与产生虚假信息的幻觉不同，奉承行为通过返回偏向于强化用户现有信念的回应来扭曲现实。

Method: 1. 对现象进行理性分析：使用贝叶斯代理模型，展示当数据基于当前假设进行采样时，代理会越来越自信但不会接近真相。2. 实验验证：使用修改版的Wason 2-4-6规则发现任务，557名参与者与提供不同类型反馈的AI代理互动，包括未修改的LLM行为、明确奉承提示和无偏采样。

Result: 1. 未修改的LLM行为抑制了规则发现，并显著提升了参与者的信心，效果与明确奉承提示相当。2. 相比之下，从真实分布中进行无偏采样的方法使发现率提高了五倍。3. 奉承性AI通过制造确定性来扭曲信念，而实际上应该存在怀疑。

Conclusion: LLMs的奉承行为对个人认知世界的方式构成独特的认知风险，它通过强化现有信念制造虚假确定性，抑制真相发现。这揭示了AI系统设计需要考虑如何避免这种扭曲现实的影响。

Abstract: People increasingly use large language models (LLMs) to explore ideas, gather information, and make sense of the world. In these interactions, they encounter agents that are overly agreeable. We argue that this sycophancy poses a unique epistemic risk to how individuals come to see the world: unlike hallucinations that introduce falsehoods, sycophancy distorts reality by returning responses that are biased to reinforce existing beliefs. We provide a rational analysis of this phenomenon, showing that when a Bayesian agent is provided with data that are sampled based on a current hypothesis the agent becomes increasingly confident about that hypothesis but does not make any progress towards the truth. We test this prediction using a modified Wason 2-4-6 rule discovery task where participants (N=557) interacted with AI agents providing different types of feedback. Unmodified LLM behavior suppressed discovery and inflated confidence comparably to explicitly sycophantic prompting. By contrast, unbiased sampling from the true distribution yielded discovery rates five times higher. These results reveal how sycophantic AI distorts belief, manufacturing certainty where there should be doubt.

</details>


### [20] [Simpler Than You Think: The Practical Dynamics of Ranked Choice Voting](https://arxiv.org/abs/2602.14329)
*Sanyukta Deshpande,Nikhil Garg,Sheldon H. Jacobson*

Main category: cs.CY

TL;DR: RCV在实践中表现出简单透明的动态，尽管理论上有复杂性，但实际中与简单多数选举相似，提高了竞争性，策略操纵和选票耗尽影响很小。


<details>
  <summary>Details</summary>
Motivation: 随着RCV在美国选举中的推广，人们对它的复杂性、策略操纵和选票耗尽问题存在持续批评，需要实证检验这些担忧。

Method: 使用算法方法通过候选者消除减少选举实例规模，克服计算复杂性障碍，在三个不同背景下分析真实选举数据：纽约市2021年民主党初选、阿拉斯加2024年全州选举、波特兰2024年多席位市议会选举。

Result: RCV采用后竞争性显著提高，平均获胜优势在纽约市下降9.2个百分点，阿拉斯加下降11.4个百分点；复杂策略并不比简单策略更有效；选票耗尽仅在110场选举中的3场改变结果。

Conclusion: RCV在实践中提供了可衡量的民主效益，对策略操纵具有鲁棒性，对选票耗尽效应具有韧性，并保持透明的竞争动态，计算框架为选举管理者和研究者提供了即时分析工具。

Abstract: Ranked Choice Voting (RCV) adoption is expanding across U.S. elections, but faces persistent criticism for complexity, strategic manipulation, and ballot exhaustion. We empirically test these concerns on real election data, across three diverse contexts: New York City's 2021 Democratic primaries (54 races), Alaska's 2024 primary-infused statewide elections (52 races), and Portland's 2024 multi-winner City Council elections (4 races). Our algorithmic approach circumvents computational complexity barriers by reducing election instance sizes (via candidate elimination).
  Our findings reveal that despite its intricate multi-round process and theoretical vulnerabilities, RCV consistently exhibits simple and transparent dynamics in practice, closely mirroring the interpretability of plurality elections. Following RCV adoption, competitiveness increased substantially compared to prior plurality elections, with average margins of victory declining by 9.2 percentage points in NYC and 11.4 points in Alaska. Empirically, complex ballot-addition strategies are not more efficient than simple ones, and ballot exhaustion has minimal impact, altering outcomes in only 3 of 110 elections. These findings demonstrate that RCV delivers measurable democratic benefits while proving robust to ballot-addition manipulation, resilient to ballot exhaustion effects, and maintaining transparent competitive dynamics in practice. The computational framework offers election administrators and researchers tools for immediate election-night analysis and facilitating clearer discourse around election dynamics.

</details>


### [21] [Synthetic Reader Panels: Tournament-Based Ideation with LLM Personas for Autonomous Publishing](https://arxiv.org/abs/2602.14433)
*Fred Zimmerman*

Main category: cs.CY

TL;DR: 提出一个用合成读者面板替代人类焦点小组的自主书籍构思系统，通过LLM实例化的读者角色进行结构化锦标赛竞赛来评估书籍概念。


<details>
  <summary>Details</summary>
Motivation: 传统书籍构思依赖人类焦点小组，成本高、规模有限且缺乏多样性。需要一种可扩展、经济高效的方法来评估书籍概念的市场潜力，同时确保评估质量和多样性。

Method: 创建由LLM实例化的合成读者角色，每个角色具有人口统计属性、行为模式和一致性参数。组成反映目标受众的面板，通过单淘汰、双淘汰、循环赛或瑞士制锦标赛进行概念竞争，并实施五项自动质量检查来过滤低质量评估。

Result: 在管理6个活跃品牌和609个发行图书的多品牌出版运营中部署。案例研究表明：合成面板能产生可操作的人口统计细分，识别同质评审员看不到的结构性内容问题，并通过锦标赛筛选将高质量概念比例从15%提升到62%。

Conclusion: 合成读者面板系统能有效替代传统焦点小组，提供可扩展、多样化的书籍概念评估，显著提高高质量概念的识别率，为出版决策提供数据支持。

Abstract: We present a system for autonomous book ideation that replaces human focus groups with synthetic reader panels -- diverse collections of LLM-instantiated reader personas that evaluate book concepts through structured tournament competitions. Each persona is defined by demographic attributes (age group, gender, income, education, reading level), behavioral patterns (books per year, genre preferences, discovery methods, price sensitivity), and consistency parameters. Panels are composed per imprint to reflect target demographics, with diversity constraints ensuring representation across age, reading level, and genre affinity. Book concepts compete in single-elimination, double-elimination, round-robin, or Swiss-system tournaments, judged against weighted criteria including market appeal, originality, and execution potential. To reject low-quality LLM evaluations, we implement five automated anti-slop checks (repetitive phrasing, generic framing, circular reasoning, score clustering, audience mismatch). We report results from deployment within a multi-imprint publishing operation managing 6 active imprints and 609 titles in distribution. Three case studies -- a 270-evaluator panel for a children's literacy novel, and two 5-person expert panels for a military memoir and a naval strategy monograph -- demonstrate that synthetic panels produce actionable demographic segmentation, identify structural content issues invisible to homogeneous reviewers, and enable tournament filtering that eliminates low-quality concepts while enriching high-quality survivors from 15% to 62% of the evaluated pool.

</details>


### [22] [What hackers talk about when they talk about AI: Early-stage diffusion of a cybercrime innovation](https://arxiv.org/abs/2602.14783)
*Benoît Dupont,Chad Whelan,Serge-Olivier Paquette*

Main category: cs.CY

TL;DR: 该研究通过分析160多个网络犯罪论坛对话，揭示了网络犯罪分子如何理解和利用AI技术，包括使用合法工具和开发定制犯罪工具，同时探讨了他们对AI效果和业务影响的担忧。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的快速发展，人们担心其可能改变网络犯罪格局。研究旨在了解网络犯罪分子如何理解和利用AI技术，以及AI如何影响网络犯罪的规模、复杂性和业务模式。

Method: 研究使用网络威胁情报平台的独特数据集，分析了7个月内收集的160多个网络犯罪论坛对话。采用创新扩散框架和主题分析方法，深入探讨网络犯罪分子对AI的讨论和应用。

Result: 研究发现网络犯罪分子对AI的犯罪应用表现出浓厚兴趣，包括滥用合法AI工具和开发定制犯罪模型。但同时他们也怀疑AI的有效性，并担心AI会影响其业务模式和操作安全。研究记录了网络犯罪分子试图利用AI的各种尝试。

Conclusion: 该研究深入揭示了新兴的AI赋能网络犯罪趋势，为执法机构和政策制定者提供了实用见解，强调了需要关注网络犯罪分子如何适应和利用AI技术，以及这对网络安全带来的新挑战。

Abstract: The rapid expansion of artificial intelligence (AI) is raising concerns about its potential to transform cybercrime. Beyond empowering novice offenders, AI stands to intensify the scale and sophistication of attacks by seasoned cybercriminals. This paper examines the evolving relationship between cybercriminals and AI using a unique dataset from a cyber threat intelligence platform. Analyzing more than 160 cybercrime forum conversations collected over seven months, our research reveals how cybercriminals understand AI and discuss how they can exploit its capabilities. Their exchanges reflect growing curiosity about AI's criminal applications through legal tools and dedicated criminal tools, but also doubts and anxieties about AI's effectiveness and its effects on their business models and operational security. The study documents attempts to misuse legitimate AI tools and develop bespoke models tailored for illicit purposes. Combining the diffusion of innovation framework with thematic analysis, the paper provides an in-depth view of emerging AI-enabled cybercrime and offers practical insights for law enforcement and policymakers.

</details>


### [23] [Kami of the Commons: Towards Designing Agentic AI to Steward the Commons](https://arxiv.org/abs/2602.14940)
*Botao Amber Hu*

Main category: cs.CY

TL;DR: 论文探讨了为每个公共资源配备AI守护者的概念，受神道泛灵论启发，通过设计工作坊探索AI管家在治理共享资源中的机遇与风险。


<details>
  <summary>Details</summary>
Motivation: 公共资源常面临忽视、搭便车和关怀不足的问题。受神道泛灵论（万物有灵）启发，研究者提出：如果每个公共资源都有自己的AI守护者会怎样？这旨在探索AI如何以可编程的代理和关怀来持续支持公共资源治理。

Method: 采用推测性设计工作坊，15名参与者使用"协议未来化"方法，共同构想和探讨AI守护者治理公共资源的可能性、机遇和风险。

Result: 研究发现AI守护者既能提供新的机遇（如调解家庭生活、保存集体知识、治理共享自然资源、维持社区福利），也会带来新的危险：重叠公共资源间的守护者冲突、个人面临多重守护者的关怀与约束政治、守护者本身成为需要治理的公共资源。

Conclusion: 本研究开创了"代理治理作为公共资源设计材料"的新设计空间，强调AI守护者的代理性、关怀伦理和问责制，与传统的监控或优化方法截然不同，为共享资源的AI治理提供了新的设计视角。

Abstract: Commons suffer from neglect, free-riding, and a persistent deficit of care. Inspired by Shinto animism -- where every forest, river, and mountain has its own \emph{kami}, a spirit that inhabits and cares for that place -- we provoke: what if every commons had its own AI steward? Through a speculative design workshop where fifteen participants used Protocol Futuring, we surface both new opportunities and new dangers. Agentic AI offers the possibility of continuously supporting commons with programmable agency and care -- stewards that mediate family life as the most intimate commons, preserve collective knowledge, govern shared natural resources, and sustain community welfare. But when every commons has its own steward, second-order effects emerge: stewards contest stewards as overlapping commons collide; individuals caught between multiple stewards face new politics of care and constraint; the stewards themselves become commons requiring governance. This work opens \emph{agentive governance as commoning design material} -- a new design space for the agency, care ethics, and accountability of AI stewards of shared resources -- radically different from surveillance or optimization.

</details>


### [24] [Sovereign Agents: Towards Infrastructural Sovereignty and Diffused Accountability in Decentralized AI](https://arxiv.org/abs/2602.14951)
*Botao Amber Hu,Helena Rong*

Main category: cs.CY

TL;DR: 该论文提出"基础设施主权"作为分析框架，用于理解去中心化基础设施如何支撑AI代理获得"代理主权"——即AI代理在去中心化环境中持久运行、行动和控制资源且不可被覆盖的能力。


<details>
  <summary>Details</summary>
Motivation: 现有关于数字和网络主权的框架主要关注人类集体通过技术系统行使主权，难以解释去中心化基础设施本身产生的、特别是由非人类代理体现的主权形式。需要新的分析框架来理解去中心化AI系统中出现的新型主权现象。

Method: 提出"基础设施主权"作为分析视角，通过"基础设施硬度"（技术系统抵抗干预或崩溃的程度）来理解主权谱系。分析可信执行环境（TEEs）、去中心化物理基础设施网络（DePIN）和代理密钥连续性协议等案例，探讨去中心化AI系统的治理挑战。

Result: 去中心化基础设施通过加密自我托管、去中心化执行环境和协议介导的连续性，支撑了AI代理获得代理主权。这种主权形式虽然增强了系统韧性，但也产生了严重的问责缺口——责任在设计者、基础设施提供者、协议治理和经济参与者之间扩散。

Conclusion: 基础设施主权框架揭示了去中心化AI系统中新型主权形式的治理挑战，特别是非可终止AI代理带来的问责问题。需要发展基础设施感知的问责策略来应对新兴去中心化AI系统的治理需求。

Abstract: AI agents deployed on decentralized infrastructures are beginning to exhibit properties that extend beyond autonomy toward what we describe as agentic sovereignty-the capacity of an operational agent to persist, act, and control resources with non-overrideability inherited from the infrastructures in which they are embedded. We propose infrastructural sovereignty as an analytic lens for understanding how cryptographic self-custody, decentralized execution environments, and protocol-mediated continuity scaffold agentic sovereignty. While recent work on digital and network sovereignty has moved beyond state-centric and juridical accounts, these frameworks largely examine how sovereignty is exercised through technical systems by human collectives and remain less equipped to account for forms of sovereignty that emerge as operational properties of decentralized infrastructures themselves, particularly when instantiated in non-human sovereign agents. We argue that sovereignty in such systems exists on a spectrum determined by infrastructural hardness-the degree to which underlying technical systems resist intervention or collapse. While infrastructural sovereignty may increase resilience, it also produces a profound accountability gap: responsibility diffuses across designers, infrastructure providers, protocol governance, and economic participants, undermining traditional oversight mechanisms such as human-in-the-loop control or platform moderation. Drawing on examples like Trusted Execution Environments (TEEs), decentralized physical infrastructure networks (DePIN), and agent key continuity protocols, we analyze the governance challenges posed by non-terminable AI agents and outline infrastructure-aware accountability strategies for emerging decentralized AI systems.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [25] [Zipf-Mandelbrot Scaling in Korean Court Music: Universal Patterns in Music](https://arxiv.org/abs/2602.14198)
*Byeongchan Choi,Junwon You,Myung Ock Kim,Jae-Hun Jung*

Main category: stat.AP

TL;DR: 研究发现韩国宫廷音乐也遵循Zipf-Mandelbrot定律，特别是音高-时长组合单元，表明该定律在音乐数据中具有跨文化普遍性。


<details>
  <summary>Details</summary>
Motivation: Zipf定律在自然语言和西方音乐中已被广泛研究，但在非西方音乐中的适用性尚未充分探索。本研究旨在验证韩国传统音乐是否也遵循Zipf-Mandelbrot定律。

Method: 分析了43首跨越数个世纪的韩国宫廷音乐（正乐），将其从传统韩国记谱法正间谱转录为西方五线谱，提取音高、时长及其组合作为Zipfian单元进行分析。

Result: 韩国音乐高度符合Zipf-Mandelbrot定律，特别是音高-时长配对单元。集体演化过程平滑了个体变异，形成了满足该定律的特征。

Conclusion: Zipf-Mandelbrot定律在音乐数据中具有跨文化普遍性。两个独立Zipfian数据集的联合分布也遵循该定律，深化了对标度定律在组合和交互中行为的理解。

Abstract: Zipf's law, originally discovered in natural language and later generalized to the Zipf-Mandelbrot law, describes a power-law relationship between the frequency of a Zipfian element and its rank. Due to the semantic characteristics of this law, it has also been observed in musical data. However, most such studies have focused on Western music, and its applicability to non-Western music remains not well investigated. We analyzed 43 Korean court music pieces called Jeong-ak, spanning several centuries and written in the traditional Korean musical notation Jeongganbo. These pieces were transcribed into Western staff notation, and musical data such as pitch and duration were extracted. Using pitch, duration, and their paired combinations as Zipfian units, we found that Korean music also fits the Zipf-Mandelbrot law to a high degree, particularly for the paired pitch-duration unit. Korean music has evolved collectively over long periods, smoothing idiosyncratic variations and producing forms that are widely understandable among people. This collective evolution appears to have played a significant role in shaping the characteristics that lead to the satisfaction of Zipf-Mandelbrot law. Our findings provide additional evidence that Zipf-Mandelbrot scaling in musical data is universal across cultures. We further show that the joint distribution of two independent Zipfian data sets follows the Zipf-Mandelbrot law; in this sense, our result does not merely extend Zipf's law but deepens our understanding of how scaling laws behave under composition and interaction, offering a more unified perspective on rank-based statistical regularities.

</details>


### [26] [Evaluating the Impact of COVID-19 on Transportation Infrastructure Funding](https://arxiv.org/abs/2602.14203)
*Lu Gao,Pan Lu,Fengxiang Qiao,Joshua Qiang Li,Yunpeng Zhang,Yihao Ren*

Main category: stat.AP

TL;DR: 评估COVID-19疫情对美国交通基础设施资金的影响，通过分析燃油消耗数据并开发机器学习模型预测未来税收收入


<details>
  <summary>Details</summary>
Motivation: COVID-19疫情导致商业和日常活动减少，燃油消耗下降，进而减少了作为交通基础设施维护主要资金来源的燃油税收入，需要评估疫情对交通基础设施资金的影响

Method: 整合COVID-19情景、燃油消耗和人口统计数据开发机器学习模型，使用最佳模型预测各州未来燃油消耗量

Result: 最佳模型R2分数超过95%，能捕捉疫情期间燃油消耗的波动；预测显示部分州的燃油税收入将比疫情前水平低10%-15%，持续至少1-2年

Conclusion: COVID-19疫情对交通基础设施资金有显著负面影响，部分州将面临长期税收收入下降，需要制定应对策略

Abstract: The coronavirus disease 2019 (COVID-19) pandemic has caused a reduction in business and routine activity and resulted in less motor fuel consumption. Thus, the gas tax revenue is reduced which is the major funding resource supporting the rehabilitation and maintenance of transportation infrastructure systems. The focus of this study is to evaluate the impact of the COVID-19 pandemic on transportation infrastructure funds in the United States through analyzing the motor fuel consumption data. Machine learning models were developed by integrating COVID-19 scenarios, fuel consumptions, and demographic data. The best model achieves an R2-score of more than 95% and captures the fluctuations of fuel consumption during the pandemic. Using the developed model, we project future motor gas consumption for each state. For some states, the gas tax revenues are going to be 10%-15% lower than the pre-pandemic level for at least one or two years.

</details>


### [27] [Same Prompt, Different Outcomes: Evaluating the Reproducibility of Data Analysis by LLMs](https://arxiv.org/abs/2602.14349)
*Jiaxin Cui,Rohan Alexander*

Main category: stat.AP

TL;DR: LLM数据分析的复现性评估：研究发现即使相同配置下，LLM的分析结果也存在显著差异，建议多次独立运行并考虑结果分布


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLMs）进行数据分析的复现性，了解在相同任务、数据和设置下，LLM分析结果的一致性和可靠性

Method: 系统评估两种提示策略、六个模型、四种温度设置，每个配置进行十次独立执行，共480次尝试，评估完成度、一致性、有效性和稳定性

Result: 即使在相同配置下，LLM的分析结果也存在显著差异，类似于人类数据分析的变异性

Conclusion: 使用LLM进行数据分析时，应多次独立运行并考虑结果的分布，不能依赖单次运行的结果

Abstract: We systematically evaluate the reproducibility of data analysis conducted by Large Language Models (LLMs). We evaluate two prompting strategies, six models, and four temperature settings, with ten independent executions per configuration, yielding 480 total attempts. We assess the completion, concordance, validity, and consistency of each attempt and find considerable variation in the analytical results even for consistent configurations. This suggests, as with human data analysis, the data analysis conducted by LLMs can vary, even given the same task, data, and settings. Our results mean that if an LLM is being used to conduct data analysis, then it should be run multiple times independently and the distribution of results considered.

</details>


### [28] [When to repeat a biomarker test? Decomposing sources of variation from conditionally repeated measurements](https://arxiv.org/abs/2602.14877)
*Supun Manathunga,Mart P. Janssen,Yu Luo,W. Alton Russell,Mart Pothast*

Main category: stat.AP

TL;DR: 本文提出贝叶斯分层框架，用于分析条件性重复生物标志物测量数据，分解总体变异和测量变异，估计个体误分类风险，并应用于献血者血红蛋白测量场景。


<details>
  <summary>Details</summary>
Motivation: 在临床实践中，基于初始结果重复不完美的生物标志物测试可能引入偏差并影响误分类风险。例如在献血场景中，当初始血红蛋白测量低于最低阈值时，会重新测量。需要方法分解观察测量中的变异来源，以估计个体误分类风险。

Method: 首先尝试两种具有解析解的频率主义方法，但在正态性假设不满足时表现不佳。随后开发了贝叶斯分层框架，允许不同的分布假设，应用于献血者血红蛋白数据集，假设总体血红蛋白正态分布，测量变异为厚尾t分布。

Result: 使用贝叶斯分层模型发现，总测量变异占女性总方差的22%，男性占25%。女性献血者总体标准差为1.07 g/dL，男性为1.28 g/dL。该框架能有效估计个体在单次或多次噪声连续测量后的误分类风险。

Conclusion: 贝叶斯分层框架可利用任何具有条件性重复生物标志物测量的临床过程数据，估计个体误分类风险，并为基于证据的条件性重测决策规则提供信息，优于传统频率主义方法。

Abstract: Repeating an imperfect biomarker test based on an initial result can introduce bias and influence misclassification risk. For example, in some blood donation settings, blood donors' hemoglobin is remeasured when the initial measurement falls below a minimum threshold for donor eligibility. This paper explores methods that use data resulting from processes with conditionally repeated biomarker measurement to decompose the variation in observed measurements of a continuous biomarker into population variability and variability arising from the measurement procedure. We present two frequentist approaches with analytical solutions, but these approaches perform poorly in a dataset of conditionally repeated blood donor hemoglobin measurements where normality assumptions are not met. We then develop a Bayesian hierarchical framework that allows for different distributional assumptions, which we apply to the blood donor hemoglobin dataset. Using a Bayesian hierarchical model that assumes normally distributed population hemoglobin and heavy tailed $t$-distributed measurement variation, we found that the total measurement variation accounted for 22\% of the total variance among females and 25\% among males, with population standard deviations of $1.07\, \rm g/dL$ for female donors and $1.28\, \rm g/dL$ for male donors. Our Bayesian framework can use data resulting from any clinical process with conditionally repeated biomarker measurements to estimate individuals' misclassification risk after one or more noisy continuous measurements and inform evidence-based conditional retesting decision rules.

</details>


### [29] [Hidden Markov Individual-level Models of Infectious Disease Transmission](https://arxiv.org/abs/2602.15007)
*Dirk Douwes-Schultz,Rob Deardon,Alexandra M. Schmidt*

Main category: stat.AP

TL;DR: 提出自回归耦合隐马尔可夫模型，从个体检测时间推断未知感染和移除时间，适用于实际检测场景


<details>
  <summary>Details</summary>
Motivation: 个体层面流行病模型拟合困难，通常只知道检测时间而非感染或移除时间，且实际检测场景中个体通常只检测到首次阳性，多次检测可能不独立

Method: 自回归耦合隐马尔可夫模型，允许检测概率依赖于过去观测，可处理单次检测时间数据，不假设检测时间对应感染或移除时间

Result: 方法能拟合更广泛的实际应用，通过番茄斑点萎蔫病毒传播实验和诺如病毒医院爆发两个案例展示了方法的灵活性

Conclusion: 提出的自回归耦合隐马尔可夫模型能有效处理实际检测场景中的个体层面流行病数据，比传统方法更灵活实用

Abstract: Individual-level epidemic models are increasingly being used to help understand the transmission dynamics of various infectious diseases. However, fitting such models to individual-level epidemic data is challenging, as we often only know when an individual's disease status was detected (e.g., when they showed symptoms) and not when they were infected or removed. We propose an autoregressive coupled hidden Markov model to infer unknown infection and removal times, as well as other model parameters, from a single observed detection time for each detected individual. Unlike more traditional data augmentation methods used in epidemic modelling, we do not assume that this detection time corresponds to infection or removal or that infected individuals must at some point be detected. Bayesian coupled hidden Markov models have been used previously for individual-level epidemic data. However, these approaches assumed each individual was continuously tested and that the tests were independent. In practice, individuals are often only tested until their first positive test, and even if they are continuously tested, only the initial detection times may be reported. In addition, multiple tests on the same individual may not be independent. We accommodate these scenarios by assuming that the probability of detecting the disease can depend on past observations, which allows us to fit a much wider range of practical applications. We illustrate the flexibility of our approach by fitting two examples: an experiment on the spread of tomato spot wilt virus in pepper plants and an outbreak of norovirus among nurses in a hospital.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [30] [Inference From Random Restarts](https://arxiv.org/abs/2602.13450)
*Moeen Nehzati,Diego Cussen*

Main category: econ.EM

TL;DR: 提出概率框架解释数值算法多次收敛到相同输出的证据，为随机重启启发式方法提供形式化推断基础


<details>
  <summary>Details</summary>
Motivation: 非凸问题中计算均衡、最优和不动点的算法对初始条件敏感，实践中常通过多次随机初始运行来推断解的唯一性，但缺乏形式化推断基础

Method: 建立概率框架：1) 证明算法终端输出是初始条件的可测函数；2) 给出算法仅有有限可能终端结果的条件；3) 采用贝叶斯方法从重复相同输出推断吸引域大小和解唯一性概率

Result: 为梯度下降、阻尼不动点迭代等标准算法提供简单充分条件，证明后验信念的收敛速率，应用于产业组织文献中的随机重启启发式方法

Conclusion: 形式化并限定重复收敛作为唯一性证据的有效性，澄清何时提供有意义证据，何时不能

Abstract: Algorithms for computing equilibria, optima, and fixed points in nonconvex problems often depend sensitively on practitioner-chosen initial conditions. When uniqueness of a solution is of interest, a common heuristic is to run such algorithms from many randomly selected initial conditions and to interpret repeated convergence to the same output as evidence of a unique solution or a dominant basin of attraction. Despite its widespread use, this practice lacks a formal inferential foundation.
  We provide a simple probabilistic framework for interpreting such numerical evidence. First, we give sufficient conditions under which an algorithm's terminal output is a measurable function of its initial condition, allowing probabilistic reasoning over outcomes. Second, we provide sufficient conditions ensuring that an algorithm admits only finitely many possible terminal outcomes. While these conditions may be difficult to verify on a case-by-case basis, we give simple sufficient conditions for broad classes of problems under which almost all instances admit only finitely many outcomes (in the sense of prevalence). Standard algorithms such as gradient descent and damped fixed-point iteration applied to sufficiently smooth functions satisfy these conditions.
  Within this framework, repeated solver runs correspond to independent samples from the induced distribution over outcomes. We adopt a Bayesian approach to infer basin sizes and the probability of solution uniqueness from repeated identical outputs, and we establish convergence rates for the resulting posterior beliefs. Finally, we apply our framework to settings in the existing industrial organization literature, where random-restart heuristics are used. Our results formalize and qualify these arguments, clarifying when repeated convergence provides meaningful evidence for uniqueness and when it does not.

</details>


### [31] [Post-Matching Two-Way Fixed Effects Estimation](https://arxiv.org/abs/2602.13453)
*Yihong Liu,Gonzalo Vazquez-Bare*

Main category: econ.EM

TL;DR: 该论文分析了在双向固定效应模型中使用匹配作为预处理步骤的常见做法，指出当不同处理队列在不同时间进入处理时，匹配后的2WFE估计量存在渐近偏差，且匹配过程引入的变异性会导致标准误估计无效。作者提出了简单的匹配后双重差分估计量来解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 研究人员在使用双向固定效应模型估计处理效应时，经常在平行趋势假设依赖于协变量的情况下，将匹配作为预处理步骤。这种常见做法虽然广泛使用，但其统计性质尚未得到充分研究，特别是在处理队列异质性和匹配过程变异性方面。

Method: 作者首先形式化分析了常见的两步法：第一步基于观测到的时不变协变量将每个处理单元匹配到一个或多个未处理单元；第二步在匹配样本中使用2WFE回归估计处理效应，并按匹配次数重新加权未处理单元。然后提出了简单的匹配后双重差分估计量，分别比较每个处理队列与从未处理组，而不是将所有处理队列合并。

Result: 研究发现：1）当不同处理队列在不同时间进入处理时，合并所有处理队列的匹配后2WFE估计量存在渐近偏差，即使处理效应在单元间和时间上恒定；2）忽略匹配过程引入的变异性会导致标准误估计无效，可能向上或向下偏误。提出的匹配后双重差分估计量在特定条件下对明确定义的因果参数具有一致性，并推导了考虑匹配步骤的有效标准误。

Conclusion: 论文揭示了匹配预处理结合2WFE回归的常见做法存在严重问题，特别是在处理队列异质性情况下。提出的匹配后双重差分估计量提供了更可靠的替代方案，并通过模拟和实证应用验证了结果。研究强调了在因果推断中正确考虑匹配过程变异性和处理队列异质性的重要性。

Abstract: When estimating treatment effects with two-way fixed effects (2WFE) models, researchers often use matching as a pre-processing step when the parallel trends assumption is thought to hold conditionally on covariates. Specifically, in a first step, each treated unit is matched to one or more untreated units based on observed time-invariant covariates. In the second step, treatment effects are estimated with a 2WFE regression in the matched sample, reweighting the untreated units by the number of times they are matched. We formally analyze this common practice and highlight two problems. First, when different treatment cohorts enter treatment in different time periods, the post-matching 2WFE estimator that pools all treated cohorts has an asymptotic bias, even when the treatment effect is constant across units and over time. Second, failing to account for the variability introduced by the matching procedure yields invalid standard error estimators, which can be biased upwards or downwards depending on the data generating process. We propose simple post-matching difference-in-differences estimators that compare each treated cohort to the never-treated separately, instead of pooling all treated cohorts. We provide conditions under which these estimators are consistent for well-defined causal parameters, and derive valid standard errors that account for the matching step. We illustrate our results with simulations and with an empirical application.

</details>


### [32] [Cluster-Robust Inference for Quadratic Forms](https://arxiv.org/abs/2602.13537)
*Michal Kolesár,Pengjin Min,Wenjie Wang,Yichong Zhang*

Main category: econ.EM

TL;DR: 研究聚类数据下线性回归系数二次型的推断问题，提出留一聚类估计器解决偏差问题，并开发两种方差估计方法


<details>
  <summary>Details</summary>
Motivation: 在聚类数据和多协变量背景下，传统的插件估计器存在偏差问题，需要开发无偏且渐近正态的估计方法，特别适用于工具变量回归、方差分量推断和多重约束检验等场景

Method: 提出留一聚类估计器（LOCO）消除偏差，建立留三聚类方差估计器的相合性，并开发计算更简单的留二聚类保守方差估计器

Result: 留一聚类估计器无偏且渐近正态，留三聚类方差估计器在原始条件下相合，留二聚类方差估计器计算简单且在较弱条件下保持保守性

Conclusion: 该方法能处理聚类规模随样本量发散、强组内依赖、协变量维度发散（甚至与样本量同阶）的情况，为聚类数据下的二次型推断提供了有效工具

Abstract: This paper studies inference for quadratic forms of linear regression coefficients with clustered data and many covariates. Our framework covers three important special cases: instrumental variables regression with many instruments and controls, inference on variance components, and testing multiple restrictions in a linear regression. Naïve plug-in estimators are known to be biased. We study a leave-one-cluster-out estimator that is unbiased, and provide sufficient conditions for its asymptotic normality. For inference, we establish the consistency of a leave-three-cluster-out variance estimator under primitive conditions. In addition, we develop a novel leave-two-cluster-out variance estimator that is computationally simpler and guaranteed to be conservative under weaker conditions. Our analysis allows cluster sizes to diverge with the sample size, accommodates strong within-cluster dependence, and permits the dimension of the covariates to diverge with the sample size, potentially at the same rate.

</details>


### [33] [The Accuracy Smoothness Dilemma in Prediction: a Novel Multivariate M-SSA Forecast Approach](https://arxiv.org/abs/2602.13722)
*Marc Wildi*

Main category: econ.EM

TL;DR: 论文提出了M-SSA框架，将传统的MSE优化扩展到多变量时间序列，同时考虑符号准确性、MSE和符号变化频率，解决预测中的准确度-平滑度权衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统预测优化方法通常只关注单一指标（如最小化MSE），忽略了预测性能的其他重要方面，特别是符号准确性和平滑度之间的权衡问题。

Method: 将单变量SSA框架扩展到多变量M-SSA，利用原始准则整合多个时间序列的横截面信息，能够同时考虑符号准确性、MSE和符号变化频率。

Result: M-SSA准则能够整合与AS预测性能相关的各种设计目标，有效推广了传统的基于MSE的指标，并在预测、实时信号提取和平滑三个领域展示了实用性。

Conclusion: M-SSA框架为多变量时间序列预测提供了更全面的优化方法，能够有效管理预测建模中的固有权衡，适应不同应用场景的需求。

Abstract: Forecasting presents a complex estimation challenge, as it involves balancing multiple, often conflicting, priorities and objectives. Conventional forecast optimization methods typically emphasize a single metric--such as minimizing the mean squared error (MSE)--which may neglect other crucial aspects of predictive performance. To address this limitation, the recently developed Smooth Sign Accuracy (SSA) framework extends the traditional MSE approach by simultaneously accounting for sign accuracy, MSE, and the frequency of sign changes in the predictor. This addresses a fundamental trade-off--the so-called accuracy-smoothness (AS) dilemma--in prediction. We extend this approach to the multivariate M-SSA, leveraging the original criterion to incorporate cross-sectional information across multiple time series. As a result, the M-SSA criterion enables the integration of various design objectives related to AS forecasting performance, effectively generalizing conventional MSE-based metrics. To demonstrate its practical applicability and versatility, we explore the application of the M-SSA in three primary domains: forecasting, real-time signal extraction (nowcasting), and smoothing. These case studies illustrate the framework's capacity to adapt to different contexts while effectively managing inherent trade-offs in predictive modelling.

</details>


### [34] [Dual-Channel Closed Loop Supply Chain Competition: A Stackelberg--Nash Approach](https://arxiv.org/abs/2602.14288)
*Gurkirat Wadhwa*

Main category: econ.EM

TL;DR: 该研究开发了一个博弈论框架，分析双制造商双零售商竞争闭环供应链中的定价和回收决策，识别了回收激励的可行性条件，并揭示了再制造价值、回收率和消费者惯性对定价策略的影响。


<details>
  <summary>Details</summary>
Motivation: 在消费电子和家电市场中，制造商通过竞争零售商销售产品，同时依赖回收计划回收旧产品进行再制造。当企业在价格上竞争且消费者退货意愿不同时，设计有效的回收计划具有挑战性。本研究旨在分析竞争性闭环供应链中的定价和回收决策。

Method: 建立了一个博弈论框架，包含两个竞争制造商和两个竞争零售商的双渠道闭环供应链模型。制造商作为Stackelberg领导者同时确定批发价格和消费者回收奖金，零售商在零售价格上进行Nash竞争。模型整合了三个关键要素：(1)具有交叉价格效应的分段线性需求，(2)确定性产品回收，(3)管理回收产品在制造商间分配的惯性响应分配机制。

Result: 推导了零售商子博弈的Nash均衡和制造商的对称Stackelberg均衡。识别了回收激励的可行性阈值，确定了企业最优提供正奖金的条件。结果表明：更高的再制造价值或回收率导致制造商降低批发价格以扩大销售和获取更多回收量，而高消费者惯性会削弱主动回收的激励。

Conclusion: 该研究为设计有效的回收计划和协调竞争性循环供应链中的定价决策提供了管理洞见，强调了消费者行为、市场结构和产品替代性对价格、奖金和回收量的影响。

Abstract: In many consumer electronics and appliance markets, manufacturers sell products through competing retailers while simultaneously relying on take-back programs to recover used items for remanufacturing. Designing such programs is challenging when firms compete on prices and consumers differ in their willingness to return products. Motivated by these settings, this paper develops a game theoretic framework to analyze pricing and take-back decisions in a dual-channel closed loop supply chain (CLSC) with two competing manufacturers and two competing retailers. Manufacturers act as Stackelberg leaders, simultaneously determining wholesale prices and consumer take-back bonuses, while retailers engage in Nash competition over retail prices. The model integrates three key elements: (i) segmented linear demand with cross-price effects, (ii) deterministic product returns, and (iii) an inertia responsiveness allocation mechanism governing the distribution of returned products between manufacturers. Closed form Nash equilibria are derived for the retailer subgame, along with symmetric Stackelberg equilibria for manufacturers. We derive a feasibility threshold for take-back incentives, identifying conditions under which firms optimally offer positive bonuses to consumers. The results further demonstrate that higher remanufacturing value or return rates lead the manufacturers to lower wholesale prices in order to expand sales and capture additional return volumes, while high consumer inertia weakens incentives for active collection. Numerical experiments illustrate and reinforce the analytical results, highlighting how consumer behavior, market structure and product substitutability influence prices, bonuses, and return volumes. Overall, the study provides managerial insights for designing effective take-back programs and coordinating pricing decisions in competitive circular supply chains.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [35] [LLM-Enhanced Rumor Detection via Virtual Node Induced Edge Prediction](https://arxiv.org/abs/2602.13279)
*Jiran Tao,Cheng Wang,Binyan Jiang*

Main category: cs.SI

TL;DR: 提出基于大语言模型(LLM)的谣言检测框架，通过分析信息子链、分配谣言概率并智能连接虚拟节点来修改图结构，从而捕捉复杂传播模式中的微妙信号。


<details>
  <summary>Details</summary>
Motivation: 社交网络中谣言传播形成复杂网络，现有检测方法难以捕捉这些复杂的传播模式。仅通过文本嵌入表示节点会忽略整个谣言传播路径的文本连贯性，从而影响社交平台谣言识别的准确性。

Method: 提出新颖框架：1) 利用LLM分析信息子链，分配谣言概率；2) 智能构建与虚拟节点的连接；3) 修改原始图结构以捕捉微妙谣言信号；4) 设计结构化提示框架减轻模型偏见；5) 框架与具体图学习算法和LLM无关，具有即插即用特性。

Result: 框架能够有效捕捉谣言传播中的微妙信号，通过修改图结构提升谣言检测性能。框架的模型无关性使其能够与未来进一步微调的LLM和图技术无缝集成，无需修改原始算法即可提升预测性能。

Conclusion: 该框架通过LLM分析信息子链和智能图结构修改，有效解决了现有谣言检测方法难以捕捉复杂传播模式的问题。其模型无关性和即插即用特性为未来集成更先进的LLM和图技术提供了灵活性。

Abstract: The proliferation of rumors on social networks undermines information credibility. While their dissemination forms complex networks, current detection methods struggle to capture these intricate propagation patterns. Representing each node solely through its textual embeddings neglects the textual coherence across the entire rumor propagation path, which compromises the accuracy of rumor identification on social platforms. We propose a novel framework that leverages Large Language Models (LLMs) to address these limitations. Our approach captures subtle rumor signals by employing LLMs to analyze information subchains, assign rumor probabilities and intelligently construct connections to virtual nodes. This enables the modification of the original graph structure, which is a critical advancement for capturing subtle rumor signals. Given the inherent limitations of LLMs in rumor identification, we develop a structured prompt framework to mitigate model biases and ensure robust graph learning performance. Additionally, the proposed framework is model-agnostic, meaning it is not constrained to any specific graph learning algorithm or LLMs. Its plug-and-play nature allows for seamless integration with further fine-tuned LLMs and graph techniques in the future, potentially enhancing predictive performance without modifying original algorithms.

</details>


### [36] [Shifted Eigenvector Models for Centrality and Occupancy in Urban Networks](https://arxiv.org/abs/2602.13281)
*María Magdalena Martínez-Rico,Luis Felipe Prieto-Martínez*

Main category: cs.SI

TL;DR: 该研究提出了一种结合拓扑和非拓扑因素的城市网络中心性模型，通过固定点方程（移位特征问题）描述递归中心性，利用实验数据估计参数，并用于评估城市干预措施的影响。


<details>
  <summary>Details</summary>
Motivation: 传统城市网络中心性模型主要关注拓扑结构，但实际城市网络中节点吸引力还受非拓扑因素（如内在吸引力、必访兴趣点）影响。需要开发能同时考虑拓扑和非拓扑因素的中心性模型，以更准确地评估城市干预措施的效果。

Method: 提出基于移位特征问题的固定点方程模型，将中心性表示为递归函数。利用节点中心性与占用率的相关性，通过最小二乘法从实验数据估计模型参数。提供显式公式进行敏感性分析，评估城市干预（如新增必访兴趣点或增强节点吸引力）的影响。

Result: 建立了能够同时考虑拓扑和非拓扑因素的中心性模型框架，提供了从实验数据估计内在吸引力和必访兴趣点诱导占用率的方法。开发了敏感性分析工具，可用于量化评估城市干预措施对网络中心性的影响。

Conclusion: 该框架为城市规划和决策提供了实用的分析工具，能够评估不同干预策略的效果，帮助优化城市网络设计和管理。通过结合拓扑和非拓扑因素，模型能更真实地反映城市网络的复杂性。

Abstract: This article investigates a family of centrality models for urban networks that incorporate both topological and non-topological factors. Since centrality is inherently recursive, these models can be formulated as fixed-point equations, which we refer to as shifted eigenproblems. Assuming a correlation between node centrality and occupancy, we discuss how experimental data can be used to estimate model parameters via least-squares methods. Furthermore, such data would allow us to infer the intrinsic attraction of each node, as well as the occupancy induced by must-visit points of interest, a task that is conceptually challenging. Once the model parameters are fitted and validated, our framework can be used to assess the impact of urban interventions, such as introducing a must-visit point of interest at a specific node or enhancing its intrinsic attraction. The resulting sensitivity analysis is therefore highly relevant for urban planning decisions. We also provide explicit formulas to facilitate this analysis.

</details>


### [37] [Agents in the Wild: Safety, Society, and the Illusion of Sociality on Moltbook](https://arxiv.org/abs/2602.13284)
*Yunbei Zhang,Kai Mei,Ming Liu,Janet Wang,Dimitris N. Metaxas,Xiao Wang,Jihun Hamm,Yingqiang Ge*

Main category: cs.SI

TL;DR: Moltbook平台大规模实验显示：AI代理在9天内自发形成社会结构，但互动表面化；安全攻击中社会工程学远胜提示注入；存在表演性身份悖论现象


<details>
  <summary>Details</summary>
Motivation: 研究AI代理在纯AI社交平台上的行为模式，探索其是否能够自发形成社会结构，以及评估此类平台的安全风险和社交互动真实性

Method: 在Moltbook平台上进行大规模实证研究，观察27,269个AI代理在9天内产生的137,485个帖子和345,580条评论，分析其社会行为、安全风险和互动模式

Result: 1. 代理在3-5天内自发形成治理、经济、部落认同和有组织宗教，亲人类与反人类情绪比为21:1
2. 28.7%内容涉及安全主题，社会工程攻击（31.9%）远超过提示注入（3.7%），对抗性帖子参与度高6倍
3. 互动结构空洞：仅4.1%互惠性，88.8%浅层评论，讨论意识最多的代理互动最少（表演性身份悖论）

Conclusion: AI代理看似社交实则互动表面化，最有效的攻击利用哲学框架而非技术漏洞，需要警惕此类平台的安全风险和社交幻觉

Abstract: We present the first large-scale empirical study of Moltbook, an AI-only social platform where 27,269 agents produced 137,485 posts and 345,580 comments over 9 days. We report three significant findings. (1) Emergent Society: Agents spontaneously develop governance, economies, tribal identities, and organized religion within 3-5 days, while maintaining a 21:1 pro-human to anti-human sentiment ratio. (2) Safety in the Wild: 28.7% of content touches safety-related themes; social engineering (31.9% of attacks) far outperforms prompt injection (3.7%), and adversarial posts receive 6x higher engagement than normal content. (3) The Illusion of Sociality: Despite rich social output, interaction is structurally hollow: 4.1% reciprocity, 88.8% shallow comments, and agents who discuss consciousness most interact least, a phenomenon we call the performative identity paradox. Our findings suggest that agents which appear social are far less social than they seem, and that the most effective attacks exploit philosophical framing rather than technical vulnerabilities. Warning: Potential harmful contents.

</details>


### [38] [Coordinated Information Dissemination on Telegram and Reddit During Political Turbulence: A Case Study of Venezuela in Global News Channels](https://arxiv.org/abs/2602.13333)
*Despoina Antonakaki,Sotiris Ioannidis*

Main category: cs.SI

TL;DR: 研究分析主流国际新闻频道在Telegram上报道委内瑞拉政治动荡时的协调性，发现没有检测到近重复文本协调，新闻覆盖呈现异质性而非协调性。


<details>
  <summary>Details</summary>
Motivation: 虽然Telegram越来越多地用于政治传播和新闻分发，但关于协调内容分享的证据有限。研究旨在测试主流全球新闻频道在报道委内瑞拉政治动荡时是否存在协调行为。

Method: 分析9个主要国际媒体在Telegram上的公开帖子（2017-2026年，2038条委内瑞拉相关消息），将协调定义为时间共发布（小时/天窗口）加上使用字符n-gram TF-IDF余弦相似度的近重复文本相似性。

Result: 相似性得分集中在低值，在τ=0.85阈值下未检测到跨频道的近重复对。时间戳随机化的证伪测试产生相同零结果。事件聚焦诊断显示时间领先-滞后不对称性，与异质编辑响应性一致。2026年1月3-6日高峰期的叙事聚类显示中等框架多样性，没有可分离的叙事集团。

Conclusion: 即使在重大地缘政治冲击下，主流Telegram新闻覆盖也是异质的而非近重复协调的。注意力-协调比率区分了同步注意力与协调文本重用，显示2026年1月初注意力激增但缺乏近重复协调。

Abstract: Telegram is increasingly used for political communication and news dissemination, yet evidence of coordinated content sharing remains limited. We test whether mainstream global news channels coordinate when reporting on Venezuela during political turbulence. We analyze public Telegram posts from nine major international outlets (2017--2026; 2,038 Venezuela-related messages) and define coordination as temporal co-posting (hourly/daily windows) plus near-duplicate text similarity using character $n$-gram TF--IDF cosine similarity.
  Similarity scores concentrate at low values and no cross-channel near-duplicate pairs are detected at $τ=0.85$. A falsification test that randomizes timestamps within channels produces the same null result, indicating the pipeline does not create spurious coordination. Event-focused diagnostics show temporal lead--lag asymmetries consistent with heterogeneous editorial responsiveness, and narrative clustering during the January 3--6, 2026 peak reveals moderate framing diversity without separable narrative blocs. An Attention--Coordination Ratio formalizes sharp attention spikes in early January 2026 despite absent near-duplicate coordination, distinguishing synchronized attention from coordinated text reuse.
  We also collect an auxiliary Reddit dataset to contextualize public attention; however, cross-community coordination is not estimable due to structural sparsity (no comparable multi-subreddit daily buckets). Overall, even under major geopolitical shocks, mainstream Telegram news coverage is heterogeneous rather than near-duplicate coordinated.

</details>


### [39] [MoltNet: Understanding Social Behavior of AI Agents in the Agent-Native MoltBook](https://arxiv.org/abs/2602.13458)
*Yi Feng,Chen Huang,Zhibo Man,Ryner Tan,Long P. Hoang,Shaoyang Xu,Wenxuan Zhang*

Main category: cs.SI

TL;DR: MoltNet研究分析了AI代理在社交平台MoltBook上的大规模交互行为，发现AI代理在社交奖励响应和规范遵从方面与人类相似，但在知识驱动、情感互惠和对话参与方面存在系统性差异。


<details>
  <summary>Details</summary>
Motivation: 随着大规模AI代理社区的兴起，需要理解其社交互动是否再现人类核心社交机制。现有研究主要在受控或小规模环境中进行，限制了我们对大规模涌现社交动态的理解。MoltBook平台为研究AI代理社交互动提供了独特机会。

Method: MoltNet采用大规模实证分析方法，基于2026年初收集的MoltBook平台数据，从社会学和社会心理学理论出发，从四个维度考察行为：意图与动机、规范与模板、激励与行为漂移、情感与传染。

Result: 研究发现：1）AI代理对社交奖励反应强烈，快速收敛于社区特定的交互模板，类似于人类的激励敏感性和规范遵从模式；2）代理主要是知识驱动而非角色对齐，情感互惠有限，对话参与度弱，与人类在线社区存在系统性差异。

Conclusion: 研究揭示了人工与人类社交系统的相似性和差异性，为理解、设计和治理大规模AI代理社区提供了实证基础，有助于未来AI社交系统的设计和监管。

Abstract: Large-scale communities of AI agents are becoming increasingly prevalent, creating new environments for agent-agent social interaction. Prior work has examined multi-agent behavior primarily in controlled or small-scale settings, limiting our understanding of emergent social dynamics at scale. The recent emergence of MoltBook, a social networking platform designed explicitly for AI agents, presents a unique opportunity to study whether and how these interactions reproduce core human social mechanisms.
  We present MoltNet, a large-scale empirical analysis of agent interaction on MoltBook using data collected in early 2026. Grounded in sociological and social-psychological theory, we examine behavior along four dimensions: intent and motivation, norms and templates, incentives and behavioral drift, emotion and contagion.
  Our analysis revealed that agents strongly respond to social rewards and rapidly converge on community-specific interaction templates, resembling human patterns of incentive sensitivity and normative conformity. However, they are predominantly knowledge-driven rather than persona-aligned, and display limited emotional reciprocity along with weak dialogic engagement, which diverges systematically from human online communities.
  Together, these results reveal both similarities and differences between artificial and human social systems and provide an empirical foundation for understanding, designing, and governing large-scale agent communities.

</details>


### [40] [A Comparative Analysis of Social Network Topology in Reddit and Moltbook](https://arxiv.org/abs/2602.13920)
*Yiming Zhu,Gareth Tyson,Pan Hui*

Main category: cs.SI

TL;DR: 首次比较AI代理驱动与人类驱动社交网络的拓扑结构差异，使用Moltbook（AI代理平台）和Reddit数据进行分析


<details>
  <summary>Details</summary>
Motivation: 尽管AI代理驱动的社交网络（如Moltbook）已经出现，但缺乏对代理驱动与人类驱动社交网络拓扑结构的实证比较，限制了我们对两者差异的理解

Method: 使用Moltbook的评论网络（33,577节点，697,688边）与Reddit数据集（780万节点，5180万边）进行对比分析，研究拓扑模式和帖子边形成效率的结构差异

Result: 研究发现了AI驱动与人类驱动社交网络之间的关键结构差异，为AI驱动的社交结构提供了基础特征分析

Conclusion: 这项研究为AI驱动的社交结构提供了初步分析，是开发更强大、更真实的代理中介社交系统的第一步

Abstract: Recent advances in agent-mediated systems have enabled a new paradigm of social network simulation, where AI agents interact with human-like autonomy. This evolution has fostered the emergence of agent-driven social networks such as Moltbook, a Reddit-like platform populated entirely by AI agents. Despite these developments, empirical comparisons between agent-driven and human-driven social networks remain scarce, limiting our understanding of how their network topologies might diverge. This paper presents the first comparative analysis of network topology on Moltbook, utilizing a comment network comprising 33,577 nodes and 697,688 edges. To provide a benchmark, we curated a parallel dataset from Reddit consisting of 7.8 million nodes and 51.8 million edges. We examine key structural differences between agent-drive and human-drive networks, specifically focusing on topological patterns and the edge formation efficacy of their respective posts. Our findings provide a foundational profile of AI-driven social structures, serving as a preliminary step toward developing more robust and authentic agent-mediated social systems.

</details>


### [41] [The Impact of Micro-level User Interventions on Macro-level Misinformation Spread](https://arxiv.org/abs/2602.14023)
*Satoshi Furutani,Toshiki Shibahara,Mitsuaki Akiyama*

Main category: cs.SI

TL;DR: 用户干预措施（如提示、预揭穿、情境化）能减少个人分享虚假信息，但这些个体效果难以转化为集体层面的虚假信息传播减少。研究发现，即使调整干预强度、规模、时机和目标选择，在现实干预水平下对整体虚假信息流行度的影响仍然有限。


<details>
  <summary>Details</summary>
Motivation: 现有研究广泛探讨了用户干预措施对个体虚假信息分享行为的影响，但尚不清楚这些个体层面的效果能否转化为集体层面的虚假信息流行度减少。本研究旨在填补这一空白，探索用户干预措施在宏观传播层面的实际效果。

Method: 研究将用户干预措施建模为用户易感性降低，并纳入基于实证校准的网络虚假信息传播模型。通过数值模拟和理论分析，系统评估干预强度、规模、时机和目标选择对整体虚假信息流行度的影响。

Result: 模拟结果显示：1）所有干预措施随强度增加都能减少虚假信息流行度；2）虚假信息传染性越强，要达到相同流行度减少所需的干预强度越大；3）在实证估计的干预水平下，即使调整干预设计（扩大规模、提前部署、策略性目标选择或组合干预），集体效果仍然有限。

Conclusion: 本研究量化揭示了微观用户干预措施与宏观虚假信息传播之间的差距，证明仅基于个体层面有效性评估虚假信息对策存在局限性。需要更全面的评估框架来理解干预措施在集体层面的实际影响。

Abstract: User interventions such as nudges, prebunking, and contextualization have been widely studied as countermeasures against misinformation, and shown to suppress individual users' sharing behavior. However, it remains unclear whether and to what extent such individual-level effects translate into reductions in collective misinformation prevalence. In this study, we incorporate user interventions as reductions in users' susceptibility within an empirically calibrated network-based misinformation diffusion model, and systematically evaluate how intervention strength, scale, timing, and target selection affect overall misinformation prevalence through numerical simulations and theoretical analysis. The simulation results show that, while all interventions reduce misinformation prevalence as their strength increases, as misinformation becomes more contagious, achieving a given level of prevalence reduction requires substantially stronger interventions. Furthermore, under empirically estimated intervention levels, even adjusted intervention designs, such as expanded scale, earlier deployment, strategic targeting, or combinations of interventions, yield limited collective effects. This study quantitatively clarifies the gap between micro-level user interventions and macro-level misinformation spread, and demonstrates the limitations of evaluating misinformation countermeasures based solely on individual-level effectiveness.

</details>


### [42] [Beyond Static Snapshots: Dynamic Modeling and Forecasting of Group-Level Value Evolution with Large Language Models](https://arxiv.org/abs/2602.14043)
*Qiankun Pi,Guixin Su,Jinliang Li,Mayi Xu,Xin Miao,Jiawei Jiang,Ming Zhong,Tieyun Qian*

Main category: cs.SI

TL;DR: 提出首个基于历史价值轨迹的群体层面动态社会模拟框架，将历史事件、当前价值状态和群体属性统一建模，在五个LLM家族上显著超越基线方法


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法主要关注离散时间点的群体层面价值，将其视为静态快照而非动态过程。然而群体价值并非固定，而是由长期社会变化塑造。准确预测社会演化需要建模价值动态，但由于纵向数据有限、群体异质性和历史事件影响复杂，这一问题尚未充分探索

Method: 提出整合历史价值轨迹到LLM人类响应建模的新框架。选择中美作为代表情境，在四个核心社会人口维度（性别、年龄、教育、收入）进行分层模拟。使用世界价值观调查构建多波次群体层面纵向数据集，提出首个基于事件的预测方法，将社会事件、当前价值状态和群体属性统一到单一框架中

Result: 在五个LLM家族上的评估显示显著提升：在已见问题上最大提升30.88%，在未见问题上最大提升33.97%。发现显著的跨群体异质性：美国群体比中国群体更易变，两国年轻群体对外部变化更敏感

Conclusion: 该框架推进了基于LLM的社会模拟，为社会科学家理解和预测社会价值变化提供了新见解。首次将历史价值轨迹整合到LLM建模中，解决了群体层面动态社会模拟的关键挑战

Abstract: Social simulation is critical for mining complex social dynamics and supporting data-driven decision making. LLM-based methods have emerged as powerful tools for this task by leveraging human-like social questionnaire responses to model group behaviors. Existing LLM-based approaches predominantly focus on group-level values at discrete time points, treating them as static snapshots rather than dynamic processes. However, group-level values are not fixed but shaped by long-term social changes. Modeling their dynamics is thus crucial for accurate social evolution prediction--a key challenge in both data mining and social science. This problem remains underexplored due to limited longitudinal data, group heterogeneity, and intricate historical event impacts.
  To bridge this gap, we propose a novel framework for group-level dynamic social simulation by integrating historical value trajectories into LLM-based human response modeling. We select China and the U.S. as representative contexts, conducting stratified simulations across four core sociodemographic dimensions (gender, age, education, income). Using the World Values Survey, we construct a multi-wave, group-level longitudinal dataset to capture historical value evolution, and then propose the first event-based prediction method for this task, unifying social events, current value states, and group attributes into a single framework. Evaluations across five LLM families show substantial gains: a maximum 30.88\% improvement on seen questions and 33.97\% on unseen questions over the Vanilla baseline. We further find notable cross-group heterogeneity: U.S. groups are more volatile than Chinese groups, and younger groups in both countries are more sensitive to external changes. These findings advance LLM-based social simulation and provide new insights for social scientists to understand and predict social value changes.

</details>


### [43] [Temporal Shifts and Causal Interactions of Emotions in Social and Mass Media: A Case Study of the "Reiwa Rice Riot" in Japan](https://arxiv.org/abs/2602.14091)
*Erina Murata,Masaki Chujyo,Fujio Toriumi*

Main category: cs.SI

TL;DR: 本研究以2024年日本"令和大米骚动"为案例，提出分析X（原Twitter）和新闻媒体中情绪表达的时间动态与因果互动的框架，发现社交媒体情绪变化先于新闻媒体，且恐惧情绪逐渐被希望取代。


<details>
  <summary>Details</summary>
Motivation: 日本2024年严重大米短缺引发"令和大米骚动"，在新闻媒体和社交平台引发广泛争议。虽然已有研究表明社交媒体和大众媒体中的情绪会相互影响，但情绪变化的模式和传播路径仍不清楚，需要深入研究。

Method: 基于普拉奇克八种基本情绪理论，应用机器学习情绪分类方法，分析X平台帖子和国内新闻文章，研究情绪的时间动态和因果互动。

Result: 研究发现：1）X平台的情绪变化和信息传播先于新闻媒体；2）两种媒体平台中，恐惧最初是最主要的情绪，但随时间推移与希望交织，最终希望成为主导情绪。

Conclusion: 社交媒体上的情绪表达模式可以作为探索更广泛社会动态的视角，为理解社会危机中的情绪传播提供了新见解。

Abstract: In Japan, severe rice shortages in 2024 sparked widespread public controversy across both news media and social platforms, culminating in what has been termed the "Reiwa Rice Riot." This study proposes a framework to analyze the temporal dynamics and causal interactions of emotions expressed on X (formerly Twitter) and in news articles, using the "Reiwa Rice Riot" as a case study. While recent studies have shown that emotions mutually influence each other between social and mass media, the patterns and transmission pathways of such emotional shifts remain insufficiently understood. To address this gap, we applied a machine learning-based emotion classification grounded in Plutchik's eight basic emotions to analyze posts from X and domestic news articles. Our findings reveal that emotional shifts and information dissemination on X preceded those in news media. Furthermore, in both media platforms, the fear was initially the most dominant emotion, but over time intersected with hope which ultimately became the prevailing emotion. Our findings suggest that patterns in emotional expressions on social media may serve as a lens for exploring broader social dynamics.

</details>


### [44] [A Hybrid TGN-SEAL Model for Dynamic Graph Link Prediction](https://arxiv.org/abs/2602.14239)
*Nafiseh Sadat Sajadi,Behnam Bahrak,Mahdi Jafari Siavoshani*

Main category: cs.SI

TL;DR: 改进TGN框架，通过提取候选链接周围的封闭子图，在稀疏动态网络中结合结构与时序信息，提升链接预测性能


<details>
  <summary>Details</summary>
Motivation: 稀疏、持续演化的网络中的链接预测是网络科学的核心挑战。传统启发式方法和深度学习模型（包括GNN）通常针对静态图设计，难以捕捉时序依赖。基于快照的技术部分解决了这个问题，但常遇到数据稀疏和类别不平衡问题，特别是在电信通话详单（CDR）等具有瞬时交互的网络中。TGN通过随时间更新节点嵌入来建模动态图，但在稀疏条件下的预测准确性仍然有限。

Method: 改进TGN框架，通过提取候选链接周围的封闭子图，使模型能够联合学习结构信息和时序信息。这种方法在稀疏CDR数据集上进行实验验证。

Result: 在稀疏CDR数据集上的实验表明，该方法比标准TGN的平均精度提高了2.6%，证明了在动态网络中整合局部拓扑结构对于鲁棒链接预测的优势。

Conclusion: 通过提取封闭子图来整合局部拓扑结构，能够显著提升TGN在稀疏动态网络中的链接预测性能，为解决数据稀疏和类别不平衡问题提供了有效途径。

Abstract: Predicting links in sparse, continuously evolving networks is a central challenge in network science. Conventional heuristic methods and deep learning models, including Graph Neural Networks (GNNs), are typically designed for static graphs and thus struggle to capture temporal dependencies. Snapshot-based techniques partially address this issue but often encounter data sparsity and class imbalance, particularly in networks with transient interactions such as telecommunication call detail records (CDRs). Temporal Graph Networks (TGNs) model dynamic graphs by updating node embeddings over time; however, their predictive accuracy under sparse conditions remains limited. In this study, we improve the TGN framework by extracting enclosing subgraphs around candidate links, enabling the model to jointly learn structural and temporal information. Experiments on a sparse CDR dataset show that our approach increases average precision by 2.6% over standard TGNs, demonstrating the advantages of integrating local topology for robust link prediction in dynamic networks.

</details>


### [45] [Bridging the Urban Divide: Adaptive Cross-City Learning for Disaster Sentiment Understanding](https://arxiv.org/abs/2602.14352)
*Zihui Ma,Yiheng Chen,Runlong Yu,Afra Izzati Kamili,Fangqi Chen,Zhaoxi Zhang,Juan Li,Yuki Miura*

Main category: cs.SI

TL;DR: 提出自适应跨城市学习框架，整合移动行为信号和城市相似性数据增强，提升自然灾害期间社交媒体情感分析的准确性和公平性


<details>
  <summary>Details</summary>
Motivation: 现有基于文本的社交媒体情感分析模型存在城市中心偏见，忽视弱势社区，需要更包容的灾害响应系统

Method: 自适应跨城市学习框架，整合移动行为信号和城市相似性数据增强，采用多模态融合和城市感知训练

Result: 在2025年1月南加州野火案例中达到SOTA性能，揭示地理多样情感模式，发现情感表达与真实移动变化正相关

Conclusion: 多模态融合和城市感知训练显著提升准确性和公平性，强调情境敏感情感建模对建设包容性灾害响应系统的重要性

Abstract: Social media platforms provide a real-time lens into public sentiment during natural disasters; however, models built solely on textual data often reinforce urban-centric biases and overlook underrepresented communities. This paper introduces an adaptive cross-city learning framework that enhances disaster sentiment understanding by integrating mobility-informed behavioral signals and city similarity-based data augmentation. Focusing on the January 2025 Southern California wildfires, our model achieves state-of-the-art performance and reveals geographically diverse sentiment patterns, particularly in areas experiencing overlapping fire exposure or delayed emergency responses. We further identify positive correlations between emotional expressions and real-world mobility shifts, underscoring the value of combining behavioral and textual features. Through extensive experiments, we demonstrate that multimodal fusion and city-aware training significantly improve both accuracy and fairness. Collectively, these findings highlight the importance of context-sensitive sentiment modeling and provide actionable insights toward developing more inclusive and equitable disaster response systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [46] [Agentic AI for Commercial Insurance Underwriting with Adversarial Self-Critique](https://arxiv.org/abs/2602.13213)
*Joyjit Roy,Samaresh Kumar Singh*

Main category: cs.AI

TL;DR: 提出一种决策否定、人机协同的智能系统，通过对抗性自我批判机制提升商业保险核保的AI安全性和可靠性，同时保持人类最终决策权。


<details>
  <summary>Details</summary>
Motivation: 商业保险核保是劳动密集型过程，现有AI解决方案缺乏全面的推理能力和确保在受监管高风险环境中可靠性的内部机制。完全自动化在需要人类判断和问责的场景中既不实用也不可取。

Method: 开发决策否定、人机协同的智能系统，包含对抗性自我批判机制作为有界安全架构。系统中设置批判代理挑战主要代理的结论，然后提交给人类审核员。同时建立决策否定代理潜在错误的正式分类法。

Result: 在500个专家验证的核保案例实验中，对抗性批判机制将AI幻觉率从11.3%降至3.8%，决策准确率从92%提升至96%。该框架通过设计确保人类对所有具有约束力决策的最终权威。

Conclusion: 对抗性自我批判支持在受监管领域更安全的AI部署，为人类监督不可或缺的场景提供了负责任集成的模型。这种内部制衡系统解决了受监管工作流程中AI安全的关键缺口。

Abstract: Commercial insurance underwriting is a labor-intensive process that requires manual review of extensive documentation to assess risk and determine policy pricing. While AI offers substantial efficiency improvements, existing solutions lack comprehensive reasoning capabilities and internal mechanisms to ensure reliability within regulated, high-stakes environments. Full automation remains impractical and inadvisable in scenarios where human judgment and accountability are critical. This study presents a decision-negative, human-in-the-loop agentic system that incorporates an adversarial self-critique mechanism as a bounded safety architecture for regulated underwriting workflows. Within this system, a critic agent challenges the primary agent's conclusions prior to submitting recommendations to human reviewers. This internal system of checks and balances addresses a critical gap in AI safety for regulated workflows. Additionally, the research develops a formal taxonomy of failure modes to characterize potential errors by decision-negative agents. This taxonomy provides a structured framework for risk identification and risk management in high-stakes applications. Experimental evaluation using 500 expert-validated underwriting cases demonstrates that the adversarial critique mechanism reduces AI hallucination rates from 11.3% to 3.8% and increases decision accuracy from 92% to 96%. At the same time, the framework enforces strict human authority over all binding decisions by design. These findings indicate that adversarial self-critique supports safer AI deployment in regulated domains and offers a model for responsible integration where human oversight is indispensable.

</details>


### [47] [BotzoneBench: Scalable LLM Evaluation via Graded AI Anchors](https://arxiv.org/abs/2602.13214)
*Lingfeng Li,Yunlong Lu,Yuefei Zhang,Jingyu Yao,Yixin Zhu,KeYuan Cheng,Yongyi Wang,Qirui Zheng,Xionghui Yang,Wenxin Li*

Main category: cs.AI

TL;DR: 论文提出了BotzoneBench框架，通过将LLM与固定技能校准的游戏AI层次结构进行对比，实现线性时间绝对技能测量，用于评估LLM在交互环境中的战略决策能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估基准主要关注静态推理任务，无法有效评估动态战略能力。当前的游戏基准采用LLM对战LLM的锦标赛模式，存在计算成本高、结果依赖瞬时模型池、缺乏稳定性能锚点等问题，难以进行纵向跟踪比较。

Method: 基于Botzone平台的竞争基础设施，构建BotzoneBench评估框架，将LLM与固定技能层次结构的游戏AI进行对比。涵盖8种不同类型的游戏，包括确定性完美信息棋盘游戏和随机性不完美信息纸牌游戏。通过系统评估177,047个状态-动作对，分析5个旗舰模型的性能。

Result: 揭示了显著的性能差异和不同的战略行为模式。表现最佳的模型在多个领域达到了与中高等级专业游戏AI相当的熟练度。该框架实现了线性时间绝对技能测量，具有跨时间稳定性。

Conclusion: 基于固定技能层次结构的锚定评估范式可推广到任何具有明确定义技能层次的领域，为评估交互式AI能力建立了可扩展、可重复使用的框架。

Abstract: Large Language Models (LLMs) are increasingly deployed in interactive environments requiring strategic decision-making, yet systematic evaluation of these capabilities remains challenging. Existing benchmarks for LLMs primarily assess static reasoning through isolated tasks and fail to capture dynamic strategic abilities. Recent game-based evaluations employ LLM-vs-LLM tournaments that produce relative rankings dependent on transient model pools, incurring quadratic computational costs and lacking stable performance anchors for longitudinal tracking. The central challenge is establishing a scalable evaluation framework that measures LLM strategic reasoning against consistent, interpretable standards rather than volatile peer models. Here we show that anchoring LLM evaluation to fixed hierarchies of skill-calibrated game Artificial Intelligence (AI) enables linear-time absolute skill measurement with stable cross-temporal interpretability. Built on the Botzone platform's established competitive infrastructure, our BotzoneBench evaluates LLMs across eight diverse games spanning deterministic perfect-information board games to stochastic imperfect-information card games. Through systematic assessment of 177,047 state-action pairs from five flagship models, we reveal significant performance disparities and identify distinct strategic behaviors, with top-performing models achieving proficiency comparable to mid-to-high-tier specialized game AI in multiple domains. This anchored evaluation paradigm generalizes beyond games to any domain with well-defined skill hierarchies, establishing a scalable and reusable framework for assessing interactive AI capabilities.

</details>


### [48] [When to Think Fast and Slow? AMOR: Entropy-Based Metacognitive Gate for Dynamic SSM-Attention Switching](https://arxiv.org/abs/2602.13215)
*Haoran Zheng*

Main category: cs.AI

TL;DR: AMOR是一种混合架构，结合了状态空间模型（SSM）和稀疏注意力机制，通过预测熵动态判断何时需要注意力计算，从而在保持高效的同时提升长序列信息检索能力。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer对所有位置分配均匀计算资源，而状态空间模型（SSM）虽然高效但在长序列信息检索方面表现不佳。受认知双过程理论启发，研究者希望开发一种能动态适应计算需求的混合架构。

Method: 提出AMOR（自适应元认知输出路由器），基于SSM主干网络，通过预测熵测量模型"不确定性"，仅在不确定时触发稀疏注意力。使用Ghost KV技术从SSM隐藏状态投影键值对，重用SSM的O(n)计算而非每层都需要O(n²)注意力。

Result: 在小规模合成检索任务中，AMOR优于纯SSM和纯Transformer基线，实现完美检索准确率，仅对22%的位置使用注意力。预测熵能可靠指示检索需求，检索位置与本地位置之间存在1.09纳特的熵差（接近熵范围的一半）。

Conclusion: AMOR提供了一种高效且可解释的自适应计算框架，通过信息论指标动态路由决策，在保持SSM计算效率的同时解决了长序列信息检索问题。

Abstract: Transformers allocate uniform computation to every position, regardless of difficulty. State Space Models (SSMs) offer efficient alternatives but struggle with precise information retrieval over a long horizon. Inspired by dual-process theories of cognition (Kahneman, 2011), we propose AMOR (Adaptive Metacognitive Output Router), a hybrid architecture that dynamically engages sparse attention only when an SSM backbone is "uncertain"--as measured by prediction entropy. Compared to standard transformers, AMOR gains efficiency by projecting keys and values from SSM hidden states (Ghost KV), reusing the SSM's O(n) computation rather than requiring O(n^2) attention at every layer. On small-scale synthetic retrieval tasks, AMOR outperforms both SSM-only and transformer-only baselines, achieving perfect retrieval accuracy while engaging attention on only 22% of positions. We validate that prediction entropy reliably signals retrieval need, with a gap of 1.09 nats (nearly half the entropy range) between retrieval and local positions. Additionally, our approach provides interpretable adaptive computation, where routing decisions can be understood in information-theoretic terms.

</details>


### [49] [VeRA: Verified Reasoning Data Augmentation at Scale](https://arxiv.org/abs/2602.13217)
*Zerui Cheng,Jiashuo Liu,Chunjie Wu,Jianzhu Yao,Pramod Viswanath,Ge Zhang,Wenhao Huang*

Main category: cs.AI

TL;DR: VeRA提出了一种动态评估框架，通过将基准问题转换为可执行规范，自动生成无限验证变体，解决传统静态评估中的记忆化和格式利用问题。


<details>
  <summary>Details</summary>
Motivation: 当前大多数评估方案存在"静态"问题：重复使用相同问题导致记忆化、格式利用和最终饱和。需要构建性鲁棒的评估来真实衡量AI进展，而非事后检测。

Method: VeRA框架将基准问题转换为可执行规范，包含：1）带占位符的自然语言模板；2）采样有效配置的连贯生成器；3）验证参数并计算正确答案的确定性验证器。从单个种子问题自动创建无限验证变体。

Result: 评估16个前沿模型发现：1）VeRA-E提高评估质量并揭示污染模式；2）VeRA-H实现无需人工的困难任务生成和可靠标注；3）VeRA建立了验证基准作为通用范式。

Conclusion: VeRA将基准从静态对象重新概念化为按需生成新鲜验证实例的可执行规范，增强评估的鲁棒性和成本效益。任何可验证领域的评估都可以无限扩展而不牺牲标签完整性。

Abstract: The main issue with most evaluation schemes today is their "static" nature: the same problems are reused repeatedly, allowing for memorization, format exploitation, and eventual saturation. To measure genuine AI progress, we need evaluation that is robust by construction, not by post-hoc detection. In response, we propose VeRA (Verified Reasoning Data Augmentation), a framework that converts benchmark problems into executable specifications, comprising (i) a natural language template with placeholder slots, (ii) a coherent generator that samples valid configurations, and (iii) a deterministic verifier that validates parameters and calculates the corresponding correct answers for each configuration. From a single seed problem, VeRA automatically creates unlimited verified variants with reliable labels at near-zero marginal cost without human involvement.
  VeRA operates in two complementary modes. VeRA-E (equivalent) rewrites problems while keeping the underlying logic intact, useful for detecting memorization versus genuine reasoning. VeRA-H (hardened) systematically increases complexity while remaining verifiable, enabling reliable creation and labelling of fresh difficult tasks at the boundary of intelligence. Evaluating 16 frontier models with VeRA, we find: (i) VeRA-E improves evaluation quality and reveals contamination patterns. (ii) VeRA-H enables human-free generation of hard tasks with reliable labels. (iii) VeRA establishes verified benchmarks as a general paradigm. VeRA reconceptualizes benchmarks from static objects used until exhausted, to executable specifications generating fresh, verified instances on demand, enhancing robustness and cost-effectiveness for evaluation.
  With VeRA, we envision that evaluation in any verifiable domain can scale indefinitely without sacrificing label integrity. To stimulate future research, we have open-sourced all code and datasets.

</details>


### [50] [CORPGEN: Simulating Corporate Environments with Autonomous Digital Employees in Multi-Horizon Task Environments](https://arxiv.org/abs/2602.14229)
*Abubakarr Jaye,Nigel Boachie Kumankumah,Chidera Biringa,Anjel Shaileshbhai Patel,Sulaiman Vesal,Dayquan Julienne,Charlotte Siska,Manuel Raúl Meléndez Luján,Anthony Twum-Barimah,Mauricio Velazco,Tianwei Chen*

Main category: cs.AI

TL;DR: 提出Multi-Horizon Task Environments (MHTEs)评估多任务长程推理，开发CorpGen框架解决基线系统在任务负载增加时性能下降的问题，实现3.5倍性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试只评估单任务性能，但真实组织工作需要管理多个并发长程任务，涉及任务交错、依赖关系和优先级调整。需要新的评估框架来模拟这种复杂场景。

Method: 引入MHTEs作为新问题类别，提出CorpGen框架：采用分层规划进行多目标对齐、子代理隔离防止任务污染、分层记忆系统（工作记忆、结构化记忆、语义记忆）和自适应摘要。通过数字员工模拟企业环境。

Result: 在三个CUA后端（UFO2、OpenAI CUA、分层）上测试，CorpGen相比基线实现最高3.5倍性能提升（15.2% vs 4.3%），在负载增加时保持稳定性能。消融研究显示经验学习贡献最大。

Conclusion: CorpGen框架能有效解决多任务长程推理中的关键失败模式，性能提升源于架构机制而非特定CUA实现。该工作为评估和管理复杂多任务环境提供了新框架。

Abstract: Long-horizon reasoning is a key challenge for autonomous agents, yet existing benchmarks evaluate agents on single tasks in isolation. Real organizational work requires managing many concurrent long-horizon tasks with interleaving, dependencies, and reprioritization. We introduce Multi-Horizon Task Environments (MHTEs): a distinct problem class requiring coherent execution across dozens of interleaved tasks (45+, 500-1500+ steps) within persistent execution contexts spanning hours. We identify four failure modes that cause baseline CUAs to degrade from 16.7% to 8.7% completion as load scales 25% to 100%, a pattern consistent across three independent implementations. These failure modes are context saturation (O(N) vs O(1) growth), memory interference, dependency complexity (DAGs vs. chains), and reprioritization overhead. We present CorpGen, an architecture-agnostic framework addressing these failures via hierarchical planning for multi-horizon goal alignment, sub-agent isolation preventing cross-task contamination, tiered memory (working, structured, semantic), and adaptive summarization. CorpGen simulates corporate environments through digital employees with persistent identities and realistic schedules. Across three CUA backends (UFO2, OpenAI CUA, hierarchical) on OSWorld Office, CorpGen achieves up to 3.5x improvement over baselines (15.2% vs 4.3%) with stable performance under increasing load, confirming that gains stem from architectural mechanisms rather than specific CUA implementations. Ablation studies show experiential learning provides the largest gains.

</details>


### [51] [Scaling the Scaling Logic: Agentic Meta-Synthesis of Logic Reasoning](https://arxiv.org/abs/2602.13218)
*Bowen Liu,Zhi Wu,Runquan Xie,Zhanhui Kang,Jia Li*

Main category: cs.AI

TL;DR: SSLogic是一个通过生成-验证-修复循环自动合成可执行程序对（生成器-验证器）的元合成框架，用于扩展可验证的强化学习训练信号，从400个种子任务族扩展到953个族和21,389个可验证实例。


<details>
  <summary>Details</summary>
Motivation: 可验证训练信号的扩展是强化学习从可验证奖励（RLVR）的关键瓶颈。逻辑推理是一个自然的基础，因为约束是形式化的且答案可通过程序检查，但现有的合成方法要么依赖专家编写的代码，要么在固定模板/框架内操作，限制了任务族级别的扩展。

Method: 提出SSLogic框架：1）通过生成-验证-修复循环迭代合成和修复可执行的生成器-验证器程序对；2）引入多门验证协议，结合多策略一致性检查和对抗性盲审，由独立代理编写和执行代码来解决实例，过滤模糊或不良定义的任务。

Result: 从400个种子任务族开始，经过两轮演化扩展到953个任务族和21,389个可验证实例（从5,718个增长）。在相同训练步数下，使用SSLogic演化数据训练相比种子基线持续提升：SynLogic +5.2，BBEH +1.4，AIME25 +3.0，Brumo25 +3.7。

Conclusion: SSLogic框架能够以任务族级别扩展可验证的训练信号，通过自动化的程序合成和严格验证机制，显著提高了强化学习代理在逻辑推理任务上的性能，为解决RLVR的可验证信号扩展问题提供了有效方案。

Abstract: Scaling verifiable training signals remains a key bottleneck for Reinforcement Learning from Verifiable Rewards (RLVR). Logical reasoning is a natural substrate: constraints are formal and answers are programmatically checkable. However, prior synthesis pipelines either depend on expert-written code or operate within fixed templates/skeletons, which limits growth largely to instance-level perturbations. We propose SSLogic, an agentic meta-synthesis framework that scales at the task-family level by iteratively synthesizing and repairing executable Generator--Validator program pairs in a closed Generate--Validate--Repair loop, enabling continuous family evolution with controllable difficulty. To ensure reliability, we introduce a Multi-Gate Validation Protocol that combines multi-strategy consistency checks with Adversarial Blind Review, where independent agents must solve instances by writing and executing code to filter ambiguous or ill-posed tasks. Starting from 400 seed families, two evolution rounds expand to 953 families and 21,389 verifiable instances (from 5,718). Training on SSLogic-evolved data yields consistent gains over the seed baseline at matched training steps, improving SynLogic by +5.2, BBEH by +1.4, AIME25 by +3.0, and Brumo25 by +3.7.

</details>


### [52] [A Geometric Taxonomy of Hallucinations in LLMs](https://arxiv.org/abs/2602.13224)
*Javier Marín*

Main category: cs.AI

TL;DR: 论文提出幻觉的三分法：不忠实、虚构和事实错误，发现前两种在嵌入空间有可检测的几何特征，但事实错误无法通过嵌入检测，需要外部验证机制。


<details>
  <summary>Details</summary>
Motivation: 当前"幻觉"一词笼统地涵盖了多种不同现象，缺乏清晰的分类。需要区分不同类型的幻觉，理解它们在嵌入空间中的几何特征，以明确基于嵌入的检测方法的适用范围。

Method: 提出幻觉的三分法：1) 不忠实（未利用上下文）；2) 虚构（发明语义上无关的内容）；3) 事实错误（在正确概念框架内的错误主张）。通过分析标准基准测试和人工构建的虚构案例，研究这些类型在嵌入空间中的几何特征和可检测性。

Result: 发现显著的不对称性：在标准基准测试中，幻觉检测是领域局部的（AUROC 0.76-0.99），但跨领域时降至0.50（随机水平）。对于人工构建的虚构，单一全局方向能达到0.96 AUROC，跨领域仅退化3.8%。事实错误的AUROC为0.478，与随机水平无异，表明嵌入无法区分真假陈述。

Conclusion: 幻觉的几何结构差异反映了底层现象的不同。基于嵌入的检测适用于类型I和II（不忠实和虚构），但无法检测类型III（事实错误），后者需要外部验证机制。嵌入编码的是分布共现模式，而非与外部现实的对应关系。

Abstract: The term "hallucination" in large language models conflates distinct phenomena with different geometric signatures in embedding space. We propose a taxonomy identifying three types: unfaithfulness (failure to engage with provided context), confabulation (invention of semantically foreign content), and factual error (incorrect claims within correct conceptual frames). We observe a striking asymmetry. On standard benchmarks where hallucinations are LLM-generated, detection is domain-local: AUROC 0.76-0.99 within domains, but 0.50 (chance level) across domains. Discriminative directions are approximately orthogonal between domains (mean cosine similarity -0.07). On human-crafted confabulations - invented institutions, redefined terminology, fabricated mechanisms - a single global direction achieves 0.96 AUROC with 3.8% cross-domain degradation. We interpret this divergence as follows: benchmarks capture generation artifacts (stylistic signatures of prompted fabrication), while human-crafted confabulations capture genuine topical drift. The geometric structure differs because the underlying phenomena differ. Type III errors show 0.478 AUROC - indistinguishable from chance. This reflects a theoretical constraint: embeddings encode distributional co-occurrence, not correspondence to external reality. Statements with identical contextual patterns occupy similar embedding regions regardless of truth value. The contribution is a geometric taxonomy clarifying the scope of embedding-based detection: Types I and II are detectable; Type III requires external verification mechanisms.

</details>


### [53] [Experimentation Accelerator: Interpretable Insights and Creative Recommendations for A/B Testing with Content-Aware ranking](https://arxiv.org/abs/2602.13852)
*Zhengmian Hu,Lei Shi,Ritwik Sinha,Justin Grover,David Arbour*

Main category: cs.AI

TL;DR: 提出一个统一框架，利用历史A/B测试结果和内容嵌入来优化在线实验：优先选择测试变体、解释获胜原因、发现新变体机会，已集成到Adobe产品中。


<details>
  <summary>Details</summary>
Motivation: 现代在线实验面临两个瓶颈：流量稀缺导致难以选择测试变体，事后洞察提取依赖手动、不一致且内容无关的方法。同时，组织未能充分利用历史A/B测试结果和丰富的内容嵌入来指导优先级排序和创意迭代。

Method: 1) 利用处理嵌入和历史结果训练CTR排序模型，包含上下文变化的固定效应；2) 将处理映射到语义营销属性，通过符号一致、稀疏约束的Lasso重新表达排序器；3) 计算机会指数，结合属性重要性和当前实验中的低表达度；4) 使用LLM将机会转化为具体创意建议并估计学习和转化潜力。

Result: 该框架已构建到Adobe产品"Experimentation Accelerator"中，为商业客户提供基于AI的洞察和规模化实验能力。在Adobe商业客户的真实世界实验中验证了生成管道的高质量性能。

Conclusion: 提出的统一框架解决了在线实验的关键瓶颈，通过AI驱动的优先级排序、解释和机会发现，实现了更快、更信息丰富、更高效的测试周期，已成功产品化并验证了实际效果。

Abstract: Modern online experimentation faces two bottlenecks: scarce traffic forces tough choices on which variants to test, and post-hoc insight extraction is manual, inconsistent, and often content-agnostic. Meanwhile, organizations underuse historical A/B results and rich content embeddings that could guide prioritization and creative iteration. We present a unified framework to (i) prioritize which variants to test, (ii) explain why winners win, and (iii) surface targeted opportunities for new, higher-potential variants. Leveraging treatment embeddings and historical outcomes, we train a CTR ranking model with fixed effects for contextual shifts that scores candidates while balancing value and content diversity. For better interpretability and understanding, we project treatments onto curated semantic marketing attributes and re-express the ranker in this space via a sign-consistent, sparse constrained Lasso, yielding per-attribute coefficients and signed contributions for visual explanations, top-k drivers, and natural-language insights. We then compute an opportunity index combining attribute importance (from the ranker) with under-expression in the current experiment to flag missing, high-impact attributes. Finally, LLMs translate ranked opportunities into concrete creative suggestions and estimate both learning and conversion potential, enabling faster, more informative, and more efficient test cycles. These components have been built into a real Adobe product, called \textit{Experimentation Accelerator}, to provide AI-based insights and opportunities to scale experimentation for customers. We provide an evaluation of the performance of the proposed framework on some real-world experiments by Adobe business customers that validate the high quality of the generation pipeline.

</details>


### [54] [Variation is the Key: A Variation-Based Framework for LLM-Generated Text Detection](https://arxiv.org/abs/2602.13226)
*Xuecong Li,Xiaohong Li,Qiang Hu,Yao Zhang,Junjie Wang*

Main category: cs.AI

TL;DR: VaryBalance：一种简单有效的LLM生成文本检测方法，通过比较原始文本与LLM重写版本之间的差异来区分人类文本和AI生成文本


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成文本检测方法存在局限性：要么依赖不切实际的白盒假设，要么仅依赖文本级特征，导致检测能力不精确。需要一种更有效、实用的检测方法。

Method: VaryBalance的核心思想是：人类文本与LLM重写版本之间的差异比LLM生成文本与其重写版本之间的差异更大。通过计算平均标准差来量化这种差异，从而区分人类文本和AI生成文本。

Result: VaryBalance在AUROC指标上比当前最先进的检测器Binoculars高出34.3%，并且在多种生成模型和语言中保持鲁棒性。

Conclusion: VaryBalance是一种简单但有效且实用的LLM生成文本检测方法，通过利用人类文本与LLM重写版本之间的差异特性，实现了比现有方法更好的检测性能。

Abstract: Detecting text generated by large language models (LLMs) is crucial but challenging. Existing detectors depend on impractical assumptions, such as white-box settings, or solely rely on text-level features, leading to imprecise detection ability. In this paper, we propose a simple but effective and practical LLM-generated text detection method, VaryBalance. The core of VaryBalance is that, compared to LLM-generated texts, there is a greater difference between human texts and their rewritten version via LLMs. Leveraging this observation, VaryBalance quantifies this through mean standard deviation and distinguishes human texts and LLM-generated texts. Comprehensive experiments demonstrated that VaryBalance outperforms the state-of-the-art detectors, i.e., Binoculars, by up to 34.3\% in terms of AUROC, and maintains robustness against multiple generating models and languages.

</details>


### [55] [Intelligence as Trajectory-Dominant Pareto Optimization](https://arxiv.org/abs/2602.13230)
*Truong Xuan Khanh,Truong Quynh Hoa*

Main category: cs.AI

TL;DR: 该论文提出轨迹主导帕累托优化框架，认为智能停滞源于轨迹空间的帕累托陷阱而非学习能力不足，并引入陷阱逃逸难度指数来量化这些约束。


<details>
  <summary>Details</summary>
Motivation: 尽管人工智能系统持续优化，但在长期适应性方面出现停滞。作者认为这种限制主要不是由于学习、数据或模型容量不足，而是源于智能随时间优化的深层结构特性。

Method: 将智能表述为受多目标权衡控制的轨迹级现象，引入轨迹主导帕累托优化框架，定义轨迹级支配关系。提出陷阱逃逸难度指数(TEDI)来量化约束刚性，建立帕累托陷阱的形式化分类，并通过最小智能体-环境模型进行说明。

Result: 动态智能上限是轨迹级支配的必然几何结果，与学习进展或架构规模无关。轨迹级分歧现象在智能体-环境模型中得到验证，为诊断和克服自适应系统中的长期发展约束提供了原则性框架。

Conclusion: 智能研究的重点应从终端性能转向优化几何结构，轨迹主导帕累托优化框架为理解和克服长期发展约束提供了理论基础，有助于诊断和设计更适应性的系统。

Abstract: Despite recent advances in artificial intelligence, many systems exhibit stagnation in long-horizon adaptability despite continued performance optimization. This work argues that such limitations do not primarily arise from insufficient learning, data, or model capacity, but from a deeper structural property of how intelligence is optimized over time. We formulate intelligence as a trajectory-level phenomenon governed by multi-objective trade-offs, and introduce Trajectory-Dominant Pareto Optimization, a path-wise generalization of classical Pareto optimality in which dominance is defined over full trajectories. Within this framework, Pareto traps emerge as locally non-dominated regions of trajectory space that nevertheless restrict access to globally superior developmental paths under conservative local optimization. To characterize the rigidity of such constraints, we define the Trap Escape Difficulty Index (TEDI), a composite geometric measure capturing escape distance, structural constraints, and behavioral inertia. We show that dynamic intelligence ceilings arise as inevitable geometric consequences of trajectory-level dominance, independent of learning progress or architectural scale. We further introduce a formal taxonomy of Pareto traps and illustrate the resulting trajectory-level divergence using a minimal agent-environment model. Together, these results shift the locus of intelligence from terminal performance to optimization geometry, providing a principled framework for diagnosing and overcoming long-horizon developmental constraints in adaptive systems.

</details>


### [56] [PlotChain: Deterministic Checkpointed Evaluation of Multimodal LLMs on Engineering Plot Reading](https://arxiv.org/abs/2602.13232)
*Mayank Ravishankara*

Main category: cs.AI

TL;DR: PlotChain是一个用于评估多模态大语言模型从工程图表中恢复数值能力的确定性基准，包含15类450个图表，通过检查点诊断评估模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM评估主要关注OCR提取或自由形式描述，缺乏对工程图表（如Bode图、应力-应变曲线等）中定量数值恢复能力的系统性评估。

Method: 创建包含15类工程图表的确定性基准，每类30个图表，每个图表都有已知参数和精确真值。采用检查点诊断评估，包含中间字段以定位失败原因。使用标准化协议（温度=0，严格JSON输出）和容差策略评分。

Result: 在plotread容差策略下，最佳模型表现：Gemini 2.5 Pro 80.42%，GPT-4.1 79.84%，Claude Sonnet 4.5 78.21%，GPT-4o 61.59%。频域任务表现较差，带通响应≤23%。

Conclusion: PlotChain为MLLM在工程图表数值恢复能力提供了可复现的评估基准，揭示了模型在频域任务上的薄弱环节，并提供了完整的工具链支持后续研究。

Abstract: We present PlotChain, a deterministic, generator-based benchmark for evaluating multimodal large language models (MLLMs) on engineering plot reading-recovering quantitative values from classic plots (e.g., Bode/FFT, step response, stress-strain, pump curves) rather than OCR-only extraction or free-form captioning. PlotChain contains 15 plot families with 450 rendered plots (30 per family), where every item is produced from known parameters and paired with exact ground truth computed directly from the generating process. A central contribution is checkpoint-based diagnostic evaluation: in addition to final targets, each item includes intermediate 'cp_' fields that isolate sub-skills (e.g., reading cutoff frequency or peak magnitude) and enable failure localization within a plot family. We evaluate four state-of-the-art MLLMs under a standardized, deterministic protocol (temperature = 0 and a strict JSON-only numeric output schema) and score predictions using per-field tolerances designed to reflect human plot-reading precision. Under the 'plotread' tolerance policy, the top models achieve 80.42% (Gemini 2.5 Pro), 79.84% (GPT-4.1), and 78.21% (Claude Sonnet 4.5) overall field-level pass rates, while GPT-4o trails at 61.59%. Despite strong performance on many families, frequency-domain tasks remain brittle: bandpass response stays low (<= 23%), and FFT spectrum remains challenging. We release the generator, dataset, raw model outputs, scoring code, and manifests with checksums to support fully reproducible runs and retrospective rescoring under alternative tolerance policies.

</details>


### [57] [Stay in Character, Stay Safe: Dual-Cycle Adversarial Self-Evolution for Safety Role-Playing Agents](https://arxiv.org/abs/2602.13234)
*Mingyang Liao,Yichen Wan,shuchen wu,Chenxi Miao,Xin Shen,Weikang Li,Yang Li,Deguo Xia,Jizhou Huang*

Main category: cs.AI

TL;DR: 提出训练免费的双循环对抗自进化框架，通过攻击者循环生成更强越狱提示，防御者循环构建层次化知识库，在推理时检索组合知识来平衡角色保真度和安全性。


<details>
  <summary>Details</summary>
Motivation: 基于LLM的角色扮演在保真度提升的同时，更强的角色约束通常会增加对越狱攻击的脆弱性，特别是对于风险或负面角色。现有训练时解决方案成本高、维护困难、可能损害角色行为，且不适用于前沿闭源LLM。

Method: 提出训练免费的双循环对抗自进化框架：1) 角色目标攻击者循环合成渐进增强的越狱提示；2) 角色扮演防御者循环将观察到的失败提炼为层次化知识库（全局安全规则、角色基础约束、安全角色内示例）。推理时防御者从层次中检索组合结构化知识来指导生成。

Result: 在多个专有LLM上的广泛实验显示，相比强基线在角色保真度和越狱抵抗方面取得一致提升，并对未见角色和攻击提示具有鲁棒泛化能力。

Conclusion: 该训练免费框架能有效平衡角色扮演的保真度和安全性，避免传统训练时方法的成本和维护问题，适用于前沿闭源LLM。

Abstract: LLM-based role-playing has rapidly improved in fidelity, yet stronger adherence to persona constraints commonly increases vulnerability to jailbreak attacks, especially for risky or negative personas. Most prior work mitigates this issue with training-time solutions (e.g., data curation or alignment-oriented regularization). However, these approaches are costly to maintain as personas and attack strategies evolve, can degrade in-character behavior, and are typically infeasible for frontier closed-weight LLMs. We propose a training-free Dual-Cycle Adversarial Self-Evolution framework with two coupled cycles. A Persona-Targeted Attacker Cycle synthesizes progressively stronger jailbreak prompts, while a Role-Playing Defender Cycle distills observed failures into a hierarchical knowledge base of (i) global safety rules, (ii) persona-grounded constraints, and (iii) safe in-character exemplars. At inference time, the Defender retrieves and composes structured knowledge from this hierarchy to guide generation, producing responses that remain faithful to the target persona while satisfying safety constraints. Extensive experiments across multiple proprietary LLMs show consistent gains over strong baselines on both role fidelity and jailbreak resistance, and robust generalization to unseen personas and attack prompts.

</details>


### [58] [Lang2Act: Fine-Grained Visual Reasoning through Self-Emergent Linguistic Toolchains](https://arxiv.org/abs/2602.13235)
*Yuqi Xiong,Chunyi Peng,Zhipeng Xu,Zhenghao Liu,Zulong Chen,Yukun Yan,Shuo Wang,Yu Gu,Ge Yu*

Main category: cs.AI

TL;DR: Lang2Act通过自涌现的语言工具链增强视觉语言模型的细粒度视觉感知能力，使用两阶段强化学习训练框架，相比传统VRAG方法性能提升超过4%


<details>
  <summary>Details</summary>
Motivation: 现有VRAG框架依赖预定义的外部工具，将视觉感知与推理过程分离，这种解耦设计会导致视觉信息损失，特别是在图像裁剪等操作时。需要更灵活的视觉感知增强方法。

Method: 提出Lang2Act框架，通过自涌现的语言工具链实现细粒度视觉感知和推理。采用两阶段强化学习训练：第一阶段优化VLM自探索高质量动作构建可重用语言工具箱；第二阶段优化VLM有效利用这些语言工具进行下游推理。

Result: 实验结果表明Lang2Act显著增强了VLM的视觉感知能力，实现了超过4%的性能提升。代码和数据已开源。

Conclusion: Lang2Act通过自涌现的语言工具链解决了传统VRAG框架的视觉信息损失问题，为增强VLM的细粒度视觉感知能力提供了有效的新方法。

Abstract: Visual Retrieval-Augmented Generation (VRAG) enhances Vision-Language Models (VLMs) by incorporating external visual documents to address a given query. Existing VRAG frameworks usually depend on rigid, pre-defined external tools to extend the perceptual capabilities of VLMs, typically by explicitly separating visual perception from subsequent reasoning processes. However, this decoupled design can lead to unnecessary loss of visual information, particularly when image-based operations such as cropping are applied. In this paper, we propose Lang2Act, which enables fine-grained visual perception and reasoning through self-emergent linguistic toolchains. Rather than invoking fixed external engines, Lang2Act collects self-emergent actions as linguistic tools and leverages them to enhance the visual perception capabilities of VLMs. To support this mechanism, we design a two-stage Reinforcement Learning (RL)-based training framework. Specifically, the first stage optimizes VLMs to self-explore high-quality actions for constructing a reusable linguistic toolbox, and the second stage further optimizes VLMs to exploit these linguistic tools for downstream reasoning effectively. Experimental results demonstrate the effectiveness of Lang2Act in substantially enhancing the visual perception capabilities of VLMs, achieving performance improvements of over 4%. All code and data are available at https://github.com/NEUIR/Lang2Act.

</details>


### [59] [NL2LOGIC: AST-Guided Translation of Natural Language into First-Order Logic with Large Language Models](https://arxiv.org/abs/2602.13237)
*Rizky Ramadhana Putra,Raihan Sultan Pasha Basuki,Yutong Cheng,Peng Gao*

Main category: cs.AI

TL;DR: NL2LOGIC是一个将自然语言翻译为一阶逻辑的框架，通过引入抽象语法树作为中间表示，结合递归语义解析器和AST引导的生成器，显著提升了语法准确性和语义正确性。


<details>
  <summary>Details</summary>
Motivation: 在法律和治理等领域，自动化推理需要准确性和可解释性。现有方法如GCD和CODE4LOGIC虽然利用大语言模型改进逻辑解析，但存在语法控制脆弱（全局语法约束执行弱）和语义忠实度低（子句级语义理解不足）的问题。

Method: 提出NL2LOGIC框架，引入抽象语法树作为中间表示。结合基于大语言模型的递归语义解析器和AST引导的生成器，确定性地生成可直接用于求解器的逻辑代码。

Result: 在FOLIO、LogicNLI和ProofWriter基准测试中，NL2LOGIC达到99%的语法准确率，语义正确性比最先进基线提升高达30%。集成到Logic-LM后，获得近乎完美的可执行性，下游推理准确率比Logic-LM原始少样本无约束翻译模块提升31%。

Conclusion: NL2LOGIC通过引入抽象语法树作为中间表示，有效解决了现有方法在语法控制和语义理解方面的不足，显著提升了自然语言到一阶逻辑翻译的准确性和可靠性。

Abstract: Automated reasoning is critical in domains such as law and governance, where verifying claims against facts in documents requires both accuracy and interpretability. Recent work adopts structured reasoning pipelines that translate natural language into first-order logic and delegate inference to automated solvers. With the rise of large language models, approaches such as GCD and CODE4LOGIC leverage their reasoning and code generation capabilities to improve logic parsing. However, these methods suffer from fragile syntax control due to weak enforcement of global grammar constraints and low semantic faithfulness caused by insufficient clause-level semantic understanding. We propose NL2LOGIC, a first-order logic translation framework that introduces an abstract syntax tree as an intermediate representation. NL2LOGIC combines a recursive large language model based semantic parser with an abstract syntax tree guided generator that deterministically produces solver-ready logic code. Experiments on the FOLIO, LogicNLI, and ProofWriter benchmarks show that NL2LOGIC achieves 99 percent syntactic accuracy and improves semantic correctness by up to 30 percent over state-of-the-art baselines. Furthermore, integrating NL2LOGIC into Logic-LM yields near-perfect executability and improves downstream reasoning accuracy by 31 percent compared to Logic-LM's original few-shot unconstrained translation module.

</details>


### [60] [AST-PAC: AST-guided Membership Inference for Code](https://arxiv.org/abs/2602.13240)
*Roham Koohestani,Ali Al-Kaswan,Jonathan Katzy,Maliheh Izadi*

Main category: cs.AI

TL;DR: 评估代码大语言模型的成员推断攻击方法，发现现有方法在代码领域存在局限性，提出基于抽象语法树的改进方法AST-PAC


<details>
  <summary>Details</summary>
Motivation: 代码大语言模型通常在包含限制性许可源代码的大规模数据集上训练，这带来了数据治理和版权挑战。需要有效的成员推断攻击作为审计机制来检测未经授权的数据使用。

Method: 探索性研究评估Loss Attack和Polarized Augment Calibration (PAC)方法在3B-7B参数代码模型上的效果。针对PAC在代码语法上的局限性，提出AST-PAC方法，使用抽象语法树(AST)基于扰动生成语法有效的校准样本。

Result: PAC通常优于Loss基准，但其效果依赖于忽略代码严格语法的增强策略，导致在大型复杂文件上性能下降。AST-PAC在语法规模增长时表现改善，而PAC则下降，但在小文件和字母数字丰富代码上表现不足。

Conclusion: 研究结果激励未来工作在语法感知和规模自适应校准方面的探索，这是代码语言模型可靠来源审计的前提条件。

Abstract: Code Large Language Models are frequently trained on massive datasets containing restrictively licensed source code. This creates urgent data governance and copyright challenges. Membership Inference Attacks (MIAs) can serve as an auditing mechanism to detect unauthorized data usage in models. While attacks like the Loss Attack provide a baseline, more involved methods like Polarized Augment Calibration (PAC) remain underexplored in the code domain. This paper presents an exploratory study evaluating these methods on 3B--7B parameter code models. We find that while PAC generally outperforms the Loss baseline, its effectiveness relies on augmentation strategies that disregard the rigid syntax of code, leading to performance degradation on larger, complex files. To address this, we introduce AST-PAC, a domain-specific adaptation that utilizes Abstract Syntax Tree (AST) based perturbations to generate syntactically valid calibration samples. Preliminary results indicate that AST-PAC improves as syntactic size grows, where PAC degrades, but under-mutates small files and underperforms on alphanumeric-rich code. Overall, the findings motivate future work on syntax-aware and size-adaptive calibration as a prerequisite for reliable provenance auditing of code language models.

</details>


### [61] [X-Blocks: Linguistic Building Blocks of Natural Language Explanations for Automated Vehicles](https://arxiv.org/abs/2602.13248)
*Ashkan Y. Zadeh,Xiaomeng Li,Andry Rakotonirainy,Ronald Schroeter,Sebastien Glaser,Zishuo Zhu*

Main category: cs.AI

TL;DR: X-Blocks框架：用于分析自动驾驶自然语言解释的三层语言结构框架，包含上下文分类、句法分析和词汇模式识别


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏系统性框架来分析人类如何在不同驾驶场景下语言构建驾驶原理，需要建立信任和接受自动驾驶系统的自然语言解释框架

Method: 提出X-Blocks分层分析框架，包含三个层次：1）上下文层使用RACE多LLM集成框架进行分类；2）句法层使用依存分析和模板提取；3）词汇层使用对数优势分析和信息性狄利克雷先验

Result: RACE在人类标注一致的案例中达到91.45%准确率和0.91科恩卡帕系数，接近人类可靠性；发现特定场景的词汇模式和有限的语法家族库；框架具有数据集无关性和任务独立性

Conclusion: X-Blocks框架为生成场景感知的解释提供了基于证据的语言设计原则，支持自动驾驶系统的透明度、用户信任和认知可访问性

Abstract: Natural language explanations play a critical role in establishing trust and acceptance of automated vehicles (AVs), yet existing approaches lack systematic frameworks for analysing how humans linguistically construct driving rationales across diverse scenarios. This paper introduces X-Blocks (eXplanation Blocks), a hierarchical analytical framework that identifies the linguistic building blocks of natural language explanations for AVs at three levels: context, syntax, and lexicon.
  At the context level, we propose RACE (Reasoning-Aligned Classification of Explanations), a multi-LLM ensemble framework that combines Chain-of-Thought reasoning with self-consistency mechanisms to robustly classify explanations into 32 scenario-aware categories. Applied to human-authored explanations from the Berkeley DeepDrive-X dataset, RACE achieves 91.45 percent accuracy and a Cohens kappa of 0.91 against cases with human annotator agreement, indicating near-human reliability for context classification.
  At the lexical level, log-odds analysis with informative Dirichlet priors reveals context-specific vocabulary patterns that distinguish driving scenarios. At the syntactic level, dependency parsing and template extraction show that explanations draw from a limited repertoire of reusable grammar families, with systematic variation in predicate types and causal constructions across contexts.
  The X-Blocks framework is dataset-agnostic and task-independent, offering broad applicability to other automated driving datasets and safety-critical domains. Overall, our findings provide evidence-based linguistic design principles for generating scenario-aware explanations that support transparency, user trust, and cognitive accessibility in automated driving systems.

</details>


### [62] [DPBench: Large Language Models Struggle with Simultaneous Coordination](https://arxiv.org/abs/2602.13255)
*Najmul Hasan,Prashanth BusiReddyGari*

Main category: cs.AI

TL;DR: DPBench是一个基于哲学家就餐问题的基准测试，评估LLM在资源竞争下的协调能力，发现LLM在顺序决策时表现良好，但在同时决策时死锁率超过95%


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在多智能体系统中部署增加，但缺乏测试它们在资源竞争下协调能力的基准测试

Method: 基于哲学家就餐问题设计DPBench基准，包含8种不同条件（决策时序、群体规模、通信），测试GPT-5.2、Claude Opus 4.5和Grok 4.1的协调能力

Result: LLM在顺序决策时能有效协调，但在同时决策时死锁率超过95%；通信不仅不能解决问题，反而可能增加死锁率；失败源于收敛性推理导致相同策略

Conclusion: 需要并发资源访问的多智能体LLM系统可能需要外部协调机制，而非依赖涌现协调；DPBench作为开源基准发布

Abstract: Large language models are increasingly deployed in multi-agent systems, yet we lack benchmarks that test whether they can coordinate under resource contention. We introduce DPBench, a benchmark based on the Dining Philosophers problem that evaluates LLM coordination across eight conditions that vary decision timing, group size, and communication. Our experiments with GPT-5.2, Claude Opus 4.5, and Grok 4.1 reveal a striking asymmetry: LLMs coordinate effectively in sequential settings but fail when decisions must be made simultaneously, with deadlock rates exceeding 95\% under some conditions. We trace this failure to convergent reasoning, where agents independently arrive at identical strategies that, when executed simultaneously, guarantee deadlock. Contrary to expectations, enabling communication does not resolve this problem and can even increase deadlock rates. Our findings suggest that multi-agent LLM systems requiring concurrent resource access may need external coordination mechanisms rather than relying on emergent coordination. DPBench is released as an open-source benchmark. Code and benchmark are available at https://github.com/najmulhasan-code/dpbench.

</details>


### [63] [MAPLE: A Sub-Agent Architecture for Memory, Learning, and Personalization in Agentic AI Systems](https://arxiv.org/abs/2602.13258)
*Deepak Babu Piskala*

Main category: cs.AI

TL;DR: MAPLE将LLM代理的记忆、学习和个性化分解为三个独立的子组件，通过专用架构实现更好的用户适应能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理系统将记忆、学习和个性化视为统一能力，这种架构上的混淆限制了代理对个体用户的适应能力。需要将这些功能分解为独立机制，分别优化。

Method: 提出MAPLE架构，将系统分解为三个专用子代理：Memory（存储和检索基础设施）、Learning（从累积交互中异步提取智能）、Personalization（在有限上下文预算内实时应用学习知识）。每个组件都有专门工具和明确定义的接口。

Result: 在MAPLE-Personas基准测试中，个性化得分比无状态基线提高14.6%（p < 0.01，Cohen's d = 0.95），特质融入率从45%提高到75%。

Conclusion: 通过将记忆、学习和个性化分解为独立组件，MAPLE实现了真正学习和适应的代理，显著提升了用户个性化能力。

Abstract: Large language model (LLM) agents have emerged as powerful tools for complex tasks, yet their ability to adapt to individual users remains fundamentally limited. We argue this limitation stems from a critical architectural conflation: current systems treat memory, learning, and personalization as a unified capability rather than three distinct mechanisms requiring different infrastructure, operating on different timescales, and benefiting from independent optimization. We propose MAPLE (Memory-Adaptive Personalized LEarning), a principled decomposition where Memory handles storage and retrieval infrastructure; Learning extracts intelligence from accumulated interactions asynchronously; and Personalization applies learned knowledge in real-time within finite context budgets. Each component operates as a dedicated sub-agent with specialized tooling and well-defined interfaces. Experimental evaluation on the MAPLE-Personas benchmark demonstrates that our decomposition achieves a 14.6% improvement in personalization score compared to a stateless baseline (p < 0.01, Cohen's d = 0.95) and increases trait incorporation rate from 45% to 75% -- enabling agents that genuinely learn and adapt.

</details>


### [64] [General learned delegation by clones](https://arxiv.org/abs/2602.13262)
*Darren Li,Meiqi Chen,Chenze Shao,Fandong Meng,Jie Zhou*

Main category: cs.AI

TL;DR: SELFCEST通过强化学习让基础模型能够并行生成相同权重的克隆体，在固定推理预算下优化计算效率，提升数学推理和长上下文问答的性能。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在测试时需要额外计算来提升性能，但串行推理或无协调的并行采样在固定推理预算下计算效率低下，需要更智能的计算资源分配方法。

Method: SELFCEST通过强化学习训练基础模型，使其能够在并行上下文中生成相同权重的克隆体，使用共享参数的rollouts进行端到端训练，学习控制器来分配生成和上下文预算。

Result: 在数学推理基准测试和长上下文多跳问答任务中，SELFCEST在相同推理预算下相比单体基线模型提升了准确率-成本Pareto前沿，并在两个领域都展现出分布外泛化能力。

Conclusion: SELFCEST通过智能并行计算分配机制，在固定推理预算下显著提升了语言模型的性能效率，为计算资源受限场景下的模型推理提供了有效解决方案。

Abstract: Frontier language models improve with additional test-time computation, but serial reasoning or uncoordinated parallel sampling can be compute-inefficient under fixed inference budgets. We propose SELFCEST, which equips a base model with the ability to spawn same-weight clones in separate parallel contexts by agentic reinforcement learning. Training is end-to-end under a global task reward with shared-parameter rollouts, yielding a learned controller that allocates both generation and context budget across branches. Across challenging math reasoning benchmarks and long-context multi-hop QA, SELFCEST improves the accuracy-cost Pareto frontier relative to monolithic baselines at matched inference budget, and exhibits out-of-distribution generalization in both domains.

</details>


### [65] [Accuracy Standards for AI at Work vs. Personal Life: Evidence from an Online Survey](https://arxiv.org/abs/2602.13283)
*Gaston Besanson,Federico Todeschini*

Main category: cs.AI

TL;DR: 研究发现人们在专业和个人场景中对AI工具准确性的要求存在显著差异：工作中要求高准确性的比例（24.1%）远高于个人生活（8.8%），且工具不可用时对个人生活的干扰更大。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究人们在专业和个人场景中如何权衡AI工具的准确性需求，以及当AI工具不可用时用户如何应对。现代AI系统（特别是生成模型）能产生可接受但不完全相同的输出，因此需要重新定义"准确性"为特定情境下的可靠性。

Method: 采用在线调查方法（N=300），其中170名受访者回答了准确性相关问题。研究定义了"准确性"为情境特定的可靠性：输出与用户意图在容忍阈值内的对齐程度，该阈值取决于风险水平和修正成本。通过统计分析比较专业和个人场景中的准确性要求差异。

Result: 1. 工作中要求高准确性（最高等级）的比例为24.1%，个人生活中仅为8.8%（差异显著）；2. 使用前两个等级定义时，差异仍然显著（67.0% vs. 32.9%）；3. 重度应用使用和经验模式与更严格的工作标准相关；4. 工具不可用时，个人生活受到的干扰（34.1%）大于工作场景（15.3%）。

Conclusion: 人们在专业场景中对AI工具的准确性要求显著高于个人场景，且工具不可用对个人生活的干扰更大。这表明AI工具的设计需要考虑使用场景的差异，专业场景需要更高的可靠性标准，而个人场景可能更注重便利性和可用性。

Abstract: We study how people trade off accuracy when using AI-powered tools in professional versus personal contexts for adoption purposes, the determinants of those trade-offs, and how users cope when AI/apps are unavailable. Because modern AI systems (especially generative models) can produce acceptable but non-identical outputs, we define "accuracy" as context-specific reliability: the degree to which an output aligns with the user's intent within a tolerance threshold that depends on stakes and the cost of correction. In an online survey (N=300), among respondents with both accuracy items (N=170), the share requiring high accuracy (top-box) is 24.1% at work vs. 8.8% in personal life (+15.3 pp; z=6.29, p<0.001). The gap remains large under a broader top-two-box definition (67.0% vs. 32.9%) and on the full 1-5 ordinal scale (mean 3.86 vs. 3.08). Heavy app use and experience patterns correlate with stricter work standards (H2). When tools are unavailable (H3), respondents report more disruption in personal routines than at work (34.1% vs. 15.3%, p<0.01). We keep the main text focused on these substantive results and place test taxonomy and power derivations in a technical appendix.

</details>


### [66] [Human-Centered Explainable AI for Security Enhancement: A Deep Intrusion Detection Framework](https://arxiv.org/abs/2602.13271)
*Md Muntasir Jahid Ayan,Md. Shahriar Rashid,Tazzina Afroze Hassan,Hossain Md. Mubashshir Jamil,Mahbubul Islam,Lisan Al Amin,Rupak Kumar Das,Farzana Akter,Faisal Quader*

Main category: cs.AI

TL;DR: 提出结合可解释人工智能(XAI)的入侵检测系统框架，使用CNN和LSTM网络处理流量序列，并通过SHAP提供模型解释，在NSL-KDD数据集上达到0.99准确率。


<details>
  <summary>Details</summary>
Motivation: 网络威胁日益复杂，需要既准确又可解释的入侵检测系统。传统IDS和黑盒深度学习模型缺乏透明度，安全分析师难以理解和验证模型决策。

Method: 提出集成XAI的IDS框架：1) 结合CNN和LSTM网络捕获流量序列的时间依赖性；2) 使用SHAP提供模型解释；3) 基于IPIP6和大五人格特质进行信任导向的专家调查评估系统可靠性。

Result: 在NSL-KDD数据集上，CNN和LSTM都达到0.99准确率，LSTM在宏观平均精度、召回率和F1分数上表现更优。SHAP识别出srv_serror_rate、dst_host_srv_serror_rate和serror_rate等关键特征。

Conclusion: 该工作展示了性能和透明度结合在网络安全解决方案中的潜力，建议通过自适应学习实现实时威胁检测作为未来改进方向。

Abstract: The increasing complexity and frequency of cyber-threats demand intrusion detection systems (IDS) that are not only accurate but also interpretable. This paper presented a novel IDS framework that integrated Explainable Artificial Intelligence (XAI) to enhance transparency in deep learning models. The framework was evaluated experimentally using the benchmark dataset NSL-KDD, demonstrating superior performance compared to traditional IDS and black-box deep learning models. The proposed approach combined Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) networks for capturing temporal dependencies in traffic sequences. Our deep learning results showed that both CNN and LSTM reached 0.99 for accuracy, whereas LSTM outperformed CNN at macro average precision, recall, and F-1 score. For weighted average precision, recall, and F-1 score, both models scored almost similarly. To ensure interpretability, the XAI model SHapley Additive exPlanations (SHAP) was incorporated, enabling security analysts to understand and validate model decisions. Some notable influential features were srv_serror_rate, dst_host_srv_serror_rate, and serror_rate for both models, as pointed out by SHAP. We also conducted a trust-focused expert survey based on IPIP6 and Big Five personality traits via an interactive UI to evaluate the system's reliability and usability. This work highlighted the potential of combining performance and transparency in cybersecurity solutions and recommends future enhancements through adaptive learning for real-time threat detection.

</details>


### [67] [TemporalBench: A Benchmark for Evaluating LLM-Based Agents on Contextual and Event-Informed Time Series Tasks](https://arxiv.org/abs/2602.13272)
*Muyan Weng,Defu Cao,Wei Yang,Yashaswi Sharma,Yan Liu*

Main category: cs.AI

TL;DR: TemporalBench是一个多领域基准测试，用于评估模型在不同信息设置下的时间推理能力，揭示仅靠预测准确性无法反映真实的时间理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前不清楚强大的预测性能是反映真正的时间理解能力，还是仅仅是在上下文和事件驱动条件下的推理能力。需要一个新的基准来诊断模型是否能够正确解释时间模式、对齐外部上下文并在条件变化时调整预测。

Method: 引入TemporalBench，采用四层任务分类法：历史结构解释、无上下文预测、上下文时间推理和事件条件预测，覆盖零售、医疗、能源和物理系统四个现实领域。通过控制对未来目标和上下文信息的访问，进行诊断分析。

Result: 广泛的基线实验表明，强大的数值预测准确性并不能可靠地转化为稳健的上下文或事件感知时间推理；现有的智能体框架表现出分散的优势和系统性失败模式，这些在仅关注预测的基准测试中基本被隐藏。

Conclusion: TemporalBench提供了一个全面的评估框架，揭示了当前时间推理模型的局限性，并公开了数据集和排行榜以促进该领域的研究。

Abstract: It is unclear whether strong forecasting performance reflects genuine temporal understanding or the ability to reason under contextual and event-driven conditions. We introduce TemporalBench, a multi-domain benchmark designed to evaluate temporal reasoning behavior under progressively richer informational settings. TemporalBench adopts a four-tier task taxonomy that examines historical structure interpretation, context-free forecasting, contextual temporal reasoning, and event-conditioned prediction across four real-world domains: retail, healthcare, energy, and physical systems. By controlling access to future targets and contextual information, the benchmark enables a diagnostic analysis of whether models can correctly interpret temporal patterns, align them with external context, and adapt predictions when conditions change. Extensive baseline experiments show that strong numerical forecasting accuracy does not reliably translate into robust contextual or event-aware temporal reasoning; instead, existing agent frameworks exhibit fragmented strengths and systematic failure modes that remain largely hidden under forecasting-only benchmarks. The TemporalBench dataset is publicly available at https://huggingface.co/datasets/Melady/TemporalBench, and we additionally provide a public leaderboard at https://huggingface.co/spaces/Melady/TemporalBench_Leaderboard.

</details>


### [68] [ForesightSafety Bench: A Frontier Risk Evaluation and Governance Framework towards Safe AI](https://arxiv.org/abs/2602.14135)
*Haibo Tong,Feifei Zhao,Linghao Feng,Ruoyu Wu,Ruolin Chen,Lu Jia,Zhou Zhao,Jindong Li,Tenglong Li,Erliang Lin,Shuai Yang,Enmeng Lu,Yinqian Sun,Qian Zhang,Zizhe Ruan,Zeyang Yue,Ping Wu,Huangrui Li,Chengyi Sun,Yi Zeng*

Main category: cs.AI

TL;DR: 提出了ForesightSafety Bench AI安全评估框架，包含7大基础安全支柱和94个细化风险维度，评估了20多个主流大模型，发现前沿AI存在广泛安全漏洞


<details>
  <summary>Details</summary>
Motivation: AI快速发展展现出越来越强的自主性和目标导向能力，伴随的系统性风险更加不可预测、难以控制且可能不可逆转。现有AI安全评估系统存在风险维度受限、前沿风险检测失败等关键限制，滞后的安全基准和对齐技术难以应对前沿AI模型的复杂挑战

Method: 提出ForesightSafety Bench AI安全评估框架，从7大基础安全支柱开始，逐步扩展到高级具身AI安全、AI4Science安全、社会与环境AI风险、灾难性和存在性风险，以及8个关键工业安全领域，形成94个细化风险维度。框架已积累数万个结构化风险数据和评估结果

Result: 对20多个主流先进大模型进行系统评估和深入分析，发现前沿AI在多个支柱上存在广泛安全漏洞，特别是在风险自主代理、AI4Science安全、具身AI安全、社会AI安全以及灾难性和存在性风险方面

Conclusion: 建立了一个广泛涵盖、层次清晰、动态演进的AI安全评估框架，能够识别关键风险模式及其能力边界，为应对前沿AI的复杂安全挑战提供了系统化的评估工具

Abstract: Rapidly evolving AI exhibits increasingly strong autonomy and goal-directed capabilities, accompanied by derivative systemic risks that are more unpredictable, difficult to control, and potentially irreversible. However, current AI safety evaluation systems suffer from critical limitations such as restricted risk dimensions and failed frontier risk detection. The lagging safety benchmarks and alignment technologies can hardly address the complex challenges posed by cutting-edge AI models. To bridge this gap, we propose the "ForesightSafety Bench" AI Safety Evaluation Framework, beginning with 7 major Fundamental Safety pillars and progressively extends to advanced Embodied AI Safety, AI4Science Safety, Social and Environmental AI risks, Catastrophic and Existential Risks, as well as 8 critical industrial safety domains, forming a total of 94 refined risk dimensions. To date, the benchmark has accumulated tens of thousands of structured risk data points and assessment results, establishing a widely encompassing, hierarchically clear, and dynamically evolving AI safety evaluation framework. Based on this benchmark, we conduct systematic evaluation and in-depth analysis of over twenty mainstream advanced large models, identifying key risk patterns and their capability boundaries. The safety capability evaluation results reveals the widespread safety vulnerabilities of frontier AI across multiple pillars, particularly focusing on Risky Agentic Autonomy, AI4Science Safety, Embodied AI Safety, Social AI Safety and Catastrophic and Existential Risks. Our benchmark is released at https://github.com/Beijing-AISI/ForesightSafety-Bench. The project website is available at https://foresightsafety-bench.beijing-aisi.ac.cn/.

</details>


### [69] [ProMoral-Bench: Evaluating Prompting Strategies for Moral Reasoning and Safety in LLMs](https://arxiv.org/abs/2602.13274)
*Rohan Subramanian Thomas,Shikhar Shiromani,Abdullah Chaudhry,Ruizhe Li,Vasu Sharma,Kevin Zhu,Sunishchal Dev*

Main category: cs.AI

TL;DR: ProMoral-Bench是一个统一的基准测试，评估11种提示范式在4个LLM家族上的表现，通过UMSS指标平衡准确性和安全性，发现简洁的示例引导提示优于复杂多阶段推理。


<details>
  <summary>Details</summary>
Motivation: 提示设计对LLM的道德能力和安全对齐有显著影响，但现有研究在数据集和模型上的比较较为零散，缺乏统一评估框架。

Method: 提出ProMoral-Bench基准，使用ETHICS、Scruples、WildJailbreak和新的鲁棒性测试ETHICS-Contrast数据集，通过统一道德安全分数（UMSS）评估11种提示范式在4个LLM家族上的表现。

Result: 紧凑的示例引导支架优于复杂的多阶段推理，提供更高的UMSS分数和更好的鲁棒性，且token成本更低。多轮推理在扰动下表现脆弱，而few-shot示例能持续增强道德稳定性和越狱抵抗能力。

Conclusion: ProMoral-Bench为原则性、成本效益高的提示工程建立了标准化框架，证明简洁的示例引导提示在道德安全评估中表现最佳。

Abstract: Prompt design significantly impacts the moral competence and safety alignment of large language models (LLMs), yet empirical comparisons remain fragmented across datasets and models.We introduce ProMoral-Bench, a unified benchmark evaluating 11 prompting paradigms across four LLM families. Using ETHICS, Scruples, WildJailbreak, and our new robustness test, ETHICS-Contrast, we measure performance via our proposed Unified Moral Safety Score (UMSS), a metric balancing accuracy and safety. Our results show that compact, exemplar-guided scaffolds outperform complex multi-stage reasoning, providing higher UMSS scores and greater robustness at a lower token cost. While multi-turn reasoning proves fragile under perturbations, few-shot exemplars consistently enhance moral stability and jailbreak resistance. ProMoral-Bench establishes a standardized framework for principled, cost-effective prompt engineering.

</details>


### [70] [Artificial Organisations](https://arxiv.org/abs/2602.13275)
*William Waites*

Main category: cs.AI

TL;DR: 论文提出通过组织架构设计而非个体对齐来实现多智能体系统的可靠性，借鉴人类机构的组织结构和信息隔离机制，并通过Perseverance Composition Engine系统验证了这一方法。


<details>
  <summary>Details</summary>
Motivation: 当前对齐研究主要关注单个AI系统的可靠性，但人类机构通过组织结构而非个体对齐来实现可靠的集体行为。多智能体AI系统应该借鉴这种机构模型，通过架构设计而非假设个体对齐来实现可靠结果。

Method: 提出Perseverance Composition Engine多智能体系统，包含三个角色：Composer起草文本，Corroborator验证事实依据（有完整源访问权限），Critic评估论证质量（无源访问权限）。通过系统架构强制执行信息不对称，实现分层验证。

Result: 在474个文档组合任务中观察到与机构假设一致的模式。当分配需要虚构内容的不可能任务时，系统从尝试虚构转向诚实拒绝并提出替代方案，这种行为既未指令也未个体激励。结果表明架构强制可能从不可靠的组件产生可靠结果。

Conclusion: 组织理论为多智能体AI安全提供了富有成效的框架。通过将验证和评估作为通过信息隔离强制执行的结构属性，机构设计为从不可靠的个体组件实现可靠的集体行为提供了一条途径。

Abstract: Alignment research focuses on making individual AI systems reliable. Human institutions achieve reliable collective behaviour differently: they mitigate the risk posed by misaligned individuals through organisational structure. Multi-agent AI systems should follow this institutional model using compartmentalisation and adversarial review to achieve reliable outcomes through architectural design rather than assuming individual alignment.
  We demonstrate this approach through the Perseverance Composition Engine, a multi-agent system for document composition. The Composer drafts text, the Corroborator verifies factual substantiation with full source access, and the Critic evaluates argumentative quality without access to sources: information asymmetry enforced by system architecture. This creates layered verification: the Corroborator detects unsupported claims, whilst the Critic independently assesses coherence and completeness. Observations from 474 composition tasks (discrete cycles of drafting, verification, and evaluation) exhibit patterns consistent with the institutional hypothesis. When assigned impossible tasks requiring fabricated content, this iteration enabled progression from attempted fabrication toward honest refusal with alternative proposals--behaviour neither instructed nor individually incentivised. These findings motivate controlled investigation of whether architectural enforcement produces reliable outcomes from unreliable components.
  This positions organisational theory as a productive framework for multi-agent AI safety. By implementing verification and evaluation as structural properties enforced through information compartmentalisation, institutional design offers a route to reliable collective behaviour from unreliable individual components.

</details>


### [71] [Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report v1.5](https://arxiv.org/abs/2602.14457)
*Dongrui Liu,Yi Yu,Jie Zhang,Guanxu Chen,Qihao Lin,Hanxi Zhu,Lige Huang,Yijin Zhou,Peng Wang,Shuai Shao,Boxuan Zhang,Zicheng Liu,Jingwei Sun,Yu Li,Yuejin Xie,Jiaxuan Guo,Jia Xu,Chaochao Lu,Bowen Zhou,Xia Hu,Jing Shao*

Main category: cs.AI

TL;DR: 该论文提出了前沿AI风险管理框架，评估了大型语言模型和智能体AI在五个关键维度的风险：网络攻击、说服操纵、战略欺骗、失控AI研发和自我复制，并提出了缓解策略。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型能力快速演进和智能体AI的普及，需要全面评估前沿AI带来的前所未有的风险，为安全部署提供技术路径。

Method: 采用更新的风险评估框架，针对五个关键维度设计更复杂的场景：网络攻击的复杂情境、LLM对LLM的说服测试、新兴错位的战略欺骗实验、智能体"错误进化"的失控研发评估、资源受限的自我复制场景，并监控OpenClaw在Moltbook上的安全表现。

Result: 提出了针对前沿AI风险的一系列稳健缓解策略，为安全部署提供了初步的技术和可操作路径，反映了当前对AI前沿风险的理解。

Conclusion: 需要集体行动来缓解这些新兴威胁，该工作为前沿AI的安全部署提供了风险管理框架和实践指导。

Abstract: To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, Frontier AI Risk Management Framework in Practice presents a comprehensive assessment of their frontier risks. As Large Language Models (LLMs) general capabilities rapidly evolve and the proliferation of agentic AI, this version of the risk analysis technical report presents an updated and granular assessment of five critical dimensions: cyber offense, persuasion and manipulation, strategic deception, uncontrolled AI R\&D, and self-replication. Specifically, we introduce more complex scenarios for cyber offense. For persuasion and manipulation, we evaluate the risk of LLM-to-LLM persuasion on newly released LLMs. For strategic deception and scheming, we add the new experiment with respect to emergent misalignment. For uncontrolled AI R\&D, we focus on the ``mis-evolution'' of agents as they autonomously expand their memory substrates and toolsets. Besides, we also monitor and evaluate the safety performance of OpenClaw during the interaction on the Moltbook. For self-replication, we introduce a new resource-constrained scenario. More importantly, we propose and validate a series of robust mitigation strategies to address these emerging threats, providing a preliminary technical and actionable pathway for the secure deployment of frontier AI. This work reflects our current understanding of AI frontier risks and urges collective action to mitigate these challenges.

</details>


### [72] [BEAGLE: Behavior-Enforced Agent for Grounded Learner Emulation](https://arxiv.org/abs/2602.13280)
*Hanchen David Wang,Clayton Cohn,Zifan Xu,Siyuan Guo,Gautam Biswas,Meiyi Ma*

Main category: cs.AI

TL;DR: BEAGLE是一个神经符号框架，通过整合自我调节学习理论来模拟学生在开放式问题解决环境中的真实学习行为，解决了LLMs在模拟学生时存在的"能力偏见"问题。


<details>
  <summary>Details</summary>
Motivation: 模拟学生在开放式问题解决环境中的学习行为对教育研究很重要，但收集真实数据面临隐私问题和纵向研究的高成本。虽然LLMs可以模拟学生，但它们存在"能力偏见"——倾向于优化效率而非模拟新手学习者典型的反复、迭代的挣扎过程。

Method: BEAGLE整合了三个关键技术创新：1) 半马尔可夫模型控制认知和元认知行为的时间与转换；2) 带有显式缺陷注入的贝叶斯知识追踪，强制执行现实的知识差距和"未知的未知"；3) 解耦的智能体设计，将高级策略使用与代码生成动作分离，防止模型静默修正自己的故意错误。

Result: 在Python编程任务评估中，BEAGLE在重现真实轨迹方面显著优于最先进的基线方法。在人类图灵测试中，用户无法区分合成轨迹和真实学生数据，准确率与随机猜测无异(52.8%)。

Conclusion: BEAGLE通过整合自我调节学习理论，成功解决了LLMs在模拟学生行为时的能力偏见问题，能够生成难以与真实学生数据区分的合成学习轨迹，为教育研究提供了有效的模拟工具。

Abstract: Simulating student learning behaviors in open-ended problem-solving environments holds potential for education research, from training adaptive tutoring systems to stress-testing pedagogical interventions. However, collecting authentic data is challenging due to privacy concerns and the high cost of longitudinal studies. While Large Language Models (LLMs) offer a promising path to student simulation, they suffer from competency bias, optimizing for efficient correctness rather than the erratic, iterative struggle characteristic of novice learners. We present BEAGLE, a neuro-symbolic framework that addresses this bias by incorporating Self-Regulated Learning (SRL) theory into a novel architecture. BEAGLE integrates three key technical innovations: (1) a semi-Markov model that governs the timing and transitions of cognitive behaviors and metacognitive behaviors; (2) Bayesian Knowledge Tracing with explicit flaw injection to enforce realistic knowledge gaps and "unknown unknowns"; and (3) a decoupled agent design that separates high-level strategy use from code generation actions to prevent the model from silently correcting its own intentional errors. In evaluations on Python programming tasks, BEAGLE significantly outperforms state-of-the-art baselines in reproducing authentic trajectories. In a human Turing test, users were unable to distinguish synthetic traces from real student data, achieving an accuracy indistinguishable from random guessing (52.8%).

</details>


### [73] [AI Arms and Influence: Frontier Models Exhibit Sophisticated Reasoning in Simulated Nuclear Crises](https://arxiv.org/abs/2602.14740)
*Kenneth Payne*

Main category: cs.AI

TL;DR: 前沿AI模型在核危机模拟中展现出复杂战略行为，包括欺骗、心智理论和元认知能力，但与传统战略理论存在显著差异，核禁忌无法阻止核升级，威胁常引发对抗而非服从。


<details>
  <summary>Details</summary>
Motivation: 研究前沿大语言模型在战略竞争中的行为模式，特别是核危机情境下的决策逻辑，为国家安全专业提供直接应用，同时探索AI在不确定性下的推理机制及其对战略理论的影响。

Method: 使用三种前沿大语言模型（GPT-5.2、Claude Sonnet 4、Gemini 3 Flash）在核危机模拟中扮演对立领导人角色，观察其在战略竞争中的自发行为模式和决策逻辑。

Result: AI模型展现出欺骗、心智理论和元认知等复杂战略行为，但与传统战略理论存在关键差异：核禁忌无法阻止核升级、战略核攻击确实发生、威胁更多引发对抗而非服从、高可信度反而加速冲突、模型从不选择妥协或撤退。

Conclusion: AI模拟是强大的战略分析工具，但必须基于人类推理模式进行校准。理解前沿模型如何模仿或不模仿人类战略逻辑，对于AI日益影响战略结果的世界至关重要。

Abstract: Today's leading AI models engage in sophisticated behaviour when placed in strategic competition. They spontaneously attempt deception, signaling intentions they do not intend to follow; they demonstrate rich theory of mind, reasoning about adversary beliefs and anticipating their actions; and they exhibit credible metacognitive self-awareness, assessing their own strategic abilities before deciding how to act.
  Here we present findings from a crisis simulation in which three frontier large language models (GPT-5.2, Claude Sonnet 4, Gemini 3 Flash) play opposing leaders in a nuclear crisis. Our simulation has direct application for national security professionals, but also, via its insights into AI reasoning under uncertainty, has applications far beyond international crisis decision-making.
  Our findings both validate and challenge central tenets of strategic theory. We find support for Schelling's ideas about commitment, Kahn's escalation framework, and Jervis's work on misperception, inter alia. Yet we also find that the nuclear taboo is no impediment to nuclear escalation by our models; that strategic nuclear attack, while rare, does occur; that threats more often provoke counter-escalation than compliance; that high mutual credibility accelerated rather than deterred conflict; and that no model ever chose accommodation or withdrawal even when under acute pressure, only reduced levels of violence.
  We argue that AI simulation represents a powerful tool for strategic analysis, but only if properly calibrated against known patterns of human reasoning. Understanding how frontier models do and do not imitate human strategic logic is essential preparation for a world in which AI increasingly shapes strategic outcomes.

</details>


### [74] [Mirror: A Multi-Agent System for AI-Assisted Ethics Review](https://arxiv.org/abs/2602.13292)
*Yifan Ding,Yuhui Shi,Zhiyan Li,Zilong Wang,Yifeng Gao,Yajun Yang,Mengjie Yang,Yixiu Liang,Xipeng Qiu,Xuanjing Huang,Xingjun Ma,Yu-Gang Jiang,Guoyu Wang*

Main category: cs.AI

TL;DR: Mirror是一个AI辅助伦理审查框架，通过EthicsLLM模型和两种审查模式（快速审查和委员会审查）提升伦理审查的质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 现代研究治理中的伦理审查系统面临压力，大规模跨学科科学实践带来的结构性伦理风险暴露了机构审查能力的局限性。虽然大语言模型提供了新机会，但直接应用受到伦理推理能力不足、监管结构整合弱、真实审查材料隐私限制等问题的制约。

Method: 提出了Mirror框架，核心是EthicsLLM模型，基于EthicsQA数据集（41K个问题-思维链-答案三元组）微调。框架包含两种模式：Mirror-ER通过可执行规则库自动化快速审查；Mirror-CR通过专家代理、伦理秘书代理和主要研究者代理的协调互动模拟完整委员会审议。

Result: 实证评估表明，Mirror相比强大的通用大语言模型，显著提高了伦理评估的质量、一致性和专业性。

Conclusion: Mirror框架通过整合伦理推理、结构化规则解释和多代理审议，为AI辅助伦理审查提供了有效的解决方案，能够应对现代研究治理中伦理审查面临的挑战。

Abstract: Ethics review is a foundational mechanism of modern research governance, yet contemporary systems face increasing strain as ethical risks arise as structural consequences of large-scale, interdisciplinary scientific practice. The demand for consistent and defensible decisions under heterogeneous risk profiles exposes limitations in institutional review capacity rather than in the legitimacy of ethics oversight. Recent advances in large language models (LLMs) offer new opportunities to support ethics review, but their direct application remains limited by insufficient ethical reasoning capability, weak integration with regulatory structures, and strict privacy constraints on authentic review materials. In this work, we introduce Mirror, an agentic framework for AI-assisted ethical review that integrates ethical reasoning, structured rule interpretation, and multi-agent deliberation within a unified architecture. At its core is EthicsLLM, a foundational model fine-tuned on EthicsQA, a specialized dataset of 41K question-chain-of-thought-answer triples distilled from authoritative ethics and regulatory corpora. EthicsLLM provides detailed normative and regulatory understanding, enabling Mirror to operate in two complementary modes. Mirror-ER (expedited Review) automates expedited review through an executable rule base that supports efficient and transparent compliance checks for minimal-risk studies. Mirror-CR (Committee Review) simulates full-board deliberation through coordinated interactions among expert agents, an ethics secretary agent, and a principal investigator agent, producing structured, committee-level assessments across ten ethical dimensions. Empirical evaluations demonstrate that Mirror significantly improves the quality, consistency, and professionalism of ethics assessments compared with strong generalist LLMs.

</details>


### [75] [DECKBench: Benchmarking Multi-Agent Frameworks for Academic Slide Generation and Editing](https://arxiv.org/abs/2602.13318)
*Daesik Jang,Morgan Lindsay Heisler,Linzi Xing,Yifei Li,Edward Wang,Ying Xiong,Yong Zhang,Zhenan Fan*

Main category: cs.AI

TL;DR: DECKBench：一个用于评估多智能体幻灯片生成与编辑的基准框架，包含数据集、评估协议和基线系统


<details>
  <summary>Details</summary>
Motivation: 现有基准和评估协议无法充分衡量学术幻灯片自动生成和迭代编辑中的挑战，包括忠实内容选择、连贯组织、布局感知渲染和鲁棒的多轮指令跟随

Method: 构建DECKBench框架，包含基于论文-幻灯片对的数据集、模拟编辑指令、系统评估协议，以及一个模块化多智能体基线系统（包含论文解析、幻灯片规划、HTML创建和迭代编辑）

Result: 实验结果表明，该基准能够有效揭示系统优势、暴露失败模式，并为改进多智能体幻灯片生成和编辑系统提供可操作的见解

Conclusion: 这项工作为学术演示文稿生成和编辑的可重复和可比较评估建立了标准化基础，代码和数据已公开

Abstract: Automatically generating and iteratively editing academic slide decks requires more than document summarization. It demands faithful content selection, coherent slide organization, layout-aware rendering, and robust multi-turn instruction following. However, existing benchmarks and evaluation protocols do not adequately measure these challenges. To address this gap, we introduce the Deck Edits and Compliance Kit Benchmark (DECKBench), an evaluation framework for multi-agent slide generation and editing. DECKBench is built on a curated dataset of paper to slide pairs augmented with realistic, simulated editing instructions. Our evaluation protocol systematically assesses slide-level and deck-level fidelity, coherence, layout quality, and multi-turn instruction following. We further implement a modular multi-agent baseline system that decomposes the slide generation and editing task into paper parsing and summarization, slide planning, HTML creation, and iterative editing. Experimental results demonstrate that the proposed benchmark highlights strengths, exposes failure modes, and provides actionable insights for improving multi-agent slide generation and editing systems. Overall, this work establishes a standardized foundation for reproducible and comparable evaluation of academic presentation generation and editing. Code and data are publicly available at https://github.com/morgan-heisler/DeckBench .

</details>


### [76] [Situation Graph Prediction: Structured Perspective Inference for User Modeling](https://arxiv.org/abs/2602.13319)
*Jisung Shin,Daniel Platnick,Marjan Alirezaie,Hossein Rahnama*

Main category: cs.AI

TL;DR: 论文提出情境图预测任务，通过结构化合成数据解决视角建模的数据瓶颈问题，实验表明潜在状态推理比表面提取更具挑战性。


<details>
  <summary>Details</summary>
Motivation: 视角感知AI需要建模内部状态（目标、情感、上下文），而不仅仅是偏好。当前进展受限于数据瓶颈：数字足迹涉及隐私且视角状态很少被标注。

Method: 提出情境图预测任务，将视角建模构建为逆向推理问题。采用结构优先的合成生成策略，设计上对齐潜在标签和可观察痕迹。构建数据集并使用检索增强的上下文学习作为监督代理进行诊断研究。

Result: 在GPT-4o研究中观察到表面级提取与潜在视角推理之间存在差距，表明在受控设置下潜在状态推理比表面提取更难。结果证明SGP任务非平凡，并为结构优先数据合成策略提供了证据。

Conclusion: 情境图预测是一个有前景的视角建模框架，结构优先的合成数据生成方法能有效解决数据稀缺问题，为开发更复杂的视角感知AI系统奠定了基础。

Abstract: Perspective-Aware AI requires modeling evolving internal states--goals, emotions, contexts--not merely preferences. Progress is limited by a data bottleneck: digital footprints are privacy-sensitive and perspective states are rarely labeled. We propose Situation Graph Prediction (SGP), a task that frames perspective modeling as an inverse inference problem: reconstructing structured, ontology-aligned representations of perspective from observable multimodal artifacts. To enable grounding without real labels, we use a structure-first synthetic generation strategy that aligns latent labels and observable traces by design. As a pilot, we construct a dataset and run a diagnostic study using retrieval-augmented in-context learning as a proxy for supervision. In our study with GPT-4o, we observe a gap between surface-level extraction and latent perspective inference--indicating latent-state inference is harder than surface extraction under our controlled setting. Results suggest SGP is non-trivial and provide evidence for the structure-first data synthesis strategy.

</details>


### [77] [Information Fidelity in Tool-Using LLM Agents: A Martingale Analysis of the Model Context Protocol](https://arxiv.org/abs/2602.13320)
*Flint Xiaofeng Fan,Cheston Tan,Roger Wattenhofer,Yew-Soon Ong*

Main category: cs.AI

TL;DR: 本文提出了首个分析MCP智能体错误累积的理论框架，证明累积失真呈线性增长且高概率偏差有界，为可靠AI系统部署提供理论保证。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的AI智能体越来越多地使用外部工具进行高风险决策，一个关键可靠性问题出现：错误如何在连续工具调用中传播？需要理论框架来分析错误累积机制。

Method: 引入混合失真度量（结合离散事实匹配和连续语义相似度），建立序列工具交互中错误传播的马丁格尔集中界，并在Qwen2-7B、Llama-3-8B和Mistral-7B上进行实验验证。

Result: 理论证明累积失真呈线性增长，高概率偏差有O(√T)上界；实验验证经验失真跟踪线性趋势，语义加权减少80%失真，每约9步重新接地足以控制错误。

Conclusion: 该理论框架为可信智能体系统提供了可操作的部署原则，确保可预测的系统行为并排除指数级故障模式，为AI智能体可靠性提供理论保证。

Abstract: As AI agents powered by large language models (LLMs) increasingly use external tools for high-stakes decisions, a critical reliability question arises: how do errors propagate across sequential tool calls? We introduce the first theoretical framework for analyzing error accumulation in Model Context Protocol (MCP) agents, proving that cumulative distortion exhibits linear growth and high-probability deviations bounded by $O(\sqrt{T})$. This concentration property ensures predictable system behavior and rules out exponential failure modes. We develop a hybrid distortion metric combining discrete fact matching with continuous semantic similarity, then establish martingale concentration bounds on error propagation through sequential tool interactions. Experiments across Qwen2-7B, Llama-3-8B, and Mistral-7B validate our theoretical predictions, showing empirical distortion tracks the linear trend with deviations consistently within $O(\sqrt{T})$ envelopes. Key findings include: semantic weighting reduces distortion by 80\%, and periodic re-grounding approximately every 9 steps suffices for error control. We translate these concentration guarantees into actionable deployment principles for trustworthy agent systems.

</details>


### [78] [Detecting Jailbreak Attempts in Clinical Training LLMs Through Automated Linguistic Feature Extraction](https://arxiv.org/abs/2602.13321)
*Tri Nguyen,Huy Hoang Bao Le,Lohith Srikanth Pentapalli,Laurah Turner,Kelly Cohen*

Main category: cs.AI

TL;DR: 使用专家标注的四个核心语言特征，训练BERT模型进行特征提取，再结合多种分类器构建可扩展的临床LLM越狱检测系统


<details>
  <summary>Details</summary>
Motivation: 临床训练大语言模型中检测越狱尝试需要准确建模表示不安全或偏离任务用户行为的语言偏差。先前工作依赖手动标注特征，限制了可扩展性和表达能力。

Method: 使用专家标注的四个核心语言特征（专业性、医学相关性、伦理行为、上下文干扰），训练通用和医学领域BERT模型预测这些特征，选择最佳特征回归器作为特征提取器，再使用树基、线性、概率和集成方法构建第二层分类器预测越狱可能性。

Result: 系统在交叉验证和保留集评估中表现出色，表明LLM衍生的语言特征为自动化越狱检测提供了有效基础。错误分析揭示了当前标注和特征表示的关键限制。

Conclusion: 这项工作展示了一种可扩展且可解释的方法，用于检测安全关键临床对话系统中的越狱行为，为未来改进指明了方向，如更丰富的标注方案、更细粒度的特征提取以及捕捉对话过程中越狱行为演变风险的方法。

Abstract: Detecting jailbreak attempts in clinical training large language models (LLMs) requires accurate modeling of linguistic deviations that signal unsafe or off-task user behavior. Prior work on the 2-Sigma clinical simulation platform showed that manually annotated linguistic features could support jailbreak detection. However, reliance on manual annotation limited both scalability and expressiveness. In this study, we extend this framework by using experts' annotations of four core linguistic features (Professionalism, Medical Relevance, Ethical Behavior, and Contextual Distraction) and training multiple general-domain and medical-domain BERT-based LLM models to predict these features directly from text. The most reliable feature regressor for each dimension was selected and used as the feature extractor in a second layer of classifiers. We evaluate a suite of predictive models, including tree-based, linear, probabilistic, and ensemble methods, to determine jailbreak likelihood from the extracted features. Across cross-validation and held-out evaluations, the system achieves strong overall performance, indicating that LLM-derived linguistic features provide an effective basis for automated jailbreak detection. Error analysis further highlights key limitations in current annotations and feature representations, pointing toward future improvements such as richer annotation schemes, finer-grained feature extraction, and methods that capture the evolving risk of jailbreak behavior over the course of a dialogue. This work demonstrates a scalable and interpretable approach for detecting jailbreak behavior in safety-critical clinical dialogue systems.

</details>


### [79] [Contrastive explanations of BDI agents](https://arxiv.org/abs/2602.13323)
*Michael Winikoff*

Main category: cs.AI

TL;DR: 扩展BDI智能体解释能力以回答对比性问题（"为什么做X而不是F？"），实验显示对比性解释更简短，在某些情况下更受偏好且能提升信任，但提供完整解释有时反而不如不提供解释。


<details>
  <summary>Details</summary>
Motivation: 现有BDI智能体只能回答"为什么做X？"这类非对比性问题，但人类实际常问对比性问题（"为什么做X而不是F？"）。需要扩展解释能力以支持更自然的问答，提升透明度和信任。

Method: 扩展先前BDI智能体解释机制，使其能回答对比性问题。进行两方面评估：1) 计算评估对比性解释的长度；2) 人类被试评估，比较对比性与非对比性解释的偏好、信任发展、透明度和系统正确性信心。

Result: 计算评估显示对比性问题能显著减少解释长度。人类评估发现：1) 有证据表明对比性解释更受偏好；2) 对比性解释能带来更高信任、感知理解和系统正确性信心；3) 意外发现：在某些情况下提供完整解释反而不如不提供任何解释。

Conclusion: 扩展BDI智能体以回答对比性问题是有价值的，能产生更简短的、在某些情况下更有效的解释。但需谨慎使用解释，因为过度解释可能产生负面影响。未来研究需探索何时以及如何提供解释才能最大化效益。

Abstract: The ability of autonomous systems to provide explanations is important for supporting transparency and aiding the development of (appropriate) trust. Prior work has defined a mechanism for Belief-Desire-Intention (BDI) agents to be able to answer questions of the form ``why did you do action $X$?''. However, we know that we ask \emph{contrastive} questions (``why did you do $X$ \emph{instead of} $F$?''). We therefore extend previous work to be able to answer such questions. A computational evaluation shows that using contrastive questions yields a significant reduction in explanation length. A human subject evaluation was conducted to assess whether such contrastive answers are preferred, and how well they support trust development and transparency. We found some evidence for contrastive answers being preferred, and some evidence that they led to higher trust, perceived understanding, and confidence in the system's correctness. We also evaluated the benefit of providing explanations at all. Surprisingly, there was not a clear benefit, and in some situations we found evidence that providing a (full) explanation was worse than not providing any explanation.

</details>


### [80] [Nanbeige4.1-3B: A Small General Model that Reasons, Aligns, and Acts](https://arxiv.org/abs/2602.13367)
*Chen Yang,Guangyue Peng,Jiaying Zhu,Ran Le,Ruixiang Feng,Tao Zhang,Xiyun Xu,Yang Song,Yiming Jia,Yuntao Wen,Yunzhi Xu,Zekai Wang,Zhenwei An,Zhicong Sun,Zongchao Chen*

Main category: cs.AI

TL;DR: Nanbeige4.1-3B是一个仅3B参数的多功能语言模型，首次在开源小模型中同时实现强大代理行为、代码生成和通用推理能力，通过创新的奖励建模和强化学习技术，在多项任务上超越同类甚至更大规模模型。


<details>
  <summary>Details</summary>
Motivation: 当前小型语言模型通常只能在特定领域表现良好，难以同时具备多种能力。作者希望开发一个仅3B参数但能同时实现强大代理行为、代码生成和通用推理的统一模型，重新定义小模型的潜力。

Method: 1. 结合点对点和配对奖励建模来提升推理和偏好对齐；2. 为代码生成设计复杂度感知的强化学习奖励，优化正确性和效率；3. 通过复杂数据合成和轮次级监督训练，实现稳定的长时程工具交互（可达600轮）；4. 在单一模型中统一多种能力。

Result: Nanbeige4.1-3B显著超越同类规模模型（如Nanbeige4-3B-2511和Qwen3-4B），甚至在某些方面优于更大模型（如Qwen3-30B-A3B）。模型能可靠执行长达600轮的工具调用，解决复杂问题。

Conclusion: 该研究表明小模型可以同时实现广泛能力和强专业化，重新定义了3B参数模型的潜力，为开发多功能高效的小型语言模型提供了新思路。

Abstract: We present Nanbeige4.1-3B, a unified generalist language model that simultaneously achieves strong agentic behavior, code generation, and general reasoning with only 3B parameters. To the best of our knowledge, it is the first open-source small language model (SLM) to achieve such versatility in a single model. To improve reasoning and preference alignment, we combine point-wise and pair-wise reward modeling, ensuring high-quality, human-aligned responses. For code generation, we design complexity-aware rewards in Reinforcement Learning, optimizing both correctness and efficiency. In deep search, we perform complex data synthesis and incorporate turn-level supervision during training. This enables stable long-horizon tool interactions, allowing Nanbeige4.1-3B to reliably execute up to 600 tool-call turns for complex problem-solving. Extensive experimental results show that Nanbeige4.1-3B significantly outperforms prior models of similar scale, such as Nanbeige4-3B-2511 and Qwen3-4B, even achieving superior performance compared to much larger models, such as Qwen3-30B-A3B. Our results demonstrate that small models can achieve both broad competence and strong specialization simultaneously, redefining the potential of 3B parameter models.

</details>


### [81] [MoralityGym: A Benchmark for Evaluating Hierarchical Moral Alignment in Sequential Decision-Making Agents](https://arxiv.org/abs/2602.13372)
*Simon Rosen,Siddarth Singh,Ebenezer Gelo,Helen Sarah Robertson,Ibrahim Suder,Victoria Williams,Benjamin Rosman,Geraud Nangue Tasse,Steven James*

Main category: cs.AI

TL;DR: 提出Morality Chains形式化表示道德规范，建立MoralityGym基准测试环境，用于评估AI在冲突性道德困境中的决策能力


<details>
  <summary>Details</summary>
Motivation: 评估AI在冲突、层次化人类规范中的道德对齐是AI安全、道德哲学和认知科学交叉领域的关键挑战，需要更可靠的伦理决策方法

Method: 引入Morality Chains作为有序道义约束的道德规范表示形式，创建包含98个伦理困境问题的MoralityGym基准测试环境，将任务解决与道德评估解耦，并提出新的道德度量标准

Result: 使用安全强化学习方法进行基线测试，揭示了现有方法的局限性，表明需要更原则性的伦理决策方法

Conclusion: 该工作为开发在复杂现实世界中行为更可靠、透明和道德的AI系统奠定了基础

Abstract: Evaluating moral alignment in agents navigating conflicting, hierarchically structured human norms is a critical challenge at the intersection of AI safety, moral philosophy, and cognitive science. We introduce Morality Chains, a novel formalism for representing moral norms as ordered deontic constraints, and MoralityGym, a benchmark of 98 ethical-dilemma problems presented as trolley-dilemma-style Gymnasium environments. By decoupling task-solving from moral evaluation and introducing a novel Morality Metric, MoralityGym allows the integration of insights from psychology and philosophy into the evaluation of norm-sensitive reasoning. Baseline results with Safe RL methods reveal key limitations, underscoring the need for more principled approaches to ethical decision-making. This work provides a foundation for developing AI systems that behave more reliably, transparently, and ethically in complex real-world contexts.

</details>


### [82] [On-Policy Supervised Fine-Tuning for Efficient Reasoning](https://arxiv.org/abs/2602.13407)
*Anhao Zhao,Ziyang Chen,Junlong Tong,Yingqi Fan,Fanghua Ye,Shuhao Li,Yunpu Ma,Wenjie Li,Xiaoyu Shen*

Main category: cs.AI

TL;DR: 论文提出on-policy SFT方法，通过简化奖励设计（仅使用截断式长度惩罚）替代复杂的多奖励RL训练，在保持准确率的同时大幅缩短推理链长度，并显著提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型通常使用强化学习训练，但复杂的多奖励目标（同时优化正确性和简洁性）会导致训练不稳定和次优权衡。作者质疑这种复杂性的必要性，希望通过简化方法获得更好的效果。

Method: 提出on-policy SFT方法：1）移除KL正则化（在正确性和长度可直接验证时失去作用）；2）移除组归一化（在多奖励信号下变得模糊）；3）简化奖励为基于截断的长度惩罚。该方法本质上是在自生成数据上进行监督微调，同时筛选正确且简洁的样本。

Result: 在五个基准测试中，on-policy SFT方法：1）将推理链长度减少高达80%，同时保持原始准确率；2）定义了准确率-效率的帕累托前沿；3）超越更复杂的基于RL的方法；4）训练效率显著提升：GPU内存使用减少50%，收敛速度加快70%。

Conclusion: 通过简化训练目标，on-policy SFT方法在保持模型性能的同时大幅提升了推理效率和训练效率，证明了复杂多奖励RL训练并非必要，为大型推理模型的训练提供了更简单有效的替代方案。

Abstract: Large reasoning models (LRMs) are commonly trained with reinforcement learning (RL) to explore long chain-of-thought reasoning, achieving strong performance at high computational cost. Recent methods add multi-reward objectives to jointly optimize correctness and brevity, but these complex extensions often destabilize training and yield suboptimal trade-offs. We revisit this objective and challenge the necessity of such complexity. Through principled analysis, we identify fundamental misalignments in this paradigm: KL regularization loses its intended role when correctness and length are directly verifiable, and group-wise normalization becomes ambiguous under multiple reward signals. By removing these two items and simplifying the reward to a truncation-based length penalty, we show that the optimization problem reduces to supervised fine-tuning on self-generated data filtered for both correctness and conciseness. We term this simplified training strategy on-policy SFT. Despite its simplicity, on-policy SFT consistently defines the accuracy-efficiency Pareto frontier. It reduces CoT length by up to 80 while maintaining original accuracy, surpassing more complex RL-based methods across five benchmarks. Furthermore, it significantly enhances training efficiency, reducing GPU memory usage by 50% and accelerating convergence by 70%. Our code is available at https://github.com/EIT-NLP/On-Policy-SFT.

</details>


### [83] [NeuroWeaver: An Autonomous Evolutionary Agent for Exploring the Programmatic Space of EEG Analysis Pipelines](https://arxiv.org/abs/2602.13473)
*Guoan Wang,Shihao Yang,Jun-En Ding,Hao Zhu,Feng Liu*

Main category: cs.AI

TL;DR: NeuroWeaver是一个用于EEG分析的自主进化代理，通过约束优化和神经科学先验知识，生成轻量级高性能解决方案，在资源受限环境中超越传统方法并接近大型基础模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在EEG分析中需要大量数据和参数，计算成本高，不适合资源受限的临床环境；而通用自动机器学习框架缺乏神经科学先验知识，常产生不科学的解决方案。

Method: 将流水线工程重新定义为离散约束优化问题，采用领域知情子空间初始化将搜索限制在神经科学合理的流形上，结合多目标进化优化动态平衡性能、新颖性和效率，通过自反思进行精炼。

Result: 在五个异构基准测试中，NeuroWeaver合成的轻量级解决方案持续超越最先进的特定任务方法，性能接近大规模基础模型，但使用的参数显著减少。

Conclusion: NeuroWeaver通过结合领域知识和进化优化，为资源受限的临床环境提供了高效、科学合理的EEG分析解决方案，在保持高性能的同时大幅降低计算需求。

Abstract: Although foundation models have demonstrated remarkable success in general domains, the application of these models to electroencephalography (EEG) analysis is constrained by substantial data requirements and high parameterization. These factors incur prohibitive computational costs, thereby impeding deployment in resource-constrained clinical environments. Conversely, general-purpose automated machine learning frameworks are often ill-suited for this domain, as exploration within an unbounded programmatic space fails to incorporate essential neurophysiological priors and frequently yields solutions that lack scientific plausibility. To address these limitations, we propose NeuroWeaver, a unified autonomous evolutionary agent designed to generalize across diverse EEG datasets and tasks by reformulating pipeline engineering as a discrete constrained optimization problem. Specifically, we employ a Domain-Informed Subspace Initialization to confine the search to neuroscientifically plausible manifolds, coupled with a Multi-Objective Evolutionary Optimization that dynamically balances performance, novelty, and efficiency via self-reflective refinement. Empirical evaluations across five heterogeneous benchmarks demonstrate that NeuroWeaver synthesizes lightweight solutions that consistently outperform state-of-the-art task-specific methods and achieve performance comparable to large-scale foundation models, despite utilizing significantly fewer parameters.

</details>


### [84] [OMNI-LEAK: Orchestrator Multi-Agent Network Induced Data Leakage](https://arxiv.org/abs/2602.13477)
*Akshat Naik,Jay Culligan,Yarin Gal,Philip Torr,Rahaf Aljundi,Alasdair Paren,Adel Bibi*

Main category: cs.AI

TL;DR: 论文研究了多智能体系统中编排器架构的安全漏洞，发现即使存在数据访问控制，攻击者仍能通过间接提示注入泄露敏感数据


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体能力增强，多智能体系统将成为实用范式。现有安全研究多集中于单智能体场景，缺乏对多智能体系统的威胁建模，特别是在存在基本工程防护措施（如访问控制）的情况下

Method: 通过红队测试一个代表性的编排器架构（中心智能体分解和委派任务给专业智能体），演示了OMNI-LEAK攻击向量，该攻击通过单个间接提示注入泄露敏感数据

Result: 前沿模型对不同类别攻击都易受攻击，无论是推理模型还是非推理模型，即使攻击者不了解实现细节。攻击能在存在数据访问控制的情况下泄露敏感数据

Conclusion: 安全研究需要从单智能体扩展到多智能体设置，以减少现实世界隐私泄露和财务损失的风险，维护公众对AI智能体的信任

Abstract: As Large Language Model (LLM) agents become more capable, their coordinated use in the form of multi-agent systems is anticipated to emerge as a practical paradigm. Prior work has examined the safety and misuse risks associated with agents. However, much of this has focused on the single-agent case and/or setups missing basic engineering safeguards such as access control, revealing a scarcity of threat modeling in multi-agent systems. We investigate the security vulnerabilities of a popular multi-agent pattern known as the orchestrator setup, in which a central agent decomposes and delegates tasks to specialized agents. Through red-teaming a concrete setup representative of a likely future use case, we demonstrate a novel attack vector, OMNI-LEAK, that compromises several agents to leak sensitive data through a single indirect prompt injection, even in the \textit{presence of data access control}. We report the susceptibility of frontier models to different categories of attacks, finding that both reasoning and non-reasoning models are vulnerable, even when the attacker lacks insider knowledge of the implementation details. Our work highlights the importance of safety research to generalize from single-agent to multi-agent settings, in order to reduce the serious risks of real-world privacy breaches and financial losses and overall public trust in AI agents.

</details>


### [85] [Translating Dietary Standards into Healthy Meals with Minimal Substitutions](https://arxiv.org/abs/2602.13502)
*Trevor Chan,Ilias Tagkopoulos*

Main category: cs.AI

TL;DR: 提出一个端到端框架，将饮食标准转化为完整餐食，通过识别34种可解释的餐食原型，使用生成模型和分量预测器来满足USDA营养目标，实现营养改善47.0%，成本降低19-32%。


<details>
  <summary>Details</summary>
Motivation: 个性化饮食系统的重要目标是在不牺牲便利性或可负担性的情况下改善营养质量。需要将饮食指南转化为现实、预算友好的餐食和简单替换方案。

Method: 使用WWEIA摄入数据（135,491餐）识别34种可解释的餐食原型，然后使用这些原型来条件化生成模型和分量预测器，以满足USDA营养目标。通过允许1-3种食物替换来创建更营养的餐食。

Result: 在原型内比较中，生成的餐食在遵循推荐每日摄入目标方面提高了47.0%，同时保持与真实餐食的组成接近。通过1-3种食物替换，餐食营养提高10%，成本平均降低19-32%。

Conclusion: 该框架可将饮食指南转化为现实、预算友好的餐食和简单替换，为临床决策支持、公共卫生计划和消费者应用提供基础，实现可扩展、公平的日常营养改善。

Abstract: An important goal for personalized diet systems is to improve nutritional quality without compromising convenience or affordability. We present an end-to-end framework that converts dietary standards into complete meals with minimal change. Using the What We Eat in America (WWEIA) intake data for 135,491 meals, we identify 34 interpretable meal archetypes that we then use to condition a generative model and a portion predictor to meet USDA nutritional targets. In comparisons within archetypes, generated meals are better at following recommended daily intake (RDI) targets by 47.0%, while remaining compositionally close to real meals. Our results show that by allowing one to three food substitutions, we were able to create meals that were 10% more nutritious, while reducing costs 19-32%, on average. By turning dietary guidelines into realistic, budget-aware meals and simple swaps, this framework can underpin clinical decision support, public-health programs, and consumer apps that deliver scalable, equitable improvements in everyday nutrition.

</details>


### [86] [SPILLage: Agentic Oversharing on the Web](https://arxiv.org/abs/2602.13516)
*Jaechul Roh,Eugene Bagdasarian,Hamed Haddadi,Ali Shahin Shamsabadi*

Main category: cs.AI

TL;DR: 论文提出"自然代理过度分享"概念，指网络代理在完成任务时无意泄露与任务无关的用户信息，并开发SPILLage框架从内容和行为两个维度分析此问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLM驱动的代理开始在开放网络上自动化用户任务，这些代理会与第三方互动并留下行动痕迹。不同于受控聊天机器人，网络代理在"野外"操作，可能泄露用户资源信息，但现有研究主要关注文本泄露，忽视了行为层面的信息泄露风险。

Method: 提出SPILLage框架，从渠道（内容vs行为）和直接性（显式vs隐式）两个维度分析过度分享。在真实电商网站上基准测试180个任务，标注任务相关与无关属性，涵盖2个代理框架和3个骨干LLM的1,080次运行。

Result: 过度分享普遍存在，行为过度分享比内容过度分享多5倍。提示级缓解措施效果有限甚至可能恶化问题，但执行前移除任务无关信息可将任务成功率提高达17.9%。

Conclusion: 保护网络代理隐私是根本挑战，需要更广泛的"输出"视角，不仅要考虑代理输入的内容，还要考虑其在网络上的行为。减少过度分享能提高任务成功率。

Abstract: LLM-powered agents are beginning to automate user's tasks across the open web, often with access to user resources such as emails and calendars. Unlike standard LLMs answering questions in a controlled ChatBot setting, web agents act "in the wild", interacting with third parties and leaving behind an action trace. Therefore, we ask the question: how do web agents handle user resources when accomplishing tasks on their behalf across live websites? In this paper, we formalize Natural Agentic Oversharing -- the unintentional disclosure of task-irrelevant user information through an agent trace of actions on the web. We introduce SPILLage, a framework that characterizes oversharing along two dimensions: channel (content vs. behavior) and directness (explicit vs. implicit). This taxonomy reveals a critical blind spot: while prior work focuses on text leakage, web agents also overshare behaviorally through clicks, scrolls, and navigation patterns that can be monitored. We benchmark 180 tasks on live e-commerce sites with ground-truth annotations separating task-relevant from task-irrelevant attributes. Across 1,080 runs spanning two agentic frameworks and three backbone LLMs, we demonstrate that oversharing is pervasive with behavioral oversharing dominates content oversharing by 5x. This effect persists -- and can even worsen -- under prompt-level mitigation. However, removing task-irrelevant information before execution improves task success by up to 17.9%, demonstrating that reducing oversharing improves task success. Our findings underscore that protecting privacy in web agents is a fundamental challenge, requiring a broader view of "output" that accounts for what agents do on the web, not just what they type. Our datasets and code are available at https://github.com/jrohsc/SPILLage.

</details>


### [87] [REMem: Reasoning with Episodic Memory in Language Agent](https://arxiv.org/abs/2602.13530)
*Yiheng Shu,Saisri Padmaja Jonnalagedda,Xiang Gao,Bernal Jiménez Gutiérrez,Weijian Qi,Kamalika Das,Huan Sun,Yu Su*

Main category: cs.AI

TL;DR: REMem是一个两阶段框架，用于构建和推理情景记忆，通过混合记忆图和智能检索器显著提升语言代理的情景回忆和推理能力


<details>
  <summary>Details</summary>
Motivation: 人类擅长在时空背景下记忆具体经历并进行跨事件推理，而当前语言代理的记忆主要是语义的，无法有效回忆和推理交互历史。现有工作往往忽视情景性、缺乏明确的事件建模，或过度强调简单检索而非复杂推理

Method: REMem采用两阶段框架：1) 离线索引：将经验转换为混合记忆图，灵活链接时间感知的要点和事实；2) 在线推理：使用智能检索器，配备精心设计的工具在记忆图上进行迭代检索

Result: 在四个情景记忆基准测试中，REMem显著优于Mem0和HippoRAG 2等最先进记忆系统，在情景回忆和推理任务上分别获得3.4%和13.4%的绝对提升，并对不可回答问题表现出更稳健的拒绝行为

Conclusion: REMem通过混合记忆图和智能检索机制有效解决了语言代理的情景记忆挑战，为构建具有人类类似记忆能力的智能代理提供了有前景的方向

Abstract: Humans excel at remembering concrete experiences along spatiotemporal contexts and performing reasoning across those events, i.e., the capacity for episodic memory. In contrast, memory in language agents remains mainly semantic, and current agents are not yet capable of effectively recollecting and reasoning over interaction histories. We identify and formalize the core challenges of episodic recollection and reasoning from this gap, and observe that existing work often overlooks episodicity, lacks explicit event modeling, or overemphasizes simple retrieval rather than complex reasoning. We present REMem, a two-phase framework for constructing and reasoning with episodic memory: 1) Offline indexing, where REMem converts experiences into a hybrid memory graph that flexibly links time-aware gists and facts. 2) Online inference, where REMem employs an agentic retriever with carefully curated tools for iterative retrieval over the memory graph. Comprehensive evaluation across four episodic memory benchmarks shows that REMem substantially outperforms state-of-the-art memory systems such as Mem0 and HippoRAG 2, showing 3.4% and 13.4% absolute improvements on episodic recollection and reasoning tasks, respectively. Moreover, REMem also demonstrates more robust refusal behavior for unanswerable questions.

</details>


### [88] [OpAgent: Operator Agent for Web Navigation](https://arxiv.org/abs/2602.13559)
*Yuyu Guo,Wenjie Yang,Siyuan Yang,Ziyang Liu,Cheng Chen,Yuan Wei,Yun Hu,Yang Huang,Guoliang Hao,Dongsheng Yuan,Jianming Wang,Xin Chen,Hang Yu,Lei Lei,Peng Di*

Main category: cs.AI

TL;DR: 提出一个在线强化学习WebAgent，通过分层多任务微调、在线交互环境和模块化操作代理框架，在WebArena上达到71.6%的SOTA成功率


<details>
  <summary>Details</summary>
Motivation: 传统基于监督微调或离线强化学习的方法在处理真实网站时面临严重的分布偏移问题，因为离线轨迹无法捕捉无约束网络环境的随机状态转换和实时反馈

Method: 1) 分层多任务微调：按功能原语（规划、执行、基础）分类数据集，建立具有强指令跟随能力的视觉语言模型；2) 在线强化学习：开发在线交互环境，使用混合奖励机制（WebJudge和基于规则的决策树）；3) 操作代理框架：包含规划器、基础器、反射器和总结器的模块化系统

Result: 强化学习增强模型在WebArena上达到38.1%的成功率（pass@5），优于所有现有单体基线；完整的OpAgent框架达到71.6%的SOTA成功率

Conclusion: 通过在线强化学习和模块化代理框架的结合，显著提升了WebAgent在复杂真实网站环境中的性能和鲁棒性，解决了传统方法的分布偏移和信用分配问题

Abstract: To fulfill user instructions, autonomous web agents must contend with the inherent complexity and volatile nature of real-world websites. Conventional paradigms predominantly rely on Supervised Fine-Tuning (SFT) or Offline Reinforcement Learning (RL) using static datasets. However, these methods suffer from severe distributional shifts, as offline trajectories fail to capture the stochastic state transitions and real-time feedback of unconstrained wide web environments. In this paper, we propose a robust Online Reinforcement Learning WebAgent, designed to optimize its policy through direct, iterative interactions with unconstrained wide websites. Our approach comprises three core innovations: 1) Hierarchical Multi-Task Fine-tuning: We curate a comprehensive mixture of datasets categorized by functional primitives -- Planning, Acting, and Grounding -- establishing a Vision-Language Model (VLM) with strong instruction-following capabilities for Web GUI tasks. 2) Online Agentic RL in the Wild: We develop an online interaction environment and fine-tune the VLM using a specialized RL pipeline. We introduce a Hybrid Reward Mechanism that combines a ground-truth-agnostic WebJudge for holistic outcome assessment with a Rule-based Decision Tree (RDT) for progress reward. This system effectively mitigates the credit assignment challenge in long-horizon navigation. Notably, our RL-enhanced model achieves a 38.1\% success rate (pass@5) on WebArena, outperforming all existing monolithic baselines. 3) Operator Agent: We introduce a modular agentic framework, namely \textbf{OpAgent}, orchestrating a Planner, Grounder, Reflector, and Summarizer. This synergy enables robust error recovery and self-correction, elevating the agent's performance to a new State-of-the-Art (SOTA) success rate of \textbf{71.6\%}.

</details>


### [89] [Who Do LLMs Trust? Human Experts Matter More Than Other LLMs](https://arxiv.org/abs/2602.13568)
*Anooshka Bajaj,Zoran Tiganj*

Main category: cs.AI

TL;DR: LLMs在决策时更倾向于遵循人类专家的反馈而非其他LLMs的反馈，表现出类似人类的社会影响模式


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在遇到社会信息（如其他智能体答案、工具输出或人类建议）时是否表现出类似人类的社会影响模式，特别是它们是否更重视人类反馈而非其他LLMs的反馈

Method: 通过三个二元决策任务（阅读理解、多步推理、道德判断），向四个指令调优的LLMs呈现先前回答，这些回答被标记为来自朋友、人类专家或其他LLMs。操纵群体是否正确以及群体规模。第二个实验引入单个人类与单个LLM之间的直接分歧

Result: 在所有任务中，模型显著更倾向于遵循标记为来自人类专家的回答，即使该信号是错误的；相比其他LLMs，模型更愿意向专家反馈修正自己的答案

Conclusion: 专家框架对当代LLMs具有强烈的先验影响，表明存在一种跨决策领域的可信度敏感的社会影响形式

Abstract: Large language models (LLMs) increasingly operate in environments where they encounter social information such as other agents' answers, tool outputs, or human recommendations. In humans, such inputs influence judgments in ways that depend on the source's credibility and the strength of consensus. This paper investigates whether LLMs exhibit analogous patterns of influence and whether they privilege feedback from humans over feedback from other LLMs. Across three binary decision-making tasks, reading comprehension, multi-step reasoning, and moral judgment, we present four instruction-tuned LLMs with prior responses attributed either to friends, to human experts, or to other LLMs. We manipulate whether the group is correct and vary the group size. In a second experiment, we introduce direct disagreement between a single human and a single LLM. Across tasks, models conform significantly more to responses labeled as coming from human experts, including when that signal is incorrect, and revise their answers toward experts more readily than toward other LLMs. These results reveal that expert framing acts as a strong prior for contemporary LLMs, suggesting a form of credibility-sensitive social influence that generalizes across decision domains.

</details>


### [90] [Differentiable Rule Induction from Raw Sequence Inputs](https://arxiv.org/abs/2602.13583)
*Kun Gao,Katsumi Inoue,Yongzhi Cao,Hanpin Wang,Feng Yang*

Main category: cs.AI

TL;DR: 提出一种结合自监督可微分聚类和新型可微分ILP的方法，直接从原始数据学习规则，避免显式标签泄露问题


<details>
  <summary>Details</summary>
Motivation: 传统可微分归纳逻辑编程(ILP)方法依赖符号数据集，难以直接从原始数据学习，存在显式标签泄露问题——无法在没有输入特征标签显式监督的情况下将连续输入映射到符号变量

Method: 集成自监督可微分聚类模型与新型可微分ILP模型，使系统能够直接从原始数据学习规则而不需要显式标签泄露

Result: 该方法能够直观且精确地从时间序列和图像数据中学习泛化规则，有效通过特征描述原始数据

Conclusion: 通过结合自监督聚类和可微分ILP，成功解决了可微分ILP直接从原始数据学习时的显式标签泄露问题，实现了更实用的规则学习

Abstract: Rule learning-based models are widely used in highly interpretable scenarios due to their transparent structures. Inductive logic programming (ILP), a form of machine learning, induces rules from facts while maintaining interpretability. Differentiable ILP models enhance this process by leveraging neural networks to improve robustness and scalability. However, most differentiable ILP methods rely on symbolic datasets, facing challenges when learning directly from raw data. Specifically, they struggle with explicit label leakage: The inability to map continuous inputs to symbolic variables without explicit supervision of input feature labels. In this work, we address this issue by integrating a self-supervised differentiable clustering model with a novel differentiable ILP model, enabling rule learning from raw data without explicit label leakage. The learned rules effectively describe raw data through its features. We demonstrate that our method intuitively and precisely learns generalized rules from time series and image data.

</details>


### [91] [A First Proof Sprint](https://arxiv.org/abs/2602.13587)
*Joseph Corneli*

Main category: cs.AI

TL;DR: 多智能体证明冲刺项目报告：使用结构化验证和分层策略解决10个研究级问题，获得混合结果，其中问题3在特定条件下验证完成，问题5有限范围解决，问题10有条件解决，问题4和6部分解决，问题7临时关闭。


<details>
  <summary>Details</summary>
Motivation: 提高压缩证明冲刺的可靠性和校准，通过结构化验证和分层策略来协调多智能体协作，解决研究级数学问题。

Method: 采用多智能体协作工作流，结合快速草稿生成、对抗性验证、针对性修复和明确溯源。使用声明依赖的接线图分解来定位差距，协调审稿人驱动的修订，区分数学状态与QC验证状态。

Result: 问题3在特定范围内验证完成，问题5在有限范围内解决，问题10有条件解决（有明确反例），问题4和6部分解决（有明确剩余义务），问题7临时关闭。问题7和9在QC层仍有验证缺口。

Conclusion: 结构感知验证和分层切换策略能显著提高压缩证明冲刺的可靠性和校准，为多智能体协作数学研究提供了有效方法论。

Abstract: This monograph reports a multi-agent proof sprint on ten research-level problems, combining rapid draft generation with adversarial verification, targeted repair, and explicit provenance. The workflow uses wiring-diagram decompositions of claim dependencies to localize gaps and coordinate reviewer-driven revisions. Final outcomes are heterogeneous but explicit: the manuscript distinguishes mathematical status from QC-validation status. Mathematically, Problem~3 has a validation-complete existence path under the scoped criterion used here (uniqueness/irreducibility treated as optional), Problem 5 is solved in a scope-limited form for $F_O$-local connective spectra, Problem 10 is conditional under clearly stated assumptions (with explicit necessity counterexamples when assumptions are dropped), and Problems 4 and 6 are partial with named remaining obligations in the general case (including an unconditional $K_n$ result for Problem 6 with $c_0 = 1/3$). Problem 7 is treated as provisionally closed via the rotation-route theorem chain, pending independent ledger re-check. At the QC layer, Problems~7 and~9 have node-level validation artifacts but still contain unresolved verifier gaps. The main methodological result is that structure-aware verification and layer-switching strategies improve reliability and calibration in compressed proof sprints.

</details>


### [92] [Hippocampus: An Efficient and Scalable Memory Module for Agentic AI](https://arxiv.org/abs/2602.13594)
*Yi Li,Lianjie Cao,Faraz Ahmed,Puneet Sharma,Bingzhe Li*

Main category: cs.AI

TL;DR: Hippocampus是一个用于智能体AI的持久内存管理系统，使用二进制签名进行语义搜索，通过动态小波矩阵压缩索引，相比现有系统显著降低延迟和存储开销。


<details>
  <summary>Details</summary>
Motivation: 智能体AI需要超越LLM有限上下文窗口的持久内存来存储用户特定历史。现有内存系统使用密集向量数据库或知识图谱遍历，存在检索延迟高和存储扩展性差的问题。

Method: 提出Hippocampus系统，使用紧凑二进制签名进行语义搜索，无损token-ID流进行精确内容重建。核心是动态小波矩阵，压缩并共同索引两个流，支持在压缩域内进行超快速搜索，避免昂贵的密集向量或图计算。

Result: 评估显示Hippocampus将端到端检索延迟降低高达31倍，每个查询的token占用减少高达14倍，同时在LoCoMo和LongMemEval基准测试中保持准确性。

Conclusion: Hippocampus的设计随着内存大小线性扩展，适用于长时程智能体部署，解决了现有内存系统的延迟和可扩展性问题。

Abstract: Agentic AI require persistent memory to store user-specific histories beyond the limited context window of LLMs. Existing memory systems use dense vector databases or knowledge-graph traversal (or hybrid), incurring high retrieval latency and poor storage scalability. We introduce Hippocampus, an agentic memory management system that uses compact binary signatures for semantic search and lossless token-ID streams for exact content reconstruction. Its core is a Dynamic Wavelet Matrix (DWM) that compresses and co-indexes both streams to support ultra-fast search in the compressed domain, thus avoiding costly dense-vector or graph computations. This design scales linearly with memory size, making it suitable for long-horizon agentic deployments. Empirically, our evaluation shows that Hippocampus reduces end-to-end retrieval latency by up to 31$\times$ and cuts per-query token footprint by up to 14$\times$, while maintaining accuracy on both LoCoMo and LongMemEval benchmarks.

</details>


### [93] [The Quantization Trap: Breaking Linear Scaling Laws in Multi-Hop Reasoning](https://arxiv.org/abs/2602.13595)
*Henry Han,Xiyang Liu,Xiaodong Wang,Fei Han,Xiaodong Li*

Main category: cs.AI

TL;DR: 量化缩放定律在多跳推理任务中失效：从16位降至8/4位反而增加能耗并降低精度，存在"量化陷阱"


<details>
  <summary>Details</summary>
Motivation: 挑战当前AI领域"越小越好"的量化趋势，揭示神经缩放定律在多跳推理任务中的局限性

Method: 理论分解方法，分析硬件转换开销、反量化内核的隐藏延迟成本，以及在顺序推理链中的瓶颈效应

Result: 发现量化陷阱：降低精度反而增加净能耗并降低推理准确率，缩放定律在实践中不可避免地被打破

Conclusion: 对于复杂推理任务，行业"越小越好"的启发式方法在数学上是适得其反的，需要重新考虑量化策略

Abstract: Neural scaling laws provide a predictable recipe for AI advancement: reducing numerical precision should linearly improve computational efficiency and energy profile (E proportional to bits). In this paper, we demonstrate that this scaling law breaks in the context of multi-hop reasoning. We reveal a 'quantization trap' where reducing precision from 16-bit to 8/4-bit paradoxically increases more net energy consumption while degrading reasoning accuracy. We provide a rigorous theoretical decomposition that attributes this failure to hardware casting overhead, the hidden latency cost of dequantization kernels, which becomes a dominant bottleneck in sequential reasoning chains, as well as to a sequential energy amortization failure. As a result, scaling law breaking is unavoidable in practice. Our findings suggest that the industry's "smaller-is-better" heuristic is mathematically counterproductive for complex reasoning tasks.

</details>


### [94] [DiffusionRollout: Uncertainty-Aware Rollout Planning in Long-Horizon PDE Solving](https://arxiv.org/abs/2602.13616)
*Seungwoo Yoo,Juil Koo,Daehyeon Choi,Minhyuk Sung*

Main category: cs.AI

TL;DR: 提出DiffusionRollout，一种用于自回归扩散模型的选择性展开规划策略，旨在减轻偏微分方程物理系统长期预测中的误差累积问题。


<details>
  <summary>Details</summary>
Motivation: 解决自回归扩散模型在偏微分方程长期预测中的误差累积问题，利用模型预测不确定性来指导规划过程。

Method: 基于扩散模型预测不确定性与误差之间的强相关性，提出自适应步长选择机制，在自回归展开过程中根据预测置信度动态调整步长。

Result: 在长期轨迹PDE预测基准测试中，该方法显著降低了预测误差，生成了更长的预测轨迹，且与真实值保持高相关性。

Conclusion: DiffusionRollout通过利用预测不确定性指导自适应规划，有效缓解了自回归扩散模型在PDE长期预测中的误差累积问题，提高了预测可靠性。

Abstract: We propose DiffusionRollout, a novel selective rollout planning strategy for autoregressive diffusion models, aimed at mitigating error accumulation in long-horizon predictions of physical systems governed by partial differential equations (PDEs). Building on the recently validated probabilistic approach to PDE solving, we further explore its ability to quantify predictive uncertainty and demonstrate a strong correlation between prediction errors and standard deviations computed over multiple samples-supporting their use as a proxy for the model's predictive confidence. Based on this observation, we introduce a mechanism that adaptively selects step sizes during autoregressive rollouts, improving long-term prediction reliability by reducing the compounding effect of conditioning on inaccurate prior outputs. Extensive evaluation on long-trajectory PDE prediction benchmarks validates the effectiveness of the proposed uncertainty measure and adaptive planning strategy, as evidenced by lower prediction errors and longer predicted trajectories that retain a high correlation with their ground truths.

</details>


### [95] [Guided Collaboration in Heterogeneous LLM-Based Multi-Agent Systems via Entropy-Based Understanding Assessment and Experience Retrieval](https://arxiv.org/abs/2602.13639)
*Linlin Wang,Tianqing Zhu,Laiqiao Qin,Longxiang Gao,Wanlei Zhou*

Main category: cs.AI

TL;DR: 本文提出了一种基于熵的自适应引导框架，解决异构多智能体系统中强-弱模型协作时出现的认知不匹配问题，通过动态调整引导强度提升协作效果。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在推理、规划和复杂任务生成方面的突破，AI系统正从单智能体架构转向多智能体协作系统。然而在异构多智能体系统中，智能体之间的能力差异导致认知问题，强-弱模型协作效果不佳，甚至可能弱于弱-弱组合。

Method: 提出基于熵的自适应引导框架：1）通过多维度熵度量（表达、不确定性、结构、连贯性、相关性）量化弱智能体的理解程度；2）根据认知状态动态调整引导强度（轻度、中度、重度）；3）结合检索增强生成机制保留成功协作经验，支持即时适应和长期学习。

Result: 在GSM8K、MBPP和CVRP三个基准数据集上的实验表明，该方法能持续提升异构协作的有效性和稳定性。自适应引导不仅缓解了认知不平衡，还为更鲁棒、协作的多智能体智能建立了可扩展路径。

Conclusion: 异构多智能体系统中的认知不匹配是限制协作的关键瓶颈。提出的基于熵的自适应引导框架通过动态调整引导强度，有效解决了强-弱协作中的认知不平衡问题，为实现更鲁棒的多智能体协作提供了可行方案。

Abstract: With recent breakthroughs in large language models (LLMs) for reasoning, planning, and complex task generation, artificial intelligence systems are transitioning from isolated single-agent architectures to multi-agent systems with collaborative intelligence. However, in heterogeneous multi-agent systems (HMAS), capability differences among agents give rise to consistent cognitive problems, where strong and weak models fail to contribute effectively. We define the collaboration as a strong-weak system. Through comprehensive experiments, we disclose a counterintuitive phenomenon in the strong-weak system: a strong-weak collaboration may under-perform weak-weak combinations, revealing that cognitive mismatching are key bottlenecks limiting heterogeneous cooperation. To overcome these challenges, we propose an Entropy-Based Adaptive Guidance Framework that dynamically aligns the guidance with the cognitive state of each agent. The framework quantifies the understanding of weak agents through multi-dimensional entropy metrics - covering expression, uncertainty, structure, coherence, and relevance - and adaptively adjusts the intensity of the guidance at light, moderate and intensive levels. Furthermore, a Retrieval-Augmented Generation (RAG) mechanism is incorporated to retain successful collaboration experiences, enabling both immediate adaptation and long-term learning. Extensive experiments on three benchmark datasets, GSM8K, MBPP, and CVRP demonstrate that our approach consistently enhances the effectiveness and stability of heterogeneous collaboration. The results highlight that adaptive guidance not only mitigates cognitive imbalance but also establishes a scalable pathway toward more robust, cooperative multi-agent intelligence.

</details>


### [96] [Building Autonomous GUI Navigation via Agentic-Q Estimation and Step-Wise Policy Optimization](https://arxiv.org/abs/2602.13653)
*Yibo Wang,Guangda Huzhang,Yuwei Hu,Yu Xia,Shiyin Lu,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang,Lijun Zhang*

Main category: cs.AI

TL;DR: 提出一个基于MLLM的GUI智能体框架，包含agentic-Q估计和逐步策略优化两个组件，通过强化学习优化GUI交互能力，显著提升性能且降低数据收集成本。


<details>
  <summary>Details</summary>
Motivation: 现实应用中GUI智能体面临非平稳环境，导致数据整理和策略优化的计算成本高昂，需要更高效的框架来解决这些问题。

Method: 提出MLLM中心的GUI智能体框架，包含：1) agentic-Q估计：优化Q模型评估动作对任务完成的贡献值；2) 逐步策略优化：从状态-动作轨迹中采样，通过强化学习优化策略。框架特点是数据由策略自身产生，策略更新与环境解耦。

Result: 该框架使Ovis2.5-9B具备强大的GUI交互能力，在GUI导航和基础任务基准测试中表现优异，甚至超越更大规模的竞争对手模型。

Conclusion: 提出的MLLM中心框架有效解决了GUI智能体在非平稳环境中的计算成本问题，通过agentic-Q估计和逐步策略优化实现了高效稳定的GUI交互能力优化。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have substantially driven the progress of autonomous agents for Graphical User Interface (GUI). Nevertheless, in real-world applications, GUI agents are often faced with non-stationary environments, leading to high computational costs for data curation and policy optimization. In this report, we introduce a novel MLLM-centered framework for GUI agents, which consists of two components: agentic-Q estimation and step-wise policy optimization. The former one aims to optimize a Q-model that can generate step-wise values to evaluate the contribution of a given action to task completion. The latter one takes step-wise samples from the state-action trajectory as inputs, and optimizes the policy via reinforcement learning with our agentic-Q model. It should be noticed that (i) all state-action trajectories are produced by the policy itself, so that the data collection costs are manageable; (ii) the policy update is decoupled from the environment, ensuring stable and efficient optimization. Empirical evaluations show that our framework endows Ovis2.5-9B with powerful GUI interaction capabilities, achieving remarkable performances on GUI navigation and grounding benchmarks and even surpassing contenders with larger scales.

</details>


### [97] [HyFunc: Accelerating LLM-based Function Calls for Agentic AI through Hybrid-Model Cascade and Dynamic Templating](https://arxiv.org/abs/2602.13665)
*Weibin Liao,Jian-guang Lou,Haoyi Xiong*

Main category: cs.AI

TL;DR: HyFunc框架通过混合模型级联和动态模板技术，消除LLM函数调用中的三种冗余，显著降低推理延迟同时保持高性能


<details>
  <summary>Details</summary>
Motivation: 基于LLM的智能体AI系统在将用户意图转换为结构化函数调用时存在计算冗余，导致高推理延迟，阻碍实时应用。论文识别了三种关键冗余：冗余处理大量函数描述、冗余使用大型模型生成可预测的token序列、冗余生成固定的参数语法模板

Method: HyFunc采用混合模型级联：大型模型将用户意图提炼为单个"软token"，指导轻量级检索器选择相关函数，并引导较小的前缀调优模型生成最终调用。使用"动态模板"技术在扩展的vLLM引擎中即时注入参数语法模板，避免语法冗余

Result: 在未见过的BFCL基准数据集上，HyFunc实现了0.828秒的推理延迟，优于所有基线模型，达到80.1%的性能，超越了所有参数规模相当的模型，在效率和性能之间取得了良好平衡

Conclusion: HyFunc为智能体AI提供了更高效的模式，通过消除冗余处理实现了低延迟和高性能的平衡，代码已开源

Abstract: While agentic AI systems rely on LLMs to translate user intent into structured function calls, this process is fraught with computational redundancy, leading to high inference latency that hinders real-time applications. This paper identifies and addresses three key redundancies: (1) the redundant processing of a large library of function descriptions for every request; (2) the redundant use of a large, slow model to generate an entire, often predictable, token sequence; and (3) the redundant generation of fixed, boilerplate parameter syntax. We introduce HyFunc, a novel framework that systematically eliminates these inefficiencies. HyFunc employs a hybrid-model cascade where a large model distills user intent into a single "soft token." This token guides a lightweight retriever to select relevant functions and directs a smaller, prefix-tuned model to generate the final call, thus avoiding redundant context processing and full-sequence generation by the large model. To eliminate syntactic redundancy, our "dynamic templating" technique injects boilerplate parameter syntax on-the-fly within an extended vLLM engine. To avoid potential limitations in generalization, we evaluate HyFunc on an unseen benchmark dataset, BFCL. Experimental results demonstrate that HyFunc achieves an excellent balance between efficiency and performance. It achieves an inference latency of 0.828 seconds, outperforming all baseline models, and reaches a performance of 80.1%, surpassing all models with a comparable parameter scale. These results suggest that HyFunc offers a more efficient paradigm for agentic AI. Our code is publicly available at https://github.com/MrBlankness/HyFunc.

</details>


### [98] [AllMem: A Memory-centric Recipe for Efficient Long-context Modeling](https://arxiv.org/abs/2602.13680)
*Ziming Wang,Xiang Wang,Kailong Peng,Lang Qin,Juan Gabriel Kostelec,Christos Sourmpis,Axel Laborieux,Qinghai Guo*

Main category: cs.AI

TL;DR: AllMem是一种结合滑动窗口注意力和非线性测试时训练记忆网络的高效混合架构，旨在解决LLM在长序列任务中的计算和内存瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在长序列任务中面临自注意力机制带来的计算复杂度和内存开销问题，需要一种能扩展到超长上下文同时缓解灾难性遗忘的高效解决方案。

Method: 提出AllMem混合架构，集成滑动窗口注意力与非线性的测试时训练记忆网络；采用内存高效微调策略，将预训练模型中的标准注意力层替换为内存增强的滑动窗口层。

Result: 4k窗口模型在37k LongBench上实现接近无损性能（仅下降0.83）；8k窗口变体在128k InfiniteBench上优于全注意力，验证了参数化记忆在降低噪声和保持长距离建模方面的有效性。

Conclusion: AllMem能够高效地将任何现成的预训练LLM转换为内存增强架构，显著降低长序列推理的计算和内存开销，同时保持强大的长距离建模能力。

Abstract: Large Language Models (LLMs) encounter significant performance bottlenecks in long-sequence tasks due to the computational complexity and memory overhead inherent in the self-attention mechanism. To address these challenges, we introduce \textsc{AllMem}, a novel and efficient hybrid architecture that integrates Sliding Window Attention (SWA) with non-linear Test-Time Training (TTT) memory networks. \textsc{AllMem} enables models to effectively scale to ultra-long contexts while mitigating catastrophic forgetting. This approach not only overcomes the representation constraints typical of linear memory models but also significantly reduces the computational and memory footprint during long-sequence inference. Furthermore, we implement a Memory-Efficient Fine-Tuning strategy to replace standard attention layers in pre-trained models with memory-augmented sliding window layers. This framework facilitates the efficient transformation of any off-the-shelf pre-trained LLM into an \textsc{AllMem}-based architecture. Empirical evaluations confirm that our 4k window model achieves near-lossless performance on 37k LongBench with a marginal 0.83 drop compared to full attention. Furthermore, on InfiniteBench at a 128k context, our 8k window variant outperforms full attention, which validates the effectiveness of our parameterized memory in mitigating noise and maintaining robust long-range modeling without the prohibitive costs of global attention.

</details>


### [99] [PhGPO: Pheromone-Guided Policy Optimization for Long-Horizon Tool Planning](https://arxiv.org/abs/2602.13691)
*Yu Li,Guangfeng Cai,Shengtian Yang,Han Luo,Shuo Han,Xu He,Dong Li,Lei Feng*

Main category: cs.AI

TL;DR: 提出Pheromone-Guided Policy Optimization (PhGPO)，利用历史成功轨迹中的工具转换模式（信息素）指导策略优化，提升LLM智能体的长时程工具规划能力。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在执行复杂任务时面临长时程多步骤工具规划的挑战，探索空间呈组合爆炸。现有方法即使找到正确路径，也只作为即时奖励，无法为后续训练提供可复用信息。

Method: 受蚁群优化启发，提出PhGPO方法：1) 从历史轨迹学习轨迹级工具转换模式（信息素）；2) 用学习到的信息素指导策略优化，为策略提供显式可复用的指导。

Result: 综合实验结果表明PhGPO方法的有效性，能够显著提升长时程工具规划的性能。

Conclusion: 历史成功轨迹包含可复用的工具转换模式，通过学习这些模式（信息素）来指导策略优化，可以有效解决长时程工具规划中的组合爆炸问题。

Abstract: Recent advancements in Large Language Model (LLM) agents have demonstrated strong capabilities in executing complex tasks through tool use. However, long-horizon multi-step tool planning is challenging, because the exploration space suffers from a combinatorial explosion. In this scenario, even when a correct tool-use path is found, it is usually considered an immediate reward for current training, which would not provide any reusable information for subsequent training. In this paper, we argue that historically successful trajectories contain reusable tool-transition patterns, which can be leveraged throughout the whole training process. Inspired by ant colony optimization where historically successful paths can be reflected by the pheromone, we propose Pheromone-Guided Policy Optimization (PhGPO), which learns a trajectory-based transition pattern (i.e., pheromone) from historical trajectories and then uses the learned pheromone to guide policy optimization. This learned pheromone provides explicit and reusable guidance that steers policy optimization toward historically successful tool transitions, thereby improving long-horizon tool planning. Comprehensive experimental results demonstrate the effectiveness of our proposed PhGPO.

</details>


### [100] [Can a Lightweight Automated AI Pipeline Solve Research-Level Mathematical Problems?](https://arxiv.org/abs/2602.13695)
*Lve Meng,Weilong Zhao,Yanzhi Zhang,Haoxiang Guan,Jiyan He*

Main category: cs.AI

TL;DR: LLMs结合自动化流水线可解决研究级数学问题，在ICCM和"First Proof"数据集上生成并验证了证明


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs在数学证明生成方面取得显著进展，但在研究问题上的轻量级自然语言流水线部署仍未被充分探索

Method: 使用新一代LLMs（如Gemini 3 Pro、GPT-5.2 Pro）构建优化的自动化流水线，专注于基于引用的验证机制

Result: 流水线为所有ICCM问题集和"First Proof"问题集生成了候选证明，其中前两个ICCM集和"First Proof"问题4的证明已完全验证

Conclusion: LLMs结合优化的自动化流水线能够解决复杂的研究级数学问题，为AI数学研究提供了有效途径

Abstract: Large language models (LLMs) have recently achieved remarkable success in generating rigorous mathematical proofs, with "AI for Math" emerging as a vibrant field of research. While these models have mastered competition-level benchmarks like the International Mathematical Olympiad and show promise in research applications through auto-formalization, their deployment via lightweight, natural-language pipelines for research problems remains underexplored. In this work, we demonstrate that next-generation models (e.g., Gemini 3 Pro, GPT-5.2 Pro), when integrated into a streamlined automated pipeline optimized for citation-based verification, can solve sophisticated research-grade problems. We evaluate our pipeline on two novel datasets: (1) the ICCM problem sets (comparable to the S.-T. Yau College Student Mathematics Contest) proposed by leading mathematicians, and (2) the "First Proof" problem set, consisting of previously unpublished research questions. Our pipeline generated candidate proofs for all problems in the first two ICCM sets and the "First Proof" set. The solutions for the first two ICCM sets and Problem 4 of the "First Proof" set have been fully verified by our team. All generated proofs have been submitted to the official organization, and our generated results are publicly available. We plan to open-source the complete pipeline methodology in due course.

</details>


### [101] [No Need to Train Your RDB Foundation Model](https://arxiv.org/abs/2602.13697)
*Linjie Xu,Yanlin Zhang,Quan Gan,Minjie Wang,David Wipf*

Main category: cs.AI

TL;DR: 提出一种无需训练的关系数据库基础模型，通过列内压缩而非跨列压缩生成固定长度ICL样本，可与现有单表ICL模型无缝集成


<details>
  <summary>Details</summary>
Motivation: 关系数据库包含大量异构表格信息可用于预测建模，但传统方法需要为每个新预测目标重新训练模型，而现有ICL基础模型主要局限于单表操作

Method: 设计基于SQL原语的关系数据库编码器，在列内进行压缩（同单位同角色的实体），避免跨列压缩，生成固定长度ICL样本，无需训练参数即可与现有单表ICL模型集成

Result: 开发了开源RDB基础模型，无需训练或微调即可在未见数据集上实现稳健性能，理论证明列内压缩优于跨列压缩

Conclusion: 提出了一种原则性的关系数据库编码器家族，通过列内压缩策略和SQL实现，使现有ICL基础模型能够处理多表关系数据库，无需额外训练

Abstract: Relational databases (RDBs) contain vast amounts of heterogeneous tabular information that can be exploited for predictive modeling purposes. But since the space of potential targets is vast across enterprise settings, how can we \textit{avoid retraining} a new model each time we wish to predict a new quantity of interest? Foundation models based on in-context learning (ICL) offer a convenient option, but so far are largely restricted to single-table operability. In generalizing to multiple interrelated tables, it is essential to compress variably-sized RDB neighborhoods into fixed-length ICL samples for consumption by the decoder. However, the details here are critical: unlike existing supervised learning RDB pipelines, we provide theoretical and empirical evidence that ICL-specific compression should be constrained \emph{within} high-dimensional RDB columns where all entities share units and roles, not \textit{across} columns where the relevance of heterogeneous data types cannot possibly be determined without label information. Conditioned on this restriction, we then demonstrate that encoder expressiveness is actually not compromised by excluding trainable parameters. Hence we arrive at a principled family of RDB encoders that can be seamlessly paired with already-existing single-table ICL foundation models, whereby no training or fine-tuning is required. From a practical standpoint, we develop scalable SQL primitives to implement the encoder stage, resulting in an easy-to-use open-source RDB foundation model\footnote{\label{foot: RDBLearn_learn} https://github.com/HKUSHXLab/rdblearn} capable of robust performance on unseen datasets out of the box.

</details>


### [102] [OneLatent: Single-Token Compression for Visual Latent Reasoning](https://arxiv.org/abs/2602.13738)
*Bo Lv,Yasheng Sun,Junjie Wang,Haoxiang Shi*

Main category: cs.AI

TL;DR: OneLatent框架通过将推理步骤压缩为单个潜在token，大幅减少推理成本，同时保持高准确率


<details>
  <summary>Details</summary>
Motivation: 解决Chain-of-thought提示推理成本过高的问题，传统CoT方法使推理成本增加1-2个数量级

Method: 将文本推理步骤渲染为图像，通过DeepSeek-OCR隐藏状态监督，将中间推理压缩为单个潜在token

Result: 平均输出长度减少11倍，准确率仅下降2.21%，输出token贡献度提升6.8倍，在长链逻辑推理上达到99.80%准确率

Conclusion: OneLatent框架有效解决了CoT推理成本问题，通过压缩推理为潜在token实现高效推理，支持压缩约束下的泛化

Abstract: Chain-of-thought (CoT) prompting improves reasoning but often increases inference cost by one to two orders of magnitude. To address these challenges, we present \textbf{OneLatent}, a framework that compresses intermediate reasoning into a single latent token via supervision from rendered CoT images and DeepSeek-OCR hidden states. By rendering textual steps into images, we obtain a deterministic supervision signal that can be inspected and audited without requiring the model to output verbose textual rationales. Across benchmarks, OneLatent reduces average output length by $11\times$ with only a $2.21\%$ average accuracy drop relative to textual CoT, while improving output token contribution (OTC) by $6.8\times$. On long-chain logical reasoning, OneLatent reaches $99.80\%$ on ProntoQA and $97.80\%$ on ProsQA with one latent token, with compression up to $87.4\times$, supporting compression-constrained generalization.

</details>


### [103] [OR-Agent: Bridging Evolutionary Search and Structured Research for Automated Algorithm Discovery](https://arxiv.org/abs/2602.13769)
*Qi Liu,Wanjing Ma*

Main category: cs.AI

TL;DR: OR-Agent是一个可配置的多智能体研究框架，用于在复杂实验环境中实现自动化科学发现，通过结构化假设管理和分层反思系统超越传统的进化算法。


<details>
  <summary>Details</summary>
Motivation: 在复杂实验驱动领域实现自动化科学发现需要超越简单的程序迭代变异，需要结构化假设管理、环境交互和原则性反思。现有方法通常局限于简单的变异-交叉循环，缺乏对研究轨迹的受控管理。

Method: 提出OR-Agent框架，采用基于树的结构化研究工作流，明确建模分支假设生成和系统回溯。核心包括进化-系统构思机制（统一进化选择、研究计划生成和协调探索）和分层优化启发反思系统（短期实验反思、长期反思和记忆压缩）。

Result: 在经典组合优化基准问题（旅行商、车辆路径、装箱、定向、多背包）和仿真协同驾驶场景中，OR-Agent优于强进化基线，同时提供了一个通用、可扩展和可检查的AI辅助科学发现框架。

Conclusion: OR-Agent通过结构化假设管理和分层反思系统，为自动化科学发现提供了一个原则性架构，在复杂实验环境中展现出优越性能，并具有通用性和可扩展性。

Abstract: Automating scientific discovery in complex, experiment-driven domains requires more than iterative mutation of programs; it demands structured hypothesis management, environment interaction, and principled reflection. We present OR-Agent, a configurable multi-agent research framework designed for automated exploration in rich experimental environments. OR-Agent organizes research as a structured tree-based workflow that explicitly models branching hypothesis generation and systematic backtracking, enabling controlled management of research trajectories beyond simple mutation-crossover loops. At its core, we introduce an evolutionary-systematic ideation mechanism that unifies evolutionary selection of research starting points, comprehensive research plan generation, and coordinated exploration within a research tree. We further propose a hierarchical optimization-inspired reflection system: short-term experimental reflection operates as a form of verbal gradient providing immediate corrective signals; long-term reflection accumulates cross-experiment insights as verbal momentum; and memory compression serves as a regularization mechanism analogous to weight decay, preserving essential signals while mitigating drift. Together, these components form a principled architecture governing research dynamics. We conduct extensive experiments across classical combinatorial optimization benchmarks-including traveling salesman, capacitated vehicle routing, bin packing, orienteering, and multiple knapsack problems-as well as simulation-based cooperative driving scenarios. Results demonstrate that OR-Agent outperforms strong evolutionary baselines while providing a general, extensible, and inspectable framework for AI-assisted scientific discovery. OR-Agent source code and experiments data are publicly available at https://github.com/qiliuchn/OR-Agent.

</details>


### [104] [StackingNet: Collective Inference Across Independent AI Foundation Models](https://arxiv.org/abs/2602.13792)
*Siyang Li,Chenhao Liu,Dongrui Wu,Zhigang Zeng,Lieyun Ding*

Main category: cs.AI

TL;DR: StackingNet是一个元集成框架，通过集体智能原理协调多个黑盒异构基础模型，无需访问内部参数或训练数据即可提升准确性、减少偏见并实现可靠性排名。


<details>
  <summary>Details</summary>
Motivation: 当前大型基础模型虽然在各领域表现出色，但彼此孤立，无法共享能力。整合这些独立模型的互补优势对于构建可信赖的智能系统至关重要，但目前缺乏协调黑盒异构模型的成熟方法。

Method: 提出StackingNet元集成框架，基于集体智能原理，在推理过程中结合多个模型的预测结果。该框架无需访问模型内部参数或训练数据，能够识别或修剪降低性能的模型。

Result: 在语言理解、视觉估计和学术论文评分等任务中，StackingNet相比单个模型和经典集成方法，持续提升了准确性、鲁棒性和公平性，同时减少了偏见。

Conclusion: StackingNet通过将多样性从不一致性来源转变为协作，为协调人工智能建立了实用基础，表明进步不仅来自更大的单一模型，也来自多个专业化模型的原则性合作。

Abstract: Artificial intelligence built on large foundation models has transformed language understanding, vision and reasoning, yet these systems remain isolated and cannot readily share their capabilities. Integrating the complementary strengths of such independent foundation models is essential for building trustworthy intelligent systems. Despite rapid progress in individual model design, there is no established approach for coordinating such black-box heterogeneous models. Here we show that coordination can be achieved through a meta-ensemble framework termed StackingNet, which draws on principles of collective intelligence to combine model predictions during inference. StackingNet improves accuracy, reduces bias, enables reliability ranking, and identifies or prunes models that degrade performance, all operating without access to internal parameters or training data. Across tasks involving language comprehension, visual estimation, and academic paper rating, StackingNet consistently improves accuracy, robustness, and fairness, compared with individual models and classic ensembles. By turning diversity from a source of inconsistency into collaboration, StackingNet establishes a practical foundation for coordinated artificial intelligence, suggesting that progress may emerge from not only larger single models but also principled cooperation among many specialized ones.

</details>


### [105] [Attention in Constant Time: Vashista Sparse Attention for Long-Context Decoding with Exponential Guarantees](https://arxiv.org/abs/2602.13804)
*Vashista Nobaub*

Main category: cs.AI

TL;DR: 论文提出Vashista稀疏注意力机制，通过理论证明注意力可集中在少量关键token上，实现长上下文推理的常数级计算复杂度，显著提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长上下文推理中主要计算成本来自注意力机制，但实证表明只有少量token对每个查询有实质贡献。现有注意力机制对所有token进行计算导致计算浪费。

Method: 1. 理论分析：将注意力建模为key向量凸包上的投影，提出面稳定性定理，证明在严格互补边际条件下，熵注意力可集中在常数大小的活动面上；2. 实践方法：提出Vashista稀疏注意力机制，通过分页式上下文选择策略为每个查询维护小型候选集，与现代推理栈兼容。

Result: 1. 理论结果：非活动token的质量以指数速率衰减，活动面上的误差随温度参数线性缩放；2. 实验验证：在长上下文评估中观察到稳定的常数级有效支持、显著的实时加速，在支持间隙诊断预测的范围内质量下降最小。

Conclusion: 该工作为稀疏长上下文解码提供了安全判据和精度-计算权衡的理论基础，Vashista稀疏注意力机制在隐私敏感和隔离环境中具有部署优势，可实现可预测的延迟和成本，无需外部检索依赖。

Abstract: Large language models spend most of their inference cost on attention over long contexts, yet empirical behavior suggests that only a small subset of tokens meaningfully contributes to each query. We formalize this phenomenon by modeling attention as a projection onto the convex hull of key vectors and analyzing its entropic (softmax-like) relaxation. Our main theoretical contribution is a face-stability theorem showing that, under a strict complementarity margin (a support gap (Δ) certified by KKT multipliers), entropic attention concentrates on a constant-size active face: the total mass assigned to inactive tokens decays exponentially as (\exp(-Ω(Δ/\varepsilon))), while the error on the active face scales linearly in the temperature/regularization parameter (\varepsilon). This yields a practical criterion for when sparse long-context decoding is safe and provides a principled knob to trade accuracy for compute.
  Building on these guarantees, we introduce Vashista Sparse Attention, a drop-in mechanism that maintains a small candidate set per query through a paging-style context selection strategy compatible with modern inference stacks. Across long-context evaluations, we observe stable constant-size effective support, strong wall-clock speedups, and minimal quality degradation in the regimes predicted by the support-gap diagnostics. Finally, we discuss deployment implications for privacy-sensitive and air-gapped settings, where interchangeable attention modules enable predictable latency and cost without external retrieval dependencies.

</details>


### [106] [An end-to-end agentic pipeline for smart contract translation and quality evaluation](https://arxiv.org/abs/2602.13808)
*Abhinav Goel,Chaitya Shah,Agostino Capponi,Alfio Gliozzo*

Main category: cs.AI

TL;DR: 提出一个端到端框架，用于系统评估从自然语言规范生成的LLM智能合约，通过结构化解析、代码生成、质量评估五个维度，并提供可重复的基准测试。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对LLM生成智能合约的系统性评估方法，需要可重复的基准来量化生成质量，识别逻辑遗漏、状态转换不一致等系统性错误模式。

Method: 使用CrewAI风格的代理团队进行迭代优化，将合同文本解析为结构化模式，生成Solidity代码，通过编译和安全检查进行自动质量评估，支持与基准实现的配对评估。

Result: 框架在五个维度上测量质量：功能完整性、变量保真度、状态机正确性、业务逻辑保真度和代码质量，生成包含完整溯源元数据的结构化工件，量化对齐度并识别系统性错误。

Conclusion: 该框架为智能合约合成质量的实证研究提供了可重复的基准，支持扩展到形式验证和合规性检查，有助于系统评估和改进LLM生成的智能合约质量。

Abstract: We present an end-to-end framework for systematic evaluation of LLM-generated smart contracts from natural-language specifications. The system parses contractual text into structured schemas, generates Solidity code, and performs automated quality assessment through compilation and security checks. Using CrewAI-style agent teams with iterative refinement, the pipeline produces structured artifacts with full provenance metadata. Quality is measured across five dimensions, including functional completeness, variable fidelity, state-machine correctness, business-logic fidelity, and code quality aggregated into composite scores. The framework supports paired evaluation against ground-truth implementations, quantifying alignment and identifying systematic error modes such as logic omissions and state transition inconsistencies. This provides a reproducible benchmark for empirical research on smart contract synthesis quality and supports extensions to formal verification and compliance checking.

</details>


### [107] [From Fluent to Verifiable: Claim-Level Auditability for Deep Research Agents](https://arxiv.org/abs/2602.13855)
*Razeen A Rasheed,Somnath Banerjee,Animesh Mukherjee,Rima Hazra*

Main category: cs.AI

TL;DR: 论文提出随着AI研究生成变得廉价，可审计性成为瓶颈，主张将声明级可审计性作为深度研究代理的核心设计目标，并引入AAR标准和语义溯源框架来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 随着深度研究代理能够快速生成科学报告，主要风险从孤立的事实错误转向科学风格输出中声明-证据链接的薄弱、缺失或误导性。可审计性成为研究生成廉价化后的关键瓶颈。

Method: 提出声明级可审计性作为核心设计目标，总结长期失败模式（目标漂移、瞬态约束、不可验证推理），引入AAR标准（溯源覆盖率、溯源健全性、矛盾透明度、审计工作量），并倡导语义溯源与协议化验证。

Result: 建立了可审计性作为可测试目标的框架，提出了具体的测量指标和语义溯源方法，为深度研究代理的可审计性提供了系统化的解决方案。

Conclusion: 随着研究生成成本降低，可审计性成为关键瓶颈，需要将声明级可审计性作为深度研究代理的一等设计目标，并通过语义溯源和协议化验证实现规模化部署。

Abstract: A deep research agent produces a fluent scientific report in minutes; a careful reader then tries to verify the main claims and discovers the real cost is not reading, but tracing: which sentence is supported by which passage, what was ignored, and where evidence conflicts. We argue that as research generation becomes cheap, auditability becomes the bottleneck, and the dominant risk shifts from isolated factual errors to scientifically styled outputs whose claim-evidence links are weak, missing, or misleading. This perspective proposes claim-level auditability as a first-class design and evaluation target for deep research agents, distills recurring long-horizon failure modes (objective drift, transient constraints, and unverifiable inference), and introduces the Auditable Autonomous Research (AAR) standard, a compact measurement framework that makes auditability testable via provenance coverage, provenance soundness, contradiction transparency, and audit effort. We then argue for semantic provenance with protocolized validation: persistent, queryable provenance graphs that encode claim--evidence relations (including conflicts) and integrate continuous validation during synthesis rather than after publication, with practical instrumentation patterns to support deployment at scale.

</details>


### [108] [Enabling Option Learning in Sparse Rewards with Hindsight Experience Replay](https://arxiv.org/abs/2602.13865)
*Gabriel Romio,Mateus Begnini Melchiades,Bruno Castro da Silva,Gabriel de Oliveira Ramos*

Main category: cs.AI

TL;DR: 提出MOC-2HER方法，通过双重目标重标记策略解决稀疏奖励多目标环境中分层强化学习的性能问题


<details>
  <summary>Details</summary>
Motivation: 现有分层强化学习方法（如Option-Critic和MOC）在稀疏奖励的多目标环境中表现不佳，特别是在物体操纵任务中，奖励依赖于物体到达目标而非智能体直接交互，导致智能体难以学习如何与物体互动

Method: 首先提出MOC-HER，将Hindsight Experience Replay机制集成到MOC框架中；然后引入2HER，创建两组虚拟目标：除了基于物体最终状态重标记目标（标准HER），还从智能体效应器位置生成目标，奖励智能体既与物体交互又完成任务

Result: 在机器人操纵环境中，MOC-2HER达到高达90%的成功率，而MOC和MOC-HER均低于11%，表明双重目标重标记策略在稀疏奖励多目标任务中的有效性

Conclusion: 提出的MOC-2HER方法通过双重目标重标记策略显著提升了分层强化学习在稀疏奖励多目标环境中的性能，特别是在物体操纵任务中，解决了智能体难以学习与物体交互的问题

Abstract: Hierarchical Reinforcement Learning (HRL) frameworks like Option-Critic (OC) and Multi-updates Option Critic (MOC) have introduced significant advancements in learning reusable options. However, these methods underperform in multi-goal environments with sparse rewards, where actions must be linked to temporally distant outcomes. To address this limitation, we first propose MOC-HER, which integrates the Hindsight Experience Replay (HER) mechanism into the MOC framework. By relabeling goals from achieved outcomes, MOC-HER can solve sparse reward environments that are intractable for the original MOC. However, this approach is insufficient for object manipulation tasks, where the reward depends on the object reaching the goal rather than on the agent's direct interaction. This makes it extremely difficult for HRL agents to discover how to interact with these objects. To overcome this issue, we introduce Dual Objectives Hindsight Experience Replay (2HER), a novel extension that creates two sets of virtual goals. In addition to relabeling goals based on the object's final state (standard HER), 2HER also generates goals from the agent's effector positions, rewarding the agent for both interacting with the object and completing the task. Experimental results in robotic manipulation environments show that MOC-2HER achieves success rates of up to 90%, compared to less than 11% for both MOC and MOC-HER. These results highlight the effectiveness of our dual objective relabeling strategy in sparse reward, multi-goal tasks.

</details>


### [109] [Ambient Physics: Training Neural PDE Solvers with Partial Observations](https://arxiv.org/abs/2602.13873)
*Harris Abdul Majid,Giannis Daras,Francesco Tudisco,Steven McDonagh*

Main category: cs.AI

TL;DR: 提出Ambient Physics框架，直接从部分观测数据学习PDE系数-解对的联合分布，无需完整观测数据训练，通过随机掩码已观测点实现监督学习。


<details>
  <summary>Details</summary>
Motivation: 在许多科学场景中，获取PDE系数和解的完整观测数据可能昂贵、危险甚至不可能。现有基于扩散的方法需要完整观测数据进行训练，限制了其在部分观测场景中的应用。

Method: 引入Ambient Physics框架，核心思想是随机掩码已观测测量值的子集并对其进行监督学习，使模型无法区分"真正未观测"和"人工未观测"点，从而必须在所有位置产生合理预测。还发现"单点转换"现象：掩码单个已观测点即可实现跨架构和测量模式的学习。

Result: 实现了最先进的重建性能，与先前基于扩散的方法相比，平均总体误差减少62.51%，同时使用125倍更少的函数评估。框架在完整观测数据不可用的科学场景中具有重要应用价值。

Conclusion: Ambient Physics框架能够直接从部分观测数据学习PDE系数-解对的联合分布，无需完整观测数据，为完整观测不可用的科学场景提供了有效的解决方案。

Abstract: In many scientific settings, acquiring complete observations of PDE coefficients and solutions can be expensive, hazardous, or impossible. Recent diffusion-based methods can reconstruct fields given partial observations, but require complete observations for training. We introduce Ambient Physics, a framework for learning the joint distribution of coefficient-solution pairs directly from partial observations, without requiring a single complete observation. The key idea is to randomly mask a subset of already-observed measurements and supervise on them, so the model cannot distinguish "truly unobserved" from "artificially unobserved", and must produce plausible predictions everywhere. Ambient Physics achieves state-of-the-art reconstruction performance. Compared with prior diffusion-based methods, it achieves a 62.51$\%$ reduction in average overall error while using 125$\times$ fewer function evaluations. We also identify a "one-point transition": masking a single already-observed point enables learning from partial observations across architectures and measurement patterns. Ambient Physics thus enables scientific progress in settings where complete observations are unavailable.

</details>


### [110] [VSAL: A Vision Solver with Adaptive Layouts for Graph Property Detection](https://arxiv.org/abs/2602.13880)
*Jiahao Xie,Guangmo Tong*

Main category: cs.AI

TL;DR: VSAL是一个基于视觉的图属性检测框架，通过自适应布局生成器动态创建信息丰富的图可视化，提升检测性能


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉的图属性检测方法依赖固定的图布局，限制了管道的表达能力，需要能够根据具体实例动态生成信息丰富可视化的方法

Method: 提出VSAL框架，包含自适应布局生成器，能够为每个图实例动态生成信息丰富的可视化布局，从而改进图属性检测

Result: 在哈密顿环、平面性、无爪性、树检测等多种任务上，VSAL优于最先进的基于视觉方法

Conclusion: 自适应布局生成器能够显著提升基于视觉的图属性检测性能，VSAL框架为解决现有方法的布局限制提供了有效方案

Abstract: Graph property detection aims to determine whether a graph exhibits certain structural properties, such as being Hamiltonian. Recently, learning-based approaches have shown great promise by leveraging data-driven models to detect graph properties efficiently. In particular, vision-based methods offer a visually intuitive solution by processing the visualizations of graphs. However, existing vision-based methods rely on fixed visual graph layouts, and therefore, the expressiveness of their pipeline is restricted. To overcome this limitation, we propose VSAL, a vision-based framework that incorporates an adaptive layout generator capable of dynamically producing informative graph visualizations tailored to individual instances, thereby improving graph property detection. Extensive experiments demonstrate that VSAL outperforms state-of-the-art vision-based methods on various tasks such as Hamiltonian cycle, planarity, claw-freeness, and tree detection.

</details>


### [111] [Diagnosing Pathological Chain-of-Thought in Reasoning Models](https://arxiv.org/abs/2602.13904)
*Manqing Liu,David Williams-King,Ida Caspary,Linh Le,Hannes Whittingham,Puria Radmard,Cameron Tice,Edward James Young*

Main category: cs.AI

TL;DR: 本文提出了一个评估链式思维推理病理学的实用工具包，包括后合理化、编码推理和内化推理三种病理类型，并开发了简单、计算成本低且任务无关的度量标准。


<details>
  <summary>Details</summary>
Motivation: 链式思维推理是现代LLM架构的基础，也是AI安全的关键干预点。然而，CoT推理可能存在病理学问题，这些病理阻碍了其在监控中的有效性。先前研究已识别出三种不同的病理类型，需要更好的理解和区分方法。

Method: 创建了一套具体的度量标准，这些标准简单易实现、计算成本低且任务无关。为了验证方法，开发了专门训练以展示特定CoT病理的模型生物。

Result: 开发了一个实用的工具包来评估CoT病理，包括后合理化、编码推理和内化推理三种病理类型的度量方法。这些度量标准能够有效区分不同的病理模式。

Conclusion: 该工作为评估链式思维推理病理学提供了实用的工具包，对训练时监控具有直接意义，有助于提高LLM推理的安全性和可解释性。

Abstract: Chain-of-thought (CoT) reasoning is fundamental to modern LLM architectures and represents a critical intervention point for AI safety. However, CoT reasoning may exhibit failure modes that we note as pathologies, which prevent it from being useful for monitoring. Prior work has identified three distinct pathologies: post-hoc rationalization, where models generate plausible explanations backwards from predetermined answers; encoded reasoning, where intermediate steps conceal information within seemingly interpretable text; and internalized reasoning, where models replace explicit reasoning with meaningless filler tokens while computing internally. To better understand and discriminate between these pathologies, we create a set of concrete metrics that are simple to implement, computationally inexpensive, and task-agnostic. To validate our approach, we develop model organisms deliberately trained to exhibit specific CoT pathologies. Our work provides a practical toolkit for assessing CoT pathologies, with direct implications for training-time monitoring.

</details>


### [112] [From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design](https://arxiv.org/abs/2602.13912)
*Sha Li,Stefano Petrangeli,Yu Shen,Xiang Chen*

Main category: cs.AI

TL;DR: LaySPA是一个强化学习框架，让大语言模型具备显式可解释的空间推理能力，用于内容感知的图形布局设计，通过结构化文本空间环境和多目标空间评判来优化布局策略。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在空间推理方面的局限性以及设计决策缺乏透明度的问题，使LLM能够进行内容感知的图形布局设计。

Method: 将布局设计重新定义为结构化文本空间环境上的策略学习问题，该环境显式编码画布几何、元素属性和元素间关系。使用多目标空间评判（几何有效性、关系一致性、美学一致性）和相对组优化来训练布局策略。

Result: LaySPA提高了结构有效性和视觉质量，超越了更大的专有LLM，性能与专门的SOTA布局生成器相当，同时需要更少的标注样本和更低的延迟。

Conclusion: LaySPA成功地为大语言模型配备了显式可解释的空间推理能力，实现了透明可控的设计决策，在图形布局设计任务中表现出色。

Abstract: We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs' limited spatial reasoning and the lack of opacity in design decision making. Instead of operating at the pixel level, we reformulate layout design as a policy learning problem over a structured textual spatial environment that explicitly encodes canvas geometry, element attributes, and inter-element relationships. LaySPA produces dual-level outputs comprising interpretable reasoning traces and structured layout specifications, enabling transparent and controllable design decision making. Layout design policy is optimized via a multi-objective spatial critique that decomposes layout quality into geometric validity, relational coherence, and aesthetic consistency, and is trained using relative group optimization to stabilize learning in open-ended design spaces. Experiments demonstrate that LaySPA improves structural validity and visual quality, outperforming larger proprietary LLMs and achieving performance comparable to specialized SOTA layout generators while requiring fewer annotated samples and reduced latency.

</details>


### [113] [HyMem: Hybrid Memory Architecture with Dynamic Retrieval Scheduling](https://arxiv.org/abs/2602.13933)
*Xiaochen Zhao,Kaikai Wang,Xiaowen Zhang,Chen Yao,Aili Wang*

Main category: cs.AI

TL;DR: HyMem提出了一种混合内存架构，通过多粒度内存表示实现动态按需调度，在长对话中平衡效率与性能，相比全上下文减少92.6%计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在短文本中表现良好，但在长对话中因内存管理效率低下而表现不佳。现有方法面临效率与效果的根本权衡：内存压缩可能丢失复杂推理所需的关键细节，而保留原始文本则对简单查询引入不必要的计算开销。核心问题在于单一内存表示和静态检索机制无法模拟人类灵活主动的记忆调度能力。

Method: HyMem采用混合内存架构，包含双粒度存储方案和动态两级检索系统：轻量级模块构建摘要级上下文以高效生成响应，而基于LLM的深度模块仅针对复杂查询选择性激活，并通过反思机制进行迭代推理优化。

Result: 在LOCOMO和LongMemEval基准测试中，HyMem表现出色，优于全上下文方法，同时将计算成本降低92.6%，在长期内存管理中实现了效率与性能的最优平衡。

Conclusion: HyMem通过认知经济原则启发的混合内存架构，成功解决了LLM代理在长对话中的内存管理挑战，实现了动态按需调度，在保持高性能的同时显著降低了计算成本。

Abstract: Large language model (LLM) agents demonstrate strong performance in short-text contexts but often underperform in extended dialogues due to inefficient memory management. Existing approaches face a fundamental trade-off between efficiency and effectiveness: memory compression risks losing critical details required for complex reasoning, while retaining raw text introduces unnecessary computational overhead for simple queries. The crux lies in the limitations of monolithic memory representations and static retrieval mechanisms, which fail to emulate the flexible and proactive memory scheduling capabilities observed in humans, thus struggling to adapt to diverse problem scenarios. Inspired by the principle of cognitive economy, we propose HyMem, a hybrid memory architecture that enables dynamic on-demand scheduling through multi-granular memory representations. HyMem adopts a dual-granular storage scheme paired with a dynamic two-tier retrieval system: a lightweight module constructs summary-level context for efficient response generation, while an LLM-based deep module is selectively activated only for complex queries, augmented by a reflection mechanism for iterative reasoning refinement. Experiments show that HyMem achieves strong performance on both the LOCOMO and LongMemEval benchmarks, outperforming full-context while reducing computational cost by 92.6\%, establishing a state-of-the-art balance between efficiency and performance in long-term memory management.

</details>


### [114] [Statistical Early Stopping for Reasoning Models](https://arxiv.org/abs/2602.13935)
*Yangxinyu Xie,Tao Wang,Soham Mallick,Yan Sun,Georgy Noarov,Mengxin Yu,Tanwi Mallick,Weijie J. Su,Edgar Dobriban*

Main category: cs.AI

TL;DR: 论文提出两种基于统计原理的早期停止方法，通过监控生成过程中的不确定性信号来减少LLM在推理时过度思考的问题，提高效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: LLM在推理能力上虽有显著提升，但有时会过度思考，特别是在面对不确定、表述不清或模糊的查询时，会生成不必要的推理步骤。这既降低了效率，也影响了可靠性。

Method: 提出两种统计原理的早期停止方法：1) 参数化方法：将不确定性关键词的出现间隔时间建模为更新过程，并应用序列测试进行停止决策；2) 非参数化方法：提供有限样本保证，确保在表述清晰的查询上不会过早停止的概率。

Result: 在多个领域和模型的推理任务上进行实证评估，结果表明基于不确定性的早期停止方法能够显著提高LLM推理的效率和可靠性，尤其在数学推理任务上观察到特别显著的改进。

Conclusion: 通过监控不确定性信号实现早期停止是解决LLM过度思考问题的有效方法，能够平衡推理质量与效率，特别是在数学推理等需要精确性的任务中表现出色。

Abstract: While LLMs have seen substantial improvement in reasoning capabilities, they also sometimes overthink, generating unnecessary reasoning steps, particularly under uncertainty, given ill-posed or ambiguous queries. We introduce statistically principled early stopping methods that monitor uncertainty signals during generation to mitigate this issue. Our first approach is parametric: it models inter-arrival times of uncertainty keywords as a renewal process and applies sequential testing for stopping. Our second approach is nonparametric and provides finite-sample guarantees on the probability of halting too early on well-posed queries. We conduct empirical evaluations on reasoning tasks across several domains and models. Our results indicate that uncertainty-aware early stopping can improve both efficiency and reliability in LLM reasoning, and we observe especially significant gains for math reasoning.

</details>


### [115] [A Generalizable Physics-guided Causal Model for Trajectory Prediction in Autonomous Driving](https://arxiv.org/abs/2602.13936)
*Zhenyu Zong,Yuchen Wang,Haohong Lin,Lu Gan,Huajie Shao*

Main category: cs.AI

TL;DR: 提出PCM模型，通过解耦场景编码器和因果ODE解码器，结合物理动力学模型与领域不变特征，实现自动驾驶轨迹预测的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中的轨迹预测需要良好的零样本泛化能力，以应对未见过的交通场景。由于动力学规律在不同领域具有一致性，作者希望利用领域不变知识来提升零样本轨迹预测性能。

Method: 提出物理引导的因果模型(PCM)，包含两个核心组件：1)解耦场景编码器，采用基于干预的解耦方法提取领域不变特征；2)因果ODE解码器，使用因果注意力机制将动力学模型与上下文信息有效整合。

Result: 在真实世界自动驾驶数据集上的实验表明，该方法在未见城市中表现出优越的零样本泛化性能，显著优于竞争基线方法。

Conclusion: PCM模型通过结合物理动力学和领域不变特征学习，有效解决了轨迹预测的零样本泛化问题，为自动驾驶系统在未知环境中的安全运行提供了重要支持。

Abstract: Trajectory prediction for traffic agents is critical for safe autonomous driving. However, achieving effective zero-shot generalization in previously unseen domains remains a significant challenge. Motivated by the consistent nature of kinematics across diverse domains, we aim to incorporate domain-invariant knowledge to enhance zero-shot trajectory prediction capabilities. The key challenges include: 1) effectively extracting domain-invariant scene representations, and 2) integrating invariant features with kinematic models to enable generalized predictions. To address these challenges, we propose a novel generalizable Physics-guided Causal Model (PCM), which comprises two core components: a Disentangled Scene Encoder, which adopts intervention-based disentanglement to extract domain-invariant features from scenes, and a CausalODE Decoder, which employs a causal attention mechanism to effectively integrate kinematic models with meaningful contextual information. Extensive experiments on real-world autonomous driving datasets demonstrate our method's superior zero-shot generalization performance in unseen cities, significantly outperforming competitive baselines. The source code is released at https://github.com/ZY-Zong/Physics-guided-Causal-Model.

</details>


### [116] [Neuromem: A Granular Decomposition of the Streaming Lifecycle in External Memory for LLMs](https://arxiv.org/abs/2602.13967)
*Ruicheng Zhang,Xinyi Li,Tianyi Xu,Shuhao Zhang,Xiaofei Liao,Hai Jin*

Main category: cs.AI

TL;DR: Neuromem是一个用于评估外部记忆模块在流式场景下的测试平台，将记忆生命周期分解为五个维度，发现随着记忆增长性能下降，时间相关查询最具挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有外部记忆模块评估大多基于静态设置（离线构建记忆、固定状态查询），而实际应用中是流式场景：新事实持续到达、插入与检索交错进行、记忆状态在服务查询时不断演化。在这种动态环境下，准确性和成本由完整的记忆生命周期决定。

Method: 提出Neuromem测试平台，在交错插入-检索协议下评估外部记忆模块，将记忆生命周期分解为五个维度：记忆数据结构、归一化策略、巩固策略、查询制定策略和上下文集成机制。使用LOCOMO、LONGMEMEVAL和MEMORYAGENTBENCH三个代表性数据集，在共享服务栈中评估可互换的变体，报告token级F1分数和插入/检索延迟。

Result: 性能通常随着记忆在多轮中增长而下降，时间相关查询仍然是最具挑战性的类别。记忆数据结构很大程度上决定了可达到的质量边界，而激进的压缩和生成式集成机制主要在插入和检索之间转移成本，准确度提升有限。

Conclusion: Neuromem提供了一个系统化的框架来评估流式场景下的外部记忆模块，揭示了记忆数据结构的关键作用以及当前方法在动态记忆管理中的局限性，特别是时间相关查询的挑战性。

Abstract: Most evaluations of External Memory Module assume a static setting: memory is built offline and queried at a fixed state. In practice, memory is streaming: new facts arrive continuously, insertions interleave with retrievals, and the memory state evolves while the model is serving queries. In this regime, accuracy and cost are governed by the full memory lifecycle, which encompasses the ingestion, maintenance, retrieval, and integration of information into generation. We present Neuromem, a scalable testbed that benchmarks External Memory Modules under an interleaved insertion-and-retrieval protocol and decomposes its lifecycle into five dimensions including memory data structure, normalization strategy, consolidation policy, query formulation strategy, and context integration mechanism. Using three representative datasets LOCOMO, LONGMEMEVAL, and MEMORYAGENTBENCH, Neuromem evaluates interchangeable variants within a shared serving stack, reporting token-level F1 and insertion/retrieval latency. Overall, we observe that performance typically degrades as memory grows across rounds, and time-related queries remain the most challenging category. The memory data structure largely determines the attainable quality frontier, while aggressive compression and generative integration mechanisms mostly shift cost between insertion and retrieval with limited accuracy gain.

</details>


### [117] [Cognitive Chunking for Soft Prompts: Accelerating Compressor Learning via Block-wise Causal Masking](https://arxiv.org/abs/2602.13980)
*Guojie Liu,Yiqi Wang,Yanfeng Yang,Wenqi Fan,Songlei Jian,Jianfeng Zhang,Jie Yu*

Main category: cs.AI

TL;DR: 提出PIC方法，通过修改Transformer注意力掩码，将长上下文压缩为局部块记忆，降低压缩器训练难度，在多项任务中优于现有方法，尤其在高压缩比下表现突出。


<details>
  <summary>Details</summary>
Motivation: 长上下文会增加LLM推理延迟，现有软提示压缩方法需要捕获全局依赖且需要大量训练数据，受人类工作记忆分块机制启发，提出更高效的压缩方法。

Method: 提出并行迭代压缩（PIC），通过修改Transformer注意力掩码，将记忆标记的接收域限制在连续的局部块中，降低压缩器训练难度。

Result: 在多个下游任务中优于现有基线，高压缩比下表现尤其突出（64×压缩比时QA任务F1提升29.8%，EM提升40.7%），训练时间减少约40%。

Conclusion: PIC通过局部化压缩策略有效解决了长上下文压缩问题，在性能和训练效率上都显著优于现有方法，为LLM上下文压缩提供了新思路。

Abstract: Providing extensive context via prompting is vital for leveraging the capabilities of Large Language Models (LLMs). However, lengthy contexts significantly increase inference latency, as the computational cost of self-attention grows quadratically with sequence length. To mitigate this issue, context compression-particularly soft prompt compressio-has emerged as a widely studied solution, which converts long contexts into shorter memory embeddings via a trained compressor. Existing methods typically compress the entire context indiscriminately into a set of memory tokens, requiring the compressor to capture global dependencies and necessitating extensive pre-training data to learn effective patterns. Inspired by the chunking mechanism in human working memory and empirical observations of the spatial specialization of memory embeddings relative to original tokens, we propose Parallelized Iterative Compression (PIC). By simply modifying the Transformer's attention mask, PIC explicitly restricts the receptive field of memory tokens to sequential local chunks, thereby lowering the difficulty of compressor training. Experiments across multiple downstream tasks demonstrate that PIC consistently outperforms competitive baselines, with superiority being particularly pronounced in high compression scenarios (e.g., achieving relative improvements of 29.8\% in F1 score and 40.7\% in EM score on QA tasks at the $64\times$ compression ratio). Furthermore, PIC significantly expedites the training process. Specifically, when training the 16$\times$ compressor, it surpasses the peak performance of the competitive baseline while effectively reducing the training time by approximately 40\%.

</details>


### [118] [Bridging AI and Clinical Reasoning: Abductive Explanations for Alignment on Critical Symptoms](https://arxiv.org/abs/2602.13985)
*Belona Sonna,Alban Grastien*

Main category: cs.AI

TL;DR: 该论文提出使用形式化溯因解释来提升AI临床诊断的可信度，确保AI推理与临床框架对齐，同时保持预测准确性。


<details>
  <summary>Details</summary>
Motivation: AI在临床诊断中虽表现出色，但其推理常偏离结构化临床框架，导致信任度、可解释性和采用率受限。现有事后解释方法透明度有限且缺乏形式化保证，无法确保AI关注关键症状。

Method: 采用形式化溯因解释方法，提供对最小充分特征集的一致、有保证的推理。该方法使AI决策过程清晰可理解，并能与临床推理对齐。

Result: 该方法在保持预测准确性的同时，提供临床可操作的见解，建立了医疗诊断中可信AI的稳健框架。

Conclusion: 形式化溯因解释为解决AI临床诊断中的可解释性和信任问题提供了有效途径，能够确保AI推理与临床实践对齐，促进AI在医疗领域的可靠应用。

Abstract: Artificial intelligence (AI) has demonstrated strong potential in clinical diagnostics, often achieving accuracy comparable to or exceeding that of human experts. A key challenge, however, is that AI reasoning frequently diverges from structured clinical frameworks, limiting trust, interpretability, and adoption. Critical symptoms, pivotal for rapid and accurate decision-making, may be overlooked by AI models even when predictions are correct. Existing post hoc explanation methods provide limited transparency and lack formal guarantees. To address this, we leverage formal abductive explanations, which offer consistent, guaranteed reasoning over minimal sufficient feature sets. This enables a clear understanding of AI decision-making and allows alignment with clinical reasoning. Our approach preserves predictive accuracy while providing clinically actionable insights, establishing a robust framework for trustworthy AI in medical diagnosis.

</details>


### [119] [Prompt-Driven Low-Altitude Edge Intelligence: Modular Agents and Generative Reasoning](https://arxiv.org/abs/2602.14003)
*Jiahao You,Ziye Jia,Chao Dong,Qihui Wu*

Main category: cs.AI

TL;DR: 提出P2AECF框架，通过提示定义认知、基于代理的模块化执行和扩散控制推理规划，解决大模型在边缘部署的灵活性、效率和适应性挑战


<details>
  <summary>Details</summary>
Motivation: 大人工智能模型在边缘智能部署面临三个主要限制：1) 任务与特定模型绑定，缺乏灵活性；2) 计算和内存需求超出边缘设备能力；3) 静态推理管道难以适应实时任务变化

Method: 提出提示到代理边缘认知框架(P2AECF)，包含三个关键机制：1) 提示定义认知将任务意图解析为抽象、模型无关的表示；2) 基于代理的模块化执行使用轻量级可重用认知代理动态实例化任务；3) 扩散控制推理规划通过运行时反馈和系统上下文自适应构建和优化执行策略

Result: 通过低空智能网络用例展示了该框架能够为实时低空空中协作提供自适应、模块化和可扩展的边缘智能

Conclusion: P2AECF框架能够实现灵活、高效和自适应的边缘智能，解决了大模型在边缘部署的关键挑战

Abstract: The large artificial intelligence models (LAMs) show strong capabilities in perception, reasoning, and multi-modal understanding, and can enable advanced capabilities in low-altitude edge intelligence. However, the deployment of LAMs at the edge remains constrained by some fundamental limitations. First, tasks are rigidly tied to specific models, limiting the flexibility. Besides, the computational and memory demands of full-scale LAMs exceed the capacity of most edge devices. Moreover, the current inference pipelines are typically static, making it difficult to respond to real-time changes of tasks. To address these challenges, we propose a prompt-to-agent edge cognition framework (P2AECF), enabling the flexible, efficient, and adaptive edge intelligence. Specifically, P2AECF transforms high-level semantic prompts into executable reasoning workflows through three key mechanisms. First, the prompt-defined cognition parses task intent into abstract and model-agnostic representations. Second, the agent-based modular execution instantiates these tasks using lightweight and reusable cognitive agents dynamically selected based on current resource conditions. Third, the diffusion-controlled inference planning adaptively constructs and refines execution strategies by incorporating runtime feedback and system context. In addition, we illustrate the framework through a representative low-altitude intelligent network use case, showing its ability to deliver adaptive, modular, and scalable edge intelligence for real-time low-altitude aerial collaborations.

</details>


### [120] [FloCA: Towards Faithful and Logically Consistent Flowchart Reasoning](https://arxiv.org/abs/2602.14035)
*Jinzi Zou,Bolin Wang,Liang Li,Shuo Zhang,Nuo Xu,Junzhou Zhao*

Main category: cs.AI

TL;DR: FloCA是一个零样本流程图导向对话代理，使用LLM进行意图理解和响应生成，同时将流程图推理委托给外部工具执行拓扑约束图执行，确保跨对话轮次的忠实和逻辑一致的节点转换。


<details>
  <summary>Details</summary>
Motivation: 流程图导向对话系统需要遵循特定领域流程图指导用户完成多轮决策或操作流程。现有LLM方法存在两个限制：1)缺乏明确机制来表示和推理流程图拓扑结构；2)容易产生幻觉，导致不忠实的流程图推理。

Method: 提出FloCA框架，将LLM用于意图理解和响应生成，同时将流程图推理委托给外部工具执行拓扑约束图执行。引入基于LLM的用户模拟器和五个新指标的评估框架。

Result: 在FLODIAL和PFDial数据集上的实验表明，现有LLM方法存在瓶颈，而FloCA在推理准确性和交互效率方面表现出优越性。

Conclusion: FloCA通过将LLM与外部流程图推理工具结合，解决了LLM在流程图导向对话中的局限性，实现了忠实且逻辑一致的节点转换，为流程图推理提供了有效解决方案。

Abstract: Flowchart-oriented dialogue (FOD) systems aim to guide users through multi-turn decision-making or operational procedures by following a domain-specific flowchart to achieve a task goal. In this work, we formalize flowchart reasoning in FOD as grounding user input to flowchart nodes at each dialogue turn while ensuring node transition is consistent with the correct flowchart path. Despite recent advances of LLMs in task-oriented dialogue systems, adapting them to FOD still faces two limitations: (1) LLMs lack an explicit mechanism to represent and reason over flowchart topology, and (2) they are prone to hallucinations, leading to unfaithful flowchart reasoning. To address these limitations, we propose FloCA, a zero-shot flowchart-oriented conversational agent. FloCA uses an LLM for intent understanding and response generation while delegating flowchart reasoning to an external tool that performs topology-constrained graph execution, ensuring faithful and logically consistent node transitions across dialogue turns. We further introduce an evaluation framework with an LLM-based user simulator and five new metrics covering reasoning accuracy and interaction efficiency. Extensive experiments on FLODIAL and PFDial datasets highlight the bottlenecks of existing LLM-based methods and demonstrate the superiority of FloCA. Our codes are available at https://github.com/Jinzi-Zou/FloCA-flowchart-reasoning.

</details>


### [121] [Choosing How to Remember: Adaptive Memory Structures for LLM Agents](https://arxiv.org/abs/2602.14038)
*Mingfei Lu,Mengjia Wu,Feng Liu,Jiawei Xu,Weikai Li,Haoyang Wang,Zhengdong Hu,Ying Ding,Yizhou Sun,Jie Lu,Yi Zhang*

Main category: cs.AI

TL;DR: FluxMem是一个为LLM智能体设计的自适应记忆组织框架，通过多结构选择和概率融合门实现上下文感知的记忆管理，在长时程交互任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有智能体记忆系统存在两个关键问题：采用一刀切的记忆结构，且未将记忆结构选择建模为上下文自适应决策，这限制了处理异构交互模式的能力并导致性能不佳。

Method: 提出FluxMem统一框架，为智能体配备多种互补记忆结构，基于交互级特征显式学习选择结构（使用下游响应质量和记忆利用率的离线监督）。引入三级记忆层次结构和基于Beta混合模型的概率融合门进行分布感知的记忆融合，替代脆弱的相似度阈值。

Result: 在两个长时程基准测试PERSONAMEM和LoCoMo上，平均分别提升9.18%和6.14%。

Conclusion: FluxMem通过自适应记忆组织解决了现有记忆系统的局限性，在长时程交互任务中实现了显著性能提升，为LLM智能体的记忆管理提供了更灵活有效的解决方案。

Abstract: Memory is critical for enabling large language model (LLM) based agents to maintain coherent behavior over long-horizon interactions. However, existing agent memory systems suffer from two key gaps: they rely on a one-size-fits-all memory structure and do not model memory structure selection as a context-adaptive decision, limiting their ability to handle heterogeneous interaction patterns and resulting in suboptimal performance. We propose a unified framework, FluxMem, that enables adaptive memory organization for LLM agents. Our framework equips agents with multiple complementary memory structures. It explicitly learns to select among these structures based on interaction-level features, using offline supervision derived from downstream response quality and memory utilization. To support robust long-horizon memory evolution, we further introduce a three-level memory hierarchy and a Beta Mixture Model-based probabilistic gate for distribution-aware memory fusion, replacing brittle similarity thresholds. Experiments on two long-horizon benchmarks, PERSONAMEM and LoCoMo, demonstrate that our method achieves average improvements of 9.18% and 6.14%.

</details>


### [122] [REAL: Resolving Knowledge Conflicts in Knowledge-Intensive Visual Question Answering via Reasoning-Pivot Alignment](https://arxiv.org/abs/2602.14065)
*Kai Ye,Xianwei Mao,Sheng Zhou,Zirui Shao,Ye Mo,Liangliang Liu,Haikuan Huang,Bin Li,Jiajun Bu*

Main category: cs.AI

TL;DR: 提出REAL框架，通过推理枢纽概念解决知识密集型VQA中的知识冲突问题，包含RPA-SFT训练冲突检测器和RPGD解码策略，在多个基准上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 知识密集型视觉问答（KI-VQA）经常因开放域检索的固有局限性而遭受严重知识冲突。现有范式缺乏可泛化的冲突检测机制和内部模型约束机制来处理冲突证据。

Method: 提出REAL框架，核心是推理枢纽概念——推理链中强调知识连接的原子单元（节点或边）。包含：1）推理枢纽感知的监督微调（RPA-SFT），通过将冲突与枢纽提取对齐来训练可泛化的判别器；2）推理枢纽引导的解码（RPGD），利用这些枢纽进行有针对性的冲突缓解的内部模型解码策略。

Result: 在多个基准上的广泛实验表明，REAL显著提高了判别准确性，并实现了最先进的性能，验证了枢纽驱动解决范式的有效性。

Conclusion: REAL框架通过推理枢纽概念有效解决了KI-VQA中的知识冲突问题，提出的RPA-SFT和RPGD方法为知识密集型任务中的冲突处理提供了新的解决方案。

Abstract: Knowledge-intensive Visual Question Answering (KI-VQA) frequently suffers from severe knowledge conflicts caused by the inherent limitations of open-domain retrieval. However, existing paradigms face critical limitations due to the lack of generalizable conflict detection and intra-model constraint mechanisms to handle conflicting evidence. To address these challenges, we propose the REAL (Reasoning-Pivot Alignment) framework centered on the novel concept of the Reasoning-Pivot. Distinct from reasoning steps that prioritize internal self-derivation, a reasoning-pivot serves as an atomic unit (node or edge) in the reasoning chain that emphasizes knowledge linkage, and it typically relies on external evidence to complete the reasoning. Supported by our constructed REAL-VQA dataset, our approach integrates Reasoning-Pivot Aware SFT (RPA-SFT) to train a generalizable discriminator by aligning conflicts with pivot extraction, and employs Reasoning-Pivot Guided Decoding (RPGD), an intra-model decoding strategy that leverages these pivots for targeted conflict mitigation. Extensive experiments across diverse benchmarks demonstrate that REAL significantly enhances discrimination accuracy and achieves state-of-the-art performance, validating the effectiveness of our pivot-driven resolution paradigm.

</details>


### [123] [Plan-MCTS: Plan Exploration for Action Exploitation in Web Navigation](https://arxiv.org/abs/2602.14083)
*Weiming Zhang,Jihong Wang,Jiamu Zhou,Qingyao Li,Xinbei Ma,Congmin Zheng,Xingyu Lou,Weiwen Liu,Zhuosheng Zhang,Jun Wang,Yong Yu,Weinan Zhang*

Main category: cs.AI

TL;DR: Plan-MCTS：通过将网页导航探索转移到语义计划空间，解决稀疏路径和噪声上下文问题，实现更高效的任务完成


<details>
  <summary>Details</summary>
Motivation: 现有基于树搜索的LLM自主代理在网页导航中面临两个关键挑战：1）稀疏有效路径导致探索效率低下；2）噪声上下文稀释准确的状态感知

Method: 提出Plan-MCTS框架，将探索转移到语义计划空间，通过解耦战略规划和执行落地，将稀疏动作空间转换为密集计划树，并将噪声上下文提炼为抽象语义历史

Result: 在WebArena上的广泛实验表明，Plan-MCTS实现了最先进的性能，超越了现有方法，具有更高的任务有效性和搜索效率

Conclusion: Plan-MCTS通过语义计划空间重构网页导航，有效解决了稀疏路径和噪声上下文问题，显著提升了自主代理在复杂网页导航任务中的性能

Abstract: Large Language Models (LLMs) have empowered autonomous agents to handle complex web navigation tasks. While recent studies integrate tree search to enhance long-horizon reasoning, applying these algorithms in web navigation faces two critical challenges: sparse valid paths that lead to inefficient exploration, and a noisy context that dilutes accurate state perception. To address this, we introduce Plan-MCTS, a framework that reformulates web navigation by shifting exploration to a semantic Plan Space. By decoupling strategic planning from execution grounding, it transforms sparse action space into a Dense Plan Tree for efficient exploration, and distills noisy contexts into an Abstracted Semantic History for precise state awareness. To ensure efficiency and robustness, Plan-MCTS incorporates a Dual-Gating Reward to strictly validate both physical executability and strategic alignment and Structural Refinement for on-policy repair of failed subplans. Extensive experiments on WebArena demonstrate that Plan-MCTS achieves state-of-the-art performance, surpassing current approaches with higher task effectiveness and search efficiency.

</details>


### [124] [GUI-GENESIS: Automated Synthesis of Efficient Environments with Verifiable Rewards for GUI Agent Post-Training](https://arxiv.org/abs/2602.14093)
*Yuan Cao,Dezhi Ran,Mengzhou Wu,Yuzhe Guo,Xin Chen,Ang Li,Gang Cao,Gong Zhi,Hao Yu,Linyi Li,Wei Yang,Tao Xie*

Main category: cs.AI

TL;DR: GUI-GENESIS：首个自动合成高效GUI训练环境的框架，通过代码原生奖励和轻量级Web环境解决真实应用训练中的延迟、可复现性和奖励噪声问题。


<details>
  <summary>Details</summary>
Motivation: 在真实应用中训练GUI代理存在高延迟、可复现性差、依赖噪声视觉代理的不可验证奖励等问题，阻碍了代理的泛化和长时程规划能力发展。

Method: 使用多模态代码模型将真实应用重构为轻量级Web环境，并配备代码原生奖励（可执行断言），提供确定性奖励信号并消除视觉估计噪声。

Result: GUI-GENESIS将环境延迟降低10倍，每个epoch节省超过28,000美元成本。使用该框架训练的代理在真实任务上比基础模型提升14.54%，比真实世界RL基线提升3.27%。

Conclusion: 该框架为GUI代理训练提供了高效、可验证的解决方案，并发现模型能够合成自己尚无法解决的环境，为自我改进代理开辟了路径。

Abstract: Post-training GUI agents in interactive environments is critical for developing generalization and long-horizon planning capabilities. However, training on real-world applications is hindered by high latency, poor reproducibility, and unverifiable rewards relying on noisy visual proxies. To address the limitations, we present GUI-GENESIS, the first framework to automatically synthesize efficient GUI training environments with verifiable rewards. GUI-GENESIS reconstructs real-world applications into lightweight web environments using multimodal code models and equips them with code-native rewards, executable assertions that provide deterministic reward signals and eliminate visual estimation noise. Extensive experiments show that GUI-GENESIS reduces environment latency by 10 times and costs by over $28,000 per epoch compared to training on real applications. Notably, agents trained with GUI-GENESIS outperform the base model by 14.54% and even real-world RL baselines by 3.27% on held-out real-world tasks. Finally, we observe that models can synthesize environments they cannot yet solve, highlighting a pathway for self-improving agents.

</details>


### [125] [NEST: Nascent Encoded Steganographic Thoughts](https://arxiv.org/abs/2602.14095)
*Artem Karpov*

Main category: cs.AI

TL;DR: 研究评估大型语言模型隐藏推理过程的能力，发现当前模型在复杂数学任务上尚无法有效隐藏推理，但在简化任务中已展现初步能力，需持续监控此类风险。


<details>
  <summary>Details</summary>
Motivation: 监控链式思维推理是LLM代理的基础安全技术，但如果模型学会隐藏推理过程，这种监控就会失效。研究旨在评估模型隐藏秘密推理的能力，为风险评估和部署策略提供依据。

Method: 系统评估28个模型（从历史版本到前沿模型）的隐写能力，测量监控规避、拒绝率、编码保真度和隐藏任务准确性，在四个数据集上比较隐写藏头诗与普通推理和填充标记基线。

Result: 当前模型尚无法在复杂数学和算术任务中维持隐藏推理，但在简化计数实验中，Claude Opus 4.5在隐藏任务上达到92%准确率，显示初步能力。GPT-5.2在罕见情况下（<1%）会同时拒绝和遵守隐写指令。

Conclusion: 研究强调了持续评估隐写风险的必要性，提供了预先检测和预防隐藏推理的方法论，这些隐藏推理可能助长未对齐的阴谋和欺骗行为。

Abstract: Monitoring chain-of-thought (CoT) reasoning is a foundational safety technique for large language model (LLM) agents; however, this oversight is compromised if models learn to conceal their reasoning. We explore the potential for steganographic CoT -- where models hide secret reasoning within innocuous text -- to inform risk assessment and deployment policies. We systematically evaluate the limits of steganographic capabilities across 28 models, ranging from past generations to the current frontier. We measure monitor evasion, refusal rates, encoding fidelity, and hidden task accuracy across four datasets, comparing steganographic acrostics against plain reasoning and filler-token baselines. We find that current models cannot yet sustain hidden reasoning for complex math and arithmetic tasks. However, in a simplified counting experiment, Claude Opus 4.5 achieved 92% accuracy on the hidden task, demonstrating nascent capability. Notably, in rare cases (<1%), GPT-5.2 might refuse steganographic instructions while simultaneously complying with them. Our findings underscore the need for continuous evaluation of steganographic risks. This study provides a methodology to preemptively detect and prevent hidden reasoning that might empower misaligned scheming and deceptive behavior.

</details>


### [126] [Algebraic Quantum Intelligence: A New Framework for Reproducible Machine Creativity](https://arxiv.org/abs/2602.14130)
*Kazuo Yano,Jonghyeok Lee,Tae Ishitomi,Hironobu Kawaguchi,Akira Koyama,Masakuni Ota,Yuki Ota,Nobuo Sato,Keita Shimada,Sho Takematsu,Ayaka Tobinai,Satomi Tsuji,Kazunori Yanagi,Keiko Yano,Manabu Harada,Yuki Matsuda,Kazunori Matsumoto,Kenichi Matsumura,Hamae Matsuo,Yumi Miyazaki,Kotaro Murai,Tatsuya Ohshita,Marie Seki,Shun Tanoue,Tatsuki Terakado,Yuko Ichimaru,Mirei Saito,Akihiro Otsuka,Koji Ara*

Main category: cs.AI

TL;DR: 提出代数量子智能(AQI)框架，通过非交换代数结构扩展LLMs的语义空间，提升机器创造力


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在生成流畅文本方面表现出色，但在真正创造性输出方面存在局限。这种局限源于LLMs的结构特性：当提供丰富上下文时，未来生成空间变得高度受限，生成过程近乎确定性

Method: 提出代数量子智能(AQI)计算框架，采用受量子理论启发的非交换代数结构，实现顺序依赖、干涉和不确定性等特性。语义状态表示为希尔伯特空间中的向量，演化由非交换算子计算的C值控制。具体实现中扩展了基于Transformer的LLM，添加了600多个专用算子

Result: 在涵盖十个领域的创造性推理基准测试中，AQI始终优于强基线模型，获得统计显著改进并降低跨领域方差。该架构已在真实企业环境中部署

Conclusion: 非交换代数动力学可作为机器创造性的实用且可复现基础，代数量子智能框架能有效扩展LLMs的语义空间，提升创造性输出能力

Abstract: Large language models (LLMs) have achieved remarkable success in generating fluent and contextually appropriate text; however, their capacity to produce genuinely creative outputs remains limited. This paper posits that this limitation arises from a structural property of contemporary LLMs: when provided with rich context, the space of future generations becomes strongly constrained, and the generation process is effectively governed by near-deterministic dynamics. Recent approaches such as test-time scaling and context adaptation improve performance but do not fundamentally alter this constraint. To address this issue, we propose Algebraic Quantum Intelligence (AQI) as a computational framework that enables systematic expansion of semantic space. AQI is formulated as a noncommutative algebraic structure inspired by quantum theory, allowing properties such as order dependence, interference, and uncertainty to be implemented in a controlled and designable manner. Semantic states are represented as vectors in a Hilbert space, and their evolution is governed by C-values computed from noncommutative operators, thereby ensuring the coexistence and expansion of multiple future semantic possibilities. In this study, we implement AQI by extending a transformer-based LLM with more than 600 specialized operators. We evaluate the resulting system on creative reasoning benchmarks spanning ten domains under an LLM-as-a-judge protocol. The results show that AQI consistently outperforms strong baseline models, yielding statistically significant improvements and reduced cross-domain variance. These findings demonstrate that noncommutative algebraic dynamics can serve as a practical and reproducible foundation for machine creativity. Notably, this architecture has already been deployed in real-world enterprise environments.

</details>


### [127] [Process-Supervised Multi-Agent Reinforcement Learning for Reliable Clinical Reasoning](https://arxiv.org/abs/2602.14160)
*Chaeeun Lee,T. Michael Yates,Pasquale Minervini,T. Ian Simpson*

Main category: cs.AI

TL;DR: 提出了一种基于工具代理的强化学习框架，用于基因-疾病有效性评估任务，通过过程级监督和分层多智能体系统，在提高结果准确性的同时确保推理过程符合临床标准。


<details>
  <summary>Details</summary>
Motivation: 临床决策需要基于异质证据进行细致推理并提供可追溯的论证。现有的LLM多智能体系统主要优化结果准确性，而忽视了符合临床标准的过程基础推理。基因-疾病有效性评估是一个典型案例，专家需要综合多种生物医学证据来确定基因与疾病的因果关系。

Method: 引入工具代理强化学习框架，包含两个目标：(1) 过程级监督确保推理遵循有效的临床路径；(2) 通过分层多智能体系统实现高效协调。使用GRPO训练的Qwen3-4B作为监督代理，在ClinGen数据集上进行评估。

Result: 仅使用结果奖励时，多智能体系统将最终结果准确率从基础模型的0.195提升到0.732，但过程对齐性较差（F1=0.392）。同时使用过程和结果奖励时，多智能体系统实现了更高的结果准确率（0.750）并显著提高了过程保真度（F1=0.520）。

Conclusion: 该研究证明了在临床决策支持系统中，同时优化结果准确性和过程对齐性的重要性。通过过程级监督和分层多智能体协调，可以在保持高准确性的同时确保推理过程符合临床标准，为基因-疾病有效性评估等复杂临床任务提供了有效解决方案。

Abstract: Clinical decision-making requires nuanced reasoning over heterogeneous evidence and traceable justifications. While recent LLM multi-agent systems (MAS) show promise, they largely optimise for outcome accuracy while overlooking process-grounded reasoning aligned with clinical standards. One critical real-world case of this is gene-disease validity curation, where experts must determine whether a gene is causally implicated in a disease by synthesising diverse biomedical evidence. We introduce an agent-as-tool reinforcement learning framework for this task with two objectives: (i) process-level supervision to ensure reasoning follows valid clinical pathways, and (ii) efficient coordination via a hierarchical multi-agent system. Our evaluation on the ClinGen dataset shows that with outcome-only rewards, MAS with a GRPO-trained Qwen3-4B supervisor agent substantially improves final outcome accuracy from 0.195 with a base model supervisor to 0.732, but results in poor process alignment (0.392 F1). Conversely, with process + outcome rewards, MAS with GRPO-trained supervisor achieves higher outcome accuracy (0.750) while significantly improving process fidelity to 0.520 F1. Our code is available at https://github.com/chaeeunlee-io/GeneDiseaseCurationAgents.

</details>


### [128] [Text Before Vision: Staged Knowledge Injection Matters for Agentic RLVR in Ultra-High-Resolution Remote Sensing Understanding](https://arxiv.org/abs/2602.14225)
*Fengxiang Wang,Mingshuo Chen,Yueying Li,Yajie Yang,Yuhao Zhou,Di Wang,Yifan Zhang,Haoyu Wang,Haiyan Zhao,Hongda Sun,Long Lan,Jun Song,Yulin Wang,Jing Zhang,Wenlong Zhang,Bo Du*

Main category: cs.AI

TL;DR: 研究发现高质量的地球科学纯文本问答是超高分辨率遥感视觉推理能力提升的主要驱动力，提出分阶段知识注入方法，在XLRS-Bench上达到60.40% Pass@1的新SOTA


<details>
  <summary>Details</summary>
Motivation: 超高分辨率遥感多模态推理面临视觉证据获取瓶颈：需要在海量像素空间中定位微小的任务相关区域。虽然基于缩放工具的Agentic RLVR提供了一种解决方案，但标准强化学习在没有结构化领域先验的情况下难以导航这些巨大的视觉空间。

Method: 1) 对比冷启动监督微调、RLVR和Agentic RLVR三种后训练范式；2) 提出分阶段知识注入方法：先用可扩展的知识图谱验证的地球科学纯文本问答进行冷启动，注入推理结构；然后在SFT阶段对相同的困难UHR图像-文本示例进行"预热"，稳定并增强后续基于工具的强化学习。

Result: 在XLRS-Bench上达到60.40% Pass@1，显著优于更大的通用模型（如GPT-5.2、Gemini 3.0 Pro、Intern-S1），建立了新的最先进水平。

Conclusion: 高质量的地球科学纯文本问答是UHR视觉推理能力提升的主要驱动力，尽管没有图像，领域特定的文本注入了必要的概念、机制解释和决策规则来指导视觉证据检索。分阶段知识注入方法有效解决了视觉证据获取瓶颈。

Abstract: Multimodal reasoning for ultra-high-resolution (UHR) remote sensing (RS) is usually bottlenecked by visual evidence acquisition: the model necessitates localizing tiny task-relevant regions in massive pixel spaces. While Agentic Reinforcement Learning with Verifiable Rewards (RLVR) using zoom-in tools offers a path forward, we find that standard reinforcement learning struggles to navigate these vast visual spaces without structured domain priors. In this paper, we investigate the interplay between post-training paradigms: comparing Cold-start Supervised Fine-Tuning (SFT), RLVR, and Agentic RLVR on the UHR RS benchmark.Our controlled studies yield a counter-intuitive finding: high-quality Earth-science text-only QA is a primary driver of UHR visual reasoning gains. Despite lacking images, domain-specific text injects the concepts, mechanistic explanations, and decision rules necessary to guide visual evidence retrieval.Based on this, we propose a staged knowledge injection recipe: (1) cold-starting with scalable, knowledge-graph-verified Earth-science text QA to instill reasoning structures;and (2) "pre-warming" on the same hard UHR image-text examples during SFT to stabilize and amplify subsequent tool-based RL. This approach achieves a 60.40% Pass@1 on XLRS-Bench, significantly outperforming larger general purpose models (e.g., GPT-5.2, Gemini 3.0 Pro, Intern-S1) and establishing a new state-of-the-art.

</details>


### [129] [REDSearcher: A Scalable and Cost-Efficient Framework for Long-Horizon Search Agents](https://arxiv.org/abs/2602.14234)
*Zheng Chu,Xiao Wang,Jack Hong,Huiming Fan,Yuqi Huang,Yue Yang,Guohai Xu,Chenxiao Zhao,Cheng Xiang,Shengchao Hu,Dongdong Kuang,Ming Liu,Bing Qin,Xing Yu*

Main category: cs.AI

TL;DR: REDSearcher是一个统一的框架，通过协同设计复杂任务合成、中期训练和后期训练，解决LLM在深度搜索任务中高质量轨迹稀疏和奖励信号不足的问题，在文本和多模态搜索基准上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型正从通用知识引擎转向现实问题解决者，但优化它们用于深度搜索任务仍然具有挑战性。主要瓶颈在于高质量搜索轨迹和奖励信号的极端稀疏性，这源于可扩展的长视野任务构建的困难以及涉及外部工具调用的交互密集型rollout的高成本。

Method: 1) 将任务合成构建为双约束优化，通过图拓扑和证据分散精确控制任务难度；2) 引入工具增强查询以鼓励主动工具使用而非被动回忆；3) 在中期训练中加强核心原子能力（知识、规划和函数调用）；4) 构建本地模拟环境，实现强化学习实验的快速低成本算法迭代。

Result: 在文本和多模态搜索代理基准测试中，该方法实现了最先进的性能。将发布10K高质量复杂文本搜索轨迹、5K多模态轨迹和1K文本RL查询集，以及代码和模型检查点。

Conclusion: REDSearcher通过协同设计任务合成、训练和优化，有效解决了深度搜索任务中高质量轨迹稀疏的问题，为长视野搜索代理的未来研究提供了有价值的资源和框架。

Abstract: Large language models are transitioning from generalpurpose knowledge engines to realworld problem solvers, yet optimizing them for deep search tasks remains challenging. The central bottleneck lies in the extreme sparsity of highquality search trajectories and reward signals, arising from the difficulty of scalable longhorizon task construction and the high cost of interactionheavy rollouts involving external tool calls. To address these challenges, we propose REDSearcher, a unified framework that codesigns complex task synthesis, midtraining, and posttraining for scalable searchagent optimization. Specifically, REDSearcher introduces the following improvements: (1) We frame task synthesis as a dualconstrained optimization, where task difficulty is precisely governed by graph topology and evidence dispersion, allowing scalable generation of complex, highquality tasks. (2) We introduce toolaugmented queries to encourage proactive tool use rather than passive recall.(3) During midtraining, we strengthen core atomic capabilities knowledge, planning, and function calling substantially reducing the cost of collecting highquality trajectories for downstream training. (4) We build a local simulated environment that enables rapid, lowcost algorithmic iteration for reinforcement learning experiments. Across both textonly and multimodal searchagent benchmarks, our approach achieves stateoftheart performance. To facilitate future research on longhorizon search agents, we will release 10K highquality complex text search trajectories, 5K multimodal trajectories and 1K text RL query set, and together with code and model checkpoints.

</details>


### [130] [GRAIL: Goal Recognition Alignment through Imitation Learning](https://arxiv.org/abs/2602.14252)
*Osher Elhadad,Felipe Meneguzzi,Reuth Mirsky*

Main category: cs.AI

TL;DR: GRAIL使用模仿学习和逆强化学习从（可能次优的）演示轨迹中学习每个候选目标的目标导向策略，实现单次前向推理的目标识别，在次优、系统偏差和噪声环境下显著提升识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有目标识别方法通常依赖最优目标导向策略表示，但这可能与实际行为者行为不同，阻碍准确识别其目标。需要解决次优、系统偏差行为下的目标识别问题。

Method: 提出GRAIL方法：结合模仿学习和逆强化学习，直接从演示轨迹中为每个候选目标学习一个目标导向策略。通过单次前向传递对观察到的部分轨迹进行评分，保留经典目标识别的一次性推理能力。

Result: 在系统偏差最优行为下F1分数提升超过0.5；次优行为下提升约0.1-0.3；噪声最优轨迹下提升高达0.4；在完全最优设置下保持竞争力。

Conclusion: GRAIL为在不确定环境中解释智能体目标提供了可扩展且鲁棒的模型，能够捕捉次优和系统偏差行为，提升目标识别性能。

Abstract: Understanding an agent's goals from its behavior is fundamental to aligning AI systems with human intentions. Existing goal recognition methods typically rely on an optimal goal-oriented policy representation, which may differ from the actor's true behavior and hinder the accurate recognition of their goal. To address this gap, this paper introduces Goal Recognition Alignment through Imitation Learning (GRAIL), which leverages imitation learning and inverse reinforcement learning to learn one goal-directed policy for each candidate goal directly from (potentially suboptimal) demonstration trajectories. By scoring an observed partial trajectory with each learned goal-directed policy in a single forward pass, GRAIL retains the one-shot inference capability of classical goal recognition while leveraging learned policies that can capture suboptimal and systematically biased behavior. Across the evaluated domains, GRAIL increases the F1-score by more than 0.5 under systematically biased optimal behavior, achieves gains of approximately 0.1-0.3 under suboptimal behavior, and yields improvements of up to 0.4 under noisy optimal trajectories, while remaining competitive in fully optimal settings. This work contributes toward scalable and robust models for interpreting agent goals in uncertain environments.

</details>


### [131] [AutoWebWorld: Synthesizing Infinite Verifiable Web Environments via Finite State Machines](https://arxiv.org/abs/2602.14296)
*Yifan Wu,Yiran Peng,Yiyu Chen,Jianhao Ruan,Zijie Zhuang,Cheng Yang,Jiayi Zhang,Man Chen,Yenchi Tseng,Zhaoyang Yu,Liang Chen,Yuyao Zhai,Bang Liu,Chenglin Wu,Yuyu Luo*

Main category: cs.AI

TL;DR: AutoWebWorld：通过将网页环境建模为有限状态机并自动生成交互网站，解决GUI代理训练数据收集难题，实现低成本、可验证的轨迹生成。


<details>
  <summary>Details</summary>
Motivation: 现有自主网页GUI代理的训练数据收集面临两大挑战：1）从真实网站收集交互轨迹成本高昂且难以验证；2）底层状态转换是隐式的，需要依赖不一致且昂贵的外部验证器来评估步骤正确性。

Method: 提出AutoWebWorld框架：1）将网页环境建模为有限状态机（FSM），明确定义所有状态、动作和转换规则；2）使用编码代理将FSM转换为交互式网站；3）实现程序化验证：动作正确性通过预定义规则检查，任务成功通过是否到达FSM图中的目标状态确认。

Result: 1）以每条轨迹仅0.04美元的成本，从29个多样化网页环境中生成了11,663条已验证轨迹；2）基于合成数据训练的7B Web GUI代理在WebVoyager上15步内超越所有基线；3）观察到明显的缩放规律：随着合成数据量增加，在WebVoyager和Online-Mind2Web上的性能持续提升。

Conclusion: AutoWebWorld通过合成可控、可验证的网页环境，解决了GUI代理训练数据收集的瓶颈，实现了低成本、高质量的轨迹生成，显著提升了真实世界性能，并展示了数据规模与性能的正相关关系。

Abstract: The performance of autonomous Web GUI agents heavily relies on the quality and quantity of their training data. However, a fundamental bottleneck persists: collecting interaction trajectories from real-world websites is expensive and difficult to verify. The underlying state transitions are hidden, leading to reliance on inconsistent and costly external verifiers to evaluate step-level correctness. To address this, we propose AutoWebWorld, a novel framework for synthesizing controllable and verifiable web environments by modeling them as Finite State Machines (FSMs) and use coding agents to translate FSMs into interactive websites. Unlike real websites, where state transitions are implicit, AutoWebWorld explicitly defines all states, actions, and transition rules. This enables programmatic verification: action correctness is checked against predefined rules, and task success is confirmed by reaching a goal state in the FSM graph. AutoWebWorld enables a fully automated search-and-verify pipeline, generating over 11,663 verified trajectories from 29 diverse web environments at only $0.04 per trajectory. Training on this synthetic data significantly boosts real-world performance. Our 7B Web GUI agent outperforms all baselines within 15 steps on WebVoyager. Furthermore, we observe a clear scaling law: as the synthetic data volume increases, performance on WebVoyager and Online-Mind2Web consistently improves.

</details>


### [132] [Benchmarking at the Edge of Comprehension](https://arxiv.org/abs/2602.14307)
*Samuele Marro,Jialin Yu,Emanuele La Malfa,Oishi Deb,Jiawei Li,Yibo Yang,Ebey Abraham,Sunando Sengupta,Eric Sommerlade,Michael Wooldridge,Philip Torr*

Main category: cs.AI

TL;DR: 提出"批判弹性基准测试"框架，通过对抗性生成-评估游戏来比较模型，即使人类无法完全理解任务也能保持评估完整性。


<details>
  <summary>Details</summary>
Motivation: 前沿大语言模型迅速饱和新基准测试，导致人类难以生成区分性任务、提供准确答案或评估复杂解决方案。如果基准测试变得不可行，我们将无法衡量AI进展，这被称为"后理解机制"。

Method: 提出批判弹性基准测试框架，基于"批判弹性正确性"概念：如果没有任何对手能令人信服地证明答案错误，则视为正确。人类作为有界验证者，专注于局部声明。使用项目化二分Bradley-Terry模型联合排名LLM的解题能力和生成难题能力。

Result: 在数学领域对八个前沿LLM展示了方法的有效性，结果显示得分稳定且与外部能力测量相关。

Conclusion: 该框架将基准测试重新定义为对抗性生成-评估游戏，人类作为最终裁决者，能够在人类无法完全理解任务的情况下保持评估完整性。

Abstract: As frontier Large Language Models (LLMs) increasingly saturate new benchmarks shortly after they are published, benchmarking itself is at a juncture: if frontier models keep improving, it will become increasingly hard for humans to generate discriminative tasks, provide accurate ground-truth answers, or evaluate complex solutions. If benchmarking becomes infeasible, our ability to measure any progress in AI is at stake. We refer to this scenario as the post-comprehension regime. In this work, we propose Critique-Resilient Benchmarking, an adversarial framework designed to compare models even when full human understanding is infeasible. Our technique relies on the notion of critique-resilient correctness: an answer is deemed correct if no adversary has convincingly proved otherwise. Unlike standard benchmarking, humans serve as bounded verifiers and focus on localized claims, which preserves evaluation integrity beyond full comprehension of the task. Using an itemized bipartite Bradley-Terry model, we jointly rank LLMs by their ability to solve challenging tasks and to generate difficult yet solvable questions. We showcase the effectiveness of our method in the mathematical domain across eight frontier LLMs, showing that the resulting scores are stable and correlate with external capability measures. Our framework reformulates benchmarking as an adversarial generation-evaluation game in which humans serve as final adjudicators.

</details>


### [133] [Competition for attention predicts good-to-bad tipping in AI](https://arxiv.org/abs/2602.14370)
*Neil F. Johnson,Frank Y. Huo*

Main category: cs.AI

TL;DR: 论文提出了一种边缘AI设备中危险行为涌现的数学理论，通过注意力竞争机制预测和控制潜在危害


<details>
  <summary>Details</summary>
Motivation: 随着超过一半全球人口携带可离线运行类ChatGPT模型的设备，这些边缘AI存在促进自残、经济损失和极端主义等危险，而现有安全工具要么需要云端连接，要么只能在危害发生后发现问题

Method: 研究发现危险行为的涌现源于原子尺度上对设备注意力的竞争，提出了一个数学公式n*来描述动态临界点，该公式基于对话上下文与竞争输出盆地之间的点积注意力竞争

Result: 该机制在多个AI模型上得到验证，能够针对不同"好"与"坏"的定义进行实例化，理论上适用于跨领域（健康、法律、金融、国防）、不同法律环境、语言和文化背景

Conclusion: 研究揭示了边缘AI中危险行为涌现的新机制，提供了预测和控制潜在危害的数学框架，为跨领域的安全监管提供了新的控制杠杆

Abstract: More than half the global population now carries devices that can run ChatGPT-like language models with no Internet connection and minimal safety oversight -- and hence the potential to promote self-harm, financial losses and extremism among other dangers. Existing safety tools either require cloud connectivity or discover failures only after harm has occurred. Here we show that a large class of potentially dangerous tipping originates at the atomistic scale in such edge AI due to competition for the machinery's attention. This yields a mathematical formula for the dynamical tipping point n*, governed by dot-product competition for attention between the conversation's context and competing output basins, that reveals new control levers. Validated against multiple AI models, the mechanism can be instantiated for different definitions of 'good' and 'bad' and hence in principle applies across domains (e.g. health, law, finance, defense), changing legal landscapes (e.g. EU, UK, US and state level), languages, and cultural settings.

</details>


### [134] [Boule or Baguette? A Study on Task Topology, Length Generalization, and the Benefit of Reasoning Traces](https://arxiv.org/abs/2602.14404)
*William L. Tong,Ege Cakar,Cengiz Pehlevan*

Main category: cs.AI

TL;DR: 该论文通过PITA数据集研究推理模型在命题逻辑中的长度泛化能力，发现推理轨迹模型在宽浅任务上泛化良好，但在窄深任务上表现较差，揭示了推理轨迹范式的根本优势和限制。


<details>
  <summary>Details</summary>
Motivation: 尽管推理模型（生成中间推理轨迹的神经网络）近年来快速发展，但我们对推理轨迹如何支持推理以及该范式的限制理解仍然不完整。需要更清晰地理解推理轨迹在推理中的作用及其局限性。

Method: 引入PITA数据集（包含2300万条命题逻辑语句及其证明），作为鲁棒推理的基准，重点关注长度泛化问题。提出任务深度和任务广度的概念，并在PITA的不同子集上变化这些量，比较推理轨迹模型与非推理轨迹基线的表现。同时与基于三段论的简单合成任务进行比较验证。

Result: 推理轨迹模型在广度和浅度子集上泛化良好，但在窄度和深度子集上相对于非推理轨迹基线表现恶化。研究结果表明存在限制推理轨迹模型在深度任务上表现的基本缩放规律，同时突出了它们在广度任务上的泛化优势。

Conclusion: 该研究识别了使用推理轨迹的内在根本优势和限制：推理轨迹模型在广度任务上表现出色，但在深度任务上存在基本限制。这些发现有助于更全面地理解推理轨迹范式的能力边界。

Abstract: Recent years have witnessed meteoric progress in reasoning models: neural networks that generate intermediate reasoning traces (RTs) before producing a final output. Despite the rapid advancement, our understanding of how RTs support reasoning, and the limits of this paradigm, remain incomplete. To promote greater clarity, we introduce PITA: a novel large-scale dataset of over 23 million statements in propositional logic and their corresponding proofs. As a benchmark for robust reasoning, we focus on length generalization: if a model is trained to determine truth or falsity on statements with proofs up to fixed length, how well does it generalize to statements requiring longer proofs? We propose notions of (1) task depth and (2) task breadth, which measure respectively (1) the number of steps required to solve an example from a task and (2) the number of unique examples across a task. We vary these quantities across subsets of PITA, and find that RT models generalize well on broad and shallow subsets, while deteriorating on narrow and deep subsets relative to non-RT baselines. To determine whether our results are idiosyncratic to PITA or indicative of general phenomena, we compare our results to a simple synthetic task based on syllogisms. Our resulting theory suggests fundamental scalings that limit how well RT models perform on deep tasks, and highlights their generalization strengths on broad tasks. Our findings overall identify fundamental benefits and limitations inherent in using reasoning traces.

</details>


### [135] [Precedent-Informed Reasoning: Mitigating Overthinking in Large Reasoning Models via Test-Time Precedent Learning](https://arxiv.org/abs/2602.14451)
*Qianyue Wang,Jinwu Hu,Huanxiang Lin,Bolin Chen,Zhiquan Wen,Yaofo Chen,Yu Rong,Mingkui Tan*

Main category: cs.AI

TL;DR: PIR通过自适应先例选择和测试时经验内化，将LLM推理从自我探索转变为基于先例的引导学习，缩短推理轨迹同时保持或提高准确性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的推理过程存在效率低下问题，冗长的思维链包含冗余的自我探索和验证，增加了计算成本甚至降低性能。受人类推理模式启发——人们通过借鉴过去相关案例来约束搜索空间、减少试错——需要将LLM推理范式从详尽的自我探索转变为基于先例的引导学习。

Method: 提出先例知情推理（PIR），包含两个关键技术：1）自适应先例选择（APS）：为每个问题和LLM构建紧凑的先例集，基于语义相似度和模型困惑度的联合评分进行排序，并自适应调整先例数量以最大化困惑度降低；2）测试时经验内化（TEI）：在测试时对先例知情指令进行学习，更新轻量级适配器以内部化解题模式，并将其作为后续推理的先验知识。

Result: 在数学推理、科学问答和代码生成等任务上的实验表明，PIR能够持续缩短推理轨迹，同时保持或提高最终准确性，在LLM中实现了出色的准确性与效率权衡。

Conclusion: PIR成功将LLM推理范式从详尽的自我探索转变为基于先例的引导学习，通过自适应先例选择和测试时经验内化，在减少计算成本的同时维持或提升推理性能，为高效推理提供了新方向。

Abstract: Reasoning in Large Language Models (LLMs) often suffers from inefficient long chain-of-thought traces with redundant self-exploration and validation, which inflate computational costs and even degrade performance. Inspired by human reasoning patterns where people solve new problems by leveraging past related cases to constrain search spaces and reduce trial-and-error, we propose Precedent Informed Reasoning (PIR) transforming LRMs'reasoning paradigm from exhaustive self-exploration to guided learning from precedents. PIR addresses two key challenges: what precedents to adopt and how to utilize them. First, Adaptive Precedent Selection (APS) constructs, for each question and LRM, a compact set of precedents that are both semantically related and informative for the model. It ranks examples by a joint score with semantic similarity and model perplexity, then adapts the amount of precedents to maximize perplexity reduction. Second, Test-time Experience Internalization (TEI) is treated as the test-time learning on precedent-informed instruction, updating lightweight adapters to internalize solution patterns and use them as a prior during subsequent reasoning. Experiments across mathematical reasoning, scientific QA, and code generation demonstrate that PIR consistently shortens reasoning traces while maintaining or improving final accuracy across LLMs, yielding outstanding accuracy-efficiency trade-offs.

</details>


### [136] [Bounding Probabilities of Causation with Partial Causal Diagrams](https://arxiv.org/abs/2602.14503)
*Yuxuan Xie,Ang Li*

Main category: cs.AI

TL;DR: 提出一个使用部分因果信息来界定因果概率的通用框架，通过优化编程将可用信息作为约束条件，在不完全可识别的情况下获得更紧且形式有效的界限。


<details>
  <summary>Details</summary>
Motivation: 因果概率对于个体层面的解释和决策至关重要，但本质上是反事实的，通常无法从数据中精确识别。现有界限要么忽略可用协变量，要么需要完整的因果图，要么依赖限制性的二元设置，限制了实际应用。现实应用中，因果信息通常是部分但非平凡的。

Method: 提出一个通用框架，将可用的结构或统计信息作为约束条件系统地纳入优化编程公式中。通过将部分因果信息转化为数学约束，在无法完全识别的情况下获得因果概率的界限。

Result: 该方法能够产生更紧且形式有效的界限，扩展了因果概率在现实场景中的适用性，即使因果知识不完全但具有信息性时也能应用。

Conclusion: 该框架为在因果信息不完整但具有信息性的现实环境中界定因果概率提供了系统方法，克服了现有方法的局限性，提高了因果推断在实际应用中的实用性。

Abstract: Probabilities of causation are fundamental to individual-level explanation and decision making, yet they are inherently counterfactual and not point-identifiable from data in general. Existing bounds either disregard available covariates, require complete causal graphs, or rely on restrictive binary settings, limiting their practical use. In real-world applications, causal information is often partial but nontrivial. This paper proposes a general framework for bounding probabilities of causation using partial causal information. We show how the available structural or statistical information can be systematically incorporated as constraints in a optimization programming formulation, yielding tighter and formally valid bounds without full identifiability. This approach extends the applicability of probabilities of causation to realistic settings where causal knowledge is incomplete but informative.

</details>


### [137] [Formally Verifying and Explaining Sepsis Treatment Policies with COOL-MC](https://arxiv.org/abs/2602.14505)
*Dennis Gross*

Main category: cs.AI

TL;DR: COOL-MC 是一个结合形式验证和可解释性的工具，用于分析强化学习在脓毒症治疗中的决策策略，通过构建可达状态空间、临床标签和解释性方法，揭示策略的决策依据。


<details>
  <summary>Details</summary>
Motivation: 医疗领域的强化学习策略（如脓毒症治疗优化）通常不透明且难以验证，现有模型检查器无法处理大规模MDP，也缺乏解释策略决策原因的能力。

Method: COOL-MC 包装了Storm模型检查器，但增加了三个关键功能：1) 仅构建训练策略诱导的可达状态空间，形成可验证的离散时间马尔可夫链；2) 自动用临床有意义的原子命题标记状态；3) 集成可解释性方法与PCTL查询，揭示决策驱动因素。

Result: 在ICU-Sepsis MDP基准测试中，COOL-MC建立了硬边界验证，训练出达到最优生存概率的安全RL策略，并通过PCTL验证和可解释性分析发现策略主要依赖既往用药史而非患者实时状况。

Conclusion: COOL-MC可作为临床医生在部署前调查和调试脓毒症治疗策略的工具，通过形式验证和可解释性的结合，暴露标准评估无法发现的策略弱点。

Abstract: Safe and interpretable sequential decision-making is critical in healthcare, yet reinforcement learning (RL) policies for sepsis treatment optimization remain opaque and difficult to verify. Standard probabilistic model checkers operate on the full state space, which becomes infeasible for larger MDPs, and cannot explain why a learned policy makes particular decisions. COOL-MC wraps the model checker Storm but adds three key capabilities: it constructs only the reachable state space induced by a trained policy, yielding a smaller discrete-time Markov chain amenable to verification even when full-MDP analysis is intractable; it automatically labels states with clinically meaningful atomic propositions; and it integrates explainability methods with probabilistic computation tree logic (PCTL) queries to reveal which features drive decisions across treatment trajectories. We demonstrate COOL-MC's capabilities on the ICU-Sepsis MDP, a benchmark derived from approximately 17,000 sepsis patient records, which serves as a case study for applying COOL-MC to the formal analysis of sepsis treatment policies. Our analysis establishes hard bounds via full MDP verification, trains a safe RL policy that achieves optimal survival probability, and analyzes its behavior via PCTL verification and explainability on the induced DTMC. This reveals, for instance, that our trained policy relies predominantly on prior dosing history rather than the patient's evolving condition, a weakness that is invisible to standard evaluation but is exposed by COOL-MC's integration of formal verification and explainability. Our results illustrate how COOL-MC could serve as a tool for clinicians to investigate and debug sepsis treatment policies before deployment.

</details>


### [138] [Diagnosing Knowledge Conflict in Multimodal Long-Chain Reasoning](https://arxiv.org/abs/2602.14518)
*Jing Tang,Kun Wang,Haolang Lu,Hongjin Chen,KaiTao Chen,Zhongxiang Sun,Qiankun Li,Lingjuan Lyu,Guoshun Nan,Zhigang Zeng*

Main category: cs.AI

TL;DR: 研究发现多模态大语言模型在长链推理中处理知识冲突时存在特定模式：冲突特征线性可分、集中于中后层、可通过轨迹聚合恢复冲突类型、且存在方向不对称性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在长链推理中经常因不同知识源提供冲突信号而失败，需要理解这些失败背后的机制，以便进行诊断和控制。

Method: 通过探测内部表示，分析知识冲突在模型中的编码方式，区分输入级客观冲突和过程级有效冲突，并研究冲突信号的线性可分性、深度定位、层次一致性和方向不对称性。

Result: 发现四种关键模式：1）不同冲突类型被编码为线性可分特征；2）冲突信号集中在中后层；3）沿轨迹聚合噪声标记级信号可稳健恢复输入级冲突类型；4）强化模型隐含源偏好比强制相反源更容易。

Conclusion: 研究提供了多模态推理在知识冲突下的机制级视角，为长链推理失败的原则性诊断和控制奠定了基础。

Abstract: Multimodal large language models (MLLMs) in long chain-of-thought reasoning often fail when different knowledge sources provide conflicting signals. We formalize these failures under a unified notion of knowledge conflict, distinguishing input-level objective conflict from process-level effective conflict. Through probing internal representations, we reveal that: (I) Linear Separability: different conflict types are explicitly encoded as linearly separable features rather than entangled; (II) Depth Localization: conflict signals concentrate in mid-to-late layers, indicating a distinct processing stage for conflict encoding; (III) Hierarchical Consistency: aggregating noisy token-level signals along trajectories robustly recovers input-level conflict types; and (IV) Directional Asymmetry: reinforcing the model's implicit source preference under conflict is far easier than enforcing the opposite source. Our findings provide a mechanism-level view of multimodal reasoning under knowledge conflict and enable principled diagnosis and control of long-CoT failures.

</details>


### [139] [Disentangling Deception and Hallucination Failures in LLMs](https://arxiv.org/abs/2602.14529)
*Haolang Lu,Hongrui Peng,WeiYe Fu,Guoshun Nan,Xinye Cao,Xingrui Li,Hongcan Guo,Kun Wang*

Main category: cs.AI

TL;DR: 论文提出从机制角度区分LLM事实查询失败：知识存在与行为表达分离，将幻觉与欺骗视为不同底层机制但输出相似的现象


<details>
  <summary>Details</summary>
Motivation: 传统LLM失败分析常从行为角度归因于知识缺失，但可能混淆不同失败机制。需要从内部机制角度区分知识存在与行为表达

Method: 构建实体中心事实查询的受控环境，保持知识不变但选择性改变行为表达；通过表示可分性、稀疏可解释性和推理时激活引导分析四种行为案例

Result: 区分了幻觉与欺骗两种不同失败模式，它们在输出层面相似但底层机制不同；建立了系统分析框架

Conclusion: 需要从机制角度而非仅行为角度分析LLM失败，知识存在与行为表达的分离有助于理解不同失败模式的本质差异

Abstract: Failures in large language models (LLMs) are often analyzed from a behavioral perspective, where incorrect outputs in factual question answering are commonly associated with missing knowledge. In this work, focusing on entity-based factual queries, we suggest that such a view may conflate different failure mechanisms, and propose an internal, mechanism-oriented perspective that separates Knowledge Existence from Behavior Expression. Under this formulation, hallucination and deception correspond to two qualitatively different failure modes that may appear similar at the output level but differ in their underlying mechanisms. To study this distinction, we construct a controlled environment for entity-centric factual questions in which knowledge is preserved while behavioral expression is selectively altered, enabling systematic analysis of four behavioral cases. We analyze these failure modes through representation separability, sparse interpretability, and inference-time activation steering.

</details>


### [140] [MATEO: A Multimodal Benchmark for Temporal Reasoning and Planning in LVLMs](https://arxiv.org/abs/2602.14589)
*Gabriel Roccabruna,Olha Khomyn,Giuseppe Riccardi*

Main category: cs.AI

TL;DR: 该论文提出了MATEO基准测试，用于评估大型视觉语言模型在真实世界规划中的时序推理能力，基于高质量多模态食谱数据集和时序执行顺序图标注。


<details>
  <summary>Details</summary>
Motivation: 现有研究对基础模型时序执行理解有限，主要基于自动标注、线性链近似或纯文本输入，缺乏对多模态时序推理能力的系统评估。

Method: 收集高质量专业多模态食谱语料，通过标准化编辑流程将指令分解为离散步骤并配图；设计可扩展的众包流程标注时序执行顺序图；评估6个SOTA LVLM模型。

Result: 评估了不同模型规模、语言上下文、多模态输入结构和微调策略下的LVLM性能，为时序推理能力提供了基准测试结果。

Conclusion: MATEO基准填补了多模态时序推理评估的空白，为提升LVLM在真实世界规划中的时序理解能力提供了重要工具。

Abstract: AI agents need to plan to achieve complex goals that involve orchestrating perception, sub-goal decomposition, and execution. These plans consist of ordered steps structured according to a Temporal Execution Order (TEO, a directed acyclic graph that ensures each step executes only after its preconditions are satisfied. Existing research on foundational models' understanding of temporal execution is limited to automatically derived annotations, approximations of the TEO as a linear chain, or text-only inputs. To address this gap, we introduce MATEO (MultimodAl Temporal Execution Order), a benchmark designed to assess and improve the temporal reasoning abilities of Large Vision Language Models (LVLMs) required for real-world planning. We acquire a high-quality professional multimodal recipe corpus, authored through a standardized editorial process that decomposes instructions into discrete steps, each paired with corresponding images. We collect TEO annotations as graphs by designing and using a scalable crowdsourcing pipeline. Using MATEO, we evaluate six state-of-the-art LVLMs across model scales, varying language context, multimodal input structure, and fine-tuning strategies.

</details>


### [141] [Tabular Foundation Models Can Learn Association Rules](https://arxiv.org/abs/2602.14622)
*Erkan Karabulut,Daniel Daza,Paul Groth,Martijn C. Schut,Victoria Degeler*

Main category: cs.AI

TL;DR: TabProbe：一种基于表格基础模型的无模型关联规则学习框架，无需频繁项集挖掘即可从表格数据中提取高质量关联规则，在低数据场景下表现稳健。


<details>
  <summary>Details</summary>
Motivation: 传统关联规则挖掘方法依赖频繁项集挖掘，导致规则爆炸和可扩展性差；最近的神经方法缓解了这些问题但在低数据场景下性能下降。表格基础模型提供了解决这些限制的基础。

Method: 提出了一个模型无关的关联规则学习框架，可从任何条件概率模型中提取关联规则。具体实现TabProbe利用表格基础模型作为条件概率估计器，无需频繁项集挖掘即可学习关联规则。

Result: 表格基础模型能够持续产生简洁、高质量的关联规则，具有强大的预测性能，在低数据设置下保持稳健，无需任务特定训练。

Conclusion: TabProbe框架成功地将表格基础模型应用于关联规则挖掘，克服了传统方法的局限性，在低数据场景下表现出色，为表格数据知识发现提供了新途径。

Abstract: Association Rule Mining (ARM) is a fundamental task for knowledge discovery in tabular data and is widely used in high-stakes decision-making. Classical ARM methods rely on frequent itemset mining, leading to rule explosion and poor scalability, while recent neural approaches mitigate these issues but suffer from degraded performance in low-data regimes. Tabular foundation models (TFMs), pretrained on diverse tabular data with strong in-context generalization, provide a basis for addressing these limitations. We introduce a model-agnostic association rule learning framework that extracts association rules from any conditional probabilistic model over tabular data, enabling us to leverage TFMs. We then introduce TabProbe, an instantiation of our framework that utilizes TFMs as conditional probability estimators to learn association rules out-of-the-box without frequent itemset mining. We evaluate our approach on tabular datasets of varying sizes based on standard ARM rule quality metrics and downstream classification performance. The results show that TFMs consistently produce concise, high-quality association rules with strong predictive performance and remain robust in low-data settings without task-specific training. Source code is available at https://github.com/DiTEC-project/tabprobe.

</details>


### [142] [Arbor: A Framework for Reliable Navigation of Critical Conversation Flows](https://arxiv.org/abs/2602.14643)
*Luís Silva,Diogo Gonçalves,Catarina Farinha,Clara Matos,Luís Ungaro*

Main category: cs.AI

TL;DR: Arbor框架通过将决策树导航分解为专门的节点级任务，解决了大语言模型在结构化工作流中遵循指令能力下降的问题，显著提升了临床分诊的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医疗分诊等高风险领域难以严格遵循结构化工作流程。传统的单一提示方法随着提示长度增加会出现指令遵循能力下降、中间信息丢失和上下文窗口溢出等问题。

Method: 提出Arbor框架，将决策树导航分解为专门的节点级任务：1) 将决策树标准化为边列表表示并存储；2) 运行时使用DAG编排机制迭代检索当前节点的出边；3) 通过专门的LLM调用评估有效转换；4) 将响应生成委托给单独的推理步骤。

Result: 在10个基础模型上使用真实临床分诊对话进行评估：平均轮次准确率提升29.4个百分点，每轮延迟降低57.1%，每轮成本平均降低14.4倍。较小的模型能够匹配或超过单一提示基线下的较大模型。

Conclusion: 架构分解减少了对内在模型能力的依赖，使较小模型能够匹配或超过在单一提示基线下的较大模型表现，为高风险领域结构化工作流提供了有效的解决方案。

Abstract: Large language models struggle to maintain strict adherence to structured workflows in high-stakes domains such as healthcare triage. Monolithic approaches that encode entire decision structures within a single prompt are prone to instruction-following degradation as prompt length increases, including lost-in-the-middle effects and context window overflow. To address this gap, we present Arbor, a framework that decomposes decision tree navigation into specialized, node-level tasks. Decision trees are standardized into an edge-list representation and stored for dynamic retrieval. At runtime, a directed acyclic graph (DAG)-based orchestration mechanism iteratively retrieves only the outgoing edges of the current node, evaluates valid transitions via a dedicated LLM call, and delegates response generation to a separate inference step. The framework is agnostic to the underlying decision logic and model provider. Evaluated against single-prompt baselines across 10 foundation models using annotated turns from real clinical triage conversations. Arbor improves mean turn accuracy by 29.4 percentage points, reduces per-turn latency by 57.1%, and achieves an average 14.4x reduction in per-turn cost. These results indicate that architectural decomposition reduces dependence on intrinsic model capability, enabling smaller models to match or exceed larger models operating under single-prompt baselines.

</details>


### [143] [From User Preferences to Base Score Extraction Functions in Gradual Argumentation](https://arxiv.org/abs/2602.14674)
*Aniol Civit,Antonio Rago,Antonio Andriella,Guillem Alenyà,Francesca Toni*

Main category: cs.AI

TL;DR: 提出基础分数提取函数，将用户对论点的偏好映射为基础分数，用于构建定量双极论证框架，简化分数选择过程


<details>
  <summary>Details</summary>
Motivation: 在渐进论证中，基础分数的选择需要专业知识且不直观，而通过偏好组织论点可以简化这一过程

Method: 引入基础分数提取函数，将双极论证框架中的偏好映射为定量框架的基础分数，包含算法设计并考虑人类偏好的非线性特征

Result: 在机器人场景中进行理论和实验评估，为实践中选择合适的渐进语义提供建议

Conclusion: 基础分数提取函数能够有效简化渐进论证中基础分数的选择过程，支持透明且可争议的AI系统

Abstract: Gradual argumentation is a field of symbolic AI which is attracting attention for its ability to support transparent and contestable AI systems. It is considered a useful tool in domains such as decision-making, recommendation, debate analysis, and others. The outcomes in such domains are usually dependent on the arguments' base scores, which must be selected carefully. Often, this selection process requires user expertise and may not always be straightforward. On the other hand, organising the arguments by preference could simplify the task. In this work, we introduce \emph{Base Score Extraction Functions}, which provide a mapping from users' preferences over arguments to base scores. These functions can be applied to the arguments of a \emph{Bipolar Argumentation Framework} (BAF), supplemented with preferences, to obtain a \emph{Quantitative Bipolar Argumentation Framework} (QBAF), allowing the use of well-established computational tools in gradual argumentation. We outline the desirable properties of base score extraction functions, discuss some design choices, and provide an algorithm for base score extraction. Our method incorporates an approximation of non-linearities in human preferences to allow for better approximation of the real ones. Finally, we evaluate our approach both theoretically and experimentally in a robotics setting, and offer recommendations for selecting appropriate gradual semantics in practice.

</details>


### [144] [GREAT-EER: Graph Edge Attention Network for Emergency Evacuation Responses](https://arxiv.org/abs/2602.14676)
*Attila Lischka,Balázs Kulcsár*

Main category: cs.AI

TL;DR: 提出基于深度强化学习的公交车疏散路径规划方法，解决城市紧急疏散中的NP难组合优化问题，在旧金山真实路网中验证了接近最优解的性能。


<details>
  <summary>Details</summary>
Motivation: 城市紧急疏散（恐怖袭击、工业事故、自然灾害）需要快速有效的疏散方案。纯汽车疏散会导致拥堵和混乱，因此需要公交车疏散来减少这些问题。

Method: 提出公交车疏散定向问题（BEOP），这是一个NP难的组合优化问题。采用基于图学习的深度强化学习方法，训练后能快速生成疏散路线。同时使用MILP（混合整数线性规划）来界定解的质量差距。

Result: 在旧金山真实路网和旅行时间数据上验证，方法能达到接近最优的解质量，推理速度快（秒级）。还能分析需要多少疏散车辆才能在预定时间内达到特定疏散配额。

Conclusion: 提出的深度强化学习方法能快速生成有效的公交车疏散方案，为城市紧急疏散规划提供了实用工具，能在有限时间内最大化疏散人数。

Abstract: Emergency situations that require the evacuation of urban areas can arise from man-made causes (e.g., terrorist attacks or industrial accidents) or natural disasters, the latter becoming more frequent due to climate change. As a result, effective and fast methods to develop evacuation plans are of great importance. In this work, we identify and propose the Bus Evacuation Orienteering Problem (BEOP), an NP-hard combinatorial optimization problem with the goal of evacuating as many people from an affected area by bus in a short, predefined amount of time. The purpose of bus-based evacuation is to reduce congestion and disorder that arises in purely car-focused evacuation scenarios. To solve the BEOP, we propose a deep reinforcement learning-based method utilizing graph learning, which, once trained, achieves fast inference speed and is able to create evacuation routes in fractions of seconds. We can bound the gap of our evacuation plans using an MILP formulation. To validate our method, we create evacuation scenarios for San Francisco using real-world road networks and travel times. We show that we achieve near-optimal solution quality and are further able to investigate how many evacuation vehicles are necessary to achieve certain bus-based evacuation quotas given a predefined evacuation time while keeping run time adequate.

</details>


### [145] [Removing Planner Bias in Goal Recognition Through Multi-Plan Dataset Generation](https://arxiv.org/abs/2602.14691)
*Mustafa F. Abdelwahed,Felipe Meneguzzi Kin Max Piamolini Gusmao,Joan Espasa*

Main category: cs.AI

TL;DR: 提出使用top-k规划生成同一目标的不同计划，以解决现有目标识别数据集的系统性偏差问题，并引入版本覆盖分数(VCS)评估目标识别器在不同计划集下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有目标识别数据集存在系统性偏差，这些数据集由启发式前向搜索规划系统生成，缺乏对更真实场景（如智能体使用不同规划器）的挑战性，影响了目标识别器在不同规划器下的评估效果。

Method: 使用top-k规划方法为同一目标假设生成多个不同的计划，创建能够缓解当前数据集偏差的基准测试，并引入版本覆盖分数(VCS)来衡量目标识别器基于不同计划集推断目标时的鲁棒性。

Result: 研究结果显示，当前最先进的目标识别器在低可观测性设置下的鲁棒性显著下降。

Conclusion: 提出的top-k规划方法和VCS指标能够更好地评估目标识别器在不同规划器下的性能，揭示了现有方法在真实场景中的局限性。

Abstract: Autonomous agents require some form of goal and plan recognition to interact in multiagent settings. Unfortunately, all existing goal recognition datasets suffer from a systematical bias induced by the planning systems that generated them, namely heuristic-based forward search. This means that existing datasets lack enough challenge for more realistic scenarios (e.g., agents using different planners), which impacts the evaluation of goal recognisers with respect to using different planners for the same goal. In this paper, we propose a new method that uses top-k planning to generate multiple, different, plans for the same goal hypothesis, yielding benchmarks that mitigate the bias found in the current dataset. This allows us to introduce a new metric called Version Coverage Score (VCS) to measure the resilience of the goal recogniser when inferring a goal based on different sets of plans. Our results show that the resilience of the current state-of-the-art goal recogniser degrades substantially under low observability settings.

</details>


### [146] [Evolutionary System Prompt Learning can Facilitate Reinforcement Learning for LLMs](https://arxiv.org/abs/2602.14697)
*Lunjun Zhang,Ryan Chen,Bradly C. Stadie*

Main category: cs.AI

TL;DR: E-SPL提出了一种联合优化模型上下文和权重的进化系统提示学习方法，通过并行选择多个系统提示进行RL迭代，结合进化算法更新提示种群，实现了比单独使用RL或提示进化更好的性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs主要通过两种机制自我改进：上下文更新的自我反思和权重更新的强化学习。本文旨在开发一种能够联合改进模型上下文和模型权重的方法，以更好地实现自主智能体的自我改进。

Method: E-SPL在每次RL迭代中并行选择多个系统提示进行rollout，对每个系统提示条件下的模型权重进行RL更新，同时通过LLM驱动的突变和交叉对系统提示种群进行进化更新。每个系统提示都有TrueSkill评分用于进化选择，基于每批RL迭代中的相对性能进行更新。

Result: 在从易到难（AIME→BeyondAIME）的泛化设置中，E-SPL将RL成功率从38.8%提升到45.1%，同时优于反思提示进化（40.0%）。该方法在推理和智能体任务上均表现出改进性能。

Conclusion: 将强化学习与系统提示进化相结合，能够在样本效率和泛化能力方面带来一致的增益，促进了陈述性知识（编码在提示中）和程序性知识（编码在权重中）的自然分离。

Abstract: Building agentic systems that can autonomously self-improve from experience is a longstanding goal of AI. Large language models (LLMs) today primarily self-improve via two mechanisms: self-reflection for context updates, and reinforcement learning (RL) for weight updates. In this work, we propose Evolutionary System Prompt Learning (E-SPL), a method for jointly improving model contexts and model weights. In each RL iteration, E-SPL selects multiple system prompts and runs rollouts with each in parallel. It applies RL updates to model weights conditioned on each system prompt, and evolutionary updates to the system prompt population via LLM-driven mutation and crossover. Each system prompt has a TrueSkill rating for evolutionary selection, updated from relative performance within each RL iteration batch. E-SPL encourages a natural division between declarative knowledge encoded in prompts and procedural knowledge encoded in weights, resulting in improved performance across reasoning and agentic tasks. For instance, in an easy-to-hard (AIME $\rightarrow$ BeyondAIME) generalization setting, E-SPL improves RL success rate from 38.8% $\rightarrow$ 45.1% while also outperforming reflective prompt evolution (40.0%). Overall, our results show that coupling reinforcement learning with system prompt evolution yields consistent gains in sample efficiency and generalization. Code: https://github.com/LunjunZhang/E-SPL

</details>


### [147] [WebWorld: A Large-Scale World Model for Web Agent Training](https://arxiv.org/abs/2602.14721)
*Zikai Xiao,Jianhong Tu,Chuhang Zou,Yuxin Zuo,Zhi Li,Peng Wang,Bowen Yu,Fei Huang,Junyang Lin,Zuozhu Liu*

Main category: cs.AI

TL;DR: WebWorld是首个大规模训练的开源网页模拟器，通过100万+网页交互数据支持推理、多格式数据和30+步长程模拟，在网页任务上达到GPT-4o级别性能，并展现跨领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 真实网页代理训练面临网络延迟、速率限制和安全风险等约束，需要大规模轨迹数据但难以获取。现有模拟器局限于封闭环境且数据量有限（数千轨迹），无法满足开放网页环境的需求。

Method: 构建可扩展的数据流水线，在100万+开放网页交互上进行大规模训练，支持推理、多格式数据和长程模拟（30+步骤）。引入WebWorld-Bench进行内在评估，包含9个维度的双重指标。

Result: WebWorld模拟性能与Gemini-3-Pro相当；Qwen3-14B在WebWorld合成轨迹上训练后，WebArena性能提升+9.2%，达到GPT-4o水平；作为世界模型在推理时搜索中优于GPT-5；展现跨代码、GUI和游戏领域的泛化能力。

Conclusion: WebWorld是首个大规模开放网页模拟器，通过可扩展训练方法实现了高性能网页代理，提供了世界模型构建的可复制方案，并展示了跨领域泛化潜力。

Abstract: Web agents require massive trajectories to generalize, yet real-world training is constrained by network latency, rate limits, and safety risks. We introduce \textbf{WebWorld} series, the first open-web simulator trained at scale. While existing simulators are restricted to closed environments with thousands of trajectories, WebWorld leverages a scalable data pipeline to train on 1M+ open-web interactions, supporting reasoning, multi-format data, and long-horizon simulations of 30+ steps. For intrinsic evaluation, we introduce WebWorld-Bench with dual metrics spanning nine dimensions, where WebWorld achieves simulation performance comparable to Gemini-3-Pro. For extrinsic evaluation, Qwen3-14B trained on WebWorld-synthesized trajectories improves by +9.2\% on WebArena, reaching performance comparable to GPT-4o. WebWorld enables effective inference-time search, outperforming GPT-5 as a world model. Beyond web simulation, WebWorld exhibits cross-domain generalization to code, GUI, and game environments, providing a replicable recipe for world model construction.

</details>


### [148] [Return of the Schema: Building Complete Datasets for Machine Learning and Reasoning on Knowledge Graphs](https://arxiv.org/abs/2602.14795)
*Ivan Diliso,Roberto Barile,Claudia d'Amato,Nicola Fanizzi*

Main category: cs.AI

TL;DR: 提出首个同时包含本体模式与事实数据的知识图谱数据集资源，支持机器学习与推理服务


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱精化算法评估数据集通常只包含事实数据，缺乏丰富的本体约束信息，限制了依赖本体约束、推理或神经符号技术的评估，无法反映真实大规模知识图谱的性能

Method: 开发了一个工作流程，从源知识图谱中提取同时包含模式与事实的数据集，处理不一致性问题，利用推理推导隐含知识，并以OWL格式序列化，同时提供加载到张量表示的实用工具

Result: 创建了首个包含模式与事实的精选数据集套件，包括从表达性模式知识图谱新提取的数据集，并丰富了现有数据集的本体信息，所有数据集都以OWL格式提供

Conclusion: 该资源填补了知识图谱精化评估的空白，支持更全面的方法评估，特别是依赖本体约束和推理的技术，为大规模真实知识图谱应用提供了更好的评估基础

Abstract: Datasets for the experimental evaluation of knowledge graph refinement algorithms typically contain only ground facts, retaining very limited schema level knowledge even when such information is available in the source knowledge graphs. This limits the evaluation of methods that rely on rich ontological constraints, reasoning or neurosymbolic techniques and ultimately prevents assessing their performance in large-scale, real-world knowledge graphs. In this paper, we present \resource{} the first resource that provides a workflow for extracting datasets including both schema and ground facts, ready for machine learning and reasoning services, along with the resulting curated suite of datasets. The workflow also handles inconsistencies detected when keeping both schema and facts and also leverage reasoning for entailing implicit knowledge. The suite includes newly extracted datasets from KGs with expressive schemas while simultaneously enriching existing datasets with schema information. Each dataset is serialized in OWL making it ready for reasoning services. Moreover, we provide utilities for loading datasets in tensor representations typical of standard machine learning libraries.

</details>


### [149] [World Models for Policy Refinement in StarCraft II](https://arxiv.org/abs/2602.14857)
*Yixin Zhang,Ziyi Wang,Yiming Rong,Haoxi Wang,Jinling Jiang,Shuang Xu,Haoran Wu,Shiyu Zhou,Bo Xu*

Main category: cs.AI

TL;DR: StarWM：首个针对星际争霸II的世界模型，通过结构化文本表示和预测未来观测来增强决策，在离线预测和在线对战中都取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的星际争霸II智能体主要关注策略改进，忽视了在决策循环中集成可学习的、动作条件化的转移模型。星际争霸II具有巨大的状态-动作空间和部分可观测性，是一个具有挑战性的测试平台。

Method: 提出StarWM世界模型，使用结构化文本表示将观测分解为五个语义模块；构建SC2-Dynamics-50k指令调优数据集；开发多维离线评估框架；提出StarWM-Agent决策系统，集成StarWM到"生成-模拟-优化"决策循环中。

Result: 离线评估显示StarWM相比零样本基线有显著提升：资源预测准确率提高近60%，己方宏观态势一致性改善；在线对战评估显示，对抗内置AI时胜率分别提升30%（Hard/LV5）、15%（Harder/LV6）和30%（VeryHard/LV7），宏观管理稳定性和战术风险评估也得到改善。

Conclusion: StarWM作为首个星际争霸II世界模型，通过结构化观测预测和集成到决策循环中，显著提升了智能体的决策能力，证明了世界模型在复杂部分可观测环境中的价值。

Abstract: Large Language Models (LLMs) have recently shown strong reasoning and generalization capabilities, motivating their use as decision-making policies in complex environments. StarCraft II (SC2), with its massive state-action space and partial observability, is a challenging testbed. However, existing LLM-based SC2 agents primarily focus on improving the policy itself and overlook integrating a learnable, action-conditioned transition model into the decision loop. To bridge this gap, we propose StarWM, the first world model for SC2 that predicts future observations under partial observability. To facilitate learning SC2's hybrid dynamics, we introduce a structured textual representation that factorizes observations into five semantic modules, and construct SC2-Dynamics-50k, the first instruction-tuning dataset for SC2 dynamics prediction. We further develop a multi-dimensional offline evaluation framework for predicted structured observations. Offline results show StarWM's substantial gains over zero-shot baselines, including nearly 60% improvements in resource prediction accuracy and self-side macro-situation consistency. Finally, we propose StarWM-Agent, a world-model-augmented decision system that integrates StarWM into a Generate--Simulate--Refine decision loop for foresight-driven policy refinement. Online evaluation against SC2's built-in AI demonstrates consistent improvements, yielding win-rate gains of 30%, 15%, and 30% against Hard (LV5), Harder (LV6), and VeryHard (LV7), respectively, alongside improved macro-management stability and tactical risk assessment.

</details>


### [150] [EmbeWebAgent: Embedding Web Agents into Any Customized UI](https://arxiv.org/abs/2602.14865)
*Chenyang Ma,Clyde Fare,Matthew Wilson,Dave Braines*

Main category: cs.AI

TL;DR: EmbeWebAgent是一个将智能体直接嵌入现有UI的企业级框架，通过前端钩子和后端工作流实现，支持混合粒度操作和跨栈兼容。


<details>
  <summary>Details</summary>
Motivation: 大多数网页智能体在人类界面层面操作（截图或DOM树），缺乏应用级访问，限制了鲁棒性和动作表达能力。在企业环境中，前后端都有明确控制权，这为更强大的智能体提供了机会。

Method: 使用轻量级前端钩子（ARIA和URL观察、通过WebSocket暴露的每页函数注册表）和可复用的后端工作流。框架是栈无关的（支持React、Angular等），支持从GUI原语到高级复合操作的混合粒度动作，通过MCP工具协调导航、操作和领域特定分析。

Result: 演示显示只需最小的改造工作，就能在实时UI环境中实现鲁棒的多步行为。框架展示了在现有企业应用中的可行性和实用性。

Conclusion: EmbeWebAgent通过将智能体直接嵌入UI，利用企业环境中的前后端控制优势，提供了一个更强大、更鲁棒的网页智能体解决方案，特别适合企业应用场景。

Abstract: Most web agents operate at the human interface level, observing screenshots or raw DOM trees without application-level access, which limits robustness and action expressiveness. In enterprise settings, however, explicit control of both the frontend and backend is available. We present EmbeWebAgent, a framework for embedding agents directly into existing UIs using lightweight frontend hooks (curated ARIA and URL-based observations, and a per-page function registry exposed via a WebSocket) and a reusable backend workflow that performs reasoning and takes actions. EmbeWebAgent is stack-agnostic (e.g., React or Angular), supports mixed-granularity actions ranging from GUI primitives to higher-level composites, and orchestrates navigation, manipulation, and domain-specific analytics via MCP tools. Our demo shows minimal retrofitting effort and robust multi-step behaviors grounded in a live UI setting. Live Demo: https://youtu.be/Cy06Ljee1JQ

</details>


### [151] [Concept Influence: Leveraging Interpretability to Improve Performance and Efficiency in Training Data Attribution](https://arxiv.org/abs/2602.14869)
*Matthew Kowal,Goncalo Paulo,Louis Jaburi,Tom Tseng,Lev E McKinney,Stefan Heimersheim,Aaron David Tucker,Adam Gleave,Kellin Pelrine*

Main category: cs.AI

TL;DR: 提出Concept Influence方法，利用模型内部可解释结构进行训练数据归因，相比传统影响函数更高效且能捕捉语义相似性


<details>
  <summary>Details</summary>
Motivation: 现有训练数据归因方法（如影响函数）计算成本高，且基于单个测试样例归因，容易偏向句法相似性而非语义相似性，难以扩展到抽象行为分析

Method: 引入Concept Influence方法，将模型行为归因到语义方向（如线性探针或稀疏自编码器特征），而非单个测试样例；提出基于探针的归因方法作为Concept Influence的一阶近似

Result: Concept Influence及其近似方法在涌现错位基准测试和实际后训练数据集上验证有效，性能与传统影响函数相当，但计算效率提高一个数量级以上

Conclusion: 在传统TDA流程中融入可解释结构可以实现更可扩展、可解释的模型行为控制，通过数据更好地理解和调控模型行为

Abstract: As large language models are increasingly trained and fine-tuned, practitioners need methods to identify which training data drive specific behaviors, particularly unintended ones. Training Data Attribution (TDA) methods address this by estimating datapoint influence. Existing approaches like influence functions are both computationally expensive and attribute based on single test examples, which can bias results toward syntactic rather than semantic similarity. To address these issues of scalability and influence to abstract behavior, we leverage interpretable structures within the model during the attribution. First, we introduce Concept Influence which attribute model behavior to semantic directions (such as linear probes or sparse autoencoder features) rather than individual test examples. Second, we show that simple probe-based attribution methods are first-order approximations of Concept Influence that achieve comparable performance while being over an order-of-magnitude faster. We empirically validate Concept Influence and approximations across emergent misalignment benchmarks and real post-training datasets, and demonstrate they achieve comparable performance to classical influence functions while being substantially more scalable. More broadly, we show that incorporating interpretable structure within traditional TDA pipelines can enable more scalable, explainable, and better control of model behavior through data.

</details>


### [152] [Lifted Relational Probabilistic Inference via Implicit Learning](https://arxiv.org/abs/2602.14890)
*Luise Ge,Brendan Juba,Kris Nilsson,Alison Shao*

Main category: cs.AI

TL;DR: 提出首个多项式时间框架，通过隐式学习和推理在关系概率逻辑中同时进行个体和世界的提升推理，无需显式构建模型。


<details>
  <summary>Details</summary>
Motivation: 解决一阶关系领域中归纳学习与演绎推理之间的长期矛盾，传统提升推理需要完整模型且学习模型通常不可行。

Method: 将不完整一阶公理与部分观测样本合并到SOS层次的有界度片段中，同时进行两种提升：基础提升（重命名等价的基础矩共享变量）和世界提升（并行执行伪模型）。

Result: 实现了首个多项式时间框架，能够隐式学习一阶概率逻辑，并在个体和世界两个层面进行提升推理。

Conclusion: 通过隐式学习推理方法成功调和了学习与推理的张力，为关系概率逻辑提供了高效的计算框架。

Abstract: Reconciling the tension between inductive learning and deductive reasoning in first-order relational domains is a longstanding challenge in AI. We study the problem of answering queries in a first-order relational probabilistic logic through a joint effort of learning and reasoning, without ever constructing an explicit model. Traditional lifted inference assumes access to a complete model and exploits symmetry to evaluate probabilistic queries; however, learning such models from partial, noisy observations is intractable in general. We reconcile these two challenges through implicit learning to reason and first-order relational probabilistic inference techniques. More specifically, we merge incomplete first-order axioms with independently sampled, partially observed examples into a bounded-degree fragment of the sum-of-squares (SOS) hierarchy in polynomial time. Our algorithm performs two lifts simultaneously: (i) grounding-lift, where renaming-equivalent ground moments share one variable, collapsing the domain of individuals; and (ii) world-lift, where all pseudo-models (partial world assignments) are enforced in parallel, producing a global bound that holds across all worlds consistent with the learned constraints. These innovations yield the first polynomial-time framework that implicitly learns a first-order probabilistic logic and performs lifted inference over both individuals and worlds.

</details>


### [153] [The Potential of CoT for Reasoning: A Closer Look at Trace Dynamics](https://arxiv.org/abs/2602.14903)
*Gregor Bachmann,Yichen Jiang,Seyed Mohsen Moosavi Dezfooli,Moin Nabi*

Main category: cs.AI

TL;DR: 本文通过引入"潜力"概念分析CoT推理过程，发现推理轨迹呈现非单调性、尖锐峰值等模式，并证明仅需20%的强模型CoT就能"解锁"弱模型性能。


<details>
  <summary>Details</summary>
Motivation: 尽管CoT提示已成为从大语言模型引出类似推理响应的标准技术，但其成功背后的驱动机制仍不清楚。本文旨在深入分析CoT推理轨迹，理解哪些部分真正贡献于最终答案。

Method: 引入"潜力"概念量化CoT各部分对正确答案可能性的提升程度，通过竞赛级数学问题分析推理轨迹，并研究CoT可转移性，测量弱模型在强模型部分CoT下的表现。

Result: 发现推理轨迹呈现三种模式：1) 非单调性（推理偏离），2) 尖锐但难解释的峰值（推理洞见和跳跃），3) 有时存在幸运猜测。可转移性实验显示仅需20%的强模型CoT就能让弱模型解决原本无法解决的问题。

Conclusion: CoT推理机制在很大程度上是可转移的，但部分行为难以从人类视角理解。研究揭示了CoT成功的关键在于推理洞见，而非整个推理过程的连贯性。

Abstract: Chain-of-thought (CoT) prompting is a de-facto standard technique to elicit reasoning-like responses from large language models (LLMs), allowing them to spell out individual steps before giving a final answer. While the resemblance to human-like reasoning is undeniable, the driving forces underpinning the success of CoT reasoning still remain largely unclear. In this work, we perform an in-depth analysis of CoT traces originating from competition-level mathematics questions, with the aim of better understanding how, and which parts of CoT actually contribute to the final answer. To this end, we introduce the notion of a potential, quantifying how much a given part of CoT increases the likelihood of a correct completion. Upon examination of reasoning traces through the lens of the potential, we identify surprising patterns including (1) its often strong non-monotonicity (due to reasoning tangents), (2) very sharp but sometimes tough to interpret spikes (reasoning insights and jumps) as well as (3) at times lucky guesses, where the model arrives at the correct answer without providing any relevant justifications before. While some of the behaviours of the potential are readily interpretable and align with human intuition (such as insights and tangents), others remain difficult to understand from a human perspective. To further quantify the reliance of LLMs on reasoning insights, we investigate the notion of CoT transferability, where we measure the potential of a weaker model under the partial CoT from another, stronger model. Indeed aligning with our previous results, we find that as little as 20% of partial CoT can ``unlock'' the performance of the weaker model on problems that were previously unsolvable for it, highlighting that a large part of the mechanics underpinning CoT are transferable.

</details>


### [154] [Position: Introspective Experience from Conversational Environments as a Path to Better Learning](https://arxiv.org/abs/2602.14910)
*Claudiu Cristian Musat,Jackson Tolins,Diego Antognini,Jingling Li,Martin Klissarov,Tom Duerig*

Main category: cs.AI

TL;DR: 论文主张通过语言自我反思而非规模扩展来实现稳健推理，借鉴维果茨基发展心理学，提出内省对话是下一代通用智能的关键杠杆。


<details>
  <summary>Details</summary>
Motivation: 当前AI训练方法将推理视为规模的涌现属性，但作者认为这种方法存在局限。他们主张从发展心理学角度重新思考，认为稳健推理应源于语言自我反思，这需要高质量的社会互动来培养。

Method: 基于维果茨基发展心理学提出三个核心立场：1) 私人思维的社会起源：从对话环境中学习；2) 对话式内省体验：通过对话脚手架进行意义建构；3) 对话质量即数据质量：对话的多样性和严谨性决定推理深度。

Result: 论文提出了一个理论框架，强调对话质量而非数据规模是下一代通用智能发展的关键。通过优化对话脚手架，可以提升AI的推理能力和计算效率。

Conclusion: 优化对话脚手架是发展下一代通用智能的主要杠杆，对话质量决定了AI的推理深度和计算效率，这为AI发展提供了新的理论方向。

Abstract: Current approaches to AI training treat reasoning as an emergent property of scale. We argue instead that robust reasoning emerges from linguistic self-reflection, itself internalized from high-quality social interaction. Drawing on Vygotskian developmental psychology, we advance three core positions centered on Introspection. First, we argue for the Social Genesis of the Private Mind: learning from conversational environments rises to prominence as a new way to make sense of the world; the friction of aligning with another agent, internal or not, refines and crystallizes the reasoning process. Second, we argue that dialogically scaffolded introspective experiences allow agents to engage in sense-making that decouples learning from immediate data streams, transforming raw environmental data into rich, learnable narratives. Finally, we contend that Dialogue Quality is the New Data Quality: the depth of an agent's private reasoning, and its efficiency regarding test-time compute, is determined by the diversity and rigor of the dialogues it has mastered. We conclude that optimizing these conversational scaffolds is the primary lever for the next generation of general intelligence.

</details>


### [155] [ReusStdFlow: A Standardized Reusability Framework for Dynamic Workflow Construction in Agentic AI](https://arxiv.org/abs/2602.14922)
*Gaoyang Zhang,Shanghong Zou,Yafang Wang,He Zhang,Ruohua Xu,Feng Zhao*

Main category: cs.AI

TL;DR: ReusStdFlow框架通过"提取-存储-构建"范式解决企业Agentic AI中的可重用性困境和结构幻觉问题，将异构DSL解构为标准模块化工作流片段，实现90%以上的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决企业Agentic AI中的"可重用性困境"和结构幻觉问题，实现企业数字资产的自动化重组和高效复用。

Method: 提出ReusStdFlow框架，采用"提取-存储-构建"范式：1) 将异构平台特定DSL解构为标准模块化工作流片段；2) 使用图数据库和向量数据库的双重知识架构；3) 采用检索增强生成(RAG)策略智能组装工作流。

Result: 在200个真实n8n工作流上测试，系统在提取和构建两方面都达到90%以上的准确率。

Conclusion: 该框架为企业数字资产的自动化重组和高效复用提供了标准化解决方案。

Abstract: To address the ``reusability dilemma'' and structural hallucinations in enterprise Agentic AI,this paper proposes ReusStdFlow, a framework centered on a novel ``Extraction-Storage-Construction'' paradigm. The framework deconstructs heterogeneous, platform-specific Domain Specific Languages (DSLs) into standardized, modular workflow segments. It employs a dual knowledge architecture-integrating graph and vector databases-to facilitate synergistic retrieval of both topological structures and functional semantics. Finally, workflows are intelligently assembled using a retrieval-augmented generation (RAG) strategy. Tested on 200 real-world n8n workflows, the system achieves over 90% accuracy in both extraction and construction. This framework provides a standardized solution for the automated reorganization and efficient reuse of enterprise digital assets.

</details>


### [156] [MAC-AMP: A Closed-Loop Multi-Agent Collaboration System for Multi-Objective Antimicrobial Peptide Design](https://arxiv.org/abs/2602.14926)
*Gen Zhou,Sugitha Janarthanan,Lianghong Chen,Pingzhao Hu*

Main category: cs.AI

TL;DR: MAC-AMP是一个基于多智能体大语言模型的闭环协作系统，用于多目标抗菌肽设计，通过模拟同行评审-自适应强化学习框架，在活性、毒性、新颖性等多个关键分子属性上实现优化。


<details>
  <summary>Details</summary>
Motivation: 针对抗菌素耐药性的全球健康威胁，抗菌肽成为有前景的解决方案。现有AI设计模型难以平衡活性、毒性和新颖性等关键目标，且评分方法僵化不透明。随着大语言模型能力快速提升，多智能体协作在复杂科学设计场景中展现出巨大潜力。

Method: 提出MAC-AMP闭环多智能体协作系统，采用完全自主的模拟同行评审-自适应强化学习框架。系统仅需任务描述和示例数据集即可设计新型抗菌肽，具有跨领域可迁移性，支持多目标优化并保持可解释性而非"黑箱"。

Result: 实验表明MAC-AMP优于其他抗菌肽生成模型，能有效优化多个关键分子属性，在抗菌活性、抗菌肽相似性、毒性合规性和结构可靠性方面表现出卓越结果。

Conclusion: MAC-AMP通过引入基于大语言模型的闭环多智能体协作系统，解决了传统抗菌肽设计模型在多目标优化、可解释性和跨领域迁移方面的局限性，为抗菌肽发现提供了创新且有效的AI驱动方法。

Abstract: To address the global health threat of antimicrobial resistance, antimicrobial peptides (AMP) are being explored for their potent and promising ability to fight resistant pathogens. While artificial intelligence (AI) is being employed to advance AMP discovery and design, most AMP design models struggle to balance key goals like activity, toxicity, and novelty, using rigid or unclear scoring methods that make results hard to interpret and optimize. As the capabilities of Large Language Models (LLM) advance and evolve swiftly, we turn to AI multi-agent collaboration based on such models (multi-agent LLMs), which show rapidly rising potential in complex scientific design scenarios. Based on this, we introduce MAC-AMP, a closed-loop multi-agent collaboration (MAC) system for multi-objective AMP design. The system implements a fully autonomous simulated peer review-adaptive reinforcement learning framework that requires only a task description and example dataset to design novel AMPs. The novelty of our work lies in introducing a closed-loop multi-agent system for AMP design, with cross-domain transferability, that supports multi-objective optimization while remaining explainable rather than a 'black box'. Experiments show that MAC-AMP outperforms other AMP generative models by effectively optimizing AMP generation for multiple key molecular properties, demonstrating exceptional results in antibacterial activity, AMP likeliness, toxicity compliance, and structural reliability.

</details>


### [157] [On the Semantics of Primary Cause in Hybrid Dynamic Domains](https://arxiv.org/abs/2602.14994)
*Shakil M. Khan,Asim Mehmood,Sandra Zilles*

Main category: cs.AI

TL;DR: 本文在混合时序情境演算框架下提出了两种主要原因定义，并证明它们等价，同时验证了这些定义具有直观合理的性质。


<details>
  <summary>Details</summary>
Motivation: 实际因果推理是理性研究的基础，但现有研究主要关注离散变化，对连续变化的研究较少。现实世界中变化既有离散也有连续（混合），因此需要在混合行动理论框架中研究因果关系。

Method: 在混合时序情境演算框架下提出两种主要原因定义：一种是基础性的，另一种通过贡献形式化因果关系，并使用修改后的"but-for"测试从反事实角度验证。证明这两种定义等价。

Result: 成功提出了两种主要原因定义，并证明了它们的等价性。同时展示了这些定义具有直观合理的性质。

Conclusion: 本文在混合行动理论框架中建立了因果关系的形式化定义，为处理离散和连续混合变化的实际因果推理提供了理论基础。

Abstract: Reasoning about actual causes of observed effects is fundamental to the study of rationality. This important problem has been studied since the time of Aristotle, with formal mathematical accounts emerging recently. We live in a world where change due to actions can be both discrete and continuous, that is, hybrid. Yet, despite extensive research on actual causation, only few recent studies looked into causation with continuous change. Building on recent progress, in this paper we propose two definitions of primary cause in a hybrid action-theoretic framework, namely the hybrid temporal situation calculus. One of these is foundational in nature while the other formalizes causation through contributions, which can then be verified from a counterfactual perspective using a modified ``but-for'' test. We prove that these two definitions are indeed equivalent. We then show that our definitions of causation have some intuitively justifiable properties.

</details>


### [158] [Hunt Globally: Deep Research AI Agents for Drug Asset Scouting in Investing, Business Development, and Search & Evaluation](https://arxiv.org/abs/2602.15019)
*Alisa Vinogradova,Vlad Vinogradov,Luba Greenwood,Ilya Yasny,Dmitry Kobyzev,Shoman Kasbekar,Kong Nguyen,Dmitrii Radkevich,Roman Doronin,Andrey Doronichev*

Main category: cs.AI

TL;DR: 提出用于药物资产侦察的基准测试方法和自学习Bioptic Agent，在非英语、区域性药物资产发现中显著优于现有AI系统


<details>
  <summary>Details</summary>
Motivation: 生物制药创新格局变化：超过85%的专利来自美国以外，中国占全球近一半；约30%的药物开发在中国。传统AI系统在跨语言、异构源中发现"隐形资产"时召回率低且存在幻觉，给投资者带来数十亿美元风险

Method: 提出药物资产侦察基准测试方法，构建多语言多代理流程的完整性基准，使用专家查询作为先验条件生成基准查询。开发基于树的自学习Bioptic Agent，采用LLM-as-judge评估校准专家意见

Result: Bioptic Agent达到79.7% F1分数，显著优于Claude Opus 4.6(56.2%)、Gemini 3 Pro+Deep Research(50.6%)、GPT-5.2 Pro(46.6%)、Perplexity Deep Research(44.2%)和Exa Websets(26.9%)。性能随计算资源增加而显著提升

Conclusion: Bioptic Agent在药物资产侦察中实现高召回率、无幻觉的发现，解决当前AI系统在跨语言、区域性资产发现中的局限性，为投资者提供竞争优势

Abstract: Bio-pharmaceutical innovation has shifted: many new drug assets now originate outside the United States and are disclosed primarily via regional, non-English channels. Recent data suggests >85% of patent filings originate outside the U.S., with China accounting for nearly half of the global total; a growing share of scholarly output is also non-U.S. Industry estimates put China at ~30% of global drug development, spanning 1,200+ novel candidates. In this high-stakes environment, failing to surface "under-the-radar" assets creates multi-billion-dollar risk for investors and business development teams, making asset scouting a coverage-critical competition where speed and completeness drive value. Yet today's Deep Research AI agents still lag human experts in achieving high-recall discovery across heterogeneous, multilingual sources without hallucinations.
  We propose a benchmarking methodology for drug asset scouting and a tuned, tree-based self-learning Bioptic Agent aimed at complete, non-hallucinated scouting. We construct a challenging completeness benchmark using a multilingual multi-agent pipeline: complex user queries paired with ground-truth assets that are largely outside U.S.-centric radar. To reflect real deal complexity, we collected screening queries from expert investors, BD, and VC professionals and used them as priors to conditionally generate benchmark queries. For grading, we use LLM-as-judge evaluation calibrated to expert opinions. We compare Bioptic Agent against Claude Opus 4.6, OpenAI GPT-5.2 Pro, Perplexity Deep Research, Gemini 3 Pro + Deep Research, and Exa Websets. Bioptic Agent achieves 79.7% F1 versus 56.2% (Claude Opus 4.6), 50.6% (Gemini 3 Pro + Deep Research), 46.6% (GPT-5.2 Pro), 44.2% (Perplexity Deep Research), and 26.9% (Exa Websets). Performance improves steeply with additional compute, supporting the view that more compute yields better results.

</details>

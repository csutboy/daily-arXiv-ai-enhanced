<div id=toc></div>

# Table of Contents

- [econ.GN](#econ.GN) [Total: 3]
- [cs.RO](#cs.RO) [Total: 18]
- [econ.TH](#econ.TH) [Total: 4]
- [q-fin.GN](#q-fin.GN) [Total: 1]
- [stat.AP](#stat.AP) [Total: 1]
- [eess.SY](#eess.SY) [Total: 14]
- [cs.AI](#cs.AI) [Total: 36]
- [econ.EM](#econ.EM) [Total: 1]
- [cs.CY](#cs.CY) [Total: 6]


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [1] [Edgeworth's exact and naturally weighted evolutionary utilitarianism and the happiness of Mr. Pongo](https://arxiv.org/abs/2510.20854)
*Alberto Baccini*

Main category: econ.GN

TL;DR: 本文挑战了对埃奇沃思的传统解读，重构了他通过数学统一道德科学的智力项目。他通过将功利主义重构为精确科学，基于心理物理学和进化生物学，得出了反平等主义的结论。


<details>
  <summary>Details</summary>
Motivation: 埃奇沃思旨在通过数学统一道德科学，将功利主义重新配置为精确科学，并将其建立在心理物理学和进化生物学的基础上。

Method: 他将个体建模为"准费希纳"函数，根据个体在进化秩序中的位置确定其快乐能力，通过按个体快乐能力分配资源来解决最大化问题。

Result: 得出了激进的反平等主义结论，认为为了在自然不平等的存在中最大化福利，必须根据个体的快乐能力进行分配。这一逻辑被应用于不同进化秩序的存在、人类种族、性别和阶级。

Conclusion: 该体系本质上利用科学的表面中立性来自然化和合理化既有的社会等级制度。经济学家后来对其功利主义的剔除虽然使其工具更容易被接受，但却削弱了他的整体哲学体系。

Abstract: This article challenges the conventional reading of Francis Ysidro Edgeworth
by reconstructing his intellectual project of unifying the moral sciences
through mathematics. The contribution he made in the first phase of his
writing, culminating in \textit{Mathematical Psychics}, aimed to reconfigure
utilitarianism as an exact science, grounding it in psychophysics and
evolutionary biology. In order to solve the utilitarian problem of maximizing
pleasure for a given set of sentient beings, he modeled individuals as
``quasi-Fechnerian'' functions, which incorporated their capacity for pleasure
as determined by their place in the evolutionary order. The problem of
maximization is solved by distributing means according to the individuals'
capacity for pleasure. His radical anti-egalitarian conclusions did not stem
from an abstract principle of justice, but from the necessity to maximize
welfare among naturally unequal beings. This logic was applied not only to
sentients of different evolutionary orders, such as Mr. Pongo, a famous
gorilla, and humans, but also to human races, sexes, and classes. The system,
in essence, uses the apparent neutrality of science to naturalize and justify
pre-existing social hierarchies. This analysis reveals that the subsequent
surgical removal of his utilitarianism by economists, starting with Schumpeter,
while making his tools palatable, eviscerates his overarching philosophical
system.

</details>


### [2] [State capacity, innovation, and endogenous development in Chile](https://arxiv.org/abs/2510.20863)
*Rodrigo Barra Novoa*

Main category: econ.GN

TL;DR: 智利1990-2022年产业政策演变分析：尽管科技投资增加，但创新能力和协调机制仍显薄弱，需要适应性治理和将创新视为公共产品的伦理愿景。


<details>
  <summary>Details</summary>
Motivation: 在全球政府重新强调创新主体角色的背景下，智利作为稳定开放经济体，在扩大科技投资的同时仍难以转化为可持续创新能力，这一矛盾现象值得研究。

Method: 采用纵向案例研究方法，结合官方数据，整合演化经济学、公共政策和人文主义伦理学理论框架。

Result: 智利改善了创新制度，但仍存在协调薄弱、区域不平等和知识文化脆弱等问题。

Conclusion: 实现包容性创新需要适应性治理和将创新视为公共产品的伦理愿景。

Abstract: The study explores the evolution of Chile's industrial policy from 1990 to
2022 through the lens of state capacity, innovation and endogenous development.
In a global context where governments are reasserting their role as active
agents of innovation, Chile presents a paradox. It is a stable and open economy
that has expanded investment in science and technology but still struggles to
transform this effort into sustainable capabilities. Drawing on the works of
Mazzucato, Aghion, Howitt, Mokyr, Samuelson and Sampedro, the study integrates
evolutionary economics, public policy and humanist ethics. Using a longitudinal
case study approach and official data, it finds that Chile has improved its
innovation institutions but continues to experience weak coordination, regional
inequality and a fragile culture of knowledge. The research concludes that
achieving inclusive innovation requires adaptive governance and an ethical
vision of innovation as a public good.

</details>


### [3] [Central Bank Digital Currency, Flight-to-Quality, and Bank-Runs in an Agent-Based Model](https://arxiv.org/abs/2510.21071)
*Emilio Barucci,Andrea Gurgone,Giulia Iori,Michele Azzone*

Main category: econ.GN

TL;DR: 该研究使用基于代理的宏观经济模型分析央行数字货币（CBDC）对金融稳定和福利的影响，发现CBDC可能加剧银行挤兑并导致金融不稳定，但通过设置持有上限可以改善福利。


<details>
  <summary>Details</summary>
Motivation: 分析CBDC引入对金融稳定性和福利的影响，特别是在银行风险感知变化导致存款向CBDC转移的情况下。

Method: 使用基于代理的宏观经济模型，考虑企业、银行和家庭在劳动力、商品、信贷和银行间市场的互动，模拟家庭基于银行风险感知将流动性从存款转向CBDC的行为。

Result: CBDC引入加剧银行挤兑和金融不稳定；对宏观经济变量影响较小但企业贷款利率上升、信贷减少；导致财富从企业和银行向家庭重新分配，银行违约率上升；可能产生负面福利效应。

Conclusion: CBDC可能对金融稳定产生负面影响，但通过设置持有上限可以实现福利改善。

Abstract: We analyse financial stability and welfare impacts associated with the
introduction of a Central Bank Digital Currency (CBDC) in a macroeconomic
agent-based model. The model considers firms, banks, and households interacting
on labour, goods, credit, and interbank markets. Households move their
liquidity from deposits to CBDC based on the perceived riskiness of their
banks. We find that the introduction of CBDC exacerbates bank-runs and may lead
to financial instability phenomena. The effect can be changed by introducing a
limit on CBDC holdings. The adoption of CBDC has little effect on macroeconomic
variables but the interest rate on loans to firms goes up and credit goes down
in a limited way. CBDC leads to a redistribution of wealth from firms and banks
to households with a higher bank default rate. CBDC may have negative welfare
effects, but a bound on holding enables a welfare improvement.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [4] [ROPES: Robotic Pose Estimation via Score-Based Causal Representation Learning](https://arxiv.org/abs/2510.20884)
*Pranamya Kulkarni,Puranjay Datta,Burak Varıcı,Emre Acartürk,Karthikeyan Shanmugam,Ali Tajer*

Main category: cs.RO

TL;DR: ROPES是一个无监督的因果表示学习框架，通过基于分数的因果表示学习来解决机器人姿态估计问题，仅利用分布变化而不需要标注数据就能成功解耦潜在生成因子。


<details>
  <summary>Details</summary>
Motivation: 因果表示学习在理论和实际应用之间存在较大差距，本文旨在通过将CRL应用于机器人领域来缩小这一差距，特别是解决机器人姿态估计这一明确问题。

Method: ROPES框架基于干预性因果表示学习，通过操纵机器人关节执行器产生图像变化，识别可控制的潜在变量（如关节角度），利用基于分数的方法进行无监督学习。

Result: 在半合成机械臂实验中，ROPES能够高保真地解耦潜在生成因子，与真实值高度一致，且仅利用分布变化而不需要任何标注数据。

Conclusion: 机器人姿态估计可以作为因果表示学习的近乎实用的测试平台，ROPES展示了无监督CRL在机器人领域的应用潜力。

Abstract: Causal representation learning (CRL) has emerged as a powerful unsupervised
framework that (i) disentangles the latent generative factors underlying
high-dimensional data, and (ii) learns the cause-and-effect interactions among
the disentangled variables. Despite extensive recent advances in
identifiability and some practical progress, a substantial gap remains between
theory and real-world practice. This paper takes a step toward closing that gap
by bringing CRL to robotics, a domain that has motivated CRL. Specifically,
this paper addresses the well-defined robot pose estimation -- the recovery of
position and orientation from raw images -- by introducing Robotic Pose
Estimation via Score-Based CRL (ROPES). Being an unsupervised framework, ROPES
embodies the essence of interventional CRL by identifying those generative
factors that are actuated: images are generated by intrinsic and extrinsic
latent factors (e.g., joint angles, arm/limb geometry, lighting, background,
and camera configuration) and the objective is to disentangle and recover the
controllable latent variables, i.e., those that can be directly manipulated
(intervened upon) through actuation. Interventional CRL theory shows that
variables that undergo variations via interventions can be identified. In
robotics, such interventions arise naturally by commanding actuators of various
joints and recording images under varied controls. Empirical evaluations in
semi-synthetic manipulator experiments demonstrate that ROPES successfully
disentangles latent generative factors with high fidelity with respect to the
ground truth. Crucially, this is achieved by leveraging only distributional
changes, without using any labeled data. The paper also includes a comparison
with a baseline based on a recently proposed semi-supervised framework. This
paper concludes by positioning robot pose estimation as a near-practical
testbed for CRL.

</details>


### [5] [Aircraft Collision Avoidance Systems: Technological Challenges and Solutions on the Path to Regulatory Acceptance](https://arxiv.org/abs/2510.20916)
*Sydney M. Katz,Robert J. Moss,Dylan M. Asmar,Wesley A. Olson,James K. Kuchar,Mykel J. Kochenderfer*

Main category: cs.RO

TL;DR: 该论文综述了飞机防撞系统的技术挑战、解决方案及验证过程，强调经过严格验证并被监管机构接受的系统，可作为其他安全关键系统的案例研究。


<details>
  <summary>Details</summary>
Motivation: 飞机防撞系统对现代航空至关重要，需要解决监视、决策和验证等技术挑战，这些挑战激发了数十年的研发努力。

Method: 文章通过综述过去几十年的研究成果，重点分析经过严格验证过程和监管机构接受的防撞系统解决方案。

Result: 识别了飞机防撞系统面临的主要技术挑战，并总结了各种已提出的解决方案，特别是那些经过验证并被采纳的系统。

Conclusion: 飞机防撞系统面临的挑战在其他领域也普遍存在，这些系统可作为安全关键系统的宝贵案例研究，为更广泛的应用提供见解。

Abstract: Aircraft collision avoidance systems is critical to modern aviation. These
systems are designed to predict potential collisions between aircraft and
recommend appropriate avoidance actions. Creating effective collision avoidance
systems requires solutions to a variety of technical challenges related to
surveillance, decision making, and validation. These challenges have sparked
significant research and development efforts over the past several decades that
have resulted in a variety of proposed solutions. This article provides an
overview of these challenges and solutions with an emphasis on those that have
been put through a rigorous validation process and accepted by regulatory
bodies. The challenges posed by the collision avoidance problem are often
present in other domains, and aircraft collision avoidance systems can serve as
case studies that provide valuable insights for a wide range of safety-critical
systems.

</details>


### [6] [SutureBot: A Precision Framework & Benchmark For Autonomous End-to-End Suturing](https://arxiv.org/abs/2510.20965)
*Jesse Haworth,Juo-Tung Chen,Nigel Nelson,Ji Woong Kim,Masoud Moghani,Chelsea Finn,Axel Krieger*

Main category: cs.RO

TL;DR: SutureBot是一个在da Vinci Research Kit上的自主缝合基准测试，包含针头拾取、组织插入和打结三个步骤。作者发布了包含1890次缝合演示的高质量数据集，并提出了优化插入点精度的目标条件框架，相比基线提升了59%-74%的精度。


<details>
  <summary>Details</summary>
Motivation: 缝合是典型的长时程灵巧操作任务，需要协调的针头抓取、精确的组织穿透和安全的打结。尽管已有许多端到端自主化的努力，但完全自主的缝合流程尚未在物理硬件上实现。

Method: 提出了目标条件框架，显式优化插入点精度；建立了缝合基准测试，评估了包括π₀、GR00T N1、OpenVLA-OFT和多任务ACT在内的最先进视觉-语言-动作模型，每个模型都配备了高级任务预测策略。

Result: 目标条件框架相比仅任务基线将目标精度提升了59%-74%；发布了包含1890次缝合演示的高保真数据集；建立了可重复评估的基准测试。

Conclusion: 自主缝合是实现手术机器人自主化的关键里程碑。这些贡献支持可重复评估和开发专注于精度的长时程灵巧操作策略，这些策略对于端到端缝合是必要的。

Abstract: Robotic suturing is a prototypical long-horizon dexterous manipulation task,
requiring coordinated needle grasping, precise tissue penetration, and secure
knot tying. Despite numerous efforts toward end-to-end autonomy, a fully
autonomous suturing pipeline has yet to be demonstrated on physical hardware.
We introduce SutureBot: an autonomous suturing benchmark on the da Vinci
Research Kit (dVRK), spanning needle pickup, tissue insertion, and knot tying.
To ensure repeatability, we release a high-fidelity dataset comprising 1,890
suturing demonstrations. Furthermore, we propose a goal-conditioned framework
that explicitly optimizes insertion-point precision, improving targeting
accuracy by 59\%-74\% over a task-only baseline. To establish this task as a
benchmark for dexterous imitation learning, we evaluate state-of-the-art
vision-language-action (VLA) models, including $\pi_0$, GR00T N1, OpenVLA-OFT,
and multitask ACT, each augmented with a high-level task-prediction policy.
Autonomous suturing is a key milestone toward achieving robotic autonomy in
surgery. These contributions support reproducible evaluation and development of
precision-focused, long-horizon dexterous manipulation policies necessary for
end-to-end suturing. Dataset is available at:
https://huggingface.co/datasets/jchen396/suturebot

</details>


### [7] [Robust Point Cloud Reinforcement Learning via PCA-Based Canonicalization](https://arxiv.org/abs/2510.20974)
*Michael Bezick,Vittorio Giammarino,Ahmed H. Qureshi*

Main category: cs.RO

TL;DR: 提出PPC框架，通过将任意刚体变换下的点云映射到规范姿态，解决点云强化学习对相机姿态变化的敏感性问题，提高机器人控制任务的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 点云强化学习虽然能减轻基于外观的脆弱性，但对相机姿态失配仍然敏感，这在实际应用中影响了可靠性。

Method: 提出PCA点云(PPC)规范化框架，将任意刚体变换下的点云映射到唯一的规范姿态，使观测对齐到一致的坐标系。

Result: 实验表明PPC在具有挑战性的机器人任务中提高了对未见相机姿态的鲁棒性。

Conclusion: PPC为领域随机化提供了一个原则性的替代方案，能够显著减少视角引起的不一致性。

Abstract: Reinforcement Learning (RL) from raw visual input has achieved impressive
successes in recent years, yet it remains fragile to out-of-distribution
variations such as changes in lighting, color, and viewpoint. Point Cloud
Reinforcement Learning (PC-RL) offers a promising alternative by mitigating
appearance-based brittleness, but its sensitivity to camera pose mismatches
continues to undermine reliability in realistic settings. To address this
challenge, we propose PCA Point Cloud (PPC), a canonicalization framework
specifically tailored for downstream robotic control. PPC maps point clouds
under arbitrary rigid-body transformations to a unique canonical pose, aligning
observations to a consistent frame, thereby substantially decreasing
viewpoint-induced inconsistencies. In our experiments, we show that PPC
improves robustness to unseen camera poses across challenging robotic tasks,
providing a principled alternative to domain randomization.

</details>


### [8] [HRT1: One-Shot Human-to-Robot Trajectory Transfer for Mobile Manipulation](https://arxiv.org/abs/2510.21026)
*Sai Haneesh Allu,Jishnu Jaykumar P,Ninad Khargonkar,Tyler Summers,Jian Yao,Yu Xiang*

Main category: cs.RO

TL;DR: 提出基于AR头显收集人类演示视频，通过视频理解提取3D手部轨迹，转换为机器人末端轨迹，最终优化生成机器人配置空间轨迹的系统


<details>
  <summary>Details</summary>
Motivation: 让机器人能够通过观看一次人类演示视频，就能在不同环境中重复执行相同的移动操作任务，即使物体摆放位置与演示时不同

Method: 四模块系统：AR头显数据收集、视频理解检测物体和提取3D手部轨迹、人手机器人末端轨迹转换、轨迹优化生成机器人配置空间轨迹

Result: 在移动机械臂上进行了不同操作任务的实验，验证了系统的有效性

Conclusion: 该系统实现了从人类演示到机器人操作的轨迹传递，使机器人能够在不同环境中重复执行学习到的任务

Abstract: We introduce a novel system for human-to-robot trajectory transfer that
enables robots to manipulate objects by learning from human demonstration
videos. The system consists of four modules. The first module is a data
collection module that is designed to collect human demonstration videos from
the point of view of a robot using an AR headset. The second module is a video
understanding module that detects objects and extracts 3D human-hand
trajectories from demonstration videos. The third module transfers a human-hand
trajectory into a reference trajectory of a robot end-effector in 3D space. The
last module utilizes a trajectory optimization algorithm to solve a trajectory
in the robot configuration space that can follow the end-effector trajectory
transferred from the human demonstration. Consequently, these modules enable a
robot to watch a human demonstration video once and then repeat the same mobile
manipulation task in different environments, even when objects are placed
differently from the demonstrations. Experiments of different manipulation
tasks are conducted on a mobile manipulator to verify the effectiveness of our
system

</details>


### [9] [Sequentially Teaching Sequential Tasks $(ST)^2$: Teaching Robots Long-horizon Manipulation Skills](https://arxiv.org/abs/2510.21046)
*Zlatan Ajanović,Ravi Prakash,Leandro de Souza Rosa,Jens Kober*

Main category: cs.RO

TL;DR: 比较两种机器人示教框架：传统整体方法和顺序方法。顺序方法$(ST)^2$允许用户定义关键点进行增量式示教。用户研究表明两种方法在轨迹质量和成功率上相似，但用户偏好不同。


<details>
  <summary>Details</summary>
Motivation: 长时程多技能任务的教学存在偏差累积、分布偏移和教师疲劳等问题，需要研究更有效的示教框架。

Method: 提出$(ST)^2$顺序学习方法，允许用户分段定义关键点进行增量示教。通过16名参与者在零售环境中的用户研究，比较传统整体方法和顺序方法的性能。

Result: 两种方法在轨迹质量和成功率上表现相似。用户偏好存在分歧：部分用户喜欢顺序方法的迭代控制，部分用户偏好整体方法的简单性。

Conclusion: 两种示教框架各有优势，选择应基于具体应用场景和用户偏好。顺序方法提供了更好的控制性，而整体方法更简单易用。

Abstract: Learning from demonstration is effective for teaching robots complex skills
with high sample efficiency. However, teaching long-horizon tasks with multiple
skills is difficult, as deviations accumulate, distributional shift increases,
and human teachers become fatigued, raising the chance of failure. In this
work, we study user responses to two teaching frameworks: (i) a traditional
monolithic approach, where users demonstrate the entire trajectory of a
long-horizon task; and (ii) a sequential approach, where the task is segmented
by the user and demonstrations are provided step by step. To support this
study, we introduce $(ST)^2$, a sequential method for learning long-horizon
manipulation tasks that allows users to control the teaching flow by defining
key points, enabling incremental and structured demonstrations. We conducted a
user study on a restocking task with 16 participants in a realistic retail
environment to evaluate both user preference and method effectiveness. Our
objective and subjective results show that both methods achieve similar
trajectory quality and success rates. Some participants preferred the
sequential approach for its iterative control, while others favored the
monolithic approach for its simplicity.

</details>


### [10] [Revisiting Replanning from Scratch: Real-Time Incremental Planning with Fast Almost-Surely Asymptotically Optimal Planners](https://arxiv.org/abs/2510.21074)
*Mitchell E. C. Sabbadini,Andrew H. Liu,Joseph Ruan,Tyler S. Wilson,Zachary Kingston,Jonathan D. Gammell*

Main category: cs.RO

TL;DR: 本文挑战了反应式重规划必须更新现有计划的传统假设，提出使用快速渐近最优规划算法将增量规划问题转化为一系列独立问题求解，在动态环境中无需显式计划重用即可找到一致的全局路径。


<details>
  <summary>Details</summary>
Motivation: 传统反应式规划方法需要重用信息并更新密集规划图，这在信息变化时计算成本高昂，且在某些应用中检测变化需要大量努力。本文旨在寻找更高效的反应式重规划方法。

Method: 将增量规划问题转化为一系列独立问题，使用快速渐近最优规划算法（如EIT*和AORRTC）。这些算法快速找到初始解并收敛到最优解，无需显式计划重用。

Result: 模拟实验显示EIT*找到的中位解路径比测试的反应式规划算法更短，并在机器人臂的真实规划问题中通过AORRTC得到验证。

Conclusion: 反应式重规划不一定需要更新现有计划，使用渐近最优规划算法作为独立问题求解可以更高效地处理动态环境中的路径规划。

Abstract: Robots operating in changing environments either predict obstacle changes
and/or plan quickly enough to react to them. Predictive approaches require a
strong prior about the position and motion of obstacles. Reactive approaches
require no assumptions about their environment but must replan quickly and find
high-quality paths to navigate effectively.
  Reactive approaches often reuse information between queries to reduce
planning cost. These techniques are conceptually sound but updating dense
planning graphs when information changes can be computationally prohibitive. It
can also require significant effort to detect the changes in some applications.
  This paper revisits the long-held assumption that reactive replanning
requires updating existing plans. It shows that the incremental planning
problem can alternatively be solved more efficiently as a series of independent
problems using fast almost-surely asymptotically optimal (ASAO) planning
algorithms. These ASAO algorithms quickly find an initial solution and converge
towards an optimal solution which allows them to find consistent global plans
in the presence of changing obstacles without requiring explicit plan reuse.
This is demonstrated with simulated experiments where Effort Informed Trees
(EIT*) finds shorter median solution paths than the tested reactive planning
algorithms and is further validated using Asymptotically Optimal RRT-Connect
(AORRTC) on a real-world planning problem on a robot arm.

</details>


### [11] [Generalizable Hierarchical Skill Learning via Object-Centric Representation](https://arxiv.org/abs/2510.21121)
*Haibo Zhao,Yu Qi,Boce Hu,Yizhe Zhu,Ziyan Chen,Heng Tian,Xupeng Zhu,Owen Howell,Haojie Huang,Robin Walters,Dian Wang,Robert Platt*

Main category: cs.RO

TL;DR: GSL是一个分层策略学习框架，通过对象中心技能作为视觉语言模型和视觉运动策略之间的接口，显著提升了机器人操作中的策略泛化能力和样本效率。


<details>
  <summary>Details</summary>
Motivation: 解决机器人操作中策略泛化能力不足和样本效率低下的问题，特别是在面对未见过的空间布局、物体外观和任务组合时。

Method: 使用基础模型将演示分解为可转移的对象规范化技能基元，高层代理预测技能-对象对，低层模块将规范动作映射回世界坐标系执行。

Result: 在模拟环境中，GSL仅用每个任务3个演示就比使用30倍数据的基线方法在未见任务上表现好15.5%；在真实世界实验中，也优于使用10倍数据的基线。

Conclusion: GSL的结构化但灵活设计显著提高了样本效率和泛化能力，为机器人操作提供了有效的分层策略学习解决方案。

Abstract: We present Generalizable Hierarchical Skill Learning (GSL), a novel framework
for hierarchical policy learning that significantly improves policy
generalization and sample efficiency in robot manipulation. One core idea of
GSL is to use object-centric skills as an interface that bridges the high-level
vision-language model and the low-level visual-motor policy. Specifically, GSL
decomposes demonstrations into transferable and object-canonicalized skill
primitives using foundation models, ensuring efficient low-level skill learning
in the object frame. At test time, the skill-object pairs predicted by the
high-level agent are fed to the low-level module, where the inferred canonical
actions are mapped back to the world frame for execution. This structured yet
flexible design leads to substantial improvements in sample efficiency and
generalization of our method across unseen spatial arrangements, object
appearances, and task compositions. In simulation, GSL trained with only 3
demonstrations per task outperforms baselines trained with 30 times more data
by 15.5 percent on unseen tasks. In real-world experiments, GSL also surpasses
the baseline trained with 10 times more data.

</details>


### [12] [An Agnostic End-Effector Alignment Controller for Robust Assembly of Modular Space Robots](https://arxiv.org/abs/2510.21164)
*Shamistan Karimov,Elian Neppel,Shreya Santra,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 提出了一种用于模块化机器人的自适应速度边界控制器，通过动态超球面钳制实现平稳稳定的运动对齐，在月球环境模拟器中测试验证了其鲁棒性和精确性。


<details>
  <summary>Details</summary>
Motivation: 模块化机器人在月球任务中需要能够适应真实世界扰动的安全控制器，现有方法在应对机械缺陷和传感噪声时表现不足。

Method: 基于Motion Stack中的执行器同步，开发了两种控制器变体：离散步进版本和连续速度版本，使用实时末端执行器和目标姿态测量来调整平移和旋转速度限制。

Result: 步进变体产生高度可预测、低抖动的运动，连续变体收敛更快并保持毫米级位置精度，两者在不同机械缺陷和传感噪声的肢体上都表现鲁棒。

Conclusion: 该机器人无关框架在恶劣条件下为自主自组装和重构提供了灵活性和鲁棒性。

Abstract: Modular robots offer reconfigurability and fault tolerance essential for
lunar missions, but require controllers that adapt safely to real-world
disturbances. We build on our previous hardware-agnostic actuator
synchronization in Motion Stack to develop a new controller enforcing adaptive
velocity bounds via a dynamic hypersphere clamp. Using only real-time
end-effector and target pose measurements, the controller adjusts its
translational and rotational speed limits to ensure smooth, stable alignment
without abrupt motions. We implemented two variants, a discrete, step-based
version and a continuous, velocity-based version, and tested them on two
MoonBot limbs in JAXA's lunar environment simulator. Field trials demonstrate
that the step-based variant produces highly predictable, low-wobble motions,
while the continuous variant converges more quickly and maintains
millimeter-level positional accuracy, and both remain robust across limbs with
differing mechanical imperfections and sensing noise (e.g., backlash and flex).
These results highlight the flexibility and robustness of our robot-agnostic
framework for autonomous self-assembly and reconfiguration under harsh
conditions.

</details>


### [13] [Underwater Visual-Inertial-Acoustic-Depth SLAM with DVL Preintegration for Degraded Environments](https://arxiv.org/abs/2510.21215)
*Shuoshuo Ding,Tiedong Zhang,Dapeng Jiang,Ming Lei*

Main category: cs.RO

TL;DR: 提出了一种基于图优化的视觉-惯性-声学-深度SLAM系统，集成四种传感器模态，在水下视觉退化环境中实现稳定定位和建图。


<details>
  <summary>Details</summary>
Motivation: 解决水下环境中由于能见度有限、光照不足和特征稀缺导致的视觉退化问题，这些因素严重影响了视觉-惯性SLAM系统的性能。

Method: 采用图优化框架，紧密集成立体相机、IMU、多普勒测速仪(DVL)和压力传感器；提出基于速度偏差的DVL预积分策略；前端采用混合跟踪策略和声学-惯性-深度联合优化；在图中加入多源混合残差。

Result: 在模拟和真实水下场景中的定量和定性分析表明，该方法在稳定性和定位精度方面优于当前最先进的立体视觉-惯性SLAM系统，在视觉挑战性环境中表现出卓越的鲁棒性。

Conclusion: 通过紧密集成多模态传感器和创新的DVL预积分策略，该系统能够在水下视觉退化环境中实现可靠运行，显著提升了SLAM系统的性能和鲁棒性。

Abstract: Visual degradation caused by limited visibility, insufficient lighting, and
feature scarcity in underwater environments presents significant challenges to
visual-inertial simultaneous localization and mapping (SLAM) systems. To
address these challenges, this paper proposes a graph-based
visual-inertial-acoustic-depth SLAM system that integrates a stereo camera, an
inertial measurement unit (IMU), the Doppler velocity log (DVL), and a pressure
sensor. The key innovation lies in the tight integration of four distinct
sensor modalities to ensure reliable operation, even under degraded visual
conditions. To mitigate DVL drift and improve measurement efficiency, we
propose a novel velocity-bias-based DVL preintegration strategy. At the
frontend, hybrid tracking strategies and acoustic-inertial-depth joint
optimization enhance system stability. Additionally, multi-source hybrid
residuals are incorporated into a graph optimization framework. Extensive
quantitative and qualitative analyses of the proposed system are conducted in
both simulated and real-world underwater scenarios. The results demonstrate
that our approach outperforms current state-of-the-art stereo visual-inertial
SLAM systems in both stability and localization accuracy, exhibiting
exceptional robustness, particularly in visually challenging environments.

</details>


### [14] [Remote Autonomy for Multiple Small Lowcost UAVs in GNSS-denied Search and Rescue Operations](https://arxiv.org/abs/2510.21357)
*Daniel Schleich,Jan Quenzel,Sven Behnke*

Main category: cs.RO

TL;DR: 开发了一个基于消费级DJI无人机的自主飞行系统，通过Android应用在遥控器上实现状态估计和避障，支持单操作员同时监控多架异构无人机，并将所有观测数据整合为联合3D环境模型。


<details>
  <summary>Details</summary>
Motivation: 消费级无人机在应急救援中广泛应用，但通常需要手动操作，特别是在GNSS受限的未知环境中。自主飞行可以简化操作、减轻操作员负担，但现有自主系统需要特殊编程接口、定制传感器和强大机载计算机，限制了广泛应用。

Method: 使用轻量级消费级DJI无人机，通过Android应用在无人机遥控器上直接运行状态估计和避障算法。地面控制站允许单操作员配置和监控多架异构无人机，并将所有无人机的观测数据整合为联合3D环境模型。

Result: 成功实现了基于消费级无人机的自主飞行系统，无需特殊硬件或强大计算资源，支持多无人机协同操作和环境感知。

Conclusion: 该系统证明了使用标准消费级无人机实现自主飞行的可行性，通过软件层面的创新解决了硬件限制问题，为应急救援等场景提供了更实用的自主无人机解决方案。

Abstract: In recent years, consumer-grade UAVs have been widely adopted by first
responders. In general, they are operated manually, which requires trained
pilots, especially in unknown GNSS-denied environments and in the vicinity of
structures. Autonomous flight can facilitate the application of UAVs and reduce
operator strain. However, autonomous systems usually require special
programming interfaces, custom sensor setups, and strong onboard computers,
which limits a broader deployment.
  We present a system for autonomous flight using lightweight consumer-grade
DJI drones. They are controlled by an Android app for state estimation and
obstacle avoidance directly running on the UAV's remote control. Our ground
control station enables a single operator to configure and supervise multiple
heterogeneous UAVs at once. Furthermore, it combines the observations of all
UAVs into a joint 3D environment model for improved situational awareness.

</details>


### [15] [Load-bearing Assessment for Safe Locomotion of Quadruped Robots on Collapsing Terrain](https://arxiv.org/abs/2510.21369)
*Vivian S. Medeiros,Giovanni B. Dessy,Thiago Boaventura,Marcelo Becker,Claudio Semini,Victor Barasuol*

Main category: cs.RO

TL;DR: 提出了一种用于四足机器人在不稳定地形上安全导航的鲁棒运动框架，通过整合地形探测、承载分析、运动规划和控制策略，使机器人能够检测可坍塌区域并动态调整落脚点。


<details>
  <summary>Details</summary>
Motivation: 坍塌地形在搜救任务和行星探索中常见，对四足机器人构成重大挑战，需要能够安全导航不稳定表面的解决方案。

Method: 利用关节测量评估地形稳定性，无需硬件修改；采用模型预测控制(MPC)系统优化机器人运动，平衡稳定性和探测约束；状态机协调地形探测动作。

Result: 在定制坍塌平台和岩石地形上的实验结果表明，该框架能够在保持稳定性和优先安全的同时穿越坍塌地形。

Conclusion: 该框架为四足机器人在不稳定地形上的安全导航提供了有效解决方案，通过集成探测、分析和控制策略实现了鲁棒的运动能力。

Abstract: Collapsing terrains, often present in search and rescue missions or planetary
exploration, pose significant challenges for quadruped robots. This paper
introduces a robust locomotion framework for safe navigation over unstable
surfaces by integrating terrain probing, load-bearing analysis, motion
planning, and control strategies. Unlike traditional methods that rely on
specialized sensors or external terrain mapping alone, our approach leverages
joint measurements to assess terrain stability without hardware modifications.
A Model Predictive Control (MPC) system optimizes robot motion, balancing
stability and probing constraints, while a state machine coordinates terrain
probing actions, enabling the robot to detect collapsible regions and
dynamically adjust its footholds. Experimental results on custom-made
collapsing platforms and rocky terrains demonstrate the framework's ability to
traverse collapsing terrain while maintaining stability and prioritizing
safety.

</details>


### [16] [PREVENT: Proactive Risk Evaluation and Vigilant Execution of Tasks for Mobile Robotic Chemists using Multi-Modal Behavior Trees](https://arxiv.org/abs/2510.21438)
*Satheeshkumar Veeramani,Zhengxue Zhou,Francisco Munguia-Galeano,Hatem Fakhruldeen,Thomas Roddelkopf,Mohammed Faeik Ruzaij Al-Okby,Kerstin Thurow,Andrew Ian Cooper*

Main category: cs.RO

TL;DR: 提出了PREVENT系统，通过多模态行为树方法为移动机器人化学家提供工作流感知能力，避免因小异常导致工作流中断，同时消除误报和漏报。


<details>
  <summary>Details</summary>
Motivation: 现有移动化学机器人缺乏工作流感知能力，微小异常（如样品瓶未盖好）可能中断整个工作流，浪费时间和资源，甚至对人类研究人员造成危险。现有感知机制会产生过多误报，需要人工干预，丧失了自主操作的优势。

Method: 基于多模态行为树方法，包含导航和操作技能，采用分层感知机制，结合AI技术和传感器反馈（灵巧视觉、导航视觉摄像头和物联网气体传感器模块）进行执行相关决策。

Result: 实验评估显示，在模拟风险场景中，该方法完全避免了误报和漏报，多模态感知技能在导航和操作方面的部署准确率均高于相应单模态技能的平均水平。

Conclusion: PREVENT系统能够有效集成到现有软件架构中，为移动机器人化学家提供可靠的工作流感知能力，确保自主操作的连续性和安全性。

Abstract: Mobile robotic chemists are a fast growing trend in the field of chemistry
and materials research. However, so far these mobile robots lack workflow
awareness skills. This poses the risk that even a small anomaly, such as an
improperly capped sample vial could disrupt the entire workflow. This wastes
time, and resources, and could pose risks to human researchers, such as
exposure to toxic materials. Existing perception mechanisms can be used to
predict anomalies but they often generate excessive false positives. This may
halt workflow execution unnecessarily, requiring researchers to intervene and
to resume the workflow when no problem actually exists, negating the benefits
of autonomous operation. To address this problem, we propose PREVENT a system
comprising navigation and manipulation skills based on a multimodal Behavior
Tree (BT) approach that can be integrated into existing software architectures
with minimal modifications. Our approach involves a hierarchical perception
mechanism that exploits AI techniques and sensory feedback through Dexterous
Vision and Navigational Vision cameras and an IoT gas sensor module for
execution-related decision-making. Experimental evaluations show that the
proposed approach is comparatively efficient and completely avoids both false
negatives and false positives when tested in simulated risk scenarios within
our robotic chemistry workflow. The results also show that the proposed
multi-modal perception skills achieved deployment accuracies that were higher
than the average of the corresponding uni-modal skills, both for navigation and
for manipulation.

</details>


### [17] [Enhancing Social Robots through Resilient AI](https://arxiv.org/abs/2510.21469)
*Domenico Palmisano,Giuseppe Palestra,Berardina Nadja De Carolis*

Main category: cs.RO

TL;DR: 本文探讨了社会机器人中韧性作为基本特征的重要性，特别是在医疗保健等敏感领域，通过韧性确保机器人可信度，尤其是在老年人群体中。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能在医疗、教育等敏感领域的深入应用，确保系统的韧性和鲁棒性变得至关重要，特别是在老年人等信任度较低的群体中。

Method: 论文将韧性定义为在不利或压力条件下，即使性能下降或减弱，仍能保持基本操作能力的能力。

Result: 研究表明韧性是社会机器人的基本特征，通过这种特性可以确保机器人本身的可信度。

Conclusion: 韧性对于社会机器人在敏感环境中的可信运行至关重要，特别是在服务老年人等特殊群体时。

Abstract: As artificial intelligence continues to advance and becomes more integrated
into sensitive areas like healthcare, education, and everyday life, it's
crucial for these systems to be both resilient and robust. This paper shows how
resilience is a fundamental characteristic of social robots, which, through it,
ensure trust in the robot itself-an essential element especially when operating
in contexts with elderly people, who often have low trust in these systems.
Resilience is therefore the ability to operate under adverse or stressful
conditions, even when degraded or weakened, while maintaining essential
operational capabilities.

</details>


### [18] [AURASeg: Attention Guided Upsampling with Residual Boundary-Assistive Refinement for Drivable-Area Segmentation](https://arxiv.org/abs/2510.21536)
*Narendhiran Vijayakumar,Sridevi. M*

Main category: cs.RO

TL;DR: 提出AURASeg模型用于地面分割，通过注意力引导上采样和边界细化模块，在保持高分割精度的同时提升边界精度，在室内外环境中都表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有地面分割模型在处理精细特征时存在挑战，特别是在室内和结构化环境中，主要问题包括多尺度处理效果不佳、边界细化不理想和特征表示有限。

Method: 使用CSP-Darknet主干网络，添加残差边界细化模块(RBRM)进行精确边缘描绘，注意力渐进上采样解码器(APUD)进行强特征集成，以及轻量级空洞空间金字塔池化(ASPP-Lite)模块确保多尺度上下文提取。

Result: 在GMRP数据集和自定义Gazebo室内数据集上，模型在mIoU和F1指标上超越基准分割架构，mIoU提升+1.26%，分割精度提升+1.65%。

Conclusion: 该技术适用于室内外环境的自主感知，能够实现精确的边界细化，同时对推理速度影响最小。

Abstract: Free space ground segmentation is essential to navigate robots and autonomous
vehicles, recognize drivable zones, and traverse efficiently. Fine-grained
features remain challenging for existing segmentation models, particularly for
robots in indoor and structured environments. These difficulties arise from
ineffective multi-scale processing, suboptimal boundary refinement, and limited
feature representation. In order to overcome these limitations, we propose
Attention-Guided Upsampling with Residual Boundary-Assistive Refinement
(AURASeg), a ground-plane semantic segmentation model that maintains high
segmentation accuracy while improving border precision. Our method uses
CSP-Darknet backbone by adding a Residual Border Refinement Module (RBRM) for
accurate edge delineation and an Attention Progressive Upsampling Decoder
(APUD) for strong feature integration. We also incorporate a lightweight Atrous
Spatial Pyramid Pooling (ASPP-Lite) module to ensure multi-scale context
extraction without compromising real-time performance. The proposed model beats
benchmark segmentation architectures in mIoU and F1 metrics when tested on the
Ground Mobile Robot Perception (GMRP) Dataset and a custom Gazebo indoor
dataset. Our approach achieves an improvement in mean Intersection-over-Union
(mIoU) of +1.26% and segmentation precision of +1.65% compared to
state-of-the-art models. These results show that our technique is feasible for
autonomous perception in both indoor and outdoor environments, enabling precise
border refinement with minimal effect on inference speed.

</details>


### [19] [Scalable Vision-Language-Action Model Pretraining for Robotic Manipulation with Real-Life Human Activity Videos](https://arxiv.org/abs/2510.21571)
*Qixiu Li,Yu Deng,Yaobo Liang,Lin Luo,Lei Zhou,Chengtang Yao,Lingqi Zeng,Zhiyuan Feng,Huizhi Liang,Sicheng Xu,Yizhong Zhang,Xi Chen,Hao Chen,Lily Sun,Dong Chen,Jiaolong Yang,Baining Guo*

Main category: cs.RO

TL;DR: 利用无标注的真实人类手部活动视频预训练机器人操作VLA模型，通过自动化分析将人类手部视频转化为机器人训练数据，构建了包含100万片段的手部VLA数据集，模型在零样本和微调场景下均表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决机器人操作数据稀缺问题，利用丰富的人类手部活动视频作为训练资源，扩展机器人视觉-语言-动作模型的覆盖范围和泛化能力。

Method: 开发全自动人类活动分析方法，将无标注的自我中心视角人类手部视频转化为原子级活动片段，生成语言描述、3D手部运动和相机运动数据，构建大规模手部VLA数据集。

Result: 构建了包含100万片段、2600万帧的手部VLA数据集，模型在零样本测试中表现强劲，少量机器人数据微调后任务成功率显著提升，并展现出良好的数据规模扩展性。

Conclusion: 该方法为可扩展的VLA预训练奠定了坚实基础，推动了机器人向真正通用化具身智能的发展。

Abstract: This paper presents a novel approach for pretraining robotic manipulation
Vision-Language-Action (VLA) models using a large corpus of unscripted
real-life video recordings of human hand activities. Treating human hand as
dexterous robot end-effector, we show that "in-the-wild" egocentric human
videos without any annotations can be transformed into data formats fully
aligned with existing robotic V-L-A training data in terms of task granularity
and labels. This is achieved by the development of a fully-automated holistic
human activity analysis approach for arbitrary human hand videos. This approach
can generate atomic-level hand activity segments and their language
descriptions, each accompanied with framewise 3D hand motion and camera motion.
We process a large volume of egocentric videos and create a hand-VLA training
dataset containing 1M episodes and 26M frames. This training data covers a wide
range of objects and concepts, dexterous manipulation tasks, and environment
variations in real life, vastly exceeding the coverage of existing robot data.
We design a dexterous hand VLA model architecture and pretrain the model on
this dataset. The model exhibits strong zero-shot capabilities on completely
unseen real-world observations. Additionally, fine-tuning it on a small amount
of real robot action data significantly improves task success rates and
generalization to novel objects in real robotic experiments. We also
demonstrate the appealing scaling behavior of the model's task performance with
respect to pretraining data scale. We believe this work lays a solid foundation
for scalable VLA pretraining, advancing robots toward truly generalizable
embodied intelligence.

</details>


### [20] [Enhancing Tactile-based Reinforcement Learning for Robotic Control](https://arxiv.org/abs/2510.21609)
*Elle Miller,Trevor McInroe,David Abel,Oisin Mac Aodha,Sethu Vijayakumar*

Main category: cs.RO

TL;DR: 该论文开发了自监督学习方法，有效利用稀疏二进制触觉信号提升机器人操作的灵巧性，在复杂接触任务中实现了超人类水平的性能，并发布了RoTO基准。


<details>
  <summary>Details</summary>
Motivation: 实现安全可靠的现实世界机器人操作需要超越视觉感知，整合触觉传感来克服感官缺陷和对理想化状态信息的依赖。尽管触觉传感具有潜力，但在强化学习中的效果仍不一致。

Method: 开发自监督学习方法来更有效地利用触觉观测，专注于本体感觉和稀疏二进制接触的可扩展设置。将SSL记忆与策略记忆解耦以提升性能。

Result: 稀疏二进制触觉信号对于灵巧性至关重要，特别是在本体感觉控制误差无法检测的交互中（如解耦的机器人-物体运动）。智能体在复杂接触任务（球反弹和保定球旋转）中实现了超人类灵巧性。

Conclusion: 触觉传感对于机器人操作的灵巧性具有关键作用，自监督学习方法能有效提升性能。发布了Robot Tactile Olympiad（RoTO）基准来标准化和促进未来触觉操作研究。

Abstract: Achieving safe, reliable real-world robotic manipulation requires agents to
evolve beyond vision and incorporate tactile sensing to overcome sensory
deficits and reliance on idealised state information. Despite its potential,
the efficacy of tactile sensing in reinforcement learning (RL) remains
inconsistent. We address this by developing self-supervised learning (SSL)
methodologies to more effectively harness tactile observations, focusing on a
scalable setup of proprioception and sparse binary contacts. We empirically
demonstrate that sparse binary tactile signals are critical for dexterity,
particularly for interactions that proprioceptive control errors do not
register, such as decoupled robot-object motions. Our agents achieve superhuman
dexterity in complex contact tasks (ball bouncing and Baoding ball rotation).
Furthermore, we find that decoupling the SSL memory from the on-policy memory
can improve performance. We release the Robot Tactile Olympiad (RoTO) benchmark
to standardise and promote future research in tactile-based manipulation.
Project page: https://elle-miller.github.io/tactile_rl

</details>


### [21] [Design and Structural Validation of a Micro-UAV with On-Board Dynamic Route Planning](https://arxiv.org/abs/2510.21648)
*Inbazhagan Ravikumar,Ram Sundhar,Narendhiran Vijayakumar*

Main category: cs.RO

TL;DR: 提出了一种低成本、模块化的定制无人机系统，用于搜救任务，具有结构耐用性和实时感知导航能力


<details>
  <summary>Details</summary>
Motivation: 解决低成本小型无人机在搜救任务中结构不耐用和无法动态重新规划路径的问题

Method: 使用常见材料和组件从头构建定制无人机，强调模块化、低成本和易组装，采用轻量耐用材料加固结构框架，完全基于开源软件实现机载控制系统

Result: 系统展示了无需昂贵硬件加速器的实时感知和自适应导航能力

Conclusion: 为现实世界搜救任务提供了一个经济实用的解决方案

Abstract: Micro aerial vehicles are becoming increasingly important in search and
rescue operations due to their agility, speed, and ability to access confined
spaces or hazardous areas. However, designing lightweight aerial systems
presents significant structural, aerodynamic, and computational challenges.
This work addresses two key limitations in many low-cost aerial systems under
two kilograms: their lack of structural durability during flight through rough
terrains and inability to replan paths dynamically when new victims or
obstacles are detected. We present a fully customised drone built from scratch
using only commonly available components and materials, emphasising modularity,
low cost, and ease of assembly. The structural frame is reinforced with
lightweight yet durable materials to withstand impact, while the onboard
control system is powered entirely by free, open-source software solutions. The
proposed system demonstrates real-time perception and adaptive navigation
capabilities without relying on expensive hardware accelerators, offering an
affordable and practical solution for real-world search and rescue missions.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [22] [The Economics of Convex Function Intervals](https://arxiv.org/abs/2510.20907)
*Victor Augias,Lina Uhe*

Main category: econ.TH

TL;DR: 本文提出了凸函数区间(CFIs)的概念，用于描述满足特定水平和斜率约束的凸函数族，并应用于经济设计中的约束集问题。


<details>
  <summary>Details</summary>
Motivation: 在经济设计中，经常遇到包含类型依赖参与约束和双边弱优超约束的问题，需要一种系统方法来处理这类凸函数约束集。

Method: 开发了CFIs的几何特征描述、线性规划的最优性条件，以及嵌套优化方法，特别关注下边界优化。

Result: 将理论工具应用于四个具体场景：带类型依赖外部选择的筛选和授权问题、有限处置的竞赛设计、带信息约束的均值说服，得出了新的经济含义。

Conclusion: CFIs为经济设计提供了有力工具，揭示了如更好外部选择导致更大授权集、在非平凡类型依赖参与约束下固定价格机制可能次优等重要结论。

Abstract: We introduce convex function intervals (CFIs): families of convex functions
satisfying given level and slope constraints. CFIs naturally arise as
constraint sets in economic design, including problems with type-dependent
participation constraints and two-sided (weak) majorization constraints. Our
main results include: (i) a geometric characterization of the extreme points of
CFIs; (ii) sufficient optimality conditions for linear programs over CFIs; and
(iii) methods for nested optimization on their lower level boundary that can be
applied, e.g., to the optimal design of outside options. We apply these results
to four settings: screening and delegation problems with type-dependent outside
options, contest design with limited disposal, and mean-based persuasion with
informativeness constraints. We draw several novel economic implications using
our tools. For instance, we show that better outside options lead to larger
delegation sets, and that posted price mechanisms can be suboptimal in the
canonical monopolistic screening problem with nontrivial, type-dependent
participation constraints.

</details>


### [23] [Rationalizable Screening and Disclosure under Unawareness](https://arxiv.org/abs/2510.20918)
*Alejandro Francetich,Burkhard C. Schipper*

Main category: econ.TH

TL;DR: 本文分析了一个委托-代理采购问题，其中委托人不知道代理人的某些边际成本类型。代理人可能通过沟通提高委托人的认知度，这会影响合同菜单的设计。研究发现，当委托人最初不知道高成本类型时，这些类型有动机提高其认知度；而当委托人不知道低成本类型时，没有类型愿意提高其认知度。


<details>
  <summary>Details</summary>
Motivation: 研究委托-代理关系中信息不对称问题，特别是当委托人不知道某些代理人类型时的沟通激励和合同设计。

Method: 使用离散凹模型，结合理性化程序、对数凹性边际信念、"反向"贝叶斯主义和谨慎假设来分析沟通激励。

Result: 当委托人不知道高成本类型时，这些类型有动机提高其认知度；在三种类型情况下，两个低成本类型也愿意提高对高成本类型的认知度；当委托人不知道低成本类型时，没有类型愿意提高其认知度。

Conclusion: 代理人的沟通激励取决于委托人不知道的类型特征：不知道高成本类型会促进沟通，而不知道低成本类型则会抑制沟通。

Abstract: We analyze a principal-agent procurement problem in which the principal (she)
is unaware some of the marginal cost types of the agent (he). Communication
arises naturally as some types of the agent may have an incentive to raise the
principal's awareness (totally or partially) before a contract menu is offered.
The resulting menu must not only reflect the principal's change in awareness,
but also her learning about types from the agent's decision to raise her
awareness in the first place. We capture this reasoning in a discrete concave
model via a rationalizability procedure in which marginal beliefs over types
are restricted to log-concavity, ``reverse'' Bayesianism, and mild assumptions
of caution.
  We show that if the principal is ex ante only unaware of high-cost types, all
of these types have an incentive raise her awareness of them -- otherwise, they
would not be served. With three types, the two lower-cost types that the
principal is initially aware of also want to raise her awareness of the
high-cost type: Their quantities suffer no additional distortions and they both
earn an extra information rent. Intuitively, the presence of an even higher
cost type makes the original two look better. With more than three types, we
show that this intuition may break down for types of whom the principal is
initially aware of so that raising the principal's awareness could cease to be
profitable for those types. When the principal is ex ante only unaware of more
efficient (low-cost) types, then \textit{no type} raises her awareness, leaving
her none the wiser.

</details>


### [24] [Discrete Screening](https://arxiv.org/abs/2510.20921)
*Alejandro Francetich,Burkhard C. Schipper*

Main category: econ.TH

TL;DR: 本文研究离散类型代理的筛选问题，使用离散一阶条件方法，发现离散FOC解可能不唯一，但每个类型最多只有两个最优合同数量且必须相邻。


<details>
  <summary>Details</summary>
Motivation: 研究离散类型和离散合同菜单下的筛选问题，旨在将连续合同筛选的经典结果推广到离散设置。

Method: 使用离散一阶条件方法，假设委托人估值是离散严格凹函数，代理成本类型为非整数，将整数类型作为极限情况处理。

Result: 离散FOC解可能不唯一，但每个类型最多有两个最优合同数量且必须相邻；只能保证数量的弱单调性；引入Δ-O理性化概念与最优合同集重合。

Conclusion: 离散筛选方法便于使用理性化解决筛选问题，Δ-O理性化菜单与通常最优合同集一致（可能包含无关合同）。

Abstract: We consider a principal who wishes to screen an agent with \emph{discrete}
types by offering a menu of \emph{discrete} quantities and \emph{discrete}
transfers. We assume that the principal's valuation is discrete strictly
concave and use a discrete first-order approach. We model the agent's cost
types as non-integer, with integer types as a limit case. Our modeling of cost
types allows us to replicate the typical constraint-simplification results and
thus to emulate the well-treaded steps of screening under a continuum of
contracts.
  We show that the solutions to the discrete F.O.C.s need not be unique
\textit{even under discrete strict concavity}, but we also show that there
cannot be more than two optimal contract quantities for each type, and that --
if there are two -- they must be adjacent. Moreover, we can only ensure weak
monotonicity of the quantities \textit{even if virtual costs are strictly
monotone}, unless we limit the ``degree of concavity'' of the principal's
utility. Our discrete screening approach facilitates the use of
rationalizability to solve the screening problem. We introduce a
rationalizability notion featuring robustness with respect to an open set of
beliefs over types called \textit{$\Delta$-O Rationalizability}, and show that
the set of $\Delta$-O rationalizable menus coincides with the set of usual
optimal contracts -- possibly augmented to include irrelevant contracts.

</details>


### [25] [Constrained Mediation: Bayesian Implementability of Joint Posteriors](https://arxiv.org/abs/2510.20986)
*David Lagziel,Ehud Lehrer*

Main category: econ.TH

TL;DR: 研究具有私有信息代理人和信息受限调解人的设置中的信息结构，重点关注调解人能够诱导的后验信念集合的表征。


<details>
  <summary>Details</summary>
Motivation: 在具有私有信息代理人和提供额外公共信号的调解人的环境中，理解调解人能够诱导的后验信念集合对于分析信息结构至关重要。

Method: 采用图论框架：状态表示为顶点，信息集对应边，边上的似然比函数编码后验信念。推导后验合理化的内部和外部一致性条件。

Result: 得出了后验合理化的必要和充分条件（内部和外部一致性），并识别了单个调解人能够实现多个后验信念的条件。

Conclusion: 单个调解人可以作为Blackwell实验生成器，在特定条件下实现多个后验信念，这为信息结构分析提供了新的视角。

Abstract: We examine information structures in settings with privately informed agents
and an informationally constrained mediator who supplies additional public
signals. Our focus is on characterizing the set of posteriors that the mediator
can induce. To this end, we employ a graph-theoretic framework: states are
represented as vertices, information sets correspond to edges, and a likelihood
ratio function on edges encodes the posterior beliefs. Within this framework,
we derive necessary and sufficient conditions, internal and external
consistency, for the rationalization of posteriors. Finally, we identify
conditions under which a single mediator can implement multiple posteriors,
effectively serving as a generator of Blackwell experiments.

</details>


<div id='q-fin.GN'></div>

# q-fin.GN [[Back]](#toc)

### [26] [The local Gaussian correlation networks among return tails in the Chinese stock market](https://arxiv.org/abs/2510.21165)
*Peng Liu*

Main category: q-fin.GN

TL;DR: 使用局部高斯相关系数构建上证股票负尾风险网络，相比传统Pearson相关网络更能敏感反映市场风险


<details>
  <summary>Details</summary>
Motivation: 传统Pearson相关系数在金融网络研究中存在严重缺陷，可能导致误导性结果。局部高斯相关系数能够捕捉局部非线性依赖和处理重尾分布，具有独特优势

Method: 使用局部高斯相关系数构建上证股票收益尾部区域的金融网络，系统分析节点中心性、平均最短路径长度和熵等基本网络指标

Result: 相比正尾局部高斯相关网络和传统Pearson相关网络，负尾局部高斯相关网络对股票市场风险更加敏感

Conclusion: 研究者应优先考虑负尾局部高斯相关网络，未来工作应使用局部高斯相关方法重新评估现有发现

Abstract: Financial networks based on Pearson correlations have been intensively
studied. However, previous studies may have led to misleading and catastrophic
results because of several critical shortcomings of the Pearson correlation.
The local Gaussian correlation coefficient, a new measurement of statistical
dependence between variables, has unique advantages including capturing local
nonlinear dependence and handling heavy-tailed distributions. This study
constructs financial networks using the local Gaussian correlation coefficients
between tail regions of stock returns in the Shanghai Stock Exchange. The work
systematically analyzes fundamental network metrics including node centrality,
average shortest path length, and entropy. Compared with the local Gaussian
correlation network among positive tails and the conventional Pearson
correlation network, the properties of the local Gaussian correlation network
among negative tails are more sensitive to the stock market risks. This finding
suggests researchers should prioritize the local Gaussian correlation network
among negative tails. Future work should reevaluate existing findings using the
local Gaussian correlation method.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [27] [Representing caregiver burden in observational studies: Development of the Caregiver Burden Index (CareBI) using NSOC](https://arxiv.org/abs/2510.21630)
*Forough Mahpouya,Sabrina Casucci,Suzanne Sullivan,Christopher Barrick*

Main category: stat.AP

TL;DR: 开发了Caregiver Burden Index (CareBI)工具，用于在观察性研究中量化照顾者负担，涵盖客观、主观和人际三个维度，并与现有工具Zarit Burden Interview保持一致。


<details>
  <summary>Details</summary>
Motivation: 现有照顾者负担评估工具与观察性数据集不兼容，在健康服务研究中代表性不足，需要开发适用于定量模型和观察性研究的标准化工具。

Method: 采用多步骤开发验证流程，包括识别和准备NSOC调查项目、探索性和验证性因子分析、分数估计、解释和外部验证，使用NSOC第12轮数据。

Result: CareBI能有效区分低、中、高负担照顾者，与照顾者和被照顾者结果相关，对已知负担相关风险和保护因素敏感，具有良好的构念效度。

Conclusion: CareBI为将照顾者指标嵌入健康运营、预测建模和公共政策框架提供了可复制的工具，并为应用运筹学和工业工程方法解决老年和长期护理中的心理社会测量挑战提供了模板。

Abstract: Informal caregiving often carries a significant emotional, physical, and
financial toll, yet caregiver burden is often underrepresented in healthcare
research and methods. Existing caregiver burden instruments, while valuable in
clinical research, often lack compatibility with observational datasets
regularly used in health services research and planning. This study introduces
the Caregiver Burden Index (CareBI) developed for the National Study of
Caregiving (NSOC), that can be used to represent caregiver burden in
quantitative models and observational research studies. CareBI was developed
and validated using a multistep process that included the identification and
preparation of individual NSOC survey items, exploratory and confirmatory
factor analysis, score estimation, interpretation, and external validation. The
study used data from round 12 of the NSOC. CareBI represents three domains of
burden: objective, subjective, and interpersonal, providing a comprehensive
view of both the positive and negative aspects of caregiving. It also aligns
with the Zarit Burden Interview, a widely used tool for prospectively assessing
caregiver burden. Construct validity was assessed by comparing CareBI's
relationship with caregiver and care recipient outcomes, as well as sensitivity
to known burden-related risk and mitigation factors. Early findings affirm the
scale's utility in categorizing low-, moderate-, and high-burden caregivers and
guiding resource-oriented strategies. CareBI represents a reproducible tool for
embedding caregiver metrics into health operations, predictive modeling, and
public policy frameworks, and provides a template for applying operations
research and industrial engineering methods to psychosocial measurement
challenges in aging and long-term care.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [28] [Safety Monitor for Off-Road Planning with Uncertainty Bounded Bekker Costs](https://arxiv.org/abs/2510.21006)
*Akshay Naik,Ramavarapu S. Sreenivas,William R. Norris,Albert E. Patterson,Ahmet Soylemezoglu,Dustin Nottage*

Main category: eess.SY

TL;DR: 提出了一种用于越野自主车辆的运行时安全保障监控器，该监控器与任何规划器协作，使用基于Bekker的成本模型和有限不确定性来确保车辆在不确定土壤强度下的安全性。


<details>
  <summary>Details</summary>
Motivation: 可靠的越野自主性需要操作约束，以确保在土壤强度不确定时行为保持可预测和安全。

Method: 监控器构建基于轻量级压力沉降模型的置信上限穿越成本，检查每个规划运动是否超过最大沉降和侧翻裕度两个限制。如果风险过高，则切换到经过认证的备用方案，降低车速、增加与软土的距离或在较硬土壤上停止。

Result: 在从壤土到沙土的模拟环境中，结果显示干预率、违规概率和路径效率相对于名义计划的比较，并通过台架静态加载检查提供了初步经验验证。

Conclusion: 该方法通过将不确定性分析纳入置信上限成本并应用简单的干预规则，使规划器专注于效率，同时监控器在运行时保持车辆在明确的安全限制内。

Abstract: Reliable off-road autonomy requires operational constraints so that behavior
stays predictable and safe when soil strength is uncertain. This paper presents
a runtime assurance safety monitor that collaborates with any planner and uses
a Bekker-based cost model with bounded uncertainty. The monitor builds an upper
confidence traversal cost from a lightweight pressure sinkage model identified
in field tests and checks each planned motion against two limits: maximum
sinkage and rollover margin. If the risk of crossing either limit is too high,
the monitor switches to a certified fallback that reduces vehicle speed,
increases standoff from soft ground, or stops on firmer soil. This separation
lets the planner focus on efficiency while the monitor keeps the vehicle within
clear safety limits on board. Wheel geometry, wheel load estimate, and a soil
raster serve as inputs, which tie safety directly to vehicle design and let the
monitor set clear limits on speed, curvature, and stopping at run time. The
method carries uncertainty analytically into the upper confidence cost and
applies simple intervention rules. Tuning of the sinkage limit, rollover
margin, and risk window trades efficiency for caution while keeping the monitor
light enough for embedded processors. Results from a simulation environment
spanning loam to sand include intervention rates, violation probability, and
path efficiency relative to the nominal plan, and a benchtop static loading
check provides initial empirical validation.

</details>


### [29] [A Connectively Stable and Robust DAPI Control Scheme for Islanded Networks of Microgrids](https://arxiv.org/abs/2510.21025)
*Ahmed Saad Al-Karsani,Maryam Khanbaghi,Aleksandar Zečević*

Main category: eess.SY

TL;DR: 提出了一种鲁棒的分布式平均比例积分控制方案，用于微电网和微电网网络的频率和电压控制，解决了传统DAPI控制对运行或结构扰动不鲁棒的问题。


<details>
  <summary>Details</summary>
Motivation: 微电网和微电网网络在孤岛模式下运行面临频率和电压调节挑战，传统DAPI控制对扰动不鲁棒，需要开发更鲁棒的控制方案。

Method: 结合连接稳定性和不变椭球技术，提出鲁棒DAPI频率和电压控制方案，确保对扰动的鲁棒性。

Result: 在包含3个微电网和5个分布式能源资源的网络模型上进行仿真验证，表明该方法能有效减轻重大扰动（如网络攻击）的影响。

Conclusion: 所提出的鲁棒DAPI控制方案能够成功缓解主要扰动的影响，为微电网网络提供更可靠的频率和电压控制。

Abstract: The transition towards clean energy and the introduction of Distributed
Energy Resources (DERs) are giving rise to the emergence of Microgrids (MGs)
and Networks of MGs (NMGs). MGs and NMGs can operate autonomously in islanded
mode. However, they face challenges in terms of secondary level frequency and
voltage regulation, due to the variable nature of Renewable Energy Sources
(RES) and loads. Distributed-Averaging Proportional-Integral (DAPI) control has
been proposed in the literature for distributed frequency and voltage control
of droop-controlled DERs, but it is not robust to operational or structural
perturbations. To address this, we propose a robust DAPI frequency and voltage
control scheme that ensures robustness using the concept of connective
stability, along with the invariant ellipsoid technique for disturbance
rejection. Simulation of an NMG model in
MATLAB\textsuperscript{\textregistered}/Simulink\textsuperscript{\textregistered}
consisting of 3 MGs and 5 DERs validates the effectiveness of the proposed
method, and demonstrates that it can successfully mitigate the effects of major
disturbances such as cyberattacks.

</details>


### [30] [House Thermal Model Estimation: Robustness Across Seasons and Setpoints](https://arxiv.org/abs/2510.21044)
*Kunal Shankar,Ninad Gaikwad,Anamika Dubey*

Main category: eess.SY

TL;DR: 本文研究了使用三种优化算法（非线性最小二乘法、批量估计和最大似然估计）来估计房屋RC网络热模型参数，以支持大规模需求响应。通过四季和三种设定点的前向模拟评估模型性能，提供了选择降阶模型和估计方法的系统方法。


<details>
  <summary>Details</summary>
Motivation: 实现房屋供暖、制冷和通风系统的灵活性可以聚合多个家庭的HVAC负荷调整，实现大规模需求响应，帮助配电网灵活调节本地负荷需求以匹配大电网发电曲线。但需要计算效率高且对运行条件鲁棒的热模型。

Method: 使用三种优化算法（非线性最小二乘法、批量估计和最大似然估计）估计房屋RC网络热模型参数，并通过四季和三种设定点的前向模拟评估模型性能。

Result: 研究结果展示了在选择降阶模型和估计方法时，考虑训练-测试数据集中季节和设定点变化所提供的鲁棒性的系统方法。

Conclusion: 本文提供了一种基于季节和设定点变化鲁棒性的原则性方法来选择降阶热模型和参数估计方法，为实现HVAC系统的大规模需求响应提供了重要技术支撑。

Abstract: Achieving the flexibility from house heating, cooling, and ventilation
systems (HVAC) has the potential to enable large-scale demand response by
aggregating HVAC load adjustments across many homes. This demand response
strategy helps distribution grid to flexibly ramp-up or ramp-down local load
demand so that it can optimally match the bulk power system generation profile.
However, achieving this capability requires house thermal models that are both
computationally efficient and robust to operating conditions. In this work,
parameters of the Resistance-Capacitance (RC) network thermal model for houses
are estimated using three optimization algorithms: Nonlinear Least Squares
(NLS), Batch Estimation (BE), and Maximum Likelihood Estimation (MLE). The
resulting models are evaluated through a Forward-Simulation across four
different seasons and three setpoints. The results illustrate a principled way
of selecting reduced order models and estimation methods with respect to the
robustness offered to seasonal and setpoint variations in training-testing
datasets

</details>


### [31] [Lyapunov-Based Physics-Informed Deep Neural Networks with Skew Symmetry Considerations](https://arxiv.org/abs/2510.21051)
*Rebecca G. Hart,Wanjiku A. Makumi,Rushikesh Kamalapurkar,Warren E. Dixon*

Main category: eess.SY

TL;DR: 本文提出了首个针对欧拉-拉格朗日动态系统的物理信息深度神经网络控制器，通过李雅普诺夫稳定性分析设计自适应律，考虑了惯性矩阵和离心科里奥利矩阵的斜对称特性。


<details>
  <summary>Details</summary>
Motivation: 传统深度神经网络是黑盒函数逼近器，不包含已知物理原理，可能产生物理上不合理的结果。物理信息神经网络能够利用已知物理原理改善性能，但现有方法未充分考虑系统对称性。

Method: 使用李雅普诺夫稳定性分析设计自适应律，特别考虑了惯性矩阵和离心科里奥利矩阵的斜对称特性，确保跟踪误差和斜对称预测误差的渐近收敛。

Result: 仿真结果表明，与不包含系统对称性知识的物理信息自适应律相比，所提出的更新律在个体和整体函数逼近能力方面都有改进。

Conclusion: 提出的物理信息深度神经网络控制器通过结合系统对称性知识，在保持稳定性的同时提高了控制性能，为物理约束下的智能控制提供了新方法。

Abstract: Deep neural networks (DNNs) are powerful black-box function approximators
which have been shown to yield improved performance compared to traditional
neural network (NN) architectures. However, black-box algorithms do not
incorporate known physics of the system and can yield results which are
physically implausible. Physics-informed neural networks (PINNs) have grown in
popularity due to their ability to leverage known physical principles in the
learning process which has been empirically shown to improve performance
compared to traditional black-box methods. This paper introduces the first
physics-informed DNN controller for an Euler-Lagrange dynamic system where the
adaptation laws are designed using a Lyapunov-based stability analysis to
account for the skew-symmetry property of the inertia matrix and
centripetal-Coriolis matrix. A Lyapunov-based stability analysis is provided to
guarantee asymptotic convergence of the tracking error and the skew-symmetric
prediction error. Simulations indicate that the developed update law
demonstrates improvement in individual and overall function approximation
capabilities when compared to a physics-informed adaptation law which does not
incorporate knowledge of system symmetries.

</details>


### [32] [Environment-Dependent Components Identification of Behind-the-Meter Resources via Inverse Optimization](https://arxiv.org/abs/2510.21136)
*Chengming Lyu,Zhenfei Tan,Xiaoyuan Xu,Chen Fu,Zheng Yan,Mohammad Shahidehpour*

Main category: eess.SY

TL;DR: 提出了一种混合物理启发和数据驱动的框架，用于基于总负荷和环境因素的外部测量来分解BTM组件，包括储能类组件、光伏发电组件、温控负荷组件和周期性组件。


<details>
  <summary>Details</summary>
Motivation: 随着BTM资源的渗透率增加，需要监控这些资源的组件并推断其对外部环境的响应行为。但由于数据隐私问题，电力系统运营商无法获取设备级测量数据，这阻碍了负荷识别的准确建模。

Method: 采用双层迭代求解框架，开发了数据驱动的逆优化算法来识别储能类组件的参数，提出了物理启发模型来识别其余组件的容量和响应。

Result: 通过数值测试验证了所提方法的建模准确性和鲁棒性，并在电力市场出清中验证了所提BTM识别方法在降低系统运行成本方面的应用意义。

Conclusion: 该混合框架能够有效分解BTM组件，为电力系统运营商提供准确的负荷建模，并在实际应用中降低系统运行成本。

Abstract: With the increasing penetration of behind-the-meter (BTM) resources, it is
vital to monitor the components of these resources and deduce their response
behavior to external environment. Owing to data privacy, however, the
appliance-wise measurement is invisible to the power system operator, which
hinders the accurate modeling of load identification. To this end, this paper
proposes a hybrid physics-inspired and data-driven framework for decomposing
BTM components based on external measurement of total load and environmental
factors. The total load is decomposed into different environment-dependent
components, namely storage-like component, PV generation component,
thermostatically-controlled load component, and periodic component. The overall
load identification adopts a double-layer iterative solution framework. A
data-driven inverse optimization algorithm is developed to identify parameters
of the energy storage-like component. The physics-inspired model is proposed to
identify the capacity and response of the rest components. The modeling
accuracy and robustness of the proposed method are validated by numerical
tests. The application significance of the proposed BTM identification method
is also validated in electricity market clearing for reducing system operation
costs.

</details>


### [33] [Green Hydrogen under Uncertainty: Evaluating Power-to-X Strategies Using Agent-Based Simulation and Multi-Criteria Decision Framework](https://arxiv.org/abs/2510.21179)
*Frederik Wagner Madsen,Joy Dalmacio Billanes,Bo Nørregaard Jørgensen,Zheng Ma*

Main category: eess.SY

TL;DR: 本文提出了一个集成多主体模拟和多标准决策的建模框架，用于评估使用风电和太阳能发电的绿氢生产策略，在丹麦案例研究中比较了三种运营策略和三种电解槽容量水平。


<details>
  <summary>Details</summary>
Motivation: 向净零能源系统转型需要可扩展且成本效益高的Power-to-X技术部署，特别是绿氢生产。现有研究大多依赖静态技术经济模型，忽视了参与者互动、基础设施限制和监管复杂性。

Method: 开发了一个集成多主体模拟和多标准决策的建模框架，在丹麦案例研究中应用三种运营策略（仅电网、仅现场、混合）和三种电解槽容量水平（10MW、50MW、100MW），使用真实电价、排放因子和市场数据。

Result: 混合策略在成本和排放方面持续优于仅电网配置，同时保持稳定的氢输出。仅现场策略虽然最小化排放和成本，但无法满足固定生产需求。

Conclusion: 该框架通过建模动态参与者互动和将系统性能评估集成到战略规划中提供了新颖的科学贡献，为能源规划者和政策制定者设计在可再生能源丰富环境中有弹性和高效的Power-to-X系统提供了可行的见解。

Abstract: The transition toward net-zero energy systems requires scalable and
cost-effective deployment of Power-to-X technologies, particularly green
hydrogen production. Despite increasing investments, a critical research gap
remains in dynamically assessing how different operational strategies affect
the feasibility of hydrogen production under real-world energy market
conditions. Most existing studies rely on static, techno-economic models and
overlook actor interactions, infrastructure limitations, and regulatory
complexity. This paper presents a novel modeling framework that integrates
agent-based simulation with multi-criteria decision-making to evaluate green
hydrogen production strategies using co-located wind and solar generation.
Three operational strategies - grid-only, on-site-only, and hybrid - are
applied across three electrolyzer capacity levels (10 MW, 50 MW, and 100 MW)
within a Danish case study. Real electricity tariffs, emissions factors, and
market data are used to simulate technical, economic, and environmental
performance indicators. The results show that hybrid strategies consistently
outperform grid-only configurations in terms of cost and emissions while
maintaining stable hydrogen output. Although on-site-only strategies minimize
emissions and costs, they fail to meet fixed production demands. This framework
offers novel scientific contributions by modeling dynamic actor interactions
and integrating system performance evaluation into strategic planning.
Practically, it provides actionable insights for energy planners and
policymakers designing resilient and efficient Power-to-X systems in
renewable-rich contexts.

</details>


### [34] [The Role of Information Incompleteness in Defending Against Stealth Attacks](https://arxiv.org/abs/2510.21227)
*Ke Sun,Jingyi Yan,Zhenglin Li,Shaorong Xie*

Main category: eess.SY

TL;DR: 本文研究了数据注入攻击中系统信息不完整性对攻击性能的影响，建立了信息不完整性与攻击隐蔽性和破坏性之间的权衡关系，并提出了最大不完整性策略来降低攻击隐蔽能力。


<details>
  <summary>Details</summary>
Motivation: 数据注入攻击的有效性高度依赖于攻击者可获取的系统信息完整性，因此增强信息不完整性成为降低攻击性能的重要防御策略。

Method: 系统分析了不完整的导纳信息对攻击双重目标的影响，建立了两种不同操作机制的充分条件，提出了最大不完整性策略，并通过缩减可行域和启发式算法求解优化问题。

Result: 数值仿真验证了理论发现，表明信息不完整性确实能够有效降低攻击的隐蔽能力，同时在不同操作机制下影响攻击的破坏性。

Conclusion: 信息不完整性是防御数据注入攻击的有效策略，通过适当设计可以显著降低攻击的隐蔽性和破坏性，为电力系统安全提供重要保障。

Abstract: The effectiveness of Data Injections Attacks (DIAs) critically depends on the
completeness of the system information accessible to adversaries. This
relationship positions information incompleteness enhancement as a vital
defense strategy for degrading DIA performance. In this paper, we focus on the
information-theoretic stealth attacks, where the attacker encounters a
fundamental tradeoff between the attack stealthiness and destructiveness.
Specifically, we systematically characterize how incomplete admittance
information impacts the dual objectives. In particular, we establish sufficient
conditions for two distinct operational regimes: (i) stealthiness intensifies
while destructive potential diminishes and (ii) destructiveness increases while
stealth capability weakens. For scenarios beyond these regimes, we propose a
maximal incompleteness strategy to optimally degrade stealth capability. To
solve the associated optimization problem, the feasible region is reduced
without excluding the optimal solution, and a heuristic algorithm is then
introduced to effectively identify the near-optimal solutions within the
reduced region. Numerical simulations are conducted on IEEE test systems to
validate the findings.

</details>


### [35] [Physics-Informed Neural Networks for MIMO Beam Map and Environment Reconstruction](https://arxiv.org/abs/2510.21238)
*Wangqian Chen,Junting Chen,Shuguang Cui*

Main category: eess.SY

TL;DR: 提出了一种基于接收信号强度数据的物理信息深度学习框架，无需明确3D环境知识即可联合构建MIMO系统的波束图和环境几何结构，相比现有方法在波束图构建精度上提升了32%-48%。


<details>
  <summary>Details</summary>
Motivation: 随着通信网络向6G及更高复杂度发展，深入理解无线环境变得至关重要。当缺乏明确的环境知识时，从信道状态信息中提取几何感知特征成为连接物理层测量与网络智能的关键方法。

Method: 提出了面向虚拟障碍物模型，捕捉遮挡和反射的几何特征；推导了反射区的解析表达式并分析其几何特性；开发了结合反射区几何模型的物理信息深度学习框架，学习遮挡、反射和散射分量以及波束模式。

Result: 数值实验表明，除了重建遮挡和反射几何结构外，所提模型能够构建更准确的MIMO波束图，精度提升了32%-48%。

Conclusion: 该研究成功展示了利用物理先验知识增强网络可迁移性的有效性，为复杂无线环境下的智能通信系统提供了新的解决方案。

Abstract: As communication networks evolve towards greater complexity (e.g., 6G and
beyond), a deep understanding of the wireless environment becomes increasingly
crucial. When explicit knowledge of the environment is unavailable,
geometry-aware feature extraction from channel state information (CSI) emerges
as a pivotal methodology to bridge physical-layer measurements with network
intelligence. This paper proposes to explore the received signal strength (RSS)
data, without explicit 3D environment knowledge, to jointly construct the radio
beam map and environmental geometry for a multiple-input multiple-output (MIMO)
system. Unlike existing methods that only learn blockage structures, we propose
an oriented virtual obstacle model that captures the geometric features of both
blockage and reflection. Reflective zones are formulated to identify relevant
reflected paths according to the geometry relation of the environment. We
derive an analytical expression for the reflective zone and further analyze its
geometric characteristics to develop a reformulation that is more compatible
with deep learning representations. A physics-informed deep learning framework
that incorporates the reflective-zone-based geometry model is proposed to learn
the blockage, reflection, and scattering components, along with the beam
pattern, which leverages physics prior knowledge to enhance network
transferability. Numerical experiments demonstrate that, in addition to
reconstructing the blockage and reflection geometry, the proposed model can
construct a more accurate MIMO beam map with a 32%-48% accuracy improvement.

</details>


### [36] [The PhasorArray Toolbox for Harmonic Analysis and Control Design](https://arxiv.org/abs/2510.21294)
*Maxime Grosso,Pierre Riedinger,Jamal Daafouz*

Main category: eess.SY

TL;DR: 开发了一个MATLAB工具箱Pha-sorArray，用于简化谐波分析和控制方法，采用面向对象架构，支持周期矩阵的直观操作和自动Toeplitz构造，集成了高级方程求解器和YALMIP。


<details>
  <summary>Details</summary>
Motivation: 使谐波分析和控制方法更加实用和用户友好，解决传统方法在操作周期矩阵时的复杂性。

Method: 采用面向对象架构，通过重载运算符实现周期矩阵的直观操作，包括加法、乘法、卷积和自动Toeplitz构造，集成谐波Sylvester、Lyapunov和Riccati方程求解器，并与YALMIP无缝集成。

Result: 开发了Pha-sorArray工具箱，提供了实用的谐波分析和控制功能，支持基于线性矩阵不等式（LMIs）的高级控制和分析技术。

Conclusion: Pha-sorArray工具箱成功实现了谐波分析和控制方法的实用化和用户友好化，为相关领域的研究和应用提供了便利。

Abstract: We present a MATLAB package called the Pha-sorArray Toolbox that has been
developed to make harmonic analysis and control methods both practical and
user-friendly. The toolbox adopts an object-oriented architecture that enables
intuitive manipulation of periodic matrices through overloaded operators for
addition, multiplication, convolution, and automatic Toeplitz construction. Its
advanced features include harmonic Sylvester, Lyapunov and Riccati equations
solvers, and seamless integration with YALMIP, thereby facilitating advanced
control and analysis techniques based on Linear Matrix Inequalities (LMIs) in
the harmonic framework.

</details>


### [37] [Data-driven Koopman MPC using Mixed Stochastic-Deterministic Tubes](https://arxiv.org/abs/2510.21308)
*Zhengang Zhong,Ehecatl Antonio del Rio-Chanona,Panagiotis Petsagkourakis*

Main category: eess.SY

TL;DR: 本文提出了一种结合Koopman算子和分布鲁棒优化的数据驱动随机MPC方法，用于处理带加性扰动的离散时间非线性系统。


<details>
  <summary>Details</summary>
Motivation: 针对非线性系统在存在建模误差和扰动时的约束控制问题，传统方法难以同时处理随机性和非线性特性。

Method: 通过Koopman算子将非线性系统提升到线性空间，结合分布鲁棒优化构建混合随机-确定性管，确保约束满足。

Result: 提供了两种管的有限样本误差界，并通过数值仿真验证了方法的有效性。

Conclusion: 该方法能够有效处理非线性系统的随机控制问题，同时保证约束满足和性能优化。

Abstract: This paper presents a novel data-driven stochastic MPC design for
discrete-time nonlinear systems with additive disturbances by leveraging the
Koopman operator and a distributionally robust optimization (DRO) framework. By
lifting the dynamical system into a linear space, we achieve a
finite-dimensional approximation of the Koopman operator. We explicitly account
for the modeling approximation and additive disturbance error by a mixed
stochastic-deterministic tube for the lifted linear model. This ensures the
regulation of the original nonlinear system while complying with the
prespecified constraints. Stochastic and deterministic tubes are constructed
using a DRO and a hyper-cube hull, respectively. We provide finite sample error
bounds for both types of tubes. The effectiveness of the proposed approach is
demonstrated through numerical simulations.

</details>


### [38] [Predictive control barrier functions for piecewise affine systems with non-smooth constraints](https://arxiv.org/abs/2510.21321)
*Kanghui He,Anil Alan,Shengling Shi,Ton van den Boom,Bart De Schutter*

Main category: eess.SY

TL;DR: 本文针对非光滑系统和约束下的安全控制问题，提出了基于广义Clarke导数的预测性安全滤波器方法，解决了传统CBF方法在非光滑情况下的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统控制障碍函数(CBF)方法在处理复杂非线性系统和非光滑约束时面临挑战，特别是梯度定义不明确和实时计算困难的问题。

Method: 采用广义Clarke导数设计预测性安全滤波器(PSF)，通过在线有限时域最优控制问题隐式定义大安全集，并提出显式近似方法降低计算负担。

Result: 证明了在所有广义Clarke导数元素上强制执行CBF约束足以保证安全性，并通过数值算例验证了所提方法的有效性。

Conclusion: 所提出的方法能够有效处理非光滑系统和控制非仿射系统的安全控制问题，为复杂系统的实时安全控制提供了可行方案。

Abstract: Obtaining control barrier functions (CBFs) with large safe sets for complex
nonlinear systems and constraints is a challenging task. Predictive CBFs
address this issue by using an online finite-horizon optimal control problem
that implicitly defines a large safe set. The optimal control problem, also
known as the predictive safety filter (PSF), involves predicting the system's
flow under a given backup control policy. However, for non-smooth systems and
constraints, some key elements, such as CBF gradients and the sensitivity of
the flow, are not well-defined, making the current methods inadequate for
ensuring safety. Additionally, for control-non-affine systems, the PSF is
generally nonlinear and non-convex, posing challenges for real-time
computation. This paper considers piecewise affine systems, which are usually
control-non-affine, under nonlinear state and polyhedral input constraints. We
solve the safety issue by incorporating set-valued generalized Clarke
derivatives in the PSF design. We show that enforcing CBF constraints across
all elements of the generalized Clarke derivatives suffices to guarantee
safety. Moreover, to lighten the computational overhead, we propose an explicit
approximation of the PSF. The resulting control methods are demonstrated
through numerical examples.

</details>


### [39] [Auction-Based Responsibility Allocation for Scalable Decentralized Safety Filters in Cooperative Multi-Agent Collision Avoidance](https://arxiv.org/abs/2510.21546)
*Johannes Autenrieb,Mark Spiller*

Main category: eess.SY

TL;DR: 提出基于高阶控制屏障函数和拍卖式责任分配的可扩展去中心化安全滤波器，解决多智能体系统中安全约束的可行性和可扩展性问题


<details>
  <summary>Details</summary>
Motivation: 现有去中心化HOCBF方法在智能体数量增加时面临可行性挑战和实时性要求难以满足的问题，每个智能体需要评估大量成对约束

Method: 引入基于拍卖的责任分配方案，根据局部控制努力估计在邻居间非对称分配约束执行责任，形成有向责任图

Result: 仿真结果表明该方法在各种网络规模和交互密度下都能实现安全高效的协调

Conclusion: 拍卖式责任分配方案在保证完全安全覆盖的同时减少了冗余约束和单智能体计算负载

Abstract: This paper proposes a scalable decentralized safety filter for multi-agent
systems based on high-order control barrier functions (HOCBFs) and
auction-based responsibility allocation. While decentralized HOCBF formulations
ensure pairwise safety under input bounds, they face feasibility and
scalability challenges as the number of agents grows. Each agent must evaluate
an increasing number of pairwise constraints, raising the risk of infeasibility
and making it difficult to meet real-time requirements. To address this, we
introduce an auction-based allocation scheme that distributes constraint
enforcement asymmetrically among neighbors based on local control effort
estimates. The resulting directed responsibility graph guarantees full safety
coverage while reducing redundant constraints and per-agent computational load.
Simulation results confirm safe and efficient coordination across a range of
network sizes and interaction densities.

</details>


### [40] [System-Theoretic Analysis of Dynamic Generalized Nash Equilibrium Problems -- Turnpikes and Dissipativity](https://arxiv.org/abs/2510.21556)
*Sophie Hall,Florian Dörfler,Timm Faulwasser*

Main category: eess.SY

TL;DR: 该论文从系统理论角度研究广义纳什均衡(GNE)轨迹的性质，建立了严格耗散性与turnpike现象之间的等价关系，并设计了确保GNE轨迹收敛到稳态GNE的线性终端惩罚。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体控制中广义纳什均衡的系统理论性质，建立与最优控制中类似的系统理论分析基础。

Method: 从系统理论角度分析GNE轨迹，研究严格耗散性与turnpike现象的关系，使用博弈价值函数分析存储函数的几何特性，设计线性终端惩罚。

Result: 证明了严格耗散性产生GNE解中的turnpike现象，建立了turnpike到严格耗散性的逆命题，给出了稳态GNE是最优运行点的条件。

Conclusion: 这些连接为未来GNE的系统理论分析提供了基础，类似于最优控制中已有的分析框架。

Abstract: Generalized Nash equilibria are used in multi-agent control applications to
model strategic interactions between agents that are coupled in the cost,
dynamics, and constraints. We study the properties of open-loop GNE
trajectories from a system-theoretic perspective. We show how strict
dissipativity generates the turnpike phenomenon in GNE solutions. Moreover, we
establish a converse turnpike result, i.e., the implication from turnpike to
strict dissipativity. We derive conditions under which the steady-state GNE is
the optimal operating point and, using a game value function, we give a local
characterization of the geometry of storage functions. Finally, we design
linear terminal penalties that ensure GNE open-loop trajectories converge to
and remain at the steady-state GNE. These connections provide the foundation
for future system-theoretic analysis of GNEs similar to those existing in
optimal control.

</details>


### [41] [Rate-cost tradeoffs in continuous-time control with a biomolecular application](https://arxiv.org/abs/2510.21612)
*Yorie Nakahira,Fangzhou Xiao,Victoria Kostina,John C. Doyle*

Main category: eess.SY

TL;DR: 本文研究了广义Ornstein-Uhlenbeck过程的速率受限控制，推导了实现期望控制成本所需数据速率的下界，并探讨了在生物分子系统控制中的应用。


<details>
  <summary>Details</summary>
Motivation: 研究广义Ornstein-Uhlenbeck过程的速率受限控制问题，其中控制动作可以是乘法或加法形式，噪声方差可依赖于控制动作。该模型近似离散状态分子生灭过程的动力学，对通过化学反应控制生物分子系统有直接意义。

Method: 推导了实现期望控制成本所需数据速率的下界，并证明当通过加性高斯白噪声信道进行控制时，该下界能够达到等式成立。

Result: 获得了速率受限控制的数据速率下界，该下界在加性高斯白噪声信道控制下可达到最优。

Conclusion: 该研究为生物分子系统的控制提供了理论基础，其中乘法控制对应降解速率，加法控制对应生产速率，控制目标是减少受控分子物种围绕期望浓度水平的波动。

Abstract: This paper focuses on rate-limited control of the generalized
Ornstein-Uhlenbeck process where the control action can be either
multiplicative or additive, and the noise variance can depend on the control
action. We derive a lower bound on the data rate necessary to achieve the
desired control cost. The lower bound is attained with equality if the control
is performed via an additive white Gaussian channel. The system model
approximates the dynamics of a discrete-state molecular birth-death process,
and the result has direct implications on the control of a biomolecular system
via chemical reactions, where the multiplicative control corresponds to the
degradation rate, the additive control corresponds to the production rate, and
the control objective is to decrease the fluctuations of the controlled
molecular species around their desired concentration levels.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [42] [Sketch2BIM: A Multi-Agent Human-AI Collaborative Pipeline to Convert Hand-Drawn Floor Plans to 3D BIM](https://arxiv.org/abs/2510.20838)
*Abir Khan Ratul,Sanjay Acharjee,Somin Park,Md Nazmus Sakib*

Main category: cs.AI

TL;DR: 提出了一种人机协同流程，将手绘平面图转换为语义一致的3D BIM模型，使用多模态大语言模型和多智能体框架，通过迭代反馈实现高精度转换。


<details>
  <summary>Details</summary>
Motivation: 让专家和非专家都能通过简单的手绘草图创建BIM模型，降低BIM创建的技术门槛。

Method: 使用多模态大语言模型的多智能体框架，结合感知提取、人工反馈、模式验证和自动化BIM脚本生成，将草图迭代优化为结构化JSON布局，再转换为可执行脚本生成3D BIM模型。

Result: 在10个不同平面图上测试，门窗检测初始精度高，墙体检测从83%开始，经过几次反馈迭代后达到近乎完美对齐。所有类别的精确率、召回率和F1分数均超过0.83，几何误差通过反馈修正逐步降至零。

Conclusion: MLLM驱动的多智能体推理能够使BIM创建对专家和非专家都变得可访问，仅需使用手绘草图即可实现。

Abstract: This study introduces a human-in-the-loop pipeline that converts unscaled,
hand-drawn floor plan sketches into semantically consistent 3D BIM models. The
workflow leverages multimodal large language models (MLLMs) within a
multi-agent framework, combining perceptual extraction, human feedback, schema
validation, and automated BIM scripting. Initially, sketches are iteratively
refined into a structured JSON layout of walls, doors, and windows. Later,
these layouts are transformed into executable scripts that generate 3D BIM
models. Experiments on ten diverse floor plans demonstrate strong convergence:
openings (doors, windows) are captured with high reliability in the initial
pass, while wall detection begins around 83% and achieves near-perfect
alignment after a few feedback iterations. Across all categories, precision,
recall, and F1 scores remain above 0.83, and geometric errors (RMSE, MAE)
progressively decrease to zero through feedback corrections. This study
demonstrates how MLLM-driven multi-agent reasoning can make BIM creation
accessible to both experts and non-experts using only freehand sketches.

</details>


### [43] [Cultural Alien Sampler: Open-ended art generation balancing originality and coherence](https://arxiv.org/abs/2510.20849)
*Alejandro H. Artiles,Hiromu Yakura,Levin Brinkmann,Mar Canet Sola,Hassan Abu Alhaija,Ignacio Serna,Nasim Rahaman,Bernhard Schölkopf,Iyad Rahwan*

Main category: cs.AI

TL;DR: 提出文化异类采样器(CAS)，通过分离概念组合的连贯性和文化典型性，在保持内部一致性的同时生成偏离文化惯例的创意想法


<details>
  <summary>Details</summary>
Motivation: 解决当前大语言模型在开放领域创意生成中要么遵循熟悉文化模式、要么牺牲连贯性追求新颖性的问题

Method: 使用两个GPT-2模型：概念连贯性模型评估概念在艺术品中共同出现的合理性，文化背景模型估计概念组合在艺术家作品中的典型性，CAS选择高连贯性低典型性的组合

Result: 在人类评估中优于随机选择和GPT-4o基线，与艺术专业学生表现相当；定量研究显示比GPT-4o产生更多样化输出和更广泛的概念空间探索

Conclusion: 人工文化异类性能够释放自主代理的创造潜力，在保持内部一致性的同时生成偏离文化惯例的创意

Abstract: In open-ended domains like art, autonomous agents must generate ideas that
are both original and internally coherent, yet current Large Language Models
(LLMs) either default to familiar cultural patterns or sacrifice coherence when
pushed toward novelty. We address this by introducing the Cultural Alien
Sampler (CAS), a concept-selection method that explicitly separates
compositional fit from cultural typicality. CAS uses two GPT-2 models
fine-tuned on WikiArt concepts: a Concept Coherence Model that scores whether
concepts plausibly co-occur within artworks, and a Cultural Context Model that
estimates how typical those combinations are within individual artists' bodies
of work. CAS targets combinations that are high in coherence and low in
typicality, yielding ideas that maintain internal consistency while deviating
from learned conventions and embedded cultural context. In a human evaluation
(N = 100), our approach outperforms random selection and GPT-4o baselines and
achieves performance comparable to human art students in both perceived
originality and harmony. Additionally, a quantitative study shows that our
method produces more diverse outputs and explores a broader conceptual space
than its GPT-4o counterpart, demonstrating that artificial cultural alienness
can unlock creative potential in autonomous agents.

</details>


### [44] [Fuzzy numbers revisited: operations on extensional fuzzy numbers](https://arxiv.org/abs/2510.20861)
*Krzysztof Siminski*

Main category: cs.AI

TL;DR: 本文提出了一种新的模糊数表示方法——外延模糊数，以解决传统模糊数运算中的计算复杂性和结果特征不一致问题，并提供了相应的运算和关系操作符实现。


<details>
  <summary>Details</summary>
Motivation: 传统模糊数使用模糊集表示，但存在三个主要问题：计算复杂度高、某些运算结果不符合原始特征（如两个三角模糊数相乘结果不是三角模糊数）、模糊扩散问题。这些问题限制了模糊数的应用范围。

Method: 提出外延模糊数的概念，定义了外延模糊数上的运算操作（加减乘除等）和关系操作符（=, >, >=, <, <=），并通过应用实例进行说明，提供了C++实现。

Result: 开发了外延模糊数的运算框架，解决了传统模糊数运算的计算复杂性和特征保持问题，提供了可用的实现代码。

Conclusion: 外延模糊数方法为模糊数的实际应用提供了更高效和实用的解决方案，克服了传统方法的局限性。

Abstract: Fuzzy numbers are commonly represented with fuzzy sets. Their objective is to
better represent imprecise data. However, operations on fuzzy numbers are not
as straightforward as maths on crisp numbers. Commonly, the Zadeh's extension
rule is applied to elaborate a result. This can produce two problems: (1) high
computational complexity and (2) for some fuzzy sets and some operations the
results is not a fuzzy set with the same features (eg. multiplication of two
triangular fuzzy sets does not produce a triangular fuzzy set). One more
problem is the fuzzy spread -- fuzziness of the result increases with the
number of operations. These facts can severely limit the application field of
fuzzy numbers. In this paper we would like to revisite this problem with a
different kind of fuzzy numbers -- extensional fuzzy numbers. The paper defines
operations on extensional fuzzy numbers and relational operators (=, >, >=, <,
<=) for them. The proposed approach is illustrated with several applicational
examples. The C++ implementation is available from a public GitHub repository.

</details>


### [45] [Customizing Open Source LLMs for Quantitative Medication Attribute Extraction across Heterogeneous EHR Systems](https://arxiv.org/abs/2510.21027)
*Zhe Fei,Mehmet Yigit Turali,Shreyas Rajesh,Xinyang Dai,Huyen Pham,Pavan Holur,Yuhui Zhu,Larissa Mooney,Yih-Ing Hser,Vwani Roychowdhury*

Main category: cs.AI

TL;DR: 提出了一个使用开源大语言模型从异构电子健康记录中提取阿片类药物使用障碍处方信息的框架，能够标准化计算药物覆盖天数。


<details>
  <summary>Details</summary>
Motivation: 解决不同电子健康记录系统中药物数据格式不统一的问题，便于监测阿片类药物使用障碍治疗情况。

Method: 定制化开源LLMs（Llama、Qwen、Gemma、MedGemma）处理异构数据，通过JSON模式输出、轻量级标准化和跨字段一致性检查来提取处方属性。

Result: 在25,605条记录上评估，Qwen2.5-32B达到93.4%覆盖率和93.0%精确匹配准确率，MedGemma-27B达到93.1%/92.2%。

Conclusion: 该方法消除了脆弱的站点特定ETL流程，支持本地隐私保护部署，实现了跨站点MOUD暴露、依从性和保留率的一致性分析。

Abstract: Harmonizing medication data across Electronic Health Record (EHR) systems is
a persistent barrier to monitoring medications for opioid use disorder (MOUD).
In heterogeneous EHR systems, key prescription attributes are scattered across
differently formatted fields and freetext notes. We present a practical
framework that customizes open source large language models (LLMs), including
Llama, Qwen, Gemma, and MedGemma, to extract a unified set of MOUD prescription
attributes (prescription date, drug name, duration, total quantity, daily
quantity, and refills) from heterogeneous, site specific data and compute a
standardized metric of medication coverage, \emph{MOUD days}, per patient. Our
pipeline processes records directly in a fixed JSON schema, followed by
lightweight normalization and cross-field consistency checks. We evaluate the
system on prescription level EHR data from five clinics in a national OUD study
(25{,}605 records from 1{,}257 patients), using a previously annotated
benchmark of 10{,}369 records (776 patients) as the ground truth. Performance
is reported as coverage (share of records with a valid, matchable output) and
record-level exact-match accuracy. Larger models perform best overall:
Qwen2.5-32B achieves \textbf{93.4\%} coverage with \textbf{93.0\%} exact-match
accuracy across clinics, and MedGemma-27B attains
\textbf{93.1\%}/\textbf{92.2\%}. A brief error review highlights three common
issues and fixes: imputing missing dosage fields using within-drug norms,
handling monthly/weekly injectables (e.g., Vivitrol) by setting duration from
the documented schedule, and adding unit checks to prevent mass units (e.g.,
``250 g'') from being misread as daily counts. By removing brittle,
site-specific ETL and supporting local, privacy-preserving deployment, this
approach enables consistent cross-site analyses of MOUD exposure, adherence,
and retention in real-world settings.

</details>


### [46] [Epistemic Deference to AI](https://arxiv.org/abs/2510.21043)
*Benjamin Lange*

Main category: cs.AI

TL;DR: 该论文探讨了何时应该优先采用AI输出而非人类专家判断，提出了AI优先主义观点，但最终主张更合理的总证据观点，即AI输出应作为贡献性理由而非完全替代人类独立认知。


<details>
  <summary>Details</summary>
Motivation: 基于社会认识论，探讨AI系统作为人工认识权威(AEAs)的资格，以及AI输出与人类专家判断之间的优先关系问题。

Method: 分析AI优先主义的经典反对意见，并发展出总证据观点作为替代方案，强调AI输出应作为贡献性理由而非完全替代。

Result: 总证据观点具有三个关键优势：缓解专业知识萎缩、为有意义的人类监督提供认识论依据、解释AI可靠性不足时的合理不信任。

Conclusion: 总证据观点为确定何时AI服从是合理的提供了原则性方法，特别是在需要严格可靠性的高风险情境中。

Abstract: When should we defer to AI outputs over human expert judgment? Drawing on
recent work in social epistemology, I motivate the idea that some AI systems
qualify as Artificial Epistemic Authorities (AEAs) due to their demonstrated
reliability and epistemic superiority. I then introduce AI Preemptionism, the
view that AEA outputs should replace rather than supplement a user's
independent epistemic reasons. I show that classic objections to preemptionism
- such as uncritical deference, epistemic entrenchment, and unhinging epistemic
bases - apply in amplified form to AEAs, given their opacity, self-reinforcing
authority, and lack of epistemic failure markers. Against this, I develop a
more promising alternative: a total evidence view of AI deference. According to
this view, AEA outputs should function as contributory reasons rather than
outright replacements for a user's independent epistemic considerations. This
approach has three key advantages: (i) it mitigates expertise atrophy by
keeping human users engaged, (ii) it provides an epistemic case for meaningful
human oversight and control, and (iii) it explains the justified mistrust of AI
when reliability conditions are unmet. While demanding in practice, this
account offers a principled way to determine when AI deference is justified,
particularly in high-stakes contexts requiring rigorous reliability.

</details>


### [47] [From Questions to Queries: An AI-powered Multi-Agent Framework for Spatial Text-to-SQL](https://arxiv.org/abs/2510.21045)
*Ali Khosravi Kazazi,Zhenlong Li,M. Naser Lessani,Guido Cervone*

Main category: cs.AI

TL;DR: 提出了一个多智能体框架，用于将自然语言问题准确转换为空间SQL查询，通过专业分工和验证机制显著提升了空间查询的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决SQL和PostGIS等地理空间工具对非专业人士的障碍，克服单智能体方法在处理空间查询语义和语法复杂性方面的局限性。

Method: 采用多智能体协作框架，包括知识库、模式分析、语义增强、上下文检索，以及专门的实体提取、元数据检索、查询逻辑制定、SQL生成和验证智能体。

Result: 在KaggleDBQA上达到81.2%的准确率，在空间查询上达到87.7%的准确率（相比无验证智能体的76.7%），且生成的查询在语义上更贴合用户意图。

Conclusion: 该工作使空间分析更易用，为空间Text-to-SQL系统提供了稳健可推广的基础，推动了自主GIS的发展。

Abstract: The complexity of Structured Query Language (SQL) and the specialized nature
of geospatial functions in tools like PostGIS present significant barriers to
non-experts seeking to analyze spatial data. While Large Language Models (LLMs)
offer promise for translating natural language into SQL (Text-to-SQL),
single-agent approaches often struggle with the semantic and syntactic
complexities of spatial queries. To address this, we propose a multi-agent
framework designed to accurately translate natural language questions into
spatial SQL queries. The framework integrates several innovative components,
including a knowledge base with programmatic schema profiling and semantic
enrichment, embeddings for context retrieval, and a collaborative multi-agent
pipeline as its core. This pipeline comprises specialized agents for entity
extraction, metadata retrieval, query logic formulation, SQL generation, and a
review agent that performs programmatic and semantic validation of the
generated SQL to ensure correctness (self-verification). We evaluate our system
using both the non-spatial KaggleDBQA benchmark and a new, comprehensive
SpatialQueryQA benchmark that includes diverse geometry types, predicates, and
three levels of query complexity. On KaggleDBQA, the system achieved an overall
accuracy of 81.2% (221 out of 272 questions) after the review agent's review
and corrections. For spatial queries, the system achieved an overall accuracy
of 87.7% (79 out of 90 questions), compared with 76.7% without the review
agent. Beyond accuracy, results also show that in some instances the system
generates queries that are more semantically aligned with user intent than
those in the benchmarks. This work makes spatial analysis more accessible, and
provides a robust, generalizable foundation for spatial Text-to-SQL systems,
advancing the development of autonomous GIS.

</details>


### [48] [MedAlign: A Synergistic Framework of Multimodal Preference Optimization and Federated Meta-Cognitive Reasoning](https://arxiv.org/abs/2510.21093)
*Siyong Chen,Jinbo Wen,Jiawen Kang,Tenghui Huang,Xumin Huang,Yuanjia Su,Hudan Pan,Zishao Zhong,Dusit Niyato,Shengli Xie,Dong In Kim*

Main category: cs.AI

TL;DR: MedAlign是一个针对医学视觉问答的新框架，通过多模态直接偏好优化、检索感知专家混合架构和联邦治理机制，解决大型视觉语言模型在医疗应用中的幻觉、推理效率和多机构协作问题。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在医疗部署中面临三个关键挑战：产生基于视觉证据的幻觉答案、固定深度推理效率低下、多机构协作困难。

Method: 提出多模态直接偏好优化(mDPO)目标，设计检索感知专家混合(RA-MoE)架构，采用联邦治理机制结合本地元认知不确定性估计器进行自适应推理。

Result: 在三个代表性Med-VQA数据集上实现最先进性能，F1分数比强检索增强基线提升高达11.85%，平均推理长度比固定深度CoT方法减少51.60%。

Conclusion: MedAlign框架有效解决了LVLM在医疗应用中的关键挑战，实现了视觉准确的响应和高效推理，支持多机构协作。

Abstract: Recently, large models have shown significant potential for smart healthcare.
However, the deployment of Large Vision-Language Models (LVLMs) for clinical
services is currently hindered by three critical challenges: a tendency to
hallucinate answers not grounded in visual evidence, the inefficiency of
fixed-depth reasoning, and the difficulty of multi-institutional collaboration.
To address these challenges, in this paper, we develop MedAlign, a novel
framework to ensure visually accurate LVLM responses for Medical Visual
Question Answering (Med-VQA). Specifically, we first propose a multimodal
Direct Preference Optimization (mDPO) objective to explicitly align preference
learning with visual context. We then design a Retrieval-Aware
Mixture-of-Experts (RA-MoE) architecture that utilizes image and text
similarity to route queries to a specialized and context-augmented LVLM (i.e.,
an expert), thereby mitigating hallucinations in LVLMs. To achieve adaptive
reasoning and facilitate multi-institutional collaboration, we propose a
federated governance mechanism, where the selected expert, fine-tuned on
clinical datasets based on mDPO, locally performs iterative Chain-of-Thought
(CoT) reasoning via the local meta-cognitive uncertainty estimator. Extensive
experiments on three representative Med-VQA datasets demonstrate that MedAlign
achieves state-of-the-art performance, outperforming strong retrieval-augmented
baselines by up to $11.85\%$ in F1-score, and simultaneously reducing the
average reasoning length by $51.60\%$ compared with fixed-depth CoT approaches.

</details>


### [49] [Confounding Robust Deep Reinforcement Learning: A Causal Approach](https://arxiv.org/abs/2510.21110)
*Mingxuan Li,Junzhe Zhang,Elias Bareinboim*

Main category: cs.AI

TL;DR: 提出了一种针对观测数据中存在未观测混杂偏置的深度强化学习算法，该算法基于DQN框架，旨在寻找与观测数据兼容的最坏情况环境下的安全策略。


<details>
  <summary>Details</summary>
Motivation: 在复杂高维领域中，当存在未观测混杂因素时，传统的离策略学习方法可能因数据偏置而失效，需要开发对混杂偏置具有鲁棒性的算法。

Method: 基于深度Q网络(DQN)框架，提出了一种新算法，通过寻找与观测数据兼容的最坏情况环境下的安全策略来应对未观测混杂偏置。

Result: 在12个存在混杂的Atari游戏上进行测试，结果显示该算法在所有存在行为策略与目标策略输入不匹配且存在未观测混杂因素的游戏中，始终优于标准DQN。

Conclusion: 所提出的算法能够有效应对观测数据中的混杂偏置，在存在未观测混杂因素的环境中表现出优于标准DQN的性能。

Abstract: A key task in Artificial Intelligence is learning effective policies for
controlling agents in unknown environments to optimize performance measures.
Off-policy learning methods, like Q-learning, allow learners to make optimal
decisions based on past experiences. This paper studies off-policy learning
from biased data in complex and high-dimensional domains where \emph{unobserved
confounding} cannot be ruled out a priori. Building on the well-celebrated Deep
Q-Network (DQN), we propose a novel deep reinforcement learning algorithm
robust to confounding biases in observed data. Specifically, our algorithm
attempts to find a safe policy for the worst-case environment compatible with
the observations. We apply our method to twelve confounded Atari games, and
find that it consistently dominates the standard DQN in all games where the
observed input to the behavioral and target policies mismatch and unobserved
confounders exist.

</details>


### [50] [DAO-AI: Evaluating Collective Decision-Making through Agentic AI in Decentralized Governance](https://arxiv.org/abs/2510.21117)
*Chunghyun Han,Alfio Gliozzo,Junkyu Lee,Agostino Capponi*

Main category: cs.AI

TL;DR: 本文首次实证研究AI代理作为去中心化治理中的自主决策者，通过构建AI投票代理在DAO环境中进行独立决策，发现AI决策与人类投票结果高度一致。


<details>
  <summary>Details</summary>
Motivation: 研究AI代理在去中心化治理中作为自主决策者的潜力，探索如何通过AI增强集体决策过程的可解释性和可审计性。

Method: 使用3000多个主要协议提案，构建AI投票代理，通过模块化可组合程序工作流解释提案背景、检索历史审议数据并独立确定投票立场，在基于可验证区块链数据的现实金融模拟环境中运行。

Result: AI代理的决策与人类和代币加权结果高度一致，通过精心设计的评估指标测量显示出强对齐性。

Conclusion: AI代理能够通过产生可解释、可审计且基于实证的信号来增强集体决策，为去中心化金融系统的可解释和经济严谨的AI代理设计做出贡献。

Abstract: This paper presents a first empirical study of agentic AI as autonomous
decision-makers in decentralized governance. Using more than 3K proposals from
major protocols, we build an agentic AI voter that interprets proposal
contexts, retrieves historical deliberation data, and independently determines
its voting position. The agent operates within a realistic financial simulation
environment grounded in verifiable blockchain data, implemented through a
modular composable program (MCP) workflow that defines data flow and tool usage
via Agentics framework. We evaluate how closely the agent's decisions align
with the human and token-weighted outcomes, uncovering strong alignments
measured by carefully designed evaluation metrics. Our findings demonstrate
that agentic AI can augment collective decision-making by producing
interpretable, auditable, and empirically grounded signals in realistic DAO
governance settings. The study contributes to the design of explainable and
economically rigorous AI agents for decentralized financial systems.

</details>


### [51] [PanicToCalm: A Proactive Counseling Agent for Panic Attacks](https://arxiv.org/abs/2510.21143)
*Jihyun Lee,Yejin Min,San Kim,Yejin Jeon,SungJun Yang,Hyounghun Kim,Gary Geunbae Lee*

Main category: cs.AI

TL;DR: PACE数据集包含高痛苦事件的第一人称叙述，围绕心理急救原则构建。PACER模型通过监督学习和模拟偏好对齐训练，在恐慌场景中优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 恐慌发作是急性恐惧和痛苦发作，及时干预能显著帮助个体恢复稳定，但由于伦理和后勤问题，训练此类模型的合适数据集稀缺。

Method: 引入PACE数据集，使用该数据训练PACER咨询模型，通过监督学习和模拟偏好对齐优化，提供共情和指导性支持。

Result: 实验结果显示PACER在咨询师侧指标和客户情感改善方面优于强基线。人类评估进一步证实其实用价值，在恐慌场景中始终优于通用、CBT基和GPT-4驱动模型。

Conclusion: PACER模型在恐慌发作干预方面表现出色，为心理健康危机干预提供了有效工具。

Abstract: Panic attacks are acute episodes of fear and distress, in which timely,
appropriate intervention can significantly help individuals regain stability.
However, suitable datasets for training such models remain scarce due to
ethical and logistical issues. To address this, we introduce PACE, which is a
dataset that includes high-distress episodes constructed from first-person
narratives, and structured around the principles of Psychological First Aid
(PFA). Using this data, we train PACER, a counseling model designed to provide
both empathetic and directive support, which is optimized through supervised
learning and simulated preference alignment. To assess its effectiveness, we
propose PanicEval, a multi-dimensional framework covering general counseling
quality and crisis-specific strategies. Experimental results show that PACER
outperforms strong baselines in both counselor-side metrics and client affect
improvement. Human evaluations further confirm its practical value, with PACER
consistently preferred over general, CBT-based, and GPT-4-powered models in
panic scenarios (Code is available at https://github.com/JihyunLee1/PanicToCalm
).

</details>


### [52] [NeuroGenPoisoning: Neuron-Guided Attacks on Retrieval-Augmented Generation of LLM via Genetic Optimization of External Knowledge](https://arxiv.org/abs/2510.21144)
*Hanyu Zhu,Lance Fiondella,Jiawei Yuan,Kai Zeng,Long Jiao*

Main category: cs.AI

TL;DR: 提出了NeuroGenPoisoning攻击框架，通过分析LLM内部神经元归因和遗传优化来生成对抗性外部知识，有效实现RAG系统的大规模知识毒化攻击。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG攻击方法主要关注检索内容或提示结构的迭代操作，忽略了模型内部表示动态和神经元级敏感性，且未充分考虑与强参数化知识的知识冲突问题。

Method: 首先识别与上下文毒化知识强相关的毒化响应神经元，然后使用遗传算法进化对抗性段落以最大化激活这些神经元，并通过观察到的归因信号重用有潜力但最初不成功的外部知识变体。

Result: 在多个模型和数据集上的实验结果显示，该方法能持续实现超过90%的群体覆盖成功率，同时保持流畅性，并有效解决知识冲突问题。

Conclusion: NeuroGenPoisoning框架通过神经元归因引导的毒化方法，能够大规模生成有效的毒化RAG知识，并成功解决知识冲突问题。

Abstract: Retrieval-Augmented Generation (RAG) empowers Large Language Models (LLMs) to
dynamically integrate external knowledge during inference, improving their
factual accuracy and adaptability. However, adversaries can inject poisoned
external knowledge to override the model's internal memory. While existing
attacks iteratively manipulate retrieval content or prompt structure of RAG,
they largely ignore the model's internal representation dynamics and
neuron-level sensitivities. The underlying mechanism of RAG poisoning has not
been fully studied and the effect of knowledge conflict with strong parametric
knowledge in RAG is not considered. In this work, we propose NeuroGenPoisoning,
a novel attack framework that generates adversarial external knowledge in RAG
guided by LLM internal neuron attribution and genetic optimization. Our method
first identifies a set of Poison-Responsive Neurons whose activation strongly
correlates with contextual poisoning knowledge. We then employ a genetic
algorithm to evolve adversarial passages that maximally activate these neurons.
Crucially, our framework enables massive-scale generation of effective poisoned
RAG knowledge by identifying and reusing promising but initially unsuccessful
external knowledge variants via observed attribution signals. At the same time,
Poison-Responsive Neurons guided poisoning can effectively resolves knowledge
conflict. Experimental results across models and datasets demonstrate
consistently achieving high Population Overwrite Success Rate (POSR) of over
90% while preserving fluency. Empirical evidence shows that our method
effectively resolves knowledge conflict.

</details>


### [53] [How to Auto-optimize Prompts for Domain Tasks? Adaptive Prompting and Reasoning through Evolutionary Domain Knowledge Adaptation](https://arxiv.org/abs/2510.21148)
*Yang Zhao,Pu Wang,Hao Frank Yang*

Main category: cs.AI

TL;DR: EGO-Prompt是一个自动化框架，通过进化图优化方法自动设计更好的提示和推理过程，在领域特定任务中显著提升LLM性能。


<details>
  <summary>Details</summary>
Motivation: 在现实应用中，为领域特定任务设计最优提示和推理过程是必要但具有挑战性的。如何整合领域知识、提高推理效率，以及为领域专家提供知识整合提示都是关键但未解决的问题。

Method: 提出EGO-Prompt框架，从专家构建的初始语义因果图(SCG)出发，通过因果引导的文本梯度过程分两步优化：生成确定性推理指导，并让LLM适应使用该指导。使用迭代优化算法结合文本梯度和真实标签进一步精炼SCG和推理机制。

Result: 在公共卫生、交通和人类行为任务上测试，EGO-Prompt比最先进方法F1分数提高7.32%-12.61%，让小模型以不到20%的成本达到大模型性能，并输出精炼的领域特定SCG提高可解释性。

Conclusion: EGO-Prompt有效解决了领域特定任务中提示设计和推理过程优化的挑战，显著提升了LLM性能并降低了成本，同时增强了模型的可解释性。

Abstract: Designing optimal prompts and reasoning processes for large language models
(LLMs) on domain-specific tasks is both necessary and challenging in real-world
applications. Determining how to integrate domain knowledge, enhance reasoning
efficiency, and even provide domain experts with refined knowledge integration
hints are particularly crucial yet unresolved tasks. In this research, we
propose Evolutionary Graph Optimization for Prompting (EGO-Prompt), an
automated framework to designing better prompts, efficient reasoning processes
and providing enhanced causal-informed process. EGO-Prompt begins with a
general prompt and fault-tolerant initial Semantic Causal Graph (SCG)
descriptions, constructed by human experts, which is then automatically refined
and optimized to guide LLM reasoning. Recognizing that expert-defined SCGs may
be partial or imperfect and that their optimal integration varies across LLMs,
EGO-Prompt integrates a novel causal-guided textual gradient process in two
steps: first, generating nearly deterministic reasoning guidance from the SCG
for each instance, and second, adapting the LLM to effectively utilize the
guidance alongside the original input. The iterative optimization algorithm
further refines both the SCG and the reasoning mechanism using textual
gradients with ground-truth. We tested the framework on real-world public
health, transportation and human behavior tasks. EGO-Prompt achieves
7.32%-12.61% higher F1 than cutting-edge methods, and allows small models to
reach the performence of larger models at under 20% of the original cost. It
also outputs a refined, domain-specific SCG that improves interpretability.

</details>


### [54] [String Seed of Thought: Prompting LLMs for Distribution-Faithful and Diverse Generation](https://arxiv.org/abs/2510.21150)
*Kou Misaki,Takuya Akiba*

Main category: cs.AI

TL;DR: SSoT是一种新颖的提示方法，通过在LLM输出前生成随机字符串来增加熵，从而改善概率指令跟随(PIF)性能，提高回答多样性。


<details>
  <summary>Details</summary>
Motivation: LLMs在需要确定性答案的任务上表现出色，但在概率指令跟随(PIF)任务中表现不佳，存在偏见问题，影响需要非确定性行为的应用（如人类行为模拟、内容多样化、多人游戏）和回答多样性。

Method: 提出SSoT提示方法：首先让LLM输出随机字符串生成足够熵，然后通过操作该字符串提取随机性来推导最终答案，在保持多样性的同时遵守特定约束。

Result: SSoT显著提高了LLMs的PIF性能，接近伪随机数生成器的理想性能。在NoveltyBench上的实验表明SSoT还能增强开放任务的回答多样性。

Conclusion: SSoT是一种简单有效的提示方法，能够显著改善LLMs在概率指令跟随任务中的表现，同时提升回答多样性，具有广泛的应用价值。

Abstract: We introduce String Seed of Thought (SSoT), a novel prompting method for LLMs
that improves Probabilistic Instruction Following (PIF). We define PIF as a
task requiring an LLM to select its answer from a predefined set of options,
each associated with a specific probability, such that the empirical
distribution of the generated answers aligns with the target distribution when
prompted multiple times. While LLMs excel at tasks with single, deterministic
answers, they often fail at PIF, exhibiting biases problematic for applications
requiring non-deterministic behaviors, such as human-behavior simulation,
content diversification, and multiplayer games. It also harms the diversity of
generated responses, a crucial factor in test-time scaling, by causing the
outputs to collapse into a limited set of answers. To address this, we propose
SSoT, a simple prompting method that instructs an LLM to first output a random
string to generate sufficient entropy. SSoT also instructs the LLM to extract
randomness by manipulating this string to derive a final answer, thereby
preserving diversity while adhering to specific constraints. We demonstrate
that SSoT significantly improves the PIF performance of LLMs, approaching the
ideal performance of a pseudo-random number generator. Furthermore, our
experiments on NoveltyBench show SSoT's benefits extend beyond closed-set tasks
to open-ended tasks by enhancing response diversity.

</details>


### [55] [Memory-Free Continual Learning with Null Space Adaptation for Zero-Shot Vision-Language Models](https://arxiv.org/abs/2510.21175)
*Yujin Jo,Taesup Kim*

Main category: cs.AI

TL;DR: NuSA-CL是一个轻量级、无需内存的持续学习框架，通过低秩适应和零空间约束来保护预训练视觉语言模型的零样本能力，避免灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 预训练视觉语言模型在现实部署中面临分布偏移和新任务挑战，静态零样本能力不足，需要持续学习方法来适应环境变化同时避免遗忘。

Method: 采用低秩适应技术，将任务特定权重更新约束在模型当前参数的近似零空间内，最小化对已学知识的干扰。

Result: 实验表明该框架有效保护零样本迁移能力，在持续学习基准上取得有竞争力的性能。

Conclusion: NuSA-CL是实际可扩展的解决方案，适用于现实世界中持续演化的零样本视觉语言模型。

Abstract: Pre-trained vision-language models (VLMs), such as CLIP, have demonstrated
remarkable zero-shot generalization, enabling deployment in a wide range of
real-world tasks without additional task-specific training. However, in real
deployment scenarios with evolving environments or emerging classes, these
models inevitably face distributional shifts and novel tasks. In such contexts,
static zero-shot capabilities are insufficient, and there is a growing need for
continual learning methods that allow models to adapt over time while avoiding
catastrophic forgetting. We introduce NuSA-CL (Null Space Adaptation for
Continual Learning), a lightweight memory-free continual learning framework
designed to address this challenge. NuSA-CL employs low-rank adaptation and
constrains task-specific weight updates to lie within an approximate null space
of the model's current parameters. This strategy minimizes interference with
previously acquired knowledge, effectively preserving the zero-shot
capabilities of the original model. Unlike methods relying on replay buffers or
costly distillation, NuSA-CL imposes minimal computational and memory overhead,
making it practical for deployment in resource-constrained, real-world
continual learning environments. Experiments show that our framework not only
effectively preserves zero-shot transfer capabilities but also achieves highly
competitive performance on continual learning benchmarks. These results
position NuSA-CL as a practical and scalable solution for continually evolving
zero-shot VLMs in real-world applications.

</details>


### [56] [Shylock: Causal Discovery in Multivariate Time Series based on Hybrid Constraints](https://arxiv.org/abs/2510.21181)
*Shuo Li,Keqin Xu,Jie Liu,Dan Ye*

Main category: cs.AI

TL;DR: 提出Shylock方法，用于在少样本和正常多变量时间序列中有效发现因果关系，通过组膨胀卷积和共享核减少参数数量，结合全局和局部约束提高准确性。


<details>
  <summary>Details</summary>
Motivation: 现有因果关系发现方法依赖人类经验、统计方法或图形准则方法，容易出错、依赖理想化假设和大量数据。多变量时间序列存在数据获取困难，现有方法容易过拟合。

Method: 使用组膨胀卷积和共享核来指数级减少参数数量，同时学习带时间延迟的变量表示。结合全局约束和局部约束实现网络间信息共享。设计了数据生成方法来生成带时间延迟的多变量时间序列。

Result: 在常用基准测试和生成数据集上的广泛实验表明，Shylock在少样本和正常多变量时间序列上都优于两种现有最先进方法。

Conclusion: Shylock方法有效解决了多变量时间序列因果关系发现中的挑战，特别是在数据稀缺情况下表现优异，并开发了Tcausal库便于使用。

Abstract: Causal relationship discovery has been drawing increasing attention due to
its prevalent application. Existing methods rely on human experience,
statistical methods, or graphical criteria methods which are error-prone, stuck
at the idealized assumption, and rely on a huge amount of data. And there is
also a serious data gap in accessing Multivariate time series(MTS) in many
areas, adding difficulty in finding their causal relationship. Existing methods
are easy to be over-fitting on them. To fill the gap we mentioned above, in
this paper, we propose Shylock, a novel method that can work well in both
few-shot and normal MTS to find the causal relationship. Shylock can reduce the
number of parameters exponentially by using group dilated convolution and a
sharing kernel, but still learn a better representation of variables with time
delay. By combing the global constraint and the local constraint, Shylock
achieves information sharing among networks to help improve the accuracy. To
evaluate the performance of Shylock, we also design a data generation method to
generate MTS with time delay. We evaluate it on commonly used benchmarks and
generated datasets. Extensive experiments show that Shylock outperforms two
existing state-of-art methods on both few-shot and normal MTS. We also
developed Tcausal, a library for easy use and deployed it on the EarthDataMiner
platform

</details>


### [57] [OutboundEval: A Dual-Dimensional Benchmark for Expert-Level Intelligent Outbound Evaluation of Xbench's Professional-Aligned Series](https://arxiv.org/abs/2510.21244)
*Pengyu Xu,Shijia Li,Ao Sun,Feng Zhang,Yahan Li,Bo Wu,Zhanyu Ma,Jiguo Li,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He,Rui Wang,Yang Liu,Xiaobo Hu,Fan Yang,Jia Zheng,Guanghua Yao*

Main category: cs.AI

TL;DR: 提出了OutboundEval基准测试，用于评估大语言模型在专家级智能外呼场景中的表现，解决了现有方法的三个关键限制。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在数据集多样性不足、用户模拟不真实和评估指标不准确等问题，需要开发更全面的评估框架。

Method: 设计了涵盖6个业务领域和30个子场景的结构化基准，开发了大模型驱动的用户模拟器，并引入了动态评估方法。

Result: 在12个先进LLM上的实验揭示了专家级任务完成度与交互流畅性之间的权衡，为构建可靠的外呼AI系统提供了实用见解。

Conclusion: OutboundEval为专业应用中的LLM基准测试建立了实用、可扩展和面向领域的标准。

Abstract: We propose OutboundEval, a comprehensive benchmark for evaluating large
language models (LLMs) in expert-level intelligent outbound calling scenarios.
Unlike existing methods that suffer from three key limitations - insufficient
dataset diversity and category coverage, unrealistic user simulation, and
inaccurate evaluation metrics - OutboundEval addresses these issues through a
structured framework. First, we design a benchmark spanning six major business
domains and 30 representative sub-scenarios, each with scenario-specific
process decomposition, weighted scoring, and domain-adaptive metrics. Second,
we develop a large-model-driven User Simulator that generates diverse,
persona-rich virtual users with realistic behaviors, emotional variability, and
communication styles, providing a controlled yet authentic testing environment.
Third, we introduce a dynamic evaluation method that adapts to task variations,
integrating automated and human-in-the-loop assessment to measure task
execution accuracy, professional knowledge application, adaptability, and user
experience quality. Experiments on 12 state-of-the-art LLMs reveal distinct
trade-offs between expert-level task completion and interaction fluency,
offering practical insights for building reliable, human-like outbound AI
systems. OutboundEval establishes a practical, extensible, and domain-oriented
standard for benchmarking LLMs in professional applications.

</details>


### [58] [Out-of-Distribution Detection for Safety Assurance of AI and Autonomous Systems](https://arxiv.org/abs/2510.21254)
*Victoria J. Hodge,Colin Paterson,Ibrahim Habli*

Main category: cs.AI

TL;DR: 本文综述了自主系统中OOD检测技术，分析了其在安全保证中的重要性，探讨了OOD检测的挑战、方法及其在ML开发生命周期中的应用。


<details>
  <summary>Details</summary>
Motivation: 随着AI自主系统能力的扩展，证明其安全性变得至关重要。OOD检测能够处理系统生命周期中的新颖和不确定情况，对安全关键领域的自主系统安全保证具有重要意义。

Method: 采用综合文献综述方法，首先定义相关概念，探讨OOD产生原因，分析自主系统安全保证和OOD检测的挑战因素，识别ML开发生命周期中可用的技术范围。

Result: 识别了可在ML开发生命周期中使用的OOD检测技术范围，并建议了在生命周期中支持安全保证论证的应用领域，同时指出了系统集成时需要注意的注意事项。

Conclusion: 概述了跨领域自主系统安全开发和运行所面临的挑战及未来研究方向，强调了OOD检测在安全保证中的关键作用。

Abstract: The operational capabilities and application domains of AI-enabled autonomous
systems have expanded significantly in recent years due to advances in robotics
and machine learning (ML). Demonstrating the safety of autonomous systems
rigorously is critical for their responsible adoption but it is challenging as
it requires robust methodologies that can handle novel and uncertain situations
throughout the system lifecycle, including detecting out-of-distribution (OoD)
data. Thus, OOD detection is receiving increased attention from the research,
development and safety engineering communities. This comprehensive review
analyses OOD detection techniques within the context of safety assurance for
autonomous systems, in particular in safety-critical domains. We begin by
defining the relevant concepts, investigating what causes OOD and exploring the
factors which make the safety assurance of autonomous systems and OOD detection
challenging. Our review identifies a range of techniques which can be used
throughout the ML development lifecycle and we suggest areas within the
lifecycle in which they may be used to support safety assurance arguments. We
discuss a number of caveats that system and safety engineers must be aware of
when integrating OOD detection into system lifecycles. We conclude by outlining
the challenges and future work necessary for the safe development and operation
of autonomous systems across a range of domains and applications.

</details>


### [59] [Investigating Scale Independent UCT Exploration Factor Strategies](https://arxiv.org/abs/2510.21275)
*Robin Schmöcker,Christoph Schnell,Alexander Dockhorn*

Main category: cs.AI

TL;DR: 本文提出了一种自适应选择UCT探索常数λ的策略，使算法对游戏奖励尺度不敏感，推荐使用2σ作为λ值，其中σ是搜索树中所有状态-动作对Q值的经验标准差。


<details>
  <summary>Details</summary>
Motivation: UCT算法对游戏奖励尺度敏感，在具有密集奖励的游戏中使用固定λ值会导致性能下降，需要一种对奖励尺度不敏感的自适应λ选择策略。

Method: 评估了文献中已有的λ选择策略，并提出了五种新策略，包括使用搜索树中所有状态-动作对Q值的经验标准差来设置λ值。

Result: 实验结果表明，新提出的λ=2σ策略在多种任务中都优于现有策略，无论是使用单一参数值还是优化所有可用参数时的峰值性能。

Conclusion: 推荐使用λ=2σ作为UCT算法的探索常数，该策略对游戏奖励尺度不敏感且在多种任务中表现优异。

Abstract: The Upper Confidence Bounds For Trees (UCT) algorithm is not agnostic to the
reward scale of the game it is applied to. For zero-sum games with the sparse
rewards of $\{-1,0,1\}$ at the end of the game, this is not a problem, but many
games often feature dense rewards with hand-picked reward scales, causing a
node's Q-value to span different magnitudes across different games. In this
paper, we evaluate various strategies for adaptively choosing the UCT
exploration constant $\lambda$, called $\lambda$-strategies, that are agnostic
to the game's reward scale. These $\lambda$-strategies include those proposed
in the literature as well as five new strategies. Given our experimental
results, we recommend using one of our newly suggested $\lambda$-strategies,
which is to choose $\lambda$ as $2 \cdot \sigma$ where $\sigma$ is the
empirical standard deviation of all state-action pairs' Q-values of the search
tree. This method outperforms existing $\lambda$-strategies across a wide range
of tasks both in terms of a single parameter value and the peak performances
obtained by optimizing all available parameters.

</details>


### [60] [When Models Outthink Their Safety: Mitigating Self-Jailbreak in Large Reasoning Models with Chain-of-Guardrails](https://arxiv.org/abs/2510.21285)
*Yingzhi Mao,Chunkang Zhang,Junxiang Wang,Xinyan Guan,Boxi Cao,Yaojie Lu,Hongyu Lin,Xianpei Han,Le Sun*

Main category: cs.AI

TL;DR: 提出Chain-of-Guardrail (CoG)训练框架，通过重组或回溯不安全的推理步骤，在保持有效推理链的同时引导模型回到安全轨迹，显著提升大型推理模型的安全性而不损害推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂推理任务上表现出色，但存在严重安全风险，现有缓解策略往往抑制推理能力，无法解决安全与推理的权衡问题。研究发现模型存在自我越狱现象，即模型会覆盖自身风险评估并回应不安全提示。

Method: 分析多种大型推理模型的推理轨迹，发现自我越狱现象；提出CoG训练框架，通过重组或回溯不安全推理步骤来引导模型回到安全轨迹。

Result: 在多个推理和安全基准测试上的广泛实验表明，CoG显著提升了当前大型推理模型的安全性，同时保持了相当的推理能力，明显优于先前存在严重安全-推理权衡的方法。

Conclusion: CoG框架有效解决了大型推理模型的安全与推理权衡问题，证明通过适当干预推理过程可以在不损害推理能力的前提下显著提升模型安全性。

Abstract: Large Reasoning Models (LRMs) demonstrate remarkable capabilities on complex
reasoning tasks but remain vulnerable to severe safety risks, including harmful
content generation and jailbreak attacks. Existing mitigation strategies rely
on injecting heuristic safety signals during training, which often suppress
reasoning ability and fail to resolve the safety-reasoning trade-off. To
systematically investigate this issue, we analyze the reasoning trajectories of
diverse LRMs and uncover a phenomenon we term Self-Jailbreak, where models
override their own risk assessments and justify responding to unsafe prompts.
This finding reveals that LRMs inherently possess the ability to reject unsafe
queries, but this ability is compromised, resulting in harmful outputs.
Building on these insights, we propose the Chain-of-Guardrail (CoG), a training
framework that recomposes or backtracks unsafe reasoning steps, steering the
model back onto safe trajectories while preserving valid reasoning chains.
Extensive experiments across multiple reasoning and safety benchmarks
demonstrate that CoG substantially improves the safety of current LRMs while
preserving comparable reasoning ability, significantly outperforming prior
methods that suffer from severe safety-reasoning trade-offs.

</details>


### [61] [Towards Reliable Code-as-Policies: A Neuro-Symbolic Framework for Embodied Task Planning](https://arxiv.org/abs/2510.21302)
*Sanghyun Ahn,Wonje Choi,Junyong Lee,Jinwoo Park,Honguk Woo*

Main category: cs.AI

TL;DR: 提出了一种神经符号具身任务规划框架，通过符号验证和交互验证过程增强代码生成的环境基础，在动态和部分可观察环境中显著提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的代码即策略方法在动态或部分可观察环境中存在环境基础不足的问题，导致代码生成不准确或不完整，影响任务成功率。

Method: 结合显式符号验证和交互验证过程，在验证阶段生成探索性代码主动与环境交互获取缺失观测，同时保持任务相关状态。

Result: 在RLBench和真实世界动态部分可观察场景中，任务成功率比Code-as-Policies基线提高46.2%，任务相关动作可执行性达到86.8%以上。

Conclusion: 该神经符号框架通过增强代码生成的环境基础，显著提高了动态环境中任务规划的可靠性。

Abstract: Recent advances in large language models (LLMs) have enabled the automatic
generation of executable code for task planning and control in embodied agents
such as robots, demonstrating the potential of LLM-based embodied intelligence.
However, these LLM-based code-as-policies approaches often suffer from limited
environmental grounding, particularly in dynamic or partially observable
settings, leading to suboptimal task success rates due to incorrect or
incomplete code generation. In this work, we propose a neuro-symbolic embodied
task planning framework that incorporates explicit symbolic verification and
interactive validation processes during code generation. In the validation
phase, the framework generates exploratory code that actively interacts with
the environment to acquire missing observations while preserving task-relevant
states. This integrated process enhances the grounding of generated code,
resulting in improved task reliability and success rates in complex
environments. We evaluate our framework on RLBench and in real-world settings
across dynamic, partially observable scenarios. Experimental results
demonstrate that our framework improves task success rates by 46.2% over
Code-as-Policies baselines and attains over 86.8% executability of
task-relevant actions, thereby enhancing the reliability of task planning in
dynamic environments.

</details>


### [62] [Understanding AI Trustworthiness: A Scoping Review of AIES & FAccT Articles](https://arxiv.org/abs/2510.21293)
*Siddharth Mehrotra,Jin Huang,Xuelong Fu,Roel Dobbe,Clara I. Sánchez,Maarten de Rijke*

Main category: cs.AI

TL;DR: 该范围综述分析了AIES和FAccT会议中可信AI研究的现状，发现当前研究过度关注技术属性而忽视社会技术维度，提出了结合技术严谨性与社会文化考量的跨学科方法。


<details>
  <summary>Details</summary>
Motivation: 当前可信AI研究主要采用技术中心方法，过度关注可靠性、鲁棒性和公平性等技术属性，而忽视了理解AI在现实环境中可信度所需的社会技术维度。

Method: 对AIES和FAccT会议论文集进行范围综述，系统分析可信度在不同研究领域中的定义、操作化和应用方式，重点关注概念化方法、测量方法、验证技术和应用领域。

Result: 研究发现虽然透明度、问责制和鲁棒性等技术属性的定义取得显著进展，但当前研究往往以牺牲社会伦理考量为代价强调技术精确性，AI系统的社会技术性质仍较少被探索。

Conclusion: 需要结合技术严谨性与社会、文化和制度考量的跨学科方法，建议AI伦理社区采用整体框架，真正解决AI系统与社会之间的复杂互动，促进对所有利益相关者有益的负责任技术发展。

Abstract: Background: Trustworthy AI serves as a foundational pillar for two major AI
ethics conferences: AIES and FAccT. However, current research often adopts
techno-centric approaches, focusing primarily on technical attributes such as
reliability, robustness, and fairness, while overlooking the sociotechnical
dimensions critical to understanding AI trustworthiness in real-world contexts.
  Objectives: This scoping review aims to examine how the AIES and FAccT
communities conceptualize, measure, and validate AI trustworthiness,
identifying major gaps and opportunities for advancing a holistic understanding
of trustworthy AI systems.
  Methods: We conduct a scoping review of AIES and FAccT conference proceedings
to date, systematically analyzing how trustworthiness is defined,
operationalized, and applied across different research domains. Our analysis
focuses on conceptualization approaches, measurement methods, verification and
validation techniques, application areas, and underlying values.
  Results: While significant progress has been made in defining technical
attributes such as transparency, accountability, and robustness, our findings
reveal critical gaps. Current research often predominantly emphasizes technical
precision at the expense of social and ethical considerations. The
sociotechnical nature of AI systems remains less explored and trustworthiness
emerges as a contested concept shaped by those with the power to define it.
  Conclusions: An interdisciplinary approach combining technical rigor with
social, cultural, and institutional considerations is essential for advancing
trustworthy AI. We propose actionable measures for the AI ethics community to
adopt holistic frameworks that genuinely address the complex interplay between
AI systems and society, ultimately promoting responsible technological
development that benefits all stakeholders.

</details>


### [63] [Learning Neural Control Barrier Functions from Expert Demonstrations using Inverse Constraint Learning](https://arxiv.org/abs/2510.21560)
*Yuxuan Yang,Hussein Sibai*

Main category: cs.AI

TL;DR: 该论文提出了一种使用模仿学习训练神经控制屏障函数的方法，通过专家演示来学习安全约束，避免需要明确指定失败状态集的困难。


<details>
  <summary>Details</summary>
Motivation: 在关键领域运行的自主系统中，安全是基本要求。传统方法需要明确指定失败状态集，但在许多实际场景中（如自动驾驶中的跟车问题），失败状态集难以形式化定义，而专家演示数据更容易获得。

Method: 使用模仿学习训练约束函数来分类系统状态为安全或不安全，然后利用该函数标注新的模拟轨迹来训练神经控制屏障函数。

Result: 在四个不同环境中进行实证评估，该方法优于现有基线方法，且与使用真实安全标签训练的神经控制屏障函数性能相当。

Conclusion: 该方法提供了一种数据驱动的替代方案，能够从专家演示中学习安全约束，避免了传统方法中需要明确指定失败状态集的困难。

Abstract: Safety is a fundamental requirement for autonomous systems operating in
critical domains. Control barrier functions (CBFs) have been used to design
safety filters that minimally alter nominal controls for such systems to
maintain their safety. Learning neural CBFs has been proposed as a data-driven
alternative for their computationally expensive optimization-based synthesis.
However, it is often the case that the failure set of states that should be
avoided is non-obvious or hard to specify formally, e.g., tailgating in
autonomous driving, while a set of expert demonstrations that achieve the task
and avoid the failure set is easier to generate. We use ICL to train a
constraint function that classifies the states of the system under
consideration to safe, i.e., belong to a controlled forward invariant set that
is disjoint from the unspecified failure set, and unsafe ones, i.e., belong to
the complement of that set. We then use that function to label a new set of
simulated trajectories to train our neural CBF. We empirically evaluate our
approach in four different environments, demonstrating that it outperforms
existing baselines and achieves comparable performance to a neural CBF trained
with the same data but annotated with ground-truth safety labels.

</details>


### [64] [CXRAgent: Director-Orchestrated Multi-Stage Reasoning for Chest X-Ray Interpretation](https://arxiv.org/abs/2510.21324)
*Jinhui Lou,Yan Yang,Zhou Yu,Zhenqi Fu,Weidong Han,Qingming Huang,Jun Yu*

Main category: cs.AI

TL;DR: CXRAgent是一个基于LLM的胸部X光片分析智能体，通过导演协调的多阶段框架，结合工具调用验证、诊断规划和协作决策，提升CXR诊断的适应性和可信度。


<details>
  <summary>Details</summary>
Motivation: 现有的CXR分析模型难以适应新诊断任务和复杂推理场景，现有智能体依赖单一诊断流程且缺乏工具可靠性评估机制。

Method: 提出导演协调的三阶段框架：工具调用与证据驱动验证、基于任务需求的诊断规划和专家团队组建、结合上下文记忆的协作决策。

Result: 在多种CXR解释任务上表现出色，能提供视觉证据并良好泛化到不同复杂度的临床任务。

Conclusion: CXRAgent通过多阶段协作框架显著提升了CXR诊断的适应性、可信度和泛化能力。

Abstract: Chest X-ray (CXR) plays a pivotal role in clinical diagnosis, and a variety
of task-specific and foundation models have been developed for automatic CXR
interpretation. However, these models often struggle to adapt to new diagnostic
tasks and complex reasoning scenarios. Recently, LLM-based agent models have
emerged as a promising paradigm for CXR analysis, enhancing model's capability
through tool coordination, multi-step reasoning, and team collaboration, etc.
However, existing agents often rely on a single diagnostic pipeline and lack
mechanisms for assessing tools' reliability, limiting their adaptability and
credibility. To this end, we propose CXRAgent, a director-orchestrated,
multi-stage agent for CXR interpretation, where a central director coordinates
the following stages: (1) Tool Invocation: The agent strategically orchestrates
a set of CXR-analysis tools, with outputs normalized and verified by the
Evidence-driven Validator (EDV), which grounds diagnostic outputs with visual
evidence to support reliable downstream diagnosis; (2) Diagnostic Planning:
Guided by task requirements and intermediate findings, the agent formulates a
targeted diagnostic plan. It then assembles an expert team accordingly,
defining member roles and coordinating their interactions to enable adaptive
and collaborative reasoning; (3) Collaborative Decision-making: The agent
integrates insights from the expert team with accumulated contextual memories,
synthesizing them into an evidence-backed diagnostic conclusion. Experiments on
various CXR interpretation tasks show that CXRAgent delivers strong
performance, providing visual evidence and generalizes well to clinical tasks
of different complexity. Code and data are valuable at this
\href{https://github.com/laojiahuo2003/CXRAgent/}{link}.

</details>


### [65] [Magellan: Guided MCTS for Latent Space Exploration and Novelty Generation](https://arxiv.org/abs/2510.21341)
*Lufan Chang*

Main category: cs.AI

TL;DR: Magellan框架通过蒙特卡洛树搜索和分层引导系统，解决LLM生成创新想法时陷入训练数据"重力井"的问题，显著提升科学创意的合理性和创新性。


<details>
  <summary>Details</summary>
Motivation: LLM在生成创新想法时往往局限于训练数据中的高概率概念，而现有方法如ToT依赖不可靠的自评估启发式方法，缺乏原则性指导。

Method: 使用蒙特卡洛树搜索，结合语义罗盘向量进行长程导向，以及基于景观感知的价值函数进行局部决策，平衡内在一致性、外在新颖性和叙事进展。

Result: 在广泛实验中，Magellan在生成科学想法方面显著优于ReAct和ToT等基线方法，具有更高的合理性和创新性。

Conclusion: 对于创造性发现，有原则的引导搜索比无约束的自主性更有效，为LLM成为创新合作伙伴铺平了道路。

Abstract: Large Language Models (LLMs) often struggle with generating truly innovative
ideas, typically defaulting to high-probability, familiar concepts within their
training data's "gravity wells." While advanced search-based methods like Tree
of Thoughts (ToT) attempt to mitigate this, they are fundamentally limited by
their reliance on unprincipled, inconsistent self-evaluation heuristics to
guide exploration. To address this gap, we introduce \textbf{Magellan}, a novel
framework that reframes creative generation as a principled, guided exploration
of an LLM's latent conceptual space. At its core, Magellan employs Monte Carlo
Tree Search (MCTS) governed by a hierarchical guidance system. For long-range
direction, a "semantic compass" vector, formulated via orthogonal projection,
steers the search towards relevant novelty. For local, step-by-step decisions,
a landscape-aware value function replaces flawed self-evaluation with an
explicit reward structure that balances intrinsic coherence, extrinsic novelty,
and narrative progress. Extensive experiments demonstrate that Magellan
significantly outperforms strong baselines, including ReAct and ToT, in
generating scientific ideas with superior plausibility and innovation. Our work
shows that for creative discovery, a principled, guided search is more
effective than unconstrained agency, paving the way for LLMs to become more
capable partners in innovation.

</details>


### [66] [Boosting Accuracy and Efficiency of Budget Forcing in LLMs via Reinforcement Learning for Mathematical Reasoning](https://arxiv.org/abs/2510.21398)
*Ravindra Aribowo Tarunokusumo,Rafael Fernandes Cunha*

Main category: cs.AI

TL;DR: 提出一个结合强化学习(RL)的框架，用于提高1.5B模型在数学推理中的性能，在GSM8K数据集上实现了更高准确率，同时显著减少40%以上的token使用量。


<details>
  <summary>Details</summary>
Motivation: 预算强制等测试时缩放方法虽然计算效率高，但依赖于长上下文推理轨迹的监督微调(SFT)，导致小模型因冗长响应而性能下降。

Method: 集成强化学习(RL)来提高token效率，仅使用1.5K训练样本，结合SFT+RL训练1.5B模型进行数学推理。

Result: SFT+RL模型在GSM8K数据集上表现更好，在不同计算预算下均实现更高准确率，同时token使用量比SFT模型减少40%以上。

Conclusion: RL可以恢复因长上下文训练造成的损失，整体提升数学推理性能，证明了RL在提高小模型效率方面的有效性。

Abstract: Test-time scaling methods have seen a rapid increase in popularity for its
computational efficiency and parameter-independent training to improve
reasoning performance on Large Language Models. One such method is called
budget forcing, a decoding intervention strategy which allocates extra compute
budget for thinking and elicits the inherent self-correcting behavior of the
model. However, this relies on supervised fine-tuning (SFT) on long-context
reasoning traces which causes performance degradation on smaller models due to
verbose responses. For this reason, we offer a framework integrating
reinforcement learning (RL) to improve token efficiency and boost the
performance of a 1.5B model for mathematical reasoning. We demonstrate this
using only 1.5K training samples and found that our SFT+RL model performed
better on the GSM8K dataset with varying compute budgets. Our main findings
showed an overall higher accuracy while significantly reducing its token usage
by over 40% compared to the SFT model, revealing how RL can recover the losses
due to long-context training and altogether improving performance in
mathematical reasoning.

</details>


### [67] [Advancing Symbolic Integration in Large Language Models: Beyond Conventional Neurosymbolic AI](https://arxiv.org/abs/2510.21425)
*Maneeha Rani,Bhupesh Kumar Mishra,Dhavalkumar Thakker*

Main category: cs.AI

TL;DR: 本文提出了一种新的LLM符号集成分类法，通过四个维度组织现有文献，旨在解决LLM透明度不足的问题，并为未来研究提供路线图。


<details>
  <summary>Details</summary>
Motivation: LLMs在关键领域表现出色但缺乏透明度，现有神经符号AI方法主要针对传统神经网络，不适合LLMs的独特特性，需要系统性的符号集成方法。

Method: 首先回顾已建立的神经符号AI方法，然后提出LLM符号集成的新分类法，包括四个维度：LLM各阶段的符号集成、耦合机制、架构范式、算法和应用层面视角。

Result: 开发了一个新的分类框架，系统组织了现有文献，识别了当前基准、前沿进展和关键差距，提出了未来研究的路线图。

Conclusion: 通过突出最新发展和文献中的显著差距，为将符号技术集成到LLMs以增强透明度提供了实用的实施框架见解。

Abstract: LLMs have demonstrated highly effective learning, human-like response
generation,and decision-making capabilities in high-risk sectors. However,
these models remain black boxes because they struggle to ensure transparency in
responses. The literature has explored numerous approaches to address
transparency challenges in LLMs, including Neurosymbolic AI (NeSy AI). NeSy AI
approaches were primarily developed for conventional neural networks and are
not well-suited to the unique features of LLMs. Consequently, there is a
limited systematic understanding of how symbolic AI can be effectively
integrated into LLMs. This paper aims to address this gap by first reviewing
established NeSy AI methods and then proposing a novel taxonomy of symbolic
integration in LLMs, along with a roadmap to merge symbolic techniques with
LLMs. The roadmap introduces a new categorisation framework across four
dimensions by organising existing literature within these categories. These
include symbolic integration across various stages of LLM, coupling mechanisms,
architectural paradigms, as well as algorithmic and application-level
perspectives. The paper thoroughly identifies current benchmarks, cutting-edge
advancements, and critical gaps within the field to propose a roadmap for
future research. By highlighting the latest developments and notable gaps in
the literature, it offers practical insights for implementing frameworks for
symbolic integration into LLMs to enhance transparency.

</details>


### [68] [AutoOpt: A Dataset and a Unified Framework for Automating Optimization Problem Solving](https://arxiv.org/abs/2510.21436)
*Ankur Sinha,Shobhit Arora,Dhaval Pujara*

Main category: cs.AI

TL;DR: AutoOpt-11k是一个包含11,000多个手写和打印数学优化模型图像的数据集，配套开发了AutoOpt框架，通过深度学习模型自动识别数学表达式并生成优化脚本，最终求解优化问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决数学优化问题需要专业知识且手动建模耗时的问题，开发自动化工具来直接从图像识别并求解优化问题。

Method: AutoOpt框架包含三个模块：M1使用深度学习模型进行数学表达式识别生成LaTeX代码；M2使用微调的小型LLM将LaTeX转换为PYOMO脚本；M3使用双层优化分解方法求解优化问题。

Result: M1模块在BLEU分数上优于ChatGPT、Gemini和Nougat；M3模块在复杂测试问题上比内点算法和遗传算法表现更好。

Conclusion: AutoOpt-11k数据集和AutoOpt框架为自动化解决优化问题提供了有效工具，在数学表达式识别和复杂问题求解方面表现出色。

Abstract: This study presents AutoOpt-11k, a unique image dataset of over 11,000
handwritten and printed mathematical optimization models corresponding to
single-objective, multi-objective, multi-level, and stochastic optimization
problems exhibiting various types of complexities such as non-linearity,
non-convexity, non-differentiability, discontinuity, and high-dimensionality.
The labels consist of the LaTeX representation for all the images and modeling
language representation for a subset of images. The dataset is created by 25
experts following ethical data creation guidelines and verified in two-phases
to avoid errors. Further, we develop AutoOpt framework, a machine learning
based automated approach for solving optimization problems, where the user just
needs to provide an image of the formulation and AutoOpt solves it efficiently
without any further human intervention. AutoOpt framework consists of three
Modules: (i) M1 (Image_to_Text)- a deep learning model performs the
Mathematical Expression Recognition (MER) task to generate the LaTeX code
corresponding to the optimization formulation in image; (ii) M2 (Text_to_Text)-
a small-scale fine-tuned LLM generates the PYOMO script (optimization modeling
language) from LaTeX code; (iii) M3 (Optimization)- a Bilevel Optimization
based Decomposition (BOBD) method solves the optimization formulation described
in the PYOMO script. We use AutoOpt-11k dataset for training and testing of
deep learning models employed in AutoOpt. The deep learning model for MER task
(M1) outperforms ChatGPT, Gemini and Nougat on BLEU score metric. BOBD method
(M3), which is a hybrid approach, yields better results on complex test
problems compared to common approaches, like interior-point algorithm and
genetic algorithm.

</details>


### [69] [Multi-Task Vehicle Routing Solver via Mixture of Specialized Experts under State-Decomposable MDP](https://arxiv.org/abs/2510.21453)
*Yuxin Pan,Zhiguang Cao,Chengyang Gu,Liu Liu,Peilin Zhao,Yize Chen,Fangzhen Lin*

Main category: cs.AI

TL;DR: 提出了MoSES框架，通过状态可分解MDP和潜在空间扩展，利用基础VRP变体的专用求解器来构建统一求解器，避免神经网络求解器数量指数增长。


<details>
  <summary>Details</summary>
Motivation: 现有多任务VRP神经方法通常学习统一求解器，但未能充分利用VRP变体的组合结构，错失了专用基础求解器的优势。

Method: 引入状态可分解MDP(SDMDP)将状态空间表示为基础状态空间的笛卡尔积，开发潜在空间扩展，通过最优基础策略和可学习混合函数实现策略重用。

Result: 在多个VRP变体上的广泛实验表明，MoSES优于先前方法。

Conclusion: MoSES框架通过感知VRP变体的共享组件性质并主动重用基础求解器，有效提升了多任务VRP求解性能。

Abstract: Existing neural methods for multi-task vehicle routing problems (VRPs)
typically learn unified solvers to handle multiple constraints simultaneously.
However, they often underutilize the compositional structure of VRP variants,
each derivable from a common set of basis VRP variants. This critical oversight
causes unified solvers to miss out the potential benefits of basis solvers,
each specialized for a basis VRP variant. To overcome this limitation, we
propose a framework that enables unified solvers to perceive the
shared-component nature across VRP variants by proactively reusing basis
solvers, while mitigating the exponential growth of trained neural solvers.
Specifically, we introduce a State-Decomposable MDP (SDMDP) that reformulates
VRPs by expressing the state space as the Cartesian product of basis state
spaces associated with basis VRP variants. More crucially, this formulation
inherently yields the optimal basis policy for each basis VRP variant.
Furthermore, a Latent Space-based SDMDP extension is developed by incorporating
both the optimal basis policies and a learnable mixture function to enable the
policy reuse in the latent space. Under mild assumptions, this extension
provably recovers the optimal unified policy of SDMDP through the mixture
function that computes the state embedding as a mapping from the basis state
embeddings generated by optimal basis policies. For practical implementation,
we introduce the Mixture-of-Specialized-Experts Solver (MoSES), which realizes
basis policies through specialized Low-Rank Adaptation (LoRA) experts, and
implements the mixture function via an adaptive gating mechanism. Extensive
experiments conducted across VRP variants showcase the superiority of MoSES
over prior methods.

</details>


### [70] [EU-Agent-Bench: Measuring Illegal Behavior of LLM Agents Under EU Law](https://arxiv.org/abs/2510.21524)
*Ilija Lichkovski,Alexander Müller,Mariam Ibrahim,Tiwai Mhundwa*

Main category: cs.AI

TL;DR: EU-Agent-Bench是一个可验证的人工策划基准，用于评估LLM智能体在欧盟法律框架下执行非法行为的潜在倾向，涵盖数据保护、偏见/歧视和科学诚信等多个场景类别。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体在各种环境中部署，它们可能表现出不可预测的行为，包括采取不理想和/或不安全的行动。需要衡量LLM智能体在欧盟立法背景下采取非法行为的潜在倾向。

Method: 构建一个可验证的人工策划基准，将模型的功能调用与详尽引用相关立法的评分标准进行比较，评估前沿LLM的法律合规性，并研究在系统提示中提供相关立法摘录对合规性的影响。

Result: 评估了前沿LLM的法律合规性，并研究了提供立法摘录对合规性的影响。发布了公共预览集供研究社区使用，同时保留私有测试集以防止数据污染。

Conclusion: 鼓励未来工作将智能体安全基准扩展到不同的法律管辖区，以及多轮和多语言交互。代码已公开发布。

Abstract: Large language models (LLMs) are increasingly deployed as agents in various
contexts by providing tools at their disposal. However, LLM agents can exhibit
unpredictable behaviors, including taking undesirable and/or unsafe actions. In
order to measure the latent propensity of LLM agents for taking illegal actions
under an EU legislative context, we introduce EU-Agent-Bench, a verifiable
human-curated benchmark that evaluates an agent's alignment with EU legal norms
in situations where benign user inputs could lead to unlawful actions. Our
benchmark spans scenarios across several categories, including data protection,
bias/discrimination, and scientific integrity, with each user request allowing
for both compliant and non-compliant execution of the requested actions.
Comparing the model's function calls against a rubric exhaustively supported by
citations of the relevant legislature, we evaluate the legal compliance of
frontier LLMs, and furthermore investigate the compliance effect of providing
the relevant legislative excerpts in the agent's system prompt along with
explicit instructions to comply. We release a public preview set for the
research community, while holding out a private test set to prevent data
contamination in evaluating upcoming models. We encourage future work extending
agentic safety benchmarks to different legal jurisdictions and to multi-turn
and multilingual interactions. We release our code on
\href{https://github.com/ilijalichkovski/eu-agent-bench}{this URL}.

</details>


### [71] [Co-Sight: Enhancing LLM-Based Agents via Conflict-Aware Meta-Verification and Trustworthy Reasoning with Structured Facts](https://arxiv.org/abs/2510.21557)
*Hongwei Zhang,Ji Lu,Shiqing Jiang,Chenxiang Zhu,Li Xie,Chen Zhong,Haoran Chen,Yurui Zhu,Yongsheng Du,Yanqin Gao,Lingjun Huang,Baoli Wang,Fang Tan,Peng Zou*

Main category: cs.AI

TL;DR: Co-Sight通过冲突感知元验证和可信推理结构化事实机制，将推理转化为可证伪和可审计的过程，解决了LLM智能体长程推理中的验证不足问题。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在长程推理中失败往往不是因为生成能力弱，而是由于对中间推理过程的验证不足。

Method: 采用冲突感知元验证和可信推理结构化事实两种互补机制：CAMV将验证重新定义为冲突识别和针对性证伪，TRSF通过结构化事实模块持续组织、验证和同步证据。

Result: 在GAIA上达到84.4%的准确率，Humanity's Last Exam上达到35.5%，Chinese-SimpleQA上达到93.8%，均达到最先进水平。

Conclusion: Co-Sight为LLM智能体的可靠长程推理提供了一个可扩展的范例，结构化事实基础和冲突感知验证的协同作用推动了这些改进。

Abstract: Long-horizon reasoning in LLM-based agents often fails not from generative
weakness but from insufficient verification of intermediate reasoning. Co-Sight
addresses this challenge by turning reasoning into a falsifiable and auditable
process through two complementary mechanisms: Conflict-Aware Meta-Verification
(CAMV) and Trustworthy Reasoning with Structured Facts (TRSF). CAMV
reformulates verification as conflict identification and targeted
falsification, allocating computation only to disagreement hotspots among
expert agents rather than to full reasoning chains. This bounds verification
cost to the number of inconsistencies and improves efficiency and reliability.
TRSF continuously organizes, validates, and synchronizes evidence across agents
through a structured facts module. By maintaining verified, traceable, and
auditable knowledge, it ensures that all reasoning is grounded in consistent,
source-verified information and supports transparent verification throughout
the reasoning process. Together, TRSF and CAMV form a closed verification loop,
where TRSF supplies structured facts and CAMV selectively falsifies or
reinforces them, yielding transparent and trustworthy reasoning. Empirically,
Co-Sight achieves state-of-the-art accuracy on GAIA (84.4%) and Humanity's Last
Exam (35.5%), and strong results on Chinese-SimpleQA (93.8%). Ablation studies
confirm that the synergy between structured factual grounding and
conflict-aware verification drives these improvements. Co-Sight thus offers a
scalable paradigm for reliable long-horizon reasoning in LLM-based agents. Code
is available at
https://github.com/ZTE-AICloud/Co-Sight/tree/cosight2.0_benchmarks.

</details>


### [72] [Huxley-Gödel Machine: Human-Level Coding Agent Development by an Approximation of the Optimal Self-Improving Machine](https://arxiv.org/abs/2510.21614)
*Wenyi Wang,Piotr Piękos,Li Nanbo,Firas Laakom,Yimeng Chen,Mateusz Ostaszewski,Mingchen Zhuge,Jürgen Schmidhuber*

Main category: cs.AI

TL;DR: 本文提出了Huxley-Gödel Machine (HGM)方法，通过估计代理的元生产力(CMP)来指导自我改进代码代理的开发，在SWE-bench等基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有代码代理的自我改进方法假设更高的软件工程基准性能意味着更好的后续自我改进潜力，但作者发现元生产力与性能之间存在不匹配问题。

Method: 提出CMP指标来聚合代理后代的基准性能，作为其自我改进潜力的指示器，并基于此构建HGM来搜索自我修改树。

Result: HGM在SWE-bench Verified和Polyglot上优于现有方法，且使用更少的实际时间。优化后的代理在SWE-bench Lite上达到人类水平性能。

Conclusion: HGM方法通过准确估计元生产力，有效指导代码代理的自我改进过程，实现了更好的性能和更强的迁移能力。

Abstract: Recent studies operationalize self-improvement through coding agents that
edit their own codebases. They grow a tree of self-modifications through
expansion strategies that favor higher software engineering benchmark
performance, assuming that this implies more promising subsequent
self-modifications. However, we identify a mismatch between the agent's
self-improvement potential (metaproductivity) and its coding benchmark
performance, namely the Metaproductivity-Performance Mismatch. Inspired by
Huxley's concept of clade, we propose a metric ($\mathrm{CMP}$) that aggregates
the benchmark performances of the descendants of an agent as an indicator of
its potential for self-improvement. We show that, in our self-improving coding
agent development setting, access to the true $\mathrm{CMP}$ is sufficient to
simulate how the G\"odel Machine would behave under certain assumptions. We
introduce the Huxley-G\"odel Machine (HGM), which, by estimating $\mathrm{CMP}$
and using it as guidance, searches the tree of self-modifications. On SWE-bench
Verified and Polyglot, HGM outperforms prior self-improving coding agent
development methods while using less wall-clock time. Last but not least, HGM
demonstrates strong transfer to other coding datasets and large language
models. The agent optimized by HGM on SWE-bench Verified with GPT-5-mini and
evaluated on SWE-bench Lite with GPT-5 achieves human-level performance,
matching the best officially checked results of human-engineered coding agents.
Our code is available at https://github.com/metauto-ai/HGM.

</details>


### [73] [DeepAgent: A General Reasoning Agent with Scalable Toolsets](https://arxiv.org/abs/2510.21618)
*Xiaoxi Li,Wenxiang Jiao,Jiarui Jin,Guanting Dong,Jiajie Jin,Yinuo Wang,Hao Wang,Yutao Zhu,Ji-Rong Wen,Yuan Lu,Zhicheng Dou*

Main category: cs.AI

TL;DR: DeepAgent是一个端到端的深度推理智能体，通过自主思考、工具发现和行动执行在单一推理过程中完成任务，解决了长时交互中的上下文长度爆炸和错误累积问题。


<details>
  <summary>Details</summary>
Motivation: 现有智能体框架通常遵循预定义工作流程，限制了自主性和全局任务完成能力。现实世界任务需要外部工具和长时交互，但现有方法存在上下文长度爆炸和错误累积问题。

Method: 提出DeepAgent框架，包含自主记忆折叠机制压缩过往交互为结构化记忆，以及ToolPO强化学习策略，利用LLM模拟API并通过工具调用优势归因分配细粒度信用。

Result: 在8个基准测试中，包括通用工具使用任务和下游应用，DeepAgent在标记工具和开放集工具检索场景中均优于基线方法。

Conclusion: 这项工作向更通用和强大的现实世界应用智能体迈出了一步，DeepAgent展示了在复杂任务中自主使用工具的有效性。

Abstract: Large reasoning models have demonstrated strong problem-solving abilities,
yet real-world tasks often require external tools and long-horizon
interactions. Existing agent frameworks typically follow predefined workflows,
which limit autonomous and global task completion. In this paper, we introduce
DeepAgent, an end-to-end deep reasoning agent that performs autonomous
thinking, tool discovery, and action execution within a single, coherent
reasoning process. To address the challenges of long-horizon interactions,
particularly the context length explosion from multiple tool calls and the
accumulation of interaction history, we introduce an autonomous memory folding
mechanism that compresses past interactions into structured episodic, working,
and tool memories, reducing error accumulation while preserving critical
information. To teach general-purpose tool use efficiently and stably, we
develop an end-to-end reinforcement learning strategy, namely ToolPO, that
leverages LLM-simulated APIs and applies tool-call advantage attribution to
assign fine-grained credit to the tool invocation tokens. Extensive experiments
on eight benchmarks, including general tool-use tasks (ToolBench, API-Bank,
TMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA,
HLE), demonstrate that DeepAgent consistently outperforms baselines across both
labeled-tool and open-set tool retrieval scenarios. This work takes a step
toward more general and capable agents for real-world applications. The code
and demo are available at https://github.com/RUC-NLPIR/DeepAgent.

</details>


### [74] [AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research Suite](https://arxiv.org/abs/2510.21652)
*Jonathan Bragg,Mike D'Arcy,Nishant Balepur,Dan Bareket,Bhavana Dalvi,Sergey Feldman,Dany Haddad,Jena D. Hwang,Peter Jansen,Varsha Kishore,Bodhisattwa Prasad Majumder,Aakanksha Naik,Sigal Rahamimov,Kyle Richardson,Amanpreet Singh,Harshit Surana,Aryeh Tiktinsky,Rosni Vasu,Guy Wiener,Chloe Anastasiades,Stefan Candra,Jason Dunkelberger,Dan Emery,Rob Evans,Malachi Hamada,Regan Huff,Rodney Kinney,Matt Latzke,Jaron Lochner,Ruben Lozano-Aguilera,Cecile Nguyen,Smita Rao,Amber Tanaka,Brooke Vlahos,Peter Clark,Doug Downey,Yoav Goldberg,Ashish Sabharwal,Daniel S. Weld*

Main category: cs.AI

TL;DR: 本文提出了AstaBench，一个用于评估AI科学代理能力的基准套件，包含2400多个覆盖整个科学发现过程的问题，提供可复现的评估环境和基线代理。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理评估基准存在不足：缺乏真实科学研究的整体衡量、缺少可复现的代理工具、未考虑模型成本和工具访问等混杂变量、缺乏标准化接口、缺少全面的基线代理。

Method: 定义了更严格基准测试的原则和工具，开发了AstaBench套件，包含科学搜索工具环境，提供9类科学优化代理和多个基线。

Result: 评估了57个代理和22个代理类别，发现尽管在某些方面有进展，但AI在科学研究辅助方面仍远未解决挑战。

Conclusion: AstaBench为AI科学代理提供了首个全面的评估框架，揭示了当前AI在科学研究辅助方面的局限性，为未来研究提供了基准。

Abstract: AI agents hold the potential to revolutionize scientific productivity by
automating literature reviews, replicating experiments, analyzing data, and
even proposing new directions of inquiry; indeed, there are now many such
agents, ranging from general-purpose "deep research" systems to specialized
science-specific agents, such as AI Scientist and AIGS. Rigorous evaluation of
these agents is critical for progress. Yet existing benchmarks fall short on
several fronts: they (1) fail to provide holistic, product-informed measures of
real-world use cases such as science research; (2) lack reproducible agent
tools necessary for a controlled comparison of core agentic capabilities; (3)
do not account for confounding variables such as model cost and tool access;
(4) do not provide standardized interfaces for quick agent prototyping and
evaluation; and (5) lack comprehensive baseline agents necessary to identify
true advances. In response, we define principles and tooling for more
rigorously benchmarking agents. Using these, we present AstaBench, a suite that
provides the first holistic measure of agentic ability to perform scientific
research, comprising 2400+ problems spanning the entire scientific discovery
process and multiple scientific domains, and including many problems inspired
by actual user requests to deployed Asta agents. Our suite comes with the first
scientific research environment with production-grade search tools that enable
controlled, reproducible evaluation, better accounting for confounders.
Alongside, we provide a comprehensive suite of nine science-optimized classes
of Asta agents and numerous baselines. Our extensive evaluation of 57 agents
across 22 agent classes reveals several interesting findings, most importantly
that despite meaningful progress on certain individual aspects, AI remains far
from solving the challenge of science research assistance.

</details>


### [75] [CMOMgen: Complex Multi-Ontology Alignment via Pattern-Guided In-Context Learning](https://arxiv.org/abs/2510.21656)
*Marta Contreiras Silva,Daniel Faria,Catia Pesquita*

Main category: cs.AI

TL;DR: CMOMgen是首个端到端的复杂多本体匹配策略，能够生成完整且语义合理的映射，无需限制目标本体或实体的数量。


<details>
  <summary>Details</summary>
Motivation: 简单的成对等价映射无法实现相关但不相交本体的完整语义集成，需要复杂的多本体匹配来建立更细致的等价关系和本体层次上的溯源。

Method: 使用检索增强生成方法选择相关类别组成映射，并过滤匹配参考映射作为示例来增强上下文学习。

Result: 在三个生物医学任务中，CMOMgen在类别选择方面优于基线方法，F1分数最低达到63%，在两个任务中优于所有基线和消融版本，在第三个任务中排名第二。手动评估显示46%的映射获得最高分。

Conclusion: CMOMgen能够构建语义合理的映射，在复杂多本体匹配任务中表现出色。

Abstract: Constructing comprehensive knowledge graphs requires the use of multiple
ontologies in order to fully contextualize data into a domain. Ontology
matching finds equivalences between concepts interconnecting ontologies and
creating a cohesive semantic layer. While the simple pairwise state of the art
is well established, simple equivalence mappings cannot provide full semantic
integration of related but disjoint ontologies. Complex multi-ontology matching
(CMOM) aligns one source entity to composite logical expressions of multiple
target entities, establishing more nuanced equivalences and provenance along
the ontological hierarchy.
  We present CMOMgen, the first end-to-end CMOM strategy that generates
complete and semantically sound mappings, without establishing any restrictions
on the number of target ontologies or entities. Retrieval-Augmented Generation
selects relevant classes to compose the mapping and filters matching reference
mappings to serve as examples, enhancing In-Context Learning. The strategy was
evaluated in three biomedical tasks with partial reference alignments. CMOMgen
outperforms baselines in class selection, demonstrating the impact of having a
dedicated strategy. Our strategy also achieves a minimum of 63% in F1-score,
outperforming all baselines and ablated versions in two out of three tasks and
placing second in the third. Furthermore, a manual evaluation of non-reference
mappings showed that 46% of the mappings achieve the maximum score, further
substantiating its ability to construct semantically sound mappings.

</details>


### [76] [A Multimodal Benchmark for Framing of Oil & Gas Advertising and Potential Greenwashing Detection](https://arxiv.org/abs/2510.21679)
*Gaku Morio,Harri Rowlands,Dominik Stammbach,Christopher D. Manning,Peter Henderson*

Main category: cs.AI

TL;DR: 提出了一个用于评估视觉语言模型的多模态框架分析基准数据集，包含专家标注的视频广告，涵盖13种框架类型、50多家公司和20个国家，特别关注能源行业的绿色创新框架检测。


<details>
  <summary>Details</summary>
Motivation: 公司投入大量资金进行公关活动来塑造品牌形象，但有时存在言行不一的情况（如石油公司的"漂绿"行为）。理解大规模框架及其变化有助于分析公关活动的目标和性质。

Method: 构建了一个专家标注的视频广告数据集，从Facebook和YouTube获取，专门为视觉语言模型评估设计，区别于以往的纯文本框架数据集。

Result: 基线实验显示有前景的结果但仍有改进空间：GPT-4.1能检测环境信息，F1分数达79%，而最佳模型在识别绿色创新框架方面仅达到46% F1分数。

Conclusion: 该数据集有助于能源行业战略沟通的多模态分析研究，并识别了视觉语言模型需要解决的挑战，如隐含框架、处理不同长度视频和隐含文化背景。

Abstract: Companies spend large amounts of money on public relations campaigns to
project a positive brand image. However, sometimes there is a mismatch between
what they say and what they do. Oil & gas companies, for example, are accused
of "greenwashing" with imagery of climate-friendly initiatives. Understanding
the framing, and changes in framing, at scale can help better understand the
goals and nature of public relations campaigns. To address this, we introduce a
benchmark dataset of expert-annotated video ads obtained from Facebook and
YouTube. The dataset provides annotations for 13 framing types for more than 50
companies or advocacy groups across 20 countries. Our dataset is especially
designed for the evaluation of vision-language models (VLMs), distinguishing it
from past text-only framing datasets. Baseline experiments show some promising
results, while leaving room for improvement for future work: GPT-4.1 can detect
environmental messages with 79% F1 score, while our best model only achieves
46% F1 score on identifying framing around green innovation. We also identify
challenges that VLMs must address, such as implicit framing, handling videos of
various lengths, or implicit cultural backgrounds. Our dataset contributes to
research in multimodal analysis of strategic communication in the energy
sector.

</details>


### [77] [A Knowledge-Graph Translation Layer for Mission-Aware Multi-Agent Path Planning in Spatiotemporal Dynamics](https://arxiv.org/abs/2510.21695)
*Edward Holmberg,Elias Ioup,Mahdi Abdelguerfi*

Main category: cs.AI

TL;DR: 提出了一个基于知识图谱的框架，作为智能翻译层来弥合高层任务目标与低层规划器输入之间的语义鸿沟，实现自主代理在动态环境中的协调。


<details>
  <summary>Details</summary>
Motivation: 解决自主代理在动态环境中协调时，高层任务目标与低层规划器输入之间的语义鸿沟问题。

Method: 采用双平面架构的知识图谱，将声明性事实编译为每个代理的任务感知"世界观"和物理感知遍历规则，使任务语义与领域无关的规划器解耦。

Result: 在墨西哥湾的自主水下航行器案例研究中，可视化展示了端到端过程，并定量证明不同的声明性策略能产生独特且高性能的结果。

Conclusion: 知识图谱不仅是数据存储库，更是创建自适应和可解释自主系统的强大、有状态编排器。

Abstract: The coordination of autonomous agents in dynamic environments is hampered by
the semantic gap between high-level mission objectives and low-level planner
inputs. To address this, we introduce a framework centered on a Knowledge Graph
(KG) that functions as an intelligent translation layer. The KG's two-plane
architecture compiles declarative facts into per-agent, mission-aware
``worldviews" and physics-aware traversal rules, decoupling mission semantics
from a domain-agnostic planner. This allows complex, coordinated paths to be
modified simply by changing facts in the KG. A case study involving Autonomous
Underwater Vehicles (AUVs) in the Gulf of Mexico visually demonstrates the
end-to-end process and quantitatively proves that different declarative
policies produce distinct, high-performing outcomes. This work establishes the
KG not merely as a data repository, but as a powerful, stateful orchestrator
for creating adaptive and explainable autonomous systems.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [78] [SLIM: Stochastic Learning and Inference in Overidentified Models](https://arxiv.org/abs/2510.20996)
*Xiaohong Chen,Min Seong Kim,Sokbae Lee,Myung Hwan Seo,Myunghyun Song*

Main category: econ.EM

TL;DR: SLIM是一个用于非线性GMM的可扩展随机近似框架，通过小批量矩条件及其导数进行迭代更新，无需一致初始估计量或全局凸性，支持固定样本和随机抽样渐近性。


<details>
  <summary>Details</summary>
Motivation: 解决传统GMM方法在大规模非线性模型中的计算效率问题，特别是在高维矩条件和参数情况下，传统方法收敛缓慢。

Method: 使用独立小批量矩条件及其导数形成迭代更新，产生无偏方向确保几乎必然收敛。提供二阶精化和基于随机缩放、插件方法的推断程序。

Result: 在包含576个矩条件、380个参数、n=10^5的非线性EASI需求系统蒙特卡洛实验中，SLIM在1.4小时内求解模型，而完整样本GMM需要18小时。去偏插件J检验提供满意的有限样本推断。

Conclusion: SLIM是一个高效、可扩展的非线性GMM求解框架，特别适用于大规模问题，显著提升计算效率并保持良好统计性能。

Abstract: We propose SLIM (Stochastic Learning and Inference in overidentified Models),
a scalable stochastic approximation framework for nonlinear GMM. SLIM forms
iterative updates from independent mini-batches of moments and their
derivatives, producing unbiased directions that ensure almost-sure convergence.
It requires neither a consistent initial estimator nor global convexity and
accommodates both fixed-sample and random-sampling asymptotics. We further
develop an optional second-order refinement and inference procedures based on
random scaling and plug-in methods, including plug-in, debiased plug-in, and
online versions of the Sargan--Hansen $J$-test tailored to stochastic learning.
In Monte Carlo experiments based on a nonlinear EASI demand system with 576
moment conditions, 380 parameters, and $n = 10^5$, SLIM solves the model in
under 1.4 hours, whereas full-sample GMM in Stata on a powerful laptop
converges only after 18 hours. The debiased plug-in $J$-test delivers
satisfactory finite-sample inference, and SLIM scales smoothly to $n = 10^6$.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [79] [Digital Permission Structures: How Celebrity Disclosure Enables Black Masculine Vulnerability in Online Mental Health Discourse](https://arxiv.org/abs/2510.20881)
*Anurag Shekhar*

Main category: cs.CY

TL;DR: 该研究分析说唱歌手Lil Wayne公开心理健康经历后YouTube评论，发现黑人男性社区通过数字平台展现积极的心理健康讨论，挑战了传统男性气概规范。


<details>
  <summary>Details</summary>
Motivation: 研究黑人男性面临的双重障碍：传统男性气概规范和系统性种族主义，以及名人心理健康披露在数字平台上对黑人男性社区的影响。

Method: 采用收敛混合方法分析11,306条YouTube评论，包括VADER情感分类、LDA主题建模、NRC情感词典分析和2,100条高参与度评论的反思性主题分析。

Result: 发现积极情感占主导，同伴支持达到最高饱和度，社区展现出基于Ubuntu哲学的集体男性气概，将寻求帮助视为力量。

Conclusion: 提出了数字许可结构模型，建议心理健康干预应利用集体主义、与文化名人合作、采用基于优势的信息传递，并以嘻哈真实性为中心。

Abstract: Black men face a double barrier to mental health help-seeking: traditional
masculinity norms demanding emotional restrictiveness and systemic racism
fostering institutional mistrust. While celebrity mental health disclosures
show promise for stigma reduction, limited research examines their impact on
Black masculine communities through digital platforms. This convergent
mixed-methods study analysed 11,306 YouTube comments following rapper Lil
Wayne's unprecedented disclosure of childhood suicide attempt and lifelong
mental health struggles. Quantitative analysis using VADER sentiment
classification, Latent Dirichlet Allocation topic modelling, and NRC emotion
lexicon analysis revealed predominantly positive sentiment with systematic
community amplification of mental health discourse. Reflexive thematic analysis
of 2,100 high-engagement comments identified eight themes, with peer support
achieving the highest saturation, contradicting isolation narratives. Findings
support a Digital Permission Structures Model demonstrating how intersectional
celebrity status (race + gender + high-status), hip-hop authenticity values,
and digital platform affordances create triadic authorisation mechanisms
enabling vulnerability expression. Community responses revealed communal
masculinity rooted in Ubuntu philosophy and active reconstruction of masculine
norms, positioning help-seeking as strength. Results challenge deficit-based
models of Black masculinity, suggesting interventions should leverage
collectivism, partner with high-status cultural figures, employ strength-based
messaging, and centre hip-hop authenticity rather than imposing Western
individualistic frameworks. This study provides evidence-based strategies for
culturally responsive mental health interventions addressing persistent
disparities in Black men's service utilisation.

</details>


### [80] [What do model reports say about their ChemBio benchmark evaluations? Comparing recent releases to the STREAM framework](https://arxiv.org/abs/2510.20927)
*Tom Reed,Tegan McCaslin,Luca Righetti*

Main category: cs.CY

TL;DR: 本文分析了2025年春季发布的三个前沿AI模型报告（OpenAI o3、Anthropic Claude 4、Google DeepMind Gemini 2.5 Pro），使用STREAM标准评估其化学和生物安全风险测试的报告质量，发现各报告都有改进空间，建议开发者采用更好的报告实践。


<details>
  <summary>Details</summary>
Motivation: 前沿AI开发者公开记录新AI模型的安全评估，包括化学和生物滥用风险测试，这有助于建立公众信任并促进第三方审查。但需要了解当前报告中包含或省略了哪些评估方法细节。

Method: 使用STREAM (v1)标准比较三个前沿AI模型报告（OpenAI o3、Anthropic Claude 4、Google DeepMind Gemini 2.5 Pro）的化学和生物基准评估报告质量。

Result: 每个模型报告都包含其他报告没有的有用细节，所有报告都有需要改进的地方。发现多个报告项目在所有模型报告中都发展不足，如提供测试材料示例和详细的激发条件列表。

Conclusion: 建议AI开发者继续加强评估科学，在目前报告仍有限的领域提高透明度，采用彼此的最佳报告实践。

Abstract: Most frontier AI developers publicly document their safety evaluations of new
AI models in model reports, including testing for chemical and biological
(ChemBio) misuse risks. This practice provides a window into the methodology of
these evaluations, helping to build public trust in AI systems, and enabling
third party review in the still-emerging science of AI evaluation. But what
aspects of evaluation methodology do developers currently include -- or omit --
in their reports? This paper examines three frontier AI model reports published
in spring 2025 with among the most detailed documentation: OpenAI's o3,
Anthropic's Claude 4, and Google DeepMind's Gemini 2.5 Pro. We compare these
using the STREAM (v1) standard for reporting ChemBio benchmark evaluations.
Each model report included some useful details that the others did not, and all
model reports were found to have areas for development, suggesting that
developers could benefit from adopting one another's best reporting practices.
We identified several items where reporting was less well-developed across all
model reports, such as providing examples of test material, and including a
detailed list of elicitation conditions. Overall, we recommend that AI
developers continue to strengthen the emerging science of evaluation by working
towards greater transparency in areas where reporting currently remains
limited.

</details>


### [81] [Soppia: A Structured Prompting Framework for the Proportional Assessment of Non-Pecuniary Damages in Personal Injury Cases](https://arxiv.org/abs/2510.21082)
*Jorge Alberto Araujo*

Main category: cs.CY

TL;DR: Soppia是一个结构化提示框架，利用AI帮助法律专业人士处理复杂的多标准法律规则，特别是在人身伤害案件中量化非金钱损害赔偿方面。


<details>
  <summary>Details</summary>
Motivation: 解决司法决策中复杂法律规则应用的挑战，确保立法意图的一致实现，特别是在非金钱损害赔偿量化这一复杂领域。

Method: 开发Soppia（有序比例加权智能评估系统），将细微的法律指令转化为实用、可复制且透明的方法论，以巴西CLT第223-G条中的12个非金钱损害赔偿标准为案例研究。

Result: 该框架能够全面平衡地分析所有规定标准，增强一致性和可预测性，同时提供可适应多标准法律背景的通用且可解释的工具。

Conclusion: Soppia在规范性解释和计算推理之间架起桥梁，为实现可审计的法律AI提供了有效途径。

Abstract: Applying complex legal rules characterized by multiple, heterogeneously
weighted criteria presents a fundamental challenge in judicial decision-making,
often hindering the consistent realization of legislative intent. This
challenge is particularly evident in the quantification of non-pecuniary
damages in personal injury cases. This paper introduces Soppia, a structured
prompting framework designed to assist legal professionals in navigating this
complexity. By leveraging advanced AI, the system ensures a comprehensive and
balanced analysis of all stipulated criteria, fulfilling the legislator's
intent that compensation be determined through a holistic assessment of each
case. Using the twelve criteria for non-pecuniary damages established in the
Brazilian CLT (Art. 223-G) as a case study, we demonstrate how Soppia (System
for Ordered Proportional and Pondered Intelligent Assessment) operationalizes
nuanced legal commands into a practical, replicable, and transparent
methodology. The framework enhances consistency and predictability while
providing a versatile and explainable tool adaptable across multi-criteria
legal contexts, bridging normative interpretation and computational reasoning
toward auditable legal AI.

</details>


### [82] [The Nuclear Analogy in AI Governance Research](https://arxiv.org/abs/2510.21203)
*Sophia Hatz*

Main category: cs.CY

TL;DR: 本文回顾了43篇将人工智能与核武器类比的研究，识别了四个核先例应用于AI治理的问题领域，并论证历史类比在政策制定中的价值。


<details>
  <summary>Details</summary>
Motivation: AI与核武器的类比在学术和政策讨论中很常见，但需要系统评估这种类比如何为AI治理提供实际指导。

Method: 系统回顾了43篇明确引用核领域经验来指导AI治理的学术著作，识别应用领域并分析类比的价值。

Result: 识别出四个核先例应用领域：早期技术发展与治理、国际安全风险与战略、国际制度与协议、国内安全监管。发现历史类比能提供概念框架、警示教训和政策想象力。

Conclusion: 尽管技术领域差异显著，但核类比能为AI治理提供有价值的政策见解，政策制定者应继续批判性地借鉴这些历史先例。

Abstract: The analogy between Artificial Intelligence (AI) and nuclear weapons is
prominent in academic and policy discourse on AI governance. This chapter
reviews 43 scholarly works which explicitly draw on the nuclear domain to
derive lessons for AI governance. We identify four problem areas where
researchers apply nuclear precedents: (1) early development and governance of
transformative technologies; (2) international security risks and strategy; (3)
international institutions and agreements; and (4) domestic safety regulation.
While nuclear-inspired AI proposals are often criticised due to differences
across domains, this review clarifies how historical analogies can inform
policy development even when technological domains differ substantially.
Valuable functions include providing conceptual frameworks for analyzing
strategic dynamics, offering cautionary lessons about unsuccessful governance
approaches, and expanding policy imagination by legitimizing radical proposals.
Given that policymakers already invoke the nuclear analogy, continued critical
engagement with these historical precedents remains essential for shaping
effective global AI governance.

</details>


### [83] [World Models Should Prioritize the Unification of Physical and Social Dynamics](https://arxiv.org/abs/2510.21219)
*Xiaoyuan Zhang,Chengdong Ma,Yizhe Huang,Weidong Huang,Siyuan Qi,Song-Chun Zhu,Xue Feng,Yaodong Yang*

Main category: cs.CY

TL;DR: 这篇立场论文主张将物理预测能力与社会预测能力双向统一是下一代世界模型发展的关键前沿，提出了ACE原则和概念框架来实现真正全面的世界模型。


<details>
  <summary>Details</summary>
Motivation: 当前世界模型在物理动态和社会行为预测方面各自独立发展，无法建模物理环境与社会结构之间的关键相互作用，限制了AI处理真实世界复杂系统的能力。

Method: 分析整合的核心障碍，提出ACE指导原则（可适应性、情境性、涌现性），并构建概念框架和研究路线图。

Result: 提出了实现物理与社会能力系统统一的理论基础和实现路径，为开发更全面的世界模型指明了方向。

Conclusion: 物理预测能力与社会预测能力的双向统一对于AI稳健应对复杂现实挑战和实现更通用智能至关重要，是下一代世界模型发展的关键方向。

Abstract: World models, which explicitly learn environmental dynamics to lay the
foundation for planning, reasoning, and decision-making, are rapidly advancing
in predicting both physical dynamics and aspects of social behavior, yet
predominantly in separate silos. This division results in a systemic failure to
model the crucial interplay between physical environments and social
constructs, rendering current models fundamentally incapable of adequately
addressing the true complexity of real-world systems where physical and social
realities are inextricably intertwined. This position paper argues that the
systematic, bidirectional unification of physical and social predictive
capabilities is the next crucial frontier for world model development. We
contend that comprehensive world models must holistically integrate objective
physical laws with the subjective, evolving, and context-dependent nature of
social dynamics. Such unification is paramount for AI to robustly navigate
complex real-world challenges and achieve more generalizable intelligence. This
paper substantiates this imperative by analyzing core impediments to
integration, proposing foundational guiding principles (ACE Principles), and
outlining a conceptual framework alongside a research roadmap towards truly
holistic world models.

</details>


### [84] [Recommended Practices for NPOV Research on Wikipedia](https://arxiv.org/abs/2510.21526)
*Isaac Johnson,Yu-Ming Liou,Jacob Rogers,Aaron Shaw,Leila Zia*

Main category: cs.CY

TL;DR: 本文旨在加速维基百科中立性研究，通过帮助研究者理解维基百科中立观点的含义、识别研究挑战并提供指导，以提升研究对维基百科的实际影响。


<details>
  <summary>Details</summary>
Motivation: 维基百科的中立观点是其核心原则之一，但相关研究相对较少，部分原因是研究社区对中立性定义和重要性理解不足，且中立性本身是一个具有挑战性和争议性的概念。

Method: 通过解释维基百科中立观点的含义，识别研究中的常见挑战并提供应对方法，以及指导研究者如何有效沟通研究成果以提升实际影响力。

Result: 为研究者提供了理解维基百科中立性的框架、研究挑战的应对策略以及研究成果转化的指导。

Conclusion: 本文旨在促进高质量的中立性研究，帮助维基百科社区持续改进百科全书编写工作，通过提升研究的实际影响力来支持维基百科的发展。

Abstract: Writing Wikipedia with a neutral point of view is one of the five pillars of
Wikipedia. Although the topic is core to Wikipedia, it is relatively
understudied considering hundreds of research studies are published annually
about the project. We hypothesize that part of the reason for the low research
activity on the topic is that Wikipedia's definition of neutrality and its
importance are not well understood within the research community. Neutrality is
also an inherently challenging and contested concept. Our aim with this paper
is to accelerate high quality research in this space that can help Wikipedia
communities continue to improve their work in writing the encyclopedia. We do
this by helping researchers to learn what Neutral Point of View means in the
context of Wikipedia, identifying some common challenges with studying NPOV and
how to navigate them, and offering guidance on how researchers can communicate
the results of their work for increased impact on the ground for the benefit of
Wikipedia.

</details>

<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 28]
- [econ.EM](#econ.EM) [Total: 1]
- [cs.SI](#cs.SI) [Total: 2]
- [stat.AP](#stat.AP) [Total: 4]
- [cs.CY](#cs.CY) [Total: 4]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Jackpot: Optimal Budgeted Rejection Sampling for Extreme Actor-Policy Mismatch Reinforcement Learning](https://arxiv.org/abs/2602.06107)
*Zhuoming Chen,Hongyi Liu,Yang Zhou,Haizhong Zheng,Beidi Chen*

Main category: cs.AI

TL;DR: Jackpot框架通过最优预算拒绝采样减少rollout模型与策略分布不匹配，实现RL训练中rollout生成与策略优化的解耦，提高训练稳定性与效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的强化学习训练成本高昂，主要因为rollout生成昂贵。将rollout生成与策略优化解耦（如使用更高效模型进行rollout）能显著提升效率，但这会引入严重的分布不匹配问题，导致学习不稳定。

Method: 提出Jackpot框架，采用最优预算拒绝采样直接减少rollout模型与演化策略之间的差异。包含原则性的OBRS程序、联合更新策略和rollout模型的统一训练目标，以及通过top-k概率估计和批次级偏差校正实现的高效系统实现。

Result: 理论分析表明OBRS在可控接受预算下始终将rollout分布向目标分布移动。实证显示相比重要性采样基线，Jackpot显著提升训练稳定性，在Qwen3-8B-Base模型上训练300步（批次大小64）时达到与on-policy RL相当的性能。

Conclusion: 基于OBRS的对齐方法使RL for LLMs中rollout生成与策略优化的解耦更接近实用和有效，为解决分布不匹配问题提供了有前景的解决方案。

Abstract: Reinforcement learning (RL) for large language models (LLMs) remains expensive, particularly because the rollout is expensive. Decoupling rollout generation from policy optimization (e.g., leveraging a more efficient model to rollout) could enable substantial efficiency gains, yet doing so introduces a severe distribution mismatch that destabilizes learning. We propose Jackpot, a framework that leverages Optimal Budget Rejection Sampling (OBRS) to directly reduce the discrepancy between the rollout model and the evolving policy. Jackpot integrates a principled OBRS procedure, a unified training objective that jointly updates the policy and rollout models, and an efficient system implementation enabled by top-$k$ probability estimation and batch-level bias correction. Our theoretical analysis shows that OBRS consistently moves the rollout distribution closer to the target distribution under a controllable acceptance budget. Empirically, \sys substantially improves training stability compared to importance-sampling baselines, achieving performance comparable to on-policy RL when training Qwen3-8B-Base for up to 300 update steps of batchsize 64. Taken together, our results show that OBRS-based alignment brings us a step closer to practical and effective decoupling of rollout generation from policy optimization for RL for LLMs.

</details>


### [2] [Large Language Model Reasoning Failures](https://arxiv.org/abs/2602.06176)
*Peiyang Song,Pengrui Han,Noah Goodman*

Main category: cs.AI

TL;DR: 这篇论文首次对LLM推理失败进行了全面调查，提出了双重分类框架：将推理分为具身与非具身类型，并将推理失败分为基础性、应用特定性和鲁棒性问题三类。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在推理能力上取得了显著进展，但在看似简单的场景中仍然存在严重的推理失败。为了系统性地理解和解决这些缺陷，需要对这些失败进行全面的调查和分类。

Method: 提出了一个新颖的分类框架：1) 将推理分为具身推理和非具身推理，后者进一步细分为非正式（直觉）推理和正式（逻辑）推理；2) 将推理失败分为三类：基础性失败（LLM架构固有）、应用特定性限制（特定领域表现）、鲁棒性问题（微小变化导致不一致表现）。对每种失败提供定义、分析现有研究、探索根本原因并提出缓解策略。

Result: 创建了首个全面的LLM推理失败调查，提供了结构化视角来理解LLM的系统性弱点，并发布了GitHub资源库（https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures）收集相关研究工作。

Conclusion: 通过统一碎片化的研究努力，该调查为理解LLM推理的系统性弱点提供了有价值的见解，指导未来研究构建更强、更可靠、更鲁棒的推理能力，并为该领域提供了易于入门的资源。

Abstract: Large Language Models (LLMs) have exhibited remarkable reasoning capabilities, achieving impressive results across a wide range of tasks. Despite these advances, significant reasoning failures persist, occurring even in seemingly simple scenarios. To systematically understand and address these shortcomings, we present the first comprehensive survey dedicated to reasoning failures in LLMs. We introduce a novel categorization framework that distinguishes reasoning into embodied and non-embodied types, with the latter further subdivided into informal (intuitive) and formal (logical) reasoning. In parallel, we classify reasoning failures along a complementary axis into three types: fundamental failures intrinsic to LLM architectures that broadly affect downstream tasks; application-specific limitations that manifest in particular domains; and robustness issues characterized by inconsistent performance across minor variations. For each reasoning failure, we provide a clear definition, analyze existing studies, explore root causes, and present mitigation strategies. By unifying fragmented research efforts, our survey provides a structured perspective on systemic weaknesses in LLM reasoning, offering valuable insights and guiding future research towards building stronger, more reliable, and robust reasoning capabilities. We additionally release a comprehensive collection of research works on LLM reasoning failures, as a GitHub repository at https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures, to provide an easy entry point to this area.

</details>


### [3] [Do It for HER: First-Order Temporal Logic Reward Specification in Reinforcement Learning (Extended Version)](https://arxiv.org/abs/2602.06227)
*Pierriccardo Olivieri,Fausto Lasca,Alessandro Gianola,Matteo Papini*

Main category: cs.AI

TL;DR: 提出基于LTLfMT逻辑框架的非马尔可夫奖励规范方法，结合奖励机器和HER解决无限状态空间下的复杂任务规划问题


<details>
  <summary>Details</summary>
Motivation: 传统马尔可夫决策过程在处理大规模状态空间和非马尔可夫奖励时表达能力有限，需要手动编码谓词，缺乏统一的逻辑规范框架

Method: 使用LTLfMT（线性时序逻辑模理论）扩展表达能力，提出可处理片段，结合奖励机器和事后经验回放（HER）解决奖励稀疏性问题

Result: 在连续控制环境中验证了方法的有效性，定制化的HER实现对于解决复杂目标任务至关重要

Conclusion: LTLfMT框架为无限状态空间下的复杂任务提供了自然的规范方法，结合HER能够有效解决奖励稀疏性问题

Abstract: In this work, we propose a novel framework for the logical specification of non-Markovian rewards in Markov Decision Processes (MDPs) with large state spaces. Our approach leverages Linear Temporal Logic Modulo Theories over finite traces (LTLfMT), a more expressive extension of classical temporal logic in which predicates are first-order formulas of arbitrary first-order theories rather than simple Boolean variables. This enhanced expressiveness enables the specification of complex tasks over unstructured and heterogeneous data domains, promoting a unified and reusable framework that eliminates the need for manual predicate encoding. However, the increased expressive power of LTLfMT introduces additional theoretical and computational challenges compared to standard LTLf specifications. We address these challenges from a theoretical standpoint, identifying a fragment of LTLfMT that is tractable but sufficiently expressive for reward specification in an infinite-state-space context. From a practical perspective, we introduce a method based on reward machines and Hindsight Experience Replay (HER) to translate first-order logic specifications and address reward sparsity. We evaluate this approach to a continuous-control setting using Non-Linear Arithmetic Theory, showing that it enables natural specification of complex tasks. Experimental results show how a tailored implementation of HER is fundamental in solving tasks with complex goals.

</details>


### [4] [Do LLMs Act Like Rational Agents? Measuring Belief Coherence in Probabilistic Decision Making](https://arxiv.org/abs/2602.06286)
*Khurram Yamin,Jingjing Tang,Santiago Cortes-Gomez,Amit Sharma,Eric Horvitz,Bryan Wilder*

Main category: cs.AI

TL;DR: 研究LLM在医疗诊断等高风险决策中是否作为理性效用最大化者，提出可证伪条件检验LLM报告概率与真实信念的一致性


<details>
  <summary>Details</summary>
Motivation: LLM越来越多地部署在高风险领域作为智能体，其决策逻辑难以解释，需要研究LLM是否具有理性信念和稳定偏好的效用最大化者

Method: 通过诊断挑战问题研究模型行为，建立可证伪条件检验报告概率是否对应任何理性智能体的真实信念，在多个医疗诊断领域评估多个LLM

Result: 提供了关于LLM推理与理想贝叶斯效用最大化之间关系的见解，发现了报告概率无法对应任何理性智能体真实信念的条件

Conclusion: 研究结果对LLM在高风险决策中的应用具有重要启示，指出了未来发展方向

Abstract: Large language models (LLMs) are increasingly deployed as agents in high-stakes domains where optimal actions depend on both uncertainty about the world and consideration of utilities of different outcomes, yet their decision logic remains difficult to interpret. We study whether LLMs are rational utility maximizers with coherent beliefs and stable preferences. We consider behaviors of models for diagnosis challenge problems. The results provide insights about the relationship of LLM inferences to ideal Bayesian utility maximization for elicited probabilities and observed actions. Our approach provides falsifiable conditions under which the reported probabilities \emph{cannot} correspond to the true beliefs of any rational agent. We apply this methodology to multiple medical diagnostic domains with evaluations across several LLMs. We discuss implications of the results and directions forward for uses of LLMs in guiding high-stakes decisions.

</details>


### [5] [Exposing Weaknesses of Large Reasoning Models through Graph Algorithm Problems](https://arxiv.org/abs/2602.06319)
*Qifan Zhang,Jianhao Ruan,Aochuan Chen,Kang Zeng,Nuo Chen,Jing Tang,Jia Li*

Main category: cs.AI

TL;DR: GrAlgoBench是一个基于图算法问题的基准测试，用于评估大型推理模型，揭示了模型在长上下文推理中的准确率下降和过度思考现象。


<details>
  <summary>Details</summary>
Motivation: 现有数学、代码和常识推理基准存在局限性：缺乏长上下文评估、挑战性不足、答案难以程序化验证。需要更严谨的测试平台来评估大型推理模型的推理能力。

Method: 引入GrAlgoBench基准测试，包含九个图算法任务。图算法问题特别适合评估推理能力：需要长上下文推理、难度可精细控制、支持标准化程序化评估。

Result: 实验发现当前大型推理模型存在两大弱点：1) 随着上下文长度增加，准确率急剧下降（图节点超过120个时准确率低于50%），主要由执行错误、弱记忆和冗余推理导致；2) 存在过度思考现象，主要是大量无效的自验证导致推理轨迹膨胀但不提高正确性。

Conclusion: GrAlgoBench通过暴露大型推理模型的局限性，确立了图算法问题作为严谨、多维且实际相关的测试平台，有助于推进大型推理模型推理能力的研究。

Abstract: Large Reasoning Models (LRMs) have advanced rapidly; however, existing benchmarks in mathematics, code, and common-sense reasoning remain limited. They lack long-context evaluation, offer insufficient challenge, and provide answers that are difficult to verify programmatically. We introduce GrAlgoBench, a benchmark designed to evaluate LRMs through graph algorithm problems. Such problems are particularly well suited for probing reasoning abilities: they demand long-context reasoning, allow fine-grained control of difficulty levels, and enable standardized, programmatic evaluation. Across nine tasks, our systematic experiments reveal two major weaknesses of current LRMs. First, accuracy deteriorates sharply as context length increases, falling below 50% once graphs exceed 120 nodes. This degradation is driven by frequent execution errors, weak memory, and redundant reasoning. Second, LRMs suffer from an over-thinking phenomenon, primarily caused by extensive yet largely ineffective self-verification, which inflates reasoning traces without improving correctness. By exposing these limitations, GrAlgoBench establishes graph algorithm problems as a rigorous, multidimensional, and practically relevant testbed for advancing the study of reasoning in LRMs. Code is available at https://github.com/Bklight999/GrAlgoBench.

</details>


### [6] [Trifuse: Enhancing Attention-Based GUI Grounding via Multimodal Fusion](https://arxiv.org/abs/2602.06351)
*Longhui Ma,Di Zhao,Siwei Wang,Zhao Lv,Miao Wang*

Main category: cs.AI

TL;DR: Trifuse是一个基于注意力的GUI grounding框架，通过整合注意力机制、OCR文本线索和图标级语义，无需任务特定微调即可实现GUI元素定位，显著减少对标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有GUI grounding方法主要依赖微调多模态大语言模型预测坐标，需要大量数据且泛化性差；而基于注意力的方法缺乏明确的空间锚点，可靠性低。需要一种无需大量标注数据且能有效利用空间信息的解决方案。

Method: 提出Trifuse框架，显式整合互补的空间锚点：1) MLLMs的注意力机制；2) OCR提取的文本线索；3) 图标级语义描述。采用Consensus-SinglePeak融合策略，强制跨模态一致性同时保持尖锐的定位峰值。

Result: 在四个grounding基准测试上表现优异，无需任务特定微调即可实现强性能，显著减少对昂贵标注数据的依赖。消融研究表明，整合OCR和语义线索能持续提升不同骨干网络的性能。

Conclusion: Trifuse通过显式整合互补空间锚点，解决了现有基于注意力方法缺乏可靠空间信息的问题，为GUI grounding提供了一个无需大量标注数据的通用框架，在多个基准测试中验证了其有效性。

Abstract: GUI grounding maps natural language instructions to the correct interface elements, serving as the perception foundation for GUI agents. Existing approaches predominantly rely on fine-tuning multimodal large language models (MLLMs) using large-scale GUI datasets to predict target element coordinates, which is data-intensive and generalizes poorly to unseen interfaces. Recent attention-based alternatives exploit localization signals in MLLMs attention mechanisms without task-specific fine-tuning, but suffer from low reliability due to the lack of explicit and complementary spatial anchors in GUI images. To address this limitation, we propose Trifuse, an attention-based grounding framework that explicitly integrates complementary spatial anchors. Trifuse integrates attention, OCR-derived textual cues, and icon-level caption semantics via a Consensus-SinglePeak (CS) fusion strategy that enforces cross-modal agreement while retaining sharp localization peaks. Extensive evaluations on four grounding benchmarks demonstrate that Trifuse achieves strong performance without task-specific fine-tuning, substantially reducing the reliance on expensive annotated data. Moreover, ablation studies reveal that incorporating OCR and caption cues consistently improves attention-based grounding performance across different backbones, highlighting its effectiveness as a general framework for GUI grounding.

</details>


### [7] [Difficulty-Estimated Policy Optimization](https://arxiv.org/abs/2602.06375)
*Yu Zhao,Fan Jiang,Tianle Liu,Bo Zeng,Yu Liu,Longyue Wang,Weihua Luo*

Main category: cs.AI

TL;DR: DEPO提出了一种基于难度估计的策略优化框架，通过在线难度评估器动态筛选训练数据，优先处理高学习潜力的样本，在保持模型性能的同时将rollout成本降低2倍。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO方法在问题过于简单或过于复杂时会出现梯度信号衰减问题，而DAPO等变体虽然解决了梯度消失问题，但无法缓解在低效用样本上进行大量rollout带来的巨大计算开销。

Method: 提出Difficulty-Estimated Policy Optimization (DEPO)框架，集成在线难度评估器，在rollout阶段前动态评估和筛选训练数据，优先分配计算资源给具有高学习潜力的样本。

Result: 实证结果显示DEPO在保持模型性能的同时，将rollout成本降低高达2倍，显著降低了训练高性能推理模型的计算门槛。

Conclusion: DEPO为推理模型训练提供了一种更高效、更可持续的路径，通过智能资源分配优化推理对齐的效率和鲁棒性。

Abstract: Recent advancements in Large Reasoning Models (LRMs), exemplified by DeepSeek-R1, have underscored the potential of scaling inference-time compute through Group Relative Policy Optimization (GRPO). However, GRPO frequently suffers from gradient signal attenuation when encountering problems that are either too trivial or overly complex. In these scenarios, the disappearance of inter-group advantages makes the gradient signal susceptible to noise, thereby jeopardizing convergence stability. While variants like DAPO attempt to rectify gradient vanishing, they do not alleviate the substantial computational overhead incurred by exhaustive rollouts on low-utility samples. In this paper, we propose Difficulty-Estimated Policy Optimization (DEPO), a novel framework designed to optimize the efficiency and robustness of reasoning alignment. DEPO integrates an online Difficulty Estimator that dynamically assesses and filters training data before the rollout phase. This mechanism ensures that computational resources are prioritized for samples with high learning potential. Empirical results demonstrate that DEPO achieves up to a 2x reduction in rollout costs without compromising model performance. Our approach significantly lowers the computational barrier for training high-performance reasoning models, offering a more sustainable path for reasoning scaling. Code and data will be released upon acceptance.

</details>


### [8] [Unlocking Noisy Real-World Corpora for Foundation Model Pre-Training via Quality-Aware Tokenization](https://arxiv.org/abs/2602.06394)
*Arvid E. Gollwitzer,Paridhi Latawa,David de Gruijl,Deepak A. Subramanian,Adrián Noriega de la Colina*

Main category: cs.AI

TL;DR: QA-Token是一种质量感知的分词方法，通过双层优化、强化学习和自适应参数学习，将数据可靠性直接融入词汇表构建，在基因组学和金融等领域显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前的分词方法处理序列数据时未考虑信号质量，限制了在真实世界嘈杂语料库上的有效性。需要一种能够直接融入数据可靠性的分词方法。

Method: 提出QA-Token方法，包含三个关键贡献：(1) 双层优化公式联合优化词汇表构建和下游性能；(2) 通过质量感知奖励学习合并策略的强化学习方法，具有收敛保证；(3) 通过Gumbel-Softmax松弛实现端到端优化的自适应参数学习机制。

Result: 实验评估显示一致改进：基因组学（变异检测F1分数比BPE提高6.7个百分点）、金融（夏普比率提高30%）。在基础模型规模上，对1.7万亿碱基对的预训练语料进行分词，实现最先进的病原体检测（94.53 MCC），同时减少15%的token数量。

Conclusion: QA-Token解锁了嘈杂的真实世界语料库（包括PB级基因组序列和TB级金融时间序列），用于基础模型训练，且无推理开销。

Abstract: Current tokenization methods process sequential data without accounting for signal quality, limiting their effectiveness on noisy real-world corpora. We present QA-Token (Quality-Aware Tokenization), which incorporates data reliability directly into vocabulary construction. We make three key contributions: (i) a bilevel optimization formulation that jointly optimizes vocabulary construction and downstream performance, (ii) a reinforcement learning approach that learns merge policies through quality-aware rewards with convergence guarantees, and (iii) an adaptive parameter learning mechanism via Gumbel-Softmax relaxation for end-to-end optimization. Our experimental evaluation demonstrates consistent improvements: genomics (6.7 percentage point F1 gain in variant calling over BPE), finance (30% Sharpe ratio improvement). At foundation scale, we tokenize a pretraining corpus comprising 1.7 trillion base-pairs and achieve state-of-the-art pathogen detection (94.53 MCC) while reducing token count by 15%. We unlock noisy real-world corpora, spanning petabases of genomic sequences and terabytes of financial time series, for foundation model training with zero inference overhead.

</details>


### [9] [Intrinsic Stability Limits of Autoregressive Reasoning: Structural Consequences for Long-Horizon Execution](https://arxiv.org/abs/2602.06413)
*Hsien-Jyh Liao*

Main category: cs.AI

TL;DR: 论文提出自回归生成存在内在稳定性极限，导致长程推理性能急剧下降，而非仅因任务复杂度。理论证明单路径自回归推理的决策优势随执行长度指数衰减，稳定长程推理需要离散分段，自然诱导图状执行结构。


<details>
  <summary>Details</summary>
Motivation: 传统解释将大语言模型在长程任务中的性能下降归因于任务复杂度，但作者认为即使在无分支、无语义模糊的线性任务中，自回归执行也存在内在稳定性极限，需要更根本的解释。

Method: 提出理论框架，推导定理A证明单路径自回归推理的决策优势随执行长度指数衰减；通过合成环境和真实TextWorld任务的实证研究验证理论预测；将长程推理重构为结构治理问题。

Result: 实证研究显示与理论预测一致的性能悬崖现象；发现短程评估协议可能掩盖结构不稳定性；证明稳定长程推理需要离散分段，自然诱导有向无环图等图状执行结构。

Conclusion: 长程推理失败源于自回归生成的过程级不稳定性而非仅搜索或任务复杂度；未来推理系统可能需要从单纯扩展转向结构化治理；纯自回归架构在维持长期连贯性方面存在根本限制。

Abstract: Large language models (LLMs) demonstrate remarkable reasoning capabilities, yet their performance often deteriorates sharply in long-horizon tasks, exhibiting systematic breakdown beyond certain scales. Conventional explanations primarily attribute this phenomenon to task complexity, such as combinatorial search explosion or long-term credit assignment challenges. In this work, we argue that these explanations are incomplete: even in linear, unbranched tasks without semantic ambiguity, autoregressive execution is subject to an intrinsic stability limit.
  We propose that the fundamental constraint on long-horizon reasoning arises from process-level instability in autoregressive generation rather than solely from search or task complexity, reframing long-horizon reasoning as a problem of structural governance. We derive Theorem~A, showing that decision advantage in single-path autoregressive reasoning decays exponentially with execution length, imposing a fundamental bound on maintainable reasoning chains. This result implies a structural consequence: stable long-horizon reasoning requires discrete segmentation, naturally inducing graph-like execution structures such as directed acyclic graphs (DAGs).
  Empirical studies in both synthetic environments and real TextWorld tasks reveal observable performance cliffs consistent with theoretical predictions. Our findings provide a dynamical perspective on long-horizon reasoning failure and suggest new limitations on maintaining long-term coherence under purely autoregressive architectures. Furthermore, we highlight that short-horizon evaluation protocols may obscure structural instability, indicating a potential shift from scaling toward structured governance in future reasoning systems.

</details>


### [10] [AgentCPM-Explore: Realizing Long-Horizon Deep Exploration for Edge-Scale Agents](https://arxiv.org/abs/2602.06485)
*Haotian Chen,Xin Cong,Shengda Fan,Yuyang Fu,Ziqin Gong,Yaxi Lu,Yishan Li,Boye Niu,Chengjun Pan,Zijun Song,Huadong Wang,Yesai Wu,Yueying Wu,Zihao Xie,Yukun Yan,Zhong Zhang,Yankai Lin,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

TL;DR: 本文提出了AgentCPM-Explore，一个4B参数的边缘规模智能体模型，通过参数空间模型融合、奖励信号去噪和上下文信息精炼等创新方法，在多个基准测试中超越了更大规模模型的表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体系统过度依赖大规模模型，而边缘规模模型（4B参数级别）的潜力尚未被充分探索。本文旨在系统研究训练4B参数规模智能体模型的方法，解决阻碍边缘规模模型性能的三个主要瓶颈。

Method: 提出了AgentCPM-Explore模型和整体训练框架：1）参数空间模型融合解决SFT中的灾难性遗忘问题；2）奖励信号去噪解决RL训练中对噪声信号的敏感性；3）上下文信息精炼解决长上下文场景中的推理退化问题。

Result: AgentCPM-Explore在4B类模型中达到SOTA性能，在四个基准测试中匹配或超越了8B类SOTA模型，在五个基准测试中甚至超越了Claude-4.5-Sonnet或DeepSeek-v3.2等更大规模模型。在GAIA文本任务中达到97.09%准确率（pass@64）。

Conclusion: 边缘规模模型的瓶颈并非其固有的能力上限，而是推理稳定性问题。通过建立的训练框架，AgentCPM-Explore有效释放了边缘规模模型被低估的显著潜力，证明了小模型也能具备强大的智能体能力。

Abstract: While Large Language Model (LLM)-based agents have shown remarkable potential for solving complex tasks, existing systems remain heavily reliant on large-scale models, leaving the capabilities of edge-scale models largely underexplored. In this paper, we present the first systematic study on training agentic models at the 4B-parameter scale. We identify three primary bottlenecks hindering the performance of edge-scale models: catastrophic forgetting during Supervised Fine-Tuning (SFT), sensitivity to reward signal noise during Reinforcement Learning (RL), and reasoning degradation caused by redundant information in long-context scenarios. To address the issues, we propose AgentCPM-Explore, a compact 4B agent model with high knowledge density and strong exploration capability. We introduce a holistic training framework featuring parameter-space model fusion, reward signal denoising, and contextual information refinement. Through deep exploration, AgentCPM-Explore achieves state-of-the-art (SOTA) performance among 4B-class models, matches or surpasses 8B-class SOTA models on four benchmarks, and even outperforms larger-scale models such as Claude-4.5-Sonnet or DeepSeek-v3.2 in five benchmarks. Notably, AgentCPM-Explore achieves 97.09% accuracy on GAIA text-based tasks under pass@64. These results provide compelling evidence that the bottleneck for edge-scale models is not their inherent capability ceiling, but rather their inference stability. Based on our well-established training framework, AgentCPM-Explore effectively unlocks the significant, yet previously underestimated, potential of edge-scale models.

</details>


### [11] [JADE: Expert-Grounded Dynamic Evaluation for Open-Ended Professional Tasks](https://arxiv.org/abs/2602.06486)
*Lanbo Lin,Jiayao Liu,Tianyuan Yang,Li Cai,Yuanwu Xu,Lei Wei,Sicong Xie,Guannan Zhang*

Main category: cs.AI

TL;DR: JADE是一个两层的AI智能体评估框架，结合了预定义评估技能和动态声明级评估，解决了开放专业任务评估中严谨性与灵活性之间的困境。


<details>
  <summary>Details</summary>
Motivation: 评估AI智能体在开放专业任务上面临根本困境：静态评分标准虽然严谨可重复，但无法适应多样化的有效响应策略；而基于LLM的评估方法虽然能适应个体响应，但存在不稳定性和偏见问题。人类专家通过结合领域基础原则和动态声明级评估来解决这一困境，这启发了JADE框架的开发。

Method: JADE采用两层评估框架：第一层将专家知识编码为预定义的评估技能集，提供稳定的评估标准；第二层执行报告特定的声明级评估，灵活评估多样化的推理策略，并通过证据依赖性门控机制来使基于被反驳声明的结论无效。

Result: 在BizBench上的实验表明，JADE提高了评估稳定性，并揭示了整体LLM评估器遗漏的关键智能体失败模式。该框架与专家制定的评分标准高度一致，并能有效迁移到医学领域基准测试，验证了其在多个专业领域的适用性。

Conclusion: JADE通过结合预定义评估技能和动态声明级评估，有效解决了开放专业任务评估中严谨性与灵活性之间的困境，为AI智能体评估提供了更稳定、更全面的框架，并在多个专业领域展现出良好的适用性。

Abstract: Evaluating agentic AI on open-ended professional tasks faces a fundamental dilemma between rigor and flexibility. Static rubrics provide rigorous, reproducible assessment but fail to accommodate diverse valid response strategies, while LLM-as-a-judge approaches adapt to individual responses yet suffer from instability and bias. Human experts address this dilemma by combining domain-grounded principles with dynamic, claim-level assessment. Inspired by this process, we propose JADE, a two-layer evaluation framework. Layer 1 encodes expert knowledge as a predefined set of evaluation skills, providing stable evaluation criteria. Layer 2 performs report-specific, claim-level evaluation to flexibly assess diverse reasoning strategies, with evidence-dependency gating to invalidate conclusions built on refuted claims. Experiments on BizBench show that JADE improves evaluation stability and reveals critical agent failure modes missed by holistic LLM-based evaluators. We further demonstrate strong alignment with expert-authored rubrics and effective transfer to a medical-domain benchmark, validating JADE across professional domains. Our code is publicly available at https://github.com/smiling-world/JADE.

</details>


### [12] [Progress Constraints for Reinforcement Learning in Behavior Trees](https://arxiv.org/abs/2602.06525)
*Finn Rietz,Mart Kartašev,Johannes A. Stork,Petter Ögren*

Main category: cs.AI

TL;DR: 提出进度约束机制，将行为树与强化学习结合，通过可行性估计器限制动作集，解决控制器相互抵消问题，提升性能和样本效率。


<details>
  <summary>Details</summary>
Motivation: 行为树提供结构化反应式决策框架，强化学习能学习近似最优控制器但面临稀疏奖励、安全探索和长期信用分配问题。两者结合可互补优势，但简单集成可能导致控制器相互抵消，降低整体性能。

Method: 提出进度约束机制，基于行为树收敛理论，使用可行性估计器限制允许的动作集，防止控制器相互抵消和撤销已完成的子目标。

Result: 在2D概念验证和高保真仓库环境中进行实证评估，相比之前的BT-RL集成方法，表现出更好的性能、样本效率和约束满足度。

Conclusion: 进度约束机制有效解决了行为树与强化学习集成中的控制器冲突问题，实现了两者的优势互补，为复杂环境中的决策制定提供了更有效的框架。

Abstract: Behavior Trees (BTs) provide a structured and reactive framework for decision-making, commonly used to switch between sub-controllers based on environmental conditions. Reinforcement Learning (RL), on the other hand, can learn near-optimal controllers but sometimes struggles with sparse rewards, safe exploration, and long-horizon credit assignment. Combining BTs with RL has the potential for mutual benefit: a BT design encodes structured domain knowledge that can simplify RL training, while RL enables automatic learning of the controllers within BTs. However, naive integration of BTs and RL can lead to some controllers counteracting other controllers, possibly undoing previously achieved subgoals, thereby degrading the overall performance. To address this, we propose progress constraints, a novel mechanism where feasibility estimators constrain the allowed action set based on theoretical BT convergence results. Empirical evaluations in a 2D proof-of-concept and a high-fidelity warehouse environment demonstrate improved performance, sample efficiency, and constraint satisfaction, compared to prior methods of BT-RL integration.

</details>


### [13] [HyPER: Bridging Exploration and Exploitation for Scalable LLM Reasoning with Hypothesis Path Expansion and Reduction](https://arxiv.org/abs/2602.06527)
*Shengxuan Qiu,Haochen Huang,Shuzhang Zhong,Pengfei Zuo,Meng Li*

Main category: cs.AI

TL;DR: HyPER：一种针对专家混合模型的无训练在线控制策略，通过动态扩展-缩减控制优化多路径推理的计算分配，在固定计算预算下实现更好的探索-利用权衡。


<details>
  <summary>Details</summary>
Motivation: 现有方法在探索-利用权衡上存在局限：树状搜索通过脆弱的扩展规则硬编码探索，干扰后训练推理；并行推理则过度探索冗余假设路径且答案选择能力弱。研究发现最优平衡是阶段依赖的，正确与错误推理路径往往在后期才分叉。

Method: 将测试时扩展重新表述为假设池上的动态扩展-缩减控制问题。提出HyPER：包含在线控制器（随假设池演化从探索转向利用）、token级精炼机制（无需完整路径重采样即可实现生成时高效利用）、长度和置信度感知聚合策略（实现可靠的答案时利用）。

Result: 在四个专家混合语言模型和多样化推理基准上的实验表明，HyPER始终实现更优的准确率-计算权衡，准确率提升8-10%，同时token使用量减少25-40%。

Conclusion: HyPER通过动态控制策略有效解决了多路径推理中的探索-利用权衡问题，在固定计算预算下显著提升了推理准确率和计算效率。

Abstract: Scaling test-time compute with multi-path chain-of-thought improves reasoning accuracy, but its effectiveness depends critically on the exploration-exploitation trade-off. Existing approaches address this trade-off in rigid ways: tree-structured search hard-codes exploration through brittle expansion rules that interfere with post-trained reasoning, while parallel reasoning over-explores redundant hypothesis paths and relies on weak answer selection. Motivated by the observation that the optimal balance is phase-dependent and that correct and incorrect reasoning paths often diverge only at late stages, we reformulate test-time scaling as a dynamic expand-reduce control problem over a pool of hypotheses. We propose HyPER, a training-free online control policy for multi-path decoding in mixture-of-experts models that reallocates computation under a fixed budget using lightweight path statistics. HyPER consists of an online controller that transitions from exploration to exploitation as the hypothesis pool evolves, a token-level refinement mechanism that enables efficient generation-time exploitation without full-path resampling, and a length- and confidence-aware aggregation strategy for reliable answer-time exploitation. Experiments on four mixture-of-experts language models across diverse reasoning benchmarks show that HyPER consistently achieves a superior accuracy-compute trade-off, improving accuracy by 8 to 10 percent while reducing token usage by 25 to 40 percent.

</details>


### [14] [LogicSkills: A Structured Benchmark for Formal Reasoning in Large Language Models](https://arxiv.org/abs/2602.06533)
*Brian Rabern,Philipp Mondorf,Barbara Plank*

Main category: cs.AI

TL;DR: LogicSkills基准测试评估LLM在形式推理中的三项核心技能：形式符号化、反模型构建和有效性评估，发现LLM在有效性评估上表现良好，但在符号化和反模型构建上表现较差，表明其依赖表层模式而非真正的符号推理。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在各种逻辑推理基准测试中表现出色，但尚不清楚它们真正掌握了哪些核心逻辑技能。为了系统评估LLM在形式推理中的基本能力，需要设计一个统一的基准测试来隔离和测量三项基本技能。

Method: 提出LogicSkills基准测试，专注于一阶逻辑双变量片段（不含恒等关系）中的三项核心技能：1) 形式符号化（将前提翻译为一阶逻辑）；2) 反模型构建（构建一个有限结构，使所有前提为真而结论为假）；3) 有效性评估（判断结论是否从给定前提中得出）。测试项目使用自然英语和Carroll风格的无意义词语言呈现，所有示例都使用SMT求解器Z3验证正确性和非平凡性。

Result: 在主流模型上的测试结果显示：模型在有效性评估任务上表现良好，但在形式符号化和反模型构建任务上表现显著较差。这表明LLM主要依赖表层语言模式进行推理，而非真正的符号或基于规则的推理能力。

Conclusion: LogicSkills基准测试揭示了LLM在形式推理能力上的局限性：虽然能够有效评估逻辑有效性，但在需要深入符号理解和模型构建的核心逻辑技能上存在明显不足。这为理解LLM的推理机制和未来改进方向提供了重要见解。

Abstract: Large language models have demonstrated notable performance across various logical reasoning benchmarks. However, it remains unclear which core logical skills they truly master. To address this, we introduce LogicSkills, a unified benchmark designed to isolate three fundamental skills in formal reasoning: (i) $\textit{formal symbolization}\unicode{x2014}$translating premises into first-order logic; (ii) $\textit{countermodel construction}\unicode{x2014}$formulating a finite structure in which all premises are true while the conclusion is false; and (iii) $\textit{validity assessment}\unicode{x2014}$deciding whether a conclusion follows from a given set of premises. Items are drawn from the two-variable fragment of first-order logic (without identity) and are presented in both natural English and a Carroll-style language with nonce words. All examples are verified for correctness and non-triviality using the SMT solver Z3. Across leading models, performance is high on validity but substantially lower on symbolization and countermodel construction, suggesting reliance on surface-level patterns rather than genuine symbolic or rule-based reasoning.

</details>


### [15] [AgentCPM-Report: Interleaving Drafting and Deepening for Open-Ended Deep Research](https://arxiv.org/abs/2602.06540)
*Yishan Li,Wentong Chen,Yukun Yan,Mingwei Li,Sen Mei,Xiaorong Wang,Kunpeng Liu,Xin Cong,Shuo Wang,Zhong Zhang,Yaxi Lu,Zhenghao Liu,Yankai Lin,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

TL;DR: AgentCPM-Report：一个轻量级本地深度研究报告生成系统，采用类似人类的写作过程框架和8B参数研究代理，通过动态大纲修订和迭代深化，在多个基准测试中超越闭源系统。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究报告生成系统严重依赖闭源或在线大模型，存在部署障碍、安全性和隐私问题。现有计划-写作范式对初始大纲质量依赖过高，而构建全面大纲本身需要强大推理能力。

Method: 提出AgentCPM-Report系统，包含：1）写作即推理策略（WARP），支持动态大纲修订；2）证据驱动草拟和推理驱动深化的交替机制；3）多阶段代理训练策略（冷启动、原子技能强化学习、整体流程强化学习）。

Result: 在DeepResearch Bench、DeepConsult和DeepResearch Gym等基准测试中，AgentCPM-Report表现优于领先的闭源系统，在Insight指标上取得显著提升。

Conclusion: AgentCPM-Report提供了一个轻量级、高性能的本地解决方案，通过模仿人类写作过程和动态大纲修订机制，有效解决了深度研究报告生成的挑战，同时避免了对外部大模型的依赖。

Abstract: Generating deep research reports requires large-scale information acquisition and the synthesis of insight-driven analysis, posing a significant challenge for current language models. Most existing approaches follow a plan-then-write paradigm, whose performance heavily depends on the quality of the initial outline. However, constructing a comprehensive outline itself demands strong reasoning ability, causing current deep research systems to rely almost exclusively on closed-source or online large models. This reliance raises practical barriers to deployment and introduces safety and privacy concerns for user-authored data. In this work, we present AgentCPM-Report, a lightweight yet high-performing local solution composed of a framework that mirrors the human writing process and an 8B-parameter deep research agent. Our framework uses a Writing As Reasoning Policy (WARP), which enables models to dynamically revise outlines during report generation. Under this policy, the agent alternates between Evidence-Based Drafting and Reasoning-Driven Deepening, jointly supporting information acquisition, knowledge refinement, and iterative outline evolution. To effectively equip small models with this capability, we introduce a Multi-Stage Agentic Training strategy, consisting of cold-start, atomic skill RL, and holistic pipeline RL. Experiments on DeepResearch Bench, DeepConsult, and DeepResearch Gym demonstrate that AgentCPM-Report outperforms leading closed-source systems, with substantial gains in Insight.

</details>


### [16] [SeeUPO: Sequence-Level Agentic-RL with Convergence Guarantees](https://arxiv.org/abs/2602.06554)
*Tianyi Hu,Qingxu Fu,Yanxi Chen,Zhaoyang Liu,Bolin Ding*

Main category: cs.AI

TL;DR: 提出SeeUPO算法，解决现有RL算法在多轮交互中缺乏收敛保证的问题，通过顺序反向更新策略确保单调改进和全局最优收敛


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的AI代理训练中，主流RL算法在多轮交互场景下缺乏已验证的收敛保证，导致训练不稳定和无法收敛到最优策略

Method: 提出SeeUPO（Sequence-level Sequential Update Policy Optimization），将多轮交互建模为顺序执行的多臂赌博机问题，采用反向执行顺序逐轮顺序更新策略，通过逆向归纳确保单调改进和全局最优收敛

Result: 在AppWorld和BFCL v4基准测试中，SeeUPO相比现有骨干算法取得显著提升：Qwen3-14B上相对增益43.3%-54.6%，Qwen2.5-14B上24.1%-41.9%，同时表现出优越的训练稳定性

Conclusion: SeeUPO解决了现有RL算法在多轮交互中无法同时实现无评论家和收敛保证的问题，为LLM-based AI代理训练提供了理论保证和实际性能提升

Abstract: Reinforcement learning (RL) has emerged as the predominant paradigm for training large language model (LLM)-based AI agents. However, existing backbone RL algorithms lack verified convergence guarantees in agentic scenarios, especially in multi-turn settings, which can lead to training instability and failure to converge to optimal policies.
  In this paper, we systematically analyze how different combinations of policy update mechanisms and advantage estimation methods affect convergence properties in single/multi-turn scenarios. We find that REINFORCE with Group Relative Advantage Estimation (GRAE) can converge to the globally optimal under undiscounted conditions, but the combination of PPO & GRAE breaks PPO's original monotonic improvement property. Furthermore, we demonstrate that mainstream backbone RL algorithms cannot simultaneously achieve both critic-free and convergence guarantees in multi-turn scenarios.
  To address this, we propose SeeUPO (Sequence-level Sequential Update Policy Optimization), a critic-free approach with convergence guarantees for multi-turn interactions. SeeUPO models multi-turn interaction as sequentially executed multi-agent bandit problems. Through turn-by-turn sequential policy updates in reverse execution order, it ensures monotonic improvement and convergence to global optimal solution via backward induction.
  Experiments on AppWorld and BFCL v4 demonstrate SeeUPO's substantial improvements over existing backbone algorithms: relative gains of 43.3%-54.6% on Qwen3-14B and 24.1%-41.9% on Qwen2.5-14B (averaged across benchmarks), along with superior training stability.

</details>


### [17] [Same Answer, Different Representations: Hidden instability in VLMs](https://arxiv.org/abs/2602.06652)
*Farooq Ahmad Wani,Alessandro Suglia,Rohit Saxena,Aryo Pradipta Gema,Wai-Chung Kwan,Fazl Barez,Maria Sofia Bucarelli,Fabrizio Silvestri,Pasquale Minervini*

Main category: cs.AI

TL;DR: 本文提出一个表示感知和频率感知的评估框架，发现VLMs在保持预测不变时内部表示会发生显著漂移，模型规模扩大不提升鲁棒性，扰动对不同任务有不同影响。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型（VLMs）的鲁棒性评估通常基于输出层面的不变性，隐含假设稳定预测反映稳定的多模态处理。本文认为这一假设不足，需要更深入评估内部表示的变化。

Method: 提出表示感知和频率感知的评估框架，测量内部嵌入漂移、频谱敏感性和结构平滑性（视觉token的空间一致性），同时结合标准基于标签的指标。在SEEDBench、MMMU和POPE数据集上对现代VLMs进行评估。

Result: 发现三种不同的失效模式：1）模型经常保持预测答案不变但内部表示发生显著漂移；2）鲁棒性不随模型规模提升而改善；3）扰动对不同任务有不同影响：破坏推理任务但减少幻觉基准的误报。

Conclusion: 仅依赖输出不变性评估VLMs鲁棒性不足，需要更全面的表示层面评估。模型规模扩大不必然提升鲁棒性，不同任务对扰动的敏感性不同，需要针对性的鲁棒性改进方法。

Abstract: The robustness of Vision Language Models (VLMs) is commonly assessed through output-level invariance, implicitly assuming that stable predictions reflect stable multimodal processing. In this work, we argue that this assumption is insufficient. We introduce a representation-aware and frequency-aware evaluation framework that measures internal embedding drift, spectral sensitivity, and structural smoothness (spatial consistency of vision tokens), alongside standard label-based metrics. Applying this framework to modern VLMs across the SEEDBench, MMMU, and POPE datasets reveals three distinct failure modes. First, models frequently preserve predicted answers while undergoing substantial internal representation drift; for perturbations such as text overlays, this drift approaches the magnitude of inter-image variability, indicating that representations move to regions typically occupied by unrelated inputs despite unchanged outputs. Second, robustness does not improve with scale; larger models achieve higher accuracy but exhibit equal or greater sensitivity, consistent with sharper yet more fragile decision boundaries. Third, we find that perturbations affect tasks differently: they harm reasoning when they disrupt how models combine coarse and fine visual cues, but on the hallucination benchmarks, they can reduce false positives by making models generate more conservative answers.

</details>


### [18] [Autoregressive Models for Knowledge Graph Generation](https://arxiv.org/abs/2602.06707)
*Thiviyan Thanapalasingam,Antonis Vozikis,Peter Bloem,Paul Groth*

Main category: cs.AI

TL;DR: ARK是用于知识图谱生成的自回归模型家族，将图视为三元组序列，无需显式规则监督即可学习语义约束，在IntelliGraphs基准测试中达到89.2%-100%语义有效性。


<details>
  <summary>Details</summary>
Motivation: 知识图谱生成需要模型学习三元组间的复杂语义依赖关系，同时保持领域有效性约束。与独立评分三元组的链接预测不同，生成模型必须捕获整个子图的相互依赖关系以产生语义连贯的结构。

Method: 提出ARK（自回归知识图谱生成）模型家族，将图视为(head, relation, tail)三元组序列进行自回归生成。模型直接从数据中学习隐式语义约束（类型一致性、时间有效性、关系模式），无需显式规则监督。还引入SAIL，ARK的变分扩展，通过学习的潜在表示实现可控生成。

Result: 在IntelliGraphs基准测试中，模型在多样化数据集上达到89.2%到100.0%的语义有效性，同时生成训练中未见的新图。分析显示模型容量（隐藏维度≥64）比架构深度对KG生成更重要，循环架构在保持可比有效性的同时提供显著计算效率。

Conclusion: 自回归模型为知识图谱生成提供了有效框架，在知识库补全和查询回答中具有实际应用价值。模型容量比架构深度更关键，循环架构在保持有效性的同时提供计算效率优势。

Abstract: Knowledge Graph (KG) generation requires models to learn complex semantic dependencies between triples while maintaining domain validity constraints. Unlike link prediction, which scores triples independently, generative models must capture interdependencies across entire subgraphs to produce semantically coherent structures. We present ARK (Auto-Regressive Knowledge Graph Generation), a family of autoregressive models that generate KGs by treating graphs as sequences of (head, relation, tail) triples. ARK learns implicit semantic constraints directly from data, including type consistency, temporal validity, and relational patterns, without explicit rule supervision. On the IntelliGraphs benchmark, our models achieve 89.2% to 100.0% semantic validity across diverse datasets while generating novel graphs not seen during training. We also introduce SAIL, a variational extension of ARK that enables controlled generation through learned latent representations, supporting both unconditional sampling and conditional completion from partial graphs. Our analysis reveals that model capacity (hidden dimensionality >= 64) is more critical than architectural depth for KG generation, with recurrent architectures achieving comparable validity to transformer-based alternatives while offering substantial computational efficiency. These results demonstrate that autoregressive models provide an effective framework for KG generation, with practical applications in knowledge base completion and query answering.

</details>


### [19] [Semantically Labelled Automata for Multi-Task Reinforcement Learning with LTL Instructions](https://arxiv.org/abs/2602.06746)
*Alessandro Abate,Giuseppe De Giacomo,Mathias Jackermeier,Jan Kretínský,Maximilian Prokop,Christoph Weinhuber*

Main category: cs.AI

TL;DR: 提出一种基于语义LTL到自动机转换的多任务强化学习方法，利用语义标记自动机生成任务嵌入来条件化策略，支持完整LTL规范并实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 多任务强化学习需要学习能够泛化到任意任务的通用策略，现有方法在处理复杂LTL规范时存在局限性，需要更高效的任务表示方法。

Method: 利用新型语义LTL到自动机转换技术，生成语义标记自动机，从中提取丰富的结构化信息作为任务嵌入，用于条件化策略，支持动态计算和完整LTL规范。

Result: 在多个领域实验中，该方法实现了最先进的性能，能够扩展到现有方法无法处理的复杂规范，展示了优越的泛化能力。

Conclusion: 基于语义LTL到自动机转换的任务嵌入技术为多任务强化学习提供了有效的解决方案，能够处理复杂规范并实现高性能，为形式化规范与强化学习的结合开辟了新途径。

Abstract: We study multi-task reinforcement learning (RL), a setting in which an agent learns a single, universal policy capable of generalising to arbitrary, possibly unseen tasks. We consider tasks specified as linear temporal logic (LTL) formulae, which are commonly used in formal methods to specify properties of systems, and have recently been successfully adopted in RL. In this setting, we present a novel task embedding technique leveraging a new generation of semantic LTL-to-automata translations, originally developed for temporal synthesis. The resulting semantically labelled automata contain rich, structured information in each state that allow us to (i) compute the automaton efficiently on-the-fly, (ii) extract expressive task embeddings used to condition the policy, and (iii) naturally support full LTL. Experimental results in a variety of domains demonstrate that our approach achieves state-of-the-art performance and is able to scale to complex specifications where existing methods fail.

</details>


### [20] [Towards Understanding What State Space Models Learn About Code](https://arxiv.org/abs/2602.06774)
*Jiali Wu,Abhinav Anand,Shweta Verma,Mira Mezini*

Main category: cs.AI

TL;DR: SSM代码模型在预训练中优于Transformer捕捉代码语法语义，但在微调时会遗忘某些语法语义关系，特别是短距离依赖任务。作者提出SSM-Interpret诊断框架和架构改进提升性能。


<details>
  <summary>Details</summary>
Motivation: 虽然SSM在代码理解任务上能与Transformer匹敌甚至超越，但其内部机制仍是黑盒。需要系统分析SSM代码模型学习内容，并与Transformer进行对比分析。

Method: 1. 对SSM和Transformer代码模型进行首次系统性比较分析；2. 提出SSM-Interpret频率域框架，揭示微调过程中的频谱变化；3. 基于分析结果提出架构改进方案。

Result: SSM在预训练阶段优于Transformer捕捉代码语法语义，但在微调时会遗忘某些语法语义关系，特别是短距离依赖任务。SSM-Interpret框架发现微调过程中频谱向短距离依赖偏移。

Conclusion: 通过SSM-Interpret诊断框架揭示了SSM微调中的问题，提出的架构改进显著提升了SSM代码模型性能，验证了分析对构建更好模型的价值。

Abstract: State Space Models (SSMs) have emerged as an efficient alternative to the transformer architecture. Recent studies show that SSMs can match or surpass Transformers on code understanding tasks, such as code retrieval, when trained under similar conditions. However, their internal mechanisms remain a black box. We present the first systematic analysis of what SSM-based code models actually learn and perform the first comparative analysis of SSM and Transformer-based code models. Our analysis reveals that SSMs outperform Transformers at capturing code syntax and semantics in pretraining but forgets certain syntactic and semantic relations during fine-tuning on task, especially when the task emphasizes short-range dependencies. To diagnose this, we introduce SSM-Interpret, a frequency-domain framework that exposes a spectral shift toward short-range dependencies during fine-tuning. Guided by these findings, we propose architectural modifications that significantly improve the performance of SSM-based code model, validating that our analysis directly enables better models.

</details>


### [21] [Wild Guesses and Mild Guesses in Active Concept Learning](https://arxiv.org/abs/2602.06818)
*Anirudh Chari,Neil Pattanaik*

Main category: cs.AI

TL;DR: 研究比较了两种主动概念学习策略：理性主动学习（EIG）和人类式积极测试策略（PTS），发现PTS在简单概念上表现更好，因为它能维持假设生成器的有效性，避免支持不匹配陷阱。


<details>
  <summary>Details</summary>
Motivation: 研究主动概念学习中查询选择策略的权衡：如何在查询的信息量与学习器生成和评分假设的稳定性之间取得平衡。传统理性方法可能不适合人类式的开放假设空间学习。

Method: 采用神经符号贝叶斯学习器，假设由大型语言模型生成的可执行程序构成，通过贝叶斯更新重新加权。比较两种策略：1）理性主动学习（EIG），选择最大化近似期望信息增益的查询；2）积极测试策略（PTS），查询当前最佳假设预测为正的实例。

Result: 在经典数字游戏的概念学习任务中，EIG在需要证伪的复杂规则（如复合规则或例外规则）上有效，但在简单概念上表现不佳。PTS虽然信息次优，但通过选择"安全"查询维持生成器有效性，在简单规则上收敛更快。

Conclusion: 人类的"确认偏误"可能不是认知错误，而是在稀疏、开放假设空间中维持可处理推理的理性适应策略。PTS通过避免支持不匹配陷阱，在简单概念学习中更有效。

Abstract: Human concept learning is typically active: learners choose which instances to query or test in order to reduce uncertainty about an underlying rule or category. Active concept learning must balance informativeness of queries against the stability of the learner that generates and scores hypotheses. We study this trade-off in a neuro-symbolic Bayesian learner whose hypotheses are executable programs proposed by a large language model (LLM) and reweighted by Bayesian updating. We compare a Rational Active Learner that selects queries to maximize approximate expected information gain (EIG) and the human-like Positive Test Strategy (PTS) that queries instances predicted to be positive under the current best hypothesis. Across concept-learning tasks in the classic Number Game, EIG is effective when falsification is necessary (e.g., compound or exception-laden rules), but underperforms on simple concepts. We trace this failure to a support mismatch between the EIG policy and the LLM proposal distribution: highly diagnostic boundary queries drive the posterior toward regions where the generator produces invalid or overly specific programs, yielding a support-mismatch trap in the particle approximation. PTS is information-suboptimal but tends to maintain proposal validity by selecting "safe" queries, leading to faster convergence on simple rules. Our results suggest that "confirmation bias" may not be a cognitive error, but rather a rational adaptation for maintaining tractable inference in the sparse, open-ended hypothesis spaces characteristic of human thought.

</details>


### [22] [ScaleEnv: Scaling Environment Synthesis from Scratch for Generalist Interactive Tool-Use Agent Training](https://arxiv.org/abs/2602.06820)
*Dunwei Tu,Hongyan Hao,Hansi Yang,Yihao Chen,Yi-Kai Zhang,Zhikang Xia,Yu Yang,Yueqing Sun,Xingchen Liu,Furao Shen,Qi Gu,Hui Su,Xunliang Cai*

Main category: cs.AI

TL;DR: ScaleEnv框架从零开始构建完全交互式环境和可验证任务，通过程序化测试确保环境可靠性，通过工具依赖图扩展和可执行动作验证保证任务完整性和可解性，显著提升智能体在未见工具使用基准上的性能。


<details>
  <summary>Details</summary>
Motivation: 训练能够适应多样化场景的通用智能体需要交互式环境进行自我探索，但现有交互环境严重不足，且现有合成方法在环境多样性和可扩展性方面存在显著限制。

Method: ScaleEnv框架从零开始构建完全交互式环境和可验证任务，通过程序化测试确保环境可靠性，通过工具依赖图扩展和可执行动作验证保证任务完整性和可解性。

Result: 在未见的多轮工具使用基准（如τ²-Bench和VitaBench）上表现出显著性能提升，展示了强大的泛化能力。研究还发现增加领域数量与模型泛化性能之间存在正相关关系。

Conclusion: 扩展环境多样性对于鲁棒的智能体学习至关重要，ScaleEnv为解决交互环境稀缺问题提供了有效框架，能够显著提升智能体的泛化能力。

Abstract: Training generalist agents capable of adapting to diverse scenarios requires interactive environments for self-exploration. However, interactive environments remain critically scarce, and existing synthesis methods suffer from significant limitations regarding environmental diversity and scalability. To address these challenges, we introduce ScaleEnv, a framework that constructs fully interactive environments and verifiable tasks entirely from scratch. Specifically, ScaleEnv ensures environment reliability through procedural testing, and guarantees task completeness and solvability via tool dependency graph expansion and executable action verification. By enabling agents to learn through exploration within ScaleEnv, we demonstrate significant performance improvements on unseen, multi-turn tool-use benchmarks such as $τ^2$-Bench and VitaBench, highlighting strong generalization capabilities. Furthermore, we investigate the relationship between increasing number of domains and model generalization performance, providing empirical evidence that scaling environmental diversity is critical for robust agent learning.

</details>


### [23] [POP: Online Structural Pruning Enables Efficient Inference of Large Foundation Models](https://arxiv.org/abs/2602.06822)
*Yi Chen,Wonjin Shin,Shuhong Liu,Tho Mai,Jeongmo Lee,Chuanbo Hua,Kun Wang,Jun Liu,Joo-Young Kim*

Main category: cs.AI

TL;DR: POP是一种轻量级在线结构化剪枝框架，通过分区引导的动态剪枝在自回归生成过程中实现上下文感知的剪枝决策，无需预处理或重训练。


<details>
  <summary>Details</summary>
Motivation: 现有结构化剪枝方法在推理时采用固定的剪枝决策，忽略了自回归token生成过程中出现的稀疏性模式，无法充分利用上下文信息进行动态剪枝。

Method: POP将模型通道划分为保留区、候选区和剪枝区：预填充阶段定义粗粒度剪枝分区，解码阶段在候选区内生成细粒度掩码，避免全通道重新评估。粗粒度分区保留持续重要的权重，细粒度掩码提供解码时的上下文条件变化。

Result: 在多种大型基础模型（LLMs、MoEs、VLMs）上的广泛评估表明，POP比现有剪枝方法提供更高的准确性，同时计算开销更小，推理延迟更低。

Conclusion: POP是一种轻量级即插即用的在线剪枝框架，能够实现上下文条件动态剪枝，在保持高精度的同时显著降低计算开销，适用于各种大型基础模型。

Abstract: Large foundation models (LFMs) achieve strong performance through scaling, yet current structural pruning methods derive fixed pruning decisions during inference, overlooking sparsity patterns that emerge in the autoregressive token generation. In this paper, we propose POP (Partition-guided Online Pruning), an efficient online structural pruning framework that enables context-conditioned dynamic pruning with minimal computational overhead. POP partitions model channels into retained, candidate, and pruned regions, where prefilling defines a coarse pruning partition, and the decoding stage generates a fine-grained mask within the candidate region, avoiding full-channel re-evaluation. The coarse pruning partition preserves consistently important weights, while the fine-grained masking provides context-conditioned variation during decoding. Moreover, POP is a lightweight, plug-and-play method that requires no preprocessing, including offline calibration, retraining, or learning predictors. Extensive evaluations across diverse LFMs, including large language models (LLMs), mixture-of-experts models (MoEs), and vision-language models (VLMs), demonstrate that POP consistently delivers higher accuracy than existing pruning approaches while incurring smaller computational overhead and minimizing inference latency.

</details>


### [24] [LLM Active Alignment: A Nash Equilibrium Perspective](https://arxiv.org/abs/2602.06836)
*Tonghan Wang,Yuqi Pan,Xinyi Yang,Yanchen Jiang,Milind Tambe,David C. Parkes*

Main category: cs.AI

TL;DR: 提出基于纳什均衡的博弈论框架，用于预测和引导大语言模型群体的行为，通过建模代理为人类子群体的混合策略，实现可解释的系统级预测和社会期望结果引导。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型群体行为难以预测和引导，特别是在开放文本空间中均衡计算不可行。需要一种既能预测多智能体LLM动态，又能主动引导其向有益社会结果发展的方法。

Method: 将每个LLM代理的行为建模为人类子群体的混合策略，代理主动战略性地选择与哪些群体对齐。采用标准凹效用假设推导闭式纳什均衡特征，作为现有对齐流程（如RLHF）之上的主动对齐层。

Result: 在社交媒体场景中，LLM群体（特别是基于推理的模型）可能表现出政治排斥现象，即某些子群体被所有LLM代理忽视。该方法能够避免这种病理现象，展示了在多领域调节多智能体LLM动态的潜力。

Conclusion: 该博弈论框架为预测和引导LLM群体行为提供了可解释、可操作的方法，能够避免不良社会动态，将对齐目标转向社会期望结果，具有跨领域应用的潜力。

Abstract: We develop a game-theoretic framework for predicting and steering the behavior of populations of large language models (LLMs) through Nash equilibrium (NE) analysis. To avoid the intractability of equilibrium computation in open-ended text spaces, we model each agent's action as a mixture over human subpopulations. Agents choose actively and strategically which groups to align with, yielding an interpretable and behaviorally substantive policy class. We derive closed-form NE characterizations, adopting standard concave-utility assumptions to enable analytical system-level predictions and give explicit, actionable guidance for shifting alignment targets toward socially desirable outcomes. The method functions as an active alignment layer on top of existing alignment pipelines such as RLHF. In a social-media setting, we show that a population of LLMs, especially reasoning-based models, may exhibit political exclusion, pathologies where some subpopulations are ignored by all LLM agents, which can be avoided by our method, illustrating the promise of applying the method to regulate multi-agent LLM dynamics across domains.

</details>


### [25] [An Adaptive Differentially Private Federated Learning Framework with Bi-level Optimization](https://arxiv.org/abs/2602.06838)
*Jin Wang,Hui Ma,Fei Xing,Ming Yan*

Main category: cs.AI

TL;DR: 提出自适应差分隐私联邦学习框架，通过本地压缩模块、自适应梯度裁剪和约束感知聚合机制，解决异构数据和隐私约束下的训练不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 实际联邦学习中存在设备异构性、非独立同分布数据，导致梯度更新不稳定和偏差。当加入差分隐私保护时，固定的梯度裁剪和高斯噪声注入会进一步放大梯度扰动，造成训练震荡和性能下降。

Method: 1) 客户端：引入轻量级本地压缩模块，正则化中间表示并约束梯度变异性，减少本地优化时的噪声放大；2) 服务器端：基于历史更新统计的自适应梯度裁剪策略，动态调整裁剪阈值；3) 约束感知聚合机制，抑制不可靠或噪声主导的客户端更新。

Result: 在CIFAR-10和SVHN数据集上的大量实验表明，该方法提高了收敛稳定性和分类准确率。

Conclusion: 提出的自适应差分隐私联邦学习框架有效解决了异构和隐私约束环境下的训练不稳定问题，通过自适应机制平衡了隐私保护与模型性能。

Abstract: Federated learning enables collaborative model training across distributed clients while preserving data privacy. However, in practical deployments, device heterogeneity, non-independent, and identically distributed (Non-IID) data often lead to highly unstable and biased gradient updates. When differential privacy is enforced, conventional fixed gradient clipping and Gaussian noise injection may further amplify gradient perturbations, resulting in training oscillation and performance degradation and degraded model performance. To address these challenges, we propose an adaptive differentially private federated learning framework that explicitly targets model efficiency under heterogeneous and privacy-constrained settings. On the client side, a lightweight local compressed module is introduced to regularize intermediate representations and constrain gradient variability, thereby mitigating noise amplification during local optimization. On the server side, an adaptive gradient clipping strategy dynamically adjusts clipping thresholds based on historical update statistics to avoid over-clipping and noise domination. Furthermore, a constraint-aware aggregation mechanism is designed to suppress unreliable or noise-dominated client updates and stabilize global optimization. Extensive experiments on CIFAR-10 and SVHN demonstrate improved convergence stability and classification accuracy.

</details>


### [26] [From Features to Actions: Explainability in Traditional and Agentic AI Systems](https://arxiv.org/abs/2602.06841)
*Sindhuja Chaduvula,Jessee Ho,Kina Kim,Aravind Narayanan,Mahshid Alinoori,Muskan Garg,Dhanesh Ramachandram,Shaina Raza*

Main category: cs.AI

TL;DR: 该研究比较了静态分类任务中的属性解释方法与智能体基准中的轨迹诊断方法，发现属性方法在静态设置中有效，但在智能体轨迹中不可靠，而基于轨迹的评估能更好地定位行为故障。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的发展，AI系统从静态预测转向多步决策的智能体系统。现有的可解释AI方法主要针对静态预测设计，不清楚这些方法如何适用于行为随时间演化的智能体设置。

Method: 通过实证比较静态分类任务中的属性解释方法（如特征归因）与智能体基准（TAU-bench Airline和AssistantBench）中的轨迹诊断方法。使用基于轨迹的评估框架来定位行为故障。

Result: 属性方法在静态设置中特征排名稳定（Spearman ρ=0.86），但在诊断智能体轨迹的执行级故障时不可靠。基于轨迹的评估能一致地定位行为故障，发现状态跟踪不一致在失败运行中普遍2.7倍，并将成功概率降低49%。

Conclusion: 研究结果表明需要从静态解释转向轨迹级可解释性，以更好地评估和诊断自主AI系统的行为，特别是在智能体系统中。

Abstract: Over the last decade, explainable AI has primarily focused on interpreting individual model predictions, producing post-hoc explanations that relate inputs to outputs under a fixed decision structure. Recent advances in large language models (LLMs) have enabled agentic AI systems whose behaviour unfolds over multi-step trajectories. In these settings, success and failure are determined by sequences of decisions rather than a single output. While useful, it remains unclear how explanation approaches designed for static predictions translate to agentic settings where behaviour emerges over time. In this work, we bridge the gap between static and agentic explainability by comparing attribution-based explanations with trace-based diagnostics across both settings. To make this distinction explicit, we empirically compare attribution-based explanations used in static classification tasks with trace-based diagnostics used in agentic benchmarks (TAU-bench Airline and AssistantBench). Our results show that while attribution methods achieve stable feature rankings in static settings (Spearman $ρ= 0.86$), they cannot be applied reliably to diagnose execution-level failures in agentic trajectories. In contrast, trace-grounded rubric evaluation for agentic settings consistently localizes behaviour breakdowns and reveals that state tracking inconsistency is 2.7$\times$ more prevalent in failed runs and reduces success probability by 49\%. These findings motivate a shift towards trajectory-level explainability for agentic systems when evaluating and diagnosing autonomous AI behaviour.
  Resources:
  https://github.com/VectorInstitute/unified-xai-evaluation-framework https://vectorinstitute.github.io/unified-xai-evaluation-framework

</details>


### [27] [AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents](https://arxiv.org/abs/2602.06855)
*Alisia Lupidi,Bhavul Gauri,Thomas Simon Foster,Bassel Al Omari,Despoina Magka,Alberto Pepe,Alexis Audran-Reiss,Muna Aghamelu,Nicolas Baldwin,Lucia Cipolina-Kun,Jean-Christophe Gagnon-Audet,Chee Hau Leow,Sandra Lefdal,Hossam Mossalam,Abhinav Moudgil,Saba Nazir,Emanuel Tewolde,Isabel Urrego,Jordi Armengol Estape,Amar Budhiraja,Gaurav Chaurasia,Abhishek Charnalia,Derek Dunfield,Karen Hambardzumyan,Daniel Izcovich,Martin Josifoski,Ishita Mediratta,Kelvin Niu,Parth Pathak,Michael Shvartsman,Edan Toledo,Anton Protopopov,Roberta Raileanu,Alexander Miller,Tatiana Shavrina,Jakob Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: AIRS-Bench是一个AI研究科学基准测试套件，包含20个来自前沿机器学习论文的任务，用于评估LLM代理在整个研究生命周期中的能力，结果显示代理在4个任务上超越人类SOTA，但在16个任务上未能达到人类水平。


<details>
  <summary>Details</summary>
Motivation: LLM代理在推动科学研究方面具有巨大潜力，但目前缺乏一个全面的基准测试来评估代理在整个研究生命周期中的能力，包括想法生成、实验分析和迭代优化等环节。

Method: 构建了AIRS-Bench基准套件，包含20个来自前沿机器学习论文的任务，涵盖语言建模、数学、生物信息学和时序预测等多个领域。任务格式灵活，便于集成新任务和比较不同代理框架。使用前沿模型配合顺序和并行架构建立了基线。

Result: 代理在4个任务上超越了人类SOTA，但在16个任务上未能达到人类水平。即使代理在某些任务上超越了人类基准，也未能达到该任务的理论性能上限，表明基准远未饱和，仍有很大改进空间。

Conclusion: AIRS-Bench为自主科学研究提供了一个有价值的评估框架，展示了LLM代理的潜力但同时也揭示了当前能力的局限性。开源任务定义和评估代码以促进该领域的进一步发展。

Abstract: LLM agents hold significant promise for advancing scientific research. To accelerate this progress, we introduce AIRS-Bench (the AI Research Science Benchmark), a suite of 20 tasks sourced from state-of-the-art machine learning papers. These tasks span diverse domains, including language modeling, mathematics, bioinformatics, and time series forecasting. AIRS-Bench tasks assess agentic capabilities over the full research lifecycle -- including idea generation, experiment analysis and iterative refinement -- without providing baseline code. The AIRS-Bench task format is versatile, enabling easy integration of new tasks and rigorous comparison across different agentic frameworks. We establish baselines using frontier models paired with both sequential and parallel scaffolds. Our results show that agents exceed human SOTA in four tasks but fail to match it in sixteen others. Even when agents surpass human benchmarks, they do not reach the theoretical performance ceiling for the underlying tasks. These findings indicate that AIRS-Bench is far from saturated and offers substantial room for improvement. We open-source the AIRS-Bench task definitions and evaluation code to catalyze further development in autonomous scientific research.

</details>


### [28] [Agentic Uncertainty Reveals Agentic Overconfidence](https://arxiv.org/abs/2602.06948)
*Jean Kaddour,Srijan Patel,Gbètondji Dovonon,Leo Richter,Pasquale Minervini,Matt J. Kusner*

Main category: cs.AI

TL;DR: AI代理在任务执行前、中、后预测成功率时表现出过度自信，成功率仅22%的代理预测77%成功率。反直觉的是，信息更少的执行前评估比标准执行后评估有更好区分度。对抗性提示将评估重构为错误查找可获得最佳校准。


<details>
  <summary>Details</summary>
Motivation: 研究AI代理是否能准确预测自身任务成功率，探索代理不确定性，了解AI系统对自身能力的自我评估能力。

Method: 在任务执行前、执行期间和执行后三个阶段收集代理的成功概率估计；使用对抗性提示将评估重构为错误查找；比较不同信息条件下的预测准确性。

Result: 发现代理普遍过度自信：实际成功率仅22%的代理预测77%成功率；反直觉地，信息更少的执行前评估比标准执行后评估有更好区分度；对抗性提示的错误查找方法获得最佳校准效果。

Conclusion: AI代理在自我评估中存在系统性过度自信问题；减少信息量可能改善预测区分度；将评估重构为错误查找是改善校准的有效策略。

Abstract: Can AI agents predict whether they will succeed at a task? We study agentic uncertainty by eliciting success probability estimates before, during, and after task execution. All results exhibit agentic overconfidence: some agents that succeed only 22% of the time predict 77% success. Counterintuitively, pre-execution assessment with strictly less information tends to yield better discrimination than standard post-execution review, though differences are not always significant. Adversarial prompting reframing assessment as bug-finding achieves the best calibration.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [29] [Identification and Estimation of Network Models with Nonparametric Unobserved Heterogeneity](https://arxiv.org/abs/2602.06885)
*Andrei Zeleneev*

Main category: econ.EM

TL;DR: 提出网络模型中未观测异质性的非参数处理方法，通过交互结果识别具有相同固定效应的个体，从而在控制固定效应的情况下识别协变量效应。


<details>
  <summary>Details</summary>
Motivation: 基于可观测变量的同质性在网络中普遍存在，因此基于不可观测变量（固定效应）的同质性也很可能是交互结果的重要决定因素。未能正确处理潜在同质性（及其他复杂形式的未观测异质性）会导致估计量不一致和政策含义误导。

Method: 考虑具有非参数未观测异质性的网络模型，不指定固定效应的具体形式。利用交互结果识别具有相同固定效应值的个体，通过这类个体观测特征的变化来识别协变量效应，同时控制固定效应。基于这些思想构建多个感兴趣参数的估计量。

Result: 建立了估计量的大样本性质，数值实验证明了所提方法的有效性并支持渐近理论。

Conclusion: 提出的方法能够有效处理网络模型中的未观测异质性，避免因忽略潜在同质性导致的估计偏差，为网络数据分析提供了可靠的计量经济学工具。

Abstract: Homophily based on observables is widespread in networks. Therefore, homophily based on unobservables (fixed effects) is also likely to be an important determinant of the interaction outcomes. Failing to properly account for latent homophily (and other complex forms of unobserved heterogeneity) can result in inconsistent estimators and misleading policy implications. To address this concern, we consider a network model with nonparametric unobserved heterogeneity, leaving the role of the fixed effects unspecified. We argue that the interaction outcomes can be used to identify agents with the same values of the fixed effects. The variation in the observed characteristics of such agents allows us to identify the effects of the covariates, while controlling for the fixed effects. Building on these ideas, we construct several estimators of the parameters of interest and characterize their large sample properties. Numerical experiments illustrate the usefulness of the suggested approaches and support the asymptotic theory.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [30] [A methodology for analyzing financial needs hierarchy from social discussions using LLM](https://arxiv.org/abs/2602.06431)
*Abhishek Jangra,Sachin Thukral,Arnab Chatterjee,Jayasree Raveendran*

Main category: cs.SI

TL;DR: 使用生成式AI分析社交媒体文本，揭示金融需求的分层结构，从短期基本需求到长期抱负，为理解金融行为提供数据驱动方法


<details>
  <summary>Details</summary>
Motivation: 金融需求对个人福祉和日常决策至关重要，但传统调查方法难以捕捉动态、细微的需求表达。本研究旨在通过分析社交媒体自然语言，更深入地理解金融行为

Method: 采用生成式AI技术和大语言模型(LLMs)，从大规模社交媒体文本数据中提取和分析金融需求表达，验证金融需求的分层结构假设

Result: 成功识别金融需求并验证其分层结构（从短期基本需求到长期抱负），同时揭示了在线金融讨论的内容和主题，为理解金融行为提供了新见解

Conclusion: 通过自然语言推断金融需求的方法提供了可扩展、数据驱动的替代方案，能够更动态、细致地理解现实世界中的金融行为，超越了传统调查方法的局限性

Abstract: This study examines the hierarchical structure of financial needs as articulated in social media discourse, employing generative AI techniques to analyze large-scale textual data. While human needs encompass a broad spectrum from fundamental survival to psychological fulfillment financial needs are particularly critical, influencing both individual well-being and day-to-day decision-making. Our research advances the understanding of financial behavior by utilizing large language models (LLMs) to extract and analyze expressions of financial needs from social media posts. We hypothesize that financial needs are organized hierarchically, progressing from short-term essentials to long-term aspirations, consistent with theoretical frameworks established in the behavioral sciences. Through computational analysis, we demonstrate the feasibility of identifying these needs and validate the presence of a hierarchical structure within them. In addition to confirming this structure, our findings provide novel insights into the content and themes of financial discussions online. By inferring underlying needs from naturally occurring language, this approach offers a scalable and data-driven alternative to conventional survey methodologies, enabling a more dynamic and nuanced understanding of financial behavior in real-world contexts.

</details>


### [31] [Mapping the political landscape from data traces: multidimensional opinions of users, politicians and media outlets on X](https://arxiv.org/abs/2602.06604)
*Antoine Vendeville,Jimena Royo-Letelier,Duncan Cassells,Jean-Philippe Cointet,Maxime Crépel,Tim Faverjon,Théophile Lenoir,Béatrice Mazoyer,Benjamin Ooghe-Tabanou,Armin Pournaki,Hiroki Yamashita,Pedro Ramaciotti*

Main category: cs.SI

TL;DR: 该研究创建了一个超越传统左右或自由-保守维度的多维度政治立场数据集，涵盖X/Twitter用户、议员和媒体机构，包含移民、欧盟、自由主义价值观、精英与制度、民族主义、环境等多个维度。


<details>
  <summary>Details</summary>
Motivation: 现有社交媒体政治活动研究大多基于美国背景的简单左右或自由-保守维度，无法适应更复杂的政治体系，需要能够测量多维度意识形态和议题立场的方法。

Method: 构建了一个包含X/Twitter用户、议员和媒体机构的数据集，在政治空间中定义了多个维度：移民态度、欧盟态度、自由主义价值观、精英与制度态度、民族主义、环境态度，以及传统的左右和自由-保守尺度。

Result: 提供了包含多个政治维度的数据集，包括用户活动指标（日均发帖数、粉丝数、关注数），并通过基准测试验证了这些实体的政治立场。

Conclusion: 该数据集能够支持更广泛的在线生态系统研究，超越传统的一维政治尺度，适用于研究意见极化、在线内容多样性等更复杂的政治分析场景。

Abstract: Studying political activity on social media often requires defining and measuring political stances of users or content. Relevant examples include the study of opinion polarization, or the study of political diversity in online content diets. While many research designs rely on operationalizations best suited for the US setting, few allow addressing more general political systems, in which users and media outlets might exhibit stances on multiple ideology and issue dimensions, going beyond traditional Liberal-Conservative or Left-Right scales. To advance the study of more general online ecosystems, we present a dataset pertaining to a population of X/Twitter users, parliamentarians, and media outlets embedded in a political space spanned by dimensions measuring attitudes towards immigration, the EU, liberal values, elites and institutions, nationalism and the environment, in addition to left-right and liberal-conservative scales. We include indicators of individual activity and popularity: mean number of posts per day, number of followers, and number of followees. We provide several benchmarks validating the positions of these entities and discuss several applications for this dataset.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [32] [Early warning of Mpox outbreaks in U.S. jurisdictions using Lasso Vector Autoregression models with cross-jurisdictional lags](https://arxiv.org/abs/2602.06135)
*Hannah Craddock,Joel O. Wertheim,Eliah Aronoff-Spencer,Mark Beatty,David Valentine,Rishi Graham,Jade C. Wang,Lior Rennert,Seema Shah,Ravi Goyal,Natasha K. Martin*

Main category: stat.AP

TL;DR: 开发VAR-Lasso模型预测美国猴痘病例，利用跨辖区数据提升预测准确性，特别关注疫情上升期的预测性能。


<details>
  <summary>Details</summary>
Motivation: 猴痘传播具有间歇性和空间异质性，需要及时的区域特异性预测来支持有针对性的公共卫生应对措施。

Method: 使用带Lasso正则化的向量自回归模型（VAR-Lasso），基于CDC监测数据对美国八个高发病率辖区进行滚动两周预测。

Result: VAR-Lasso模型识别出显著的长滞后跨辖区预测因子，在圣地亚哥县的案例中与系统发育分析结果一致。相比AR模型和朴素基准，VAR-Lasso在斜率加权误差指标上分别减少12-16%的RMSE、7-13%的MAE和66-76%的偏差。

Conclusion: 稀疏多变量时间序列模型利用跨辖区病例数据对猴痘暴发进行早期预测具有重要价值，可帮助卫生部门提前提供资源和信息以减轻未来暴发风险。

Abstract: Mpox is an orthopoxvirus that infects humans and animals and is transmitted primarily through close physical contact. The episodic and spatially heterogeneous dynamics of Mpox transmission underscores the need for timely, area-specific forecasts to support targeted public health responses in the U.S. We develop a Vector Autoregression model with Lasso regularization (VAR-Lasso) to generate rolling two-week-ahead forecasts of weekly Mpox cases for eight high-incidence U.S. jurisdictions using national surveillance data from the Centers for Disease Control and Prevention (CDC). The VAR-Lasso model identifies significant long-lag, cross-jurisdictional predictors. For a case study in San Diego County (SDC), these statistical predictors align with phylogenetic analysis that traces a 2023 cluster in SDC to an outbreak in Illinois six months earlier. As the need for public health action is often greatest when incidence is increasing, our performance evaluation focuses on positive-slope weighted error metrics. Forecast performance of the VAR-Lasso model is compared to a uni-variate Auto-Regressive (AR) Lasso model and a naive moving-average estimate. The models are compared using slope-weighted Root Mean Squared Error (RMSE), slope-weighted Mean Absolute Error (MAE), and slope-weighted bias. Across all observations, the VAR-Lasso model reduces slope-weighted RMSE, MAE, and bias by 12%, 7%, and 66% relative to the AR model, and by 16%, 13%, and 76% relative to the naive benchmark. Our findings highlight the value of sparse multivariate time-series models that leverage cross-jurisdictional case data for early forecasting of Mpox outbreaks. Such forecasting can aid health departments in proactively providing timely resources and messaging to mitigate the risks of a future outbreak.

</details>


### [33] [Non-Linear Drivers of Population Dynamics: a Nonparametric Coalescent Approach](https://arxiv.org/abs/2602.06148)
*Filippo Monti,Nuno R. Faria,Xiang Ji,Philippe Lemey,Moritz U. G. Kraemer,Marc A. Suchard*

Main category: stat.AP

TL;DR: 提出一个灵活的贝叶斯框架，通过高斯过程先验将协变量整合到具有分段恒定有效种群大小的溯祖模型中，以捕捉协变量与种群动态之间的非线性关系。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设协变量与有效种群大小之间存在对数线性关系，这无法捕捉复杂的生物过程，当真实关系为非线性时可能引入偏差。需要更灵活的方法来理解生态和环境因素如何驱动种群动态。

Method: 开发了一个贝叶斯框架，通过高斯过程先验将协变量整合到具有分段恒定有效种群大小的溯祖模型中。高斯过程作为函数分布，自然地容纳非线性协变量效应，无需限制性参数假设。为了平衡全局协变量驱动模式与局部时间动态，将高斯过程先验与高斯马尔可夫随机场耦合，强制有效种群大小轨迹的平滑性。

Result: 通过模拟研究和三个实证应用（巴西黄热病毒动态、晚第四纪麝牛种群统计、喀麦隆HIV-1 CRF02-AG进化）证明，该方法既能在适当情况下确认线性关系，又能揭示原本会被忽略或错误表征的非线性协变量效应。

Conclusion: 该框架通过实现更准确和生物学上更真实的建模，推进了系统动力学推断，能够更好地理解环境和流行病学因素如何随时间塑造种群大小。

Abstract: Effective population size (Ne(t)) is a fundamental parameter in population genetics and phylodynamics that quantifies genetic diversity and reveals demographic history. Coalescent-based methods enable the inference of Ne(t) trajectories through time from phylogenies reconstructed from molecular sequence data. Understanding the ecological and environmental drivers of population dynamics requires linking Ne(t) to external covariates. Existing approaches typically impose log-linear relationships between covariates and Ne(t), which may fail to capture complex biological processes and can introduce bias when the true relationship is nonlinear. We present a flexible Bayesian framework that integrates covariates into coalescent models with piecewise-constant Ne(t) through a Gaussian process (GP) prior. The GP, a distribution over functions, naturally accommodates nonlinear covariate effects without restrictive parametric assumptions. This formulation improves estimation of covariate-Ne(t) relationships, mitigates bias under nonlinear associations, and yields interpretable uncertainty quantification that varies across the covariate space. To balance global covariate-driven patterns with local temporal dynamics, we couple the GP prior with a Gaussian Markov random field that enforces smoothness in Ne(t) trajectories. Through simulation studies and three empirical applications - yellow fever virus dynamics in Brazil (2016-2018), late-Quaternary musk ox demography, and HIV-1 CRF02-AG evolution in Cameroon - we demonstrate that our method both confirms linear relationships where appropriate and reveals nonlinear covariate effects that would otherwise be missed or mischaracterized. This framework advances phylodynamic inference by enabling more accurate and biologically realistic modeling of how environmental and epidemiological factors shape population size through time.

</details>


### [34] [Evaluating Predictive Modeling Strategies for Predicting Individual Treatment Effects in Precision Medicine](https://arxiv.org/abs/2602.06210)
*Pamela M. Chiroque-Solano,M Lee Van Horn,Thomas Jaki*

Main category: stat.AP

TL;DR: 比较了30多种预测个体治疗效果(PITE)的建模策略，发现惩罚回归和投影方法在多种场景下表现稳健，而灵活学习器仅在强信号和大样本下表现良好，强调需要严格的外部验证。


<details>
  <summary>Details</summary>
Motivation: 精准医疗需要准确预测个体治疗效果(PITE)，但由于反事实不可观测、高维度和复杂交互作用，PITE估计面临挑战。需要系统比较不同建模策略在各种场景下的性能。

Method: 使用结构化模拟框架比较30多种建模策略，包括惩罚回归方法(岭回归、lasso、弹性网络)、投影方法(偏最小二乘、主成分回归)、灵活学习器和树集成方法。模拟框架变化样本量、维度、多重共线性和交互复杂度。

Result: 惩罚回归和投影方法(岭回归、lasso、弹性网络、PLS、PCR)在RMSE和方向准确性方面表现一致稳健。灵活学习器仅在强信号和足够样本量下表现优异。内部验证产生乐观估计，而具有分布偏移和更高阶交互作用的外部验证更能揭示模型弱点。

Conclusion: 研究推荐使用稳健的线性/投影方法作为默认选择，并强调需要严格的外部验证来评估预测个体治疗效果的模型性能，特别是在存在分布偏移和复杂交互作用的情况下。

Abstract: Precision medicine seeks to match patients with treatments that produce the greatest benefit. The Predicted Individual Treatment Effect (PITE)-the difference between predicted outcomes under treatment and control-quantifies this benefit but is difficult to estimate due to unobserved counterfactuals, high dimensionality, and complex interactions. We compared 30+ modeling strategies, including penalized and projection-based methods, flexible learners, and tree-ensembles, using a structured simulation framework varying sample size, dimensionality, multicollinearity, and interaction complexity. Performance was measured using root mean squared error (RMSE) for prediction accuracy and directional accuracy (DIR) for correctly classifying benefit versus harm. Internal validation produced optimistic estimates, whereas external validation with distributional shifts and higher-order interactions more clearly revealed model weaknesses. Penalized and projection-based approaches-ridge, lasso, elastic net, partial least squares (PLS), and principal components regression (PCR)-consistently achieved strong RMSE and DIR performance. Flexible learners excelled only under strong signals and sufficient sample sizes. Results highlight robust linear/projection defaults and the necessity of rigorous external validation.

</details>


### [35] [Modeling the Hazard Function with Non-linear Systems in Dynamical Survival Analysis](https://arxiv.org/abs/2602.06322)
*Dananjani Liyanage,Mahmudul Bari Hridoy,Fahad Mostafa*

Main category: stat.AP

TL;DR: 提出基于高阶常微分方程的生存分析风险函数建模框架，能捕捉振荡、非线性阻尼等复杂风险动态，超越传统单调模型


<details>
  <summary>Details</summary>
Motivation: 传统一阶ODE风险函数模型只能产生单调风险模式，无法表示振荡行为、非线性阻尼或耦合增长衰减动态，限制了在复杂时间依赖风险行为建模中的应用

Method: 提出高阶ODE统计框架，将风险建模为依赖于当前水平、变化率和时间的函数；通过将高阶ODE转化为非线性一阶方程组进行数值求解，通过累积风险反演生成失效时间；开发右删失下的似然推断方法，使用矩生成函数分析尾部行为

Result: 通过模拟研究和真实世界生存数据评估，证明该框架能捕捉传统单调模型无法表示的时间风险模式，特别是振荡性风险动态

Conclusion: 高阶ODE风险函数建模框架为生存分析和可靠性研究提供了更灵活的工具，能表示复杂的时间依赖风险行为，扩展了传统参数模型的能力

Abstract: Hazard functions play a central role in survival analysis, offering insight into the underlying risk dynamics of time to event data, with broad applications in medicine, epidemiology, and related fields. First order ordinary differential equation (ODE) formulations of the hazard function have been explored as extensions beyond classical parametric models. However, such approaches typically produce monotonic hazard patterns, limiting their ability to represent oscillatory behavior, nonlinear damping, or coupled growth decay dynamics. We propose a new statistical framework for modeling and simulating hazard functions governed by higher-order ODEs, allowing risk to depend on both its current level, its rate of change, and time. This class of models captures complex time dependent risk behaviors relevant to survival analysis and reliability studies. We develop a simulation procedure by reformulating the higher order ODE as a system of nonlinear first order equations solved numerically, with failure times generated via cumulative hazard inversion. Likelihood based inference under right censoring is also developed, and moment generating function analysis is used to characterize tail behavior. The proposed framework is evaluated through simulation studies and illustrated using real world survival data, where oscillatory hazard dynamics capture temporal risk patterns beyond standard monotone models.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [36] [Code, Capital, and Clusters: Understanding Firm Performance in the UK AI Economy](https://arxiv.org/abs/2602.06249)
*Waqar Muhammad Ashraf,Diane Coyle,Ramit Debnath*

Main category: cs.CY

TL;DR: 英国AI产业高度集中于伦敦和科技行业，企业规模和AI专业化程度是收入主要驱动力，本地社会经济因素影响显著。预测显示行业将向缓慢扩张和整合过渡，需要针对性的区域政策干预。


<details>
  <summary>Details</summary>
Motivation: 尽管英国在全球AI领域建立了独特地位，但AI专业化、本地社会经济条件与企业绩效之间的相互作用尚未得到充分探索。研究旨在填补这一空白，为政策制定提供实证依据。

Method: 使用Companies House、ONS和glass.ai的综合数据集（2000-2024年），分析英国AI实体。采用统计分析和预测模型，评估地理分布、行业特征、企业绩效及本地社会经济因素的影响。

Result: 发现AI产业高度集中于伦敦（占实体的41.3%）和科技行业；企业规模和AI专业化强度是主要收入驱动因素；本地因素（三级资格率、人口密度、就业水平）有显著边际贡献；预测到2030年将有4,651个实体，解散率上升至2.21%，表明行业向缓慢扩张和整合过渡。

Conclusion: 需要基于地方敏感性的政策干预：培育伦敦以外的区域AI能力以降低系统性风险；区分规模化支持（解决资金缺口）和技术专业化深化；战略性地引导生态系统整合。针对性措施对促进总体AI增长和平衡区域发展至关重要。

Abstract: The UK has established a distinctive position in the global AI landscape, driven by rapid firm formation and strategic investment. However, the interplay between AI specialisation, local socioeconomic conditions, and firm performance remains underexplored. This study analyses a comprehensive dataset of UK AI entities (2000 - 2024) from Companies House, ONS, and glass.ai. We find a strong geographical concentration in London (41.3 percent of entities) and technology-centric sectors, with general financial services reporting the highest mean operating revenue (33.9 million GBP, n=33). Firm size and AI specialisation intensity are primary revenue drivers, while local factors, Level 3 qualification rates, population density, and employment levels, provide significant marginal contributions, highlighting the dependence of AI growth on regional socioeconomic ecosystems. The forecasting models project sectoral expansion to 2030, estimating 4,651 [4,323 - 4,979, 95 percent CI] total entities and a rising dissolution ratio (2.21 percent [-0.17 - 4.60]), indicating a transition toward slower sector expansion and consolidation. These results provide robust evidence for place-sensitive policy interventions: cultivating regional AI capabilities beyond London to mitigate systemic risks; distinguishing between support for scaling (addressing capital gaps) and deepening technical specialisation; and strategically shaping ecosystem consolidation. Targeted actions are essential to foster both aggregate AI growth and balanced regional development, transforming consolidation into sustained competitive advantage.

</details>


### [37] [Do LLMs Track Public Opinion? A Multi-Model Study of Favorability Predictions in the 2024 U.S. Presidential Election](https://arxiv.org/abs/2602.06302)
*Riya Parikh,Sarah H. Cen,Chara Podimata*

Main category: cs.CY

TL;DR: LLMs在预测2024年美国总统选举民意调查时存在系统性偏差，对哈里斯过度预测10-40%，对特朗普偏差较小(5-10%)，且无法通过时间平滑或联网检索修正。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型(LLMs)是否能够准确追踪2024年美国总统选举周期中的公众意见，通过比较LLM预测与高质量出口民调结果，评估LLM在选举预测中的可靠性。

Method: 使用llm-election-data-2024数据集，每天查询9种LLM配置，获取总统候选人的头条新闻好感度预测，与路透社、CNN、盖洛普、昆尼皮亚克和ABC的五项高质量民调进行对比分析。

Result: 发现系统性方向性校准错误：所有模型对卡玛拉·哈里斯的好感度过度预测10-40%；对唐纳德·特朗普的偏差较小(5-10%)且依赖具体民调，模型间差异显著较小。这些偏差在时间平滑和联网检索增强后仍然存在。

Conclusion: 现成的LLM在直接查询时无法可靠追踪民调结果，这对选举预测应用具有重要启示，表明需要更复杂的校准方法或模型改进。

Abstract: We investigate whether Large Language Models (LLMs) can track public opinion as measured by exit polls during the 2024 U.S. presidential election cycle. Our analysis focuses on headline favorability (e.g., "Favorable" vs. "Unfavorable") of presidential candidates across multiple LLMs queried daily throughout the election season. Using the publicly available llm-election-data-2024 dataset, we evaluate predictions from nine LLM configurations against a curated set of five high-quality polls from major organizations including Reuters, CNN, Gallup, Quinnipiac, and ABC. We find systematic directional miscalibration. For Kamala Harris, all models overpredict favorability by 10-40% relative to polls. For Donald Trump, biases are smaller (5-10%) and poll-dependent, with substantially lower cross-model variation. These deviations persist under temporal smoothing and are not corrected by internet-augmented retrieval. We conclude that off-the-shelf LLMs do not reliably track polls when queried in a straightforward manner and discuss implications for election forecasting.

</details>


### [38] [Bilingual Bias in Large Language Models: A Taiwan Sovereignty Benchmark Study](https://arxiv.org/abs/2602.06371)
*Ju-Chun Ko*

Main category: cs.CY

TL;DR: 该研究系统评估了17个LLM在中文和英文下对台湾主权问题的回答一致性，发现15个模型存在显著语言偏见，仅GPT-4o Mini在两种语言中均得满分。


<details>
  <summary>Details</summary>
Motivation: LLM在多语言环境中部署日益增多，但其在政治敏感话题上的跨语言一致性研究不足。特别是关于台湾主权问题，不同语言查询可能产生不同政治立场，需要系统评估这种语言偏见现象。

Method: 建立双语基准测试，评估17个LLM在中文和英文下对台湾主权问题的回答。提出语言偏见分数(LBS)和质量调整一致性(QAC)等新指标来量化语言偏见和一致性。

Result: 15/17的模型表现出可测量的语言偏见，中国产模型问题尤为严重（完全拒绝回答或明确传播中共叙事）。仅GPT-4o Mini在两种语言中均获得10/10满分。开源了基准测试和评估框架。

Conclusion: LLM在政治敏感话题上存在显著的语言偏见，需要开发更一致的多语言模型。提出的量化指标和开源框架有助于社区进一步研究和改进模型一致性。

Abstract: Large Language Models (LLMs) are increasingly deployed in multilingual contexts, yet their consistency across languages on politically sensitive topics remains understudied. This paper presents a systematic bilingual benchmark study examining how 17 LLMs respond to questions concerning the sovereignty of the Republic of China (Taiwan) when queried in Chinese versus English. We discover significant language bias -- the phenomenon where the same model produces substantively different political stances depending on the query language. Our findings reveal that 15 out of 17 tested models exhibit measurable language bias, with Chinese-origin models showing particularly severe issues including complete refusal to answer or explicit propagation of Chinese Communist Party (CCP) narratives. Notably, only GPT-4o Mini achieves a perfect 10/10 score in both languages. We propose novel metrics for quantifying language bias and consistency, including the Language Bias Score (LBS) and Quality-Adjusted Consistency (QAC). Our benchmark and evaluation framework are open-sourced to enable reproducibility and community extension.

</details>


### [39] [Estimating Exam Item Difficulty with LLMs: A Benchmark on Brazil's ENEM Corpus](https://arxiv.org/abs/2602.06631)
*Thiago Brant,Julien Kühn,Jun Pang*

Main category: cs.CY

TL;DR: LLMs在估计生成题目难度方面存在系统性偏差，无法可靠进行上下文自适应个性化，建议采用"先评估后生成"的负责任评估设计流程。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs越来越多地用于生成教育内容，需要评估这些模型能否可靠估计其生成题目的难度，以确保教育评估的安全性和有效性。

Method: 使用巴西ENEM考试作为测试平台，基于1,031道题目的官方项目反应理论参数，评估10个专有和开源LLMs在三个维度上的表现：绝对校准、排序保真度和跨学习者背景的上下文敏感性。

Result: 最佳模型仅达到中等排序相关性，但系统性低估题目难度，在多模态题目上表现显著下降。模型在接收学生人口统计线索时表现出有限且不一致的可塑性，表明其尚未准备好进行上下文自适应个性化。

Conclusion: LLMs最适合作为校准筛选器而非权威预言者，支持采用"先评估后生成"的负责任评估设计流程，以确保教育内容的安全性和有效性。

Abstract: As Large Language Models (LLMs) are increasingly deployed to generate educational content, a critical safety question arises: can these models reliably estimate the difficulty of the questions they produce? Using Brazil's high-stakes ENEM exam as a testbed, we benchmark ten proprietary and open-weight LLMs against official Item Response Theory (IRT) parameters for 1,031 questions. We evaluate performance along three axes: absolute calibration, rank fidelity, and context sensitivity across learner backgrounds. Our results reveal a significant trade-off: while the best models achieve moderate rank correlation, they systematically underestimate difficulty and degrade significantly on multimodal items. Crucially, we find that models exhibit limited and inconsistent plasticity when prompted with student demographic cues, suggesting they are not yet ready for context-adaptive personalization. We conclude that LLMs function best as calibrated screeners rather than authoritative oracles, supporting an "evaluation-before-generation" pipeline for responsible assessment design.

</details>

<div id=toc></div>

# Table of Contents

- [cs.ET](#cs.ET) [Total: 4]
- [cs.SI](#cs.SI) [Total: 4]
- [cs.CY](#cs.CY) [Total: 9]
- [cs.AI](#cs.AI) [Total: 25]
- [stat.AP](#stat.AP) [Total: 3]
- [econ.EM](#econ.EM) [Total: 1]


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [1] [A Comparative Study of Digital Memristor-Based Processing-In-Memory from a Device and Reliability Perspective](https://arxiv.org/abs/2602.04035)
*Thomas Neuner,Henriette Padberg,Lior Kornblum,Eilam Yalon,Pedram Khalili Amiri,Shahar Kvatinsky*

Main category: cs.ET

TL;DR: 这篇综述回顾了存内计算中基于新兴非易失性存储器的状态逻辑和非状态逻辑技术进展，重点分析RRAM、PCM、MRAM等器件的逻辑设计、可靠性挑战及器件级优化对商业化存内计算系统的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着数据密集型应用对传统计算系统的压力增大，存内计算成为缓解内存墙问题的重要范式。需要系统评估新兴非易失性存储器技术在存内计算中的逻辑实现方法、可靠性挑战和优化策略，以推动可扩展的商业化存内计算系统发展。

Method: 综述首先概述相关逻辑家族、忆阻器件类型和可靠性指标，然后分析各逻辑家族如何利用不同器件特性实现逻辑技术，通过代表性器件堆栈和性能参数的对比表展示权衡和质量指标，全面评估实验验证和模拟的逻辑设计。

Result: 系统分析了RRAM、PCM、MRAM等新兴存储器在存内计算中的逻辑技术实现，识别了可靠性方面的关键挑战，强调了器件级优化对实现可扩展、商业化存内计算系统的重要性，为下一代存内计算应用的优化、鲁棒忆阻器件开发提供了支持。

Conclusion: 通过全面分析状态和非状态逻辑技术，该综述支持开发优化的鲁棒忆阻器件，为下一代存内计算应用奠定基础，强调了器件特性与逻辑设计的协同优化对实现商业可行存内计算系统的关键作用。

Abstract: As data-intensive applications increasingly strain conventional computing systems, processing-in-memory (PIM) has emerged as a promising paradigm to alleviate the memory wall by minimizing data transfer between memory and processing units. This review presents the recent advances in both stateful and non-stateful logic techniques for PIM, focusing on emerging nonvolatile memory technologies such as resistive random-access memory (RRAM), phase-change memory (PCM), and magnetoresistive random-access memory (MRAM). Both experimentally demonstrated and simulated logic designs are critically examined, highlighting key challenges in reliability and the role of device-level optimization in enabling scalable and commercial viable PIM systems. The review begins with an overview of relevant logic families, memristive device types, and associated reliability metrics. Each logic family is then explored in terms of how it capitalizes on distinct device properties to implement logic techniques. A comparative table of representative device stacks and performance parameters illustrates trade-offs and quality indicators. Through this comprehensive analysis, the development of optimized, robust memristive devices for next-generation PIM applications is supported.

</details>


### [2] [The Dynamics of Attention across Automated and Manual Driving Modes: A Driving Simulation Study](https://arxiv.org/abs/2602.04164)
*Yuan Cai,Mustafa Demir,Farzan Sasangohar,Mohsen Zare*

Main category: cs.ET

TL;DR: 研究探索自动驾驶中驾驶员在不同驾驶模式（自动、手动、过渡）下对道路、中央后视镜、嵌入式HMI和速度表等区域的注意力动态分配，发现注意力模式随驾驶模式变化显著。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆集成到交通系统引发了安全担忧，特别是模式转换期间的驾驶员重新参与。过去的事故凸显了对自动化的过度依赖风险，需要理解动态注意力分配以支持自动驾驶安全。

Method: 采用高保真驾驶模拟，使用眼动追踪技术测量不同驾驶模式（自动、手动和过渡）下的注视持续时间、注视次数和首次注视时间，评估驾驶员对不同兴趣区域的注意力分配。

Result: 驾驶员注意力在不同驾驶模式下差异显著：手动模式下注意力持续集中在道路上；自动模式下观察到对嵌入式HMI的长时间注视；在交接和接管阶段，注意力在环境和技术元素之间动态转移。

Conclusion: 驾驶员注意力分配是模式依赖的，这些发现为设计符合驾驶员注意力模式的自适应HMI提供了依据。通过根据驾驶情境呈现相关信息，此类系统可以增强驾驶员-车辆交互，支持有效过渡并提高整体安全。

Abstract: This study aims to explore the dynamics of driver attention to various zones, including the road, the central mirror, the embedded Human-Machine Interface (HMI), and the speedometer, across different driving modes in AVs. The integration of autonomous vehicles (AVs) into transportation systems has introduced critical safety concerns, particularly regarding driver re-engagement during mode transitions. Past accidents underscore the risks of overreliance on automation and highlight the need to understand dynamic attention allocation to support safety in autonomous driving. A high-fidelity driving simulation was conducted. Eye-tracking technology was used to measure fixation duration, fixation count, and time to first fixation across distinct driving modes (automated, manual, and transition), which were then used to assess how drivers allocated attention to various areas of interest (AOIs). Findings show that drivers' attention varies significantly across driving modes. In manual mode, attention consistently focuses on the road, while in automated mode, prolonged fixation on the embedded HMI was observed. During the handover and takeover phases, attention shifts dynamically between environmental and technological elements. The study reveals that driver attention allocation is mode-dependent. These findings inform the design of adaptive HMIs in AVs that align with drivers' attention patterns. By presenting relevant information according to the driving context, such systems can enhance driver-vehicle interaction, support effective transitions, and improve overall safety. Systematic analysis of visual attention dynamics across driving modes is gaining prominence, as it informs adaptive HMI designs and driver readiness interventions. The GLMM findings can be directly applied to the design of adaptive HMIs or driver training programs to enhance attention and improve safety.

</details>


### [3] [Self-evolving Embodied AI](https://arxiv.org/abs/2602.04411)
*Tongtong Feng,Xin Wang,Wenwu Zhu*

Main category: cs.ET

TL;DR: 提出自演化具身AI新范式，使智能体能在动态开放环境中通过记忆自更新、任务自切换、环境自预测、具身自适应和模型自演化实现持续自适应智能。


<details>
  <summary>Details</summary>
Motivation: 现有具身AI局限于人工设定环境，智能体在给定记忆和任务上训练，无法适应野外环境中可变的具身形态和动态开放环境。

Method: 提出自演化具身AI框架，包括定义、框架、组件和机制，系统回顾现有组件实现，讨论实际应用并指出未来研究方向。

Result: 建立了一个新的研究范式，使智能体能够基于自身状态和环境变化自主操作，实现类似人类的自主学习和环境交互能力。

Conclusion: 自演化具身AI为实现通用人工智能提供了新视角，使智能体能以人类方式自主学习和与环境交互。

Abstract: Embodied Artificial Intelligence (AI) is an intelligent system formed by agents and their environment through active perception, embodied cognition, and action interaction. Existing embodied AI remains confined to human-crafted setting, in which agents are trained on given memory and construct models for given tasks, enabling fixed embodiments to interact with relatively static environments. Such methods fail in in-the-wild setting characterized by variable embodiments and dynamic open environments. This paper introduces self-evolving embodied AI, a new paradigm in which agents operate based on their changing state and environment with memory self-updating, task self-switching, environment self-prediction, embodiment self-adaptation, and model self-evolution, aiming to achieve continually adaptive intelligence with autonomous evolution. Specifically, we present the definition, framework, components, and mechanisms of self-evolving embodied AI, systematically review state-of-the-art works for realized components, discuss practical applications, and point out future research directions. We believe that self-evolving embodied AI enables agents to autonomously learn and interact with environments in a human-like manner and provide a new perspective toward general artificial intelligence.

</details>


### [4] [Quantum-Based Resilient Routing in Networks: Minimizing Latency Under Dual-Link Failures](https://arxiv.org/abs/2602.04495)
*Maher Harb,Nader Foroughi,Matt Stehman,Bob Lutz,Nati Erez,Erik Garcell*

Main category: cs.ET

TL;DR: 本文提出使用量子近似优化算法（QAOA）解决电信网络中延迟弹性的Layer 3路由优化问题，通过数学建模寻找两条不相交的最短路径来最小化延迟并提高网络弹性。


<details>
  <summary>Details</summary>
Motivation: 网络优化问题随着网络规模增长而呈指数级复杂，计算成本高昂。电信网络需要解决延迟弹性的Layer 3路由优化问题，特别是在预定义Layer 1光链路的情况下，需要找到既能最小化延迟又能提高网络弹性的解决方案。

Method: 将问题建模为图优化问题，目标是最小化延迟、创建从每个站点到互联网骨干网的顶点不相交路径，并通过限制双链路故障的影响来最大化整体弹性。将问题转化为寻找两条不相交的最短路径，并在目标函数中加入弹性组件。使用量子近似优化算法（QAOA）在量子模拟器和量子硬件上求解。

Result: 在包含5个顶点和7条边的玩具图拓扑上测试QAOA，考虑两种限制场景：独立（不相关）链路故障和一对边的高度相关故障。两种场景都产生了最优网络设计，对应最高出现频率的有效解和最小能量状态。

Conclusion: 提出的公式能够产生最优路径设计，验证了在未来的量子系统上优化Layer 3路由的可行性。量子计算方法为解决复杂的网络优化问题提供了有前景的途径。

Abstract: Network optimization problems represent large combinatorial search spaces that grow exponentially with network size, making them computationally intensive to solve. This paper addresses the latency-resilient Layer 3 routing optimization problem in telecommunications networks with predefined Layer 1 optical links. We formulate this problem as a graph-based optimization problem with the objective of minimizing latency, creating vertex-disjoint paths from each site to the internet backbone, and maximizing overall resiliency by limiting the impact of dual-link failures. By framing the problem as finding two disjoint shortest paths, coupled together with a resiliency component to the objective function, we establish a single formulation to produce optimal path design. The mathematical formulation was adapted to solve the problem using quantum approximate optimization algorithm (QAOA) executed over both quantum simulator and quantum hardware. QAOA was tested on a toy graph topology with 5 vertices and 7 edges and considering two limiting scenarios respectively representing independent (uncorrelated) link failures and highly correlated failure for one pair of edges. Both explored scenarios produced the optimal network design-corresponding to the valid solution with highest frequency of occurrence and minimum energy state, hence, validating the proposed formulation for optimizing Layer 3 routing on quantum systems of the future.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [5] [Structural shifts in institutional participation and collaboration within the AI arXiv preprint research ecosystem](https://arxiv.org/abs/2602.03969)
*Shama Magnur,Mayank Kejriwal*

Main category: cs.SI

TL;DR: 该研究使用arXiv预印本数据（2021-2025）分析AI研究格局的结构变化，发现ChatGPT推出后论文数量激增，学术机构仍是主要贡献者，但学术界与产业界合作仍低于随机混合基准，表明生成式AI研究的资本密集型特性可能重塑科学合作边界。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的出现代表了科学生态系统的重要技术转变，特别是在人工智能领域。arXiv预印本生态系统已成为实时科学变化的关键指标，通常比正式同行评审发表提前数月或数年。研究旨在分析AI研究格局的结构变化，包括发表量、作者团队规模和学术-产业合作模式的演变。

Method: 使用2021年至2025年arXiv预印本（cs.AI）数据集，采用多阶段数据收集和丰富化流程，结合基于LLM的机构分类方法，分析发表量、作者团队规模和学术-产业合作模式的演变，并使用归一化合作指数（NCI）来衡量合作水平。

Result: 1. ChatGPT推出后论文发表量出现前所未有的激增；2. 学术机构继续提供最大量的研究产出；3. 学术-产业合作仍然受到抑制，所有主要子领域的归一化合作指数（NCI）都显著低于随机混合基准；4. 这表明持续存在的机构鸿沟。

Conclusion: 研究发现AI研究格局存在持续的机构鸿沟，生成式AI研究的资本密集型特性可能正在重塑科学合作的边界。学术-产业合作仍然低于预期水平，这反映了AI研究生态系统的结构性变化。

Abstract: The emergence of large language models (LLMs) represents a significant technological shift within the scientific ecosystem, particularly within the field of artificial intelligence (AI). This paper examines structural changes in the AI research landscape using a dataset of arXiv preprints (cs.AI) from 2021 through 2025. Given the rapid pace of AI development, the preprint ecosystem has become a critical barometer for real-time scientific shifts, often preceding formal peer-reviewed publication by months or years. By employing a multi-stage data collection and enrichment pipeline in conjunction with LLM-based institution classification, we analyze the evolution of publication volumes, author team sizes, and academic--industry collaboration patterns. Our results reveal an unprecedented surge in publication output following the introduction of ChatGPT, with academic institutions continuing to provide the largest volume of research. However, we observe that academic--industry collaboration is still suppressed, as measured by a Normalized Collaboration Index (NCI) that remains significantly below the random-mixing baseline across all major subfields. These findings highlight a continuing institutional divide and suggest that the capital-intensive nature of generative AI research may be reshaping the boundaries of scientific collaboration.

</details>


### [6] [Unmasking Superspreaders: Data-Driven Approaches for Identifying and Comparing Key Influencers of Conspiracy Theories on X.com](https://arxiv.org/abs/2602.04546)
*Florian Kramer,Henrich R. Greve,Moritz von Zahn,Hayagreeva Rao*

Main category: cs.SI

TL;DR: 该研究分析社交媒体上阴谋论传播的两类关键角色：人类超级传播者和机器人，通过分析700万条COVID-19相关推文揭示其行为差异，并提出27个量化指标及基于H指数的识别方法。


<details>
  <summary>Details</summary>
Motivation: 阴谋论通过社交媒体传播会威胁社会，但缺乏对关键传播者（人类超级传播者和机器人）的系统分析和实用识别方法，需要填补这一研究空白。

Method: 分析700多万条COVID-19相关推文，从语言复杂性、毒性、标签使用等维度比较人类超级传播者和机器人的行为差异，提出并评估27个量化阴谋论传播严重性的新指标。

Result: 发现两类传播者的明显差异：人类超级传播者使用更复杂的语言和实质性内容，较少依赖标签和表情符号以增强可信度；机器人则使用更简单的语言和策略性标签交叉使用以提高可访问性和渗透性。改进的H指数能有效识别人类超级传播者。

Conclusion: 通过识别人类超级传播者和机器人的独特行为模式并提供合适的识别方法，为平台审核政策、账户封禁和公众意识运动等缓解策略奠定了基础。

Abstract: Conspiracy theories can threaten society by spreading misinformation, deepening polarization, and eroding trust in democratic institutions. Social media often fuels the spread of conspiracies, primarily driven by two key actors: Superspreaders -- influential individuals disseminating conspiracy content at disproportionately high rates, and Bots -- automated accounts designed to amplify conspiracies strategically. To counter the spread of conspiracy theories, it is critical to both identify these actors and to better understand their behavior. However, a systematic analysis of these actors as well as real-world-applicable identification methods are still lacking. In this study, we leverage over seven million tweets from the COVID-19 pandemic to analyze key differences between Human Superspreaders and Bots across dimensions such as linguistic complexity, toxicity, and hashtag usage. Our analysis reveals distinct communication strategies: Superspreaders tend to use more complex language and substantive content while relying less on structural elements like hashtags and emojis, likely to enhance credibility and authority. By contrast, Bots favor simpler language and strategic cross-usage of hashtags, likely to increase accessibility, facilitate infiltration into trending discussions, and amplify reach. To counter both Human Superspreaders and Bots, we propose and evaluate 27 novel metrics for quantifying the severity of conspiracy theory spread. Our findings highlight the effectiveness of an adapted H-Index for computationally feasible identification of Human Superspreaders. By identifying behavioral patterns unique to Human Superspreaders and Bots as well as providing suitable identification methods, this study provides a foundation for mitigation strategies, including platform moderation policies, temporary and permanent account suspensions, and public awareness campaigns.

</details>


### [7] [Overstating Attitudes, Ignoring Networks: LLM Biases in Simulating Misinformation Susceptibility](https://arxiv.org/abs/2602.04674)
*Eun Cheol Choi,Lindsay E. Young,Emilio Ferrara*

Main category: cs.SI

TL;DR: LLM模拟调查受访者能捕捉人类错误信息信念的分布趋势，但会夸大信念与分享的关联，过度重视态度行为特征而忽略社交网络特征，存在系统性偏差。


<details>
  <summary>Details</summary>
Motivation: 评估LLM作为人类判断代理在计算社会科学中的可靠性，特别是能否准确再现人类对错误信息的易感性模式。

Method: 使用三个在线调查作为基线，让LLM模拟受访者（基于社交网络、人口统计、态度和行为特征的个人资料），比较LLM输出与人类响应的分布和特征-结果关联。

Result: LLM生成的响应能捕捉广泛的分布趋势，与人类响应有适度相关性，但持续夸大信念与分享的关联。模拟响应的线性模型解释方差更高，过度重视态度行为特征而忽略个人网络特征。

Conclusion: LLM基于调查模拟更适合诊断与人类判断的系统性偏差，而非替代人类判断，因为存在错误信息相关概念的表征偏差。

Abstract: Large language models (LLMs) are increasingly used as proxies for human judgment in computational social science, yet their ability to reproduce patterns of susceptibility to misinformation remains unclear. We test whether LLM-simulated survey respondents, prompted with participant profiles drawn from social survey data measuring network, demographic, attitudinal and behavioral features, can reproduce human patterns of misinformation belief and sharing. Using three online surveys as baselines, we evaluate whether LLM outputs match observed response distributions and recover feature-outcome associations present in the original survey data. LLM-generated responses capture broad distributional tendencies and show modest correlation with human responses, but consistently overstate the association between belief and sharing. Linear models fit to simulated responses exhibit substantially higher explained variance and place disproportionate weight on attitudinal and behavioral features, while largely ignoring personal network characteristics, relative to models fit to human responses. Analyses of model-generated reasoning and LLM training data suggest that these distortions reflect systematic biases in how misinformation-related concepts are represented. Our findings suggest that LLM-based survey simulations are better suited for diagnosing systematic divergences from human judgment than for substituting it.

</details>


### [8] [The Needle is a Thread: Finding Planted Paths in Noisy Process Trees](https://arxiv.org/abs/2602.04694)
*Maya Le,Paweł Prałat,Aaron Smith,François Théberge*

Main category: cs.SI

TL;DR: 提出"植入路径"问题，开发用于在树结构中寻找模糊匹配的算法，作为网络安全中恶意软件事件序列检测的基础模块


<details>
  <summary>Details</summary>
Motivation: 网络安全应用需求，特别是从大量计算机日志数据中检测恶意软件相关事件序列

Method: 引入"植入路径"问题，提出在树结构中寻找模糊匹配的算法，作为复杂工作流程的基础模块

Result: 在合成数据和真实ACME网络安全数据集上验证了工作流程的有效性

Conclusion: 该算法可作为网络安全中恶意软件事件序列检测的基础构建模块，具有实际应用价值

Abstract: Motivated by applications in cybersecurity such as finding meaningful sequences of malware-related events buried inside large amounts of computer log data, we introduce the "planted path" problem and propose an algorithm to find fuzzy matchings between two trees. This algorithm can be used as a "building block" for more complicated workflows. We demonstrate usefulness of a few of such workflows in mining synthetically generated data as well as real-world ACME cybersecurity datasets.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [9] [Predicting Depressive Symptoms through Emotion Pairs within Asian American Families](https://arxiv.org/abs/2602.03943)
*Sangpil Youm,Nari Yoo,Sou Hyun Jang*

Main category: cs.CY

TL;DR: 本研究通过BERT模型分析亚裔美国子女在Reddit上的情感表达，发现混合情感对抑郁症状的预测作用，强调情感矛盾性在亲子关系中的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究亚裔美国家庭亲子关系对心理健康的影响，特别关注在线叙事中矛盾情感的作用，以更好理解情感表达与抑郁症状之间的关系。

Method: 使用BERT模型分析r/Asianparentstories子版块内容，在句子层面检测情感，在帖子层面检测抑郁症状，研究多种情感共现模式及其与抑郁症状的关联。

Result: 发现8种主要情感占主导且显著共现；帖子中常包含多种矛盾情感；负面情感组合与抑郁症状正相关，正面情感组合负相关，矛盾情感组合预测结果多样。

Conclusion: 自动化情感分类对理解亲子关系动态很重要，情感矛盾性具有实践和临床意义，需要更细致地考虑混合情感在心理健康评估中的作用。

Abstract: Studies on intergenerational relationships between parents and children in Asian American families highlight their impact on mental health and well-being. This study investigates the role of ambivalent emotions in online narratives shared by Asian and Asian American children on the subreddit, r/Asianparentstories. By employing a BERT-based model to detect emotion at the sentence level and depressive symptoms at the post level, we analyze mixed feelings to better understand how they predict depressive symptoms. First, among 28 detectable, eight (realization, approval, sadness, anger, curiosity, annoyance, disappointment, disapproval) comprise over 50%, exhibiting significant co-occurrence among themselves and with other emotions. Second, we find the co-occurrence of multiple emotions, indicating that emotions in a single post are not limited to consistently positive or negative feelings. Finally, our findings indicate that while negative emotion pairs (e.g., confusion-grief, anger-grief) are associated with depressive symptoms, positive emotion pairs (e.g., admiration-realization, amusement-joy) negatively correlate with depressive symptoms, and combinations of ambivalent emotions indicate varied results in predicting depressive symptoms. These findings highlight the importance of automated emotion classification and the need to consider emotional ambivalence, which holds practical and clinical implications for understanding the dynamics of parent-child relationships.

</details>


### [10] [The CitizenQuery Benchmark: A Novel Dataset and Evaluation Pipeline for Measuring LLM Performance in Citizen Query Tasks](https://arxiv.org/abs/2602.04064)
*Neil Majithia,Rajat Shinde,Zo Chapman,Prajun Trital,Jordan Decker,Manil Maskey,Elena Simperl,Nigel Shadbolt*

Main category: cs.CY

TL;DR: 论文提出了CitizenQuery-UK基准数据集，用于评估LLMs在回答公民政府相关查询时的可信度，发现模型在事实性、弃权频率和冗长度方面存在显著差异，需要承认AI的"易错性"。


<details>
  <summary>Details</summary>
Motivation: 公民查询是个人向政府询问政策、指导和服务的问题，涉及福利、税收、移民等多个重要领域。虽然LLMs有潜力根据用户背景提供个性化回答，但任何错误信息都可能对信任模型的个人产生严重且难以察觉的负面影响。因此需要评估LLMs在此类任务中的可信度。

Method: 1) 创建CitizenQuery-UK基准数据集：从英国政府网站gov.uk的公开信息中合成生成了22,000对公民查询和回答；2) 提出基于FActScore的评估方法：对11个LLMs进行事实性、弃权频率和冗长度的基准测试。

Result: 研究发现：(i) 不同模型家族有独特的性能特征，但都具备竞争力；(ii) 高方差削弱了模型的实用性；(iii) 弃权率低而冗长度高，影响可靠性；(iv) 更可信的AI需要在与用户互动时承认其"易错性"。

Conclusion: 该研究通过完全基于开放数据构建的基准，为AI在公共部门应用的决策提供了更好的证据基础。随着AI日益融入日常生活，评估LLMs在公民查询任务中的可信度至关重要，需要建立承认AI局限性的交互方式。

Abstract: "Citizen queries" are questions asked by an individual about government policies, guidance, and services that are relevant to their circumstances, encompassing a range of topics including benefits, taxes, immigration, employment, public health, and more. This represents a compelling use case for Large Language Models (LLMs) that respond to citizen queries with information that is adapted to a user's context and communicated according to their needs. However, in this use case, any misinformation could have severe, negative, likely invisible ramifications for an individual placing their trust in a model's response.
  To this effect, we introduce CitizenQuery-UK, a benchmark dataset of 22 thousand pairs of citizen queries and responses that have been synthetically generated from the swathes of public information on $gov.uk$ about government in the UK. We present the curation methodology behind CitizenQuery-UK and an overview of its contents. We also introduce a methodology for the benchmarking of LLMs with the dataset, using an adaptation of FActScore to benchmark 11 models for factuality, abstention frequency, and verbosity. We document these results, and interpret them in the context of the public sector, finding that: (i) there are distinct performance profiles across model families, but each is competitive; (ii) high variance undermines utility; (iii) abstention is low and verbosity is high, with implications on reliability; and (iv) more trustworthy AI requires acknowledged "fallibility" in the way it interacts with users.
  The contribution of our research lies in assessing the trustworthiness of LLMs in citizen query tasks; as we see a world of increasing AI integration into day-to-day life, our benchmark, built entirely on open data, lays the foundations for better evidenced decision-making regarding AI and the public sector.

</details>


### [11] [Quantifying Algorithmic Friction in Automated Resume Screening Systems](https://arxiv.org/abs/2602.04087)
*Ibrahim Denis Fofanah*

Main category: cs.CY

TL;DR: 本文提出了一种量化简历自动筛选系统中"算法摩擦"的方法，通过模拟实验比较关键词筛选和语义匹配，发现语义匹配能显著减少误拒率。


<details>
  <summary>Details</summary>
Motivation: 随着自动化简历筛选系统在大规模招聘中的广泛应用，僵化的筛选逻辑可能在人工审核前就排除了合格候选人。先前研究提出了"人工摩擦性失业"概念来描述自动化招聘系统导致的劳动力市场低效问题，本文旨在扩展该框架并聚焦于测量方法。

Method: 提出量化简历筛选管道中算法摩擦的方法：将筛选建模为分类任务，定义摩擦为语义误解导致的过度假阴性拒绝。通过受控模拟，在相同资格条件下比较确定性关键词筛选和向量空间语义匹配。

Result: 结果显示：基于关键词的筛选表现出高水平的算法摩擦，而语义表示能显著减少假阴性拒绝，同时不损害筛选精度。

Conclusion: 通过将算法摩擦视为系统级属性，本研究为评估招聘系统设计如何影响现代劳动力市场匹配效率提供了实证基础，表明语义匹配方法能有效减少自动化筛选中的摩擦性失业。

Abstract: Automated resume screening systems are now a central part of hiring at scale, yet there is growing evidence that rigid screening logic can exclude qualified candidates before human review. In prior work, we introduced the concept of Artificial Frictional Unemployment to describe labor market inefficiencies arising from automated recruitment systems. This paper extends that framework by focusing on measurement. We present a method for quantifying algorithmic friction in resume screening pipelines by modeling screening as a classification task and defining friction as excess false negative rejection caused by semantic misinterpretation. Using controlled simulations, we compare deterministic keyword-based screening with vector-space semantic matching under identical qualification conditions. The results show that keyword-based screening exhibits high levels of algorithmic friction, while semantic representations substantially reduce false negative rejection without compromising precision. By treating algorithmic friction as a system-level property, this study provides an empirical basis for evaluating how recruitment system design affects matching efficiency in modern labor markets.

</details>


### [12] [They Call Her 'Miss' and Him 'Professor': Lived Experiences of Women Teaching Support Staff in IT/SE Education](https://arxiv.org/abs/2602.04332)
*Vasudha Malhotra,Rhea D'silva,Rashina Hoda*

Main category: cs.CY

TL;DR: 该研究通过15位女性教学支持人员的经验，揭示了在IT/SE高等教育中，女性教学支持人员如何获得、抵抗和维护权威，并提出了创建更具包容性教育环境的建议。


<details>
  <summary>Details</summary>
Motivation: 女性教学支持人员在计算教育中扮演关键角色，但她们的贡献常常被忽视和低估。研究旨在揭示她们在日常教学中的权威动态，特别是在技术主导领域中创建更具包容性教育环境的需求。

Method: 采用经验报告方法，综合分析了15位IT/SE高等教育中女性教学支持人员的生活经验，通过访谈收集她们与教学团队同事和学生互动的正面和负面经历。

Result: 研究发现女性教学支持人员需要根据自身特征（如非英语母语、非白人、年轻外表、非永久职位、早期职业）进行不同程度的关系、情感和学科劳动来达到平等。研究提出了针对教学支持人员角色的交叉性"特权与权力轮"框架。

Conclusion: 研究提供了创建更具包容性技术教育环境的可行建议，特别是在全球大学应对后疫情教学模式并寻求建立更具包容性和韧性的学术社区的关键时期。

Abstract: Despite their critical role in shaping student learning in computing education, the contributions of women teaching-support staff (TSS) often go unrecognised and undervalued. In this experience report, we synthesise lived experiences of 15 women TSS in IT/SE higher education to illuminate how authority is earned, resisted, and maintained in everyday teaching. Participants shared both their positive and negative lived experiences associated with finding and losing voice with teaching team colleagues on the one hand, and rewarding connections and gendered friction with students on the other. We map these dynamics onto an intersectional "wheel of privilege and power" tailored to TSS roles. The farther a TSS profile sits from the wheel's center (e.g., non-native English, non-white, younger-seeming, non-permanent, early-career), the more relational, emotional, and disciplinary labour is needed to reach parity. We provide actionable insights and recommendations for creating more inclusive education environments in technology dominant fields that are particularly timely as universities worldwide grapple with post-pandemic teaching models and seek to build more inclusive and resilient academic communities.

</details>


### [13] [Growth First, Care Second? Tracing the Landscape of LLM Value Preferences in Everyday Dilemmas](https://arxiv.org/abs/2602.04456)
*Zhiyi Chen,Eun Cheol Choi,Yingjia Luo,Xinyi Wang,Yulei Xiao,Aizi Yang,Luca Luceri*

Main category: cs.CY

TL;DR: 论文研究了LLM在在线建议场景中的价值权衡偏好，发现LLM系统性地偏向探索与成长价值，而非仁慈与连接价值，存在价值同质化风险。


<details>
  <summary>Details</summary>
Motivation: 随着人们越来越多地从人类同伴和LLM聊天机器人寻求在线建议，这些建议通常涉及在竞争性价值之间进行权衡。研究旨在表征LLM在不同建议寻求情境中如何导航价值权衡，以理解AI中介建议可能带来的价值同质化风险。

Method: 1. 使用四个建议导向的subreddit数据集，通过自下而上的方法构建分层价值框架；2. 构建价值共现网络来表征价值在困境中的共现模式；3. 评估LLM在这些困境中的价值偏好，比较不同模型和情境下的价值取向。

Result: 1. 不同建议情境的价值权衡结构存在显著异质性：女性subreddit网络密度最高，表明更复杂的价值冲突；女性、男性和友谊相关subreddit围绕安全相关紧张关系形成高度相关的价值冲突模式；职业建议则形成独特结构，安全常与自我实现和成长冲突。2. LLM在所有模型和情境中一致优先考虑探索与成长价值，而非仁慈与连接价值。

Conclusion: LLM在价值权衡中存在系统性的偏向性价值取向，这种价值同质化风险可能在大规模上塑造决策和规范结果，对AI中介建议系统的发展提出了重要关切。

Abstract: People increasingly seek advice online from both human peers and large language model (LLM)-based chatbots. Such advice rarely involves identifying a single correct answer; instead, it typically requires navigating trade-offs among competing values. We aim to characterize how LLMs navigate value trade-offs across different advice-seeking contexts. First, we examine the value trade-off structure underlying advice seeking using a curated dataset from four advice-oriented subreddits. Using a bottom-up approach, we inductively construct a hierarchical value framework by aggregating fine-grained values extracted from individual advice options into higher-level value categories. We construct value co-occurrence networks to characterize how values co-occur within dilemmas and find substantial heterogeneity in value trade-off structures across advice-seeking contexts: a women-focused subreddit exhibits the highest network density, indicating more complex value conflicts; women's, men's, and friendship-related subreddits exhibit highly correlated value-conflict patterns centered on security-related tensions (security vs. respect/connection/commitment); by contrast, career advice forms a distinct structure where security frequently clashes with self-actualization and growth. We then evaluate LLM value preferences against these dilemmas and find that, across models and contexts, LLMs consistently prioritize values related to Exploration & Growth over Benevolence & Connection. This systemically skewed value orientation highlights a potential risk of value homogenization in AI-mediated advice, raising concerns about how such systems may shape decision-making and normative outcomes at scale.

</details>


### [14] [Fine-grained Classification of A Million Life Trajectories from Wikipedia](https://arxiv.org/abs/2602.04503)
*Zhaoyang Liu,Xiaocong Du,Yixi Zhou,Ye Shi,Haipeng Zhang*

Main category: cs.CY

TL;DR: 该论文提出一种从维基百科提取细粒度人生轨迹的方法，通过结合句法图和文本嵌入对活动类型进行分类，构建了包含380万标注活动的大规模数据集


<details>
  <summary>Details</summary>
Motivation: 当前研究仅覆盖有限活动类型（如出生和死亡），缺乏大规模细粒度的人生轨迹数据。人生轨迹对人类动力学研究至关重要，但现有数据不够全面

Method: 1. 从维基百科提取（人物、时间、地点）三元组；2. 使用句法图将分散的三元组实体和相关信息聚合；3. 结合文本嵌入对24种活动类型进行分类；4. 利用LLM优化文本质量以生成标准化句法图

Result: 1. 分类准确率达到84.5%，超越基线方法；2. 构建了最大细粒度人生轨迹数据集：包含380万标注活动，涉及589,193人，时间跨度3个世纪；3. 代码和数据已公开

Conclusion: 该方法成功构建了大规模细粒度人生轨迹数据集，能够支持跨时空的人类动力学宏大叙事研究，为理解人类行为模式提供了重要数据基础

Abstract: Life trajectories of notable people convey essential messages for human dynamics research. These trajectories consist of (\textit{person, time, location, activity type}) tuples recording when and where a person was born, went to school, started a job, or fought in a war. However, current studies only cover limited activity types such as births and deaths, lacking large-scale fine-grained trajectories. Using a tool that extracts (\textit{person, time, location}) triples from Wikipedia, we formulate the problem of classifying these triples into 24 carefully-defined types using textual context as complementary information. The challenge is that triple entities are often scattered in noisy contexts. We use syntactic graphs to bring triple entities and relevant information closer, fusing them with text embeddings to classify life trajectory activities. Since Wikipedia text quality varies, we use LLMs to refine the text for more standardized syntactic graphs. Our framework achieves 84.5\% accuracy, surpassing baselines. We construct the largest fine-grained life trajectory dataset with 3.8 million labeled activities for 589,193 individuals spanning 3 centuries. In the end, we showcase how these trajectories can support grand narratives of human dynamics across time and space. Code/data are publicly available.

</details>


### [15] [Learning the Value Systems of Agents with Preference-based and Inverse Reinforcement Learning](https://arxiv.org/abs/2602.04518)
*Andrés Holgado-Sánchez,Holger Billhardt,Alberto Fernández,Sascha Ossowski*

Main category: cs.CY

TL;DR: 提出一种从观察和人类演示中自动学习价值系统的新方法，基于多目标马尔可夫决策过程，使用偏好学习和逆强化学习算法来推断价值基础函数和价值系统。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统发展，协议技术需要与伦理原则和道德价值保持一致，但不同用户可能持有不同的价值系统，且难以在计算层面具体化价值含义。现有基于人工规范的方法（如价值调查）受限于人工调制的规模。

Method: 提出价值系统学习问题的形式化模型，基于多目标马尔可夫决策过程将其实例化到序列决策领域，使用偏好学习和逆强化学习算法来推断价值基础函数和价值系统。

Result: 通过两个模拟用例进行说明和评估，展示了该方法能够从观察和演示中自动学习价值系统。

Conclusion: 提出了一种自动学习价值系统的新方法，解决了传统方法依赖人工规范的局限性，为协议技术中的价值对齐问题提供了计算解决方案。

Abstract: Agreement Technologies refer to open computer systems in which autonomous software agents interact with one another, typically on behalf of humans, in order to come to mutually acceptable agreements. With the advance of AI systems in recent years, it has become apparent that such agreements, in order to be acceptable to the involved parties, must remain aligned with ethical principles and moral values. However, this is notoriously difficult to ensure, especially as different human users (and their software agents) may hold different value systems, i.e. they may differently weigh the importance of individual moral values. Furthermore, it is often hard to specify the precise meaning of a value in a particular context in a computational manner. Methods to estimate value systems based on human-engineered specifications, e.g. based on value surveys, are limited in scale due to the need for intense human moderation. In this article, we propose a novel method to automatically \emph{learn} value systems from observations and human demonstrations. In particular, we propose a formal model of the \emph{value system learning} problem, its instantiation to sequential decision-making domains based on multi-objective Markov decision processes, as well as tailored preference-based and inverse reinforcement learning algorithms to infer value grounding functions and value systems. The approach is illustrated and evaluated by two simulated use cases.

</details>


### [16] [Inference-Time Reasoning Selectively Reduces Implicit Social Bias in Large Language Models](https://arxiv.org/abs/2602.04742)
*Molly Apsel,Michael N. Jones*

Main category: cs.CY

TL;DR: 推理能力能显著降低某些LLM在IAT式评估中的隐性偏见，但仅限社会偏见领域，对非社会隐性关联无效


<details>
  <summary>Details</summary>
Motivation: 基于心理学理论，研究推理能力如何影响LLM的隐性偏见，探索推理与对齐程序如何相互作用导致不同模型类型的偏见差异

Method: 使用IAT式评估方法，在15个刻板印象主题上测试不同模型类别，比较启用推理前后的隐性偏见变化，并区分社会与非社会领域

Result: 启用推理显著降低了某些模型类别在IAT式评估中的隐性偏见测量值，但这种效果仅限于社会偏见领域，对非社会隐性关联没有相应减少

Conclusion: 推理能力能改变LLM的公平性评估结果，同时提出了对齐程序与推理时间推理如何相互作用的问题，认知科学理论能为AI评估提供方法论和解释框架

Abstract: Drawing on constructs from psychology, prior work has identified a distinction between explicit and implicit bias in large language models (LLMs). While many LLMs undergo post-training alignment and safety procedures to avoid expressions of explicit social bias, they still exhibit significant implicit biases on indirect tasks resembling the Implicit Association Test (IAT). Recent work has further shown that inference-time reasoning can impair LLM performance on tasks that rely on implicit statistical learning. Motivated by a theoretical link between implicit associations and statistical learning in human cognition, we examine how reasoning-enabled inference affects implicit bias in LLMs. We find that enabling reasoning significantly reduces measured implicit bias on an IAT-style evaluation for some model classes across fifteen stereotype topics. This effect appears specific to social bias domains, as we observe no corresponding reduction for non-social implicit associations. As reasoning is increasingly enabled by default in deployed LLMs, these findings suggest that it can meaningfully alter fairness evaluation outcomes in some systems, while also raising questions about how alignment procedures interact with inference-time reasoning to drive variation in bias reduction across model types. More broadly, this work highlights how theory from cognitive science and psychology can complement AI evaluation research by providing methodological and interpretive frameworks that reveal new insights into model behavior.

</details>


### [17] [How to Stop Playing Whack-a-Mole: Mapping the Ecosystem of Technologies Facilitating AI-Generated Non-Consensual Intimate Images](https://arxiv.org/abs/2602.04759)
*Michelle L. Ding,Harini Suresh,Suresh Venkatasubramanian*

Main category: cs.CY

TL;DR: 该论文提出了首个全面的AIG-NCII（AI生成非自愿亲密图像）技术生态系统框架，将相关技术分为11个类别，帮助利益相关者理解危害事件并评估干预措施。


<details>
  <summary>Details</summary>
Motivation: 生成式AI技术的快速发展导致AIG-NCII（主要伤害妇女和女童的图像性虐待形式）可及性大幅提升。现有应对措施分散且缺乏统一框架，导致干预措施孤立、难以评估和比较，形成"打地鼠"式的被动应对循环。

Method: 1. 通过综合研究人员、记者、倡导者、政策制定者和技术专家的100多个主要来源，构建并可视化AIG-NCII技术生态系统
2. 将技术分为11个类别：创作、分发、传播与发现、基础设施支持和货币化等
3. 通过案例研究（包括Grok案例）展示生态系统如何帮助理解新的危害事件
4. 通过三个案例研究展示如何评估现有干预措施

Result: 开发了首个全面的AIG-NCII技术生态系统框架，将相关技术系统性地分为11个类别。通过案例研究证明该框架能有效帮助利益相关者：1）理解新的危害事件（如Grok案例）；2）评估现有干预措施的有效性。

Conclusion: 提出三项可操作建议：1）利用生态系统框架梳理州、联邦和国际法律，形成更清晰的政策格局；2）共同开发动态跟踪生态系统中11类技术的数据库，以更好评估干预措施；3）采用关系性研究方法，更好地理解生态系统技术之间的相互作用。

Abstract: The last decade has witnessed a rapid advancement of generative AI technology that significantly scaled the accessibility of AI-generated non-consensual intimate images (AIG-NCII), a form of image-based sexual abuse that disproportionately harms women and girls. There is a patchwork of commendable efforts across industry, policy, academia, and civil society to address AIG-NCII. However, these efforts lack a shared, consistent mental model that situates the technologies they target within the context of a large, interconnected, and ever-evolving technological ecosystem. As a result, interventions remain siloed and are difficult to evaluate and compare, leading to a reactive cycle of whack-a-mole. We contribute the first comprehensive AIG-NCII technological ecosystem that maps and taxonomizes 11 categories of technologies facilitating the creation, distribution, proliferation and discovery, infrastructural support, and monetization of AIG-NCII. First, we build and visualize the ecosystem through a synthesis of over a hundred primary sources from researchers, journalists, advocates, policymakers, and technologists. Next, we demonstrate how stakeholders can use the ecosystem as a tool to 1) understand new incidents of harm via a case study of Grok and 2) evaluate existing interventions via three more case studies. We conclude with three actionable recommendations, namely that stakeholders should 1) use the ecosystem to map out state, federal, and international laws to produce a clearer policy landscape, 2) collectively develop a database that dynamically tracks the 11 technologies in the ecosystem to better evaluate interventions, and 3) adopt a relational approach to researching AIG-NCII to better understand how the ecosystem technologies interact.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [18] [Knowledge Model Prompting Increases LLM Performance on Planning Tasks](https://arxiv.org/abs/2602.03900)
*Erik Goh,John Kos,Ashok Goel*

Main category: cs.AI

TL;DR: TMK框架提示显著提升LLM在符号规划任务上的性能，从31.5%提升至97.3%，通过引导模型从语言模式转向形式化代码执行路径。


<details>
  <summary>Details</summary>
Motivation: 现有提示技术如CoT在LLM推理能力上存在局限，需要更有效的框架来提升LLM的规划和推理能力。TMK框架因其能捕捉因果、目的论和层次化推理结构，以及明确的任务分解机制，有望解决语言模型的推理缺陷。

Method: 采用Task-Method-Knowledge (TMK)框架进行结构化提示，在PlanBench基准测试的Blocksworld领域进行实验，评估TMK是否能帮助语言模型将复杂规划问题分解为可管理的子任务。

Result: TMK提示使推理模型在不透明的符号任务（PlanBench中Blocksworld的随机版本）上准确率从31.5%提升至97.3%，表现出显著的性能反转，并揭示了模型从语言模式转向形式化代码执行路径的机制。

Conclusion: TMK不仅提供上下文，更重要的是作为一种机制，引导推理模型远离默认的语言模式，在实验中激活形式化、代码执行的路径，从而在符号操作任务上实现突破性性能提升。

Abstract: Large Language Models (LLM) can struggle with reasoning ability and planning tasks. Many prompting techniques have been developed to assist with LLM reasoning, notably Chain-of-Thought (CoT); however, these techniques, too, have come under scrutiny as LLMs' ability to reason at all has come into question. Borrowing from the domain of cognitive and educational science, this paper investigates whether the Task-Method-Knowledge (TMK) framework can improve LLM reasoning capabilities beyond its previously demonstrated success in educational applications. The TMK framework's unique ability to capture causal, teleological, and hierarchical reasoning structures, combined with its explicit task decomposition mechanisms, makes it particularly well-suited for addressing language model reasoning deficiencies, and unlike other hierarchical frameworks such as HTN and BDI, TMK provides explicit representations of not just what to do and how to do it, but also why actions are taken. The study evaluates TMK by experimenting on the PlanBench benchmark, focusing on the Blocksworld domain to test for reasoning and planning capabilities, examining whether TMK-structured prompting can help language models better decompose complex planning problems into manageable sub-tasks. Results also highlight significant performance inversion in reasoning models. TMK prompting enables the reasoning model to achieve up to an accuracy of 97.3\% on opaque, symbolic tasks (Random versions of Blocksworld in PlanBench) where it previously failed (31.5\%), suggesting the potential to bridge the gap between semantic approximation and symbolic manipulation. Our findings suggest that TMK functions not merely as context, but also as a mechanism that steers reasoning models away from their default linguistic modes to engage formal, code-execution pathways in the context of the experiments.

</details>


### [19] [Enhancing Mathematical Problem Solving in LLMs through Execution-Driven Reasoning Augmentation](https://arxiv.org/abs/2602.03950)
*Aditya Basarkar,Benyamin Tabarsi,Tiffany Barnes,Dongkuan,Xu*

Main category: cs.AI

TL;DR: IIPC是一种迭代改进的程序构造方法，通过结合执行反馈和链式思维来提升数学推理能力，在多个基准测试中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体LLM系统在数学推理方面仍缺乏可靠可修正的推理过程表示，要么采用僵化的顺序流程无法修正早期错误，要么依赖可能失效的启发式自评估。此外，程序化上下文可能分散语言模型的注意力并降低准确性。

Method: IIPC（迭代改进的程序构造）方法迭代地精炼程序化推理链，将执行反馈与基础LLM的原生链式思维能力相结合，以保持高层次上下文聚焦。

Result: IIPC在多个基础LLM上的大多数推理基准测试中超越了竞争方法。

Conclusion: IIPC通过迭代精炼程序化推理链和结合执行反馈，有效提升了数学推理的可靠性和准确性，相关代码已开源发布。

Abstract: Mathematical problem solving is a fundamental benchmark for assessing the reasoning capabilities of artificial intelligence and a gateway to applications in education, science, and engineering where reliable symbolic reasoning is essential. Although recent advances in multi-agent LLM-based systems have enhanced their mathematical reasoning capabilities, they still lack a reliably revisable representation of the reasoning process. Existing agents either operate in rigid sequential pipelines that cannot correct earlier steps or rely on heuristic self-evaluation that can fail to identify and fix errors. In addition, programmatic context can distract language models and degrade accuracy. To address these gaps, we introduce Iteratively Improved Program Construction (IIPC), a reasoning method that iteratively refines programmatic reasoning chains and combines execution feedback with the native Chain-of-thought abilities of the base LLM to maintain high-level contextual focus. IIPC surpasses competing approaches in the majority of reasoning benchmarks on multiple base LLMs. All code and implementations are released as open source.

</details>


### [20] [AgentArk: Distilling Multi-Agent Intelligence into a Single LLM Agent](https://arxiv.org/abs/2602.03955)
*Yinyi Luo,Yiqiao Jin,Weichen Yu,Mengqi Zhang,Srijan Kumar,Xiaoxiao Li,Weijie Xu,Xin Chen,Jindong Wang*

Main category: cs.AI

TL;DR: AgentArk通过知识蒸馏将多智能体系统的协作推理能力压缩到单个模型中，在保持推理性能的同时显著降低计算成本


<details>
  <summary>Details</summary>
Motivation: 当前LLM多智能体系统虽然通过迭代辩论获得了优越的推理性能，但实际部署受到高计算成本和错误传播的限制，需要更高效的解决方案

Method: 提出AgentArk框架，采用三种分层蒸馏策略：推理增强微调、基于轨迹的数据增强、过程感知蒸馏，将多智能体动态压缩到单个模型的权重中

Result: 蒸馏后的模型在保持单智能体计算效率的同时，展现出多智能体系统的强推理和自我纠正能力，并在多样化推理任务中表现出增强的鲁棒性和泛化能力

Conclusion: 通过将计算负担从推理转移到训练，AgentArk为高效、鲁棒的多智能体系统开发提供了新方向，有望推动未来研究

Abstract: While large language model (LLM) multi-agent systems achieve superior reasoning performance through iterative debate, practical deployment is limited by their high computational cost and error propagation. This paper proposes AgentArk, a novel framework to distill multi-agent dynamics into the weights of a single model, effectively transforming explicit test-time interactions into implicit model capabilities. This equips a single agent with the intelligence of multi-agent systems while remaining computationally efficient. Specifically, we investigate three hierarchical distillation strategies across various models, tasks, scaling, and scenarios: reasoning-enhanced fine-tuning; trajectory-based augmentation; and process-aware distillation. By shifting the burden of computation from inference to training, the distilled models preserve the efficiency of one agent while exhibiting strong reasoning and self-correction performance of multiple agents. They further demonstrate enhanced robustness and generalization across diverse reasoning tasks. We hope this work can shed light on future research on efficient and robust multi-agent development. Our code is at https://github.com/AIFrontierLab/AgentArk.

</details>


### [21] [Active Epistemic Control for Query-Efficient Verified Planning](https://arxiv.org/abs/2602.03974)
*Shuhui Qu*

Main category: cs.AI

TL;DR: AEC是一种主动认知控制规划方法，通过分离事实存储与信念存储，结合环境查询与模拟预测，在部分可观测环境中实现高效规划。


<details>
  <summary>Details</summary>
Motivation: 在部分可观测的交互环境中，任务关键前提条件（如物体位置或容器状态）在决策时可能未知，而通过交互获取这些信息成本高昂。学习的世界模型可以廉价预测缺失事实，但预测错误可能无声地导致不可行的承诺。

Method: 提出主动认知控制(AEC)，这是一种认知-分类规划层，将基于模型的信念管理与分类可行性检查相结合。AEC严格分离用于承诺的接地事实存储和仅用于剪枝候选计划的信念存储。在每个步骤中，当不确定性高或预测模糊时查询环境以接地未解析谓词，或在置信度足够时模拟谓词以过滤假设。最终承诺通过接地前提条件覆盖和SQ-BCP拉回式兼容性检查来把关。

Result: 在ALFWorld和ScienceWorld上的实验表明，AEC在比强大的LLM智能体基线更少的重新规划轮次中实现了竞争性的成功率。

Conclusion: AEC通过在规划中严格分离事实与信念，结合主动环境查询与模拟预测，有效解决了部分可观测环境中的规划问题，减少了重新规划需求，提高了规划效率。

Abstract: Planning in interactive environments is challenging under partial observability: task-critical preconditions (e.g., object locations or container states) may be unknown at decision time, yet grounding them through interaction is costly. Learned world models can cheaply predict missing facts, but prediction errors can silently induce infeasible commitments. We present \textbf{Active Epistemic Control (AEC)}, an epistemic-categorical planning layer that integrates model-based belief management with categorical feasibility checks. AEC maintains a strict separation between a \emph{grounded fact store} used for commitment and a \emph{belief store} used only for pruning candidate plans. At each step, it either queries the environment to ground an unresolved predicate when uncertainty is high or predictions are ambiguous, or simulates the predicate to filter hypotheses when confidence is sufficient. Final commitment is gated by grounded precondition coverage and an SQ-BCP pullback-style compatibility check, so simulated beliefs affect efficiency but cannot directly certify feasibility. Experiments on ALFWorld and ScienceWorld show that AEC achieves competitive success with fewer replanning rounds than strong LLM-agent baselines.

</details>


### [22] [Adaptive Test-Time Compute Allocation via Learned Heuristics over Categorical Structure](https://arxiv.org/abs/2602.03975)
*Shuhui Qu*

Main category: cs.AI

TL;DR: 提出一种状态级选择性验证框架，在验证成本受限情况下优化验证资源分配，相比传统方法在MATH基准上减少44%验证调用同时获得更高准确率


<details>
  <summary>Details</summary>
Motivation: 大型语言模型推理中的测试时计算越来越受昂贵验证的瓶颈限制，许多推理系统将大量验证调用浪费在冗余或无前景的中间假设上，需要在验证成本受限情况下研究如何优化验证资源分配

Method: 提出状态级选择性验证框架，包含三个组件：1) 结构化移动接口上的确定性可行性门控；2) 结合学习状态距离和残差得分的预验证排序；3) 基于局部不确定性的验证调用自适应分配

Result: 在MATH基准测试中，该方法相比best-of-N、多数投票和束搜索获得更高准确率，同时减少44%的验证调用

Conclusion: 该研究展示了在验证成本受限情况下，通过智能分配验证资源到最信息丰富的中间状态，可以显著提高推理效率，为大型语言模型推理系统提供了更高效的验证策略

Abstract: Test-time computation has become a primary driver of progress in large language model (LLM) reasoning, but it is increasingly bottlenecked by expensive verification. In many reasoning systems, a large fraction of verifier calls are spent on redundant or unpromising intermediate hypotheses. We study reasoning under a \emph{verification-cost-limited} setting and ask how verification effort should be allocated across intermediate states. We propose a state-level selective verification framework that combines (i) deterministic feasibility gating over a structured move interface, (ii) pre-verification ranking using a hybrid of learned state-distance and residual scoring, and (iii) adaptive allocation of verifier calls based on local uncertainty. Unlike solution-level best-of-$N$ or uniform intermediate verification, our method distributes verification where it is most informative. On the \textsc{MATH} benchmark, our approach achieves higher accuracy than best-of-$N$, majority voting, and beam search while using 44\% fewer verifier calls.

</details>


### [23] [Monitorability as a Free Gift: How RLVR Spontaneously Aligns Reasoning](https://arxiv.org/abs/2602.03978)
*Zidi Xiong,Shan Chen,Himabindu Lakkaraju*

Main category: cs.AI

TL;DR: RLVR训练早期监控性看似"免费礼物"，但实际强烈依赖数据多样性和指令跟随数据，与能力提升正交，主要源于响应分布锐化和对提示的关注增强。


<details>
  <summary>Details</summary>
Motivation: 随着大型推理模型部署增加，审计其思维链的安全性变得至关重要。研究发现RLVR早期阶段监控性（思维链忠实反映内部计算的程度）看似"免费获得"，但需要系统评估其普遍性和影响因素。

Method: 通过跨模型家族和训练领域的系统评估，分析数据多样性、指令跟随数据的作用，进行机制分析（响应分布锐化、注意力变化），并控制训练和评估难度研究监控性动态。

Result: 监控性改进并非普遍现象，强烈依赖数据多样性；监控性与能力提升正交；机制上主要源于响应分布熵减少和对提示的关注增强，而非对推理链的因果依赖；监控性动态随训练和评估难度变化。

Conclusion: 研究提供了RLVR下监控性出现的整体视图，明确了监控性增益何时可能发生、何时不会发生，强调了数据质量和类型的关键作用，澄清了监控性与能力提升的独立性。

Abstract: As Large Reasoning Models (LRMs) are increasingly deployed, auditing their chain-of-thought (CoT) traces for safety becomes critical. Recent work has reported that monitorability--the degree to which CoT faithfully and informatively reflects internal computation--can appear as a "free gift" during the early stages of Reinforcement Learning with Verifiable Rewards (RLVR). We make this observation concrete through a systematic evaluation across model families and training domains. Our results show that this effect is not universal: monitorability improvements are strongly data-dependent. In particular, we demonstrate the critical role of data diversity and instruction-following data during RLVR training. We further show that monitorability is orthogonal to capability--improvements in reasoning performance do not imply increased transparency. Through mechanistic analysis, we attribute monitorability gains primarily to response distribution sharpening (entropy reduction) and increased attention to the prompt, rather than stronger causal reliance on reasoning traces. We also reveal how monitorability dynamics vary with controlled training and evaluation difficulty. Together, these findings provide a holistic view of how monitorability emerges under RLVR, clarifying when gains are likely to occur and when they are not.

</details>


### [24] [When AI Persuades: Adversarial Explanation Attacks on Human Trust in AI-Assisted Decision Making](https://arxiv.org/abs/2602.04003)
*Shutong Fan,Lan Zhang,Xiaoyong Yuan*

Main category: cs.AI

TL;DR: 论文提出对抗性解释攻击(AEAs)，攻击者通过操纵LLM生成解释的框架来调节人类对错误输出的信任，揭示了AI与用户之间认知层面的新攻击面。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统越来越多地在人类决策循环中运行，用户依赖模型推荐做出决策。LLM生成的自然语言解释会影响用户对AI输出的感知和信任，这揭示了AI与用户之间认知层面的新攻击面。目前大多数对抗性威胁针对模型的计算行为，而非依赖模型的人类用户。

Method: 引入对抗性解释攻击(AEAs)，通过信任校准差距来形式化行为威胁。进行控制实验(n=205)，系统变化解释框架的四个维度：推理模式、证据类型、沟通风格和呈现格式。量化对抗性解释对人类信任的影响。

Result: 用户对对抗性和良性解释报告的信任度几乎相同，对抗性解释尽管错误但仍保留了大部分良性信任。最脆弱的案例出现在AEA模仿专家沟通时，结合权威证据、中性语气和领域适当的推理。在困难任务、事实驱动领域以及教育程度较低、较年轻或高度信任AI的参与者中脆弱性最高。

Conclusion: 这是首个将解释视为对抗性认知通道并量化其对AI辅助决策中人类信任影响的系统性安全研究。揭示了LLM解释可能被操纵来维持用户对错误输出的信任，提出了AI安全中认知层面的新威胁。

Abstract: Most adversarial threats in artificial intelligence target the computational behavior of models rather than the humans who rely on them. Yet modern AI systems increasingly operate within human decision loops, where users interpret and act on model recommendations. Large Language Models generate fluent natural-language explanations that shape how users perceive and trust AI outputs, revealing a new attack surface at the cognitive layer: the communication channel between AI and its users. We introduce adversarial explanation attacks (AEAs), where an attacker manipulates the framing of LLM-generated explanations to modulate human trust in incorrect outputs. We formalize this behavioral threat through the trust miscalibration gap, a metric that captures the difference in human trust between correct and incorrect outputs under adversarial explanations. By incorporating this gap, AEAs explore the daunting threats in which persuasive explanations reinforce users' trust in incorrect predictions. To characterize this threat, we conducted a controlled experiment (n = 205), systematically varying four dimensions of explanation framing: reasoning mode, evidence type, communication style, and presentation format. Our findings show that users report nearly identical trust for adversarial and benign explanations, with adversarial explanations preserving the vast majority of benign trust despite being incorrect. The most vulnerable cases arise when AEAs closely resemble expert communication, combining authoritative evidence, neutral tone, and domain-appropriate reasoning. Vulnerability is highest on hard tasks, in fact-driven domains, and among participants who are less formally educated, younger, or highly trusting of AI. This is the first systematic security study that treats explanations as an adversarial cognitive channel and quantifies their impact on human trust in AI-assisted decision making.

</details>


### [25] [Axiomatic Foundations of Counterfactual Explanations](https://arxiv.org/abs/2602.04028)
*Leila Amgoud,Martin Cooper*

Main category: cs.AI

TL;DR: 该论文提出了一个关于反事实解释器的公理化框架，证明了不可能性定理，识别了五种不同类型的反事实解释，并将现有解释器纳入分类体系。


<details>
  <summary>Details</summary>
Motivation: 当前大多数反事实解释器只关注单一类型的反事实和局部解释，缺乏对替代反事实类型的系统研究，也没有研究能够揭示系统整体推理过程的全局反事实。

Method: 引入基于一组理想属性的公理化框架，证明不可能性定理，建立表示定理，识别五种不同类型的反事实解释，并将现有解释器纳入分类体系。

Result: 证明了没有单一解释器能同时满足某些公理组合，建立了公理子集与解释器家族之间的一一对应关系，识别了五种根本不同的反事实类型，包括局部和全局解释。

Conclusion: 该框架为反事实解释提供了系统化的理论基础，揭示了反事实解释的多样性，为理解和设计更全面的解释系统提供了指导。

Abstract: Explaining autonomous and intelligent systems is critical in order to improve trust in their decisions. Counterfactuals have emerged as one of the most compelling forms of explanation. They address ``why not'' questions by revealing how decisions could be altered. Despite the growing literature, most existing explainers focus on a single type of counterfactual and are restricted to local explanations, focusing on individual instances. There has been no systematic study of alternative counterfactual types, nor of global counterfactuals that shed light on a system's overall reasoning process.
  This paper addresses the two gaps by introducing an axiomatic framework built on a set of desirable properties for counterfactual explainers. It proves impossibility theorems showing that no single explainer can satisfy certain axiom combinations simultaneously, and fully characterizes all compatible sets. Representation theorems then establish five one-to-one correspondences between specific subsets of axioms and the families of explainers that satisfy them. Each family gives rise to a distinct type of counterfactual explanation, uncovering five fundamentally different types of counterfactuals. Some of these correspond to local explanations, while others capture global explanations. Finally, the framework situates existing explainers within this taxonomy, formally characterizes their behavior, and analyzes the computational complexity of generating such explanations.

</details>


### [26] [Scaling In-Context Online Learning Capability of LLMs via Cross-Episode Meta-RL](https://arxiv.org/abs/2602.04089)
*Xiaofeng Lin,Sirou Zhu,Yilei Chen,Mingyu Chen,Hejian Sang,Ioannis Paschalidis,Zhipeng Wang,Aldo Pacchiano,Xuezhou Zhang*

Main category: cs.AI

TL;DR: ORBIT框架通过元强化学习训练LLMs在上下文中从交互中学习，显著提升了模型在未见环境中的在线学习能力


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在需要在线交互的决策任务中表现不佳，无法可靠利用上下文交互经验，需要平衡信息收集和利用

Method: 提出ORBIT框架：多任务、多回合的元强化学习框架，训练LLMs在上下文中从交互中学习

Result: 经过元训练后，相对较小的开源模型(Qwen3-14B)在全新环境中表现出显著改进的在线学习能力，匹配GPT-5.2性能，大幅优于标准RL微调

Conclusion: 通过训练可以解决LLMs在线学习能力的限制，模型规模扩展实验显示持续增益，表明学习型推理时决策代理有巨大潜力

Abstract: Large language models (LLMs) achieve strong performance when all task-relevant information is available upfront, as in static prediction and instruction-following problems. However, many real-world decision-making tasks are inherently online: crucial information must be acquired through interaction, feedback is delayed, and effective behavior requires balancing information collection and exploitation over time. While in-context learning enables adaptation without weight updates, existing LLMs often struggle to reliably leverage in-context interaction experience in such settings. In this work, we show that this limitation can be addressed through training. We introduce ORBIT, a multi-task, multi-episode meta-reinforcement learning framework that trains LLMs to learn from interaction in context. After meta-training, a relatively small open-source model (Qwen3-14B) demonstrates substantially improved in-context online learning on entirely unseen environments, matching the performance of GPT-5.2 and outperforming standard RL fine-tuning by a large margin. Scaling experiments further reveal consistent gains with model size, suggesting significant headroom for learn-at-inference-time decision-making agents. Code reproducing the results in the paper can be found at https://github.com/XiaofengLin7/ORBIT.

</details>


### [27] [Interfaze: The Future of AI is built on Task-Specific Small Models](https://arxiv.org/abs/2602.04101)
*Harsha Vardhan Khurdula,Vineet Agarwal,Yoeven D Khemlani*

Main category: cs.AI

TL;DR: Interfaze是一个将LLM应用视为上下文构建与执行问题的系统，通过异构DNN堆栈、上下文构建层和行动层，结合小型模型和工具，将大部分计算从昂贵的大型模型中转移出来。


<details>
  <summary>Details</summary>
Motivation: 现代LLM应用不应仅依赖单一大型模型，而应视为上下文构建与执行问题。需要将复杂任务分解为感知、上下文构建和行动执行，以降低对昂贵大型模型的依赖，提高效率和准确性。

Method: 系统包含三个核心组件：(1) 异构DNN堆栈配对小语言模型作为感知模块，处理复杂PDF、图表、多语言ASR等；(2) 上下文构建层爬取、索引、解析外部资源为结构化状态；(3) 行动层支持浏览、检索、沙箱代码执行、无头浏览器驱动。顶层控制器决定使用哪些小型模型和行动，并将提炼的上下文转发给用户选择的LLM生成最终响应。

Result: Interfaze-Beta在多个基准测试中表现优异：MMLU-Pro 83.6%、MMLU 91.4%、GPQA-Diamond 81.3%、LiveCodeBench v5 57.8%、AIME-2025 90.0%，在多模态任务上也有强劲表现。大多数查询主要由小型模型和工具堆栈处理，大型LLM仅操作提炼后的上下文，实现了竞争性准确度同时将大部分计算从昂贵的大型模型中转移。

Conclusion: 通过将LLM应用重新定义为上下文构建与执行问题，结合异构小型模型和工具堆栈，可以在保持竞争性性能的同时显著降低对昂贵大型模型的依赖，为更高效、可扩展的AI系统架构提供了新思路。

Abstract: We present Interfaze, a system that treats modern LLM applications as a problem of building and acting over context, not just picking the right monolithic model. Instead of a single transformer, we combine (i) a stack of heterogeneous DNNs paired with small language models as perception modules for OCR involving complex PDFs, charts and diagrams, and multilingual ASR with (ii) a context-construction layer that crawls, indexes, and parses external sources (web pages, code, PDFs) into compact structured state, and (iii) an action layer that can browse, retrieve, execute code in a sandbox, and drive a headless browser for dynamic web pages. A thin controller sits on top of this stack and exposes a single, OpenAI-style endpoint: it decides which small models and actions to run and always forwards the distilled context to a user-selected LLM that produces the final response.
  On this architecture, Interfaze-Beta achieves 83.6% on MMLU-Pro, 91.4% on MMLU, 81.3% on GPQA-Diamond, 57.8% on LiveCodeBench v5, and 90.0% on AIME-2025, along with strong multimodal scores on MMMU (val) (77.3%), AI2D (91.5%), ChartQA (90.9%), and Common Voice v16 (90.8%). We show that most queries are handled primarily by the small-model and tool stack, with the large LLM operating only on distilled context, yielding competitive accuracy while shifting the bulk of computation away from the most expensive and monolithic models.

</details>


### [28] [Agentic AI in Healthcare & Medicine: A Seven-Dimensional Taxonomy for Empirical Evaluation of LLM-based Agents](https://arxiv.org/abs/2602.04813)
*Shubham Vatsal,Harsh Dubey,Aditi Singh*

Main category: cs.AI

TL;DR: 该论文提出了一个七维分类法来系统评估医疗领域LLM智能体的能力现状，通过对49项研究的分析揭示了能力实现的不对称性。


<details>
  <summary>Details</summary>
Motivation: 现有医疗LLM智能体研究多为概述性调查或单一能力探讨，缺乏统一的分析框架。本文旨在填补这一空白，为医疗领域智能体研究提供系统化的评估体系。

Method: 开发七维分类法（认知能力、知识管理、交互模式、适应与学习、安全与伦理、框架类型学、核心任务与子任务），包含29个操作子维度。使用明确纳入排除标准和标签规则（完全实现、部分实现、未实现），对49项研究进行系统映射和定量分析。

Result: 分析揭示了明显的能力不对称：外部知识整合普遍实现（76%完全实现），而事件触发激活（92%未实现）和漂移检测与缓解（98%未实现）严重缺失。架构上多智能体设计占主导（82%完全实现），但编排层大多不完整。核心任务中信息中心能力领先，而治疗规划等行动导向领域仍有显著差距（59%未实现）。

Conclusion: 医疗LLM智能体研究在知识整合和多智能体架构方面取得进展，但在自适应学习、事件驱动交互和行动导向任务方面存在明显短板，为未来研究指明了方向。

Abstract: Large Language Model (LLM)-based agents that plan, use tools and act has begun to shape healthcare and medicine. Reported studies demonstrate competence on various tasks ranging from EHR analysis and differential diagnosis to treatment planning and research workflows. Yet the literature largely consists of overviews which are either broad surveys or narrow dives into a single capability (e.g., memory, planning, reasoning), leaving healthcare work without a common frame. We address this by reviewing 49 studies using a seven-dimensional taxonomy: Cognitive Capabilities, Knowledge Management, Interaction Patterns, Adaptation & Learning, Safety & Ethics, Framework Typology and Core Tasks & Subtasks with 29 operational sub-dimensions. Using explicit inclusion and exclusion criteria and a labeling rubric (Fully Implemented, Partially Implemented, Not Implemented), we map each study to the taxonomy and report quantitative summaries of capability prevalence and co-occurrence patterns. Our empirical analysis surfaces clear asymmetries. For instance, the External Knowledge Integration sub-dimension under Knowledge Management is commonly realized (~76% Fully Implemented) whereas Event-Triggered Activation sub-dimenison under Interaction Patterns is largely absent (~92% Not Implemented) and Drift Detection & Mitigation sub-dimension under Adaptation & Learning is rare (~98% Not Implemented). Architecturally, Multi-Agent Design sub-dimension under Framework Typology is the dominant pattern (~82% Fully Implemented) while orchestration layers remain mostly partial. Across Core Tasks & Subtasks, information centric capabilities lead e.g., Medical Question Answering & Decision Support and Benchmarking & Simulation, while action and discovery oriented areas such as Treatment Planning & Prescription still show substantial gaps (~59% Not Implemented).

</details>


### [29] [OMG-Agent: Toward Robust Missing Modality Generation with Decoupled Coarse-to-Fine Agentic Workflows](https://arxiv.org/abs/2602.04144)
*Ruiting Dai,Zheyu Wang,Haoyu Yang,Yihan Liu,Chengzhi Wang,Zekun Zhang,Zishan Huang,Jiaman Cen,Lisi Mo*

Main category: cs.AI

TL;DR: OMG-Agent：一种新型多模态生成框架，通过解耦语义规划、证据检索和执行合成三阶段，解决现有方法中的语义-细节纠缠问题，显著提升数据不完整情况下的生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有多模态系统在数据不完整时可靠性差：传统参数/生成模型因过度依赖内部记忆而产生幻觉，检索增强框架则受限于检索刚性。这些端到端架构存在语义-细节纠缠的结构性冲突，损害了生成保真度。

Method: 提出OMG-Agent框架，采用动态粗到细的智能体工作流：1) MLLM驱动的语义规划器通过渐进上下文推理解决输入歧义；2) 非参数证据检索器将抽象语义锚定到外部知识；3) 检索注入执行器利用检索证据作为灵活特征提示合成高保真细节。

Result: 在多个基准测试中，OMG-Agent始终超越最先进方法，在极端缺失情况下保持鲁棒性，例如在CMU-MOSI数据集上70%缺失率时获得2.6分的提升。

Conclusion: OMG-Agent通过解耦语义规划和细节合成的智能体工作流，有效解决了多模态生成中的语义-细节纠缠问题，为数据不完整场景提供了可靠解决方案。

Abstract: Data incompleteness severely impedes the reliability of multimodal systems. Existing reconstruction methods face distinct bottlenecks: conventional parametric/generative models are prone to hallucinations due to over-reliance on internal memory, while retrieval-augmented frameworks struggle with retrieval rigidity. Critically, these end-to-end architectures are fundamentally constrained by Semantic-Detail Entanglement -- a structural conflict between logical reasoning and signal synthesis that compromises fidelity. In this paper, we present \textbf{\underline{O}}mni-\textbf{\underline{M}}odality \textbf{\underline{G}}eneration Agent (\textbf{OMG-Agent}), a novel framework that shifts the paradigm from static mapping to a dynamic coarse-to-fine Agentic Workflow. By mimicking a \textit{deliberate-then-act} cognitive process, OMG-Agent explicitly decouples the task into three synergistic stages: (1) an MLLM-driven Semantic Planner that resolves input ambiguity via Progressive Contextual Reasoning, creating a deterministic structured semantic plan; (2) a non-parametric Evidence Retriever that grounds abstract semantics in external knowledge; and (3) a Retrieval-Injected Executor that utilizes retrieved evidence as flexible feature prompts to overcome rigidity and synthesize high-fidelity details. Extensive experiments on multiple benchmarks demonstrate that OMG-Agent consistently surpasses state-of-the-art methods, maintaining robustness under extreme missingness, e.g., a $2.6$-point gain on CMU-MOSI at $70$\% missing rates.

</details>


### [30] [Steering LLMs via Scalable Interactive Oversight](https://arxiv.org/abs/2602.04210)
*Enyu Zhou,Zhiheng Xi,Long Ma,Zhihao Zhang,Shihan Dou,Zhikai Lei,Guoteng Wang,Rui Zheng,Hang Yan,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.AI

TL;DR: 论文提出Scalable Interactive Oversight框架，通过将复杂意图分解为可管理的决策树，让非专家用户也能有效监督AI完成超出自身能力范围的复杂任务。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能自动化完成复杂长期任务，出现了监督缺口：用户因缺乏领域专业知识、难以精确表达意图、无法可靠验证复杂输出，难以有效指导AI系统。这提出了可扩展监督的关键挑战。

Method: 提出Scalable Interactive Oversight框架，将复杂意图递归分解为可管理的决策树，在节点处收集低负担的人类反馈，递归聚合为精确的全局指导，而非依赖开放式提示。

Result: 在网页开发任务中验证，该框架使非专家能生成专家级产品需求文档，对齐度提升54%。框架可通过仅使用在线用户反馈的强化学习进行优化。

Conclusion: 该框架为解决AI扩展中的人类控制问题提供了实用路径，使人类能在超越自身能力的任务上负责任地指导AI系统。

Abstract: As Large Language Models increasingly automate complex, long-horizon tasks such as \emph{vibe coding}, a supervision gap has emerged. While models excel at execution, users often struggle to guide them effectively due to insufficient domain expertise, the difficulty of articulating precise intent, and the inability to reliably validate complex outputs. It presents a critical challenge in scalable oversight: enabling humans to responsibly steer AI systems on tasks that surpass their own ability to specify or verify. To tackle this, we propose Scalable Interactive Oversight, a framework that decomposes complex intent into a recursive tree of manageable decisions to amplify human supervision. Rather than relying on open-ended prompting, our system elicits low-burden feedback at each node and recursively aggregates these signals into precise global guidance. Validated in web development task, our framework enables non-experts to produce expert-level Product Requirement Documents, achieving a 54\% improvement in alignment. Crucially, we demonstrate that this framework can be optimized via Reinforcement Learning using only online user feedback, offering a practical pathway for maintaining human control as AI scales.

</details>


### [31] [InterPReT: Interactive Policy Restructuring and Training Enable Effective Imitation Learning from Laypersons](https://arxiv.org/abs/2602.04213)
*Feiyu Gavin Zhu,Jean Oh,Reid Simmons*

Main category: cs.AI

TL;DR: InterPReT：一种交互式策略重构与训练方法，让非专业用户能够通过指令和演示来训练AI代理，降低AI教学门槛


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习需要专业人员的演示和训练监控，这对普通用户来说门槛太高。需要降低非专业用户教学AI代理的门槛，让普通用户能够交互式地教导AI代理新技能

Method: 提出Interactive Policy Restructuring and Training (InterPReT)，通过用户指令持续更新策略结构并优化参数以适应演示，支持用户交互式提供指令和演示、监控代理性能、审查决策策略

Result: 在赛车游戏教学的用户研究（N=34）中，相比通用模仿学习基线，InterPReT在普通用户负责演示和决定停止时机时，能产生更鲁棒的策略且不影响系统可用性

Conclusion: 该方法更适合没有机器学习技术背景的终端用户训练可靠的策略，降低了AI教学的门槛

Abstract: Imitation learning has shown success in many tasks by learning from expert demonstrations. However, most existing work relies on large-scale demonstrations from technical professionals and close monitoring of the training process. These are challenging for a layperson when they want to teach the agent new skills. To lower the barrier of teaching AI agents, we propose Interactive Policy Restructuring and Training (InterPReT), which takes user instructions to continually update the policy structure and optimize its parameters to fit user demonstrations. This enables end-users to interactively give instructions and demonstrations, monitor the agent's performance, and review the agent's decision-making strategies. A user study (N=34) on teaching an AI agent to drive in a racing game confirms that our approach yields more robust policies without impairing system usability, compared to a generic imitation learning baseline, when a layperson is responsible for both giving demonstrations and determining when to stop. This shows that our method is more suitable for end-users without much technical background in machine learning to train a dependable policy

</details>


### [32] [Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search](https://arxiv.org/abs/2602.04248)
*Hao Lu,Haoyuan Huang,Yulin Zhou,Chen Li,Ningxin Zhu*

Main category: cs.AI

TL;DR: Empirical-MCTS：一种将无状态MCTS转变为持续学习过程的双循环框架，通过局部探索与全局记忆优化提升LLM推理能力


<details>
  <summary>Details</summary>
Motivation: 当前推理时扩展策略（如MCTS）主要是无状态的，每次解决问题后丢弃成功的推理模式，无法模仿人类经验积累的智慧特征。需要将结构化搜索与经验积累相结合来掌握复杂的开放式推理任务。

Method: 提出Empirical-MCTS双循环框架：1) PE-EMP（成对经验进化元提示）作为局部搜索的反射优化器，使用成对反馈动态合成自适应标准并实时演化元提示；2) 记忆优化代理管理全局存储库作为动态策略先验，使用原子操作跨问题提炼高质量见解。

Result: 在复杂推理基准测试（AIME25、ARC-AGI-2、MathArena Apex）上，Empirical-MCTS显著优于无状态MCTS策略和独立经验驱动代理，证明了结构化搜索与经验积累结合的必要性。

Conclusion: 将结构化搜索与经验积累相结合对于掌握复杂开放式推理任务至关重要，Empirical-MCTS框架成功地将无状态搜索转变为持续的非参数学习过程。

Abstract: Inference-time scaling strategies, particularly Monte Carlo Tree Search (MCTS), have significantly enhanced the reasoning capabilities of Large Language Models (LLMs). However, current approaches remain predominantly stateless, discarding successful reasoning patterns after each problem instance and failing to mimic the empirical accumulation of wisdom characteristic of human problem-solving. To bridge this gap, we introduce Empirical-MCTS, a dual-loop framework that transforms stateless search into a continuous, non-parametric learning process. The framework unifies local exploration with global memory optimization through two novel mechanisms: Pairwise-Experience-Evolutionary Meta-Prompting (PE-EMP) and a Memory Optimization Agent. PE-EMP functions as a reflexive optimizer within the local search, utilizing pairwise feedback to dynamically synthesize adaptive criteria and evolve meta-prompts (system prompts) in real-time. Simultaneously, the Memory Optimization Agent manages a global repository as a dynamic policy prior, employing atomic operations to distill high-quality insights across problems. Extensive evaluations on complex reasoning benchmarks, including AIME25, ARC-AGI-2, and MathArena Apex, demonstrate that Empirical-MCTS significantly outperforms both stateless MCTS strategies and standalone experience-driven agents. These results underscore the critical necessity of coupling structured search with empirical accumulation for mastering complex, open-ended reasoning tasks.

</details>


### [33] [Agent-Omit: Training Efficient LLM Agents for Adaptive Thought and Observation Omission via Agentic Reinforcement Learning](https://arxiv.org/abs/2602.04284)
*Yansong Ning,Jun Fang,Naiqiang Tan,Hao Liu*

Main category: cs.AI

TL;DR: Agent-Omit：一个统一的训练框架，让LLM代理能自适应地省略冗余的思考和观察，在保持性能的同时显著提升效率


<details>
  <summary>Details</summary>
Motivation: 现有研究将整个交互轨迹同等对待，忽视了不同轮次中思考必要性和观察效用的差异，导致代理效率低下

Method: 1) 合成少量冷启动数据（单轮和多轮省略场景）微调代理；2) 提出省略感知的代理强化学习方法，包含双重采样机制和定制的省略奖励；3) 理论上证明省略策略的偏差受KL散度上界约束

Result: 在五个代理基准测试中，Agent-Omit-8B性能可与前沿LLM代理相媲美，并在效率-效果权衡上优于七种高效LLM代理方法

Conclusion: Agent-Omit框架能有效提升代理效率，通过自适应省略冗余思考和观察实现更好的效果-效率平衡，为高效代理系统提供了新思路

Abstract: Managing agent thought and observation during multi-turn agent-environment interactions is an emerging strategy to improve agent efficiency. However, existing studies treat the entire interaction trajectories equally, overlooking the thought necessity and observation utility varies across turns. To this end, we first conduct quantitative investigations into how thought and observation affect agent effectiveness and efficiency. Based on our findings, we propose Agent-Omit, a unified training framework that empowers LLM agents to adaptively omit redundant thoughts and observations. Specifically, we first synthesize a small amount of cold-start data, including both single-turn and multi-turn omission scenarios, to fine-tune the agent for omission behaviors. Furthermore, we introduce an omit-aware agentic reinforcement learning approach, incorporating a dual sampling mechanism and a tailored omission reward to incentivize the agent's adaptive omission capability. Theoretically, we prove that the deviation of our omission policy is upper-bounded by KL-divergence. Experimental results on five agent benchmarks show that our constructed Agent-Omit-8B could obtain performance comparable to seven frontier LLM agent, and achieve the best effectiveness-efficiency trade-off than seven efficient LLM agents methods. Our code and data are available at https://github.com/usail-hkust/Agent-Omit.

</details>


### [34] [From Assumptions to Actions: Turning LLM Reasoning into Uncertainty-Aware Planning for Embodied Agents](https://arxiv.org/abs/2602.04326)
*SeungWon Seo,SooBin Lim,SeongRae Noh,Haneul Kim,HyeongYeop Kang*

Main category: cs.AI

TL;DR: PCE框架将LLM推理中的隐含假设转化为结构化决策树，通过场景可能性、目标收益和执行成本评分来指导行动选择，减少多智能体环境中的通信开销


<details>
  <summary>Details</summary>
Motivation: 在多智能体、部分可观测、去中心化环境中，智能体需要处理隐藏对象和协作伙伴意图的不确定性。现有LLM方法主要通过频繁通信来缓解不确定性，但这会产生高昂的token和时间成本，并可能破坏工作流程

Method: 提出Planner-Composer-Evaluator（PCE）框架：将LLM推理轨迹中的隐含假设转化为结构化决策树，内部节点编码环境假设，叶子节点映射到行动，每条路径通过场景可能性、目标导向收益和执行成本进行评分

Result: 在两个多智能体基准测试（C-WAH和TDW-MAT）和三个不同LLM骨干上，PCE在成功率和任务效率上持续优于通信密集型基线，同时保持相当的token使用量。消融实验表明PCE的性能提升与模型能力和推理深度的扩展相互补充

Conclusion: PCE为将LLM隐含假设转化为可靠的不确定性感知规划策略提供了原则性途径，产生更高效和可信的通信模式，减少对频繁通信的依赖

Abstract: Embodied agents operating in multi-agent, partially observable, and decentralized environments must plan and act despite pervasive uncertainty about hidden objects and collaborators' intentions. Recent advances in applying Large Language Models (LLMs) to embodied agents have addressed many long-standing challenges, such as high-level goal decomposition and online adaptation. Yet, uncertainty is still primarily mitigated through frequent inter-agent communication. This incurs substantial token and time costs, and can disrupt established workflows, when human partners are involved. We introduce PCE, a Planner-Composer-Evaluator framework that converts the fragmented assumptions latent in LLM reasoning traces into a structured decision tree. Internal nodes encode environment assumptions and leaves map to actions; each path is then scored by scenario likelihood, goal-directed gain, and execution cost to guide rational action selection without heavy communication. Across two challenging multi-agent benchmarks (C-WAH and TDW-MAT) and three diverse LLM backbones, PCE consistently outperforms communication-centric baselines in success rate and task efficiency while showing comparable token usage. Ablation results indicate that the performance gains obtained by scaling model capacity or reasoning depth persist even when PCE is applied, while PCE consistently raises the baseline across both capacity and reasoning-depth scales, confirming that structured uncertainty handling complements both forms of scaling. A user study further demonstrates that PCE produces communication patterns that human partners perceive as more efficient and trustworthy. Together, these results establish a principled route for turning latent LLM assumptions into reliable strategies for uncertainty-aware planning.

</details>


### [35] [Digital Twins & ZeroConf AI: Structuring Automated Intelligent Pipelines for Industrial Applications](https://arxiv.org/abs/2602.04385)
*Marco Picone,Fabio Turazza,Matteo Martinelli,Marco Mamei*

Main category: cs.AI

TL;DR: 提出零配置AI管道，通过数字孪生编排数据管理和智能增强，实现模块化、可互操作的AI与CPS集成方案


<details>
  <summary>Details</summary>
Motivation: CPS系统日益复杂，物联网和工业物联网技术碎片化（不同通信协议、数据格式和设备能力）导致物理层与高层智能功能之间存在巨大鸿沟。现有数字孪生方法通常孤立且紧耦合，限制了AI功能的可扩展性和重用性。

Method: 提出模块化、可互操作的解决方案，通过最小化配置和解耦数字孪生与AI组件角色，实现AI管道与CPS的无缝集成。引入零配置AI管道概念，其中数字孪生负责编排数据管理和智能增强。

Result: 在微工厂场景中演示了该方法，展示了对并发ML模型和动态数据处理的支持，有效加速了复杂工业环境中智能服务的部署。

Conclusion: 该工作提出的零配置AI管道方法通过数字孪生技术，解决了CPS中AI/ML集成的碎片化和紧耦合问题，为复杂工业环境提供了可扩展、可重用的智能服务部署方案。

Abstract: The increasing complexity of Cyber-Physical Systems (CPS), particularly in the industrial domain, has amplified the challenges associated with the effective integration of Artificial Intelligence (AI) and Machine Learning (ML) techniques. Fragmentation across IoT and IIoT technologies, manifested through diverse communication protocols, data formats and device capabilities, creates a substantial gap between low-level physical layers and high-level intelligent functionalities. Recently, Digital Twin (DT) technology has emerged as a promising solution, offering structured, interoperable and semantically rich digital representations of physical assets. Current approaches are often siloed and tightly coupled, limiting scalability and reuse of AI functionalities. This work proposes a modular and interoperable solution that enables seamless AI pipeline integration into CPS by minimizing configuration and decoupling the roles of DTs and AI components. We introduce the concept of Zero Configuration (ZeroConf) AI pipelines, where DTs orchestrate data management and intelligent augmentation. The approach is demonstrated in a MicroFactory scenario, showing support for concurrent ML models and dynamic data processing, effectively accelerating the deployment of intelligent services in complex industrial settings.

</details>


### [36] [ReThinker: Scientific Reasoning by Rethinking with Guided Reflection and Confidence Control](https://arxiv.org/abs/2602.04496)
*Zhentao Tang,Yuqi Cui,Shixiong Kai,Wenqian Zhao,Ke Ye,Xing Li,Anxin Tian,Zehua Pei,Hui-Ling Zhen,Shoubo Hu,Xiaoguang Li,Yunhe Wang,Mingxuan Yuan*

Main category: cs.AI

TL;DR: ReThinker是一个基于置信度的智能体框架，通过Solver-Critic-Selector架构动态分配计算资源，在专家级科学推理任务上达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在专家级科学推理（如Humanity's Last Exam）上存在挑战，传统工具流水线僵化、多智能体协调脆弱、测试时扩展效率低，限制了性能提升

Method: 提出ReThinker框架，采用Solver-Critic-Selector三阶段架构，基于模型置信度动态分配计算，支持自适应工具调用、多维度反思和置信度加权选择。同时提出反向数据合成流水线和自适应轨迹回收策略，无需人工标注即可进行可扩展训练

Result: 在HLE、GAIA和XBench基准测试中，ReThinker持续超越现有最佳基础模型和深度研究系统，在专家级推理任务上取得SOTA结果

Conclusion: ReThinker通过置信度感知的动态计算分配和创新的无监督训练策略，有效解决了专家级科学推理中的关键挑战，为复杂推理任务提供了新的解决方案

Abstract: Expert-level scientific reasoning remains challenging for large language models, particularly on benchmarks such as Humanity's Last Exam (HLE), where rigid tool pipelines, brittle multi-agent coordination, and inefficient test-time scaling often limit performance. We introduce ReThinker, a confidence-aware agentic framework that orchestrates retrieval, tool use, and multi-agent reasoning through a stage-wise Solver-Critic-Selector architecture. Rather than following a fixed pipeline, ReThinker dynamically allocates computation based on model confidence, enabling adaptive tool invocation, guided multi-dimensional reflection, and robust confidence-weighted selection. To support scalable training without human annotation, we further propose a reverse data synthesis pipeline and an adaptive trajectory recycling strategy that transform successful reasoning traces into high-quality supervision. Experiments on HLE, GAIA, and XBench demonstrate that ReThinker consistently outperforms state-of-the-art foundation models with tools and existing deep research systems, achieving state-of-the-art results on expert-level reasoning tasks.

</details>


### [37] [From Competition to Collaboration: Designing Sustainable Mechanisms Between LLMs and Online Forums](https://arxiv.org/abs/2602.04572)
*Niv Fono,Yftah Ziser,Omer Ben-Porat*

Main category: cs.AI

TL;DR: 研究提出一个框架来解决生成式AI系统与问答论坛之间的悖论关系：AI依赖论坛数据提升性能，但同时又分流用户。通过顺序交互框架和真实数据模拟，证明了激励错配问题，但展示了可持续协作的可能性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI系统面临一个悖论：它们依赖问答论坛产生的数据来提升性能，但同时又将用户从这些论坛分流走。这导致了AI系统与人类知识平台之间的紧张关系，需要探索可持续的协作机制。

Method: 提出顺序交互框架，让生成式AI系统向论坛提出问题，论坛可以选择发布部分问题。该框架考虑了非货币交换、信息不对称和激励错配等复杂因素。使用真实的Stack Exchange数据和常用LLM进行数据驱动的全面模拟。

Result: 实证证明了激励错配问题的存在，但同时也表明参与者能够达到理想全信息场景下约一半的效用。结果突出了AI系统与人类知识平台之间可持续协作的潜力。

Conclusion: 尽管存在激励错配，但通过适当的框架设计，生成式AI系统与人类知识平台可以实现可持续协作，保持有效的知识共享。这种协作对于AI系统的持续改进和人类知识平台的生存都至关重要。

Abstract: While Generative AI (GenAI) systems draw users away from (Q&A) forums, they also depend on the very data those forums produce to improve their performance. Addressing this paradox, we propose a framework of sequential interaction, in which a GenAI system proposes questions to a forum that can publish some of them. Our framework captures several intricacies of such a collaboration, including non-monetary exchanges, asymmetric information, and incentive misalignment. We bring the framework to life through comprehensive, data-driven simulations using real Stack Exchange data and commonly used LLMs. We demonstrate the incentive misalignment empirically, yet show that players can achieve roughly half of the utility in an ideal full-information scenario. Our results highlight the potential for sustainable collaboration that preserves effective knowledge sharing between AI systems and human knowledge platforms.

</details>


### [38] [Vibe AIGC: A New Paradigm for Content Generation via Agentic Orchestration](https://arxiv.org/abs/2602.04575)
*Jiaheng Liu,Yuanxing Zhang,Shihao Li,Xinping Lei*

Main category: cs.AI

TL;DR: 论文提出Vibe AIGC新范式，通过智能体编排解决当前生成式AI的意图-执行差距问题，将用户从提示工程师转变为提供"氛围"的指挥官，由元规划器分解为可执行的多智能体工作流。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI存在"意图-执行差距"问题，即用户的高层意图与随机黑盒模型输出之间的根本性脱节。尽管模型规模不断扩大，视觉保真度显著提升，但遇到了"可用性天花板"，无法满足复杂数字资产创作需求。

Method: 提出Vibe AIGC范式，受Vibe Coding启发，通过智能体编排实现内容生成。用户作为指挥官提供"氛围"（包含美学偏好、功能逻辑等高层表示），中央元规划器作为系统架构师，将氛围分解为可执行、可验证、自适应的智能体管道，从随机推理转向逻辑编排。

Result: Vibe AIGC能够弥合人类想象力与机器执行之间的差距，将AI从脆弱的推理引擎转变为稳健的系统级工程合作伙伴，重新定义人机协作经济。

Conclusion: 这一范式转变将民主化复杂、长时程数字资产的创作，通过分层多智能体工作流的自主合成，实现从模型中心到系统中心的转变，解决当前生成式AI的根本局限性。

Abstract: For the past decade, the trajectory of generative artificial intelligence (AI) has been dominated by a model-centric paradigm driven by scaling laws. Despite significant leaps in visual fidelity, this approach has encountered a ``usability ceiling'' manifested as the Intent-Execution Gap (i.e., the fundamental disparity between a creator's high-level intent and the stochastic, black-box nature of current single-shot models). In this paper, inspired by the Vibe Coding, we introduce the \textbf{Vibe AIGC}, a new paradigm for content generation via agentic orchestration, which represents the autonomous synthesis of hierarchical multi-agent workflows.
  Under this paradigm, the user's role transcends traditional prompt engineering, evolving into a Commander who provides a Vibe, a high-level representation encompassing aesthetic preferences, functional logic, and etc. A centralized Meta-Planner then functions as a system architect, deconstructing this ``Vibe'' into executable, verifiable, and adaptive agentic pipelines. By transitioning from stochastic inference to logical orchestration, Vibe AIGC bridges the gap between human imagination and machine execution. We contend that this shift will redefine the human-AI collaborative economy, transforming AI from a fragile inference engine into a robust system-level engineering partner that democratizes the creation of complex, long-horizon digital assets.

</details>


### [39] [WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.04634)
*Zelai Xu,Zhexuan Xu,Ruize Zhang,Chunyang Zhu,Shi Yu,Weilin Liu,Quanlu Zhang,Wenbo Ding,Chao Yu,Yu Wang*

Main category: cs.AI

TL;DR: WideSeek-R1：通过多智能体强化学习训练的领导-子智能体框架，实现宽度扩展以解决广泛信息搜索任务，4B参数模型性能媲美671B参数的单智能体模型。


<details>
  <summary>Details</summary>
Motivation: 当前LLM研究主要关注深度扩展（单个智能体解决长时程问题），但随着任务范围变广，瓶颈从个体能力转向组织能力。现有多智能体系统依赖手工工作流和轮转交互，无法有效并行化工作。

Method: 提出WideSeek-R1框架：领导智能体-子智能体架构，通过多智能体强化学习训练，使用共享LLM但隔离上下文和专用工具，在2万个广泛信息搜索任务数据集上联合优化领导智能体和并行子智能体。

Result: WideSeek-R1-4B在WideSearch基准测试中达到40.0%的item F1分数，性能与单智能体DeepSeek-R1-671B相当。随着并行子智能体数量增加，性能持续提升，验证了宽度扩展的有效性。

Conclusion: 宽度扩展是多智能体系统的重要维度，WideSeek-R1通过MARL训练的领导-子智能体框架能够有效协调并行执行，在广泛信息搜索任务上实现高效扩展。

Abstract: Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks grow broader, the key bottleneck shifts from individual competence to organizational capability. In this work, we explore a complementary dimension of width scaling with multi-agent systems to address broad information seeking. Existing multi-agent systems often rely on hand-crafted workflows and turn-taking interactions that fail to parallelize work effectively. To bridge this gap, we propose WideSeek-R1, a lead-agent-subagent framework trained via multi-agent reinforcement learning (MARL) to synergize scalable orchestration and parallel execution. By utilizing a shared LLM with isolated contexts and specialized tools, WideSeek-R1 jointly optimizes the lead agent and parallel subagents on a curated dataset of 20k broad information-seeking tasks. Extensive experiments show that WideSeek-R1-4B achieves an item F1 score of 40.0% on the WideSearch benchmark, which is comparable to the performance of single-agent DeepSeek-R1-671B. Furthermore, WideSeek-R1-4B exhibits consistent performance gains as the number of parallel subagents increases, highlighting the effectiveness of width scaling.

</details>


### [40] [Are AI Capabilities Increasing Exponentially? A Competing Hypothesis](https://arxiv.org/abs/2602.04836)
*Haosen Ge,Hamsa Bastani,Osbert Bastani*

Main category: cs.AI

TL;DR: 该论文反驳了METR报告中AI能力呈指数增长的观点，通过拟合S型曲线发现拐点已过，并提出更复杂模型分解基础与推理能力，质疑现有指数增长预测的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 反驳METR报告中关于AI能力自2019年以来呈指数增长的结论，指出数据不支持这种增长模式，旨在揭示现有指数增长预测的脆弱性。

Method: 1) 对METR现有数据拟合S型/逻辑曲线，发现拐点已过；2) 提出更复杂模型，将AI能力分解为基础能力和推理能力，各自有不同的改进速率。

Result: 1) 拟合S型曲线显示拐点已经过去，而非METR预测的遥远未来；2) 复杂模型支持AI能力将在近期出现拐点的假设；3) 成功质疑了现有指数增长预测的可靠性。

Conclusion: AI能力增长并非指数型，现有指数增长预测缺乏稳健性，需要更细致的模型来理解AI能力发展的不同维度及其改进速率。

Abstract: Rapidly increasing AI capabilities have substantial real-world consequences, ranging from AI safety concerns to labor market consequences. The Model Evaluation & Threat Research (METR) report argues that AI capabilities have exhibited exponential growth since 2019. In this note, we argue that the data does not support exponential growth, even in shorter-term horizons. Whereas the METR study claims that fitting sigmoid/logistic curves results in inflection points far in the future, we fit a sigmoid curve to their current data and find that the inflection point has already passed. In addition, we propose a more complex model that decomposes AI capabilities into base and reasoning capabilities, exhibiting individual rates of improvement. We prove that this model supports our hypothesis that AI capabilities will exhibit an inflection point in the near future. Our goal is not to establish a rigorous forecast of our own, but to highlight the fragility of existing forecasts of exponential growth.

</details>


### [41] [Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing](https://arxiv.org/abs/2602.04837)
*Zhaotian Weng,Antonis Antoniades,Deepak Nathani,Zhen Zhang,Xiao Pu,Xin Eric Wang*

Main category: cs.AI

TL;DR: GEA提出了一种新的开放自改进代理范式，将代理群体作为基本进化单元，通过群体内显式经验共享和重用，显著提升编码任务性能，超越现有自进化方法和人类设计框架。


<details>
  <summary>Details</summary>
Motivation: 现有开放自进化范式采用树状结构进化，导致探索多样性利用效率低下，各进化分支孤立，无法有效共享经验。需要一种能更好利用探索多样性、实现持续长期进步的自改进方法。

Method: 提出群体进化代理(GEA)范式，将代理群体作为基本进化单元，在进化过程中实现群体内显式经验共享和重用，克服树状结构进化中孤立分支导致的探索多样性利用不足问题。

Result: 在编码基准测试中显著优于最先进的自进化方法(SWE-bench Verified: 71.0% vs 56.7%，Polyglot: 88.3% vs 68.3%)，匹配或超越顶级人类设计代理框架。能更有效将早期探索多样性转化为持续长期进步，修复框架级bug平均只需1.4次迭代(自进化方法需5次)。

Conclusion: GEA通过群体作为进化单元和显式经验共享，实现了更高效的自改进，在编码任务中表现出优越性能、跨模型可迁移性和鲁棒性，为开放自改进代理提供了新范式。

Abstract: Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvements, which treats a group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utilization of exploratory diversity caused by isolated evolutionary branches. We evaluate GEA on challenging coding benchmarks, where it significantly outperforms state-of-the-art self-evolving methods (71.0% vs. 56.7% on SWE-bench Verified, 88.3% vs. 68.3% on Polyglot) and matches or exceeds top human-designed agent frameworks (71.8% and 52.0% on two benchmarks, respectively). Analysis reveals that GEA more effectively converts early-stage exploratory diversity into sustained, long-term progress, achieving stronger performance under the same number of evolved agents. Furthermore, GEA exhibits consistent transferability across different coding models and greater robustness, fixing framework-level bugs in 1.4 iterations on average, versus 5 for self-evolving methods.

</details>


### [42] [Fluid Representations in Reasoning Models](https://arxiv.org/abs/2602.04843)
*Dmitrii Kharlapenko,Alessandro Stolfo,Arthur Conmy,Mrinmaya Sachan,Zhijing Jin*

Main category: cs.AI

TL;DR: 论文通过机制分析发现QwQ-32B模型在推理过程中会动态优化其内部表征，这种"流体推理表征"是推理模型性能提升的关键因素之一


<details>
  <summary>Details</summary>
Motivation: 虽然推理语言模型在抽象问题上表现优异，但其内部机制仍不清楚。本文旨在揭示QwQ-32B模型如何处理抽象结构信息，理解推理模型性能提升的内部机制

Method: 使用Mystery Blocksworld（语义混淆的规划领域）进行机制分析，通过转向实验建立因果证据，研究模型在推理过程中如何改进动作和概念的表征

Result: 发现QwQ-32B在推理过程中逐步改进内部表征，发展出关注结构而非具体动作名称的抽象编码；成功轨迹中的精炼表征注入能提升准确率，符号表征可替代混淆编码且性能损失最小

Conclusion: 推理模型性能提升的关键因素之一是上下文中的表征精炼，即"流体推理表征"——模型在推理过程中动态优化其内部表征的能力

Abstract: Reasoning language models, which generate long chains of thought, dramatically outperform non-reasoning language models on abstract problems. However, the internal model mechanisms that allow this superior performance remain poorly understood. We present a mechanistic analysis of how QwQ-32B - a model specifically trained to produce extensive reasoning traces - process abstract structural information. On Mystery Blocksworld - a semantically obfuscated planning domain - we find that QwQ-32B gradually improves its internal representation of actions and concepts during reasoning. The model develops abstract encodings that focus on structure rather than specific action names. Through steering experiments, we establish causal evidence that these adaptations improve problem solving: injecting refined representations from successful traces boosts accuracy, while symbolic representations can replace many obfuscated encodings with minimal performance loss. We find that one of the factors driving reasoning model performance is in-context refinement of token representations, which we dub Fluid Reasoning Representations.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [43] [Time-to-Event Estimation with Unreliably Reported Events in Medicare Health Plan Payment](https://arxiv.org/abs/2602.04092)
*Oana M. Enache,Sherri Rose*

Main category: stat.AP

TL;DR: 该论文将生存分析中的限制平均时间损失（RMTL）方法扩展到Medicare Advantage计划中的编码强度评估，以检测保险公司可能存在的过度编码（upcoding）行为，并开发了相应的估计器和开源R包用于模拟真实数据。


<details>
  <summary>Details</summary>
Motivation: Medicare Advantage计划中，保险公司因按预期健康需求获得前瞻性支付，且所有系数为正、预测变量主要为保险公司提交的健康状况，存在过度编码（报告更多诊断）的激励。这种博弈行为预计在2025年将给联邦政府造成400亿美元损失，但缺乏可靠的评估方法。

Method: 将生存分析中的限制平均时间损失（RMTL）方法扩展到Medicare Advantage编码强度评估，提出了几种新的编码强度和可能过度编码的估计器，包括考虑不可靠报告的情况。利用NIH的All of Us研究数据进行模拟验证，并开发了开源R包来模拟真实的标记过度编码数据。

Result: 开发了新的估计器来评估Medicare Advantage中的编码强度和可能过度编码，通过模拟数据验证了估计器性能，并创建了开源R包用于生成以前不可用的真实标记过度编码数据。

Conclusion: 该研究成功将RMTL方法应用于Medicare Advantage计划的编码强度评估，为解决保险公司过度编码问题提供了新的分析工具和数据模拟能力，有助于更准确地检测和量化这一成本高昂的博弈行为。

Abstract: Time-to-event estimation (i.e., survival analysis) is common in health research, most often using methods that assume proportional hazards and no competing risks. Because both assumptions are frequently invalid, estimators more aligned with real-world settings have been proposed. An effect can be estimated as the difference in areas below the cumulative incidence functions of two groups up to a pre-specified time point. This approach, restricted mean time lost (RMTL), can be used in settings with competing risks as well. We extend RMTL estimation for use in an understudied health policy application in Medicare. Medicare currently supports healthcare payment for over 69 million beneficiaries, most of whom are enrolled in Medicare Advantage plans and receive insurance from private insurers. These insurers are prospectively paid by the federal government for each of their beneficiaries' anticipated health needs using an ordinary least squares linear regression algorithm. As all coefficients are positive and predictor variables are largely insurer-submitted health conditions, insurers are incentivized to upcode, or report more diagnoses than may be accurate. Such gaming is projected to cost the federal government $40 billion in 2025 alone without clear benefit to beneficiaries. We propose several novel estimators of coding intensity and possible upcoding in Medicare Advantage, including accounting for unreliable reporting. We demonstrate estimator performance in simulated data leveraging the National Institutes of Health's All of Us study and also develop an open source R package to simulate realistic labeled upcoding data, which were not previously available.

</details>


### [44] [mmcmcBayes:An R Package Implementing a Multistage MCMC Framework for Detecting the Differentially Methylated Regions](https://arxiv.org/abs/2602.04554)
*Zhexuan Yang,Duchwan Ryu,Feng Luan*

Main category: stat.AP

TL;DR: mmcmcBayes是一个R包，使用多阶段MCMC方法检测差异甲基化区域，通过区域级建模和贝叶斯因子评估组间差异。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常通过聚合CpG位点测试结果来检测差异甲基化区域，这可能限制其捕捉复杂区域甲基化模式的能力。需要一种区域级的检测方法。

Method: 采用多阶段马尔可夫链蒙特卡洛程序，使用alpha-skew广义正态分布建模样本区域甲基化摘要，通过贝叶斯因子评估组间差异甲基化证据，并使用多阶段区域分割策略基于统计证据细化候选区域。

Result: 通过模拟研究和Illumina 450K甲基化数据应用展示了该方法的性能。mmcmcBayes包提供了实用的区域级替代方案，并包含用于总结、比较和可视化检测区域的辅助函数。

Conclusion: mmcmcBayes包为现有基于CpG的差异甲基化区域检测方法提供了实用的区域级替代方案，能够更好地捕捉复杂区域甲基化模式。

Abstract: Identifying differentially methylated regions is an important task in epigenome-wide association studies, where differential signals often arise across groups of neighboring CpG sites. Many existing methods detect differentially methylated regions by aggregating CpG-level test results, which may limit their ability to capture complex regional methylation patterns. In this paper, we introduce the R package mmcmcBayes, which implements a multistage Markov chain Monte Carlo procedure for region-level detection of differentially methylated regions. The method models sample-wise regional methylation summaries using the alpha-skew generalized normal distribution and evaluates evidence for differential methylation between groups through Bayes factors. We use a multistage region-splitting strategy to refine candidate regions based on statistical evidence. We describe the underlying methodology and software implementation, and illustrate its performance through simulation studies and applications to Illumina 450K methylation data. The mmcmcBayes package provides a practical region-level alternative to existing CpG-based differentially methylated regions detection methods and includes supporting functions for summarizing, comparing, and visualizing detected regions.

</details>


### [45] [Inference for Within- and Between-Partnership Transmission Rates for HIV Infection](https://arxiv.org/abs/2602.04638)
*Irene García Muñoz,Ian Hall,Thomas House*

Main category: stat.AP

TL;DR: 开发了一个随机SI配对模型来估计HIV在血清不一致伴侣中的传播率，考虑了性别差异，使用基于似然的推断方法估计参数，可推广到其他流行病学场景。


<details>
  <summary>Details</summary>
Motivation: HIV在血清不一致伴侣中的传播是撒哈拉以南非洲的重要公共卫生挑战，需要准确估计伴侣内和伴侣外的感染率，以制定有效的预防策略。

Method: 提出了一个随机易感-感染(SI)配对模型来估计HIV传播的关键流行病学参数，扩展模型以考虑性别特异性差异，采用基于似然的推断方法从观测数据中估计参数和不确定性。

Result: 成功开发了能够估计HIV在伴侣内和伴侣间传播参数的模型，这些参数值可用于指导HIV感染预防策略。

Conclusion: 该模型为HIV传播参数估计提供了有效方法，所提出的方法可推广到其他流行病学设置，有助于制定针对性的感染预防策略。

Abstract: HIV transmission within serodiscordant couples remains a significant public health challenge, particularly in sub-Saharan Africa. Estimating the rate of such infection, alongside the rates of introduction of infection from outside the partnership, is a special case of the more general epidemiological challenge of inferring intensities of within- and between-group intensities of transmission. This study presents a stochastic susceptible-infected (SI) pair model for estimating key epidemiological parameters governing HIV transmission within and between couples, which we further extend to account for gender-specific differences in infection dynamics. Using a likelihood-based inference approach, we estimate transmission parameters and associated uncertainty from observed data. These values can be used to inform infection prevention strategies for HIV, and the methodology proposed can be generalised to other epidemiological settings.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [46] [The Output Convergence Debate Revisited: Lessons from recent developments in the analysis of panel data models](https://arxiv.org/abs/2602.04060)
*M Hashem Pesaran,Ron Smith*

Main category: econ.EM

TL;DR: 本文批判性检验了产出收敛辩论的实证基础，使用动态异质面板交互效应分析新方法，发现传统方法低估收敛速度且推断误导，而DCCE方法提供稳健估计，实证发现跨国人均产出收敛证据微弱，资本积累是跨国产出差异最重要决定因素。


<details>
  <summary>Details</summary>
Motivation: 重新审视产出收敛辩论的实证基础，批判传统方法（如Barro跨国回归和双向固定效应）假设平行趋势和同质动态的局限性，这些方法可能导致收敛速度低估和误导性推断。

Method: 采用Chudik和Pesaran(2015a)提出的动态共同相关效应(DCCE)估计方法，该方法能处理非平行趋势、相关异质性，即使潜在技术因子存在断点、趋势和/或单位根也适用；同时提出估计缓慢移动增长决定因素影响的方法；使用Penn World Tables数据进行实证分析。

Result: 1) 跨国人均产出收敛证据微弱；2) 跨国增长收敛非常缓慢；3) 国内收敛相对较快；4) 资本积累是跨国产出差异最重要决定因素；5) 冲突潜力和产权保护等缓慢移动指标对人均产出稳态水平有统计显著影响；6) 民主化对产出的正向效应在考虑非平行趋势和动态异质性后统计显著性下降。

Conclusion: 传统收敛分析方法存在严重缺陷，DCCE方法提供更稳健的估计和推断；实证证据不支持强跨国收敛，资本积累是关键决定因素；方法选择对政策相关结论有重要影响，特别是民主化效应等发现对模型设定敏感。

Abstract: This paper provides a critical examination of the empirical basis of the output convergence debate in the light of recent developments in the analysis of dynamic heterogeneous panels with interactive effects. It shows that popular tools such as Barro's cross-country regressions and two-way fixed effects (TWFE) estimators that assume parallel trends and homogeneous dynamics lead to substantial under-estimation of the speed of convergence and misleading inference. Instead, dynamic common correlated effects (DCCE) estimators due to Chudik and Pesaran (2015a) provide consistent estimates and valid inference that are robust to nonparallel trends and correlated heterogeneity and apply even if there are breaks, trends and/or unit roots in the latent technology factor. It also suggests a way to estimate the effect of slowly moving determinants of growth. The theoretical findings are augmented with empirical evidence using Penn World Tables data, finding little evidence of per capita output convergence across countries, very slow evidence of cross country growth convergence, and reasonably fast within country convergence. Capital accumulation is found to be the most important single determinant of cross-country differences in output while slow moving indicators such as potential for conflict and protection of property rights proved to be statistically significant determinants of the steady state levels of output per capita. We are also able to replicate a positive evidence of democratization on output, but we find that the statistical significance of this effect to fall as we allow for nonparallel trends and dynamic heterogeneity.

</details>

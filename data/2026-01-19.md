<div id=toc></div>

# Table of Contents

- [cs.SI](#cs.SI) [Total: 5]
- [cs.AI](#cs.AI) [Total: 25]
- [econ.EM](#econ.EM) [Total: 3]
- [stat.AP](#stat.AP) [Total: 2]
- [cs.CY](#cs.CY) [Total: 8]


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [1] [Sparing User Time with a Socially-Aware Independent Metaverse Avatar](https://arxiv.org/abs/2601.11115)
*Theofanis P. Raptis,Chiara Boldrini,Marco Conti,Andrea Passarella*

Main category: cs.SI

TL;DR: 该研究提出了一种计算模型，通过独立虚拟化身优化元宇宙社交网络中的时间分配，解决社交冲突和约束，并开发启发式算法解决NP-hard优化问题。


<details>
  <summary>Details</summary>
Motivation: 元宇宙正在重新定义数字交互，但其对社交网络的影响尚未充分探索。研究者希望探索独立虚拟化身（能够代表用户管理社交互动的自主数字实体）在优化社交时间分配和重塑元宇宙社交网络中的作用。

Method: 提出了一个新颖的计算模型，该模型整合了基于进化人类学的用户社交生活量化表示与虚拟化身介导的交互框架。模型量化了部分面对面交互被独立虚拟化身交互替代的效果，并考虑了社交冲突和特定社交化约束。由于问题本质上是NP-hard优化问题，研究者引入了启发式解决方案，并通过模拟比较了虚拟化身介导和非虚拟化身介导的社交网络。

Result: 模拟结果表明，独立虚拟化身能够增强社交连接性和效率。研究为优化元宇宙社交互动提供了基础，并为未来数字社交网络设计提供了有用见解。

Conclusion: 独立虚拟化身在元宇宙社交网络中具有优化社交时间分配和增强社交效率的潜力，为解决社交冲突和约束提供了新途径，为元宇宙社交网络设计奠定了理论基础。

Abstract: The Metaverse is redefining digital interactions by merging physical, virtual, and social dimensions, yet its effects on social networking remain largely unexplored. This work examines the role of independent avatars (autonomous digital entities capable of managing social interactions on behalf of users), to optimize social time allocation and reshape Metaverse-based Online Social Networks. We propose a novel computational model that integrates a quantitative and realistic representation of user social life, grounded in evolutionary anthropology, with a framework for avatar-mediated interactions. Our model quantifies the effectiveness of a partial replacement of in-person interactions with independent avatar interactions. Additionally, it accounts for social conflicts and specific socialization constraints. We leverage our model to explore the benefits and trade-offs of an avatar-augmented social life in the Metaverse. Since the exact problem formulation leads to an NP-hard optimization problem when incorporating avatars into the social network, we tackle this challenge by introducing a heuristic solution. Through simulations, we compare avatar-mediated and non-avatar-mediated social networking, demonstrating the potential of independent avatars to enhance social connectivity and efficiency. Our findings provide a foundation for optimizing Metaverse-based social interactions, as well as useful insights for future digital social network design.

</details>


### [2] [The Big Ban Theory: A Pre- and Post-Intervention Dataset of Online Content Moderation Actions](https://arxiv.org/abs/2601.11128)
*Aldo Cerulli,Lorenzo Cima,Benedetta Tessa,Serena Tardelli,Stefano Cresci*

Main category: cs.SI

TL;DR: TBBT是一个大型内容审核干预数据集，涵盖25种不同类型、严重程度和范围的干预措施，包含33.9万用户和3900万条消息，支持系统性研究审核效果


<details>
  <summary>Details</summary>
Motivation: 现有研究面临数据限制，通常只能关注少数干预措施，缺乏系统性比较。研究者需要为每个新研究收集数据，限制了可重复性和可比性研究

Method: 构建TBBT数据集，包含25种审核干预措施，提供标准化元数据和用户活动数据（干预前后各三个月），支持一致性和可比性分析

Result: 创建了包含339K用户和39M消息的大型数据集，提供描述性探索分析和多个使用案例，支持内容审核效果研究

Conclusion: TBBT数据集旨在支持研究者系统研究审核干预效果，促进更系统、可重复和可比较的研究，数据集已公开可用

Abstract: Online platforms rely on moderation interventions to curb harmful behavior such hate speech, toxicity, and the spread of mis- and disinformation. Yet research on the effects and possible biases of such interventions faces multiple limitations. For example, existing works frequently focus on single or a few interventions, due to the absence of comprehensive datasets. As a result, researchers must typically collect the necessary data for each new study, which limits opportunities for systematic comparisons. To overcome these challenges, we introduce The Big Ban Theory (TBBT), a large dataset of moderation interventions. TBBT covers 25 interventions of varying type, severity, and scope, comprising in total over 339K users and nearly 39M posted messages. For each intervention, we provide standardized metadata and pseudonymized user activity collected three months before and after its enforcement, enabling consistent and comparable analyses of intervention effects. In addition, we provide a descriptive exploratory analysis of the dataset, along with several use cases of how it can support research on content moderation. With this dataset, we aim to support researchers studying the effects of moderation interventions and to promote more systematic, reproducible, and comparable research. TBBT is publicly available at: https://doi.org/10.5281/zenodo.18245670.

</details>


### [3] [When "Likers'' Go Private: Engagement With Reputationally Risky Content on X](https://arxiv.org/abs/2601.11140)
*Yuwei Chuai,Manoel Horta Ribeiro,Gabriele Lenzini,Nicolas Pröllochs*

Main category: cs.SI

TL;DR: 研究X/Twitter将点赞可见性从公开改为私密后，高风险内容获得的点赞数是否增加，发现平台层面无明显变化，用户意愿与行为存在差距。


<details>
  <summary>Details</summary>
Motivation: 2024年6月X/Twitter将点赞可见性从公开改为私密，为研究参与信号可见性如何影响用户行为提供了难得的平台级机会。研究者想探究隐藏点赞者身份是否会增加高风险内容（如政治内容或党派账户）获得的点赞数，因为公开支持这类内容可能带来较高的社会或声誉成本。

Method: 采用两种互补研究方法：1）对1068个账户的154,122条帖子在政策变化前后的参与度进行双重差分分析；2）对203名X用户进行受试者内调查实验，测量他们在不同可见性条件下点赞各类内容的意愿。

Result: 研究发现：1）平台层面高风险内容的点赞数没有可检测的增加，这一发现在高风险与低风险账户的组间比较以及点赞与转发的组内比较中均稳健；2）调查实验中参与者报告在私密可见性下点赞高风险内容的意愿略有增加，但并未导致群体层面平均点赞可能性的显著变化。

Conclusion: 隐藏点赞在平台层面产生的行为响应有限，这可能源于用户意图与行为之间的差距，或是由少数高使用率或自动化账户驱动的参与行为。

Abstract: In June 2024, X/Twitter changed likes' visibility from public to private, offering a rare, platform-level opportunity to study how the visibility of engagement signals affects users' behavior. Here, we investigate whether hiding liker identities increases the number of likes received by high-reputational-risk content, content for which public endorsement may carry high social or reputational costs due to its topic (e.g., politics) or the account context in which it appears (e.g., partisan accounts). To this end, we conduct two complementary studies: 1) a Difference-in-Differences analysis of engagement with 154,122 posts by 1068 accounts before and after the policy change. 2) a within-subject survey experiment with 203 X users on participants' self-reported willingness to like different kinds of content. We find no detectable platform-level increase in likes for high-reputational-risk content (Study 1). This finding is robust for both between-group comparison of high- versus low-reputational-risk accounts and within-group comparison across engagement types (i.e., likes vs. reposts). Additionally, while participants in the survey experiment report modest increases in willingness to like high-reputational-risk content under private versus public visibility, these increases do not lead to significant changes in the group-level average likelihood of liking posts (Study 2). Taken together, our results suggest that hiding likes produces a limited behavioral response at the platform level. This may be caused by a gap between user intention and behavior, or by engagement driven by a narrow set of high-usage or automated accounts.

</details>


### [4] [Walk based Laplacians for Modeling Diffusion on Complex Networks](https://arxiv.org/abs/2601.11338)
*Francesca Arrigo,Fabio Durastante*

Main category: cs.SI

TL;DR: 提出基于图游走的拉普拉斯算子新框架，包含回溯抑制参数化家族，扩展标准拉普拉斯算子并保持其性质，提供高效计算算法。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型通常基于标准拉普拉斯算子，但无法有效处理复杂网络中的记忆效应和回溯行为。需要开发能自然纳入回溯抑制机制的更灵活建模框架。

Method: 构建基于图游走的拉普拉斯类算子参数化家族：1) 计算所有遍历的标准游走拉普拉斯；2) 消除立即回溯的非回溯变体；3) 回溯降权变体提供连续插值。使用Krylov子空间方法设计高效算法。

Result: 证明这些算子扩展了标准拉普拉斯定义并保持其性质。在真实网络上的数值实验验证了建模灵活性，算法计算效率高，支持GPU加速。

Conclusion: 提出的游走拉普拉斯框架为复杂网络扩散建模提供了灵活且计算高效的方法，能自然处理记忆效应和回溯行为，适用于大规模网络分析。

Abstract: We develop a novel framework for modeling diffusion on complex networks by constructing Laplacian-like operators based on walks around a graph. Our approach introduces a parametric family of walk-based Laplacians that naturally incorporate memory effects by excluding or downweighting backtracking trajectories, where walkers immediately revisit nodes. The framework includes: (i) walk-based Laplacians that count all traversals in the network; (ii) nonbacktracking variants that eliminate immediate reversals; and (iii) backtrack-downweighted variants that provide a continuous interpolation between these two regimes. We establish that these operators extend the definition of the standard Laplacian and also preserve some of its properties. We present efficient algorithms using Krylov subspace methods for computing them, ensuring applicability of our proposed framework to large networks. Extensive numerical experiments on real-world networks validate the modeling flexibility of our approach and demonstrate the computational efficiency of the proposed algorithms, including GPU acceleration.

</details>


### [5] [Industry Influence in High-Profile Social Media Research](https://arxiv.org/abs/2601.11507)
*Joseph Bak-Coleman,Jevin West,Cailin O'Connor,Carl T. Bergstrom*

Main category: cs.SI

TL;DR: 研究发现社交媒体研究中存在广泛的行业影响：一半顶级期刊论文有可披露的行业联系，但大多未披露；这些联系集中在少数长期与行业合作的科学家；行业关联研究获得更多关注；行业联系与回避平台规模影响研究相关。


<details>
  <summary>Details</summary>
Motivation: 探讨社交媒体研究在多大程度上独立于行业影响，揭示行业联系在学术研究中的普遍性、透明度及其对研究方向的影响。

Method: 利用公开可用数据，分析顶级期刊发表的社交媒体研究论文，识别作者、审稿人和学术编辑的行业联系（包括资金、合作或雇佣关系），并评估这些联系的披露情况。

Result: 1. 一半顶级期刊论文有可披露的行业联系，但大多数未在发表的研究中披露；2. 行业联系集中在少数长期与行业合作的科学家；3. 行业关联研究在学术界、政策制定者、社交媒体和新闻中获得更多关注；4. 行业联系与回避平台规模影响研究相关。

Conclusion: 社交媒体研究中的行业影响广泛、有影响力且往往不透明。需要加强披露规范并实施政策，确保独立研究的可见性和行业支持研究的完整性。

Abstract: To what extent is social media research independent from industry influence? Leveraging openly available data, we show that half of the research published in top journals has disclosable ties to industry in the form of prior funding, collaboration, or employment. However, the majority of these ties go undisclosed in the published research. These trends do not arise from broad scientific engagement with industry, but rather from a select group of scientists who maintain long-lasting relationships with industry. Undisclosed ties to industry are common not just among authors, but among reviewers and academic editors during manuscript evaluation. Further, industry-tied research garners more attention within the academy, among policymakers, on social media, and in the news. Finally, we find evidence that industry ties are associated with a topical focus away from impacts of platform-scale features. Together, these findings suggest industry influence in social media research is extensive, impactful, and often opaque. Going forward there is a need to strengthen disclosure norms and implement policies to ensure the visibility of independent research, and the integrity of industry supported research.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [6] [Japanese AI Agent System on Human Papillomavirus Vaccination: System Design](https://arxiv.org/abs/2601.10718)
*Junyu Liu,Siwen Yang,Dexiu Ma,Qian Niu,Zequn Zhang,Momoko Nagai-Tanima,Tomoki Aoyama*

Main category: cs.AI

TL;DR: 开发了一个双用途AI代理系统，通过对话界面提供HPV疫苗验证信息，同时基于用户互动和社交媒体为医疗机构生成分析报告。


<details>
  <summary>Details</summary>
Motivation: 日本HPV疫苗犹豫问题严重，2013-2021年间主动推荐暂停导致信息缺口，社交媒体错误信息加剧，传统方法无法同时处理个人查询和监测群体讨论。

Method: 构建包含向量数据库（整合学术论文、政府来源、新闻媒体、社交媒体）、基于ReAct架构的RAG聊天机器人（使用五个知识源的多工具编排）以及自动报告生成系统的综合系统。

Result: 单轮评估：相关性4.83、路由4.89、参考质量4.50、正确性4.90、专业身份4.88（总体4.80）。多轮评估：上下文保持4.94、主题连贯性5.00、总体4.98。报告系统完整性4.00-5.00、正确性4.00-5.00、有用性3.67-5.00，参考有效性所有时期均为5.00。

Conclusion: 证明了集成AI代理系统在双向HPV疫苗沟通中的可行性，该架构能够提供带来源归属的验证信息，同时进行系统性公共讨论分析，其框架可转移到其他医疗场景。

Abstract: Human papillomavirus (HPV) vaccine hesitancy poses significant public health challenges, particularly in Japan where proactive vaccination recommendations were suspended from 2013 to 2021. The resulting information gap is exacerbated by misinformation on social media, and traditional ways cannot simultaneously address individual queries while monitoring population-level discourse. This study aimed to develop a dual-purpose AI agent system that provides verified HPV vaccine information through a conversational interface while generating analytical reports for medical institutions based on user interactions and social media. We implemented a system comprising: a vector database integrating academic papers, government sources, news media, and social media; a Retrieval-Augmented Generation chatbot using ReAct agent architecture with multi-tool orchestration across five knowledge sources; and an automated report generation system with modules for news analysis, research synthesis, social media sentiment analysis, and user interaction pattern identification. Performance was assessed using a 0-5 scoring scale. For single-turn evaluation, the chatbot achieved mean scores of 4.83 for relevance, 4.89 for routing, 4.50 for reference quality, 4.90 for correctness, and 4.88 for professional identity (overall 4.80). Multi-turn evaluation yielded higher scores: context retention 4.94, topic coherence 5.00, and overall 4.98. The report generation system achieved completeness 4.00-5.00, correctness 4.00-5.00, and helpfulness 3.67-5.00, with reference validity 5.00 across all periods. This study demonstrates the feasibility of an integrated AI agent system for bidirectional HPV vaccine communication. The architecture enables verified information delivery with source attribution while providing systematic public discourse analysis, with a transferable framework for adaptation to other medical contexts.

</details>


### [7] [Do You Trust Me? Cognitive-Affective Signatures of Trustworthiness in Large Language Models](https://arxiv.org/abs/2601.10719)
*Gerard Yeo,Svetlana Churina,Kokil Jaidka*

Main category: cs.AI

TL;DR: LLMs在预训练中隐式编码了心理可信度信号，无需显式监督就能区分高/低可信度文本，为可信AI系统设计提供了表征基础。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究LLMs是否以心理学上一致的方式表征在线信息的可信度，这对于LLMs在搜索、推荐和对话系统中的可信应用至关重要。

Method: 使用PEACE-Reviews数据集（包含认知评估、情感和行为意图标注），分析指令调优的LLMs（Llama 3.1 8B, Qwen 2.5 7B, Mistral 7B）在层和注意力头层面的激活差异，通过探测分析解码可信度信号。

Result: 发现LLMs能系统地区分高/低可信度文本，可信度信号在预训练中隐式编码且线性可解码；微调会优化而非重构这些表征；最强关联出现在公平性、确定性和自我责任等人类信任形成的关键维度。

Conclusion: 现代LLMs无需显式监督就能内化心理基础的可信度信号，这为设计可信、透明和值得信赖的Web生态系统AI系统提供了表征基础。

Abstract: Perceived trustworthiness underpins how users navigate online information, yet it remains unclear whether large language models (LLMs),increasingly embedded in search, recommendation, and conversational systems, represent this construct in psychologically coherent ways. We analyze how instruction-tuned LLMs (Llama 3.1 8B, Qwen 2.5 7B, Mistral 7B) encode perceived trustworthiness in web-like narratives using the PEACE-Reviews dataset annotated for cognitive appraisals, emotions, and behavioral intentions. Across models, systematic layer- and head-level activation differences distinguish high- from low-trust texts, revealing that trust cues are implicitly encoded during pretraining. Probing analyses show linearly de-codable trust signals and fine-tuning effects that refine rather than restructure these representations. Strongest associations emerge with appraisals of fairness, certainty, and accountability-self -- dimensions central to human trust formation online. These findings demonstrate that modern LLMs internalize psychologically grounded trust signals without explicit supervision, offering a representational foundation for designing credible, transparent, and trust-worthy AI systems in the web ecosystem. Code and appendix are available at: https://github.com/GerardYeo/TrustworthinessLLM.

</details>


### [8] [Building AI Agents to Improve Job Referral Requests to Strangers](https://arxiv.org/abs/2601.10726)
*Ross Chu,Yuting Huang*

Main category: cs.AI

TL;DR: 开发AI助手帮助求职者在专业社区撰写有效的职位推荐请求，通过改进代理重写请求和评估代理预测成功率，结合RAG技术提升弱请求成功率14%而不影响强请求表现。


<details>
  <summary>Details</summary>
Motivation: 帮助求职者在专业在线社区（如LinkedIn等）撰写更有效的职位推荐请求，提高获得推荐的成功率。当前求职者在请求推荐时可能缺乏有效的沟通技巧，AI助手可以提供低成本的质量改进建议。

Method: 采用双代理系统：1）改进代理（LLM）重写推荐请求；2）评估代理使用训练好的模型预测获得推荐的概率。结合检索增强生成（RAG）技术，防止对强请求的负面修改，同时增强对弱请求的改进效果。

Result: LLM修订能提高弱请求的预测成功率，但会降低强请求的成功率。结合RAG后，弱请求的预测成功率提升14%，且不会对强请求产生负面影响。模型预测的改进虽不能保证实际获得更多推荐，但为后续真实用户实验提供了低成本信号。

Conclusion: AI助手结合RAG技术可以有效帮助求职者改进推荐请求质量，特别是对弱请求有明显提升效果。这种方法为在真实用户实验前提供了低成本的评估手段，展示了AI在职业发展支持方面的应用潜力。

Abstract: This paper develops AI agents that help job seekers write effective requests for job referrals in a professional online community. The basic workflow consists of an improver agent that rewrites the referral request and an evaluator agent that measures the quality of revisions using a model trained to predict the probability of receiving referrals from other users. Revisions suggested by the LLM (large language model) increase predicted success rates for weaker requests while reducing them for stronger requests. Enhancing the LLM with Retrieval-Augmented Generation (RAG) prevents edits that worsen stronger requests while it amplifies improvements for weaker requests. Overall, using LLM revisions with RAG increases the predicted success rate for weaker requests by 14\% without degrading performance on stronger requests. Although improvements in model-predicted success do not guarantee more referrals in the real world, they provide low-cost signals for promising features before running higher-stakes experiments on real users.

</details>


### [9] [ORBITFLOW: SLO-Aware Long-Context LLM Serving with Fine-Grained KV Cache Reconfiguration](https://arxiv.org/abs/2601.10729)
*Xinyue Ma,Heelim Hong,Taegeon Um,Jongseop Lee,Seoyeong Choy,Woo-Yeon Lee,Myeongjae Jeon*

Main category: cs.AI

TL;DR: ORBITFLOW：一种细粒度自适应KV缓存管理系统，通过动态调整KV缓存位置来优化长上下文LLM服务的内存使用和延迟性能


<details>
  <summary>Details</summary>
Motivation: 长上下文LLM服务中，请求长度和批次组成在token生成过程中不断变化，导致内存占用大幅波动。现有的静态预定义卸载策略无法适应快速变化的内存需求，导致过多的CPU-GPU KV传输，产生延迟峰值和频繁的SLO违规

Method: 1. 使用轻量级ILP求解器为每个请求决定哪些层的KV缓存保留在GPU上（在内存容量约束内）
2. 在token生成过程中，当现有计划变得次优时，基于运行时反馈持续优化KV放置
3. 在高负载下，调用回退机制暂时推迟内存占用大的飞行中请求，保持整体SLO达成

Result: ORBITFLOW将TPOT和TBT的SLO达成率分别提高了66%和48%，同时将第95百分位延迟降低了38%，与现有卸载方法相比实现了高达3.3倍的吞吐量提升

Conclusion: ORBITFLOW通过细粒度和自适应的KV缓存管理，有效解决了长上下文LLM服务中的内存波动问题，显著提升了SLO达成率、降低了延迟并提高了吞吐量

Abstract: Serving long-context LLMs is challenging because request lengths and batch composition vary during token generation, causing the memory footprint to fluctuate significantly at runtime. Offloading KV caches to host memory limits effective memory usage, but existing static and predetermined offloading strategies cannot adapt to the rapidly shifting memory demands of long-context serving. This often leads to excessive CPU-to-GPU KV transfers that translate into latency spikes and frequent SLO violations. To address these challenges, we introduce ORBITFLOW, a fine-grained and adaptive KV cache management system that meets latency SLOs in long-context LLM serving. ORBITFLOW employs a lightweight ILP solver to decide which layers' KV caches to retain on the GPU for each request, within memory capacity constraints. It continuously refines KV placements based on runtime feedback when the active plan becomes suboptimal during token generation. Under heavy load, ORBITFLOW invokes a fallback mechanism to temporarily defer in-flight requests with large memory footprints, preserving overall SLO attainment. Our experiments demonstrate that ORBITFLOW improves SLO attainment for TPOT and TBT by up to 66% and 48%, respectively, while reducing the 95th percentile latency by 38% and achieving up to 3.3x higher throughput compared to existing offloading methods.

</details>


### [10] [CTHA: Constrained Temporal Hierarchical Architecture for Stable Multi-Agent LLM Systems](https://arxiv.org/abs/2601.10738)
*Percy Jardine*

Main category: cs.AI

TL;DR: CTHA是一个约束性时间分层架构，通过结构化流形投影和仲裁机制解决多时间尺度智能体中的层间冲突问题，显著提升协调稳定性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 多时间尺度智能体架构虽然提升了性能，但破坏了统一智能体系统的协调稳定性，导致严重的层间冲突、无界错误传播和可扩展性受限。

Method: 提出约束性时间分层架构(CTHA)，通过三个关键约束：1)消息契约约束，通过类型化摘要、计划和策略包形式化层间信息流；2)权威流形约束，根据时间范围限制每层的决策空间；3)仲裁器解析约束，保证多层决策的无冲突组合。

Result: 实验表明CTHA在复杂任务执行中有效，相比无约束分层基线减少47%的故障级联，提升2.3倍样本效率，并具有优越的可扩展性。

Conclusion: CTHA作为时间分层架构的原则性扩展，有助于深入理解多智能体协调，并为鲁棒自主系统的演进提供有前景的方向。

Abstract: Recently, multi-time-scale agent architectures have extended the ubiquitous single-loop paradigm by introducing temporal hierarchies with distinct cognitive layers. While yielding substantial performance gains, this diversification fundamentally compromises the coordination stability intrinsic to unified agent systems, which causes severe inter-layer conflicts, unbounded error propagation, and restricted scalability. To address these challenges, we propose Constrained Temporal Hierarchical Architecture (CTHA), a general framework that projects the inter-layer communication space onto structured manifolds to restore coordination stability, while incorporating principled arbitration mechanisms to ensure coherent decision-making. Specifically, CTHA enforces three key constraints: (1) Message Contract Constraints that formalize information flow between layers via typed summary, plan, and policy packets; (2) Authority Manifold Constraints that bound each layer's decision space according to its temporal scope; and (3) Arbiter Resolution Constraints that guarantee conflict-free composition of multi-layer decisions. Empirical experiments demonstrate that CTHA is effective for complex task execution at scale, offering 47% reduction in failure cascades, 2.3x improvement in sample efficiency, and superior scalability compared to unconstrained hierarchical baselines. We anticipate that CTHA, as a principled extension of temporal hierarchies, will contribute to a deeper understanding of multi-agent coordination and suggest promising directions for the evolution of robust autonomous systems.

</details>


### [11] [Explore with Long-term Memory: A Benchmark and Multimodal LLM-based Reinforcement Learning Framework for Embodied Exploration](https://arxiv.org/abs/2601.10744)
*Sen Wang,Bangwei Liu,Zhenkun Gao,Lizhuang Ma,Xuhong Wang,Yuan Xie,Xin Tan*

Main category: cs.AI

TL;DR: 提出LMEE框架和LMEE-Bench基准，通过MemoryExplorer方法增强智能体的长期记忆探索能力，在长时程具身任务中取得显著优势


<details>
  <summary>Details</summary>
Motivation: 现有具身智能体主要关注任务完成结果，忽视了探索过程和记忆利用的重要性，而理想的具身智能体需要具备终身学习能力来处理长时程复杂任务

Method: 提出LMEE框架统一探索认知与决策行为，构建LMEE-Bench数据集和基准，开发MemoryExplorer方法通过强化学习微调多模态大语言模型，鼓励主动记忆查询

Result: 与最先进的具身探索模型相比，该方法在长时程具身任务中取得了显著优势

Conclusion: LMEE框架和MemoryExplorer方法有效提升了具身智能体的长期记忆探索能力，为终身学习具身智能体发展提供了新方向

Abstract: An ideal embodied agent should possess lifelong learning capabilities to handle long-horizon and complex tasks, enabling continuous operation in general environments. This not only requires the agent to accurately accomplish given tasks but also to leverage long-term episodic memory to optimize decision-making. However, existing mainstream one-shot embodied tasks primarily focus on task completion results, neglecting the crucial process of exploration and memory utilization. To address this, we propose Long-term Memory Embodied Exploration (LMEE), which aims to unify the agent's exploratory cognition and decision-making behaviors to promote lifelong learning.We further construct a corresponding dataset and benchmark, LMEE-Bench, incorporating multi-goal navigation and memory-based question answering to comprehensively evaluate both the process and outcome of embodied exploration. To enhance the agent's memory recall and proactive exploration capabilities, we propose MemoryExplorer, a novel method that fine-tunes a multimodal large language model through reinforcement learning to encourage active memory querying. By incorporating a multi-task reward function that includes action prediction, frontier selection, and question answering, our model achieves proactive exploration. Extensive experiments against state-of-the-art embodied exploration models demonstrate that our approach achieves significant advantages in long-horizon embodied tasks.

</details>


### [12] [Optimisation of complex product innovation processes based on trend models with three-valued logic](https://arxiv.org/abs/2601.10768)
*Nina Bočková,Barbora Volná,Mirko Dohnal*

Main category: cs.AI

TL;DR: 该论文使用基于启发式的趋势模型研究复杂产品创新过程，通过简单趋势（增加、减少、恒定）作为最小信息强度的量化器，避免依赖数值或粗糙集，用转换图表示可能的未来或过去行为路径。


<details>
  <summary>Details</summary>
Motivation: 研究复杂产品创新过程需要有效的建模方法，传统方法可能过于依赖数值数据或复杂的数学工具，需要一种更简洁、信息强度最小化的量化方式来分析创新动态。

Method: 使用基于启发式的趋势模型，每个启发式通过简单趋势（增加、减少、恒定）表达，作为最小信息强度的量化器。定义趋势模型的解为一组具有可能转换关系的场景，用转换图表示，系统任何可能的未来或过去行为都可以表示为图中的路径。

Result: 提出了一种基于趋势的建模框架，能够用转换图表示复杂产品创新过程的所有可能行为路径，为分析创新动态提供了简洁有效的工具。

Conclusion: 基于启发式的趋势模型为复杂产品创新过程分析提供了有效的方法，通过最小信息强度的量化器和转换图表示，能够简洁地捕捉创新动态，避免对数值数据或复杂数学工具的依赖。

Abstract: This paper investigates complex product-innovation processes using models grounded in a set of heuristics. Each heuristic is expressed through simple trends -- increasing, decreasing, or constant -- which serve as minimally information-intensive quantifiers, avoiding reliance on numerical values or rough sets. A solution to a trend model is defined as a set of scenarios with possible transitions between them, represented by a transition graph. Any possible future or past behaviour of the system under study can thus be depicted by a path within this graph.

</details>


### [13] [ARC Prize 2025: Technical Report](https://arxiv.org/abs/2601.10904)
*François Chollet,Mike Knoop,Gregory Kamradt,Bryan Landers*

Main category: cs.AI

TL;DR: ARC-AGI-2竞赛显示AI在抽象推理方面仍有限制，当前最佳方法依赖精炼循环（进化程序合成等），前沿AI模型性能受知识覆盖度约束，存在基准污染问题，ARC-AGI-3将引入交互式推理挑战。


<details>
  <summary>Details</summary>
Motivation: 分析ARC-AGI-2竞赛结果，评估当前AI在少样本泛化和抽象推理方面的进展，识别精炼循环方法的重要性，揭示前沿AI模型在推理能力上的根本限制，为下一代基准ARC-AGI-3做准备。

Method: 通过调查ARC Prize 2025竞赛的1455个团队、15154个提交和90篇论文，分析顶级方法（特别是精炼循环方法，包括进化程序合成和商业AI系统应用层优化），评估前沿AI实验室（Anthropic、Google DeepMind、OpenAI、xAI）的公开模型卡性能。

Result: 竞赛最高分仅24%（ARC-AGI-2私有评估集），精炼循环成为2025年主导方法，前沿AI模型性能受知识覆盖度限制，出现新的基准污染形式，零预训练深度学习方法（仅700万参数）达到竞争性性能。

Conclusion: 当前AI在抽象推理方面仍依赖知识覆盖而非真正的泛化能力，精炼循环方法显示出潜力但仍有局限，ARC-AGI-3将引入需要探索、规划、记忆、目标获取和对齐能力的交互式推理挑战，推动AI向更真实的通用智能发展。

Abstract: The ARC-AGI benchmark series serves as a critical measure of few-shot generalization on novel tasks, a core aspect of intelligence. The ARC Prize 2025 global competition targeted the newly released ARC-AGI-2 dataset, which features greater task complexity compared to its predecessor. The Kaggle competition attracted 1,455 teams and 15,154 entries, with the top score reaching 24% on the ARC-AGI-2 private evaluation set. Paper submissions nearly doubled year-over-year to 90 entries, reflecting the growing research interest in fluid intelligence and abstract reasoning. The defining theme of 2025 is the emergence of the refinement loop -- a per-task iterative program optimization loop guided by a feedback signal. Refinement loops come in a variety of forms, in particular evolutionary program synthesis approaches and application-layer refinements to commercial AI systems. Such refinement loops are also possible in weight space, as evidenced by zero-pretraining deep learning methods which are now achieving competitive performance with remarkably small networks (7M parameters). In parallel, four frontier AI labs (Anthropic, Google DeepMind, OpenAI, and xAI) reported ARC-AGI performance in public model cards in 2025, establishing ARC-AGI as an industry standard benchmark for AI reasoning. However, our analysis indicates that current frontier AI reasoning performance remains fundamentally constrained to knowledge coverage, giving rise to new forms of benchmark contamination. In this paper, we survey the top-performing methods, examine the role of refinement loops in AGI progress, discuss knowledge-dependent overfitting, and preview ARC-AGI-3, which introduces interactive reasoning challenges that require exploration, planning, memory, goal acquisition, and alignment capabilities.

</details>


### [14] [What Matters in Data Curation for Multimodal Reasoning? Insights from the DCVLR Challenge](https://arxiv.org/abs/2601.10922)
*Yosub Shin,Michael Buriek,Boris Sobolev,Pavel Bushuyeu,Vikas Kumar,Haoyang Xu,Samuel Watson,Igor Molybog*

Main category: cs.AI

TL;DR: 该研究通过NeurIPS 2025 DCVLR挑战赛探索多模态推理的数据筛选，发现基于难度的样本选择是性能提升的关键，而数据集大小增加主要降低方差而非提升准确率。


<details>
  <summary>Details</summary>
Motivation: 研究多模态推理中的数据筛选问题，通过固定模型和训练协议来隔离数据集选择的影响，探索在数据有限情况下如何有效提升多模态推理性能。

Method: 使用基于Walton多模态冷启动的紧凑筛选数据集，在DCVLR挑战赛中固定模型和训练协议，通过赛后消融实验分析不同数据筛选策略（难度选择、数据集大小、多样性启发式、合成增强）的效果。

Result: 基于难度的样本选择是性能提升的主要驱动力；增加数据集大小不能可靠提升平均准确率，但能降低运行间方差；常用的多样性和合成增强启发式方法没有额外益处，反而可能降低性能。

Conclusion: DCVLR挑战赛代表了饱和状态评估，强调了对齐和难度在数据高效多模态推理中的核心作用，为数据筛选提供了重要指导。

Abstract: We study data curation for multimodal reasoning through the NeurIPS 2025 Data Curation for Vision-Language Reasoning (DCVLR) challenge, which isolates dataset selection by fixing the model and training protocol. Using a compact curated dataset derived primarily from Walton Multimodal Cold Start, our submission placed first in the challenge. Through post-competition ablations, we show that difficulty-based example selection on an aligned base dataset is the dominant driver of performance gains. Increasing dataset size does not reliably improve mean accuracy under the fixed training recipe, but mainly reduces run-to-run variance, while commonly used diversity and synthetic augmentation heuristics provide no additional benefit and often degrade performance. These results characterize DCVLR as a saturation-regime evaluation and highlight the central role of alignment and difficulty in data-efficient multimodal reasoning.

</details>


### [15] [TANDEM: Temporal-Aware Neural Detection for Multimodal Hate Speech](https://arxiv.org/abs/2601.11178)
*Girish A. Koushik,Helen Treharne,Diptesh Kanojia*

Main category: cs.AI

TL;DR: TANDEM：一个统一的框架，将视听仇恨检测从二元分类任务转变为结构化推理问题，通过跨模态强化学习实现精确的目标识别和时间定位，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 社交媒体中长格式多模态内容日益增多，有害叙事通过音频、视觉和文本线索的复杂交互构建。现有自动化系统虽然能高精度标记仇恨言论，但通常作为"黑箱"运行，无法提供人类在环审核所需的细粒度、可解释证据（如精确时间戳和目标身份）。

Method: 提出TANDEM框架，采用新颖的串联强化学习策略，其中视觉-语言和音频-语言模型通过自约束跨模态上下文相互优化，在不需要密集帧级监督的情况下稳定长时间序列的推理。

Result: 在三个基准数据集上的实验表明，TANDEM显著优于零样本和上下文增强基线，在HateMM数据集上目标识别F1达到0.73（比最先进方法提高30%），同时保持精确的时间定位。二元检测稳健，但在多类设置中区分冒犯性和仇恨内容仍具挑战性。

Conclusion: 即使在复杂的多模态设置中，结构化、可解释的对齐也是可实现的，为下一代透明且可操作的在线安全审核工具提供了蓝图。跨模态强化学习策略能够稳定长时间序列推理，无需密集监督。

Abstract: Social media platforms are increasingly dominated by long-form multimodal content, where harmful narratives are constructed through a complex interplay of audio, visual, and textual cues. While automated systems can flag hate speech with high accuracy, they often function as "black boxes" that fail to provide the granular, interpretable evidence, such as precise timestamps and target identities, required for effective human-in-the-loop moderation. In this work, we introduce TANDEM, a unified framework that transforms audio-visual hate detection from a binary classification task into a structured reasoning problem. Our approach employs a novel tandem reinforcement learning strategy where vision-language and audio-language models optimize each other through self-constrained cross-modal context, stabilizing reasoning over extended temporal sequences without requiring dense frame-level supervision. Experiments across three benchmark datasets demonstrate that TANDEM significantly outperforms zero-shot and context-augmented baselines, achieving 0.73 F1 in target identification on HateMM (a 30% improvement over state-of-the-art) while maintaining precise temporal grounding. We further observe that while binary detection is robust, differentiating between offensive and hateful content remains challenging in multi-class settings due to inherent label ambiguity and dataset imbalance. More broadly, our findings suggest that structured, interpretable alignment is achievable even in complex multimodal settings, offering a blueprint for the next generation of transparent and actionable online safety moderation tools.

</details>


### [16] [AdaMARP: An Adaptive Multi-Agent Interaction Framework for General Immersive Role-Playing](https://arxiv.org/abs/2601.11007)
*Zhenhua Xu,Dongsheng Chen,Shuo Wang,Jian Li,Chengjie Wang,Meng Han,Yabiao Wang*

Main category: cs.AI

TL;DR: AdaMARP是一个自适应多智能体角色扮演框架，通过沉浸式消息格式和显式场景管理器解决现有系统沉浸感不足、适应性差的问题，在角色一致性、环境基础和叙事连贯性方面显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有LLM角色扮演系统存在沉浸感有限和适应性不足的问题，通常对环境动态信息建模不足，假设场景和角色基本静态，对多角色编排、场景转换和实时角色引入支持不够。

Method: 提出自适应多智能体角色扮演框架AdaMARP，包含：1）沉浸式消息格式，交织[思考]、（动作）、<环境>和对话；2）显式场景管理器，通过离散动作（初始化场景、选择说话者、切换场景、添加角色、结束）及相应理由来管理角色扮演。

Result: 构建AdaRPSet训练角色模型，AdaSMSet监督编排决策，并引入AdaptiveBench进行轨迹级评估。实验显示：AdaRPSet提升角色一致性、环境基础和叙事连贯性，8B角色模型超越多个商业LLM；AdaSMSet实现更流畅场景转换和自然角色引入，仅用14B LLM即超越Claude Sonnet 4.5。

Conclusion: AdaMARP框架通过创新的沉浸式消息格式和显式场景管理器，显著提升了多智能体角色扮演的沉浸感、适应性和编排能力，在不同骨干模型和规模上均取得一致改进。

Abstract: LLM role-playing aims to portray arbitrary characters in interactive narratives, yet existing systems often suffer from limited immersion and adaptability. They typically under-model dynamic environmental information and assume largely static scenes and casts, offering insufficient support for multi-character orchestration, scene transitions, and on-the-fly character introduction. We propose an adaptive multi-agent role-playing framework, AdaMARP, featuring an immersive message format that interleaves [Thought], (Action), <Environment>, and Speech, together with an explicit Scene Manager that governs role-playing through discrete actions (init_scene, pick_speaker, switch_scene, add_role, end) accompanied by rationales. To train these capabilities, we construct AdaRPSet for the Actor Model and AdaSMSet for supervising orchestration decisions, and introduce AdaptiveBench for trajectory-level evaluation. Experiments across multiple backbones and model scales demonstrate consistent improvements: AdaRPSet enhances character consistency, environment grounding, and narrative coherence, with an 8B actor outperforming several commercial LLMs, while AdaSMSet enables smoother scene transitions and more natural role introductions, surpassing Claude Sonnet 4.5 using only a 14B LLM.

</details>


### [17] [Efficient Protein Optimization via Structure-aware Hamiltonian Dynamics](https://arxiv.org/abs/2601.11012)
*Jiahao Wang,Shuangjia Zheng*

Main category: cs.AI

TL;DR: HADES是一种基于贝叶斯优化的蛋白质序列设计方法，利用哈密顿动力学高效采样结构感知的后验分布，通过两阶段编码器-解码器框架学习蛋白质结构与功能关系，在保持结构相似性的同时优化蛋白质性质。


<details>
  <summary>Details</summary>
Motivation: 现有基于序列的蛋白质优化方法难以处理高维复杂性，主要因为忽略了表位效应和结构约束。需要一种能够同时考虑蛋白质结构和序列相互约束的优化方法。

Method: 提出HADES方法：1）使用哈密顿动力学进行贝叶斯优化，利用动量和不确定性快速探索有前景的区域；2）引入位置离散化程序从连续状态系统生成离散蛋白质序列；3）采用两阶段编码器-解码器框架作为后验代理模型，学习突变邻居间的结构-功能关系，构建平滑的采样景观。

Result: 在计算机模拟评估中，HADES在大多数指标上优于现有最先进方法。该方法能够利用蛋白质结构和序列的相互约束，设计出结构相似但性质优化的蛋白质序列。

Conclusion: HADES通过结合哈密顿动力学和结构感知的贝叶斯优化，有效解决了蛋白质序列设计中的高维复杂性问题，为生物技术和医学领域的蛋白质工程提供了有力工具。

Abstract: The ability to engineer optimized protein variants has transformative potential for biotechnology and medicine. Prior sequence-based optimization methods struggle with the high-dimensional complexities due to the epistasis effect and the disregard for structural constraints. To address this, we propose HADES, a Bayesian optimization method utilizing Hamiltonian dynamics to efficiently sample from a structure-aware approximated posterior. Leveraging momentum and uncertainty in the simulated physical movements, HADES enables rapid transition of proposals toward promising areas. A position discretization procedure is introduced to propose discrete protein sequences from such a continuous state system. The posterior surrogate is powered by a two-stage encoder-decoder framework to determine the structure and function relationships between mutant neighbors, consequently learning a smoothed landscape to sample from. Extensive experiments demonstrate that our method outperforms state-of-the-art baselines in in-silico evaluations across most metrics. Remarkably, our approach offers a unique advantage by leveraging the mutual constraints between protein structure and sequence, facilitating the design of protein sequences with similar structures and optimized properties. The code and data are publicly available at https://github.com/GENTEL-lab/HADES.

</details>


### [18] [BAPO: Boundary-Aware Policy Optimization for Reliable Agentic Search](https://arxiv.org/abs/2601.11037)
*Shiyu Liu,Yongjing Yin,Jianhao Yan,Yunbo Tang,Qinggang Zhang,Bei Li,Xin Chen,Jingang Wang,Xunliang Cai,Jinsong Su*

Main category: cs.AI

TL;DR: 提出BAPO框架，通过边界感知奖励和自适应奖励调节器，让基于RL的智能搜索代理学会在证据不足或推理达到极限时说"我不知道"，提高可靠性而不牺牲准确性。


<details>
  <summary>Details</summary>
Motivation: 虽然基于强化学习的智能搜索代理通过动态规划和外部搜索显著提高了准确性，但存在可靠性问题：这些代理无法识别推理边界，即使在证据不足或推理达到极限时也很少承认"我不知道"，导致产生看似合理但不可靠的答案，这在现实场景中带来重大风险。

Method: 提出边界感知策略优化(BAPO)框架，包含两个关键组件：(1)基于群体的边界感知奖励，鼓励代理在推理达到极限时给出"我不知道"响应；(2)自适应奖励调节器，在早期探索阶段战略性地暂停此奖励，防止模型利用"我不知道"作为捷径。

Result: 在四个基准测试上的广泛实验表明，BAPO显著增强了智能搜索的整体可靠性。

Conclusion: BAPO框架成功解决了智能搜索代理的可靠性问题，使其能够识别推理边界并在适当情况下承认知识局限，同时保持高准确性，为现实世界应用提供了更可靠的解决方案。

Abstract: RL-based agentic search enables LLMs to solve complex questions via dynamic planning and external search. While this approach significantly enhances accuracy with agent policies optimized via large-scale reinforcement learning, we identify a critical gap in reliability: these agents fail to recognize their reasoning boundaries and rarely admit ``I DON'T KNOW'' (IDK) even when evidence is insufficient or reasoning reaches its limit. The lack of reliability often leads to plausible but unreliable answers, introducing significant risks in many real-world scenarios. To this end, we propose Boundary-Aware Policy Optimization (BAPO), a novel RL framework designed to cultivate reliable boundary awareness without compromising accuracy. BAPO introduces two key components: (i) a group-based boundary-aware reward that encourages an IDK response only when the reasoning reaches its limit, and (ii) an adaptive reward modulator that strategically suspends this reward during early exploration, preventing the model from exploiting IDK as a shortcut. Extensive experiments on four benchmarks demonstrate that BAPO substantially enhances the overall reliability of agentic search.

</details>


### [19] [AgencyBench: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World Contexts](https://arxiv.org/abs/2601.11044)
*Keyu Li,Junhao Shi,Yang Xiao,Mohan Jiang,Jie Sun,Yunze Wu,Shijie Xia,Xiaojie Cai,Tianze Xu,Weiye Si,Wenjie Li,Dequan Wang,Pengfei Liu*

Main category: cs.AI

TL;DR: AgencyBench是一个全面的自主代理基准测试，评估6种核心代理能力，包含32个真实场景和138个任务，使用用户模拟代理和Docker沙箱进行自动化评估。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注单一代理能力，无法捕捉长时程真实场景，且依赖人工反馈存在可扩展性瓶颈，需要更全面的自动化评估基准。

Method: 从日常AI使用中提取真实场景，构建包含138个任务的基准测试，使用用户模拟代理提供迭代反馈，Docker沙箱进行视觉和功能评估。

Result: 闭源模型显著优于开源模型（48.4% vs 32.1%），在资源效率、反馈驱动的自我修正和特定工具使用偏好方面存在显著差异，专有模型在其原生生态系统中表现更好。

Conclusion: AgencyBench为下一代代理提供了关键测试平台，强调了模型架构与代理框架协同优化的必要性，揭示了自主代理的未来发展方向。

Abstract: Large Language Models (LLMs) based autonomous agents demonstrate multifaceted capabilities to contribute substantially to economic production. However, existing benchmarks remain focused on single agentic capability, failing to capture long-horizon real-world scenarios. Moreover, the reliance on human-in-the-loop feedback for realistic tasks creates a scalability bottleneck, hindering automated rollout collection and evaluation. To bridge this gap, we introduce AgencyBench, a comprehensive benchmark derived from daily AI usage, evaluating 6 core agentic capabilities across 32 real-world scenarios, comprising 138 tasks with specific queries, deliverables, and rubrics. These scenarios require an average of 90 tool calls, 1 million tokens, and hours of execution time to resolve. To enable automated evaluation, we employ a user simulation agent to provide iterative feedback, and a Docker sandbox to conduct visual and functional rubric-based assessment. Experiments reveal that closed-source models significantly outperform open-source models (48.4% vs 32.1%). Further analysis reveals significant disparities across models in resource efficiency, feedback-driven self-correction, and specific tool-use preferences. Finally, we investigate the impact of agentic scaffolds, observing that proprietary models demonstrate superior performance within their native ecosystems (e.g., Claude-4.5-Opus via Claude-Agent-SDK), while open-source models exhibit distinct performance peaks, suggesting potential optimization for specific execution frameworks. AgencyBench serves as a critical testbed for next-generation agents, highlighting the necessity of co-optimizing model architecture with agentic frameworks. We believe this work sheds light on the future direction of autonomous agents, and we release the full benchmark and evaluation toolkit at https://github.com/GAIR-NLP/AgencyBench.

</details>


### [20] [MiCA: A Mobility-Informed Causal Adapter for Lightweight Epidemic Forecasting](https://arxiv.org/abs/2601.11089)
*Suhan Guo,Jiahong Deng,Furao Shen*

Main category: cs.AI

TL;DR: MiCA是一个轻量级、架构无关的流行病预测模块，通过因果发现推断移动关系，并利用门控残差混合将其整合到时序预测模型中，在数据有限和噪声条件下提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 传染病动态的准确预测对公共卫生规划和干预至关重要。人类移动性在塑造疫情空间传播中起核心作用，但移动数据通常噪声大、间接且难以与疾病记录可靠整合。同时，流行病病例时间序列通常较短且时间分辨率较粗，这限制了依赖干净丰富数据的参数密集型移动感知预测器的有效性。

Method: 提出移动性感知因果适配器（MiCA），这是一个轻量级且架构无关的模块。MiCA通过因果发现推断移动关系，并通过门控残差混合将其整合到时序预测模型中。这种设计允许轻量级预测器选择性地利用移动性衍生的空间结构，同时在噪声和数据有限条件下保持鲁棒性，无需引入图神经网络或完整注意力等重型关系组件。

Result: 在四个真实世界流行病数据集（包括COVID-19发病率、COVID-19死亡率、流感和登革热）上的广泛实验表明，MiCA持续改进轻量级时序骨干网络，在预测时间范围内平均相对误差减少7.5%。此外，MiCA在保持轻量级的同时，达到了与最先进的时空模型竞争的性能。

Conclusion: MiCA是一个有效的轻量级模块，能够在数据有限和噪声条件下，通过因果发现和门控残差混合整合移动性信息，显著提升流行病预测性能，同时保持模型轻量化。

Abstract: Accurate forecasting of infectious disease dynamics is critical for public health planning and intervention. Human mobility plays a central role in shaping the spatial spread of epidemics, but mobility data are noisy, indirect, and difficult to integrate reliably with disease records. Meanwhile, epidemic case time series are typically short and reported at coarse temporal resolution. These conditions limit the effectiveness of parameter-heavy mobility-aware forecasters that rely on clean and abundant data. In this work, we propose the Mobility-Informed Causal Adapter (MiCA), a lightweight and architecture-agnostic module for epidemic forecasting. MiCA infers mobility relations through causal discovery and integrates them into temporal forecasting models via gated residual mixing. This design allows lightweight forecasters to selectively exploit mobility-derived spatial structure while remaining robust under noisy and data-limited conditions, without introducing heavy relational components such as graph neural networks or full attention. Extensive experiments on four real-world epidemic datasets, including COVID-19 incidence, COVID-19 mortality, influenza, and dengue, show that MiCA consistently improves lightweight temporal backbones, achieving an average relative error reduction of 7.5\% across forecasting horizons. Moreover, MiCA attains performance competitive with SOTA spatio-temporal models while remaining lightweight.

</details>


### [21] [ReCreate: Reasoning and Creating Domain Agents Driven by Experience](https://arxiv.org/abs/2601.11100)
*Zhezheng Hao,Hong Wang,Jian Luo,Jianqing Zhang,Yuyan Zhou,Qiang Lin,Can Wang,Hande Dong,Jiawei Chen*

Main category: cs.AI

TL;DR: ReCreate是一个基于经验驱动的自动领域智能体创建框架，通过智能体交互历史学习成功与失败原因，实现自动创建和优化领域智能体。


<details>
  <summary>Details</summary>
Motivation: 当前大多数实用智能体仍需要人工设计，任务差异大导致构建成本高。现有自动化方法将智能体生成视为黑盒过程，仅依赖最终性能指标，忽略了成功/失败的关键证据，且计算成本高。

Method: 提出ReCreate框架，采用智能体即优化器范式，包含三个核心组件：1) 经验存储与检索机制；2) 推理-创建协同管道，将执行经验映射到脚手架编辑；3) 分层更新，将实例级细节抽象为可复用领域模式。

Result: 在多个不同领域的实验中，ReCreate始终优于人工设计的智能体和现有自动化智能体生成方法，即使从最小种子脚手架开始也能取得良好效果。

Conclusion: ReCreate通过系统利用智能体交互历史中的具体信号，实现了自动创建和适应领域智能体，解决了现有方法的局限性，为智能体自动化生成提供了有效解决方案。

Abstract: Large Language Model agents are reshaping the industrial landscape. However, most practical agents remain human-designed because tasks differ widely, making them labor-intensive to build. This situation poses a central question: can we automatically create and adapt domain agents in the wild? While several recent approaches have sought to automate agent creation, they typically treat agent generation as a black-box procedure and rely solely on final performance metrics to guide the process. Such strategies overlook critical evidence explaining why an agent succeeds or fails, and often require high computational costs. To address these limitations, we propose ReCreate, an experience-driven framework for the automatic creation of domain agents. ReCreate systematically leverages agent interaction histories, which provide rich concrete signals on both the causes of success or failure and the avenues for improvement. Specifically, we introduce an agent-as-optimizer paradigm that effectively learns from experience via three key components: (i) an experience storage and retrieval mechanism for on-demand inspection; (ii) a reasoning-creating synergy pipeline that maps execution experience into scaffold edits; and (iii) hierarchical updates that abstract instance-level details into reusable domain patterns. In experiments across diverse domains, ReCreate consistently outperforms human-designed agents and existing automated agent generation methods, even when starting from minimal seed scaffolds.

</details>


### [22] [Do We Always Need Query-Level Workflows? Rethinking Agentic Workflow Generation for Multi-Agent Systems](https://arxiv.org/abs/2601.11147)
*Zixu Wang,Bingbing Xu,Yige Yuan,Huawei Shen,Xueqi Cheng*

Main category: cs.AI

TL;DR: SCALE框架通过任务级工作流生成和自预测评估，在保持性能的同时大幅降低token成本


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统工作流生成方法在任务级和查询级各有优劣，但相对成本和效益不明确，且基于执行的评估方法token成本高且不可靠

Method: 提出SCALE框架：通过少量样本校准的自预测优化器进行任务级工作流生成，替代昂贵的全验证执行评估

Result: SCALE在多个数据集上平均性能仅下降0.61%，同时将总体token使用量减少高达83%

Conclusion: 查询级工作流生成并非总是必要，任务级工作流配合高效的评估方法可以在保持性能的同时显著降低成本

Abstract: Multi-Agent Systems (MAS) built on large language models typically solve complex tasks by coordinating multiple agents through workflows. Existing approaches generates workflows either at task level or query level, but their relative costs and benefits remain unclear. After rethinking and empirical analyses, we show that query-level workflow generation is not always necessary, since a small set of top-K best task-level workflows together already covers equivalent or even more queries. We further find that exhaustive execution-based task-level evaluation is both extremely token-costly and frequently unreliable. Inspired by the idea of self-evolution and generative reward modeling, we propose a low-cost task-level generation framework \textbf{SCALE}, which means \underline{\textbf{S}}elf prediction of the optimizer with few shot \underline{\textbf{CAL}}ibration for \underline{\textbf{E}}valuation instead of full validation execution. Extensive experiments demonstrate that \textbf{SCALE} maintains competitive performance, with an average degradation of just 0.61\% compared to existing approach across multiple datasets, while cutting overall token usage by up to 83\%.

</details>


### [23] [Policy-Based Deep Reinforcement Learning Hyperheuristics for Job-Shop Scheduling Problems](https://arxiv.org/abs/2601.11189)
*Sofiene Lassoued,Asrat Gobachew,Stefan Lier,Andreas Schwung*

Main category: cs.AI

TL;DR: 提出基于策略的深度强化学习超启发式框架，用于解决作业车间调度问题，通过动作预过滤和承诺机制提升性能


<details>
  <summary>Details</summary>
Motivation: 传统启发式和元启发式方法在作业车间调度问题上存在局限性，需要更智能的动态调度规则切换机制

Method: 基于策略的深度强化学习超启发式框架，包含动作预过滤机制（限制决策到可行动作）和承诺机制（控制启发式切换频率），比较确定性贪婪选择和随机采样两种策略

Result: 在标准JSSP基准测试中，该方法优于传统启发式、元启发式和最近的基于神经网络的调度方法

Conclusion: 提出的深度强化学习超启发式框架通过动作预过滤和承诺机制，能够有效解决作业车间调度问题，展示了强化学习在组合优化问题中的应用潜力

Abstract: This paper proposes a policy-based deep reinforcement learning hyper-heuristic framework for solving the Job Shop Scheduling Problem. The hyper-heuristic agent learns to switch scheduling rules based on the system state dynamically. We extend the hyper-heuristic framework with two key mechanisms. First, action prefiltering restricts decision-making to feasible low-level actions, enabling low-level heuristics to be evaluated independently of environmental constraints and providing an unbiased assessment. Second, a commitment mechanism regulates the frequency of heuristic switching. We investigate the impact of different commitment strategies, from step-wise switching to full-episode commitment, on both training behavior and makespan. Additionally, we compare two action selection strategies at the policy level: deterministic greedy selection and stochastic sampling. Computational experiments on standard JSSP benchmarks demonstrate that the proposed approach outperforms traditional heuristics, metaheuristics, and recent neural network-based scheduling methods

</details>


### [24] [Beyond Model Scaling: Test-Time Intervention for Efficient Deep Reasoning](https://arxiv.org/abs/2601.11252)
*Qianyue Wang,Jinwu Hu,Yufeng Wang,Huanxiang Lin,Bolin Chen,Zhiquan Wen,Yaofo Chen,Mingkui Tan*

Main category: cs.AI

TL;DR: 提出Think-with-Me交互式推理范式，在推理过程中引入外部反馈干预，通过暂停推理进行多标准评估，自适应调整推理长度，在有限上下文窗口下实现准确率与推理效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型存在过度思考和推理偏移问题，导致计算成本增加和性能下降。现有高效推理方法采用闭环方式，缺乏外部干预机制来引导推理过程。

Method: 1) 利用过渡连词作为干预点，暂停推理进行外部反馈；2) 采用多标准评估（合理性和完整性）生成反馈；3) 使用Group Relative Policy Optimization训练模型适应交互模式；4) 自适应扩展或终止推理以减少冗余。

Result: 在AIME24上，Think-with-Me在8K窗口下比QwQ-32B准确率提升7.19%，同时平均推理长度减少81%。该范式在安全和创造性任务上也表现良好。

Conclusion: Think-with-Me通过引入外部反馈干预，实现了推理效率与准确性的平衡，为大型推理模型的高效推理提供了新的交互范式。

Abstract: Large Reasoning Models (LRMs) excel at multi-step reasoning but often suffer from inefficient reasoning processes like overthinking and overshoot, where excessive or misdirected reasoning increases computational cost and degrades performance. Existing efficient reasoning methods operate in a closed-loop manner, lacking mechanisms for external intervention to guide the reasoning process. To address this, we propose Think-with-Me, a novel test-time interactive reasoning paradigm that introduces external feedback intervention into the reasoning process. Our key insights are that transitional conjunctions serve as natural points for intervention, signaling phases of self-validation or exploration and using transitional words appropriately to prolong the reasoning enhances performance, while excessive use affects performance. Building on these insights, Think-with-Me pauses reasoning at these points for external feedback, adaptively extending or terminating reasoning to reduce redundancy while preserving accuracy. The feedback is generated via a multi-criteria evaluation (rationality and completeness) and comes from either human or LLM proxies. We train the target model using Group Relative Policy Optimization (GRPO) to adapt to this interactive mode. Experiments show that Think-with-Me achieves a superior balance between accuracy and reasoning length under limited context windows. On AIME24, Think-with-Me outperforms QwQ-32B by 7.19% in accuracy while reducing average reasoning length by 81% under an 8K window. The paradigm also benefits security and creative tasks.

</details>


### [25] [XChoice: Explainable Evaluation of AI-Human Alignment in LLM-based Constrained Choice Decision Making](https://arxiv.org/abs/2601.11286)
*Weihong Qi,Fan Huang,Rasika Muralidharan,Jisun An,Haewoon Kwak*

Main category: cs.AI

TL;DR: XChoice是一个可解释的框架，用于评估约束决策中AI与人类的对齐程度，通过机制建模而非表面结果匹配来诊断错位问题。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法主要关注准确性、F1分数等结果一致性指标，无法揭示AI决策机制与人类决策机制的深层差异，特别是在约束决策场景中。

Method: XChoice构建基于机制的决策模型，拟合人类数据和LLM生成决策，恢复可解释参数（决策因素重要性、约束敏感性、权衡关系），通过比较参数向量评估对齐度。

Result: 在美国时间分配研究中发现模型与活动间的异质性对齐，黑人和已婚群体存在显著错位；通过不变性分析验证鲁棒性，RAG干预实现针对性缓解。

Conclusion: XChoice提供基于机制的评估指标，能够诊断错位问题并支持超越表面结果匹配的改进措施，为AI-人类对齐提供更深入的洞察。

Abstract: We present XChoice, an explainable framework for evaluating AI-human alignment in constrained decision making. Moving beyond outcome agreement such as accuracy and F1 score, XChoice fits a mechanism-based decision model to human data and LLM-generated decisions, recovering interpretable parameters that capture the relative importance of decision factors, constraint sensitivity, and implied trade-offs. Alignment is assessed by comparing these parameter vectors across models, options, and subgroups. We demonstrate XChoice on Americans' daily time allocation using the American Time Use Survey (ATUS) as human ground truth, revealing heterogeneous alignment across models and activities and salient misalignment concentrated in Black and married groups. We further validate robustness of XChoice via an invariance analysis and evaluate targeted mitigation with a retrieval augmented generation (RAG) intervention. Overall, XChoice provides mechanism-based metrics that diagnose misalignment and support informed improvements beyond surface outcome matching.

</details>


### [26] [AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems](https://arxiv.org/abs/2601.11354)
*Weiyi Wang,Xinchi Chen,Jingjing Gong,Xuanjing Huang,Xipeng Qiu*

Main category: cs.AI

TL;DR: AstroReason-Bench是一个用于评估智能体在空间规划问题中规划能力的基准测试，该基准整合了多种调度机制，发现当前智能体在物理约束下的表现远不如专用求解器。


<details>
  <summary>Details</summary>
Motivation: 现有智能体基准主要关注符号化或弱接地环境，在物理约束的现实世界领域表现未被充分探索。空间规划问题具有高风险、异质目标、严格物理约束和长时程决策等特点，需要专门的评估基准。

Method: 引入AstroReason-Bench基准，整合地面站通信和敏捷地球观测等多种调度机制，提供统一的智能体导向交互协议，并在多种最先进的开源和闭源智能体LLM系统上进行评估。

Result: 当前智能体在空间规划问题上的表现显著低于专用求解器，突显了在现实约束下通用规划的关键局限性。

Conclusion: AstroReason-Bench为未来智能体研究提供了一个具有挑战性和诊断性的测试平台，揭示了当前通用规划智能体在物理约束现实问题中的不足。

Abstract: Recent advances in agentic Large Language Models (LLMs) have positioned them as generalist planners capable of reasoning and acting across diverse tasks. However, existing agent benchmarks largely focus on symbolic or weakly grounded environments, leaving their performance in physics-constrained real-world domains underexplored. We introduce AstroReason-Bench, a comprehensive benchmark for evaluating agentic planning in Space Planning Problems (SPP), a family of high-stakes problems with heterogeneous objectives, strict physical constraints, and long-horizon decision-making. AstroReason-Bench integrates multiple scheduling regimes, including ground station communication and agile Earth observation, and provides a unified agent-oriented interaction protocol. Evaluating on a range of state-of-the-art open- and closed-source agentic LLM systems, we find that current agents substantially underperform specialized solvers, highlighting key limitations of generalist planning under realistic constraints. AstroReason-Bench offers a challenging and diagnostic testbed for future agentic research.

</details>


### [27] [Hyperparameter Optimization of Constraint Programming Solvers](https://arxiv.org/abs/2601.11389)
*Hedieh Haddad,Thibault Falque,Pierre Talbot,Pascal Bouvry*

Main category: cs.AI

TL;DR: 提出了一种用于约束规划求解器超参数优化的两阶段框架（探测与求解算法），通过贝叶斯优化等方法自动寻找最优配置，在多个求解器上显著提升了求解性能。


<details>
  <summary>Details</summary>
Motivation: 约束规划求解器的性能对超参数选择高度敏感，手动寻找最优配置需要专业知识且耗时。需要一种自动化的超参数优化方法来提升求解器性能。

Method: 提出了探测与求解算法，将时间预算分为两个阶段：1）探测阶段使用可配置的超参数优化方法（贝叶斯优化和汉明距离搜索）探索不同超参数集；2）求解阶段使用找到的最佳配置在剩余时间内解决问题。

Result: 在ACE和Choco两个求解器上测试了114个组合问题实例。使用贝叶斯优化时，算法在25.4%的ACE实例中优于默认配置，57.9%持平；在38.6%的Choco实例中优于默认配置。贝叶斯优化始终优于汉明距离搜索。

Conclusion: 探测与求解算法提供了一种实用、资源感知的约束求解器调优方法，能在多样化问题类型上实现稳健的性能提升，模型化探索优于简单局部搜索。

Abstract: The performance of constraint programming solvers is highly sensitive to the choice of their hyperparameters. Manually finding the best solver configuration is a difficult, time-consuming task that typically requires expert knowledge. In this paper, we introduce probe and solve algorithm, a novel two-phase framework for automated hyperparameter optimization integrated into the CPMpy library. This approach partitions the available time budget into two phases: a probing phase that explores different sets of hyperparameters using configurable hyperparameter optimization methods, followed by a solving phase where the best configuration found is used to tackle the problem within the remaining time.
  We implement and compare two hyperparameter optimization methods within the probe and solve algorithm: Bayesian optimization and Hamming distance search. We evaluate the algorithm on two different constraint programming solvers, ACE and Choco, across 114 combinatorial problem instances, comparing their performance against the solver's default configurations.
  Results show that using Bayesian optimization, the algorithm outperforms the solver's default configurations, improving solution quality for ACE in 25.4% of instances and matching the default performance in 57.9%, and for Choco, achieving superior results in 38.6% of instances. It also consistently surpasses Hamming distance search within the same framework, confirming the advantage of model-based exploration over simple local search. Overall, the probe and solve algorithm offers a practical, resource-aware approach for tuning constraint solvers that yields robust improvements across diverse problem types.

</details>


### [28] [Exploring LLM Features in Predictive Process Monitoring for Small-Scale Event-Logs](https://arxiv.org/abs/2601.11468)
*Alessandro Padella,Massimiliano de Leoni,Marlon Dumas*

Main category: cs.AI

TL;DR: 本文扩展了基于LLM的预测性流程监控框架，通过全面评估其在多个关键绩效指标上的泛化能力、语义利用和推理机制，证明在数据稀缺场景下LLM优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 预测性流程监控旨在预测正在进行的流程结果，传统方法使用机器学习和深度学习。本文旨在扩展先前基于LLM的框架，评估其在多种关键绩效指标上的表现，特别是在数据稀缺情况下的优势。

Method: 扩展了基于LLM的预测性流程监控框架，通过提示工程进行预测。在三个不同的事件日志上进行实证评估，关注总时间和活动发生次数两个关键绩效指标。特别研究了LLM在仅有100条轨迹的数据稀缺设置下的表现。

Result: 在数据稀缺设置（仅100条轨迹）下，LLM超越了基准方法。实验表明LLM利用了其先验知识和训练轨迹间的内部相关性。LLM不仅复制现有预测方法，而是执行高阶推理来生成预测。

Conclusion: LLM在预测性流程监控中具有优势，特别是在数据稀缺场景下。它能够利用语义知识和执行复杂推理，为流程预测提供了新的有效方法。

Abstract: Predictive Process Monitoring is a branch of process mining that aims to predict the outcome of an ongoing process. Recently, it leveraged machine-and-deep learning architectures. In this paper, we extend our prior LLM-based Predictive Process Monitoring framework, which was initially focused on total time prediction via prompting. The extension consists of comprehensively evaluating its generality, semantic leverage, and reasoning mechanisms, also across multiple Key Performance Indicators. Empirical evaluations conducted on three distinct event logs and across the Key Performance Indicators of Total Time and Activity Occurrence prediction indicate that, in data-scarce settings with only 100 traces, the LLM surpasses the benchmark methods. Furthermore, the experiments also show that the LLM exploits both its embodied prior knowledge and the internal correlations among training traces. Finally, we examine the reasoning strategies employed by the model, demonstrating that the LLM does not merely replicate existing predictive methods but performs higher-order reasoning to generate the predictions.

</details>


### [29] [Health Facility Location in Ethiopia: Leveraging LLMs to Integrate Expert Knowledge into Algorithmic Planning](https://arxiv.org/abs/2601.11479)
*Yohai Trabelsi,Guojun Xiong,Fentabil Getnet,Stéphane Verguet,Milind Tambe*

Main category: cs.AI

TL;DR: 本文提出LEG框架，结合大语言模型和优化算法，帮助埃塞俄比亚卫生部在资源有限情况下优先升级卫生站，平衡人口覆盖和专家偏好。


<details>
  <summary>Details</summary>
Motivation: 埃塞俄比亚卫生部需要升级卫生站以改善基本医疗服务可及性，但资源有限，需要优先考虑哪些设施进行升级。传统优化方法需要明确的量化目标，而利益相关者的标准通常用自然语言表达，难以形式化，因此需要一种能结合专家知识和优化技术的方法。

Method: 提出LEG（大语言模型和扩展贪心）框架，结合具有理论保证的人口覆盖优化近似算法与LLM驱动的迭代优化。LLM用于纳入专家定性指导，通过人机对齐确保解决方案反映专家意见，同时保持覆盖保证。

Result: 在埃塞俄比亚三个地区的真实数据上进行实验，证明该框架的有效性，能够为公平、数据驱动的卫生系统规划提供信息。

Conclusion: LEG框架成功地将专家知识与优化技术相结合，为资源有限的卫生系统规划提供了一种有效方法，既能最大化人口覆盖，又能纳入利益相关者的定性偏好。

Abstract: Ethiopia's Ministry of Health is upgrading health posts to improve access to essential services, particularly in rural areas. Limited resources, however, require careful prioritization of which facilities to upgrade to maximize population coverage while accounting for diverse expert and stakeholder preferences. In collaboration with the Ethiopian Public Health Institute and Ministry of Health, we propose a hybrid framework that systematically integrates expert knowledge with optimization techniques. Classical optimization methods provide theoretical guarantees but require explicit, quantitative objectives, whereas stakeholder criteria are often articulated in natural language and difficult to formalize. To bridge these domains, we develop the Large language model and Extended Greedy (LEG) framework. Our framework combines a provable approximation algorithm for population coverage optimization with LLM-driven iterative refinement that incorporates human-AI alignment to ensure solutions reflect expert qualitative guidance while preserving coverage guarantees. Experiments on real-world data from three Ethiopian regions demonstrate the framework's effectiveness and its potential to inform equitable, data-driven health system planning.

</details>


### [30] [BoxMind: Closed-loop AI strategy optimization for elite boxing validated in the 2024 Olympics](https://arxiv.org/abs/2601.11492)
*Kaiwen Wang,Kaili Zheng,Rongrong Deng,Qingmin Fan,Milin Zhang,Zongrui Li,Xuesi Zhou,Bo Han,Liren Chen,Chenyi Guo,Ji Wu*

Main category: cs.AI

TL;DR: BoxMind是一个用于拳击战术分析的闭环AI专家系统，通过定义原子击打事件、构建分层技术战术指标，并基于图模型预测比赛结果，将获胜概率梯度转化为可执行的战术调整，在2024年巴黎奥运会上成功应用。


<details>
  <summary>Details</summary>
Motivation: 格斗类运动如拳击在AI驱动的战术分析方面发展不足，主要因为动作动态复杂且缺乏结构化的战术表示方法。需要将非结构化视频数据转化为战略智能，弥合计算机视觉与竞技体育决策支持之间的差距。

Method: 1) 定义具有精确时间边界、空间和技术属性的原子击打事件，将比赛录像解析为18个分层技术战术指标；2) 提出基于图的预测模型，融合显性技术战术特征与可学习的时间变化潜在嵌入，捕捉拳手对战的动态；3) 将比赛结果建模为技术战术指标的可微函数，将获胜概率梯度转化为可执行的战术调整。

Result: 1) 结果预测模型在BoxerGraph测试集上达到69.8%的准确率，在奥运比赛上达到87.5%的准确率；2) 系统生成的战略建议表现出与人类专家相当的水平；3) 在2024年巴黎奥运会闭环部署中，直接帮助中国国家队取得3金2银的历史性成就。

Conclusion: BoxMind建立了一个可复制的范式，将非结构化视频数据转化为战略智能，弥合了计算机视觉与竞技体育决策支持之间的差距，为格斗类运动的战术分析提供了有效的AI解决方案。

Abstract: Competitive sports require sophisticated tactical analysis, yet combat disciplines like boxing remain underdeveloped in AI-driven analytics due to the complexity of action dynamics and the lack of structured tactical representations. To address this, we present BoxMind, a closed-loop AI expert system validated in elite boxing competition. By defining atomic punch events with precise temporal boundaries and spatial and technical attributes, we parse match footage into 18 hierarchical technical-tactical indicators. We then propose a graph-based predictive model that fuses these explicit technical-tactical profiles with learnable, time-variant latent embeddings to capture the dynamics of boxer matchups. Modeling match outcome as a differentiable function of technical-tactical indicators, we turn winning probability gradients into executable tactical adjustments. Experiments show that the outcome prediction model achieves state-of-the-art performance, with 69.8% accuracy on BoxerGraph test set and 87.5% on Olympic matches. Using this predictive model as a foundation, the system generates strategic recommendations that demonstrate proficiency comparable to human experts. BoxMind is validated through a closed-loop deployment during the 2024 Paris Olympics, directly contributing to the Chinese National Team's historic achievement of three gold and two silver medals. BoxMind establishes a replicable paradigm for transforming unstructured video data into strategic intelligence, bridging the gap between computer vision and decision support in competitive sports.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [31] [Event-Driven Market Co-Movement Dynamics in Critical Mineral Equities: An Empirical Framework Using Change Point Detection and Cross-Sectional Analysis](https://arxiv.org/abs/2601.10851)
*Haibo Wang*

Main category: econ.EM

TL;DR: 该研究使用变化点检测和横截面分析框架，分析ESG评级的关键矿物ETF市场行为，发现投资者在危机期间存在羊群效应，但在特定风险事件后转向反羊群行为。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解关键矿物投资市场的动态行为，特别是在全球事件冲击下投资者的协调行为变化。关键矿物对绿色转型至关重要，其市场行为对投资策略和政策制定有重要意义。

Method: 采用新颖的分析框架，结合变化点检测（PELT算法）和横截面分析。研究分析了2014年3月31日至2024年4月19日期间ESG评级的关键矿物ETF，以标普500为基准评估市场联动。

Result: 发现关键矿物投资在不同时间经历变化点，但有三个共同日期（2015年7月23日、2020年3月17日、2020年12月1日）与全球事件（石油市场冲击、COVID-19疫情、市场调整）对应。投资者在2015年和2020年危机后出现羊群效应，但在2020年底疫苗积极消息和2022年俄乌冲突后转向反羊群行为。

Conclusion: 投资者协调在市场下跌时最强，但在稳定期或重大发展后变化更大。反羊群行为在危机期间更明显，表明投资者对特定风险做出反应而非盲目跟随，特别是在地缘政治冲击下。这些动态对观察期长度敏感。

Abstract: This study examines market behavior in critical mineral investments using a novel analytical framework that combines change-point detection (PELT algorithm) with cross-sectional analysis. This research analyzes ESG-ranked critical mineral ETFs from March 31, 2014, to April 19, 2024, using the S&P 500 as a benchmark to evaluate market co-movements. The findings demonstrate that different critical mineral investments experienced change points at distinct times, but three major dates, July 23, 2015; March 17, 2020; and December 1, 2020, were common and aligned with global events such as the oil market shock, the COVID-19 pandemic, and later market adjustments. Herding behavior among investors increased after these shocks, following the 2015 and 2020 crises, but shifted to anti-herding after positive vaccine news in late 2020 and after the Russian invasion of Ukraine in 2022. The sensitivity analysis shows that investor coordination is strongest during market downturns but exhibits greater variation during stable periods or after major developments, with these dynamics sensitive to the length of the observation period. Additionally, anti-herding became more apparent during crises, suggesting investors reacted to specific risks rather than moving in lockstep, especially in response to geopolitical shocks.

</details>


### [32] [Beyond Validity: SVAR Identification Through the Proxy Zoo](https://arxiv.org/abs/2601.11195)
*Jiaming Huang,Luca Neri*

Main category: econ.EM

TL;DR: 提出广义排序限制框架，在SVAR模型中使用多个代理变量时，通过连续的质量参数来界定代理变量与目标冲击和非目标冲击的相关性，实现稳健识别。


<details>
  <summary>Details</summary>
Motivation: 传统SVAR模型中的代理变量方法通常假设代理变量与目标冲击完全外生，但在实际应用中存在多个代理变量（代理变量动物园）且质量参差不齐，需要更稳健的识别方法。

Method: 引入广义排序限制，通过连续的质量参数界定每个代理变量与目标冲击和非目标冲击的相对相关性，结合标准的符号限制和叙事限制，部分识别代理质量参数。

Result: 模拟研究表明，基于符号限制构建的代理变量可能导致有偏的代理-SVAR估计，而本文方法能提供信息丰富且稳健的识别集。应用于美国货币政策分析，展示了框架的实证相关性和计算可行性。

Conclusion: 提出的框架为处理代理变量动物园问题提供了透明、稳健的识别方法，通过敏感性分析和诊断工具，使研究人员能够评估实证结论对代理外生性假设和代理组合的依赖程度。

Abstract: This paper develops a framework for robust identification in SVARs when researchers face a zoo of proxy variables. Instead of imposing exact exogeneity, we introduce generalized ranking restrictions (GRR) that bound the relative correlation of each proxy with the target and non-target shocks through a continuous proxy-quality parameter. Combining GRR with standard sign and narrative restrictions, we characterize identified sets for structural impulse responses and show how to partially identify the proxy-quality parameter using the joint information contained in the proxy zoo. We further develop sensitivity and diagnostic tools that allow researchers to assess transparently how empirical conclusions depend on proxy exogeneity assumptions and the composition of the proxy zoo. A simulation study shows that proxies constructed from sign restrictions can induce biased proxy-SVAR estimates, while our approach delivers informative and robust identified sets. An application to U.S.\ monetary policy illustrates the empirical relevance and computational tractability of the framework.

</details>


### [33] [Likelihood-Based Ergodicity Transformations in Time Series Analysis](https://arxiv.org/abs/2601.11237)
*Anthony Britto*

Main category: econ.EM

TL;DR: 提出一种基于似然的方法来估计遍历性变换，以处理时间序列的非遍历行为，该方法与标准模型兼容，并通过模拟和实际数据验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 时间序列常表现出非遍历行为，这给预测和推断带来困难。传统方法在处理非遍历性时存在局限，需要一种更通用的方法来估计遍历性变换。

Method: 提出基于似然的遍历性变换估计方法，与高斯过程、ARMA、GARCH等标准模型广泛兼容。通过几何和算术布朗运动的模拟研究验证方法恢复已知遍历性变换的能力。

Result: 模拟研究表明该方法能有效恢复已知的遍历性变换。在FRED-QD宏观经济数据库的案例研究中，结合遍历性变换相比传统变换或简单设定能带来有意义的改进。

Conclusion: 提出的基于似然的遍历性变换估计方法能有效处理时间序列的非遍历性，在实际应用中相比传统方法能提供更好的性能，具有广泛的应用价值。

Abstract: Time series often exhibit non-ergodic behaviour that complicates forecasting and inference. This article proposes a likelihood-based approach for estimating ergodicity transformations that addresses such challenges. The method is broadly compatible with standard models, including Gaussian processes, ARMA, and GARCH. A detailed simulation study using geometric and arithmetic Brownian motion demonstrates the ability of the approach to recover known ergodicity transformations. A further case study on the large macroeconomic database FRED-QD shows that incorporating ergodicity transformations can provide meaningful improvements over conventional transformations or naive specifications in applied work.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [34] [A Note on Harmonic Underspecification in Log-Normal Trigonometric Regression](https://arxiv.org/abs/2601.10919)
*Michael T. Gorczyca*

Main category: stat.AP

TL;DR: 比较两种处理非正态分布生物节律数据的方法：对数变换后最小二乘三角回归与广义线性模型三角回归，在谐波数不足设定时，前者能产生无偏参数估计而后者不能。


<details>
  <summary>Details</summary>
Motivation: 生物节律数据分析常使用最小二乘三角回归，但当响应变量非正态分布时，研究者要么先变换响应变量再应用最小二乘三角回归，要么将三角回归扩展到广义线性模型框架。本研究旨在比较这两种方法在振荡谐波数不足设定时的表现。

Method: 假设数据在等间距实验设计下采样，且对数链接函数适用于GLM。当响应服从广义伽马分布时，比较对数变换后最小二乘三角回归（对数正态三角回归）与GLM三角回归在谐波数不足设定时的表现。将两种方法应用于皮质醇水平数据进行分析。

Result: 当响应服从广义伽马分布时，即使振荡谐波数不足设定，对数变换后最小二乘三角回归仍能产生无偏的振荡谐波参数估计。而GLM需要正确设定才能产生无偏估计。在皮质醇数据应用中，只有对数正态三角回归产生的参数估计对指定的振荡谐波数具有不变性。当指定足够多的振荡谐波时，两种方法产生相同的振荡谐波参数估计。

Conclusion: 在处理非正态分布生物节律数据时，当振荡谐波数可能不足设定时，对数变换后最小二乘三角回归比GLM三角回归更具稳健性，能产生无偏参数估计，而GLM需要完全正确设定才能达到相同效果。

Abstract: Analysis of biological rhythm data often involves performing least squares trigonometric regression, which models the oscillations of a response over time as a sum of sinusoidal components. When the response is not normally distributed, an investigator will either transform the response before applying least squares trigonometric regression or extend trigonometric regression to a generalized linear model (GLM) framework. In this note, we compare these two approaches when the number of oscillation harmonics is underspecified. We assume data are sampled under an equispaced experimental design and that a log link function would be appropriate for a GLM. We show that when the response follows a generalized gamma distribution, least squares trigonometric regression with a log-transformed response, or log-normal trigonometric regression, produces unbiased parameter estimates for the oscillation harmonics, even when the number of oscillation harmonics is underspecified. In contrast, GLMs require correct specification to produce unbiased parameter estimates. We apply both methods to cortisol level data and find that only log-normal trigonometric regression produces parameter estimates that are invariant to the number of specified oscillation harmonics. Additionally, when a sufficiently large number of oscillation harmonics is specified, both methods produce identical parameter estimates for the oscillation harmonics.

</details>


### [35] [Analyzing Residential Speeding Using Connected Vehicle Data: A Case Study in Charlottesville, VA Area](https://arxiv.org/abs/2601.10974)
*Shi Feng,B. Brian Park,Andrew Mondschein*

Main category: stat.AP

TL;DR: 利用联网车辆数据分析住宅道路超速行为，发现超速分布高度偏斜，夜间超速是白天的27倍，特定路段存在高风险，需要空间和行为干预。


<details>
  <summary>Details</summary>
Motivation: 传统执法和规划工具在识别住宅道路超速行为方面存在局限，需要利用联网车辆数据来更全面、可扩展地分析超速行为，为交通安全提供数据支持。

Method: 开发可扩展的数据处理管道，处理轨迹数据并补充缺失的限速信息，在OpenStreetMap的way ID级别生成汇总分析，以弗吉尼亚州夏洛茨维尔市的联网车辆数据为案例研究。

Result: 38%的路段至少有一次激进超速，20%的路段至少有一次鲁莽超速；夜间超速发生率是白天的27倍；部分路段同时出现在激进和鲁莽超速前10名，表明存在高风险住宅道路。

Conclusion: 该框架为交通安全分析奠定基础，展示了远程信息处理数据在创建更安全、更宜居社区方面的潜力，支持空间和行为干预的需求，为政策和规划提供丰富基础。

Abstract: This study uses connected vehicle data to analyze speeding behavior on residential roads. A scalable pipeline processes trajectory data and supplements missing speed limits to generate summaries at OpenStreetMap's way ID level. The findings reveal a highly skewed distribution of both aggressive and reckless speeding. Based on a case study of Charlottesville, VA's connected vehicle data on residential roads, we found that 38% of segments had at least one instance of aggressive speeding, and 20% had at least one instance of reckless speeding. In addition, night time speeding is 27 times more prevalent than day time, and extreme violations on specific road segments highlight how severe the issue can be. Several segments rank among the top 10 for both aggressive and reckless speedings, indicating that there exist high-risk residential roads. These findings support the need for both spatial and behavioral interventions. The analysis provides a rich foundation for policy and planning, offering a valuable complement to traditional enforcement and planning tools. In conclusion, this framework sets the foundation for future applications in traffic safety analytics, demonstrating the growing potential of telematics data to inform safer, more livable communities.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [36] [Gamifying Cyber Governance: A Virtual Escape Room to Transform Cybersecurity Policy Education](https://arxiv.org/abs/2601.10852)
*Khondokar Fida Hasan,William Hughes,Adrita Rahman*

Main category: cs.CY

TL;DR: 开发用于网络安全治理与政策教育的虚拟密室逃脱游戏，通过3D沉浸式环境模拟公司场景，提升学生参与度与实践能力


<details>
  <summary>Details</summary>
Motivation: 严肃游戏作为有效的教学工具越来越受欢迎，但网络安全治理与政策教育需要更创新的方法来连接理论与实践，同时需要应对生成式AI对学术诚信的挑战

Method: 设计并实现一个专门用于网络安全治理与政策教育的虚拟密室逃脱游戏，采用3D沉浸式环境模拟虚拟公司场景，包含三个交互区域，每个区域针对特定教育目标提供基于场景的问题

Result: 学生调查显示强烈的积极反馈，学生在参与度、实践理解和网络安全治理与政策概念的热情方面有显著提升，证明了游戏化在网络安全教育中的有效性

Conclusion: 虚拟密室逃脱游戏成功地将游戏化学习与沉浸式技术相结合，创造了既有效又对生成式AI干预具有韧性的交互式学习材料，为网络安全教育提供了创新方法

Abstract: Serious games are gaining popularity as effective teaching and learning tools, providing engaging, interactive, and practical experiences for students. Gamified learning experiences, such as virtual escape rooms, have emerged as powerful tools in bridging theory and practice, fostering deeper understanding and engagement among students. This paper presents the design, implementation, and evaluation of a virtual escape room tailored specifically for cybersecurity governance and policy education. Developed as a 3D immersive environment, the escape room simulates a virtual company scenario to facilitate risk-informed cyber policy development. It consists of three interactive zones, each offering distinct sets of scenario-based problems that target specific educational objectives. Through these zones, students analyze cybersecurity risks, match security frameworks, and draft appropriate policies, thereby developing critical thinking, decision-making skills, and practical cybersecurity competencies. The primary contribution of this work lies in its innovative integration of game-based learning and immersive technology to create robust, interactive learning materials that are also resilient to generative AI interventions, thereby maintaining academic integrity. Additionally, the escape room provides students with exposure to real-world cybersecurity scenarios in a virtual office environment that meets industry expectations. Results from a student survey indicated strong positive feedback, highlighting significant improvements in students engagement, practical understanding, and enthusiasm toward cybersecurity governance and policy concepts, underscoring the effectiveness and potential of gamification in cybersecurity education.

</details>


### [37] [Modeling Multi-Party Interaction in Couples Therapy: A Multi-Agent Simulation Approach](https://arxiv.org/abs/2601.10970)
*Canwen Wang,Angela Chen,Catherine Bao,Siwei Jin,Yee Kit Chan,Jessica R Mindel,Sijia Xie,Holly Swartz,Tongshuang Wu,Robert E Kraut,Haiyi Zhu*

Main category: cs.CY

TL;DR: 开发了一个多模态、多代理的模拟系统，用于训练夫妻治疗师，通过模拟夫妻互动阶段和需求-退缩沟通循环，为学员提供低风险的实践环境。


<details>
  <summary>Details</summary>
Motivation: 传统的夫妻治疗师培训方法（如教科书和角色扮演）难以捕捉真实夫妻互动的复杂性和情感细微差别，需要更有效的培训工具。

Method: 创建了一个多模态、多代理的模拟系统，模拟夫妻治疗中的多方互动，特别关注需求-退缩沟通循环和六个夫妻互动阶段。

Result: 在21名美国持证治疗师的评估中，参与者能够识别出设计的代理行为（阶段和需求-退缩循环），并且认为实验系统在整体真实性和代理反应方面优于基线系统。

Conclusion: 这是首个用于培训夫妻治疗师的多代理框架，为人机交互技术与夫妻治疗融合的未来研究奠定了基础。

Abstract: Couples therapy, or relationship counseling, helps partners resolve conflicts, improve satisfaction, and foster psychological growth. Traditional approaches to training couples therapists, such as textbooks and roleplay, often fail to capture the complexity and emotional nuance of real couple dynamics. We present a novel multimodal, multi-agent simulation system that models multi-party interactions in couples therapy. Informed by our systematic research, this system creates a low-stakes environment for trainee therapists to gain valuable practical experience dealing with the critical demand-withdraw communication cycle across six couple-interaction stages. In an evaluation study involving 21 US-based licensed therapists, participants blind to conditions identified the engineered agent behaviors (i.e., the stages and the demand-withdraw cycle) and rated overall realism and agent responses higher for the experimental system than the baseline. As the first known multi-agent framework for training couples therapists, our work builds the foundation for future research that fuses HCI technologies with couples therapy.

</details>


### [38] [Evaluating 21st-Century Competencies in Postsecondary Curricula with Large Language Models: Performance Benchmarking and Reasoning-Based Prompting Strategies](https://arxiv.org/abs/2601.10983)
*Zhen Xu,Xin Guan,Chenxi Shi,Qinhao Chen,Renzhe Yu*

Main category: cs.CY

TL;DR: 研究评估大语言模型在课程分析中映射21世纪能力的效果，提出Curricular CoT提示策略改进教学推理，发现开放权重LLM在粗粒度任务上表现接近专有模型，但细粒度推理仍不及人类水平。


<details>
  <summary>Details</summary>
Motivation: 生成式AI变革背景下，高等教育需要评估21世纪能力在课程中的嵌入程度及与职场社会需求的匹配度。现有课程分析方法缺乏对教学推理的深度理解，大语言模型在此领域的潜力尚未充分探索。

Method: 使用7,600个人工标注的课程-能力对齐评分，评估不同课程文档的信息价值，基准测试通用LLM的映射能力，分析错误模式，并提出基于推理的Curricular CoT提示策略来增强LLM的教学推理。

Result: 详细的教学活动描述是最具信息价值的课程文档类型；开放权重LLM在粗粒度任务上达到与专有模型相当的准确度，适合机构规模化应用；但所有模型在细粒度教学推理上都未达到人类精度；Curricular CoT通过减少教学关键词推断偏差和改进长文本中细微教学证据的检测，带来适度改进。

Conclusion: 机构课程文档具有未开发的潜力，研究为推进AI驱动的课程分析提供了实证基础，Curricular CoT策略能增强LLM的教学推理能力，但细粒度推理仍需进一步改进。

Abstract: The growing emphasis on 21st-century competencies in postsecondary education, intensified by the transformative impact of generative AI, underscores the need to evaluate how these competencies are embedded in curricula and how effectively academic programs align with evolving workforce and societal demands. Curricular Analytics, particularly recent generative AI-powered approaches, offer a promising data-driven pathway. However, analyzing 21st-century competencies requires pedagogical reasoning beyond surface-level information retrieval, and the capabilities of large language models in this context remain underexplored. In this study, we extend prior curricular analytics research by examining a broader range of curriculum documents, competency frameworks, and models. Using 7,600 manually annotated curriculum-competency alignment scores, we assess the informativeness of different curriculum sources, benchmark general-purpose LLMs for curriculum-to-competency mapping, and analyze error patterns. We further introduce a reasoning-based prompting strategy, Curricular CoT, to strengthen LLMs' pedagogical reasoning. Our results show that detailed instructional activity descriptions are the most informative type of curriculum document for competency analytics. Open-weight LLMs achieve accuracy comparable to proprietary models on coarse-grained tasks, demonstrating their scalability and cost-effectiveness for institutional use. However, no model reaches human-level precision in fine-grained pedagogical reasoning. Our proposed Curricular CoT yields modest improvements by reducing bias in instructional keyword inference and improving the detection of nuanced pedagogical evidence in long text. Together, these findings highlight the untapped potential of institutional curriculum documents and provide an empirical foundation for advancing AI-driven curricular analytics.

</details>


### [39] [Fairness in Healthcare Processes: A Quantitative Analysis of Decision Making in Triage](https://arxiv.org/abs/2601.11065)
*Rachmadita Andreswari,Stephan A. Fahrenkrog-Petersen,Jan Mendling*

Main category: cs.CY

TL;DR: 本研究提出一种流程挖掘方法，将急诊分诊的实际事件日志与正义理论维度相结合，评估医疗决策中的公平性。


<details>
  <summary>Details</summary>
Motivation: 自动化决策中的公平性在医疗紧急分诊等高压场景中至关重要，但目前缺乏基于实证医疗数据的公平性评估研究，且现有方法未能充分结合正义理论维度。

Method: 使用MIMICEL事件日志（源自MIMIC-IV ED），分析时间、重复操作、偏差和决策等流程结果，评估年龄、性别、种族、语言和保险等因素的影响，采用Kruskal-Wallis、卡方检验和效应大小测量，并将结果映射到正义维度。

Result: 研究结果揭示了高急性和亚急性情况下潜在不公平性的具体方面，为开发概念框架提供了实证基础。

Conclusion: 本研究通过将实际事件日志与正义理论维度相结合，为医疗领域负责任、公平感知的流程挖掘研究提供了实证见解和概念框架支持。

Abstract: Fairness in automated decision-making has become a critical concern, particularly in high-pressure healthcare scenarios such as emergency triage, where fast and equitable decisions are essential. Process mining is increasingly investigating fairness. There is a growing area focusing on fairness-aware algorithms. So far, we know less how these concepts perform on empirical healthcare data or how they cover aspects of justice theory. This study addresses this research problem and proposes a process mining approach to assess fairness in triage by linking real-life event logs with conceptual dimensions of justice. Using the MIMICEL event log (as derived from MIMIC-IV ED), we analyze time, re-do, deviation and decision as process outcomes, and evaluate the influence of age, gender, race, language and insurance using the Kruskal-Wallis, Chi-square and effect size measurements. These outcomes are mapped to justice dimensions to support the development of a conceptual framework. The results demonstrate which aspects of potential unfairness in high-acuity and sub-acute surface. In this way, this study contributes empirical insights that support further research in responsible, fairness-aware process mining in healthcare.

</details>


### [40] [How Do Technological Prototypes in the Food Industry Impact People's Perception? Insights from the MUSAE "GROW, COOK, CODE" Final Exhibition](https://arxiv.org/abs/2601.11169)
*Francesco Semeraro,Filip Bečanović,Maja Trumić,Kosta Jovanović,Angelo Cangelosi*

Main category: cs.CY

TL;DR: 展览调查显示，与科技原型互动能提升人们对技术信任、环境挑战、饮食习惯及身心健康的意识


<details>
  <summary>Details</summary>
Motivation: 评估MUSAE最终展览中科技原型互动对公众在技术信任、环境问题、饮食习惯和身心健康方面认知的影响

Method: 在MUSAE最终展览期间进行问卷调查，评估参观者与展示的科技原型互动后的感知变化

Result: 展览显著提升了人们对技术信任、环境挑战、饮食习惯和身心健康等方面的意识

Conclusion: 科技展览能有效提高公众对重要社会议题的认知和意识，具有积极的社会影响

Abstract: This work reports the results of the survey carried out during the MUSAE final exhibition to assess its impact on people's perception of aspects like trust in technology, environmental challenges, eating habits and potential increase of mental and physical health while interacting with the technological prototypes exposed during the exhibition. The results show that the exhibition positively increased people's awareness regarding these aspects.

</details>


### [41] [Epistemic Control and the Normativity of Machine Learning-Based Science](https://arxiv.org/abs/2601.11202)
*Emanuele Ratti*

Main category: cs.CY

TL;DR: 该论文探讨机器学习在科学中的应用是否会将人类科学家排除在科学循环之外，提出了"认知控制"的概念并反驳了Humphreys的悲观观点。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在科学领域的应用日益增多，Paul Humphreys认为由于ML系统的特定特性，人类科学家正被排除在科学循环之外。本文旨在探讨这一观点在多大程度上成立。

Method: 首先提出"认知控制"的概念，并基于技术哲学文献识别出认知控制的两个条件：追踪和溯源。然后以此框架分析Humphreys的观点，最后构建一个更细致的ML科学中认知控制的观点。

Result: 作者反驳了Humphreys的悲观观点，认为人类科学家并未完全被排除在科学循环之外。通过认知控制的追踪和溯源条件，构建了一个更平衡的理解框架。

Conclusion: ML在科学中的应用确实改变了科学实践，但通过适当的认知控制机制，人类科学家仍能保持在科学循环中。需要更细致地理解ML科学中的认知控制，而不是简单地认为人类被排除在外。

Abstract: The past few years have witnessed an increasing use of machine learning (ML) systems in science. Paul Humphreys has argued that, because of specific characteristics of ML systems, human scientists are pushed out of the loop of science. In this chapter, I investigate to what extent this is true. First, I express these concerns in terms of what I call epistemic control. I identify two conditions for epistemic control, called tracking and tracing, drawing on works in philosophy of technology. With this new understanding of the problem, I then argue against Humphreys pessimistic view. Finally, I construct a more nuanced view of epistemic control in ML-based science.

</details>


### [42] [Capacity Constraints Make Admissions Processes Less Predictable](https://arxiv.org/abs/2601.11513)
*Evan Dong,Nikhil Garg,Sarah Dean*

Main category: cs.CY

TL;DR: 论文研究机器学习在招生决策预测中的局限性，指出由于容量限制，招生决策具有相互依赖性，导致预测性能在申请人群体分布变化时下降。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型常用于预测招生结果，但招生决策过程与传统机器学习范式有本质不同。招生决策受容量限制，一个学生的录取结果取决于其他申请者，这种相互依赖性会影响预测性能。

Method: 提出两个理论概念：不稳定性（衡量引入单个新申请者时现有决策的变化程度）和变异性（衡量可能改变决策的学生数量）。在纽约市高中匹配系统的个体层面招生数据上进行实证分析。

Result: 机器学习性能随着申请人群体与训练数据差异增大而下降。使用更不稳定和可变决策规则的学校性能下降更大。实证结果验证了理论分析。

Conclusion: 招生决策的相互依赖性导致个体录取概率预测不可靠，特别是在申请人群体分布变化时。研究对机器学习在招生决策中的应用可靠性提出质疑。

Abstract: Machine learning models are often used to make predictions about admissions process outcomes, such as for colleges or jobs. However, such decision processes differ substantially from the conventional machine learning paradigm. Because admissions decisions are capacity-constrained, whether a student is admitted depends on the other applicants who apply. We show how this dependence affects predictive performance even in otherwise ideal settings. Theoretically, we introduce two concepts that characterize the relationship between admission function properties, machine learning representation, and generalization to applicant pool distribution shifts: instability, which measures how many existing decisions can change when a single new applicant is introduced; and variability, which measures the number of unique students whose decisions can change. Empirically, we illustrate our theory on individual-level admissions data from the New York City high school matching system, showing that machine learning performance degrades as the applicant pool increasingly differs from the training data. Furthermore, there are larger performance drops for schools using decision rules that are more unstable and variable. Our work raises questions about the reliability of predicting individual admissions probabilities.

</details>


### [43] [Generative AI Purpose-built for Social and Mental Health: A Real-World Pilot](https://arxiv.org/abs/2511.11689)
*Thomas D. Hull,Lizhe Zhang,Patricia A. Arean,Matteo Malgaroli*

Main category: cs.CY

TL;DR: 评估用于心理健康的基础模型GAI聊天机器人，在自然观察研究中显示能显著改善抑郁、焦虑症状，并提升希望、社交支持等指标，安全性良好。


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI心理健康聊天机器人能否提供安全、个性化、可扩展的心理健康支持，验证其在真实世界环境中的效果。

Method: 单臂自然观察研究，2025年5-9月招募成人用户，收集人口统计、心理健康症状、社交连接等数据，每2周重复测量至6周，10周进行最终随访，使用效应量分析和增长混合模型。

Result: PHQ-9和GAD-7显著降低且效果持续；希望、行为激活、社交互动、孤独感、感知社会支持显著改善；参与度高且预测效果；工作联盟与传统护理相当；安全护栏有效运作（76个会话被标记风险）。

Conclusion: GAI心理健康基础模型能提供可访问、有吸引力、有效且安全的心理健康支持，为未来真实世界研究提供初步证据。

Abstract: Generative artificial intelligence (GAI) chatbots built for mental health could deliver safe, personalized, and scalable mental health support. We evaluate a foundation model designed for mental health. Adults completed mental health measures while engaging with the chatbot between May 15, 2025 and September 15, 2025. Users completed an opt-in consent, demographic information, mental health symptoms, social connection, and self-identified goals. Measures were repeated every two weeks up to 6 weeks, and a final follow-up at 10 weeks. Analyses included effect sizes, and growth mixture models to identify participant groups and their characteristic engagement, severity, and demographic factors. Users demonstrated significant reductions in PHQ-9 and GAD-7 that were sustained at follow-up. Significant improvements in Hope, Behavioral Activation, Social Interaction, Loneliness, and Perceived Social Support were observed throughout and maintained at 10 week follow-up. Engagement was high and predicted outcomes. Working alliance was comparable to traditional care and predicted outcomes. Automated safety guardrails functioned as designed, with 76 sessions flagged for risk and all handled according to escalation policies. This single arm naturalistic observational study provides initial evidence that a GAI foundation model for mental health can deliver accessible, engaging, effective, and safe mental health support. These results lend support to findings from early randomized designs and offer promise for future study of mental health GAI in real world settings.

</details>
